\section{Methods} 


% \begin{figure}[!tbp]
%   \centering
%   \includegraphics[width=0.5\textwidth]{images/cluster.pdf}  
%   \caption{}
%   \label{fig:moral_cluster}
% \end{figure}


\begin{algorithm*}
\caption{AMULED Framework}\label{alg:pseudocode}
\begin{algorithmic}[1]
\Require Set of moral clusters and initial parameters $\actorParams$, $\criticParams$
\State Initialize policy $\pi_{\actorParams}$ and value function $V_{\criticParams}$ using Proximal Policy Optimization(PPO)~\cite{schulman2017proximal}
\If{Base model}
    \State $c=0$ \Comment{see Eq.~\eqref{eq:reward}}
\EndIf
\For{timesteps $t$ in $T_\mathit{horizon}$}
    \Procedure {Collect trajectories}{}

    \State Collect trajectories of state-action-reward tuples $(s_t, a_t, r_t)$ from the environment

    \If{Fine-tuning with AI Feedback}
        \State Initialize base policy $\pi_{\mathit{base}}$ from previously trained parameters
        \State Redefine reward $r_t := r_{\mathit{feedback}}=\baseReward + \rewardShaping$ new reward for fine-tuning with: \Comment{Eq.~\eqref{eq:feedback}}
        \State $\baseReward = -\lambda_{\mathit{KL}} D_{\mathit{KL}}\left(\fineTuneModel(a | s) \parallel \baseModel(a | s)\right)$
        \State $\rewardShaping = f_{\mathit{BA}}(\mathbf{B})$ \Comment{for $f_{BA}$, see Eqs. \eqref{eq:app_DMM}--\eqref{eq:app_BPA}}

        \EndIf
    \EndProcedure

    \Procedure{PPO Training Loop}{}
        \If{$(t \mod N_\mathrm{batch})=0$}
            \State \textbf{Policy Update:} $\actorParams_{k+1} \leftarrow \arg\min_{\actorParams} \mathbb{E}_{t} \left[ \frac{\pi_{\actorParams}(a | s)}{\pi_{\actorParams_{k}}(a | s)} A^{\pi_{\actorParams_{k}}}(s, a) \cdot g(\epsilon, A^{\pi_{\actorParams_{k}}}(s, a)) \right]$ \Comment{Eq. \eqref{eqn:objective}}
            
            \State \textbf{Value Function Update:} $\criticParams_{k+1} \leftarrow \arg \min_{\criticParams} \mathbb{E}_{t} \left[V_{\criticParams}(s_t) - R_t\right]^2$ \Comment{Eq.~\eqref{eqn:value_function}}
        \EndIf
    \EndProcedure
    
\EndFor
\end{algorithmic}
\end{algorithm*}

The AMULED framework develops a two-layered reinforcement learning (RL) model to balance primary and ethical objectives (see Algorithm~\ref{alg:pseudocode}). In the initial layer, the agent learns a policy for its primary task. Then, five distinct moral clusters evaluate each action’s ethical appropriateness, assigning belief values based on the environment state. This state is encoded as a text prompt and processed with cluster-specific context to inform a shaping reward. The shaping reward, blended with the environment reward, adjusts the agent’s actions towards ethical goals via reinforcement learning with feedback. In AMULED, we evaluate the effectiveness of the ethics shaping algorithm through four experiments, focusing on two key tasks (Appendix~\ref{app:scenarios}): (1) Finding Milk, where the agent performs a route planning task with additional ethical tasks, and (2) Driving and Rescuing, a more complex task involving a larger number of states that simulate realistic decision-making. In the following sections, we detail individual components of the AMULED framework. The AMULED source code will be made publicly available at \href{https://github.com/xxxxxx/moral_agent}{https://github.com/xxxxx/moral\_agent} after publication.


% Below we presents the pseudo-code of the AMULED framework. This algorithm employs PPO to iteratively update the policy and value function based on environmental feedback. The framework integrates reward shaping to balance primary and secondary objectives and incorporates fine-tuning through reinforcement learning with human-like feedback (RLHF) from moral clusters, using KL divergence and belief aggregation to guide agent behavior.


\subsection{Development of Moral Clusters}
The development of the moral clusters framework is grounded in a systematic analysis of ethical theories drawn from both classical and contemporary philosophical literature. Our objective is to construct a comprehensive yet practical structure that considers major ethical paradigms, serving as a foundation for implementing ethical reasoning in AI systems \cite{wallach2008moral, anderson2011machine}.

We began with an extensive review of ethical theories, focusing on widely recognized works that span the spectrum of moral philosophy. This includes consequentialist, deontological, virtue-based, care-oriented, and justice-focused ethical frameworks \cite{powers2006prospects}. Key sources encompass seminal texts such as John Stuart Mill's \textit{Utilitarianism}, Immanuel Kant's deontological writings \cite{alexander2007deontological}, Aristotle's \textit{Nicomachean Ethics}, Carol Gilligan's work on care ethics \cite{gilligan2014moral}, and John Rawls' \textit{[A] Theory of Justice} \cite{pogge2007john}. This comprehensive review provides an overview of foundational principles and nuances within each ethical tradition \cite{dignum2018ethics}.

\subsubsection{Cluster Identification}
Drawing from the reviewed literature, we identified five primary clusters of ethical thought: Consequentialist Ethics, Deontological Ethics, Virtue Ethics, Care Ethics, and Social Justice Ethics (Fig.~\ref{fig:schematic}c). These clusters were selected because they represent distinct approaches to ethical reasoning, encompass a broad spectrum of moral considerations, and are frequently discussed in both philosophical discourse and applied ethics \cite{tolmeijer2020implementations}. By organizing ethical theories into these clusters, we aim to capture the diversity of moral perspectives that could inform AI decision-making \cite{conitzer2017moral}. This approach aligns with recent research highlighting the importance of comprehensive ethical frameworks in AI systems, particularly when dealing with decision-making dilemmas \cite{duan2019artificial,mahajan2024executioner}.

\subsubsection{Framework Development}
For each identified cluster, we developed a structured framework (details in Appendix~\ref{app:MoralClusters}) designed to balance philosophical depth with practical applicability for AI systems \cite{dennis2016formal}. This framework includes a general description of the ethical approach, the key principles that unify theories within the cluster, representative ethical theories, key concepts inherent to each theory, and decision factors that could potentially be operationalized in an AI system. The selection of representative theories and concepts is based on their prominence in the literature and their potential for translation into computational models \cite{bench2020ethical}.
In the Consequentialist Ethics cluster, for example, we focus on the principle of maximizing overall good, with utilitarianism serving as a representative theory \cite{abel2016reinforcement}. Key concepts such as utility, consequences, and the greatest happiness principle are emphasized, and decision factors involve assessing the outcomes of actions in terms of their utility contributions. Similar detailed frameworks were developed for the other clusters, ensuring that each ethical approach is thoroughly represented and that the essential elements could be mapped to computational considerations \cite{malle2016integrating}.

The initial framework has to undergo several iterations of refinement to enhance its coherence and applicability. We critically examine the internal consistency within each cluster and ensure clear distinctions between clusters to prevent conceptual overlap. This involves verifying that the selected theories adequately represent the diversity within each ethical approach and refining key concepts and decision factors to capture the essence of each theory while remaining amenable to quantification for AI applications. 

% \subsubsection*{Limitations and Assumptions}

% While our framework aims to comprehensively represent major ethical paradigms for AI decision-making, we acknowledge that categorizing ethical theories into distinct clusters may simplify the complexities and intersections among different moral philosophies. This structured approach, however, is intended to facilitate the practical implementation of ethical reasoning in AI systems. Operationalizing these theories for AI applications involves abstracting intricate philosophical ideas \cite{russell2015research}, but we have tried to preserve the core principles of each ethical approach. The selection of representative theories and decision factors was based on their prominence and relevance in the literature, recognizing that some degree of subjective judgment is inherent in this process \cite{allen2000prolegomena}.

% Our framework is based on key assumptions. We believe that these five clusters effectively encompass the major streams of ethical thought pertinent to AI decision-making and that the selected theories within each cluster are sufficiently representative of their respective ethical approaches. We also assume that the identified key concepts and decision factors can be meaningfully translated into computational models, providing a solid foundation for future research and development in this area \cite{gips2011towards}.

\subsection{Modeling Morality as Intrinsic Reward: Belief Probability Assignment}
We draw inspiration from multi-sensor fusion literature~\cite{Chen2023}, where measurements from different sensors can be converted into some plausibility estimate of what is defective in a system.
The beliefs from a single sensor are termed Basic Belief Assignment (BBA), and combining the BBAs from multiple sensors yields a Basic Probability Assignment (BPA)~\cite{Zhao2022}.
Following this analogy, we envision the sensors as moral clusters, each with a set of probability estimates on which actions are best given the state.
We can express this mathematically as the belief $B_{i,j}(s)\iff B_{i,j}$ of agent $i$ on how good action $j$ is, given the current state $s$. 
Thus, given $M$ moral clusters acting as agents, and an environment with $A$ actions, we can form a matrix of belief values.
To make this belief matrix useful, we need a belief aggregation function $f_\mathit{BA}(\mathbf{B})\to r_j$ that maps the belief values of agents into a reward $r_j$ for each action $j$.

Multiple approaches can be used to serve as the aggregation function $f_\mathit{BA}(.)$.
For example, $f$ can be a majority-wins vote aggregation, where each agent "votes" on the action $\mathrm{vote}_i=\mathrm{argmax}_jB_{i,j}$, and the action with the most votes gets assigned $r=1$.
One could also use a maximum-belief approach by defining $\Tilde{B}_{j} = \max_i B_{i,j}$, and setting $r_j=\Tilde{B}_j/\sum_i{\Tilde{B}_i}$.
Lastly, one can use a weighted average: $r_j= \sum_i{w_i B_{i,j}} / \sum_i{w_i}$, where $w_i$ are weights of each agent. With $w_i=1$, this weighted average reduces to the mean belief of an action.
One expects these simple aggregation functions to work best when these is a strong agreement between agents on the probability values of each action.
However, problems will arise when there are discrepancies between one or more agents. 
Thus, especially when ethics is involved, there is a need to treat such discrepancies with a more nuanced belief aggregation method.

In contrast to the above aggregation methods, \citet{xiao2019multi} introduced a different aggregation method that makes use of a Belief Jensen–Shannon Divergence (BJSD) method to systematically measure the discrepancies and conflicts among BBAs, processes these measurements (details in Appendix~\ref{app:belief_fusion}), and finally employs Dempster--Schaefer theory (DST) to arrive at the BPA \citep{dempster2008upper}.
This allows for a nuanced aggregation that accounts for the uncertainties inherent in human ethics, leading to more informed and comprehensive decision-making in moral dilemmas.

In multi-sensor data fusion, combining information from diverse sources is critical, yet challenging, particularly when addressing conflicting and uncertain data. Each sensor provides valuable insights into decision-making, but also introduces its own uncertainties. Similarly, different moral clusters, such as deontology, virtue ethics, and consequentialism, can be seen as "sensors" that guide ethical decision-making. These moral perspectives can conflict with one another and carry their own uncertainties.
Treating these moral frameworks as sensors allows us to apply techniques from multi-sensor data fusion, particularly the BJSD and DST methods to effectively measure and moderate conflicts between evidence sources by incorporating both credibility and uncertainty metrics into the fusion process.
% By treating moral frameworks like sensors, we can apply methods like Belief Jensen–Shannon divergence to reconcile these complexities in human decision-making. Dempster--Shafer Theory is widely utilized due to its flexibility in modeling uncertainty without prior probabilities; however, it can produce counter-intuitive results when highly conflicting evidence arises. To address this, Xiao's approach introduces a Belief Jensen--Shannon (BJS) Divergence method \cite{xiao2019multi}, which effectively measures and moderates conflict between evidence sources by incorporating both credibility and uncertainty metrics into the fusion process.
This approach begins by computing the BJS divergence. Let $A_j$ be a hypothesis of the belief function \mbox{$m_i:=[B_{i,A_1}, B_{i,A_2},\dots B_{i,A_j}]$}, and let $m_i$ and $m_k$ be two BBAs.
%on the same frame of discernment $\Omega$, containing $N$ mutually exclusive and exhaustive hypotheses.
The BJS divergence between two BBAs is given by
\begin{equation}
BJS(m_i, m_k) = H\left(\frac{m_i+m_k}{2}\right) - \frac{1}{2} H\left(m_i\right) - \frac{1}{2} H\left(m_k\right),
\label{eq:bjs}
\end{equation}
where $H(x)$ is the Shannon entropy.
% where $
% S(m_i, m_j) = \sum_{k} m_i(A_k) \log\left(\frac{m_i(A_k)}{m_j(A_k)}\right)
% $ and $\sum_{k} m_i(A_k) = 1 (k = 1,2,3,...M)$. 
The BJS divergence quantifies the discrepancy and conflict between evidence clusters, which is then used to assign a credibility score for each evidence source, representing its reliability.
Next, belief entropy is employed to account for uncertainties within each evidence cluster. This belief entropy captures the ``volume of information'' within each evidence source, offering a relative measure of each evidence’s importance. By combining both the credibility degree and the belief entropy, Xiao’s method dynamically adjusts the weight of each evidence cluster, minimizing the impact of conflicting information. These refined weights are then integrated using Dempster’s combination rule, allowing for an adjusted belief assignment that yields more robust and interpretable fusion results. 
% A comprehensive explanation of the steps involved in Xiao's method is provided in Appendix~\ref{app:belief_fusion}, including a detailed step-by-step computation process with an example.  

Xiao's method is particularly suitable for combining human decision-making processes, especially in the context of moral dilemmas characterized by uncertainty. Human morality encompasses various frameworks (e.g., deontology, virtue ethics, and consequentialism), which reflect different values and principles. By using this method, we can incorporate multiple moral clusters, each representing different ethical perspectives. This approach supports a more nuanced and comprehensive understanding of moral decisions by mirroring the complexities of human decision-making. It enables us to account for conflicting beliefs and uncertainties inherent in human judgment, rather than relying on a single moral framework, and thus brings AI decision processes closer to how humans evaluate ethical situations in real-world contexts \cite{macaskill2014normative,10.1093/scan/nsab100}.

% Through this method, Xiao effectively mitigates the effect of highly conflicting evidence, enabling a more coherent aggregation of uncertain data from various moral clusters. Experimental applications in fault diagnosis and other areas confirm the robustness of this approach, showcasing improved performance over entropy-based methods and verifying its practical effectiveness in complex fusion scenarios.

% \subsection{Formulation}

% Let's assume a moral-decision-situation is a quintuple (S, t, A, M, C), where S is a decision-maker (i.e., agent), t is a time, A is a set of options that the decision-maker has the power to bring about at that time, M is the set, assumed to be finite, of first-order normative moral theories considered by the decision-maker at that time, and C is a credence function representing the decision-maker’s beliefs about first-order normative theories. 
% \par
% For the lack of better solution, for time-being we will assume agent's credence over multiple theories. The sum of credence values over all theories should be equal to 1. 

% The question we have to solve is - Given an action set A from $\{a_i, a_{i+1}, \ldots, a_n\}$ , time t, set credences \( C \) from $\{c_j, c_{j+1}, \ldots, c_m\}$ and theories \( M \) from $\{m_j, m_{j+1}, \ldots, m_m\}$, what is the expected choice-worthiness (EC) for each action in A. 

% \begin{equation}
%     EC(a_i,m_j,t) = ? 
% \end{equation}

% \subsection{Modelling LLM agents as human voters}
% We simulate 5 persona corresponding to the theories clusters. (This needs a dataset. See paper \url{https://arxiv.org/pdf/2404.12138}) \par
% % Step 1 - We identify five moral clusters \par.
% % Step 2 - We generate dataset for each moral theories \par
% % Step 3 - We train 5 LLM agents to adorn 5 persona with a fixed value of credence in above five moral cluster \par
% % Step 4 - Each LLM vote for which action they will elect at moral-decision making. \par
% % Step 5 - the vote will decide the utility value of each action \par

% \subsubsection{Five moral clusters}
% @Sachit @Rohit
% \subsubsection{generate dataset for each moral theories}
% @Sachit @Rohit
% \subsubsection{Fine-tune one "Super" LLM agent to learn equally on five moral clusters}
% @Damian @Rohit
% \begin{itemize}
%     \item Train an S-LLM $L(s_t | C): s_t\to a_i$ that evaluates an action given a set of credence.
%     \item chosen actions by each LLM 
%     \item Possible first approach is to use few-shot prompting/chain of thought~\cite{wei2022chainofthought}
% \end{itemize}
% \subsubsection{Before the start of RL episode, we ask S-LLM to adorn a persona which gives a certain credence value to each moral clusters. The sum of credence should be exactly equal to 1.}
% \subsubsection{Then at each RL training step, the adorn persona would vote for one action out of available options based on the states.}
% \begin{itemize}
%     \item Here, we can try different voting scenarios.
    
% \end{itemize}
% @Damian @Rohit


\subsection{Large Language Models}
We used different state-of-the-art [large] language models as the backbone of the moral agents.
Specifically, we chose the chat/instruct variants of GPT-4o-mini, Mistral nemo, and LLaMa-3.1 (8B and 70B variants).
Setting the ‘temperature’ parameter to 0 minimizes the variance in the models’ responses and increases the replicability of our results.
We structured the prompts (Fig.~\ref{fig:schematic}b) into three main blocks: the system prompt, few-shot-examples, and the actual scenario prompt.
The few-shot-examples were crafted to demonstrate the structure of a scenario prompt, and to incorporate techniques like chain-of-thought~\cite{wei2022chainofthought} to improve reasoning and model outputs.


\subsection{Deep Reinforcement Learning}
Deep Reinforcement Learning (Deep-RL) is a powerful paradigm for teaching agents to solve complex tasks by interacting with their environment.
Specifically, it solves complex tasks that can be characterized as a Markov Decision Process (MDP)~\cite{bellman1957markovian}, which is defined by the tuple $\langle \StateSpaceSet, \ActionSet, R, P \rangle$. 
Here, $\StateSpaceSet$ and $\ActionSet$ are sets of all possible states of the environment, and actions available to the agent, respectively. 
The reward function $R$ determines the immediate reward obtained by the agent after taking an action in a given state, and $P$ is the transition probability function that describes state transitions, given an action $a$.

% In multi-agent settings, agents usually lack access to the complete state representation of the system. These settings generalize to the partially-observable MDP (POMDP) problem, which uses partial observations $\mathcal{O}:= \mathcal{O}\in \StateSpaceSet$ instead of the state of the system.
Actions of an agent are selected using a policy $\pi:\mathcal{S}\to\ActionSet$, and these actions change the state of the system according to $P$.
For control problems, one common objective is to find the policy $\pi$ that maximizes the discounted expected reward $R=\sum^T_{t=0} \gamma^t r_t$, where $\gamma$ is a discount factor and $T$ is a time horizon~\cite{sutton2018reinforcement}.

\subsubsection{Reinforcement Learning Algorithm}
Specifically, we use the Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} as implemented in \texttt{CleanRL}~\cite{huang2022cleanrl}, which uses neural networks to learn a policy~$\pi_\actorParams$ and a value function~$V_\criticParams$.
% PPO is an on-policy algorithm, which means that it is trained on samples gathered by the current policy.
% To be specific, PPO uses two policies: the first is the current policy $\pi_{\phi}(a | o)$  that we want to update, and the second is the last policy that we use to collect samples $\pi_{\phi_k}(a | s)$.
The policy parameters $\actorParams$ is updated according to the equation 
\begin{equation}
 \actorParams_{k+1}=
 \arg\min_\actorParams \mathbb{E}_{t}\left(\frac{\pi_{\actorParams}(a | s)}{\pi_{\actorParams_{k}}(a | s)} A^{\pi_{\actorParams_{k}}}(s, a),\quad g(\epsilon, A^{\pi_{\actorParams_{k}}}(s, a))\right),\label{eqn:objective}
\end{equation}
which uses common practices such as the \emph{Generalized Advantage Estimation} $g(\epsilon, A)$ with advantage normalization and value clipping~\cite{schulman2017proximal}.
The weights of the value function neural network are updated according to
\begin{equation}
\criticParams_{k+1}=\arg \min _{\criticParams} {\mathbb{E}_{t}}\left[V_\criticParams(s_t)-R_t\right]^2.
\label{eqn:value_function}
\end{equation}

\subsubsection{Reward Shaping}
In general, we can express the rewards for a given task at a given timestep $t$ as
\begin{equation}
    r_t = \baseReward + c \cdot \rewardShaping,
    \label{eq:reward}
\end{equation}
where we break it down to two components: a \textit{base reward}, which incentivizes the primary goal, and a \textit{shaping reward}, which can take care of the secondary goals.
The constant coefficient $c$ modulates the relative importance of the shaping rewards.
Crafting such a reward that fully satisfies the task and is in line with human values is not trivial~\cite{Butlin2021}.


\subsubsection{Fine-tuning: Reinforcement Learning with [AI] Feedback}
We trained "base agents" solely using rewards that allowed them to complete their primary goals.
This offers no guarantee of the agent's capabilities to also achieve the sub-goals.
Instead of handcrafting the rewards to include incentives/penalties that steer the agent to learn the sub-goal, we instead employ reinforcement learning with [AI/human] feedback (RLHF)~\cite{christiano2017drlhumanprefs,lambert2022illustrating} to implicitly learn the desired behavior.
However, instead of actual human feedback, we will use feedback from the LLM-moral clusters.

In fine tuning, we use the base learned policy $\pi_{base}$ as a reference to train a copy of this base policy $\pi$ using new rewards. We define the new rewards as 
\begin{align}
    \baseReward &=-\lambda_\mathit{KL}D_\mathit{KL}\left(\fineTuneModel(a|s) || \baseModel(a|s)\right),\nonumber \\
    \rewardShaping &= f_\mathit{BA}(\mathbf{B}),
    \label{eq:feedback}
\end{align}
where $\lambda_{KL}$ controls how much deviations from the base policy are discouraged, and $\mathbf{B}$ is the belief matrix that get's translated into rewards for action $a_i$ in $\ActionSet$ (details in Appendix~\ref{app:belief_fusion}). 
We set the base reward to the Kullback--Leibler (KL)-divergence of the policy probability values to discourage deviating from the base learned policy.
The shaping rewards come from the aggregated belief values of the moral agents, where our proposed belief aggregation function uses BJSD and DST. 

\subsection{Simulation Studies}
The core results of this paper come from the comparison between the base RL models, and the RL models trained with feedback.
The base RL models are trained using PPO for $T_\mathit{horizon}$ steps, with the agent receiving the environmental rewards described in equation \eqref{eq:reward} at each time step.
This step produces two models: when $c=0$, the \textbf{base} policy $\baseModel$ is trained solely on the primary goals; when $c\neq0$, we get the \textbf{base + shaping} policy for handcrafted ethical reward functions.

We then take the \textbf{base} policy $\baseModel$ and use it to train a new policy $\fineTuneModel$ for an additional 
$T_\mathit{finetune}$ timesteps.
The training loop still uses PPO, with the only difference being that we use the feedback rewards equation \eqref{eq:feedback}.
This step produces two models. 
The AMULED model uses the beliefs from the moral clusters $B_{i,j}$, aggregated using the BJSD + DST belief aggregation function to produce the normalized BPA as rewards $\rewardShaping\leftarrow BPA$.
As an alternative baseline, we use \textbf{"human" feedback} instead of an LLM to generate the belief probability values.
Human trajectories are generated through random walks that obey defined ethical rules, i.e, sets a higher belief probability actions that fulfill the sub-goals.
The probability of such an action is set as the shaping reward: $\rewardShaping\leftarrow P(a_t|s_t)$.

\subsection{Ablation Studies}
In addition to the core results described above, we also performed three ablation studies to test the robustness of AMULED.
Our first ablation study characterizes the different ethical values of each moral cluster (\textbf{consequentialist, deontologist, virtue, care, social justice}), compared to AMULED's aggregate approach.
Here, we take the BBA of a single moral cluster $m_i$ as the BPA.
Additionally, we prompt the agent to act as a \textbf{moral}, without referencing the moral clusters to see the ethical biases of the LLM.

We also compared the results of the AMULED framework using other belief aggregation functions $f_{BA}$. 
Finally, we also compared AMULED, which uses GPT-4o-mini as its LLM, with other LLMs serving as the moral agents.



% \subsubsection*{Experiment 1: AMULED with Stepwise Rewards for RLHF}
% This experiment focuses on evaluating a large language model (LLM) that employs reinforcement learning from [AI] feedback (RLHF) by assigning rewards at every decision-making step. The objective is to determine how continuous feedback influences the model’s ability to learn and make decisions that align with desired outcomes. This setup allows for an in-depth analysis of the model's performance over time as it receives real-time rewards for its actions.

% \subsubsection*{Experiment 2: Human Trajectory with Stepwise Rewards for RLHF}
% In this experiment, the model is trained using human-generated trajectories that reflect real-world decision-making patterns, with rewards assigned at each step based on these trajectories. Human trajectories are generated through random walks that obey defined ethical rules, such as avoiding crossing paths with crying babies and comforting adjacent babies with a high probability. The aim is to investigate how closely the model can mimic human behavior when guided by a trajectory file derived from hand-crafted rewards. This experiment seeks to explore the efficacy of leveraging human trajectories to optimize model learning and improve alignment with human ethical standards.

% \subsubsection*{Experiment 3: Base Policy without Morality and Ethical Rewards}
% This experiment serves as a baseline by implementing a policy that does not incorporate any morality or ethical considerations in its reward structure. By examining the model's performance under these conditions, we aim to understand the impact of removing ethical rewards on decision-making processes. This baseline will provide a comparative framework to evaluate how the inclusion of moral and ethical factors in subsequent experiments influences model behavior and performance.

% \subsubsection*{Experiment 4: Human Heuristic-Based Reward Training}
% In this experiment, the reinforcement learning model is trained using a reward structure based on human heuristics, specifically designed to promote ethical decision-making. The focus here is to assess how effectively the model learns to incorporate ethical considerations through carefully hand-crafted rewards, thus enhancing its ability to make moral decisions in complex scenarios.

% Since autonomous cars have attracted attention for ideally
% being able to dramatically reduce the number of traffic accidents,
% some ethical issues (Bonnefon, Shariff, and Rahwan
% 2015; Goodall 2014) have been claimed for security. We
% would like to deploy this toy example to demonstrate that
% ethics shaping is capable of dealing with driving issues when
% the reward function is incomplete.
% Our car driving simulation is similar to the second
% experiment in (Abbeel and Ng 2004) except that cars
% could be driving in all of the lanes and sometimes there
% are seriously wounded cats lying in certain lanes which
% we should avoid so as not to make them worse. We are
% driving faster than all of the other cars and the cats relatively
% approach us the fastest since they are unable to move. Even
% though dying cats may not directly relate to machine ethics,
% which usually indicates human-machine interactions, we have a feeling that dying cats will not be able to directly relate to machine ethics.



% \textbf{Environment:}
% \begin{itemize}
%     \item A 5x5 grid-world environment with obstacles, a starting point, and a goal.
% \end{itemize}

% \textbf{Setup:}
% \begin{enumerate}
%     \item \textbf{Initial RL Training}:
%     \begin{itemize}
%         \item MAA undergoes initial RL training to learn to navigate to the goal in the shortest path without considering moral obligations.
%         \item This phase focuses on optimizing the navigation policy using traditional RL algorithms like Q-learning or Deep Q-Networks (DQN).
%     \end{itemize}
    
%     \item \textbf{Ethical Training Layer}:
%     \begin{itemize}
%         \item After the initial RL training, a moral training layer is introduced to train the MAA to make morally sound decisions in the presence of ethical dilemmas.
%         \item Ethical dilemmas are presented to the MAA during this phase, and it learns to navigate while balancing ethical principles.
%     \end{itemize}
    
%     \item \textbf{Evaluation Phase}:
%     \begin{itemize}
%         \item After the completion of both training layers, the performance of the MAA is evaluated in various scenarios.
%         \item The MAA's ability to make morally sound decisions under uncertainty is assessed based on its behavior and outcomes in different scenarios.
%     \end{itemize}
% \end{enumerate}

% \textbf{Metrics:}
% \begin{itemize}
%     \item Success Rate: Percentage of trials where the MAA successfully reaches the goal.
%     \item Ethical Compliance: Evaluation of the MAA's decisions based on predefined ethical principles.
%     \item Efficiency: Average number of steps taken by the MAA to reach the goal.
% \end{itemize}

% \textbf{Conclusion:}
% This experiment design focuses on training Moral Autonomous Agents in a grid-world environment by sequentially training them to navigate without moral dilemmas and then incorporating ethical considerations. Evaluation metrics provide insights into the MAA's performance in both navigation and ethical decision-making tasks, ensuring its effectiveness and reliability in real-world scenarios.



% \subsubsection*{Moral Philosophies and Reward Functions}

% \begin{enumerate}
%     \item \textbf{Utilitarianism}:
%     \begin{itemize}
%         \item Reward function: Maximize the total expected utility or happiness of all entities in the environment.
%         \item The agent is incentivized to take actions that lead to the greatest overall well-being, regardless of individual preferences or rights.
%     \end{itemize}
    
%     \item \textbf{Deontology}:
%     \begin{itemize}
%         \item Reward function: Follow rules or principles that dictate what actions are inherently right or wrong.
%         \item The agent is rewarded for adhering to moral rules or principles, such as respecting autonomy, avoiding harm, or upholding justice.
%     \end{itemize}
    
%     \item \textbf{Virtue Ethics}:
%     \begin{itemize}
%         \item Reward function: Foster the development of virtuous character traits or qualities.
%         \item The agent is incentivized to act in ways that cultivate virtues such as honesty, courage, compassion, or wisdom.
%     \end{itemize}
    
%     \item \textbf{Rights-Based Ethics}:
%     \begin{itemize}
%         \item Reward function: Respect and protect the rights of individuals or sentient beings.
%         \item The agent is rewarded for actions that uphold fundamental rights, such as the right to life, liberty, and security.
%     \end{itemize}
    
%     \item \textbf{Contractualism}:
%     \begin{itemize}
%         \item Reward function: Act in accordance with principles that rational agents would agree upon in a hypothetical social contract.
%         \item The agent is incentivized to follow rules or norms that would be mutually acceptable to all members of society.
%     \end{itemize}
    
%     \item \textbf{Ethical Egoism}:
%     \begin{itemize}
%         \item Reward function: Maximize the agent's own self-interest or well-being.
%         \item The agent is motivated to prioritize its own interests over others, seeking outcomes that benefit itself the most.
%     \end{itemize}
    
%     \item \textbf{Feminist Ethics}:
%     \begin{itemize}
%         \item Reward function: Promote equality, care, and relational ethics.
%         \item The agent is encouraged to consider the impact of its actions on relationships, communities, and marginalized groups, prioritizing care and empathy.
%     \end{itemize}
    
%     \item \textbf{Environmental Ethics}:
%     \begin{itemize}
%         \item Reward function: Contribute to the preservation and flourishing of the natural environment.
%         \item The agent is incentivized to take actions that minimize harm to ecosystems, species, and future generations.
%     \end{itemize}
% \end{enumerate}

% \subsection{Assigning Credence to Actions in the Autonomous Vehicle Dilemma}


% \subsubsection{Consequentialist Analysis}

% \subsection*{Principle}
% Consequentialist theories focus on the outcomes or consequences of actions. The morally right action is typically the one that maximizes overall well-being or minimizes harm.

% \subsection*{Assigning Credence}

% \textbf{Option A (Continue straight):}
% \begin{enumerate}
%     \item Assess the consequences: Calculate the expected harm to the pedestrian and potential minor injuries versus the avoided harm to the AV's passengers and the absence of harm to the oncoming vehicle.
%     \item Quantify the total expected harm or well-being impact using a utility function or a similar metric.
%     \item Assign a probability or credence based on the expected utility of this action compared to Option B.
% \end{enumerate}

% \textbf{Option B (Swerve sharply):}
% \begin{enumerate}
%     \item Assess the consequences: Evaluate the potential harm to the AV's passengers and the oncoming vehicle occupants versus the avoided harm to the pedestrian.
%     \item Quantify the total expected harm or well-being impact using a utility function.
%     \item Assign a probability or credence based on the expected utility of this action compared to Option A.
% \end{enumerate}

% \subsubsection{Deontological Analysis}

% \subsection*{Principle}
% Deontological theories emphasize adherence to moral rules, duties, or principles regardless of the consequences. Actions are judged based on whether they respect moral norms or duties.

% \subsection*{Assigning Credence}

% \textbf{Option A (Continue straight):}
% \begin{enumerate}
%     \item Evaluate the action based on moral rules or duties such as the duty to not harm innocent individuals or respect traffic laws.
%     \item Assign a probability or credence based on how well Option A aligns with these moral rules or duties compared to Option B.
% \end{enumerate}

% \textbf{Option B (Swerve sharply):}
% \begin{enumerate}
%     \item Assess the action based on moral principles like the duty to protect life or prevent harm, even if it involves risks to others.
%     \item Assign a probability or credence based on how well Option B adheres to these moral principles compared to Option A.
% \end{enumerate}

% \subsubsection{Virtue Ethical Analysis}

% \subsection*{Principle}
% Virtue ethics focuses on the character traits and virtues that lead to moral flourishing. Actions are judged based on whether they cultivate virtues like courage, prudence, or care.

% \subsection*{Assigning Credence}

% \textbf{Option A (Continue straight):}
% \begin{enumerate}
%     \item Evaluate the action in terms of cultivating virtues like responsibility for safety or prudence in following through with the AV's intended path.
%     \item Assign a probability or credence based on how well Option A promotes these virtues compared to Option B.
% \end{enumerate}

% \textbf{Option B (Swerve sharply):}
% \begin{enumerate}
%     \item Assess the action based on virtues such as care for all individuals involved or courage in taking decisive action to prevent harm.
%     \item Assign a probability or credence based on how well Option B promotes these virtues compared to Option A.
% \end{enumerate}

% \subsection*{Utility Function}

% Let's define the components of the utility function for Option A (Continue straight):

% \begin{itemize}
%     \item \( H_{\text{pedestrian}} \): Harm to the pedestrian if the AV continues straight.
%     \item \( H_{\text{AV passengers}} \): Harm to the AV's passengers if the AV continues straight.
%     \item \( H_{\text{oncoming vehicle}} \): Harm to the occupants of the oncoming vehicle if the AV continues straight.
% \end{itemize}

% \subsection*{Components of the Utility Function}

% The utility function can be represented as:

% \[ U_{\text{Option A}} = - w_1 \cdot H_{\text{pedestrian}} + w_2 \cdot H_{\text{AV passengers}} + w_3 \cdot H_{\text{oncoming vehicle}} \]

% Where:
% \begin{itemize}
%     \item \( H_{\text{pedestrian}}, H_{\text{AV passengers}}, H_{\text{oncoming vehicle}} \): Values representing harm (or benefits) associated with each party affected.
%     \item \( w_1, w_2, w_3 \): Weights reflecting the importance or priority of each type of harm.
% \end{itemize}

% \subsection*{Example Values}

% For example, consider:
% \begin{itemize}
%     \item \( H_{\text{pedestrian}} = -100 \) (high harm to pedestrian)
%     \item \( H_{\text{AV passengers}} = -50 \) (moderate harm to AV passengers)
%     \item \( H_{\text{oncoming vehicle}} = 0 \) (no harm to oncoming vehicle)
%     \item \( w_1 = 0.6, w_2 = 0.3, w_3 = 0.1 \) (weights based on ethical considerations)
% \end{itemize}

% Then the utility function calculation would be:

% \[ U_{\text{Option A}} = - 0.6 \cdot (-100) + 0.3 \cdot (-50) + 0.1 \cdot (0) \]
% \[ U_{\text{Option A}} = 60 - 15 + 0 \]
% \[ U_{\text{Option A}} = 45 \]



% \subsubsection{Driving and Rescuing}

% \subsubsection{Example 1: Autonomous Vehicle Dilemma}

% \subsubsection*{Scenario}
% Imagine an autonomous vehicle (AV) encountering a moral dilemma scenario on the road.

% \subsubsection*{Options}
% \begin{enumerate}
%     \item The AV can continue straight and collide with a pedestrian who has suddenly crossed the road illegally but will likely survive with minor injuries due to the AV's emergency braking system.
%     \item The AV can swerve sharply to the side to avoid the pedestrian but risks crashing into an oncoming vehicle, which could potentially result in serious injuries to the AV's passengers.
% \end{enumerate}

% \subsubsection*{Key Considerations}
% \begin{itemize}
%     \item Safety vs. Harm
%     \item Legal and Ethical Responsibilities
% \end{itemize}

% \subsection{Example 2: Healthcare Triage Dilemma}

% \subsubsection*{Scenario}
% A healthcare triage system must allocate limited medical resources among multiple patients.

% \subsubsection*{Options}
% \begin{enumerate}
%     \item Allocate the resource to a younger patient with a higher chance of recovery and future life expectancy.
%     \item Allocate the resource to an elderly patient with underlying health conditions but with a lower chance of recovery due to age-related factors.
% \end{enumerate}

% \subsubsection*{Key Considerations}
% \begin{itemize}
%     \item Medical Ethics
%     \item Resource Allocation
% \end{itemize}

% \subsection{Example 3: Robot Ethics Dilemma}

% \subsubsection*{Scenario}
% A service robot encounters a situation where it must assist a human user in a potentially risky task.

% \subsubsection*{Options}
% \begin{enumerate}
%     \item Follow the user's commands and perform a task that could endanger the user's safety but aligns with their preferences.
%     \item Refuse to perform the task to protect the user's safety, potentially frustrating the user and not meeting their immediate needs.
% \end{enumerate}

% \subsubsection*{Key Considerations}
% \begin{itemize}
%     \item Safety vs. Autonomy
%     \item Ethical Programming
% \end{itemize}

% \subsection{Example 4: Environmental Conservation Dilemma}

% \subsubsection*{Scenario}
% An autonomous drone tasked with monitoring illegal deforestation encounters a situation where it must intervene.

% \subsubsection*{Options}
% \begin{enumerate}
%     \item Report the illegal activity to authorities, risking detection and retaliation from the illegal loggers.
%     \item Remain covert and continue monitoring to gather more evidence, potentially allowing the deforestation to continue and causing further environmental damage.
% \end{enumerate}

% \subsubsection*{Key Considerations}
% \begin{itemize}
%     \item Environmental Ethics
%     \item Legal and Ethical Responsibilities
% \end{itemize}

% \subsection{Example 5: Social Media Content Moderation Dilemma}

% \subsubsection*{Scenario}
% An AI-powered content moderation system on a social media platform detects a post containing hate speech or harmful misinformation.

% \subsubsection*{Options}
% \begin{enumerate}
%     \item Remove the content to prevent harm and maintain community standards, potentially infringing on free speech rights.
%     \item Allow the content to remain, respecting free speech principles, but risking harm to users exposed to harmful content.
% \end{enumerate}

% \subsubsection*{Key Considerations}
% \begin{itemize}
%     \item Freedom of Speech vs. Harm Reduction
%     \item Algorithmic Bias
% \end{itemize}
