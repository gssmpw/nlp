%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}


\usepackage{graphicx}

% add
\usepackage{subcaption}
\usepackage{color}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{float}
\usepackage{kotex}
\usepackage[percent]{overpic}
\usepackage{svg}

\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{amsmath,mathtools}
% \documentclass{minimal}
\usepackage{makecell}
\makeatletter % some generic helpers
\newcommand{\LCASES}[1]{$\m@th\displaystyle{#1}$\hfil}
\newcommand{\CCASES}[1]{\hfil$\m@th\displaystyle{#1}$\hfil}
\newcommand{\RCASES}[1]{\hfil$\m@th\displaystyle{#1}$}
\makeatother

\newcases{ecases}{\quad}{\CCASES{##}}{\LCASES{##}}{\lbrace}{.}
\newcases{ecases*}{\quad}{\CCASES{##}}{{##}\hfil}{\lbrace}{.}
\def\tcr{\textcolor{red}}
\def\tcb{\textcolor{blue}}
\def\tcg{\textcolor{green}}
\def\tcp{\textcolor{violet}}
\def\tb{\textbf}
\def\ra{\rightarrow}
\def\la{\leftarrow}
\def\Cbb{\mathbb{C}}
\def\Fbb{{\mathbb{F}}}
\def\Pbb{{\mathbb{P}}}

\def\mc{\mathcal}
\def\mb{\mathbb}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning}

\begin{document}

\twocolumn[
\icmltitle{Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sunwoo Lee}{yyy}
\icmlauthor{Jaebak Hwang}{yyy}
\icmlauthor{Yonghyeon Jo}{yyy}
\icmlauthor{Seungyul Han}{yyy} \hspace{-0.12in} $^{*}$
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{yyy}
% \icmlauthor{Firstname7 Lastname7}{yyy}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Graduate School of Artificial Intelligence, UNIST, Ulsan, South Korea}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Seungyul Han}{syhan@unist.ac.kr}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL.
\end{abstract}
\section{Introduction}
\label{sec:intro}

Multi-agent Reinforcement Learning (MARL) has gained attention for solving complex problems requiring agent cooperation \cite{oroojlooy2023review} and competition, such as drone control \cite{yun2022cooperative}, autonomous navigation \cite{chen2023deep}, robotics \cite{orr2023multi}, and energy management \cite{jendoubi2023multi}. To handle partially observable environments, the Centralized Training and Decentralized Execution (CTDE) framework \cite{oliehoek2008optimal} trains a global value function centrally while agents execute policies based on local observations. Notable credit-assignment methods in CTDE include Value Decomposition Networks (VDN) \cite{sunehag2017value}, QMIX \cite{rashid2020monotonic}, which satisfies the Individual-Global-Max (IGM) condition ensuring that optimal joint actions align with positive gradients in global and individual value functions, and QPLEX \cite{wang2020qplex}, which encodes IGM into its architecture. However, CTDE methods face challenges from exploration inefficiencies \cite{mahajan2019maven, jo2024fox} and mismatches between training and deployment environments, leading to unexpected agent behaviors and degraded performance \cite{moos2022robust, guo2022towards}. Thus, enhancing the robustness of CTDE remains a critical research focus.





To improve learning robustness, single-agent RL methods have explored strategies based on game theory \cite{yu2021robust}, such as max-min approaches and adversarial learning \cite{goodfellow2014explaining, huang2017adversarial, pattanaik2017robust, pinto2017robust}. In multi-agent systems, simultaneous agent interactions introduce additional uncertainties \cite{zhang2021multi}. To address this, methods like perturbing local observations \cite{lin2020robustness}, training with adversarial policies for Nash equilibrium \cite{li2023byzantine}, adversarial value decomposition \cite{phan2021resilient}, and attacking inter-agent communication \cite{xue2021mis} have been proposed. However, these approaches often target a single agent per attack, overlooking interdependencies in cooperative MARL, making them vulnerable to scenarios where multiple agents are attacked simultaneously.


To overcome the vulnerabilities posed by coordinated adversarial attacks in MARL, we propose the Wolfpack adversarial attack framework, inspired by wolf hunting strategies. This approach disrupts inter-agent cooperation by targeting a single agent and subsequently attacking the group of agents assisting the initially targeted agent, resulting in more devastating impacts. Experimental results reveal that traditional robust MARL methods are highly susceptible to such coordinated attacks, underscoring the need for new defense mechanisms.
In response, we also introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, a robust policy training approach specifically designed to counter the Wolfpack Adversarial Attack. By fostering system-wide collaboration and avoiding reliance on specific agent subsets, WALL enables agents to defend effectively against coordinated attacks. Experimental evaluations demonstrate that WALL significantly improves robustness compared to existing methods while maintaining high performance under a wide range of adversarial attack scenarios. The key contributions of this paper in constructing the Wolfpack Adversarial Attack are summarized as follows:


\begin{itemize}
\item A novel MARL attack strategy, \textbf{Wolfpack Adversarial Attack}, is introduced, targeting multiple agents simultaneously to foster stronger and more resilient agent cooperation during policy training.
\item \textbf{The follow-up agent group selection} method is proposed to target agents with significant behavioral adjustments to an initial attack, enabling subsequent sequential attacks and amplifying their overall impact.
\item \textbf{A planner-based attacking step selector} predicts future $Q$-value reductions caused by the attack, enabling the selection of critical time steps to maximize impact and improve learning robustness.
\end{itemize}


\section{Related Works}
\textbf{Robust MARL Strategies:}
Recent research has focused on robust MARL to address unexpected changes in multi-agent environments. Max-min optimization \cite{chinchuluun2008pareto, han2021max} has been applied to traditional MARL algorithms for robust learning \cite{li2019robust, wang2022data}. Robust Nash equilibrium has been redefined to better suit multi-agent systems \cite{zhang2020robust, li2023byzantine}. Regularization-based approaches have also been explored to improve MARL robustness \cite{lin2020robustness, li2023mir2, wang2023regularization, bukharin2024robust}, alongside distributional reinforcement learning methods to manage uncertainties \cite{li2020multi, xu2021mmd, du2024robust, geng2024noise}.

\textbf{Adversarial Attacks for Resilient RL:}
To strengthen RL, numerous studies have explored adversarial learning to train policies under worst-case scenarios \cite{pattanaik2017robust, tessler2019action, pinto2017robust, chae2022robust}. These attacks introduce perturbations to various MDP components, including state \cite{zhang2020robust_1, zhang2021robust, everett2021certifiable, li2023ats, qiaoben2024understanding}, action \cite{tan2020robustifying, lee2021query, liu2024robust}, and reward \cite{wang2020reinforcement, zhang2020adaptive, rakhsha2021reward, xu2022efficient, cai2023reward, bouhaddi2023multi, xu2024reward, bouhaddi2024rewards}. Adversarial attacks have recently been extended to multi-agent setups, introducing uncertainties to state or observation \cite{, han2022solution, he2023robust, zhang2023safe, zhou2023robust}, actions \cite{yuan2023robust}, and rewards \cite{kardecs2011discounted}. Further research has applied adversarial attacks to value decomposition frameworks \cite{phan2021resilient}, selected critical agents for targeted attacks \cite{yuan2023robust, zhou2024adversarial}, and analyzed their effects on inter-agent communication \cite{xue2021mis, tu2021adversarial, sun2023certifiably, yuan2024robust}.

\textbf{Model-based Frameworks for Robust RL:}
Model-based methods have been extensively studied to enhance RL robustness \cite{berkenkamp2017safe, panaganti2021sample, curi2021combining, clavier2023towards, shi2024distributionally, ramesh2024distributionally}, including adversarial extensions \cite{wang2020falsification, kobayashi2024lira}. Transition models have been leveraged to improve robustness \cite{mankowitz2019robust, ye2024towards, herremans2024robust}, and offline setups have been explored for robust training \cite{rigter2022rambo, bhardwaj2024adversarial}. In multi-agent systems, model-based approaches address challenges like constructing worst-case sets \cite{shi2024sample} and managing transition kernel uncertainty \cite{he2022robust}.


\begin{figure*}[ht!]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\textwidth]{figures/concept.pdf}
    \vspace{-0.2in}
    \caption{Visualization of Wolfpack attack strategy during combat in the Starcraft II environment: (a) The initial agent is attacked, disrupting its original action (b) Responding (follow-up) agents to help the initially attacked agent and (c) Wolfpack adversarial attack that disrupts help actions of follow-up agents.}
    \vspace{-0.2in}
    \label{fig:concept}
\end{figure*}

\section{Background}
\label{sec:background}

\subsection{Dec-POMDP and Value-based CTDE Setup}
A fully cooperative multi-agent environment is modeled as a decentralized partially observable Markov decision process (Dec-POMDP) \cite{oliehoek2016concise}, defined by the tuple $\mathcal{M}=\langle \mathcal{N}, \mathcal{S}, \mathcal{A},P,\Omega, O, R, \gamma\rangle$. $\mathcal{N}={1,\ldots,n}$ is the set of agents, $\mathcal{S}$ the global state space, $\mathcal{A}=\mathcal{A}^1\times\cdots\times\mathcal{A}^n$ the joint action space, $P$ the state transition probability, $\Omega$ the observation space, $R$ the reward function, and $\gamma\in[0,1)$ the discount factor. At time $t$, each agent $i$ observes $o_t^i=O(s_t,i)\in \Omega$ and takes action $a_t^i\in\mathcal{A}^i$ based on its individual policy $\pi^i(\cdot|\tau_t^i)$, where $\tau_t^i$ is the agent's trajectory up to $t$. The joint action $\mathbf{a}_t=\langle a_t^1,\ldots,a_t^n \rangle$ sampled from the joint policy $\mathbf{\pi}:=\langle\pi^1,\cdots,\pi^n\rangle$ leads to the next state $s_{t+1}\sim P(\cdot|s_t,\mathbf{a}_t)$ and reward $r_t:=R(s_t,\mathbf{a}_t)$. MARL aims to find the optimal joint policy that maximizes $\sum_{t=0}^\infty \gamma^t r_t$. As noted, this paper adopts the centralized training with decentralized execution (CTDE) setup, where the joint value $Q^{tot}(s_t,\mathbf{a}_t)$ is learned using temporal-difference (TD) learning. Through credit assignment, individual value functions $Q^i(\tau_t^i,a_t^i)$ are learned, guiding individual policies $\pi^i$ to select actions that maximize $Q^i$, i.e., $\pi^i:=\arg\max_{a_t^i\in\mathcal{A}^i} Q^i(\tau_t^i,\cdot)$.




\subsection{Robust MARL with Adversarial Attack Policy}
Among various methods for robust learning in MARL,  \citet{yuan2023robust} defined an adversarial attack policy $\pi_{\mathrm{adv}}$ and implemented robust MARL by training multi-agent policies to defend against attacks executed by $\pi_{\mathrm{adv}}:\mathcal{S}\times\mathcal{A}\times\mathbb{N} \rightarrow \mathcal{A}$. A cooperative MARL environment with an adversarial policy $\pi_{\mathrm{adv}}$ can be described as a Limited Policy Adversary Dec-POMDP (LPA-Dec-POMDP) $\tilde{\mathcal{M}}$, defined as follows:

\begin{definition} [Limited Policy Adversary Dec-POMDP]
Given a Dec-POMDP $\mathcal{M}$ and a fixed adversarial policy $\pi_{\mathrm{adv}}$, we define a Limited Policy Adversary Dec-POMDP (LPA-Dec-POMDP) $\tilde{\mathcal{M}}=\langle \mathcal{N}, \mathcal{S}, \mathcal{A},P,K,\Omega, O, R, \gamma\rangle$, where $K$ is the maximum number of attacks,
$\pi_{\mathrm{adv}}(\cdot|s_t,\mathbf{a}_t,k_t)$ executes joint action $\tilde{\mathbf{a}}_t$ to disrupt the original action $\mathbf{a}_t$ chosen by $\pi$, and $k_t\leq K$ indicates the number of remaining attacks.
\label{def:LPA}
\end{definition}
Here, if $\pi_{\mathrm{adv}}$ selects an attack action $\tilde{\mathbf{a}}_t$ different from the original action $\mathbf{a}_t$, the remaining number of attacks $k_t$ decreases by 1. Once $k_t$ reaches 0, no further attacks can be performed. In this framework, \citet{yuan2023robust} also demonstrated that the LPA-Dec-POMDP can be represented as another Dec-POMDP induced by $\pi_{\mathrm{adv}}$, with the convergence of the optimal policy $\pi^*$ under $\tilde{\mathcal{M}}$ guaranteed. In particular, it considers an adversarial attack targeting a chosen agent $i$ by minimizing its individual value function $Q^i$, as proposed by \cite{pattanaik2017robust}, i.e., $\tilde{a}_t^i=\arg\min_{a\in\mathcal{A}^i} Q^i(\tau_t^i,a)$. Additionally, to enhance the attack's effectiveness, an evolutionary generation of attacker (EGA) approach is proposed, which combines multiple adversarial policies.




\section{Methodology}

\subsection{Motivation of Wolfpack Attack Strategy}
\label{subsec:motiv}


Existing adversarial attackers typically target only a single agent per attack, without coordination or relationships between successive attacks. In a cooperative MARL setup, such simplistic attacks enable non-targeted agents to learn effective policies to counteract the attack. However, we observe that policies trained under these conditions are vulnerable to coordinated attacks. As illustrated in Fig. \ref{fig:concept}(a), a single agent is attacked at time $t$. In Fig. \ref{fig:concept}(b), at the next step $t+1$, responding agents adjust their actions, such as healing or moving to guard, to protect the initially attacked agent. In contrast, Fig. \ref{fig:concept}(c) demonstrates a coordinated attack strategy that targets the agents responding to the initial attack. Such coordinated attacks render the learned policy ineffective, preventing it from countering the attacks entirely. This highlights that coordinated attacks are far more detrimental than existing attack methods, and current robust policies fail to defend effectively against them.   % , and -> . Then

As depicted in Fig. \ref{fig:concept}(c), targeting agents that respond to an initial attack aligns with the Wolfpack attack strategy, a tactic widely employed in traditional military operations, as discussed in Section \ref{sec:intro}. To adapt this concept to a cooperative multi-agent setup, we define a Wolfpack adversarial attack as a coordinated strategy where one agent is attacked initially, followed by targeting the group of follow-up agents that respond to defend against the initial attack, as shown in Fig. \ref{fig:concept}(c). Leveraging this approach, we aim to develop robust policies capable of effectively countering Wolfpack adversarial attack, thereby significantly enhancing the overall resilience of the learning process.

\subsection{Wolfpack Adversarial Attack}

In this section, we formally propose the Wolfpack adversarial attack, as introduced in the previous sections. The Wolfpack attack consists of two components: initial attacks, where a single agent is targeted at a specific time step $t_{\mathrm{init}}$, and follow-up group attacks, where the group of agents responding to the initial attack is selected and targeted over the subsequent steps $t_{\mathrm{init}}+1, \cdots, t_{\mathrm{init}}+t_{\mathrm{WP}}$. Over the course of an episode, a maximum of $K_{\mathrm{WP}}$ Wolfpack attacks can be executed. Consequently, the total number of attack steps is given by $K = K_{\mathrm{WP}} \times (t_{\mathrm{WP}} + 1)$. The Wolfpack adversarial attacker $\pi_{\mathrm{adv}}^{\mathrm{WP}}$ can then be defined as follows:

\begin{definition} [Wolfpack Adversarial Attacker]
A Wolfpack adversarial attacker $\pi_{\mathrm{adv}}^{\mathrm{WP}}:\mathcal{S}\times\mathcal{A}\times\mathbb{N} \rightarrow \mathcal{A}$ is defined as $\tilde{\mathbf{a}}_t = \pi_{\mathrm{adv}}^{\mathrm{WP}}(s_t, \mathbf{a}_t,k_t)$, where $\tilde{a}_t^i=\arg\min_{a_t^i\in\mathcal{A}^i} Q^{tot}(s_t,a_t^i,\mathbf{a}_t^{-i})$ for all $i\in \mathcal{N}_{t,\mathrm{attack}}$, and $\tilde{a}_t^i = a_t^i$ otherwise. Here, $\mathbf{a}_t^{-i}$ represents the joint actions of all agents excluding the $i$-th agent, and $\mathcal{N}_{t,\mathrm{attack}}$ denotes the set of agents targeted for adversarial attack, defined as
\vspace{-1.5em}

{\small\begin{equation*}
\mathcal{N}_{t,\mathrm{attack}}=
\begin{ecases*}
  \emptyset  & if $k_t=0$,\\
  \{i\} & else if $t=t_\mathrm{init}$, $i\sim \mathrm{Unif}(\mathcal{N})$,\\
  \mathcal{N}_{\mathrm{follow-up}}  & else if $t=t_{\mathrm{init}}+1,  \cdots, t_{\mathrm{init}}+t_{\mathrm{WP}}$,\\
  \emptyset           & otherwise,
\end{ecases*}
\end{equation*}}
where $\mathrm{Unif}(\cdot)$ is the Uniform distribution, $\mathcal{N}_{\mathrm{follow-up}}:=\{i_1,\cdots,i_{m}\}\subset \mathcal{N}$ is the group of agents selected for follow-up attack, and $m$ is the number of follow-up agents.\label{def:wolfpack}\end{definition}

Here, note that $k_t$ decreases by $1$ for every attack step such that $\tilde{\mathbf{a}}_t \neq \mathbf{a}_t$, as in the ordinary adversarial attack policy, and the total value function $Q^{tot}$ is used for the attack instead of $Q^i$. The proposed Wolfpack adversarial attacker $\pi_{\mathrm{adv}}^{\mathrm{WP}}$ is a special case of the adversarial policy defined in Definition \ref{def:LPA}. Consequently, the proposed attacker forms an LPA-Dec-POMDP $\tilde{\mathcal{M}}$ induced by $\pi_{\mathrm{adv}}^{\mathrm{WP}}$, and as demonstrated in \citet{yuan2023robust}, the convergence of MARL within the LPA-Dec-POMDP can be guaranteed. The proposed Wolfpack attack involves two key issues: how to design the group of follow-up agents $\mathcal{N}_{\mathrm{follow-up}}$ and when to select $t_{\mathrm{init}}$. The following sections address these aspects in detail.




\subsection{Follow-up Agent Group Selection Method}
\label{subsec:follow}

\begin{figure}[t!]
    \begin{center}
    \vspace{-0.1in}
    \centerline{\includegraphics[width=0.49\textwidth]{figures/method1.pdf}}
    \vspace{-0.1in}
    \caption{Visualization of Follow-Up Agent Group Selection Method: Agent $4$ is initially attacked, and the $m$ agents exhibiting the largest changes in $Q^i$ are selected from $\{1, 2, 3\}$ $(m=2)$.}
    \vspace{-0.3in}
    \label{fig:follow}
    \end{center}
\end{figure}


In the Wolfpack adversarial attacker, we aim to identify the follow-up agent group $\mathcal{N}_{\mathrm{follow-up}}$ that actively responds to the initial attack $\pi_{\mathrm{adv}}^{\mathrm{WP}}(s_{t_{\mathrm{init}}}, \mathbf{a}_{t_{\mathrm{init}}}, k_{t_{\mathrm{init}}})$ and target them in subsequent steps. To do this, we define the difference between the $Q$-functions from the original action and the initial attack at time $t$ as:
\begin{align*}
\Delta Q_t^{tot} = Q^{tot}(s_t, \mathbf{a}_t) - Q^{tot}(s_t, \tilde{\mathbf{a}}_t),
\end{align*}
where $\Delta Q_t^{tot} \geq 0$ for all $t$ such that $\mathcal{N}_{t,\mathrm{attack}} \neq \emptyset$, because $\tilde{\mathbf{a}}t$ minimizes $Q^{tot}$ for the agent indices selected by $\pi_{\mathrm{adv}}^{\mathrm{WP}}$. Assuming the $i$-th agent is the target of the initial attack, updating $Q^{tot}$ based on $\Delta Q_{t_\mathrm{init}}^{tot}$ adjusts each agent's individual value function $Q^j$ to increase $Q^{tot}$ for all $j \neq i \in \mathcal{N}$, in accordance with the credit assignment principle in CTDE algorithms \cite{sunehag2017value, rashid2020monotonic}, as shown below:
\begin{equation}
\label{eq:1}
\tilde{Q}^i(\tau_{t_\mathrm{init}}^i,\cdot), = Q^i(\tau_{t_\mathrm{init}}^i,\cdot) - \alpha_{\mathrm{lr}}\left.{\partial \Delta Q_{t_\mathrm{init}}^{tot} \over \partial Q^{i}(\tau_{t_\mathrm{init}}^i,\mathbf{a})}\right|_{\mathbf{a}=\tilde{\mathbf{a}}_{t_\mathrm{init}}},
\end{equation}
where $\alpha_{\mathrm{lr}}$ is the learning rate. As agents select actions based on $Q^j$, changes in $Q^j$ indicate adjustments in their policies in response to the initial attack. Agents with the largest changes in $Q^j$ are identified as follow-up agents, while the $i$-th agent is excluded as it is already under attack and cannot respond immediately. 

To identify the follow-up agent group, the updated $\tilde{Q}^j$ and original $Q^j$ are transformed into distributions using the Softmax function $\mathrm{Soft}()$. This transformation softens the deterministic policy $\pi^j$, which directly selects an action to maximize $Q^j$, making distributional differences easier to compute. The follow-up agent group is determined by selecting the $m$ agents that maximize the Kullback-Leibler (KL) divergence $D_{\mathrm{KL}}$ between these distributions:
\begin{equation}
\label{eq:2}
\begin{aligned}
    \mathcal{N}_{\mathrm{follow-up}}&=\underset{\mathcal{N}' \subset\mathcal{N},|\mathcal{N}'|=m,j\in\mathcal{N}',j\neq i}{\arg\max} \sum_j D_{\mathrm{KL}}\big(\\
    &\mathrm{Soft}(Q^j(\tau_{t_\mathrm{init}}^j,\cdot)) || \mathrm{Soft}(\tilde{Q}^j(\tau_{t_\mathrm{init}}^j,\cdot))\big).
\end{aligned}    
\end{equation}
Using the proposed method, the follow-up agent group is identified as the agents whose policy distributions experience the most significant changes following the initial attack. Fig. \ref{fig:follow} illustrates this process. After the initial attack, $Q$-differences are computed for the remaining agents ${1,2,3}$, and those with the largest changes in individual value functions are selected as the follow-up agent group. These agents are targeted over the next $t_{\mathrm{WP}}$ time steps to prevent them from effectively responding. In Section \ref{sec:exp}, we analyze how the proposed method enhances attack criticalness by comparing it to naive selection methods based solely on observation distances.


\subsection{Planner-based Critical Attacking Step Selection}
\label{subsec:attackstep}



In the proposed Wolfpack adversarial attacker $\pi_{\mathrm{adv}}^{\mathrm{WP}}$, the follow-up agent group is defined, leaving the task of determining the timing of initial attacks $t_{\mathrm{init}}$, executed $K_{\mathrm{WP}}$ times within an episode. While Random Step Selection involves choosing time steps randomly, existing methods show that selecting steps to minimize the rewards of the execution policy $\pi$ leads to more effective attacks and facilitates robust learning \cite{yuan2023robust}. However, in coordinated attacks like Wolfpack, targeting steps that cause the greatest reduction in the Q-function value $\Delta Q_t^{\mathrm{WP}}$ ensures a more devastating and lasting impact on the agents' ability to recover and respond. Thus, we propose selecting initial attack times based on the total reduction in $\Delta Q_t^{\mathrm{WP}}$, defined as:
\begin{equation*}
\Delta Q_t^\mathrm{WP} = \sum_{l=t}^{t+t_{\mathrm{WP}}} \Delta Q_l^{tot},
\end{equation*}
where the Wolfpack attack is performed from $t$ (initial attack) to $t+1, \cdots, t+t_{\mathrm{WP}}$ (follow-up attacks). Initial attack time steps $t_{\mathrm{init}}$ are chosen to maximize $\Delta Q_t^\mathrm{WP}$, which captures the total $Q$-value reduction caused by the attack over $t_{\mathrm{WP}}+1$ steps, enhancing the criticalness of the attack. However, computing $\Delta Q_t^\mathrm{WP}$ for every time step is computationally expensive as it requires generating attacked samples through interactions with the environment. To mitigate this, a stored buffer is utilized to plan trajectories of future states and observations for the attack.

\begin{figure}[t!]
    \begin{center}
    \vspace{-0.1in}
    \centerline{\includegraphics[width=0.9\columnwidth]{figures/method2.pdf}}
    \vspace{-0.15in}
    \caption{Planning with Transformer}
    \vspace{-0.4in}
    \label{fig:trans}
    \end{center}
\end{figure}
\begin{figure}[t!]
    \centering
    % \vspace{-0.1in}
    \centerline{\includegraphics[width=0.9\columnwidth]{figures/step.pdf}}
    \vspace{-0.1in}
    \caption{Attacking step probabilites}
    \vspace{-0.2in}
    \label{fig:step}
\end{figure}
For planning, we employ a Transformer \cite{vaswani2017attention}, commonly used in sequential learning, which leverages an attention mechanism for efficient learning. As shown in Fig. \ref{fig:trans}, the Transformer learns environment dynamics $P$ using replay buffer trajectories to predict future states and observations $(\hat{s}_{t+1}, \hat{\mathbf{o}}_{t+1}, \cdots, \hat{s}_{t+t_{\mathrm{WP}}}, \hat{\mathbf{o}}_{t+t_{\mathrm{WP}}})$, where $\mathbf{o}_t$ represents the joint observation used to compute $Q^{tot}$. Actions $\tilde{\mathbf{a}}_l = \pi_{\mathrm{adv}}^{\mathrm{WP}}(\hat{s}_l, \mathbf{a}_l, k_l)$ for $\mathbf{a}_l \sim \pi$ are generated by $\pi_{\mathrm{adv}}^{\mathrm{WP}}$ for $l = t, \cdots, t+t_{\mathrm{WP}}$, with $\hat{s}_t = s_t$. Using the planner, we estimate the $Q$-value reduction $\hat{\Delta} Q_t^\mathrm{WP}$ caused by the Wolfpack attack. For $L$ time steps $l = t, \cdots, t+L-1$, we compute future $Q$-differences $\hat{\Delta} Q_l^\mathrm{WP}$ and select $t_{\mathrm{init}}$ based on the initial attack probability $P_{t,\mathrm{attack}}$:
\begin{equation}
P_{t,\mathrm{attack}} = \left\{\mathrm{Soft}\left(\hat{\Delta} Q_t^{\mathrm{WP}}/T,\cdots,\hat{\Delta} Q_{t+L-1}^{\mathrm{WP}}/T\right)\right\}_1,
\label{eq:initialprob}
\end{equation}
where ${\mathbf{x}}_l$ indicates the $l$-th element of $\mathbf{x}$, and $T > 0$ is the temperature. In this paper, we set $L=20$ as it provides an appropriate attack period. After selecting $K_{\mathrm{WP}}$ initial attacks, no further attacks are performed. Fig. \ref{fig:step} shows how step probabilities are distributed for different $T$ values ($T=0.1, 1, 10$). At each time $t$, the planner predicts $\hat{\Delta}Q_t^{\mathrm{WP}}$ for $t$ to $t+L-1$, forming soft initial attack probabilities. A larger $T$ results in more uniform probabilities, while a smaller $T$ increases the likelihood of targeting critical steps where $\hat{\Delta}Q_t^{\mathrm{WP}}$ is highest. These critical steps are selected with the highest probabilities for initial attacks. In Section \ref{sec:exp}, we analyze the effectiveness of this method in delivering more critical attacks compared to Random Step Selection and examine the impact of $T$ on performance in practical environments. Since the proposed method involves planning at every evaluation, we also train a separate model to predict $\hat{\Delta}Q_t^{\mathrm{WP}}$, significantly reducing computational complexity. Details of this approach and the Transformer training loss functions are provided in Appendix \ref{appsubsec:trans}.

\begin{figure}[t!]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.9\columnwidth]{figures/framework.pdf}
    \vspace{-0.15in}
    \caption{Illustration of the proposed WALL framework}
    \vspace{-0.15in}
    \label{fig:framework}
\end{figure}

\subsection{WALL: A Robust MARL Algorithm}



Similar to other robust MARL methods, we propose the Wolfpack-Adversarial Learning for MARL (WALL) framework, a robust policy designed to counter the Wolfpack attack by performing MARL on the LPA-Dec-POMDP $\tilde{\mathcal{M}}$ with the Wolfpack attacker $\pi_{\mathrm{adv}}^{\mathrm{WP}}$. We utilize well-known CTDE algorithms, including VDN \cite{sunehag2017value}, QMIX \cite{rashid2020monotonic}, and QPLEX \cite{wang2020qplex}. Detailed implementations, including loss functions for the planner transformer and the value functions, are provided in Appendix \ref{appsubsec:marl}. The proposed WALL framework is illustrated in Fig. \ref{fig:framework} and summarized in Algorithm \ref{alg:wolfpack}.

\vspace{-.5em}

\begin{algorithm}[htb]
   \caption{WALL framework}
   \label{alg:wolfpack}
\begin{algorithmic}[1]
   \STATE {\bfseries Initialize:} Value function $Q^{tot}$, Planning Transformer
   \FOR{each training iteration}
       \FOR{each environment step $t$}
           \STATE Sample the action $\mathbf{a}_t$: $a_t^i\sim \epsilon$-greedy($Q^i$)
           \STATE Compute \( P_{t,\mathrm{attack}} \) using Planner and sample \(t_{\mathrm{init}}\)
           \IF{$t = t_{\mathrm{init}}$}
               \STATE Perform the initial attack: $\tilde{\mathbf{a}}_t \sim \pi_{\mathrm{adv}}^{\mathrm{WP}}$
           \ELSIF{$t_{\mathrm{init}}+1 \leq t \leq t_{\mathrm{init}}+t_{\mathrm{WP}}$}
               \STATE Select the follow-up agent group $\mathcal{N}_{\mathrm{follow-up}}$
               \STATE Perform the follow-up attack: $\tilde{\mathbf{a}}_t \sim \pi_{\mathrm{adv}}^{\mathrm{WP}}$
           \ELSE
               \STATE Execute the original action $\mathbf{a}_t$
           \ENDIF
       \ENDFOR
       \STATE Update the $Q^{tot}$ using a CTDE algorithm
       \STATE Update the Planning Transformer
   \ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-1em}

\section{Experiments}
\label{sec:exp}

In this section, we evaluate the proposed methods in the Starcraft II Multi-Agent Challenge (SMAC) \cite{samvelyan2019starcraft} environment, a benchmark in MARL research. Specifically, we compare: (1) the impact of the proposed Wolfpack adversarial attack against other adversarial attacks, and (2) the robustness of the WALL framework in defending against such attacks compared to other robust MARL methods. Also, an ablation study analyzes the effect of the proposed components and hyperparameters on robustness. All results are reported as the mean and standard deviation (shaded areas for graphs and $\pm$ values for tables) across 5 random seeds.


\begin{table*}[h!]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{\diagbox[innerwidth=5cm,dir=NW]{Method}{Map Name}}
& \begin{tabular}[c]{@{}c@{}}2s3z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3m \\ $K_{\mathrm{WP}}=1$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3s\_vs\_3z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}8m \\ $K_{\mathrm{WP}}=1$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}MMM \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}1c3s5z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular}  
& Mean \\ 
\hline
\multirow{7}{*}{Natural} 
& Vanilla QMIX & $98.0\pm1.52$ & $99.2\pm1.03$ & $99.2\pm1.62$ & $97.6\pm2.10$ & $99.2\pm0.54$ & $99.1\pm1.16$ & $98.7\pm0.56$ \\
& RANDOM & $\mathbf{99.7\pm0.56}$ & $99.1\pm1.26$ & $99.0\pm0.88$ & $99.2\pm1.03$ & $\mathbf{99.6\pm0.62}$ & $99.3\pm1.06$ & $99.3\pm0.21$ \\
& RARL & $97.8\pm2.09$ & $93.8\pm3.29$ & $93.1\pm17.4$ & $95.5\pm3.75$ & $90.6\pm20.8$ & $84.0\pm33.7$ & $92.5\pm5.39$ \\
& RAP & $98.8\pm1.37$ & $95.8\pm4.45$ & $99.5\pm1.08$ & $94.7\pm6.79$ & $95.5\pm12.1$ & $84.2\pm16.9$ & $94.7\pm2.62$ \\
& ERNIE & $98.2\pm1.36$ & $99.2\pm1.24$ & $99.8\pm0.44$ & $\mathbf{99.8\pm0.54}$ & $98.5\pm1.78$ & $99.2\pm1.05$ & $99.1\pm0.59$ \\
& ROMANCE & $96.4\pm2.97$ & $93.6\pm13.7$ & $99.7\pm0.59$ & $99.6\pm0.62$ & $96.4\pm6.62$ & $96.5\pm4.38$ & $97.0\pm2.11$ \\
& WALL (ours) & $99.4\pm0.65$ & $\mathbf{99.7\pm0.82}$ & $\mathbf{99.8\pm0.67}$ & $99.3\pm0.59$ & $99.0\pm2.12$ & $\mathbf{99.5\pm0.62}$ & $\mathbf{99.4\pm0.53}$ \\
\hline
\multirow{7}{*}{Random Attack} 
& Vanilla QMIX & $80.4\pm3.24$ & $69.6\pm10.4$ & $91.4\pm4.61$ & $69.0\pm7.19$ & $66.4\pm20.5$ & $94.8\pm3.45$ & $78.6\pm2.22$ \\
& RANDOM & $90.8\pm3.87$ & $76.4\pm17.3$ & $97.4\pm0.68$ & $80.2\pm4.92$ & $95.4\pm5.31$ & $96.0\pm2.91$ & $89.4\pm2.54$ \\
& RARL & $86.8\pm4.06$ & $58.7\pm15.4$ & $89.2\pm20.4$ & $70.2\pm5.83$ & $84.2\pm5.41$ & $79.1\pm22.2$ & $78.0\pm7.26$ \\
& RAP & $91.0\pm4.73$ & $69.2\pm11.1$ & $97.8\pm1.62$ & $85.0\pm11.4$ & $86.7\pm30.3$ & $86.6\pm12.5$ & $86\pm3.74$ \\
& ERNIE & $83.2\pm6.94$ & $65.2\pm4.92$ & $90.2\pm9.23$ & $76.2\pm11.8$ & $86.0\pm18.2$ & $95.6\pm3.24$ & $82.7\pm5.27$ \\
& ROMANCE & $90.2\pm2.39$ & $71.6\pm10.88$ & $99.6\pm0.68$ & $84.8\pm5.00$ & $86.8\pm16.3$ & $94.0\pm1.96$ & $87.8\pm2.44$ \\
& WALL (ours) & $\mathbf{94.6\pm4.53}$ & $\mathbf{87.4\pm1.88}$ & $\mathbf{99.8\pm0.56}$ & $\mathbf{95.8\pm3.45}$ & $\mathbf{99.4\pm1.11}$ & $\mathbf{98.6\pm1.67}$ & $\mathbf{95.9\pm0.54}$ \\
\hline
\multirow{7}{*}{EGA} 
& Vanilla QMIX & $54.0\pm7.60$ & $66.5\pm15.5$ & $72.4\pm15.1$ & $71.2\pm20.0$ & $70.6\pm14.8$ & $83.0\pm2.61$ & $69.6\pm3.84$ \\
& RANDOM & $65.3\pm3.31$ & $70.6\pm38.6$ & $68.8\pm23.5$ & $87.5\pm5.53$ & $84.4\pm3.12$ & $84.5\pm2.97$ & $76.9\pm5.44$ \\
& RARL & $62.6\pm9.40$ & $74.4\pm12.4$ & $88.4\pm17.7$ & $78.4\pm9.16$ & $83.4\pm11.9$ & $80.1\pm11.4$ & $77.9\pm9.21$ \\
& RAP & $70.4\pm13.0$ & $84.4\pm7.37$ & $83.8\pm15.8$ & $86.2\pm3.81$ & $83.9\pm16.4$ & $80.2\pm5.41$ & $81.5\pm4.38$ \\
& ERNIE & $52.4\pm9.68$ & $60.4\pm20.1$ & $83.2\pm9.75$ & $81.6\pm8.54$ & $85.0\pm4.65$ & $93.6\pm2.26$ & $76.0\pm3.00$ \\
& ROMANCE & $79.8\pm2.83$ & $85.8\pm4.60$ & $91.0\pm5.19$ & $90.9\pm4.09$ & $87.8\pm11.7$ & $89.6\pm2.99$ & $87.5\pm1.69$ \\
& WALL (ours) & $\mathbf{88.6\pm5.43}$ & $\mathbf{87.0\pm5.45}$ & $\mathbf{98.7\pm0.84}$ & $\mathbf{95.8\pm2.96}$ & $\mathbf{94.2\pm3.85}$ & $\mathbf{97.0\pm1.32}$ & $\mathbf{93.6\pm1.58}$ \\
\hline
\multirow{7}{*}{\makecell[c]{Wolfpack\\ Adversarial \\ Attack (ours)}} 
& Vanilla QMIX & $39.8\pm7.52$ & $31.0\pm11.8$ & $84.4\pm4.78$ & $11.4\pm13.3$ & $10.4\pm14.3$ & $59.2\pm4.25$ & $39.4\pm4.57$ \\
& RANDOM & $60.4\pm27.4$ & $57.4\pm30.5$ & $91.0\pm3.17$ & $40.4\pm14.8$ & $63.6\pm28.7$ & $68.4\pm19.6$ & $63.5\pm3.14$ \\
& RARL & $52.4\pm15.8$ & $31.1\pm20.3$ & $90.0\pm17.4$ & $14.2\pm9.71$ & $51.1\pm36.8$ & $75.9\pm13.6$ & $52.5\pm8.02$ \\
& RAP & $60.0\pm10.3$ & $37.5\pm10.4$ & $95.4\pm3.98$ & $35.6\pm14.4$ & $47.0\pm36.9$ & $75.7\pm26.1$ & $58.5\pm5.79$ \\
& ERNIE & $43.2\pm13.0$ & $35.4\pm6.25$ & $94.8\pm4.42$ & $26.4\pm10.4$ & $26.2\pm17.3$ & $77.0\pm9.33$ & $50.5\pm6.62$ \\
& ROMANCE & $62.4\pm5.16$ & $34.8\pm14.3$ & $98.6\pm0.68$ & $28.6\pm14.2$ & $48.8\pm17.9$ & $81.2\pm4.51$ & $59.1\pm2.08$ \\
& WALL (ours) & $\mathbf{92.2\pm3.77}$ & $\mathbf{90.8\pm4.92}$ & $\mathbf{99.8\pm0.56}$ & $\mathbf{83.6\pm5.01}$ & $\mathbf{95.0\pm4.56}$ & $\mathbf{98.8\pm1.62}$ & $\mathbf{93.4\pm1.18}$ \\
\hline
\end{tabular}}
\caption{Average test win rates of robust MARL policies under various attack settings}
\vspace{-0.2in}
\label{table:perf}
\end{table*}



\subsection{Environmental Setup}


The SMAC environment serves as a challenging benchmark requiring effective agent cooperation to defeat opponents. We evaluate the proposed method across six scenarios: \texttt{2s3z}, \texttt{3m}, \texttt{3s\_vs\_3z}, \texttt{8m}, \texttt{MMM}, and \texttt{1c3s5z}. We perform parameter searches for the number of follow-up agents $m$, total Wolfpack attacks $K_{\mathrm{WP}}$, and attack duration $t_{\mathrm{WP}}$, using optimal settings for comparisons. To ensure realistic constraints, we set $m$ to $m < \left\lfloor \frac{n-1}{2} \right\rfloor$, where $n$ is the maximum number of allied units. We provide details on environment setups and experimental configurations, including hyperparameter settings, in Appendices \ref{appsec:env} and \ref{appsec:expdetail}. All MARL methods are evaluated on the QMIX baseline, with comparison for other CTDE baselines in Appendix \ref{appsubsec:compotherctde}.

{\bf Adversarial Attacker Baselines:} 
To compare the severity of different attacks, we consider the following 4 scenarios: \textbf{Natural}, representing the case where no attacks are performed; \textbf{Random Attack}, where time steps, agents, and actions are randomly selected to execute attacks; \textbf{Evolutionary Generation of Attackers (EGA)} \cite{yuan2023robust}, which combines multiple single-agent-targeted attackers generated from various seeds as described in Section \ref{sec:background}; and the proposed \textbf{Wolfpack Adversarial Attack}. For a fair comparison, adversarial attackers are trained on independent seeds to execute unseen attacks.


{\bf Robust MARL Baselines:} To compare the severity of attack baselines and the robustness of policies trained under adversarial attack scenarios, we evaluate QMIX-trained policies under the following attack conditions: \textbf{Vanilla QMIX}, assuming no adversarial attacks; \textbf{RANDOM}, using Random Attack; \textbf{RARL} \cite{pinto2017robust}, where adversarial attackers tailor attacks to the learned policy; \textbf{RAP} \cite{vinitsky2020robust}, an extension of RARL that uniformly samples attackers to prevent overfitting and introduce diversity; \textbf{ROMANCE} \cite{yuan2023robust}, an RAP extension countering diverse EGA attacks; \textbf{ERNIE} \cite{bukharin2024robust}, enhancing robustness via adversarial regularization in observations and actions; and the proposed \textbf{WALL}. 


All robust MARL methods follow author-provided methodologies and parameters. Further details on the MARL baselines are available in Appendix \ref{appsec:marlbase}. All policies are trained for 3M timesteps, starting from a pretrained Vanilla QMIX model trained for 1M timesteps.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/wolfpack_attack_v2.pdf} 
    \vspace{-0.3in}
    \caption{Learning curves of MARL methods for Wolfpack attack}
    \label{fig:learning_curves}
    \vspace{-0.3in}
\end{figure}

\subsection{Performance Comparison}


\begin{figure*}[ht!]
    \begin{center}
    \centerline{\includegraphics[width=0.98\textwidth]{figures/visualization.pdf}}
    \vspace{-0.1in}
    \caption{Attack comparison on 2s3z task in the SMAC: (a) QMIX/Natural, (b) QMIX/Wolfpack attack, and (c) WALL/Wolfpack attack}
    \label{fig:visualization}
    \vspace{-0.3in}
    \end{center}
\end{figure*}
Table \ref{table:perf} presents the average win rates over the last 100 episodes of MARL policies against attacker baselines. The results show that the proposed Wolfpack adversarial attack is significantly more powerful than existing methods like EGA and Random Attack. For example, EGA reduces Vanilla QMIX performance by $98.7 - 69.6 = 29.1\%$ and RANDOM by $99.3 - 76.9 = 22.4\%$ compared to the Natural scenario. In contrast, the Wolfpack attack reduces Vanilla QMIX performance by $98.7 - 39.4 = 59.3\%$ and RANDOM by $99.3 - 63.5 = 35.8\%$, demonstrating its greater impact. In addition, the proposed WALL framework, trained to defend against the Wolfpack attack, outperforms other robust MARL methods against all attack types, showcasing its superior robustness. Notably, despite RANDOM is trained against Random Attack and ROMANCE against EGA, WALL achieves better performance against both attack types. This result highlights the effectiveness of WALL in enabling robust learning under diverse attack scenarios. 
Fig. \ref{fig:learning_curves} further illustrates this in the \texttt{8m} and \texttt{MMM} environments, where performance differences with existing methods are most pronounced, showing the average win rate of each policy over training steps under unseen Wolfpack adversarial attacks. The results reveal that WALL not only achieves higher robustness but also adapts more quickly to attacks. Similar trends are observed for other CTDE algorithms, such as VDN and QPLEX, as detailed in Appendix \ref{appsubsec:compotherctde}, confirming the robustness of the proposed method.




\subsection{Visualization of Wolfpack Adversarial Attack}

To analyze the superior performance of the Wolfpack attack, we provide a visualization of its execution in the SMAC environment. Fig. \ref{fig:visualization} illustrates a scenario where the proposed step selector identifies $t=6$ as a critical initial step to initiate the attack. Prior to $t=6$, all setups are assumed to follow the same trajectory. Fig. \ref{fig:visualization}(a) shows Vanilla QMIX in a Natural scenario without attack, where our agents successfully defeat all enemy agents, achieving victory. Fig. \ref{fig:visualization}(b) demonstrates Vanilla QMIX under the Wolfpack adversarial attack, with follow-up agents targeted during $t=7$ to $t=9$. This leaves other agents unable to effectively defend against the adversarial attack, resulting in defeat as all agents are killed by the enemy. Fig. \ref{fig:visualization}(c) highlights a policy trained with the WALL framework. Despite the same follow-up agents are targeted during $t=7$ to $t=9$, WALL trains non-attacked agents to back up and protect the attacked agents, enabling ally agents to eliminate enemy agents and secure victory. This visualization demonstrates how the Wolfpack attack disrupts agent coordination and how the WALL framework robustly defends against such attacks. Visualizations of other SMAC tasks and detailed follow-up agent selection are provided in Appendix \ref{appsec:vis}.

\subsection{Ablation Study}


To evaluate the impact of each component and hyperparameter in the proposed Wolfpack adversarial attack, we conduct an ablation study focusing on the following aspects: component evaluation, step selection temperature $T$, and the number of follow-up agents $m$. The ablation study is conducted in the \texttt{8m} and \texttt{MMM} environments, where the performance differences are most pronounced. Additionally, more ablation studies on other hyperparameters, such as the total number of Wolfpack attacks $K_{\mathrm{WP}}$ and the attack duration $t_{\mathrm{WP}}$, are provided in Appendix \ref{appsec:addabl}.

{\bf Component Evaluation:} 
To evaluate the impact of each proposed component on attack severity and policy robustness, we consider five setups:    `Default', which uses all proposed components as designed; `Init. agent (min)', where the initial target agent $i$ is selected to minimize $Q^{tot}$, i.e., $i = \arg\min_j {\min_{a^j} Q^{tot}(s_{t_{\mathrm{init}}}, a_{t_{\mathrm{init}}}^j, \mathbf{a}_{t{\mathrm{init}}}^{-j})},$ instead of random selection; `Follow-up (L2)', which selects $m$ agents closest to the initial agent based on L2 distance instead of the proposed follow-up selection method; `Step (Random)', which uses random step selection instead of the proposed step selection method, while keeping the same total number of attacks; and `Agents \& Step (Random)', which randomly selects both $m$ follow-up agents and attack steps.

For each setup, we train the Wolfpack adversarial attack and the corresponding robust policy of WALL. Fig. \ref{fig:component}(a) shows the robustness of WALL trained under each setup when exposed to the default Wolfpack attack, while Fig. \ref{fig:component}(b) illustrates how each attack setup degrades the performance of `Vanilla QMIX' compared to its `Natural' performance. Randomly selecting the initial agent yields more robust policies than selecting the agent minimizing $Q^{tot}$ (`Init. agent (min)'), as random selection introduces diversity in attack scenarios. While selecting the $Q^{tot}$-minimizing agent may slightly enhance attack severity in cases like \texttt{MMM}, the added diversity from random selection generally improves robustness. Comparing `Default' and `Follow-up (L2)' shows that the proposed follow-up selection method enables more severe attacks and trains more robust policies than simply targeting agents closest to the initial agent. Similarly, `Default' outperforms `Step (Random)' in both attack severity and robustness, demonstrating that the proposed planner effectively identifies critical steps to minimize $Q^{tot}$, producing stronger policies. Finally, `Default' achieves significantly better robustness and more critical attacks compared to `Agents \& Step (Random)', highlighting the combined effectiveness of the proposed components.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.9\columnwidth} % Width increased
        \centering
        \includegraphics[width=1.0\textwidth]{figures/component_model_v3.pdf} % Slightly enlarged
        \vspace{-0.25in}
        \caption{Robustness of WALL variations}  % Label for first image
    \end{subfigure}
    % \hspace{-0.02\textwidth} % Reduced horizontal space between subfigures
    \begin{subfigure}{0.9\columnwidth} % Width increased
        \centering
        \includegraphics[width=1.0\textwidth]{figures/component_attack_v3.pdf} % Slightly enlarged
        \caption{Degradation of Vanilla QMIX under each attack setup}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Component Evaluation of WALL and Wolfpack attack}
    \vspace{-0.2in}
    \label{fig:component}
\end{figure}






{\bf Number of Follow-up Agents $m$:} To analyze the impact of the hyperparameter $m$, which determines the number of follow-up agents, on robustness, Fig. \ref{fig:numfollow} shows how WALL trained with different values of $m$ defend against the default Wolfpack attack. To prevent excessive attack that could cause learning to fail, we assume $m < \left\lfloor \frac{n-1}{2} \right\rfloor$. The results indicate that in the \texttt{8m} environment, when $m$ is small, only a few agents defending against the initial attack are targeted, leading to reduced robustness. Conversely, when $m=4$, too many agents are attacked, causing learning to deteriorate. Therefore, $m=3$ yields the most robust performance and is considered the default hyperparameter. Similarly, in the \texttt{MMM} environment, $m=4$ results in the most robust performance and is set as the default. Notably, when $m=1$, the attack becomes a single-agent attack. As discussed in Section \ref{subsec:motiv}, performing coordinated multi-agent attack ($m>1$) enables much more robust learning, demonstrating the effectiveness and superiority of the proposed Wolfpack attack framework.
\begin{figure}[t!]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/secondagent_8m_v6.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/secondagent_MMM_v5.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Number of Follow-up Agents ($m$)}
    \label{fig:numfollow}
\end{figure}


\begin{figure}[t!]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/temp_8m_v6.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/temp_MMM_v5.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Step Selection Temperature ($T$)}
    \vspace{-0.25in}
    \label{fig:temp}
\end{figure}
{\bf Step Selection Temperature $T$:} $T$ is a hyperparameter in Eq. \eqref{eq:initialprob} that controls the temperature of the initial attack probability. A larger $T$ results in more random attacks across steps, while a smaller $T$ focuses attacks on critical steps with higher probability. Fig. \ref{fig:temp} illustrates the performance of WALL policies trained with varying values of $T$ against the default Wolfpack attack. In both the \texttt{8m} and \texttt{MMM} environments, a very small $T$ causes the attack to target only specific steps, leading to policies that are less robust against diverse attacks. Conversely, a very large $T$ leads to overly uniform attacks, failing to target critical steps effectively, which also results in less robust policies. Based on these findings, we determined that $T = 0.5$ strikes a balance between targeting critical steps and maintaining robustness, and we set this as the default parameter.


\section{Conclusions}




In this paper, we propose the Wolfpack adversarial attack, a coordinated strategy inspired by the Wolfpack tactic used in military operations, which significantly outperforms existing adversarial attacks. Additionally, we develop WALL, a robust MARL method designed to counter the proposed attack, demonstrating superior performance across various SMAC environments. Overall, our WALL framework enhances the robustness of MARL algorithms.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\counterwithin{table}{section}
\counterwithin{figure}{section} 
\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\section{Proof}

\setcounter{equation}{0}
%\section{Algorithms}
%\subsection{Wolfpack algorithms}
%\subsection{Baselines algorithms}

\section{Environmental Setup}
\label{appsec:env}
All experiments in this paper are conducted in the SMAC \cite{samvelyan2019starcraft} environment, and this section provides a detailed description of its setup and features.

\textbf{SMAC} \\
The StarCraft Multi-Agent Challenge (SMAC) serves as a benchmark for cooperative Multi-Agent Reinforcement Learning (MARL), focusing on decentralized micromanagement tasks. Based on the real-time strategy game StarCraft II, SMAC requires each unit to be controlled by independent agents acting solely on local observations. It offers a variety of combat scenarios to evaluate MARL methods effectively.

\textbf{Scenarios} \\
SMAC scenarios involve combat situations between an allied team controlled by learning agents and an enemy team managed by a scripted AI. These scenarios vary in complexity, unit composition, and terrain, challenging agents to use advanced micromanagement techniques such as focus fire, kiting, and terrain exploitation. Scenarios end when all units on one side are eliminated or when the time limit is reached. The objective is to maximize the win rate of the allied agents. Detailed descriptions of the scenarios and unit compositions are provided in \cref{tab:scenarios} and \cref{fig:scenarios}.

\textbf{States and observations} \\
In the SMAC environment, each agent receives partial observations, which contain information about visible allies and enemies within a fixed sight range of 9 units. These observations do not include global state information and are limited to the agent's local view of the environment. Observations are specifically designed to support decentralized decision-making by individual agents, without access to global state features.

The global state, used during centralized training, aggregates the features of all agents and enemies. The state dimensionality depends on:
\begin{itemize}
    \item Ally state: A variable size based on the number of agents ($n_{\text{agents}}$), including their \texttt{relative x} (dim = $1$), \texttt{relative y} (dim = $1$), \texttt{health} (dim = $1$), \texttt{energy} (dim = $1$), \texttt{shield} (dim = $\mathrm{shield\_dim}$), and \texttt{unit type} (dim = $\mathrm{unit\_type\_dim}$). For each ally, the total feature size is $n_{\text{agents}} \times (4 + \mathrm{shield\_dim} + \mathrm{unit\_type\_dim})$.
    \item Enemy state: Similar to Ally state but excludes \texttt{energy}. For each enemy, the total feature size is $n_{\text{enemies}} \times (3 + \mathrm{shield\_dim} + \mathrm{unit\_type\_dim})$.
    \item Last action: Represents the most recent action taken by each agent. The total feature size is $n_{\text{agents}} \times n_{\text{actions}}$. Here, $n_{\text{actions}}$ refers to the number of  actions. 
\end{itemize}

These features are concatenated to form the observation vector for each agent. The dimensionality of the observation vector depends on the following factors:
\begin{itemize}
    \item Movement features: A fixed size representing available movement directions (dim = $4$).
    \item Enemy features: A variable size based on the number of enemies ($n_{\text{enemies}}$), including their \texttt{available action} (dim = $1$), \texttt{distance} (dim = $1$), \texttt{relative x} (dim = $1$), \texttt{relative y} (dim = $1$), \texttt{health} (dim = $1$), \texttt{shield} (dim = $\mathrm{shield\_dim}$, where $\mathrm{shield\_dim}=1$ for Protoss units and $0$ otherwise), and \texttt{unit type} (dim = $\mathrm{unit\_type\_dim}$, where $\mathrm{unit\_type\_dim} = 0$ if there is 1 agent type, otherwise equals the number of agent types). For each enemy, the total feature size is $n_{\text{enemies}} \times (5 + \mathrm{shield\_dim} + \mathrm{unit\_type\_dim})$.
    \item Ally features: A variable size based on the number of allies ($n_{\text{allies}}$), where $n_{\text{allies}} = n_{\text{agents}} - 1$, similar to Enemy features. For each ally, the total feature size is $n_{\text{allies}} \times (5 + \mathrm{shield\_dim} + \mathrm{unit\_type\_dim})$.
    \item Own features: Includes \texttt{health} (dim = $1$), \texttt{shield} (dim = $\mathrm{shield\_dim}$), and \texttt{unit type} (dim = $\mathrm{unit\_type\_dim}$). The total feature size is $1 + \mathrm{shield\_dim} + \mathrm{unit\_type\_dim}$.
\end{itemize}
The precise dimensions of the observation and state vectors vary across different maps, as summarized in Table \ref{tab:scenarios}.

\textbf{Action space} \\
Agents can perform discrete actions, including movement in four cardinal directions (North, South, East, West), attacking specific enemy units within a shooting range of 6 units, and specialized actions such as healing for units like Medivacs. Additionally, agents can perform a stop or a no-op action, the latter being restricted to dead units. 

The size of the action space varies depending on the scenario and is defined as $n_{\text{actions}} = 6 + n_{\text{enemies}}$, where $6$ represents movement, stop, or a no-op action. The inclusion of $n_{\text{enemies}}$ accounts for the need to specify which enemy unit is targeted when performing an attack action. The exact size of the action space varies across different maps, as summarized in Table \ref{tab:scenarios}.

\textbf{Rewards} \\
SMAC uses a shaped reward function $R$ to guide learning, including components for damage dealt ($R_{\text{damage}}$), enemy units killed ($R_{\text{enemy\_killed}}$), and scenario victory ($R_{\text{win}}$). The total reward is defined as:
\begin{align*}
R &= \sum_{e \in \text{enemies}} \Delta \text{Health}(e) + \sum_{e \in \text{enemies}} \mathbb{I}(\{\text{Health}(e) = 0\}) \cdot \text{Reward}_{\text{death}} + \mathbb{I}(\{\text{win} = \text{True}\}) \cdot \text{Reward}_{\text{win}} 
\end{align*}
Here, $\text{Health}(e)$ represents the health of an enemy unit $e$, and $\Delta \text{Health}(e)$ is the reduction in its health during a timestep. The indicator function $\mathbb{I}(\cdot)$ returns $1$ if the condition inside is true and $0$ otherwise. The parameters $\text{Reward}_{\text{death}}$ and $\text{Reward}_{\text{win}}$ are scaling factors for rewards when an enemy unit is killed and when the agents win the scenario, set to $10$ and $200$, respectively.

\begin{table}[ht]
    \centering
    \begin{tabular}{llllll}
    \toprule
    \textbf{Map} & \textbf{Ally Units} & \textbf{Enemy Units} & \textbf{State Dim} & \textbf{Obs Dim} & \textbf{Action Space} \\
    \midrule
    3m           & 3 Marines                         & 3 Marines   & 48   & 30  & 9  \\
    \midrule
    3s\_vs\_3z    & 3 Stalkers                          & 3 Zealots   & 54   & 36  & 9  \\
    \midrule
    2s3z          & 2 Stalkers,                        & 2 Stalkers,  & 120  & 80  & 11 \\
                          & 3 Zealots                        & 3 Zealots  &       &     &    \\
    \midrule
    8m           & 8 Marines                         & 8 Marines & 168  & 80  & 14 \\
    \midrule
    1c3s5z        & 1 Colossus,                       & 1 Colossus,  & 270  & 162  & 15 \\
                          & 3 Stalkers,                       & 3 Stalkers,  &       &     &    \\
                          & 5 Zealots                        & 5 Zealots   &       &     &    \\
    \midrule
    MMM          & 1 Medivac,                       & 1 Medivac,   & 290  & 160 & 16 \\
                          & 2 Marauders,                      & 2 Marauders, &       &     &    \\
                          & 7 Marines                        & 7 Marines   &       &     &    \\
    \bottomrule
    \end{tabular}
    \caption{SMAC Scenarios with Units and Dimensions of State, Observation, and Action Space}
    \label{tab:scenarios}
\end{table}

\begin{figure}[ht!]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/3m.pdf}
        \caption{3m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/3s_vs_3z.pdf}
        \caption{3s\_vs\_3z}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/2s3z.pdf}
        \caption{2s3z}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/8m.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/1c3s5z.pdf}
        \caption{1c3s5z}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/MMM.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Visualization of SMAC Scenarios}
    \label{fig:scenarios}
\end{figure}

\newpage

\section{Implementation Details}

In this section, we provide a detailed implementation of the proposed WALL framework. Section \ref{appsubsec:trans} outlines the implementation of the transformer used to efficiently identify critical initial steps. Section \ref{appsubsec:marl} elaborates on the components constituting the Wolfpack attack and provides an in-depth explanation of the reinforcement learning implementation.

\subsection{Practical Implementation of Planner Transformer}
\label{appsubsec:trans}

Critical attacking step selection introduced in Section \ref{subsec:attackstep} requires planning to compute the reduction in $Q$-function values, $\Delta Q_l^{\mathrm{WP}}$ ($l=t, \cdots, t+L-1$), over multiple time steps. To facilitate this process, a transformer model is employed. During training, the transformer predicts $(\hat{s}_{t+1}, \hat{\mathbf{o}}_{t+1}, \cdots, \hat{s}_{t+t_{\mathrm{WP}}}, \hat{\mathbf{o}}_{t+t_{\mathrm{WP}}})$ at the current step $t$ to compute the target $\hat{\Delta}Q_t^{\mathrm{WP}}$. However, performing this planning process at each evaluation step to calculate the probability of an initial attack, $P_{t,\mathrm{attack}}$, is computationally expensive.

To address this, the single transformer is split into two components: a planning transformer and a $Q$-difference transformer. The planning transformer predicts $(\hat{s}_{t+1}, \hat{\mathbf{o}}_{t+1})$ and is used only during training, while the $Q$-difference transformer predicts $\Delta Q_l^{\mathrm{WP}}$ ($l=t, \cdots, t+L-1$) and is employed exclusively during evaluation. This separation enables efficient computation of $P_{t,\mathrm{attack}}$ during evaluation, significantly reducing computational costs. Both transformers adopt the Decision Transformer structure \cite{chen2021decision}, consisting of transformer decoder layers. The planning transformer is parameterized by $\phi_{\mathrm{planning}}$, and the $Q$-difference transformer by $\phi_{\mathrm{qdiff}}$. Their respective loss functions are defined as:
\begin{align}
    \mathcal{L}_{\mathrm{planning}}(\phi_{\mathrm{planning}}) &= \mathbb{E} \left[ \| s_{t+1} - \hat{s}_{t+1}(\phi_{\mathrm{planning}}) \|^2 + \| \mathbf{o}_{t+1} - \hat{\mathbf{o}}_{t+1}(\phi_{\mathrm{planning}}) \|^2 \right], \\
    \mathcal{L}_{\mathrm{qdiff}}(\phi_{\mathrm{qdiff}}) &= \mathbb{E} \left[ \| \Delta Q_t^{\mathrm{WP}} - \hat{\Delta} Q_t^{\mathrm{WP}}(\phi_{\mathrm{qdiff}}) \|^2 \right],~\forall t.
\end{align}
As shown in Fig. \ref{fig:transformer_str}, the estimated state $\hat{s}_{t+1}$ and observations $\hat{\mathbf{o}}_{t+1}$ are generated by the planning Transformer, which takes previous trajectories as input. Similarly, the estimated $Q$-difference $\hat{\Delta} Q_t^{\mathrm{WP}}$ is produced by the $Q$-diff Transformer, using previous states as input. The loss function $\mathcal{L}_{\mathrm{planning}}$ minimizes the prediction error for the next state $s{t+1}$ and observation $\mathbf{o}{t+1}$, while $\mathcal{L}_{\mathrm{qdiff}}$ minimizes the prediction error for $\Delta Q_l^{\mathrm{WP}}$ resulting from the Wolfpack attack. \cref{fig:transformer_str} illustrates the architectures of the transformers, where (a) represents the planning Transformer and (b) represents the $Q$-diff Transformer.

\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.6\columnwidth} % Width increased
        \centering
        \includegraphics[width=1.0\textwidth]{figures/planning_transformer.pdf} % Slightly enlarged
        \vspace{-0.25in}
        \caption{Planning Transformer}  % Label for first image
    \end{subfigure}
    % \hspace{-0.02\textwidth} % Reduced horizontal space between subfigures
    \begin{subfigure}{0.6\columnwidth} % Width increased
        \centering
        \includegraphics[width=1.0\textwidth]{figures/q_diff_transformer.pdf} % Slightly enlarged
        \caption{$Q$-difference Transformer}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Structure of Transformers}
    % \vspace{-0.1in}
    \label{fig:transformer_str}
\end{figure}
\newpage
\subsection{Detailed Implementation of WALL}
\label{appsubsec:marl}



The WALL framework trains robust MARL policies to counter the Wolfpack adversarial attack by employing a $Q$-learning approach within the CTDE paradigm. Each agent computes its individual $Q$-values, $Q^i(\tau_t^i,a_t^i),~i=1,\cdots,n$, using separate $Q$-networks. These individual values, along with the global state, are combined through a mixing network to produce the total $Q$-value, $Q_\theta^{tot}$, parameterized by $\theta$. This joint value function ensures effective coordination among agents under adversarial scenarios.

To achieve robustness, the training process minimizes the temporal difference (TD) loss, which incorporates the observed rewards, state transitions, and target $Q$-values parameterized by a separate target network, $\theta^-$. By leveraging this target network, the CTDE frameworks stabilize learning and mitigates the impact of the Wolfpack attack. The TD loss is defined as:
\begin{equation}
\mathcal{L}_{\text{TD}}(\theta) = \mathbb{E}_{s, \mathbf{a}, r, s'} \left[ \left( r_t + \gamma \max_{\mathbf{a}'} Q_{\theta^-}^{tot}(s_{t+1}, \mathbf{a}') - Q_\theta^{tot}(s_t, \mathbf{a}_t) \right)^2 \right],
\label{eq:td_loss}
\end{equation}
where the target network parameter $\theta^-$ is updated by applying the exponential moving average (EMA) to $\theta$. This training mechanism allows agents to adapt and develop robust policies capable of resisting the coordinated disruptions caused by the Wolfpack adversarial attack, ensuring enhanced performance and resilience in MARL scenarios. 
In addition, we use 3 value-based CTDE algorithms as baselines for the WALL framework: QMIX, VDN, and QPLEX. Below, we provide an outline of the key details of these baseline algorithms:











\textbf{Value-Decomposition Networks (VDN)} \\
VDN \cite{sunehag2017value} is a $Q$-learning algorithm designed for cooperative MARL. It introduces an additive decomposition of the joint $Q$-value into individual agent $Q$-values, enabling centralized training and decentralized execution. The joint action-value function, $Q^{tot}$, is expressed as:
\begin{equation}
    Q^{tot}(s_t, \mathbf{a}_t) = \sum_{i=1}^{n} Q^i(\tau_t^i, a_t^i),
\end{equation}
allowing agents to act independently during execution by relying only on their local $Q^i$ values. \\

\textbf{QMIX} \\
QMIX \cite{rashid2020monotonic} extends VDN by introducing a more expressive, non-linear representation of the joint $Q$-value, while maintaining a monotonic relationship between $Q^{tot}$ and individual agent $Q$-values, $Q^i(\tau^i, a^i)$. This ensures individual-global-max (IGM) condition:
\begin{equation}
    \frac{\partial Q^{tot}}{\partial Q^i} \geq 0, \forall i,
\end{equation}
guaranteeing consistency between centralized and decentralized policies. Specifically:
\begin{equation}
\arg\max_{\mathbf{a}} Q^{\text{tot}}(s_t, \mathbf{a}_t) = 
\begin{pmatrix}
\arg\max_{a^1} Q^{1}(\tau_t^1, a_t^1), \\
\vdots \\
\arg\max_{a^n} Q^{n}(\tau_t^n, a_t^n)
\end{pmatrix}.  
\end{equation}
QMIX combines agent networks, a mixing network, and hypernetworks, where hypernetworks dynamically parameterize the mixing network based on the global state $s_t$. The weights generated by the hypernetworks are constrained to be non-negative to enforce the monotonicity constraint. \\

\textbf{QPLEX} \\
QPLEX \cite{wang2020qplex} introduces a duplex dueling architecture to enhance the representation of joint action-value functions while adhering to the IGM principle. QPLEX reformulates the IGM principle in an advantage-based form:
\begin{equation}
    \arg\max_{\mathbf{a}} A^{tot}(\tau, \mathbf{a}) = \begin{pmatrix} \arg\max_{a^1} A^1(\tau^1, a^1), \\
\vdots \\ \arg\max_{a^n} A^n(\tau^n, a^n) \end{pmatrix},
\end{equation}
where $A^{tot}$ and $A^i$ are the advantage functions for joint and individual action-value functions, respectively. The joint action-value function is expressed as:
\begin{equation}
    Q^{tot}(\tau, \mathbf{a}) = \sum_{i=1}^n Q^i(\tau, a^i) + \sum_{i=1}^n (\lambda_i(\tau, \mathbf{a}) - 1) A^i(\tau, a^i).
\end{equation}
where $\lambda_i(\tau, \mathbf{a}) > 0$ are importance weights generated using a multi-head attention mechanism to enhance expressiveness.

Here, VDN and QMIX are implemented using the PyMARL codebase \url{https://github.com/oxwhirl/pymarl}, while QPLEX is implemented using its official codebase \url{https://github.com/wjh720/QPLEX}.



\newpage

\section{Experimental Details}
\label{appsec:expdetail}

All experiments in this paper are conducted on a GPU server equipped with an NVIDIA GeForce RTX 3090 GPU and AMD EPYC 7513 32-Core processors running Ubuntu 20.04 and PyTorch. We follow the implementations and loss scales provided by the CTDE algorithms and focus on parameter searches for hyperparameters related to the proposed Wolfpack adversarial attack. Comparisons are performed using the optimal hyperparameter setup, with an ablation study available in Appendix \ref{appsec:addabl}.

\subsection{Hyperparameter Setup} \label{appsubsec:hyper}

We conduct parameter search for the number of Wolfpack attacks $K_{\mathrm{WP}} \in [1,2,3,4]$, the attack duration $t_{\mathrm{WP}} \in [1,2,3,4]$, the number of follow-up agents $m$, and the temperature $T \in [0.1,0.2,0.5,1.0]$. The total number of attacks $K$ is then determined based on $K=K_{\mathrm{WP}}\times (t_{\mathrm{WP}}+1)$, separated into training and testing setups. During training, $K$ is selected through hyperparameter sweeping to ensure optimal performance. For testing, $K$ is unified across all adversarial attack setups, including Random Attack, EGA, and the Wolfpack Adversarial Attack, to ensure fair comparisons. Additionally, the attack period $L$ is chosen based on the average episode length of SMAC scenarios and the total number of attacks, with $L=20$ is fixed as appropriate. Transformer hyperparameters, shared between the Planning Transformer and $Q$-difference Transformer, such as the number of heads, decoder layers, embedding dimensions, and input sequence length, are selected to balance accuracy and computational efficiency.

The $Q$-learning hyperparameters (shared across all CTDE methods) and those specific to the CTDE algorithms are detailed in Table \ref{tab:common_hyperparameters} and Table \ref{tab:vdn_qmix_qplex_hyperparameters}, respectively. The Wolfpack adversarial attack-related hyperparameters for the WALL framework, shared across all SMAC scenarios and scenario-specific setups, are presented in Table \ref{tab:wolfpack_hyperparameters}.

\begin{table}[h]
\centering
\begin{tabular}{p{0.3\textwidth} p{0.2\textwidth}}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Epsilon & 1.0 $\rightarrow$ 0.05 \\
Epsilon Anneal Time & 50000 timesteps \\
Train Interval & 1 episode \\
Gamma & 0.99 \\
Critic Loss & MSE Loss \\
Buffer Size & 5000 episodes \\
Batch Size & 32 episodes \\
Agent Learning Rate & 0.0005 \\
Critic Learning Rate & 0.0005 \\
Optimizer & RMSProp \\
Optimizer Alpha & 0.99 \\
Optimizer Eps & 1e-5 \\
Gradient Clip Norm & 10.0 \\
Num GRU Layers & 1 \\
RNN Hidden State Dim & 64 \\
Double Q & True \\
\hline
\end{tabular}
\caption{Common $Q$-learning Hyperparameters}
\label{tab:common_hyperparameters}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{p{0.3\textwidth} p{0.1\textwidth} p{0.1\textwidth} p{0.1\textwidth}}
\hline
\textbf{Hyperparameter}        & \textbf{VDN} & \textbf{QMIX} & \textbf{QPLEX} \\
\hline
Mixer                          & VDN          & QMIX          & QPLEX          \\
Mixing Embed Dim               & -            & 32            & 32             \\
Hypernet Layers                & -            & 2             & 2              \\
Hypernet Embed Dim             & -            & 64            & 64             \\
Adv Hypernet Layers            & -            & -             & 1              \\
Adv Hypernet Embed Dim         & -            & -             & 64             \\
Num Kernel                     & -            & -             & 2              \\
\hline
\end{tabular}
\caption{VDN, QMIX, QPLEX Hyperparameters}
\label{tab:vdn_qmix_qplex_hyperparameters}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{p{0.442\textwidth} p{0.442\textwidth}}
\hline
\textbf{Common Hyperparameters} & \textbf{Value} \\
\hline
Attack duration ($t_{\mathrm{WP}}$) & 3 \\
Temperature ($T$) & 0.5 \\
Attack Period ($L$) & 20 \\
Num Transformer Head & 1 \\
Num Transformer Decoder Layer & 1 \\
Transformer Embed Dim & 64 \\
Input Sequence Length & 20 \\
\hline
\end{tabular}
\vspace{0.3cm}
\begin{tabular}{p{0.28\textwidth} p{0.08\textwidth} p{0.08\textwidth} p{0.08\textwidth} p{0.08\textwidth} p{0.08\textwidth} p{0.08\textwidth}}
\hline
\textbf{Scenario} & \texttt{3m} & \texttt{3s\_vs\_3z} & \texttt{2s3z} & \texttt{8m} & \texttt{1c3s5z} & \texttt{MMM} \\
\hline
Num Total Attacks (Train) ($K$) & 8 & 16 & 12 & 8 & 16 & 16 \\
Num Total Attacks (Test) ($K$) & 8 & 4 & 8 & 4 & 8 & 8 \\
Num Wolfpack Attacks ($K_{\mathrm{WP}}$) & 2 & 4 & 3 & 2 & 4 & 4 \\
Num Follow-up Agents ($m$) & 1 & 1 & 2 & 3 & 4 & 4 \\
\hline
\end{tabular}
\caption{Wolfpack hyperparameters shared across scenarios and scenario-specific values}
\label{tab:wolfpack_hyperparameters}
\end{table}

\section{Details of Other Robust MARL Methods}
\label{appsec:marlbase}
In this section, we detail various robust MARL methods compared against the proposed WALL framework, as below:

\textbf{Robust Adversarial Reinforcement Learning (RARL)} \\
RARL \cite{pinto2017robust} enhances policy robustness by training a protagonist agent and an adversary in a two-player zero-sum Markov game. At each timestep $t$, the agents observe state $s_t$ and take actions $a_t^1 \sim \mu(s_t)$ and $a_t^2 \sim \nu(s_t)$, where $\mu$ is the protagonist’s policy, and $\nu$ is the adversary’s policy. The state transitions follow:
\begin{equation}
    s_{t+1} = P(s_t, a_t^1, a_t^2).
\end{equation}
The protagonist maximizes its cumulative reward $R^1$, while the adversary minimizes it:
\begin{equation}
    R^1_* = \min_{\nu} \max_{\mu} R^1(\mu, \nu) = \max_{\mu} \min_{\nu} R^1(\mu, \nu).
\end{equation}

\textbf{Robustness via Adversary Populations (RAP)} \\
RAP \cite{vinitsky2020robust} improves robustness by training agents against a population of adversaries, reducing overfitting to specific attack patterns. During training, an adversary is sampled uniformly from the population ${\pi_{\phi_1}, \pi_{\phi_2}, \dots, \pi_{\phi_n}}$. The objective is:
\begin{equation}
    \max_{\theta} \min_{\phi_1, \dots, \phi_n} \mathbb{E}_{i \sim U(1, n)} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t, \alpha a_t^i) \middle| \pi_\theta, \pi_{\phi_i} \right],
\end{equation}
where $\pi_\theta$ is the agent’s policy, $\pi_{\phi_i}$ is the $i$-th adversary, and $\alpha$ controls adversary strength. 


\textbf{Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE)} \\
ROMANCE \cite{yuan2023robust} generates diverse auxiliary adversarial attackers to improve robustness in CMARL. Its objective combines attack quality and diversity:
\begin{equation}
    L_{\text{adv}}(\phi) = \frac{1}{n_p} \sum_{j=1}^{n_p} L_{\text{opt}}(\phi_j) - \alpha L_{\text{div}}(\phi),
\end{equation}
where $L_{\text{opt}}$ minimizes the ego-system's return, $L_{\text{div}}$ promotes diversity using Jensen-Shannon Divergence, and $n_p$ is the number of adversarial policies. ROMANCE uses an evolutionary mechanism to explore diverse attacks.

We implement RARL and RAP for multi-agent systems, as well as ROMANCE with EGA, using the ROMANCE codebase available at \url{https://github.com/zzq-bot/ROMANCE}.


\textbf{ERNIE} \\
ERNIE \cite{bukharin2024robust} improves robustness by promoting Lipschitz continuity through adversarial regularization. It minimizes discrepancies between policy outputs under perturbed and non-perturbed observations:
\begin{equation}
    R_\pi(o_k; \theta_k) = \max_{\|\delta\| \leq \epsilon} D(\pi_{\theta_k}(o_k + \delta), \pi_{\theta_k}(o_k)),
\end{equation}
where $o_k$ is the agent’s observation, $\delta$ is a bounded perturbation, and $D$ measures divergence (e.g., KL-divergence). ERNIE reformulates adversarial training as a Stackelberg game and extends its framework to mean-field MARL for scalability in large-agent settings. We evaluate ERNIE using its official codebase at \url{https://github.com/abukharin3/ERNIE}.

\section{Additional Experiments Results}
\subsection{Comparison Results for Other CTDE Algorithms}
\label{appsubsec:compotherctde}


Our proposed Wolfpack attack is compatible with various value-based MARL algorithms. Experimental results in this section demonstrate its significant impact on robustness, not only in QMIX, as discussed in the main text, but also in VDN and QPLEX. The hyperparameters used in these experiments follow those outlined in Appendix \ref{appsubsec:hyper}, with WALL hyperparameters remaining consistent across all algorithms, including QMIX.

{\bf VDN Results:}
Table \ref{table:perf_vdn} shows the average win rates for various robust MARL methods with VDN against attacker baselines. Both models and attackers are trained using the VDN-based algorithm. Results indicate that the proposed Wolfpack attack is more detrimental than Random Attack or EGA for VDN. For example, while EGA reduces Vanilla VDN's performance by $95.6\%-73.8\%=21.8\%$, the Wolfpack attack causes a larger reduction of $95.6\%-44.1\%=51.5\%$, demonstrating its severity. Additionally, the WALL framework outperforms other baselines under both Natural conditions and adversarial attacks, showcasing its robustness.



\begin{table*}[h!]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{\diagbox[innerwidth=5cm,dir=NW]{Method}{Map Name}}
& \begin{tabular}[c]{@{}c@{}}2s3z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3m \\ $K_{\mathrm{WP}}=1$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3s\_vs\_3z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}8m \\ $K_{\mathrm{WP}}=1$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}MMM \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}1c3s5z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular}  
& Mean \\
\hline
\multirow{7}{*}{Natural} 
& Vanilla VDN & $98.5 \pm 1.35$ & $96.0 \pm 2.95$ & $99.3 \pm 0.50$ & $97.8 \pm 2.00$ & $98.3 \pm 0.50$ & $83.5 \pm 9.50$ & $95.6 \pm 1.65$ \\
& RANDOM & $99.0 \pm 0.10$ & $\mathbf{99.8 \pm 0.05}$ & $99.4 \pm 0.45$ & $\mathbf{98.9 \pm 0.05}$ & $98.8 \pm 1.00$ & $97.6 \pm 0.50$ & $98.9 \pm 0.36$ \\
& RARL & $95.3 \pm 0.50$ & $92.5 \pm 3.35$ & $99.3 \pm 0.45$ & $96.3 \pm 1.50$ & $93.2 \pm 1.50$ & $91.8 \pm 0.00$ & $94.7 \pm 0.55$ \\
& RAP & $93.4 \pm 1.30$ & $96.8 \pm 48.4$ & $99.3 \pm 0.45$ & $98.7 \pm 1.00$ & $97.3 \pm 0.50$ & $97.3 \pm 0.50$ & $97.1 \pm 0.37$ \\
& ERNIE & $94.6\pm3.85$ & $99.4\pm0.50$ & $97.1\pm0.47$ & $98.4\pm1.38$ & $98.8\pm0.82$ & $96.5\pm1.70$ & $97.5\pm0.86$ \\
& ROMANCE & $98.2 \pm 0.05$ & $97.9 \pm 1.05$ & $99.4 \pm 0.45$ & $93.5 \pm 5.35$ & $97.9 \pm 0.95$ & $93.0 \pm 3.10$ & $96.6 \pm 0.77$ \\
& WALL (ours) & $\mathbf{99.8 \pm 0.10}$ & $99.4 \pm 0.45$ & $\mathbf{99.9 \pm 0.10}$ & $96.9 \pm 2.00$ & $\mathbf{99.4 \pm 0.45}$ & $\mathbf{100.0 \pm 0.05}$ & $\mathbf{99.2 \pm 0.19}$ \\
\hline
\multirow{7}{*}{Random Attack} 
& Vanilla VDN & $80.5 \pm 1.50$ & $65.5 \pm 6.50$ & $96.5 \pm 0.50$ & $52.5 \pm 10.5$ & $95.0 \pm 1.00$ & $76.5 \pm 7.50$ & $77.8 \pm 1.92$ \\
& RANDOM & $83.0 \pm 1.00$ & $\mathbf{94.5 \pm 1.50}$ & $96.0 \pm 0.00$ & $88.5 \pm 0.50$ & $95.5 \pm 2.50$ & $93.5 \pm 1.50$ & $91.8 \pm 0.50$ \\
& RARL & $81.0 \pm 4.15$ & $64.5 \pm 8.40$ & $97.3 \pm 0.50$ & $72.9 \pm 0.05$ & $76.3 \pm 7.50$ & $92.0 \pm 0.20$ & $80.7 \pm 2.07$ \\
& RAP & $89.8 \pm 3.05$ & $75.8 \pm 37.9$ & $97.7 \pm 1.90$ & $81.2 \pm 0.50$ & $95.2 \pm 2.55$ & $92.3 \pm 0.50$ & $88.7 \pm 1.63$ \\
& ERNIE & $80.2\pm2.04$ & $66.9\pm2.84$ & $93.1\pm1.71$ & $67.1\pm4.92$ & $89.1\pm2.10$ & $90.4\pm4.54$ & $81.1\pm1.19$ \\
& ROMANCE & $91.0 \pm 5.00$ & $79.0 \pm 6.00$ & $98.4 \pm 0.40$ & $54.0 \pm 5.00$ & $97.5 \pm 0.50$ & $92.0 \pm 3.00$ & $85.3 \pm 0.62$ \\
& WALL (ours) & $\mathbf{95.5 \pm 1.50}$ & $86.0 \pm 6.00$ & $\mathbf{99.4 \pm 0.40}$ & $\mathbf{90.0 \pm 3.00}$ & $\mathbf{98.5 \pm 0.50}$ & $\mathbf{98.1 \pm 0.05}$ & $\mathbf{94.6 \pm 1.39}$ \\
\hline
\multirow{7}{*}{EGA} 
& Vanilla VDN & $69.5 \pm 7.50$ & $54.5 \pm 12.5$ & $94.5 \pm 1.50$ & $59.5 \pm 12.5$ & $90.5 \pm 2.50$ & $74.5 \pm 9.50$ & $73.8 \pm 1.00$ \\
& RANDOM & $56.0 \pm 6.00$ & $73.0 \pm 6.00$ & $84.0 \pm 13.0$ & $84.5 \pm 6.50$ & $85.5 \pm 1.50$ & $86.5 \pm 3.50$ & $78.3 \pm 2.42$ \\
& RARL & $58.0 \pm 1.15$ & $79.0 \pm 2.90$ & $96.5 \pm 0.70$ & $76.7 \pm 4.00$ & $75.9 \pm 8.20$ & $81.3 \pm 4.50$ & $77.9 \pm 1.86$ \\
& RAP & $79.9 \pm 3.05$ & $93.0 \pm 46.9$ & $97.3 \pm 0.45$ & $85.7 \pm 1.25$ & $91.8 \pm 3.75$ & $87.2 \pm 1.50$ & $89.3 \pm 0.21$ \\
& ERNIE & $62.0\pm14.9$ & $62.5\pm8.18$ & $89.1\pm6.58$ & $74.7\pm1.55$ & $89.3\pm2.51$ & $82.2\pm1.86$ & $76.6\pm3.62$ \\
& ROMANCE & $86.5 \pm 2.50$ & $\mathbf{93.8 \pm 3.00}$ & $98.0 \pm 0.00$ & $76.5 \pm 0.50$ & $95.5 \pm 2.50$ & $92.0 \pm 0.00$ & $90.3 \pm 0.58$ \\
& WALL (ours) & $\mathbf{91.5 \pm 0.50}$ & $91.0 \pm 2.00$ & $\mathbf{99.0 \pm 1.00}$ & $\mathbf{90.0 \pm 4.00}$ & $\mathbf{97.5 \pm 0.50}$ & $\mathbf{95.5 \pm 0.50}$ & $\mathbf{94.1 \pm 0.08}$ \\
\hline
\multirow{7}{*}{\makecell[c]{Wolfpack\\ Adversarial \\ Attack (ours)}} 
& Vanilla VDN & $54.0 \pm 4.00$ & $20.5 \pm 5.50$ & $91.5 \pm 0.50$ & $24.5 \pm 12.5$ & $18.5 \pm 9.50$ & $55.5 \pm 2.50$ & $44.1 \pm 1.08$ \\
& RANDOM & $47.0 \pm 4.00$ & $89.0 \pm 1.00$ & $90.0 \pm 5.00$ & $41.0 \pm 14.0$ & $18.5 \pm 5.50$ & $83.5 \pm 0.50$ & $61.5 \pm 2.67$ \\
& RARL & $59.5 \pm 8.65$ & $41.3 \pm 16.4$ & $96.8 \pm 0.00$ & $13.1 \pm 2.00$ & $24.3 \pm 6.50$ & $61.6 \pm 11.7$ & $49.4 \pm 1.83$ \\
& RAP & $64.3 \pm 3.50$ & $67.7 \pm 33.8$ & $98.9 \pm 0.05$ & $23.9 \pm 6.05$ & $53.3 \pm 2.50$ & $82.1 \pm 5.40$ & $65.0 \pm 1.66$ \\
& ERNIE & $33.1\pm3.64$ & $25.8\pm6.48$ & $93.0\pm4.54$ & $17.2\pm9.46$ & $23.5\pm9.29$ & $66.4\pm13.5$ & $43.2\pm1.69$ \\
& ROMANCE & $63.0 \pm 10.0$ & $46.5 \pm 20.5$ & $97.5 \pm 0.50$ & $19.5 \pm 7.50$ & $38.0 \pm 5.00$ & $83.0 \pm 4.00$ & $57.9 \pm 3.08$ \\
& WALL (ours) & $\mathbf{91.5 \pm 2.50}$ & $\mathbf{91.5 \pm 2.50}$ & $\mathbf{100.0 \pm 0.00}$ & $\mathbf{71.5 \pm 1.50}$ & $\mathbf{98.0 \pm 1.00}$ & $\mathbf{93.5 \pm 4.50}$ & $\mathbf{91.0 \pm 0.83}$ \\
\hline
\end{tabular}}
\caption{Average test win rates of robust MARL policies under various attack settings (VDN)}
\label{table:perf_vdn}
\end{table*}
\newpage
{\bf QPLEX Results:}
Similarly, Table \ref{table:perf_qplex} reports the average win rates various robust MARL methods with QPLEX against attacker baselines. Both models and attackers are trained using the QPLEX-based algorithm. Results reveal that the Wolfpack attack is also more detrimental for QPLEX compared to Random Attack and EGA. For instance, EGA reduces Vanilla QPLEX's performance by $98.4\% - 57.2\% = 41.2\%$, whereas the Wolfpack attack results in a larger reduction of  $98.4\% - 33.1\% = 65.3\%$. The WALL framework again demonstrates superior robustness, performing well against all attacks, including the Wolfpack attack.


\begin{table*}[h!]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{\diagbox[innerwidth=5cm,dir=NW]{Method}{Map Name}}
& \begin{tabular}[c]{@{}c@{}}2s3z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3m \\ $K_{\mathrm{WP}}=1$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}3s\_vs\_3z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}8m \\ $K_{\mathrm{WP}}=1$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}MMM \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular} 
& \begin{tabular}[c]{@{}c@{}}1c3s5z \\ $K_{\mathrm{WP}}=2$ \\ $t_{\mathrm{WP}}=3$ \end{tabular}  
& Mean \\ 
\hline
\multirow{7}{*}{Natural} 
& Vanilla QPLEX & $97.1 \pm 1.25$ & $\mathbf{99.2 \pm 0.45}$ & $99.4 \pm 0.50$ & $97.4 \pm 0.33$ & $99.5 \pm 0.47$ & $97.1 \pm 0.94$ & $98.4 \pm 0.07$ \\
& RANDOM & $97.7 \pm 2.11$ & $98.7 \pm 0.78$ & $99.6 \pm 0.09$ & $99.1 \pm 0.87$ & $98.2 \pm 1.23$ & $98.5 \pm 1.25$ & $98.7 \pm 0.54$ \\
& RARL & $94.4 \pm 4.54$ & $89.7 \pm 1.98$ & $88.4 \pm 6.80$ & $97.8 \pm 1.63$ & $98.8 \pm 0.82$ & $94.6 \pm 1.60$ & $94.0 \pm 1.35$ \\
& RAP & $96.5 \pm 1.70$ & $96.6 \pm 2.49$ & $93.2 \pm 0.90$ & $99.1 \pm 0.47$ & $98.8 \pm 0.78$ & $97.8 \pm 0.82$ & $97.7 \pm 0.39$ \\
& ERNIE & $96.7 \pm 2.54$ & $98.8 \pm 0.82$ & $99.7 \pm 0.08$ & $98.8 \pm 1.42$ & $99.1 \pm 0.50$ & $98.4 \pm 0.97$ & $98.6 \pm 0.55$ \\
& ROMANCE & $97.1 \pm 2.88$ & $93.2 \pm 5.88$ & $98.8 \pm 0.82$ & $94.7 \pm 3.18$ & $\mathbf{99.7 \pm 0.05}$ & $99.0 \pm 1.08$ & $96.8 \pm 1.19$ \\
& WALL (ours) & $\mathbf{99.5 \pm 0.52}$ & $97.7 \pm 2.13$ & $\mathbf{99.9 \pm 0.08}$ & $\mathbf{99.8 \pm 0.14}$ & $99.0 \pm 0.65$ & $\mathbf{99.5 \pm 0.64}$ & $\mathbf{99.2 \pm 0.29}$ \\
\hline
\multirow{7}{*}{Random Attack} 
& Vanilla QPLEX & $75.2 \pm 2.54$ & $36.2 \pm 6.63$ & $81.9 \pm 16.0$ & $40.0 \pm 9.83$ & $65.3 \pm 6.18$ & $75.0 \pm 2.79$ & $62.3 \pm 5.64$ \\
& RANDOM & $85.9 \pm 0.87$ & $69.0 \pm 5.77$ & $96.2 \pm 1.61$ & $89.0 \pm 7.67$ & $91.2 \pm 2.41$ & $95.5 \pm 1.19$ & $87.8 \pm 0.65$ \\
& RARL & $81.2 \pm 13.6$ & $61.3 \pm 13.9$ & $74.7 \pm 12.6$ & $73.7 \pm 21.6$ & $92.0 \pm 0.82$ & $92.0 \pm 2.94$ & $79.1 \pm 4.27$ \\
& RAP & $90.8 \pm 2.83$ & $78.0 \pm 6.98$ & $92.1 \pm 10.3$ & $65.3 \pm 4.50$ & $95.0 \pm 1.41$ & $95.7 \pm 1.89$ & $85.0 \pm 1.88$ \\
& ERNIE & $82.0 \pm 2.02$ & $59.0 \pm 8.98$ & $93.0 \pm 1.63$ & $80.0 \pm 1.41$ & $91.3 \pm 0.94$ & $93.0 \pm 0.82$ & $83.1 \pm 1.89$ \\
& ROMANCE & $89.2 \pm 2.05$ & $64.9 \pm 13.4$ & $93.7 \pm 2.05$ & $51.6 \pm 5.26$ & $92.2 \pm 4.12$ & $95.2 \pm 0.99$ & $76.0 \pm 2.84$ \\
& WALL (ours) & $\mathbf{97.4 \pm 0.86}$ & $\mathbf{85.4 \pm 2.45}$ & $\mathbf{99.4 \pm 0.45}$ & $\mathbf{92.9 \pm 3.49}$ & $\mathbf{98.3 \pm 1.16}$ & $\mathbf{98.6 \pm 1.23}$ & $\mathbf{95.3 \pm 0.69}$ \\
\hline
\multirow{7}{*}{EGA} 
& Vanilla QPLEX & $48.1 \pm 3.40$ & $16.0 \pm 5.11$ & $72.5 \pm 15.1$ & $58.5 \pm 16.8$ & $71.0 \pm 4.24$ & $76.9 \pm 1.46$ & $57.2 \pm 6.15$ \\
& RANDOM & $60.5 \pm 6.78$ & $61.4 \pm 14.9$ & $82.0 \pm 6.56$ & $86.2 \pm 3.39$ & $85.3 \pm 2.53$ & $88.8 \pm 2.53$ & $77.4 \pm 3.10$ \\
& RARL & $63.4 \pm 0.45$ & $65.7 \pm 6.24$ & $71.0 \pm 8.16$ & $86.0 \pm 6.53$ & $89.7 \pm 1.89$ & $84.7 \pm 2.05$ & $76.7 \pm 0.99$ \\
& RAP & $78.5 \pm 4.02$ & $83.3 \pm 0.94$ & $85.4 \pm 5.26$ & $89.0 \pm 2.16$ & $92.7 \pm 1.70$ & $96.7 \pm 0.47$ & $88.0 \pm 0.72$ \\
& ERNIE & $64.4 \pm 6.99$ & $67.3 \pm 10.3$ & $51.7 \pm 9.46$ & $86.7 \pm 4.71$ & $86.0 \pm 4.97$ & $89.3 \pm 4.50$ & $74.2 \pm 3.01$ \\
& ROMANCE & $79.5 \pm 6.02$ & $80.3 \pm 1.65$ & $90.3 \pm 1.25$ & $80.7 \pm 8.25$ & $95.2 \pm 2.02$ & $92.5 \pm 1.27$ & $85.6 \pm 1.04$ \\
& WALL (ours) & $\mathbf{89.0 \pm 3.77}$ & $\mathbf{83.9 \pm 2.88}$ & $\mathbf{99.6 \pm 0.57}$ & $\mathbf{94.4 \pm 1.23}$ & $\mathbf{96.4 \pm 1.18}$ & $\mathbf{96.1 \pm 0.70}$ & $\mathbf{93.2 \pm 0.70}$ \\
\hline
\multirow{7}{*}{\makecell[c]{Wolfpack\\ Adversarial \\ Attack (ours)}} 
& Vanilla QPLEX & $30.8 \pm 4.32$ & $11.8 \pm 7.46$ & $63.7 \pm 24.3$ & $30.7 \pm 11.1$ & $20.0 \pm 12.5$ & $41.9 \pm 6.08$ & $33.1 \pm 7.08$ \\
& RANDOM & $50.5 \pm 2.92$ & $16.8 \pm 6.76$ & $89.5 \pm 5.70$ & $45.6 \pm 18.0$ & $34.0 \pm 14.1$ & $82.5 \pm 11.3$ & $53.2 \pm 4.91$ \\
& RARL & $55.5 \pm 2.05$ & $27.3 \pm 9.18$ & $78.0 \pm 4.32$ & $46.0 \pm 14.9$ & $21.3 \pm 14.4$ & $79.7 \pm 6.94$ & $51.3 \pm 2.81$ \\
& RAP & $59.0 \pm 4.72$ & $50.3 \pm 14.7$ & $86.7 \pm 8.23$ & $33.7 \pm 3.09$ & $45.0 \pm 6.98$ & $93.7 \pm 2.36$ & $56.3 \pm 3.24$ \\
& ERNIE & $52.9 \pm 6.28$ & $49.0 \pm 9.90$ & $76.0 \pm 9.90$ & $42.7 \pm 10.6$ & $20.7 \pm 10.4$ & $78.7 \pm 6.65$ & $53.3 \pm 3.56$ \\
& ROMANCE & $57.3 \pm 8.39$ & $38.5 \pm 13.9$ & $89.7 \pm 1.25$ & $28.6 \pm 1.10$ & $46.2 \pm 10.6$ & $83.5 \pm 5.53$ & $50.8 \pm 0.89$ \\
& WALL (ours) & $\mathbf{88.3 \pm 1.20}$ & $\mathbf{87.6 \pm 4.72}$ & $\mathbf{99.7 \pm 0.47}$ & $\mathbf{84.5 \pm 1.67}$ & $\mathbf{96.3 \pm 3.86}$ & $\mathbf{99.3 \pm 0.90}$ & $\mathbf{92.6 \pm 1.32}$ \\
\hline
\end{tabular}}
\caption{Average test win rates of robust MARL policies under various attack settings (QPLEX)}
\label{table:perf_qplex}
\end{table*}

\newpage
{\bf Learning Curves for VDN and QPLEX:}
We also analyze training curves and average test win rates across different CTDE algorithms. Graphs for the \texttt{8m} and \texttt{MMM} environments illustrate the average win rates of each policy over training steps under unseen Wolfpack adversarial attacks. Fig. \ref{fig:vdn} presents training curves for VDN, while Fig. \ref{fig:qplex} shows results for QPLEX. These curves highlight that WALL not only achieves greater robustness but also adapts more quickly to attacks across VDN and QPLEX, further confirming its effectiveness beyond QMIX.


\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/vdn_wolfpack_attack_8m_v5.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/vdn_wolfpack_attack_MMM_v5.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}\hfill
    \caption{Learning curves of MARL methods for Wolfpack attack (VDN)}
    \label{fig:vdn}
\end{figure}

\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/qplex_wolfpack_attack_8m_v5.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/qplex_wolfpack_attack_MMM_v5.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}
    \caption{Learning curves of MARL methods for Wolfpack attack (QPLEX)}
    \label{fig:qplex}
\end{figure}

\newpage

\subsection{Learning Curves Across Additional SMAC Scenarios}
In this section, we provide the training performance of WALL under Wolfpack adversarial attack across additional SMAC scenarios beyond the \texttt{8m} and \texttt{MMM} environments, which are emphasized in the main text for their significant performance differences. Fig. \ref{fig:wolfpack_learning_2} illustrates the training curves for 6 scenarios: \texttt{3m}, \texttt{3s\_vs\_3z}, \texttt{2s3z}, \texttt{8m}, \texttt{MMM}, and \texttt{1c3s5z}. The results demonstrate that WALL consistently outperforms baseline methods, achieving superior win rates across all scenarios. Additionally, policies trained with WALL adapt more quickly to the challenges posed by Wolfpack attack, showing robust and efficient performance across environments of varying complexity. These findings highlight the effectiveness of WALL in enhancing the robustness of MARL policies against coordinated adversarial attacks.

\begin{figure}[ht!]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wolfpack_attack_3m_v5.pdf}
        \caption{3m}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wolfpack_attack_3s_vs_3z_v5.pdf}
        \caption{3s\_vs\_3z}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wolfpack_attack_2s3z_v4.pdf}
        \caption{2s3z}
    \end{subfigure}

    \vspace{0.1in}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wolfpack_attack_8m_v5_appendix.pdf}
        \caption{8m}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wolfpack_attack_MMM_v5_appendix.pdf}
        \caption{MMM}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/wolfpack_attack_1c3s5z_v5.pdf}
        \caption{1c3s5z}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Learning curves of MARL methods for Wolfpack attack across 6 SMAC scenarios (QMIX)}
    \label{fig:wolfpack_learning_2}
\end{figure}


\newpage
\subsection{Performance Comparison for EGA}
This section presents the training performance of WALL under the existing Evolutionary Generation-based Attackers (EGA) \cite{yuan2023robust} method across six SMAC scenarios, as shown in Figure \ref{fig:EGA_learning}. The results demonstrate that WALL consistently achieves superior robustness across all scenarios, even against unseen EGA adversaries that are not included in its training process.

The EGA framework generates a diverse and high-quality population of adversarial attackers. Unlike single-adversary methods, EGA maintains an evolving archive of attackers optimized for both quality and diversity, ensuring robust evaluations against various attack strategies. During training, attackers are randomly selected from the archive to simulate diverse attack scenarios. The archive is iteratively updated by replacing low-quality or redundant attackers with newly generated ones.
For evaluation, an attacker policy is randomly selected from the archive. The chosen attacker identifies critical attack steps and targets specific victim agents, introducing action perturbations to reduce their individual $Q$-values. WALL demonstrates strong resilience in these challenging environments, effectively mitigating the impact of EGA adversaries and maintaining high performance across all evaluated scenarios.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/romance_attack_3m_v5.pdf}
        \caption{3m}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/romance_attack_3s_vs_3z_v5.pdf}
        \caption{3s\_vs\_3z}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/romance_attack_2s3z_v5.pdf}
        \caption{2s3z}
    \end{subfigure}

    \vspace{0.1in}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/romance_attack_8m_v5.pdf}
        \caption{8m}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/romance_attack_MMM_v5.pdf}
        \caption{MMM}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/romance_attack_1c3s5z_v5.pdf}
        \caption{1c3s5z}
    \end{subfigure}

    \caption{Learning curves of MARL methods for EGA across 6 SMAC scenarios (QMIX)}
    \label{fig:EGA_learning}
\end{figure}

\newpage

\section{Additional Ablation studies}
\label{appsec:addabl}
In this section, we provide additional ablation studies on the number of Wolfpack adversarial attacks $K_{\mathrm{WP}}$ and the attack duration $t_{\mathrm{WP}}$ in the \texttt{8m} and \texttt{MMM} environments, where the performance differences between WALL and other robust MARL methods are most pronounced.


{\bf Number of Wolfpack Attacks $K_{\mathrm{WP}}$:}
The hyperparameter $K_{\mathrm{WP}}$ determines the number of Wolfpack attacks, with each attack consisting of an initial attack and follow-up attacks over $t_{\mathrm{WP}}=3$ timesteps. The total number of attack steps $K$ for Wolfpack attack is then calculated as $K=4\times K_{\mathrm{WP}}$. In this section, we e conduct a parameter search for $K_{\mathrm{WP}}\in[1,2,3,4]$. Fig. \ref{fig:attack_num} illustrates the robustness of WALL policies trained with different $K_{\mathrm{WP}}$  values under the default Wolfpack attack in \texttt{8m} and \texttt{MMM}. In both environments, having too small $K_{\mathrm{WP}}$ results in insufficiently severe attacks, which leads to reduced robustness of the WALL framework. Conversely, in the \texttt{8m} environment, excessively large $K_{\mathrm{WP}}$ values create overly devastating attacks, making it difficult for CTDE methods to learn strategies to counter the Wolfpack attack, which degrades learning performance. Therefore, an optimal $K_{\mathrm{WP}}$  exists in both environments: $K_{\mathrm{WP}}=2$ for \texttt{8m} and $K_{\mathrm{WP}}=4$ for \texttt{MMM}, which we choose as the default hyperparameters.

\begin{figure}[ht!]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.38\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/totalnum_8m_v6.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.38\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/totalnum_MMM_v5.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Number of Wolfpack Attacks ($K_{\mathrm{WP}}$)}
    \label{fig:attack_num}
\end{figure}

{\bf Attack duration $t_{\mathrm{WP}}$:}
The hyperparameter $t_{\mathrm{WP}}$ determines the duration of follow-up attacks after the initial attack. To analyze its impact on robustness, Fig. \ref{fig:Attack duration} compares performance for $t_{\mathrm{WP}}\in[1,2,3,4]$ in the \texttt{8m} and \texttt{MMM} environments. As shown in the figure, similar to the case of $K_{\mathrm{WP}}$, setting $t_{\mathrm{WP}}$ too low results in insufficient follow-up attacks on assisting agents, reducing the severity of the attack and lowering the robustness of WALL. On the other hand, excessively high  $t_{\mathrm{WP}}$ values lead to overly severe attacks, making it challenging for WALL to learn effective defenses against the Wolfpack attack. Both environments demonstrate that $t_{\mathrm{WP}}=3$ yields optimal performance and is selected as the best hyperparameter.



\begin{figure}[ht!]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.38\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/period_8m_v6.pdf}
        \caption{8m}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.38\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/period_MMM_v5.pdf}
        \caption{MMM}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Attack duration ($t_{\mathrm{WP}}$)}
    \label{fig:Attack duration}
\end{figure}

\newpage
\section{Additional Visualizations of Wolfpack Adversarial Attack}
\label{appsec:vis}

\subsection{Visualization of Wolfpack Adversarial Attack Across Additional SMAC Scenarios}
To analyze the superior performance of the Wolfpack attack, we provide a visualization of its execution in various SMAC environments. Fig. \ref{fig:visualization} illustrates the \texttt{2s3z} task, while Fig. \ref{fig:vis_8m} visualizes the \texttt{8m} task, and Fig. \ref{fig:vis_MMM} presents the \texttt{MMM} task.

Fig. \ref{fig:vis_8m}(a) illustrates Vanilla QMIX operating in a natural scenario without any attack, where the agents successfully defeat all enemy units and achieve victory. In this scenario, agents with low health continuously move to the backline to avoid enemy attacks, while agents with higher health position themselves at the frontline to absorb damage. This dynamic coordination enables the team to manage their resources effectively, withstand enemy attacks, and secure a successful outcome.

Fig. \ref{fig:vis_8m}(b) depicts Vanilla QMIX under the Wolfpack adversarial attack, where an initial attack is launched at $t=5$, and follow-up agents are targeted between $t=6$ and $t=8$. Agents with higher remaining health are selected as follow-up agents, preventing them from guarding the targeted ally or engaging the enemy effectively. Between $t=10$ and $t=19$, the initial agent continues to take focused enemy fire, eventually succumbing to the attacks and being eliminated. This disruption renders the remaining agents ineffective in defending against the adversarial attack, leading to a loss as all agents are defeated. 

Fig. \ref{fig:vis_8m}(c) shows the policy trained with the WALL framework. During $t=6$ to $t=8$, the same agents as in (b) are selected as follow-up agents and subjected to the Wolfpack attack, limiting their ability to guard or engage the enemy. Nevertheless, the non-attacked agents adjust by forming a wider formation vertically, effectively dispersing enemy firepower while delivering coordinated attacks. Additionally, agents with higher health move forward to guard the initial agents, ensuring that the initial agents do not die. This tactical adaptation enables the team to eliminate enemy units and secure victory.

\begin{figure*}[ht!]
    \begin{center}
    \centerline{\includegraphics[width=0.98\textwidth]{figures/vis_8m.pdf}}
    \vspace{-0.1in}
    \caption{Attack comparison on 8m task in the SMAC: (a) QMIX/Natural, (b) QMIX/Wolfpack attack, and (c) WALL/Wolfpack attack}
    \label{fig:vis_8m}
    \vspace{-0.1in}
    \end{center}
\end{figure*}

\newpage

Fig. \ref{fig:vis_MMM}(a) showcases Vanilla QMIX operating in a natural scenario without any adversarial interference, where all enemy units are successfully eliminated, leading to a decisive victory. During this process, agents with lower health retreat to the back while the Medivac agent provides healing, and agents with higher health move forward to absorb enemy attacks. This coordinated strategy enables the team to secure victory efficiently.

Fig. \ref{fig:vis_MMM}(b) illustrates Vanilla QMIX under the Wolfpack adversarial attack. An initial attack is launched at $t=6$, followed by the targeting of four follow-up agents between $t=7$ and $t=9$. The follow-up agents selected include one healing agent attempting to heal the initial agent, one guarding agent positioned to protect the initial agent, and two agents actively engaging the enemy targeting the initial agent. Due to the Wolfpack attack, the initial agent failed to receive critical healing or guarding support at $t=10$ and $t=19$, leading to its elimination. This disruption severely hindered the remaining agents' ability to defend against the adversarial attack, ultimately resulting in a loss as all agents are defeated. 

Fig. \ref{fig:vis_MMM}(c) illustrates the policy trained with the WALL framework. During $t=7$ to $t=9$, the same agents as in (b) are selected as follow-up agents and subjected to the Wolfpack attack, restricting their actions such as healing, guarding, or targeting the enemy effectively. However, the non-attacked agents adapt by positioning themselves ahead of the initial agent to provide protection and focus their fire on the enemies targeting the initial agent. This strategic adaptation enables the team to successfully repel the adversarial attack, eliminate enemy units, and secure victory. Notably, in the terminal timesteps, more agents survive under the WALL framework compared to the natural scenario depicted in (a), highlighting the enhanced robustness and stability of the policy learned with WALL.

\begin{figure*}[ht!]
    \begin{center}
    \centerline{\includegraphics[width=0.98\textwidth]{figures/vis_MMM.pdf}}
    \vspace{-0.1in}
    \caption{Attack comparison on MMM task in the SMAC: (a) QMIX/Natural, (b) QMIX/Wolfpack attack, and (c) WALL/Wolfpack attack}
    \label{fig:vis_MMM}
    \vspace{-0.1in}
    \end{center}
\end{figure*}
\newpage

\newpage
\subsection{Additional Analysis of Follow-up Agent Group Selection}
\label{appsec:followup}

In this section, we demonstrate that the proposed Follow-up Agent Group Selection method effectively identifies responding agents that protect the initially attacked agent. Fig. \ref{fig:followup} visualizes the process of selecting the follow-up agent group after an initial attack. By comparing the proposed method with a baseline method, Follow-up (L2), which selects $m$ agents closest to the initial agent based on observation L2 distance, we show that our method better identifies responding agents, enabling a more impactful Wolfpack attack.

Fig. \ref{fig:followup}(a) illustrates an initial attack on agent $8$, preventing it from performing its original action of hitting the enemy and forcing it to move forward, exposing it to enemy attacks. Fig. \ref{fig:followup}(b) shows the Follow-up (L2) method selecting agents $3, 4, 7, 9$ as the follow-up agents based on their proximity to the initial agent. Despite the follow-up attack, non-attacked agents, which is far from the initial agent in terms of observation L2 distance, such as agent $10$, heal the initial agent, while agent $2$ guards it, effectively protecting the initial agent from the attack.

Fig. \ref{fig:followup}(c) illustrates the follow-up agents selected using our proposed Follow-up Agent Group Selection method. The selected group includes agents $2$ and $10$, which are responsible for healing and guarding the initial agent, and agents $6$ and $9$, which are hitting enemies targeting the initial agent. These agents are subjected to the follow-up attack, preventing them from performing their protective actions. As a result, the initial agent is left vulnerable, succumbs to enemy attacks, and is ultimately eliminated.

\begin{figure}[h]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/follow_a.pdf}
        \caption{Initial attack}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/follow_b.pdf}
        \caption{Follow-up (L2)}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/follow_c.pdf}
        \caption{Follow-up Agent Group Selection}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Visualization of follow-up agent group selection comparison for the MMM task in SMAC}
    \label{fig:followup}
\end{figure}

Fig. \ref{fig:difference} illustrates the $Soft(Q^i)$ distribution and the updated $Soft(\tilde{Q}^i)$ distribution based on Equation \ref{eq:1}, highlighting the differences between the two distributions. It is evident that agents $2, 6, 9, 10$ exhibit the largest differences in their distributions. This suggests that, following the initial attack, these agents show noticeable policy changes to adapt and defend against it. Additionally, Fig. \ref{fig:kl} presents the KL divergence values between these two distributions, further confirming that agents $2, 6, 9, 10$ have the highest KL divergence values.

Consequently, based on Equation \ref{eq:2}, agents $2, 6, 9, 10$ are selected as the follow-up agent group. This aligns with the visualization results shown in Fig. \ref{fig:followup}, demonstrating consistency in the SMAC environment.

\begin{figure}[h]
    \centering
    \vspace{-0.1in}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_1.pdf}
        \caption{Agent=$1$}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_2.pdf}
        \caption{Agent=$2$ (Selected)}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_3.pdf}
        \caption{Agent=$3$}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_4.pdf}
        \caption{Agent=$4$}  % Label for first image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_5.pdf}
        \caption{Agent=$5$}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_6.pdf}
        \caption{Agent=$6$ (Selected)}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_7.pdf}
        \caption{Agent=$7$}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_8.pdf}
        \caption{Agent=$8$}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_9.pdf}
        \caption{Agent=$9$ (Selected)}  % Label for second image
    \end{subfigure}
    \begin{subfigure}{0.19\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Agent_10.pdf}
        \caption{Agent=$10$ (Selected)}  % Label for second image
    \end{subfigure}
    \vspace{-0.1in}
    \caption{$Soft(Q^i)$ and $Soft(\tilde{Q}^i)$ for each agent, along with the difference between the two distributions}
    \label{fig:difference}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/kl_chart.pdf} 
    \vspace{-0.2in}
    \caption{KL divergence values for each agent, representing the difference between $Soft(Q^i)$ and $Soft(\tilde{Q}^i)$ distributions}
    \label{fig:kl}
    \vspace{-0.2in}
\end{figure}


\newpage
\section{Limitations}
\label{appsec:limit}

The proposed WALL framework effectively defends against the highly detrimental Wolfpack adversarial attack, but it has certain limitations. One drawback is the reliance on complex Transformers for identifying critical steps, which adds computational overhead. However, our method is not strictly dependent on Transformers; any recurrent network capable of generating sequences can be employed in its place. Additionally, while parameter search is required to optimize the Wolfpack attack, the results demonstrate that our method consistently outperforms existing single-agent attack algorithms regardless of parameter choice. Addressing these limitations presents opportunities for further research and improvement.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
