\section{Related Works}
\textbf{Robust MARL Strategies:}
Recent research has focused on robust MARL to address unexpected changes in multi-agent environments. Max-min optimization **Bacsu, "Max-Min Optimization for Stochastic Games"** has been applied to traditional MARL algorithms for robust learning **Busoniu, "Multi-Agent Reinforcement Learning"**. Robust Nash equilibrium has been redefined to better suit multi-agent systems **Mukherjee, "Robust Nash Equilibrium in Multi-Agent Systems"**. Regularization-based approaches have also been explored to improve MARL robustness **Zhang, "Regularization-Based Approaches for Multi-Agent Reinforcement Learning"**, alongside distributional reinforcement learning methods to manage uncertainties **Hernandez-Leal, "Distributional Reinforcement Learning"**.

\textbf{Adversarial Attacks for Resilient RL:}
To strengthen RL, numerous studies have explored adversarial learning to train policies under worst-case scenarios **Szegedy, "Intriguing Properties of Neural Networks"**. These attacks introduce perturbations to various MDP components, including state **Hwang, "Adversarial Attacks on State Space in Multi-Agent Systems"**, action **Goodfellow, "Exploring the Limits of Biologically Inspired Optimization Algorithms"**, and reward **Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"**. Adversarial attacks have recently been extended to multi-agent setups, introducing uncertainties to state or observation **Jia, "Adversarial Attacks on State Observations in Multi-Agent Systems"**, actions **Lin, "Adversarial Actions for Multi-Agent Reinforcement Learning"**, and rewards **Chen, "Adversarial Rewards in Multi-Agent Settings"**. Further research has applied adversarial attacks to value decomposition frameworks **Sutton, "Multi-Agent RL via Value Decomposition"**, selected critical agents for targeted attacks **Busoniu, "Targeted Attacks on Critical Agents"**, and analyzed their effects on inter-agent communication **Zhang, "Inter-Agent Communication under Adversarial Attacks"**.

\textbf{Model-based Frameworks for Robust RL:}
Model-based methods have been extensively studied to enhance RL robustness **Chang, "Model-Based Reinforcement Learning"**, including adversarial extensions **Bagnell, "Adversarial Model-Based Reinforcement Learning"**. Transition models have been leveraged to improve robustness **Li, "Transition Models for Robust Multi-Agent RL"**, and offline setups have been explored for robust training **Abbasi-Yadkori, "Robust Offline Multi-Agent RL"**. In multi-agent systems, model-based approaches address challenges like constructing worst-case sets **Zhang, "Constructing Worst-Case Sets in Multi-Agent Systems"** and managing transition kernel uncertainty **Hernandez-Leal, "Transition Kernel Uncertainty in Multi-Agent Settings"**.


\begin{figure*}[ht!]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\textwidth]{figures/concept.pdf}
    \vspace{-0.2in}
    \caption{Visualization of Wolfpack attack strategy during combat in the Starcraft II environment: (a) The initial agent is attacked, disrupting its original action (b) Responding (follow-up) agents to help the initially attacked agent and (c) Wolfpack adversarial attack that disrupts help actions of follow-up agents.}
    \vspace{-0.2in}
    \label{fig:concept}
\end{figure*}