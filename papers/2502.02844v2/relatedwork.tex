\section{Related Works}
\textbf{Robust MARL Strategies:}
Recent research has focused on robust MARL to address unexpected changes in multi-agent environments. Max-min optimization \cite{chinchuluun2008pareto, han2021max} has been applied to traditional MARL algorithms for robust learning \cite{li2019robust, wang2022data}. Robust Nash equilibrium has been redefined to better suit multi-agent systems \cite{zhang2020robust, li2023byzantine}. Regularization-based approaches have also been explored to improve MARL robustness \cite{lin2020robustness, li2023mir2, wang2023regularization, bukharin2024robust}, alongside distributional reinforcement learning methods to manage uncertainties \cite{li2020multi, xu2021mmd, du2024robust, geng2024noise}.

\textbf{Adversarial Attacks for Resilient RL:}
To strengthen RL, numerous studies have explored adversarial learning to train policies under worst-case scenarios \cite{pattanaik2017robust, tessler2019action, pinto2017robust, chae2022robust}. These attacks introduce perturbations to various MDP components, including state \cite{zhang2020robust_1, zhang2021robust, everett2021certifiable, li2023ats, qiaoben2024understanding}, action \cite{tan2020robustifying, lee2021query, liu2024robust}, and reward \cite{wang2020reinforcement, zhang2020adaptive, rakhsha2021reward, xu2022efficient, cai2023reward, bouhaddi2023multi, xu2024reward, bouhaddi2024rewards}. Adversarial attacks have recently been extended to multi-agent setups, introducing uncertainties to state or observation \cite{, han2022solution, he2023robust, zhang2023safe, zhou2023robust}, actions \cite{yuan2023robust}, and rewards \cite{kardecs2011discounted}. Further research has applied adversarial attacks to value decomposition frameworks \cite{phan2021resilient}, selected critical agents for targeted attacks \cite{yuan2023robust, zhou2024adversarial}, and analyzed their effects on inter-agent communication \cite{xue2021mis, tu2021adversarial, sun2023certifiably, yuan2024robust}.

\textbf{Model-based Frameworks for Robust RL:}
Model-based methods have been extensively studied to enhance RL robustness \cite{berkenkamp2017safe, panaganti2021sample, curi2021combining, clavier2023towards, shi2024distributionally, ramesh2024distributionally}, including adversarial extensions \cite{wang2020falsification, kobayashi2024lira}. Transition models have been leveraged to improve robustness \cite{mankowitz2019robust, ye2024towards, herremans2024robust}, and offline setups have been explored for robust training \cite{rigter2022rambo, bhardwaj2024adversarial}. In multi-agent systems, model-based approaches address challenges like constructing worst-case sets \cite{shi2024sample} and managing transition kernel uncertainty \cite{he2022robust}.


\begin{figure*}[ht!]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=0.8\textwidth]{figures/concept.pdf}
    \vspace{-0.2in}
    \caption{Visualization of Wolfpack attack strategy during combat in the Starcraft II environment: (a) The initial agent is attacked, disrupting its original action (b) Responding (follow-up) agents to help the initially attacked agent and (c) Wolfpack adversarial attack that disrupts help actions of follow-up agents.}
    \vspace{-0.2in}
    \label{fig:concept}
\end{figure*}