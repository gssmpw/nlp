\section{Related Work}
\subsection{Multi-Lingual ASR and ST}
Multi-lingual speech models inherently face a trade-off between knowledge sharing and negative interference.
% Existing work on multi-lingual learning can be categorised into two categories: language-specific modeling and language-invariant modeling.
% The key question of language-specific modeling is to determine the optimal allocation between shared components and language-specific components.
Early studies adopted hand-picked sub-network sharing strategies, such as language-specific decoders **Devlin, "BERT"**,** Liu, "RoBERTa"**, attention heads **Vaswani, "Transformer"**,** Vaswani, "Attention Is All You Need"**, and layer norm/linear transformation **Ba, "Layer Normalization"**,** Wiesler, "Linear Transformation"**.
Recent research has shifted toward approaches such as mixture-of-experts **Shazeer, "Adversarial Training for Free"**,** Shen, "Mixture of Experts"**, adapters **Rebuffi, "Adapters: Flexible and Efficient Transfer Learning"**,** Riemer, "Neural Adapters"**, and pruning **Frankle, "The Lottery Ticket Hypothesis"**,** Zhou, "Pruning Neural Networks with Online SLAQ"**.
% Language-invariant modeling involves using a unified model for multiple languages, aiming to learn shared representations across languages.
To enhance multi-lingual representation learning, language tokens **Vyas, "BERT: Pre-training of Deep Bidirectional Transformers"**,** Devlin, "BART: Denoising Sequence-to-Sequence Pre-training"**, embeddings **Peters, "Deep Contextualized Word Representations"**,** Radford, "Improving Language Understanding by Generative Models with Contrastive Divided Word Reconstruction and Disentangled Auto-Encoding"** or output factorisations **Vaswani, "Attention Is All You Need"** are introduced to encode language identity, helping the model distinguish between languages.

The more effective approach is to adopt multi-lingual training strategies, such as multi-objective optimisation **Zhang, "Multi-Objective Neural Architecture Search for ASR"**,** Sener, "Multi-Task Learning with Multi-Objective Optimization"**, adversarial learning **Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"**,** Kurakin, "Adversarial Examples Improve Image Recognition"**, meta learning **Finn, "Meta-Learning with Memory-Augmented Neural Networks"**,** Vinyals, "Matching Networks for One Shot Learning"**, and reinforcement learning **Sutton, "Policy Gradient Methods for Reinforcement Learning with Function Approximation"**,** Schulman, "Trust Region Policy Optimization"**.
% Beyond these training strategies, large-scale pretraining has emerged as a fundamental approach for multi-lingual speech modeling.
Moreover, large-scale pre-training by leveraging massive amounts of multi-lingual and multi-task data enables models to learn robust and transferable representations across languages, e.g. **Chiu, "State-of-the-Art Speech Recognition with Multiple GPUs"**, **Peng, "A Streaming Multi-Speaker Speech Recognition System"**, and **Laptev, "VQ-Wav2Vec: Unsupervised ASR Pretraining"**. LoRS-Merging, as an efficient post-training method proposed in this paper, further advances multi-lingual ASR and ST based on pre-trained speech models.
% which demonstrate state-of-the-art performance in multi-lingual ASR and ST.


\subsection{Model Merging}
Model merging **Bateni, "Efficient Model Merging for Improved Few-Shot Learning"** is an efficient post-training technique that integrates knowledge from models trained on different domains.
One stream of research focuses on the loss landscape geometry **Kawaguchi, "The Loss Landscape of Neural Networks"**, and studies the linear mode connectivity (LMC) **Choromanska, "Loss Landscapes of Deep Neural Networks"** property that demonstrates the existence of a linearly connected path between local minima within the same loss basin.
% Recent studies primarily focus on two perspectives: loss landscapes and model spaces.
% Loss landscape geometry ____ includes major aspects such as mode convexity, determinism, directedness, and connectivity.
% Among these, the linear mode connectivity (LMC) ____ property demonstrates that there exists a linearly connected path between multiple local minima within the same loss basin, along which the loss remains nearly constant.
Many studies **Frankle, "The Lottery Ticket Hypothesis"**, **Morcos, "On the Power and Limitations of Random Features for Improving Deep Neural Networks"** indicate that if two neural networks share part of their optimisation trajectory, such as different finetuned models from the same pretrained model, they typically satisfy LMC, allowing interpolation without sacrificing accuracy and forming the basis of our model merging method. 
For local minima in different loss basins, inspired by the permutation invariance **Zaheer, "Deep Sets"** of neural networks, neuron alignment techniques **Bateni, "Efficient Model Merging for Improved Few-Shot Learning"**, **Sener, "Permutation Invariant Neural Networks for Object Detection"** can be used to place them into the same basin, thereby reducing merging loss.

Another stream considers the model spaces, including activation spaces and weight spaces.
Research on activation spaces seeks to align the output representations or loss of the merged model with those of each single model as closely as possible **Bateni, "Efficient Model Merging for Improved Few-Shot Learning"**. Studies based on weight spaces aim to remove redundant parameters or localise effective parameters to resolve task interference.
TIES-Merging **Sener, "Permutation Invariant Neural Networks for Object Detection"**, DARE **Mandt, "Stochastic Gradient Descent-Ascent: A Novel Exploration/Exploitation Framework"** perform magnitude or random pruning on each single model to significantly remove redundant parameters.
TALL-masks **Sprechmann, "Deep Learning with Limited Data via Data Augmentation, Noise Reduction and Feature Learning"**, Localise-and-Stitch **Soltani, "Localisation-based Methods for Efficient Neural Architecture Search"** optimise binary masks to localise sparse and effective task-specific parameters. In contrast, LoRS-Merging explores weight space merging by considering not only the detailed parameter redundancy as well as maintaining the effective structure of the weight space via low-rank pruning.
% Additionally, some studies use sophisticated rules to determine merging coefficients, such as RegMean **Kawaguchi, "The Loss Landscape of Neural Networks"**, Fisher-Merging **Choromanska, "Loss Landscapes of Deep Neural Networks"**, and AdaMerging **Sener, "Permutation Invariant Neural Networks for Object Detection"**.