\section{Intersectionality in Fair Rankings}
\label{ch:intersectionality_in_fair_rankings}%

We now discuss how intersectionality translates into fair rankings in computer science. 
%
While intersectionality and fairness may be confused with one another, they have distinct implications, making it crucial to analyze their interaction.
Intersectionality underlines the need for fairness to be comprehensive, encompassing the multiplicity of identities. Yet, fairness does not guarantee intersectionality.
A system could be designed to be fair in terms of gender or race separately. However, it might need to be revised when dealing with overlapping identities, hence ignoring the principle of intersectionality. This distinction underscores the necessity of implementing intersectionality within fair rankings, especially when it can significantly influence people's lives.

We shall now analyze the methodologies for incorporating intersectionality into fair rankings available in the literature. We aim to compare the main ideas, centering our discussion on their effectiveness in guaranteeing intersectionality. We focus on three main categories:


\begin{itemize}
  \item \emph{Constraint-based methods}, which employ a variety of common constraints to ensure fairness. However, these conventional constraints are not always sufficient to guarantee intersectionality.
  
  \item \emph{Inference Model-based methods} represent a new paradigm in constructing fair ranking models, explicitly designed to address intersectionality. The main challenge lies in the subjectivity of ethical boundaries, which remains a core issue.
  
  \item \emph{Metrics-based methods} use metrics to enable the quantification and comparison of fairness levels. The common fairness metrics widely employed in research often fail to ensure intersectionality. This has led to the advent of novel metrics, as seen in our literature review, specifically developed with intersectionality in mind.
\end{itemize}

It becomes evident that, while traditional approaches to fairness can be useful, they may not be sufficient when intersectionality is taken into account. Notably, our investigation suggests that incorporating intersectionality into fair rankings does not necessarily compromise the system's effectiveness, but can be accomplished without significant drawbacks.
%
However, this task is not without challenges. Determining an acceptable threshold that satisfies all stakeholders while maintaining the desired level of fairness is a complex task. We now dive deeper into these topics, offering a comprehensive overview of intersectionality in rankings. The broad range of approaches reveals the complex and multifaceted nature of the task, especially given the intricate intersections of identities and groups that are often involved.
%
In the following, in order to root our explanations in more practical terms, we also develop a complete example of usage for each category of methods.

% \subsection{Methods}   
% \label{sec:intrank_methods}

\subsection{Constraint-Based Methods} 
\label{subsec:intrank_meth_constr}

While diverse in their approaches, the methods discussed in this section pivot around a common theme: implementing constraints to promote fairness and intersectionality in rankings.

%[9]
\subsubsection{Diversity Constraints and In-Group Fairness}
\label{subsec:intrank_meth_constr_9n}


In \cite{yang2019balanced}, the authors found that considering two or more sensitive attributes in fair rankings where only one sensitive attribute was considered may lead to unfairness.
Even with representation constraints and utility maximization, individuals belonging to multiple historically disadvantaged groups might still face unfair treatment. 

The authors commence by establishing \emph{diversity constraints} in the ranking algorithm, specifically setting lower boundaries for each group $I_v$. 
These constraints 
%\eqref{eq:constr_diversity}
 ensure that, for each attribute value $v$ and position $p$, at least $\ell_{v, p}$ items from $I_v$ are included in the top $p$ positions of the output. In this way, the group proportion (for instance, females) in a chosen set or the top $p$ ranks of a ranking mirror that of the entire population.

% \begin{equation}
% \sum_{i \in I_v} \sum_{q \in[p]} x_{i, q} \geqslant \ell_{v, p}, \quad \forall v \in L, \forall p \in[k]
% \label{eq:constr_diversity}
% \end{equation}

While diversity constraints ensure representation across various groups, they could potentially decrease \emph{in-group fairness}, that is, fairness within a single group. To counter this, the paper presents two measurements of approximate in-group fairness to secure the best $k_v$ items from each group $I_v$.

\begin{itemize}
    \item \emph{IGF-Ratio},
     % \eqref{eq:constr_igfratio}, 
     defined as the ratio of the lowest score of an accepted item $a_v$ to the highest score of a rejected item $b_v$ within the same group: $\operatorname{IGF-Ratio}(v)=\frac{a_v}{b_v}$. A higher IGF-Ratio value corresponds to increased in-group fairness.

    % \begin{equation}
    %     \operatorname{IGF-Ratio}(v)=\frac{a_v}{b_v}
    %     \label{eq:constr_igfratio}
    % \end{equation}

    \item \emph{IGF-Aggregated}
     % \eqref{eq:constr_igfraggregated}: This metric 
     aims to guarantee that the aggregated utility of all accepted items with scores equal to or higher than a selected item is a close approximation of the aggregate utility of all items (whether accepted or rejected) with scores equal or higher than that item: $        \text{IGF-Aggregated}(v)=\min _{i \in A_v}\left\{\frac{\sum_{h \in A_{i, v}} s_h}{\sum_{h \in I_{i, v}} s_h}\right\}
$.
    Considering a selected item $i$ from the group $A_v$ of accepted items in group $I_v$, the numerator, $\sum_{h \in A_{i, v}} s_h$, represents the aggregate utility of all accepted items $h$ in $I_v$ with a score $s_h$ greater than or equal to the score of item $i$; the denominator, $\sum_{h \in I_{i, v}} s_h$, represents the aggregate utility of all items (both accepted and rejected) $h$ in $I_v$ with a score $s_h$ greater than or equal to the score of item $i$.
Higher values of IGF-Aggregated also indicate greater in-group fairness.
    % \begin{equation}
    %     \text{IGF-Aggregated}(v)=\min _{i \in A_v}\left\{\frac{\sum_{h \in A_{i, v}} s_h}{\sum_{h \in I_{i, v}} s_h}\right\}
    %     \label{eq:constr_igfraggregated}
    % \end{equation}

\end{itemize}

To balance in-group fairness across different groups, the authors formulate the problem as an \emph{Integer Linear Program} that aims to maximize total \emph{utility} while adhering to diversity and in-group fairness constraints. Although verifying the feasibility of a set of diversity constraints is NP-hard, the authors argue that standard integer programming libraries can solve reasonable instances within a small time frame.\\
The paper also introduces the \emph{Leximin} solution to distribute in-group fairness values evenly across all groups. This solution aims to maximize the minimum in-group fairness and, in the case of ties, considers the second smallest happiness value, and so on.

\begin{example}
% \textbf{3. Example Diversity Constraints} 

Suppose we are ranking candidates for a prestigious scholarship. There are 8 applicants, $C_1, \ldots, C_8$, split into two groups based on a specific label (e.g., gender, ethnicity, or another demographic characteristic): group $I_v$ of applicants with label $v$ and group $I_u$ of applicants without label $v$. The candidates’ scores $S$ (on a scale from 0 to 100) are as follows: 
\begin{itemize}
    \item Group $I_v$. $C_1$: 95, $C_2$: 85, $C_3$: 80, $C_4$: 60;
    \item Group $I_u$. $C_5$: 90, $C_6$: 88, $C_7$: 70, $C_8$: 65.
\end{itemize}

The goal is to select the top 4 candidates, with diversity constraints requiring that at least 2 candidates from each group are included in the final selection. After ranking based on scores and applying the diversity constraint, a possible selection of candidates is $A_v=\{C_1,C_2,C_5,C_6\}$.

% Selected Candidates: $C_1$ and $C_2$ from $I_v$ (with scores 95 and 85, resp.), and $C_5$ and $C_6$ from $I_u$ (with scores 90 and 88, resp.).

\noindent\emph{\underline{Applying IGF-Ratio}.}
%
The IGF-Ratio for Group $I_v$ measures the fairness of this outcome based on the scores of the selected and rejected candidates within the group. 

Minimum score in $A_v$ (lowest score of selected candidates in Group $I_v$): $a_v=85$ ($C_2$) 

Maximum score in $I_v\setminus A_v$ (highest score of rejected candidates in Group $I_v$): $b_v =80$ ($C_3$) 

% IGF-Ratio Calculation:
\(\text{IGF-Ratio}(v)=\frac{a_v}{b_v}=\frac{85}{80}=1.0625 \)

Since the IGF-Ratio is above 1, this suggests relatively fair in-group treatment for Group $I_v$ in this outcome. If a highly qualified candidate in Group $I_v$ were excluded in favor of a less qualified candidate, this ratio would be lower, indicating unfairness. 

\noindent\emph{\underline{Applying IGF-Aggregated}.} 
%
Now, let’s use the IGF-Aggregated metric to evaluate the fairness for Group $I_v$. Specifically, we need to compute the worst-case ratio for any selected candidate $i \in A_v$. For each selected candidate $i$, we need to calculate the ratio:
\(Ratio_i = \frac{\sum_{h \in A_{v}, s_h> s_i} s_h}{\sum_{h \in I_{v}, s_h> s_i} s_h}\)

For all four candidates, the ratio is 1, because they are also the four overall top-scorers in $I_v$.

% IGF-Aggregated Calculation:
\(\text{IGF-Aggregated}(v)=min (1,1,1,1)= 1 \)

This value indicates the proportion of total utility retained within Group $I_v$ for accepted candidates. Higher values (closer to 1) indicate better in-group fairness, i.e, more qualified candidates within the group are selected.
\qedfull
\end{example}


%[10]
\subsubsection{L-constraints}
\label{subsec:intrank_meth_constr_10n}
%

Diversity constraints are proposed also in~\cite{celis2020interventions}
 % proposes the same constraints from \eqref{eq:constr_diversity} 
as \emph{L-constraints}. While implementing these constraints can be challenging, the paper introduces distribution-independent constraints, which hold regardless of the utility distributions, providing a more straightforward way to mitigate bias.

The method first delineates the problem regarding \emph{groups and bias}.
% , as shown in \eqref{eq:constr_obs_utility}.
Each item could be a member of one or more intersectional groups,
 % $G_1, G_2, \ldots, G_p$, 
each of which
% These groups are subsets of the set $[m]$, each represented as $G_s$, where $s$ ranges from 1 to $p$.
is associated with an implicit bias parameter.
 % $\beta_s$.
% , ranging between 0 and 1. 
Assuming that items belonging to the intersection of several groups experience an amplified bias, the implicit bias parameter for such items can be defined as the product of the implicit biases of each group the item belongs to.
% Thus, the observed utility of $\hat{w}_i$ item $i$, where $i$ is an element of the set $[m]$, is:
% \begin{equation}
% $\hat{w}_i:=\left(\prod_{s \in[p]: G_s \ni i} \beta_s\right) \cdot w_i.$
% \label{eq:constr_obs_utility}
% \end{equation}
In any selection process, each item is also characterized by a true \emph{latent} utility (the one it generates if chosen) and an observed utility, which is a (possibly biased) estimate of the former made by the selector.
The overarching goal is to construct a ranking that corrects for implicit bias in observed utilities, approximates the true latent utility as closely as possible, and satisfies fairness-related constraints defined by the L-constraints.
% The goal is then to discover a ranking that, when subjected to L-constraints, has a latent utility approximating the maximum latent utility as closely as possible.

The authors argue that L-constraints are enough to recover optimal latent utility while optimizing observed utility under specific constraints. The idea is to reorder items through swapping to keep the latent utility unchanged while satisfying the constraints. However, this process can be challenging because the constraints depend on the utilities of the items.
%
The method then introduces \emph{distribution-independent constraints},
which are shown to
%  (Theorem 3.2 in \cite{celis2020interventions}), which function regardless of the utility distributions to address this issue. The authors show that these constraints hold for any continuous distribution. The proof includes substituting the utilities with their cumulative distribution functions, which does not alter the optimal ranking. 
% The study presents theoretical results that show how these constraints 
effectively minimize bias and reclaim optimal latent utility.


%[12]
\subsubsection{Dealing with Noisy Protected Attributes}
\label{subsec:intrank_meth_constr_12n}

Traditional fair subset selection algorithms assume that protected attributes (e.g., gender, race) are accurately known. However, in real-world scenarios, these attributes can be noisy, i.e., erroneous or approximated, due to data collection errors or imputation.

The method introduced in~\cite{mehrotra2021mitigating} addresses a subset selection problem where protected attributes are noisy.
% This can occur in scenarios with a dataset of multiple items, each possessing a utility value and some protected attributes.
The goal of the classical Subset Selection Problem is to select a subset $S$ from a larger set $U$ such that the subset maximizes a utility function $f(S)$ while satisfying fairness constraints so as to avoid a disproportion favoring or disadvantaging any group based on protected attributes.
The possible noise in the protected attributes is modeled probabilistically and the fairness constraints are defined using various fairness metrics, including demographic parity, equal opportunity, and more.
%
% The goal is to select a subset of these items that maximizes the total utility while maintaining fairness despite the imperfect information about protected attributes.
The solution involves formulating a ``Denoised'' Selection Problem capturing all of these fairness metrics.
%
Since the denoised selection problem is NP-hard, the authors propose an approximation algorithm based on linear programming (LP). The linear program expresses balance between utility and fairness under the noisy conditions and aims to maximize the expected utility while incorporating the probabilistic information about the protected attributes. The linear program is then solved to obtain a fractional solution and subsequently rounded to obtain an integral solution that represents the selected subset $S$.
The obtained LP-based approximation algorithm is proved to achieve with high probability the desired fairness goal with minimal violation, even under noisy conditions.


% The \emph{Denoised problem} is set up as follows:

% \begin{equation}
% \begin{aligned}
% & \max_{x \in\{0,1\}^m} \sum_{i=1}^m w_i x_i \\
% & \text{s.t. } L_{\ell}^{(k)}-\delta n \leq \sum_{i=1}^m q_{i \ell}^{(k)} x_i \leq U_{\ell}^{(k)}+\delta n, \forall k \in[s], \ell \in\left[p_k\right], \\
% & \quad \sum_{i=1}^m x_i=n .
% \end{aligned}
% \label{eq:constr_denoised_prob}
% \end{equation}

% Here, the goal is to maximize the objective function, the sum of the utility values $w_i$ weighted by the binary selection vector $x_i$, to select a subset of items that maximizes the total value. The constraints ensure that for each attribute-value pair $(k, \ell)$, the sum of selected items from the group $G^{(k)}_\ell$ should be within the range of the lower and upper bounds $L^{(k)}_\ell - \delta_n$ and $U^{(k)}_\ell + \delta_n$, where $\delta$ is a small positive value. These bounds ensure that a minimum and a maximum number of items from each group is selected, considering the noise estimates $q^{(k)}_{i\ell}$.
% The total number of selected items should equal $n$, where $n$ is the desired number of items in the selection.

% The second part of the method is used to tackle the NP-hard nature of the denoised problem. It entails constructing a \emph{linear programming-based approximation algorithm} capable of providing a solution that satisfies the problem's constraints with high likelihood.
% Algorithm \ref{alg:constr_noise} is used for this purpose and consists of three steps. It begins with an input containing a natural number $n$, a probability matrix $q$, a utility vector $w$, and constraint vectors $L$ and $U$. The algorithm first identifies a basic feasible solution $x$ to the Linear Programming (LP) relaxation of the Program Denoised, using the inputs $(n, q, w, L, U)$. The solution $x$ is rounded to $x$ and returned as the output.

% \begin{algorithm}[H]
%     \caption{Algorithm for Denoised problem}
%     \label{alg:constr_noise}
%     \begin{algorithmic}[1]
%     \STATE \textbf{Input:} A number $n \in \mathbb{N}$, probability matrix $q \in [0, 1]^{m \times p}$, utility vector $w \in \mathbb{R}^m$, constraint vectors $L, U \in \mathbb{R}^p_{\geq 0}$.
%     \STATE \textbf{Solve} $x \leftarrow$ Find a basic feasible solution to LP-relaxation of Program Denoised with inputs $(n, q, w, L, U)$.
%     \STATE \textbf{Set} $x'_{i} \leftarrow \lceil x_{i} \rceil$ for all $i \in [m]$. // Round solution
%     \STATE \textbf{Return} $x'$.
%     \end{algorithmic}
% \end{algorithm}

This method also considers intersectionality in fair rankings by factoring in intersectional groups such as multi-racial candidates. This allows for increased flexibility and fairness during the selection process.
%
Compared to previous noise-oblivious approaches, this method offers better trade-offs between utility and fairness. Consequently, it is a viable solution for fair subset selection in the presence of noisy protected attributes.

%[13]
\subsubsection{Satisfactory Regions}
\label{subsec:intrank_meth_constr_13n}

In order to balance fairness and quality in rankings, \cite{asudeh2019designing} considers the problem of ranking items according to a linear scoring function $f$ and using a ``fairness oracle'' that determines whether the outcome provided by $f$ on a dataset $D$ is fair. The problem they solve is that of finding the scoring function $f'$ closest to $f$ and such that its outcome on $D$ is fair.

The geometric interpretation of these notions, especially if we focus on the 2D case, helps to visualize the problem:
a (2D) item corresponds to a point in the Cartesian plane, while a linear scoring function $f(x,y)=w_1 x + w_2 y$ maps to a ray starting from the origin and passing through the point $\langle w_1, w_2\rangle$. With this, the ranking of items is given by the order of their projections onto the ray, with points closer to the origin ranked better.
Under this interpretation, the distance between two scoring functions is given by the angular distance of the corresponding rays.
Since any ray (function) produces an outcome on $D$ that is either fair or not, it is useful to determine the ``satisfactory sectors'' (i.e., sets of contiguous rays) in the plane that correspond to fair outcomes. To this end, the authors propose a ``ray-sweeping'' technique that finds the so-called \emph{ordering exchanges}, i.e., those particular scoring functions at the border of satisfactory sectors (past which the ranks of two tuples are swapped).
For instance, $t_1=\langle 1, 2\rangle$ and $t_2=\langle 2,1 \rangle$ are ranked equally by $f(x,y)=x+y$ (i.e., the bisector, forming an angle $\theta=\pi/4$ with the $x$-axis), whereas $t_1$ is ranked better when $\theta>\pi/4$ and worse otherwise, so $\theta=\pi/4$ is an ordering exchange.

Finding all the ordering exchanges is done with an offline phase
that orders items based on one axis and updates the order as it sweeps the ray towards the other axis, maintaining a min-heap to track ordering exchanges. When a satisfactory sector is found, the algorithm adds adjacent sectors until no more satisfactory sectors can be added. This algorithm uses the dual transformation of items and the intersection of lines.
Finally, an online phase goes through a binary search on the ordering exchanges to find the border of a satisfactory sector closest to $f$ (or returning $f$ itself, if $f$ satisfies fairness).

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\textwidth]{Images/4-INTFAIR/4.constr1.png}
%     \caption{Satisfactory regions method example \cite{asudeh2019designing}.}
%     \label{fig:4.constr1}
% \end{figure}


% The system uses pre-processed data in the \emph{online phase} to help users interactively create a scoring function that meets their quality and fairness criteria. This is achieved through a binary search algorithm that uses the sorted list of satisfactory regions to suggest modifications to a user's query function if it does not result in a fair ranking. For a query function, the algorithm either confirms that it lies in a satisfactory region or suggests the closest satisfactory border to the query.

For \emph{multi-dimensional data}, ordering exchanges form hyperplanes in an angle coordinate system. The partitioning process is more complex due to the high-dimensional objects involved, and the authors proposed to index the hyperplanes and exploit the index in the online phase. Due to the impracticality of the non-linear nature of the problems that need to be solved to determine the satisfactory regions, the authors also propose an approximation technique and sampling approaches to speed up the pre-processing phase. As a limitation, despite its efficiency, the adopted function sampling techniques cannot ensure the discovery of all possible rankings or the non-existence of a satisfactory function.

We observe that the proposed method supports intersectionality in fair rankings by considering multiple intersecting factors while ensuring fairness.
% By defining fairness in terms of minimum limits on the number of selected members of a protected group, the system can consider intersectionality in user-defined rankings.

\subsection{Inference Model-Based Methods}
\label{subsec:intrank_meth_inf}

In this subsection, we examine papers primarily relying on causal inference structural models to support intersectionality. The Causal Model and the Causal Multi-Level Fairness methods will be presented to elucidate their underlying principles and methodologies, and their specific approaches to tackling intersectionality in fair rankings.
 

%[14]
\subsubsection{Causal Model}
\label{subsec:intrank_meth_inf_14n}

The method discussed in~\cite{yang2020causal} employs a \emph{Structural Causal Model} (SCM), i.e., a Directed Acyclic Graph (DAG) that represents the causal relationships between variables, where vertices denote observed or latent variables, while edges symbolize causal connections between these variables.
%
Common vertices in these DAGs include sensitive attributes, like $G$ (gender) and $R$ (race), an outcome variable $Y$, and a set of a priori non-sensitive variables $\mathbf{X}$. Those variables in $\mathbf{X}$ that are part of a path from a sensitive variable to the outcome are called \emph{mediators}.
While, commonly, additive functions are utilized for predictive modeling, the authors advocate for the adoption of non-additive models so as to accommodate intersectionality concerns regarding the presence of multiple sensitive attributes. In particular, they employ functions consisting of three components: two additive components (one for the non-sensitive and one for the sensitive attributes, where the coefficients capture the marginal causal effect of structural oppression related to the sensitive attribute on the non-sensitive one) plus one non-additive component for the interplay of a pair of non-sensitive attributes (with coefficients expressing the combination of adversity resulting from multiple intersecting forms of structural oppression related to the involved sensitive attributes).

Intersectionality is captured by distinguishing the mediators in \emph{resolving} (i.e., actively affecting the outcome) or \emph{non-resolving}, with the possibility to have mediators resolving for one sensitive attribute but not for another.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.25\textwidth]{Images/4-INTFAIR/4.inf1.jpg}
%     \caption{Example of SCM \cite{yang2020causal}. $G$ and $R$ are the sensitive attributes, $X$ is a covariate, and $Y$ is the utility score.}
%     \label{fig:4.constr2}
% \end{figure}

% A critical aspect of this model is the \emph{intersectionality-sensitive function} $f_j$ \eqref{eq:inf_interactions}, which describes non-additive interactions between sensitive attributes like gender and race, represented as $G$ and $R$, respectively. Contrary to typical linear or additive models, this function embraces the intersectional impact of multiple sensitive attributes.

% \begin{equation}
% \label{eq:inf_interactions}
% f_j\left(\mathrm{pa}_j\right)=\sum_l \beta_{j, l} V_l+\sum_l \eta_{j, l} A_l+\sum_r \sum_{l \neq r} \eta_{j, r, l} A_r A_l .
% \end{equation}

% Here, the variable $pa_j$ includes sensitive attributes $(A1, A2, ...)$ and non-sensitive attributes $(V1, V2, ...)$. The equation consists of three terms: the first term represents the linear main effects of the non-sensitive attributes $V_l$ with corresponding weights $\beta_{j,l}$; the second term represents the linear effects of the sensitive attributes $A_l$ with weights $\eta_{j,l}$; and the third term represents the non-additive interactions between pairs of sensitive attributes $A_r$ and $A_l$ with weights $\eta_{j,r,l}$.\\
% The coefficients or weights, such as $\beta_{j,l}$, capture the marginal causal effect of structural oppression related to the sensitive attribute $A_l$ on the non-sensitive variable $V_j$. Similarly, the coefficients $\eta_{j,r,l}$ capture the non-additive combination of adversity resulting from multiple intersecting forms of structural oppression related to the sensitive attributes $A_r$ and $A_l$. \\
% While the provided example focuses on binary-sensitive attributes and simpler models, the framework can be extended to handle higher-order interactions and more complex scenarios. 

% In addition to sensitive attributes, the model considers an outcome variable $Y$ as a utility score in the ranking task. A set of a priori non-sensitive predictor variables is denoted as $X$. Some variables in $X$ may act as \emph{mediators}, linking sensitive attributes to the outcome variable $Y$. These mediators can be resolving (actively affecting the outcome) or non-resolving. Notably, the model can account for mediators resolving for one sensitive attribute rather than the other, thus capturing intersectionality nuances.\\
The model uses \emph{counterfactuals}, which can be understood as the potential value of $Y$ if $G$ or $R$ were different from their actual values. The computation of these counterfactuals is based on modifying the observed value of a sensitive attribute and allowing this change to propagate through the DAG.
A \emph{counterfactually fair ranking} definition is then suggested
 % \eqref{eq:inf_cf_rank},
using these counterfactuals to treat every individual or item in the dataset as having a particular intersection of advantages or disadvantages.
Here a ranking 
% $\hat{\boldsymbol{\tau}}$
 is considered counterfactually fair if, regardless of the specific values of actual and counterfactual sensitive attributes ($a$ and $a^{\prime}$), the probability of observing a certain rank $k$ for a given input $x$ and attribute values $a$ is equal to the probability of observing the same rank $k$ with the counterfactual attribute values $a^{\prime}$.
  % This equality holds for any rank $k$, and tie-breaking is randomized to ensure fairness.

% \begin{equation}
% \mathbb{P}\left(\hat{\boldsymbol{\tau}}\left(Y_{\mathbf{A} \leftarrow \mathbf{a}}(U)\right)=k \mid \mathbf{X}=\mathbf{x}, \mathbf{A}=\mathbf{a}\right)=\mathbb{P}\left(\hat{\boldsymbol{\tau}}\left(Y_{\mathbf{A} \leftarrow \mathbf{a}^{\prime}}(U)\right)=k \mid \mathbf{X}=\mathbf{x}, \mathbf{A}=\mathbf{a}\right)
% \label{eq:inf_cf_rank}
% \end{equation}

The method's implementation first estimates the causal model parameters using the dataset, then computes the counterfactual records by transforming each observation to a reference subgroup. Finally, it sorts the counterfactual outcome scores in descending order to produce the counterfactually fair ranking.

% Provided that the assumed causal model $M$ is identifiable and correctly specified, this implementation will yield counterfactually fair rankings in score-based ranking and cf-LTR tasks, as demonstrated in the study.

This method facilitates intersectionality in rankings by enabling the ranking algorithm to consider multiple intersecting attributes simultaneously. It provides a framework for generating intersectionally fair rankings, capturing the combined impact of multiple forms of structural oppression. Using counterfactuals also highlights potential biases or inequalities in the system by demonstrating how changes in sensitive attributes could alter the rankings.
However, given that it is a causal approach, it relies on \emph{strong assumptions}, such as the construction of the SCM.

%\framedtext{\textbf{Example Causal Model}
\begin{example}
A moving company is hiring workers based on their overall qualification scores $Y$. The following factors are considered:
\begin{itemize}
    \item Gender (G): Male/Female. 
    \item Race (R): White/Black.
    \item Weightlifting Ability (X): Measured quantitatively and influenced by G and R.
\end{itemize}
The hiring decision ranks candidates based on Y, but X plays a key role in determining Y, and it is causally influenced by G and R. This creates potential biases in the ranking if X reflects systemic disadvantages tied to G or R.

Here’s a simplified causal graph in which G and R influence both X and Y:
    % G ----> X ----> Y
    %  \            /
    %   \---> R ---/

\begin{itemize}
    \item G→X: Gender affects weightlifting ability.
    \item R→X: Race affects weightlifting ability.
    \item X→Y: Weightlifting ability affects the qualification score.
    \item G→Y and R→Y: Gender and race directly affect qualification scores (e.g., due to societal biases).
\end{itemize}

Consider the 4 candidates shown in Table~\ref{tab:causalModel}.

\newcommand{\xA}{80}
\newcommand{\xB}{72}
\newcommand{\xC}{75}
\newcommand{\xD}{78}
\newcommand{\biasGA}{10}
\newcommand{\biasGB}{0}
\newcommand{\biasGC}{10}
\newcommand{\biasGD}{0}
\newcommand{\biasRA}{5}
\newcommand{\biasRB}{0}
\newcommand{\biasRC}{0}
\newcommand{\biasRD}{5}
\newcommand{\deltabiasGA}{8}
\newcommand{\deltabiasGB}{0}
\newcommand{\deltabiasGC}{8}
\newcommand{\deltabiasGD}{0}
\newcommand{\deltabiasRA}{4}
\newcommand{\deltabiasRB}{0}
\newcommand{\deltabiasRC}{0}
\newcommand{\deltabiasRD}{4}
\newcommand{\weightX}{0.7}
\newcommand{\weightG}{0.2}
\newcommand{\weightR}{0.1}
\newcommand{\yA}{\fpeval{round(\xA * \weightX + \biasGA * \weightG + \biasRA * \weightR, 1)}}
\newcommand{\yB}{\fpeval{round(\xB * \weightX + \biasGB * \weightG + \biasRB * \weightR, 1)}}
\newcommand{\yC}{\fpeval{round(\xC * \weightX + \biasGC * \weightG + \biasRC * \weightR, 1)}}
\newcommand{\yD}{\fpeval{round(\xD * \weightX + \biasGD * \weightG + \biasRD * \weightR, 1)}}
\newcommand{\xpA}{\fpeval{round(\xA - \deltabiasGA - \deltabiasRA, 1)}}
\newcommand{\xpB}{\fpeval{round(\xB - \deltabiasGB - \deltabiasRB, 1)}}
\newcommand{\xpC}{\fpeval{round(\xC - \deltabiasGC - \deltabiasRC, 1)}}
\newcommand{\xpD}{\fpeval{round(\xD - \deltabiasGD - \deltabiasRD, 1)}}
\newcommand{\ypA}{\fpeval{round(\xpA * \weightX + \biasGA * \weightG + \biasRA * \weightR, 1)}}
\newcommand{\ypB}{\fpeval{round(\xpB * \weightX + \biasGB * \weightG + \biasRB * \weightR, 1)}}
\newcommand{\ypC}{\fpeval{round(\xpC * \weightX + \biasGC * \weightG + \biasRC * \weightR, 1)}}
\newcommand{\ypD}{\fpeval{round(\xpD * \weightX + \biasGD * \weightG + \biasRD * \weightR, 1)}}

The qualification score $Y$ is computed as a weighted combination of weightlifting ability ($X$) and direct biases from $G$ and $R$. Assume this combination is as follows: \( Y=\weightX X+\weightG Bias(G)+ \weightR Bias(R) \).
The values of $Bias(G)$ and $Bias(R)$, in turn, are computed from the probability distributions to reflect the amount of discrimination in hiring; assume, in this case, $Bias(G)=\biasGA$ for Males, $\biasGB$ for Females, and $Bias(R)=\biasRA$ for Whites, $\biasRB$ for Blacks.
% (obtained from a previously using $Bias(G)=P(Y \vert do(G=Male)) - P(Y\vert do(G=Female))$, similarly for R in which Bias(R)=5 for Whites, 0 for Blacks.

Computing the Adjusted Qualification Score (${Y'}$) means resolving the weightlifting ability $X$, which is influenced by systemic biases tied to $G$ and $R$. The adjusted score aims to reflect a counterfactual world where all candidates have the same opportunities for developing $X$, irrespective of $G$ and $R$.
Assume, for simplicity, that 
systemic biases affecting the weightlifting ability ($X$) amount to the following values:
\begin{itemize}
    \item Males gain $\Delta Bias(G)=+\deltabiasGA$ points in $X$ due to systemic gender bias, while females have $\Delta Bias(G)=\deltabiasGB$.
    \item Whites gain $\Delta Bias(R)=+\deltabiasRA$ points in X due to systemic racial bias, Black people have $\Delta Bias(R)=\deltabiasRB$.
\end{itemize}
We can then adjust X by subtracting these biases as follows: \(X' =X - \Delta Bias(G) - \Delta Bias(R) \)

Then:
\(Y'=\weightX X'+\weightG Bias(G)+\weightR Bias(R) \)

The adjusted scores $Y'$  reflect a fairer ranking, ensuring that candidates' outcomes are not unfairly influenced by systemic biases in $G$ or $R$. Candidate B benefits significantly from fairness adjustments, since, according to ${Y'}$, she is now the second in the ranking.
% followed by Candidate D.



% \xpA \xpB \xpC \xpD
\begin{table*}
    \centering
    \begin{tabular}{c | p{1.5cm} p{1.3cm} p{2cm} p{2cm} p{2cm} p{2cm}}
        \toprule
            \textbf{Candidate} & \textbf{Gender ($G$)} & \textbf{Race ($R$)}& \textbf{Weightlifting Ability ($X$)}  & \textbf{Qualification Score ($Y$)} &\textbf{Adjusted Weightlifting Ability ($X'$)}  & \textbf{Adjusted Qualification Score ($Y'$)}\\
            \midrule
            A &	Male &	White &	\xA & \yA & \xpA & \ypA  \\
            B & Female & Black & \xB & \yB & \xpB & \ypB  \\
            C & Male & Black & \xC & \yC & \xpC & \ypC  \\
            D & Female & White & \xD & \yD & \xpD & \ypD  \\
    \bottomrule
    
\end{tabular}
    \caption{Causal Model example. The company assigns a qualification score $Y$ based on $X$ and possibly $G$ and $R$. $Y$ could be affected directly by biases in $G$ and $R$ or indirectly through their influence on $X$. A fairness-adjusted score $Y'$ aims to counteract the indirect discrimination mediated through $X$, which is adjusted as $X'$. This score ensures fairer rankings across intersectional subgroups.}
    \label{tab:causalModel}
\end{table*}
\qedfull
\end{example}

%[15]
\subsubsection{Causal Multi-Level Fairness}
\label{subsec:intrank_meth_inf_15n}

The Causal Multi-Level Fairness method described in \cite{mhasawade2021causal} utilizes tools from causal inference to account for sensitive attributes and potential sources of bias at both the individual and macro (structural or societal) levels.
The method centers on a causal model and counterfactual fairness (described in Subsection \ref{subsec:intrank_meth_inf_14n}). Though the models are somewhat similar in using counterfactuals, \cite{yang2020causal} explicitly emphasizes the intersectionality of sensitive attributes.
In contrast, the multi-level path-specific fairness model of~\cite{mhasawade2021causal} focuses more on the hierarchy of macro and individual level attributes and their diverse causal paths of influence.

\emph{Path-specific} effects isolate the causal effect of a sensitive attribute on an outcome along a particular pathway. For instance, a specific effect of racial discrimination on health behavior might be considered unfair and therefore needs to be corrected.
On the other hand, the impact of factors like neighborhood socioeconomic status (macro-level attribute) might be considered fair and thus preserved. This discrimination between fair and unfair pathways is often guided by domain experts and policymakers.
Path-specific fairness isolates and corrects only the unfair pathways, preserving fair individual information regarding sensitive attributes.

A key extension of this concept is \emph{multi-level path-specific} fairness, which considers not only individual-level sensitive attributes but macro-level sensitive attributes that affect properties at the individual level. For instance, neighborhood socioeconomic status (macro-level attribute) can affect the individual perception of racial discrimination (individual-level attribute), affecting health behavior. Estimating the path-specific effect of macro and individual-level sensitive attributes becomes non-trivial.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.25\textwidth]{Images/4-INTFAIR/4.inf2.jpg}
%     \caption{Example of multi-level fairness, where the outcome $Y$, such as health behavior, is influenced by various factors at different levels. These factors include multi-level sensitive attributes $A_I$ and $A_P$, which encompass individual-level attributes like perceived racial discrimination ($A_I$) as well as macro-level variables such as neighborhood socioeconomic status ($A_P$) and other factors related to the specific geographic area, such as zip code-level factors ($P$). These diverse factors operating at different levels collectively impact the outcome $Y$, shaping individuals' behaviors and experiences in various contexts \cite{mhasawade2021causal}.}
%     \label{fig:4.constr2}
% \end{figure}

The model removes the path-specific effect of the sensitive attribute along the discriminatory causal paths from the model predictions. In doing so, the model maintains fairness towards individuals by aligning decisions with those that would have been made in a counterfactual world in which the sensitive attribute along the unfair pathways was set to a counterfactual value.
After identifying discriminatory pathways, the multi-level path-specific fairness model calculates and corrects the path-specific effects. It constructs a fair predictor by removing the unfair path-specific effect from the prediction. The discriminatory causal pathways are identified, and their effects are subtracted from the outcome variable, leading to fair predictions.


\subsection{Metrics-Based Methods}
\label{subsec:intrank_meth_metr}

In this subsection, we review papers that critique the limitations of prevalent fairness metrics in ensuring intersectionality and propose innovative alternatives.


%[17]
\subsubsection{Representation and Attention-based Metrics}
\label{subsec:intrank_meth_metr_17n}

The proposal of~\cite{ghosh2021fair} centers around the Deterministic Constrained Sorting algorithm (DetConstSort), initially presented in \cite{geyik2019fairness}, and the application of various fairness metrics. The primary objective is to assess the degree of success of fair ranking algorithms in achieving their fairness objectives when they are fed with data that includes both ground truth and inferred demographic information.

\emph{DetConstSort} was chosen because of its ability to handle protected attributes with many possible attribute values.
% , distinguishing it from other greedy fair ranking algorithms described in the same paper.
The algorithm receives as inputs an unfairly sorted list and an integer $k$. Then it generates a fairness-aware list of the top $k$ candidates, ensuring that 
the ranking does not disproportionately favor or disadvantage any particular demographic group.
% the fraction of candidates in each subgroup matches their fraction in the underlying population. 
DetConstSort also aims to enhance sorting quality by re-ranking candidates that appear above a specific rank in the list, provided that feasibility criteria are met.

The authors then introduce several fairness metrics to evaluate the performance of DetConstSort. These metrics include both representation-based and attention-based metrics. \emph{Representation-based metrics} include Skew and Normalized Discounted Kullback–Leibler (NDKL) Divergence.

The Skew metric assesses the representation of each demographic group in a given list
 % and is calculated using Equation \eqref{eq:inf_skew}. 
%
% The \emph{Skew} 
and is defined as the ratio between the proportion of members belonging to a given subgroup $sg_i$ within the top $k$ items of the ranked list 
%$p_{\tau^k, s g_i}$ 
and the proportion of members belonging to the subgroup $sg_i$ in the overall population.
 % $p_{q, s g_i}$. 
Ideally, a Skew value close to one indicates a proportional representation of the subgroup $sg_i$ in the ranked list relative to the population. A Skew value greater than one suggests over-representation of the subgroup among the top $k$ candidates, while a Skew value less than one indicates under-representation.
% \begin{equation}
% \text { Skew }_{s g_i} @ k(\tau)=\frac{p_{\tau^k, s g_i}}{p_{q, s g_i}}
% \label{eq:inf_skew}
% \end{equation}

 The NDKL Divergence
 % , calculated using Equation \eqref{eq:inf_ndkl}, 
can be interpreted as a weighted average of the logarithm of the Skew scores for all groups in a ranked list.
%
The \emph{NDKL} is a measure defined for a ranked list $\tau$, and it is computed by summing the Kullback-Leibler (KL) divergences between the individual distributions $D_{\tau^i}$ and the reference distribution $D_r$, weighted by a logarithmic factor. The KL divergence measures the dissimilarity between two probability distributions. The NDKL score is a weighted average of the logarithm of the Skew scores for all the groups in the ranked list.
A low NDKL value close to zero indicates that people from all subgroups are represented proportionally in the ranked list. This occurs when the distributions of the subgroups in the top $k$ candidates closely resemble the underlying population distribution. Conversely, a higher NDKL score indicates a significant difference in the distributions of the subgroups in the top $k$ ranked candidates, implying potential disparities in representation.

% \begin{equation}
% \mathrm{NDKL}(\tau)=\frac{1}{Z} \sum_{i=1}^{|\tau|} \frac{1}{\log _2(i+1)} d_{K L}\left(D_{\tau^i} \| D_r\right)
% \label{eq:inf_ndkl}
% \end{equation}

The authors also use \emph{attention-based metrics}, recognizing that users do not pay equal attention to all items in ranked lists. They use a geometric distribution to model the decay in attention. They defined the attention measure at position $k$ 
as $100p (1-p)^{k-1}$, where
% using Equation \eqref{eq:inf_attention}. Here, 
% $\tau$ is the ranked list, and 
$p$ represents the proportion of attention provided to the first result. Then, the mean attention score for each protected attribute is calculated, and the aggregate attention is computed across groups.

% \begin{equation}
% \text { Attention }_p @ k(\tau)=100 \times(1-p)^{k-1} \times(p)
% \label{eq:inf_attention}
% \end{equation}

To measure the quality of the rankings, the authors use the Normalized Discounted Cumulative Gain (NDCG) metric, which evaluates search rankings, and Rank Change metrics, which measure the distortion from the original list to the fairness-aware re-ranked list.
% 
\emph{NDCG} provides a holistic measure of the quality of a ranking by considering both the utility of items and their positions in the ranking.
%
\emph{Rank Change metrics} are defined in two ways: as Rank Boost for an individual item and as Average Rank Change (ARC) for a subgroup. The Maximum Absolute Rank Change (MARC) is then calculated as the maximum ARC over all subgroups in the list. These metrics indicate how much each item or subgroup has moved in the ranking, emphasizing shifts towards or away from fairness.

These methods allow for considering intersectionality in fair rankings by acknowledging the importance of subgroup representation and the attention different subgroups receive in the ranking results. 

\begin{example}
    We now show an example of Fair Ranking with DetConstSort (DCS). We aim to rank candidates for a recommendation list while maintaining fairness by enforcing minimum representation counts for attributes such as group membership (e.g., race or gender).  Candidates belong to two groups: Group A (Majority group), and Group B (Minority group). We establish 2 minimum representation requirements: at least 2 candidates must appear in the top 5 positions for Group A, and at least 3 for Group B. The candidates are ranked based on scores assigned by the model. However, DCS adjusts the ranking to satisfy fairness constraints.
    Consider the following 10 candidates $C_1,\ldots,C_{10}$ with their corresponding scores.
    \begin{itemize}
        \item Group A: $C_1$: 95, $C_2$: 90, $C_3$: 85, $C_4$: 80, $C_5$: 75,
        \item Groups B: $C_6$: 74, $C_7$: 73, $C_8$: 72, $C_9$: 71, $C_{10}$: 70.
    \end{itemize}
The recommendation list is initially empty. During the first iteration, both Group A and Group B have candidates that can increase their representation requirements ($C_1$, $C_6$), which are then ordered by descending score, so that $C_1$ is added first. During the second iteration, $C_2$ and $C_6$ are considered for improving the representation requirements, and $C_2$ is added, having a higher score. From the third iteration on, only Group B has candidates remaining to meet the requirement, so $C_6$ is first added, then $C_7$, and, finally, $C_8$. In the original ranking (based purely on scores), all the top 5 candidates belong to Group A; instead, by applying DCS, the final recommendation list ensures both a fairer representation (2 candidates from Group A and 3 candidates from Group B are included in the top 5) and quality preservation (i.e., higher-scoring candidates within each group are selected first).
With this, DCS attains a measure of Skew
% Measuring Skew, %and NDKL, 
% this algorithm brings this metric 
much closer to 1 than the original ranking.

\noindent\emph{\underline{Skew calculation:}} The group proportion is 50\% for both groups. Considering a final ranking of 5 candidates, in the original ranking there are only candidates from Group A and 0 from Group B, thus
$Skew_{A,orig}= \frac{1.0}{0.5}=2$ and $Skew_{B,orig}= \frac{0.0}{0.5}=0$. With DCS, 40\% of the candidates are from Group A and 60\% from Group B, thus $Skew_{A,DCS}= \frac{0.4}{0.5}=0.8$ and $Skew_{B,DCS}= \frac{0.6}{0.5}=1.2$.
   \qedfull
\end{example}


%[18]
\subsubsection{Double-corrected Variance Estimator}
\label{subsec:intrank_meth_metr_18n}

Existing metrics such as the max-min difference, max-min ratio, max absolute difference, mean absolute deviation, and variance are criticized in~\cite{lum2022biasing}. Despite their widespread usage, these metrics are inherently biased, as they neglect statistical uncertainty in the base metrics they summarize. This often leads to an exaggerated representation of performance disparities across groups.

To counter this issue, the authors introduced a statistically unbiased estimator called the \emph{double-corrected variance estimator}. This estimator considers both the inherent sampling variance of the base metric and the additional sampling variance induced by the bootstrap procedure.
Through simulations and real-world datasets, the authors showed this estimator effective and reliable in summarizing group performance disparities and quantifying associated uncertainties.

This method calculates a performance metric $Y_k$ for every group $k$ as $m(w_k, f(x_k))$, where $m$ is a group-wise model performance metric, and $Y_k$ represents a summary statistic of model performance across all individuals in group $k$.
The notation employs the function $m$ to calculate the model performance, taking into account the weights $w_k$ assigned to the group and the input features $f(x_k)$ associated with the group. 

The double-corrected estimator
% (Equation \ref{eq:metr_double}) 
is used to estimate the variance of the model performance across different groups. It is derived by correcting the statistical bias in the variance estimate. It is given by subtracting the average squared standard error of the estimate of the model performance for each group
%, represented as $\hat{\sigma}_k^2$, 
from the original model variance.
 % $M_{var}(Y)$.
The double-corrected estimator provides a conceptually transparent and easily calculable estimate of between-group model performance variance. It is statistically unbiased when unbiased estimates of the standard errors are available. This estimator is appealing due to its simplicity and the ability to calculate it without relying on specialized statistical software packages. 

% \begin{equation}
% \hat{M}_{\operatorname{var}}(\mu)=M_{\operatorname{var}}(Y)-\frac{1}{K} \sum_k \hat{\sigma}_k^2
% \label{eq:metr_double}
% \end{equation}

However, while the method quantifies performance disparities across groups, it does not provide a threshold for acceptable disparities or confirm that the chosen grouping variables are appropriate. Consequently, interpreting these measurements requires \emph{contextual understanding} and remains crucial to addressing intersectional bias.


%[20]
\subsubsection{Group Rank Metric and Ranking Correlation Metric}
\label{subsec:intrank_meth_metr_20n}

A new evaluation metric is introduced in~\cite{wang2022towards}, where the inadequacies of current evaluation methods when dealing with intersectionality are also discussed.

The paper identifies two conceptualizations of these existing metrics: \emph{one-vs-all} and \emph{pairwise comparisons}. The one-vs-all metrics often result in minority groups showing the highest deviation, as the majority group has the greatest influence on the definition of `all'. The pairwise comparisons conceptualization only incorporates values of two groups, typically maximum and minimum, thus ignoring the rest, which becomes a more pronounced issue as the number of groups increases.
To represent these two conceptualizations, the authors define a category of metrics termed \emph{max difference}, which includes metrics like the max True Positive Rate (TPR)\footnote{TPR is defined similarly to FPR: $\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$.} difference, based on the maximum difference between either one group and all others or between two groups.
The authors observe that these metrics, in a multi-attribute setting, can overlook the relative performance of multiple groups. They provide an example where two different distributions with three groups' TPRs as $\{.1, .2, .8\}$ and $\{.1, .6, .8\}$ would report the same max difference of .7, despite the distinct distributions.

To demonstrate the limitations of these existing metrics, the authors propose two new metrics that consider group rankings and correlations in rankings, offering a finer evaluation of fairness across multiple groups.

\begin{itemize}
  \item \emph{Group Rank Metric:} This metric measures group rankings based on specific characteristics, such as the positive label base rate or TPR. The authors argue that such a rank-based metric can highlight systematic discrimination when one group consistently ranks below another, even if max difference metrics suggest that the fairness criteria have been met.
  
  \item \emph{Ranking Correlation Metric:} This metric measures the correlation between rankings and helps to understand the degree to which existing social hierarchies are preserved in the model predictions. A high correlation between these rankings would indicate the solidification of existing hierarchies, thereby preserving existing disparities. This metric is calculated using \emph{Kendall's Tau}~\cite{kendall1938new} as a measure of rank correlation, and \emph{p-values}~\cite{fisher1925statistical} are combined across runs of random seeds using Fisher's combined probability test.
\end{itemize}

By incorporating these new evaluation metrics, the authors suggest how to resist evaluation metrics that do not substantively incorporate intersectionality and extrapolate from existing metrics. It is also worth noting that these metrics should not be seen as a replacement for the existing metrics, but rather as a supplement providing additional perspectives on fairness.

%[19]
\subsubsection{Quantile Demographic Drift}
\label{subsec:intrank_meth_metr_19n}

Continuous model monitoring for bias is explored in~\cite{ghosh2022faircanary}. This method uses a metric called Quantile Demographic Drift (QDD), within the FairCanary system, for evaluating prediction distributions across subgroups to identify bias in machine learning models, thereby maintaining fairness in model predictions over time.
While QDD is not explicitly designed for fair ranking problems (it is a metric that measures bias in prediction events based on quantile bins), it may offer some insights into bias. Yet, additional considerations and methodologies tailored to fair ranking problems are likely to be needed to address the complexities and subtle distinctions of fairness in rankings.

This metric employs quantile binning to measure differences in prediction distributions across varied demographic groups or subgroups. QDD diverges from traditional threshold-based bias metrics as it does not require developers to predetermine specific fairness thresholds or depend on outcome labels which may not be available at runtime.

The QDD metric
% , elaborated in Equation \ref{eq:metr_qdd}, 
is defined for two groups, $G_1$ and $G_2$, equipped with two distributional samples of model scores, $S_1$ and $S_2$. These samples are divided into
 % $B$ 
 bins of equal size,
 % , $N_1$ and $N_2$, 
 and the QDD metric is computed for each bin $b$ as $Q D D_b=\mathbb{E}_{G_{1, b}}\left[S_1\right]-\mathbb{E}_{G_{2, b}}\left[S_2\right]$, i.e, comparing the expected values of $S_1$ and $S_2$.
In this respect, QDD can provide insights into individual fairness by comparing individuals at the identical rank or percentile without complex counterfactuals. The principle behind this is that, in the absence of bias between groups, individuals at identical ranks should have a zero distance in the prediction space.

% \begin{equation}
% Q D D_b=\mathbb{E}_{G_{1, b}}\left[S_1\right]-\mathbb{E}_{G_{2, b}}\left[S_2\right]
% \label{eq:metr_qdd}
% \end{equation}

The \emph{FairCanary} system, designed for continuous model monitoring, integrates this QDD metric to 
% . FairCanary monitors live model predictions and 
identify drift in the model's fairness concerning various protected groups. It also alerts the operator when performance metrics change significantly.
This method addresses intersectionality in fair rankings by acknowledging the performance of various intersectional groups and monitoring for disparities in outcomes. The QDD metric measures disparities at group and individual levels, while FairCanary's alert system allows for the timely detection and mitigation of emerging biases.


% \subsection{Other Approaches}
% \label{subsec:intrank_meth_other}

% This last subsection reports on related methods that do not fall in any of the previous categories, although they might prove valuable for intersectionality in fair rankings.
 % serves as a miscellany of additional papers and themes related to the topic, such as the problem of ranking aggregation and an insightful visualization tool for subgroup coverage in a ranking. 

%21
\subsubsection{Information-theoretic Intersectional Fairness}
\label{subsubsec:intrank_meth__other_extra_21n}

The method in \cite{kang2022infofair} introduces an approach to deal with intersectional fairness in ranking algorithms called `information-theoretic intersectional fairness' (INFOFAIR). This approach integrates multiple sensitive attributes at the same time, seeking to ensure statistical parity among demographic groups represented by these attributes.
 
The \emph{mutual information} $I(x ; y)$
% \eqref{eq:other_mutualinfo} 
measures the amount of information shared between two random variables, $x$ and $y$, and is defined as the difference between the entropy of $x$, $H(x)$, and the conditional entropy of $x$ given $y$, $H(x \mid y)$: 
$I(x ; y)=H(x)-H(x \mid y)=\int_x \int_y p_{x, y} \log \frac{p_{x, y}}{p_x p_y} d x d y$,
where $p_{x,y}$ is the joint distribution between $x$ and $y$, while the marginal distributions are denoted as $p_x$ and $p_y$. 
% The mutual information, denoted as $I(x ; y)$, 
 The entropy $H(x)$ represents the average information required to describe the random variable $x$. In contrast, the conditional entropy $H(x \mid y)$ quantifies the remaining uncertainty of $x$ when $y$ is known. The mutual information is then calculated as the integral over the joint distribution $p_{x,y}$, comparing it to the product of the marginal distributions $p_x$ and $p_y$.

% \begin{equation}
% I(x ; y)=H(x)-H(x \mid y)=\int_x \int_y p_{x, y} \log \frac{p_{x, y}}{p_x p_y} d x d y
% \label{eq:other_mutualinfo}
% \end{equation}

INFOFAIR is formalized as a problem of minimizing mutual information, which measures the dependency between two variables: learning outcomes and vectorized sensitive attributes.
The goal is to reduce the correlation between these two variables, ensuring that the learning outcomes are not biased by the sensitive attributes, hence achieving statistical parity.


INFOFAIR uses a \emph{variational representation} of mutual information to accomplish this. This model encapsulates the variational distribution between the learning outcomes and sensitive attributes and the density ratio between the variational and original distributions. This approach overcomes computational challenges posed by high-dimensional data by using a variational lower bound of mutual information instead of directly computing the mutual information. The variational representation of mutual information is further represented as the Kullback-Leibler 
%(KL) 
divergence between the joint distribution and the product of the marginal distributions.

The INFOFAIR approach demonstrates its applicability across various learning tasks using a gradient-based optimizer, and it can be adapted to different statistical notions of fairness. This flexibility is a significant asset in ensuring intersectionality in rankings, as it allows multiple sensitive attributes to be considered simultaneously. It ensures fairness across all demographic groups represented by these attributes while maintaining the quality of learning outcomes.

%22
\subsubsection{Rank Aggregation}
\label{subsubsec:intrank_meth__other_aggr_22n}


The \emph{Multi-attribute Fair Consensus Ranking} (MFCR) problem is addressed in~\cite{cachel2022mani} through a multi-faceted approach. This approach is about ensuring fairness and preserving the preferences in consensus rankings (a collective ranking derived from multiple individual rankings, and intended to best represent the group's overall preferences or judgments) for candidates characterized by various protected attributes. 

The methodology is divided into two main components: the fairness component and the preference representation component. Based on statistical parity, the fairness component employs \emph{Multiple Attribute and Intersectional Rank} (MANI-RANK). The fairness of the ranking is evaluated at both the individual and intersectional group levels.

\emph{Pairwise Disagreement Loss} is used in the preference representation component. This measure quantifies the degree to which the preferences of the rankers are not reflected in the consensus ranking. The objective is to minimize this loss, thereby maximizing the degree of preference representation in the consensus ranking.

A series of algorithms are introduced to solve the MFCR problem efficiently, built upon the MANI-RANK formulation. These algorithms, named Fair-Kemeny, Fair-Copeland, Fair-Schulze, and Fair-Borda, are based on the efficient consensus generation methods of Kemeny, Copeland, Schulze, and Borda, respectively \cite{kemeny1959mathematics}, \cite{copeland1951reasonable}, \cite{schulze}, \cite{borda1781m}. These algorithms generate a consensus ranking that complies with the abovementioned fairness and preference representation components.

In more detail, the Fair-Kemeny algorithm is an optimal solution that uses MANI-Rank as a constraint on the exact Kemeny consensus ranking; however, Fair-Kemeny inherits NP-hardness from the classical Kemeny problem. The other algorithms, Fair-Copeland, Fair-Schulze, and Fair-Borda, are polynomial-time algorithms suitable for more extensive candidate databases that enforce the MANI-Rank group fairness while minimizing the increase in Pairwise Disagreement Loss caused by the introduction of fairness.

The approach also introduces a measure of positive outcome allocation for a group of candidates called the Favored Pair Representation (FPR) score, calculated by the number of pairs each candidate is favored over another, normalized by the total number of mixed pairs for the group. A score of 0 signifies that the group is at the bottom of the ranking, 1 at the top, and 0.5 implies fair treatment.

Further, Attribute Rank Parity (ARP) and Intersectional Rank Parity (IRP) measures are introduced to ensure fairness at the granularity of the attribute. These measures translate directly into the fair treatment of protected and intersectional attribute groups.

Finally, the provided method integrates the notion of Multiple Attribute and Intersection Rank (MANI-Rank) group fairness. This unified fairness measure, defined using a threshold parameter for \emph{fine-tuning} a desired degree of fairness, applies to consensus and single rankings over candidate databases with multiple protected attributes.
Ultimately, the method accommodates intersectionality by ensuring fair treatment across multiple protected attributes and their intersections. 


%24
% \subsubsection{Visualization}
% \label{subsubsec:intrank_meth_other_vis_24n}

% The \emph{MithraCoverage} system presented in \cite{jin2020mithracoverage} is designed to examine and tackle population bias in datasets, explicitly focusing on intersectional subgroups which are inadequately represented. This system is crucial in promoting intersectionality in data rankings by ensuring that all intersecting groups within a dataset are well represented.

% The system operates as a web application, offering an interactive visual interface that lets data scientists identify and analyze subgroups with poor coverage. It exploits the \emph{coverage} concept, representing intersectional subgroups in the dataset. Each subgroup, an intersection of multiple attributes, must contain an adequate number of entries. A threshold parameter defines this adequate number.
% The system identifies subgroups represented below this threshold as `Uncovered Patterns', and trieds to discover the most general uncovered patterns, which it refers to as \emph{Maximal Uncovered Patterns} (MUPs).
% In cases where specific subgroups are unrealistic or uncommon, they can be specified as invalid, and a new MUPs chart can be created that excludes them.

% The system uses algorithms to identify MUPs within the dataset. The user can then interact with the system to explore these MUPs through a visual interface
% % (Figure \ref{fig:4.oth1}). The visual tool allows the user 
% so as to select the dataset, set investigation parameters, and provide visual information to explore the MUPs.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Images/4-INTFAIR/4.oth1.jpg}
%     \caption{Steps example in MithraCoverage web application \cite{jin2020mithracoverage}.}
%     \label{fig:4.oth1}
% \end{figure}


\subsection{Synoptic Table}
\label{sec:intrank_table}
The synoptic table shown in Table~\ref{tab:synoptic_table} provides a comparative view of the papers discussed in the previous sections.
The table has been designed to focus on the distinguishing features of each work, while offering a broad perspective on the main aspects relevant to intersectionality in the fair rankings landscape.

% The essence of this table lies in its ability to enable easy comparison across different studies. It will serve as a reference guide, facilitating an understanding of how each work contributes to the larger discourse of intersectionality in fair rankings. The tabular format will assist in visualizing the distinctions and similarities between various methodologies in the current literature.

\subsubsection{Table's Structure}
\label{subsec:intrank_table_struct}

% The synoptic table designed to gather and compare research papers on intersectionality in fair rankings is organized with a clear, systematic structure. It is designed to encapsulate essential aspects of each research paper, providing a comprehensive overview that facilitates understanding and comparison.

Each row of the table represents an individual research paper. The columns correspond to distinct facets of each study, which comprise:
% effectively deconstructing each paper into fundamental attributes. Here is a brief description of each column:

\begin{itemize}
    \item \emph{Task}: The specific problem or task that the paper addresses within the context of intersectionality in fair rankings.
    
    \item \emph{Input}: The nature of the input data that the method requires.
    
    \item \emph{Method}: The method's or algorithm's name among those described in the previous sections.
    
    \item \emph{Output}: The nature of the result of applying the method.
    
    \item \emph{Process}: The time of application of the method, i.e., whether it works before obtaining the ranking (pre-processing) or after that (post-processing).
    
    \item \emph{Dataset}: The datasets upon which the method was tested or validated. Subsection~\ref{subsec:intrank_table_ds} presents a list of the real-world datasets used in the table.
    
    \item \emph{Fairness Metrics}: The metrics used to quantify fairness and validate the results.
    
    \item \emph{Performance Metrics}: The metrics used to evaluate the performance quality of the ranking in order to highlight differences in utility before and after the application of the method.
    
    \item \emph{Balance}: This attribute indicates if the paper discusses the problem of balancing fairness and performance, a critical aspect of evaluating the utility and practicality of each approach.

\end{itemize}


\subsubsection{Datasets}
\label{subsec:intrank_table_ds}

This section provides an overview of the real-world datasets employed to test and validate the methods included in Table~\ref{tab:synoptic_table}.
% proposed in these research papers in the field of intersectionality in fair rankings.


\begin{itemize}
    \item \emph{Medical Expenditure Panel Survey (MEPS)} \cite{cohen2009medical}: A real-world dataset providing detailed information on health services used by Americans, the frequency with which they are used, the cost of these services, and how they are paid for.
    
    \item \emph{CS department rankings} \cite{csranking}: It consists of information on 51 computer science departments in the US. It includes publication count, department size (large or small), and geographic area. The dataset aims to rank the top CS departments while ensuring representation across different sizes and locations.
    
    \item \emph{IIT-JEE Dataset} \cite{team2011}: It consists of scores obtained by candidates in the Joint Entrance Exam (JEE Advanced), which is the admission test for undergraduate programs in the Indian Institutes of Technology (IITs).
    
    \item \emph{US 2010 Census Dataset}
    \cite{bureau2010frequently}:
    It consists of information on last names in the United States. It includes 151,671 distinct last names that occurred at least 100 times in the census, with 23,656 last names occurring at least 1,000 times. This dataset provides insights into the distribution of last names and the racial and ethnic composition of individuals associated with those names based on the US 2010 Census.

    \item \emph{Income Dataset} \cite{bureaufinc02}: The Income Dataset provides aggregated family income data categorized by race. The US Census Bureau compiles it from the Current Population Survey of 2018. The dataset includes information on 83,508,000 families and consists of four racial categories (White, Black, Asian, and Hispanic), 12 age categories, and 41 income categories. For each combination of race, age, and income category, the dataset provides the number of families whose reference person falls within those specific categories. This dataset offers insights into the distribution of family income among different racial groups, age ranges, and income levels.
    
    
    \item \emph{COMPAS} \cite{angwin2016machine}: It contains demographic information, recidivism scores generated by the COMPAS software, and details about criminal offenses for 6,889 individuals.
    It was used to investigate racial bias in criminal risk assessment software.
    
    
    \item \emph{US Department of Transportation (DOT) Dataset} \cite{dot}: It refers to the flight on-time database published by DOT, commonly utilized by external websites. This dataset serves as an example for exploring sampling techniques in large-scale scenarios.
    
    
    \item \emph{UCI Adult Dataset} \cite{lichman2013}: A widely-used real-world dataset from the UCI Machine Learning Repository that contains census data extracted from the 1994 Census database.
    The dataset comprises information such as age, working class, education level, marital status, occupation, relationship, race, gender, capital gain and loss, working hours, and nationality for 48,842 individuals.
    
    
    \item \emph{Chess Rankings} \cite{fide}: a list of chess players ranked according to their ratings by the World Chess Federation (FIDE). It also includes the players' full names, images, and self-identified binary gender.
    

    \item \emph{Crunchbase Entrepreneurs Ranking} \cite{crunchbase}: a list of startup founders in the United States who received Series A funding in the past five years from Crunchbase. The dataset includes the founders' names, images, and self-identified binary gender.

    \item \emph{Equestrian Rankings} \cite{fei}: a list of equestrian athletes ranked based on their ratings from the official Fédération Equestre Internationale (FEI) website. The dataset includes the athletes' ratings, full names, images, and self-identified binary gender.
    
    \item \emph{AirBnB Dataset} \cite{airbnb}: It comprises information from approximately 2 million real properties on the popular online peer-to-peer travel marketplace, AirBnB. The dataset includes 41 attributes for each property, with 36 being boolean attributes indicating the availability of facilities such as TV, internet, washer, and dryer.    
\end{itemize}


%SYNOPTIC TABLE
\newpage

\begin{landscape}

\centering
\tiny 
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.8}
\bgroup
\def\arraystretch{1.5}
\begin{longtable}{|p{0.05\linewidth}|p{0.077\linewidth}|p{0.171\linewidth}|p{0.092\linewidth}|p{0.121\linewidth}|p{0.042\linewidth}|p{0.077\linewidth}|p{0.06\linewidth}|p{0.094\linewidth}|p{0.052\linewidth}|}  
\hline
\textbf{No.} & \textbf{Task} & \textbf{Input} & \textbf{Method} & \textbf{Output} & \textbf{Process} & \textbf{Dataset}  & \textbf{Fairness Metrics} & \textbf{Performance Metrics} & \textbf{Balance} \\ \hline
\endfirsthead 
\hline
\multicolumn{10}{|c|}{\textbf{Table \thetable}: Continued} \\
\hline
\textbf{No.} & \textbf{Task} & \textbf{Input} & \textbf{Method} & \textbf{Output} & \textbf{Process} & \textbf{Dataset}  & \textbf{Fairness Metrics} & \textbf{Performance Metrics} & \textbf{Balance} \\ \hline
\endhead 
\hline
\multicolumn{10}{|r|}{{Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot

\multicolumn{10}{|c|}{\textbf{CONSTRAINTS BASED}} \\ \hline
\cite{yang2019balanced} %[9n]
& - Balancing diversity and in-group fairness. 
& - Set of items with a score associated for each item. \newline
- Set of sensitive attributes.\newline
- A predefined set of diversity and in-group fairness lower bounds.\newline
- A value $k$ representing the desired size of the subset to be selected.\newline
- An in-group fairness measure. \newline
& - Diversity constraints.\newline
- ILP formulation.\newline
- Leximin. 
& - Utility maximizing ranking of $k$ items that satisfies the diversity and in-group fairness constraints. \newline
& - Pre and post. 
& - MEPS.\newline
- CS departments.  
& - IGF Ratio.\newline
- IGF Aggregated. 
& - Utility loss. 
& Yes. \\ \hline

\cite{celis2020interventions} %10n
& - Mitigate the effects of implicit bias in ranking and subset selection problems. 
& - Set of items to be ranked.\newline
- Each item's latent utility, represented by its weight or utility $w$.\newline
- Each item's membership to one or more intersectional groups.\newline
- Implicit bias parameters for each group.\newline
- Position-based discount for each position in the ranking.\newline
- Constraints for the representation of underprivileged groups in the ranking. 
& - L-constraints. 
& - A ranking that maximizes the latent utility subject to the given constraints.
& - Pre. 
& - IIT-JEE.  
& / 
& / 
& / \\ \hline


\cite{mehrotra2021mitigating} %12n 
& - Handle noisy protected attributes to improve fairness. 
& - Set of items, where each item has a utility $w_i \geq 0$.\newline
- A desired subset size $n$.\newline
- Protected attributes for each item (which may be noisy, missing, or imputed using proxy information).\newline
- Unbiased probabilistic information about the true protected attributes. 
& - LP approximation of denoising problem. 
& - A subset of $n$ items with the largest total utility that also satisfies the fairness constraints given the noisy protected attributes and the unbiased probabilistic information about the true protected attributes. 
& - Pre. 
& - US 2010 Census.\newline
- Income.\newline
- Synthetic.  
& - 'Risk difference'.
& - 'Utility ratio'. 
& - Yes. \\ \hline

\cite{asudeh2019designing} %13n
& - Improve fairness by choosing weights of the score function. 
& - $n$ items, each with several potentially relevant attributes.\newline
- Fairness constraints specifying a minimum bound on the number of selected members of a protected group at the $top-k$.\newline
- Scoring function that associates non-negative weights with item attributes and computes item scores. 
& - Satisfactory regions. 
& - A fair scoring function that satisfies the fairness constraint and is as close as possible to the user-specified scoring function in terms of attribute weights.\newline
- An ordered list of items sorted by their scores under the fair scoring function. \newline\newline
& - Pre and post. 
& - COMPAS.\newline
- US Department of Transportation.  
& / 
& / 
& Yes. \\ \hline

\multicolumn{10}{|c|}{\textbf{INFERENCE MODEL BASED}} \\ \hline
\cite{yang2020causal} %14n
& - Consider intersectionality in score based tasks and learn to rank tasks in a new way. 
& - Structural causal model.\newline
- Sensitive attributes.\newline
- $Y$ outcome variable used as a utility score in the ranking task.\newline
- Non-sensitive predictor variables.\newline
- Training dataset. 
& - Causal inference model. 
& - Score-based result: Counterfactually fair ranking of the dataset based on the utility score $Y$.\newline
- Learning to rank result: A learned model based on the counterfactual training data that can be used to rank the unmodified test data. 
& - Post. 
& - COMPAS.\newline
- MEPS.\newline
- Synthetic.  
& - Demographic parity.\newline
- Equal opportunity.\newline
- NDKL divergence.\newline
- IGF Ratio. 
& - $Y$ utility loss at $top-k$.\newline
- Average precision. 
& Yes. \\ \hline

\cite{mhasawade2021causal} %15n
& - Improve fairness by considering a causal multi-level fairness model. 
& - Set of variables associated with the system including individual-level and macro-level sensitive attributes.\newline
- Individual-level variables, macro-level variables.\newline
- An outcome of interest.\newline
- A causal graph consisting of V which accurately represents the data-generating process.
& - Causal inference model.\newline
- Multi-level path-specific fairness. 
& - A classifier that is fair with respect to both individual-level and macro-level sensitive attributes. 
& - Post. 
& - UCI Adult dataset.\newline
- Synthetic.  
& / 
& / 
& Yes. \\ \hline

\multicolumn{10}{|c|}{\textbf{METRICS BASED}} \\ \hline
\cite{ghosh2021fair} %17n
& - Highlight how demographic inference impacts fairness in rankings. 
& - Demographic information (either ground-truth or inferred) of individuals being ranked.\newline
- Error rates of demographic inference algorithms.\newline
- Fair rankings of individuals. 
& - DetConstSort. \newline
& - Evaluation of the impact of demographic inference errors on the fairness of the rankings. 
& - Post. 
& - Chess Rankings.\newline
- Crunchbase Entrepreneurs Ranking.\newline
- Equestrian Rankings.\newline
- Synthetic.  
& - Represen-\newline tation based: Skew; NDKL.\newline
- Attention-based: Attention per group; Attention bias ratio. 
& - NDCG.\newline
- Rank Change. 
& Yes. \\ \hline

\cite{lum2022biasing} %18n
& - Develop a statistically unbiased measure of group-wise performance disparities. 
& - Performance differences across socially or culturally relevant groups.\newline
- Statistical bias in current metrics used to measure group-wise performance disparities.
& - Double-corrected variance estimator. 
& - A statistically unbiased estimator for summarizing group performance disparities with associated uncertainty quantification. 
& - Post. 
& - Adult Income.\newline
- Synthetic.  
& - 'meta-metrics', showing their biases. 
& - Selection rate.\newline
- FPR.\newline
- TPR. 
& Yes. \\ \hline

\cite{lum2022biasing} %20n
& - Highlight the inadequacies of current evaluation methods in intersectionality. 
& - Demographic attributes for intersectional analysis. 
& - Introduction of new metrics. 
& - Comparisons between new and current metrics. 
& - Post. 
& - US 2010 Census.  
& - Max TPR difference.\newline
- Group rank. \newline
- Ranking correlation.
& / 
& / \\ \hline

\cite{ghosh2022faircanary} %19n
& - Solve the problem of drift in machine learning and artificial intelligence models, specifically with regards to ensuring fairness over time. 
& - Continuous model monitoring systems for detecting issues.
& - Quantile Demographic Drift.\newline
- FairCanary system. 
& - Identification of issues with models at deployment time and mitigations applied by developers. 
& - Post. 
& - Synthetic.  
& - Statistical Parity Difference.\newline
- Disparate Impact. 
& / 
& / \\ \hline


% \multicolumn{10}{|c|}{\textbf{OTHER APPROACHES}} \\ \hline
\cite{kang2022infofair} %21n
& - Ensure statistical parity among demographic groups formed by multiple sensitive attributes of interest. 
& - A set of sensitive attributes.\newline
- A set of data points that includes feature vectors, labels, and sensitive attribute values.\newline
- A learning algorithm. 
& - INFOFAIR method. 
& - Revised learning outcomes that mitigate bias and optimize classification accuracy.\newline
- Statistical parity on multiple sensitive attributes. 
& - Post. 
& - COMPAS.\newline
- Adult Income.  
& - Average statistical imparity.\newline
- Relative bias reduction. 
& - Micro/Macro F1. 
& Yes. \\ \hline

\cite{cachel2022mani} %22n
& - Combine the preferences of many rankers while ensuring fair treatment for candidates with multiple protected attributes. 
& - Multiple base rankings over candidates defined by multiple and multi-valued protected attributes.
& - MANI-RANK method. 
& - A consensus ranking that reflects the preferences of all rankers while ensuring fair treatment for candidates with multiple protected attributes. 
& - Post. 
& - Synthetic. 
& - Pairwise Disagreement loss.\newline
- Favored Pair Representation.\newline
- Attribute Rank Parity.\newline
- Intersectional Rank Parity. 
& - Kendall's Tau distance.
& Yes. \\ \hline

% \cite{jin2020mithracoverage} %24n
% & - Identify visually intersectional subgroups with poor coverage in a dataset. 
% & - Dataset in csv file.\newline
% - Population bias.\newline
% - Subgroups.\newline
% - Coverage threshold.
% & - MithraCoverage.
% & - Uncovered subgroups in 'MUP' chart. 
% & - Post. 
% & - COMPAS.\newline
% - Income.\newline
% - AirBnB. 
% & / 
% & / 
% & / \\ \hline

\caption{Synoptic table on intersectionality in rankings}
\label{tab:synoptic_table} 
\end{longtable}

\egroup
\end{landscape}
\newpage
\normalsize
% \raggedright



\subsubsection{Discussion}
\label{subsec:intrank_table_results}

The systematic analysis of the studies presented in this section as well as a comparison via a synoptic table has highlighted the significant role of intersectionality in the context of fair rankings.

Our analysis demonstrates that intersectionality has several practical repercussions and can be implemented in actual 
% is more than an abstract or theoretical notion. It is possible to implement it in the \emph{practical context} of 
ranking systems.
Indeed, the reviewed papers provided solid instances where intersectional considerations were effectively integrated into algorithmic processes.
% thus, making fairness a feasible goal in real-world applications.

Moreover, the significance of intersectionality in achieving truly equitable rankings has been vividly showcased.
 % in the research conclusions. 
While providing a certain level of equality, classical approaches to fairness often overlook intersecting identities and experiences. For example, in \cite{lum2022biasing} and \cite{wang2022towards} (Subsection \ref{subsec:intrank_meth_metr}), it is convincingly argued that traditional fairness metrics may need to catch up in capturing the complexities of multi-dimensional identities and their influence on outcomes. As a result, conventional methods can be considered inadequate when intersectionality is at stake.

One of the most crucial findings from our survey is that incorporating intersectionality in fair rankings does not necessarily lead to a significant loss in \emph{utility}. A notable example is \cite{mehrotra2021mitigating} (Subsection \ref{subsec:intrank_meth_constr}), where the authors achieved intersectional fairness without considerably compromising the algorithm's effectiveness.
This suggests that it is not just possible to introduce intersectionality in fair rankings, but it can be accomplished in most cases without causing significant drawbacks. Several methods are provided to balance potential losses, thereby ensuring the maintenance of intersectionality.

However, implementing intersectionality and, in general, fairness in rankings poses several challenges.
Among these, determining an acceptable threshold or balance that satisfies all stakeholders while maintaining the intended level of fairness is an extremely difficult task. Similarly, constructing a robust and universally acceptable ethical model, as discussed in \cite{yang2020causal} (Subsection \ref{subsec:intrank_meth_inf}), is another non-trivial issue.
