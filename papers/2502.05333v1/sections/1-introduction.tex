\section{Introduction}

In today's increasingly data-driven world, rankings
% , i.e., the relative position of an item or person with respect to its importance, quality or success, 
are a crucial aspect of decision-making processes in various domains, such as search engines~\cite{mcdonald2021search}, recommendation systems~\cite{pitoura2022fairness,DBLP:conf/edbt/AzzaliniACDMA24}, hiring practices~\cite{balagopalan2023role,fabris2023fairness}, crowdsourcing~\cite{chen2019fairness,singh2018fairness,DBLP:conf/icde/CiceriFMT16}, and more. However, the algorithms that generate these rankings are often \emph{biased}, leading to unfair and discriminatory outcomes~\cite{zehlike2022fairness1,zehlike2022fairness2}.
There is a growing concern over the ethical implications of these biased rankings, particularly about marginalized and underrepresented groups~\cite{stoyanovich2020responsible}. In response to these concerns, research on fairness and ethical ranking has gained significant attention in computer science.
The primary goal of these studies is to develop algorithms that promote \emph{fairness in rankings} by minimizing discrimination against protected attributes such as race, gender, or socio-economic status.

Moreover, responsible data science (RDS) and responsible artificial intelligence (RAI) have emerged as prominent areas of research and practice, aiming to address issues of ethics in artificial intelligence, legal compliance, data quality, algorithmic fairness and diversity, transparency of data, privacy, and data protection. Integrating RDS and RAI into developing fair ranking algorithms is essential for promoting a more equitable and ethical future for data-driven systems~\cite{lewis2021teaching,stoyanovich2020responsible}.

However, most existing fairness-aware ranking methods consider only one protected attribute at a time, overlooking the complex interplay of multiple attributes that define an individual's identity.
The concept of \emph{intersectionality} is a framework that helps to capture the experiences of individuals who belong to multiple marginalized and underrepresented groups, ensuring that fair ranking algorithms do not inadvertently perpetuate existing inequalities \cite{jagadish2022manyF}. Intersectionality was first introduced by Kimberl√© Crenshaw \cite{crenshaw1990mapping}, and it is essential in understanding and addressing this interplay between multiple social identities and sources of discrimination. The framework has been recognized as critical for understanding how biases and unfairness manifest in data-driven systems \cite{zehlike2022fairness1}.
In the context of fair rankings, intersectionality offers a more nuanced understanding of the various dimensions of bias and discrimination. By considering the combined effect of multiple protected attributes, intersectional approaches to fair rankings can provide more equitable outcomes for individuals from diverse backgrounds.
%examples
Given the importance of intersectionality in understanding the complex nature of bias and discrimination, it is crucial to incorporate this perspective into fairness-aware ranking algorithms.
We further illustrate this concept through examples.
\begin{example}
In \cite{kang2022infofair}, the authors describe a scenario involving job applications where the sensitive attributes of gender and race are under consideration. When examining gender (e.g., A or B) or race (say, 1 or 2) separately, the system may appear fair due to equal acceptance rates (e.g., three people accepted in both cases). However, when considering the intersection of gender and race, forming finer-grained gender-race groups (say, A-1, A-2, B-1, B-2), fairness may be skewed -- for instance, the acceptance rates for two intersectional groups (i.e., A-1 and B-2) may be notably lower than those of the other two intersectional groups (A-2 and B-1). This demonstrates how apparent fairness concerning individual sensitive attributes can mask biases when intersectionality is not considered.\markfull
\end{example}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.47\textwidth]{Images/1-INTRO/1.ex1.jpg}
%     \caption{Example of bias in job application classification \cite{kang2022infofair}.}
%     \label{fig:1.ex1}
% \end{figure}

\begin{example}
Another example of the importance of intersectionality is given in~\cite{yang2020causal},
% (Figure~\ref{fig:1.ex2})
focusing on the ranking of Computer Science departments in the United States~\cite{csranking}. The original ranking prioritizes large departments, particularly those located in the North East and the West, potentially marginalizing small departments and those in other geographical areas. However, after the application of a fairness-aware ranking method
 % (explained in Subsection \ref{subsec:intrank_meth_inf})
that considers size and location as intersectional sensitive attributes, the ranking becomes more balanced, including small departments among the top 20 and providing a more geographically diverse representation.
\markfull
\end{example}

% \framedtext{\textbf{1. Example Binary Attribute: University Admission Rankings} 

% Top-5 Ranking by Race: (1) Hispanic, (2) Black, (3) Hispanic, (4) Black, (5) Black

% Looks Fair by Race: This ranking looks fair by race, with both racial groups equally represented. 

% Top-5 Ranking by Socioeconomic Status (Income): (1) High, (2) Low, (3) High, (4) High, (5) Low

% Intersectional Fairness Issue: When socioeconomic status is considered, it reveals that high-income Hispanic students are ranked above low-income Hispanic students, despite having representation by race. This intersectional view shows an imbalance that was not visible by only examining race. }

% \framedtext{\textbf{2. Example Multiple Non-Binary Attributes: Leadership Program Selection} 

\begin{example}
For an example including a non-binary attribute, consider a selection process in which the focus on the top-6 candidates highlights the following results:
\begin{itemize}
\item Top-6 by Gender: (1) Woman, (2) Man, (3) Woman, (4) Man, (5) Woman, (6) Man.
\item Top-6 by Race: (1) Hispanic, (2) White, (3) White, (4) Black, (5) Hispanic, (6) Black.
\end{itemize}

Both rankings are apparently fair, showing balance in both gender and race.
However, a closer look reveals a possible intersectional fairness issue: when considering race and gender together, it turns out that Black women and Hispanic men are not at all present in the selected part of the ranking. %The intersectional imbalance becomes clear, showing a preference for White candidates that was not apparent from gender analysis alone. 
\markfull\end{example}

% \begin{figure}[H]
%     \centering
%     \subfloat[Original rank.\label{fig:1.ex2.0}]{
%         \includegraphics[scale=0.3]{Images/1-INTRO/1.ex2.jpg}
%     }
%     \quad
%     \subfloat[Mitigated rank.\label{fig:1.ex2.1}]{
%         \includegraphics[scale=0.3]{Images/1-INTRO/1.ex2.1.jpg}
%     }
%     \caption[Example of mitigation in CSranking]{CSRanking in 2020 by weighted publication count, considering intersectional groups based on department size (large - L and small - S) and location (North East - N, West - W, South East - S) \cite{yang2020causal}.}
%     \label{fig:1.ex2}
% \end{figure}

These examples highlight the importance of incorporating intersectionality, thus considering the complex interplay of multiple sensitive attributes, and providing a more equitable and accurate representation of the data.

Our main contribution is an exhaustive analysis and comparison of the present literature, investigating the methods and approaches proposed thus far for the implementation of intersectionality in rankings. The objective is not only to explain the diversity of methodologies but also to establish the necessity of intersectionality within fair rankings, along with its practical significance. To this end, for each category of methods, we also include an exhaustive example showing how such methods may work in practice.

In particular, we provide a comprehensive visual comparison of the explored literature in the form of a synoptic table, which effectively summarizes the salient features, strengths, and weaknesses of each contribution, thus facilitating an informed selection of the most suitable method relative to specific scenarios. This table not only makes the existing research more accessible, but also provides a useful tool for researchers and practitioners aiming to address intersectionality in their work.

Our analysis underscores how intersectionality extends beyond being a mere theoretical concept, playing a pivotal role in defining fair rankings. Consistently with the reviewed literature, our analysis shows that fairness without intersectionality often results in inadvertent discrimination. For instance, when single attributes are considered in isolation, the discriminatory impact on intersectional groups may be overlooked, thereby perpetuating a cycle of bias and unfairness. By revealing these insights, we envision a more comprehensive view of fairness, which inherently involves intersectionality.

In addition to our comparative review and theoretical observations, we show that incorporating intersectionality into fairness-aware ranking algorithms can be achieved without significant utility loss. This addresses concerns regarding the practical implications of integrating intersectionality, such as computational complexity and feasibility.

\emph{Outline.} After reviewing the main components characterizing ethical ranking in Section~\ref{ch:ethical_ranking}, we explore the concept of intersectionality, along with a brief history of its origin, in Section~\ref{ch:intersectionality}.
Section~\ref{ch:intersectionality_in_fair_rankings} represents the core of our contributions and includes detailed descriptions of the main methods for ensuring fair ranking while also considering intersectional aspects. Besides arranging the various methods in corresponding categories, Section~\ref{ch:intersectionality_in_fair_rankings} also includes a table summarizing the main characteristics of each method.
Finally, we present our concluding remarks in Section~\ref{ch:conclusions}.

