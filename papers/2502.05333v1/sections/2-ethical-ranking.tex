\section{Ethical Ranking}
\label{ch:ethical_ranking}%
% The \label{...}% enables to removal of the small indentation that is generated; always leaving the % symbol.

After briefly summarizing the main approaches to ranking, we introduce the ethical factors that are relevant for the analysis and development of ranking algorithms. We will focus on four key aspects, namely Bias, Fairness, Diversity, and Stability, each contributing significantly to the integrity and reliability of rankings.
%
Then, we will mainly concentrate on Bias and Fairness, as these are the most critical aspects with respect to our central theme of intersectionality in rankings. 

\subsection{Ranking}
\label{sec:ranking}

Ranking queries, also known as top-$k$ queries~\cite{DBLP:journals/csur/IlyasBS08}, are the main tool for extracting the most relevant results from a dataset. The underlying notion of relevance 
%(appropriateness, suitability, etc.) 
is typically defined through a so-called \emph{scoring function} (a.k.a. \emph{utility function}), which maps the (numeric) attributes of every item in a dataset into a real number, called \emph{score} or \emph{utility}.
In this respect, the scoring function provides a criterion for expressing preferences, since sorting the items by their score and limiting the sorted list to $k$ elements is the most common way to determine a \emph{ranking} of the (best) items in the dataset, according to the specified criterion.
Assuming $d$ numeric attributes $A_1,\ldots,A_d$, the scoring function $s(t)$ applied to a generic item $t$ is most typically expressed as a linear combination $s(t)=w_1 t[A_1] + \ldots + w_d t[A_d]$ of the values $t[A_1], \ldots, t[A_d]$ over those attributes, using \emph{weights} (i.e., coefficients) $w_1,\ldots,w_d$ indicating the relative importance of each attribute in determining the ranking.
%
Parameter $k$ in top-$k$ queries allows the user to precisely control the output size and restrict it to the desired number of elements (e.g., the number of candidates to be selected in a hiring process).

While ranking queries are the most practical means for addressing \emph{multi-objective optimization},
 % they have several limitations of their own, some of which we will discuss next. 
a few alternative approaches have been studied in the relevant literature, among which \emph{skylines} have received constant attention since their first proposal~\cite{DBLP:conf/icde/BorzsonyiKS01}. The notion of skyline is based on the geometric concept of \emph{dominance} ($a$ dominates $b$ if $a$ is never worse than $b$, on any attribute, and strictly better for at least one attribute): the skyline is then defined as the set of all non-dominated items, i.e., a Pareto-optimal set, which fairly includes all potentially optimal alternatives.
Indeed, the very motivation behind the introduction of skylines was to provide users with \emph{all} (not just $k$) relevant results, the other items being dominated, thus less relevant.
%
However, while numerous efficient processing and indexing techniques exist for top-$k$ queries, the inherently quadratic complexity of skylines makes them unsuitable for most practical applications.
In addition to that, skylines offer no control on the output size (which can be huge, and thus of little use, especially in the case of anticorrelated data) and are completely preference-agnostic.

Yet, top-$k$ queries, too, have well-known shortcomings, one of which is the difficulty in
specifying the scoring function exactly, especially in the presence of many attributes~\cite{DBLP:journals/tods/CiacciaM20,DBLP:conf/sigmod/MouratidisL021,DBLP:journals/pvldb/NanongkaiSLLX10}. This is common and especially evident when a user has to indicate the weights of a linear scoring function: not only is it hard to specify exact values, but unfairness may easily arise~\cite{asudeh2018obtaining}.
%
An even more subtle issue is that some types of scoring function may prevent some items to ever be top-scorers -- and this even against the intuition of the user expressing the preferences. For instance, it is well-known that, with a linear scoring function, only the items lying in the \emph{convex hull} of the dataset may appear on top of the resulting ranking~\cite{DBLP:conf/sigmod/ChangBCLLS00}, i.e., 
some non-dominated items might never be top-1 (no matter what linear scoring function is adopted).
%
Another counter-intuitive effect of common (e.g., linear) weight-based scoring functions is that they may tend to favor items with very unbalanced attribute values, even in the face of perfectly balanced weights. For instance, assessing candidates in a hiring process based on, say, their GRE score and their interview score, weighing both equally, might very well result in top candidates that are extremely good in one aspect but extremely poor in the other, while a more balanced output would (usually) be implicitly expected.

A large body of research has tried to address the aforementioned limitations, sometimes proposing hybridizations of top-$k$ queries and skylines~\cite{DBLP:journals/pvldb/CiacciaM17,DBLP:journals/tods/CiacciaM20,DBLP:conf/cikm/CiacciaM18,DBLP:conf/sebd/CiacciaM18,DBLP:conf/sisap/BedoCMO19,DBLP:conf/sebd/CiacciaM19,DBLP:conf/sigmod/MouratidisL021}. In particular, top-$k$ queries have been studied in the absence of exact values for the weights of a scoring function~\cite{soliman2011ranking}, while skylines have been extended with the ability to express preferences in the form of constraints on weights~\cite{DBLP:journals/pvldb/CiacciaM17}. Such constraints
% , available within the framework of the so-called \emph{flexible skylines}~\cite{DBLP:journals/tods/CiacciaM20}, 
allow reasoning on the space of weights so as to accommodate notions orthogonal to utility (including fairness) and identify suitable regions therein (such as the satisfactory regions of~\cite{asudeh2019designing}). Subsequent work has further extended skylines to counter their lack of control on the output size~\cite{DBLP:conf/sigmod/MouratidisL021} and has studied measures for preferentially selecting results that are balanced with respect to the weights expressed in a top-$k$ query~\cite{DBLP:journals/paccmod/CiacciaM25}.

It is important to observe that the approaches to ranking discussed so far are fundamental to correctly position the ideas discussed in the present document, but none of these explicitly considers the ethical factors, presented in the next subsections, that are at the core of our analysis.

\subsection{Bias}
\label{sec:bias}


Bias concerns the presence of unfair favoritism or discrimination within algorithms.
A computer system is defined as biased when it discriminates against specific individuals or groups in favor of others. This unfair discrimination can occur when an opportunity or benefit is denied, or an undesirable outcome is assigned to someone based on unreasonable or inappropriate factors \cite{friedman1996bias}.

\subsubsection{Types of bias}
\label{subsec:bias_types}

Bias in computer systems, as described in \cite{friedman1996bias}, can be of three types: preexisting, technical, and emergent. \emph{Preexisting bias} arises from social norms, practices, or attitudes.
An example of these (often unconscious) prejudices is the gender bias in specific job recommendation systems that arises from historical data trends, reflecting societal biases.
Constraints or considerations within the technology itself introduce \emph{technical bias}. For instance, limitations in a web crawling algorithm may lead to specific web pages being indexed more frequently, resulting in a bias towards these pages in search results.
\emph{Emergent bias} refers to a type of bias that arises when using a system rather than inherent in the system design or implementation. While preexisting bias and technical bias can be identified during the creation or implementation of a system, emergent bias appears over time due to changing societal knowledge, population, or cultural values.

%Further categorization of biases can help analyze and address these issues more efficiently.
Further distinctions are observed in~\cite{pitoura2018measuring},
where the authors suggest that \emph{user bias} exists when specific protected attributes such as race or gender influence the results presented to users.
For example, on a social media platform, the algorithm may show users more content from individuals who share similar demographic characteristics.
In contrast, \emph{content bias} refers to a bias in the information itself delivered to users, independent of the user's attributes. For example, in a news recommendation system, the algorithm may prioritize news articles from a particular political ideology, regardless of the user's preferences or attributes.
User bias can manifest without explicit usage of protected attributes. For instance, user data like geographical location, though seemingly unrelated, could imply information about protected attributes such as race. Consequently, ranking outcomes influenced by such data might entail a certain level of bias. 

%Besides ranking bias, it is essential to consider broader categories of bias that can affect ranking algorithms. 
In~\cite{pitoura2020social}, the authors distinguish between statistical and societal biases. \emph{Statistical biases} refer to errors in measurement or non-representative sampling, for example when in a survey to assess the satisfaction levels of public transportation services, the study only collects responses from individuals in urban areas, inadvertently excluding rural populations.
Instead, \emph{societal biases} reflect objectionable social structures, human biases, and preferences manifested in data, for example when in a dataset analyzing income levels, the data predominantly includes salary information from male employees, while disregarding income data from female employees due to historical gender pay gaps.
These biases can be harder to detect and address, as they often stem from deeply ingrained societal norms and prejudices.

\subsubsection{Bias in Rankings}
\label{subsec:bias_rank}

Bias is a significant factor in ranking algorithms and can affect their outcomes in various ways. According to \cite{mehrabi2021survey}, \textbf{ranking bias} is a crucial type resulting from systemic deviation. This deviation is caused by an item's position in a list, affecting its visibility and accessibility. Ranking bias is particularly prevalent in web search engines and recommendation systems, where items are displayed in a ranked order. The top-ranked items are more likely to be viewed and interacted with, while those at the bottom may need to be noticed, regardless of their relevance or quality. This can create a self-reinforcing cycle where popular items gain visibility, and lesser-known or new items struggle to gain attention.

However, bias does not exist in isolation~\cite{baeza2018bias}:
% . As shown in Figure~\ref{fig:2.bias1} from
different types of bias are indeed interconnected, with one bias influencing and being influenced by others. For example, a user's preference for scrolling can affect their mouse movement and which screen elements they click. This interplay of biases can significantly impact the design and performance of web-based systems. These systems can learn to reinforce their own biases or those of linked systems, leading to suboptimal solutions and self-fulfilling prophecies, where the system's predictions or recommendations become increasingly skewed over time.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Images/2-ETHRANK/2.bias1.jpg}
%     \caption{Interdependent biases impacting user interaction \cite{baeza2018bias}.}
%     \label{fig:2.bias1}
% \end{figure}

\subsection{Fairness}
\label{sec:fairness}


Fairness is closely tied to bias but aims for equitable treatment in algorithmic processes and is a critical aspect of algorithmic systems, particularly in the context of rankings and recommendations. As algorithmic systems become increasingly prevalent, concerns have been raised about the potential biases and discrimination embedded within these systems \cite{zehlike2022fairness1,zehlike2022fairness2}.

The definition of fairness, in this context, involves mitigating preexisting bias (introduced in Section~\ref{subsec:bias_types}) concerning a protected feature in the data. The protected feature refers to the membership of individuals in legally-protected categories, such as people with disabilities or under-represented minorities based on gender or ethnicity. These categories are referred to as protected groups, and the attributes that define them, such as ethnicity, are considered as sensitive ones. Discrimination in rankings occurs when the outcome is systematically unfavorable, such as systematically placing individuals with minority ethnicity or female gender at lower ranks \cite{asudeh2019designing}.


\subsubsection{Taxonomy}
\label{subsec:fairness_tax}

In this section, we refer to the comprehensive taxonomy of fairness as delineated in~\cite{pitoura2022fairness}, exploring the complex facets of fairness in the context of ranking and recommendation systems. The taxonomy 
% (Figure~\ref{fig:fairness_taxonomy}) 
distinguishes fairness models according to their applicability, their level, the aspect of the algorithm they focus on, and whether they pertain to single or multiple outputs.

%   \begin{center}
%   \begin{tikzpicture}[
%     level 1/.style={sibling distance=4cm},
%     level 2/.style={sibling distance=3cm, },
%     level 3/.style={sibling distance=4cm},
%     level 4/.style={sibling distance=4cm},
%     level distance=2.5cm,
%     scale=0.8
%     %style={draw, circle, minimum size=1cm}
%   ]

%     \node {Fairness}
%       child[shift={(1cm,0cm)}] {node {Level}
%         child[shift={(-2cm,0cm)}] {node {Individual}
%             child[shift={(-1cm,0cm)}] {node[text width=1.5cm, align=center] {Distance based}}
%             child[shift={(-2cm,0cm)}] {node {Counterfactual}}
%             }
%         child[shift={(-2.5cm,0cm)}] {node {Group}
%             child[shift={(4.2cm,0cm)}] {node[text width=1.5cm, align=center] {Base Rates}}
%             child[shift={(3cm,0cm)}] {node[text width=1.5cm, align=center] {Accuracy based}}
%             child[shift={(2.5cm,0cm)}] {node[text width=1.5cm, align=center] {Calibration based}}
%             }
%         }
%       child {node {Side}
%         child {node {Consumer}}
%         child {node {Producer}}
%         }
%       child {node {Output Multiplicity}
%         child[shift={(1.3cm,0cm)}] {node {Single}}
%         child[shift={(1cm,0cm)}] {node {Multiple}}  
%       };
%   \end{tikzpicture}
%   \captionof{figure}{Fairness Taxonomy}
%   \label{fig:fairness_taxonomy}
% \end{center}


\paragraph{Levels of Fairness.}
\label{subsubsec:fairness_tax_lev}
The concept of levels of fairness refers to the plane at which fairness is evaluated, assessed, and compared within the system's operation. It considers whether the algorithm's fairness should be judged based on its impact on individual entities or broader groups.

\begin{itemize}
  \item \emph{Individual:} This addresses treating similar entities similarly. This level of fairness can further be divided into two sub-categories:
  \begin{itemize}
  \item \emph{Distance-based:} This form of fairness is tied to the proximity between entities and the outcomes produced by an algorithm. It asserts that the algorithm's output should also be similar or close if two entities have similar characteristics. This approach relies on the Lipschitz property, where the disparity between the probability distributions assigned by a classifier should not exceed the distance between the entities.
  \item \emph{Counterfactual:} This form of fairness operates on the hypothetical plane. It would consider an output fair towards an entity if the same output was produced even if the entity belonged to a different demographic group. This type of fairness is formalized using the tools of causal inference (see Section~\ref{subsec:intrank_meth_inf}).
\end{itemize}

  \item \emph{Group:} This considers fairness between two groups, usually protected and non-protected groups. Three types of group fairness are mentioned in \cite{pitoura2022fairness}:
  \begin{itemize}
  \item \emph{Base Rates:} This approach focuses exclusively on the algorithm's output. It seeks to ensure that the probability of a favorable outcome is the same between the protected and non-protected groups, a state known as demographic or statistical parity.
  \item \emph{Accuracy-based:} Unlike base rates, accuracy approaches consider both the algorithm's output and the ground truth. These approaches aim to balance classification errors across different groups. An example is `equal opportunity', which ensures equal accurate favorable rates across groups.
  \item \emph{Calibration-based:} These approaches examine predicted probability and the ground truth. They aim to ensure that a classifier is equally well-calibrated for all groups. In practice, this could mean that at any given predicted probability score, the chance of a favorable outcome should be the same for individuals from any group.
\end{itemize}
\end{itemize}

\paragraph{Multi-sided Fairness.}
\label{subsubsec:fairness_tax_side}

This considers fairness from two sides: the items ranked or recommended (producer or item-side fairness) and the users receiving the rankings or recommendations (consumer or user-side fairness).

\begin{itemize}
  \item \emph{Consumer:} The focus is on the users receiving the data. The aim is to provide similar users, or groups of users, with similar rankings or recommendations.
  \item \emph{Producer:} The goal here is to ensure similar items or groups of items are ranked or recommended similarly.
\end{itemize}


\paragraph{Output Multiplicity.}
\label{subsubsec:fairness_tax_output}

 This dimension of fairness involves considering how much fairness is achieved over an algorithm's single or multiple outputs. This becomes particularly pertinent in cases where single-instance fairness might not be feasible, but fair treatment can be achieved over multiple instances. It broadly differentiates between two types:

\begin{itemize}
  \item \emph{Single:} This type of fairness ensures that each instance of an algorithm's output, such as a single recommendation or ranking, is fair. The primary aim is to ensure that each output independently complies with the fairness criteria defined.
  \item \emph{Multiple:} This type focuses on achieving fairness over a series of algorithmic outputs, whether rankings or recommendations. Here, it is not necessarily about each output being fair but rather about ensuring overall fairness across a sequence or series of outputs. This can sometimes be called `eventual' or `amortized' fairness. This approach is particularly relevant in dynamic systems like sequential recommenders, where user interactions are continuous and recommendations evolve. In such systems, fairness corrections can be made while moving from one interaction to the next, ensuring overall fair treatment in the long run.
\end{itemize}





\subsubsection{Fair Ranking Tasks}
\label{subsec:fairness_rank}


Two types of ranking tasks are distinguished in \cite{zehlike2022fairness1} and \cite{zehlike2022fairness2}: score-based and supervised learning. In \emph{score-based ranking}, candidates are arranged according to a score attribute, potentially computed in real-time, and returned in the sorted sequence. \emph{Supervised learning-to-rank} entails a preference-supplemented training set of candidates, with preferences indicated via scores, preference pairs, or lists. This training set trains a model that predicts the ranking of unseen candidates. In both ranking tasks, the aim is to identify the best-ranked $k$ candidates, i.e., the top-$k$.

Although supervised learning-to-rank may seem similar to classification, a fundamental difference exists. The objective of classification is to allocate a class label to each item independently. In contrast, learning-to-rank situates items in relation to each other, making the outcome for one item interdependent of the outcomes for the others. This interdependence has profound ramifications for the structure of learning-to-rank methodologies in general and fair learning-to-rank in particular.

Applying appropriate methodologies and metrics to ensure fairness becomes crucial in this context. The choice of these methods and metrics will depend on the specific nature of the ranking task, the definition of fairness applied, and the particular characteristics of the data.

In Section~\ref{ch:intersectionality_in_fair_rankings}, we will dive deeper into these subjects, discussing various methods and metrics for promoting and measuring intersectionality in rankings.

\subsubsection{Fairness Scenarios}
\label{subsec:fairness_scenarios}

The Fairness Tree~\cite{baresi2023understanding, saleiro2020dealing}
% (Figure~\ref{fig:2.fair1}), 
% is discussed in depth in . 
is a tool for differentiating fairness scenarios and suggesting the adoption of primary fairness metrics for measuring bias and fairness. These metrics, mainly derived from the confusion matrix used in binary classification models~\cite{evaluation2008},
 % (Table \ref{tab:confusion-matrix}) 
play a crucial role not only in classification tasks but are also fundamental in the domain of rankings.
Consistently with the standard usage, we shall indicate as TP (\emph{True Positive}) the number of positive instances correctly classified as positive by the model, while FP (\emph{False Positive}) indicates those incorrectly classified as positive; symmetrically, we also have TN (\emph{True Negative}) and FN (\emph{False Negative}). 
%With this, it is customary to compute several ratios to reflect the fairness of a model's predictions: with reference to Table~\ref{tab:fairness-metrics},
Additionally, we define a group $g$ as a set of entities (data points) that have a specific attribute value in common. For instance, the group containing all the females in the dataset is characterized by the value `female' for the attribute `gender'. Fairness metrics are defined at the group level and summarized in Table~\ref{tab:fairness-metrics}.
Each such metric is satisfied if its values are equal across different groups.

$FPR$ and $FNR$ indicate the fraction of individuals with actual negative (resp., positive) outcomes incorrectly classified as positive (resp., negative) by the model, while $FDR$ and $FOR$ represent the proportion of individuals the model predicts as positive (resp., negative) but with a negative (resp., positive) label. Precision refers to the proportion of correctly predicted outcomes out of all positive predictions, while recall is the proportion of correctly predicted positive outcomes out of all positive individuals. $GF$ indicates the fraction of individuals with positive predictions over the total size of the group (i.e., $|g|$). $OAE$ indicates the fraction of individuals with correct predictions over the total size of the group. TE is the ratio between wrong predictions.
$EOD$ consists of two components (indicated as $EOD_1$ and $EOD_2$) since it requires satisfaction of two metrics from the list, namely $Rec$ and $FPR$. Similarly, $CUA$ is also twofold and requires satisfying both $Pre$ and $FOR$ (while $CUA_2$ is not written in the same way as $FOR$, satisfying one or the other is equivalent).
% and CUA combine in and logical two metrics from the list. EOD combines 1-Recall and FPR and CUA combines Precision and 1-FOR


%   \begin{equation}
%     R = \frac{TP}{TP + FN}
%     \label{eq:R}
%   \end{equation}
% \end{itemize}
% \begin{table}[htbp]
%   \centering
%   \begin{tabular}{l|c|c|}
%     \multicolumn{1}{c}{} & \multicolumn{1}{c}{Predicted Positive} & \multicolumn{1}{c}{Predicted Negative} \\
%     \cline{2-3}
%     Actual Positive & True Positive (TP) & False Negative (FN) \\
%     \cline{2-3}
%     Actual Negative & False Positive (FP) & True Negative (TN) \\
%     \cline{2-3}
%   \end{tabular}
% \caption{Confusion Matrix}
% \label{tab:confusion-matrix} 
% \end{table}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\textwidth]{Images/2-ETHRANK/2.fair1.jpg}
%     \caption{Fairness Tree \cite{saleiro2020dealing}. 'GS' stands for Group Size.}
%     \label{fig:2.fair1}
% \end{figure}

\begin{table}
\centering
  \begin{tabular}{|r|c|}
  \hline
\emph{False Positive Rate} &
$FPR = \frac{FP}{FP + TN}$\\
\hline
\emph{False Negative Rate} &
$FNR = \frac{FN}{FN + TP}$\\
\hline
\emph{False Discovery Rate} &
$FDR = \frac{FP}{FP + TP}$\\
\hline
\emph{False Omission Rate} &
$FOR = \frac{FN}{FN + TN}$\\
\hline
\emph{Precision} &
$Pre = \frac{TP}{TP + FP}$\\
\hline
\emph{Recall} &
$Rec = \frac{TP}{TP + FN}$\\
\hline
\emph{Group Fairness} &
$GF = \frac{TP+FP}{|g|}$\\
\hline
\emph{Overall Accuracy Equality} &
$OAE = \frac{TP+TN}{|g|}$\\
\hline
\emph{Treatment Equality} &
$TE = \frac{FN}{FP}$\\
\hline
\emph{Equalized Odds} &
$EOD_1 = \frac{TP}{TP+FN};\quad EOD_2 = \frac{FP}{FP+TN}$\\
\hline
\emph{Conditional Use Accuracy Equality} &
$CUA_1 = \frac{TP}{TP+FP};\quad CUA_2 = \frac{TN}{TN+FN}$\\
\hline
\end{tabular}
\caption{Primary fairness metrics.}
\label{tab:fairness-metrics} 
\end{table}


The Fairness Tree~\cite{ saleiro2020dealing} is a common tool to determine the most appropriate metrics based on the nature of the intervention. If the intervention is punitive, prioritizing metrics that focus on false positives may be more critical, while for assistive interventions, metrics focusing on false negatives might be prioritized.

% Some metrics may be less useful in situations of significantly constrained resources, requiring a different approach to fairness measurement.



\subsection{Diversity}
\label{sec:diversity}

Diversity in ranking~\cite{DBLP:journals/sigmod/DrosouP10,DBLP:conf/sigmod/FraternaliMT12,DBLP:journals/tods/CatalloCFMT13} emphasizes broad representation and has become increasingly significant in computer science, especially in machine learning, algorithm design, information retrieval, search, and recommendation systems. Ethical ranking, in particular, relies heavily on diversity to achieve inclusivity, representation, and user satisfaction~\cite{stoyanovich2018online}.
% 
Diversity aims to
% refers to the representation of different categories within a selection. It arises from the 
meet the need to ensure that all potential user intents or attribute categories are equitably represented in algorithmic processes \cite{wang2021user}. In fields such as information retrieval and recommendation systems, diversity helps mitigate ambiguity, diversify content, and boost user engagement \cite{pitoura2022fairness}.


\subsubsection{Measures}
\label{subsec:diversity_approaches}

Diversity is typically measured according to two categories of measures: \emph{distance-based} and \emph{coverage-based}~\cite{pitoura2022fairness}. The former maximize the average or minimum pairwise distance between entities, while the latter ensure representation from all categories. While the number of items selected from each category depends on the total number of items and categories, a minimum number of items from each category can also be set to avoid \emph{tokenism} (selecting a unique representative from each category).

We observe that, while diversity and fairness may seem interconnected, diversity focuses on representativeness and inclusivity, while fairness aims at non-discrimination and the absence of bias~\cite{pitoura2022fairness}. For instance, a diverse output does not guarantee a fair one, and vice versa.

Diversity also has implications for overall utility in ethical ranking: \cite{wang2021user} shows how maximizing diversity inherently maximizes a lower bound on the overall utility.

Despite its importance, the implementation of diversity in algorithmic processes is not without challenges. Issues like homophily (the inclination of individuals to form connections or interact with others similar to them) in social networks, over-personalization in services, and individual tendencies like confirmation bias threaten diversity. These phenomena can lead to intellectual isolation states, belief reinforcement situations (`echo chambers'), and the creation of polarized parties, undermining the representation and inclusivity that diversity aims to provide \cite{pitoura2022fairness}.


\subsection{Stability} 
\label{sec:stability}

% Stability of a ranking system entails robustness with respect to minor variations in the input or algorithm parameters.
% In algorithmic ranking systems it pertains to the \textbf{consistency} of a specific output or ranking order, even when there are minor variations in the input or the parameters of the algorithm.
A ranking system is considered stable if slight adjustments to the input or algorithm parameters or methodology do not result in significant changes in the output~\cite{asudeh2018obtaining}.

Stability is commonly visualized as a hyper-sphere within the weight vector space of the scoring function. This hyper-sphere, centered around a specific weight vector, encompasses the region where modifications have minimal impact on the computed ranking up to a predetermined extent. The radius of this hyper-sphere serves as a quantifiable measure of stability, indicating the degree of permissible alterations to the input that do not cause any significant shift in the output~\cite{soliman2011ranking}.

Furthermore, stability enhances algorithmic \emph{transparency} by demonstrating that rankings are not arbitrary but can withstand minor perturbations. Rankers rely on stability to justify their ranking methodologies, instilling trust among consumers \cite{yang2018nutritional}.
