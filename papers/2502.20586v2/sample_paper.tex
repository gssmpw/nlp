\documentclass[twoside]{article}
%\usepackage{aistats2025}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{hyperref}
%\usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{placeins}
\usepackage{verbatim}
\usepackage{lipsum}
\usepackage[round]{natbib}
\usepackage{amsthm}
\usepackage{xurl}
\usepackage{thmtools,thm-restate}
%\PassOptionsToPackage{table}{xcolor}
%\numberwithin{figure}{section}
%\numberwithin{table}{section}
%\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\newcommand{\albert}[1]{{\color{red}\bf [Albert: #1]}}
\newcommand{\tao}[1]{{\color{orange}\bf [Tao: #1]}}
\newcommand{\youngsuk}[1]{{\color{blue}[YP: #1]}}


% \newcommand{\albert}[1]{{}}
% \newcommand{\tao}[1]{{}}
% \newcommand{\youngsuk}{{}}


\begin{document}
\runningauthor{Albert Tseng, Tao Yu, Youngsuk Park}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Training LLMs with MXFP4}

\aistatsauthor{ Albert Tseng\textsuperscript\textdagger \And Tao Yu \And  Youngsuk Park }

\aistatsaddress{ Cornell University \\ \href{mailto:albert@cs.cornell.edu}{\texttt{albert@cs.cornell.edu}} \And  AWS AI \\ \href{mailto:taou@amazon.com}{\texttt{taou@amazon.com}} \And AWS AI \\ \href{mailto:pyoungsu@amazon.com}{\texttt{pyoungsu@amazon.com}} } ]

\begin{abstract}
Low precision (LP) datatypes such as MXFP4 can accelerate matrix multiplications (GEMMs) and reduce training costs. 
However, directly using MXFP4 instead of BF16 during training significantly degrades model quality. 
In this work, we present the first near-lossless training recipe that uses MXFP4 GEMMs, which are $2\times$ faster than FP8 on supported hardware.
Our key insight is to compute unbiased gradient estimates with stochastic rounding (SR), resulting in more accurate model updates.
However, directly applying SR to MXFP4 can result in high variance from block-level outliers, harming convergence.
To overcome this, we use the random Hadamard tranform to theoretically bound the variance of SR.
We train GPT models up to 6.7B parameters and find that our method induces minimal degradation over mixed-precision BF16 training.
Our recipe computes $>1/2$ the training FLOPs in MXFP4, enabling an estimated speedup of $>1.3\times$ over FP8 and $>1.7\times$ over BF16 during backpropagation.
\end{abstract}

\input{sections/introduction.tex}
\input{sections/background.tex}
\input{sections/mxfp4training.tex}
\input{sections/experiments.tex}
\input{sections/conclusion.tex}
\pagebreak


% \bibliographystyle{abbrvnat}
\bibliographystyle{plainnat}
\bibliography{sample_paper.bib}


% References follow the acknowledgements.  Use an unnumbered third level
% heading for the references section.  Please use the same font
% size for references as for the body of the paper---remember that
% references do not count against your page length total.



% \begin{thebibliography}{}
% \setlength{\itemindent}{-\leftmargin}
% \makeatletter\renewcommand{\@biblabel}[1]{}\makeatother
% \bibitem{} J.~Alspector, B.~Gupta, and R.~B.~Allen (1989).
%     \newblock Performance of a stochastic learning microchip.
%     \newblock In D. S. Touretzky (ed.),
%     \textit{Advances in Neural Information Processing Systems 1}, 748--760.
%     San Mateo, Calif.: Morgan Kaufmann.

% \bibitem{} F.~Rosenblatt (1962).
%     \newblock \textit{Principles of Neurodynamics.}
%     \newblock Washington, D.C.: Spartan Books.

% \bibitem{} G.~Tesauro (1989).
%     \newblock Neurogammon wins computer Olympiad.
%     \newblock \textit{Neural Computation} \textbf{1}(3):321--323.
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section*{Checklist}

 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [\textbf{Yes}]
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [\textbf{Yes}]
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [\textbf{No}]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [\textbf{Yes}]
   \item Complete proofs of all theoretical results. [\textbf{Yes}]
   \item Clear explanations of any assumptions. [\textbf{Yes}]     
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [\textbf{Yes}]
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [\textbf{Yes}]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [\textbf{Not Applicable}]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [\textbf{Yes}]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [\textbf{Yes}]
   \item The license information of the assets, if applicable. [\textbf{Not Applicable}]
   \item New assets either in the supplemental material or as a URL, if applicable. [\textbf{Not Applicable}]
   \item Information about consent from data providers/curators. [\textbf{Not Applicable}]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [\textbf{Not Applicable}]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [\textbf{Not Applicable}]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [\textbf{Not Applicable}]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [\textbf{Not Applicable}]
 \end{enumerate}

 \end{enumerate}
\end{comment}
\clearpage

\input{sections/appendix.tex}


\end{document}
