\section{Experiments}
\label{sec:experiments}

Our main experiments focus on pretraining GPT 345M, 1.3B, and 6.7B \citep{gpt}.
We follow prior low precision training works and train for at least 20 billion tokens, which is sufficient to determine overall training performance on a longer full-scale run \citep{msamp}.
We use the Megatron-LM codebase to train our models \citep{megatronpaper}, the publicly available GPT2 Wikipedia dataset \citep{gpt2wiki}, and the bit-accurate Microsoft \texttt{microxcaling} library for MX emulation \citep{microxcaling}.
Since pretraining is expensive, we stopped certain experiments short when it was clear they did not match BF16.
Our analysis below uses validation perplexity from a holdout set, but we observe the same behavior with training perplexity. 
Training perplexity plots and additional experiments can be found in the Appendix.
%Finally, although the results here show MXFP4 only in the backward pass, we also tried using MXFP4 in the forward pass as well.
%We were unable to match BF16 with MXFP4 in the forward pass (see Appendix) and leave that for future work.

\subsection{GPT Pretraining Results}

\begin{table}[t]
\caption{Final losses for GPT models trained on the GPT2 Wikipedia corpus. All models were trained with BF16 mixed precision for the forward pass.}
\label{tab:bf16fw}
\centering
\renewcommand{\tabcolsep}{5pt}
\begin{tabular}{@{}ccccc@{}}
\toprule
Params. & Toks. & Bwd. Prec.   & \begin{tabular}[c]{@{}c@{}}Train.\\ Loss\end{tabular} & \begin{tabular}[c]{@{}c@{}}Val.\\ Loss\end{tabular} \\ \midrule
\rowcolor[HTML]{EFEFEF} 
345M    & 33B   & BF16         & 2.58                                                  & 2.49                                                \\
\rowcolor[HTML]{EFEFEF} 
345M    & 33B   & MXFP4        & 2.73                                                  & 2.60                                                \\
\rowcolor[HTML]{EFEFEF} 
345M    & 33B   & MXFP4+RHT    & 2.60                                                  & 2.51                                                \\
\rowcolor[HTML]{EFEFEF} 
345M    & 33B   & MXFP4+RHT+SR & 2.60                                                  & 2.51                                                \\
1.3B    & 42B   & BF16         & 2.28                                                  & 2.32                                                \\
1.3B    & 42B   & MXFP4        & 2.44                                                  & 2.40                                                \\
1.3B    & 42B   & MXFP4+RHT    & 2.30                                                  & 2.33                                                \\
1.3B    & 42B   & MXFP4+RHT+SR & 2.29                                                  & 2.32                                                \\
1.3B    & 42B   & MXFP4+SR     & 2.29                                                  & 2.32                                                \\
\rowcolor[HTML]{EFEFEF} 
1.3B    & 210B  & BF16         & 2.06                                                  & 2.29                                                \\
\rowcolor[HTML]{EFEFEF} 
1.3B    & 210B  & MXFP4+RHT    & 2.09                                                  & 2.31                                                \\
\rowcolor[HTML]{EFEFEF} 
1.3B    & 210B  & MXFP4+RHT+SR & 2.07                                                  & 2.29                                                \\
\rowcolor[HTML]{EFEFEF} 
1.3B    & 210B  & MXFP4+SR     & 2.08                                                  & 2.29                                                \\
6.7B    & 21B   & BF16         & 2.04                                                  & 2.27                                                \\
6.7B    & 21B   & MXFP4+RHT    & 2.05                                                  & 2.28                                                \\
6.7B    & 21B   & MXFP4+RHT+SR & 2.08                                                  & 2.27                                                \\ \bottomrule
\end{tabular}
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% Params. & Toks. & Bwd. Prec.   & \begin{tabular}[c]{@{}c@{}}Train.\\ Loss\end{tabular} & \begin{tabular}[c]{@{}c@{}}Val.\\ Loss\end{tabular} \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% 345M    & 33B   & BF16         & 2.58                                                  & 2.49                                                \\
% \rowcolor[HTML]{EFEFEF} 
% 345M    & 33B   & MXFP4    & \textcolor{red}{2.73}                                                  & \textcolor{red}{2.60}                                                \\
% \rowcolor[HTML]{EFEFEF} 
% 345M    & 33B   & MXFP4+RHT    & \textcolor{Green}{2.60}                                                  & \textcolor{Green}{2.51}                                                \\
% \rowcolor[HTML]{EFEFEF} 
% 345M    & 33B   & MXFP4+RHT+SR & \textcolor{Green}{2.60}                                                  & \textcolor{Green}{2.51}                                                \\
% %\rowcolor[HTML]{EFEFEF} 
% %345M    & 33B   & MXFP4+SR &                                                 &                                                 \\
% 1.3B    & 42B   & BF16         & 2.28                                                  & 2.32                                                \\
% 1.3B    & 42B   & MXFP4         & \textcolor{red}{2.44}                                                  & \textcolor{red}{2.40}                                                \\
% 1.3B    & 42B   & MXFP4+RHT    & \textcolor{Green}{2.30}                                                  & \textcolor{Green}{2.33}                                                \\
% 1.3B    & 42B   & MXFP4+RHT+SR & \textcolor{Green}{2.29}                                                  & \textcolor{Green}{2.32}                                                \\
% 1.3B    & 42B   & MXFP4+SR &  \textcolor{Green}{2.29}                                                &  \textcolor{Green}{2.32}                                              \\
% \rowcolor[HTML]{EFEFEF} 
% 6.7B    & 21B   & BF16         & 2.04                                                  & 2.27                                                \\
% \rowcolor[HTML]{EFEFEF} 
% 6.7B    & 21B   & MXFP4+RHT    & \textcolor{Green}{2.05}                                                  & \textcolor{Green}{2.28}                                                \\
% \rowcolor[HTML]{EFEFEF} 
% 6.7B    & 21B   & MXFP4+RHT+SR & \textcolor{Green}{2.08}                                                  & \textcolor{Green}{2.27}                                                \\ \bottomrule
% \end{tabular}
\end{table}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.32\linewidth]{figs/345m.pdf}
% \includegraphics[width=0.32\linewidth]{figs/1b3.pdf}
% \includegraphics[width=0.32\linewidth]{figs/6b7.pdf}
% \caption{GPT validation perplexity curves (with BF16 forward pass) for (L) 345M, (C) 1.3B and (R) 6.7B. With the RHT and SR, our MXFP4 approach can match the performance of BF16 in the backward pass.}
% \label{fig:valppl}
% \end{figure*}

\begin{figure}[t]
\includegraphics[width=\linewidth]{figs/345m.pdf}
\caption{GPT 345M validation perplexity curves with BF16 forward pass. With RHT and SR, MXFP4 can match the performance of BF16 in the backward pass.}
\label{fig:345mbf16}
\end{figure}

\begin{figure}[t]
\includegraphics[width=\linewidth]{figs/1b3.pdf}
\caption{GPT 1.3B validation perplexity curves with BF16 forward pass. With RHT and SR, MXFP4 can match the performance of BF16 in the backward pass.}
\label{fig:1b3bf16}
\end{figure}

% \begin{figure}[ht]
% \includegraphics[width=\linewidth]{figs/1b3_init.pdf}
% \caption{Initial convergence for GPT 1.3B. Stochastic rounding's variance makes it converge slower than BF16 and the RHT variants.}
% \label{fig:1b3init}
% \end{figure}

\begin{figure}[t]
\includegraphics[width=\linewidth]{figs/6b7.pdf}
\caption{GPT 6.7B validation perplexity curves with BF16 forward pass. With RHT and SR, MXFP4 can match the performance of BF16 in the backward pass. The MXFP4-only run was stopped early to save resources.}
\label{fig:6b7bf16}
\end{figure}

\begin{figure*}[t]
\includegraphics[width=\linewidth]{figs/gpt_1b3_210b_val.pdf}
\caption{Validation perplexity for training GPT 1.3B for 210 billion tokens, or $5\times$ longer than in Table \ref{tab:bf16fw}. All experiments used BF16 in the forward pass and the specified backward precision.}
\label{fig:200b}
\end{figure*}


\begin{table}[t]
\centering
\renewcommand{\tabcolsep}{3pt}
\begin{tabular}{@{}cccccc@{}}
\toprule
Model          & ArcC & ArcE & PiQA & BoolQ & Wino \\ \midrule
BF16  & 23.1 & 49.2 & 60.5 & 53.3  & 52.0       \\
MXFP4$\star$ & 22.2 & 47.8 & 61.3 & 59.6  & 49.6       \\ \midrule
BF16 \small{\textsc{Tulu V2}} & 25.6 & 50.6 & 62.7 & 59.6 & 51.6 \\
MXFP4$\star$ \small{\textsc{Tulu V2}} & 25.9 & 49.9 & 62.9 & 60.5 & 51.8 \\ \bottomrule
\end{tabular}
\caption{GPT 6.7B model trained on 20B tokens before and after Tulu V2 fine-tuning. Both BF16 and our MXFP4+RHT+SR (MXFP4$\star$) model exhibit similar performance before and after fine-tuning. 
%\tao{don't we have the downstream evals for 200B tokens 1.3B model? this MXFP4* feels WORSE compared to bf16 before finetuning}
%\albert{no, because one of the 200b checkpoints got deleted for some reason. mxfp4 is better in 2 evals and slightly worse on the other three. these are the same numbers from the rebuttal.}
}
\label{tab:finetune}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{@{}cccccc@{}}
\toprule
BW Pass  & BF16  & g=32  & g=64  & g=128 & g=256 \\ \midrule
Val. PPL & 11.89 & 12.02 & 12.01 & 11.98 & 11.98 \\ \bottomrule
\end{tabular}
\caption{Validation perplexity for training GPT 345M on 33B tokens with various RHT blocksizes. Increasing the RHT block size improves performance by reducing the variance of stochastic rounding.}
\label{tab:rhtbs}
\end{table}

\begin{table*}[t]
\centering
\begin{tabular}{@{}ccccccccc@{}}
\toprule
BW Pass   & FP16  & \begin{tabular}[c]{@{}c@{}}INT8 \\ \small{\textsc{no RHT}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}INT4 \\ \small{\textsc{no RHT}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}+ RHT\\ \small{\textsc{g=64}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}+ RHT\\ \small{\textsc{g=128}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}+ RHT\\ \small{\textsc{g=256}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}+ RHT\\ \small{\textsc{g=1024 dense}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}+ RHT\\ \small{\textsc{g=1024} $\mathcal{O}(n \log n)$}\end{tabular} \\ \midrule
E2E tok/s & 46983 & 55469                                                                   & 67306                                                                   & 64335                                                                 & 64171                                                                  & 63979                                                                  & 61186                                                                         & 62640                                                                                           \\
BW tok/s  & 72563 & 94688                                                                   & 133952                                                                  & 123056                                                                & 122734                                                                 & 121823                                                                 & 112299                                                                        & 120495                                                                                          \\ \bottomrule
\end{tabular}
\caption{Throughput for a FP16 forward pass and specified backward pass of a Llama 2 70B decoder layer. Measured on a NVIDIA A100; see Section \ref{sec:overhead} for more details. Since the A100 can perform INT4 GEMMs $4\times$ faster than FP16 GEMMs, 
%\youngsuk{FP16 or BF16? FP16, its llama 2b}
these numbers represent the expected speedup of MXFP4 on supported hardware.}% \tao{this is all measured with TP=1, right?}\albert{yes, 1 gpu}}
\label{tab:e2e}
\end{table*}


Table \ref{tab:bf16fw} and Figure \ref{fig:200b} show our main results with using BF16 in the forward pass and various MXFP4 constructions in the backward pass.
We ablate on using the RHT only, which produces a biased but reduced-distortion GEMM, and RHT and SR, which produces an unbiased, lower varianace GEMM.
For GPT 1.3B, we also measure the performance of MXFP4+SR only, which gives an unbiased but higher variance GEMM.
All experiments use Megatron-LM's mixed precision implementation with separate FP32 master weights and BF16 parameter copies.
For the backward pass for decoder linear layers, the BF16 and gradients are quantized to MXFP4.
Experiments with the RHT use $g=64$, which mixes across 2 MX blocks. 

Table \ref{tab:bf16fw} shows that for shorter runs (20-40 billion tokens), using either the RHT or SR with MXFP4 is sufficient to achieve near-lossless training all tested model sizes.
However, Figure \ref{fig:200b} shows that for longer runs (210 billion tokens), having an unbiased gradient estimator is necessary to maintain performance.
Whereas using the RHT only results in an $\approx 0.1$ perplexity gap, using stochastic rounding (with or without the RHT) results in \textit{no validation perplexity gap}.


Figures \ref{fig:345mbf16}, \ref{fig:1b3bf16} and \ref{fig:6b7bf16} show the validation perplexity curves for the experiments in Table \ref{tab:bf16fw}.
At all scales, the MXFP4+RHT+SR curve closely tracks BF16. 
In contrast, although the final performance of MXFP4+SR matches MXFP4+SR+RHT, MXFP4+SR exhibits slower initial convergence than BF16 and MXFP4+RHT+SR. %\tao{this is indeed to observe from the figure?}\albert{yes, the purple line is initially worse}.
We suspect that this is due to loss of gradient information without the RHT. 
Although using only SR will give an unbiased gradient estimator, small values will still get stochastically flushed to 0 (Equation \ref{eqn:unifdither}), resulting in loss of gradient information.
In contrast, the RHT transforms the gradient to a different space.
This reduces variance and also significantly reduces the probability that a single gradient entry in the original space will be set to 0.
To verify this, Table \ref{tab:rhtbs} shows an ablation on the RHT block size -- increasing the block size improves quality.
%\albert{add black flushing experiment}

These figures also include curves for using pure MXFP4 (no RHT and no SR) MP in the backward pass.
Using only MXFP4 (the orange curve) results in significant degradation and a large perplexity gap at all sizes.
Even further, if we consider that FP8 is near-lossless \citep{msamp} vs. BF16 and is only an estimated 30-40\% slower end-to-end than pure MXFP4, then pure MXFP4 isn't even ``worth it.''
For a fixed amount of wall clock time, simply training with FP8 for fewer steps would give a better model than using pure MXFP4.
In contrast, our techniques close the gap to BF16 and FP8, making MXFP4 practical for training.
Our techniques are also compatible with FP8 forward passes (Figure \ref{fig:fp8fw}, more details in Appendix), further pushing the speed-quality tradeoff curve.

To further evaluate our MXFP4 models, we ran zeroshot evaluation for downstream tasks on our 20B token GPT 6.7B models. 
Both the BF16 and MXFP4+RHT+SR models perform around the same.
To test how well these models can be fine-tuned, we fine-tuned them using the publicly-available Tulu V2 dataset (657M tokens) and codebase \citep{tulu2}. 
We used the hyperparameters in the Tulu V2 codebase and trained for 5 epochs with BF16/FP32 mixed precision. 
The BF16 model reached a final training perplexity of 1.96, and the MXFP4+RHT+SR model 1.98.
Like before finetuning, both models achieve similar zeroshot performance, indicating that they are of similar quality.
Table \ref{tab:finetune} summarizes these results.

\begin{figure*}[t]
\centering
\includegraphics[width=0.4\linewidth]{figs/1b3_fp8.pdf}
\hspace{1cm}
\includegraphics[width=0.4\linewidth]{figs/6b7_fp8.pdf}
\caption{GPT 1.3B and 6.7B perplexity curves with a FP8 forward pass, our MXFP4 backward pass, and the same settings as Figures \ref{fig:1b3bf16} and \ref{fig:6b7bf16}. Our method is compatible with FP8 forward passes for additional speedups. See Appendix for details.
% \tao{this figure margin should be adjusted for better layout}\albert{I lost the code to regenerate the plots and don't want to remake the plots}
}
\label{fig:fp8fw}
\end{figure*}

% This suggests that having an unbiased gradient estimator can help mitigate the quantization distortion inherent to low precision training.
%  shows the training validation perplexity curves for training GPT-1.3B for 200 billion tokens (5$\times$ longer than Figure \ref{fig:1b3bf16} (C)) with BF16 MP in the forward pass.
% Here, we find that stochastic rounding and unbiased gradients are necessary to truly match BF16.
% While MXFP4+RHT is able to match BF16 initially, it starts to deviate near the end with a 0.1 perplexity gap at 151B tokens.
% This is most likely due to having a biased gradient that affects convergence.
% In contrast, using both the RHT and SR to obtain an unbiased gradient results in \textit{no perplexity gap} at even 151 billion tokens.
% There is an approximately 0.1 validation perplexity gap between MXFP4+RHT only (10.02 ppl) and BF16 (9.92 ppl), whereas MXFP4+RHT+SR matches BF16 (9.90 ppl). 
% This suggests that stochastic rounding is important for near-lossless full-scale FP4 training.



% Furthermore, whereas our RHT construction is fast on a wide class of accelerators, SR requires hardware support to be efficient.
% This indicates that there is value in having hardware support for SR, which is necessary to achieve an unbiased gradient estimate with MXFP4.





% These figures also 
% One benefit of the RHT is preventing non-outlier group entries from being flushed to 0 when there is an outlier in the group.
% However, stochastic rounding by itself is also able reduce this underflow effect.
% For small values $\alpha$ that would be flushed to zero with nearest rounding SR will alternate between choosing 0 and $sign(\alpha)0.5$ (the smallest FP4 subnormal) with probability $1-2\alpha$ and $2\alpha$, respectively.
% Figure \label{fig:1b3bf16} shows that using only SR with MXFP4 also achieves strong results. 
% However, Figure \label{fig:1b3init} shows that the initial convergence for only using SR is slower than both RHT experiments and BF16.
% This aligns with 






%something something robust across hardware. 

\subsection{Overhead Calculations}
\label{sec:overhead}

%\albert{cite msamp and te and nv benchmarks and make clear this is a gemm cost estimate}
The goal of MXFP4 training is to achieve a wall-clock time speedup over FP8 training. 
Unfortunately, we do not have access to FP4 hardware yet so we cannot measure empirical wall-clock speedups over FP8.
However, we can estimate the overhead of the RHT and stochastic rounding with proxy benchmarks.

Our RHT construction operates on a small ``tile'' in the operand and is memory bound, so we can conceivably fuse it with the MXFP4 GEMM and avoid writing its output to memory.
We can estimate the performance of this setup in two ways.
First, we measured the overhead of RHT-GEMM kernels for \textit{FP8}.
% , which we do have hardware for.
Specifically, we timed $A\texttt{.to(E4M3)}B\texttt{.to(E4M3)}^T$, $A \in \texttt{BF16}^{n \times k}, B \in \texttt{BF16}^{m \times k}$ with and without the RHT along the $k$ dimension.
We generated Triton \citep{triton} kernels with \texttt{torch.compile} \citep{torch}, an RHT size of $g=64$, and benchmarked 7B and 70B-sized matrices: $(m,n,k)=(32768, 8192, 8192)$ and $(16384, 28672, 28672)$.
On a NVIDIA H100 GPU, the RHT adds 9.7\% overhead for the 7B-sized setup and 1.6\% for the 70B-sized setup.
Assuming MXFP4 has twice the throughput of FP8, these numbers would double to $19.4\%$ and $3.2\%$, respectively, which is still faster than a FP8 GEMM.

Second, we measured the overhead of the RHT on the HuggingFace implementation \citep{hf} of single Llama 2 70B decoder layer decoder layer on a NVIDIA A100 GPU.
Specifically, we report the end-to-end tokens per second for computing the forward pass in FP16 and backward pass in either FP16 or INT4, which has the same hardware speedup ($4\times$) on the NVIDIA A100 vs. FP16 as MXFP4 has on modern hardware.
We also include INT8 as a proxy for the expected speedup of a FP8 backward pass.
We use a batch size of 4 sequences with 4K tokens each (16K tokens/batch), Flash Attention 2 \citep{fa2}, \texttt{torch.compile}, and the CUTLASS INT4 and INT8 GEMM kernels. 
We were unable to use CUDA graphs since the HuggingFace implementation is not compatible with CUDA graphs; we expect CUDA graphs to improve speedup ratios by masking kernel launch overhead.

Table \ref{tab:e2e} summarizes these results.
End to end with a FP16 FW pass, an INT4+RHT backward pass is over $40\%$ faster than a FP16 backward pass and over $20\%$ faster than an INT8 backward pass.
If we only consider the backward pass, INT4+RHT is $\approx 70\%$ faster than a FP16 backward pass and $\approx 30\%$ faster than a INT8 backward pass.
The HuggingFace Llama implementation is not known to be fast, so a more efficient implementation would achieve better INT4 and INT8 speedups over FP16.
Table \ref{tab:e2e} also shows that the RHT adds less than $5\%$ E2E overhead and is memory bound in the operands until $g \approx 256$. 
Interestingly, the recently released $\mathcal{O}(n \log n)$ HadaCore kernel \citep{hadacore} recovers most of the dense GEMM penalty at $g=1024$, but is still slower than smaller $g$.
%Finally, compared to an INT8 backward pass, an INT4 + RHT backward pass is still $30\%$ faster. \tao{the last sentence is redundant, isn't it, we just mentioned 30\% early in the paragraph.}


To measure the overhead of stochastic rounding, we used an Amazon Trainium 1 chip (EC2 \textit{Trn1} instance), which is one of the few widely available chips that has dedicated stochastic rounding hardware \citep{trainium}.
Our experiments show that for most matrix sizes, using SR to quantize GEMM operands from FP32 to BF16 adds less than 2\% overhead over the BF16 GEMM itself.
Assuming a 4$\times$ increase in GEMM throughput when going from BF16 to FP4, this would mean SR adds less than 10\% overhead.








