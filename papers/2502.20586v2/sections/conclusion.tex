
\section{Conclusion}

While hardware support for low precision datatypes continues to advance, it is becoming increasingly difficult to train with these datatypes without suffering from significant model degradation.
In this work, we demonstrate the first MXFP4 training recipe that achieves near-lossless model quality vs. FP32/BF16 mixed precision training.
Our method hinges on computing low variance, unbiased gradient estimates for decoder linear layer, which enables us to make more accurate model updates.
%Although MXFP4 shares a scale across blocks of FP4 num#bers, this shared scale is not enough to compensate for FP4's distortion.
To do this, we propose using stochastic rounding (SR) and the random Hadamard transform (RHT). %to calculate the gradients of decoder linear layers in the backward pass.
Stochastic rounding produces unbiased gradient estimates, and the RHT reduces the variance of SR and the chance of losing gradient information from underflow.
Our experiments pretraining GPT models up to 6.7B show that both the RHT and SR are crucial for near-lossless MXFP4 training.
Finally, our benchmarks show that our method can be implemented with minimal overhead, giving an estimated 30\% speedup over FP8 and 70\% speedup over BF16 in the backward pass.

\section*{Acknowledgements}

We thank Chris De Sa for valuable feedback. We also thank Yida Wang and George Karypis for their support within AWS AI Research.
% \tao{@Youngsuk, do we need to ack Yida?}\youngsuk{done}
\vfill
\pagebreak