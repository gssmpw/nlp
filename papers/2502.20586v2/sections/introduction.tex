\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/mxfp4.pdf}
\caption{Our method uses stochastic rounding (SR) to compute unbiased gradients and the random Hadamard transform to bound the variance of SR. This enables us to perform more accurate model updates with MXFP4 in the backward pass, enabling a speedup of $>1.3\times$ over FP8 and $>1.7\times$ over BF16.}
\label{fig:overview}
\end{figure}

The latest large language models (LLMs) have billions of parameters that are trained on trillions of tokens, making them incredibly expensive to train. 
For example, training Llama 3.1 405B required $3\times 10^{24}$ floating point operations (FLOPs), or over 10000 GPUs for multiple months \citep{llama3}. 
Recent hardware accelerators have started supporting low precision floating point ($\le$ 16 bit) matrix mulitiplications (GEMMs).
Compared to 32-bit GEMMs, hardware-accelerated low precision (LP) GEMMs run at significantly higher throughputs.
For example, FP8 GEMMs can be $4\times$ faster than FP32 GEMMs and also more energy efficient \citep{blackwell}.

Since LLM training is compute bound in matrix multiplications, LP GEMMs can accelerate training.
Almost all modern LLMs are trained with 16 bit GEMMs \citep{llama2, llama3}, and some even use FP8 GEMMs \citep{msamp}.
Using 16 bit GEMMs halves the cost of matrix multiplications and improves end-to-end throughput by almost $2\times$ \citep{mpiclr}.
%Even further, FP8 is emerging as a viable training datatype that can achieve another $30-50\%$ end-to-end improvement over 16-bit training \citep{msamp}.
%\tao{not 50\% for end to end, see MS-AMP for example, 40\% might be more appropriate} \youngsuk{1) CITE, 2) from where? e.g., reduced communication} improvement in end-to-end throughput over 16-bit training.
However, there is no free lunch with low precision training.
Reducing the GEMM precision increases quantization distortion and can cause numerical instability.
%As such, most FP8 training recipes require fine-grained scaling \textit{within} a tensor to minimize degradation over 16 bit training.
%FP8 recipes also use different FP8 constructions during the forward and backward passes to minimize quantization distortion. \youngsuk{cite/mention papers like https://arxiv.org/html/2405.18710v1, pointing (naive) FP8 solution is unstable.}

%\let\thefootnote\relax\footnotetext{Correspondence to \href{mailto:albert@cs.cornell.edu}{albert@cs.cornell.edu} and \href{mailto:taou@amazon.com}{taou@amazon.com}}.

To counteract these issues, the recently introduced Microscaling (MX) family of datatypes uses a shared blockwise scale across multiple floating point numbers \citep{ocpmx}.
For example, MXFP4 uses an INT8 scale $s$ for every contiguous block $v$ of 32 FP4 numbers to represent $2^{s-1}v$, where 1 is the exponent bias for FP4.
This scale enables a significantly wider range of representable numbers at the cost of an extra $8/32 = 0.25$ bits per entry.
%In hardware, such as on NVIDIA Blackwell GPUs, MXFP4 GEMMS can be computed at almost twice the throughput as FP8 GEMMs \tao{references}, meaning that this built-in scale is essentially free in compute bound settings.
However, MX alone is not enough to enable lossless low precision training with FP4.
As we show in Section \ref{sec:experiments}, directly using MXFP4 in even only the backward pass of decoder linear layers significantly degrades model quality.

In this work, we introduce two techniques that enable near-lossless distributed training with MXFP4.
Our method hinges on computing low-variance, unbiased gradient estimates that enable more accurate model updates.
First, we use stochastic rounding to compute unbiased GEMMs.
Then, we use a memory-bound construction of random Hadamard transform to reduce the effect of outliers and theoretically bound the variance of SR, aiding convergence.
We apply our method to decoder linear layers and show that it incurs minimal degradation over BF16 mixed precision training when pretraining GPT models up to 6.7B parameters.
Our recipe computes over half the training FLOPs in MXFP4 and can significantly accelerate pretraining.
In summary, we:

%This enables us to perform accurate model updates in expectation, 
% Our method hinges on the observation that minibatched training methods compute unbiased gradient estimates, so further computing the gradient estimate with LP GEMMs should be fine as long as the final estimate is still unbiased with low variance. \tao{here better mention lightly the introduced techniques don't cause additional overheads, which are important to give some impression}
%Indeed, our method lets us compute the gradients of decoder linear layers during training with minimal final model degradation.
%Since decoder linear layer backpropagation consists of over half the training FLOPs, our MXFP4 recipe can significantly accelerate LLM training \citep{flopcount, megatronpaper}.

\begin{itemize}
\item Introduce a MXFP4 training recipe that uses stochastic rounding and the random Hadamard transform compute unbiased, low-variance gradient estimates during backpropagation.
\item Pretrain GPT models up to 6.7B and show that our training recipe closes the MXFP4-BF16 gap to $<0.1$ validation perplexity. %\tao{can we use a percentage gap?}\albert{afaik perplexity isn't measured in terms of perc gap since its on a log scale}
\item Show that our RHT and SR constructions add minimal overhead to MXFP4 GEMMs, giving a theoretical speedup of $>1.3\times$ and $>1.7\times$ over a FP8 and BF16 backward pass, respectively.% \tao{maybe combine the 1st and 3rd bullet as they are directly related}
\end{itemize}





