\thispagestyle{empty}
\onecolumn
\aistatstitle{Training LLMs with MXFP4: \\
Supplementary Materials}

\section{Additional Results}

\subsection{FP8 Forward Pass Results}

This section contains experiments using mixed-precision FP8 in the forward pass and MXFP4 in the backward pass.
Prior works have shown that mixed-precision FP8 forward and backward passes can be close to lossless over mixed-precision BF16 training \citep{msamp, nvte}.
To test if MXFP4 backward passes are still practical with FP8 forward passes, we trained GPT 1.3B and 6.7B models with NVIDIA's TransformerEngine (TE) FP8 (E4M3) implementation \citep{nvte} in the forward pass and our MXFP4 formulation in the backward pass. 
We did not test GPT 345M since TE FP8 already had a $>0.1$ validation perplexity gap vs. BF16 in our experiments.
For GPT 6.7B, since we did not have access to FP8-capable hardware with fast interconnects necessary for tensor-parallel training, we emulated FP8 matrix multiplications by dequantizing FP8 GEMM operands into BF16 and performing a BF16 GEMM.
While this is not bit-accurate vs. a FP8 GEMM, the relative error of the output is $\approx 0.3\%$ for random Gaussian inputs.
Furthermore, this is essentially how PyTorch emulates FP8 GEMMs \citep{torch}.
At both model scales, we find that FP8 forward passes and MXFP4 backward passes are sufficient to essentially match BF16 training.


\FloatBarrier
\begin{figure}[h!]
\centering
%\includegraphics[width=0.48\linewidth]{figs/1b3_fp8.pdf}
\includegraphics[width=0.48\linewidth]{figs/1b3_fp8_train.pdf}
\caption{Training perplexity curves for GPT 1.3B for 33 billion tokens. Using FP8 in the forward pass and MXFP4 in the backward pass does not result in noticeable degradation.}
\end{figure}

\begin{figure}[h!]
\centering
%\includegraphics[width=0.48\linewidth]{figs/6b7_fp8.pdf}
\includegraphics[width=0.48\linewidth]{figs/6b7_fp8_train.pdf}
\caption{Training perplexity curves for GPT 6.7B for the first 13 billion tokens of a 20 billion token run. Due to time constraints and the cost of training a 6.7B parameter model, we were unable to include the full run. Like 1.3B, using FP8 in the forward pass and MXFP4 in the backward pass does not result in noticeable degradation.}
\end{figure}
\FloatBarrier


\subsection{GPT 345M Validation Curves with Stochastic Rounding Only}

This plot is the same as those from the main body except that it includes an experiment with stochastic rounding only (no RHT). Like 1.3B, SR starts off ``worse'' than the RHT variants but is able to match their performance at the end of the training run. 
% \tao{but you didn't put the outlier parts at the beginning of training, actually the purple line is lower than the rest at the beginning from this figure} \albert{That's from the curve smoothing, there was some missing data from TB for the beginning of the plot}

% \FloatBarrier
% \begin{figure}[h!]
% \centering
% \includegraphics[width=\linewidth]{figs/345m_train.pdf}
% \caption{Training perplexity for training GPT 345M for 33 billion tokens. All experiments used BF16 in the forward pass and the specified backward precision. All curves were smoothed with a Savitzky-Golay filter to enhance readability.}
% \end{figure}

\begin{figure}[h!]
\includegraphics[width=\linewidth]{figs/345m_wsr.pdf}
\caption{Validation perplexity for training GPT 345M for 33 billion tokens. All experiments used BF16 in the forward pass and the specified backward precision. This plot is the same as Figure \ref{fig:345mbf16} in the main body except that it adds an experiment with MXFP4+SR only. }
\end{figure}

\FloatBarrier
\subsection{Training Curve for GPT 1.3B}
\FloatBarrier
\begin{figure}[h!]
\includegraphics[width=\linewidth]{figs/1b3_train.pdf}
\caption{Training perplexity for training GPT 1.3B for 40 billion tokens. All curves used BF16 in the forward pass and the specified backward precision.}
\end{figure}
\FloatBarrier

\subsection{Training Perplexity for GPT 1.3B on 200 Billion Tokens}

This section contains the full 210B token GPT 1.3B run referenced in Section \ref{sec:experiments} of the main body.
There is an approximately 0.1 validation perplexity gap between MXFP4+RHT only (10.02 ppl) and BF16 (9.92 ppl), whereas MXFP4+RHT+SR matches BF16 (9.90 ppl). 
This suggests that stochastic rounding is important for near-lossless full-scale FP4 training.

\begin{figure}[h!]
\includegraphics[width=\linewidth]{figs/1b3_210b.pdf}
\caption{Validation perplexity for training GPT 1.3B for 210 billion tokens. All experiments used BF16 in the forward pass and the specified backward precision.}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=\linewidth]{figs/1b3_210b_train.pdf}
\caption{Training perplexity for training GPT 1.3B for 210 billion tokens. All experiments used BF16 in the forward pass and the specified backward precision. The BF16 and MXFP4+RHT curves have lower variance due to a configuration difference with the logger.}
\end{figure}
\FloatBarrier

\subsection{Training Curve for GPT 6.7B}

\FloatBarrier
\begin{figure}[h!]
\includegraphics[width=\linewidth]{figs/6b7_train.pdf}
\caption{Training perplexity for training GPT 6.7B for 20 billion tokens. All experiments used BF16 in the forward pass and the specified backward precision.}
\end{figure}
\FloatBarrier




\section{Experimental Setup Details}

All experiments were run on AWS P4 and G6e EC2 instances.
Our code was based off of the Megatron-LM codebase at Github commit \texttt{a4ad305d4b117217141730b9b18af52dda069450} and the Microsoft microxcaling codebase at Github commit \texttt{7bc41952de394f5cc5e782baf132e7c7542eb4e4}.
We used the NVIDIA Pytorch + Ubuntu 24.04 docker image, which contains a version of Transformer Engine 1.5 for the FP8 experiments.
All models were trained with the AdamW optimizer, FlashAttention \citep{fa1}, and the following hyperparameters:


\begin{table}[h]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
Hyperparameter             & GPT 345M & GPT 1.3B & GPT 6.7B \\ \midrule
Decoder Layers             & 24       & 24       & 32       \\
Hidden Size                & 1024     & 2048     & 4096     \\
Attention Heads            & 16       & 16       & 32       \\
Context Length             & 1024     & 2048     & 2048     \\
Max. Positional Embeddings & 1024     & 2048     & 2048     \\
Batch Size                 & 64       & 1024     & 256      \\
Learning Rate (LR)         & 0.00015  & 0.0002   & 0.00012  \\
Training Iterations        & 500000   & 20000    & 40000    \\
LR Scheduler               & Cosine   & Cosine   & Cosine   \\
LR Decay Iterations        & 320000   & 20000    & 40000    \\
Minimum LR                 & 1e-5     & 2e-5     & 1.2e-5   \\
Weight Decay               & 1e-2     & 0.1      & 0.1      \\
LR Warmup Fraction         & 0.01     & 0.01     & 0.01     \\
Gradient Clipping          & 1.0      & 1.0      & 1.0      \\ \bottomrule
\end{tabular}
\end{table}

\FloatBarrier
\section{Proof of Lemma 3.1}

\srlemma*

\begin{proof}
First, we show that Algorithm \ref{alg:sr2mx} produces an unbiased MXFP4 estimate of $\frac{3}{4}$ the input vector $v$.
Let $v\in \mathbb{R}^g$, where $g$ is the MX group size.
The input to $\texttt{stochastic\_round\_to\_FP4}$ is given by $w = \frac{3}{4} v/X$, where $X = 2^{\lfloor\log_2(\mbox{argmax}(|v|))\rfloor - 2}$.
Let $m = \mbox{argmax}(|v|)$.
Observe that the largest magnitude element of $w$ is 
$$\frac{3}{4} \frac{m}{2^{\lfloor\log_2(m)\rfloor - 2}} < \frac{3}{4} \frac{m}{2^{\log_2(m) - 3}} = \frac{3}{4}\times 8 = 6$$
By definition, $\texttt{stochastic\_round\_to\_FP4}(x)$ produces an unbiased estimate of $x$ as long as $x$ is ``within range'' -- i.e. it does not overflow outside of the range of representable values in FP4.
Since the maximum normal in FP4 is 6, $\texttt{stochastic\_round\_to\_FP4}(w)$ will give an unbiased FP4 estimate of $\frac{3}{4}v/X$.
Finally, from linearity of expectation, $X * \texttt{stochastic\_round\_to\_FP4}(w)$ gives an unbiased estimate of $\frac{3}{4}v$, as desired.

Now, we show that Algorithm \ref{alg:rhtbw} produces unbiased estimates of $\frac{dL}{dx}$ and $\frac{dL}{dW}$.
Let $C = \texttt{MXFP4\_GEMM}(A, B^T)$, where $A \in \mathbb{R}^{b\times n}$ and $B \in \mathbb{R}^{m \times n}$, and $g | n$.
We have that 
\begin{align}
\mathbb{E}\left[C_{ij}\right] &= \mathbb{E}\left[\sum_{k = 0}^{n/g}\left(X_{A_{i, kg:(k+1)g}}X_{B_{j, kg:(k+1)g}} \sum_{l=0}^g \left(A^{FP4}_{i, kg:(k+1)g}\right)_l \left(B^{FP4}_{j, kg:(k+1)g}\right)_l \right)\right] \\
&= \sum_{k = 0}^{n/g}\left(X_{A_{i, kg:(k+1)g}}X_{B_{j, kg:(k+1)g}} \sum_{l=0}^g \mathbb{E}\left[\left(A^{FP4}_{i, kg:(k+1)g}\right)_l \left(B^{FP4}_{j, kg:(k+1)g}\right)_l \right] \right) 
\end{align}

Where $A_{i, kg:(k+1)g}$ denotes the $k$-th size $g$ vector of the $i$-th row of $A$, $X_{A_{i, kg:(k+1)g}}$ is the scale of applying Algorithm \ref{alg:sr2mx} to $A_{i, kg:(k+1)g}$, and $A^{FP4}_{i, kg:(k+1)g}$ is the FP4 component of applying Algorithm \ref{alg:sr2mx} to $A_{i, kg:(k+1)g}$.
Since stochastic rounding is implemented with independent noise, $A^{FP4}_{i, kg:(k+1)g}$ and $B^{FP4}_{j, kg:(k+1)g}$ are independent random variables.
Thus, 
\begin{align}
\mathbb{E}\left[C_{ij}\right] &= \sum_{k = 0}^{n/g}\left(X_{A_{i, kg:(k+1)g}}X_{B_{j, kg:(k+1)g}} \sum_{l=0}^g \mathbb{E}\left[\left(A^{FP4}_{i, kg:(k+1)g}\right)_l \left(B^{FP4}_{j, kg:(k+1)g}\right)_l \right] \right) \\
&= \sum_{k = 0}^{n/g}\left(X_{A_{i, kg:(k+1)g}}X_{B_{j, kg:(k+1)g}} \sum_{l=0}^g \mathbb{E}\left[\left(A^{FP4}_{i, kg:(k+1)g}\right)_l\right] \mathbb{E}\left[\left(B^{FP4}_{j, kg:(k+1)g}\right)_l \right] \right) \\
&= \sum_{k = 0}^{n/g}\left(X_{A_{i, kg:(k+1)g}}X_{B_{j, kg:(k+1)g}} \sum_{l=0}^g\frac{3}{4}\frac{\left(A_{i, kg:(k+1)g}\right)_l}{X_{A_{i, kg:(k+1)g}}} \frac{3}{4}\frac{\left(B_{j, kg:(k+1)g}\right)_l}{X_{B_{j, kg:(k+1)g}}} \right)  \\
&= \frac{9}{16} \sum_{h=0}^{n} A_{ih}B_{jh} = \frac{9}{16} (AB^T)_{ij}
\end{align}

For $\frac{dL}{dx}$, $A = \frac{dL}{dy} \mbox{diag}(S) H$ and $B = W^T \mbox{diag}(S) H$, where $H$ is the block-diagonal ``small'' Hadamard matrix constructed in Section \ref{sec:smallrht}.% \tao{missing ref here}.
Here, $\mathbb{E}\left[\texttt{MXFP4\_GEMM}(A, B^T)\right] = \frac{9}{16} \frac{dL}{dy} \mbox{diag}(S) H H^T \mbox{diag}(S) W = \frac{9}{16}\frac{dL}{dy} W$.
For $\frac{dL}{dW}$, $A = \frac{dL}{dy}^T \mbox{diag}(S) H$ and $B = x^T \mbox{diag}(S) H$.
Here, $\mathbb{E}\left[\texttt{MXFP4\_GEMM}(A, B^T)\right] = \frac{9}{16} \frac{dL}{dy}^T \mbox{diag}(S) H H^T \mbox{diag}(S) x = \frac{9}{16}\frac{dL}{dy}^T x$.
Finally, scaling both values by 16/9 in lines 10 and 11 gives the desired unbiased gradient estimators.

\end{proof}


\section{Bounding the variance of SR with the RHT}
\rhtvar*

\begin{proof}
Consider two vectors of size $b$: $A, B \in \mathbb{R}^{b}$. 
Then, the output of Algorithm \ref{alg:sr2mx} on $A$ is a scale $X_A$ and vector $Q_A$ such that $\mathbb{E}[{Q_A}_i] = A_i/X_A$ and the expectation is taken over runs of Algorithm \ref{alg:sr2mx}.
Likewise, the output of Algorithm \ref{alg:sr2mx} on $B$ is a scale $X_B$ and vector $Q_B$ s.t. $\mathbb{E}[{Q_B}_i] = B_i/X_B$.
Let $C = X_AX_B\sum_{i = 1}^{b} {Q_A}_i{Q_B}_i$.
Since stochastic rounding is implemented with dithering on $A$ and $B$ with independent random noise,
\begin{align}
\mbox{Var}(C) &= X_AX_B\sum_{i = 1}^{b} \mbox{Var}({Q_A}_i{Q_B}_i) \\
&= X_AX_B\left(\sum_{i=1}^{b} \mbox{Var}({Q_A}_i)\mbox{Var}({Q_B}_i) + \mbox{Var}({Q_A}_i)\mbox{E}({Q_B}_i)^2 + \mbox{Var}({Q_B}_i)\mbox{E}({Q_A}_i)^2\right) \\
&= X_AX_B\left(\sum_{i=1}^{b} \mbox{Var}({Q_A}_i)\mbox{Var}({Q_B}_i) + \mbox{Var}({Q_A}_i)\left(\frac{B_i}{X_B}\right)^2 + \mbox{Var}({Q_B}_i)\left(\frac{A_i}{X_A}\right)^2\right) \\ 
&= \sum_{i=1}^{b} X_AX_B\mbox{Var}({Q_A}_i)\mbox{Var}({Q_B}_i) + \mbox{Var}({Q_A}_i)\left(\frac{X_AB_i^2}{X_B}\right) + \mbox{Var}({Q_B}_i)\left(\frac{X_BA_i^2}{X_A}\right).
\end{align}

Let $\alpha = A_i/X_A$. 
Since ${Q_A}_i$ is the output of stochastic rounding to FP4, ${Q_A}_i$ takes on values $f(\alpha)$ with probabilty $\frac{c(\alpha) - \alpha}{c(\alpha) - f(\alpha)}$ and $c(\alpha)$ with probability $\frac{\alpha - f(\alpha)}{c(\alpha) - f(\alpha)}$, where $f(\alpha)$ denotes the largest representable FP4 value $\le \alpha$ and $c(\alpha)$ denotes the smallest representable FP4 value $\ge \alpha$.
Observe that $f(\alpha)$ and $c(\alpha)$ are both guaranteed to exist due to line 4 in Algorithm \ref{alg:sr2mx}. 
Then, 

\begin{align}
\mbox{Var}({Q_A}_i) &= \frac{f(\alpha)^2(c(\alpha) - \alpha) + c(\alpha)^2(\alpha - f(\alpha))}{c(\alpha) - f(\alpha)} - \alpha^2 \\
&= \frac{(c(\alpha)^2 - f(\alpha)^2)\alpha + (f(\alpha) - c(\alpha))f(\alpha)c(\alpha)}{c(\alpha) - f(\alpha)} - \alpha^2 \\
&= (c(\alpha) + f(\alpha))\alpha - f(\alpha)c(\alpha) - \alpha^2.
\end{align}

Let $\delta^+ = c(\alpha) - \alpha$ and $\delta^- = f(\alpha) - \alpha$. 
Then, %\tao{all these variances at the left side of equation 16-22 should be $\mbox{Var}({Q_A}_i)$ not $\mbox{Var}(\alpha)$, right?} \albert{yes}

\begin{align}
\mbox{Var}({Q_A}_i) &= (c(\alpha) + f(\alpha))\alpha - f(\alpha)c(\alpha) - \alpha^2 \\
&= (2\alpha + \delta^- + \delta^+)\alpha - (\alpha + \delta^-)(\alpha + \delta^+) - \alpha^2 \\
&= -\delta^-\delta^+ = (c(\alpha) - \alpha)(\alpha - f(\alpha)) \\
&= \mathcal{O}((c(\alpha) - f(\alpha))^2).
\end{align}

Since $c(\alpha) - f(\alpha)$ is $O(\Delta)$, 
\begin{align}
\mbox{Var}(C) &= \mathcal{O}\left(b\Delta^4X_AX_B + \Delta^2\frac{X_A}{X_B} \sum_{i=1}^{b} B_i^2 + \Delta^2\frac{X_B}{X_A} \sum_{i=1}^{b} A_i^2\right) \\
&= \mathcal{O}\left(b\Delta^4X_AX_B + \Delta^2\frac{X_A}{X_B} \|B\|^2 + \Delta^2\frac{X_B}{X_A} \|A\|^2 \right).
\end{align}

Since $X_A = \Theta(\|A\|_\infty)$ and likewise for $B$, this reduces to 
\begin{align}
\mbox{Var}(C) &= \mathcal{O}\left(b\Delta^4\|A\|_\infty \|B\|_\infty + 2b\Delta^2\|A\|_\infty \|B\|_\infty \right)\\
&= \mathcal{O}(b\Delta^4\|A\|_\infty \|B\|_\infty)
\end{align}
% \tao{why? $X_A$, the $shared_exp$ has a $-emax$ term, with the float to value representation of it, this more like to be $\Theta(1)$, in any case, it can't be $\mathcal{O}(\|A\|_\infty)$, we can discuss $\Theta(1)$ or $\Theta(\|A\|_\infty)$}\tao{problematic, connected to last comment}.
% \albert{$X_A$ is the scale from mx quantization. It is the largest magnitude entry in the vector scaled s.t. the largest element after dividng by $X_A$ is the largest normal (roughly) in FP4. I guess it should be $\Theta(||A||_\infty)$ then.}
If $A$ and $B$ are transformed by the RHT in the way Algorithm X does (i.e. $\tilde A \gets ASH^T$ and $\tilde B \gets HSB$), then we can bound $\|\tilde A\|_\infty$ and $\|\tilde B\|_\infty$. %\tao{this should be bound on $\|ASH^T\|_\infty$ and $\|HSB\|_\infty$}. 
From \cite{qs}, $\forall i, 1 \le i \le b$, 

\begin{equation}
\mathbb{P}(|e_i\tilde A| \ge \epsilon) = \mathbb{P}(|e_iASH^T| \ge \epsilon) \le 2 \exp \left(\frac{-\epsilon^2 b}{2\|A\|^2}\right).
\end{equation}

From the union bound, 

\begin{align}
\mathbb{P}\left(\max_i |e_iASH^T| \ge \epsilon\right) &\le 2b \exp \left(\frac{-\epsilon^2 b}{2\|A\|^2}\right) \\
\mathbb{P}\left(\max_i |e_iASH^T| \ge \sqrt{\frac{2\|A\|^2}{b} \log \left(\frac{2b}{\epsilon}\right)} \right) &\le \epsilon.
\end{align}

so with probability $\ge 1-\epsilon$, %\tao{the bound holds for $\|ASH^T\|_\infty$ first, then connects to $X_A$, the rest follows if it's $\Theta(\|ASH^T\|_\infty)$, not big O}

\begin{equation}
\|A\|_\infty = \mathcal{O}\left(\sqrt{\frac{2\|A\|^2}{b} \log \left(\frac{2b}{\epsilon}\right)}\right)
\end{equation}

and with probability $\ge (1-\epsilon)^2$,

\begin{equation}
\mbox{Var}(C) = \mathcal{O}\left(\Delta^4 \|A\|\|B\| \log\left(\frac{2b}{\epsilon}\right)\right).
\end{equation}
\end{proof}