\section{Training with MXFP4}

%MXFP4 is appealing for a variety of reasons. 
%Although hardware FP4 support has only recently begun, high volume chips such as NVIDIA's Blackwell GPUs implement FP4 with microscaling \citep{blackwell}.
%On actual hardware, initial benchmarks have shown that MXFP4 GEMMs can offer close to $2\times$ the real-world throughput of FP8 GEMMs.
The rest of this paper describes our approach that enables near-lossless \textit{training} with MXFP4-accelerated GEMMs.
Although our paper focuses on MXFP4, our analysis also applies to other low precision datatypes such as MXINT4.
We chose MXFP4 due to its relevance and hardware support on the latest accelerators.
To the best of our knowledge, MXFP4 has only been successfully used for near-lossless inference \citep{mxtrain,bwmlperf}.
Although certain works have achieved near-lossless training with MXFP4 weights, these require the activations and gradients to kept in higher precision.
These recipes run at the throughput of the higher precision operand, making them slower than pure-FP4 recipes.

%\albert{make the reasons appear earlier in the paragraph}
Our method hinges on obtaining unbiased, low-variance gradient estimates with pure-MXFP4 GEMMs in the backward pass, enabling more accurate model updates. 
Since the backward pass consists of $>1/2$ training FLOPs, our recipe can significantly accelerate training without reducing the representational power of the model from LP forward passes \citep{kumar2025scaling}.
To do this, we first modify the OCP MX quantization algorithm to perform unbiased quantization with scaling and stochastic rounding. 
Then, we show that by first transforming the GEMM operands with a memory-bound construction of the random Hadamard transform (RHT) \textit{before quantization}, we can bound the variance of the GEMM output.
Our method adds minimal overhead while significantly improving the quality of trained models, making MXFP4 practical for training.


%Instead, we show that with the random Hadamard transform (RHT) and stochastic rounding (SR), we can use MXFP4 in the backward pass and BF16 in the forward pass without noticeable degradation over full BF16 training.
%\youngsuk{mention (exact) datatype used for forward}
%The RHT concentrates the operands before quantization to reduce distortion and underflow.
%SR enables unbiased MXFP4 GEMMs and thus gradient estimates.

% \begin{table}[h]
% \centering
% \renewcommand{\tabcolsep}{2pt}
% \caption{Mean squared error (MSE) and maximum absolute error (MAE) of an MXFP4 matmul where both operands have outliers. ``+RHT'' denotes applying the RHT to both operands before multiplication, and ``+SR'' denotes using Algorithm \ref{alg:sr2mx} instead of Algorithm \ref{alg:float2mx} when quantizing to MXFP4. The RHT reliably improves both metrics, whereas SR improves both metrics in expectation.}
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
%     & MXFP4   & +RHT    & +SR (1 iter) & +RHT+SR (1 iter) & +SR (5 iter) & +RHT+SR (5 iter) \\ \midrule
% MSE & 1.22E-3 & 7.45E-4 & 2.58E-3      & 1.37E-3          & 5.05E-4      & 2.76E-4          \\
% MAE & 7.43    & 4.71    & 8.56         & 2.29             & 4.37         & 0.92             \\ \bottomrule
% \end{tabular}
% \end{table}

\subsection{Unbiased Quantization to MXFP4}

Algorithm \ref{alg:float2mx} describes the ``reference'' MX quantization algorithm to convert a scalar matrix to an MX matrix.
Algorithm \ref{alg:float2mx} finds, for each group of 32 entries, value with the largest magnitude $m=\max_i (|V_i|)$.
% \youngsuk{$m= \log \max V_i$}
% \albert{no, m is just the largest value. floor(log2(m)) is the exponent associated with m}
% \youngsuk{ok!}
Then, it calculates a shared exponent as a function of $m$ and $\texttt{emax}_{\texttt{elem}}$, the largest exponent of a normal number in the base data format.
% \youngsuk{is it the same as e in EeMm? the description could be confusing, what is normal number? may be good to have the example below}
% \albert{no, Ee is the number of exponent bits, its emax is ($2^e$ - 1) - bias}\tao{normal numbers (values abs$>$1) and subnormal numbers (values abs$<$1) are standard terms in FP; here we should make it clear this $\texttt{emax}_{\texttt{elem}}$ is a fixed value of the underlying FP, but not computed from elements in the block}
% \youngsuk{right. why not just define emax as ($2^e$ - 1) - bias, then? is there any case where there is no normal number? in that case what emax value we set?}
% \albert{It may happen depending on how IEEE adds new datatypes. OCP uses emax\_elem so I'm following them.}
For example,  $\texttt{emax}_{\texttt{elem}}= 2$ for FP4 since its maximum normal value is $6 = 2^2*1.5$.

Finally, group elements are normalized by the shared exponent and rounded to the base datatype.
% \youngsuk{general description of MX format can go to section 2.5?}

For MXFP4, line 1 of Algorithm \ref{alg:float2mx} returns $\texttt{shared\_exp} \gets \lfloor \log_2(m)\rfloor - 2$.
Observe that after dividing the entire group by $2^{\texttt{shared\_exp}}$, $m$ becomes 
\begin{equation}
m \gets \frac{m}{2^{\texttt{shared\_exp}}} < \frac{m}{2^{\log_2(m) - 3}} = 8
\end{equation}
%\youngsuk{want to indicate $\overset{\text{fp}4}{\leftarrow }$? or $\text{FP4}\left(\frac{m}{2^{\lfloor \log_2(m)\rfloor - 2}} \right)$ to indicate FP4 transformation of scaled value? or simply use $p_{\max}$ on the LHS?}
Since the maximum representable normal value in FP4 is 6, values scaled to between 6 and 8 will get clipped, making Algorithm \ref{alg:float2mx} inherently biased.
% \youngsuk{isn't $\lfloor \log_2(m)\rfloor - 2=0$ for max input $m=6$, so $\frac{6}{2^{\lfloor \log_2(m)\rfloor - 2}}= 6$ , what am I missing? I may have some misunderstanding on emax. Could you give an example the scale value is actually larger than $6$?}
% \albert{consider 7. floor(log2(7)) = $2 -> 2^0 = 1 -> 7$ gets scaled to 7 $>$ 6}\tao{Note $m$ here is the high precision value before quantization, not after quantization (FP4), so it can be any value larger than 6}
% \youngsuk{right $m$ can come from higher precision. Then, @albert I believe
% \begin{equation}
% m \gets \frac{m}{2^{\texttt{shared\_exp}}} < \frac{m}{2^{\log_2(m) - 3}} = 8
% \end{equation}
% should be correct.
% At the same time, m is lower bounded by $4$. 
% }
% \youngsuk{@albert, let's clarify the following in algo 1 or in the text or both. 1) $V_i$ is any number, 2) emax and quantize-to-element-format is determined by output dtype like fp4. even simply emax$= 2^e-1-bias$ and that long function into dtype($V_i/X)$ where dtype=FP4. techinially speaking that data conversion function should be also part of requirement?}
Although the proportion clipped depends on the input matrix, we can empirically check that for a wide distribution of matrices, roughly 3\% of the entries will get clipped. 

We can make Algorithm $\ref{alg:float2mx}$ unbiased with two simple modifications, both of which can be efficiently implemented in hardware.
First, we scale $V_i/X$ by $3/4$ to prevent clipping.
Then, we use stochastic rounding to quantize $Q'$ to FP4, which gives an unbiased estimate of $Q'$.
Algorithm \ref{alg:sr2mx} summarizes these modifications.
The resulting MX matrix is an unbiased estimate of 3/4 the original matrix.
%Note that without scaling, SR would not be able to give an unbiased estimator for elements $>6$ since there are no FP4 values $>6$.
Since SR is implemented with uniform independent dithering in hardware, the resulting GEMM output is an unbiased estimator of $(3/4)^2=9/16$ of the correct output.
To get an unbiased output, we can simply scale the high precision accumulator output by 16/9.

\begin{restatable}{lemma}{srlemma}
\label{lem:srlemma}
Assume stochastic rounding is implemented with dithering with independent noise. Then, Algorithm \ref{alg:sr2mx} produces a MXFP4 matrix that is an unbiased estimate of $3/4$ its input. Furthermore, Algorithm \ref{alg:rhtbw} with Algorithm \ref{alg:sr2mx} as a subroutine produces an unbiased estimate of $\frac{dL}{dx}$ and $\frac{dL}{dW}$.
\end{restatable}


%These changes allow us to compute unbiased estimators for the weight and activation gradients with MXFP4.
%This is important since the gradient-based optimization algorithms that LLMs are trained with suffer adversely from biased gradient estimates.
%While our experiments show that having a biased gradient estimate is not catastrophic, there is still an empirical benefit to having an unbiased gradient estimate.

%\youngsuk{@albert, can we highlight this finding in the intro/experiments more? It is theoretically more sound, appealing to AISTATS folks and could help other applications, although SR does not contribute too much under RHT in the experiments. }

\begin{algorithm}[t]
\caption{Unbiased quantization of $V \in \texttt{HP\_DTYPE}^k$ to an MXFP4 block
$\{X, P \in \texttt{LP\_DTYPE}^k\}$}
\label{alg:sr2mx}
\begin{algorithmic}[1]
\REQUIRE $\texttt{emax}_\texttt{elem}$ = exponent of the largest normal number in \texttt{LP\_DTYPE}
\STATE \texttt{shared\_exp} $\gets \lfloor \log_2(\max_{i}(|V_i|)) \rfloor - \texttt{emax}_\texttt{elem}$
\STATE $X \gets 2^{\texttt{shared\_exp}}$
\FOR{$i=1$ to $k$}
\STATE $V_i \gets \frac{3}{4} V_i$
\STATE $P_i = \texttt{stochastic\_round\_to\_FP4}(V_i / X)$
\ENDFOR
\RETURN $X,\>\{P_i\}_{i=1}^k$
\end{algorithmic}
\end{algorithm}

\subsection{Bounding the Variance of SR with the Random Hadamard Transform}
\label{sec:smallrht}
The backward pass for a linear layer ($y = xW^T$) requires computing $\frac{dL}{dx} = \frac{dL}{dy} W$ and $\frac{dL}{dW} = \frac{dL}{dy}^T x$.
LLMs have been known to have activation ($x$) and weight ($W$) ``outliers'' as well as sparse gradients ($\frac{dL}{dy}$) \citep{int4training, qs}.
Recall that MXFP4 quantization relies on groupwise statistics such as the largest magnitude element, so blocks with outliers will suffer from high quantization distortion and stochastic rounding variance.

Although Lemma \ref{lem:srlemma} tells us that Algorithm \ref{alg:sr2mx} produces an unbiased estimate of the true GEMM, high variance estimates can still degrade model quality by effectively adding noise to the gradient estimate.
To remedy this, we use the randomized Hadamard transform to concentrate gradients, activations, and weights before quantization, which asymptotically reduces the variance of the GEMM output.

The random Hadamard transform performs $x \gets HSx$, where $x \in \mathbb{R}^{j\times k}, S \in \{\pm 1\}^k$ (a random sign vector), and $H$ is the $k$-dimensional Hadamard matrix \citep{rht}. 
Hadamard matrices are recursively defined orthogonal matrices that satisfy the following:
\begin{equation}
\label{eqn:hadrec}
H_n = \frac{1}{2^{n/2}}\begin{bmatrix}
H_{n-1} & H_{n-1} \\
H_{n-1} & -H_{n-1}
\end{bmatrix},
\end{equation}
where $H_1 = \begin{bmatrix} 1 \end{bmatrix}$. 
Since both $H$ and $diag(S)$ are orthogonal, the RHT is fully invertible.
This means that we can apply the RHT to GEMM operands without inverting the RHT -- that is, $(HSA)^T(HSB) = A^TB$.

\begin{restatable}{theorem}{rhtvar}
\label{thm:rhtvar}
Let $A$ and $B$ be two size-$b$ vectors $\in \mathbb{R}^b$, and let $\mathcal{Q}$ perform Algorithm \ref{alg:sr2mx}. Then, the variance of $\mathcal{Q}(A)^T\mathcal{Q}(B)$ is $\mathcal{O}(b\Delta^4\|A\|_\infty\|B\|_\infty)$ and the variance of $\mathcal{Q}(HSA)^T\mathcal{Q}(HSB)$ is, with probability $\ge (1-\epsilon)^2$, $\mathcal{O}(\Delta^4\|A\|\|B\|\log(2b/\epsilon))$, where the largest gap between two consecutive representable points in $\mathcal{Q}$'s quantizer is $\Delta$. 
%\tao{and what is $\Delta$ here, do we define in the paper anywhere?}
% \tao{what is $\sigma$}
% \youngsuk{I read the proof. yeah, the most non-solid part is about the assumption $\mathcal{O}(\sigma)$ put uniformly over $b$ elements. To me, 
% 1) should be okay to hide $\sigma$ part as it is a common factor over two variance, only mentioning in the proof or 2) defining it more being aligned with $N(0, \sigma^2)$ used in figure 2, where $\sigma$ was used diffirently from one in the proof. But, given the timeline we have, just go for it, if we really want, can modify after arxiv upload.}
% \albert{this is standard practice that we used in quip. You need to mention the quantizer distortion otherwise using a better quantizer would have no effect, which doesn't make sense.}
\end{restatable}

Theorem \ref{thm:rhtvar} tells us that the variance of a MX matrix multiplication with respect to stochastic rounding is linear in the product of the largest magnitude elements in the operands. 
Applying the RHT to a vector effectively concentrates it to have a sub-Gaussian tail distribution. 
From \citet{qs}, we know that
\begin{equation}
\label{eqn:hadtail}
\mathbb{P}\left( |e_iHSx| \ge a\right) \le 2\exp\left(\frac{-a^2k}{2\|x\|^2}\right),
\end{equation}

letting us bound the the variance of the SR GEMM in Theorem \ref{thm:rhtvar}. 
Specifically, applying the RHT reduces the variance from a linear dependence on blocksize to a log-dependence on blocksize, albeit with the $L_2$ norm of the input instead of the $L_\infty$ norm.

We can verify this empirically by measuring the variance of a SR GEMM with and without the RHT. 
Figure \ref{fig:rhtvar} shows the mean variance of $\mathcal{Q}(A)^T\mathcal{Q}(B)$ vs. $\mathcal{Q}(HSA)^T\mathcal{Q}(HSB)$ over 4K samples of $A, B \in \mathbb{R}^{b} \sim \mathcal{N}(0, I)$ with proportion $p$ outliers from $\mathcal{N}(0, 5I)$, where $\mathcal{Q}$ performs Algorithm \ref{alg:sr2mx}.
That is, $A, B \sim \mathcal{N}(0, I) + \mbox{Bernoulli}(p)*\mathcal{N}(0, 5I)$.
As expected from Theorem \ref{thm:rhtvar}, the variance grows much slower as a function of $b$ with the RHT vs. without.
% \youngsuk{where is $p$ part demonstrated?}\albert{x axis}


\begin{figure}[t]
\includegraphics[width=\linewidth]{figs/var_b.pdf}
\caption{Mean variance of $\mathcal{Q}(A)^T\mathcal{Q}(B)$ vs. $\mathcal{Q}(HSA)^T\mathcal{Q}(HSB)$ over 4K samples of $A, B \in \mathbb{R}^{b} \sim \mathcal{N}(0, I)$ with proportion $p$ outliers from $\mathcal{N}(0, 5I)$. $\mathcal{Q}$ performs Algorithm \ref{alg:sr2mx}. Variance with the RHT grows much slower than without.
%\youngsuk{@alber, examining dependence on $b$ could be more important than $\sigma$?}
%\albert{I will plot that too}
}
\label{fig:rhtvar}
\end{figure}

\begin{algorithm}[t]
\caption{MXFP4 linear layer (no bias) backward pass with the random Hadamard transform.}
\label{alg:rhtbw}
\begin{algorithmic}[1]
\REQUIRE Gradient of output $\frac{dL}{dy} \in \mathbb{R}^{b\times m}$, activations $x \in \mathbb{R}^{b \times n}$, weights $W \in \mathbb{R}^{m \times n}$, block size $g \le 256, 32|g, g|m, g|n$.
\STATE $H \gets \text{Hadamard matrix } H_b \in \mathbb{R}^{m \times m}$.
\STATE Sample random sign vector $S \in \{\pm1\}^b$.
\STATE $G' \gets \left(\left(\frac{dL}{dy}\right).\texttt{view}\left(\frac{bm}{g}, g\right)\right)\texttt{diag}(S)H$
\STATE $W' \gets H^T \texttt{diag}(S) \left(W.\texttt{view}\left(g, \frac{nm}{g}\right)\right)$
\STATE $GT' \gets \left(\left(\frac{dL}{dy}^T\right).\texttt{view}\left(\frac{bm}{g}, g\right)\right)\texttt{diag}(S)H$
\STATE $X' \gets H^T \texttt{diag}(S)\left(x.\texttt{view}\left(\frac{bn}{g}, g\right)\right)$
\STATE $\frac{dL}{dx} \gets \texttt{MXFP4\_GEMM}(G', W')$
\STATE $\frac{dL}{dW} \gets \texttt{MXFP4\_GEMM}(GT', X')$
\\\COMMENT{Where \texttt{MXFP4\_GEMM} forms MX groups along the reduction dimension and uses either Algorithm \ref{alg:float2mx} or \ref{alg:sr2mx} to quantize to MXFP4.}
\IF{Using Algorithm \ref{alg:sr2mx}}
\STATE $\frac{dL}{dx} \gets \frac{16}{9} \frac{dL}{dx}$
\STATE $\frac{dL}{dW} \gets \frac{16}{9} \frac{dL}{dW}$
\ENDIF
\RETURN $\frac{dL}{dx}, \frac{dL}{dW}$
\end{algorithmic}
\end{algorithm}

However, the RHT is not free.
First, observe that when computing $\frac{dL}{dW} \approx \mathcal{Q}(HS\frac{dL}{dy})^T\mathcal{Q}(HSx)$, the RHT ``mixes'' along the batch dimension.
In data-parallel settings (e.g. FSDP \citep{fsdp} or ZeRO-3 \citep{zero}) where activations are sharded across GPUs, the full RHT would require expensive cross-GPU communication.
Even with fast interconnects, this would immediately bottleneck gradient computation.
Second, although Equation \ref{eqn:hadrec} admits an $O(n \log n)$ time matrix-vector product algorithm, the RHT step occurs in high precision.
Reducing this overhead is critical -- if the RHT is slower than a FP4 matmul, one should just use FP8 instead.

To solve these problems, we apply the RHT as a dense matrix multiplication over a small number of MX blocks, which makes it \textit{memory bound} in the GEMM operands (see Table \ref{tab:e2e}).
Specifically, let the RHT block size be $g, 32 | g$. %\youngsuk{what is 32 $|$ g?} \albert{32 divides g. this is standard notation} \youngsuk{thanks!}
Applying this block-wise RHT as a dense matmul gives a runtime of $O((b+m)ng)$ and IO cost of $O(bn+nm+bm)$.
Since modern AI accelerators have high compute to memory ratios, this ``blockwise'' RHT is memory bound when $g \lessapprox 256$. 
Algorithm \ref{alg:rhtbw} summarizes how we use the RHT in the backward pass of a linear layer. 
Since $g$ is smaller than the sequence length of any reasonably large model, Algorithm \ref{alg:rhtbw} works as a drop-in replacement for a linear layer even in data-parallel settings.
Furthermore, although lines 3-6 are written out for clarity, an efficient implementation could fuse them into lines 7 and 8, reducing costly memory accesses. %\tao{do we have an ablation table of RHT block size - running time or throughput (in TP=1 and 8)? that would explain better our claim; looks like table 5? can we link to it}\albert{I can link the table. TP8 is too annoying to set up}

The tradeoff to doing this blockwise RHT is that equation $\ref{eqn:hadtail}$ depends on $g$ ($k$ in the equation) -- the higher $g$ is, the tighter the concentration will be. 
However, in practice, we observe $g = 64$ is sufficient to get a tight distribution and MX can handle scale differences across blocks.
Finally, note that this construction also lets us use \textit{any} random orthogonal transformation. %\tao{isn't every orthogonal matrix can be ran with a dense matmul?}
We chose the RHT since it is fast to randomize (by sampling a single $g$-dim sign vector) and has good concentration, but other matrices could work as well.



% % \begin{figure*}[t]
% % \centering
% % \includegraphics[width=0.32\linewidth]{figs/direct.pdf}
% % \includegraphics[width=0.32\linewidth]{figs/rhtpq.pdf}
% % \includegraphics[width=0.32\linewidth]{figs/rht.pdf}
% % \caption{(L) Using Algorithm \ref{alg:float2mx} to quantize a 64-element tensor $x$ with outliers results in small values getting ``flushed'' to 0. In practice, this means losing gradient information during the backward pass. (C) Applying the RHT ($x \gets HSx$) concentrates the entries in the matrix and reduces outliers. (R) Quantizing in the RHT space solves MXFP4's underflow issue.}
% % \label{fig:flush}
% % \end{figure*}

% \subsection{The Randomized Hadamard Transform}
% \label{sec:smallrht}
% %\youngsuk{may want to move some of content to background?}






% Theorem \ref{thm:rhtvar}.

% We take advantage of this later by quantizing $ASH$ and $BSH$, which are ``easier'' to quantize than $A$ and $B$.
% %\youngsuk{here or around Algo 2 description. Need a description how to apply R into any general matrix $XY$ or backward of our interest $\frac{dL}{dx} = \frac{dL}{dy} W$ or $\frac{dL}{dW} = \frac{dL}{dy}^T x$, basically with $H, H^T$. At least mention Y inverseRHT(Q(RHT(X))).This will help understanding Algo 2.}


% This is important due to the way Algorithm \ref{alg:float2mx} quantizes vectors to MXFP4. 
% If a group has a maximum value $m$, values $\lessapprox \frac{m}{24}$ 
% % \tao{how is the 24 factor computed?} \albert{max normal is 6, min normal is 0.5, values under 0.25 get flushed to 0. 6/0.25 = 24. The approx is because depending on MX scaling the max value may go to 8 so the ratio would be 32.}
% will get rounded to 0.
% Thus, the existence of outliers in a group will result in information from the rest of the group being lost.
% Figure \ref{fig:flush} shows this visually for vector $x$ with two outliers.
% Without the RHT, Algorithm \ref{alg:float2mx} flushes over half the values to 0 (L).
% With the RHT, which concentrates $x$ (C), small values are preserved after quantization (R).


% % If we apply the RHT to each row of and operand before quantizing it to MXFP4, then each row becomes approximately Gaussian distributed, allowing us to bound the quantization distortion $\delta = MSE(Q_{MXFP4}(x), x)$.
% % Since the representable values of FP4 are also close to Gaussian-distributed, $\delta = 0.013$ when x $\overset{iid}{\sim} \mathcal{N}(0,1)$.
% % Note that this is very close to the optimal distortion for a 4 bit scalar quantizer (Lloyd-Max), $\delta = 0.012$.
% %\albert{put bound on distortion for matmul} 
% %\begin{equation}
% %\label{eqn:gemmdist}
% %\delta \le \left(2R + R^2\right) \frac{\|A\|}{\sigma_{\min} A}
% %\end{equation}





