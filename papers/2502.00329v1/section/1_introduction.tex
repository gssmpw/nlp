\section{Introduction}
\label{sec:intro}

Large language models promise to usher in a new wave of innovation in data analytics~\citep{llm_data_wrangling2022,fernandez2023large,zhou2024llm,anderson2024design}. 
%
With LLMs, users will be spared the time-consuming tasks of discovering relevant data in messy data lakes, integrating diverse sources, and preparing the data for further use. Once the data is identified and prepared, users should be able to solve problems simply by asking questions in natural language, without having to navigate complex database schemas or domain-specific query languages. 
%
One method to realize these goals is by improving LLM performance on data-related tasks through prompt engineering~\citep{llm_data_wrangling2022,chorus}. 
%
However, this approach requires careful selection of optimal instructions and few-shot demonstrations for specific tasks, and it has not yet proven to be sufficient, often yielding erroneous answers~\citep{zhu2024chat2query}. In a related line of work,  models such as Table-GPT~\citep{table-gpt}, TableLlama~\citep{zhang2024tablellama}, TableLLM~\citep{wu2024tablebench}, and TableGPT2~\citep{su2024tablegpt2} have been finetuned with specific instructions for various table understanding and data wrangling tasks. Despite this progress, these works do not adequately address tasks that require a deep understanding of business concepts and their mapping to database schema and datasets. Existing works largely focus on tasks based on a single or a pair of tables and do not explore the relationships across various tables, which necessitates strong data modeling and integration capabilities.


To realize the vision mentioned above, we argue that LLMs must also be able to deal with messy collections of data, as often witnessed in data lakes. To do that, LLMs need to grasp a broad set of data management concepts. This includes understanding basic tabular representations, how business concepts are represented as complex database schemas or collections of interlinked datasets within a data lake, how tables can be created from other tables using views or other forms of computation, and the principles of data wrangling and integration~\citep{doan2012principles}. One key challenge is that models like GPT-4~\cite{achiam2023gpt} and Llama~\citep{touvron2023llama}, are primarily trained on general knowledge derived from web data, which limits their exposure to the specific training data from which they can learn fundamental data management concepts. Consequently, current LLMs perform poorly on tasks such as searching for relevant data within a large data lake that contains multiple interconnected datasets or evaluating hypotheses that require integrating data from diverse sources.

This work takes a first step toward developing foundation models that perform well on a broader set of data analytics tasks. 
We introduce a new data recipe designed for post-training any LLM, enhancing their ability to understand the "messy" reality of data management. More specifically, we post-train \modelname, a 12-billion-parameter foundation model based on Mistral-NeMo-Instruct, using our well-curated training corpus. Initially, we fine-tune \modelname on a large set of synthetically generated instruction-response pairs to enhance domain-specific knowledge. Next, we improve its data comprehension and problem-solving abilities by contributing two new table-text alignment tasks, followed by instruction fine-tuning on a smaller set of task-specific examples that focus on data discovery and real-world SQL code generation. Our \modelname demonstrates significant improvement over the base model and performs competitively with other state-of-the-art models, including GPT-4o, across various tasks, including three newly curated \mmlu datasets, three table discovery datasets, and two public \texttosql tasks. 


\newpage
\noindent In this paper we make the following contributions:
\vspace{-3mm}
\begin{itemize}
    \item This work is the first to curate a large instructed training corpus designed for data analytics (summarized in Table~\ref{tbl:train-corpus}). The training corpus contains over 1 billion tokens and 2 million input-output examples.
    \item We propose a scalable synthetic data generation method that extracts and synthesizes instruction data from web corpora. This method grounds responses in reference documents to enhance diversity and avoid hallucination.
    \item We contribute two evaluation tasks and associated datasets. The first, named \mmlu, measures massive multitask language understanding in data analytics problems. The second, \wikipage is a human-annotated table selection benchmark with complex multi-table questions involving both textual and tabular data.
    \item We post-train a new 12B foundation model, \modelname, using the curated training corpus. 
    In the \mmlu evaluation, it surpasses GPT-3.5-Turbo, Mixtral-8x7B, and Mistral-Small. For \datadiscovery, it outperforms GPT-4o by 12.1\%. In the \texttosql task, it achieves an average improvement of 24.9\% compared to the base model.
    \vspace{-2mm}
\end{itemize}


\section{\modelname Overview}
In this section, we give an overview and motivation for how we curate data to train and evaluate \modelname for analytics tasks.

\para{Scalable Data Curation Methods}
Developing an expert-level LLM for data analytics is challenging due to the lack of high-quality, diverse, and supervised datasets. Human-annotated instruction datasets are limited in scale and can be expensive to obtain, while purely synthetic data often contains factual errors and lack diversity. To address these issues, we adopt an \emph{extraction-and-synthesis} strategy that leverages a large-scale web corpus rich in analytics-relevant knowledge and use cases. This approach involves identifying naturally occurring instruction data, such as question-answer pairs from Stack Overflow, and augmenting documents with instruction-response pairs grounded in their content. After content distillation, we remove the plain text documents to form the final dataset. It is worth noting that by grounding responses in reference documents during the generation process, we can reduce hallucinations and increase the diversity of the dataset.  Empirical studies show that post-training with instruction-aware data enhances model performance more than plain text, as it aligns the model better with domain-specific queries and responses~\citep{chung2024scaling,instruction_pre_training}.

\para{New Training Tasks}
To improve the model's understanding of the relationship between natural language and tabular data, we design two table and text alignment tasks. The first task, \texttotable, is to generate a table schema from textual scenario description, which allows the to model understand how different pieces of information relate to, and are expressed with, various business entities. The second task, \tabletotext, aims to generate a text description for every row in a table. Mastering this task is essential for enhancing the model's ability to translate structured information into human-readable formats and vice versa, which is crucial for tasks such as generating reports, summaries, and data motivations. 


\begin{table}[t!] 
 \centering
 \caption{Training Corpus Overview.
 \vspace{-4mm}
 } 
 \label{tbl:train-corpus}
 \resizebox{\columnwidth}{!}
 {
    \begin{tabular}{llrrc}
        \toprule[1.2pt]
        & \textbf{Tasks (per chapter)} & \begin{tabular}[c]{@{}r@{}} \#Documents/ \\ \#Examples \end{tabular} &  \#Tokens & Source  \\
        \midrule
        \multicolumn{4}{l}{\hspace{-.5em} \textit{Chapter1: Knowledge Corpus}} \\
        \cmidrule{2-5}
        & Plain Text & 5.8M & 1.9B & Web\\ \cmidrule{3-5}
        & Instruction-Response Pairs & 8.8M & 0.9B & \begin{tabular}[c]{@{}r@{}}Web \& \\ Synthetic \end{tabular}  \\
        \midrule
        \multicolumn{4}{l}{\hspace{-.5em} \textit{Chapter2: Text\&Table Alignment}} \\
        \cmidrule{2-5}
        & \texttotable & 4.8K  & 7.4M  & Synthetic\\
        & \tabletotext & 53.6K & 12.3M & Synthetic \\
        \midrule
        \multicolumn{4}{l}{\hspace{-.5em} \textit{Chapter3: Analytics Tasks}} \\
        \cmidrule{2-5}
        & \datadiscovery & 17.4K   & 37.8M  & Public \\
        & \texttosql     & 121.8K  & 42.6M  & Public \\
        \bottomrule[1.2pt]
        \multicolumn{2}{l}{\hspace{.5em} Total} & 9.0M & 1.0B
    \end{tabular}
}
\vspace{-3mm}
\end{table}


\para{A Large-scale Training Corpus}
The curated data consists of approximately 2 million examples and 1 billion training tokens. We organize this training corpus into distinct "chapters", much like a well-structured textbook.
\textit{Chapter 1} covers knowledge in data management and analysis, including data modeling concepts, instructions for using analysis tools, machine learning techniques, and more. We prepare this chapter by filtering relevant content from the \fineweb dataset~\citep{penedo2024fineweb} and applying the extraction-and-synthesis method to automatically generate instruction pairs.
\textit{Chapter 2} focuses on the foundational schema-level and row-level table and text alignment tasks, as mentioned above.
\textit{Chapter 3} addresses specific analytics tasks, particularly those that require identifying and integrating data from multiple sources. We chose \datadiscovery and \texttosql.
The goal of \datadiscovery is to identify one or more datasets that contain the necessary information to answer a user-specified question. \texttosql is a well-established task that involves translating natural language instructions into executable SQL queries. 

\para{New Evaluation Tasks and Datasets}
To evaluate the foundation modelâ€™s performance, we introduce \mmlu, a new benchmark with three datasets designed to assess the model's language understanding and reasoning capabilities in the analytics domain. This benchmark consists of thousands of multiple-choice questions covering the areas of database management, data analysis, and machine learning. We collect questions from existing textbooks and synthetically generate additional question-answer pairs with \claudetf. All answers are manually reviewed and revised. 
Additionally, we prepare three new datasets for the \datadiscovery (TS) task. The key challenge in this task is that the model needs to determine how many tables are required in the answer. Three datasets cover distinct settings:
\birdselect -- derived from BIRD~\citep{bird}, where candidate tables come from a single database with well-structured schemas, 
\openwikiselect -- sourced from Open-WikiTable~\citep{open_wikitable}, where the candidate tables are semantically similar, 
and \wikipage -- a newly annotated benchmark, which draws input from a single Wikipedia page containing multiple tables and text descriptions on the same topic. For \wikipage, we aim at questions that require multi-hop reasoning, in which models must integrate information from multiple tables or perform sequential logical steps to derive the correct answer.



\subsection{Summary of Evaluation Results}
We evaluate \modelname on three key aspects: knowledge testing, data selection, and SQL code generation. Overall, \modelname shows significant improvement over the base Mistral-NeMo model and delivers competitive performance compared to the leading models.
\begin{itemize}
    \item \modelname achieves the highest overall score of 0.697, making it the best-performing model.
    \item In the \mmlu evaluation: \modelname surpasses most other models, including GPT-3.5 Turbo and Mixtral-8x7B, although it falls relatively 5.8\% short of GPT-4o.
    \item For \datadiscovery, \modelname outperforms GPT-4o by 12.1\%, making it the most potent model for this task. 
    \item   In the \texttosql evaluation, \modelname achieves an average execution accuracy of 0.576, outperforming all other models and demonstrating an average of 24.9\% relative improvement compared to the base model.
\end{itemize}

