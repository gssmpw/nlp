\subsection{Model Architecture}

\modelname is built on top of Mistral-NeMo-Instruct, a decoder-only Transformer of 12B parameters, with the following specifications:
\begin{itemize}
    \item 128k vocabulary size
    \item Rotary Positional Embedding (RoPE)~\citep{su2024roformer}
    \item 40 Transformer layers~\citep{vaswani2017attention}
    \item 5,120 token dimension and 14,436 hidden dimensions
    \item 128 head dimensions
    \item 32 attention heads and 8 grouped query attention~\citep{ainslie2023gqa}
    \item SwiGAU activation~\citep{shazeer2020glu}
\end{itemize}

\subsection{\modelname Training}

We train \modelname using the Nvidia NeMo framework, leveraging its capabilities for efficient distributed training with data and model parallelism techniques. 

We begin the \textit{supervised instruction-tuning} process using input-output pairs from Chapter 1 (see Section~\ref{sec:chapter1}). By focusing on the basic knowledge of data analytics, the model can establish a strong understanding of key concepts, terminology, and reasoning patterns specific to data analytics. This foundational knowledge is crucial before the model addresses more complex, task-specific scenarios. We train the Chapter 1 data for one epoch. As noted by~\citet{hernandez2022scaling}, repeated tokens can negatively impact performance.

Next, we continue with supervised fine-tuning using data from Chapters 2 and 3 to improve the model's generalization capabilities, enabling it to apply its foundational knowledge to diverse downstream tasks more effectively. To ensure consistency across datasets, we standardize the format of these chapters in our analytic data corpus, aligning them with a unified data structure. This standardization ensures that the fine-tuned models can process data uniformly, regardless of the original chapter formats, thereby streamlining the training process and enhancing its efficiency.


\para{Training Configurations}
We use the AdamW~\citep{loshchilov2017fixing} optimizer with $\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-8}$, and a weight decay of $0.01$. We set the maximum learning rate to $lr_{\text{max}} = 1e^{-6}$, the minimum learning rate to $lr_{\text{min}} = 0$, with a Cosine learning rate scheduler to allow the model to make fine-grained adjustments with the labeled data. 

\para{How to choose the \modelname checkpoint?} We train the Chapter 1 data for one epoch, followed by training on Chapter 2 and Chapter 3 data for two epochs while mixing 10\% uniformly selected Chapter 1 data. The final endpoint is selected as \modelname for evaluation in all the following experiments. 