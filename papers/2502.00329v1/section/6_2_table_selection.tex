\subsection{Table Selection}
Table selection aims to identify the most relevant subset of tables from a pool of candidate tables to answer a specific natural language question. Understanding how datasets complement or contradict each other helps users better determine which datasets provide the most relevant information. For example, related datasets may share common attributes or originate from similar domains, resulting in richer insights when combined. Conversely, recognizing discrepancies or redundancies among datasets can help void errors and misinterpretations in analysis. 

\para{Datasets} 
We have created three evaluation benchmarks to evaluate various scenarios for table selection (TS). The first two datasets are derived from BIRD~\citep{bird} and Open-WikiTable~\citep{open_wikitable}. The third dataset is our newly annotated benchmark, which includes text and table data, with carefully selected questions requiring rows from multiple tables. Specifically, \birdselect provides a controlled environment for assessing the model's performance on organized, relational data. \openwikiselect tests models' ability to discern and utilize subtle differences among similar tables. \wikipage tests the model's capacity for integrating information from various sources within a cohesive topic.
%
Table~\ref{tab: dataset_discovery} describes the statistics of the datasets. We mark \birdselect and \openwikiselect as seen datasets because the training corpus includes examples from the training sets of BIRD and Open-WikiTable as described in Section~\ref{sec:chapter3_table_selection}. \wikipage is an entirely new dataset primarily used for evaluation purposes.

\begin{table}[tb]
\caption{Summary of three evaluation datasets for \datadiscovery. "\#Cand. Table" stands for the average number of candidate tables per question. }
\resizebox{\linewidth}{!}
{
\begin{tabular}{lrccc}
\toprule[1.2pt]
\multicolumn{1}{l}{}   & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{\#Input}\\  \textbf{Token}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{\#Cand.} \\ \textbf{Table}\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} \textbf{Data}\\  \textbf{Characteristics}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{\#Ground Truth}\\ \textbf{Table}\end{tabular}} \\ \midrule
\birdselect & 1.1K & 9.1 & Schema only & Multiple \\ \midrule
\begin{tabular}[c]{@{}l@{}}\texttt{Open-Wiki-}\\ \texttt{Table-TS}\end{tabular} & 2.3K & 10 & \begin{tabular}[c]{@{}r@{}}Metadata+\\ 3 sample rows\end{tabular} & Single \\ \midrule
\begin{tabular}[c]{@{}l@{}}\wikipage \end{tabular}  &  12.5K  & 21.6 & Text + Table  & Multiple \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\label{tab: dataset_discovery}
\end{table}

\para{\birdselect: Candidate tables with well-designed schemas}
We employ a data processing method similar to that outlined in Section~\ref{sec:chapter3_table_selection} for the development data of BIRD, from which we select a total of 1,534 questions. For each question, we use all tables from the same database as candidate tables, relying solely on the well-designed schemas without using table content. The ground truth tables are derived from the corresponding gold SQL queries. It is important to note that some questions in this dataset require multiple tables to generate correct answers, highlighting the need for a deeper understanding of relational database design and the interdependencies between tables.

\para{\openwikiselect: Similar candidate tables with metadata and sample rows} For Open-WikiTable, we apply a similar method as described in Section~\ref{sec:chapter3_table_selection}, this time using the test set. We obtain 5,134 questions from the test set, ensuring ten candidate tables are retrieved via BM25, including the gold table. We also include metadata for the WikiTable as model input, such as page titles, section titles, and captions, when available. To enhance scalability when working with large tables, we limit three sampled rows per table to the LLMs. It is worth noting that each question in this dataset requires only one ground truth table. Therefore, we compare two retrieval-based methods: BM25~\citep{bm25} and an encoder-based method, \texttt{BGE-M3}~\citep{chen-etal-2024-m3}. Both methods learn the embeddings of the tables and questions, using cosine similarity to compute relevance scores. The table with the highest similarity score is then selected as the final table.

\para{Human-annotated \wikipage: Same Topic Text and Tables}
Existing Text-to-SQL and multi-table QA datasets~\citep{pal-etal-2023-multitabqa} consider table joins when generating programming languages or textual answers. However, these datasets assume that the key-foreign-key constraints (i.e., join relationships) are already provided. In data lakes, this assumption does not always hold. Moreover, it is more common to encounter both textual and tabular data in these contexts. 
%
To reflect this real-world scenario, we introduce a new human-annotated dataset that includes a Wikipedia page's data containing multiple tables and descriptions under the same topic.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/wikipage_example.png}
    \caption{A question and wikipage data sample from \wikipage. In this example, we need to understand that \texttt{Heat\_2\_1}, \texttt{Heat\_3\_1}, and \texttt{Heat\_4\_1} occurred in chronological order. In \texttt{Heat\_2\_1}, Marlene ran 11.5 seconds, which matched the Olympic record. In \texttt{Heat\_3\_1}, Betty ran 11.4 seconds, breaking the Olympic record. By \texttt{Heat\_4\_1}, although Heather ran 11.5 seconds, the Olympic record had already been lowered to 11.4 seconds, so Heather did not match the new record. The answer is \textit{2 athletes}. For the \datadiscovery task, we regard all referenced tables as ground truth tables.}
    \label{fig:wikipage_heat}
\end{figure}

The data on the Wikipedia page contains various complex scenarios that require the model to possess both general knowledge and strong data interpretation capabilities. For example, in Figure~\ref{fig:wikipage_heat}, the question is: \textit{How many athletes matched or broke the Olympic records in the Women's 100 meters at the 1956 Summer Olympics?}. To answer this, we first identify the existing Olympic record from the \texttt{Record\_1} table, which is \textit{11.5 seconds}. The second step is to analyze the athletes' recorded times during the different stages of the competition (heats, semifinals, and finals) in chronological order to determine whether they matched or broke the record. In this example, the model needs to recognize that \texttt{Heat\_2\_1}, \texttt{Heat\_3\_1}, and \texttt{Heat\_4\_1} occurred sequentially. In \texttt{Heat\_2\_1}, Marlene ran 11.5 seconds, which matched the Olympic record. In \texttt{Heat\_3\_1}, Betty ran 11.4 seconds, breaking the Olympic record. By \texttt{Heat\_4\_1}, although Heather ran 11.5 seconds, the Olympic record had already been lowered to 11.4 seconds, so Heather did not match the new record. The correct answer is \textit{2 athletes}. If the model fails to grasp the definition of matching or breaking records, the chronological order of the heats, or that records can be broken at any point during the competition, it would struggle to provide the correct response.
% 
The dataset creation and annotation steps are as follows:
\begin{enumerate}
    \item \parait{Step 1} We download approximately 100 Wikipedia pages related to the sports domain and convert them from HTML to plain text. The table data is converted into markdown format using \texttt{pandas.read\_html().to\_markdown()}. We name each table based on its section name, followed by a unique identifier. We flatten the structure of nested tables and omitted image data, links, or styling elements like bold text, borders, or colors. 
    \item \parait{Step 2} For each Wikipedia page, we use \claudetf to generate questions and identify the ground truth tables needed to answer them. We prompt the model to create questions requiring multi-table reasoning, ensuring that necessary information spans across multiple tables. Additionally, we instruct \claudetf to provide detailed step-by-step answers for each question. Although we do not explicitly use the answers, they serve as a reference for human annotators to more accurately evaluate the correctness of the reference tables in the next step. 
    \item \parait{Step 3} Three human annotators review and edit the examples to ensure the validity of the question and the accuracy of the ground truth tables (i.e., the labels). If we find any questions invalid, we return to step 2 to regenerate the question, reference tables, and answer triplets.
\end{enumerate}

Finally, we annotate 104 examples for the \wikipage dataset. On average, each question had 21.6 candidate tables, and the overall token count for a single Wikipedia page is approximately 12K, which is significantly longer than the other two datasets (Table~\ref{tab: dataset_discovery}).

\para{Evaluation Metrics and Baselines}
%
Unlike traditional dataset search settings that specify a predefined number of tables to retrieve, we allow the model to decide the number of necessary tables. We evaluate performance by using the \textit{accuracy} score, which measures the exact match between the predicted table set and the ground truth table set. The accuracy can be either 0 or 1. It equals 1 when the predicted set contains exactly the same tables as the ground truth set.
%
For \birdselect and \wikipage, the number of ground truth tables varies, making it challenging to apply traditional embedding-based methods to obtain the correct table sets. In contrast, for \openwikiselect, each question requires only one table, allowing us to use the top retrieved table as the prediction. Therefore, we include a lexical retrieval method, BM25~\citep{bm25}, and an encoder-based method, \texttt{BGE-M3}~\citep{chen-etal-2024-m3} as baselines.
 
\para{Task Prompts}\label{sec:table_selection_prompts}
%
We include the data information, the task instruction, and a question in the input prompt. For the data information, we use table schema for \birdselect, table metadata, three sample rows for \openwikiselect, and the entire Wikipedia page for \wikipage. These representations help tailor the data context for each dataset in the prompt. Here, we show an input-output example from \wikipage, which is based on the \href{https://en.wikipedia.org/wiki/100_metres_at_the_Olympics}{"100 metres at the Olympics"} Wikipedia page. To obtain the expected results for the question, we need first to identify the name of the male athlete who was the oldest champion in the \texttt{age\_1} table -- which is \emph{Linford Christie}. Then, we locate \emph{the year} when Linford Christie won the gold medal from the \texttt{Men\_1} table. 

\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{Verbatim}
# Input:
<Table Schema> / <Table Metadata>+<Sample Rows> / <Wikipedia Page>

Given the above data, identify one table or multiple tables that 
contain the necessary information to answer the following question.
\textbf{Question}: In which year did the oldest male champion win the 
100-meter Olympic games?
Provide the table name(s) within the <Tables> tag, with one table 
name per line.

# Output: 
<Tables>
Age_1
Men_1
</Tables>
\end{Verbatim}
\end{tcolorbox}

\begin{table}[t!]
\caption{The accuracy scores (\%) from \openwikiselect on two task-specific embedding-based baseline models.} 
\vspace{-3mm}
\label{tbl:open-wikitable-baseline}
\small
\begin{tabular}{r| rr |r}
 \toprule[1.2pt]
         & BM-25 & BGE-M3 & \modelname \\ \midrule
Accuracy & 71.1  & 77.8   & 91.9   \\
 \bottomrule[1.2pt]
\end{tabular}
\end{table}


\para{Main Results and Analysis}
\modelname demonstrates superior performance across all datasets. For the "seen" \birdselect and \texttt{Open- WikiTable-TS} datasets, where a subset of examples is used for training \modelname, performance improves by 89.8\% on \birdselect and 213.7\% on \openwikiselect compared to the base Mistral-NeMo model. Additionally, it outperforms the top model by 9.1\% on \birdselect and 29.3\% on \openwikiselect.

We observe a performance jump on the unseen \wikipage dataset, which has no examples included in the training phase. A key characteristic of this dataset is its long input context, averaging 12.5K tokens, with some examples exceeding \texttt{GPT-3.5-Turbo}'s maximum context length of 16,385 tokens. As a result, \texttt{GPT-3.5-Turbo} encounters Context Length Errors (CLE) during inference. The base \texttt{Mistral-Nemo} model fails on the \wikipage dataset because of its long input context and complex questions with table and text as input. Additionally, we observe that it fails to follow instructions and correctly output the required XML tag "<Tables>". To address this, we apply additional answer extraction techniques, treating the entire model output as a list of table names and interpreting each line as a separate table name. The refinement boosts accuracy from 0 to 0.288.  However, this result still falls far behind \modelname, which achieves 0.558 accuracy, representing a relative 93.7\% improvement over the base model. Results labeled "LCE" indicate long context errors, meaning the outputs are meaningless. Even after applying the above answer refinement technique, the results remain close to 0.

In comparison to embedding-based methods outlined in Table~\ref{tbl:open-wikitable-baseline}, where BM25 computes sparse representations, and BGE-M3 produces dense vectors, it is important to note that these baselines are applicable only to \openwikiselect since it is the only dataset with a single reference table. For this dataset, we use the top-1 predicted table as the final answer. However, for the other two datasets, where the ground truth labels contain multiple tables, embedding-based methods cannot handle such cases effectively. We observe that \modelname improves accuracy from 0.711 and 0.778 to 0.919, demonstrating superior data selection capabilities.
