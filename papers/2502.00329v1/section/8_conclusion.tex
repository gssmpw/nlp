\section{Conclusion and and Future Work}
This work takes an initial step toward developing an expert analytics  model. We have taken the approach of curating a "textbook" that contains data to facilitate supervised instruction-tuning for a set of tasks that have not been considered in the past (e.g., \texttotable, \tabletotext, \datadiscovery), specifically in the context of foundation models for tabular data or analytics.  We argue that curating datasets and establishing benchmarks are crucial steps for advancing the next generation of LLMs for data analytics. While this is a laborious undertaking, it is an essential one. 
%
We believe that future work can focus on the following:

\para{RAG Systems and Tool Usage} Integrating retrieval-augmented generation (RAG) ~\citep{lewis2020rag} with \modelname to inject more fine-grained context or additional knowledge to the model is a promising direction. In addition, training the model to use advanced analytics tools~\citep{dseval, InfiAgent-DABench, datainterpreter} is crucial to tackle more complex tasks.

\para{Improved Training \& Evaluation Benchmarks} The human-annotated \wikipage benchmark is still in its early stages. While it is already useful to evaluate the current version of \modelname, the benchmark does not incorporate image or icon data found in Wiki pages. Second, we can  annotate the benchmark with step-wise supervision~\citep{lightman2024lets} for answering the questions. This type of annotations is useful to improve the model's reasoning ability. We are addressing the issues and planning a new release of the benchmark.

