\subsection{\texttosql}
\label{sec:exp_text2sql}

\para{Datasets}
We evaluate model performance on the development sets of two public benchmarks, BIRD~\citep{bird} and Spider~\citep{yu-etal-2018-spider}, consisting of 1,534 and 1,034 examples, respectively.

\para{Evaluation Metrics} 
We use \textit{execution accuracy}~\cite{bird} as our primary evaluation metric. This measure evaluates whether the execution results of the predicted SQL query exact match those of the gold standard SQL query.  To compute execution accuracy, we adopt the evaluation script from the BIRD codebase.

\para{Task Prompts}  In this task, the model, acting as an SQL expert, receives the database schema and a question and outputs an appropriate SQL query to retrieve the information that answers the question.
To ensure a fair comparison of the modelâ€™s core capability in SQL generation, we adopt a standard zero-shot \texttosql prompt rather than optimizing for maximum accuracy with demonstrations, chain-of-thought reasoning, or data pre-processing techniques. Our zero-shot prompt consists of three main components: 
(1) \textit{Task instruction}: an initial prompt introducing the \texttosql task; 
(2) \textit{Data}: The table schema, including table names, column names, data types, as well as domain-specific knowledge when applicable (for the BIRD dataset only); 
and (3) \textit{Question}: the question for which a SQL statement needs to be generated. Note that we do not include any sample cell values in this prompt.


\begin{table*}[t!]
\caption{The accuracy scores (\%) on different model variants across all the datasets. We test two versions of Chapter 1 data: one with raw plain text and the other with instruction-response pairs derived from it. "Chapter1-Instructed+2+3" means that the model was first trained on the instructed version of Chapter 1 data and followed by training on Chapters 2 and 3. }
\vspace{-2mm}
\label{tbl:ablation}
\small
\begin{tabular}{r || ccc|| ccc|| cc}
\toprule[1.0pt]
 \multirow{4}{*}{\textbf{Model Variants}} & \multicolumn{3}{c||}{\mmlu} & \multicolumn{3}{c||}{\datadiscovery} & \multicolumn{2}{c}{\texttosql} \\  \cmidrule{2-9}
 & \texttt{MCQ-DA} & \texttt{MCQ-DB} & \texttt{MCQ-ML} & \begin{tabular}[c]{@{}c@{}} \texttt{BIRD-TS} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \texttt{Open-Wiki} \\ \texttt{Table-TS} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} \texttt{WikiPage-TS} \end{tabular}  &  \begin{tabular}[c]{@{}c@{}} \texttt{Spider-dev} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} \texttt{BIRD-dev} \end{tabular} \\ \midrule \midrule
  Base Model & 68.7 & 71.5& 72.7 & 41.2 & 29.3 & 28.8 \tiny{(FE)} & 65.2 & 27.0 \\ \midrule
  Chapter1{\footnotesize -PlainText} & 65.2   & 68.6   & 63.5   & 4.6 \tiny{(FE)} & 0 \tiny{(LCE)} & 0 \tiny{(LCE)} & 6.5 \tiny{(FE)} & 26.7 \tiny{(FE)} \\
  Chapter1{\footnotesize -Instructed} & 68.1  & 73.2  & 71.4  & 50.1 & 80.5 \tiny{(FE)}  & 23.1 \tiny{(FE)} & 68.3 & 29.1  \\
  Chapter1{\footnotesize -Instructed}+3  & 70.5  & 72.0  & 72.1  & 78.0 & 89.9 & 46.2 & 77.2 & 35.6  \\ 
  Chapter2+3  & 73.5 & 73.6   & 73.7   & 79.3 & 92.8 & 53.8 & 74.2 & 33.8 \\
  Chapter1{\footnotesize -PlainText}+2+3 & 75.6  & 72.4  & 69.2  & 59.8 & 86.2 & 27.9 & 74.4  &  34.3  \\ \midrule
  CoddLLM -- Chapter1{\footnotesize -Instructed}+2+3  & \textbf{77.1}  & \textbf{77.6}  & \textbf{74.3}  & \textbf{78.2} & \textbf{91.9} & \textbf{55.8} & \textbf{78.1} & \textbf{37.1} \\
\bottomrule[1.0pt]
\end{tabular}
\end{table*}


\noindent An example prompt is provided below:
\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{Verbatim}
CREATE TABLE `Country` (
	`id`    INTEGER PRIMARY KEY AUTOINCREMENT,
	`name`  TEXT UNIQUE
) 
CREATE TABLE "Team" (
...

(Optional) -- External Knowledge: Perform better in crossing 
actions refers to MAX(crossing)

-- Using valid SQLite (and understanding External Knowledge), 
answer the following questions for the tables provided above.
# Question
Who are the top 5 players who perform better in crossing actions?
Generate the SQL within the <SQL> tag.
\end{Verbatim}
\end{tcolorbox}

We opt for this straightforward prompt to enable a fair comparison of the \texttosql capabilities without introducing complexities, such as multi-step reasoning. As noted by ~\citet{wretblad2024understandingeffectsnoisetexttosql} and ~\citet{kapoor2024aiagentsmatter}, such complexities can introduce unnecessary overhead, increase inference costs, and risk overfitting to specific dataset patterns. Nevertheless, as demonstrated in other tasks, \modelname effectively follows instructions and can be paired with complementary prompt engineering techniques when needed. 

\para{Main Results and Analysis} 
\modelname demonstrates strong performance in \texttosql, achieving an 8.9\% improvement over the zero-shot GPT-4o setting on Spider and a 7.8\% improvement on BIRD using the same prompt. Notably, GPT-4o, along with GPT-3.5-Turbo, GPT-4o-mini, and Mixtral-8x7B, struggled to adhere to the formatting instruction to "generate the SQL within the <SQL> tag". To address this, we apply an additional \verb|```sql```| code block template to extract more answers when the initial XML tag format fails. This augmented answer extraction resulted in execution accuracies of 0.717 on Spider and 0.344 on BIRD for GPT-4o.
Additionally, \modelname surpassed the base Mistral-Nemo model with improvements of 19.8\% on Spider and 37.4\% on BIRD.

Note that the accuracy scores in Table~\ref{tbl:exp-main} are not directly comparable to those on the public leaderboard. Our goal is to fairly assess the model's core capability in generating SQL queries rather than optimizing for higher scores across different datasets. We leave the exploration of using \modelname to generate more accurate SQL queries for future work. This includes providing demonstrations for in-context-learning~\citep{gao2024xiyan,pourreza2024chase}, generating multiple SQL candidates and then refining the best one~\citep{gao2024xiyan}, encouraging chain-of-thought reasoning~\citep{pourreza2024chase}, and applying schema pruning pre-processing methods~\citep{talaei2024chess}, which are used by leaderboard submissions.

Notably, BIRD is harder than Spider, involving more complex schema, queries that require the retrieval and joining of multiple tables, as well as incorporation of external knowledge. Our error analysis reveals that \modelname outperforms the base model by demonstrating (1) improved detection and generation of table joins using foreign key relationships,  
and (2) the ability to combine hints from the given external knowledge pieces with table schema.
For example, the following question from the BIRD dataset requires correctly joining the "schools" table with the "frpm" (an abbreviated name) table using the appropriate foreign key relationship. The base model fails to perform this task and overlooks the hint from the given domain knowledge. In contrast, \modelname generates a correct SQL query:

\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{Verbatim}
# Question
Please list the zip code of all the \textbf{charter schools} in Fresno 
County Office of Education.
# Domain Knowledge
\textbf{Charter schools} refers to \textbf{`Charter School (Y/N)` = 1} 
in the table frpm

# Prediction by base model
SELECT Zip FROM schools WHERE \textbf{Charter = 1} 
AND County = 'Fresno' AND District = 'Fresno County [...]]';

# Prediction by CoddLLM
SELECT DISTINCT s.Zip  FROM \textbf{schools s}
\textbf{JOIN frpm p ON s.CDSCode = p.CDSCode} 
WHERE s.District = 'Fresno County [...]' 
AND \textbf{p.`Charter School (Y/N)` = 1;}
\end{Verbatim}
\end{tcolorbox}
