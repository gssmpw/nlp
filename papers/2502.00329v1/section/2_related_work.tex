\section{Related Work}
\label{sec:related_work}

% This section positions \modelname in the context of LLMs for data analytics tasks and other domain-specific models.


\subsection{LLMs for Data Analytics}
LLMs are revolutionizing data management~\citep{wehrsteintowards} and analysis ~\citep{zhou2024llm}, and enabling new capabilities~\citep{fernandez2023large} across various tasks like data discovery~\citep{chorus}, metadata enrichment~\citep{nameguess,feuer2023archetype}, SQL query synthesis~\citep{zhang2024benchmarking}, and entity matching~\citep{zhang2024directions,peeters2023entity,zhang2024anymatch}.  
Several studies have employed "prompt engineering" techniques, carefully optimizing task instructions and few-shot examples to tackle specific tasks~\citep{llm_data_wrangling2022,peeters2023entity,chorus}. This approach is further enhanced by strategies like retrieval-augmented generation (RAG)~\citep{zhao2024chat2data}, which reduces hallucinations by incorporating domain-specific knowledge, and vector databases that enhance efficiency through semantic search capabilities~\citep{patel2024lotus}. 
%
LLM-based agents go beyond simple automation by LLMs to planning, writing code, executing it in an external sandbox, and interpreting results to solve complex data science~\citep{ds1000, dseval} and analysis challenges~\citep{InfiAgent-DABench,cao2024spider2}.
%
In addition to prompting LLMs without updating model weights, task-specific fine-tuning techniques have been widely adopted to boost performance on specific tasks~\citep{korini2024column,zhang2024anymatch}. 
%
Domain-specific fine-tuned models, like those for table understanding~\citep{table-gpt,su2024tablegpt2} and data wrangling~\citep{zhang2024directions, zhang2023jellyfish}, have been developed to enhance LLM performance across various tasks. 
%
Our proposed \modelname belongs to this category but stands out by leveraging a broader range of training tasks and examples. It focuses on understanding complex data relationships and specializes in natural language query-based tasks that require integrating data from multiple sources.


\subsection{Domain-specific Foundation Models}
Unlike training from scratch~\citep{wu2023bloomberggpt}, post-training is a commonly-used approach to build a domain-specific model across diverse fields such as mathematics~\citep{yue2024mammoth}, science~\citep{taylor2023galactica}, code~\citep{gunasekar2023textbooks}, finance~\citep{wu2023bloomberggpt}, and medicine~\citep{wu2024pmc}. Two
main approaches are continual domain-adaptive pretraining~\citep{mendieta2023towards,xie2023efficient} and instruction tuning~\citep{zhang2023instruction}. 
Continual pretraining involves training LLMs on large-scale, domain-specific text corpora. While this approach enriches the model with domain-specific knowledge, it may also degrade its ability to follow instructions effectively~\citep{ke2023continual}.
%
Instruction tuning applies the loss to well-prepared answers, necessitating a large number of input-output instruction pairs. 
%
MAmmoTH2~\citep{yue2024mammoth2} extracts instruction-response pairs from large-scale web corpus to enhance the model's reasoning capabilities. SciInstruct~\citep{zhangsciinstruct} employs a self-reflective instruction annotation approach to generate step-by-step reasoning for unlabeled scientific questions. Magicoder~\citep{wei2024magicoder} utilizes open-source code snippets to generate a diverse set of instructional data for coding tasks.
\modelname also applies instruction tuning to our well-curated, large analytics-specific training corpus. Unlike existing methods for creating instructional data, we leverage reference documents to synthesize large-scale, high-quality instruction-response pairs.