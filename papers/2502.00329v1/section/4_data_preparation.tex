\section{Training Corpus Creation}\label{sec:data_curation}
We curate a large and diverse collection of analytics-specific training corpus with three specific objectives: (1) ensuring broad coverage of various data analytics knowledge concepts, (2) improving the model's comprehension of database schema and table content, and (3) incorporating task-specific input-output examples to enhance the model's capabilities in solving real-world analytics problems.

The training corpus is formulated as akin to a textbook, presenting each data component as a distinct chapter. 
%
\emph{Chapter 1} contains vast knowledge filtered from web-crawled data using a purposely trained classifier. These knowledge elements are formatted into the input-output pairs to enhance the model’s learning efficiency.
%
\emph{Chapter 2} expands on this foundational stack by introducing two new tasks: 1) \texttotable, which involves designing a schema based on a provided scenario description, and 2) \tabletotext, which generates a text description for each row in a given table. These tasks enhance the model's ability to understand and translate between the modalities of natural language text and tabular data.
%
\emph{Chapter 3} includes \datadiscovery and
\texttosql as representative downstream analytics tasks of the real world.


\subsection{Chapter 1 - Analytics-specific Knowledge} 
\label{sec:chapter1}

Training a domain-specific LLM requires a vast corpus of relevant knowledge. However, well-organized datasets on data analytics are still scarce. Recent studies have explored the use of seed data to prompt LLMs in order to expand domain-specific datasets~\citep{amini2019mathqa,zhou2024llm,yumetamath}. While this approach shows promise, synthesized data that lacks proper references often exhibits significant biases, lacks diversity, and is prone to hallucinations~\cite {yue2024mammoth2}.
%
To address these challenges, we use a three-step pipeline to curate large-scale analytics-related instruction-response pairs (see Figure ~\ref{fig:chapt1-pipeline}): 
\begin{enumerate}
    \item \textbf{Filtering:} Identifying and extracting data analytics-related documents -- such as finance analysis, sales prediction, and code-related data -- from large-scale web sources;
    \item \textbf{Instruction Creation}: Converting  plain text documents into question-answer pairs via extraction and synthesis;
    \item \textbf{Assessment:} Evaluating the extracted QA pairs to eliminate low-scored examples and enhance dataset quality.
\end{enumerate}

\begin{figure}[tb]
 \centering
 \includegraphics[width=\linewidth]{figures/pipeline.pdf}
 \caption{Building Chapter 1 data. Step 1 (the top
and middle figures): We first train a document classifier using annotations from a LLM. After training, we use the classifier to filter documents related to data analytics from the \fineweb dataset. Step 2 and Step 3 (the bottom figure): The filtered documents are then converted into question-answer pairs using a rule-based extractor or a LLM-based synthesizer. Finally, we adopt LLM-as-a-judge to eliminate low-quality examples.}
 \label{fig:chapt1-pipeline}
 \vspace{-2mm}
\end{figure} 

\subsubsection{Step 1: Filtering Analytics-specific Documents}
We use the \fineweb dataset~\citep{lozhkov2024fineweb-edu} as our source, which contains 1.3 trillion tokens of educational-quality web pages. Due to the dataset's multi-domain content, manually identifying data analytics-related documents presents significant challenges. To address this, we develop a model-based filtering approach to extract relevant documents. This process consists of three stages: a) \emph{LLM as a document rater}, by leveraging LLMs to assess document relevance in order to obtain labels; b) \emph{Training a content classifier}, by developing a supervised model based on rated samples; c) \emph{Deploying the trained filter} by applying the classifier to extract analytics-related documents. We now detail these stages.

\para{LLM as a Document Rater} 
Training the document classifier requires both positive samples (high-quality documents relevant to data analytics) and negative samples (low-quality or minimally relevant documents). 
Inspired by the effectiveness of LLM-as-a-judge in automatic text evaluation~\citep{gunasekar2023textbooks}, we sample approximately 0.1\% of the \fineweb dataset and use \claudetf to score each document’s relevance to data analytics on a scale of 1 to 5. 

The following example illustrates how \claudetf evaluates a document based on its textual quality and relevance to data analytics.
The document consists of a brief explanation of SQL statements, describing their structure and syntax. The rater assigns a score of 5, indicating high relevance to SQL and data analytics.

\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{verbatim}
# Document: 
SQL statements have a specific structure and syntax that must be 
followed precisely. A basic SQL statement is composed of several 
Clauses:
```sql
SELECT column1, column2
FROM table_name
WHERE condition;
```

# Rater Output:
This text clearly focuses on SQL, which is a fundamental tool in 
data analytics...
Score: 5 
\end{verbatim}
\end{tcolorbox}


\para{Training a Document Classifier} After obtaining the annotated data, we train a Transformer-based text classification model. Considering that most documents are quite long -- some exceed 2,000 tokens, we build the classifier on top of LongFormer~\citep{beltagy2020longformer}, a Transformer model supporting a maximum context length of 4,096 tokens. The transformer categorizes the documents into five classes according to the annotated scores (1 to 5). After training, the classifier achieves 78\% test accuracy and 87\% recall in retrieving high-rating documents, demonstrating its effectiveness in filtering analytics-related content from the raw corpus.

\para{Deploying the trained filter on \fineweb} We then apply the trained classifier to filter documents in the \fineweb dataset, selecting those with a score of $4$ or higher.
This filtering process reduces the dataset to 4.9\% of its original size, consisting of over 5.8 million documents and 1.9 billion tokens.
% 
The following passages present an example document from \fineweb, which provides a clear and accurate introduction to the concept of the "foreign key", a crucial concept in databases. Due to its relevance to data analytics, it is classified as "score = 5" and included in Chapter 1.

\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{Verbatim}
# An example of a filtered document (score = 5)

In database design, the concept of relationships between tables 
is crucial for maintaining data integrity and ensuring efficient
data retrieval. When two tables need to be linked, a special type 
of attribute is used to establish this connection. This attribute, 
known as a foreign key, serves as a reference to the primary key
of another table.
\end{Verbatim}
\end{tcolorbox} 

\subsubsection{Step 2: Instruction Extraction/Synthesis}
Recent studies have demonstrated the effectiveness of instruction data in post-training LLMs, showing that LLMs perform better when trained on instruction-response formatted data rather than plain text with the same semantic meaning~\citep{instruction_pre_training}. Motivated by the extraction and refinement strategy for extracting mathematical contents from raw documents~\citep{yue2024mammoth2}, we propose an \emph{extraction and synthesis} strategy. For each filtered document, we either extract native question-answer pairs using predefined rules or generate synthetic question-answer pairs by grounding them in the documents and ensuring explicit references to the content. The goal is to construct a synthetic instructed version through content distillation and then remove the plain text documents to form the final dataset.


\para{Extraction} We first use regular expressions to identify potential indicators of QA pairs -- "[Qq]uestion:" and "[Aa]nswer:" -- to classify the 5.8M collected analytics-related documents. Among these, about 2.8K documents contained both question and answer keywords. We then leverage \claudetf to extract question-answer pairs from these documents, yielding a total of 46K QA pairs, as a single document might contain multiple QA pairs. After conducting a pilot study with two human annotators on 300 samples, we observed that 97\% of the extracted pairs were deemed valid and, therefore, can be used as our training corpus.

\para{Synthesis} For the remaining documents where explicit question-answer pairs cannot be extracted, we prompt \claudetf to synthesize question-answer pairs of varying difficulty levels, ensuring the generation of relevant QA pairs of high quality. Specifically, we require that the generated QA pairs meet the following criteria: 1) Varying difficulty: Include questions ranging from basic common sense to advanced data-related topics. 2) Context-dependency: Ensure simple questions rely on common knowledge, while complex ones require provided context for answers. 3) Diverse format: Use varied question types beyond "How" and "What", and encourage longer, detailed questions and answers where possible. Following this QA generation pipeline, we obtain about 8.8M QA pairs. Below is an example of the synthesized question-answer pair and the original document content, where the original document is a passage about the two outputs from a forecasting operation in Excel.

\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{Verbatim}

# Original document:
"There are two outputs generated from the forecasting operation. 
Firstly, \textbf{a worksheet is added to the workbook, which shows the} 
\textbf{projected values as a dashed line}. Secondly, \textbf{the projected values} 
\textbf{are added sequentially to the original table.} It should be noted 
that the chart is actually generated from the original table and 
the forecasts that were appended as part of data mining operation."

# A question-answer pair
"question": "What are the two outputs generated from  the 
forecasting operation in the Excel add-in?"
"answer": "The two outputs generated from the forecasting 
operation in the Excel add-in are: 1)\textbf{ a worksheet added to the} 
\textbf{workbook showing the projected values as a dashed line}, and 2) 
\textbf{the projected values appended sequentially to the original table.}"
\end{Verbatim}
\end{tcolorbox}

\subsubsection{Step 3: Assessment}
After obtaining all QA pairs, we employ a widely-used LLM-as-a-Judge, \texttt{Prometheus-eval}~\citep{kim2024prometheus}, to conduct two rounds of filtering for each QA pair, further improving the quality of Chapter 1 data. In the first round of screening, we focus on the completeness of Questions and Answers, ensuring that all information necessary to arrive at the answer is contained within the Question text. In the second round of screening, we further evaluate the accuracy of the answers, ensuring that only correct question-answer pairs are retained in the training dataset. Eventually, we curate a dataset of 8.8 million instruction-response pairs, comprising a total of 0.9 billion training tokens.


\subsection{Chapter 2 - Table and Text Alignment}
\label{sec:chapter2}
In this chapter of the textbook, we design two tasks aimed at enhancing the LLM's ability to understand and process both tabular and textual data. These tasks are designed to facilitate the model's ability to translate between textual and structured data modalities.

\subsubsection{\texttotable}
The objective of this task is to generate a table schema from an input description of a database or information system (e.g., a credit card transaction system). Training on this task enables the model to understand how different pieces of information relate to each other within a structured representation. The underlying hypothesis here is that by learning to design schemas, the model will also develop the ability to interpret schemas when it receives them as inputs~\citep{schemamatch}.

The main challenge in acquiring scenario descriptions and schema pairs stems from their infrequent co-occurrence within a single database or document. To tackle this issue, we generate synthetic pairs by translating well-structured schemas into multiple scenario descriptions using LLMs. The data generation steps begin with filtering 4,399 high-quality schemas from the overall 221,171 schemas in SchemaPile~\citep{schemapile}. We use \claudetf to assess the quality of the data. After classification, three different scenario descriptions are generated for each schema. These generated descriptions are then evaluated using Prometheus-eval~\citep{kim2024prometheus} based on the criterion, ``Is the scenario description concise and relevant?''. Only descriptions that receive a score higher than four are retained. As a result, we generate 4.8 thousand examples (refer to Table~\ref{tbl:train-corpus}).



\subsubsection{Row-to-Text} 

Given a table and contextual data, the goal is to generate a detailed text description for a given row in the table. Training a model on a large number of \tabletotext examples improves its ability to convert structured tabular data into meaningful, fluent, and accurate text. Below, we present an input-output example from the sports domain. The input includes a task instruction and a single row from a table, along with the page title, section title, and caption. The output is a detailed text description. 

\begin{tcolorbox}[left=1pt, right=0pt, top=1pt, bottom=1pt]
\begin{Verbatim}
# Input:
Write a detailed description for the row in the table.
Enclose the description in <row_description> tags.

Page Title: Badminton at the Pan American Games
Section Title: Medal table
Caption: Medal table
| Rank |    Nation     | Gold | Silver |  Bronze | Total |
| ---- | ------------- | ---- | ------ | ------- | ----- |
\textbf{|  1   | Canada        |  16  |   16   |    11   |   43  |}

# Output:
<row_description>
\textbf{Canada} won \textbf{16} gold medals, \textbf{16} silver medals, and \textbf{11} bronze medals, 
totaling \textbf{43} medals in badminton events at the Pan American Games,
ranking \textbf{first} among all participating nations.
</row_description>
\end{Verbatim}
\end{tcolorbox}

The key idea behind the data generation process is to take real-world tables as input, leverage LLMs to generate row descriptions, and then apply a filtering process to keep high-scored examples. We utilize tens of thousands of WikiTables from the open-wikitable dataset~\citep{open_wikitable}. \claudetf is employed to generate descriptions for each row within these tables. To ensure the quality and accuracy of these descriptions, we use an entailment classifier to filter tables, keeping only those where all row descriptions were classified as either entailment or neutral. This filtering process guarantees that each generated description aligns with the original table entries. Overall, we generate 46.3 thousand \tabletotext pairs and 10.9 million tokens (refer to Table~\ref{tbl:train-corpus} for additional data statistics).

\subsection{Chapter 3 - Data Analytics Tasks}
\label{sec:chapter3}
Chapter 3 presents carefully curated training examples designed to enhance model performance on downstream analytics tasks, with a focus on real-world applications. We prioritize two critical tasks: \datadiscovery and \texttosql conversion. These foundational tasks are selected to provide our model with a comprehensive understanding of datasets, enabling and augmenting their capacity to execute more complex analytics tasks.

\subsubsection{\datadiscovery}
\label{sec:chapter3_table_selection}
Relevant data selection from large collections of datasets is a long-standing challenge that organizations struggle with to this day~\citep{hulsebos2024took}. In this task, we focus on \textit{question-based search}, where the goal is to identify one or more data pieces that contain the necessary information to answer a user-specified question. The data can be structured or unstructured, often requiring integration from multiple sources. In this context, the ability to recognize relationships between datasets can significantly enhance both the accuracy and efficiency of task completion.

%
To create training examples, we use the training set of BIRD and Open-WikiTable and convert these datasets to serve the needs of the \datadiscovery task.  Section~\ref{sec:table_selection_prompts} gives details on task examples.

\para{Leveraging BIRD for \datadiscovery} 
The BIRD dataset~\citep{bird} was originally designed for text-to-SQL tasks, where each example includes a question, multiple tables from a database, and a corresponding ground truth SQL query. We select 8,954 questions from its training set and include all tables from the same database as potential candidates. We use only the schemas to highlight table relationships, without using any table contents. We identify the ground truth tables from their associated gold SQL queries. 


\para{Leveraging Open-WikiTable for \datadiscovery} The Open-The WikiTable~\cite{open_wikitable} dataset was designed for open-domain question answering over tables. Each question is paired with its corresponding reference ground truth table. For each question, we use BM25~\citep{bm25}, a term-based sparse encoding ranking algorithm, to retrieve ten tables from a pool of 24,680 WikiTables as the candidate table set. These ten retrieved tables are ranked based on their relevance to the query, with each table sharing some degree of term-based similarity to the question. However, the retrieved tables may not be directly related to one another. 
We retain 8,484 questions from the training set and ensure that the retrieved ten tables include the gold tables. We provide metadata for the WikiTables, including page titles, section titles, and captions, if available. To ensure scalability for handling large tables, we present only three sampled rows from each table to the LLMs. 

\vspace{-1mm}
\subsubsection{\texttosql}

\texttosql is a well-established data analytical task that aims to convert natural language questions into SQL queries, extracting desired information with proper computation and transformation performed on relational databases. The challenge lies in accurately interpreting the semantics of the input text, mapping it to the appropriate database schema, and generating a syntactically correct SQL query that fulfills the user's intent. 
Unlike traditional methods that rely on predefined templates or rules\citep{li2014constructing}, LLMs have become the predominant tools for this task, as highlighted in recent studies~\citep{gao2024xiyan, pourreza2024chase, talaei2024chess}. Common approaches with LLMs include prompt engineering and task-specific fine-tuning. 
Prompt engineering employs techniques such as few-shot learning and multi-step reasoning without updating the model weight, but sometimes producing incorrect results for complex SQL queries. On the other hand, fine-tuning enhances performance by training the LLM on task-specific data, but it compromises the model's general instruction-following ability. In contrast, \modelname takes a different approach by offering a domain expert model that excels not only in \texttosql but also across other analytical tasks.

We extract a sample of approximately 9K examples from the BIRD~\citep{bird} dataset's training set and 7K examples from the Spider~\citep{yu-etal-2018-spider} dataset's training set. Additionally, we incorporate 105.8K examples from synthetic text-to-SQL~\citep{gretel-synthetic-text-to-sql-2024}.
For each example, we construct the input text prompt by concatenating three components: \textsc{<Schema> <Task Instruction> <Question>}. To represent the schema, we use \textsc{CREATE TABLE} SQL statements for the tables. The output is the SQL query enclosed within the \texttt{<SQL>} tags. For more data processing details, see Section~\ref{sec:exp_text2sql}.
