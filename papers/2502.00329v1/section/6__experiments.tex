\section{Evaluation Tasks and Experiments}

We evaluate \modelname's reasoning capabilities across various analytics tasks, mainly focusing on domain-knowledge testing, table selection, and text-to-SQL tasks. 


\para{LLM Baselines} We compare \modelname with five open-source LLMs, all obtained from Hugging Face: \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{\texttt{Mistral-7B-Instruct-v0.3}}, \href{https://huggingface.co/mistralai/Codestral-22B-v0.1}{\texttt{Codestral-22B-v0.1}}, \href{https://huggingface.co/mistralai/Mistral-Small-Instruct-2409}{\texttt{Mistral-Small-Instruct-2409}}, \href{https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1}{\texttt{Mixtral-} \\ \texttt{8x7B-Instruct-v0.1}}, as well as the base model \href{https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407}{\texttt{Mistral-Nemo-} \\ \texttt{Instruct-2407}}. We use the instruction-tuned versions for all baseline models that have been fine-tuned on extensive general-purpose tasks. Meanwhile, we include three closed-source OpenAI models: \texttt{GPT-3.5-Turbo}, \texttt{GPT-4o-mini}, and \texttt{GPT-4o}, establishing strong baselines. Additionally, we included some task-specific baselines where appropriate. Unless explicitly specified, we report results on \modelname and all the baselines in a zero-shot setting, with no demonstration examples provided during inference.

\begin{table}[t!] 
 \centering
 \caption{Overview of all the evaluation benchmarks. "Exec. Acc." refers to Execution Accuracy. "MCQ" stands for Multiple Choice Questions. We newly introduce three \mmlu datasets and a human-annotated \wikipage dataset. Our training corpus does not include any subset of these four datasets, so we consider them "unseen" datasets.} 
 \label{tbl:all_eval_dataset}
 \vspace{-3mm}
 \resizebox{\columnwidth}{!}
 {
    \begin{tabular}{llccccc c c}
        \toprule[1.2pt]
        & \textbf{Dataset} &  \#Example  & Metric & Sources &  \begin{tabular}[l]{@{}l@{}} Seen/Unseen \\ In Training \end{tabular}  \\
        \midrule
        \multicolumn{3}{l}{\hspace{-.5em} \textit{\mmlu}} \\
        \cmidrule{2-6}
        & MCQ-DB & 882  & Accuracy & New & Unseen \\
        & MCQ-DA & 332  & Accuracy & New & Unseen \\
        & MCQ-ML & 556  & Accuracy & New & Unseen \\
        \midrule
        \multicolumn{3}{l}{\hspace{-.5em} \textit{\datadiscovery}} \\
        \cmidrule{2-6}
        & \birdselect     & 1,534   & Accuracy & Re-Purpose   & Seen \\
        & \begin{tabular}[l]{@{}l@{}} \texttt{Open-Wiki-} \\ \texttt{Table-TS} \end{tabular} & 5,134   & Accuracy & Re-Purpose  & Seen \\
        & \wikipage       & 104     & Accuracy & New  & Unseen \\
        \midrule
        \multicolumn{3}{l}{\hspace{-.5em} \textit{\texttosql}} \\
        \cmidrule{2-6}
        & Spider-dev & 1,034 & Exec. Acc. & Public & Seen \\
        & BIRD-dev   & 1,534 & Exec. Acc. & Public & Seen \\
        \bottomrule[1.2pt]
    \end{tabular}
  }
\end{table}

\para{Evaluation Datasets} Table~\ref{tbl:all_eval_dataset} provides an overview of evaluation benchmarks categorized into three main tasks: \mmlu, \datadiscovery, and \texttosql. It lists datasets used for evaluation, the number of examples in each, the metric applied, their data sources, and whether there are training sets included in the training corpus (i.e., in or out of distribution). We provide a detailed explanation of the data generation procedure in the next sections.


\para{Inference Sampling Hyperparameters}  We experiment with different sampling hyperparameters: temperatures of 0.0, 0.7, and 1.0, and top\_p values of 0.99 and 0.95. Lower temperatures yield more predictable responses, while higher temperatures encourage creativity. The top\_p value used in the nucleus sampling defines the range of tokens considered during generation, with higher values expanding the range. After tuning, we fix the hyperparameters for all subsequent experiments. For \mmlu, \datadiscovery and \texttosql, we set the temperature to be 0.0, 0.7, 1.0 and top\_p as 0.99, 0.95, 1.0, respectively.

\para{Inference and Model Serving} We use the vLLM model-serving framework~\citep{vllm} for inference. First, we convert the saved model checkpoints from Nemo format to Huggingface format using a conversion toolkit. We then deploy the Huggingface-formatted model using vLLM.


\begin{table*}[t!] 
 \centering
 \caption{The accuracy scores (\%) on \mmlu, \datadiscovery, and \texttosql tasks. "Unseen" datasets refer to those training sets that are not observed during \modelname post-training. "FE" (Format Error) indicates that the output answer format does not follow the prompt instructions. Results labeled “(FE)” include additional answer templates to improve extraction recall. "LCE" (Long Context Error) denotes the model's failure to comprehend lengthy input contexts. "CLE"(Context-Length-Exceeded) indicated that the input exceeds the model's maximum context length.} 
 \label{tbl:exp-main}
 \resizebox{\linewidth}{!}{
    \begin{tabular}{rr|ccc|c|| ccc|c || cc | c || c }
 \toprule[1.2pt]
 \multirow{5}{*}{Model} &  \multirow{5}{*}{Size}   & \multicolumn{4}{c||}{\mmlu} & \multicolumn{4}{c||}{\datadiscovery} & \multicolumn{3}{c||}{\texttosql} & \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}} Over- \\ all \end{tabular}} \\  \cmidrule{3-13}
 & & \texttt{MCQ-DA} & \texttt{MCQ-DB} & \texttt{MCQ-ML} & \multirow{2}{*}{Avg.} & \begin{tabular}[c]{@{}c@{}} \texttt{BIRD} \\ \texttt{-TS} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \texttt{Open-Wiki} \\ \texttt{Table-TS} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} \texttt{WikiPage} \\ \texttt{-TS} \end{tabular}  & \multirow{2}{*}{Avg.} &  \begin{tabular}[c]{@{}c@{}} \texttt{Spider} \\ \texttt{-dev} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} \texttt{BIRD} \\ \texttt{-dev} \end{tabular} & \multirow{2}{*}{Avg.} & \\ \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-12}
 & & Unseen & Unseen & Unseen &  & Seen & Seen & Unseen & & Seen & Seen & &  \\
 \midrule \midrule
Mistral-7B    & 7B  & 69.0 & 67.9 & 66.5 & 67.8   &  5.7 \tiny{(FE)} & 41.1 \tiny{(FE)}  & 28.8 \tiny{(FE)}  & 25.2   & 54.3 & 19.1 & 36.7  & 43.2 \\
Codestral     & 22B & 69.0 & 67.8 & 69.6 & 68.8   & 56.9 & 57.6 & 40.4 & 51.6  & 69.9 & 27.8 & 48.9  &  56.4\\
Mistral-Small & 22B & 71.1 & 74.3 & 70.9 & 72.1   & 48.6 & 66.3 & 25.0 & 46.6  & 18.6 & 5.8  & 12.2  & 43.6 \\
Mixtral-8x7B  & 47B & 75.0 & 72.9 & 70.7 & 72.9   & 35.8 & 2.3 \tiny{(FE)}  & 0.0 \tiny{(LCE)}  & 12.7  & 67.9 \tiny{(FE)} & 27.0 \tiny{(FE)} & 47.5 & 44.3 \\
 \midrule
GPT-3.5 Turbo & -   & 75.6 & 73.6 & 72.1 & 73.8   & 61.5 & 60.5 & CLE  & -     & 62.4 \tiny{(FE)} & 29.3 \tiny{(FE)} & 45.9  & - \\
GPT-4o-mini   & -   & 78.6 & 79.7 & 75.0 & 77.8   & 66.1 & 65.5 & 47.1 & 59.6  & 70.4 \tiny{(FE)} & 30.9 \tiny{(FE)} & 50.7  & 62.7 \\
GPT-4o        & -   & \textbf{80.1} & \textbf{82.9} & \textbf{80.0} & 81.0   & 71.7 & 71.1 & \textbf{58.7} & 67.2  & 71.7 \tiny{(FE)} & 34.4 \tiny{(FE)} & 53.1  &  67.1 \\
 \midrule
Mistral-NeMo & 12B & 68.7 & 71.5 & 72.7 & 71.0    & 41.2 & 29.3 & 28.8 \tiny{(FE)}  & 33.1  & 65.2 & 27.0 & 46.1 & 50.1\\
\modelname   & 12B & \textbf{77.1} & \textbf{77.6} & \textbf{74.3} & 76.3   & \textbf{\underline{78.2}} & \textbf{\underline{91.9}} & \textbf{55.8} & 75.3 &  \textbf{\underline{78.1}} & \textbf{\underline{37.1}} & 57.6 &  \textbf{\underline{69.7}} \\
 \midrule
 \multicolumn{2}{l}{ \textit{\footnotesize Improvement (\%) over base}} & 12.2\%$\uparrow$ & 8.5\%$\uparrow$ & 2.2\%$\uparrow$ & 7.6\%$\uparrow$ & 89.8\%$\uparrow$ & 213.7\%$\uparrow$ & 93.7\%$\uparrow$ & 127.5\%$\uparrow$ & 19.8\%$\uparrow$ &  37.4\%$\uparrow$ & 24.9\%$\uparrow$ & 39.3\%$\uparrow$\\
  \multicolumn{2}{l}{ \textit{\footnotesize Improvement (\%) over best}} & 3.7\%$\downarrow$ & 6.4\%$\downarrow$ & 7.1\%$\downarrow$ & 5.8\%$\downarrow$ & 9.1\%$\uparrow$ & 29.3\%$\uparrow$ & 4.9\%$\downarrow$ & 12.1\%$\uparrow$ & 8.9\%$\uparrow$ &  7.8\%$\uparrow$ & 8.6\%$\uparrow$ & 4.0\%$\uparrow$\\

 \bottomrule[1.2pt]
    \end{tabular}
  }
\end{table*}


\para{Results Overview}
Table~\ref{tbl:exp-main} presents the accuracy results across eight datasets, along with the average accuracy scores for each task. Notably, \modelname achieves the highest overall score of 0.697, surpassing the base model by 39.3\% and outperforming the best model (GPT-4o) by 4.0\%.

The evaluation consists of three tasks across eight datasets. Among these, the three \mmlu datasets are completely new. We do not explicitly include any multi-choice questions in the training corpus; however, the synthesized QA pairs in Chapter 1 may contain such question types. For the \datadiscovery task, we add the training sets of \birdselect and \openwikiselect into the training corpus as in Chapter 3 and evaluate on the test sets. Notably, we have not included any examples from \wikipage in the training corpus, make it a total new dataset for the post-trained model. Therefore, we mark \texttt{MCQ-DB}, \texttt{MCQ-DA}, \texttt{MCQ-ML}, and \wikipage as "unseen" datasets and others as "seen" datasets, where a subset of examples are included in the training corpus.
%
From Table~\ref{tbl:exp-main}, we find that  \modelname surpasses GPT-4o on all seen datasets and notably achieves a 213.7\% improvement over the base model on \openwikiselect. Analyzing the results on unseen datasets sheds light on the model's ability to generalize to new tasks or datasets. While \modelname does not always exceed GPT-4o's performance in this category, it nonetheless shows a significant boost over its base model. The largest relative improvement is observed on \wikipage, where it earned a 93.7\% enhancement.
