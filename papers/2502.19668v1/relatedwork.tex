\section{Related Work}
\noindent \textbf{ECG Supervised Learning.} ECG supervised learning (eSL) methods, using CNNs or Transformers in Figure~\ref{fig:related}(a$-$b), achieve high accuracy in cardiovascular disease diagnosis. CNNs excel at capturing spatial and temporal patterns in 1D ECG signals or 2D ECG images \citep{tesfai2022lightweight, degirmenci2022arrhythmic, mashrur2019automatic, huang2022snippet}, while Transformers use attention mechanisms to model global dependencies \citep{natarajan2020wide, jiang2021hadln, he2023transformers}. Despite their strengths, eSLs rely heavily on large-scale datasets with expert-verified annotations, making them costly and impractical for pre-training tasks \citep{strodthoff2020deep}. This dependence limits their scalability and generalizability, particularly when addressing diverse datasets or unseen cardiac conditions.

\noindent \textbf{ECG Self-supervised Learning.} To overcome the annotation bottleneck, ECG self-supervised learning (eSSL) methods have been introduced, enabling representation learning from unannotated ECG signals in Figure~\ref{fig:related}(c$-$d). Contrastive learning frameworks, such as CLOCS and ASTCL \citep{kiyasseh2021clocs, wang2023adversarial}, explore temporal and spatial invariance in ECG data \citep{eldele2021time, chen2020simple, chen2021empirical}. Generative eSSL techniques reconstruct masked segments to capture signal-level features \citep{zhang2022maefe, sawano2022masked, na2024guiding, jinreading}. Despite their successes, eSSLs fail to incorporate clinical semantics from associated medical reports and require fine-tuning for downstream tasks \citep{liu2023improving, liu2023pixmim, he2022masked}, limiting their utility in zero-shot scenarios.

\noindent \textbf{Biomedical Multimodal Learning.} Multimodal learning has advanced significantly in biomedical applications, especially in vision-language pre-training (VLP) frameworks for radiology \citep{liu2023g2d, liu2023m, wan2024med, zhang2023knowledge, wu2023medklip}, which align radiology images with structured knowledge from reports to reduce noise and improve robustness. However, multimodal learning for ECG remains underexplored. Methods like MERL \citep{liu2024zero} and ECG-LM \citep{yangecg} integrate ECG signals and raw text reports but struggle with noise and inconsistencies in unstructured reports. Others, such as KED \citep{tian2024foundation}, use structured labels and contrastive learning strategies but face challenges from label noise and LLM-generated knowledge hallucinations. Our approach addresses these issues by structuring reports into meaningful entities, reducing noise, and aligning them with ECG signals without reliance on LLM-augmented content, minimizing hallucination risks while enabling efficient representation learning and downstream flexibility.

% Methods like MERL \citep{liu2024zero} and ECG-LM \citep{yangecg} integrate ECG signals with unstructured text reports but suffer from noise and inconsistencies. KED \citep{tian2024foundation} leverages structured labels and contrastive learning but faces label noise and hallucinations in LLM-generated knowledge. Our approach mitigates these issues by structuring reports into entities, reducing noise, and aligning them with ECG signals, ensuring efficient learning without LLM-induced hallucinations.