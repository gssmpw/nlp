\section{Related Work}
\noindent \textbf{ECG Supervised Learning.} ECG supervised learning (eSL) methods, using CNNs or Transformers in Figure~\ref{fig:related}(a$-$b), achieve high accuracy in cardiovascular disease diagnosis. CNNs excel at capturing spatial and temporal patterns in 1D ECG signals or 2D ECG images ____, while Transformers use attention mechanisms to model global dependencies ____. Despite their strengths, eSLs rely heavily on large-scale datasets with expert-verified annotations, making them costly and impractical for pre-training tasks ____. This dependence limits their scalability and generalizability, particularly when addressing diverse datasets or unseen cardiac conditions.

\noindent \textbf{ECG Self-supervised Learning.} To overcome the annotation bottleneck, ECG self-supervised learning (eSSL) methods have been introduced, enabling representation learning from unannotated ECG signals in Figure~\ref{fig:related}(c$-$d). Contrastive learning frameworks, such as CLOCS and ASTCL ____, explore temporal and spatial invariance in ECG data ____. Generative eSSL techniques reconstruct masked segments to capture signal-level features ____. Despite their successes, eSSLs fail to incorporate clinical semantics from associated medical reports and require fine-tuning for downstream tasks ____, limiting their utility in zero-shot scenarios.

\noindent \textbf{Biomedical Multimodal Learning.} Multimodal learning has advanced significantly in biomedical applications, especially in vision-language pre-training (VLP) frameworks for radiology ____, which align radiology images with structured knowledge from reports to reduce noise and improve robustness. However, multimodal learning for ECG remains underexplored. Methods like MERL ____ and ECG-LM ____ integrate ECG signals and raw text reports but struggle with noise and inconsistencies in unstructured reports. Others, such as KED ____, use structured labels and contrastive learning strategies but face challenges from label noise and LLM-generated knowledge hallucinations. Our approach addresses these issues by structuring reports into meaningful entities, reducing noise, and aligning them with ECG signals without reliance on LLM-augmented content, minimizing hallucination risks while enabling efficient representation learning and downstream flexibility.

% Methods like MERL ____ and ECG-LM ____ integrate ECG signals with unstructured text reports but suffer from noise and inconsistencies. KED ____ leverages structured labels and contrastive learning but faces label noise and hallucinations in LLM-generated knowledge. Our approach mitigates these issues by structuring reports into entities, reducing noise, and aligning them with ECG signals, ensuring efficient learning without LLM-induced hallucinations.