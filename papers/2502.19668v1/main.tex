% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{
%   Mingsheng Cai \\
%   School of Informatics \\
%   University of Edinburgh \\
%   \texttt{m.cai@ed.ac.uk} \\\And
%   Che Liu \\
%   Data Science Institute \\
%   Imperial College London \\
%   \texttt{che.liu21@imperial.ac.uk} \\\And
%   Rossella Arcucci \\
%   Data Science Institute \\
%   Imperial College London \\
%   \texttt{r.arcucci@imperial.ac.uk}
% }

\author{
 \textbf{Mingsheng Cai\textsuperscript{1,3}},
 \textbf{Jiuming Jiang\textsuperscript{1,3}},
 \textbf{Wenhao Huang\textsuperscript{2}},
 \textbf{Che Liu\textsuperscript{3\thanks{Correspondence: \href{mailto:che.liu21@imperial.ac.uk}{che.liu21@imperial.ac.uk}}}},
 \textbf{Rossella Arcucci\textsuperscript{3}}
% % \\
% %  \textbf{Fifth Author\textsuperscript{1,2}},
% %  \textbf{Sixth Author\textsuperscript{1}},
% %  \textbf{Seventh Author\textsuperscript{1}},
% %  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% % \\
% %  \textbf{Ninth Author\textsuperscript{1}},
% %  \textbf{Tenth Author\textsuperscript{1}},
% %  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
% %  \textbf{Twelfth Author\textsuperscript{1}},
% % \\
% %  \textbf{Thirteenth Author\textsuperscript{3}},
% %  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
% %  \textbf{Fifteenth Author\textsuperscript{1}},
% %  \textbf{Sixteenth Author\textsuperscript{1}},
% % \\
% %  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
% %  \textbf{Eighteenth Author\textsuperscript{3,4}},
% %  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
% %  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1}The University of Edinburgh,
 \textsuperscript{2}Shenzhen Yinwang Intelligent Technology Co., Ltd, \\
 \textsuperscript{3}Imperial College London
 % \textsuperscript{4}Affiliation 4,
 % \textsuperscript{5}Affiliation 5
\\
  \texttt{\{m.cai, jiuming.jiang\}@ed.ac.uk}\textsuperscript{1},
  \texttt{huangwenhao@yinwang.com}\textsuperscript{2}, \\
  \texttt{\{mingsheng.cai23, jiuming.jiang23, che.liu21, r.arcucci\}@imperial.ac.uk}\textsuperscript{3} \\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:che.liu21@imperial.ac.uk}{che.liu21@imperial.ac.uk}
 % }
}

\begin{document}

\maketitle

\begin{abstract}

Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning.
%
To address these challenges, we propose \textbf{SuPreME}, a \textbf{Su}pervised \textbf{Pre}-training framework for \textbf{M}ultimodal \textbf{E}CG representation learning. SuPreME applies Large Language Models (LLMs) to extract structured clinical entities from free-text ECG reports, filter out noise and irrelevant content, enhance clinical representation learning, and build a high-quality, fine-grained labeled dataset. By using text-based cardiac queries instead of traditional categorical labels, SuPreME enables zero-shot classification of unseen diseases without additional fine-tuning.
%
We evaluate SuPreME on six downstream datasets covering 127 cardiac conditions, achieving superior zero-shot AUC performance over state-of-the-art eSSL and multimodal methods by over 1.96\%\footnote{All code and data will be released upon acceptance.}. Results demonstrate the effectiveness of SuPreME in leveraging structured, clinically relevant knowledge for high-quality ECG representations.

\end{abstract}

\section{Introduction}

Supervised learning methods have proven effective in classifying cardiac conditions using Electrocardiogram (ECG), a widely utilized clinical tool for monitoring the heart's electrical activity \citep{SPN,SPNv2}. However, these methods typically rely on large-scale, high-quality annotated datasets, which are costly to create and difficult to scale.

To reduce dependence on annotations, recent advancements in ECG self-supervised learning (eSSL) have enabled the extraction of representative features from large-scale unannotated ECGs using contrastive or generative tasks \citep{eldele2021time, kiyasseh2021clocs, na2024guiding}. Despite their promise, these methods often rely on strong signal-level augmentations that may distort the semantic integrity of the signal and require complex pretext task designs \citep{liu2024zero, kiyasseh2021clocs}.
%
Multimodal learning approaches \citep{liu2024zero, li2024frozen} have also been proposed to learn ECG representations by leveraging free-text ECG reports. However, these methods face challenges due to noise in textual data and the complexities of language grammar, which can hinder learning efficiency \citep{wu2023medklip}.

\begin{figure}[tp!]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/framework.pdf}
    % \vspace{-20pt}
    \caption{Overview of SuPreME framework, including (a) ECG clinical entity extraction with LLMs, (b) pre-training framework for SuPreME, and (c) zero-shot prompted evaluation.}
    \label{fig:framework}
    % \vspace{-5pt}
\end{figure}

To address these limitations and develop a scalable, simple, and effective ECG pre-training framework, we propose \textbf{SuPreME}, a \textbf{Su}pervised \textbf{Pre}-training framework for \textbf{M}ultimodal \textbf{E}CG representation learning, as illustrated in Figure~\ref{fig:framework}. Our contributions are threefold:
%
\textbf{(a)} We introduce an automated pipeline for extracting high-quality clinical entities from raw ECG reports via LLMs and professional medical databases, yielding a fine-grained ECG dataset with 295 standardized medical terminologies. This approach requires no manual labeling, ensuring both scalability and consistency while capturing richer semantics than coarse-grained or free-text labels.
%
\textbf{(b)} Building on these entities, we propose SuPreME, which avoids data augmentation and complex pretext tasks by directly aligning ECG signals with structured entity labels. This approach surpasses prior multimodal methods (e.g., MERL) that rely on raw free-text, underscoring the benefits of explicit entity-level supervision.
%
\textbf{(c)} We train SuPreME on 771,500 ECG signals paired with extracted entities from the MIMIC-IV-ECG dataset \citep{gow2023mimic}. On six downstream datasets, it significantly outperforms state-of-the-art multimodal methods \citep{liu2024zero} in zero-shot scenario, even those relying on human-assisted prompt engineering. SuPreME also surpasses fully fine-tuned eSSLs using \textbf{only 20\%} of the pre-training data in a purely zero-shot setting, underscoring its simplicity, effectiveness, and high data efficiency.

\begin{figure}[tp!]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/related.pdf}
    % \vspace{-20pt}
    \caption{Current ECG representation learning methods, including (a) CNN-based supervised learning, (b) Transformer-based supervised learning, (c) contrastive learning, and (d) generative learning.}
    \label{fig:related}
    \vspace{-5pt}
\end{figure}

\section{Related Work}

\noindent \textbf{ECG Supervised Learning.} ECG supervised learning (eSL) methods, using CNNs or Transformers in Figure~\ref{fig:related}(a$-$b), achieve high accuracy in cardiovascular disease diagnosis. CNNs excel at capturing spatial and temporal patterns in 1D ECG signals or 2D ECG images \citep{tesfai2022lightweight, degirmenci2022arrhythmic, mashrur2019automatic, huang2022snippet}, while Transformers use attention mechanisms to model global dependencies \citep{natarajan2020wide, jiang2021hadln, he2023transformers}. Despite their strengths, eSLs rely heavily on large-scale datasets with expert-verified annotations, making them costly and impractical for pre-training tasks \citep{strodthoff2020deep}. This dependence limits their scalability and generalizability, particularly when addressing diverse datasets or unseen cardiac conditions.

\noindent \textbf{ECG Self-supervised Learning.} To overcome the annotation bottleneck, ECG self-supervised learning (eSSL) methods have been introduced, enabling representation learning from unannotated ECG signals in Figure~\ref{fig:related}(c$-$d). Contrastive learning frameworks, such as CLOCS and ASTCL \citep{kiyasseh2021clocs, wang2023adversarial}, explore temporal and spatial invariance in ECG data \citep{eldele2021time, chen2020simple, chen2021empirical}. Generative eSSL techniques reconstruct masked segments to capture signal-level features \citep{zhang2022maefe, sawano2022masked, na2024guiding, jinreading}. Despite their successes, eSSLs fail to incorporate clinical semantics from associated medical reports and require fine-tuning for downstream tasks \citep{liu2023improving, liu2023pixmim, he2022masked}, limiting their utility in zero-shot scenarios.

\noindent \textbf{Biomedical Multimodal Learning.} Multimodal learning has advanced significantly in biomedical applications, especially in vision-language pre-training (VLP) frameworks for radiology \citep{liu2023g2d, liu2023m, wan2024med, zhang2023knowledge, wu2023medklip}, which align radiology images with structured knowledge from reports to reduce noise and improve robustness. However, multimodal learning for ECG remains underexplored. Methods like MERL \citep{liu2024zero} and ECG-LM \citep{yangecg} integrate ECG signals and raw text reports but struggle with noise and inconsistencies in unstructured reports. Others, such as KED \citep{tian2024foundation}, use structured labels and contrastive learning strategies but face challenges from label noise and LLM-generated knowledge hallucinations. Our approach addresses these issues by structuring reports into meaningful entities, reducing noise, and aligning them with ECG signals without reliance on LLM-augmented content, minimizing hallucination risks while enabling efficient representation learning and downstream flexibility.

% Methods like MERL \citep{liu2024zero} and ECG-LM \citep{yangecg} integrate ECG signals with unstructured text reports but suffer from noise and inconsistencies. KED \citep{tian2024foundation} leverages structured labels and contrastive learning but faces label noise and hallucinations in LLM-generated knowledge. Our approach mitigates these issues by structuring reports into entities, reducing noise, and aligning them with ECG signals, ensuring efficient learning without LLM-induced hallucinations.

\section{Methodology}

SuPreME extracts structured clinical entities from ECG reports via LLMs (Section~\ref{sec:ner4ecg}) and embeds both ECG signals and text-based cardiac queries into a shared space. A Cardiac Fusion Network (CFN, Section~\ref{sec:spt4ecg}) then aligns these embeddings, enabling zero-shot classification of unseen cardiac conditions without fine-tuning (Section~\ref{sec:zero-shot}), yielding scalable, clinically meaningful representations.

\subsection{ECG Clinical Entity Extraction}
\label{sec:ner4ecg}

\noindent \textbf{Enriching LLM with Domain-specific Knowledge.}
ECG reports, generated by 12-lead devices (see Appendix~\ref{app:ecg12lead}), provide diverse insights into cardiac activity. To harness this diversity and enhance model performance, We utilize domain-specific knowledge, including ECG terminologies and abbreviations (e.g., "sinus rhythm", "LVH" for Left Ventricular Hypertrophy), compiled systematically with GPT-4o from clinician-validated databases like UMLS\footnote{\href{https://www.nlm.nih.gov/research/umls}{https://www.nlm.nih.gov/research/umls}} and SNOMED CT\footnote{\href{https://www.snomed.org/}{https://www.snomed.org/}} \citep{bodenreider2004unified, donnelly2006snomed}. SCP codes from datasets like PTB-XL further enrich this knowledge base.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/ner4ecg.pdf}
    % \vspace{-20pt}
    \caption{Design of ECG report entity extraction with (a) knowledge-enhanced prompt engineering, and (b) candidate entity deduplication and mapping.}
    \label{fig:ner4ecg}
    % \vspace{-5pt}
\end{figure}

% \noindent \textbf{Knowledge-Enhanced Prompt Engineering.}
% %
% We guide the Llama3.1-70B-Instruct model\footnote{\href{https://huggingface.co/meta-llama}{https://huggingface.co/meta-llama}.} using ECG domain-specific knowledge (e.g., key concepts, terminologies, and diagnostic attributes) and few-shot examples from manually annotated clinical reports. 
% %
% The model processes free-text ECG narratives to extract relevant entities (e.g., waveforms, diagnoses), then classifies them at a ``Global'' level into `Normal', `Abnormal', or `Uncertain' 
% (in Figure~\ref{fig:ner4ecg}(a)) to ensure early filtering of unreliable diagnostic information. Entities deemed similar within the same report are merged to maintain consistency, laying the groundwork for cross-report deduplication.

\noindent \textbf{Knowledge-Guided ECG Entity Extraction.}
%
Before performing Named Entity Recognition (NER) with Llama3.1-70B-Instruct\footnote{\href{https://huggingface.co/meta-llama}{https://huggingface.co/meta-llama}.}, we first provide domain-specific knowledge, including key concepts, terminologies, and diagnostic structures, ensuring the model internalizes relevant medical context. Few-shot examples from manually annotated clinical reports further refine its understanding.
%
Once primed, the model extracts entities (e.g., waveforms, diagnoses) from free-text ECG reports, classifying them at a ``Global'' level into `Normal', `Abnormal', or `Uncertain' (in Figure~\ref{fig:ner4ecg}(a)) to filter unreliable diagnostics. Similar entities within a report are merged for consistency, laying the groundwork for cross-report deduplication.

\noindent \textbf{Entity Deduplication with Professional Medical Database.}
%
Despite structured extraction and filtering, variations in physician writing styles and device-specific formats introduce redundancies and inconsistencies in the extracted entities. To resolve this, we compile a standardized ECG terminology library using LLMs to identify terms from professional medical databases like UMLS and SNOMED CT.
%
We then use Medical Contrastive Pre-trained Transformers\footnote{\href{https://huggingface.co/ncbi/MedCPT-Query-Encoder}{https://huggingface.co/ncbi/MedCPT-Query-Encoder}.} (MedCPT), a pre-trained medical BERT model, to align extracted entities with standardized terms (in Figure~\ref{fig:ner4ecg}(b)). Using the PubMedBERT-initialized query encoder (QEnc) \citep{gu2021domain}, cosine similarity between embeddings maps each entity to its closest standardized term.
%
This deduplication and mapping process reduces 3,138 initial entities to 295 standardized terms, as shown in Figure~\ref{fig:freq}, significantly enhancing consistency and ensuring alignment with professional medical terminology.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/freq.pdf}
    % \vspace{-20pt}
    \caption{Frequency distribution of standardized ECG entities after deduplication and mapping.}
    \label{fig:freq}
    \vspace{-10pt}
\end{figure}


\subsection{Multimodal ECG Supervised Learning}
\label{sec:spt4ecg}

\noindent \textbf{ECG Embedding with Vision Transformer.} 
%
The Vision Transformer (ViT) \citep{dosovitskiy2020image}, designed for 2D image processing, reshapes images into sequences of flattened patches for Transformer-based analysis. Similarly, ECG signals exhibit temporal and structural patterns analogous to the spatial relationships in images. We then adapt its architecture by dividing ECG time series into fixed-size patches, as shown in Figure~\ref{fig:vit}.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/vit.pdf}
    \caption{ECG 1D ViT encoder in the SuPreME framework, with both lead-wise and position embedding.}
    \label{fig:vit}
    % \vspace{-5pt}
\end{figure}

An ECG signal sequence is represented as a 2D matrix $\mathbf{x} \in \mathbb{R}^{L \times T}$, where $L$ is the number of leads and $T$ is the time series length. The time series is segmented into $N = T / P$ fixed-length patches, each containing $P$ time steps with $L$ leads, resulting in patches $\mathbf{x}_p \in \mathbb{R}^{L \times P}$. Each patch is flattened into a vector $\mathbf{x}_p \in \mathbb{R}^{L \cdot P}$, and concatenated to form a sequence $\mathbf{X}_p \in \mathbb{R}^{N \times (L \cdot P)}$. A learnable linear projection $\mathbf{W}_p \in \mathbb{R}^{(L \cdot P) \times D}$ maps each patch to the $D$-dimensional latent space:

% \vspace{-10pt}
\begin{equation}
    \mathbf{Z}_p = \mathbf{X}_p \cdot \mathbf{W}_p, \mathbf{Z}_p \in \mathbb{R}^{N \times D}
\end{equation}
\vspace{-7pt}

Similar to the $[class]$ token in BERT, we add a unique learnable embedding $\mathbf{e}_i \in \mathbb{R}^D$ for each lead $i$ and append it to the corresponding patch embedding $\mathbf{Z}_p$ to capture lead-specific features in the Transformer encoder.
%
Positional embeddings $\mathbf{p}_j$ are added to preserve the order of patches in the time series. We use standard 1D positional embeddings $\mathbf{p}_j$ since it is sufficient to retain the sequential order in time series data. After incorporating $\mathbf{e}i$ and $\mathbf{p}j$, the final patch embedding $\mathbf{Z}{\text{final}}^{(i, j)}$ for each patch $j$ of lead $i$ is obtained. All embeddings $\mathbf{Z}{\text{final}}^{(i, j)}$ are concatenated along the lead dimension to form the sequence $\mathbf{Z} \in \mathbb{R}^{[B, L \times N, D]}$, where $B$ is the batch size.

\vspace{-8pt}
\begin{equation}
    \begin{array}{c}
        \mathbf{Z}_{\text{final}}^{(i, j)} = \mathbf{Z}_p^{(i, j)} + \mathbf{e}_i + \mathbf{p}_j \\[0.5em]
        \mathbf{Z} = \text{Concat}(
        \mathbf{Z}_{\text{final}}^{(1, 1)}, 
        % \mathbf{Z}_{\text{final}}^{(1, 2)}, 
        \dots, 
        \mathbf{Z}_{\text{final}}^{(1, N)}, 
        % \mathbf{Z}_{\text{final}}^{(2, 1)}, 
        % \mathbf{Z}_{\text{final}}^{(2, 2)}, 
        \dots, 
        \mathbf{Z}_{\text{final}}^{(L, N)})
    \end{array}
\end{equation}
\vspace{-10pt}

The concatenated sequence $\mathbf{Z}$ is then processed through multiple Transformer blocks to extract high-level ECG features. Each block consists of self-attention and feed-forward layers with residual connections. To improve generalization, we incorporate stochastic depth dropout in the residual pathways. The detailed structure and mathematical formulation of the Transformer blocks are provided in Appendix \ref{app:implementation}.

\noindent \textbf{Cardiac Query Embedding with MedCPT.}
%
Rather than using traditional category labels, our framework adopts a flexible and semantically meaningful approach based on cardiac text queries, defined by the Standard Communication Protocol (SCP) system for ECG diagnoses. Descriptive queries are constructed using interpretations from medical databases like UMLS and SNOMED CT, ensuring consistency with the target representations in entity deduplication.
%
Afterwards, MedCPT \citep{jin2023medcpt}, a pre-trained medical BERT model, is employed to embed text queries into a shared latent space with ECG signal embeddings, using its query encoder (QEnc) initialized with PubMedBERT \citep{gu2021domain} in the retriever stage.

For a given query text $q$, we generate its embedding vector $\mathbf{E}(q)$ in $D$-dimensional latent space by passing the text through the $\mathbf{QEnc}$, which utilizes the special tokens $[\text{CLS}]$ and $[\text{SEP}]$ from BERT to denote the start and separator of the text. The encoding operation is defined as follows, where $\mathbf{Trm}$ refers to the Transformer encoder:

\vspace{-9pt}
\begin{equation}
    \mathbf{E}(q) = \mathbf{QEnc}(q) = \mathbf{Trm}([\text{CLS}] \ q \ [\text{SEP}])
\end{equation}
\vspace{-7pt}

To ensure alignment with ECG signal embeddings for multimodal fusion, we apply a learnable projection layer that maps text query embeddings into the same latent space as the ECG features, as detailed in Appendix~\ref{app:implementation}. This enables effective cross-modal representation learning, facilitating the integration of structured clinical knowledge with ECG signals.

\noindent \textbf{Alignment by Cardiac Fusion Network.} 
%
The Cardiac Fusion Network (CFN) in SuPreME consists of multi-layer Transformer decoders, which align ECG signals with text-based cardiac queries by treating textual features as input queries and signal features as memory, as illustrated in Figure~\ref{fig:cfn}.

\begin{figure}[h]
    % \vspace{-10pt}
    \includegraphics[width=\columnwidth]{figures/cfn.pdf}
    % \vspace{-20pt}
    \caption{Architecture of the Cardiac Fusion Network (CFN) in the SuPreME.}
    \label{fig:cfn}
    % \vspace{-10pt}
\end{figure}

Given a batch of signal features $\mathbf{F}_\text{signal} \in \mathbb{R}^{B \times N \times D}$ and text features $\mathbf{F}_\text{text} \in \mathbb{R}^{M \times D}$, where $B$ is the batch size, $N$ is the number of image blocks, $M$ is the number of text queries, and $D$ is the embedding dimension, the CFN learns a joint representation that captures cross-modal dependencies. The Transformer decoder attends to the ECG features while grounding the interpretation in medical queries. The fused output is processed by an MLP head for classification.

Further architectural details, including initialization, normalization, and implementation specifics, are described in Appendix \ref{app:implementation}.

\subsection{Zero-shot Prompted Classification}
\label{sec:zero-shot}

\noindent \textbf{Cardiac Query Design.}
%
In our framework, zero-shot classification is achieved by converting cardiac conditions in unseen downstream datasets into descriptive text queries. All SCP codes from the downstream datasets are compiled and paired with concise, clinically relevant textual descriptions, which serve as inputs to the textual modality of SuPreME. These cardiac queries enable the model to effectively align previously unseen cardiac conditions with ECG signal features, allowing it to generalize beyond the entities encountered during pre-training and facilitating inference on novel categories within downstream datasets.

To construct these cardiac queries, we employ simplified Clinical Knowledge-Enhanced Prompt Engineering (CKEPE) \citep{liu2024zero}. While traditional CKEPE involves querying LLMs to extract detailed clinical knowledge from external medical databases (i.e., UMLS and SNOMED CT), including disease subtypes and signal pattern attributes, our approach follows a similar pipeline but directly interprets SCP codes from downstream datasets into concise cardiac descriptions. The robust representation capabilities of our CFN allow us to rely on simplified query content without compromising performance. By reducing redundancy in the queries, this method avoids overloaded textual information that might hinder the model's ability to align features effectively across modalities.

\noindent \textbf{Evaluation with Pre-trained SuPreME.}
%
During the evaluation process, ECGs and cardiac queries are encoded into $\mathbf{Emb}_{\text{ECGs}}$ and $\mathbf{Emb}_{\text{Queries}}$ respectively, using the ECG encoder and text encoder components of the pre-trained SuPreME framework. The embeddings are then input into the pre-trained CFN for alignment and fusion, which processes them using cross-modal attention mechanisms and outputs logits through an MLP head (i.e., a linear classification layer). Subsequently, the logits are passed through a sigmoid activation function $\sigma$ to produce probabilities $\mathbf{Pred}$ for each cardiac condition:

\vspace{-11pt}
\begin{equation}
    \mathbf{Pred} = \sigma(\text{CFN}(\mathbf{Emb}_\text{ECGs}, \mathbf{Emb}_\text{Queries}))
\end{equation}
\vspace{-9pt}

We assess the model's classification performance using the Area Under the Receiver Operating Characteristic curve (AUROC, also referred to as AUC; further details are provided in Appendix \ref{app:metrics}). Given the multi-label nature of this classification task, the AUC is computed separately for each category, and the mean AUC across all categories is reported as the overall performance metric.

\noindent \textbf{Overlap Analysis on Pre-train and Downstream Datasets.}
%
We analyze the cardiac query overlap between the pre-train dataset and six downstream datasets specified in Section~\ref{sec:config4exp}, as well as among the downstream datasets themselves. Specifically, we embed all entities from the pre-train dataset and cardiac queries from the downstream datasets, compute their cosine similarity, and apply a threshold of 0.95 to filter overlapping queries. This analysis reveals that 57 cardiac queries overlap between the pre-train dataset and the downstream datasets. Details of the overlapping queries are provided in the Appendix~\ref{app:overlap}.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=0.48\linewidth]{figures/heatmap.pdf} \hfill
    \includegraphics[width=0.48\linewidth]{figures/stackedbar.pdf}
    % \vspace{-5pt}
    \caption {Overlap between pre-train and downstream datasets, with left panel showing pairwise downstream overlaps and right panel showing pre-train overlaps.}
    \label{fig:overlap}
    % \vspace{-10pt}
\end{figure}

Furthermore, we analyze the overlaps both among the downstream datasets and between each downstream dataset and the pre-train dataset, as illustrated in Figure~\ref{fig:overlap}. The heatmap on the left shows that pairwise overlaps among downstream datasets are generally limited, reflecting the diversity of cardiac query prompts. The bar chart on the right shows that while the pre-train dataset shares some similar queries, a substantial portion of queries remains unique to the downstream datasets, allowing the pre-train process to establish robust general-purpose representations while leaving room for downstream-specific adaptation.

\section{Experiments}

\subsection{Configuration and Settings}
\label{sec:config4exp}

\noindent \textbf{Clinical Entity Extraction.} We utilize domain-specific prompts enriched with professional medical knowledge to extract entities from the MIMIC-IV-ECG dataset (see Appendix \ref{app:dataset}). Entity extraction is performed using Llama3.1-70B-Instruct with structured constraints for clinical NER. Deduplication and mapping leverage MedCPT embeddings to group similar entities (similarity > 0.8) and map them to standardized terminologies from UMLS and SNOMED CT (average similarity > 0.75). Experiments are conducted on 8 NVIDIA A100-SMX4-80GB GPUs with vLLM\footnote{\href{https://github.com/vllm-project/vllm}{https://github.com/vllm-project/vllm}.}.

\noindent \textbf{Supervised ECG Pre-training.} A 1D ViT-tiny model serves as the ECG encoder, with a patch size of 125 (0.25 seconds per patch). MedCPT, with frozen weights~\citep{zhang2023knowledge}, is used as the text encoder. We use the AdamW optimizer with a learning rate of $1 \times 10^{-3}$ and weight decay of $1 \times 10^{-8}$. A cosine annealing scheduler adjusts the learning rate with an initial cycle length $T_0$ of 5000 steps and $T_{\text{mult}} = 1$, down to a minimum learning rate of $1 \times 10^{-8}$. Pre-training runs for 100 epochs with early stopping tolerance as 10 epochs and uses a batch size of 256 per GPU on 4 NVIDIA A100-PCIE-40GB GPUs.

\noindent \textbf{Downstream Classification Task.} Zero-shot classification is evaluated on six unseen datasets (e.g., PTB-XL, CPSC-2018, and Chapman-Shaoxing-Ningbo; see Appendix \ref{app:dataset}) using SuPreME and customized prompts created via CKEPE (Section~\ref{sec:zero-shot}). Ablation studies assess the impact of different ECG and text encoder backbones and the CFN module. Mainstream eSSLs are evaluated with linear probing by freezing ECG encoders and fine-tuning a linear layer on 1\%, 10\%, and 100\% of labeled data from the six datasets. All Downstream tasks are evaluated using the average AUC across datasets, following the data splits from \citep{liu2024zero} (see Appendix \ref{app:datasplit}). Detailed setups are provided in Appendix \ref{app:downstream}.

\subsection{Evaluation with Mainstream eSSLs}

We evaluate SuPreME against mainstream eSSL frameworks across 127 classes in six downstream ECG datasets, conducting linear probing with eSSL ECG encoders across varying data proportions to facilitate performance comparison. Table~\ref{tab:overall} demonstrates AUC results of SuPreME and eSSLs under different evaluation approaches.

\begin{table}[ht]
\centering
% \vspace{-5pt}
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{l|c|c|ccccccc}
        \toprule[1.2pt]
        & Evaluation & Zero-shot & \multicolumn{7}{c}{Linear Probing} \\
        Framework & Approach & 0\% &  & 1\% &  & 10\% &  & 100\% &  \\
        \midrule[1.2pt]
        \multicolumn{10}{l}{\textbf{\textit{From Scratch}}} \\
        \midrule
        Random Init (CNN) & \textit{L} & - &  & 55.09 &  & 67.37 &  & 77.21 & \\
        Random Init (Transformer) & \textit{L} & - &  & 53.53 &  & 65.54 &  & 75.52 & \\
        \midrule
        \multicolumn{10}{l}{\textbf{\textit{ECG Only}}} \\
        \midrule
        SimCLR \citep{chen2020simple} & \textit{L} & - &  & 58.24 &  & 66.71 &  & 72.82 &  \\
        BYOL \citep{grill2020bootstrap} & \textit{L} & - &  & 55.78 &  & 70.61 &  & 74.92 &  \\
        BarlowTwins \citep{zbontar2021barlow} & \textit{L} & - &  & 58.92 &  & 70.85 &  & 75.39 &  \\
        MoCo-v3 \citep{chen2021empirical} & \textit{L} & - &  & 57.92 &  & 72.04 &  & 75.59 &  \\
        SimSiam \citep{chen2021exploring} & \textit{L} & - &  & 59.46 &  & 69.32 &  & 75.33 &  \\
        TS-TCC \citep{eldele2021time} & \textit{L} & - &  & 54.66 &  & 69.37 &  & 76.95 &  \\
        CLOCS \citep{kiyasseh2021clocs} & \textit{L} & - &  & 56.67 &  & 70.91 &  & 75.86 & \\
        ASTCL \citep{wang2023adversarial} & \textit{L} & - &  & 57.53 &  & 71.15 &  & 75.98 &  \\
        CRT \citep{zhang2023self} & \textit{L} & - &  & 56.62 &  & 72.03 &  & 76.65 &  \\
        ST-MEM \citep{na2024guiding} & \textit{L} & - &  & 56.42 &  & 63.39 &  & 69.60 &  \\
        \midrule
        \multicolumn{10}{l}{\textbf{\textit{Multimodal Learning}}} \\
        \midrule
        MERL (ResNet) \citep{liu2024zero} & \textit{Z \& L} & \colorbox{gray!15}{75.24} &  & \textbf{65.97} &  & \textbf{81.70} &  & \textbf{86.65} &  \\
        MERL (ViT) \citep{liu2024zero} & \textit{Z \& L} & 73.54 &  & \colorbox{gray!15}{63.57} &  & \colorbox{gray!15}{78.35} &  & 83.68 &  \\
        \midrule
        \textbf{SuPreME (Ours)} & \textit{Z \& L} & \textbf{77.20} &  & 63.24 &  & 72.34 &  & \colorbox{gray!15}{84.48} & \\
        \bottomrule[1.2pt]
    \end{tabular}
    }
% \vspace{-5pt}
\caption{Overall AUC performance of SuPreME and eSSLs, with '\textit{Z}' for zero-shot and '\textit{L}' for linear probing. Best results are \textbf{bolded} and second best \colorbox{gray!15}{gray}-flagged.}
\label{tab:overall}
% \vspace{-10pt}
\end{table}

Our results demonstrate that SuPreME achieves superior performance compared to traditional eSSL frameworks. With an overall zero-shot AUC of 77.20\%, SuPreME outperforms most non-multimodal eSSL models, which require linear probing even with 1\% (best: 59.46\%) or 10\% (best: 72.04\%) of labeled data, showcasing its strong generalization capabilities and efficient utilization of pre-trained knowledge. Even without the CFN module, SuPreME remains highly competitive (in Appendix \ref{app:lp}) using only the pre-trained ECG encoder for linear probing. It consistently outperforms non-multimodal eSSL models across 1\%, 10\%, and 100\% labeled data, and and achieves comparable performance to multimodal contrastive learning frameworks like MERL, despite not explicitly leveraging contrastive objectives. Notably, SuPreME achieves this with just 16 training epochs, compared to MERL’s 50 epochs.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/radar.pdf}
    % \vspace{-15pt}
    \caption{Comparison of SuPreME (zero-shot learning) and eSSLs (linear probing with 1\% data on the left and 10\% data on the right) across downstream datasets.}
    \label{fig:radar}
    % \vspace{-5pt}
\end{figure}

Figure~\ref{fig:radar} presents framework performance across individual datasets. SuPreME's advantage on the PTB-XL-Superclass dataset is minimal, likely due to the dataset's simplicity, as it includes only 5 broad cardiac condition labels (e.g., NORM, STTC, MI), making it difficult to differentiate model performance. Additionally, all frameworks perform poorly on the PTB-XL-Form dataset, which focuses on 19 ECG waveform types that do not directly correspond to cardiac conditions, leading to ambiguous associations and reduced performance for all models.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/data.pdf}
    % \vspace{-15pt}
    \caption{Data efficiency of SuPreME (zero-shot) and eSSLs (MERL in zero-shot, others in linear probing).}
    \label{fig:data}
    % \vspace{-11pt}
\end{figure}

To investigate SuPreME's sensitivity to pre-training data, we evaluate its zero-shot performance across varying data proportions (in Figure~\ref{fig:data}). SuPreME’s performance improves steadily with more pre-training data, maintaining a significant edge over eSSL frameworks. Notably, with just 20\% of pre-train data, SuPreME rivals or surpasses many eSSLs using 10\% labeled data for linear probing. It also achieves comparable performance to multimodal eSSLs with only 60\% of pre-train data, showcasing its efficiency and robust generalization, even in low-resource settings.

\subsection{Evaluation of SuPreME Architecture}

Beyond comparisons with eSSLs, we directly assess SuPreME’s zero-shot classification performance by varying its core modules, including the ECG backbone and CFN. Table~\ref{tab:zero-shot} reports the results for SuPreME and its variants, where the ViT + CFN architecture achieves the highest average AUC of 77.20\%, with particularly strong performance on PTB-XL-Rhythm and CSN.

\begin{table}[h]
% \vspace{-5pt}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
    \toprule[1.2pt]
    & \multicolumn{2}{c}{\textbf{\textit{Linear Classification}}} & \multicolumn{2}{c}{\textbf{\textit{Cardiac Fusion Network}}} \\
    \midrule
    Dataset & ResNet & ViT & ResNet & ViT \\
    \midrule
    PTB-XL-Superclass & 67.55 & 66.80 & 68.75 & \textbf{78.20} \\
    PTB-XL-Subclass & 73.77 & 71.51 & 68.02 & \textbf{77.52}  \\
    PTB-XL-Form & \textbf{64.34} & 62.10 & 58.85 & 60.67  \\
    PTB-XL-Rhythm & 75.68 & 75.34 & 68.69 & \textbf{86.79}  \\
    CPSC-2018 & \textbf{83.35} & 79.13 & 60.38 & 79.83 \\
    CSN & 72.61 & 72.32 & 65.07 & \textbf{80.17} \\
    \midrule
    \textbf{Overall} & 72.88 & 71.23 & 64.96 & \textbf{77.20} \\
    \bottomrule[1.2pt]
\end{tabular}
}
% \vspace{-5pt}
\caption{Zero-shot classification AUC performance of SuPreME and its variants on six downstream ECG datasets, with best results \textbf{bolded}.}
\label{tab:zero-shot}
% \vspace{-5pt}
\end{table}

Under the linear classification setup, ResNet outperforms ViT across most datasets, demonstrating its effectiveness in extracting essential features without contextual mechanisms. However, the CFN significantly improves performance, with SuPreME (ViT + CFN) achieving a notable boost in AUC, particularly on PTB-XL-Superclass (78.20\%), PTB-XL-Rhythm (86.79\%), and CSN (80.17\%). Figure~\ref{fig:variant} highlights the strength of ViT’s attention mechanisms combined with CFN, which excels at capturing complex temporal and spatial dependencies in ECG signals.

By employing prompts to generate meaningful class queries, the CFN enables a more flexible and adaptable approach to classification, allowing flexible adaptation to diverse downstream class structures. Unlike linear classification, SuPreME dynamically aligns pre-trained knowledge with new cardiac conditions, eliminating the need for explicit class mapping between pre-training and downstream datasets and improving generalization.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/variants.pdf}
    \vspace{-15pt}
    \caption{Specific zero-shot performance of SuPreME and its variants across downstream datasets.}
    \label{fig:variant}
    % \vspace{-10pt}
\end{figure}

We further analyze performance on specific cardiac conditions in PTB-XL-Subclass (others in Appendix \ref{app:scatters}), shown in Figure~\ref{fig:scatter_ptbxl_subclass_selected}. SuPreME consistently achieves high AUC scores, particularly for critical conditions like LAFB/LPFB, CRBBB, CLBBB, IRBBB, and RVH (AUC > 90). In contrast, other variants show lower performance, especially for conditions requiring nuanced spatial and temporal patterns. ResNet + Linear performs competitively on simpler cases but struggles with complex conditions. ResNet + CFN exhibits significant drops, particularly for IMI, AVB, RAO/RAE, ISCA, and IRBBB, highlighting its limitations in effectively capturing intricate dependencies.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Subclass-selected.pdf}
    % \vspace{-15pt}
    \caption{Specific zero-shot classification AUC performance of SuPreME and its variants on selected detailed categories in PTB-XL-Subclass.}
    \label{fig:scatter_ptbxl_subclass_selected}
    % \vspace{-10pt}
\end{figure}

\subsection{Ablation Analysis}
\label{sec:ablation}

\noindent \textbf{Clinical Entity Mapping.}
%
To assess the impact of mapping labels to a standardized medical database, we compare SuPreME trained on the original 341 raw labels vs.\ our 295 deduplicated set (Table~\ref{tab:dedup&backbone}(a)). Notably, deduplication boosts the zero-shot AUC from 65.94\% to 77.20\%, which could be attributed to the removal of noisy or synonymous labels, thus clarifying the model’s representation of distinct cardiac conditions.


\begin{table}[ht]
    % \vspace{-11pt}
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \subcaption{Entity deduplication.}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c}
            \toprule[1.2pt]
            Deduplication & Zero-shot AUC \\
            \midrule[1.2pt]
            Not Deduplicated & 65.94 \\
            \midrule
            \textbf{Deduplicated (Ours)} & \textbf{77.20} \\
            \bottomrule[1.2pt]
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \subcaption{ECG encoder backbones.}
        \resizebox{0.77\linewidth}{!}{
        \begin{tabular}{c|c}
            \toprule[1.2pt]
            Backbone & Zero-shot AUC \\
            \midrule[1.2pt]
            ResNet & 64.96 \\
            \midrule
            \textbf{ViT (Ours)} & \textbf{77.20} \\
            \bottomrule[1.2pt]
        \end{tabular}
        }
    \end{minipage}
    % \vspace{-5pt}
    \caption{Effects of pre-train dataset entity deduplication and different ECG encoder backbones.}
    \label{tab:dedup&backbone}
    % \vspace{-10pt}
\end{table}


\noindent \textbf{ECG Encoder Backbone.}  
%
To assess the ECG backbone impact, we replace the ViT-tiny with ResNet18 and evaluate zero-shot performance. ResNet18 yields a lower AUC of 64.96\% (drops by 12.24\%, Table~\ref{tab:dedup&backbone}(b)), indicating that ResNet struggles to capture longer-range ECG dependencies compared to ViT's self-attention mechanism.

\noindent \textbf{Clinical Text Encoder.}
%
For three different medical text encoders - BioClinicalBERT \citep{alsentzer2019publicly}, PubMedBERT \citep{gu2021domain}, and MedCPT - MedCPT achieves the highest AUC, surpassing the others by 6.47\% and 19.75\% (Table~\ref{tab:lm&cfn}(a)). This appears to stem from MedCPT’s contrastive objectives, which more effectively capture fine-grained sample similarities and differences than traditional masked language modeling.

\begin{table}[tb!]
    % \vspace{-6pt}
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \subcaption{Language model encoders.}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c}
            \toprule[1.2pt]
            Language Model & Zero-shot AUC \\
            \midrule[1.2pt]
            BioClinicalBERT & 57.45 \\
            \midrule
            PubMedBERT & 70.73 \\
            \midrule
            \textbf{MedCPT (Ours)} & \textbf{77.20} \\
            \bottomrule[1.2pt]
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \subcaption{Cardiac Fusion Network.}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c}
            \toprule[1.2pt]
            Module & Zero-shot AUC \\
            \midrule[1.2pt]
            w/o CFN (Linear) & 71.23 \\
            \midrule
            \textbf{CFN (Ours)} & \textbf{77.20} \\
            \bottomrule[1.2pt]
        \end{tabular}
        }
    \end{minipage}
    % \vspace{-5pt}
    \caption{Effects of different language models and Cardiac Fusion Network (CFN).}
    \label{tab:lm&cfn}
    % \vspace{-17pt}
\end{table}

\noindent \textbf{Cardiac Fusion Network Module.}
%
We compare CFN-based fusion with a simple linear projection (Table~\ref{tab:lm&cfn}(b)). CFN lifts the zero-shot AUC from 71.23\% to 77.20\%, highlighting the benefits of cross-attention in capturing multimodal synergies between ECG signals and text queries.

\noindent \textbf{Customized Cardiac Prompts.}
%
We evaluate three prompt strategies - GPT-4o Generated, CKEPE (detailed), and CKEPE (simplified). As shown in Table~\ref{tab:ckepe&drop}(a), the simplified CKEPE achieves the highest AUC, delivering at least a 12.5\% improvement. This suggests that concise, clinically focused prompts help reduce noise and enhance alignment.

\begin{table}[ht]
    % \vspace{-10pt}
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \subcaption{Zero-shot query prompts.}
        \resizebox{1.1\linewidth}{!}{
        \begin{tabular}{c|c}
            \toprule[1.2pt]
            Prompt Strategy & Zero-shot AUC \\
            \midrule[1.2pt]
            GPT-4o Generated & 61.30 \\
            \midrule
            CKEPE Detailed & 64.70 \\
            \midrule
            \textbf{CKEPE Simplified (Ours)} & \textbf{77.20} \\
            \bottomrule[1.2pt]
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \subcaption{Pre-train dropout ratios.}
        \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{c|c}
            \toprule[1.2pt]
            Dropout Ratio & Zero-shot AUC \\
            \midrule[1.2pt]
            0.05 & 76.65 \\
            \midrule
            \textbf{0.10 (Ours)} & \textbf{77.20} \\
            \midrule
            0.15 & 75.97 \\
            \midrule
            0.20 & 76.72 \\
            \bottomrule[1.2pt]
        \end{tabular}
        }
    \end{minipage}
    % \vspace{-5pt}
    \caption{Effects of different zero-shot query prompts and pre-train dropout ratios.}
    \label{tab:ckepe&drop}
    % \vspace{-10pt}
\end{table}

\noindent \textbf{Dropout Ratio.}
%
During pre-training, we compare dropout rates of \{0.05, 0.10, 0.15, 0.20\}. In Table~\ref{tab:ckepe&drop}(b), a rate of 0.10 yields the best AUC (77.20\%), likely striking a balance between regularization and signal retention.

\section{Conclusion}

We present a novel LLM-based method for ECG clinical entity extraction and introduce SuPreME, a scalable supervised pre-training framework for multimodal ECG representation learning that aligns ECG signals with fine-grained, standardized medical terminologies rather than free-text reports. Its Cardiac Fusion Network (CFN) and Clinical Knowledge-Enhanced Prompt Engineering (CKEPE) eliminate the need for further fine-tuning, enabling robust zero-shot classification with concise cardiac queries.
%
Benchmarked on six downstream datasets, SuPreME achieves superior zero-shot performance against 11 eSSLs, underscoring both data efficiency and diagnostic precision. Our results highlight the value of explicit entity-level supervision over raw text alignment in ECG multimodal learning, providing a strong basis for clinically oriented ECG representation learning.

\section*{Limitations}

While SuPreME demonstrates robust classification performance and superior results compared to existing eSSL frameworks, there are some limitations to consider. The framework’s reliance on LLMs for clinical entity extraction may face challenges in capturing highly specialized or ambiguous medical knowledge without domain-specific fine-tuning. Additionally, although SuPreME achieves impressive zero-shot classification performance, the approach assumes that the pre-trained model can generalize well across diverse clinical contexts, which may not always hold in real-world scenarios with varying data quality and noise levels. Future work will focus on enhancing the robustness of clinical entity extraction and developing more adaptive strategies for diverse ECG data, as well as exploring methods to further optimize the pre-training process.

% \textbf{Granularity of Extracted ECG Report Entities.} While entity extraction enhances ECG-text alignment, the current classification scheme may still be too coarse-grained. A more structured hierarchical classification - such as explicitly separating waveform features (e.g., "ST elevation", "T wave inversion") from diagnostic terms (e.g., "myocardial infarction", "left ventricular hypertrophy") - could yield richer clinical representations. This finer granularity could reduce label noise, improve interpretability, and potentially enhance performance in downstream tasks by allowing models to learn more specialized feature representations for different types of extracted entities.

% \textbf{Lead-Specific Information in ECG Reports.} Many extracted entities in ECG reports reference specific leads (e.g., "ST elevation in V3") or lead groups (e.g., "inferior lead abnormalities" referring to II, III, and aVF). These references often carry diagnostic significance, as different cardiac conditions manifest in distinct leads. Instead of always using the full 12-lead ECG, a more adaptive lead selection strategy could be explored, where only the most relevant leads are used as input based on entity mentions. This could potentially reduce computational redundancy, focus the model on clinically meaningful features, and enhance interpretability. However, such an approach would need to address challenges such as inter-lead dependencies and ensuring that key contextual information is not lost.

% \textbf{Comparison with ICD-based Diagnoses.} Our framework currently extracts cardiac entities from free-text ECG reports, but it remains unclear whether structured ICD-coded diagnoses would offer superior performance. ICD codes provide standardized, expert-annotated diagnoses that may be less noisy and more clinically validated compared to entities extracted from unstructured text. If ICD-based labels lead to better classification results, this could suggest an alternative multimodal paradigm, where ECG signals are combined with diagnostic history rather than free-text reports. Such an approach could improve clinical relevance and robustness, particularly in settings where structured diagnostic data is available.

\section*{Acknowledgments}

This work is supported by the School of Informatics at the University of Edinburgh and the Data Science Institute at Imperial College London. 
%
We extend our gratitude to the Edinburgh Compute and Data Facility\footnote{\href{http://www.ecdf.ed.ac.uk/}{http://www.ecdf.ed.ac.uk/}} (ECDF) and the Imperial College Research Computing Service\footnote{\href{http://doi.org/10.14469/hpc/2232}{http://doi.org/10.14469/hpc/2232}} for providing essential computational resources for this research.

\clearpage

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{main}

\clearpage

\appendix

\section{Appendix}

\subsection{Electrocardiogram (ECG)}
\label{app:ecg12lead}

In the medical field, electrocardiogram (ECG) is an important tool for recording and analyzing patients' cardiac activities, which helps healthcare professionals identify various kinds of cardiac problems by detecting the electrical changes in different leads. The standard 12-lead ECG is the most common method of recording ECGs, and it can capture relatively comprehensive range of cardiac signals through placing electrodes at different locations on the body, providing information of the heart's health conditions.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/12lead.pdf}
    % \vspace{-15pt}
    \caption{Standard 12-lead Electrocardiogram (ECG) showing 'sinus rhythm'.}
    \label{fig:ecg12lead}
    % \vspace{-5pt}
\end{figure}

The basic components of the 12-lead ECG include the limb leads and the precordial leads. The limb leads contain I, II, III, aVR, aVL, and aVF, each of them consists of a combination of electrodes located primarily in the right arm, left arm, left leg, and right leg (as shown in Figure~\ref{fig:ecg12lead}). The precordial leads contain V1, V2, V3, V4, V5, and V6, which all correspond to specific single electrodes at different locations on the chest, and are used to observe in detail the electrical activity of the anterior, lateral, and posterior walls of the heart.

\subsection{Implementation Details}
\label{app:implementation}

\textbf{Transformer Block Structure.}
%
The Transformer architecture \citep{vaswani2017attention} is widely used for seq2seq modeling, learning global dependencies via self-attention instead of recurrent or convolutional structures. It consists of an encoder-decoder design, where both the encoder and decoder utilize stacked self-attention and feed-forward layers, as shown in Figure~\ref{fig:transformer}.

\begin{figure}[h]
    % \vspace{-5pt}
    \begin{center}
        \includegraphics[width=0.75\columnwidth]{figures/transformer.pdf}
    \end{center}
    % \vspace{-15pt}
    \caption{Encoder-decoder structure of Transformer, quoted from \citep{vaswani2017attention}.}
    \label{fig:transformer}
    \vspace{-25pt}
\end{figure}

Each encoder block applies a residual connection around its multi-head self-attention ($\mathbf{MHA}$) and position-wise feed-forward ($\mathbf{FF}$) sublayers, followed by layer normalization:

% \vspace{-10pt}
\begin{equation}
    \begin{array}{c}
        \mathbf{Z}^{(k,1)} = \mathbf{Z}^{(k-1)} + \text{Drop}(\mathbf{MHA}(\text{Norm}(\mathbf{Z}^{(k-1)}))) \\[0.5em]
        \mathbf{Z}^{(k,2)} = \mathbf{Z}^{(k,1)} + \text{Drop}(\mathbf{FF}(\text{Norm}(\mathbf{Z}^{(k,1)}))) \\[0.5em]
        \mathbf{Z}^{\text{norm}} = \text{Norm}(\mathbf{Z}^{\text{(final)}})
    \end{array}
\end{equation}
\vspace{-10pt}

where $\mathbf{Z}^{(k-1)}$ is the input to the $k$-th Transformer block, $\mathbf{Z}^{(k,1)}$ represents the intermediate state after multi-head attention and residual connection, and $\mathbf{Z}^{(k,2)}$ is the output after the feed-forward network. The final normalized representation $\mathbf{Z}^{\text{norm}}$ is used for downstream ECG classification.

The decoder extends the encoder structure by introducing an additional multi-head attention sublayer that attends to encoder outputs, while also incorporating masked self-attention to ensure autoregressive sequence modeling. These layers collectively enable flexible cardiac feature extraction in SuPreME.

\noindent \textbf{Projection of Text Query Embeddings.}
%
To align the $D$-dimensional text query embeddings $E(q)$ with ECG embeddings for multimodal fusion, $\mathbf{E}(q) \in \mathbb{R}^D$ is projected into the same latent space $\mathbb{R}^{\text{proj\_out}}$ with ECG embeddings using a two-layer feedforward neural network with a $\text{ReLU}$ activation function:

\vspace{-8pt}
\begin{equation}
    \mathbf{E}'(q) = \text{Proj}(\mathbf{E}(q)) = \mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \cdot \mathbf{E}(q))
\end{equation}
\vspace{-12pt}

where $\mathbf{W}_1 \in \mathbb{R}^{\text{proj\_input} \times \text{hidden}}$ and $\mathbf{W}_2 \in \mathbb{R}^{\text{proj\_hidden} \times \text{proj\_out}}$ are the weights of the linear layers. The final embedding $\mathbf{E}'(q)$  represents the query in the target latent space, ready for downstream multimodal fusion with ECG signal embeddings.

\noindent \textbf{Initialization of Cardiac Fusion Network.}
%
All weights in linear layers and attention modules are initialized with a normal distribution, \( \mathbf{W} \sim \mathcal{N}(0, 0.02) \). To support batch processing, the text embeddings $\mathbf{F}_\text{text}$ are expanded to match the batch size $B$. Both ECG and text embeddings undergo layer normalization to improve training stability and convergence.

\subsection{Performance Evaluation Metric}
\label{app:metrics}

We use zero-shot learning and linear probing to evaluate the performance of SuPreME and mainstream eSSL frameworks. The primary metric is Area Under the Receiver Operating Characteristic (AUROC, also referred to as AUC). AUROC is widely used to evaluate the performance of binary classification models. The ROC curve plots the True Positive Rate (TPR) on the vertical axis against the False Positive Rate (FPR) on the horizontal axis. By varying the classifier's threshold, $\mathbf{TPR}$ and $\mathbf{FPR}$ are calculated and then plotted to form the curve, where $\mathbf{TP}$ refers to True Positive, $\mathbf{FN}$ refers to False Negative, $\mathbf{FP}$ refers to False Positive, and $\mathbf{TN}$ refers to True Negative.:

% \vspace{3pt}
\begin{equation}
    \begin{array}{c}
        \mathbf{TPR} = \dfrac{\mathbf{TP}}{\mathbf{TP} + \mathbf{FN}} \\ \\
        \mathbf{FPR} = \dfrac{\mathbf{FP}}{\mathbf{FP} + \mathbf{TN}}
    \end{array}
\end{equation}
\vspace{1pt}

AUROC is the area under the ROC curve, with values ranging from 0 to 1, reflecting the overall classification ability of the model. \textbf{AUROC = 0.5} indicates that the model's classification ability is equivalent to random guessing, while \textbf{AUROC $>$ 0.5} and values closer to \textbf{1} indicate that the model is able to classify with greater accuracy.

\subsection{Overlap Analysis}
\label{app:overlap}

Table~\ref{tab:overlap} shows the overlap between entities from the pre-train dataset and cardiac queries from the downstream datasets, filtered using a cosine similarity threshold of 0.95.

\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{l|l|c}
            \toprule[1.2pt]
            \textbf{Pre-train Dataset Entity}      & \textbf{Downstream Cardiac Query}    & \textbf{Similarity Score} \\ 
            \midrule[1.2pt]
            atrial fibrillation                    & atrial fibrillation                  & 1.0000                    \\ 
            supraventricular tachycardia           & supraventricular tachycardia         & 1.0000                    \\ 
            ventricular preexcitation              & ventricular preexcitation            & 1.0000                    \\ 
            right bundle branch block              & right bundle branch block            & 1.0000                    \\ 
            myocardial infarction                  & myocardial infarction                & 1.0000                    \\ 
            atrial premature complex               & atrial premature complex             & 1.0000                    \\ 
            Prolonged QT interval                  & prolonged qt interval                & 1.0000                    \\ 
            T wave abnormalities                   & t wave abnormalities                 & 1.0000                    \\ 
            ST depression                          & st depression                        & 1.0000                    \\ 
            AV block                               & av block                             & 1.0000                    \\ 
            T wave Changes                         & t wave changes                       & 1.0000                    \\ 
            sinus bradycardia                      & sinus bradycardia                    & 1.0000                    \\ 
            left anterior fascicular block         & left anterior fascicular block       & 1.0000                    \\ 
            sinus arrhythmia                       & sinus arrhythmia                     & 1.0000                    \\ 
            left bundle branch block               & left bundle branch block             & 1.0000                    \\ 
            sinus tachycardia                      & sinus tachycardia                    & 1.0000                    \\ 
            abnormal Q wave                        & abnormal q wave                      & 1.0000                    \\ 
            ventricular premature complex          & ventricular premature complex        & 1.0000                    \\ 
            Prolonged PR interval                  & prolonged pr interval                & 1.0000                    \\ 
            Atrial Tachycardia                     & atrial tachycardia                   & 1.0000                    \\ 
            Supraventricular Tachycardia           & supraventricular tachycardia         & 1.0000                    \\ 
            left posterior fascicular block        & left posterior fascicular block      & 1.0000                    \\ 
            normal                                 & normal                               & 1.0000                    \\ 
            second degree AV block                 & second degree av block               & 1.0000                    \\ 
            anterior myocardial infarction         & anterior myocardial infarction       & 1.0000                    \\ 
            incomplete left bundle branch block    & incomplete left bundle branch block  & 1.0000                    \\ 
            incomplete right bundle branch block   & incomplete right bundle branch block & 1.0000                    \\ 
            Atrial Flutter                         & atrial flutter                       & 1.0000                    \\ 
            ST elevation                           & st elevation                         & 1.0000                    \\ 
            atrial flutter                         & atrial flutter                       & 1.0000                    \\ 
            Sinus Tachycardia                      & sinus tachycardia                    & 1.0000                    \\ 
            Sinus Bradycardia                      & sinus bradycardia                    & 1.0000                    \\ 
            first degree AV block                  & first degree av block                & 1.0000                    \\ 
            premature complex                      & premature complex                    & 1.0000                    \\ 
            premature atrial complex               & atrial premature complex             & 0.9961                    \\ 
            ST-T change                            & st-t changes                         & 0.9968                    \\ 
            right ventricle hypertrophy            & right ventricular hypertrophy        & 0.9920                    \\ 
            left ventricle hypertrophy             & left ventricular hypertrophy         & 0.9924                    \\ 
            complete right bundle branch block     & right bundle branch block            & 0.9891                    \\ 
            Q wave present                         & q wave                               & 0.9903                    \\ 
            high QRS voltage                       & high qrs voltages                    & 0.9878                    \\ 
            complete left bundle branch block      & left bundle branch block             & 0.9861                    \\ 
            second degree AV block(Type one)       & second degree av block               & 0.9817                    \\ 
            anteroseptal myocardial infarction     & anteroseptal infarction              & 0.9809                    \\ 
            ischemic                               & ischemia                             & 0.9804                    \\ 
            second degree AV block(Type two)       & second degree av block               & 0.9795                    \\ 
            third degree av block                  & second degree av block               & 0.9795                    \\ 
            abnormal QRS                           & abnormal qrs morphology              & 0.9737                    \\ 
            low amplitude T wave                   & high t wave amplitude                & 0.9741                    \\ 
            suggests digitalis-effect              & digitalis effect                     & 0.9726                    \\ 
            left front bundle branch block         & left bundle branch block             & 0.9537                    \\ 
            inferior myocardial infarction         & inferior infarction                  & 0.9512                    \\ 
            right atrial hypertrophy               & right atrial enlargement             & 0.9570                    \\ 
            paroxysmal supraventricular tachycardia & supraventricular tachycardia         & 0.9611                    \\ 
            anterolateral myocardial infarction    & anterolateral infarction             & 0.9667                    \\ 
            supraventricular arrhythmia            & supraventricular tachycardia         & 0.9684                    \\ 
            \bottomrule[1.2pt]
        \end{tabular}
    }
% \vspace{-5pt}
\caption{Overlap between pre-train dataset entities and downstream cardiac queries, filtered with similarity threshold of 0.95, sorted by similarity score.}
\label{tab:overlap}
% \vspace{-5pt}
\end{table}

\subsection{Dataset Overview}
\label{app:dataset}

\textbf{MIMIC-IV-ECG.}
%
MIMIC-IV-ECG\footnote{\href{https://physionet.org/content/mimic-iv-ecg/1.0/}{https://physionet.org/content/mimic-iv-ecg/1.0/}.} is a comprehensive database containing 800,035 diagnostic ECG samples from 161,352 unique patients, with 12-lead recordings in 10 second length and sampled at 500 Hz \citep{gow2023mimic}. These data have been matched with patient records in the MIMIC-IV clinical database, allowing for the association of waveforms with reports when a cardiologist’s report is available through provided linking information. To enhance the usability of the data, we exclude empty reports as well as reports containing fewer than 3 words, and replace 'NaN' and 'Inf' values in the ECG records with the average of 6 neighboring points. Ultimately, the dataset used for clinical entity extraction tasks includes 771,500 samples, each comprising 18 machine-generated ECG reports based on rules and the corresponding ECG data. After clinical NER and dedplication on the 18 ECG reports of each sample, the dataset holds 295 labels of professional medical terminologies.

\noindent \textbf{PTB-XL.}
%
PTB-XL\footnote{\href{https://physionet.org/content/ptb-xl/1.0.3/}{https://physionet.org/content/ptb-xl/1.0.3/}.} is a large open-source ECG dataset, comprising 21,799 clinical ECG records from 18,869 patients, with each lead sampled at a rate of 500 Hz and a duration of 10 seconds \citep{wagner2020ptb}. A total of 71 different ECG reports are SCP-ECG compliant, covering diagnostic, form and rhythm reports.
%
PTB-XL also provides a recommended train-test split and includes multi-level ECG annotations, covering Superclass (5 categories), Subclass (23 categories), Form (19 categories), and Rhythm (12 categories). Notably the 4 subsets have different sample sizes.

\noindent \textbf{CPSC-2018.} 
%
The CPSC-2018\footnote{\href{http://2018.icbeb.org/Challenge.html}{http://2018.icbeb.org/Challenge.html}.} dataset originates from the China Physiological Signal Challenge (CPSC) 2018, including 6,877 records from 9,458 patients, with durations ranging from 6 to 60 seconds \citep{liu2018open}. The standard 12-lead ECG data is sampled at a rate of 500 Hz, collected from 11 hospitals and categorized into 9 different labels: 1 normal type and 8 abnormal types.

\noindent \textbf{Chapman-Shaoxing-Ningbo (CSN).}
%
The CSN\footnote{\href{https://physionet.org/content/ecg-arrhythmia/1.0.0/}{https://physionet.org/content/ecg-arrhythmia/1.0.0/}.} 12-lead ECG dataset is created with the support of Chapman University, Shaoxing People's Hospital and Ningbo First Hospital, which includes 12-lead ECGs from 45,152 patients, with a sampling rate of 500 Hz and a duration of 10 seconds \citep{zheng2020optimal, zheng2022large}. It contains expert annotated features that cover variety of common heart rhythms and other cardiovascular conditions. We exclude ECG records with "unknown" annotations and get 23,026 ECG records with 38 different labels.

\subsection{Downstream Data Split}
\label{app:datasplit}

For PTB-XL, we adopt the official train-test split recommended by the dataset authors \citep{wagner2020ptb}, ensuring consistency with prior works and a balanced distribution of ECG categories. This split is directly applied across the Superclass, Subclass, Form, and Rhythm subsets of PTB-XL. For CPSC-2018 and CSN, we follow the data splitting approach used by \citep{liu2024zero}, which randomly divides the datasets into training, validation, and testing subsets in a 70\%:10\%:20\% ratio.

Details of the splits, including the specific number of samples allocated to each subset, are summarized in Table \ref{tab:datasplit}.

\begin{table}[ht]
% \vspace{-5pt}
\centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule[1.2pt]
        Dataset & Category Number & Train Set & Validation Set & Test Set \\ 
        \midrule[1.2pt]
        \textbf{\textit{PTB-XL}} \\
        \midrule
        Superclass & 5 & 17,084 & 2,146 & 2,158 \\
        Subclass & 23 & 17,084 & 2,146 & 2,158 \\
        Form & 19 & 7,197 & 901 & 880 \\
        Rhythm & 12 & 16,832 & 2,100 & 2,098 \\
        \midrule
        \textbf{\textit{Others}} \\
        \midrule
        CPSC-2018 & 9 & 4,950 & 551 & 1,376 \\
        CSN & 38 & 16,546 & 1,860 & 4,620 \\
         \bottomrule[1.2pt]
    \end{tabular}
}
% \vspace{-5pt}
\caption{Data splits and sample distribution for downstream datasets.}
\label{tab:datasplit}
% \vspace{-15pt}
\end{table}

\subsection{Downstream Experiment Configuration}
\label{app:downstream}

The training configurations for downstream tasks, including optimizer, scheduler, and relevant hyperparameters, are detailed in Table \ref{tab:hyper}.

\begin{table*}[tb!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule[1.2pt]
            Hyperparameter & PTB-XL-Superclass & PTB-XL-Subclass & PTB-XL-Form & PTB-XL-Rhythm & CPSC-2018 & CSN\\
            \midrule[1.2pt]
            \textbf{\textit{Optimizer}} \\
            \midrule
            Type & AdamW & AdamW & AdamW & AdamW & AdamW & AdamW \\
            Learning Rate & 1e-3 & 1e-3 & 1e-3 & 1e-3 & 1e-3 & 1e-3 \\
            Weight Decay & 1e-8 & 1e-8 & 1e-8 & 1e-8 & 1e-8 & 1e-8 \\
            \midrule
            \textbf{\textit{Scheduler}} \\
            \midrule
            Type & Cosine Anealing & Cosine Anealing & Cosine Anealing & Cosine Anealing & Cosine Anealing & Cosine Anealing \\
            Warmup Steps & 5 & 5 & 5 & 5 & 5 & 5 \\
            \midrule
            \textbf{\textit{General}} \\
            \midrule
            Batch Size & 16 & 16 & 16 & 16 & 16 & 16 \\
            Epochs & 100 & 100 & 100 & 100 & 100 & 100 \\
            \bottomrule[1.2pt]
        \end{tabular}
    }
% \vspace{-5pt}
\caption{Downstream dataset information and split proportions.}
\label{tab:hyper}
% \vspace{-10pt}
\end{table*}

\subsection{Performance on Linear probing.}
\label{app:lp}

Table~\ref{tab:linearprobing} shows the linear probing AUC performance of SuPreME's and other eSSLs' ECG encoders on specific six downstream datasets.

\begin{table*}[tb!]
% \vspace{-20pt}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc|ccc|ccc|ccc|ccc|ccc}
    \toprule[1.2pt]
     & \multicolumn{3}{c}{PTB-XL-Superclass} & \multicolumn{3}{c}{PTB-XL-Subclass} 
     & \multicolumn{3}{c}{PTB-XL-Form} & \multicolumn{3}{c}{PTB-XL-Rhythm} 
     & \multicolumn{3}{c}{CPSC-2018} & \multicolumn{3}{c}{CSN} \\
    Framework & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% 
           & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% & 1\% & 10\% & 100\% \\
    \midrule[1.2pt]
    \textbf{\textit{From Scratch}} \\
    \midrule
    Random Init (CNN) & 70.45 & 77.09 & 81.61 & 55.82 & 67.60 & 77.91 & 55.82 & 62.54 & 73.00 & 46.26 & 62.36 & 79.29 & 54.96 & 71.47 & 78.33 & 47.22 & 63.17 & 73.13 \\
    Random Init (Transformer) & 70.31 & 75.27 & 77.54 & 53.56 & 67.56 & 77.43 & 53.47 & 61.84 & 72.08 & 45.36 & 60.33 & 77.26 & 52.93 & 68.00 & 77.44 & 45.55 & 60.23 & 71.37 \\
    \midrule
    \textbf{\textit{ECG Only}} \\
    \midrule
    SimCLR & 63.41 & 69.77 & 73.53 & 60.84 & 68.27 & 73.39 & 54.98 & 56.97 & 62.52 & 51.41 & 69.44 & 77.73 & 59.78 & 68.52 & 76.54 & 59.02 & 67.26 & 73.20 \\
    BYOL & 71.70 & 73.83 & 76.45 & 57.16 & 67.44 & 71.64 & 48.73 & 61.63 & 70.82 & 41.99 & 74.40 & 77.17 & 60.88 & 74.42 & 78.75 & 54.20 & 71.92 & 74.69 \\
    BarlowTwins & 72.87 & 75.96 & 78.41 & 62.57 & 70.84 & 74.34 & 52.12 & 60.39 & 66.14 & 50.12 & 73.54 & 77.62 & 55.12 & 72.75 & 78.39 & 60.72 & 71.64 & 77.43 \\
    MoCo-v3 & 73.19 & 76.65 & 78.26 & 55.88 & 69.21 & 76.69 & 50.32 & 63.71 & 71.31 & 51.38 & 71.66 & 74.33 & 62.13 & 76.74 & 75.29 & 54.61 & 74.26 & 77.68 \\
    SimSiam & 73.15 & 72.70 & 75.63 & 62.52 & 69.31 & 76.38 & 55.16 & 62.91 & 71.31 & 49.30 & 69.47 & 75.92 & 58.35 & 72.89 & 75.31 & 58.25 & 68.61 & 77.41 \\
    TS-TCC & 70.73 & 75.88 & 78.91 & 53.54 & 66.98 & 77.87 & 48.04 & 61.79 & 71.18 & 43.34 & 69.48 & 78.23 & 57.07 & 73.62 & 78.72 & 55.26 & 68.48 & 76.79 \\
    CLOCS & 68.94 & 73.36 & 76.31 & 57.94 & 72.55 & 76.24 & 51.97 & 57.96 & 72.65 & 47.19 & 71.88 & 76.31 & 59.59 & 77.78 & 77.49 & 54.38 & 71.93 & 76.13 \\
    ASTCL & 72.51 & 77.31 & 81.02 & 61.86 & 68.77 & 76.51 & 44.14 & 60.93 & 66.99 & 52.38 & 71.98 & 76.05 & 57.90 & 77.01 & 79.51 & 56.40 & 70.87 & 75.79 \\
    CRT & 69.68 & 78.24 & 77.24 & 61.98 & 70.82 & 78.67 & 46.41 & 59.49 & 68.73 & 47.44 & 73.52 & 74.41 & 58.01 & 76.43 & 82.03 & 56.21 & 73.70 & 78.80 \\
    ST-MEM & 61.12 & 66.87 & 71.36 & 54.12 & 57.86 & 63.59 & 55.71 & 59.99 & 66.07 & 51.12 & 65.44 & 74.85 & 56.69 & 63.32 & 70.39 & 59.77 & 66.87 & 71.36 \\
    \midrule
    \textbf{\textit{Multimodal Learning}} \\
    \midrule
    MERL (ResNet) & \textbf{82.39} & \textbf{86.27} & \textbf{88.67} & \colorbox{gray!15}{64.90} & \textbf{80.56} & \colorbox{gray!15}{84.72} & \colorbox{gray!15}{58.26} & \textbf{72.43} & \textbf{79.65} & \colorbox{gray!15}{53.33} & \textbf{82.88} & \textbf{88.34} & \textbf{70.33} & \textbf{85.32} & \textbf{90.57} & \textbf{66.60} & \textbf{82.74} & \colorbox{gray!15}{87.95} \\
    MERL (ViT) & \colorbox{gray!15}{78.64} & \colorbox{gray!15}{83.90} & 85.27 & 61.41 & \colorbox{gray!15}{77.55} & 82.98 & 56.32 & \colorbox{gray!15}{69.11} & \colorbox{gray!15}{77.66} & 52.16 & \colorbox{gray!15}{78.07} & 81.83 & \colorbox{gray!15}{69.25} & \colorbox{gray!15}{82.82} & \colorbox{gray!15}{89.44} & 63.66 & \colorbox{gray!15}{78.67} & 84.87 \\
    \midrule
    \textbf{SuPreME (Ours)} & 73.58 & 79.07 & \colorbox{gray!15}{87.67} & \textbf{66.30} & 74.20 & \textbf{84.84} & \textbf{58.94} & 58.93 & 74.06 & \textbf{56.92} & 76.27 & \colorbox{gray!15}{84.42} & 58.28 & 70.51 & 86.74 & \colorbox{gray!15}{65.42} & 75.08 & \textbf{89.16} \\
    \bottomrule[1.2pt]
\end{tabular}
}
% \vspace{-5pt}
\caption{Specific linear probing performance of SuPreME and eSSLs across six downstream datasets. Best results are \textbf{bolded} and second best \colorbox{gray!15}{gray}-flagged.}
\label{tab:linearprobing}
% \vspace{-20pt}
\end{table*}

\subsection{Performance on Specific Cardiac Conditions}
\label{app:scatters}

\textbf{PTB-XL-Superclass.} Figure~\ref{fig:scatter_ptbxl_superclass} records the AUC performance of SuPreME on specific cardiac conditions in PTB-XL-Superclass dataset.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Superclass.pdf}
    % \vspace{-15pt}
    \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in PTB-XL-Superclass.}
    \label{fig:scatter_ptbxl_superclass}
    % \vspace{-5pt}
\end{figure}

\textbf{PTB-XL-Subclass.} Figure~\ref{fig:scatter_ptbxl_subclass} records the AUC performance of SuPreME on specific cardiac conditions in PTB-XL-Subclass dataset.

\newpage

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Subclass-1.pdf}
    % \vspace{-15pt}
    % \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in PTB-XL-Subclass.}
    % \label{fig:scatter_ptbxl_subclass}
    \vspace{-50pt}
\end{figure}

\newpage

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Subclass-2.pdf}
    \vspace{-15pt}
    \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in PTB-XL-Subclass.}
    \label{fig:scatter_ptbxl_subclass}
    \vspace{-5pt}
\end{figure}

\textbf{PTB-XL-Form.} Figure~\ref{fig:scatter_ptbxl_form} records the AUC performance of SuPreME on specific cardiac conditions in PTB-XL-Form dataset.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Form-1.pdf}
    % \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in PTB-XL-Form.}
    % \label{fig:scatter_ptbxl_form}
    \vspace{-50pt}
\end{figure}

\newpage

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Form-2.pdf}
    % \vspace{-15pt}
    \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in PTB-XL-Form.}
    \label{fig:scatter_ptbxl_form}
    % \vspace{-5pt}
\end{figure}

\textbf{PTB-XL-Rhythm.} Figure~\ref{fig:scatter_ptbxl_rhythm} records the AUC performance of SuPreME on specific cardiac conditions in PTB-XL-Rhythm dataset.

\begin{figure}[h!]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/PTB-XL-Rhythm.pdf}
    % \vspace{-15pt}
    \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in PTB-XL-Rhythm.}
    \label{fig:scatter_ptbxl_rhythm}
    % \vspace{-100pt}
\end{figure}

\textbf{CPSC-2018.} Figure~\ref{fig:scatter_cpsc} records the AUC performance of SuPreME on specific cardiac conditions in CPSC-2018 dataset.

\newpage

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/CPSC-2018.pdf}
    % \vspace{-15pt}
    \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in CPSC-2018.}
    \label{fig:scatter_cpsc}
    % \vspace{-5pt}
\end{figure}

\textbf{CSN.} Figure~\ref{fig:scatter_csn} records the AUC performance of SuPreME on specific cardiac conditions in CSN dataset.

\begin{figure}[h]
    % \vspace{-5pt}
    \includegraphics[width=\columnwidth]{figures/CSN-1.pdf}
    % \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in CSN.}
    % \label{fig:scatter_csn}
    \vspace{-100pt}
\end{figure}

\newpage

\begin{figure}[htb!]
    % \vspace{-10pt}
    \includegraphics[width=\columnwidth]{figures/CSN-2.pdf}
    % \vspace{-15pt}
    \caption{Zero-shot learning performance of SuPreME and its variants on specific categories in CSN.}
    \label{fig:scatter_csn}
    % \vspace{-5pt}
\end{figure}

\end{document}
