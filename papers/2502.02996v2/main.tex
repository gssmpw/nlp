%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{enumitem}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{subcaption}


\newcommand{\qb}[1]{\textcolor{orange}{\textbf{[QB:}  #1]}}
\newcommand{\ls}[1]{\textcolor{olive}{\textbf{[LS:}  #1]}}
%\newcommand{\fb}[1]{\textcolor{red}{\textbf{[FB:}  #1]}}
\newcommand{\tsf}[1]{\text{\sf #1}}


\newcommand{\pred}{\text{pred}}
\DeclareMathOperator*{\lse}{\text{LSE}}
\DeclareMathOperator*{\softmax}{\text{softmax}}
\DeclareMathOperator*{\logsoftmax}{\text{logsoftmax}}
\DeclareMathOperator*{\argmin}{\text{argmin}}
\DeclareMathOperator*{\argmax}{\text{argmax}}

\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}

\newcommand{\E}{\mathbf{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rb}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Building Bridges between Regression, Clustering, and Classification}

\begin{document}

\twocolumn[
\icmltitle{Building Bridges between Regression, Clustering, and Classification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lawrence Stewart}{inriaens}
\icmlauthor{Francis Bach}{inriaens}
\icmlauthor{Quentin Berthet}{gdm}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{inriaens}{INRIA \& ENS \\ Paris, France}
\icmlaffiliation{gdm}{Google DeepMind \\ Paris, France}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Lawrence Stewart}{lawrence.stewart@ens.fr}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.




\begin{abstract}
Regression, the task of predicting a continuous scalar target $y$ based on some features $x$ is one of the most fundamental tasks in machine learning and statistics. It has been observed and theoretically analyzed that the classical approach, mean-squared error minimization, can lead to suboptimal results when training neural networks. In this work, we propose a new method to improve the training of these models on regression tasks, with continuous scalar targets. Our method is based on casting this task in a different fashion, using a target encoder, and a prediction decoder, inspired by approaches in classification and clustering. We showcase the performance of our method on a wide range of real-world datasets.
\end{abstract}

\section{Introduction}

Neural network architectures have become ubiquitous in machine learning, becoming the de facto go-to models for a wide array of tasks. This is particularly true for classification tasks, where the goal is to predict a discrete label based on observed features---e.g., in image classification \cite{krizhevsky2012imagenet, he2016deep}, language modeling \cite{sutskever2014sequence, bahdanau2015neural, vaswani2017attention}, and audio generation \cite{borsos2023audiolm, van2016wavenet}. Whilst attaining state-of-the-art results on regression problems, e.g., pose estimation, point estimation and robotics \citep{sun13deep, toshev2014deeppose, belagiannis2015robust,liu20163d}, the amount of scientific work applying neural networks to classification tasks significantly outweighs that for regression problems \citep[see, e.g.,][and references therein]{stewart2023regression}, where the objective is to predict a real-valued target $y\in \mathbb{R}^m$.


A widely observed phenomenon is that the discretization of a regression problem (sometimes referred to as ``binning'') can be beneficial for these problems. There, one transforms the real-valued labels into one-hot vectors, allowing for one to optimize the neural network's weights by minimizing the cross-entropy loss, instead of the square loss typically seen in standard regression. Real-valued predictions can be obtained from the predicted probabilities of a classification model by taking the expected value over the midpoints of the bin. Surprisingly, such discretizations can often yield better performance, despite the cross entropy loss having no notion of distance.

This behavior has been reported across a range of disciplines, e.g., computer vision \citep{zhang2016colorful, van2016pixel}, robotics \citep{rogez2017lcr,akkaya2019solving}, reinforcement learning \citep{schrittwieser2020mastering, farebrother2024stop}, biology \citep{gao2024foldtoken,picek2024geoplant}, among others \citep{lee2024binning,abe2023pathologies,ansari2024chronos}.

Understanding the cause of this pattern remains an open research problem. Analyzing the gradient dynamics of over-parametrized neural networks, \citep{globalconvergence_bach,chistikov2023learning,boursier2022gradient}, \citet{stewart2023regression} show that the implicit bias of models trained on the square loss can lead to convergence to spurious minima;  reformulating the problem to classification was observed to alleviate the under-fitting due to a change in the implicit bias. \citet{grinsztajn2022tree} observe empirically that neural networks can under-perform on regression problems due to their bias to overly-smooth solutions, as well as the lack of robustness of dense multilayer perceptron (MLP) layers to uninformative features, supporting the prior claim.

% classical problems have been tailored to fit this framework, and models are trained by minimizing a cross entropy loss between their logits and the target labels.
% \ls{Worth citing real world cases where they did discrete binning. see our last paper}





% This pattern extends to a wide range of problems that would be more classically framed as regression problems. Both classification and regression are supervised learning problems, but in the latter, the target data that we aim to predict is continuous, often scalar, rather than a discrete object. Regression, and the associated classical methods and losses to solve it - such as least-squares, are some of the oldest and most classical statistical and machine learning tasks. However, perhaps surprisingly, these methods have been observed to perform poorly on these tasks, across a wide range of problems, both in supervised learning \cite{sun13deep,}, and in downstream applications such as in reinforcement learning \cite{farebrother2024stop}. There, these problems are re-cast as classification problems. This can be very naturally achieved by quantization choosing a finite number of possible values, thus making the target space discrete and finite, assigning a class label to each of the possible values (see \citet{stewart2023regression, farebrother2024stop} as well as Section~\ref{sec:methods} and Figures~\ref{fig:framework} and~\ref{fig:embeding} for further details). A model can then be trained on this classification tasks with these newly minted class labels - note that this is done independently of the actual {\em values} associated to the classes - e.g. by minimizing a cross-entropy loss. At inference, we can then predict the prediction of a class with the value it is associated to.

% Despite the apparent naive approach of this procedure, and the risk that training without using the values associated to each class (such as ordinal information) would harm performance, it has been widely used across a wide range of problems. This phenomenon has also been analyzed from a theoretical point of view, and connected to implicit biases in training dynamics \cite{stewart2023regression}.

However, there are some limitations to reformulating regression problems as classification, including excessive quantization in the outputs of the model and inefficient binning of the target space, which can harm the test-time performance and also make training less efficient. In this work, we propose a generalization of these methods centered around the use of a learned target encoder-decoder pair, which allows for the end-to-end learning of the transformations that (1) generate the distributional representation of target data (i.e., the encoding), and (2) decode the distributional representation back into the target space. 



These methods offer several advantages: firstly, we show that they allow for additional improvements in prediction performance over the known gain in the usual comparisons between regression and classification. One of the explanations for these improvements is that embedding the low-dimensional target space (especially when it is scalar) into an intermediate continuous space (distributions over $k$ classes) improves the training dynamics when using high-dimensional features $x \in \cX$. 

% FB:  "including as" is unclear
% We show that these gains are available even with very simple architectures, which conveniently allow for interpretability, including as a soft version of the usual one-hot encodings, and a latent probabilistic model. 

% LS: modified to this --> LMK if this is what you had in mind @quentin

We demonstrate that these gains can be achieved even with simple architectures (a logistic model). Moreover, one can interpret the target encoder as a probabilistic latent model, which provides a smoother alternative to traditional one-hot encodings.

We also show that framing the problem in this fashion allows us to interpolate smoothly between different objectives, connecting in a natural and less binary fashion the regression and classification tasks, but also both supervised and unsupervised approaches to the target encoding. %In particular, the approaches to target encoding that we consider are also naturally related to clustering of the target space---see Section~\ref{sec:discussion} for further discussion. Finally, not only does the simplicity of this method allows us to implement over a wide range of settings, we also show in our evaluations that even very small prediction models can be trained to be performant over various datasets. 



%the simplicity of this approach, allows to easily 

% and how it remedies details of the previous approach.

%Several interpretations

%Generalizes (subset) - probabilistic and interpretability %general target encoder and decoder to obtain target distributions and 

%add soft binning, add the general point of view of target encoding, add joint training


%Natural link with clustering, discussion on clustering

%Clear explanation how the encoded labels with cross entropy loss can be also viewed as a new regression, (bridging the gap)



%Prior literature including earlier work, work we previously cited and new works that cited our earlier work.


%\qb{move the following to the discussion section}


\paragraph{Main contributions.} In this work, we introduce a general framework for supervised regression tasks. To summarize, we make the following contributions:
\begin{itemize}[topsep=0pt,itemsep=2pt,parsep=2pt,leftmargin=10pt]

    \item We introduce a range of methods, based on the idea of target encoding into a distribution space, to improve the performance, thereby generalizing the framing of regression problems as classification.

    \item We consider in these methods a differentiable and smooth target encoding, which allows us to learn the target encoding from data, both in an unsupervised fashion from targets, and as part of a joint end-to-end loss.

    \item We showcase the improvements that our methods obtain over existing approaches over a wide range of datasets, for different data modalities, with $25 \%$ improvement in average over the least-squares baseline in regression tasks, for our fully end-to-end method.
\end{itemize}


\paragraph{Notations.} We denote by $\cX$ a general space of features, and by $\R^m$ the canonical real vector space of dimension $m$ for some positive integer $m \ge 1$, and $e_i$ the $i$-th element of its canonical basis (i.e., the one-hot vector for label $i$). For any positive integer $k$, we denote by $[k]$ the finite set $\{1, \ldots, k\}$, and by $\Delta_k \subset \R^k$ the unit simplex in dimension~$k$, of vectors with nonnegative coefficients that sum to 1. It is the convex hull of $e_1, \ldots, e_k$, and the space of discrete probabilities over $k$ elements. We denote by $H$ the entropy function from $\Delta_k$ to $\R$, defined for any $p \in \Delta_k$ by
\[
H(p) = - \sum_{i\in[k]} p_i \log(p_i)\, ,
\]
and by $\tsf{KL}$ the associated Kullback-Leibler divergence, defined for any $p, q \in \Delta_K$ by
\[
\tsf{KL}(p, q) = \sum_{i\in[k]} p_i \log\Big(\frac{p_i}{q_i}\Big)\, .
\]
We also define the $\text{softmax}$ function from $\R^k$ to $\Delta_k$, defined for $x \in \R^k$, elementwise for all $i \in [k]$ by
\[
\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{j \in [k]} \exp(x_j)}\, .
\]
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.85\textwidth]{images/framework_final.pdf}

    \caption{Framework description. Our framework is based on a \textbf{target encoder} $\psi_w$ (in red) that yields for each $y$ an encoded distribution $\psi_w(y)$ over $k$ classes. A \textbf{classification model} $\pi_\theta = \softmax(g_\theta)$ is trained with a KL objective on this distribution. A \textbf{decoder model} $\mu$ (in blue) decodes this distribution in the target space $\R^m$. The target encoder and decoder can be trained using an autoenconding loss, as well as a joint end-to-end objective (see Section~\ref{sec:methods}).}
    \label{fig:framework}
\end{figure*}

\section{Problem formulation and methods}
\label{sec:methods}

 

We are interested in this work in regression problems, where the aim is to infer a potentially multivariate continuous target $y \in \R^m$ based on observed features $x \in \cX$. This supervised learning problem can be tackled by using a parametrized predictor function that can be trained on a dataset of coupled examples $(x_i, y_i) \in \cX \times \R^m$, $i \in [n]$.

The several approaches to train this function that we consider follow broadly two settings and architectures, as described in Figure~\ref{fig:framework}. The most common, and most end-to-end approach is to predict directly $z = f_\eta(x) \in \R^m$, for a parametrized function (with parameter $\eta$) $$f_\eta: \cX \to \R^m,$$ and to compare it to $y$. In the approach that we propose, we consider instead several elements. The first one is a \textbf{target encoder model}, a parametrized function (with parameter~$w$) $$\psi_w:\R^m\to\Delta_k,$$ for some integer $k$. It is used to map the target $y$ to a vector of probabilities over $k$ classes. The second one is a \textbf{classification model} with logits  (parametrized by $\theta$) $$g_\theta:\cX\to\R^k,$$ used to predict a probability vector
$$\pi_\theta(x) = \softmax(g_\theta(x)) \in \Delta_k.$$ Finally, we consider a \textbf{decoder model} in the form of a linear head parametrized by a matrix $\mu \in \R^{k \times m}$, used to predict 
$$z = \mu^\top \pi_\theta(x) \in \R^m.$$
 
%\qb{what do you think about adding a small paragraph earlier here, introducing $f_\eta$, $g_\theta$, maybe also $\pi_\theta = \softmax(g_\theta)$, the encoder $\psi_w$ and decoder $\mu$? It could make the following paragraphs lighter, but possibly more confusing also.}

As discussed in further details below (see Section~\ref{sec:discussion}), this simple decoder allows for simple interpretability: each of the $k$ classes is associated to a decoder $\mu_i \in \R^m$, and the decoded prediction is an average of all the $\mu_i$, weighted by the probabilities $\pi_\theta(x)$.

\subsection{Least-squares regression}
\label{sec:least-squares}
As described above, the first classical baseline that we consider is end-to-end direct prediction of $z = f_\eta(x)$, for a parametrized function $f_\eta: \cX \to \R^m$. The parameters $\eta$ of $f_\eta$ are often trained by minimizing a loss function of the form $\ell(y,z) = L(y-z)$, with typically, $L(y-z) = \| y - z\|_2^2$. Other losses $L$ have been considered to improve robustness, such as the Huber loss \cite{huber1964robust}, or 
even nonconvex functions \citep[see, e.g.,][and references therein]{barron2019general}.

This approach consists in classical end-to-end training aiming to solve
\begin{equation}\label{eq:regression_baseline1}
    \min_{\eta} \E_{(x, y)} \big[ L (y- f_\eta(x)) \big],
\end{equation}
where $ \E_{(x, y)}$ denotes a potentially empirical expectation over features $x$ and responses $y$.

As shown in earlier work, implicit bias in regression sometimes leads to underfitting \cite{grinsztajn2022tree, stewart2023regression}. 

\subsection{Least squares with softmax layer}
\label{sec:regression-softmax}
Since several of the methods that we propose in this work (below) reframe this task using a classification model $\pi_\theta = \softmax(g_\theta)$ with outputs in $\Delta_k$ and prediction using a linear layer, with $z = \mu^\top \pi_\theta(x) \in \R^m$, we also consider the case where $f_\eta$ has this specific architecture and also compare in all our results the performance of our method with regression
\begin{eqnarray}
\label{eq:regression-softmax}
& & \min_{\theta, \mu}\! \E_{(x, y)} \big[ L (y- \mu^\top \pi_\theta(x)) \big] \\
\notag & \!\!\!\!=\!\!\! & \min_{\theta,  \mu }\! \E_{(x, y)} \big[ L (y- \mu^\top \softmax(g_\theta(x)) \big].
\end{eqnarray}
When the logit vector $g_\theta$ is a neural network output. This corresponds to adding an extra layer with $k$ neurons and a joint softmax non-linearity. The parameters $\theta$ and $\mu$ can be trained by end-to-end learning by first-order methods such as stochastic gradient descent. In our experiments (see Section~\ref{sec:expe}), this often already improves over least-squares, but not as much as using the explicit output embedding $\psi_w$. % FB: I have added this to plant the message earlier

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{images/landscape_and_hist_grid_final.pdf}
    \hspace{1cm}
    \includegraphics[width=0.85\textwidth]{images/landscape_and_hist_learnt_final.pdf}
    

    \caption{Embedding and binning the target space $\R^m$ (here $m=2$) into $\Delta_k$ (here $k=9$), for both a fixed grid of encoders (\textbf{Top}) and a learnt encoder (\textbf{Bottom}). For both cases we display the encoders, including an highlighted one, for a fixed $i \in [k]$ and a target $y \in \R^m$ (blue cross). We illustrate first {\em hard binning} (\textbf{Left}) where $y$ (and any $y$ in the same highlighted region) is assigned to one class (via a one-hot), and {\em soft binning} both with the contour plot of $\psi_w(\cdot)_i$ for one $i \in [k]$ (\textbf{Center}), and $\psi_w(y)$ as a distribution in $\Delta_k$ (\textbf{Right}).}
    \label{fig:embeding}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Hard-binning-encoder classification} 
\label{sec:hard-binning}
An alternative existing approach is to transform the problem, reformulating it as a classification problem. This can be done by partitioning the label space $\rb^m$ (often for $m=1$), effectively by implementing with the encoder model a map $\psi_w$ from $\rb^m$ to $[k]$, represented by one-hot vectors, the extreme points of $k$-dimensional simplex $\Delta_k \subset \rb^k$.  

The main idea behind this method is to divide the target space into bins, and to identify each bin as a classification label, in order to train a classification model for prediction. This can be achieved with $k$ center points $c_1, \ldots, c_k \in \R^m$, mapping each $y$ to the label one-hot representing the nearest center---in this case $\psi_w(y) = e_i$, where $i = \argmin_{i \in [k]} \|y - c_i\|_2^2$. This approach can be interpreted as vector quantization in the target space \citep{van2017neural}.

A classification model is then trained on these newly discretized labels for the logits $g_\theta: \cX \to \R^k$ by minimizing a classification loss between the encoded target $\psi_w(y)$ and $\pi_\theta(x) = \softmax(g_\theta(x))$, such as the Kullback-Leibler divergence
\begin{equation}
\label{eq:classif-kl}
    \min_{\theta} \E_{(x, y)} \big[ \tsf{KL}(\psi_w(y) \| \pi_\theta(x))\big]\, .
\end{equation}
We note that up to a constant, this is equivalent to the more common cross-entropy loss 
\begin{eqnarray*}
    &  & \min_{\theta  } \E_{(x, y)} \big[ - \psi_w(y)^\top \log \pi_\theta(x) \big] \\
    & = &  \min_{\eta  } \E_{(x, y)} \big[ - \psi_w(y)^\top \log \softmax (g_\theta(x)) \big] \, .
\end{eqnarray*}
Indeed, these two losses only differ by a term equal to the entropy of $\psi_w(y)$. Furthermore, for this method, since the encoder maps to one-hot vectors, this entropy term is equal to 0. In a more general setting (see below) where the target encoder maps to soft vectors in the interior of the simplex, this term is either a constant (if the encoder is frozen, and we are only training the parameters $\theta$ of the classification model), or an additional term in the loss (if the two models are being trained jointly) and we state it explicitly. 

In order to use the classification model as a prediction function for $y \in \R^m$, we decode $\pi_\theta(x)$ in the target space by $z = \mu^\top \pi_\theta(x) = \mu^\top {\rm softmax}(g_\theta(x)) $ for some hand-picked decoder model $\mu \in \rb^{k \times m}$. A natural choice is to take $\mu_i = c_i$ for $i \in [k]$: when predicting the class $i$ (corresponding to targets that had $c_i$ as the nearest-center), the predicted value is $c_i \in \R^m$ (see Figure~\ref{fig:framework}).

This fixed set of encoder $\psi_w$ and linear decoder parametrized by $\mu$ has been considered for $m=1$ (e.g., by binning the $y$-space with equal size intervals or intervals with similar mass under $y$) \citep[see, e.g.][]{stewart2023regression}. This particular encoding is similar to a \textbf{clustering} approach for the target space (here done with fixed centroids). We note that other choices of one-hot encoders and decoders are also possible. %\qb{add something about limit of this approach}



\subsection{Soft-binning-encoder classification} 
\label{sec:soft-binning}
The first generalization that we propose in this work is to modify the classification method, by using more general target encoders that utilize the whole simplex (not only one-hot vectors). In particular, to generalize binning around $k$ centers $c_1, \ldots, c_k \in \R^m$, we consider a so-called {\em soft labels}, akin to performing a soft binning of the target space. Similarly to the previous approach, a classification model $\pi_\theta = \softmax(g_\theta)$ is then trained on these soft labels using the KL divergence as described in Equation~(\ref{eq:classif-kl}).

One way to implement this {\em soft partition} is by taking $\psi_w$ a target encoder that approximates the one-hot binning by replacing max by softmax: % FB: please check below
\begin{align*}
    \psi_w(y) &= \textstyle \text{softmax}\big(- \frac{\|c_1 - y\|_2^2}{2\sigma^2},\dots,- \frac{\|c_k - y\|_2^2}{2\sigma^2}\big)\\
    &=  \textstyle  \softmax \big(\frac{c_1^\top y - \frac{1}{2}\|c_1\|_2^2}{\sigma^2},\dots,\frac{c_k^\top y - \frac{1}{2}\|c_k\|_2^2}{\sigma^2}  \big)\, , \stepcounter{equation} \tag{\theequation} \label{eq:encoder_gaussian_form}
\end{align*}
for  $\sigma>0$, i.e., for all $i \in [k]$,
\begin{align*}
\psi_w(y)_i &= \frac{\exp(-\frac{1}{2\sigma^2} \|c_i - y\|_2^2)}{\sum_{j \in [k]} \exp\big(-\frac{1}{2\sigma^2} \|c_j - y\|_2^2\big)}\\
    &= \frac{\exp\big(\frac{c_i^\top y}{\sigma^2} - \frac{\|c_i\|_2^2}{2\sigma^2} \big)}{\sum_{j \in [k]}\exp\big(\frac{c_j^\top y}{\sigma^2} - \frac{\|c_j\|_2^2}{2\sigma^2} \big)}\, .\stepcounter{equation} \tag{\theequation} \label{eq:encoder_reformulated form}
\end{align*}
The two representations are mathematically equivalent (all $k$ values differ only by $\|y\|_2^2/2\sigma^2$, and the softmax function is invariant by constant shifts). The latter shows that the encoder can take a convenient form (with affine logits)
\begin{equation}\label{eq:softmax_encoder}
    \psi_w(y) = \text{softmax}(w_1^\top y+ w_2)\, ,
\end{equation}
whist the prior is connected to a classical probabilistic interpretation of softmax regression by a generative model~\citep[see, e.g.,][Section 14.2]{ltfp}, since we then have
\[
\psi_w(y)_i = \mathbf{P}(Z = i | Y=y)\, , 
\]
in a probabilistic model with a latent variable $Z \in [k]$, and isotropic Gaussian class-conditional densities with mean $c_i$ and variance $\sigma^2 I$ for the distribution of $y$ given $Z=i$. This approach, in its full generality, extends upon soft labelling methods used by, e.g.,  \citet{imani2018improving, farebrother2024stop}.

The prediction model $\pi_\theta = \softmax(g_\theta)$ is then trained by minimizing the KL divergence between $\pi_\theta(x)$ and $\psi_w(y)$, both in $\Delta_k$ as in Equation~(\ref{eq:classif-kl}).


\subsection{Pre-trained encoder} 
\label{sec:pre-trained}
The second method that we propose is a further generalization on this method, by pre-training a target encoder-decoder $(\psi_w, \mu)$, instead of hand-picking it, e.g., by minimizing an auto-encoding objective (Stage 1)
\begin{equation}
\label{eq:autoencoder}
\min_{w, \mu}\E_{y} \big[ L(y - \mu^\top  \psi_w(y)) \big]\,,
\end{equation}
and then to use this frozen target encoder to generate soft-label targets $\psi_w(y)$, to train the classification model $\pi_\theta = \softmax(g_\theta)$  as in Equation~(\ref{eq:classif-kl}) (Stage 2).

Note that the first stage can be done without access to the features $x\in \cX$, and could even be performed with synthetic data (e.g., uniform sampling on the target space if it is compact). To generalize hand-picked soft encoders, it can be chosen as a simple  model, with architecture
\[
\psi_w(y) = \text{softmax}(w_\text{lin}^\top y+ w_\text{bias})\, .
\]

Naively minimizing the auto-encoder objective in Stage 1 can afflict an implicit bias to the encoder, and yield close-to-uniform $\psi_w(y)$. To avoid this effect we can penalize the entropy, that is, minimize instead
\begin{equation}
\label{eq:loss_codec}
\min_{w, \mu} \E_y \big[ L(y-\mu^\top \psi_w(y))  - \alpha H(\psi_w(y))],
\end{equation}
with a positive parameter $\alpha>0$.
%\qb{add a remark that $\alpha$ can be very small because not a perfec reconstruction}
%\qb{add a remark on still interpreting the encoder / decoder}
%, that we could choose, e.g., by taking the largest one such that the loss part
%$\E \big[ L(y-\mu^\top \psi_w(y)) \big] $ is at 1\% of $\min_{c \in \rb^n} \E \big[ L(y-c) \big]$ (that is, we lose only a small percent of performance compared to the constant predictor). This allows to have a ``parameter-free'' encoder-decoder (still with $k$ to be determined). 

% FB: we give up the result showing that if k>m, we can get trivially zero loss?

\paragraph{Initialization of encoder-decoder.}
For $m=1$, we propose initializing the decoder weights $\mu$ as a uniform spacing over the target space, where $\delta_\mu$ denotes the magnitude of the spacing. We remark that this closely resembles discretized binning \cite{stewart2023regression}. For the encoder weights, we propose setting $\sigma = \lambda_\sigma \cdot \delta_\mu$, e.g,. $\lambda_\sigma = 1$, and initializing with $c=\mu$ using the connection between Equations (\ref{eq:encoder_gaussian_form}) and~(\ref{eq:softmax_encoder}). For this initialization, the autoencoder loss $L(y - \mu^\top \psi_w(y))$ goes to $0$ for growing values of $k$, but we show experimentally that it is not necessary. For $m>1$, we suggest using a clustering algorithm such as K-means++ \citep{arthur2006k} to initialize~$\mu$. In this case $\delta_\mu$ would refer to average intra-cluster distance, and one can initialize the encoder weights in the same fashion as for $m=1$.

\begin{table*}[t]
\centering
\caption{Dataset properties. % FB: should be add commas? 5197 -> 5,197? # ls : ok , added them
\label{table:datasetproperties}}
\begin{tabular}{lccccccc|c}
\hline
 & \multicolumn{7}{c}{Tabular} & \multicolumn{1}{c}{Computer Vision} \\
 \hline
% \cline{2-8} \cline{9} 
 & WN & AE & BS & SC & EL & CA & DM  & RM \\
\hline
\#num. features & 7 & 33 & 6 & 79 & 16 & 21 & 6  & (3, 28, 28) \\
\#num. train points & 5,197 & 11,000 & 13,903 & 17,010 & 13,279 & 6,553 & 43,152 & 1,080 \\
\#num. val points & 650 & 1,375 & 1,738 & 2,126 & 1,660 & 819 & 5,394  & 120 \\
\#num. test points & 650 & 1,375 & 1,738 & 2,127 & 1,660 & 819 & 5,394  & 400 \\
Train batch size & 256 & 512 & 512 & 512 & 512 & 256 & 1,024  & 64 \\
% Label Type & $\mathbb{Z}$ & $\mathbb{R}$ & $\mathbb{Z}$ & $\mathbb{R}$ & $\mathbb{R}$ & $\mathbb{Z}$ & $\mathbb{R}$ & $\mathbb{R}$ & $\mathbb{Z}$ 
\hline
\end{tabular}
\end{table*}

\subsection{End-to-end joint encoder classification}
\label{sec:end-to-end}
Our third proposed method is to combine these different objectives to jointly train the targent encoder and decoder, as well as the classification objective in Equation~(\ref{eq:classif-kl}), by minimizing the following loss, with scalar hyperparameters $\lambda_{\tsf{auto}}, \lambda_{\tsf{KL}}, \lambda_{\tsf{pred}} \ge  0$:
\begin{align*}
\min_{w, \mu, \theta} &\;
\lambda_{\tsf{auto}}\E_y \big[ L(y-\mu^\top \psi_w(y))  - \alpha H(\psi_w(y))\big] 
\\
& \hspace*{1.5cm}+\lambda_{\tsf{KL}} 
 \E_{(x, y)} \big[ \tsf{KL} \big( \psi_w(y) \| \pi_\theta(x) \big)\big] \, . \stepcounter{equation} \tag{\theequation} \label{eq:kl_and_auto}
\end{align*}

The previous approach above can be thought of as minimizing with $\lambda_{\tsf{KL}} = 0^+$---or alternatively, with $\lambda_{\tsf{KL}} = 0$ and $\lambda_{\tsf{auto}} > 0$ in Stage 1, and $\lambda_{\tsf{KL}} > 0$ and $\lambda_{\tsf{auto}} = 0$ in Stage 2. Framing it in this fashion allows for more general training of these models.

We can also add a final term that allows to stabilize the prediction loss, that is, minimize
\begin{align*}
\min_{w, \mu, \theta} &\;
\lambda_{\tsf{auto}}\E_y \big[ L(y-\mu^\top \psi_w(y))  - \alpha H(\psi_w(y))\big] 
\\
&\hspace*{1.5cm} +\lambda_{\tsf{KL}} 
 \E_{(x, y)} \big[ \tsf{KL} \big( \psi_w(y) \| \pi_\theta(x) \big)\big] \\
&\hspace*{1.5cm} + \lambda_{\tsf{pred}} \E_{(x, y)} \big[ L(y - \mu^\top  \pi_\theta(x) )\big]\, . \stepcounter{equation} \tag{\theequation} \label{eq:joint}
\end{align*}


Optimizing with different values of the loss hyperparameters $\lambda_{\tsf{auto}}, \lambda_{\tsf{KL}}, \lambda_{\tsf{pred}}$ allows us to interpolate between the different methods considered above, since it considers a linear combination of their loss objectives.

%Starting from method $\#1$, method $\#2$ gradually increases $\beta$. As $\beta$ gets large, the model should start to collapse in having $ \psi_w(y)$ and ${\rm softmax}(f_\eta(x))$ uniform.
%We can do so by starting from $\gamma=0$ and $\beta=0+$ and gradually increase $\beta$ and $\gamma$ in a way to be determined. This is method $\#3$.



\section{Discussion}
\label{sec:discussion}

The methods that we propose are aligned with frequent observations that regression problems can be more efficiently once framed as classification problems, and in this work, we address the natural question of {\em how} they should be framed as such. Our approach to tackle this question is to use a target encoder and decoder pair, the two main advantages being that first, these models lead to {\em soft binning}, i.e., the targets are mapped not to one-hots (or labels over $k$ classes) but to whole distributions over $[k]$, and second that they are conveniently parametrized and therefore can be learnt from data, either in a two-stage, or an end-to-end fashion. As such, this work is part of a large literature on connecting discrete and continuous methods in end-to-end differentiable systems for machine learning \citep[see, e.g.][]{berthet2020learning, blondel2020fast, vlastelica2019differentiation, llinares2023deep, stewart2023differentiable}. 

Further, by smoothing over the transition between a discrete and a continuous task, the method that we propose leads to possible interpretability of the learnt codes as representations of the target data. As noted above, the decoded predictions are necessarily in the convex hull of the $\mu_i$'s, that can be interpreted as a quantization of the data. When there is an {\em a priori} natural  underlying clustering to the feature and target space, it is natural to investigate whether the learnt classes correspond to the natural ones. We observed in several experiments (see Section~\ref{sec:expe}) that the while the entropy of learnt encoded distributions $\psi_w(y) \in \Delta_k$ for targets $y$ from the data is quite low, these distributions are not typically very close to one-hots, as is more common in classification. The reason for this behavior could be connected to implicit biases and training dynamics as observed in a classification setting \cite{stewart2023regression}.

The final objective that we propose in Section~\ref{sec:end-to-end} is both strongly connected to the end-to-end paradigm of machine learning, as all objectives are jointly optimized, and going against it: naively optimizing an square loss over the same prediction $\mu^\top \pi_\theta(x)$ without considering a structured loss, with autoencoding and classification is not as performant (see Section~\ref{sec:expe}).

%There are several possible explanations for the

%There are several interesting directions for future research - first, the regression vs. classification paradigm has been theoretically studied in different settins, and it would be interesting to explain this phenomenon in a general encoding setting. On the applied side, we focused on supervised problems, what about settings where regression is used in a downstream fashion such as reinforcement learning \cite{farebrother2024stop}





\begin{figure*}[t]\label{fig:rmses_datasets}
    \centering
    \includegraphics[width=0.95\textwidth]{images/barplots_all_final.pdf}
    

    \caption{Experimental results across datasets. We report for all methods the test root mean squared error (rMSE), over 8 different datasets (see \textbf{Datasets} above), for the 6 methods listed in Section~\ref{sec:expe-methods}, all for $k=25$. They are displayed in each of the 8 groups from left to right. All are normalized to the error of the first baseline: least squares is set to 1.0 in each dataset, and the others proportionally. % FB: BIGGER FONTS
    }
    \label{fig:expe-barplots}
\end{figure*}

\section{Experiments}
\label{sec:expe}



\paragraph{Datasets.} We demonstrate our methodology across a diverse set of real-world regression datasets, spanning engineering, social sciences, medicine, physics, and other interdisciplinary fields, all of which are publicly available. In particular we use the following OpenML \citep{vanschoren2014openml} datasets: Ailerons (AE), Elevators (EL), Computer Activity (CA), Diamonds (DM);  the following UCI datasets: Wine Quality (WN) \citep{wine}, Bike Sharing (BS) \cite{bike_sharing}, Superconductivity (SC) \citep{superconduct}, as well as the Retina MNIST dataset (RM) from the Medical MNIST benchmark \cite{medmnistv2}. The train, validation, test split sizes and feature dimensions for each of the datasets are listed in Table \ref{table:datasetproperties}. For tabular data points, we applied min-max scaling, for images we standardize across channels, and all labels are scaled to $[0, 1]$.

% so that all features line in the unit interval. For image data points, we standardize by subtracting the mean and dividing by the standard deviation across channels. Finally, as common place in the regression literature, we scale all labels to $[0, 1]$.


\paragraph{Models.} For tabular datasets we followed the convention of prior literature \citep{gorishniy2021revisiting}, by using a multilayer perceptron (MLP), with hidden dimension 128, ReLU non-linearity, and a dropout \cite{srivastava2014dropout} of $0.3$. For image datasets we used a convolutional neural network \cite{lecun1998gradient}, using three layers of convolutions with average pooling between layers, followed by two fully-connected layers. For the convolutions, we use $(3, 3)$ kernel size, with a stride of one, and for the average pooling we use a $(2, 2)$ size with a stride of two. The two fully-connected layers have hidden dimension $256$, and use a dropout of $0.5$, with ReLU as the non-linearity. For the exact implementations of all models, data processing and training, we refer the reader to view our code repository, (implemented with PyTorch). 

% FB: should we mention that we are going to make the code availabl? I think we should
% LS :Agreed, added a sentence to say it will be available.


\paragraph{Training.}

We trained all models using the Adam optimizer \cite{kingma2014adam} with an $\ell_2$ weight decay of $10^{-4}$ for the MLP and encoder-decoder, whilst an $\ell_2$ decay of $10^{-2}$ for the CNN. All models use a gradient clipping equal to 1. The training batch sizes for all datasets are listed in Table~\ref{table:datasetproperties}. Hyper-parameters for experiments (e.g., max learning rate, $\lambda_{\tsf{KL}}$, $\lambda_{\tsf{auto}}$, $\lambda_{\tsf{pred}}$) were selected for each model via a log-space sweep. We run repeat trials of each experiment, for which we report mean values. All experiments were ran using an NVIDIA V100 GPU.
\begin{figure}[!t] 
    \centering

    \includegraphics[width=0.35\textwidth]{images/barplot_avg_final.pdf} 
    \caption{Average experimental results on average over all datasets. We observe an overall hierarchy between the different methods considered. % FB: BIGGER FONTS
    }
    \label{fig:dataset_average_global}
\end{figure}

\subsection{Comparison of methods}
\label{sec:expe-methods}
For each of the datasets, we trained models by minimizing the objectives listed throughout Section \ref{sec:methods}, namely:

\begin{enumerate}
    \item \textbf{Least-squares}: The most classical, end-to-end training loss for regression as our main baseline, see Section~\ref{sec:least-squares}.
    \item \textbf{Least-squares with softmax layer}: Allows one to compare how the capacity change of the network (by effectively adding an extra layer) affects performance, see Section~\ref{sec:regression-softmax}.
    % and outputs a linear function (parametrized by $\mu$), of this layer, in $\R^m$. These parameters are all trained jointly, in an end-to-end fashion, on the square loss.
    \item \textbf{Hard-binning encoder classification}: The most common practice discretization, to transform a regression problem into a classification task, as described by \citet{stewart2023regression}---see Section~\ref{sec:hard-binning}.
    \item \textbf{Soft-binning encoder classification}: We use instead a hand-picked, smooth target encoder and decoder pair, and train the model as in a classification task---see Section~\ref{sec:soft-binning}.
    \item \textbf{Pre-trained encoder classification}: We train the encoder-decoder pair (Equation \ref{eq:autoencoder}) in an unsupervised fashion on the targets, prior to classification training of the model---see Section~\ref{sec:pre-trained}.
    \item \textbf{End-to-end learning}: All terms in this task are jointly trained, as described in Equation~\eqref{eq:joint}---see Section~\ref{sec:end-to-end}. 
\end{enumerate}

% \paragraph{Influence of entropic regularization} We observed empirically that when initializing the encoder-decoder weights using our proposed methodology, the entropy coefficient $\alpha$ in equation \ref{eq:loss_codec} can be set to zero. Increasing the entropy regularization by small amounts gave no additional gain. We remark this is a consequence of our proposed initialization, and $\alpha > 0$ could be necessary for random initializations, to avoid the collapse to uniform $\psi_w(y)$, mentioned in Section \ref{sec:methods}.
% \qb{maybe rephrase this paragraph}

% \paragraph{Results.}
\subsection{Results}
For the provided list of methodologies, we report the normalized test set root mean square error (RMSE) across all datasets, evaluating with the model weights which attained the best validation set RMSE. We evaluate all methodologies for $k\in \{5, 15, 25\}$. For $k=25$, the results across datasets are depicted in Figure \ref{fig:expe-barplots}, with the global dataset average depicted in Figure \ref{fig:dataset_average_global}. We observed the same general behavior across all values of $k$. For a table containing the full set of results, we refer the reader to Appendix~\ref{app:expe}, Table~\ref{table:all_results}.


We remark that across all datasets, reformulating regression as a classification problem via both hard-binning and soft-binning yielded improvements, with soft-binning performing globally better. This reinforces the observations of prior literature \citep{stewart2023regression,farebrother2024stop}, and secondly demonstrates the benefits of mapping targets into the interior of the simplex (our proposed initialized encoder-decoder), rather than to an extremal one-hot vector (discretized binning).

Further, we observe that training a classification model on targets generated from a trained encoder-decoder model, yielded better performance across datasets than both hand-picked soft and hard-binning. Fitting our proposed softmax encoder-decoder on the train targets is both fast and computationally light-weight, and is also promising for scenarios where the auto-encoding loss (Equation (\ref{eq:autoencoder})) at initialization can be decreased substantially (for example, in a case where $k$ is not large enough for the target distribution of $y\in \mathbb{R}^m$).


For one of our baselines, a least-squares objective for a model with softmax layer, we can see that adding the decoder's extra trainable parameters to the regression model and training with the square loss results in varied results. For some datasets (e.g., Super Conductivity), it leads to performance gains (likely due to a greater model capacity), whilst for others (and globally on average), it leads to degraded performance, even compared to the initial least-squares baseline. Our gains are therefore not due to architectural choices and the presence of a softmax layer.

Finally, it can be seen  from Figures \ref{fig:expe-barplots} \& Table \ref{table:all_results} that the proposed ``end-to-end'' objective (Equation (\ref{eq:joint})) leads to the best performance across all datasets. We stipulate this is because this approach (1) optimizes the encoder-decoder to attain a low auto-encoding error, (so decoding of classification model that has learned to predict with high accuracy the target encoding would result in a low RMSE), and (2) bridges classification and regression, with the prior potentially yielding the benefits of task reformulation, and the latter ensuring both the classification model and decoder are jointly trained by gradients coming from the regression objective.

\paragraph{Hyper-parameters.} A key hyper-parameter of our methods, as well as hard-binning is the choice of $k$, the number of classification classes (or size of the encoded distribution). For small $k$, the encoder-decoder will have less capacity to auto-encode the targets, which may hurt performance on the regression task, but larger values of $k$ yield increased optimization costs. Figure \ref{fig:ablation} depicts the relationship between $k$ and final test RMSE for soft-binning encoder classification. As $k$ increases, we can see improvements in performance, followed by a plateau with no further gains. We conclude that the choice of $k$ depends on the exact model, dataset and optimization used.

We observed empirically that when initializing the encoder-decoder weights using our proposed methodology,  the results are robust across datasets to the choice of the entropic regularization coefficient $\alpha$ in Equation (\ref{eq:loss_codec}), and taking a very small value, e.g., $10^{-6}$ suffices, (on the other hand, too large values of $\alpha$ will negatively affect the auto-encoding loss listed in Equation (\ref{eq:autoencoder})).

For end-to-end training, we observed that it is important to find a good balance between the classification and regression loss terms, via the choice of $\lambda_{\tsf{KL}}$ and $\lambda_{\tsf{pred}}$. % FB: select??
Whilst for some select datasets and values of $k$, we observed that training with only the $\tsf{KL}$ objective on fitted encodings produced similar performance, we overall found that there were no single values of $\lambda_{\tsf{KL}}$, $\lambda_{\tsf{pred}}$ that were optimal for all datasets. In general we set $\lambda_{\tsf{pred}} = 1$ and performed a sweep to find $\lambda_{\tsf{KL}}$, the best values for each dataset being listed in Table \ref{table:kl_sweep} 
within Appendix \ref{app:expe}. Figure \ref{fig:ablation} depicts impact of this parameter for the DM dataset, and highlights how the combination of the regression and classification loss can lead to better results than just one of the losses. We remark that overall, the dependency of the final results on these hyper-parameters was low, indicating a robustness to these choices.

% FB: we never say that if $\lambda_{KL}=0$ we are only keeping the $\mu$'s


\paragraph{Conclusion.} For regression problems we have proposed  introducing a light-weight target encoder-decoder, trained jointly (or frozen) with a classification model using a loss (Equation \ref{eq:joint}) that balances regression, classification and auto-encoding of the targets. We empirically explore the effect of each of our proposed generalizations (Section \ref{sec:methods}), as well as ablating hyper-parameter choices. Notably, our end-to-end method consistently outperforming the prior regression and classification baselines \cite{stewart2023regression}, across a wide range of real-world datasets.






\begin{figure}[t] 
    \centering
    \vspace{-0.2em}
    \includegraphics[width=0.45\textwidth]{images/impact_k_final.pdf} 
    \includegraphics[width=0.45\textwidth]{images/impact_lambda_kl_final.pdf} 
    \caption{Impact of different architecture and training hyperparameters on the performance of the methods. \textbf{Top}: for the soft-binning approach, the impact of $k$ for values between $3$ and $45$. \textbf{Bottom}: for the end-to-end approach, the impact of the value of $\lambda_{\tsf{KL}}$ on the final value.%., showing the robustness of our method. % FB: BIGGER FONTS
    }
    \label{fig:ablation}
\end{figure}

% \begin{figure}[t] 
%     \centering

%     \includegraphics[width=0.45\textwidth]{images/impact_k.pdf} 
%     \includegraphics[width=0.45\textwidth]{images/impact_lambda_kl.pdf} 
%     \caption{Impact of different architecture and training hyperparameters on the performance of the methods. \textbf{Top} }
%     \label{fig:ablation}
% \end{figure}




%\begin{figure*}[t]\label{fig:rmses_datasets}
%    \centering
%    \includegraphics[width=0.95\textwidth]{Bridging Regression and Classification ICML 2025/images/tmp/to_replace.pdf}
    

%    \caption{Replace with nicer style plot }
%    \label{fig:dim_Z}
%\end{figure*}

% \begin{figure}[t] 
%     \centering

%     .\includegraphics[width=0.4\textwidth]{images/tmp/recon_entr.png} 
%     \caption{Test set reconstruction $rMSE$ for varied $\alpha_{\text{ent-codec}}$.  }
%     \label{fig:recon_error_ent}
% \end{figure}

% \ref{fig:recon_error_ent} depicts the minimum error a classifier model can attain via decoding. It can be seen that the choice of $\alpha$ is broadly consistent across $k$.


% \begin{figure}[t] 
%     \centering
%     \includegraphics[width=0.4\textwidth]{images/tmp/init-encoder-weights.png} 
%     \caption{Test set $rMSE$ for varied objective functions, with untrained frozen encoder/decoder weights, which are initialized as evenly spaced Gaussians with $\sigma_k = \Delta_\mu \quad \forall k \in [K]$.}
%     \label{fig:recon_error_ent}
% \end{figure}

% \begin{figure}[t] 
%     \centering
%     \includegraphics[width=0.4\textwidth]{images/tmp/trained-enc-weights-frozen.png} 
%     \caption{Test set $rMSE$ for varied objective functions, with trained encoder-decoder frozen.}
%     \label{fig:recon_error_ent}
% \end{figure}


% \begin{figure}[t] 
%     \centering
%     \includegraphics[width=0.4\textwidth]{images/tmp/end-to-end.png} 
%     \caption{End to end training on the MNIST brightness task. Using the regression encoder objective function. }
%     \label{fig:recon_error_ent}
% \end{figure}

% \ls{Include a features summary for each of the datasets in a table form e.g.}



% \begin{figure*}[!ht]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Bridging Regression and Classification ICML 2025/images/tmp.png}

%     \caption{TMP}
%     % \label{fig:framework}
% \end{figure*}


% \begin{table*}[t]
% \centering
% \caption{Dataset properties.}
% \begin{tabular}{lccccccccc}
% \hline
%  & WN & AE & BS & SC & EL & CA & DM & RB & RM \\
% \hline
% Tabular (T) / Image I (I) & T & T & T & T & T & T & T & I & I  \\
% \#num. features & 7 & 33 & 6 & 79 & 16 & 21 & 6 & (3, 256, 256) & (3, 28, 28)  \\
% \#num. train points & - & - & - & - & - & - & - & 5k & - \\
% \hline
% \end{tabular}

% \end{table*}


% \begin{table*}[t]\label{table:datasetproperties}
% \centering
% \caption{Dataset properties.}
% \begin{tabular}{lccccccc|cc}
% \hline
%  & \multicolumn{7}{c}{Tabular} & \multicolumn{2}{c}{Computer Vision} \\
% \cline{2-8} \cline{9-10} 
%  & WN & AE & BS & SC & EL & CA & DM & RB & RM \\
% \hline
% \#num. features & 7 & 33 & 6 & 79 & 16 & 21 & 6 & (3, 256, 256) & (3, 28, 28) \\
% \#num. train points & 5197 & 11,000 & 13,903 & 17,010 & 13,279 & 6553 & 43,152 & 4500 & 1080 \\
% \#num. val points & 650 & 1375 & 1738 & 2126 & 1660 & 819 & 5394 & 500 & 120 \\
% \#num. test points & 650 & 1375 & 1738 & 2127 & 1660 & 819 & 5394 & 500 & 400 \\
% % Label Type & $\mathbb{Z}$ & $\mathbb{R}$ & $\mathbb{Z}$ & $\mathbb{R}$ & $\mathbb{R}$ & $\mathbb{Z}$ & $\mathbb{R}$ & $\mathbb{R}$ & $\mathbb{Z}$ \\
% \hline
% \end{tabular}
% \end{table*}











% \begin{table}[!t]

% \centering
% \caption{Test Root Mean Square Error, Two stage CompAct dataset varied losses.}
% \begin{tabular}{cccccc}
% \toprule
% & $D_{KL}$ & $D_{KL} + 1e^{-3}\ell_2$ & $D_{KL} + 1e^{-2}\ell_2$ & $D_{KL} + 1e^{-1}\ell_2$ & $\ell_2$ \\
% \midrule
% $k=5$  & 0.01886688 & 0.01886689 & 0.01886699 & 0.01886642 & 0.01825372 \\
% $k=15$  & 0.01843127  & 0.01843123 & 0.01843094 & 0.01842774 & 0.01833234 \\
% $k=25$  & 0.01859003 & 0.01859002 & 0.0185899 & 0.01858887 & 0.01749952 \\

% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[!t]

% \centering
% \caption{Test Root Mean Square Error, Two stage CompAct dataset varied losses.}
% \begin{tabular}{cccccc}
% \toprule
% & $D_{KL}$ & $D_{KL} + 1e^{-3}\ell_2$ & $D_{KL} + 1e^{-2}\ell_2$ & $D_{KL} + 1e^{-1}\ell_2$ & $\ell_2$ \\
% \midrule
% $k=5$  & 0.019 & 0.019 & 0.019 & 0.019 & 0.018 \\
% $k=15$  & 0.018  & 0.018 & 0.018 & 0.018 & 0.018 \\
% $k=25$  & 0.019 & 0.019 & 0.019 & 0.019 & 0.017 \\

% \bottomrule
% \end{tabular}
% \end{table}


%




% \begin{table}[!t]

% \centering
% \caption{Test Root Mean Square Error, Baselines Tabular Datasets. \ls{Takeaway: \citep{stewart2023regression} and soft encoder are seen to improve across regression baselines in all tabular datasets.} 2-Layer MLP on Tabular and X on Vision. \ls{Will rerun these experiments with better batch size / early stopping configs as there is lots of variance betweend datasets.}}
% \begin{tabular}{cccccccccc}
% \toprule
%  & WN & AE & BS & SC & EL & CA & DM & RB & RM   \\
% \midrule
% $\ell_2$ & 0.1256 & 0.04923 & 0.1505 & 0.09749 & 0.04208 & 0.08972 & 0.06515 & - & - \\
% \midrule 
% Bin, $k=5$ & 0.1268 & 0.05668 & 0.1488 & 0.0816 & 0.05666 & 0.05007 & 0.06337 & - & - \\
% $\psi_w$, $k=5$ & 0.126 & 0.05848 & 0.1594 & 0.1084 & 0.06968 & 0.07618 & 0.07064 & - & - \\
% \midrule
% Bin, $k=15$ &  0.1248 & 0.04623 & 0.1466 & 0.07788 & 0.03324 & 0.02573 & \textbf{0.06104} & - & - \\
% $\psi_w$, $k=15$ & 0.1251 & \textbf{0.04502} & 0.1481 & 0.07995 & 0.03325 & 0.0246 & 0.06148 & - & - \\
% \midrule
% Bin, $k=25$ & 0.1249 & 0.04528 & \textbf{0.1457} & \textbf{0.07712} & 0.03244 & 0.02512 & 0.06114 & - & - \\
% $\psi_w$, $k=25$ & \textbf{0.1247} & 0.04507 & 0.1471 & 0.07846 & \textbf{0.03241} & \textbf{0.0244} & 0.06117 & - & - \\

% \bottomrule
% \end{tabular}
% \end{table}



% \begin{table}[!t]

% \centering
% \caption{Test Root Mean Square Error, Baselines Tabular Datasets}
% \begin{tabular}{cccccccccc}
% \toprule
%  & WN & AE & BS & SC & EL & CA & DM & RB & RM   \\
% \midrule
% RegMLP 3L & 0.126 & 0.04903 & 0.1505 & 0.09793 & 0.04213 & 0.08875 & 0.06502 & - & - \\
% \midrule
% BinMLP 3L 5Z & 0.1272 & 0.05656 & 0.1489 & 0.08216 & 0.05699 & 0.04904 & 0.06376 & - & -\\
% SoftMLP 3L 5Z & 0.1259 & 0.05802 & 0.1597 & 0.1071 & 0.06851 & 0.07552 & 0.06948 & - & - \\
% \midrule
% BinMLP 3L 15Z & 0.1247 & 0.04617 & 0.1465 & 0.07828 & 0.03299 & 0.02575 & 0.06106 & - & - \\
% SoftMLP 3L 15Z  & 0.1254 & 0.045 & 0.1479 & 0.07975 & 0.03295 & 0.02456 & 0.06157 & - & - \\
% \midrule
% BinMLP 3L 15Z  & 0.1252 & 0.04551 & 0.1453 & 0.07785 & 0.03232 & 0.02502 & 0.06112 & - & - \\
% SoftMLP 3L 15Z  & 0.125 & 0.04509 & 0.147 & 0.07804 & 0.03205 & 0.02438 & 0.06131 & - & - \\
% \bottomrule
% \end{tabular}
% \end{table}


% \begin{table}[!t]

% \centering
% \caption{Performance}
% \begin{tabular}{ccccccccccc}
% \toprule
%  & WN & AE & BS & SC & EL & CA & DM & RB & RM  & rank (std) \\
% \midrule

% RegMLP 2L & $0.126$ & $0.0493$ & $0.151$ & $0.0975$ & $0.0421$  & $0.0904$ & $0.0653$ & - & - & 7.5 (2.0) \\
% RegMLP 3L &  $0.126$ & $0.0492$ & $0.151$ & $0.0984$ & $0.0422$  & $0.0896$ & $0.0653$ & - & - & 7.5 (2.0) \\

% \midrule
% BinMLP 2L 5Z & 0.1268 & 0.0567 & 0.1488 & 0.0816 & 0.0567 & 0.0501 & 0.0634 & - & - & - \\
% BinMLP 2L 15Z & 0.1249 & 0.0462 & 0.1466 & 0.0779 & 0.0332 & 0.0257 & 0.0610 & - & - & - \\
% BinMLP 2L 25Z & 0.1247 & 0.0453 & 0.1456 & 0.0771 & 0.0324 & 0.0251 & 0.0611 & - & - & - \\

% BinMLP 3L 5Z & 0.1272 & 0.0566 & 0.1489 & 0.0822 & 0.0570 & 0.0490 & 0.0638 & - & - & - \\
% BinMLP 3L 15Z & 0.1252 & 0.0462 & 0.1465 & 0.0783 & 0.0330 & 0.0258 & 0.0611 & - & - & - \\
% BinMLP 3L 25Z & 0.1247 & 0.0455 & 0.1453 & 0.0778 & 0.0323 & 0.0250 & 0.0611 & - & - & - \\

% \midrule

% SoftMLP 2L 5Z & 0.1259 & 0.0585 & 0.1594 & 0.1083 & 0.0697 & 0.0762 & 0.0706 & - & - & - \\
% SoftMLP 2L 15Z & 0.1251 & 0.0450 & 0.1481 & 0.0799 & 0.0332 & 0.0246 & 0.0615 & - & - & - \\
% SoftMLP 2L 25Z & 0.1247 & 0.0451 & 0.1471 & 0.0785 & 0.0324 & 0.0244 & 0.0612 & - & - & - \\

% \bottomrule
% \end{tabular}
% \end{table}
%For regression data $(X, Y) \in \mathbb{R}^d \times \mathbb{R}$, we are interested in learning $Y | X$. Instead of using the typical approach of using the $\ell_2$ loss to train a model (e.g. a Neural Network) to learn $Y | X$, we can instead take inspiration from a model in the following form:





% \newpage

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.
\newpage
 \section*{Acknowledgments}
The authors would like to thank Tianlin Liu, Michaël Sander and David Holzmuller for their help and feedback on preliminary versions of this work. The French government partly funded this work under the management
of Agence Nationale de la Recherche as part of the “France 2030” program, reference
ANR-23-IACL-0008 (PR[AI]RIE-PSAI).
 
 \section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning, by proposing improvements to regression tasks in supervised learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
\section{Additional experimental results}
\label{app:expe}
We provide further experimental details

\begin{table}[!h]

\centering
\caption{Test set RMSE (averaged over random seeds) for all methodologies across datasets. \label{table:all_results}}
\begin{tabular}{ccccccccc}
\toprule
 & WN & AE & BS & SC & EL & CA & DM  & RM   \\
\midrule
Least Squares & 0.097 & 0.037 & 0.109 & 0.078 & 0.03 & 0.057 & 
0.051  & 0.195 \\
\midrule
Least Squares w. Softmax $k=5$  & 0.097 & 0.089 & 0.090 & 0.055 & 0.024 & 0.022 & 0.049  & 0.200 \\
Least Squares w. Softmax  $k=15$  & 0.095 & 0.089 & 0.079 & 0.051 & 0.023 & 0.019 & 0.049  & 0.298 \\
Least Squares w. Softmax  $k=25$  & 0.095 & 0.089 & 0.080 & 0.051 & 0.023 & 0.019 & 0.049  & 0.298 \\

\midrule 
Hard Binning Classification $k=5$ & 0.095 & 0.042 & 0.106 & 0.057 & 0.042 & 0.041 & 
0.050  & 0.210 \\
Hard Binning Classification $k=15$ & 0.095 &0.034 & 0.106 & 0.055 & 0.025 & 0.020 & 
0.048  & 0.195 \\
Hard Binning Classification  $k=25$ & 0.096 &0.034 & 0.106 & 0.055 & 0.024 & 0.020 & 
0.048  & 0.195 \\
\midrule 
Soft Binning Classification $\lambda_\sigma = 1$, $k=5$  & 0.096 & 0.043 & 0.132 & 0.096 & 0.060 & 0.067 & 0.055  & 0.210 \\
Soft Binning Classification $\lambda_\sigma = 1$, $k=15$  & 0.095 & 0.033 & 0.108 & 0.058 & 0.025 & 0.019 & 0.048  & 0.193 \\
Soft Binning Classification $\lambda_\sigma = 1$, $k=25$  & 0.096 & 0.033 & 0.106 & 0.057 & 0.024 & 0.019 & 0.048  & 0.190\\
\midrule
Soft Binning Classification $\lambda_\sigma = 0.5$, $k=5$  & 0.095 & 0.033 & 0.107 & 0.062 & 0.027 & 0.019 & 0.049  & 0.190 \\
Soft Binning Classification $\lambda_\sigma = 0.5$, $k=15$  & 0.095 & 0.033 & 0.105 & 0.056 & 0.024 & 0.019 &0.048  & 0.190 \\
Soft Binning Classification $\lambda_\sigma = 0.5$, $k=25$  & 0.095 & 0.033 & 0.105 & 0.055 & 0.023 & 0.019 & 0.048  & 0.191 \\
\midrule
Trained Encoder Classification $k=5$  & 0.095 & 0.033 & 0.107 & 0.058 & 0.025 & 0.019 & 0.049  & 0.189 \\
Trained Encoder Classification $k=15$  & 0.095 & 0.033 & 0.105 & 0.055 & 0.023 & 0.018 & 0.048  & 0.188 \\
Trained Encoder Classification $k=25$  & 0.095 & 0.033 & 0.104 & 0.054 & 0.022 & 0.018 & 0.047  & 0.188 \\
\midrule
End to End $k=5$ & 0.095 & 0.033  & 0.091 & 0.054 & 0.024 & 0.018 & 0.048  & 0.189 \\
End to End $k=15$ & 0.093 & 0.033   & 0.080 & 0.049 & 0.022 & 0.018 & 0.048  &  0.189 \\
End to End $k=25$ & 0.093  &  0.032  & 0.074 & 0.048 & 0.022 &  0.018 & 0.047  & 0.189 \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[!h]

\centering
\caption{Optimal sweep $\lambda_{\tsf{KL}}$ for datasets, with fixed $\lambda_{pred}=1$.\label{table:kl_sweep}}
\begin{tabular}{ccccccccc}
\toprule
 & WN & AE & BS & SC & EL & CA & DM  & RM   \\
 \midrule
 $k=5$ & 0.068 & 0.068  & 0.068 & 0.068 & 0.068 & 0.068 & 0.068  & 0.147 \\
 $k=15$ & 0.068 & 0.068  & 0.068 & 0.147 & 0.068 & 0.068 & 0.068  & 3.163 \\
 $k=25$ & 0.316 & 0.147  & 0.068 &  0.068 & 0.147 & 0.068 & 0.068  & 0.147 \\
\bottomrule
\end{tabular}
\end{table}


\section{Additional figures}

We provide more detailed illustration of the target encoding functions $\psi_w(\cdot)_i$ over $\R^2$ for $i \in \{1,\ldots, 9\}$ from Figure~\ref{fig:embeding}, in Figures~\ref{fig:all_psi_grid} \& \ref{fig:all_psi_learnt} below
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{images/grid_all_psi_grid.pdf}
    

    \caption{Embedding the target space $\R^m$ (here $m=2$), representation of all the coefficients of the target encoding}
    \label{fig:all_psi_grid}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{images/grid_all_psi_learnt.pdf}
    

    \caption{Embedding the target space $\R^m$ (here $m=2$), representation of all the coefficients of the target encoding}
    \label{fig:all_psi_learnt}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
