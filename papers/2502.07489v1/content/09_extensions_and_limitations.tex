
\normalsize
\section{Extensions and Limitations}\label{sec:limit}
Currently, most datasets in \Bench~still show a good potential for improvement:
even the best performing methods are far away from the Bayes error of
0.0025 MSE\@.
In case authors of future methods get the impression that their method
could benefit from larger datasets, i.e., more instances per dataset than 2000,
they easily can generate larger ones with 10,000, 100,000, etc.\ instances.

%The Bayes error is a very optimistic measure for the performance ML methods
%could achieve on a dataset. One can compute a tighter bound by estimating the
%ODE constants per group of instances with shared constants
%and the initial values of every instance
%for the generating ODE itself, i.e., a model that has the ground truth
%ODE system $F$ as background knowledge, and then forecast with the
%estimated ODE\@.
\Bench~is not based on observation data as in these areas
observed data usually is expensive to get, and thus only limited
amounts exists or are publically available, at least.
In fact, we scanned through the respective papers of our top 50 ODE models 
and could not find the original experimental data for a single one. 
Hence, if one wants to conduct machine learning experiments on these complex
and rich biological processes one is forced to create them using the ODE models describing the process.
\begin{wrapfigure}{r}{0.45\textwidth}
	%\vspace{-\baselineskip}
	\centering
	% \scriptsize
	\begin{subfigure}{\linewidth}
		\centering
	\import{../figures/}{mse_avtl}
	\end{subfigure}
	\caption{Test MSE of the best performing model vs $\JGD$-score across 50 datasets.}\label{fig:mse_jgd}
	\vspace{-\baselineskip}
\end{wrapfigure}

Consequently, we build the benchmark on top of real-world ODEs:
their system, their constants and their initial values establish
a real-world connection that has been created in hundreds
of scientific publications. To delineate our benchmark from purely
synthetic data we therefore call it \emph{semi-synthetic}.

The fact that the datasets are generated by ODEs implies that some general ODE-based models are expressive enough to mimic the generating process and to perform well for large amounts of data.
For example, LinODEnet can represent a Koopman-type~\citep{Koopman1931.Hamiltonian,Koopman1932.Dynamical} linear approximation of any nonlinear ODE under some assumptions. 
However, being expressive enough does not mean that these models are  best when trained on a limited amount of data.
Other, non-ODE models can beat ODE-based models with limited training data, even if the ground truth is an ODE\@.
In fact, a main purpose of the benchmark is to answer questions about the best forecasting models for data generated from an (unknown) ODE ground truth.  
Despite our limitation to ODEs from biology the perspective is forecasting for many scientific domains where dynamics are \emph{in principle} governed by ODEs.

The inductive bias of ODE-based models (LinODEnet~\citep{Scholz2022.Latenta}) is rather weak.
Being based on ODEs does not mean that these models would only perform well on data generated from ODEs.
When one spells out the expressiveness of LinODEnet in terms of basis functions,
it would just mean that after a nonlinear transformation the time dependent functions can be expressed by some basis functions given as solutions of some linear ODE\@. 
The representation is entirely trainable and nothing about the time series in question is hard-coded apart from smoothness.

The number of instances per dataset contained in \Bench~is deliberately small (2000). Our goal was not to create
huge datasets just for the sake of being huge, but to create
datasets for the purpose of research: the smallest datasets that
would allow complex, ODE-based models to demonstrate their
strengths. However, it is trivial to create an arbitrary number of time series, if larger datasets are desired.  

While this work focuses on IMTS forecasting, \Bench~can be easily extended to regular time series forecasting.
In \Cref{app:regular}, we conduct experiments with three datasets created with \Bench. 
Our results show that these datasets have an interesting property: They force models to learn channel dependencies. 
This contradicts with the results on traditional evaluation datasets, in which models like PatchTST~\citep{Nie2023.Time} are more effective when they ignore 
inter-channel correlations.  
