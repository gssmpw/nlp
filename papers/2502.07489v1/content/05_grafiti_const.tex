\section{Time-Constant Approaches Outperform ODE-based Approaches
in the Current Evaluation Scenario}\label{sec:classif_forec}

\paragraph{Current Evaluation Scenario.}
Traditionally, IMTS forecasting has relied on datasets like PhysioNet2012~\citep{Silva2012.Predictingb},
MIMIC-III~\citep{Johnson2016.MIMICIIIb}, and MIMIC-IV~\citep{Johnson.MIMICIV}
obtained from Kaggle challenges designed for IMTS classification.
Additionally, \citet{DeBrouwer2019.GRUODEBayesd} proposed to create an IMTS forecasting task,
based on USHCN~\citep{Menne2016.LongTerm}, a fully observed regularly sampled time series dataset
containing climate data. We present characteristics of these datasets in \Cref{tab:data} (appendix).
%
\begin{table}[t]
\scriptsize
\caption{%
    Test MSE for forecasting next 50\% after 50\% observation time.
    OOM refers to out of memory. We highlight the best model in \textbf{bold}
	and \UL{underline} the second best. $^\dagger$ indicates that we show
	the results from \citet{Yalavarthi2024.GraFITi}
}\label{tab:classif_forec}
\centering
\begin{tabular}{l cccc}
	\toprule
	Model		& USHCN			& PhysioNet-12	& MIMIC-III	& MIMIC-IV
\\	\midrule
	GRU-ODE     &  1.017±0.325  	& 0.653±0.023$^\dagger$       	& 0.653±0.023$^\dagger$      	& 0.439±0.003$^\dagger$
\\	LinODEnet   & \UL{0.662±0.126} 	& 0.411±0.001$^\dagger$  		& \UL{0.531 ± 0.022}$^\dagger$ 	& 0.336±0.002$^\dagger$
\\	CRU         & 0.730±0.264 		& 0.467±0.002$^\dagger$ 		& 0.619±0.028$^\dagger$         & OOM$^\dagger$
\\	Neural Flow & 1.014±0.336       & 0.506±0.002$^\dagger$        	& 0.651±0.017$^\dagger$         & 0.465±0.003$^\dagger$
\\	GraFITi     & \BF{0.636±0.161} 	& \BF{0.401±0.001}$^\dagger$  	& \BF{0.491 ± 0.014}$^\dagger$ 	& \BF{0.285±0.002}$^\dagger$
\\	GraFITi-C   & 0.875±0.204 		& \UL{0.407±0.001}				& 0.543±0.024 					& \UL{0.324±0.002}
\\ \bottomrule
\end{tabular}
\end{table}

The GraFITi model is the state-of-the-art on PhysioNet2012, MIMIC-III, MIMIC-IV, and USHCN, demonstrating a significant performance advantage over neural ODE-based methods~\citep{Yalavarthi2024.GraFITi}.
To further analyze this gap, we designed a \emph{constant}
version of GraFITi (GraFITi-C),
which is restricted to  make the exact same forecast
for every query time point.
This is achieved by training a GraFITi model that cannot access the actual query time point
($t^\qry_k$), but is instead always provided with a constant dummy query time point.

The results shown in \Cref{tab:classif_forec} indicate,
that neural ODE-based models are indeed outperformed
by this allegedly easy-to-beat baseline on PhysioNet2012 and
MIMIC-IV\@. On MIMIC-III the best performing ODE-based
model achieves a lower average test MSE than GraFITi-C,
however the difference is within the standard deviation.
GraFITi on the other hand, consistently outperforms its
constant variant. However, it also cannot separate in by a drastic margin.
These findings could relate to important covariants, which are not contained
in the dataset. For instance, it is impossible to predict a patient's blood glucose level without information about any food intake.

We designed \Bench~to be an extension to the currently used evaluation protocol.
For our ODE generated datasets, we can guarantee that all necessary covariants are given.
Furthermore, we can take full control about the simulated observation process.

%\paragraph{Understanding the Data Gap}%
%Consider weather classification as an example.
%Here, the goal is to categorize a month as ``warm'' or ``cool'' based on temperature data.
%If the average daytime temperature exceeds a threshold, say 25\textdegree C, it is classified as warm; otherwise, it is cool.
%We can readily classify a month's data based on its temperature readings.
%However, forecasting the temperature for the next 15 days using only the first 15 days' data becomes more complex.
%In such cases, relying solely on temperature data might not be sufficient.
%Interestingly, a simple statistical model could achieve similar performance to a well-trained deep learning model for this specific forecasting task.
%\textbf{Stimmt der letzte Satz/der ganze Paragraph}???

%\paragraph{Investigating the Issue in IMTS Datasets}%
%To assess if existing IMTS classification datasets suffer from this limitation,
%we conducted the following experiment. We trained a model to always predict a fixed value
%for any future query (timepoint, channel ID).
%% todo: name of the model
%Subsequently, we compared the forecasting error of this model with the
%performance of existing state-of-the-art deep learning models. The results,
%presented in Table~\ref{tab:classif_forec}, demonstrate that the time-independent
%model achieves competitive to superior results to SoTA models, which shows that
%these datasets may indeed not be suited for forecasting tasks.
