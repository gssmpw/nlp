\section{Generating Challenging IMTS Datasets from ODEs}

\paragraph{Sampling ODEs to Generate IMTS Data.}\label{sec:sampling}

In science and engineering, processes that evolve in continuous time are usually described by
differential equations, in the simpler cases by ordinary differential equations (ODEs).
All possible observations of such a process with $C$ \textbf{channels}
over \textbf{duration} $T$ at any time $t$ can be captured by a time-varying function
${x\colon [0,T]\to\R^C}$ as $x(t)_c$.
An ODE characterizes such a function implicitly by two conditions:
%
\begin{enumerate}
\item A relation between observation time $t$, observation value $x(t)$ and
the instantaneous change in observation values $x'(t)$, i.e.\ the first derivative of $x$.
This relation is described by a function $F: [0, T]\times\R^C\times\R^C\to\R$
called the \textbf{system}.

\item The \textbf{initial values} $x_0\in\R^C$ of the function at time $t=0$:
%
\begin{align}\label{eq:initial_value_problem}
	F(t, x(t), x'(t)) = 0 \quad \forall t\in[0,T], \qquad  x(0) = x_0
\end{align}
\end{enumerate}

%
Note that ODEs can be defined including higher order derivatives, but w.l.o.g.\ one can assume they are of first order, since the higher order system can be transformed into a first order system by a change of variables~\citep{Teschl2012.Ordinary}.
In a few special cases the solution $x(t)$ can be computed analytically. For example, the one-dimensional system $F(t, x(t), x'(t))\coloneqq a x(t) - x'(t)$ characterizes
the exponential growth function $x(t) = e^{at}x_0$. However, in most cases, it
can only be computed by a numerical ODE solver.


Specific parameters that occur in a system $F$ like the growth factor $a$
in our example are called \textbf{constants} and one writes $F(t, x(t), x'(t); a)$
with $a\in\R^A$ to denote a system with $A$-many constants.
Triples $(F, a, x_0)$ of systems, constants and initial values can be turned into a simulator,
i.e.\ a generative model, by complementing them with two more components:
%
\begin{enumerate}
\item A \textbf{sampling process} $s$ for observation time points and channels that
	can generate finite sequences $((t_1, c_1), \ldots, (t_I, c_I))$ of
	pairs of observation times $t_i\in[0,T]$ and observed channels $c_i\in\{1,\ldots,C\}$.
\item A \textbf{noise model} $p_\noise$ for the observation values, that is a conditional
	distribution ${p_\noise(x^\obs \mid x^\true)}$ of the observed value
	given the ground truth value.
\end{enumerate}
%
Then one can generate irregular sampled instances $(t_i, c_i, x^\obs_i)_{i=1{:}I}$
of the (only implicitly given) underlying function $x$ with missing values
and affected by observation noise simply via:
%
\begin{align}\label{eq:sampling}
		\bigl((t_i, c_i)\bigr)_{i=1:I} &\sim s
	&	x_i^\true  &\coloneqq \text{ode-solve}(F, a, x_0, t_i)_{c_i}
	&	x_i^\obs   &\sim p_\noise(x^\obs_i \mid x^\true_i)
\end{align}
%
% \begin{align*}
% ((t_1, c_1), \ldots, (t_I, c_I)) & \sim s
% &
% \begin{aligned}
% 	x_i^\true  & \coloneqq \text{ode-solve}(F, a, x_0, t_i)_{c_i} &&\forall i
% \\	x_i^\obs & \sim p_\noise(x^\obs \mid x^\true) 				&&\forall i
% \end{aligned}
% \end{align*}
%
% We call this data semi-synthetic, because it has been simulated, but from ODEs

\paragraph{Identifying Challenging Time Series.}\label{sec:mean-gradient-deviation}

While any ODE implicitly describes a time-varying function $x\colon [0,T] \to \R$,
not all such functions are challenging to forecast: For example,
some just describe very simple convergence processes that approach a limit value over a long time.
In order to discover challenging time series, we propose to measure the \emph{mean gradient deviation} ($\MGD$), that is the time-normalized $L^2$ norm of $x'(t)$ around its mean:
%
\begin{align}\label{eq:mdg}
	\MGD(x) &\coloneqq \min_{c} \sqrt{\frac{1}{T} \int_{0}^T (x'(t) - c)^2 \dd{t}}
\end{align}
%
Note that the minimum is obtained precisely for the mean gradient
%
\begin{equation}\label{eq:mgd}
	{c=\frac{1}{T}\int_{0}^T x'(t) \dd{t}} = \frac{x(T)-x(0)}{T}.
\end{equation}
%
For a linear test function $x(t) = a t + b$, the $\MGD$ is zero, while for a sine wave $x(t) = \sin(\omega t)$, the $\MGD$  scales linearly with the frequency $\omega$.
To identify not just individually challenging time series, but whole distributions (i.e.\ a generator $p$), one can compute the expectation of $\MGD$.
We found that the MGD grows super-exponentially with the Lipschitz constant, which is described in more detail in~\Cref{app:lip}

With MGD one does assess only the heterogeneity within each function,
not heterogeneity between different functions,
which is an important precondition for an interesting data set
for machine learning tasks. To capture this second aspect,
we propose the \emph{mean point-wise gradient deviation} ($\MPGD$):
%
\begin{align}\label{eq:mpgd}
	\MPGD(p) \coloneqq \frac{1}{T}\int_0^T  \std_{x\sim p}[x'(t)] \dd{t}
	= \frac{1}{T}\int_0^T\!\!\sqrt{%
		\E_{x\sim p}\bigl[\bigl(x'(t) - \E_{y\sim p}[y'(t)]\bigr)^2\bigr]
	}\dd{t}
\end{align}
%
We combine both measures multiplicatively as \emph{joint gradient deviation} ($\JGD$) for
distributions of functions, i.e.\ function generators:
%
\begin{align}\label{eq:jgd}
   \JGD(p) \coloneqq \MPGD(p) \cdot \E_{x\sim p}[\MGD(x)]
\end{align}
%
Note, that we can approximate $\MGD$ with a finite number of
samples:

\begin{lemma}\label{lemma:approx-tvl}
For a function $x \in \mathcal{C}^{1}([0,T])$ and $\epsilon > 0$ such that
$\frac{T}{\epsilon} \in \mathbb{N}$ we consider the divided differences
%
\begin{align*}
	x^{\diff,\epsilon}_t
	&\coloneqq \frac{x(t) - x(t-\epsilon)}{\epsilon}
	,\quad t=\epsilon,2\epsilon,\ldots,T.
\end{align*}
%
It then holds that the numerical estimator ${\numstd[(x_k)_{k=1:K}] \coloneqq \sqrt{\frac{1}{K}\sum_{k=1}^{K} (x_k - \bar{x})^2 }}$ of the standard deviation of the divided differences converges to the $\MGD$ of the function:
%
\begin{align}\label{eq:mgd_convergence}
	\numstd[x^{\diff,\epsilon}] \overset{\epsilon \to 0}{\longrightarrow} \MGD(x)
\end{align}
\end{lemma}

We can also approximate $\MPGD$ with a finite amount of samples under assumptions we explain in~\Cref{sec:proofs}.
%
\begin{lemma}\label{lemma:approx-atv}
The mean point-wise gradient deviation of a distribution $p$ of
functions can be approximated on a sample $(x_n)_{n=1:N}$ of $N$-many sequence from $p$ on a fixed grid of time points as follows: Given the divided differences:
%
\begin{align*}
	x^{\diff}_{n,t} & \coloneqq \frac{x_n(t) - x_n(t-\epsilon)}{\epsilon},\quad t=\epsilon,2\epsilon,\ldots,T
\end{align*}
%
Then the average of the point-wise std.\ of the divided differences converges to the $\MPGD$:
%
\begin{align}\label{eq:mpgd-approx}
\frac{\epsilon}{T}\sum_{t=\epsilon}^T \numstd[(x^{\text{diff}}_{n,t})_{n=1:N}]
	&\longrightarrow \MPGD(p)
	\qq{for} \epsilon\to 0 \qq{and} N\to\infty \qq{almost surely}
\end{align}
\end{lemma}
%
The proofs can be found in~\Cref{sec:proofs}. Due to \Cref{lemma:approx-tvl} and \Cref{lemma:approx-atv},
we can approximate the $\MGD$, $\MPGD$ and therefore the $\JGD$ based on the set of evenly spaced sequences ${X \in \R^{N \times T}}$, where $x_{n,t}$ denotes the $value$ at time-step $t$ in the $n$-th sequence, with $x_n \sim p$.
Given ${x^{\diff}_{n,t} \coloneqq  x_{n, t+1} - x_{n,t}}$,
it holds that ${\JGD(p) \approx  \MPGD(p) \cdot \frac{1}{N}\sum_{n=1}^{N} \MGD(x_n)}$ and furthermore:
%
\begin{align}\label{eq:jgd-approx}
	\MGD(x_n) &\approx \numstd[x^{\diff}_n]
&	\MPGD(p)  &\approx \frac{\epsilon}{T}\sum_{t=\epsilon}^{T}
	\numstd[(x^{\diff}_{n,t})_{n=1:N}]
\end{align}
%
\Cref{lemma:approx-tvl} and \Cref{lemma:approx-atv} thus allow us to approximate the $\JGD$-score by sampling the values of several functions on a shared grid from a generator.
Its definition on the full function guarantees that we will always get the same results (in the bounds of the approximation error).

\paragraph{JGD for ODE models.}\label{sec:jdg-ode-models}

To simplify the initial definition, we considered the $\JGD$ for a univariate function
${x\colon [0,T] \to \R}$, and we interpret a single channel within a multivariate ODE system as such a function.
Computing the $\JGD$ for a multivariate ODE model requires combining the $\JGD$ values of each channel.
We found that simply averaging all channels disadvantages models with a high number of channels.
To address this, we compute the mean of the ten channels with the highest $\JGD$\@.
For models with ten or fewer channels, we revert to using the mean of all channels.
Note that $\JGD$ is only a useful measure when applied to standardized data, otherwise the units are incomparable.
Consequently, we normalize each channel to have a mean of 0 and standard deviation of 1 over all
time steps and time series instances.

\paragraph{Varying Single ODE Instances.}\label{sec:pointwise-gradient-deviation}

From the scientific literature one often gets single ODE instances:
a triplet $(F, a^\lit, x_0^\lit)$ of a parametrized ODE system $F$,
its parameters/constants $a^\lit$ and the initial conditions $x_0^\lit$,
all three describing some specific experiment.
To generate data by sampling only from such a single ODE instance will create functions
that differ only in observation time points and channels and in the noise. These
instances are too similar to each other, without noise there is only one function,
and hence the $\MPGD$ of the distribution would be zero.
We therefore propose to also carefully vary
  (i) the initial conditions $x_0$,
  (ii) the ODE constants $a$ and
  (iii) the total duration $T$ and
of the ODE sampling process.
We sample each from a distribution % $p_{\text{initial}}, p_{\text{const}}, p_{\text{onset}}$, respectively,
that yields the values given in the scientific literature on expectation
and is controlled by a spread parameter $\sigma$ each:
%
\begin{align}\label{eq:ode-sampling}
	x_0	&\sim p_{\text{initial}}(\,\cdot\mid x_0^\lit, \sigma_{\text{initial}})
&	a	&\sim p_{\text{const}}(\,\cdot\mid a^\lit, \sigma_{\text{const}})
&	T	&\sim p_{\text{dur}}(\,\cdot\mid \sigma_{\text{dur}})
\end{align}
%
Let $p$ be the two-stage function generator that first samples tuples ${(x_0, a, T)}$
from their respective distributions and then yields as function $x$ the solution to the ODE
$F$ with initial values $x_0$, constants $a$ and running for a duration of $T$.
Instead of blindly choosing the spreads $\sigma$, we optimize them w.r.t.\ the $\JGD$-value of the generator:
%
\begin{align}\label{eq:optimal-sigma}
	(\sigma^*_{\text{initial}}, \sigma^*_{\text{const}}, \sigma^*_{\text{dur}})
	&\coloneqq \argmax_{\sigma_{\text{initial}}, \sigma_{\text{const}}, \sigma_{\text{dur}}}
	\JGD\bigl(
		p(\,\cdot \mid \sigma_{\text{initial}}, \sigma_{\text{const}}, \sigma_{\text{dur}})
	\bigr)
\end{align}
%
The $\JGD$ of each generator is estimated by 100 randomly sampled fully observed time series
with a sequence length of 100. Since we are interested in the difficulty of
the forecasting task, we compute the $\JGD$ on the final 50 steps of sequence only.
We search $\sigma_{\text{initial}}$ from the range of $\{0.1,0.3,0.5\}$,
$\sigma_{\text{const}}$ from $\{0.05,0.1,0.3\}$,
and $\sigma_{\text{dur}}$ from $\{0.33, 1, 3.3, 10, 30\}$.
The unit of $\sigma_\text{dur}$ varies for each ODE model,
it is given by Physiome~\citep{Yu2011.Physiome} and listed in \Cref{tab:ds-info} (appendix).

\begin{figure}
	\centering%
	\begin{subfigure}{0.495\textwidth}%
		\centering%
		\import{figures/}{ts_1}%
		\caption*{DUP01 --- $\JGD$: 2.697}%
	\end{subfigure}
	%
	\begin{subfigure}{0.495\textwidth}%
		\centering%
		\import{figures/}{ts_2}%
		\caption*{LEN01 --- $\JGD$: 0.178}
	\end{subfigure}

	\begin{subfigure}{0.495\linewidth}%
		\centering%
		\import{figures/}{ts_3}%
		\caption*{GUP01 --- $\JGD$: 0.014}
	\end{subfigure}
	\begin{subfigure}{0.495\linewidth}%
		\centering%
		\import{figures/}{ts_4}%
		\caption*{NEL01 --- $\JGD$: 0.007}
	\end{subfigure}
	\caption{Demonstration of time series realized by 4 ODEs of different prediction difficulties without adding any noise. Each line / color represents a channel. 
	Trajectories are shown for a duration of the respective $\sigma_\text{dur}$ as shown in \Cref{tab:ds-info}.}\label{fig:plots}
\end{figure}


\paragraph{Creating the $\Bench$ Benchmark.}\label{sec:rejection}
In the Physiome database we identified $N=208$ multivariate ODE models
${(F^n,a^{\lit,n},x_0^{\lit,n})_{n=1:N}}$, that are defined by
automatically generated Python code.
We ranked all ODE-models with according to their optimized $\JGD$-value
(see \Cref{tab:ds-info}) and selected the highest 50 datasets for
$\Bench$.
For each of the final datasets, we created a dataset with 2000 instances each.
To further increase the range of initial states, we create datasets of lengths
${\sigma^*_{\text{dur}}}$, but with 200 observation steps.
For each time series instance, we sample the initial time step
${t_\text{onset} \sim \{t_i\}_{1\leq i \leq 100}}$
and then select the next 100 steps for each time series instance.
Consequently, each time series instance has a duration of ${\frac{1}{2}\sigma^*_\text{dur}}$.
While the time series instances are sampled regularly at first,
we create IMTS instances by randomly dropping observations with a chance of 80\%.
For $p_{noise}$ we select to add $\epsilon \sim \mathcal{N} (0,0.05)$.
In \Cref{fig:plots} we show four trajectories contained in $\Bench$ with the
respective $\JGD$.

\paragraph{Handling exploding ODEs.}\label{sec:explosions}

Varying the constants and initial states of the ODEs can lead to inconsistent states,
which could never occur in real world scenarios.
In general, this is not an issue regarding the machine learning perspective on our experiments.
Occasionally however, certain combinations of inconsistent states and constants can lead to extreme values which are magnitudes greater than values that occur in other time series instances.
These extreme values will dominate the training and evaluation loss in a problematic manner.
We prevent this by rejecting all $(\sigma_\text{initial},\sigma_\text{const},\sigma_\text{dur})$ containing any values greater than 10 times the channel-wise standard deviation in the 100 samples created for evaluation.
If such values nevertheless occur within the final 2000 samples created for \Bench, we drop the respective instances.
We follow the same protocol to handle errors thrown by the ode-solver.
% TODO: mention: this allows to estimate $a$ for each ODE F and to compare to ODE estimation methods
%   as an oracle.
% TODO: can we add a reference ?

% TODO: now add all the nasty details:
% - which distribs did we use for p_initial etc.
% - which grids for \sigma
% - p_observation_noise
% - p_timepoint_sampling

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main-lst"
%%% End:
