\subsection{Current Forecasting Models}
In our experiments we compare five recent models.
Four of these five network architecture are variants of the neural
ODE~\citep{Chen2018.Neurald}, which uses an ODE, that is defined by a neural network, to infer the hidden state in-between observations. At
observation times the
neural ODE-based estimation of the hidden state is updated on the actual observation.
For more details, we refer to the papers. We use the following models:

\textbf{GRU-ODE-Bayes}~\citep{DeBrouwer2019.GRUODEBayesd}
utilizes a continuous version of Gated Recurrent Units (GRU) as
the ODE defining network $f$.
Additionally, GRU-Bayes applies a GRU-based Bayesian update to the
hidden state at observations.
\textbf{LinODEnet}~\citep{Scholz2022.Latenta} restricts $f$ to be linear functions but encodes data non-linearly into latent space.
 At observations LinODEnet updates its hidden state with a so-called nonlinear KalmanCell, a module inspired by
Kalman Filtering~\citep{Kalman1960.New}.
\textbf{Continuous Recurrent Units}~\citep{Schirmer2022.Modelingb} replaces the neural ODE with
 a stochastic differential equation (SDE). CRU is able to infer the hidden state
 at any time in closed form using continuous-discrete Kalman filtering,
 instead of solving a neural ODE\@.
\textbf{Neural Flows}~\citep{Bilos2021.Neurald} infers the solution curve of an ODE directly with
invertible neural networks.
\textbf{GraFITi}~\citep{Yalavarthi2024.GraFITi} is substantially different from neural ODE\@. It
encodes the observations from IMTS into a graph structure and infers forecasts by solving a
graph completion problem with a graph attention network. On the established evaluation datasets
GraFITi significantly outperforms the neural ODE-based models in terms of forecasting accuracy.
