\section{Experiments}\label{sec:experiments}

\paragraph{Baseline models.}

We compare on 5 recent IMTS forecasting models, namely \textbf{GRU-ODE-Bayes}~\citep{DeBrouwer2019.GRUODEBayesd}, \textbf{Neural Flows}~\citep{Bilos2021.Neurald}, Continuous Recurrent Unit (\textbf{CRU};~\citealp{Schirmer2022.Modelingb}), \textbf{LinODEnet}~\citep{Scholz2022.Latenta,mione_workflow_management_system_2024} and \textbf{GraFITi}~\citep{Yalavarthi2024.GraFITi}.
GRU-ODE-Bayes, Neural Flows, CRU and LinODEnet are differential equations based models whereas GraFITi is a graph based model.
We also introduce a naive model called GraFITi-Constant (GraFITi-C) which predicts a fixed value for all the future time points in all the target channels for an instance.


\paragraph{Experimental Protocol.}
In our experiments models have to predict the last 50\% of the time series after observing the first 50\%.
We use 5-fold cross validation, and for each fold we split the data into training, validation and test set with a ratio of 70:20:10.
Additionally, we resample the sparse observation mask to transform each instance into an IMTS\@.
The evaluation metric is the mean square error (MSE).
For simplicity, we randomly sample 10 hyperparameter configurations and fitted each model on a single fold per dataset, selecting the configuration with the lowest validation MSE\@.
The winning configuration per model is then trained and evaluated on all 5 folds (see \Cref{app:hype}).
We run the benchmark experiments on a cluster of 4 NVIDIA 4090 GPUs with 24 GB\@.

\paragraph{Empirical Results.}\label{sec:experiments_imts_bench}
The numbers presented in \Cref{tab:main_results} are the mean and standard deviation of MSE for 5 folds.
LinODEnet~\citep{Scholz2022.Latenta,mione_workflow_management_system_2024} performs the best in 29 datasets followed by GraFITi~\citep{Yalavarthi2024.GraFITi}.
On the other hand, the average rank of GraFITi (2.20) is close to that of LinODEnet (2.10).
Surprisingly, GraFITi-C performs better than Neural Flows and GRU-ODE-Bayes and has an average rank of 2.86.
This is because datasets like \texttt{DUP01} and \texttt{DOK01} are too hard to forecast, hence the complex models cannot improve upon the accuracy of a constant model.
Other datasets are too simple that even a constant model yields MSE close to 0.0025,
which is the lowest value possible to achieve due to the normal-distributed noise with a variance of 0.05.
%Datasets created by chaotic ODE~\citep{Gilpin2021.Chaosa} would most likely all fall into this category.

In order to check how good our $\JGD$ score can measure the difficulty in forecasting of a time series we plot $\JGD$ score vs the best MSE score achieved in \Cref{fig:mse_jgd}.
It can be observed that $\JGD$ fulfills its purpose of
finding challenging and interesting ODE models. More specifically $\JGD$ and the test MSE of the
best performing model are correlated with a Spearman coefficient of 0.77.

\begin{table}
\centering
\caption{%
	Experimental results on various baseline models.
	\Bench~datasets ranked by $\JGD$-score.
	We highlight the best model in \textbf{bold}
	and \UL{underline} the second best.
}\label{tab:main_results}
\scriptsize
\scalebox{0.95}{
\input{results.tex}
}
\end{table}
