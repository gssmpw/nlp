
\section{Proofs}\label{sec:proofs}
\subsection{Proof of Lemma~\ref{lemma:approx-tvl}}
\begin{proof}
Due to~\Cref{eq:mgd}, it is sufficient to show that
%
\begin{equation}\label{eq:convergence_numstd}
	\numstd[x^{\diff,\epsilon}]^2
	\xrightarrow{\epsilon \to 0}
	\frac{1}{T}\int_{t=0}^{T} \left(x'(t) - \frac{(x(T)-x(0))}{T}\right)^{2} \dd{t}.
\end{equation}
%
Due to the mean value theorem, there exists ${\zeta_{t,\epsilon} \in [t-\epsilon,t]}$
with ${\frac{x(t) - x(t-\epsilon)}{\epsilon} = x'(\zeta_{t,\epsilon})}$
for all ${t=\epsilon,2\epsilon,\dots,T}$.
Given ${N \coloneqq \frac{T}{\epsilon}}$ and ${\mu_N\coloneqq \frac{1}{N} \sum_{t=\epsilon,2\epsilon,\dots,T} x^{\diff,\epsilon}_{t}}$, we thus have
%
\begin{subequations}
\begin{align}
	\mu_N &= \frac{1}{N}\sum_{t=\epsilon,2\epsilon,\dots,T} x'(\zeta_{t,\epsilon})
\\	&=\frac{1}{T}\sum_{t=\epsilon,2\epsilon,\dots,T}\underset{=\epsilon}{\underbrace{\frac{T}{N}}} x'(\zeta_{t,\epsilon})
\\	&\xrightarrow{\epsilon \to 0} \frac{1}{T} \int_{t=0}^{T} x'(t) \dd{t}
\\	&= \frac{x(T)-x(0)}{T}
\end{align}
\end{subequations}
%
due to the definition of the Riemann integral. Furthermore, we have
%
\begin{align}
	\numstd[x^{\diff,\epsilon}]^2
	&= \frac{1}{N}\Bigl(\sum_{t=\epsilon,2\epsilon,\dots,T} (x^{\diff,\epsilon}_t)^{2}\Bigr)
	- \mu_N^{2}
\end{align}
%
Therefore, again by the mean value theorem and the definition of the Riemann integral:
%
\begin{subequations}
\begin{align}
	\frac{1}{N}\Bigl(\sum_{t=\epsilon,2\epsilon,\dots,T} (x^{\diff,\epsilon}_t)^{2}\Bigr)
	&= \frac{1}{N} \Bigl(\sum_{t=\epsilon,2\epsilon,\dots,T} x'(\zeta_{t,\epsilon})^{2}\Bigr)
\\	&= \frac{1}{T} \sum_{t=\epsilon,2\epsilon,\dots,T}  \underset{=\epsilon}{\underbrace{\frac{T}{N}}} x'(\zeta_{t,\epsilon})^{2}
\\	&\xrightarrow{\epsilon \to 0} \frac{1}{T} \int_{t=0}^{T} x'(t)^{2} \dd{t}
\end{align}
\end{subequations}
%
Consequently,
%
\begin{align}
	\numstd[x^{\diff,\epsilon}]^2
	&\xrightarrow{\epsilon \to 0}
	\frac{1}{T}\int_{t=0}^{T} x'(t)^{2} \dd{t} - \left(\frac{(x(T)-x(0))}{T}\right)^{2}
\\	&= \frac{1}{T}\int_{t=0}^{T} \left(x'(t) - \frac{(x(T)-x(0))}{T}\right)^{2} \dd{t}.
\end{align}
\end{proof}


% \textbf{Alternative Maybe:}
% Instead of $\tvl(x)$ we use $\Var(x) = \frac{1}{T}\int_{t=0}^{T}
% \left[x(t) - \frac{1}{T}\int_{s=0}^{T} x(s) ds \right]^{2} \dd{t}$
% \begin{lemma}\label{lemma:approx-tvl}
%   The total-variation-to-linear measure of a function $x$ can be approximated
%   on a grid of sampled values by the variance of its first difference:
%   \begin{align*}
%     x^{\diff}_t & \coloneq x_t - x_{t-\epsilon},\quad t=\epsilon,2\epsilon,\ldots,T
% \\  \Var( x^{\diff} ) & \to \tvl(x)
%     \quad \text{for } \epsilon\to 0
%   \end{align*}
% \end{lemma}
% TODO: check, proof.
% TODO: can we even state the approximation error?


\subsection{Assumptions and Proof of Lemma~\ref{lemma:approx-atv}}
For the lemma to hold we make the technical assumption:

(A) The distribution $p$ of continuous functions $[0,T]\to \R$ is given by %a family of continuously differentiable functions
a continuous map\footnote{%
	We define $\phi$ on the larger interval $[-1,T]$, instead of $[0,T]$, in order to use one-sided difference quotients.
} ${\phi\colon\Theta\times[-1,T]\to \R, (\theta,t)\mapsto \phi(\theta,t)}$,
with a compact parameter space $\Theta\subset \R^d$,
and a probability measure $\mu$ on $\Theta$.
We require that the map $\pdv{\phi}{t}$ is continuous, too.

We sometimes use $x_\theta$ for $\phi(\theta,\cdot\,)$  and $x'_\theta$ for $\pdv{\phi}{t}(\theta,\cdot\,)$.
This is no restriction for the differential
equations we consider, as we can always obtain a solution on $[-1,T]$, given parameters (including  $x(0)=x_0)$.

Sampling from this distribution is done by sampling from the probability measure on $\Theta$.
A sequence of i.i.d.\ random variables $\theta_n$, $n\in\N_{>0}$, distributed according to $\mu$, defines a sequence of random functions $(x_n)_{n\in\N_{>0}}$, $x_n(t) = \phi(\theta_n, t)$,
as in the statement of the \Cref{lemma:approx-atv}.

For the proof we will need a uniform law of large numbers (ULLN).
There exist many versions of such theorems
(e.g.\ \citep{jennrich_asymptotic_properties_non-linear_1969}, or \citep{andrews_consistency_1987}),
but we prefer to state and prove a version with strong assumptions appropriate for our setting.

\begin{lemma}[Uniform Law of Large Numbers]\label{lem:ulln}
Let $f\colon\Theta\times I \to \R$ be a continuous function on compact metric spaces $\Theta$ and $I$.
Let $\theta_i$, $i\in\N_{>0}$ be i.i.d.\ random variables with values in $\Theta$,
all distributed according to a probability measure $\mu$ on $\Theta$.
Then
%
\begin{align}
	\frac{1}{N}\sum_{i=1}^N  f(\theta_i, t)
	\xrightarrow{N\to\infty} \int_{\Theta} f(\theta, t) \dd{\mu(\theta)}
\end{align}
%
uniformly in $t\in I$ almost surely.
\end{lemma}

\begin{proof}\label{proof:ulln}
As $\Theta\times I$ is compact and $f$ continuous, $f$ is even uniformly continuous.
Hence, for any $\epsilon>0$, we can then choose a $\delta$ such that
%
\begin{align}
	d(\theta,\theta') < \delta
	\qq{and}
	(t,t')<\delta
	&\Longrightarrow \left|f(\theta',t')-f(\theta,t)\right|<\epsilon
\end{align}
%
Where $d$ is the metric on $\Theta$.
Now by compactness of $I$, finitely many $\delta$-balls in $I$ cover it,
i.e.\ there are ${t_1, \ldots, t_k\in I}$, such that the $B_\delta(t_j)$, ${j=1, \ldots, k}$ cover $I$.
For each $(t_j)_{j=1:k}$ we can apply a strong law of large numbers to obtain almost surely convergence of ${\frac{1}{N} \sum_{i=1}^N f(\theta_i, t_j)}$ to ${\int_{\Theta} f(\theta, t_j) \dd{\mu(\theta)}}$.

Hence, almost surely, the following holds: For $\epsilon>0$ we can choose $N_0>0$ such that
for all natural numbers $N\geq N_0$ and all $j\in\{1, \ldots, k\}$:
%
\begin{align}
	\left|
		\frac{1}{N} \sum_{i=1}^N f(\theta_i, t_j)
		- \int_{\Theta} f(\theta, t_j) \dd{\mu(\theta)}
	\right|
	&<\epsilon
\end{align}
%
Now for any $t\in I$ there is a $j\in\{1, \ldots, k\}$ such that
%
\begin{align}
	\left|\frac{1}{N} \sum_{i=1}^N \bigl(f(\theta_i, t_j) - f(\theta_i, t)\bigr)\right| &< \epsilon
	&&\text{and}&
	\left| \int_{\Theta} f(\theta, t_j) - f(\theta,t) \dd{\mu(\theta)}\right| &< \epsilon
\end{align}
%
The triangle inequality yields
%
\begin{align}
	\left|
		\frac{1}{N} \sum_{i=1}^N  f(\theta_i, t)
		-  \int_{\Theta} f(\theta, t) \dd{\mu(\theta)}
	\right|
	&< 3\epsilon
\end{align}
%
for all $t\in I$, which proves the claim.
\end{proof}


\begin{proof} (\Cref{lemma:approx-atv})
%	Let $t\mapsto x_\theta(t)$ be a function in the probability distribution $p$ of functions $[-1, T]\to\R$ corresponding to a parameter value $\theta\in\Theta\subset\R^d$, which
%	encodes all the randomness.
%	By the assumptions of the lemma,  the map $\phi:\Theta\to C^1([-1,T])$,
%	$\theta\mapsto x_\theta'$ is continuous, and by compactness of $\Theta$, uniformly continuous,
%
%	For ODE models without noise $\theta$ encodes the  parameters and initial values.
%	A random function in this family can be obtained by drawing $\theta$ from this distribution.
%
Let $\theta_i$, $i\in\N_{>0}$ be i.i.d.\ random variables
with values in $\Theta$, all distributed according to a probability measure $\mu$ on $\Theta$.

Define $\psi:\Theta\times[0,1]\times [0,T]$ by
%
\begin{align}
	\psi(\theta, \epsilon, t)
	&=
	\begin{cases}
		\frac{\phi(\theta, t)-\phi(\theta,t-\epsilon)}{\epsilon} & \text{if $\epsilon>0$}
	\\	\pdv{\phi}{t}(\theta, t) & \text{if $\epsilon=0$}
	\end{cases}
\end{align}
%
For $\epsilon>0$ the function is obviously continuous as a composition of continuous functions.
Let now be $\epsilon=0$ and $t\in[0,T]$, $\theta\in\Theta$. We claim that $\psi$ is continuous
at $(\theta,0,t)$. $\pdv{\phi}{t}$ is uniformly continuous on its compact domain of definition.

Let $\delta$ be greater than zero. We can choose a $\gamma>0$ such that for ${d(\theta,\theta')<\gamma}$ and ${|t-t'|<\gamma}$ we have ${\left|\pdv{\phi}{t}(\theta', t') - \pdv{\phi}{t}(\theta, t)\right|<\delta}$.

Now chose ${(\theta', \epsilon', t')}$ with ${d(\theta,\theta')<\gamma}$ and $d(t,t')<\gamma/2$ and $0\leq\epsilon'<\gamma/2$.
If $\epsilon'=0$, we directly obtain
%
\begin{align}
	\left|\psi(\theta',0, t') - \psi(\theta, 0, t)\right|
	&=  \left|\pdv{\phi}{t}(\theta',t') - \pdv{\phi}{t}(\theta,t)\right|
	< \delta
\end{align}
%
If $\epsilon'>0$, the mean value theorem implied that there is a $\tau\in(t'-\epsilon',t')$ such that:
%
\begin{align}
	\left|\psi(\theta',\epsilon', t')-\psi(\theta, 0, t)\right|
	&=  \left|\pdv{\phi}{t}(\theta',\tau) - \pdv{\phi}{t}(\theta,t)\right|
	< \delta
\end{align}
%
as ${d(\tau, t) \leq d(\tau,t') + d(t',t) \leq \epsilon'+\gamma/2 < \gamma/2+\gamma/2 = \gamma}$.
This establishes the continuity of $\psi$.

Therefore, $\psi$ is also uniformly continuous on its compact domain of definition.

Define $\Psi\colon[0,1]\times[0,T]\to \R$ by
%
\begin{align}
	\Psi(\epsilon, t) &= \int_{\Theta} \psi(\theta, \epsilon, t) \dd{\mu(\theta)}
\end{align}
%
We also define $\Psi^N\colon[0,1]\times[0,T]\to \R$
%
\begin{align}
	\Psi^N(\epsilon, t) &= \frac{1}{N} \sum_{i=1}^{N}\psi(\theta_i, \epsilon, t)
\end{align}
%
The uniform law of large numbers shows that almost surely  $\Psi^N$ converges uniformly
to $\Psi$ for $N\to\infty$.

Analogously we define $\Xi\colon[0,1]\to \R$ by
%
\begin{align}
	\Xi(\epsilon, t) &= \int_{\Theta} \psi(\theta, \epsilon, t)^2 \dd{\mu(\theta)}
\end{align}
%
We also define $\Psi^N\colon[0,1]\times[0,T]\to \R$
%
\begin{align}
	\Xi^N(\epsilon, t) &= \frac{1}{N} \sum_{i=1}^{N}\psi(\theta_i, \epsilon, t)^2
\end{align}
%
Again, the uniform law of large numbers shows that almost surely  $\Xi^N$ converges uniformly
to $\Xi$ for $N\to\infty$. Hence,
%
\begin{align}
	\numstd[(\psi(\theta_i,\epsilon,t))_{i=(1:N)}]
	&= \sqrt{\Xi^N(\epsilon,t)-\Psi^N(\epsilon, t)^2}
\end{align}
%
converges almost surely uniformly to $\sqrt{\Xi(\epsilon, t)-\Psi(\epsilon, t)^2}=\std_{\theta\sim \mu}[\Psi(\epsilon,t)]$ for $N\to\infty$.


On the other hand  by uniform continuity
%
\begin{align}
	\numstd[(\psi(\theta_i,\epsilon,t))_{i=(1:N)}]
	&\xrightarrow[\text{uniformly}]{\epsilon\to 0} \numstd[(\psi(\theta_i,0,t))_{i=(1:N)}]
\\	\sqrt{\Xi(\epsilon, t)-\Psi(\epsilon, t)^2}
	&\xrightarrow[\text{uniformly}]{\epsilon\to 0} \std_{\theta\sim \mu}[(\Psi(0,t))]
	= \std_{x\sim p} x'(t)
\end{align}

Now, given some $\delta>0$, we can choose an $\epsilon'$ and almost surely an $N_0$ such that
for all $0<\epsilon<\epsilon'$ and $N\geq N_0$:
%
\begin{align}
	\left|
		\numstd[(\psi(\theta_i,\epsilon,t))_{i=(1:N)}]
		- \numstd[(\psi(\theta_i,0,t))_{i=(1:N)}]
	\right|_{C^0([0,T])} &\leq \delta
	\\
	%| \numstd[(\psi(\theta_i,0,t))_{i=(1:N)}] - \sqrt{(\Xi^N(0,t)-\Psi^N(0, t)^2} |_{C^0([0,T]))}& \leq %\delta \\
	\left|
		\numstd[(\psi(\theta_i,0,t))_{i=(1:N)}]  - \std_{x\sim p} x'(t)
	\right|_{C^0([0,T])}
	&\leq \delta
\end{align}
%
and for $t,t'\in [0,T]$, $|t-t'|<\epsilon$:
%
\begin{align}
	\left|
		\numstd[(\psi(\theta_i,\epsilon,t))_{i=(1:N)}]
		- \numstd[(\psi(\theta_i,\epsilon,t'))_{i=(1:N)}]
	\right|
	&\leq \delta
\end{align}
%
Therefore,
%
\begin{align}
	\left|
		\frac{1}{T} \int_0^T \numstd[(\psi(\theta_i,\epsilon,t))_{i=(1:N)}] \dd{t}
		- \frac{1}{T} \int_0^T \std_{x\sim p} x'(t) \dd{t}
	\right|
	&< 2\delta
\end{align}
%
and for $T/\epsilon\in\N$ the Riemann sum with step size $\epsilon$ differs from the integral by at most $T\delta$, i.e.
%
\begin{align}
	\left|
		\frac{1}{T} \int_0^T \numstd[(\psi(\theta_i,\epsilon,t))_{i=(1:N)}] \dd{t}
		- \frac{\epsilon}{T} \sum_{k=1}^{T/\epsilon} \numstd[(\psi(\theta_i,\epsilon,k\epsilon))_{i=(1:N)}]
	\right|
	&<\delta
\end{align}
%
In the notations of the lemma, with $\frac{1}{T} \int_0^T \std_{x\sim p} x'(t) \dd{t} = \MPGD(p)$ we get:
%
\begin{align}
	\left|
		\frac{\epsilon}{T} \sum_{t=\epsilon}^{T} \numstd[(x^{\diff}_{i,t})_{i=1:N}] - \MPGD(p)
	\right|
	&< 3\delta
\end{align}
%
Thus proving the claim that the left term of the difference converges almost surely for $\epsilon\to 0 $ and $N\to\infty$ to the right term of the difference.
Please note that $N$ and $\epsilon$ were chosen independently. The limits can be taken simultaneously or in arbitrary order.
%
%	As $(\epsilon,t)\mapsto \sqrt{\int_{\Theta}\bigl(\psi_\epsilon(\theta)(t)-f_\epsilon(t)\bigr)^2 \dd{\mu(\theta)}}$
%	is uniformly continuous on $(0,1]\times [0,1]$, we can find an $\epsilon''$,  $0<\epsilon''<\epsilon'$, such
%	that for all $0<\epsilon<\epsilon''$ the approximation of
%	$\sqrt{\int_{\Theta}\bigl(\psi_\epsilon(\theta)(t)-f_\epsilon(t)\bigr)^2 \dd{\mu(\theta)}}$ by a step function
%	with step size $\epsilon$ has a maximal error $<\delta/T$, hence
%	%
%	\begin{align*}
%		\epsilon\sum_{t=\epsilon}^T \numstd[x^{\text{diff}}_{n,t}  \mid n=1{:}N]
%	\end{align*}
%	%
%	differs from
%	%
%	\begin{align*}
%		\int_{0}^{T}\std(\psi_\epsilon(\theta_i))\dd{t}
%	\end{align*}
%	%
%	by an error of less than $\delta$. With Equations~\eqref{eq:ineq1} and~\eqref{eq:ineq2} we get that
%	%
%	\begin{align*}
%		\left|
%		\int_0^T \std(x'(t) \mid x\sim p) \dd{t}
%		- \epsilon \sum_{t=\epsilon}^T \numstd[x^{\text{diff}}_{n,t}  \mid n=1{:}N]
%		\right|
%		&< 3\delta
%	\end{align*}
%	%
%	for $N>N_0$ and $0<\epsilon<\epsilon''$, where the construction works for almost all samples $(\theta_1,\ldots,\theta_N)$, that is with probability 1.
%	We multiply both sides with $1/T$ to obtain the result.
\end{proof}
%
%
%\begin{proof}
%Let $t\mapsto x_\theta(t)$ be a function in the probability distribution $p$ of functions $[-1, T]\to\R$ corresponding to a parameter value $\theta\in\Theta\subset\R^d$, which
%encodes all the randomness.
%By the assumptions of the lemma,  the map $\phi:\Theta\to C^1([-1,T])$,
%$\theta\mapsto x_\theta'$ is continuous, and by compactness of $\Theta$, uniformly continuous.
%For ODE models without noise $\theta$ encodes the  parameters and initial values.
%A random function in this family can be obtained by drawing $\theta$ from this distribution.
%We assume that $\theta$ is distributed according to some probability measure $\mu$ on the compact set $\Theta$.
%
%Let	$\psi_\epsilon(\theta)$, $\epsilon<1$ be the function $[0,T]\to \R$ defined by the difference quotient:
%%
%\begin{align*}
%	\psi_\epsilon(\theta)(t) &:= \frac{x_\theta(t)-x_\theta(t-\epsilon)}{\epsilon}
%\end{align*}
%%
%By assumption $\psi_\epsilon(\theta)$ converges point-wise to $x_\theta'$.
%%By compactness of $\Theta$ and continuity of $\phi$,
%%there is a $C>0$ such that $\|x_\theta\|_{C^1([-1,T])}\|<C$ for all $\theta$.
%By the mean value theorem the difference quotients $\psi_\epsilon(\theta)(t)$ are equal to the derivatives $x_\theta'$ at some $\tau\in(t-\epsilon, t)$.
%$x_\theta'$ is uniformly continuous on the compact interval $[-1,T]$,
%hence $\psi_{\epsilon}(\theta)$ converges uniformly to $x_\theta'$ on $[0,T]$ for $\epsilon\to 0$.
%Compactness of $\Theta$  allows a simple covering argument to obtain:
%%As the map $(\theta,t)\mapsto x_\theta'(t)$ is continuous (following from the assumption (A) using
%%~\citep{von-querenburg_mengentheoretische_topologie_2001}, Theorem 14.17) and $\Theta$ is compact, by a similar argument
%%we get:
%
%(C1) the convergence of $\psi_{\epsilon}(\theta)$ to $x_\theta'$ in $C([0,T])$
%is uniform in $\theta\in\Theta$.
%
%%, so
%%the constant function $[0,T]\to \R, t\mapsto C$ is a majorant for all $\psi_\epsilon(\theta)$. This majorant
%%is in $L^p([0,T])$ for all $p\geq 1$ By Lebesgue's
%%dominated convergence theorem, $\psi_\epsilon$ converges to $x_\theta'$ in $L^p$ for $p\geq 2$.
%Therefore,
%
%(C2) for any continuous family $[0,1]\to C[0,T]$, $\epsilon \mapsto f_\epsilon$,  the square $(\psi_\epsilon(\theta)-f_\epsilon)^2$ converges
%uniformly to $(x_\theta'-f_0)^2$, where this convergence is uniform in $\theta$.
%
%We now consider a sample $(\theta_1,\ldots, \theta_N)$ of size $N$ from the probability distribution on $\Theta$.
%By a uniform law of large numbers~\citep{jennrich_asymptotic_properties_non-linear_1969} (with weaker requirements):
% %
%\begin{align*}
%	\frac{1}{N}\sum_{i=1}^N \psi_{\epsilon}(\theta_i)
%	&\to \int_\Theta  \psi_\epsilon(\theta)\dd{\mu(\theta)}
%\end{align*}
%%
%uniformly almost surely, and likewise
%%
%\begin{align*}
%	\frac{1}{N}\sum_{i=1}^N x_{\theta_i}'
%	&\to \int_\Theta  x_\theta' \dd{\mu(\theta)}
%\end{align*}
%%
%uniformly almost surely.
%We define the continuous family $[0,1]\to C[0,T]$, $\epsilon \mapsto f_\epsilon$ via expectation values
%%
%\begin{align*}
%	f_\epsilon &=
%	\begin{cases}
%		\int_\Theta  \psi_\epsilon(\theta)\dd{\mu(\theta)}& \text{if $\epsilon>0$}
%	\\	\int_\Theta  x_\theta' \dd{\mu(\theta)} & \text{if $\epsilon=0$}
%	\end{cases}
%\end{align*}
%%
%Continuity at $\epsilon=0$ ensues from (C1). Now we invoke again the uniform law of large numbers and state
%%
%\begin{align*}
%	\frac{1}{N}\sum_{i=1}^N (\psi_\epsilon(\theta_i)-f_\epsilon)^2
%	&\to \int_\Theta (\psi_\epsilon(\theta)-f_\epsilon)^2 \dd{\mu(\theta)}
%\end{align*}
%%
%uniformly almost surely. For a sequence of samples $\theta_i$ and  a given $\delta>0$ we can almost surely choose an $N_0\in \N$, such that for $N\geq N_0$ and all $0<\epsilon<1$:
%
%%
%\begin{align}\label{eq:ineq1}
%	\left\|\sqrt{
%		\frac{1}{N}\sum_{i=1}^{N} (\psi_\epsilon(\theta_i)
%		- \frac{1}{N}\sum_{j=1}^{N} \psi_\epsilon(\theta_j))^2}
%		- \sqrt{\int_\Theta (\psi_\epsilon(\theta)-f_\epsilon)^2 \dd{\mu(\theta)}}
%	\right\|_{C^0([0,T])}
%	&< \tfrac{1}{T}\delta
%\end{align}
%The first expression in the difference is the (not bias corrected) standard deviation of the samples of the difference quotients, the second one is the standard deviation of distribution of the difference quotients.%
%By (C2) one  can find an $\epsilon'>0$ such that for all $0<\epsilon<\epsilon'$:
%%
%\begin{align}\label{eq:ineq2}
%	\left\|
%		\sqrt{\int_{\Theta}(\psi_\epsilon(\theta)-f_\epsilon)^2 \dd{\mu(\theta)}}
%		- \sqrt{\int_{\Theta}(x_\theta'-f_0)^2}
%	\right\|_{C^0([0,T])}
%	&< \tfrac{1}{T}\delta
%\end{align}
%%
%As $(\epsilon,t)\mapsto \sqrt{\int_{\Theta}\bigl(\psi_\epsilon(\theta)(t)-f_\epsilon(t)\bigr)^2 \dd{\mu(\theta)}}$
%is uniformly continuous on $(0,1]\times [0,1]$, we can find an $\epsilon''$,  $0<\epsilon''<\epsilon'$, such
%that for all $0<\epsilon<\epsilon''$ the approximation of
%$\sqrt{\int_{\Theta}\bigl(\psi_\epsilon(\theta)(t)-f_\epsilon(t)\bigr)^2 \dd{\mu(\theta)}}$ by a step function
%with step size $\epsilon$ has a maximal error $<\delta/T$, hence
%%
%\begin{align*}
%	\epsilon\sum_{t=\epsilon}^T \numstd[x^{\text{diff}}_{n,t}  \mid n=1{:}N]
%\end{align*}
%%
%differs from
%%
%\begin{align*}
%	\int_{0}^{T}\std(\psi_\epsilon(\theta_i))\dd{t}
%\end{align*}
%%
%by an error of less than $\delta$. With Equations~\eqref{eq:ineq1} and~\eqref{eq:ineq2} we get that
%%
%\begin{align*}
%	\left|
%		\int_0^T \std(x'(t) \mid x\sim p) \dd{t}
%		- \epsilon \sum_{t=\epsilon}^T \numstd[x^{\text{diff}}_{n,t}  \mid n=1{:}N]
%	\right|
%	&< 3\delta
%\end{align*}
%%
%for $N>N_0$ and $0<\epsilon<\epsilon''$, where the construction works for almost all samples $(\theta_1,\ldots,\theta_N)$, that is with probability 1.
%We multiply both sides with $1/T$ to obtain the result.
%\end{proof}
