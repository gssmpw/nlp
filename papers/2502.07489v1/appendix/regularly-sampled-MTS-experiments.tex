\section{Regularly Sampled Time Series Data Experiments}\label{app:regular}

In this chapter, we show important findings related to generating \textbf{regularly sampled multivariate time series data} with \Bench. 
We discuss the generation process of 3 datasets that were chosen based on 2 important criterions.
First is the previously mentioned JGD which was shown to be important indicator for the complexity of the dataset. 
Second criterion is for these datasets to include a high number of channels as we show through the following small experiment, how channel dependency matters more and have more effect when tested on our ODE-datasets compared to current benchmark datasets.

The datasets used here were DOK01, INA01, and HYN01 which all have a number of channels greater than 15 with 18, 29, and 22 channels respectively. 
Furthermore, they are ranked highly with respect to the JGD metric having values of 2,277, 2,218, and 1,548 respectively which makes them the most complex high dimensional (more than 15 channels) datasets in our benchmark.
Datasets were generated based on the configuration at~\Cref{sec:dsinfo}, generating 100 samples for each dataset based on 100 different initial conditions that are drawn uniformly at random from a distribution based on the configuration given to the model.
Each time series sample spans 100 timesteps. 
The datasets were split into a train/val/test split of 70/20/10 which is similar to the split done normally on the TSF literature for the weather, electricity and traffic datasets~\citep{Chen2023.TSMixerb, Zeng2023.Are, Nie2023.Time}.   

We conduct experiments on state-of-the-art Time Series Forecasting models PatchTST~\citep{Nie2023.Time}, TSMixer~\citep{Chen2023.TSMixerb}, and DLinear~\citep{Zeng2023.Are}. 
For DLinear, there is only a univariate model (doesn't capture channel dependency), but for the other models, they can be implemented as univariate or multivariate (capture channel dependency) variants.
So overall, we label the univariate models as \textbf{CI} (\textbf{C}hannel \textbf{I}ndepedent), and multivariate models as \textbf{CD} (\textbf{C}hannel \textbf{D}ependent). 
These models are tested with the 3 ODE datasets mentioned earlier tuning the hyperparameters for the 3 models through random search of 20 trials for a predefined range of parameters.

For PatchTST, we set the following hyperparameter range:

\begin{itemize}
	
	\item learning rate (range of floats) $\rightarrow$ [$10^{-7}$, $10^{-2}$].
	\item number of encode layers (range of integers) $\rightarrow$ [1, 6].
	\item Dimensionality of fully connected layer (categorical) $\rightarrow$ [256, 512, 1024].
	\item Embedding dimension of the attention layer (categorical) $\rightarrow$ [128, 256, 512, 1024].
	\item dropout (range of floats) $\rightarrow$ [0, 0.9].
	\item fully connected layer dropout (range of floats) $\rightarrow$ [0, 0.9].
	\item patch size (categorical) $\rightarrow$ [4, 8].
	\item stride (categorical) $\rightarrow$ [2, 4].
\end{itemize}

For TSMixer, we set the following hyperparameter range: 


\begin{itemize}
	
	\item learning rate (range of floats) $\rightarrow$ [$10^{-7}$, $10^{-2}$].
	\item number of blocks (range of integers) $\rightarrow$ [1, 5].
	\item hidden size of MLP layers (categorical) $\rightarrow$ [32, 64, 256, 1024].
	\item dropout (range of floats) $\rightarrow$ [0, 0.9].
	\item activation (categorical) $\rightarrow$ [ReLU, GeLU].
	
\end{itemize}

For DLinear, learning rate was tuned on the same range of values mentioned above.
All models were tuned on the validation set. 
The results for the best performing model on validation set (lowest MSE error) were reported on the test set.
We used a lookback window of 24 and forecasting horizon of 12 for all the conducted experiments on the ODE datasets.
On the other hand, we report the results of the mentioned models on  well-established evaluation datasets such as ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, and Traffic on a forecasting horizon of 96. 
The reported results from~\Cref{tab:regular_time_series_forecasting_results} give us interesting insights about our benchmark on the regularly sampled time series data.

\begin{table}[!h]
	\centering
	\scriptsize
	\begin{threeparttable}
		
		\caption{%
			MSE Test loss reported on 3 ODE datasets as well as on 7 of the most popular regular TSF datasets.
			We highlight the best performing version (CI/CD) of each model in \textbf{bold}.
			The results were reported on 5 different baselines showing a clear trend of CD models being better on our generated ODE datasets.
			On the other hand, for benchmark datasets, this trend is not there as CI models are performing quite well.
		}\label{tab:regular_time_series_forecasting_results}
		
		\begin{tabular}{l ccccc}
			\toprule
			Dataset & PatchTST(CI) & PatchTST(CD) & TSMixer(CI) & TSMixer(CD) & DLinear
			\\  \midrule
			INA01 & 0.077 & \textbf{0.049} & 0.348 & \textbf{0.150} & 0.444 \\ 
			DOK01 & 0.094 & \textbf{0.033} & 0.206 & \textbf{0.102} & 1.294 \\
			HYN01 & \textbf{0.003} & \textbf{0.003} & 0.069 & \textbf{0.060} & 0.217 \\
			\hline
			ETTh1 & \textbf{0.375*} & 0.416* & 0.361* & \textbf{0.359*} & 0.375* \\
			ETTh2 & \textbf{0.274*} & 0.334* & \textbf{0.274*} & 0.275* & 0.289* \\
			ETTm1 & \textbf{0.290*} & 0.326* & 0.285* & \textbf{0.284*} & 0.299* \\
			ETTm2 & \textbf{0.165*} & 0.195* & 0.163* & \textbf{0.162*} & 0.167* \\
			Weather & \textbf{0.152*} & 0.168* & \textbf{0.145*} & \textbf{0.145*} & 0.176* \\
			Electricity & \textbf{0.130*} & 0.196* & \textbf{0.131*} & 0.132* & 0.140* \\
			Traffic & \textbf{0.367*} & 0.595* & 0.376* & \textbf{0.370*} & 0.410* 
			\\  \bottomrule
		\end{tabular}
		\begin{tablenotes}
			\small
			\item[*] Results are taken directly for PatchTST, TSMixer, and DLinear from the respective papers \citep{Nie2023.Time}, \citep{Chen2023.TSMixerb}, and \citep{Zeng2023.Are}. 
		\end{tablenotes}
		
	\end{threeparttable}
\end{table}

We notice from the results how unpredictable the results can be for the benchmark datasets, with \textbf{CI} models being better on some datasets and \textbf{CD} models being better on the rest. 
Also, one more interesting finding is how close the test loss is between a very simple CI model such as DLinear and the state-of-the-art models which can be concerning regarding how much channel dependency there is on the available benchmark datasets. 
On the other hand, for our ODE-generated datasets, the results are more consistent with clear better performance for \textbf{CD} models over their \textbf{CI} counterparts.
Also, the best model overall was noticed to be always a \textbf{CD} model which shows the importance of capturing channel dependency on the ODE generated datasets.
One more interesting finding is how worse is a DLinear model on our benchmark datasets where its results are at least 10 times worse than the best performing model.
That shows how complex these datasets are, as well as how multivariate models can clearly benefit from intrinsic relation between the channels on such datasets. 