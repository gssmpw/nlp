\section{Related Work}
% \textbf{LLM Inference.} For example, FlashAttention \citep{flashatten, flashatten2} introduces an innovative approach to GPU attention kernel design. VLLM \citep{vLLM} and TensorRT-LLM \citep{trt-LLM} have optimized end-to-end inference performance using systematic techniques. llama.cpp \cite{llamacpp} has made notable contributions by delivering superior performance on edge devices. Additionally, T-MAC presents a LUT-based mpGEMM method that further enhances the performance of low-bit LLMs.

% \noindent \textbf{BitNet b1.58 and ternary LLMs.} BitNet b1.58 \cite{wang2023bitnet} introduces a new era of 1-bit LLMs by quantizing all attention weights to ternary, reducing the bpw to 1.58. Recently, more ternary LLMs have been released, such as TriLM \cite{trillm}, Llama3-8B-1.58 \cite{llama3-1.58b}, and BitNet a4.8 \cite{wang2024bitneta484bitactivations}, further pushing the boundaries of 1-bit LLMs. Ternary LLMs also exhibit promising traits by adhering to scaling laws \cite{ma2024era}, suggesting that their performance can improve as model sizes grow.

% \noindent \textbf{MAD-based and LUT-based mpGEMM.}
% For mpGEMM, the MAD-based solution involves performing dot products, while the LUT-based solution relies on lookup tables to store precomputed intermediate values, which are then accumulated through table lookups. Recently, there has been growing interest in using LUT-based solutions for LLM inference. For example, methods such as \cite{lut-gemm} and \cite{gpu-lut1} leverage LUTs to compute GEMM on GPUs. However, in practice, LUT-based solutions for GPU inference are generally slower than MAD-based solutions, such as \cite{cutlass} and \cite{bitblas}, primarily due to the inefficiencies associated with table memory access. In contrast, T-MAC demonstrates that bit-wise LUT-based solutions can significantly outperform MAD-based solutions in edge inference. Despite this, there remains room for improvement in the context of ternary LLMs, which led us to develop the element-wise LUT design.

% \noindent \textbf{Lossless inference for BitNet b1.58}
% A common issue is that full-precision LLMs obtained through pretraining cannot be deployed on edge devices due to the significant computational load caused by floating-point operations. In most cases, post-training quantization (PTQ) can address this problem. PTQ refers to converting a full-precision LLM to a low-precision without retraining, with related works including \cite{smoothquant, AWQ, quip, gptq}. However, although successful PTQ does not lead to a dramatic drop in accuracy, it inevitably results in quantization loss, causing differences between the LLM outputs before and after quantization. In some critical downstream tasks, these differences may be unacceptable. In contrast, Quantization-Aware Training (QAT) effectively avoids this issue. QAT involves retraining a pretrained model to obtain a quantized model, thus mitigating quantization loss. Relevant works include \cite{liu2023llmqatdatafreequantizationaware, eff-qat}. BitNet B1.58 adopts QAT, which performs ternary quantization on weights and int8 per-tensor quantization on activations during training, creating conditions for lossless inference in the system. Based on this, if the constraints imposed during training remain unchanged during inference, we can achieve lossless inference specifically for BitNet.
% In BitNet b1.58, a unique feature is that both the weights and activations are quantized: weights to ternary values and activations to int8 using the per-tensor absmax method. When both the weights and activations preserve these quantization characteristics during inference, we refer to the process as lossless inference.

% For mpGEMM, the LUT-based solution refers to using lookup tables to store precomputed results. For example, methods such as \cite{lut-gemm, gpu-lut1} use LUT to compute GEMM on GPU. However, in practice, LUT-based solutions for inference on GPU tend to be slower than MAD-based solutions such as \cite{cutlass, bitblas}. In contrast, MAD-based refers to direct per-element multiplication and accumulation. The core instructions for both approaches are shown in Table \ref{tab:wise-instruct}. The classification of mpGEMM for LLMs on edge devices is illustrated in Figure \ref{fig:classfication}, with QX\_0 and QX\_K both implementing in llama.cpp.

\begin{figure*}[t]
  \includegraphics[width=\linewidth, trim=0 245 0 45, clip]{assets/typenewnew.pdf}
  \caption {\label{fig:diff}A simple example to explain the differences between various methods for completing mpGEMM when $K=4$: (1) represents the MAD-based solution, where the result is obtained via the dot product; (2) represents the bit-wise LUT-based solution, where the weights are split into different bit indices, and the result is obtained by performing a lookup in the LUT, followed by bit-shifting and accumulation; (3) represents the element-wise LUT-based solution, where all possible values of the weights are enumerated to obtain the index, and the result is obtained by performing a lookup in the LUT, followed by accumulation. $A_x$ refers to the $x_{th}$ bit in weight $A$. In (2), $g = 4$ and $b = 2$; whereas in (3) $g = 2$ and $C = 3$.}
\end{figure*}