


\paragraph{Main results.} 
The experiments were conducted on three language models, demonstrating their effectiveness in combination with existing continual learning methods, such as Incremental Lora~\cite{hu2021lora} (\textbf{IncLora}), Elastic Weight Consolidation~\cite{Kirkpatrick_2017} (\textbf{EWC}), Orthogonal Lora~\cite{wang2023orthogonal} (\textbf{OLora}), and Instruction-based Memory Replay~\cite{wang2024inscl} (\textbf{InsCL}).
% , and Interference-Free Lora (\textbf{InfLora}). 
Besides the three task sequences we adopted in the previous sections, we also addressed the effectiveness of our approach on the public benchmark TRACE~\cite{wang2023trace}, which includes a learning sequence comprised of multi-choice QA, code generation, mathematical reasoning, and summarization tasks. Table~\ref{table:main} shows the continual instruction tuning performance on four benchmarks, leading to several key observations:
% In our comparison, we prioritized training with hyper-parameters mentioned in previous works. We loaded the base LM into torch.bfloat16 to save memory and ran the experiments on 4 NVIDIA A100 GPUs.
% Table~\ref{table:main} shows the continual instruction tuning performance on four benchmarks, leading to several key observations:

%IV-guided training significantly prevents the loss of general and reasoning capabilities. Unlike typical continual learning methods that often suffer from substantial forgetting of general abilities, our IV-guided approach reduces the average forgetting rate on $HP$ to -0.16, a stark improvement over the 5.03 seen with other methods. Moreover, it enhances in-context performance from 37.90 to 50.05, underscoring the benefits of maintaining the computation graph.
\begin{wrapfigure}{hr}{0.53\textwidth}
  \centering
  \vspace{-1.8em}
  \includegraphics[width=1.0\linewidth]{pdf_figs/sec6_1_fvsh.pdf}
  \vspace{-2.5em}
  \caption{The shifts in function vector with 0/5-shot performance with function vector guided training. {\textit{\textbf{Main conclusion:} FVG prevents the shift in FV (yellow bar) and thus mitigates forgetting (orange line).}}} 
  \label{fig:sec6:fvshit}
  \vspace{-1.0em}
\end{wrapfigure}


\textit{Observation 1}: Function vector guided training significantly reduces forgetting in both general and in-context learning capabilities. 
Traditional continual learning methods experience substantial forgetting, with InsCL performing somewhat better yet still experiencing a minimum decline of 11.34\% on \textbf{IP}. By contrast, function vector guided training achieves consistent and substantial gains in combating this issue, enhancing performance in the Llama2-7b-chat across all baselines, achieving average increases of 5.44 in \textbf{GP} and 17.52 in \textbf{IP}. Function vector-guided training provides a methodology to break through the forgetting of general knowledge in existing models.



\textit{Observation 2}: function vector guided training does not compromise the plasticity in learning new tasks. This technique demonstrates a negligible reduction in the \textbf{FP} metric, signifying a well-maintained balance between plasticity and stability. For the TRACE datasets with the IncLora method, our method shows an improvement of 12.13 on the Llama2-7b-chat model, showing large potential in protecting performance on the training sequence as well. Nonetheless, under certain scenarios, like when utilizing the InsCL replay method on NI-Seq-M1, our strategy yields a 2.8 drop in \textbf{FP}. This could be attributed to the conflict between the diverse gradient information from the memory buffer and our regularization component.




\textit{Observation 3}: The effectiveness of continual learning methods varies with the type of training sequence. As discussed above, different training tasks impact existing capabilities in distinct ways. For instance, while learning classification tasks results in relatively minor forgetting, sequences of generative tasks tend to lead to more pronounced issues. Consequently, the conventional approach of uniformly applying the same strategy to every training task demonstrates significant variance in performance across different sequences. Specifically, InsCL exhibits a performance decline on the NI-Seq-C1 dataset compared to IncLora, while it shows improvements on the other three datasets. In contrast, our method retains the model's task-specific functions and thoroughly accounts for the characteristics of the current learning task, thereby achieving consistent and considerable enhancements across a diverse range of datasets.


\textbf{How does function vector-guided training work?} Following our principle of designing function vector-guided training, we test the mechanics of this approach by examining the shifts in function vectors during training. We visualize the alteration of function vectors during function vector-guided training in Fig.~\ref{fig:sec6:fvshit}, following the setting in Sec.~\ref{sec4}. It becomes evident that the methods we propose can effectively maintain the shifts in function vectors for general tasks, even without the need to access their training data. As the FVs remain stable during the fine-tuning phase, the performance on general tasks is effectively safeguarded.
