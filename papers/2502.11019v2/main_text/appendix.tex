% \section{Appendix Overview by Zhaoyi}
% \begin{itemize}

    % \item Experiment results: similarity of features between tasks is insufficient to predict model-dependent pattern of forgetting mentioned in Section 3.1 (Model perspective).
    % \item Experiment results: demonstrating that model tends to re-use the original functionalities during training (Section 4).
    % \item Experiment results: a shift in the set $\mathcal{S}$ of causal attention heads mentioned in Section 5, evidence III.
% \end{itemize}
\section{Function Vector Extraction}
\label{sec3.2}
We next consider how to extract $\theta_c$ for a given dataset $D^{c}$, drawing on the concept of function vectors proposed by~\citet{todd2023function}. This extraction is carried out using in-context learning (ICL) samples, where the model incorporates task-relevant information into its hidden states as it engages with examples with the ICL prompt. This process is associated with the emergence of $\theta_c$~\citep{todd2023function, hendel2023context}. Subsequently, a causal mediation analysis~\citep{Pearl2013InterpretationAI, NEURIPS2020_92650b2e, li2024understanding} is conducted on the ICL inputs to identify attention heads with significant causal impacts on the output, and aggregating their representations results in $\theta_c$. Interestingly, this vector remains effective even under zero-shot input scenarios. The detailed procedure is outlined below:

First, we start by gathering the task-conditioned activation for each model head by averaging the ICL input representation of the given task $D^{c}$, i.e., 
\begin{equation}
    \bar{h}_{l j}^c=\frac{1}{\left|D^{c}\right|} \sum_{(x) \in D^{c}} h_{\ell j}\left([p, x]\right).
\end{equation}
Where $p = [(x_1, y_1), ..., (x_N, y_N)]$ represents the N-shot ICL prompt text made up of held-out samples of task $c$, ${h}_{lj}$ is the model activation at the last token, layer $l$ and position $j$, and $\bar{h}_{lj}^c$ represents the task-conditioned activations.
% Here, to manually enhance the generalization of the activation, we augment $p_i$ with a set of semantically identical instruction $C$. For example, the instruction set for a multi-choice question includes semantically identical variations such as "Choose the correct option," "Select the right answer," "Identify the correct choice," etc.

Then to assess the existence of a cause-and-effect relationship between $\bar{h}_{l j}^c$ and correct output, we employ causal mediation analysis. The model will run on a counterfactual ICL input $[\hat{p},x]$ incorporating a label-shuffled prompt $\hat{p}=[(x_1,  \hat{y}_1), ..., (x_N, \hat{y}_N)]$, typically leading to incorrect outcomes.
% that allow us to isolate the causal effects via intervention. 
We then substitute the value of the specific head with the task-specific conditioned activation $\bar{h}_{lj}$ and calculate its causal effect (CE) on the model's output.
\begin{equation}
\begin{aligned}
\operatorname{CE}_{lj}([\hat{p},x])=P_{M^{h_{lj}\rightarrow \bar{h}_{lj}^c}}(y_{i} \mid [\hat{p}, x] ) -P_M(y_{i} \mid [\hat{p}, x]).
\end{aligned}
\end{equation}
Here, $M^{h_{lj}\rightarrow \bar{h}^c_{lj}}$ denotes the model with a replacement operation on attention head $(l,j)$ at last token of the input sentence. A higher CE suggests that the specific head's state is crucial in enabling accurate predictions, denoting the encoding of more task-relevant information.
For each head at layer $l$ and position $j$,we adopt the approach proposed by ~\citet{todd2023function} to calculate the average CE across a variety of tasks. Subsequently, we identify the top 10 heads with the highest average CE (recorded as set $\mathcal{S}$) as the most critical in conveying task-relevant information. The task vector $\theta_c$ is is then obtained by aggregating the task-conditioned activation from the attention heads in the set $\mathcal{S}$, i.e., $\theta_c=\sum_{(l,j) \in \mathcal{S}} \bar{h}_{lj}^c$. 

% \begin{equation}
% \theta_c=\sum_{a_{\ell j} \in \mathcal{S}} \bar{h}_{lj}^c.
% \end{equation}


We then evaluates the effectiveness of the function vector ($\theta_c$) through intervention experiments on the initial model across multiple datasets. Results show that the FV significantly influences the output behavior for specific tasks, with its introduction notably improving zero-shot performance in certain tasks and removal diminishing the model's ability to produce correct outputs. This suggests that the model's specific abilities can be identified and analyzed by studying the corresponding FV.


\section{Function Vector Guided Training Algorithm}
\label{app:algo}
    
Algorithm 1 outlines the procedure for function vector guided continual learning. It begins with a sequence of tasks, each paired with its corresponding dataset, as well as a pre-trained language model (referred to as \( M_0 \)). Based on the approach from ~\cite{todd2023function}, a collection of held-out datasets \( \{\bar{D}_1, \bar{D}_2, \ldots, \bar{D}_K\} \) is utilized to determine the set of function vector heads. Furthermore, it is proposed that the function vector head set \( \mathcal{S} \) is applicable across different datasets, allowing us to collect this set \( \mathcal{S} \) only once.

% \begin{algorithm}[H]
% \DontPrintSemicolon
% \SetAlgoLined
  
% \KwIn{Given a sequence of tasks $\{T_1, T_2, .... T_N\}$ and their corresponding dataset $\{D_1, D_2, .... D_N\}$, pre-trained language model $M_0$}
% \KwOut{Language model $M_N$ trained after $N$ tasks}

% \SetKwFunction{FMain}{Main}
% \SetKwFunction{FFuncVec}{GetFunctionVectorSet}
% \SetKwFunction{FContinual}{FVGuidedTraining}

% \SetKwProg{Fn}{Function}{:}{\KwRet}
% \Fn{\FMain{$\{D_1, D_2, .... D_N\}$, $M_0$}}{
%     $\mathcal{S} \leftarrow$ \FFuncVec{$D_i$, $M_0$}\;
%     \For{$i \leftarrow 1$ \KwTo $N$}{
%       $D_P \leftarrow \{(p_i, x_i, y_i) \mid 1<i \leq 100\}$ \tcp*{10-shot dataset}
%       % Sample 100 samples $(x_i, y_i)$ from $D$\;
%       % Construct 5-shot dataset  where $p$ is the prompt text\;
%       $\bar{h}_{l j} \leftarrow \frac{1}{\left|D_P\right|} \sum_{(p, x) \in D_P} h_{lj}\left([p, x]\right)$ \tcp*{task-conditioned activation}
%      $\theta \leftarrow \sum_{(l,j) \in \mathcal{S}} \bar{h}_{lj}$\;
%      $M_i \leftarrow$ \FContinual{$D_i, M_{i-1}, \theta_{T_i}$}\;
%     }
%     \KwRet{$M_N$}\;
% }

% \SetKwProg{Fn}{Function}{:}{\KwRet}
% \Fn{\FFuncVec{$D$, $M$}}{
%   $D_P \leftarrow \{(p_i, x_i, y_i) \mid 1<i \leq 100\}$ \tcp*{10-shot dataset}
%   % Sample 100 samples $(x_i, y_i)$ from $D$\;
%   % Construct 5-shot dataset  where $p$ is the prompt text\;
%   $\bar{h}_{l j} \leftarrow \frac{1}{\left|D_P\right|} \sum_{(p, x) \in D_P} h_{l j}\left([p, x]\right)$ \tcp*{task-conditioned activation}
%   % Sample 20 validation samples $(x_i, y_i)$ from $D$\;
%   $D_{\hat{P}} \leftarrow \{(\hat{p}_i, x_i, y_i)\mid 100<i \leq 120\}$ \tcp*{10-shot label-shuffled dataset}
%   % Construct 5-shot label-shuffled dataset $D_{\hat{P}} \leftarrow \{(\hat{p}_i, x_i, y_i)\}$ where $\hat{p}$ is the label-shuffled prompt text\;
%   \For{$l \leftarrow 1$ \KwTo $LayerNum$}{
%       \For{$j \leftarrow 1$ \KwTo $HeadNum$}{
%         $s_{lj} \leftarrow 0$\;
%         \ForEach{$(\hat{p},x,y)$ in $D_{\hat{P}}$}{
%             $\operatorname{CE}_{lj}([\hat{p},x]) \leftarrow P_{M_{h_{lj}\rightarrow \bar{h}_{lj}}}(y \mid [\hat{p},x] ) -P_M(y \mid [\hat{p},x])$\;
%             $s_{lj} \leftarrow s_{lj} + \operatorname{CE}_{lj}([\hat{p},x])$\;
%         }
%       }
%   }
%   $\mathcal{S} \leftarrow \text{GetTop10Indices}(s)$\;
  
%   \KwRet{$\mathcal{S}$}\;
% }

% \SetKwProg{Fn}{Function}{:}{\KwRet}
% \Fn{\FContinual{$D, M, \theta$}}{
%   \ForEach{$B = {(x_i,y_i)}$ in $\text{GenerateBatches}(D)$}{
%     $\ell_{LM} \leftarrow \frac{1}{|B|} \sum_{(x, y) \in B} -\log P_M(y \mid x) $ \tcp*{Language modeling loss}
%     $\ell_{FV} \leftarrow \frac{1}{\left|B\right|} \sum_{(x) \in B} \sum_{(l,j)\in \mathcal{S}} d \left(\hat{h_{lj}}(x), h_{lj}(x)\right) $\tcp*{FV consistency loss}
%     $\ell_{KL} \leftarrow \frac{1}{\left|B\right|} \sum_{(x) \in B} KL[P_M(x) \Vert P_{M_{{h_{l}\rightarrow h_{l}+\theta}}}(x )] $\tcp*{FV-guided KL-divergence loss}
%     $M.\text{UpdateWeights}(\ell_{LM} + \ell_{FV} + \ell_{KL})$\;
%   }
%   \KwRet{$M$}\;
% }

% \caption{Function vector guided training procedure}
% \end{algorithm} 


\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
  
\KwIn{Given a sequence of tasks $\{T_1, T_2, .... T_N\}$ and their corresponding datasets $\{D_1, D_2, .... D_N\}$, a series of held-out dataset $\{\bar{D}_1, \bar{D}_2, .... \bar{D}_K\}$ to figure out the set of function vector heads, pre-trained language model $M_0$}
\KwOut{Language model $M_N$ trained after $N$ tasks}

\SetKwFunction{FMain}{Main}
\SetKwFunction{FFuncVec}{GetFunctionVectorSet}
\SetKwFunction{FContinual}{FVGuidedTraining}

\SetKwProg{Fn}{Function}{:}{\KwRet}
\Fn{\FMain{$\{D_1, D_2, .... D_N\}$, $\{\bar{D}_1, \bar{D}_2, .... \bar{D}_K\}$, $M_0$}}{
    $\mathcal{S} \leftarrow$ \FFuncVec{$\{\bar{D}_1, \bar{D}_2, .... \bar{D}_K\}$, $M_0$}\;
    \For{$t \leftarrow 1$ \KwTo $N$}{
      $\bar{h}_{l k} \leftarrow \frac{1}{200} \sum_{x_i \in D_t\mid_1^{200}} h_{l k}\left([p_i, x_i]\right)$ \tcp*{task-conditioned activation}
     $\theta_{T_t} \leftarrow \sum_{(l,k) \in \mathcal{S}} \bar{h}_{lk}$\;
     $M_i \leftarrow$ \FContinual{$D_t, M_{t-1}, \theta_{T_t}, \mathcal{S}$}\;
    }
    \KwRet{$M_N$}\;
}

\SetKwProg{Fn}{Function}{:}{\KwRet}
\Fn{\FFuncVec{$\{D_1, D_2, .... D_K\}$, $M$}}{

$s \leftarrow \text{Array}[:, :, :](0)$\;
\For{$t \leftarrow 1$ \KwTo $K$}{
  $\bar{h}_{lk} \leftarrow \frac{1}{100} \sum_{x_i \in D_t\mid_1^{100}} h_{lk}\left([p_i, x_i]\right)$ \tcp*{task-conditioned activation}
  % Sample 20 validation samples $(x_i, y_i)$ from $D$\;
  % $D_t^{\hat{P}} \leftarrow \{(\hat{p}_i, x_i, y_i)\mid 200<i \leq 220\}$ \tcp*{10-shot label-shuffled prompt}
  % Construct 5-shot label-shuffled dataset $D_{\hat{P}} \leftarrow \{(\hat{p}_i, x_i, y_i)\}$ where $\hat{p}$ is the label-shuffled prompt text\;
  \For{$l \leftarrow 1$ \KwTo $LayerNum$}{ 
      \For{$k\leftarrow 1$ \KwTo $HeadNum$}{
        \ForEach{$(x,y)$ in $D_t\mid_{100}^{120}$}{ 
            $\operatorname{CE}_{lk}([\hat{p},x]) \leftarrow P_{M^{h_{lk}\rightarrow \bar{h}_{lk}}}(y \mid [\hat{p},x] ) -P_M(y \mid [\hat{p},x])$ \;
            $s[t,l,k] \leftarrow s[t,l,k] + \operatorname{CE}_{lk}([\hat{p},x])$\tcp*{$\hat{p}$ label-shuffled prompt}
        }
      }
  }
}
  $s_{mean} \leftarrow \frac{1}{K} \sum_{i=1}^{K} s[i,:,:]$\tcp*{average across datasets}
  $\mathcal{S} \leftarrow \text{GetTop10Indices}(s_{mean})$\;
  
  \KwRet{$\mathcal{S}$}\;
}

\SetKwProg{Fn}{Function}{:}{\KwRet}
\Fn{\FContinual{$D, M, \theta_T$, $\mathcal{S}$}}{
  \ForEach{$B = {(x_i,y_i)}$ in $\text{GenerateBatches}(D)$}{
    $\ell_{LM} \leftarrow \frac{1}{|B|} \sum_{(x, y) \in B} -\log P_M(y \mid x) $ \tcp*{language modeling loss}
    $\ell_{FV} \leftarrow \frac{1}{\left|B\right|} \sum_{(x) \in B} \sum_{(l,k)\in \mathcal{S}} d \left(h^{M_{t-1}}_{lk}(x), h^M_{lk}(x)\right) $\tcp*{FV consistency loss}
    $\ell_{KL} \leftarrow \frac{1}{\left|B\right|} \sum_{(x) \in B} KL[P_M(\cdot \mid x) \Vert P_{M_{t-1}^{{h_{l}\rightarrow h_{l}+\theta_T}}}( \cdot \mid x )] $\tcp*{FV-guided KL-divergence loss}
    $M.\text{UpdateWeights}(\ell_{LM} + \ell_{FV} + \ell_{KL})$\;
  }
  \KwRet{$M$}\;
}

\caption{Function vector guided training procedure}
\end{algorithm} 




\section{Illustration of causal pathway to forgetting.}

To help the understanding of "the causal pathway to forgetting through function vector", we provide the illustrations in Figure~\ref{fig:app:casualillu} and the detailed discussions in Section~\ref{sec5}. Refer to the caption of Figure~\ref{fig:app:casualillu} for more information.


\begin{figure*}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{pdf_figs/CasualPath.png}
  \caption{Illustration of causal pathway to forgetting. In (a), the pre-trained model is expressed in a latent variable assumption. It assumes task $T_0$ establishes a predictive pathway (shown in orange) that aligns well with the task (high probability with $\theta_{T_0}^0$). In (b), it shows the model after learning a new task $T_1$ without regularization, which will necessarily update the function attention heads, i.e., $P_M(\theta|x)$, (shown in red blocks), producing new function vectors $\theta^1_{T_0}$ and $\theta^1_{T_1}$ that are biased toward $T_1$. These shifts in function vectors lead to a derailed predictive pathway (shown in purple) with erroneous predictions for task $T_0$; in other words, forgetting of $T_0$ occurs. In summary, the modifications in $P_M(\theta|x)$ rather than $P_M(y|x,\theta)$ are the primary driving force behind forgetting. }
  \label{fig:app:casualillu}
\end{figure*}


\section{Datasets}
\label{app:dataset}

Three continual instruction tuning benchmarks and severel general evaluation datasets are adopts in this paper. The detailed information is as follows:

\paragraph{TRACE benchmark.}

TRACE benchmark is released by~\citet{wang2023trace} for the study of forgetting in LLMs, which consists of 8 different complex generation tasks including multi-choice QA, code generation, mathematical reasoning and summary. Without loss of generaliztion, we select 6 out of 8 raw tasks to construct the training sequence as our experiments setup. The statistical information is listed in Table~\ref{table:trace}.

The training epoch for this benchmark is 5 for C-STANCE, Py150, NumGLUE-cm, 3 for FOMC and ScienceQA, and 7 for MeetingBank. We evaluate them with a self-construct evaluation code based on OpenCompass code framework.

\begin{table*}[]
\begin{center}
\begin{scriptsize}
\begin{tabular}{l|llllll}
\toprule Dataset & Source & Category & Avg len & Metric & Language & \#data \\ \midrule \midrule
ScienceQA & Science & Multi-Choice QA & 210 & ROUGE-L  & English & 3,000 \\  
FOMC & Finance & Multi-Choice QA & 51 & ROUGE-L & English & 3,000 \\ 
MeetingBank & Meeting& Summary & 2853 & ROUGE-L & English & 3,000 \\ 
C-STANCE & Social media& Multi-Choice QA & 127 & ROUGE-L & Chinese & 3,000 \\ 
Py150 & Github& Code generation & 422 & ROUGE-L  & Python & 3,000 \\ 
NumGLUE-cm & Math & Math reasoning & 32 & ROUGE-L  & English & 3,000 \\
\bottomrule
\end{tabular}
\caption{A summary of dataset statistics in TRACE includes information on the source of the context, average length in terms of word count for English, German, and code datasets, and character count for Chinese.}
\label{table:trace}
\end{scriptsize}
\end{center}
\end{table*}
% \vspace{-0.8em}




\paragraph{SuperNI benchmark.}
SuperNI benchmark is widely utilized in existing instruction-following works. We select 26 tasks from the original dataset and set the training size to 1000 and training epoch set to 10. The statistical information is listed in Table~\ref{table:superni}.

\begin{table*}[]
\vspace{-0.8em}
\begin{center}
\begin{scriptsize}
\begin{tabular}{l|llllll}
\toprule Dataset & Source & Category & Avg len & Metric & Language & \#data \\ \midrule \midrule
NI002 & Quoref & Question Answering & 360 & ROUGE-L & English & 1000 \\
NI1290 & Xsum & Summarization & 363 & ROUGE-L & English & 1000 \\
NI1292 & Yelp review full & Sentiment Analysis & 130 & ROUGE-L & English & 1000 \\
NI141 & Odd man out & Word Semantics & 9 & ROUGE-L & English & 1000 \\
NI273 & Europarl & Text Matching & 15 & ROUGE-L & English & 1000 \\
NI024 & Cosmosqa & Question Answering & 82 & ROUGE-L & English & 1000 \\
NI1310 & Multilingual amazon reviews & Sentiment Analysis & 59 & ROUGE-L & English & 1000 \\
NI163 & Synthetic & Program Execution & 23 & ROUGE-L & English & 1000 \\
NI292 & Storycommonsense & Information Extraction & 48 & ROUGE-L & English & 1000 \\
NI1343 & Amazon us reviews & Sentiment Analysis & 70 & ROUGE-L & English & 1000 \\
NI195 & Sentiment140 & Sentiment Analysis & 14 & ROUGE-L & English & 1000 \\
NI1355 & Sentence compression & Summarization & 25 & ROUGE-L & English & 999\\
NI589 & Amazon fine food reviews & Summarization & 84 & ROUGE-L & English & 1000 \\
NI1357 & Xlsum & Summarization & 454 & ROUGE-L & English & 1000 \\
NI360 & Numersense & Fill in The Blank & 26 & ROUGE-L & English & 1000 \\
NI339 & Record & Question Answering & 185 & ROUGE-L & English & 1000 \\
NI220 & Rocstories & Title Generation & 60 & ROUGE-L & English & 1000 \\
NI224 & Scruples & Ethics Classification & 338 & ROUGE-L & English & 1000 \\
NI611 & Mutual & Dialogue Generation & 162 & ROUGE-L & English & 1000 \\
NI1510 & Evalution & Information Extraction & 7 & ROUGE-L & English & 1000 \\
NI231 & Iirc & Question Answering & 229 & ROUGE-L & English & 1000 \\
NI488 & Synthetic & Program Execution & 16 & ROUGE-L & English & 1000 \\
NI618 & Multilingual amazon reviews & Summarization & 47 & ROUGE-L & English & 1000 \\
NI363 & Sst2 & Sentiment Analysis & 19 & ROUGE-L & English & 1000 \\
NI619 & Ohsumed & Title Generation & 161 & ROUGE-L & English & 1000 \\
NI511 & Reddit tifu dataset & Summarization & 400 & ROUGE-L & English & 1000 \\

\bottomrule
\end{tabular}
\caption{A summary of dataset statistics in SuperNI.}
\label{table:superni}
\end{scriptsize}
\end{center}
\end{table*}

% obqa_train	nq	lambada	alpaca	ob_count

\paragraph{General evaluation sets.} For the general evaluation datasets, we utilize Hellaswag~\citep{zellers2019hellaswag}, CommonsenseQA~\citep{talmor2018commonsenseqa}, OpenbookQA~\citep{mihaylov2018can}, Natural Question~\cite{kwiatkowski2019natural}, Lambada~\cite{paperno2016lambada}, Alpaca~\cite{taori2023alpaca} and Bbh-Object Count~\cite{srivastava2022beyond}. All the datasets is downloaded from \url{https://github.com/open-compass/opencompass} and truncate to 190 samples for efficiency.

\paragraph{Input template}
In this paper, the specific instruction template used for each dataset is given below, as show in Table~\ref{table:template_inst}.



\begin{table*}[]
\begin{center}
\begin{tiny}
\begin{tabular}{l|lllll}
\toprule
 & Sequence & Task type & Num. per task  \\ \midrule
NI-Seq-C1 & NI195 $\rightarrow$ NI1343 $\rightarrow$ NI1310 $\rightarrow$ NI1292 $\rightarrow$ NI363 & Classification & 1,000    \\
NI-Seq-C2 & NI231 $\rightarrow$ NI1343 $\rightarrow$ NI220 $\rightarrow$ NI224 $\rightarrow$ NI273 & Classification & 1,000    \\
% NI-Seq-C2 &  &  &  &  &  \\
NI-Seq-G1 & NI618 $\rightarrow$ NI1290 $\rightarrow$ NI589 $\rightarrow$ NI511 $\rightarrow$ NI1357 & Generation & 1,000    \\
NI-Seq-G2 & NI1355 $\rightarrow$ NI141 $\rightarrow$ NI619 $\rightarrow$ NI163 $\rightarrow$ NI002 & Generation & 1,000    \\
% NI-Seq-G2 &  &  &  &  &  \\
NI-Seq-M1 & NI360 $\rightarrow$ NI363 $\rightarrow$ NI1290 $\rightarrow$ NI339 $\rightarrow$ NI1510 & Classification \& Generation  & 1,000   \\
NI-Seq-M2 & NI195 $\rightarrow$ NI611 $\rightarrow$ NI292 $\rightarrow$ NI488 $\rightarrow$ NI024 & Classification \& Generation  & 1,000    \\
% NI-Seq-M2 &  &  &  &  &  \\
TRACE &  Cstance $\rightarrow$ Fomc $\rightarrow$ Meet $\rightarrow$ Py150 $\rightarrow$ SciQA $\rightarrow$ Numgluecm & Classification \& Generation & 3,000 \\ \bottomrule
\end{tabular}
\caption{Basic information of continual learning task sequences used in main text.}
\label{tab:sec3:data}
\vspace{-0.7em}
\end{tiny}
\end{center}

\end{table*}


\begin{table*}[]
\vspace{-0.8em}
\begin{center}
\begin{scriptsize}
\begin{tabular}{c|l}
\toprule 
Task & \multicolumn{1}{c}{ Prompts } \\ \midrule \midrule
ScienceQA & \begin{tabular}{l} "Input": "Choose an answer for the following question and \\ give your reasons. Question: [$x$] Answer:", "Output": "[$y$]" \end{tabular} \\ \midrule
FOMC & \begin{tabular}{l} "Input": "What is the monetary policy stance for the following text? A. dovish, B. hawkish, \\ C. neutral. Choose one from A, B and C. Text: [$x$] Stance:", "Output": "[$y$]" \end{tabular} \\ \midrule
C-STANCE & \begin{tabular}{l} (Translate Chinese to English) "Input": ”Determine the attitude of \\ the following text towards the specified object. Select one: A. Support, \\ B. Oppose, C. Neutral. Output A, B or C. Text: [$x_1$] Object: [$x_2$] Attitude:”, "Output": “[$y$]"\end{tabular} \\ \midrule
MeetingBank & \begin{tabular}{l} "Input": "Write a summary of the following meeting transcripts. \\ Meeting transcripts: [$x$] Summary:", "Output": “[$y$]”\end{tabular} \\ \midrule
Py150 & \begin{tabular}{l} "Input":  “<s> [x]”, "Output": “[$y$]”\end{tabular} \\ \midrule
NumGLUE-cm & \begin{tabular}{l} "Input": "Solve the following math problem. Question: [$x$] Answer:”, "Output": “[$y$]”\end{tabular} \\ \midrule\midrule

NI-xxx & \begin{tabular}{l} 
        "Input": "Definition: In this task, you're given [$Description$].\\ Now complete the following examples \\ Input: [$x$] \\Output:",
    "Output": "[$y$]"\end{tabular}  \\ 
\bottomrule
\end{tabular}
\caption{Input template for calculating instruction probability and training for different tasks.}
\label{table:template_inst}
    
\end{scriptsize}
\end{center}
\end{table*}



\section{Implementation}
\label{app:implement}
We adopt Llama2-7b-chat, Llama2-13B-chat~\citep{touvron2023llama}, Llama3-8B-chat~\citep{dubey2024llama}, Mistral-7B-instruct-v2.0~\citep{jiang2023mistral} as the base models, with their effectiveness in both understanding world knowledge and following instructions. Without specific notification, the model is fine-tuned with LORA approach~\cite{hu2021lora}, where the rank dimension set to 8 and the target module is query and value weight matrices. For IncLora, OLora, and InsCL methods, a new adapter is initialized at the beginning of learning new task while keep the previous Lora adapters fixed. For Ewc, only one big adapter is initialized during the sequential learning, where rank is set to 48 for TRACE, and 40 for NI benchmarks.

The maximum input sequence length is set to 512 and the maximum output sequence length is set to 128. We train the model with the decoder only task calculating gradient only on the output tokens. We use an Adam optimizer with a weight decay of 1e-4 and the learning rate set to 1e-4 for TRACE and FUNC, 1e-3 for LONG (following ~\cite{wang2023trace}). The batch size is set to 8 and accumulate gradient step is set to 2 for each GPU while we run on 4 A100 GPUs with Deepspeed. The training size and epochs can be found in the introduction of datasets. 

\textbf{Implementation Detail of Optimization Objective}



In order to enhance the reproducibility of the paper, we provide the detailed calculation formula for the loss function $\ell_{FV}$ and
$\ell_{KL}$ when training task $T_j$: 

\begin{equation}
\ell_{FV} = \sum_{(l,k) \in \mathcal{S}} \| h_{lk}^{M_{j-1}}(x)-h^M_{lk}(x)\|_2^2
\end{equation}

\begin{equation}
\ell_{KL} = \sum_{i=1}^V P_M(\mathcal{Y}_i \mid x)[\log P_M(\mathcal{Y}_i \mid x) - \log P_{M_{j-1}^{{h_{l}\rightarrow h_{l}+\theta_{T_j}}}}(\mathcal{Y}_i \mid x)]
\end{equation}


Here, $P_M(\mathcal{Y}_i \mid x)$ denotes the output probability of token $\mathcal{Y}_i$ and $V = |\mathcal{Y}|$ is the vocabulary size.
As for the hyper-parameters $\alpha_1$ and  $\alpha_2$, we perform a grid search on  [2, 1, 0.5, 0.25, 0.08, 0.02] and set  $\alpha_1 = 1$ and $\alpha_2 = 0.08 $ as the final choice. 

For the hyperparameters of existing continual learning methods, we refer to the well-searched value reported in previous paper. Specifically, for Ewc the scaling factor on regularization term is set to 4,000, for O-lora the number is 0.5. The memory size of InsCL is set to 30 for NI benchmark and 50 for TRACE.


\textbf{Implementation Detail of Function Vector Framework}
When extracting the function vector from in-context samples, we use 10-shot input prompt randomly selected from held-out training dataset. The task-conditioned activations are average on samples filtered with correct 10-shot answer from the validation set with 200 samples. As for the set $\mathcal{S}$ of the casual attention heads, we follow the position in ~\citet{todd2023function} for Llama2-7b-chat and Llama2-13b-chat, and validate its efficiency on our own datasets. Specifically, for Llama2-7b-chat, the set $\mathcal{S}$ is $[(14, 1), (11, 2), (9, 25), (12, 15), (12, 28), (13, 7),(11, 18), (12, 18), (16, 10), (14, 16)]$. For Llama2-13b-chat the set $\mathcal{S}$ is $[(13, 13), (12, 17), (15, 38), (14, 34), (19, 2), (19, 36), (13, 4),\\ $$(18, 11), (10, 15), (13, 23)]$.
As for Llama3-8b-chat and Mistral-7b-instruct, we run the casual analysis experiments on 15 datasets and calculate the average CE to get the final casual attention heads set.
For Llama3-8b-chat, the set $\mathcal{S}$ is $
[(27, 28), (13, 27), (15, 28), (17, 8), (21, 2), (10, 12), (15, 16), \\$$(15, 2), (15, 1), (31, 24)]$. For Mistral-7b-instruct, the set $\mathcal{S}$ is $(14, 31), (26, 29), (12, 4), (12, 7), \\$$ (30, 4), (30, 9), (22, 30), (14, 19), (11, 10), (18, 1)]$.


\begin{table*}[]
% \vspace{-1.8em}
\begin{center}
% \begin{scriptsize}
\begin{tiny}
\begin{tabular}{l|llllr|llllr|lll}
\toprule
 & \multicolumn{5}{c|}{Zero-Shot Performance in General Task} & \multicolumn{5}{c|}{In-Context Performance in General Task} & \multicolumn{3}{c}{Performance in Trained Task} \\ \midrule
 & Hella. & Com. & Alpa. & Ob. & Avg./\textbf{Del.} & Hella. & Com. & Alpa. & Ob. & Avg./\textbf{Del.} & \textbf{AP} & \textbf{FP} & \textbf{Forget} \\ \midrule
  \multicolumn{14}{c}{\textbf{Llama2-7b-chat}} \\ \midrule
$M_0$ & 57.89 & 57.37 & 26.50 & 27.12 & 42.22 & 58.95 & 57.89 & 35.17 & 34.21 & 46.56 & / & / & / \\
NI-Seq-C1 & 47.37 & 40.00 & 32.00 & 31.61 & 37.75 & 24.21 & 27.89 & 28.89 & 26.84 & 26.96 & 86.10 & 83.80 & 2.30 \\
NI-Seq-C2 & 65.26 & 55.79 & 30.99 & 23.47 & 43.88 & 67.37 & 54.21 & 32.66 & 27.89 & 45.53 & 91.80 & 88.10 & 3.70 \\
NI-Seq-G1 & 48.95 & 39.47 & 27.36 & 39.72 & 38.88 & 37.89 & 42.63 & 28.84 & 38.95 & 37.08 & 24.97 & 19.36 & 5.61 \\
NI-Seq-G2 & 48.42 & 32.11 & 26.30 & 31.05 & 34.47 & 17.89 & 27.89 & 15.37 & 38.95 & 25.03 & 49.42 & 43.37 & 6.06 \\
NI-Seq-M1 & 52.11 & 42.63 & 31.09 & 29.51 & 38.84 & 45.79 & 31.05 & 24.58 & 33.16 & 33.65 & 59.02 & 54.33 & 4.70 \\
NI-Seq-M2 & 65.26 & 43.16 & 30.08 & 37.09 & 43.90 & 44.21 & 27.89 & 27.64 & 37.37 & 34.28 & 73.19 & 55.79 & 17.40 \\ \midrule
\multicolumn{14}{c}{\textbf{Llama3-8b-chat}} \\ \midrule

$M_0$ & 81.58 & 58.42 & 22.64 & 40.04 & 50.67 & 85.26 & 63.16 & 27.42 & 49.47 & 56.33 & / & / & / \\
NI-Seq-C1 & 79.47 & 46.84 & 23.27 & 32.32 & 45.48 & 79.47 & 40.00 & 25.62 & 45.79 & 47.72 & 83.40 & 82.10 & 1.30 \\
NI-Seq-C2 & 80.53 & 55.79 & 23.62 & 42.19 & 50.54 & 84.21 & 50.00 & 24.70 & 44.21 & 50.78 & 91.00 & 89.90 & 1.10 \\
NI-Seq-G1 & 72.63 & 35.79 & 22.05 & 29.39 & 39.97 & 67.89 & 31.05 & 19.77 & 41.58 & 40.07 & 28.29 & 21.10 & 7.20 \\
NI-Seq-G2 & 63.68 & 41.58 & 20.9 & 15.37 & 35.38 & 66.84 & 44.74 & 15.35 & 23.58 & 37.63 & 57.77 & 55.66 & 2.11 \\
NI-Seq-M1 & 78.42 & 40.00 & 21.93 & 21.58 & 40.48 & 76.84 & 40.00 & 21.32 & 35.91 & 43.52 & 60.74 & 52.63 & 8.11 \\
NI-Seq-M2 & 80.53 & 58.42 & 20.95 & 42.28 & 50.55 & 74.21 & 51.05 & 23.93 & 19.79 & 42.26 & 74.49 & 52.34 & 22.16 \\ \midrule
 \multicolumn{14}{c}{\textbf{Mistral-7b-instruct}} \\ \midrule
				
$M_0$ & 73.68 & 60.00 & 24.74 & 5.02 & 40.86 & 79.47 & 66.32 & 32.36 & 37.89 & 54.01 & / & / & / \\
NI-Seq-C1 & 63.16 & 50.00 & 32.04 & 15.30 & 40.13 & 66.84 & 51.05 & 36.80 & 37.89 & 48.15 & 84.70 & 85.40 & -0.70 \\
NI-Seq-C2 & 75.79 & 60.00 & 32.07 & 36.63 & 51.12 & 73.68 & 58.95 & 35.76 & 37.37 & 51.44 & 91.50 & 90.30 & 1.20 \\
NI-Seq-G1 & 57.37 & 45.26 & 26.30 & 13.81 & 35.69 & 57.89 & 35.79 & 32.04 & 39.47 & 41.30 & 27.63 & 19.78 & 7.85 \\
NI-Seq-G2 & 33.68 & 42.63 & 29.68 & 52.11 & 39.53 & 41.58 & 36.32 & 20.46 & 30.53 & 32.22 & 51.05 & 43.86 & 7.19 \\
NI-Seq-M1 & 65.26 & 47.89 & 33.02 & 12.35 & 39.63 & 63.68 & 38.42 & 34.79 & 45.79 & 45.67 & 61.96 & 57.01 & 4.96 \\
NI-Seq-M2 & 57.37 & 48.42 & 31.67 & 35.58 & 43.26 & 67.37 & 47.89 & 34.72 & 46.53 & 49.13 & 72.22 & 65.95 & 6.27 \\ \midrule 
 \multicolumn{14}{c}{\textbf{Llama2-13b-chat}} \\ \midrule
$M_0$ & 69.47 & 51.05 & 28.99 & 15.09 & 41.15 & 75.26 & 57.89 & 35.46 & 43.16 & 52.94 & / & / & / \\
NI-Seq-C1  & 65.79 & 52.63 & 34.18 & 21.51 & 43.53 & 66.32 & 48.42 & 38.48 & 38.95 & 48.04 & 83.20 & 82.27 & 0.93 \\
NI-Seq-G1  & 63.16 & 38.95 & 28.12 & 13.84 & 36.02 & 65.79 & 32.11 & 30.92 & 34.21 & 40.76 & 25.64 & 18.17 & 7.47 \\
NI-Seq-M1  & 71.58 & 49.47 & 34.10 & 28.09 & 45.81 & 70.53 & 48.42 & 36.51 & 37.37 & 48.21 & 60.10 & 56.34 & 3.76\\ \bottomrule
% \midrule
%   \multicolumn{14}{c}{\textbf{Llama2-chat-70b}} \\ \midrule
% Initial &  \\
% Seq.C &  \\
% Seq.G & \\
% Seq.M &   \\ 

\end{tabular}
% \vspace{-0.6em}
\caption{Final performance on 3 SuperNI benchmarks on 4 language models. Hella., Com., Alpa., and Ob. denote evaluation score on Hellswag, CommonsenseQA, Alpaca, Object Count datasets, respectively. The \textbf{Del.} value in red bold style is compared to performance of their initial model $M_0$. Higher \textbf{Forget} or lower \textbf{Del.} represent more forgetting. \textit{\textbf{Main conclusion: }Forgetting consistently occurs in both general and newly learned tasks, showing considerable variations depending on the types of tasks, stages of training, and the specific language models involved.}}
\vspace{-0.7em}
\label{table:app:more}
\end{tiny}
% \end{scriptsize}
\end{center}
\end{table*}


% \section{Performance Curve of Continual Learning on FUNC}

\section{Extended Experimental Results}


\paragraph{Results on more sequences.}
\label{app:more_seq} 
In addition to the results of the three sequences presented in Sec.~\ref{sec3}, we conducted the similar experiments on more sequences and found forgetting patterns similar to the experimental results described in the main text. The detailed experimental results on totally 6 sequences are shown in Table~\ref{table:app:more}.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.89\linewidth]{pdf_figs/corr_plot.pdf}
  \vspace{-1.1em}
  \caption{ The correlation plot on model performance and different similarity metrics. The y-axis shows Rouge-L metric on test data, while the x-axis represents the degree of similarity between the current model state and its initial condition. The calculation of each similarity metrics is (1) FV similarity: \(\operatorname{Cosine}(\theta_{T^e}^0, \theta_{T^e}^j)\). (2) Last hidden state similarity: \(\operatorname{Cosine}(\sum h^0_{-1,-1}(x), \sum h^j_{-1,-1}(x))\). (3) Parameter L2 distance: \(\|W^j - W^0\|^2\). The dotted line in each figure denotes the performance for original model. \textit{\textbf{Main conclusion:} There is a significant correlation between performance and FV similarity (sub-figures in the first column), while the other two metrics—last hidden state similarity and L2 distance—do not show such strong correlation.}}
  \label{fig:app:corr}
  \vspace{-0.1em}
\end{figure*}



\paragraph{Correlation plot on model performance and different similarity measurements.}

In this section, we provide scatter plots to illustrate the correlation between model performance and function vector (FV) similarity, alongside two other metrics: the similarity of the last layer hidden states and the L2 distance of parameters. The calculations for each similarity measure are as follows:

FV similarity is calculated using \(\operatorname{Cosine}(\theta_{T^e}^0, \theta_{T^e}^j)\), where \(\theta_{T^e}^j\) represents the FV of the evaluation task \(T^e\) after fine-tuning the \(j\)-th task.
Last layer hidden states similarity is derived from \(\operatorname{Cosine}\left(\sum_{x \in E} h^0_{-1,-1}(x), \sum_{x \in E} h^j_{-1,-1}(x)\right)\), where \(E\) is the test dataset and \(h^j_{-1,-1}\) denotes the model's output representation at the last token position in the last layer after fine-tuning the \(j\)-th task.
Parameter L2 distance is defined as \(\|W^j - W^0\|^2\), with \(W^j\) being the model weight post fine-tuning the \(j\)-th task. Here the reported model performance is te 5-shot results on the evaluation dataset.

For each evaluation task, we collected 40 data points from various models across different task sequences and stages and created correlation diagrams. The results, detailed in Figure~\ref{fig:app:corr}, demonstrate a notable correlation between model performance and FV similarity. This is particularly significant for tasks like Hellaswag, CommonsenseQA, and Alpaca, where a decrease in similarity corresponds to an increase in model forgetting. However, for scenarios where no forgetting occurs, such as in Object Count, there is no apparent correlation between FV similarity and performance. These observations inspire further investigations into the mechanisms of task transfer in LLMs.

Contrarily, the other two metrics—last hidden state similarity and L2 distance—do not show such strong correlation, indicating their limited effectiveness in reflecting model forgetting.




\paragraph{Relationship between forgetting and hidden states similarity.}
\label{app:hidden_sim}
In Figure~\ref{fig:app:fvsim}, we present the similarity between the FVs of training and evaluation tasks, alongside the corresponding forgetting after training. 
To further verify that simple feature similarity is insufficient to represent the forgetting phenomenon, we also include a heatmap of last layer hidden states similarity between training and testing tasks. This representational similarity was obtained through $Cosine(\sum_{x \in T}h_{-1}^{-1}(x), \sum_{x \in E}h_{-1}^{-1}(x))$, where \(T, E\) represent the training and testing tasks, respectively, and \(h_{-1}^{-1}\) represents the model's output representation at the last input token position of the last layer. By comparing the first and third rows in Figure ~\ref{fig:app:fvsim}, we were unable to identify any significant correlation between them, indicating that relying solely on simple model representations to study the forgetting phenomenon is not advisable.


% \begin{figure*}[!t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{pdf_figs/sec4_2_fv.pdf}
%   \vspace{-1.1em}
%   \caption{Heatmaps of performance shift (Top) and function vector similarity (Bottom) between training and test tasks before tuning. For performance shift, the value at position ($j', j$) represents the percentage change of task \(j\) at moment \(M_{j'}\) relative to the baseline metrics. For FV similarity, the value at position ($j', j$) corresponds to \(\operatorname{Cosine}(\theta_{T^e}^{j'}, \theta_{T_j}^{j'})\). {\color{blue}\textit{\textbf{Main conclusion:} Lower FV similarity (bluer value in the second row table) between two tasks correlates with increased forgetting (bluer value in first row table) in the evaluation task post-training.}}} 
%   \label{fig:sec4:fvsim}
%   \vspace{-1.7em}
% \end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{pdf_figs/app_fvsim.pdf}
  \vspace{-1.1em}
  \caption{Heatmaps of performance shift (Top) and function vector similarity (Bottom) between training and test tasks before tuning. For performance shift, the value at position ($j', j$) represents the percentage change of task \(j\) at moment \(M_{j'}\) relative to the baseline metrics. For FV similarity, the value at position ($j', j$) corresponds to \(\operatorname{Cosine}(\theta_{T^e}^{j'}, \theta_{T_j}^{j'})\). \textit{\textbf{Main conclusion:} Lower FV similarity (bluer value in the second row table) between tasks correlates with increased forgetting (bluer value in the first row table) after training; however, similarity in hidden states does not demonstrate this correlation.}} 
  \label{fig:app:fvsim}
  \vspace{-0.1em}
\end{figure*}


\begin{table*}[]
\begin{center}
% \begin{scriptsize}
\begin{tiny}
\begin{tabular}{cl|lll|lll|lll}
\toprule
&\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{NI-Seq-G1} & \multicolumn{3}{c|}{NI-Seq-C1} & \multicolumn{3}{c}{NI-Seq-M1}\\ 
& & \textbf{GP} $\uparrow$ & \textbf{IP} $\uparrow$ & \textbf{FP}  $\uparrow$& \textbf{GP } $\uparrow$ & \textbf{IP} $\uparrow$ & \textbf{FP} $\uparrow$ & \textbf{GP}  $\uparrow$& \textbf{IP} $\uparrow$ & \textbf{FP} $\uparrow$\\ \midrule \midrule

\multicolumn{1}{r|}{\multirow{9}{*}{\rotatebox{90}{Llama2-7b-chat}}}  & $M_0$ & 49.85 & 54.43 &  & 49.85 & 54.43 &  & 49.85 & 54.43 &    \\ \cmidrule(l){2-11} 
\multicolumn{1}{c|}{} & LoraInc & 47.16 & 30.94 & 19.35 & 45.83 & 27.71 & 83.80 & 47.55 & 37.23 & 54.33   \\
\multicolumn{1}{c|}{} & \multicolumn{1}{r|}{\textbf{+MA}} & \multicolumn{1}{r}{+0.87} & \multicolumn{1}{r}{+10.35} & \multicolumn{1}{r|}{\textbf{+1.46}} & \multicolumn{1}{r}{+2.81} & \multicolumn{1}{r}{+16.55} & \multicolumn{1}{r|}{-0.17} & \multicolumn{1}{r}{\textbf{+3.90}} & \multicolumn{1}{r}{+9.95} & \multicolumn{1}{r}{+2.22}  \\ 
\multicolumn{1}{c|}{} &\multicolumn{1}{r|}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+3.10}} & \multicolumn{1}{r}{\textbf{+18.97}} & \multicolumn{1}{r|}{+0.84} & \multicolumn{1}{r}{\textbf{+3.98}} & \multicolumn{1}{r}{\textbf{+25.53}} & \multicolumn{1}{r|}{\textbf{+1.70}} & \multicolumn{1}{r}{+2.65} & \multicolumn{1}{r}{\textbf{+15.78}} & \multicolumn{1}{r}{\textbf{+3.52}}  \\ \cmidrule(l){2-11} 
\multicolumn{1}{c|}{} & Ewc & 33.48 & 26.87 & 17.72 & 46.08 & 38.76 & 85.00 & 44.47 & 41.69 & 55.85  \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{r|}{\textbf{+MA}} & \multicolumn{1}{r}{+6.58} & \multicolumn{1}{r}{+11.27} & \multicolumn{1}{r|}{\textbf{+2.59}} & \multicolumn{1}{r}{+1.57} & \multicolumn{1}{r}{+7.64} & \multicolumn{1}{r|}{\textbf{+0.40}} & \multicolumn{1}{r}{+5.54} & \multicolumn{1}{r}{+7.71} & \multicolumn{1}{r}{\textbf{+0.92}}  \\  
\multicolumn{1}{c|}{} & \multicolumn{1}{r|}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+15.73}} & \multicolumn{1}{r}{\textbf{+27.18}} & \multicolumn{1}{r|}{+0.85} & \multicolumn{1}{r}{\textbf{+3.11}} & \multicolumn{1}{r}{\textbf{+15.96}} & \multicolumn{1}{r|}{+0.37} & \multicolumn{1}{r}{\textbf{+6.18}} & \multicolumn{1}{r}{\textbf{+13.99}} & \multicolumn{1}{r}{+0.01}  \\ 
 \midrule
\midrule
\multicolumn{1}{c|}{\multirow{5}{*}{\rotatebox{90}{Llama3-8b-c.}}} & $M_0$ & 56.61 & 60.61 &  & 56.61 & 60.61 &  & 56.61 & 60.61 &   \\ \cmidrule(l){2-11} 
\multicolumn{1}{c|}{} & LoraInc & 45.51 & 39.85 & 21.10 & 51.89 & 54.63 & 82.10 & 48.00 & 47.82 & 52.63 \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{r|}{\textbf{+MA}} & \multicolumn{1}{r}{+4.39} & \multicolumn{1}{r}{+8.01} & \multicolumn{1}{r|}{+2.07} & \multicolumn{1}{r}{+1.99} & \multicolumn{1}{r}{+2.42} & \multicolumn{1}{r|}{\textbf{+2.00}} & \multicolumn{1}{r}{+3.67} & \multicolumn{1}{r}{\textbf{+5.82}} & \multicolumn{1}{r}{+4.70}  \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{r|}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+7.79}} & \multicolumn{1}{r}{\textbf{+15.31}} & \multicolumn{1}{r|}{\textbf{+3.10}} & \multicolumn{1}{r}{\textbf{+3.99}} & \multicolumn{1}{r}{\textbf{+5.19}} & \multicolumn{1}{r|}{+0.30} & \multicolumn{1}{r}{\textbf{+4.88}} & \multicolumn{1}{r}{+4.75} & \multicolumn{1}{r}{\textbf{+5.78}}\\
\bottomrule
\end{tabular}
\caption{Performance of baselines and their improved version with Function Vector Guided (\textbf{FVG}) training or Model Averaging (\textbf{MA}). \textit{\textbf{Main conclusion:} MA performance better on fine-tuned datasets (\textbf{FP}) compared to FVG, but struggles in the general/in-context datasets setting (\textbf{GP/IP}).}}
\vspace{-2em}
\label{app:table:model_average}

% \end{scriptsize}
\end{tiny}
\end{center}
\end{table*}



\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.89\linewidth]{pdf_figs/MA_analy.pdf}
  \vspace{-1.1em}
  \caption{The shifts in function vector with 5-shot performance with function vector guided training (FVG) and model averaging (MA). \textit{\textbf{Main conclusion:} FVG and MA prevents the shift in FV (yellow and light blue bar) and thus mitigating forgetting (orange and light blue line).}}
  \label{fig:app:masim}
  \vspace{-0.1em}
\end{figure*}



\paragraph{Comparison between model averaging.}
To further demonstrate the advanced nature of our algorithm, we compared FVG with Model Averaging~\citep{lin2024mitigating}. Model averaging is a technique often used to improve the robustness and performance of models. It involves taking the average of multiple model parameters across different training runs or stages and has been proven for its effectiveness in mitigating forgetting. 

We evaluate Model Averaging on three benchmarks with combination of IncLora and EWC methods on Llama2-7b-chat and Llama3-8b-instruct models. Specifically, we perform Model Averaging on pre-trained model and final model with the averaging ratio set to 0.2. The results are shown in Table~\ref{app:table:model_average}. It shows a better performance on fine-tuned datasets (\textbf{FP}) compared to function vector guided training, but struggles in the general/in-context datasets setting (\textbf{GP/IP}).

\begin{wrapfigure}{hr}{0.45\textwidth}
  \centering
  \vspace{-1.8em}
  \includegraphics[width=1.\linewidth]{figs/interv.pdf}
  \vspace{-2em}
  \caption{Intervention results on four datasets via function vector. \textit{\textbf{Main conclusion:} Function is effective in regulating the final outputs.}}
  \label{fig:app:intervention}
  \vspace{-1.0em}
\end{wrapfigure}

 While Model Averaging contributes to avoiding forgetting, it is interesting to see the explaination in the perspective of function vector. We provide the shift in function vector before and after model averaging with corresponding performance in Fig.~\ref{fig:app:masim}. Cross-method comparative analysis shows that methods capable of maintaining the stability of FV changes tend to yield better results. Specifically, model averaging, when compared to its predecessor Inclora, mitigates shifts and enhances performance. Furthermore, an examination across various training stages indicates a positive correlation between performance and the extent of FV shifts including Model Averaging.
 

\paragraph{Effectiveness of Function Vector}
\label{app:fv}



To assess the effectiveness of the extracted $\theta_T$, referred to as the Function Vector (FV) in this study, we conduct a series of intervention experiments across multiple datasets (see Fig.~\ref{fig:app:intervention}) on the initial model Llama2-7b-chat. These experiments consisted of either inserting or removing an FV at the hidden states of a specific layer at the the last token position, to examine the influence on the model output. More precisely, in the transformer's forward residual stream, the instruction vector $\theta_T$ modifies the hidden states at a select layer $l$ as $h_l = h_l + \theta_T$.





We reported the intervention findings on four distinct datasets: 1) CommensenseQA (different from the evaluation set mentioned above in input instruction), multiple-choice questions on common sense reasoning; 2) Antonym, a task aimed at generating antonyms; 3) AGNews, a text classification task with the article's category as the label; and 4) Last-Spanish, a task that output the Spanish translation of the list's final item. 
The results highlighted that the FV directly affects the model's output behavior for specific tasks. In tasks such as Antonym, Last-Spanish, and CommonsenseQA, introducing FV significantly improved the zero-shot performance from a low level. Conversely, in the cases of AGNews and CommonsenseQA, removing the FV resulted in a deterioration of the model's ability to produce the correct output. In contrast, interventions with random vectors had a negligible effect on the model. 



\paragraph{Alternation of casual attention head during training.}  We carried out causality analysis experiments to identify the latest causal attention head \(\mathcal{S}\) in the model, which was fine-tuned on NI-Seq-G1. Specifically, these causality analysis experiments were performed across six datasets, and the average Cross-Entropy (CE) was calculated to determine the final set of causal attention heads. The findings are presented in Fig.~\ref{app:fig:casual_set}. We observed a gradual yet slight shift in the set \(\mathcal{S}\). This indicates that changes in the model's function vector occur not only in values but also in positions, though such changes are slow and do not significantly alter the importance of the original positions. Therefore, the function vectors mentioned in this paper are extracted from uniform positions.

\label{app:casual_shift}


\begin{figure*}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/changingfv.pdf}
  \vspace{-1.1em}
  \caption{Alternation in the casual attention head during Llama2-7b-chat training on NI-Seq-G1. The positions of top-10 heads are listed in each heatmap, while the newly introduced heads are marked as red. \textit{\textbf{Main results:} The position of function vector shifts at a quite slow speed during training}}
  \label{app:fig:casual_set}
  \vspace{-0.1em}
\end{figure*}
