
\section{Function Vector Guided Training Design}
\label{sec6}

In this section, we present the function vector guided training design that serves as an effective mechanism for mitigating the forgetting of general abilities applicable to a wide range of language models and continual learning baselines. We introduce the overall architecture, present experimental results, and analyze how function vector guided training works.

\textbf{Function vector guided training.}
The correlation between forgetting and the function vector implies a principle for training method design. That is, training should be capable of maintaining the correct mapping of inputs to function vectors and thus mitigate forgetting. Based on this principle, we propose function vector-guided training as a simple yet efficient design to mitigate forgetting. Our method introduces two novel regularization terms:

% This method, combined with existing continual learning algorithms, effectively reduces the forgetting of general abilities while preserving in-context reasoning capabilities, with minimal impact on plasticity.

First, to mitigate the introduction of a biased function vector during training, we restrict the alterations in FVs tied to the training tasks, effectively maintaining the model's \(P_M(\theta_T|x)\) unchanged. To this end, restrictions are imposed on the activation values of specific heads signifying the function vector with a FV consistency loss. When training task $T_j$, the loss is specified as follows: 

\begin{equation}
\ell_{FV} = \sum_{(l,k)\in \mathcal{S}} d \left(h^{M_{j-1}}_{lk}(x), h^M_{lk}(x)\right),
\end{equation}
where $h^{M}_{lk}(x)$ denotes the activations on last input token of head $j$ in layer $l$ on input $x$ from model $M$ and $M_{j-1}$ is the model before training task $T_j$. $d(\cdot,\cdot)$ is the distance metrics between variables, and we adopt L2 distance in this paper.

% Secondly, to prevent the model from learning input-output shortcuts due to insufficient activation of task functions within the model when dealing with zero-shot tasks, we further propose distilled learning based on function vectors. That is, by explicitly using the function vector to intervene in the model's function \(P_M(y|x,\theta_t)\), the model's inherent capability to handle tasks is distilled into processing zero-shot inputs. This approach also aims to fit the target tasks on this basis. We believe this method maintains the model's reasoning pathway \(P_M(y|x,\theta_t)P_M(\theta_t|x)\) and reduces the introduction of biased functions.

Furthermore, we introduce a FV-guided KL-divergence loss to align the task function raised by zero-shot inputs and the FV-intervened one. The detailed function for training task $T_j$ is as bellow:

\begin{equation}
\begin{aligned}
    \ell_{KL} = KL[&P_M(\cdot \mid x) \Vert 
    P_{M_{j-1}^{{h_{l}\rightarrow h_{l}+\theta_{T_j}^0}}}(\cdot \mid x)].
\end{aligned}
\end{equation}
$P_M(\cdot \mid x)$ is the predicted probability distribution for each token in the vocabulary $\mathcal{Y}$ given input $x$. $M_{j-1}^{{h_{l}\rightarrow h_{l}+\theta_{T_j}^0}}$ denotes the model train after $j-1$ tasks with intervention by the function vector $\theta_{T_j}^0$, that is  $\theta_{T_j}^0$ is added to the activation $h_{l}$ of layer $l$ during forward. $l$ is selected as 9 in this paper.
Then, the overall optimization objective is to minimize to loss $\ell = \ell_{LM} + \alpha_1 \ell_{FV} + \alpha_2 \ell_{KL}$, where $\ell_{LM} = -\log P_M(y|x)$ is the language modeling loss~\citep{brown2020language} and $\alpha_1, \alpha_2$ are trade-off hyper-parameters. The algorithm procedure and implementation detail are provided in ~\ref{app:algo} and ~\ref{app:implement}, respectively. 

This FV-guided fine-tuning leverages the function \(P_M(y|x,\theta_T)\) within the model to guide the fine-tuning, ensuring that the model retains a robust causal pathway \(P_M(y|x,\theta_T)P_M(\theta_t|x)\) after fine-tuning and minimizes the impact of newly introduced functions on past abilities.




\begin{table*}[]
\begin{center}
% \begin{scriptsize}
\begin{tiny}
\begin{tabular}{cl|lll|lll|lll|lll}
\toprule
&\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{NI-Seq-G1} & \multicolumn{3}{c|}{NI-Seq-C1} & \multicolumn{3}{c}{NI-Seq-M1} & \multicolumn{3}{c}{TRACE} \\ 
& & \textbf{GP} $\uparrow$ & \textbf{IP} $\uparrow$ & \textbf{FP}  $\uparrow$& \textbf{GP } $\uparrow$ & \textbf{IP} $\uparrow$ & \textbf{FP} $\uparrow$ & \textbf{GP}  $\uparrow$& \textbf{IP} $\uparrow$ & \textbf{FP} $\uparrow$ & \textbf{GP} $\uparrow$ & \textbf{IP} & \textbf{FP}  $\uparrow$\\ \midrule \midrule

\multicolumn{1}{r|}{\multirow{9}{*}{\rotatebox{90}{Llama2-7b-chat}}}  & $M_0$ & 49.85 & 54.43 &  & 49.85 & 54.43 &  & 49.85 & 54.43 &  & 49.85 & 54.43 &  \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & LoraInc & 47.16 & 30.94 & 19.35 & 45.83 & 27.71 & 83.80 & 47.55 & 37.23 & 54.33 & 46.17 & 38.08 & 41.20 \\
\multicolumn{1}{c|}{} &\multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+3.34}} & \multicolumn{1}{r}{\textbf{{+25.25}}} & \multicolumn{1}{r|}{\textbf{{+2.84}}} & \multicolumn{1}{r}{\textbf{+3.98}} & \multicolumn{1}{r}{\textbf{+25.53}} & \multicolumn{1}{r|}{\textbf{+1.70}} & \multicolumn{1}{r}{\textbf{+2.65}} & \multicolumn{1}{r}{\textbf{+15.78}} & \multicolumn{1}{r|}{\textbf{+3.52}} & \multicolumn{1}{r}{\textbf{+6.92}} & \multicolumn{1}{r}{\textbf{+16.17}} & \multicolumn{1}{r}{\textbf{+12.13}} \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & Ewc & 33.48 & 26.87 & 17.72 & 46.08 & 38.76 & 85.00 & 44.47 & 41.69 & 55.85 & 49.07 & 47.98 & 54.22 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+15.73}} & \multicolumn{1}{r}{\textbf{+27.18}} & \multicolumn{1}{r|}{\textbf{+0.85}} & \multicolumn{1}{r}{\textbf{+3.11}} & \multicolumn{1}{r}{\textbf{+15.96}} & \multicolumn{1}{r|}{\textbf{+0.37}} & \multicolumn{1}{r}{\textbf{+6.18}} & \multicolumn{1}{r}{\textbf{+13.99}} & \multicolumn{1}{r|}{\textbf{+0.01}} & \multicolumn{1}{r}{\textbf{+2.21}} & \multicolumn{1}{r}{\textbf{+6.01}} & \multicolumn{1}{r}{\textbf{-8.77}} \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & O-lora & 45.15 & 31.90 & 22.67 & 41.54 & 20.54 & 79.33 & 50.16 & 39.52 & 56.94 & 36.96 & 29.38 & 37.13 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+4.89}} & \multicolumn{1}{r}{\textbf{+23.59}} & \multicolumn{1}{r|}{\textbf{+0.11}} & \multicolumn{1}{r}{\textbf{+8.38}} & \multicolumn{1}{r}{\textbf{+33.93}} & \multicolumn{1}{r|}{\textbf{+6.2}} & \multicolumn{1}{r}{\textbf{+0.29}} & \multicolumn{1}{r}{\textbf{+14.95}} & \multicolumn{1}{r|}{\textbf{-0.42}} & \multicolumn{1}{r}{\textbf{+14.32}} & \multicolumn{1}{r}{\textbf{+24.61}} & \multicolumn{1}{r}{\textbf{+8.32}} \\ \cmidrule(l){2-14} 

\multicolumn{1}{c|}{} & InsCL & 45.80 & 41.79 & 27.14 & 44.03 & 35.69 & 81.67 & 49.76 & 43.09 & 60.83 & 46.46 & 41.63 & 52.95 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+2.65}} & \multicolumn{1}{r}{\textbf{+8.30}} & \multicolumn{1}{r|}{\textbf{+0.91}} & \multicolumn{1}{r}{\textbf{+5.00}} & \multicolumn{1}{r}{\textbf{+16.11}} & \multicolumn{1}{r|}{\textbf{+1.23}} & \multicolumn{1}{r}{\textbf{+0.98}} & \multicolumn{1}{r}{\textbf{+8.32}} & \multicolumn{1}{r|}{\textbf{-2.22}} & \multicolumn{1}{r}{\textbf{+6.70}} & \multicolumn{1}{r}{\textbf{+11.04}} & \multicolumn{1}{r}{\textbf{+0.92}} 
 \\ \midrule
\midrule
\multicolumn{1}{c|}{\multirow{5}{*}{\rotatebox{90}{Llama3-8b-c.}}} & $M_0$ & 56.61 & 60.61 &  & 56.61 & 60.61 &  & 56.61 & 60.61 &  & 56.61 & 60.61 &  \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & LoraInc & 45.51 & 39.85 & 21.10 & 51.89 & 54.63 & 82.10 & 48.00 & 47.82 & 52.63 & 50.31 & 52.61 & 27.14 \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+7.79}} & \multicolumn{1}{r}{\textbf{+15.31}} & \multicolumn{1}{r|}{\textbf{+3.10}} & \multicolumn{1}{r}{\textbf{+3.99}} & \multicolumn{1}{r}{\textbf{+5.19}} & \multicolumn{1}{r|}{\textbf{+0.30}} & \multicolumn{1}{r}{\textbf{+4.88}} & \multicolumn{1}{r}{\textbf{+4.75}} & \multicolumn{1}{r|}{\textbf{+5.78}} & \multicolumn{1}{r}{\textbf{+3.84}} & \multicolumn{1}{r}{\textbf{+6.29}} & \multicolumn{1}{r}{\textbf{+7.22}} \\
 \cmidrule(l){2-14} 
% \multicolumn{1}{c|}{} & Ewc & 37.71 & 36.74 & 18.24 & 48.83 & 47.54 & 83 & 50.54 & 49.51 & 58.08 & 52.24 & 55.49 & 44.97 \\
% \multicolumn{1}{c|}{} & +FVG & 14.42 & 18.39 & 1.43 & 7.15 & 12.78 & 1.7 & 5.74 & 10.28 & -0.28 &  &  &  \\ \cmidrule(l){2-14} 
% \multicolumn{1}{c|}{} & O-lora & 49.29 & 51.97 & 23.64 & 51.9 & 56.94 & 82.1 & 51.68 & 54.97 & 59.78 & 47.97 & 58.62 & 32.73 \\
% \multicolumn{1}{c|}{} & +FVG & 3.18 & 5.71 & 2.33 & 2.38 & 2.05 & 0.3 & 2.1 & 2.26 & 0.59 &  &  &  \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & InsCL & 46.48 & 49.46 & 28.53 & 52.11 & 57.30 & 82.50 & 49.46 & 53.50 & 60.92 & 51.87 & 51.22 & 37.32 \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+6.60}} & \multicolumn{1}{r}{\textbf{+8.06}} & \multicolumn{1}{r|}{\textbf{-0.85}} & \multicolumn{1}{r}{\textbf{+3.52}} & \multicolumn{1}{r}{\textbf{+1.58}} & \multicolumn{1}{r|}{\textbf{-0.60}} & \multicolumn{1}{r}{\textbf{+4.34}} & \multicolumn{1}{r}{\textbf{+2.75}} & \multicolumn{1}{r|}{\textbf{-2.80}} & \multicolumn{1}{r}{\textbf{+2.04}} & \multicolumn{1}{r}{\textbf{+7.92}} & \multicolumn{1}{r}{\textbf{+6.66}}   \\  \midrule
\midrule
\multicolumn{1}{c|}{\multirow{5}{*}{\rotatebox{90}{Mistral-7b-i.}}} & $M_0$ & 47.55	& 57.51 &  & 47.55	& 57.51 &  & 47.55	& 57.51 &  & 47.55	& 57.51 &  \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & LoraInc & 42.81 & 38.82 & 19.78 & 48.00 & 53.00 & 85.4 & 49.79 & 51.02 & 57.01 & 51.91 & 51.37 & 44.68  \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+4.49}} & \multicolumn{1}{r}{\textbf{+16.61}} & \multicolumn{1}{r}{\textbf{+0.64}} & \multicolumn{1}{r}{\textbf{+2.35}} & \multicolumn{1}{r}{\textbf{+2.67}} & \multicolumn{1}{r}{\textbf{-0.50}} & \multicolumn{1}{r}{\textbf{-2.41}} & \multicolumn{1}{r}{\textbf{+4.02}} & \multicolumn{1}{r}{\textbf{+0.43}} & \multicolumn{1}{r}{\textbf{-1.49}} & \multicolumn{1}{r}{\textbf{+5.14}} & \multicolumn{1}{r}{\textbf{+10.37}}  \\ \cmidrule(l){2-14} 
\multicolumn{1}{c|}{} & InsCL & 43.46 & 51.06 & 25.78 & 40.77 & 49.49 & 83.03 & 42.38 & 52.27 & 58.01 & 50.90 & 50.39 & 55.99  \\
\multicolumn{1}{c|}{} & \multicolumn{1}{r}{\textbf{+FVG}} & \multicolumn{1}{r}{\textbf{+2.71}} & \multicolumn{1}{r}{\textbf{+4.64}} & \multicolumn{1}{r}{\textbf{-0.30}} & \multicolumn{1}{r}{\textbf{+6.75}} & \multicolumn{1}{r}{\textbf{+4.27}} & \multicolumn{1}{r}{\textbf{+2.07}} & \multicolumn{1}{r}{\textbf{+6.13}} & \multicolumn{1}{r}{\textbf{+3.40}} & \multicolumn{1}{r}{\textbf{-0.84}} & \multicolumn{1}{r}{\textbf{-0.98}} & \multicolumn{1}{r}{\textbf{+6.12}} & \multicolumn{1}{r}{\textbf{+1.19}} \\ 
% \midrule
 % & \multicolumn{12}{c}{Mistral-7b-instruct} \\ \midrule
% Init & 47.55 & 57.51 &  & 47.55 & \textbf{57.51} &  & 47.55 & 57.51 &  & \textbf{47.55} & 57.51 &  \\
% LoraInc & 42.81 & 38.82 & 19.78 & 48 & \textbf{53} & 85.4 & 49.79 & 51.02 & 57.01 & \textbf{51.91} & 51.37 & 44.68 \\
% +FVG & 4.49 & 16.61 & 0.64 & 2.35 & 2.67 & -0.5 & -2.41 & 4.02 & 0.43 & \textbf{} &  &  \\
% Ewc & 36.86 & 36.5 & 19.61 & 48.79 & \textbf{48.61} & 82.87 & 38.74 & 52.79 & 60.75 & \textbf{39.02} & 51.18 & 57.79 \\
% +FVG &  &  &  &  & \textbf{} &  &  &  &  & \textbf{} &  &  \\
% O-lora & 38.6 & 36.39 & 21.4 & 47.45 & \textbf{53.73} & 81.7 & 47.57 & 44.08 & 58.67 & \textbf{50.49} & 50.93 & 38.39 \\
% +FVG &  &  &  & 0.13 & 3.13 & 1.7 &  &  &  & \textbf{} &  &  \\
% InsCL &  &  &  &  & \textbf{} &  &  &  &  & \textbf{} &  &  \\
% +FVG &  &  &  &  & \textbf{} &  &  &  &  & \textbf{} &  &  \\ 
\bottomrule
\end{tabular}
% \vspace{-1em}
\caption{Performance of baselines and their improved version with Function Vector Guided (\textbf{FVG}) training on four benchmarks. \textit{\textbf{Main conclusion:} \textbf{FVG} significantly prevent forgetting in general and in-context learning capabilities (\textbf{GP} and \textbf{IP}).}}
% Results reported are averaged over 4 random seeds.
\vspace{-1em}
\label{table:main}

% \end{scriptsize}
\end{tiny}
\end{center}
\end{table*}



