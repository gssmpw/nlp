
\section{Related work}


\paragraph{Catastrophic forgetting in fine-tuned language models.}
% Zhaoyi wrote
% level-1: LLMs
% catastrophic forgetting and existing mitigating methods
Fine-tuning foundational LLMs~\citep{touvron2023llama_1,touvron2023llama} has become a generic technique for enhancing their capacity of following instructions~\citep{wei2022finetuned,zhang2024llamaadapter,zhang2024instruction} and mastering domain-specific content~\citep{yue2023disclawllm,christophe2024med42}. 
However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting~\citep{Kirkpatrick_2017,luo2024empirical,kotha2024understanding,wu2024continual}.
In context of LLMs, existing approaches towards mitigating this issue can mostly be categorized into three types: regularizing the update of model parameters~\citep{huang-etal-2021-continual,cha2021cpr}, replaying previous or self-synthesized data~\citep{scialom-etal-2022-fine,huang2024mitigating} and resisting interference via parameter-efficient fine-tuning~\citep{razdaibiedina2023progressive,wang2023orthogonal}.
In this work, we aims to characterize CF in LLMs through the function vector, concluding that such forgetting primarily stems from biases in function activation
rather than the overwriting of task processing functions. To this end, We propose function vector guided training, a regularization-based method to protect task activation from being improperly destroyed during fine-tuning to cure the forgetting issue.
\paragraph{Mechanistic analysis to fine-tuning.}
% Zhaoyi wrote
% level-2: MI analysis for fine-tuning 
% level-3: the special point and the target of this paper
Existing works on analyzing the internal mechanism~\citep{r√§uker2023transparent,ferrando2024primer} of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal transformation on top of the original capability~\citep{jain2024mechanistically} (wrappers), subtractable and reusable parameter shift vectors~\citep{huang2024chat,gao2024ethos} (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage~\citep{ren2024learning}. 
Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point. We have successfully identified the compact task representation, known as the function vector, can tracks task forgetting in LLMs. Our empirical data indicate a strong correlation between shifts in the function vector and the phenomenon of task forgetting.
% Our empirical findings firstly suggest that in the continual fine-tuning setup the forgetting of the instruction-following ability~\citep{wei2022finetuned} dominates in comparison with the world knowledge and concept~\citep{yu2023kola}. Then we leverage a causal mediation analysis technique, dubbed function vectors~\citep{todd2023function}, to precisely identify attention heads in correspondence to the instruction-following ability. We interpret the catastrophic forgetting by tracing such head representations throughout the continual fine-tuning process.
