\section{Preliminaries}
\label{sec2}


\subsection{Catastrophic Forgetting}
Continual learning~\citep{serra2018overcoming, wu2024meta, wu2024continual} seeks to tackle the core challenge of incrementally learning from a sequence of real-world tasks over time,
%. This approach addresses one of the fundamental challenges in machine learning: 
% the ability of models 
specifically addressing how
to adapt to new tasks without forgetting previously learned knowledge -- a phenomenon widely known as catastrophic forgetting~\citep{mccloskey1989catastrophic, Kirkpatrick_2017}. 

In this paper, we focus on continual learning of a language model $M_0$, which has already been pre-trained on a vast data corpus $\mathcal{D}_{PT}$ using language modeling tasks~\citep{brown2020language, radford2019language} followed by preference optimization via human feedback~\citep{ouyang2022training}. 
% This universal learning stage equips $M_0$ with a broad and robust capability to comprehend and process a wide array of tasks.
Specifically, we assume 
% Then $M_0$ moves into a continual learning setting, where it come across 
a stream of tasks $T_1, T_2, \ldots, T_N$, where each $j$-th task $T_j$ consists of  a dataset $\mathcal{D}_j = \{x^i_j, y^i_j\},$ with $x^i_j$ and $y^i_j$ representing the inputs and outputs text sequences, respectively. 
On each task $T_j$, the model $M_{j-1}$ is optimized towards minimization of the loss
% $M_{k-1}$ updates its parameters to minimize the loss 
$\mathcal{L}_{T_j}(M_{j-1})$, coupled with a continual learning objective if applied, resulting in the updated model $M_j$. 
This continual learning process, applied into a language model, is commonly referred to as continual instruction tuning~\citep{peng2023instruction, chung2024scaling}.
% on this new task, balanced with a continual learning objective to mitigate catastrophic forgetting. Here, $M_k$ is the model post training $k$-th task.

% This catastrophic forgetting is usually measured by the performance drop on the sequential trained tasks, $\{T_1, T_2, \ldots, T_{k-1}\}$. In this paper, we also focus on losing proficiency in the general knowledge encompassed within  pre-training dataset, $\mathcal{D}_{PT}$. That is, we evaluate the model performance not only on the trained tasks but also on benchmark general tasks (e.g., CommonsenseQA~\citep{talmor2018commonsenseqa},  MMLU~\citep{hendrycks2020measuring}).


\subsection{Function Vector}
% [TO UPDATE]

Following the mechanistic interpretability work in LLMs ~\citep{todd2023function, hendel2023context}, we investigate the internal workings of a task on LLMs through function vector â€” compact vector representation of input-output task identified within the hidden states of transformers during in-context learning (ICL~\citep{brown2020language}). 
An activation patching~\citep{meng2022locating, meng2023massediting, wang2023interpretability} procedure is performed on the ICL hidden states to determine the casual set of attention heads that mediate tasks. These heads collaborate to convey a function vector (FV) which represents the LLM's function specific to input-output task. Function vector is regraded as an efficient way to characterize function execution within LLMs~\citep{todd2023function}.


Formally, for a given dataset $\mathcal{D}^{T}$ of task $T$, the function vector $\theta_T$ is derived through two steps:

First, the activation patching is performed to determine the attention heads set (denoted as $\mathcal{S}$)  with significant cause-and-effect relationship between ICL inputs and correct outputs. Specifically, the model will run on a counterfactual ICL input $[\hat{p},x]$ incorporating a label-shuffled prompt $\hat{p}=[(x_1, \hat{y}_1), ..., (x_n, \hat{y}_n)]$, which typically leading to incorrect outcomes. Then the representation at specific head for $[\hat{p},x]$ is substitute by the averaged task activation $\bar{h}_{lk}^T$ and calculate its causal effect (CE) on the model's output. 
\begin{equation}
\begin{aligned}
\operatorname{CE}_{lk}([\hat{p},x])=P_{M^{h^T_{lk}\rightarrow \bar{h}_{lk}^T}}(y \mid [\hat{p},x] )  -P_M(y \mid [\hat{p},x]).
\end{aligned}
\end{equation}
Here, $\bar{h}_{lk}^T \in \mathbb{R}^d$ symbolizes the mean activations of in-context learning input for task $T$ at the last token position across layer $l$ and head $k$, with 
$d$ being the dimension of the layer output as $h_{lk}^T = head_{lj}W^O_{lj}$ is the equivalent of the attention head output in the residual stream~\citep{elhage2021mathematical}. $M^{h^T_{lk}\rightarrow \bar{h}_{lk}^T}$ denotes the model with a replacement operation on attention head $(l,k)$ at last token. A higher CE implies that the specific head's state is critical for accurate predictions which encoding of more task-relevant information. In this paper, $\mathcal{S}$ is the attention head with top-10 CE.


Second, function vector $\theta_T$ is assembled by summing up the averaged ICL inputs activation from the attention heads within $\mathcal{S}$, formulated as $\theta_T=\sum_{(l, k)\in \mathcal{S}} \bar{h}_{lk}^T \in \mathbb{R}^d$. The comprehensive methodology for this extraction process can be found in the Appendix~\ref{app:fv}.


% The differences in our usage of FV
In this paper, we revisit the definition of FV and study its dynamics before and after learning a new task, providing a surrogate lens to uncover the inherent mechanisms of forgetting in LLMs. 
% Through the analysis of FV's consistency before and after tuning, this paper elucidates the fundamental mechanisms of forgetting within LLMs. 


