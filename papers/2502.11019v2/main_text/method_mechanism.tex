
\section{Causal Pathway to Forgetting through Function Vectors}
\label{sec5}
% While function vectors demonstrated a direct correlation with various phenomena of forgetting, we go deeper to the inner forgetting mechanism in ？language models. 
% \textbf{Revisit function vector with latent variable models.} 
Before delving into how function vectors causally influence forgetting, we revisit
the function vector $ \theta_T $ from the perspective of latent variable models~\citep{baum1966statistical,gruber2007hidden}. 
% FV identified within the hidden states of transformers during in-context learning represents specific model functions [CITE HERE]. 
The specific FV $\theta_T$ in Sec.~\ref{sec2} is derived from the sum of activations of certain attention heads in the LLM via causal analysis, provided with ICL input of task $T$. 
$\theta_T$ represents a latent variable descriptive of task $T$; conditioning predictions on $\theta_T$ produces 
% Here, $\theta_t$ is called the function vector and corresponding to the latent variable of task $t$. Thus, injecting 
% function vector $\theta_t$ into the model will return 
a model function specific to task $T$,~i.e., 
\begin{equation}
\label{eq:func}
P_M(y|x, \sum_{(l, k) \in \mathcal{S}} h_{lk} = \theta_{T}) \rightarrow f_T(y|x).
\end{equation} 
Here, \( f_T \) characterizes the function of task $T$ within the model, where different function vectors activate distinct functions, enabling the model to exhibit diverse abilities.



% Despite substantial empirical evidence demonstrating their effectiveness across various tasks and models~\citep{todd2023function}, it remains unclear in theoretical framework and relationship to the working of in-context learning. 
% In this discussion, we revisit it with the perspective of latent variable model~\citep{baum1966statistical,gruber2007hidden}, deepen our understanding of function vector.
Prior research~\citep{xie2021explanation, wang2024large} has established that under a latent variable assumption, in-context learning in LLMs can be rewritten as:
\begin{equation}
P_M\left(y \mid x^T_1, y^T_1, \ldots, x_n^T, y_n^T, x\right)=\int_{\Theta} P_M(y \mid \theta, x) P_M\left( \theta \mid x^T_1, y^T_1, \ldots, x_n^T, y_n^T, x\right) d \theta,
\end{equation}
where \( P_M \) denotes the predictive probability of the LLM $M$, and $\theta$ is a potentially high dimensional latent variable within the space $\Theta$. For example, in the task of
predicting the antonym ($y$) of a word ($x$), the latent variable could correspond to “writing the antonym of the word" ($\theta$).
This framework indicates that ICL boosts performance by deducing the correct $\theta$ from observed input-output pairs.

% Meantime, we revisit the approach of FV in Sec.~\ref{sec2}, which takes the sum of activations of certain attention heads in the model and obtains the specific $\theta_T$ from the representation of ICL input of task $T$ through causal analysis. 
% $\theta_T$ represents a latent variable descriptive of task $T$; conditioning the prediction on $\theta_T$ obviously returns 
% % Here, $\theta_t$ is called the function vector and corresponding to the latent variable of task $t$. Thus, injecting 
% % function vector $\theta_t$ into the model will return 
% a model function specific to task $T$, i.e., 
% \begin{equation}
% \label{eq:func}
% P_M(y|x, \sum_{(l, k) \in \mathcal{S}} h_{lk} = \theta_{T}) \rightarrow f_T(y|x).
% \end{equation} 
% Here, \( f_T \) represents task $t$'s function within the model. Different function vectors activate varying processing function within the model, enabling diverse abilities.
% It's evident that manipulating \( \theta_f \), such as through addition or subtraction, can significantly influence the LLM's proficiency in the relevant functions (cite).

\begin{wrapfigure}{hr}{0.50\textwidth}
  \centering
  \vspace{-1.6em}
  \includegraphics[width=1.1\linewidth]{pdf_figs/sec5_1_inter.pdf}
  \vspace{-1.5em}
  \caption{Intervention results on fine-tuned model. '+ Source FV' and '- Target FV' refers to Evidence I and Evidence II, respectively. \textit{\textbf{Main conclusion:} intervention with related function vector mitigating forgetting.} }
  \label{fig:sec5:intervention}
  \vspace{-2.0em}
\end{wrapfigure}
By comparing this framework with Eq.~\ref{eq:func}, we conclude that LLM predictions hinge on two critical factors: first, \textit{the prediction given by the model's task-specific function \(P_M(y|x, \theta_T)\)}, and second, \textit{the capacity of an input \(x\) to accurately trigger its corresponding Function Vector \(\theta_T\)} (i.e., $P_M(\theta_T|x)$). Previous work in continual learning has considered the role of task-specific functions (\emph{a.k.a,} model parameters) on forgetting, sparkling methods such as gradient orthogonalization~\citep{lopez2017gradient,chaudhry2018efficient} and parameter regularization~\citep{Kirkpatrick_2017,wu2024meta} to mitigate its impact. However, our research hypothesizes that in continual instruction tuning of LLMs, the \textit{intrinsic cause of forgetting is the bias of the latent variable $\theta_T$ elicited by the input $x$}, as supported by the following empirical findings. %In this section, we provide empirical evidence for this assumption.


\textbf{Finding \uppercase\expandafter{\romannumeral 1}: forgetting is mitigated by adding the source  FV with respect to the evaluation task.}
%model function related to source FV maintains good performance.}  
For each evaluation task $T^e$, we conducted an intervention experiment %on fine-tuned model 
during continual instruction tuning
 %evaluation task $e$ 
 by adding the source function vector, i.e., $\theta_{T^e}^0$, into the LLMs (blue line in Figure~\ref{fig:sec5:intervention}). Here, the source FV $\theta_{T^e}^0$ is extracted from the original LLM  $M_0$ following the procedure described in Section~\ref{sec2}. The intervention involves adding the function vector to a specific layer during inference ($h_l=h_l + \theta^0_{T^e}$), and the reported results reflect the best performance obtained by iterating over layers [3, 6, 9, 12, 15]. The findings indicate that integrating the source FV $\theta_{T^e}^0$ into inference -- which explicit transforms $P_M(y|x)$ to  $P_M(y|x,\theta_{T^e}^0)$ -- substantially restores model performance. For instance, Figure~\ref{fig:sec5:intervention}(a) shows an average performance increase of 7.3 on Hellaswag  through this intervention.
This improvement signifies that explicitly injecting the latent variable $\theta^0_{T^e}$ helps identify the function specific to the evaluation task $T^e$, i.e.,\(P_M(y|x,\theta^0_{T^e})\), thereby mitigating forgetting. % thethe model function \(P_M(y|x,\theta_{T^e})\) 
%maintains its efficacy for processing task \(_{T^e}\).


\textbf{Finding \uppercase\expandafter{\romannumeral 2}: forgetting is mitigated by subtracting the biased FV with respect to the current training task.} 
Similarly, we conduct the intervention experiment again on each evaluation task $T^e$ by subtracting the target FV, i.e., $\theta_{T_j}^j$, as depicted by the red line in Figure~\ref{fig:sec5:intervention}. Here, the target FV $\theta_{T_j}^j$ represents the function of task $T_j$ after the model has been trained on $j$-th task. It is calculated as $\sum_{(l, k)\in \mathcal{S}} \bar{h}_{lk}^{T_j,j}$, where $\bar{h}^{T_j,j}$ denotes the mean activation from in-context learning inputs in task $T_j$. The intervention involves modifying the representations in a specific layer during inference through subtracting the function vector ($h_l=h_l -  \theta_{T_j}^j$). 
The results reveal a surprising  reduction in forgetting. 
%By extracting the target FV, we effectively isolate the function \(P_M(y|x,\theta_{T^e}^0)\) from the comprehensive model function \(P_M(y|x)\), we observed significant performance boosts. 
For example, removing the target FV during Hellaswag inference, as shown in Fig.~\ref{fig:sec5:intervention} (b) top, increases the model M5's performance from 39.5 to 49.1. This finding suggests that the groundtruth latent variable $\theta_{T^e}^0$ has been confounded by the biased target  FV introduced during training on incoming tasks; substracting it mitigates this interference and thus reduces forgetting.



\textbf{Finding \uppercase\expandafter{\romannumeral 3}: 
forgetting of the in-context learning ability is severe.} In Table~\ref{tab:sec3:main} and Fig.~\ref{fig:sec3:heat}, one of the most significant observations is that the forgetting of in-context learning (ICL) ability is notably more severe than the zero-shot ability when evaluated on general tasks. For instance, in Llama2-7b-chat trained on NI-Seq-C1, the forgetting of Hellaswag reaches 34.7 for ICL, while in the zero-shot setting, the value is just 8.94. 
% This is non-trival as 
% ICL, as an efficient way to improve model performance through learning from input-output demonstration~\citep{wei2022chain, brown2020language}, 
% We attribute this to the overfitting of the mapping $P_M\left( \theta \mid x^T_1, y^T_1, \ldots, x_n^T, y_n^T, x\right)$ to the biased target FV when trained on task $T_j$, along with the expanded input space of the mapping under the ICL setup. The distorted and more complicated mapping makes it increasingly difficult to identify the groundtruth latent variable $\theta_{T^e}^0$.
% % Our analysis suggests that the core reason lies in the shift of the function vector. As discussed above, ICL 
% % can be regarded as a way to augment the model with task-specific knowledge, . 
% % Yes, when the risen function vector is biased, ICL may also lead a worse performance compared to zero-shot approach.
% This pattern is clear in the relationship between CommonsenseQA 5-shot evaluations and FV shifts during NI-Seq-G1's training on Llama2-7b-chat, as shown in Figure~\ref{fig:sec4:fv-shift}'s first row and column. 
% Specifically, it can be seen that the 5-shot evaluation performance decreases more rapidly than the a zero-shot performance when the FV undergoes shifts from M2 to M3. In contrast, performance swiftly recovers once the FV stabilizes at M5.
We attribute this to the overfitting of the mapping \(P_M\left(\theta \mid x^T_1, y^T_1, \ldots, x_n^T, y_n^T, x\right)\) to a biased target FV in task \(T_j\). This results in a distorted, complex mapping as evident from the correlation between CommonsenseQA 5-shot evaluations and FV shifts during NI-Seq-G1 training on Llama2-7b-chat (Figure~\ref{fig:sec4:fv-shift}). Notably, 5-shot performance declines faster than zero-shot as the FV shifts from M2 to M3, but recovers when it stabilizes at M5.

% \textbf{Finding \uppercase\expandafter{\romannumeral 3}: sequential data learning incurs alterations in both the direction and the position of the function vector, thereby leading to a forgetting in general ability}
% a causal mediate analysis, similar to Sec.~\ref{sec2}, is conducted on the fine-tuned models, revealing a shift in the set \(\mathcal{S}\) of causal attention heads, with the results reported in Appendix~\ref{app:casual_shift}. This shift signifies that sequential data learning incurs alterations in both the direction and the position of the function vector, thereby leading to a forgetting in general ability.

% \textbf{Finding \uppercase\expandafter{\romannumeral 4}: forgetting is associated with the shifting in FV.} Sec.~\ref{sec4} details the correlations between the phenomenon of forgetting and changes in FVs, elucidating that forgetting events coincide with the FV similarity change.  
%Such observations affirm a causal role of FV on forgetting.

%processes and FV dynamics.
%thereby substantiating the interrelation with \(P_M(\theta_t|x)\).


% Moreover, a causal mediate analysis, similar to Sec.~\ref{sec2}, is conducted on the fine-tuned models, revealing a shift in the set \(\mathcal{S}\) of causal attention heads, with the results reported in Appendix~\ref{app:casual_shift}. This shift signifies that sequential data learning incurs alterations in both the direction and the position of the function vector, thereby leading to a forgetting in general ability.



% \textbf{Summary: what is the inherent mechanism behind forgetting in large language model?}
To summarize, the above empirical findings, alongside the strong correlations between forgetting and changes in FVs demonstrated in Section~\ref{sec4}, and our causal mediation analysis in Appendix~\ref{app:casual_shift} which reveals a shift in the set \(\mathcal{S}\) of causal attention heads (i.e., signifying the magnitude and direction of the function vector) during continual instruction tuning, collectively advocate that:
% Evidence \uppercase\expandafter{\romannumeral 1} underscores that the function \(P_M(y|x, \theta_e)\) associated with the source task \(e\) remains largely unaffected post-training.  Conversely, Evidence \uppercase\expandafter{\romannumeral 2} and Evidence \uppercase\expandafter{\romannumeral 3} establish the correlation between forgetting and variations in \(P_M(\theta_e|x)\).  Consequently, we deduce that 
(1) forgetting is primarily driven by modifications in \(P_M(\theta|x)\), rather than changes in \(P_M(y|x, \theta)\); (2) in LLMs, the function responsible for handling a specific task is not overwritten but is instead overshadowed by newly introduced functions.
