

\section{Correlations between function vectors and Forgetting}
\label{sec4}
% The preceding section 
% In this section, we explore how task-specific training impacts existing abilities by leveraging function vectors (abbreviated as FV, Sec.~\ref{sec2}) as probes into the model's internal function. 
% \textbf{Revisit function vector with latent variable models.} The function vector $ \theta_t $ is identified within the hidden states of transformers during in-context learning, representing specific model function. Despite substantial empirical evidence demonstrating their effectiveness across various tasks and models~\citep{todd2023function}, it remains unclear in theoretical framework and relationship to the working of in-context learning. In this discussion, we revisit it with the perspective of latent variable model~\citep{baum1966statistical,gruber2007hidden}, deepen our understanding of function vector. By referencing prior researches~\citep{xie2021explanation, wang2024large}, the in-context learning of LLMs under latent variable model assumption can be rewritten as:
% \begin{equation}
% P_M\left(y \mid x^t_1, y^t_1, \ldots, x_k^t, y_k^t, x\right)=\int_{\Theta} P_M(y \mid \theta, x) P_M\left( \theta \mid x^t_1, y^t_1, \ldots, x_k^t, y_k^t, x\right) d \theta,
% \end{equation}
% where \( P_M \) denotes the output probability of the large language model $M$, $\theta$ is a potentially high dimensional latent variable and $\Theta$ is the space of the variable. For example, in the task of
% predicting the antonym ($y$) of a word ($x$), the concept behind the task is “writing the antonym of the word" ($\theta$).
% This framework implies that in-context learning boosts performance by deducing the correct $\theta$ from observed input-output pairs.
% To this end, revisiting the approach of FV in Sec.~\ref{sec2}, it interprets $\theta$ as the sum of activations of certain attention heads in the model, and obtains the specific $\theta_t$ from the representation of ICL input of task $t$ through causal analysis. Here, $\theta_t$ is called the function vector and corresponding to the latent variable of task $t$. Thus, injecting function vector $\theta_t$ into the model will return a model function specific to task $t$, i.e., 
% \begin{equation}
% \label{eq:func}
% P_M(y|x, \sum_{(l, j) \in \mathcal{S}} h_{lj} = \theta_{t}) \rightarrow f_t(y|x).
% \end{equation} 
% Here, \( f_t \) represents task $t$'s function within the model. Different function vectors activate varying processing function within the model, enabling diverse abilities.
% % It's evident that manipulating \( \theta_f \), such as through addition or subtraction, can significantly influence the LLM's proficiency in the relevant functions (cite).
% Contrasting with traditional approaches that assess task similarities via feature similarity~\citep{ramasesh2020anatomy,lee2021continual} and readout similarity~\citep{lee2021continual}, which compute through a held-out 
% they  or model distance~\citep{lin2023theory, evron2024joint}  which  function vectors facilitate comparisons based on the task function with in the model. 
% While the function vector encapsulates the model's interpretation of a task, by examining the similarities based on function vectors, we can devise a nuanced, model-dependent task similarity metric. Contrarily, 
The previous section prompts a more effective measure for characterizing catastrophic forgetting, surpassing those traditionally used in continual learning research with small models, such as 
% for evaluating task similarities—such as those based on 
feature similarity~\citep{ramasesh2020anatomy,lee2021continual} and readout similarity~\citep{lee2021continual} between tasks. %They fail to overlook the distinctive task-related information inherent in different models. 
We have proven them loosely correlated with forgetting under LLMs. 
Other model-dependent measures, such as the $\ell$2-distance of model parameters after tuning on new tasks~\citep{lin2023theory, evron2024joint}, necessitate expensive training for their computation. 
In this section, we first establish that \emph{the similarity between function vectors (FV, see Sec.~\ref{sec2}) is strongly correlated with diverse forgetting patterns across task types, training stages, and language models}. 

% they  or model distance~\citep{lin2023theory, evron2024joint}  which  function vectors facilitate comparisons based on the task function with in the model. 

% The function vector defines the model's own understanding of the task. By learning the similarity of function vectors across tasks, we can better establish a task similarity index related to the model, thereby better explaining the model's different forgetting patterns across various types of tasks, training stages of continual learning, and language models.
% By exploring similarities and dynamics through function vectors, we can better understand why and how the model adapts to or forgets specific tasks over time.

% \begin{figure*}[!b]


\textbf{Forgetting coincides with changes in FV similarity between model updates.} 
We now explore the relationship between forgetting and variations in function vectors across evaluation tasks. As shown in Figure~\ref{fig:sec4:fv-shift}, we evaluate the performance on the four general evaluation tasks throughout the sequence for both generation and classification settings, alongside shifts in function vectors $\theta_{T^e}$. ``Fv sim'' in the diagram refers to $\operatorname{Cosine}(\theta_{T^e}^0, \theta_{T^e}^j)$, where $\theta_{T^e}^j$ is the FV of evaluation task $T^e$ after fine-tuning the $j$-th task.
We observe a clear consistency between the performance decline and variations in function vectors. Specifically, as performance drops, the similarity between FV $\theta_{T^e}^0$ and FV $\theta_{T^e}^j$ generally declines, whereas this similarity tends to increase as performance recovers. For example, in the Hellaswag task within NI-Seq-G1 (top right subplot in Figure~\ref{fig:sec4:fv-shift}),  the correlation coefficient ($R^2$ value) between zero-shot performance and our proposed similarity measure reaches 0.873. This finding underscores that fluctuations in the FV often coincide with model forgetting, and 
%, emphasizing the causal relationship between them. It further 
justifies the rationale of characterizing forgetting through function vectors.
% , which exposes the conflicts and transitions involved in the model's processing of inputs.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=1.\linewidth]{pdf_figs/sec4_1_fvsim.pdf}
  \vspace{-2.0em}
  \caption{The shifts in function vector with 0/5-shot performance during tuning. The bar chart corresponding to the left y-axis shows the similarity of function vectors to their initial state. The line graph corresponding to the right y-axis depicts the model's Rouge-L metric on test data. {\textit{\textbf{Main conclusion:} A significant correlation between performance (line data) and FV similarity (bar data).} The correlation plots with more data point are provided in Fig.~\ref{fig:app:corr}.}}
  \label{fig:sec4:fv-shift}
  \vspace{-1.0em}
\end{figure*}


% Tasks share similar FV are 
% \textbf{Forgetting strongly correlates with similarity between training and test FVs.} 
\textbf{Forgetting coincides with changes in  FV similarity between tasks.} 
% Next, we aim to leverage the FV to explore the similarity of tasks within the model and investigate under what conditions the model is prone to forgetting.
In Figure~\ref{fig:app:fvsim}, we present the similarity between the FVs of training and evaluation tasks, alongside the corresponding forgetting after training. This similarity between FVs is defined as $\operatorname{Cosine}(\theta_{T^e}^{j-1}, \theta_{T_j}^{j-1})$, where $\theta_{T^e}^{j-1}, \theta_{T_j}^{j-1}$ are the function vectors extracted from model $M_{j-1}$ on evaluation task $T^e$ and the current training task $T_j$, respectively. 
We observe a non-trivial phenomenon: the lower the similarity between the FVs of two tasks, the greater the extent of forgetting on the evaluation task after training. For instance, in the first column of Figure~\ref{fig:app:fvsim}, the most severe forgetting occurs at task $T_2$ while function vectors exhibit low similarity. This contrasts with prior findings~\citep{ramasesh2020anatomy, lee2021continual}, where higher task feature similarity was linked to greater forgetting. 
We hypothesize that this phenomenon stems from the substantial capacity and universality of LLMs, enabling them to construct new functions based on old ones without overwriting as continual instruction tuning proceeds. The lower similarity between the FVs of the training and evaluation tasks indicates more diverse functions introduced, which exacerbates the challenge of locating the groundtruth function corresponding to the evaluation task and thus leads to forgetting.
We defer proof of this hypothesis to Section~\ref{sec5}.
% Hence, the old functional model remains effective even after adjustment, leading to less deviation in the test task the higher the similarity between training and test tasks. 
% We validate this in Appendix~\ref{}, demonstrating the naturalness of reusing original functionalities during training.



% \textbf{Robustness across various language models}





