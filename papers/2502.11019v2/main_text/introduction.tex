
\section{Introduction}


Continual instruction tuning~\citep{peng2023instruction, chung2024scaling} has emerged as an indispensable ingredient in the development of Large Language Models (LLMs)~\citep{brown2020language,radford2019language,touvron2023llama}, enabling them to meet the demands of specific domains~\citep{roziere2023code, thirunavukarasu2023large, xue2024famma} and human preferences~\citep{ouyang2022training}. However, a notable concern with such continual tuning is "catastrophic forgetting"~\citep{mccloskey1989catastrophic, Kirkpatrick_2017}, where models may lose essential skills~\citep{dou2023loramoe,chen2023chatgpt} such as mathematical reasoning while adjusting to user instructions. While instruction tuning effectively evolves LLMs, it's critical to characterize and mitigate forgetting within these models.
% This raises questions about which abilities are most susceptible to forgetting and the underlying causes of these losses in LLMs.

Research on LLM forgetting~\citep{luo2024empirical, wang2023trace, wu2024llama} generally examines alterations in specific abilities like reading comprehension, factual knowledge, mathematical reasoning skills, and so on, underscoring the universal existence of catastrophic forgetting. As they have primarily studied from a single training sequence, they fail to establish the connection between model forgetting and the characteristics of training data. Concurrently, there is a notable gap in understanding the internal mechanisms that underlie model forgetting. To date, only a limited body of research has ventured into this area; notably, the work of \citet{kotha2024understanding}, proposing the task inference hypothesis, explores how conflicts between task processors lead to forgetting. Nevertheless, the existing literature still struggles to track the internal mechanisms behind forgetting, which is crucial for understanding why and when forgetting occurs in language models after learning new tasks and how to avoid it.


% n this section, we evaluate catastrophic forgetting of multiple LLMs in several scenarios focusing on instruction tuning stage. Our investigation concentrates on the research question: When does forgetting happen? Through empirical studies, we aim to uncover the unified rule behind the forgetting across different types of tasks, training stages of continual learning and language models.

In this study, we conduct thorough experiments on various continual instruction tuning benchmarks covering multiple language models, task sequences, and evaluation metrics. Our investigation focuses on the research question: When does forgetting happen? The experimental results suggest that model forgetting is a complex outcome of various factors, including the nature of training and test tasks, and the state of the model. However, traditional methodologies for evaluating task similarities to characterize forgetting—such as those based on feature similarity~\citep{ramasesh2020anatomy, lee2021continual} and readout similarity~\citep{lee2021continual}—tend to overlook the distinctive task-related information inherent in different models. Meanwhile, purely model-dependent measurements, like the L2 distance of model parameters after being fitted on a new task~\citep{lin2023theory, evron2024joint}, necessitate training to compute task similarity. We identify a critical gap in the availability of robust tools to dissect and understand the processes underlying forgetting.

% To this end, we first present a novel perspective to investigate catastrophic forgetting in LLMs, focusing on the capabilities developed during pre-training and alignment phases. We suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through \textit{Knowledge Probability} $P(y|x)$ and \textit{Instruction Probability} $P(y^c|c, x)$, respectively (as depicted in Fig.~\ref{fig:sec2:case}).
% Our empirical analysis within a continual instruction tuning framework reveals distinct forgetting patterns between these two aspects, with shifts in instruction following primarily driving performance declines.

% Specifically, the comprehending of world knowledge build through pre-training is measured by $Knowledge Probability$ while the performance on the task-specific instruction referred as $Instruction Probability$. When discussing catastrophic forgetting of a task, we monitor changes in both of these metrics.  Through our empirical analysis carried out in a continual instruction tuning framework, we observe a pronounced discrepancy in forgetting between tasks associated with knowledge and instruction-following. This reveals that diminished abilities to follow instructions predominantly account for the forgetting in task performance.

% \begin{figure}{hr}{0.43\textwidth}
%   \centering
%   \vspace{-1.5em}
%   \includegraphics[width=1.00\linewidth]{Styles/figs/FV-instervention.png}
%   \vspace{-2em}
%   \caption{Intervention results on four datasets via Enhanced Instruction Vector.}
%   \label{fig:sec3:intervention}
% \end{figure}



% \begin{wrapfigure}{hr}{0.53\textwidth}
%   \centering
%   \includegraphics[width=1.\linewidth]{figs/hypothesis.pdf}
%   \vspace{-0.6em}
%   \caption{Instruction vector hypothesis for LLM understanding. $\theta_c$ is extracted by aggregating representations of attention heads identified to have causal influence to the output. Forgetting is resulted from the suppression of instruction vector associated computation graph.}
%   \label{fig:sec1:hypothesis}
%   \vspace{-0.6em}
% \end{wrapfigure}

To this end, we utilize the Function Vector approach~\citep{todd2023function}, a method grounded in mechanistic interpretability~\citep{wang2023interpretability, bills2023language}, which represents the input-output function within a model into a compact vector form. Our analysis begins with a revisitation of the theoretical formulation of FV through the perspective of latent variable models~\citep{baum1966statistical, gruber2007hidden}, establishing that the FV serves as a traceable latent variable in LLMs. We then examine model forgetting through the Function Vector perspective, successfully identifying occurrence of forgetting by evaluating the similarity between training and testing tasks (Sec.~\ref{sec3}). Subsequent empirical investigations lead us to conclude that the fundamental driver of forgetting is the shift in the mapping from the input \(x\) to the latent concept variable \(\theta\), rather than the erasure of previously learned task functions (Sec.~\ref{sec4}).

% To this end, we introduce Function Vector~\citep{todd2023function} - a mechanistic interpretability~\cite{wang2023interpretability,bills2023language} method that can represent input-output function in the model as a compact vector - to studying the internal behavior of language models behind forgetting. Specifically, we first revisit the theoretical formulation of function vector with latent variable models~\citep{baum1966statistical,gruber2007hidden} and figure out FV is the traceable latent variable for LLMs (Sec.~\ref{sec3}.1). Then we characterize forgetting from the perspective of function vector, successfully indicate the occurrence of forgetting with the similarity between train and test task (Sec.~\ref{sec3}). Finally, with more empirical study, we deduce that the intrinsic cause of forgetting is the shift of the latent concept variable $\theta$
% raised by the input x, rather than the overwritten of previous task function.



% To this end, we propose the Instruction Vector (IV) framework, a novel method designed to isolate and analyze the model representations closely linked to specific instruction following throughout the fine-tuning phase. Initially, we posit a hypothesis regarding the causal structure in LLMs, suggesting the presence of a deterministic latent factor, $\theta_c$, critical for the model's successful task  prediction.  This concept aligns with the essence of In-context learning and is validated through causal intervention experiments~\ref{sec3.2}. Then, we extract $\theta_c$ by aggregating representation of a specific set of attention heads within the model that have been identified to have a significant causal influence on the model's output.  Our findings reveal two main insights: (1) Fine-tuning introduces a bias in the neuron hidden states towards the task's EIV, culminating in a notable alteration in zero-shot performance; and (2) The stability of EIV pre- and post-fine-tuning is less crucial in averting forgetting. Contrary to initial expectations, it seems that fine-tuning overlays new, task-specific patterns on top of the model’s existing competencies, leading to the false impression of modified capabilities.

Based on our analysis, we conclude that minimizing the shift in the function vector during training serves as a key strategy for mitigating forgetting. We propose a function vector guided training mechanism as a simple yet efficient design for mitigating forgetting. This approach involves limiting changes to the function vectors associated with training tasks through a regularization term, coupled with the adoption of a function vector-guided Kullback-Leibler (KL) divergence loss. This loss function aims to diminish the discrepancies between logits derived from zero-shot input and those adjusted by function vector intervention, ensuring the fine-tuned model remains consistent with the inner task function. Validated across multiple datasets and models, this method significantly alleviates forgetting in both general and in-context learning abilities, confirming the correlation between FV dynamics and forgetting.

% Our analysis suggests that controlling the shift in the function vector during training is crucial for reducing forgetting. We introduce a function vector guided training approach that incorporates a regularization term to stabilize function vectors and uses a FV-guided Kullback-Leibler (KL) divergence loss. This loss minimizes discrepancies between zero-shot logits and those modified by FV intervention, maintaining task function consistency in the fine-tuned model. Tested across various datasets and models, this method effectively alleviates forgetting in both general and in-context learning scenarios, underscoring the link between FV dynamics and forgetting.


\textbf{Main findings and contributions.}
\textbf{(1)} We investigate catastrophic forgetting in LLMs covering multiple language models, task sequences, and evaluation metrics, discovering that forgetting in LLMs is highly model-dependent, asserting a new analytical tool for characterizing forgetting behavior.
\textbf{(2)} Using empirical and theoretical analysis based on the function vector framework, we reveal that forgetting generally results from the activation of biased model functions rather than overwriting previous functions.
\textbf{(3)} We have developed a function vector guided training approach that preserves and aligns function vectors during fine-tuning, significantly improving both general and in-context learning across multiple continual learning datasets.


% Through these operations, we ensure that the training process protects the original computation graph, preventing the suppression of existing knowledge by newly acquired knowledge, thereby reducing forgetting. Our method can be combined with existing continual learning approaches and has been validated across multiple datasets, resulting in significant improvements in preventing general ability and in-context learning ability. This application further confirms the connection between instruction vector and forgetting. 

% To conclude, our work makes the following contributions: (1). (2). (3).

