In this section, we provide background on why sliding window attention (SWA) is inefficient in high-dimensional settings, particularly for Video Diffusion Transformers~\citep{peebles2023scalable}. For clarity, our notation assumes cubic window, tile, and video sizes unless stated otherwise, though our approach extends to non-cubic configurations.

\subsection{Attention in Video DiTs}
\label{sec:background_attention}
State-of-the-art Video DiTs employ 3D full attention to mix signals across tokens, allowing each token to attend to any other token. Given a video latent of shape $(L, L, L)$ (often encoded via a VAE), this is achieved by flattening the 3D data into a sequence of length $L^3$ and applying full bidirectional attention. However, as the sequence length grows cubically with $L$, even a small increase in resolution or duration leads to a significant computational burden. As a result, applying 3D attention to high-resolution, long-duration videos becomes prohibitively expensive.

Formally, let $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{N \times d}$ represent the query, key, and value of input sequences for a single attention head, where $N=L^3$, and $d$ is the dimension of each head. Let \( M \in \{-\infty, 0\}^{N \times N} \) represents the attention mask. The attention operation is defined as:
\begin{equation}
    S = \frac{QK^\top}{\sqrt{d_k}}, \quad
    A = \text{Softmax}(S + M) , \quad
    O = AV 
    \label{eq:attention}
\end{equation}

A naive attention implementation constructs the full \( S, A, M \in \mathbb{R}^{N \times N} \) on GPU HBM, leading to both \(\mathcal{O}(N^2)\) memory overhead and excessive data transfers between HBM and SRAM. FlashAttention mitigates this issue by tiling input sequences into smaller blocks. Through online softmax, each GPU SM loads a block of queries, keys, and values into SRAM, performs computation, and writes the final result to HBM, avoiding the materialization of \( A \) and \( S \). To reduce computation cost, we can apply attention mask to control sparsity, with the mask computed on-chip per block to avoid the \(\mathcal{O}(N^2)\) memory cost of a global mask. This sparsity can reduce latency by skipping masked-out attention regions. However, as we show in the next section, this is not always effective.

Sliding window attention (SWA) is a widely used sparse attention method that reduces computation costs while preserving locality. In SWA, a query attends only to keys within a fixed window, and stacking multiple attention layers naturally expands the receptive field beyond the window size. SWA has been extensively studied in natural language processing~\citep{beltagy2020longformerlongdocumenttransformer,jiang2023mistral7b}. As motivated in \S\ref{sec:introduction}, state-of-the-art video diffusion models exhibit a strong \textit{3D locality} pattern, making them a natural candidate for applying 3D SWA. However, directly applying SWA to high-dimensional data fails to fully utilize GPU computation. Existing 2D/3D SWA kernels, such as Tiled NATTEN~\citep{hassani2023neighborhood}, shift window centers at image and video boundaries to ensure each query attends to a constant number of keys. It also improves kernel efficiency through input tiling and kernel fusion, but its performance suffers from problems described next.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth,trim=0cm 0.7cm 4.5cm 0.4cm,clip]{media/Fig4/figure4-3.pdf}
    \caption{The attention map of NATTEN, Tiled NATTEN, and \methodnameshort. We plot with an image size 24$\times$24 and a 12$\times$12 local window. The tile size is set to 4$\times$4. (a) NATTEN creates many mixed blocks that are very inefficient for Flash Attention computation. (b) Tiled NATTEN increases the number of dense blocks, but the mixed blocks persist. (c) \methodnameshort~completely eliminates the mixed block, making the computation extremely friendly for GPU. Note that we mainly show \methodnameshort's application in 3D scenarios for video generation in this paper, but for better illustration, we present the 2D scenario in this plot.}
    \label{fig:attention_map}
\end{figure*}

\subsection{Inefficiency of Sliding Window Attention}
\label{sec:3.1}
Implementing 2D/3D SWA with FlashAttention requires defining its attention mask. As discussed in  \S\ref{sec:background_attention}, FlashAttention calculates and applies masks at the block level. Based on different intra-block masking strategies, we categorize attention blocks into three types:
dense (with all attention scores retained), empty (mask out all values), and mixed (with some scores removed), shown in Figure~\ref{fig:attention_map}.
Empty blocks can be entirely skipped during computation. Mixed blocks, while sparser than dense blocks, introduce significant overhead.  First, they require full computation for the entire block before applying masks to retain or discard attention scores, introducing unnecessary computations.
Second, to determine which position to retain or discard, the attention kernel needs to calculate the value of the mask based on the SWA pattern and the block's position relative to the entire attention mask. The mask calculation introduces substantial overhead -- in the case of calculating the simple causal mask, FlexAttention~\citep{dong2024flexattentionprogrammingmodel} reports a 15\% overhead. We show in  \S\ref{sec:benchmark_Attn} that the mask overhead dominates the attention latency for more complex masking patterns such as sliding windows. In SWA, each query attends to a distinct set of keys, resulting in a zigzag pattern in the attention map and generating numerous mixed blocks, as shown in Figure~\ref{fig:attention_map}~(a). Although the state-of-the-art sliding window attention implementation, Tiled NATTEN, aims to reorder the inputs to increase the number of dense blocks, a significant portion of blocks remains as mixed blocks. 

In summary, SWA beyond 1D sequences introduces two major inefficiencies: (1) mixed blocks do not reduce FLOPs due to sparsity, and (2) they incur additional mask evaluation overhead, making them slower than dense blocks.  
Efficient sparse attention in 3D should minimize mixed blocks while maintaining locality. This insight drives the development of our novel STA method, which significantly reduces mixed blocks while preserving the locality property of SWA.


\subsection{Alternative Methods for 3D Attention}
Other than sparsifying the attention maps, some approaches accelerate video diffusion by decomposing the 3D attention into alternating spatial and temporal components~\citep{singer2022makeavideotexttovideogenerationtextvideo, wang2023lavie, chen2023videocrafter1opendiffusionmodels, ma2024latte}. However, these methods have been largely superseded by full 3D attention in state-of-the-art video models~\citep{opensora,lin2024open, genmo2024mochi, kong2025hunyuanvideosystematicframeworklarge}. We hypothesize that this is because alternating spatial and temporal attention fails to capture interactions between tokens that are offset in both spatial and temporal dimensions. For instance, a query at (1, 1, 1) can attend to keys at (1, X, X) or (X, 1, 1), but not at (2, 2, 2), even though they are spatially close. This disrupts the \textit{3D locality} pattern, which we have shown as a key characteristic of video diffusion models.


% SWA shares similarities with Convolutional Neural Networks (CNNs) and can be interpreted as a depth-wise CNN with input-dependent weights~\citep{han2022connectionlocalattentiondynamic}. Despite these connections, SWA has seen limited application in 2D and 3D tasks, such as image or video processing, due to the lack of efficient implementations for higher-dimensional cases. Efforts by \citet{hassani2023neighborhood, hassani2024faster} to optimize CUDA kernels for SWA have not fully realized the theoretical FLOPS speedups expected from its sparsity. This is because SWA inherently challenges  GPU computation: each query interacts with a unique set of keys, complicating batch execution~\citep{hu2019localrelationnetworksimage, ramachandran2019standaloneselfattentionvisionmodels,liu2021swintransformerhierarchicalvision}. SwinTransformer~\citep{liu2021swintransformerhierarchicalvision} addresses this issue by using fixed attention windows within a layer and shifting attention patterns across layers to expand the receptive field. However, this approach sacrifices locality, as nearby tokens may fall into separate windows that cannot interact within a single layer. To our knowledge, \methodname~is the first window attention mechanism to maintain both locality and achieve efficient implementation.


% SWA shares similarities with Convolutional Neural Networks (CNNs) and can be interpreted as a depth-wise CNN with input-dependent weights~\citep{han2022connectionlocalattentiondynamic}. Despite these connections, SWA has seen limited application in 2D and 3D tasks, such as image or video processing, due to the lack of efficient implementations for higher-dimensional cases. Efforts by \citet{hassani2023neighborhood, hassani2024faster} to optimize CUDA kernels for SWA have not fully realized the theoretical FLOPS speedups expected from its sparsity. This is because SWA inherently challenges  GPU computation: each query interacts with a unique set of keys, complicating batch execution~\citep{hu2019localrelationnetworksimage, ramachandran2019standaloneselfattentionvisionmodels,liu2021swintransformerhierarchicalvision}. SwinTransformer~\citep{liu2021swintransformerhierarchicalvision} addresses this issue by using fixed attention windows within a layer and shifting attention patterns across layers to expand the receptive field. However, this approach sacrifices locality, as nearby tokens may fall into separate windows that cannot interact within a single layer. To our knowledge, \methodname~is the first window attention mechanism to maintain both locality and achieve efficient implementation.