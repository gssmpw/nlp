
% \noindent \textbf{Sub-Quadratic attention.}
% Transformers suffer from the quadratic complexity of the self-attention module. There are two main approaches to address this issue. One line of work focuses on kernel-based linear attentions~\citep{wang2020linformer, liu2021swin, arar2021learned, yang2024GLA}, which decomposes the softmax operation using kernel or gate functions to achieve linear complexity. Another approach involves attention mask like those presented in ~\citep{ding2023longnet, lee2023sea, hassani2023neighborhood, xiao2023efficient, yang2024pyramidinfer} which selectively compute only a low-order subset of attention operations, resulting in sub-quadratic complexity.
We review additional related work in diffusion acceleration.
Linear attention methods~\citep{wang2020linformer, liu2021swin, arar2021learned, yang2024GLA} can decompose the softmax operation in quadratic attention using kernel or gate functions to achieve linear complexity. However, these methods have not yet been successful in video DiTs.
Another major bottleneck in diffusion models is the large number of diffusion steps. Several techniques have been proposed to accelerate sampling without sacrificing quality, including DDIM~\citep{song2020denoising} and faster ODE and PDE solvers using numerical methods~\citep{song2019generative, lu2022dpm, lu2022dpm++, jolicoeur2021gotta}. New methods have also emerged to further reduce the number of sampling steps, such as consistency distillation~\citep{kim2023consistency, song2023consistency, salimans2024multistep, xie2024mlcm}, adversarial distillation~\citep{sauer2023adversarial}, and other distillation approaches~\citep{li2024t2v, yin2023onestep, yin2024one}. STA is largely complementary to these methods.


% Another line of work focuses on diffusion cache skip and reuse techniques~\citep{zhao2024real, chen2024delta}. While these methods do not directly reduce the number of steps, they reduce computational costs by reusing intermediate features between steps.

% For system-level optimization, parallel inference~\citep{li2024distrifusion, wang2024pipefusion,chen2024asyncdiff,zhao2024real} and specialized kernels~\citep{zhang2024sageattn,yuan2024ditfastattn} have been developed for diffusion models.


