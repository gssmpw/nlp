





We evaluate \methodnameshort~on HunyuanVideo, a state-of-the-art open video DiT comparable to many proprietary ones\footnote{We skip evaluating on other open models~\cite{lin2024open, opensora, ma2024latte} due to their significantly lower overall quality compared to Hunyuan.}.
% If the output videos are already subpar, it is hard for human to discern the potential quality degradation caused by the fast sampling methods.
We generate Hunyuan outputs with 117 frames at a 1280×768 resolution. After VAE compression and tokenization, this corresponds to a latent video of shape (30, 48, 80). Beyond video, we also apply \methodnameshort~on the leading image diffusion model, FLUX~\citep{flux2023}, to demonstrate its effectiveness in 2D.
We evaluate both \emph{efficiency} and \emph{video quality}. 
\methodnameshort~kernel’s efficiency is measured using standard metrics such as MFU and latency, as detailed in \S\ref{sec:benchmark_Attn}. For end-to-end speedup on DiT, we report measured wall-clock latency, excluding time spent on VAE and text encoder.
% We use measured world-clock latency with torch.compile to benchmark the end-to-end diffusion transformer speedup without considering VAE and text encoder\footnote{One exception is $\Delta$-DiT, a cache method that introduces conditional branches in the model forward that makes static optimization in torch.compile non-trivial. We thus report the theoretical latency instead, which is a lower bound of world-clock latency.}. 
For generated video quality, we find existing automated metrics are often unreliable. Following~\citet{polyak2024movie}, we emphasize human evaluation and present the results in  \S\ref{sec:human_eval}. For completeness, we also report automated metrics, including VBench~\citep{huang2024vbench}, SSIM, PSNR, and CD-FVD~\citep{ge2024contentbiasfrechetvideo}. 
We provide an example in Figure \ref{fig:qualitative_examples2a}, with additional qualitative results available in Appendix Section \ref{sec:qualitative_example}.


\noindent \textbf{Baseline methods.}
We compare \methodnameshort~to other sparse or window attention designed for image or video, including CLEAR~\citep{liu2024clearconvlikelinearizationrevs}, NATTEN~\cite{hassani2023neighborhood}, and Swin~\citep{liu2021swintransformerhierarchicalvision}. Also, we adapt the caching-based method, $\Delta$-DiT~\citep{chen2024delta}, for evaluation on HunyuanVideo. Further details on the baseline methods and their implementations can be found in Appendix \ref{sec:baseline}.

\subsection{Efficiency of ~\methodname}
\label{sec:benchmark_Attn}
We benchmark the efficiency of various attention algorithms assuming generating 720P 5s videos using Hunyuan, shown in Table \ref{tab:benchmark_attn_table1}. The configuration ensures that all sparse kernels maintain approximately 90\% sparsity, with additional results for a lower sparsity setting (56\%) provided in table~\ref{tab:benchmask_sta_appendix}. 
Since \methodnameshort~builds on FA3 and ThunderKittens, we use ThunderKittens' FA3 as the baseline and report the relative speedup of all sparse attention kernels. To quantify efficiency, We introduce \emph{kernel efficiency}, defined as the ratio of a sparse kernel's MFU to that of full attention. This metric captures how well sparse kernels translate theoretical FLOP reductions into actual latency improvements.

The results highlight the inefficiency of existing methods.
Despite reducing TFLOPs to 15.65, CLEAR incurs a 0.86× slowdown. 
Similarly,  NATTEN variants, despite reaching 0.91 sparsity, still suffers from inefficiency: its vanilla version slows down by 0.85$\times$, while its optimized tiled variant in FlexAttention achieves only a modest 1.27$\times$ speedup. 
Among existing methods, Swin~\citep{liu2021swin} is the only kernel with MFU exceeding 40\% and kernel efficiency above 60\%. However, Swin is not a sliding-window-based attention, and we argue its efficiency comes at the cost of expressiveness in \S\ref{sec:finetuning_results}.
 
Compared to Tiled NATTEN, one of the most optimized sliding window attention implementations, the key algorithmic difference in \methodname~is changing the sliding unit from a token to a tile. Despite its simplicity, this modification significantly improves efficiency. 
To ensure a direct comparison with tiled NATTEN, we also implement \methodnameshort~in FlexAttention -- \methodnameshort~improves MFU from 8.20\% to 41.03\%. Further, with our optimized kernel for asynchronous data loading and inter-block mask management in ThunderKittens, \methodnameshort~ achieves a 10.45$\times$ speedup over full attention. Additionally, we evaluate \methodnameshort~with 58.33\% sparsity, where it achieves 2.37x speedup. This efficiency gain enables a significantly larger window size while still outperforming NATTEN. 
To our knowledge, \methodnameshort~is the first sliding-window sparse attention that achieves both 3D locality and hardware efficiency. 

\subsection{Human Evaluations}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.88\columnwidth]{media/Fig5/human_eval.png} % Replace 'example-image' with your file name
    % \vspace{-10pt}
    \caption{Human evaluation on 200 prompts from the MovieGen Bench~\citep{polyak2024movie}. \methodnameshort~achieves a 1.36× end-to-end speedup while maintaining performance comparable to the original HunyuanVideo. Additionally, \methodnameshort~consistently outperforms $\Delta$-DiT across different inference budgets.}
    \label{fig:Human_Evaluation}
\end{figure}


\label{sec:human_eval}
We assess human preference across five models that achieve the best quality performance:
(1) \emph{HunyuanVideo}; 
(2) \emph{\methodnameshort-tf-1.36x}: HunyuanVideo with 1.36× speedup via training-free mask search, 
(3) \emph{\methodnameshort-t-2.43x}: HunyuanVideo with 2.43× speedup via finetuning with \methodnameshort, (4-5) two variants of $\Delta$-DiT (1.36× and 1.8× speedup). Other baselines such as CLEAR or Swin are either prohibitively slow or produce subpar quality. Following MovieGen~\citep{polyak2024movie}, we randomly sample 200 prompts from the MovieGen Bench and conduct pairwise comparisons between these models. Evaluators select the video with higher overall quality or mark both as a tie.

In Figure \ref{fig:Human_Evaluation}, \methodnameshort-t-2.43x decisively outperforms $\Delta$-DiT-1.8x, achieving a dominant 69.8\% win rate versus 11.1\%, despite a greater speedup. Similarly, \methodnameshort-tf-1.36x surpasses $\Delta$-DiT-1.36x with a 58.5\% win rate against 11.0\%. Compared to the original HunyuanVideo, \methodnameshort~maintains competitive quality, with \methodnameshort-tf-1.36x achieving a 74.5\% tie rate—indicating near-parity in most cases. Though it has a 5.5 percentage point lower win rate than its loss rate, this tradeoff comes with a 1.36× speedup, demonstrating strong quality preservation alongside significant efficiency gains. These results establish \methodnameshort~as achieving a superior quality-efficiency tradeoff compared to $\Delta$-DiT.

\subsection{Training-free Results}

In Table \ref{tab:training_free comparison}, we evaluate mask-search \methodnameshort~ and $\Delta$-DiT on VBench prompts, testing robustness across different sampling steps. Our focus is on preserving video quality in a low-step regime. For each diffusion step count, we report SSIM, PSNR, and CD-FVD, using HunyuanVideo’s outputs at the same step count as the reference. We set the scheduler shift to 17 for 10-step inference and 7 for 25- and 50-step inference following Hunyuan's default settings.

\begin{table}[t]
\caption{Training-free performance with varying sampling steps. $\Delta$-DiT shows consistently worse quality compared to \methodnameshort, and that gap widens as the number of inference steps decrease.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccc}
    \toprule
    \textbf{Model} & \textbf{SSIM} $\uparrow$ & \textbf{PSNR} $\uparrow$ & \textbf{CD-FVD} $\downarrow$ & \textbf{Latency} & \textbf{Speedup}\\ 
    \midrule
    \rowcolor[gray]{0.95} \multicolumn{6}{l}{\textit{\textbf{steps = 50}}} \\ 
    $\Delta$-DiT     &72.86\%   & 18.09  & 122.74   & 693s & $1.36\times$ \\ 
    \methodnameshort~   &76.21\%   & 19.94  & 97.03   & 695s  & $1.36\times$ \\ 
    \midrule
    \rowcolor[gray]{0.95}\multicolumn{6}{l}{\textit{\textbf{steps = 25}}} \\ 
    $\Delta$-DiT     &77.91\%   & 19.86  & 196.25   & 352s & $1.34\times$ \\  
    \methodnameshort~   &82.47\%   & 22.53  & 95.86   & 348s  & $1.36\times$ \\ 
    \midrule
    \rowcolor[gray]{0.95} \multicolumn{6}{l}{\textit{\textbf{steps = 10}}} \\ 
    $\Delta$-DiT    &83.19\%   & 21.20  & 201.24   & 144s & $1.32\times$ \\ 
    \methodnameshort~  &87.15\%   & 24.04  & 80.41   & 139s  & $1.36\times$ \\ 
    \bottomrule
\end{tabular}}
% \vspace{-25pt}
\label{tab:training_free comparison}
\end{table}

Our training-free \methodnameshort~consistently outperforms $\Delta$-DiT, with the quality gap widening as the step count decreases. At 50 steps, $\Delta$-DiT's CD-FVD score is 25.71 higher than \methodnameshort~(122.74 vs. 97.03, where lower is better). This gap grows to 100.39 at 25 steps and 120.83 at 10 steps. 
Qualitatively, $\Delta$-DiT produces structurally similar but visually degraded outputs, explaining the narrow SSIM gap—SSIM captures structural similarity but not fine details. 
These results suggest that caching-based methods degrade in very low-step sampling, while \methodnameshort~maintains fidelity to the original model.  Intuitively, caching methods exploit temporal redundancy, which diminish with fewer steps, whereas \methodnameshort~remains effective. 
 This distinction suggests \methodnameshort~may complement step-reduction techniques like consistency distillation~\citep{song2023consistency}, which we leave for future work.



\begin{table*}[h!]
\centering
\scriptsize
\caption{Performance on VBench across different sparse attention patterns. \methodnameshort~achieves both high-quality video generation and significant speedup, while CLEAR and Tiled NATTEN suffer from efficiency issues and Swin suffers from quality degradation.}
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{Methods} & \textbf{Config} & \makecell{\textbf{VBench} \\ \textbf{Quality}} & \makecell{\textbf{VBench} \\ \textbf{Semantic}} & \makecell{\textbf{VBench} \\ \textbf{Total} } & \textbf{Attn Sparsity} & \textbf{PFLOPS} & \textbf{Latency} & \textbf{Speedup} \\
\midrule
FA2 & -- & 85.34\% & 72.17\% & 82.71\% & 0.00\% & 574.16 & 1496s & 0.63$\times$ \\
 FA3 & -- & 85.34\% & 72.17\% & 82.71\% & 0.00\% & 574.16 & 945s & 1.00$\times$ \\
\midrule
\rowcolor[gray]{0.95} \multicolumn{9}{l}{\textit{\textbf{w.o training}}} \\
CLEAR & r=32 & 84.41\% & 74.20\% & 82.37\% & 56.23\%& 280.90 & 2567s & 0.37$\times$ \\
Tiled NATTEN & w=(30,41,41) & 84.61\% & 75.00\% & 82.69\% & 58.33\% & 269.92 & 1858s & 0.51$\times$ \\
Swin & w=(48,64,64) & 80.91\% & 71.35\% & 79.00\% & 55.81\% & 283.11 & 762s & 1.24$\times$ \\
Swin & w=(30,40,40) & 78.84\% & 72.28\% & 77.53\% & 76.49\% & 175.20 & 497s & 1.90$\times$ \\
\methodnameshort & w=(30,40,40) & 84.63\% & 73.83\% & 82.46\% & 58.33\% & 269.92 & 527s & 1.79$\times$ \\
\methodnameshort & w=(18,24,24) & 81.47\% & 77.03\% & 80.58\% & 91.00\% & 99.54 & 268s & 3.53$\times$ \\
\midrule
\rowcolor[gray]{0.95} \multicolumn{9}{l}{\textit{\textbf{w. training}}} \\
Swin & w=(30,40,40) & 77.50\% & 67.39\% & 75.48\% &  55.81\% & 283.08 & 497s & 1.90$\times$ \\
\methodnameshort & w=(30,24,40) & 85.37\% & 73.52\% & 83.00\% & 75.00\% & 182.99 & 388s & 2.44$\times$ \\
\methodnameshort & w=(18,24,24) & 84.76\% & 74.05\% & 82.62\% &  91.00\%  & 99.54 & 268s & 3.53$\times$ \\
\bottomrule
\end{tabular}%
% \vspace{-20pt}
}\label{tab:quantitative_results_on_vbench}
\end{table*}





\subsection{Finetuning Results}
\label{sec:finetuning_results}

Fine-tuning on new data introduces slight distribution shifts, meaning the same prompt may yield different, yet high-quality, video variants. Consequently, similarity metrics like PSNR become less suitable, and we instead rely on VBench\citep{huang2024vbench}, a comprehensive benchmark for video generation. We first examine the impact of directly replacing full attention with sparse attention, without tuning, to evaluate how well each algorithm approximates full 3D attention. In Table \ref{tab:quantitative_results_on_vbench}, CLEAR and Tiled NATTEN retain reasonable video quality (VBench scores of 82.37\% and 82.68\%, respectively) compared to full attention (82.71\%). However, despite sparsifying attention, these methods paradoxically increase end-to-end inference latency. Swin presents the opposite challenge: while it achieves moderate speedup (1.24$\times$–1.90$\times$), its rigid, non-overlapping window partitions prevent local queries and keys from attending to each other if they fall into separate windows, violating the 3D locality property. This results in degraded video quality, and crucially, fine-tuning with Swin attention not only fails to recover performance but further lowers the VBench score.
In contrast, \methodnameshort~addresses both quality and efficiency limitations. With a window configuration of w$_t$=(3,3,3), it achieves 91.00\% attention sparsity, yielding a 5.76$\times$ FLOPs reduction and a 3.53$\times$ actual latency reduction.\footnote{Other memory-bound operations, such as LayerNorm and modulation, likely contribute to inference overhead, preventing the full FLOPs reduction from translating directly into speedup.} Importantly, this efficiency gain comes with minimal quality tradeoff: \methodnameshort~maintains an 80.58\% VBench score in the training-free setting and improves to 82.62\% with fine-tuning.

\subsection{Results on Image Super-Resolution}
We also apply \methodnameshort~to speed up image superresolution with SDEdit~\citep{meng2022sdeditguidedimagesynthesis}. We find that FLUX with \methodnameshort~achieves comparable generation quality to CLEAR while offering significantly higher efficiency. Experiments for can be found in Appendix \ref{sec:image_hyper_resolution}.
