
\section{Further Details of ~\methodname}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1\textwidth,trim=1cm 4cm 1cm 1cm,clip]{media/appendix/QKV-VIS.pdf}
%     \caption{\textit{Left}: 2D sliding window attention with window size (3, 3). \textit{Right}: 2D \methodname~with tile size (2, 2) and window size (6, 6). SWA slides the window token by token while \methodnameshort~slides the window tile by tile. For each window, \methodnameshort~calculate the attention between the centering query tiles with the keys in the window.}
%     \label{fig:QKV_VIS}
% \end{figure*}


% \textbf{Visialization of 2D SWA and STA} In Figure~\ref{fig:QKV_VIS}, we illustrate how query tokens attend to its window key tokens. In 2D-SWA, the window slides token by token. For each window, SWA calculates the attention between the center q with all keys within the window. \methodnameshort~ defines a tile as a group of tokens in close proximity spatially. For example, we define tile in Figure~\ref{fig:QKV_VIS}(b) to be a (2, 2) square with window size (6,6) in 2D case. STA calculates the attention between all q from the tile with all keys within the window. It is a sliding window attention operating on the tile granularity: slides the window tile by tile instead of token by token.




\textbf{Mask of 3D NATTEN}
Algorithm~\ref{alg:mask_natten} defines the attention mask in 3D NATTEN. First, it computes the window center for each query token. If the query is near the video edges, the center shifts inward to stay within bounds. Next, it determines the query’s attention window within the spatiotemporal neighborhood. Finally, the mask is constructed by enforcing spatiotemporal constraints on query-key distances.
\begin{algorithm}[h]
\caption{Mask Definition of 3D NATTEN}
\label{alg:mask_natten}
\begin{algorithmic}
\REQUIRE Query coordinates $(q_t, q_h, q_w)$, Key coordinates $(k_t, k_h, k_w)$, Video size $(L_t, L_h, L_w)$, Window size $(\text{W}_t, \text{W}_h, \text{W}_w)$

\STATE \textbf{Compute window center:}
\STATE $q_{ct} \gets \max\left(\min\left(q_t, L_t - 1 - \frac{\text{W}_t}{2} \right), \frac{\text{W}_t}{2} \right)$
\STATE $q_{ch} \gets \max\left(\min\left(q_h, L_h - 1 - \frac{\text{W}_h}{2} \right), \frac{\text{W}_h}{2} \right)$
\STATE $q_{cw} \gets \max\left(\min\left(q_w, L_w - 1 - \frac{\text{W}_w}{2} \right), \frac{\text{W}_w}{2} \right)$

\STATE \textbf{Compute masks:}
\STATE $\text{time\_constraint} \gets |q_{ct} - k_t| \leq \frac{\text{W}_t}{2}$
\STATE $\text{hori\_constraint} \gets |q_{ch} - k_h| \leq \frac{\text{W}_h}{2}$
\STATE $\text{vert\_constraint} \gets |q_{cw} - k_w| \leq \frac{\text{W}_w}{2}$

\STATE \textbf{return} $\text{time\_constraint} \land \text{hori\_constraint} \land \text{vert\_constraint}$
\end{algorithmic}
\end{algorithm}

\textbf{Mask of 3D STA}
Algorithm~\ref{alg:mask_STA} defines the mask for \methodnameshort, introducing a tile-based coordinate framework that differs from 3D NATTEN. First, query and key coordinates are mapped to tile coordinates, where each QK pair is assigned a tile ID, with queries and keys in the same tile sharing the same ID. \methodnameshort~also computes the window center within tile coordinates, ensuring queries remain within valid bounds. Finally, neighboring keys are selected based on their tile distance from the query’s window center.
\begin{algorithm}[h]
%\begin{minipage}{0.95\linewidth}
\caption{Mask Definition of 3D \methodnameshort}
\label{alg:mask_STA}
\begin{algorithmic}
\REQUIRE Query coordinates $(q_t, q_h, q_w)$, key coordinates $(k_t, k_h, k_w)$, video size $(L_t, L_h, L_w)$, kernel size $(\text{W}_t, \text{W}_h, \text{W}_w)$, tile size $(\text{T}_t, \text{T}_h, \text{T}_w)$ 
\STATE \textbf{Compute QK coordinates in:}
\STATE $q_{t,\text{tile}} \gets q_t // T_t$
\STATE $q_{h,\text{tile}} \gets q_h // T_h$
\STATE $q_{w,\text{tile}} \gets q_w // T_w$
\STATE $k_{t,\text{tile}} \gets k_t // T_t$
\STATE $k_{h,\text{tile}} \gets k_h // T_h$
\STATE $k_{w,\text{tile}} \gets k_w // T_w$
\STATE \textbf{Compute window size in tiles:}
\STATE $\text{W}_{t,\text{tile}} \gets \text{W}_t // T_t$
\STATE $\text{W}_{h,\text{tile}} \gets \text{W}_h // T_h$
\STATE $\text{W}_{w,\text{tile}} \gets \text{W}_w // T_w$

\STATE \textbf{Compute window center:}
\STATE $q_{ct} \gets \max\left(\min\left(q_{t,\text{tile}}, (L_t // T_t - 1) - \frac{\text{W}_{t,\text{tile}}}{2}\right), \frac{\text{W}_{t,\text{tile}}}{2} \right)$
\STATE $q_{ch} \gets \max\left(\min\left(q_{h,\text{tile}}, (L_h // T_h - 1) - \frac{\text{W}_{h,\text{tile}}}{2}\right), \frac{\text{W}_{h,\text{tile}}}{2} \right)$
\STATE $q_{cw} \gets \max\left(\min\left(q_{w,\text{tile}}, (L_w // T_w - 1) - \frac{\text{W}_{w,\text{tile}}}{2}\right), \frac{\text{W}_{w,\text{tile}}}{2} \right)$

\STATE \textbf{Compute masks:}
\STATE $\text{time\_constraint} \gets |q_{ct} - k_{t,\text{tile}}| \leq \frac{\text{W}_{t,\text{tile}}}{2}$
\STATE $\text{hori\_constraint} \gets |q_{ch} - k_{h,\text{tile}}| \leq \frac{\text{W}_{h,\text{tile}}}{2}$
\STATE $\text{vert\_constraint} \gets |q_{cw} - k_{w,\text{tile}}| \leq \frac{\text{W}_{w,\text{tile}}}{2}$

\STATE \textbf{return} $\text{time\_constraint} \land \text{hori\_constraint} \land \text{vert\_constraint}$
\end{algorithmic}
\end{algorithm}


\textbf{Tiling in ~\methodnameshort} Figure~\ref{fig:reorder} illustrates \methodnameshort's token tiling and ordering mechanism in a 2D scenario, which extends naturally to 3D. Unlike conventional approaches that flatten 2D/3D data into 1D sequences using a zigzag pattern, \methodnameshort~organizes tokens into tiles, ensuring that tokens within a tile maintain neighboring sequence IDs. This ordering strategy preserves locality, so when a tile attends to another tile, the resulting attention map forms a dense block, as all participating sequence IDs remain consecutive.



\begin{figure}[h]
    \centering
    \includegraphics[width=0.9 \columnwidth]{media/appendix/reorder.png} % Replace 'example-image' with your file name
    \caption{\textit{Left}: Conventional zigzag flattening strategy. \textit{Right}: \methodnameshort' sequence flattening strategy. The plot is given assuming a (9, 9) image with (3, 3) tile size.}
    \label{fig:reorder}
\end{figure}

\textbf{Visialization of 2D SWA} In Figure~\ref{fig:f6_swa}, we illustrate how query tokens attend to its window key tokens. In 2D-SWA, the window slides token by token. For each window, SWA calculates the attention between the center q with all keys within the window.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9 \columnwidth]{media/Fig6/f6-swa.png} % Replace 'example-image' with your file name
    \caption{2D Sliding Window Attention visualization.}
    \label{fig:f6_swa}
\end{figure}



\section{Finetuning Details}
\label{sec:appendix_finetuning}
We train on 2,000 synthetically generated videos from HunyuanVideo at a resolution of 1280×768 with 117 frames. The prompts are sourced from the Mixkit dataset~\citep{lin2024open}. To reduce memory usage and accelerate training, we precompute VAE-encoded latents and text encoder states. Training involves fine-tuning for 1,600 steps with a batch size of 2 and a learning rate of 2e-5. We optimize using the loss function from Eq.~\eqref{eq:mix_loss} with coefficients $\alpha=1$, $\beta=0.5$, and $\gamma=0.5$. To prevent overfitting on a single guidance scale, we alternate between guidance scales of 1 and 6 at odd and even steps. The entire process runs on 8 H100 GPUs with FSDP and context parallelism for training (8 hours) and sequence parallelism for inference.
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1 \columnwidth]{media/Fig5/f5-pdf.pdf} % Replace 'example-image' with your file name
%     \caption{Training Dynamics. \textcolor{red}{Yongqi}}
%     \label{fig:training_dynamics}
% \end{figure}





% Our experiments demonstrate that our \methodname achieves a 10× speedup in attention computation and 3× end-to-end acceleration during inference while preserving the visual quality and temporal consistency of the generated videos.




\section{Further Details of Baselines}
\label{sec:baseline}
\textbf{Swin Transformer}~\citep{liu2021swintransformerhierarchicalvision} introduces a hierarchical vision transformer with a shifted window-based attention mechanism. Instead of computing self-attention globally, it partitions the image into non-overlapping windows and applies attention locally, improving computational efficiency. A key innovation is the alternating window partitioning strategy: one layer uses standard window partitioning, while the next shifts the windows to enable cross-window connections and better information exchange. Swin attention is typically used in a train-from-scratch setting. A limitation of this approach is that it disrupts local connectivity within a single attention layer. Tokens in adjacent regions may not attend to each other if they fall into separate windows. In this paper, we apply Swin attention to HunyuanVideo and shift the window every other layer accordingly.


\textbf{CLEAR}~\citep{liu2024clearconvlikelinearizationrevs} achieves linear attention by replacing the original full attention with a circular window-based attention mechanism where each query token only attends to key-value tokens within a radius r, maintaining the same scaled dot-product attention formula but restricting its computation to local windows. The authors implement CLEAR with FlexAttention. 


\textbf{$\Delta$-DiT}~\citep{chen2024delta} optimizes inference speed by caching feature offsets instead of full feature maps. It employs a staged caching strategy: residuals from later DiT blocks are stored for early-step sampling, while residuals from earlier blocks are cached for later steps. The key parameters in \(\Delta\text{-DiT}\) include the residual cache interval \(N\), the number of cached blocks \(N_c\), and the timestep boundary \(b\), which determines the cache position. Since the official \(\Delta\text{-DiT}\) implementation is unavailable, we reimplemented its method based on the paper to accelerate video generation. Given a speedup budget, we vary \(N_c\) , \(N\), and \(b\) to pick the best hyperparameters, ensuring a fair evaluation of its effectiveness. For the 50-step 1.36$\times$ speedup, we set \(N_c=24\), \(N=3\), and \(b=24\). For 1.8$\times$ speedup, we set  \(N_c=28\), \(N=6\), and \(b=24\).


\section{Results on Image Super-Resolution}
\label{sec:image_hyper_resolution}

\begin{table}[h]
\caption{Image superresolution results with FLUX~\cite{flux2023} on 1000 captions randomly sampled from COCO-2014~\citep{lin2015microsoftcococommonobjects} validation dataset.}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccccc}
    \toprule
    \textbf{Methods}  & \textbf{SSIM} & \textbf{PSNR} & \textbf{Sparsity} & \textbf{Latency} & \textbf{Speedup} \\ 
    \midrule
    \rowcolor[gray]{0.95} \multicolumn{6}{l}{\textit{\textbf{1K $\rightarrow$2K}}} \\ 
    CLEAR r=16  & 0.9291 & 28.1142 &  96.12\% & 13s & $1.54\times$ \\
    CLEAR r=32  & 0.9443 & 29.6722 &  85.94\% & 15s & $1.33\times$ \\
    \methodnameshort~w=(48,72) & 0.9357 & 29.1086 & 81.25\% &14s & $1.43\times$ \\ 
    \midrule
    \rowcolor[gray]{0.95} \multicolumn{6}{l}{\textit{\textbf{2K$\rightarrow$4K}}} \\ 
    CLEAR r=16 & 0.9394 & 29.0463 &  98.98\% & 67s & $2.90\times$ \\
    CLEAR r=32 & 0.9455 & 30.0742 &  96.08\% & 92s & $2.11\times$ \\
    \methodnameshort~w=(48,72) & 0.9470 & 30.1939 &  95.31\% & 57s & $3.40\times$ \\
    \bottomrule
\end{tabular}}
\label{tab:flexattention_scaling}
\end{table}


\section{More Experiment Results}



\subsection{Kernel Performance}
We additionally benchmark various sparse attention kernels at a sparsity level of around 56\% and present the results in Table \ref{tab:benchmask_sta_appendix}. With lower sparsity, sparse kernels generally have a higher MFU, but the findings in Table \ref{tab:benchmark_attn_table1} remain unchanged. 


\begin{table*}[h!]
\centering
\caption{Speedup with sparse attention kernels on H100.}
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{Methods} & \textbf{Implementation} & \textbf{Config} & \textbf{Sparsity} & \textbf{TFLOPS} & \textbf{Latency(ms)} & \textbf{MFU} & \textbf{Kernel Efficiency} & \textbf{Speedup} \\
\midrule
FA 3 & ThunderKittens & - & 0.00\% & 164.03 & 265.28 & 62.49\% & 100.00\% & 1.00× \\
FA 3 & CUDA & - & 0.00\% & 164.03 & 256.59 & 64.61\% & 103.39\% & 1.03× \\
\midrule
CLEAR & FlexAttention & r=32 & 56.23\% & 71.80 & 675.05 & 10.75\% & 17.20\% & 0.39× \\
NATTEN & FlexAttention & w=(30,41,41) & 56.22\% & 71.81 & 804.62 & 9.02\% & 14.43\% & 0.33× \\
Tiled NATTEN & CUDA & w=(29,41,41) & 57.68\% & 69.41 & 173.57 & 4.04\% & 6.47\% & 0.15x \\
Tiled NATTEN & FlexAttention & w=(30,41,41) & 56.22\% & 71.81 & 409.89 & 17.70\% & 28.33\% & 0.65× \\

Swin & FlexAttention & w=(48,64,64) & 55.81\% & 72.49 & 127.51 & 57.46\% & 91.95\% & 2.08× \\
\midrule
\methodnameshort & FlexAttention & w=(30,40,40) & 58.33\% & 68.35 & 174.17 & 39.66\% & 63.46\% & 1.52× \\
\methodnameshort & ThunderKittens & w=(30,40,40) & 58.33\% & 68.35 & 111.73 & 61.82\% & 98.93\% & 2.37× \\
\bottomrule
\end{tabular}}
\label{tab:benchmask_sta_appendix}
\end{table*}


\subsection{Detailed VBench Results}

In Tables \ref{tab:model-comparison-1} and \ref{tab:model-comparison-2}, we present detailed comparisons of VBench scores across key dimensions originally summarized in Table \ref{tab:quantitative_results_on_vbench}. Our analysis reveals that \methodnameshort\  surpasses swin attention in video quality metrics such as Imaging Quality and Multiple Objects, while achieving comparable or superior scores to CLEAR and Tiled NATTEN. For training-free models, we observe a systematic degradation in quality-related metrics (e.g., temporal flickering, motion smoothness) as sparsity increases in the \methodnameshort{} attention mechanism. Conversely, semantic-aligned dimensions—including Appearance Style, Color, and Spatial Relationships—improve under higher sparsity regimes, a phenomenon driven by the text embeddings’ amplified role in attention computation when spatial-temporal attention is sparsified. Furthermore, the trained \methodnameshort\ demonstrates significant gains in video quality metrics over its untrained counterpart, while maintaining semantic coherence at comparable levels which underscores the efficacy of training in refining low-level visual fidelity without compromising text-video alignment.

\begin{table*}[h!]
\centering
\caption{Model Performance Comparison - Part 1}
\label{tab:model-comparison-1}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccccccc}
\hline
\textbf{Model} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Subject} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} & \makecell{\textbf{Temporal} \\ \textbf{Flickering}} & \makecell{\textbf{Motion} \\ \textbf{Smoothness}} & \makecell{\textbf{Dynamic} \\ \textbf{Degree}} & \makecell{\textbf{Aesthetic} \\ \textbf{Quality}} & \makecell{\textbf{Imaging} \\ \textbf{Quality}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} \\
\hline
FA3 & 18.43\% & 94.22\% & 96.74\% & 99.21\% & 99.15\% & 75.00\% & 64.63\% & 67.97\% & 25.96\% \\
\midrule
\rowcolor[gray]{0.95} \multicolumn{10}{l}{\textit{\textbf{w.o training}}} \\
CLEAR & 18.73\% & 93.63\% & 96.51\% & 98.99\% & 99.01\% & 68.06\% & 63.75\% & 68.35\% & 26.23\% \\
Tiled NATTEN & 18.79\% & 94.59\% & 96.61\% & 98.75\% & 98.85\% & 70.83\% & 63.79\% & 68.16\% & 26.53\% \\
Swin w=(48,64,64) & 20.85\% & 91.74\% & 95.48\% & 98.67\% & 97.77\% & 77.78\% & 51.01\% & 62.22\% & 25.27\% \\
Swin w=(30,40,40) & 20.62\% & 90.33\% & 93.09\% & 98.78\% & 96.53\% & 75.00\% & 48.10\% & 61.89\% & 25.62\% \\
\methodnameshort\ w=(30,40,40) & 18.79\% & 94.75\% & 96.50\% & 98.82\% & 98.83\% & 69.44\% & 64.18\% & 68.39\% & 26.47\% \\
\methodnameshort\ w=(18,24,24) & 21.25\% & 89.66\% & 91.64\% & 98.46\% & 97.27\% & 83.33\% & 59.75\% & 64.23\% & 26.61\% \\
\midrule
\rowcolor[gray]{0.95} \multicolumn{10}{l}{\textit{\textbf{w. training}}} \\
Swin w=(30,40,40) & 20.07\% & 89.78\% & 94.93\% & 98.86\% & 96.64\% & 70.83\% & 44.91\% & 55.99\% & 26.00\% \\
\methodnameshort\ w=(30,24,40) & 18.90\% & 94.90\% & 97.60\% & 99.68\% & 99.23\% & 73.61\% & 63.77\% & 66.21\% & 26.58\% \\
\methodnameshort\ w=(18,24,24) & 18.90\% & 94.64\% & 96.76\% & 99.22\% & 99.11\% & 69.44\% & 64.52\% & 66.67\% & 26.09\% \\
\hline
\end{tabular}
}
\end{table*}

\begin{table*}[h!]
\centering
\caption{Model Performance Comparison - Part 2}
\label{tab:model-comparison-2}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccccc|ccc}
\hline
\textbf{Model} & \makecell{\textbf{Object} \\ \textbf{Classification}} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} & \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color} & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Quality} \\ \textbf{Score}} & \makecell{\textbf{Semantic} \\ \textbf{Score}} & \makecell{\textbf{Final} \\ \textbf{Score}} \\
\hline
FA3 & 85.76\% & 70.12\% & 90.00\% & 88.66\% & 71.28\% & 35.25\% & 85.34\% & 72.17\% & 82.71\% \\
\midrule
\rowcolor[gray]{0.95} \multicolumn{10}{l}{\textit{\textbf{w.o training}}} \\
CLEAR & 88.13\% & 77.97\% & 88.00\% & 91.10\% & 77.49\% & 32.85\% & 84.41\% & 74.20\% & 82.37\% \\
Tiled NATTEN & 83.54\% & 72.18\% & 94.00\% & 92.28\% & 81.21\% & 37.94\% & 84.61\% & 75.00\% & 82.69\% \\
Swin w=(48,64,64) & 78.16\% & 58.54\% & 87.00\% & 93.68\% & 77.45\% & 37.79\% & 80.91\% & 71.35\% & 79.00\% \\
Swin w=(30,40,40) & 79.19\% & 60.44\% & 88.00\% & 93.68\% & 77.24\% & 35.54\% & 78.84\% & 72.28\% & 77.53\% \\
\methodnameshort\ w=(30,40,40) & 80.54\% & 71.19\% & 93.00\% & 89.81\% & 79.25\% & 36.77\% & 84.63\% & 73.83\% & 82.47\% \\
\methodnameshort\ w=(18,24,24) & 88.13\% & 75.46\% & 91.00\% & 91.61\% & 82.52\% & 42.15\% & 81.47\% & 77.03\% & 80.58\% \\
\midrule
\rowcolor[gray]{0.95} \multicolumn{10}{l}{\textit{\textbf{w. training}}} \\
Swin w=(30,40,40) & 77.14\% & 48.86\% & 73.00\% & 87.00\% & 63.38\% & 39.03\% & 77.50\% & 67.39\% & 75.48\% \\
\methodnameshort\ w=(30,24,40) & 91.77\% & 68.45\% & 86.00\% & 89.59\% & 72.76\% & 39.53\% & 85.37\% & 73.52\% & 83.00\% \\
\methodnameshort\ w=(18,24,24) & 92.96\% & 74.16\% & 93.00\% & 84.50\% & 73.41\% & 38.23\% & 84.76\% & 74.05\% & 82.62\% \\
\hline
\end{tabular}
}
\end{table*}

\newpage

% \section{Qualitative Examples}
% % We show qualitatively show videos generated by the original HunyuanVideo, STA, and $\Delta$-DiT in Figure \ref{fig:qualitative_examples1} and Figure \ref{fig:qualitative_examples2}.  While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by $\Delta$-DiT are generally less sharp than those generated by the original HunyuanVideo and ~\methodnameshort.

\newpage
\section{Qualitative Examples}
\label{sec:qualitative_example}
We show qualitatively show videos generated by the original HunyuanVideo, STA, and $\Delta$-DiT in Figure \ref{fig:qualitative_examples1} and Figure \ref{fig:qualitative_examples2}.  While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by $\Delta$-DiT are generally less sharp than those generated by the original HunyuanVideo and ~\methodnameshort.

\begin{figure*}[h]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.9\textwidth,trim=3.3cm 0.8cm 5.2cm 0.25cm,clip]{media/appendix/demo1.pdf}
    \end{subfigure}
    % \vspace{0em}  % Adjust spacing between figures as needed
    \begin{subfigure}
        \centering
        \includegraphics[width=0.9\textwidth,trim=3.1cm 0.8cm 5cm 0.22cm,clip]{media/appendix/demo2.pdf}
    \end{subfigure}
    \caption{Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by $\Delta$-DiT are generally less sharp than those generated by the original HunyuanVideo and ~\methodnameshort.}
    \label{fig:qualitative_examples1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.9\textwidth,trim=3.1cm 0.2cm 5cm 0.25cm,clip]{media/appendix/demo4.pdf}
    \end{subfigure}
    % \vspace{0em}  % Adjust spacing between figures as needed
    \begin{subfigure}
        \centering
        \includegraphics[width=0.88\textwidth,trim=3.1cm 0.2cm 5cm 0.25cm,clip]{media/appendix/demo5.pdf}
    \end{subfigure}
    \caption{Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by $\Delta$-DiT are generally less sharp than those generated by the original HunyuanVideo and ~\methodnameshort.}
    \label{fig:qualitative_examples2}
\end{figure*}



% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.9\textwidth,trim=3.1cm 0.2cm 5cm 0.25cm,clip]{media/appendix/demo4.pdf}
%     \caption{Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by $\Delta$-DiT are generally less sharp than those generated by the original HunyuanVideo and ~\methodnameshort.}
%     \label{fig:qualitative_examples2}
% \end{figure*}