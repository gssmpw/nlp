\label{sec:introduction}

Diffusion Transformers (DiTs) have emerged as the leading architecture for high-resolution video generation, capable of synthesizing long-duration, visually coherent outputs~\citep{peebles2023scalable, openai_sora}. Central to their success is 3D attention mechanism, which models spatial and temporal dependencies by flatterning video frames as a unified sequence of visual tokens~\citep{yang2024cogvideox, genmo2024mochi, kong2025hunyuanvideosystematicframeworklarge}. 
However, this design introduces significant computational overhead due to the quadratic complexity of attention, making training and inference prohibitively slow as video resolutions and durations increase. As illustrated in Figure \ref{fig:kernel_sparsity_speedup}, attention computation dominates the overall inference cost. 
Even with FlashAttention 3 (FA3)~\cite{shah2024flashattention3fastaccurateattention} and a high-end H100 GPU, HunyuanVideo~\citep{kong2025hunyuanvideosystematicframeworklarge} still requires \emph{16 minutes} to generate a 5-seconds 720p video. This bottleneck severely limits the practical deployment of DiTs.
% especially in real-time applications.

% \begin{figure}[t]


\begin{figure}[t]
   \centering
   \includegraphics[width=1.15\columnwidth,trim=2.85cm 18cm 0.55cm 2cm,clip]{media/Fig1/figure1-3.pdf}
   \caption{(a) Generating a 5s 720P clip in Hunyuan involves processing 115K tokens, making attention the dominant cost. (b) Attention latency comparison: existing methods fail to translate FLOP reduction into wall-clock speedup; \methodnameshort~is hardware-efficient and achieves proportional speedup with sparsity.}
   \vspace{-15pt}
   \label{fig:kernel_sparsity_speedup}
\end{figure}



% \begin{figure}[t]
%     \raggedleft % Align figures to the right
%     \begin{minipage}[t]{0.48\columnwidth} % Right half container
%         \centering
%         % Left subfigure
%         \begin{subfigure}[t]{0.48\textwidth}
%             \includegraphics[width=0.8\linewidth]{media/attn_seqlen.png}
%             \caption{First figure}
%             \label{fig:left}
%         \end{subfigure}%
%         \hfill
%         % Right subfigure
%         \begin{subfigure}[t]{0.48\textwidth}
%             \includegraphics[width=0.8\linewidth]{media/latency_density_comparison.png}
%             \caption{Second figure}
%             \label{fig:right}
%         \end{subfigure}
%         \caption{Combined caption for both figures}
%         \label{fig:pair}
%     \end{minipage}
% \end{figure}

%\begin{figure*}[!htp]
%    \centering
%    \includegraphics[width=2.0\columnwidth]{media/attn_locality_spatialization.png} % Replace 'example-image' with your file name
%    \caption{Attention locality and head specialization.\textcolor{red}{Hangliang}}
%    \label{fig:attn_locality}
%\end{figure*}

Video data inherently exhibit high redundancy -- adjacent frames exhibit minimal differences, and spatially close pixels tend to have stronger correlations. This redundancy suggests that treating every token independently in 3D attention may be unnecessarily expensive. In this paper, we hypothesize that such redundancies are carried by 3D full attention in pretrained video diffusion models, which, if properly exploited, can drastically accelerate inference. To verify this, we visualize the attention scores of HunyuanVideo in Figure \ref{fig:visualize_locality}.
The results reveal an intriguing \textit{3D locality} pattern: queries assign significantly higher attention scores to spatially and temporally nearby keys. 
To quantify this effect, we compute attention recall, measuring the fraction of total attention scores concentrated within a local window. 
As shown in Figure \ref{fig:attention_analysis} left, despite training with full 3D attention, HunyuanVideo exhibits strong locality: on average, a local window covering only 15.52\% of the total token space accounts for 70\% of the total attention score.

% Numerous works have been proposed to speed up attention computation. One line of work focuses on sparsifying the attention map~\citep{roy2020efficientcontentbasedsparseattention, liu2024clearconvlikelinearizationrevs,hassani2024faster,zhu2023biformervisiontransformerbilevel}. However, as illustrated in Figure  \ref{fig:kernel_sparsity_speedup},  reducing theoretical FLOPS does not always translate to lower wall-clock latency. 

% As a result, exact attention with aggressive hardware optimization -- Flash Attention~\citep{dao2022flashattentionfastmemoryefficientexact}, remains the preferred choice for DiTs~\citep{peebles2023scalable} and ViTs~\citep{dosovitskiy2021imageworth16x16words}.  

% Another line of work factorized the 3D attention into spatial and temporal components~\citep{singer2022makeavideotexttovideogenerationtextvideo, chen2023videocrafter1opendiffusionmodels, ma2024latte, wang2023lavie}. However, those methods were quickly replaced by full 3D attention in video model development~\citep{opensora,lin2024open, genmo2024mochi, kong2025hunyuanvideosystematicframeworklarge}. We hypothesize that this shift occurred because factorized spatial-temporal attention cannot attend to tokens that are off by one in both spatial and temporal planes~\footnote{For example, a query at coordinate (1, 1, 1) cannot attend to a key at (2, 2, 2) in a single factorized spatial-temporal attention layer, even though the tokens are close in space.}.


\begin{figure}[t!]
    \centering
    \begin{minipage}[b]{1\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{media/Fig2/visualize_locality.pdf}
    \end{minipage}
    \caption{Visualization of attention locality. The green point means the query point and the magma-colored regions indicate areas of high attention values in response to the query. Instead of attending to the entire image, the query's attention forms a concentrated local hotspot.}
    \label{fig:visualize_locality}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{media/Fig2/avg_recall.pdf}
       % \text{(a) SBA recall rate}
    \end{minipage}
    \hfill
    %\begin{minipage}[b]{0.32\columnwidth}
    %    \centering
    %    \includegraphics[width=\textwidth]{media/Fig2/cross_step_std.pdf}
    %    \text{(b) Cross-step stability}
    %\end{minipage}
    %\hfill
    \begin{minipage}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{media/Fig2/cross_prompt_std.pdf}
        %\text{(b) Data-independent}
    \end{minipage}    
    \caption{\textit{Left}: Fraction of attention scores within a (12, 24, 24) local window across diffusion steps and 10 different prompts. Most heads show high recall, indicating a local attention pattern.
\textit{Right}: Despite the different recall across heads, the standard deviation across prompts remains low.}
    \label{fig:attention_analysis}
\end{figure}




This observation seemingly suggests that sliding window attention (SWA) is an ideal alternative to retain attention expressiveness while reducing computational cost. However, existing 2D or 3D SWA implementations, such as NATTEN~\citep{hassani2023neighborhood} and CLEAR~\citep{liu2024clearconvlikelinearizationrevs}, fail to translate FLOP reductions into proportional wall-clock speedups, as shown in Figure~\ref{fig:kernel_sparsity_speedup}b. Their inefficiency arises because higher-order (2D/3D) sliding window attention creates a highly irregular attention mask, wasting many computations and generating significant masking overhead. The computation pattern for SWA is inherently \emph{GPU-unfriendly}, resulting in poor hardware utilization (see \S\ref{sec:3.1}).  

To overcome this, we develop \methodname~(\methodnameshort), a hardware-aware attention mechanism that rethinks sliding window computation via system-algorithm co-design. We define a tile as a contiguous group of tokens forming a spatial-temporal cube, with its size determined by the block size in FlashAttention.Instead of sliding over contiguous tokens, \methodnameshort~operates tile-by-tile, enabling efficient memory access and parallelism while effectively preserving the 3D locality.
% a co-design of algorithm and system that modifies the sliding mechanism to operate tile-by-tile rather than token-by-token. 
% This small but critical adjustment enables efficient hardware implementation while preserving the \textit{3D locality} pattern. 
Inspired by FA3, \methodnameshort~adopts a consumer-producer paradigm, where producer warpgroups \emph{asynchronously} load data from HBM to SRAM while consumer warpgroups compute attention. 
% Our \methodname~kernel adopts a consumer-producer paradigm. Producer warp groups asynchronously transfer data from HBM to shared memory, while consumer warp groups handle computation~\citep{shah2024flashattention3fastaccurateattention, spector2024thunderkittenssimplefastadorable}. 
Because \methodnameshort~slides over tiles instead of individual tokens, it eliminates the need for explicit attention masking at computation, a significant overhead observed in other SWA implementations.
The sparse attention mask is managed entirely by the producer warpgroups; hence, the computation on consumer wrap groups remains dense and hardware-efficient.
As a result, \methodnameshort~is the first higher-order sliding-window-like attention to achieve wallcock speedups proportional to sparsity (Figure~\ref{fig:kernel_sparsity_speedup} (b)). 

Besides efficient computation, selecting optimal window sizes is crucial to preserving generation quality.
% To realize faster video generation with no quality loss, we still need to learn how to configure an optimal window size.
We find that different attention heads exhibit specialized locality patterns -- some heads focus on finer details in a small area, yet others capture broader context at a larger window -- which we term as \emph{head specialization}. Importantly, this head specialization remains agonistic to prompts, as evidenced in Figure~\ref{fig:attention_analysis}. Based on this property, we develop a simple yet effective method to automatically configure the optimal window size \emph{per head} via profiling, striking a balance between efficiency and quality.
% From Figure \ref{fig:attention_analysis}, we discover that the attention recall score varies significantly across attention heads, but their value remains stable across input prompts.  We term this phenomenon \textit{head specialization}-- different heads have different intrinsic window sizes. 
% Since the attention pattern is stable across prompts, we only need to determine the effective attention range of each head and configure \methodnameshort~to the appropriate window size. We develop simple heuristics to efficiently identify the optimal window size. 
With \methodnameshort, HunyuanVideo can generate a 5-second 720P video in 695s or 578s with no or minimal quality loss in a plug-and-play manner. In comparison, HunyuanVideo with FlashAttention-2 takes 1496s, while FlashAttention-3 takes 945s. STA achieves a end-to-end speedup of 2.15–2.59× over FlashAttention-2 and 1.35–1.63× over FlashAttention-3. Additionally, by fine-tuning diffusion models under more radical attention sparsity, we unlock even greater efficiency, delivering a 2.43 - 3.53$\times$ end-to-end speedup compared with FlashAttention-3.

This paper makes the following contributions: (1) We identify and quantify \textit{3D locality} and \textit{head specialization} in state-of-the-art video DiTs, revealing substantial redundancy in full 3D attention. (2) We introduce \methodname, a tile-based sliding window attention mechanism. Our optimized kernel achieves minimum overhead compared to FlashAttention 3 with an MFU of 58.79\%. (3) \methodnameshort~accelerates attention by $>10\times$ and end-to-end video generation by up to 3.53$\times$ with no or minimum quality loss.



