





\subsection{\methodname}


In vanilla sliding window attention, each query attends to a local window centered around it, resulting in different queries attending to distinct key groups. This lack of shared attention key groups is the root cause of irregularities in SWAâ€™s attention map, creating mixed blocks. We propose \methodname~(\methodnameshort), a novel sliding window attention variant that exclusively operates on dense blocks and empty blocks. As shown in Figure \ref{fig:sta_illustration}, \methodnameshort~organizes queries and keys into tiles. All queries in the same tile attend to the same set of keys within their common local window,  ensuring a more structured attention pattern. By setting the tile area equal to the block size in FlashAttention and arranging queries in a tile with consecutive token indices, we form dense FlashAttention blocks. This design allows each query tile to attend densely to key tiles within the window, eliminating mixed blocks and improving compute efficiency. We illustrate \methodnameshort~attention mask in Figure \ref{fig:attention_map} (c).


\begin{table}[t]
\vspace{-15px}
\caption{Ratio of dense and mixed blocks for tiled NATTEN and \methodnameshort~ with tile size (4,4,4) and video size (48,48,48). \methodnameshort~generate only dense blocks, which is more computationally friendly than mixed blocks in GPU.}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lccc}
    \toprule
    \textbf{Attention}  & \textbf{Window Size} & \textbf{Dense Block} & \textbf{Mixed Block} \\ 
    \midrule
    Tiled NATTEN &(11,11,11) & 0.06\% & 7.17\% \\ 
    \methodnameshort & (12, 12, 12) & 1.56\%  & 0.0\%\\ 
    \methodnameshort & (20, 20, 20) & 7.23\%  & 0.0\%\\ 
    \bottomrule
\end{tabular}}
\label{tab:block_sparsity_comparison}
\end{table}

Formally, for 3D STA, given a video of dimension $(L, L, L)$ and a FlashAttention block size of $(B, B)$, \methodnameshort~sets the tile size $T$ such that $B = T^3$. It further assumes that both the video size $L$ and window size $W$ are integer multiples of $T$. The video is partitioned into non-overlapping tiles of size $(T, T, T)$, and flattened into 1D sequence in a way that tokens within the same tile have consecutive sequence indices. Conceptually, \methodnameshort~slides the window with a step size of $(T, T, T)$. For each step, it computes attention between the central query tiles and all key tiles within the window, producing $\left( \frac{W}{T} \right)^3$ dense attention blocks without mixed blocks. 






To demonstrate \methodnameshort~superiority in creating a \textit{GPU-friendly} compute pattern,  we give the following formula to quantitatively measure the different types of blocks in 3D Tiled NATTEN and 3D \methodnameshort. 





\begin{theorem}
\label{thm:natten_mixed}
Consider a tiled NATTEN configuration with tile size $(T, T, T)$, window size $(W, W, W)$, and video size $(L, L, L)$. Let the FA block size be $(B, B)$, where $B = T^3$. Ignoring boundary effects, the number of dense blocks is given by:
\[
N_{\mathrm{dense}} = \left( \max\!\Bigl( 2 \Bigl\lfloor \frac{W + 1}{2T} \Bigr\rfloor - 1, 0 \Bigr) \right)^3 \cdot \left(\frac{L}{T}\right)^3.
\]
The number of mixed blocks in tiled NATTEN is:
\[
N_{\mathrm{mix}} = \left( 2 \left\lceil \frac{W - 1}{2T} \right\rceil + 1 \right)^3 \cdot \left(\frac{L}{T}\right)^3 - N_{\mathrm{dense}}.
\]
\end{theorem}
Intuitively, for a block to be dense in NATTEN, the window size should be at least twice the size of the tile size, such that the left-most query in the tile can attend to the right-most query. On the other hand, the left-most query in a tile can still attend to keys that are $\frac{W - 1}{2T}$ tiles further left, creating mixed blocks.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7 \columnwidth]{media/Fig6/f6-sta.png} % Replace 'example-image' with your file name
    \caption{2D \methodname~with tile size (2, 2) and window size (6, 6). After attending to all the key tiles, each query tile will generate nine 4x4 dense blocks in the attention map. We showcase 2D STA for better illustration. 3D STA can be inferred similarly.}
    \label{fig:sta_illustration}
\end{figure}


\begin{theorem}
\label{thm:sile_sparsity}
With the same notation, if $W$ is an integer multiple of $T$, the number of dense blocks in \methodname~is:
\[
S_{\mathrm{dense}} = \left( \frac{W}{T} \right)^3 \cdot \left(\frac{L}{T}\right)^3.
\]
All remaining blocks are empty and there is no mixed blocks.
\end{theorem}
Intuitively, each query tile will only attend to its local window in ~\methodnameshort, which has $\left( \frac{W}{T} \right)^3$ tiles of keys, creating the same number of blocks in the attention map. We apply Theorem \ref{thm:natten_mixed} and  Theorem \ref{thm:sile_sparsity} to calculate the ratio of different blocks and report them in Table \ref{tab:block_sparsity_comparison}.

\noindent \textbf{Kernel-level optimization.}
\methodnameshort~can be efficiently implemented with FlexAttention, which provides enough functionality to skip all empty blocks and avoid adding unnecessary intra-block mask on the dense blocks. We can further optimize the sparse attention masks by disaggregating the inter-block mask logic from the compute kernels. Thus, we implement our attention kernels based on ThunderKittens~\citep{spector2024thunderkittenssimplefastadorable} and FlashAttention3~\citep{shah2024flashattention3fastaccurateattention}.  Our implementation split the threadblock into compute warpgroups and data warpgroups, and the inter-block mask is completely managed by the data warpgroups.  Each compute warpgroup is responsible for calculating one query block, which always resides in the SRAM (Split-Q~\cite{dao2023flashattention2fasterattentionbetter}). The data warpgroup is responsible for asynchronously loading the KV blocks from HBM to SRAM. For each block of query, the data warpgroup needs to decide which key and value blocks the query block will attend to in \methodnameshort~ and only load those blocks. Since the data warpgroups are asynchronous, the overhead of calculating the inter-block mask in \methodnameshort~ and deciding which data to load can be hidden with overlapping. On the other hand, the compute worker is completely oblivious of the sparse attention pattern. It performs attention computation with the key value blocks in shared memory loaded by data workers, and once all data is consumed in the circular cache, the computation is finished. 


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.05\textwidth]{media/Fig4/figure4-2.png}
%     \caption{Comparison of attention mechanisms}
%     \label{fig:4}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{\textwidth}  % Explicit width
%         \centering
%         \includegraphics[width=0.25\textwidth]{media/Fig4/f_1DSWA.png}
%         \hspace{0.05\textwidth}
%         \includegraphics[width=0.65\textwidth]{media/Fig4/f4-2.png}  % Reduced from 0.8
%     \end{subfigure}
%     \vspace{1em}
%     \caption{Comparison of attention mechanisms}
%     \label{fig:4}
% \end{figure*}





\subsection{Applying \methodnameshort~to Video Diffusion Model}

\begin{algorithm}
\caption{\methodnameshort~Mask Search }
\label{alg:compression_plan}
\begin{algorithmic}

\REQUIRE Transformer Model $M$, total steps $T$, Mask Pattern list $\mathcal{P}$, threshold $\delta$
\ENSURE Dictionary $\mathrm{dict}$ that stores mask pattern result

\STATE Initialize $\mathrm{dict}$
\FOR{$t = 1$ to $T$}
  \STATE $O \gets$ (output of original $M$)
  \FOR{each layer head combination $(l, h)$ in $M$}
    \FOR{each $p$ in $\mathcal{P}$ (ordered by descending sparsity)}
      \STATE Mask head $h$ for layer $l$ using mask pattern $p$
      \STATE $O' \gets$ (output of $M$ after masking)
      \IF{$MSE(O,O') < \delta$}
        \STATE Record $m$ for $(h,l,t)$ in $\mathrm{dict}$
        \STATE \textbf{break} \
      \ENDIF
    \ENDFOR
  \ENDFOR
\ENDFOR

\STATE \textbf{return} $\mathrm{dict}$

\end{algorithmic}
\end{algorithm}

We can either apply \methodnameshort~to directly replace the 3D attention in pretrained video DiTs without training, or with small amount of training which enables even greater sparsity.



\begin{figure*}[t]
\centering
\includegraphics[width=0.93\textwidth,trim=3.1cm 0.8cm 5cm 0.25cm,clip]{media/appendix/demo3.pdf}
\caption{Qualitative example of 720P 5-second videos. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by $\Delta$-DiT are generally less sharp than those generated by the original HunyuanVideo and ~\methodnameshort.}
\label{fig:qualitative_examples2a}
\vspace{-15px}
\end{figure*}

\noindent \textbf{Training-free.} As illustrated in Fig.~\ref{fig:attention_analysis}, video diffusion models exhibit a pronounced \textit{3D locality}~and \textit{head specialization} pattern. Different transformer heads have different levels of locality, but their pattern is largely consistent across different prompts. We can exploit this property to search for the optimal window size for each head on a very small number of prompts, and expect the search result to work well on other prompts. We develop a simple heuristics to find such configuration in Algorithm \ref{alg:compression_plan}. By adjusting the hyperparameter threshold $\delta$, we can balance the speedup and the quality loss.




% \begin{algorithm}[h!]
%    \caption{Mask Search \textcolor{red}{Runlong}}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}



\textbf{Finetuning.} Beyond searching for the optimal mask per attention head without tuning, we can fix a window size with a high sparsity and fine-tune the model to adapt. Since \methodnameshort~follows the \textit{3D locality} property, this adaptation can be learned efficiently with minimal training overhead (in our experiments, 8 hours on 8 H100, which is minimal compared to the pretrain cost of video diffusion models). Although each attention layer is restricted to a local window, the receptive field expands through stacked transformer layers, enabling the Diffusion Transformer to generate globally coherent videos in the end. 



We use three different loss terms during finetuning. The attention distillation loss directly supervises the intermediate attention patterns of our \methodnameshort~to match the original dense attention behaviors:
\begin{equation}
 \mathcal{L}_{attn} = \frac{1}{N}\sum_{i=1}^N \|f_{\phi}^{(i)}(x_t, t, c) - f_{\psi}^{(i)}(x_t, t, c)\|_2^2,
\end{equation}
where $f_{\phi}^{(i)}$ and $f_{\psi}^{(i)}$ denote the intermediate attention outputs from the $i$-th transformer layer of our sliding tile model and the original attention teacher. This loss ensures each sparse attention layer to approximate its corresponding dense attention teacher. We also add a final layer loss to align the final output of the student and teacher:
\begin{equation}
 \mathcal{L}_{final} = \|f_{\phi}(x_t, t, c) - f_{\psi}(x_t, t, c)\|_2^2
\end{equation}

\begin{table*}[t!]
% \vspace{-0.4cm}
\centering
\caption{Forward speed of sparse attention kernels in a setup aligned with HunyuanVideoâ€™s inference configuration (bf16, 720P, 5s, 115.2K seq\_len, $d_{head}$ = 128, \# heads = 24). Config controls the window size of each sparse attention.}
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\textbf{Methods} & \textbf{Implementation} & \textbf{Config} & \textbf{Sparsity} & \textbf{TFLOPS} & \textbf{Latency(ms)} & \textbf{MFU} & \textbf{Kernel Efficiency} & \textbf{Speedup} \\
\midrule
FA 3 & ThunderKittens & - & 0.00\% & 164.03 & 265.28 & 62.49\% & 100.00\% & $1.00\times$ \\
FA 3 & CUDA & - & 0.00\% & 164.03 & 256.59 & 64.61\% & 103.39\% & $1.03\times$ \\
\midrule
% Flex full & FlexAttention & - & 0.00\% & 164.03 & 389.09 & 42.60\% & 68.18\% & 0.68\times \\
% \midrule
CLEAR & FlexAttention & r=16 & 90.46\% & 15.65 & 307.44 & 5.15\% & 8.24\% & $0.86\times$ \\

NATTEN & FlexAttention & w=(19,25,25) & 89.69\% & 16.91 & 313.92 & 5.44\% & 8.71\% & $0.85\times$ \\
Tiled NATTEN & CUDA & w=(19,25,25) & 89.69\% & 16.91 & 458.36 & 3.73\% & 5.97\% & $0.58\times$ \\
Tiled NATTEN & FlexAttention & w=(19,25,25) & 89.69\% & 16.91 & 208.36 & 8.20\% & 13.12\% & $1.27\times$ \\
Swin & FlexAttention & w=(24,32,32) & 87.42\% & 20.64 & 47.90 & 43.55\% & 69.69\% & $5.54\times$ \\
\midrule
~\methodnameshort& FlexAttention & w=(18,24,24) & 91.00\% & 14.76 & 36.36 & 41.03\% & 65.66\% & $7.30\times$ \\
~\methodnameshort & ThunderKittens & w=(30,40,40) & 58.33\% & 68.35 & 111.73 & \textbf{61.82\%} & \textbf{98.93\%} & $2.37\times$ \\
~\methodnameshort & ThunderKittens & w=(18,24,24) & 91.00\% & 14.76 & \textbf{25.38} & 58.79\% & 94.09\% & $\textbf{10.45}\times$ \\
\bottomrule
\end{tabular}}
\label{tab:benchmark_attn_table1}
\end{table*}

Additionally, we employ a data loss following the flow matching formulation~\cite{esser2024scaling, lipman2022flow}:
\begin{equation}
 \mathcal{L}_{data} = \|(f - x_0) - f_{\phi}(x_t, t, c)\|_2^2,
\end{equation}
where $x_0$ represents the VAE latent of the input frame, $x_t$ is the noised latent at diffusion step $t$, and $c$ denotes the text embedding.

The complete objective combines these terms:
\begin{equation}
 \min_{\phi} \mathbb{E}_{x\sim p(x), c\sim N(0,1), t} [\alpha \mathcal{L}_{data} + \beta \mathcal{L}_{final} + \gamma \mathcal{L}_{attn}]
 \label{eq:mix_loss}
\end{equation}
% with loss weights $\alpha$, $\beta$, and $\gamma$. We set them to 1, 0.5, 0.5, respectively.

The detailed training setup can be found in Appendix \ref{sec:appendix_finetuning}.





