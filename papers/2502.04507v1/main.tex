%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
\usepackage{multirow}
\usepackage{afterpage}
\usepackage{subcaption}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath} 
\usepackage[compact]{titlesec}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,      % Color of internal links (e.g., \ref, \eqref)
    citecolor=blue,     % Color of citations (\cite)
    urlcolor=magenta    % Color of URLs (\href)
}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% \usepackage[arxiv]{icml2025}
\usepackage[table]{xcolor}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\newcommand{\methodname}{\textsc{Sliding Tile Attention}}
\newcommand{\methodnameshort}{\textsc{STA}}

\newcommand{\junda}[1]{{\color{green}{\bf\sf [junda: #1]}}}
\newcommand{\yichao}[1]{{\color{purple}{\bf\sf [yichao: #1]}}}
\newcommand{\siqi}[1]{{\color{blue}{\bf\sf [siqi: #1]}}}
\newcommand{\zheyu}[1]{{\color{orange}{\bf\sf [zheyu: #1]}}}
\newcommand{\andy}[1]{{\color{magenta}{\bf\sf [andy: #1]}}}
\newcommand{\hao}[1]{{\color{blue}{\bf\sf [Hao: #1]}}}
\newcommand{\aurick}[1]{{\color{cyan}{\bf\sf [Aurick: #1]}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Fast Video Generation with \methodname}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Peiyuan Zhang}{ucsd}
\icmlauthor{Yongqi Chen}{equal,umich}
\icmlauthor{Runlong Su}{equal,ucsd}
\icmlauthor{Hangliang Ding}{thu}
\\
\icmlauthor{Ion Stoica}{ucb}
\icmlauthor{Zhengzhong Liu}{mbzuai}
\icmlauthor{Hao Zhang}{ucsd}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% Affiliations
\icmlaffiliation{ucsd}{University of California, San Diego}
\icmlaffiliation{umich}{University of Michigan, Ann Arbor}
\icmlaffiliation{thu}{Tsinghua University} 
\icmlaffiliation{ucb}{University of California, Berkeley}
\icmlaffiliation{mbzuai}{Mohamed bin Zayed University of Artificial Intelligence}

\icmlcorrespondingauthor{Hao Zhang}{haozhang@ucsd.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{\icmlEqualContribution}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the 
    observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being \emph{hardware-efficient}. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79\% MFU. Precisely, STA accelerates attention by 2.8–17× over FlashAttention-2 (FA2) and 1.6–10× over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s \emph{without} quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09\% drop on VBench.

% . However, as resolutions and durations increase, the attention layer with quadratic complexity can account for over 80\% of transformer computation, significantly slowing down the generation process. 

% In this paper, we identify that most attention scores in a pre-trained video diffusion model remain confined within a local 3D window. In response, we introduce \methodname~(\methodnameshort), a window attention variant that co-designs the algorithm and system to optimize both attention locality and efficient execution. Algorithmically, we adopt a tile-wise sliding approach, where attention windows slide tile-by-tile instead of token-by-token. System-wise, we spatially tile the QKV matrix along the sequence dimension. We further utilize warp specialization to enable asynchronous data workers to manage sparse attention maps independently of compute workers. 
% To our knowledge, we are the first to offer an efficient 2D/3D sliding-window-like attention implementation that attains an MFU of 58.79\%, which is 7.17$\times$ faster than prior implementations. Applied to video diffusion models, our method accelerates attention computation by 4-10$\times$ compared to Flash Attention 3, achieving a 2.43-3.53$\times$ end-to-end speedup with no or negligible quality degradation.





\end{abstract}

\section{Introduction}
\input{contents/intro}
\section{Problem}
\input{contents/background}

\section{Methods}

\input{contents/method}


\section{Experiments}
\input{contents/experiments}
\section{Related Work}
\input{contents/related_work}

\section{Conclusion and Future Work}
We introduce \methodname~to accelerate video diffusion models, with an optimized kernel for high-order sliding-window-like attention, enabling efficient GPU execution while preserving the locality property. Experiments demonstrate that \methodname~accelerates video generation with minimal or no quality loss. Conceptually, our method is orthogonal to other acceleration techniques, such as caching and consistency distillation. We plan to explore their combined effectiveness for further efficiency gains in future work.


\label{submission}

\section*{Impact Statement}
Our work addresses computational bottlenecks in Diffusion Transformers by introducing efficient attention kernels that reduce video generation time while maintaining output quality. The improved efficiency makes video generation more practical for researchers and developers working with limited computing resources, potentially benefiting AI-driven video applications across creative industries, education, and so on. While faster video generation could potentially enable misuse, existing content detection and watermarking techniques can help mitigate such risks. \\
Overall, the benefits of more efficient video generation significantly outweigh potential concerns, representing a meaningful step toward accessible video AI systems.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage
\appendix
\input{contents/appendix}
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
