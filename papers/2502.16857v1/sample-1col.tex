%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
% twocolumn,
% hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support 
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
\usepackage{color,soul}
%% auto break lines
\lstset{breaklines=true}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2022}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{De-Factify 4.0 : Fourth Workshop on Multimodal Fact Checking and Hate Speech Detection
February, 2025, co-located with AAAI 2025 | Philadelphia, Pennsylvania, USA}

%%
%% The "title" command
\title{Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models}

% \tnotemark[1]
% \tnotetext[1]{You can use this document as the template for preparing your
%   publication. We recommend using the latest version of the ceurart style.}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.


\author[1]{Avinash Trivedi}[
email=avinashtrivedi.2008@gmail.com
]
\cormark[1]
\address[1]{National Institute of Technology, Tiruchirapalli,India}
\author[1]{Sangeetha Sivanesan}[]
\cortext[1]{Corresponding author}



%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text.  Our team (\textit{Sarang}) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  AI-generated text detection \sep
  DeBERTa \sep
  Noise \sep
  De-Factify 4.0@AAAI2025
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Large Language Models (LLMs), such as ChatGPT \cite{schulman2022chatgpt}, are really good at writing long pieces of text that sound very human. While these developments have various beneficial applications, they also raise concerns about potential misuse, such as the automatic creation of fake news articles and academic contents \cite{natureChatGPTWrites}. To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques \cite{kirchenbauer2024watermarklargelanguagemodels}, tools like GPTZero \cite{gptzeroDetectorOriginal}, DetectGPT \cite{mitchell2023detectgptzeroshotmachinegeneratedtext}, and OpenAI's text classifier \cite{openai-text}. 

The task of detecting AI-generated text is inherently challenging, as recent research \cite{sadasivan2024aigeneratedtextreliablydetected} highlights the increasing sophistication of newer, more capable LLMs. Early studies demonstrated that humans struggle to tell if something was written by a computer or a human. Given the ethical implications and the complexity of the problem, creating robust detection systems remains an active area of research. 
The Defactify 4.0 shared task \footnote{\url{https://defactify.com/ai_gen_txt_detection.html}} \cite{roy-2025-defactify-overview-text}, part of the fourth workshop on multimodal fact-checking and hate speech detection, featured two subtasks: Task-A focused on distinguishing between AI-generated and human-authored text, while Task-B aimed to identify the specific LLM responsible for generating the text. This paper proposes an ensemble based DeBERTa model, trained and validated on noisy dataset to make the model more robust. This work highlights how adding noise to the dataset makes the model remain resilient to disturbances. It captures features invariant under perturbations and demonstrates significantly improved robustness against such disturbances.

The rest of the paper is as follows. Section 2 contains related work, section 3 describes the dataset, section 4 describes our methodology, section 5 contains experimental results and section 6 includes conclusions and future work.

\section{Related work}
Recent advancements have demonstrated significant progress in methods for detecting AI-generated text. These methods broadly fall into three categories: statistical approaches, classifier-based detectors, and watermarking techniques.

\textbf{Traditional statistical detection methods} leverage metrics such as entropy, perplexity, and n-gram frequency to identify differences in linguistic patterns between human and machine-generated text \cite{Lavergne2008DetectingFC,Gehrmann2019GLTRSD}. A recent innovation, DetectGPT \cite{mitchell2023detectgptzeroshotmachinegeneratedtext}, builds on these principles, focusing on the negative curvature areas of a model's log probability. By generating and comparing perturbed variations of text, DetectGPT determines its likelihood of being machine-generated based on log probabilities. This method achieves significantly higher AUROC scores compared to other zero-shot detection approaches, making it a notable advancement in statistical detection techniques.

\textbf{Classifier-based detection methods} are commonly employed in identifying fake news and misinformation \cite{schildhauer2022fake,zou2021ai}. OpenAI, for example, fine-tuned a GPT model using datasets from Wikipedia, WebText, and human-labeled samples to create a classifier capable of discerning machine-generated text. This model combines automated classification with human evaluation, demonstrating its efficacy in detecting AI-generated content. Such advancements contribute to mitigating the spread of misinformation and improving societal trust in online content \cite{kshetri2022deep}.

\textbf{Watermark-Based Identification} has emerged as a compelling alternative for machine-generated text detection. Historically used in image processing for copyright protection and data hiding \cite{langelaar2000watermarking,wmark_old1}, watermarking techniques have recently been adapted for natural language. \cite{kirchenbauer2023watermark} proposed a novel watermarking approach that utilizes language model logits to embed invisible watermarks in text. This method categorizes tokens into \textit{green} and \textit{red} lists, guiding token selection to create patterns that are imperceptible to human readers. These advancements not only enhance content authentication and copyright enforcement but also pave the way for secure communication, digital rights management, and privacy protection.

While existing methods effectively identify unaltered LLM-generated content, their reliability against user-modified versions remains underexplored. Research shows that even small changes can significantly weaken the performance of these detection techniques. We proposed a noise-driven, DeBERTa based ensemble approach to address the issue, as it remains largely unaffected by disturbances and highlight differences between human and LLM-generated text. This method improves robustness in detecting perturbed LLM-generated content.

Our method is inspired from \cite{karpukhin2019training,wei-zou-2019-eda,xie2017data}, where \cite{karpukhin2019training} observed that training machine translation models on a balanced mix of simple synthetic noise enhances robustness to character-level variations, such as typos, without compromising performance on clean text. Authors in \cite{wei-zou-2019-eda} introduces Easy Data Augmentation (EDA) four simple yet effective techniques, synonym replacement, random insertion, random swap, and random deletion. It significantly improves text classification performance, especially for smaller datasets, achieving comparable results with reduced training data.
Authors in \cite{xie2017data} highlights that adding noise can be beneficial for model generalization. It proposes two Noising technique, First is Unigram Noising, Which randomly replaces tokens in a sequence with words sampled from the unigram frequency distribution at a probability \( \gamma \), introducing corpus-wide diversity. Second is Blank Noising, Which replaces tokens with a placeholder token (“\_”) at a probability \( \gamma \), simulating missing context to enhance generalization. We used DeBERTa \cite{he2020deberta} as our base model, which is becoming popular in NLP because they can predict words using both the left and right context and are trained on a large amount of plain text from the internet.

\section{Dataset}
The dataset \cite{roy-2025-defactify-dataset-text} provided in this shared task consists of three columns namely Text, Label\_A and Label\_B, where Text is the AI or Human generated text, Label\_A denoting class 0 or 1 (Human/AI), Label\_B denotes one of the specific LLMs (Human\_story, gemma-2-9b, mistral-7B, qwen-2-72B, llama-8B, yi-large or GPT\_4-o) that generated the text. Table \ref{tab:data_distribution} contains dataset distribution. 

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Class} & \textbf{Validation Set Count} & \textbf{Training Set Count} \\ 
\midrule
Mistral-7B     & 1569                          & 7321                        \\ 
Llama-8B       & 1569                          & 7321                        \\ 
GPT\_4-o       & 1569                          & 7321                        \\ 
Qwen-2-72B     & 1569                          & 7321                        \\ 
Yi-Large       & 1569                          & 7321                        \\ 
Gemma-2-9B     & 1569                          & 7321                        \\ 
Human\_Story   & 1569                          & 7321                        \\ 
\midrule
\textbf{Total}       & \textbf{10983}                & \textbf{51247}        \\ 
\bottomrule
\end{tabular}
\caption{Data distribution for validation and training set}
\label{tab:data_distribution}
\end{table}

\section{Methodology}
\subsection{Finetuning of DeBERTa on Original Dataset}
We started by fine-tuning a set of DeBERTa models on original train set and validated on original val set to create a trustworthy baseline for our investigations. Table \ref{tab:baseline_score} provides a summary of the findings from these experiments. Out of all the evaluated configurations, the \textit{DeBERTa-v3-small} model performed the best on Task-A, showing the most promising result on test set. This suggests that the model is capable of successfully capturing the subtleties and patterns required to meet Task-A's requirements. 

While working on Task-B, we found that all models, including \textit{DeBERTa-v3-small}, exhibited signs of overfitting in spite of our various attempts. The training and testing performances diverged significantly as a result of this overfitting, which eventually resulted in less than ideal generalisation for Task-B. These results imply that in order to enhance performance on Task-B while preserving strong outcomes for Task-A, further tactics, such as regularisation schemes, data augmentation, or different modelling approaches, might be required.

\begin{table}[h!]
\centering
\begin{tabular}{llcc}
\toprule
\textbf{Data} & \textbf{Model} & \multicolumn{2}{c}{\textbf{Test F1-Score}} \\ 
\cmidrule(lr){3-4}
& & \textbf{Label-A} & \textbf{Label-B} \\ 
\midrule
& deberta-v3-xsmall & 0.9521 & 0.2418 \\ 
Original train/val & deberta-v3-small & \textbf{0.9985} & 0.2885 \\ 
& deberta-v3-base & 0.9515 & 0.4322 \\ 
\bottomrule
\end{tabular}
\caption{Baseline scores on test set}
\label{tab:baseline_score}
\end{table}

\subsection{Best performing system}

Inspired from \cite{karpukhin2019training,wei-zou-2019-eda,xie2017data}, we implemented a data noising strategy to enhance the robustness of our language model. This technique, inspired by the authors' insights on the benefits of noise injection for smoothing, involves introducing controlled disruptions to the dataset. The architecture of our best-performing system is
illustrated in Fig \ref{FIG:arch}. We noised our dataset by injecting 10\% junk or garbled words into each data points as shown in Table \ref{tab:noised_data}. These junk words were randomly generated with lengths varying between 3 to 8 characters, ensuring the injected noise was both unpredictable and diverse. This approach mimics real-world scenarios where noisy or corrupted data is often encountered, enabling the model to learn more resilient representations.

\begin{figure}
	\centering
		\includegraphics[scale=.15]{defactify.drawio.png}
	\caption{Ensemble based model architecture}
	\label{FIG:arch}
\end{figure}

Once the noisy dataset was prepared, we utilized it to fine-tune the DeBERTa model, a state-of-the-art transformer-based language model. By exposing the model to this noised data during fine-tuning, we aimed to evaluate its ability to generalize effectively and maintain performance in the presence of noisy inputs. We have finetuned \textit{deberta-v3-small} on original and noisy train data, also further finetune \textit{deberta-v3-small} on noisy train data which was earlier finetuned on original train dataset. We created a weighted ensemble (60:40) combining two models: one fine-tuned on noisy training data and the other sequentially fine-tuned on both original and noisy training data. Our primary goal was to test whether data noising could serve as a regularization technique to reduce overfitting and enhance the model's robustness.

% \begin{figure}
% 	\centering
% 		\includegraphics[scale=.15]{defactify.drawio.png}
% 	\caption{Ensemble based model architecture}
% 	\label{FIG:arch}
% \end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{p{7 cm}|p{7 cm}}
\hline
\textbf{Original Text} & \textbf{Noised text} \\ \hline
Photos of the Day: Greece and Elsewhere Migrants started small fires to keep warm near Idomeni, Greece, where they waited to cross the border into Macedonia. Photos of the Day: Greece and Elsewhere Greece and More — Pictures of the Day Slideshow controls                            & Photos of the Day: Greece and \hl{gkkas} Elsewhere Migrants \hl{nvvwe} started small fires to keep warm near Idomeni, Greece, where they waited to cross the border into Macedonia. Photos of the Day: \hl{visrqmy} Greece and Elsewhere Greece and More — Pictures of the Day Slideshow controls                     \\ \hline
                            
Living In ... Edison, N.J. Roosevelt Park, which dates to 1917, is the oldest county park in Middlesex County, N.J. Living In ... Edison, N.J. The Middlesex County township is an hour from Manhattan, with easy access to transportation and a vibrant Asian community. Slideshow controls                            &   Living In ... Edison, N.J. Roosevelt Park, which dates to 1917, is the oldest county park in Middlesex County, N.J. \hl{ipktg} Living In ... Edison, N.J. The Middlesex \hl{mjy} County township is an hour from Manhattan, \hl{fcfhkw} with easy access to transportation and a vibrant Asian community. Slideshow controls                     \\ \hline

\end{tabular}
\caption{Sample noised dataset}
\label{tab:noised_data}
\end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{lccc}
% \toprule
% \textbf{Hyperparameter} & & \multicolumn{2}{c}{\textbf{Value}} \\ 
% \cmidrule(lr){3-4}
% & & \textbf{Label-A} & \textbf{Label-B} \\ 
% \midrule
% max token length & & 768 & 768 \\ 
% learning rate & & 5e-05 & 5e-05 \\ 
% per device train batch size & & 6 & 24 \\ 
% per device eval batch size & & 6 & 24 \\ 
% num train epochs & & 1 & 5 \\ 
% gradient accumulation steps & & 4 & 4 \\ 
% weight decay & & 0.01 & 0.01 \\ 
% warmup steps & & 500 & 500 \\ 
% \bottomrule
% \end{tabular}
% \caption{Hyperparameter values for Label-A and Label-B}
% \label{tab:hyperparameters}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{llcc}
% \toprule
% \textbf{Data} & \textbf{Model} & \multicolumn{2}{c}{\textbf{Test F1-Score}} \\ 
% \cmidrule(lr){3-4}
% & & \textbf{Label-A} & \textbf{Label-B} \\ 
% \midrule
% & deberta-v3-xsmall & 0.9521 & 0.2418 \\ 
% Original train/val & deberta-v3-small & \textbf{0.9985} & 0.2885 \\ 
% & deberta-v3-base & 0.9515 & 0.4322 \\ 
% \bottomrule
% \end{tabular}
% \caption{Baseline scores on test set}
% \label{tab:baseline_score}
% \end{table}

\section{Experimental Results}

The result of our experiments, detailed in Table \ref{tab:baseline_score} and Table \ref{tab:noise_result}, reveal valuable insights into the impact of noised data on model performance. It also highlights the potential benefits of incorporating noise into training dataset, particularly for tasks requiring high resilience to incomplete inputs. These noisy data-driven experiments demonstrates significant performance improvements, particularly for the \textit{DeBERTa-v3-small} model, across both Task-A and Task-B with F1-score of 1.0 and 0.9454 respectively. This improvement underscores the effectiveness of noise injection as a regularization strategy, enhancing the model's generalization capabilities.

\begin{table}[h!]
\centering
\begin{tabular}{llcc}
\toprule
\textbf{Data} & \textbf{Model} & \multicolumn{2}{c}{\textbf{Test F1-Score}} \\ 
\cmidrule(lr){3-4}
& & \textbf{Label-A} & \textbf{Label-B} \\ 
\midrule
& deberta-v3-xsmall & 0.9985 & 0.6382 \\ 
& deberta-v3-small & \textbf{1.0} & 0.9454 \\ 
& deberta-v3-base & 0.9989 & 0.6570 \\ 
Noised train/val & Double Finetune (deberta-v3-small) & - & 0.9167 \\ 
& Ensemble (deberta-v3-small + Double Finetune) & - & \textbf{0.9531} \\ 
& \textbf{Best performing submission} & 1.0 & 0.9531 \\ 
\bottomrule
\end{tabular}
\caption{Major system submissions}
\label{tab:noise_result}
\end{table}

Additionally, for Task-B, we explored a sequential fine-tuning approach by further training the \textit{DeBERTa-v3-small} model on the noisy dataset after it had already been fine-tuned on the original training data. It achieves F1-score of  0.9167, Although this method did not outperform the results achieved through direct fine-tuning (F1-score of 0.9454) on the noisy dataset, it captured distinct patterns and nuances within the data. These unique patterns proved valuable when integrating the outputs into a weighted (60:40) ensemble model for Task-B which reports the best F1-score of 0.9531.

By leveraging the complementary strengths of models fine-tuned on raw and noisy datasets, the ensemble model was able to outperform all our previous experiments. This approach highlights the utility of combining diverse training strategies to capture a broader range of data characteristics, ultimately leading to a more robust and effective system. Our findings underscore the relevance of data noising as a practical tool for improving language model generalization and stability in AI generated text detection. Table \ref{tab:hyperparameters} contains hyperparameter values for the best-performing system.

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Hyperparameter} & & \multicolumn{2}{c}{\textbf{Value}} \\ 
\cmidrule(lr){3-4}
& & \textbf{Label-A} & \textbf{Label-B} \\ 
\midrule
max token length & & 768 & 768 \\ 
learning rate & & 5e-05 & 5e-05 \\ 
per device train batch size & & 6 & 24 \\ 
per device eval batch size & & 6 & 24 \\ 
num train epochs & & 1 & 5 \\ 
gradient accumulation steps & & 4 & 4 \\ 
weight decay & & 0.01 & 0.01 \\ 
warmup steps & & 500 & 500 \\ 
\bottomrule
\end{tabular}
\caption{Hyperparameter values for Label-A and Label-B}
\label{tab:hyperparameters}
\end{table}

\section{Conclusions and Future Work}
We have performed various experiments which include fine-tuning on original train data, noise-injected dataset, and a combination of sequential fine-tuning and weighted ensemble modeling. Among these, the \textit{DeBERTa-v3-small} model fine-tuned directly on the noisy dataset demonstrated the most promising results, achieving significant improvements across both Task-A and Task-B. Furthermore, utilizing an ensemble of models fine-tuned on original and noisy data allowed us to capture diverse patterns, ultimately outperforming all previous individual model attempts. 

Our findings highlight the importance of data augmentation technique, such as noise injection, for enhancing model generalization and robustness. Future work will aim to explore further the potential of dynamic data noising strategies and ensemble techniques with more variants of the DeBERTa models.

%%%%%%%%%%%%  Appendices Start %%%%%%%%%%%% 

% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|\appendix|'' command:
% \begin{lstlisting}
% \appendix
% \end{lstlisting}
% and note that in the appendix, sections are lettered, not
% numbered. 

%%%%%%%%%%%%  Appendices End %%%%%%%%%%%%

%%
%% The acknowledgments section is defined using the "acknowledgments" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

% \begin{acknowledgments}
%   Thanks to the developers of ACM consolidated LaTeX styles
%   \url{https://github.com/borisveytsman/acmart} and to the developers
%   of Elsevier updated \LaTeX{} templates
%   \url{https://www.ctan.org/tex-archive/macros/latex/contrib/els-cas-templates}.  
% \end{acknowledgments}


%% The declaration on generative AI comes in effect
%% in Janary 2025. See also
%% https://ceur-ws.org/GenAI/Policy.html

%%%% GEN AI

% \section*{Declaration on Generative AI}
%   % {\em Either:}\newline
%   The author(s) have not employed any Generative AI tools.
%   % \newline
  
 % \noindent{\em Or (by using the activity taxonomy in ceur-ws.org/genai-tax.html):\newline}
 % During the preparation of this work, the author(s) used X-GPT-4 and Gramby in order to: Grammar and spelling check. Further, the author(s) used X-AI-IMG for figures 3 and 4 in order to: Generate images. After using these tool(s)/service(s), the author(s) reviewed and edited the content as needed and take(s) full responsibility for the publication’s content. 


%%
%% Define the bibliography file to be used
\bibliography{sample-ceur}


%%%%%%%%%%%% appendix start

% %%
% %% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Online Resources}


% The sources for the ceur-art style are available via
% \begin{itemize}
% \item \href{https://github.com/yamadharma/ceurart}{GitHub},
% % \item \href{https://www.overleaf.com/project/5e76702c4acae70001d3bc87}{Overleaf},
% \item
%   \href{https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/pkfscdkgkhcq}{Overleaf
%     template}.
% \end{itemize}


%%%%%%%%%%%% appendix end

\end{document}

%%
%% End of file
