\section{Related Work}\label{sec:related}

\paragraph{Large Language Models (LLMs).}

Transformer-based LLMs~\cite{minaee2024large} have achieved remarkable performance in various natural language processing tasks.
They are mainly pre-trained with causal language modeling (e.g. LLaMA2~\cite{touvron2023llama}) or masked language modeling (e.g. BERT~\cite{devlin-etal-2019-bert}).
BERT-like models employ a bidirectional attention mechanism to learn contextual representations, typically adopting an encoder-only architecture.
In contrast, LLaMA-like models utilize a decoder-only architecture and are trained autoregressively with causal self-attention, modeling the probability distribution of the next token in a sequence.
In our observations, LLaMA-like models exhibit extremely high-norm tokens in their hidden states, whereas BERT-like models do not.
In this work, we focus on LLaMA-like models and investigate the characteristics of these high-norm tokens.

\paragraph{High-Norm Tokens in ViTs and LLMs.}

Several recent works have observed the presence of high-norm defective tokens in the feature maps of ViTs~\cite{darcetvision,wang2024sinder} and proposed methods to repair them.
In particular, \citet{wang2024sinder} used the leading left singular vector of the matrix representing a linear approximation of the transformer layer to predict the direction of the defective tokens.
In the context of LLMs, \citet{sun2024massive} noticed that certain activations in the hidden states have a huge magnitude (i.e., massive activations).
They observed that massive activations are consistently present in very few fixed dimensions but did not provide a mathematical explanation.

To account for the different properties of high-norm tokens in ViTs and LLMs, we extend the theory of \citet{wang2024sinder} to LLMs, providing a systematic explanation for their high-norm tokens.
Furthermore, while \citet{sun2024massive} study massive activations from the perspective of the individual scalar values within a token, we analyze high-norm tokens as a whole vector and provide a mathematical framework to explain high-norm tokens in LLMs.

\paragraph{Applications of High-Norm Tokens.}
While recent studies have identified high-norm tokens in LLMs, their practical implications remain largely unexplored. We highlight two key applications: Quantization and Model Signature.
High-norm tokens induce activation outliers, posing challenges for tensor-wise quantization methods~\cite{dettmers2022gpt3, xiao2023smoothquant, grattafiori2024llama}.
Additionally, as more open-source LLMs become available, tracing model lineage is increasingly important.
Prior works~\cite{yu2024neural, mu2023model} investigate model tracing through internal representations, yet, they can only deal with models in vision.
To the best of our knowledge, we are the first to leverage insights from the high-norm token analysis to address both challenges in LLMs.
