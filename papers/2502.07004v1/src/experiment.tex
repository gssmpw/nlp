\section{Applications}\label{sec:exp}

In this section, we showcase two applications motivated by the insights of previous sections.
In \cref{sec:quantization}, we improve quantization by specializing it according to the explosion and decay layers.
In \cref{sec:signature}, we show that the direction of high-norm tokens can be used as a signature of an LLM\@.

\subsection{Improving the Design of Quantization}\label{sec:quantization}

LLMs are memory and computation intensive because of their huge number of parameters.
Quantization is a common technique to reduce memory footprint and improve inference speed.
Quantization in LLM mainly follows two settings: W8A8 quantization~\cite{dettmers2022gpt3,xiao2023smoothquant}, where both the weights and activations are quantized to 8-bit; and low-bit weight-only quantization~\cite{lin2024awq,frantar2022gptq}.
We focus on W8A8 quantization here.
Converting a model from 16-bit floating-point to 8-bit integer can reduce the model size by half.
Int8 quantization~\cite{jacob2018quantization} can be expressed as \(\bar X^{\textrm{int8}}=\left\lfloor \frac{X^{\textrm{fp16}}}{\Delta} \right\rceil\), where \(\Delta=\frac{\max(|X|)}{2^{N-1}-1}\) is the quantization step size.
Note that \(\Delta\) is determined by the maximum absolute value of \(X\).
Therefore, outliers in the input tensor will lead to non-negligible underflow and cause large quantization errors.

Outlier channels in quantization are different from high-norm tokens~\cite{sun2024massive}.
High-norm tokens refer to hidden states after each transformer layer, whereas outlier channels denote the input activations to linear layers within a transformer layer.
Most current LLMs adopt pre-LayerNorm, where a normalization layer is prepended before the self-attention and FFN modules.
Hence, the input activations for the query, key, value projections in attention, and \(W_1\), \(W_3\) in FFN, are normalized.
These inputs are not high-norm tokens, yet they possess outlier channels.

Outliers in activations are more severe than in weights~\cite{xiao2023smoothquant}.
As such, the quantization of activations is more challenging than that of weights.
For example, tensor-wise W8A8 quantization, where both weights and activations are quantized to int8, does not work well for LLaMA3~\cite{grattafiori2024llama}.
To tackle this issue, LLaMA3 resorts to the row-wise quantization, where the quantization step size is computed across rows of activation and weight matrices.
However, hardware support for row-wise quantization is limited.
For instance, the FBGEMM~\cite{FBGEMM}
implementation of row-wise quantization only supports recent GPUs such as Nvidia H100 and AMD MI300X.
Furthermore, tensor-wise quantization has lower latency compared to row-wise quantization~\cite{xiao2023smoothquant}.
It is therefore desirable to improve the quantization strategy to make tensor-wise quantization effective.

We found that the main obstacle to tensor-wise quantization in LLMs is related to the outlier channels induced by the high-norm tokens.
In the explosion layer and the decay layer, the residual modules produce high-norm tokens (\cref{sec:method}), and thus, in the middle of the computation, the intermediate token norms can be very large.
Specifically, inside FFN, the norm of some tokens after \(\textrm{silu}(W_1 x)\odot (W_3 x)\) is extremely high, hence affecting the subsequent linear layer \(F_2 x\).

Our solution is straightforward: we simply allow the linear layer \(F_2\) of the FFN module in explosion and decay layers to operate at a higher precision.
We compare the performance of our solution with the original tensor-wise quantization in \cref{tab:quantization}.
The perplexities of both the standard RTN (Round-To-Nearest) and the recent SM (SmoothQuant) strategies are significantly reduced after applying our solution.

\begin{table}[t]
    \caption{Improving quantization by accounting for the explosion and decay layers (marked with *).
    PPL is the perplexity on the validation set of WikiText2-v1.
    }\label{tab:quantization} \vspace{-0.2cm}
    \begin{center}
        \begin{small}
            \begin{sc}
                \setlength{\tabcolsep}{2pt}
                \begin{tabular}{lccccr}
                    \toprule
                    Model                       & Method & Weight     & Activation  & PPL\(\downarrow\) \\
                    \midrule
                    \multirow{ 5}{*}{LLaMA2-7B} & -      & -          & -           & 5.38              \\
                                                & RTN    & Per-Tensor & Per-Tensor  & 9.84              \\
                                                & RTN*   & Per-Tensor & Per-Tensor* & 6.75              \\
                                                & SQ     & Per-Tensor & Per-Tensor  & 12.97             \\
                                                & SQ*    & Per-Tensor & Per-Tensor* & 6.64              \\
                    \midrule
                    \multirow{ 5}{*}{LLaMA3-8B} & -      & -          & -           & 6.22              \\
                                                & RTN    & Per-Tensor & Per-Tensor  & 63.31             \\
                                                & RTN*   & Per-Tensor & Per-Tensor* & 10.86             \\
                                                & SQ     & Per-Tensor & Per-Tensor  & 58.02             \\
                                                & SQ*    & Per-Tensor & Per-Tensor* & 11.18             \\
                    \bottomrule
                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
\end{table}
\vspace{-0.5em}

\subsection{Signature of an LLM}\label{sec:signature}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/paper/llama2_7b_sd_diff.pdf}
    \vspace{-0.5em}
    \caption{The singular defect directions (at explosion layer 2 and decay layer 31 for LLaMA2-7B) remain stable after fine-tuning.
    }\label{fig:stable_dir_finetune}
    \vspace{-0.5em}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/paper/llama2_7b_sd_dist.pdf}
    \vspace{-1em}
    \caption{Model distances based on angles between model signatures.
        la2, la3, la3.1, vi1.1, vi1.3, vi1.5, mpt mean LLaMA2-7B, LLaMA3-8B, LLaMA3.1-8B, Vicuna-7B, MPT-7B, respectively.
    }\label{fig:model_dist}
    \vspace{-1em}
\end{figure}
In \cref{sec:discussion}, we have shown that the direction of the high-norm tokens gradually stabilizes during training.
By extension, this suggests that \emph{the singular defect direction is also robust to fine-tuning}, since fine-tuning is a continuation of the training process.
To verify this claim, in \cref{fig:stable_dir_finetune}, we compare the pairwise angles of the layer-wise singular defect directions between the base model LLaMA2-7B and its fine-tuned variants LLaMA2-7B-Chat, LLaMA2-7B-Code, LLaMA2-7B-Code-Python, LLaMA2-7B-Code-Instruct.
For the explosion layer 2 and the decay layer 31, the pairwise angles are very small, which confirms that the direction of the high-norm tokens is stable w.r.t. fine-tuning.
Additionally, we added LLaMA3-8B into the comparison, and the pairwise angles are all nearly 90 degrees, suggesting that the singular defect direction could distinguish different models.

An application of this stability is to use the singular defect direction as a signature of an LLM\@.
Specifically, we define the \emph{signature of a LLM} as the empirical high-norm direction.
Based on our analysis, the leading left singular vector of either the explosion layer or the decay layer serves as a reliable approximation of the empirical high-norm direction. This approach offers two key advantages: it is data-independent and eliminates the need for manually selecting a threshold to define high norms.
By defining the distance between two models (with the same hidden dimension) as the acute angle between their signature vectors, we can \emph{test whether one model is derived from another by computing their signature distances}.
In practice, to avoid having to manually locate the explosion/decay layers, we can simply compute the acute angle between the corresponding layer-wise singular defect directions for all layers and take the minimum value as the distance between the two models.

\cref{fig:model_dist} presents the pairwise distances among several series of LLMs, including the LLaMA2-7B, LLaMA3/3.1-8B, Vicuna-7B, and MPT-7B families.
These models naturally cluster into several groups, with models from the same family appearing close to each other.
We can infer from the figure that LLaMA3 was trained from scratch without reusing the LLaMA2 checkpoint, while LLaMA3.1 was obtained by continuing training from the LLaMA3 checkpoint.
Besides, we confirm that Vicuna1.5 is fine-tuned from LLaMA2 instead of Vicuna1.1/1.3.
Altogether, these results indicate that our LLM signature effectively captures and differentiates the relationships between models.
These findings are aligned with ground truth, as confirmed by publicly available training details. The observed relationships between models match the official training procedures, further validating our LLM signature as a reliable method for tracing model lineage.
