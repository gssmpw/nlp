\vspace{-1em}
\section{Introduction}\label{sec:intro}
\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/llama2_7b_norm_3d.pdf}
        \caption{LLama2-7B}\label{fig:llama2_7b_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/phi3_medium_norm_3d.pdf}
        \caption{Phi3-Medium}\label{fig:phi3_medium_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/mpt_7b_norm_3d.pdf}
        \caption{MPT-7B}\label{fig:mpt_7b_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/pythia_160m_norm_3d_step143000.pdf}
        \caption{Pythia-160M}\label{fig:pythia_160m_norm_3d_step143000}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/vicuna1.5_7b_norm_3d.pdf}
        \caption{Vicuna1.5-7B}\label{fig:vicuna15_7b_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/falcon2_11b_norm_3d.pdf}
        \caption{Falcon2-11B}\label{fig:falcon2_11b_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/gpt2_medium_norm_3d.pdf}
        \caption{GPT2-Medium}\label{fig:gpt2_medium_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/paper/qwen2.5_1.5b_norm_3d.pdf}
        \caption{Qwen2.5-1.5B}\label{fig:qwen25_15b_norm_3d}
    \end{subfigure}
\vspace{-0.5em}
\caption{High norm tokens in various LLMs.
    Each subfigure plots the norm of the first few tokens and the token `\texttt{.}' in the sentence `\texttt{The quick brown fox jumps over the lazy dog.}'. The \(x\)-axis is the layer id, the \(y\)-axis shows different tokens, and the \(z\)-axis is the norm.
    Layer 0 is the input embedding layer, and the others are transformer layers.
        More models are shown in \cref{sec:more_high_norm}.
    }\label{fig:llm_norm}
\vspace{-0.5em}
\end{figure*}

While large foundation models are proving increasingly effective, their intrinsic behavior remains poorly understood. An example of this is the emergence of tokens whose norm is unexpectedly higher than that of the majority of the other tokens during the forward pass, as illustrated in \cref{fig:llm_norm}. This behavior has been observed in both large vision transformers~\cite{darcetvision,wang2024sinder} and large language models (LLMs)~\cite{sun2024massive}.


In particular, in vision foundation model DINOv2~\cite{oquab2024dinov2,darcetvision}, high-norm tokens appear as defective patch tokens, degrading the feature quality and hindering performance on downstream dense prediction tasks.
In pursuit of repairing these defects, \citet{wang2024sinder} revealed a connection between the direction of the high-norm tokens and the leading left singular vector of the matrix that provides a linear approximation of a transformer layer, and therefore name these tokens as \emph{singular defects}.

In the context of LLMs, high-norm hidden states have also been observed~\cite{sun2024massive} and referred to as \emph{massive activations}. Specifically, the following properties were empirically identified:
(1) The appearance of a massive activation is abrupt, emerging suddenly in one layer and diminishing later in the model after another.
(2) Massive activations appear mostly for the initial token and the delimiter tokens.
These properties differ from those made with DINOv2, where the norm of the defective tokens increases gradually layer by layer, and the defective tokens are randomly scattered across the feature map, with a tendency to appear in low-semantic regions.
Given these different behaviors of high-norm tokens in LLMs vs. ViTs, it is natural to wonder \emph{(i) whether the theory of singular defects can be applied to LLMs; and (ii) how to further explain the new observations related to massive activations?}

In this paper, we confirm that singular defects can predict the direction of high-norm tokens in LLMs.
However, it falls short of explaining the high-norm properties that are unique to LLMs.
To understand the full life cycle of the high-norm tokens,
we thus expand the theory and
provide the following insights:
\begin{enumerate}
    \item (\emph{Development}) The explosion of the initial\footnote{We define the initial token as the first token in the user input.} token norm is linked to self-attention, whereas that of the noninitial high-norm tokens is unrelated to self-attention.
    \item (\emph{Trigger}) A norm explosion is initiated when the input vector has a projection onto the leading right singular vector of the linear approximation of the explosion layer's feed-forward network (FFN) module.
    \item (\emph{Explosion}) Once triggered, the high-norm token in the layer's output aligns with the direction of the layer-wise singular defect direction.
    \item (\emph{Decay}) The layer that decays the high-norm tokens has a negative eigenvalue associated with an eigenvector that aligns with the singular defect direction.
\end{enumerate}

We empirically validate these findings on a variety of LLMs, including LLaMA2~\cite{touvron2023llama}, Phi3~\cite{abdin2024phi}, MPT~\cite{MosaicML2023Introducing}, Pythia~\cite{biderman2023pythia}, Vicuna1.5~\cite{platzer2021vicuna}, Falcon2~\cite{malartic2024falcon211btechnicalreport}, GPT2~\cite{radford2019language}, Qwen2.5~\cite{qwen25}, to name a few.

In addition, we discuss the behavior of high-norm tokens during training.
Our experiments reveal that the direction of high-norm tokens gradually stabilizes in training and remains consistent even after fine-tuning.
Moreover, we conjecture that the causal self-attention mechanism is one of the defining factors for the emergence of high-norm tokens.

Finally, we demonstrate that a better understanding of the high-norm tokens in LLMs can lead to novel applications.
\emph{i) High-Norm Aware Quantization Design}.
Outlier activations induced by high-norm tokens cause significant performance in low-bit quantization.
To mitigate this, we propose a high-norm aware quantization strategy that selectively preserves precision for these critical layers, improving robustness without compromising efficiency.
\emph{ii) LLM Signature via Singular Defects}.
The singular defect direction, which stabilizes in the late training stages and persists through fine-tuning, serves as a robust model signature.
This signature enables distinguishing whether an LLM was fine-tuned from another model and thus detect model infringement.

Ultimately, we believe that understanding singular defects will not only stimulate novel applications but also spur new insights into the internal mechanism of LLMs. We will therefore make our code publicly available.
