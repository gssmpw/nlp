
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/paper/pythia_1b_angle_iter_diff.pdf}
    \vspace{-0.5em}
    \caption{Direction of high-norm tokens stabilizes during training.
    The \(y\)-axis is the angle between the empirical high-norm directions of adjacent Pythia-1B checkpoints at a \(10,000\) interval.
    }\label{fig:stable_dir}
    \vspace{-0.5em}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \includegraphics[width=\textwidth]{figures/paper/mistral3_7b_norm_3d.pdf}
        \caption{Mistral-7B-v0.3}\label{fig:mistral3_7b_norm_3d}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \includegraphics[width=\textwidth]{figures/paper/bert_large_cased_norm_3d.pdf}
        \caption{BERT-Large-Cased}\label{fig:bert_large_cased_norm_3d}
    \end{subfigure}
    \vspace{-0.5em}
    \caption{(a) Modifying the causal self-attention mechanism affects the emergence of high-norm tokens.
    (b) Bi-directional models do not show high-norm tokens.
    More examples are in \cref{sec:more_bert}.
    }\label{fig:factors}
    \vspace{-0.5em}
\end{figure}

\begin{figure*}[t]
    \centering
    \rotatebox{90}{\tiny\qquad Iter 143K}~\includegraphics[width=0.24\textwidth]{figures/paper/pythia_410m_norm_2d_step143000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_1.4b_norm_2d_step143000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_2.8b_norm_2d_step143000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_6.9b_norm_2d_step143000.pdf}\\
    \rotatebox{90}{\tiny\qquad Iter 50K}~\includegraphics[width=0.24\textwidth]{figures/paper/pythia_410m_norm_2d_step50000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_1.4b_norm_2d_step50000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_2.8b_norm_2d_step50000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_6.9b_norm_2d_step50000.pdf}\\
    \rotatebox{90}{\tiny~\qquad Iter 5K}~\includegraphics[width=0.24\textwidth]{figures/paper/pythia_410m_norm_2d_step5000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_1.4b_norm_2d_step5000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_2.8b_norm_2d_step5000.pdf}
    \includegraphics[width=0.24\textwidth]{figures/paper/pythia_6.9b_norm_2d_step5000.pdf}\\
    {\tiny\qquad Pythia-410M\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad Pythia-1.4B\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad Pythia-2.8B\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad Pythia-6.9B}
    \vspace{-0.5em}
    \caption{High norm tokens in the Pythia model suite.
    Each row shows a different training iteration, with the top row being the final state at 143K iteration.
    Each column is a different model size, including 410M, 1.4B, 2.8B, and 6.9B\@.
    }\label{fig:pythia}
    \vspace{-1em}
\end{figure*}

\section{Discussion}\label{sec:discussion}

\paragraph{High-Norm Tokens During Training}

High-norm tokens in ViTs appear only in the later stage of training and only in large model variants~\cite{darcetvision}.
However, \emph{high-norm tokens in LLMs emerge early in training and across a variety of model sizes}, as demonstrated in \cref{fig:pythia} using the suite of Pythia models~\cite{biderman2023pythia}.
After 5K out of a total of 143K iterations, the norms of both the initial token and the token `\texttt{.}' have become significantly higher than that of the other tokens in the hidden states.
Furthermore, analyzing the change in the high-norm tokens as training progresses, we found that \emph{the high-norm direction gradually stabilizes during training}, as shown in \cref{fig:stable_dir}.

\paragraph{Which Factor Matters for the Emergence of High-Norm?}

From our extensive experiments with various LLMs,
we observed that the presence of high-norm tokens is not significantly affected by factors such as the training dataset, positional embedding scheme, FFN design, the use of grouped-query attention, parallel attention-FFN structures~\cite{wang2022gpt}, supervised fine-tuning(SFT), reinforcement learning with human feedback (RLHF), languages, context length, model size, or training iterations.
The root cause therefore remains an \emph{open question}.
Nevertheless, empirical evidence suggests that \emph{the causal self-attention mechanism may be one of the defining factors for the emergence of high-norm tokens}.
Firstly, note that all the high-norm examples we presented so far were trained with causal attention.
Secondly, Mistral~\cite{jiang2023mistral7b}, which uses a sliding window attention mechanism, does not exhibit high-norm tokens among the initial tokens, as shown in \cref{fig:factors}.
This indicates that modifying the causal self-attention mechanism affects the emergence pattern of high-norm tokens.
Thirdly, we did not observe high-norm tokens in bidirectional models, such as BERT, DistilBERT~\cite{sanh2019distilbert}, and RoBERTa~\cite{liu2019roberta}.
This implies that masked language modeling
does not induce high-norm tokens as in causal language modeling.
