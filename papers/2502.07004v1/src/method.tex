
\section{Analysis of High-Norm Tokens in LLM}\label{sec:method}

As shown in \cref{fig:llm_norm}, most recent LLMs
manifest high-norm tokens in intermediate layers, regardless of the model size, training data, variations in model architecture, etc.
Echoing the observations in~\cite{sun2024massive}, we note that the high-norm tokens appear abruptly in a certain layer and decay suddenly after another layer, with the initial token consistently having a high norm in the middle layers, while certain later tokens, such as the delimiter `\texttt{.}', also exhibit a high norm in some models.
These phenomena differ from those in vision transformers, where norms gradually increase without sudden drop (see Figure~4 in~\cite{darcetvision}), and high-norm tokens appear at random spatial locations.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/stages.png}
    \vspace{-0.5em}
    \caption{The four properties of high-norm tokens. The
        \(x\)-axis is the layer id, and the \(y\)-axis is the norm of the tokens.
        The results are obtained from LLaMA2-7B.
    }\label{fig:stages}
    \vspace{-0.5em}
\end{figure}

This section provides a comprehensive understanding of the distinctive high-norm phenomenon in LLMs by expanding the theory of singular defects that was originally developed for ViTs.
As illustrated in \cref{fig:stages}, we study this phenomenon from four perspectives, covering the full life cycle of the high-norm tokens:
(1) Development: the computational pathways leading to norm increases.
(2) Trigger: the cause of the norm increase just before the explosion layer.
(3) Explosion: the appearance of high-norm tokens in intermediate layers and their correlation with network parameters.
(4) Decay: how high-norm tokens disappear.

\subsection{\emph{Explosion}: Appearance of High-Norm Tokens}\label{sec:singular-defects}

The most obvious pattern in \cref{fig:llm_norm} is that the hidden states of some tokens have extremely high norms in the intermediate layers, and so consistently across different LLMs.
Interestingly, we find that for each model, all the high-norm tokens share \emph{the same direction}, regardless of the input text, the layer, and the position of the token in the sentence.
Taking LLaMA2-7B as an example, we extract the hidden states of 1K random rows from the \texttt{WikiText2-v1} dataset~\cite{merity2017pointer} across all layers and compute the norm of each token in each layer.
We collect all hidden states with a norm larger than 500.
The average pairwise angle between any two high-norm tokens is only \(3.1\) degrees, which verifies our claim.
Additional statistics for more models are provided in \cref{sec:high_norm_statistics}, demonstrating that this phenomenon is general across models.
We define the average of the high-norm tokens as the \emph{empirical high-norm direction}.
Notably, this behavior differs from DINOv2 where the direction of the high-norm tokens varies across layers.
Despite the distinctions between high-norm tokens in ViTs and LLMs, we next verify whether the theory of singular defects developed for DINOv2 is applicable to LLMs.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/transformer.png}
    \vspace{-0.5em}
    \caption{Transformer layer in LLaMA2.
        Given an input token \(x\), we show the approximate output of each module in \textcolor{red}{red}.
    }\label{fig:transformer}
    \vspace{-0.5em}
\end{figure}
Let us first review the basic concepts introduced in~\cite{wang2024sinder}.
The core of the theory of singular defects is the linear approximation of transformer layers under the single-token assumption.
It assumes that only a single token is provided as input, so that the interaction across tokens in self-attention can be ignored, making the mathematical analysis tractable.
Note that most recent LLMs are trained with causal self-attention, where a token can only attend to itself and the previous tokens.
Thus, the inference of the first token perfectly matches the single-token assumption.

We use LLaMA2 as an example to illustrate how to approximate the transformer layer as a linear operator.
The structure of the transformer layer is shown in \cref{fig:transformer}, where the self-attention module and the FFN module are the two residual components.
The self-attention module can be approximated as a matrix-vector multiplication,
\begin{equation}\label{eq:attention}
    \textrm{Attention}(x) \approx Ax:= A_2 A_1 A_0 x,
\end{equation}
where \(A_0\) is a diagonal matrix representing the weight of the attention RMSNorm, \(A_1\) is the weight matrix of the value projection, and \(A_2\) is the weight matrix of the output projection.
Note that for LLMs that use a different self-attention design, the approximation of the self-attention module can be adjusted accordingly.
For example, LLaMA3 uses a grouped query attention~\cite{ainslie2023gqa} whose key-value heads are repeated.
Then, we can, accordingly, repeat the weight matrix of the value in the approximation.

The FFN module can be approximated as
\begin{equation}\label{eq:ffn}
    \textrm{FFN}(x) \approx Fx := F_2 F_1 F_0 x,
\end{equation}
where \(F_0\) is a diagonal matrix representing the weight of the feed-forward RMSNorm, \(F_1\) is the least-square linear approximation (see~\cite{wang2024sinder} for details) of the nonlinear function \(F_1 x \approx \textrm{silu}(W_1 x) \odot (W_3 x)\), and \(F_2\) is the weight matrix of the output projection.
For LLMs that use alternative FFN designs, e.g., \(F_1 x \approx \textrm{GELU}(W_1 x)\) in Pythia, the approximation of the FFN module can be adjusted accordingly.

Combining the two modules with the identity paths, the transformer layer can be approximated as
\begin{equation}\label{eq:layer}
    \textrm{Layer}(x) \approx L x := x + (A+F+FA)x =: (I + R)x,
\end{equation}
where the right-hand side decomposes the approximated matrix into an identity path \(I\) and a residual path \(R\).
The \emph{layer-wise singular defect direction} is then defined as the leading left singular vector of the matrix \(L\) for each layer.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/paper/llama2_7b_angle.pdf}
    \vspace{-0.5em}
    \caption{Acute angle between the predicted layer-wise singular defect directions and the empirical high-norm direction for LLaMA2-7B.
    The predictions for more LLMs are provided in \cref{sec:more_explosion}.
    }\label{fig:llama2_angle}
    \vspace{-0.5em}
\end{figure}

We compute the layer-wise singular defect directions of LLaMA2-7B and compare them with the empirical high-norm direction.
The acute angles between the predicted directions and the empirical one are provided in \cref{fig:llama2_angle}.
We observe layer-2 and layer-31 to yield very small angles (\(6.05\) and \(2.93\) degrees, respectively) with the empirical high-norm direction.
These two layers correspond to the layer that increases the norm and decreases the norm of the tokens, respectively (see~\cref{fig:llama2_7b_norm_3d}).
Layers 3--30 only cause small perturbations to the high-norm tokens.
Thus, the high-norm tokens in the hidden states are created by layer-2, then preserved by the identity path between layer-3 and layer-30, and suppressed by layer-31.
This explains why there always exists a set of \emph{fixed} channels that have high activations~\cite{sun2024massive}: the high-norm tokens are (nearly) unchanged in the intermediate layers.
Unsurprisingly, we observe that the angles between the layer-wise predicted directions and the empirical directions are large, as they do not modify the high-norm tokens.
Note that the accumulated singular defect directions (see~\cite{wang2024sinder} for a formal definition) may mislead us about the contribution of the intermediate layers to the high norms.
Therefore, in the remainder of the paper, we will focus on layer-wise singular defect directions.

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=0.9\columnwidth]{figures/paper/llama2_7b_eig.pdf}}
        \vspace{-0.5em}
        \caption{For LLama2-7B,
            the minimum angles between the eigenvectors of \(R\) and the empirical high-norm direction are shown in \textcolor{red}{red}, and the corresponding eigenvalues are shown in \textcolor{blue}{blue}.
        }\label{fig:llama2_7b_negeig}
        \vspace{-2.5em}
    \end{center}
\end{figure}

\subsection{\emph{Decay}: Eigenvalues of Decay Layers}\label{sec:eigenvalue}

The leading left singular vector of the matrix \(L\) encodes the output direction having the largest norm for all possible unit-length inputs, and the corresponding singular value is the norm of the output in that direction.
However, this interpretation contrasts with the behavior of the \emph{decay layer}, which greatly reduces the norms of high-norm tokens.
To better describe the behavior of such a layer, we instead propose to use eigenvalue and eigenvector decomposition.

Consider the decomposition \(L=I+R\) in \cref{eq:layer}, and let \(x\) be a high-norm token input to a decay layer.
If after the layer, the token does not have a high norm anymore, then \(Lx\approx 0\), and therefore \(Rx\approx -x\).
This implies that \emph{the high-norm token \(x\) should be an eigenvector of the residual matrix \(R\) with a negative eigenvalue}.

To verify this intuition, we compute the minimum angle between the eigenvectors of the residual matrix \(R\) and the empirical high-norm direction for LLaMA2-7B.
In \cref{fig:llama2_7b_negeig}, we plot these angles for different layers, together with the corresponding eigenvalues.
At the decay layer-31, the eigenvector of the residual matrix \(R\) forms a small angle with the empirical high-norm direction, with a relatively large negative eigenvalue\footnote{
For simplicity, we only consider the real part of eigenvalues and eigenvectors, having observed that the imaginary part of the eigenvalues and eigenvectors of interest is close to zero.}.
We verify this for other models in \cref{sec:more_diminish}.
A corollary of this observation is that, \emph{if the input to the decay layer is not a high-norm token, it will not produce a new high-norm token},
i.e., the layer specializes in shrinking the token norm in the high-norm direction.

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=0.9\columnwidth]{figures/paper/llama2_7b_2nd_type.pdf}}
    \vspace{-0.5em}
    \caption{Attention-independent exploding path of LLaMA2-7B.
            The \(y\)-axis is the norm of each token at the output of layer-2 after removing all self-attention blocks from the model.
            The largest five tokens together with their norms are annotated in the figure.
        }\label{fig:llama2_7b_noattn}
        \vspace{-1.5em}
    \end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=0.9\columnwidth]{figures/paper/llama2_7b_1st_type.pdf}}
    \vspace{-0.5em}
    \caption{Attention-related exploding path of LLaMA2-7B for initial tokens.
            The \(y\)-axis is the norm of each token at the output of layer-2.
            We also show the norm (sorted) of 32,000 random input token embeddings (not learned by the network) after layer-2.
        }\label{fig:llama2_7b_withattn}
    \vspace{-1.5em}
\end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=0.9\columnwidth]{figures/paper/llama2_7b_ffn_output.pdf}}
    \vspace{-0.5em}
    \caption{Norm of output tokens of FFN at layer-2 of LLaMA2-7B using the right singular vectors of \(F\) as input tokens to FFN.
        More examples are provided in \cref{sec:more_subspace}.
        }\label{fig:llama2_7b_ffn_output}
    \vspace{-1.5em}
\end{center}
\end{figure}

\subsection{\emph{Development}: The Two Types of Exploding Paths}\label{sec:development}

We now focus on the development of the high-norm tokens before the ``explosion''.
To this end, we refer to the computation path related to the appearance of high norms between the input token embedding and the explosion layer as the \emph{exploding path}.
We differentiate two types of high-norm tokens: the \emph{initial} token and the \emph{noninitial} high-norm tokens.
An example of the noninitial high-norm token is the delimiter token `\texttt{.}' in LLaMA2-7B.
We observed that the first occurrence of the `\texttt{.}' token has a high norm in the intermediate hidden states, regardless of the content before it.
This implies that \emph{the self-attention layers, the only places where inter-token interactions occur, play an insignificant role in noninitial high-norm tokens}.

To verify this argument, we remove the self-attention layers and feed each token available in the tokenizer individually to the model.
The norm of the hidden states after the explosion layer is shown in \cref{fig:llama2_7b_noattn}.
Out of the five tokens with the highest norm, four of them belong to noninitial high-norm tokens, including `\texttt{.}', `\texttt{\(_\circ\)}', `\texttt{\textless s\textgreater}' and `\texttt{\textbackslash n}'.
Among them, the Chinese delimiter token `\texttt{\(_\circ\)}' and the special token `\texttt{\textless s\textgreater}' are newly discovered high-norm tokens that were not identified in previous work.

Given that the high norm of noninitial tokens is unrelated to self-attention, such tokens can also be analyzed under the single-token assumption.
Therefore, our methodology for predicting the high-norm directions can be applied to both the initial token and the noninitial high-norm tokens.
This sheds light on why the two types of high-norm tokens share the same direction.

For the initial token, we observed the exploding path to be related to self-attention.
For example, in \cref{fig:llama2_7b_withattn}, we plot the norm of all the trained tokens after the explosion layer-2 in LLaMA2-7B.
Almost all tokens have a high norm.
This aligns with the observation that the initial token always has a high norm, regardless of the word it encodes.
Surprisingly, even a random token embedding that is not learned by the network also has a higher norm than a noninitial normal token, whose average norm is \(46.88\) in the explosion layer-2 of LLaMA2-7B, although the norm of the random token may be lower than that of the learned tokens.
The analysis for other models is provided in \cref{sec:more_development}.

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=0.9\columnwidth]{figures/paper/llama2_7b_subspace_coef.pdf}}
    \vspace{-0.5em}
    \caption{Coefficient of tokens projected to the leading right singular vector of \(F\) just before the FFN in layer-2 of LLaMA2-7B.
        Analysis for more models is provided in \cref{sec:more_subspace}.
        }\label{fig:llama2_7b_subspace_coef}
    \vspace{-1.5em}
\end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.45\linewidth]{figures/paper/llama2_7b_norm_3d_no_right_sv.pdf}
    \vspace{-0.5em}
    \caption{Norm of tokens after removing the component on the explosion subspace before layer-2's FFN in LLaMA2-7B\@.
            High-norm tokens disappear, however, the scale of the final outputs becomes abnormal, leading to low-quality generated text.
        }\label{fig:llama2_7b_trim}
    \vspace{-1.5em}
\end{center}
\end{figure}

\subsection{\emph{Trigger}: The Explosion Subspace}

Let us now focus on the explosion layer where the norm of tokens undergoes a sharp increase.
Tracking the change in norm within that transformer layer, we observed that the abrupt increase occurs in the FFN module.
For example, in the explosion layer of LLaMA2-7B (layer 2), the average norm of the initial tokens before/after FFN is \(3.49/932.25\), respectively.
Let \(F\) be the linear approximation of this FFN module following \cref{eq:ffn}, and the SVD of \(F\) be \(F=U\Sigma V^T\).
The columns of \(V\) form an orthonormal basis of the input space.
We feed these unit-length base vectors to the FFN and plot their output norms in \cref{fig:llama2_7b_ffn_output}.
Note that the leading right singular vector of \(F\) is the only direction that undergoes norm explosion when passed through FFN.
We thus refer to the 1-dimensional subspace spanned by this vector as the \emph{explosion subspace}.
This lets us conjecture that \emph{the high-norm tokens have a large component in the explosion subspace, while the normal tokens do not}.
This is verified in \cref{fig:llama2_7b_subspace_coef}.
For any token at the initial position in the input text (which is thus a high-norm token), the projection of its feature just before the FFN module at layer-2 onto the explosion subspace has a large coefficient (around 1.5).
However, when the same token is placed at the second position in the input (where it is not a high-norm token, except for the few noninitial high-norm tokens discussed in \cref{sec:development}), the coefficient is much smaller.

One potential way to avoid high-norm in intermediate layers could therefore be to remove the component in the explosion subspace just before the FFN at the explosion layer.
An example of the resulting token norms is shown in \cref{fig:llama2_7b_trim}.
However, we observe that the network loses its ability to generate high-quality text and instead produces random texts.
This shows that \emph{high-norm tokens are important for the performance of an existing trained model}.

The life cycle of high-norm tokens can be outlined as follows.
Firstly, the potential high-norm tokens develop a sufficiently large component in the explosion subspace.
Then, this component explodes, yielding a high-norm token,
whose direction can be predicted by the singular defect theory.
Finally, the decay layer produces a negative direction to neutralize the high-norm token, which is described by the negative eigenvalue and eigenvector of that layer.
