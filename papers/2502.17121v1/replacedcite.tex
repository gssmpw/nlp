\section{Related Work}
\label{sec:related-work}
\looseness -1 

\paragraph{Label poisoning.} 
____ first analyzed label poisoning attacks, showing that flipping a small number of training labels severely degrades SVM performance. 
____ later formalized optimal label flip attacks under budget constraints as a bilevel optimization problem, which then expanded to transferable attacks on black-box models ____, considering arbitrary attacker objectives.
Beyond SVMs, recent works have explored label poisoning in backdoor attack scenarios, where adversaries inject triggers or alter triggerless data with poisoned labels in multi-label settings ____.
In contrast, our approach focuses on triggerless poisoning attacks.
\looseness -1

Defenses against these attacks include heuristic-based kernel correction ____, which uses expectation for $Q$ in (\ref{eq:outer-problem-objective}),
though assuming independent label flipping with equal probability--a condition not guaranteed in practice.
Other defenses such as clustering-based filtering ____, data complexity analysis ____, re-labeling ____ and label smoothing ____ offer straightforward solutions, however, they do not scale well to high-dimensional or large datasets.
Sample weighting based on local intrinsic dimensionality (LID) ____ shows promise, but relies on accurate and computationally expensive LID estimation.
Our approach, however, avoids strong assumptions about the data distribution or the attacker, preserves feasibility, and scales effectively to large-scale problem instances as demonstrated in Section~\ref{sec:experiments}. \looseness -1
Additionally, while learning under noisy labels ____ may seem relevant,
our work focuses specifically on \textit{adversarial} label noise ____, where the adversary \textit{intentionally} crafts the most damaging label perturbations. 
\looseness -1 
\vspace{-0.2cm}
\paragraph{Adversarial training (AT).} 
Adversarial examples, introduced by ____, revealed how small perturbations cause misclassification in deep neural networks (DNNs).
Building on this, AT ____ emerged as a prominent defense, training models on both original and adversarially perturbed data.
Defenses have utilized adversarial examples generated by methods such as the Fast Gradient Sign Method (FGSM) ____, PGD ____, Carlini \& Wagner attack ____, among others ____.
For SVMs, ____ formulated convex AT for linear SVMs, later extended to kernel SVMs by ____ via doubly stochastic gradients under feature perturbations.
Despite this progress, AT for label poisoning remains underexplored.
\textsc{Floral} fills this gap, by leveraging AT specifically for label poisoning scenarios, using PGD to train models on poisoned datasets rather than generating adversarial examples.
\looseness -1

In parallel, game-theoretical approaches have modeled adversarial interactions 
as simultaneous games, where classifiers and adversaries select strategies independently
____, or as Stackelberg games with a leader-follower dynamic ____. 
AT has further linked these concepts, particularly in simultaneous zero-sum games ____ to non-zero-sum formulations ____.
We adopt a sequential setup, using the Stackelberg framework where the leader commits to a strategy and the follower responds accordingly.
\looseness -1