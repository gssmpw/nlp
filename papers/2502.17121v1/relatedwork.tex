\section{Related Work}
\label{sec:related-work}
\looseness -1 

\paragraph{Label poisoning.} 
\citet{biggio2012poisoning} first analyzed label poisoning attacks, showing that flipping a small number of training labels severely degrades SVM performance. 
\citet{adversarial-flip-svm} later formalized optimal label flip attacks under budget constraints as a bilevel optimization problem, which then expanded to transferable attacks on black-box models \citep{label-contamination-linear}, considering arbitrary attacker objectives.
Beyond SVMs, recent works have explored label poisoning in backdoor attack scenarios, where adversaries inject triggers or alter triggerless data with poisoned labels in multi-label settings \citep{label-poisoning, chen2022clean}.
In contrast, our approach focuses on triggerless poisoning attacks.
\looseness -1

Defenses against these attacks include heuristic-based kernel correction \citep{svm-adversarial-noise}, which uses expectation for $Q$ in (\ref{eq:outer-problem-objective}),
though assuming independent label flipping with equal probability--a condition not guaranteed in practice.
Other defenses such as clustering-based filtering \citep{curie,tavallali2022adversarial}, data complexity analysis \citep{chan2018data}, re-labeling \citep{label-sanitization} and label smoothing \citep{rosenfeld2020certified} offer straightforward solutions, however, they do not scale well to high-dimensional or large datasets.
Sample weighting based on local intrinsic dimensionality (LID) \citep{defending-svms, ma2018characterizing} shows promise, but relies on accurate and computationally expensive LID estimation.
Our approach, however, avoids strong assumptions about the data distribution or the attacker, preserves feasibility, and scales effectively to large-scale problem instances as demonstrated in Section~\ref{sec:experiments}. \looseness -1
Additionally, while learning under noisy labels \citep{frenay2013classification, learning-w-noisy-labels,hallaji2023label, zhang2024effective} may seem relevant,
our work focuses specifically on \textit{adversarial} label noise \citep{svm-adversarial-noise}, where the adversary \textit{intentionally} crafts the most damaging label perturbations. 
\looseness -1 
\vspace{-0.2cm}
\paragraph{Adversarial training (AT).} 
Adversarial examples, introduced by \citet{szegedy2013intriguing}, revealed how small perturbations cause misclassification in deep neural networks (DNNs).
Building on this, AT \citep{goodfellow-2014} emerged as a prominent defense, training models on both original and adversarially perturbed data.
Defenses have utilized adversarial examples generated by methods such as the Fast Gradient Sign Method (FGSM) \citep{goodfellow-2014}, PGD \citep{madry2017towards}, Carlini \& Wagner attack \citep{carlini2017towards}, among others \citep{chen2017zoo, moosavi2016deepfool}.
For SVMs, \citet{zhou2012adversarial} formulated convex AT for linear SVMs, later extended to kernel SVMs by \citet{fast-scalable-adv-svm} via doubly stochastic gradients under feature perturbations.
Despite this progress, AT for label poisoning remains underexplored.
\textsc{Floral} fills this gap, by leveraging AT specifically for label poisoning scenarios, using PGD to train models on poisoned datasets rather than generating adversarial examples.
\looseness -1

In parallel, game-theoretical approaches have modeled adversarial interactions 
as simultaneous games, where classifiers and adversaries select strategies independently
\citep{adversarial-classification}, or as Stackelberg games with a leader-follower dynamic \citep{bruckner2011stackelberg, zhou2019survey, chivukula2020game}. 
AT has further linked these concepts, particularly in simultaneous zero-sum games \citep{hsieh2019finding, pinot2020randomization, pal2020game} to non-zero-sum formulations \citep{at-nonzero-game}.
We adopt a sequential setup, using the Stackelberg framework where the leader commits to a strategy and the follower responds accordingly.
\looseness -1