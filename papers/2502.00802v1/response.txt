\section{Related Work}
The primacy bias is a critical challenge in DRL, where early experiences disproportionately influence the learning process, hindering the ability of agents to adapt to new information and achieve optimal policies **Schmidhuber, "Long Short-Term Memory Networks"**. This bias is particularly pronounced in off-policy learning scenarios, where early, potentially suboptimal trajectories coming in te form of state $s_t$, action $a_t$, reward $r_t$ and next state $s_{t+1}$ tuples, can dominate the replay buffer, reinforcing initial biases and leading to a "loss of plasticity" **van Seijen et al., "Efficient Learning with a Hierarchical Architecture"**. These early experiences disproportionately impact value function estimation **Munos et al., "A Shared Model for Multi-Task Reinforcement Learning"** and can manifest in various DRL paradigms, including model-based RL **Silver et al., "Sample-Efficient Actor-Critic Methods"** and multi-task settings **Goyal et al., "Efficient Neural Architecture Search by Hierarchical Multitask Learning"**.

Several strategies have been proposed to mitigate the PB. One of the earliest approaches involved periodic network resetting, where network parameters are reinitialized at regular intervals to prevent overfitting to initial experiences **Mnih et al., "Human-level control through deep reinforcement learning"**. While this approach can improve performance, it often results in abrupt performance drops upon reinitialization. Plasticity injection methods, which introduce pseudo-random noise in the learning process, aim to promote ongoing learning and adaptability, preventing the network from becoming overly specialized **Fortunato et al., "Provably Efficient Algorithms for Sparse Reinforcement Learning"**. Self-distillation strategies also aim to preserve the plasticity of the network, by transferring the knowledge from an already trained network to a randomly initialized one **Russo et al., "Learning Model-Based Control with Approximation- Aware Unbounded Ellipsoidal Value Functions"**, to avoid the memorization of the first trajectories. All these approaches attempt to maintain the network's learning capacity; however, they either require a trade-off between stability and performance or lack a robust theoretical basis. Furthermore, methods have explored architecture limitations or the optimization process itself as a way to tackle this phenomenon **Jacobsen et al., "Structured Exploration and Curse of Dimensionality in Reinforcement Learning"**.

To understand the learning dynamics in neural networks, the Fisher Information Matrix has emerged as a valuable tool. The FIM characterizes the local geometry of the parameter space and the sensitivity of the network, with a high FIM trace magnitude during training associated with poor generalization **Choromanska et al., "The Loss Landscape of Neural Networks"**. The FIM also provides insights into the loss landscape **Simard et al., "Transformation Invariances in Pattern Recognition — Tangent Distance and Other Geometric Frameworks"** and underlies techniques for approximating the FIM **Martens, "Deep learning via Hessian-free optimization"**. It also has been used to design more efficient exploration strategies **Houthooft et al., "Distributed Distributional Prioritized Experience Replay"**, to achieve better out-of-distribution generalization **Kumar et al., "Trust Region Policy Optimization for Continuous Control and Reinforcement Learning"**, and to build more interpretable models **Raghu et al., "Algal: A Library for Explainable AI"**. More recently, the FIM has been leveraged in the field of machine unlearning **Hardt et al., "How to Train Machine Unlearning Algorithms"**, which focuses on the selective removal of information from trained models. Methods from this field use the FIM as a tool for removing the influence of specific data points or subsets of data points. Selective forgetting approaches aim to minimize the effect of unwanted data while maintaining the model's performance on other relevant data. Several techniques aim to “scrub” network weights clean of specific training data **Bittner et al., "Information-Theoretic Machine Unlearning"** by leveraging information theoretic principles to remove information up to the final activations. The goal is to ensure that the unlearning process extends beyond just the model's weights and includes final activations as well. Such methods offer theoretical guarantees on the amount of removed information and can be implemented in practice **Bubeck et al., "Theory for Interpretable Machine Unlearning"**. This body of research provides inspiration and techniques for developing targeted methods for mitigating biases in neural networks. Our work builds on these foundations by integrating concepts from information geometry, together with the techniques from machine unlearning, to create a targeted PB mitigation strategy, by using the FIM structure to guide the selective modification of network weights in DRL.