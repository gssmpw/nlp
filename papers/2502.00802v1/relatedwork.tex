\section{Related Work}
The primacy bias is a critical challenge in DRL, where early experiences disproportionately influence the learning process, hindering the ability of agents to adapt to new information and achieve optimal policies \cite{nikishin2022primacy}. This bias is particularly pronounced in off-policy learning scenarios, where early, potentially suboptimal trajectories coming in te form of state $s_t$, action $a_t$, reward $r_t$ and next state $s_{t+1}$ tuples, can dominate the replay buffer, reinforcing initial biases and leading to a "loss of plasticity" \cite{abbas2023loss, d2022sample}. These early experiences disproportionately impact value function estimation \cite{lyle2022learning, lyle2022understanding, van2018deep} and can manifest in various DRL paradigms, including model-based RL \cite{qiao2023primacy} and multi-task settings \cite{chohard}.

Several strategies have been proposed to mitigate the PB. One of the earliest approaches involved periodic network resetting, where network parameters are reinitialized at regular intervals to prevent overfitting to initial experiences \cite{nikishin2022primacy}. While this approach can improve performance, it often results in abrupt performance drops upon reinitialization. Plasticity injection methods, which introduce pseudo-random noise in the learning process, aim to promote ongoing learning and adaptability, preventing the network from becoming overly specialized \cite{sokar2023dormant, nikishin2024deep}. Self-distillation strategies also aim to preserve the plasticity of the network, by transferring the knowledge from an already trained network to a randomly initialized one \cite{li2024eliminating}, to avoid the memorization of the first trajectories. All these approaches attempt to maintain the network's learning capacity; however, they either require a trade-off between stability and performance or lack a robust theoretical basis. Furthermore, methods have explored architecture limitations or the optimization process itself as a way to tackle this phenomenon \cite{obando2024value, asadi2024resetting, li2023efficient}.

To understand the learning dynamics in neural networks, the Fisher Information Matrix has emerged as a valuable tool. The FIM characterizes the local geometry of the parameter space and the sensitivity of the network, with a high FIM trace magnitude during training associated with poor generalization \cite{jastrzebski2021catastrophic}. The FIM also provides insights into the loss landscape \cite{hochreiter1997flat} and underlies techniques for approximating the FIM \cite{martens2015optimizing, george2018fast}. It also has been used to design more efficient exploration strategies \cite{kakade2001natural} and to achieve better out-of-distribution generalization \cite{pascanu2013revisiting, rame2022fishr} and to build more interpretable models \cite{luber2023structural}. More recently, the FIM has been leveraged in the field of machine unlearning \cite{Xu2023MachineUA}, which focuses on the selective removal of information from trained models. Methods from this field use the FIM as a tool for removing the influence of specific data points or subsets of data points. Selective forgetting approaches aim to minimize the effect of unwanted data while maintaining the model's performance on other relevant data. Several techniques aim to “scrub” network weights clean of specific training data \cite{golatkar2020forgetting, golatkar2020eternal} by leveraging information theoretic principles to remove information up to the final activations. The goal is to ensure that the unlearning process extends beyond just the model's weights and includes final activations as well. Such methods offer theoretical guarantees on the amount of removed information and can be implemented in practice \cite{ramkumar2024effectiveness}. This body of research provides inspiration and techniques for developing targeted methods for mitigating biases in neural networks. Our work builds on these foundations by integrating concepts from information geometry, together with the techniques from machine unlearning, to create a targeted PB mitigation strategy, by using the FIM structure to guide the selective modification of network weights in DRL.