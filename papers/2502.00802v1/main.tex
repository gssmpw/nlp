


%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{xcolor} % Required for defining custom colors
\definecolor{lightgray}{rgb}{0.9,0.9,0.9} % Light gray for the first column
\definecolor{myyellow}{rgb}{0.95, 0.9, 0.5} % Example yellow definition
\definecolor{mygreen}{rgb}{0.6, 0.8, 0.4}  % Example green definition

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{colortbl}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\definecolor{MRED}{HTML}{60aba0}
\definecolor{MTEAL}{HTML}{59a89c}
\definecolor{MGOLD}{HTML}{f0c572}
\definecolor{myMagenta}{rgb}{1, 0, 1}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Fisher-Guided Selective Forgetting for Deep Reinforcement Learning}


\begin{document}

\twocolumn[
\icmltitle{Fisher-Guided Selective Forgetting: \\
            Mitigating The Primacy Bias in Deep Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Massimiliano Falzari}{equal,yyy}
\icmlauthor{Matthia Sabatelli}{equal,yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Bernulli Institute, University of Groningen, Groningen, Netherlands}

\icmlcorrespondingauthor{Massimiliano Falzari}{massimiliano@falzari.dev}
\icmlcorrespondingauthor{Matthia Sabatelli}{m.sabatelli@rug.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.




\begin{abstract}
Deep Reinforcement Learning (DRL) systems often tend to overfit to early experiences, a phenomenon known as the primacy bias (PB). This bias can severely hinder learning efficiency and final performance, particularly in complex environments.
This paper presents a comprehensive investigation of PB through the lens of the Fisher Information Matrix (FIM). We develop a framework characterizing PB through distinct patterns in the FIM trace, identifying critical memorization and reorganization phases during learning. Building on this understanding, we propose Fisher-Guided Selective Forgetting (FGSF), a novel method that leverages the geometric structure of the parameter space to selectively modify network weights, preventing early experiences from dominating the learning process. 
Empirical results across DeepMind Control Suite (DMC) environments show that FGSF consistently outperforms baselines, particularly in complex tasks. We analyze the different impacts of PB on actor and critic networks, the role of replay ratios in exacerbating the effect, and the effectiveness of even simple noise injection methods. 
Our findings provide a deeper understanding of PB and practical mitigation strategies, offering a FIM-based geometric perspective for advancing DRL.



\end{abstract}

\section{Introduction}

Deep Reinforcement Learning (DRL) agents often suffer from a critical issue known as the primacy bias (PB), where early experiences disproportionately influence the learning process, hindering the ability to adapt to new information and achieve optimal performance \cite{nikishin2022primacy}. This phenomenon, related to the primacy effect in human cognition, can lead to suboptimal policies and limit generalization, presenting a significant bottleneck in the development of robust and efficient DRL systems. The core of the PB problem lies in the interplay between neural network learning dynamics and the non-stationary nature of reinforcement learning \cite{abbas2023loss,lyle2023understanding}. Early interactions often occur during the exploration phase, when an agent's policy is far from optimal. The neural network then tends to overfit to these initial experiences, shaping its representation in a way that makes subsequent learning from novel situations more difficult \cite{lyle2022learning}. While crucial for stable off-policy learning, the replay buffer exacerbates this effect by continually reinforcing these early, potentially misleading experiences \cite{nikishin2022primacy}. This can lead to a "loss of plasticity," as the network loses its capacity to adapt effectively to new scenarios \cite{abbas2023loss}. To mitigate the negative impact of the PB, various techniques have been proposed, ranging from periodic network resetting \cite{nikishin2022primacy} to pseudo-random noise injection in the learning process \cite{sokar2023dormant}. However, these methods often lack a deep understanding of the phenomenon's underlying mechanisms. In this paper, we address the PB through the lens of information geometry. Specifically, we leverage the Fisher Information Matrix (FIM), a tool to characterize the local geometry of the parameter space and measure network sensitivity \cite{amari2016information}. Through this lens, we identify distinctive phases in the learning process that are characterized by a unique pattern in the evolution of the FIM’s trace \cite{achille2018critical,jastrzebski2021catastrophic}, and develop a targeted and principled mitigation strategy, Fisher-Guided Selective Forgetting (FGSF). 

The contributions of this paper are therefore threefold: first, we propose a novel characterization of the PB by introducing a new method that quantifies the PB via the FIM trace evolution and its derivatives; second we introduce Fisher-Guided Selective Forgetting (FGSF) a principled mitigation strategy that relies on a geometric understanding of the PB to selectively modify network weights; third through extensive experiments across multiple environments we systematically evaluate FGSF, compare its performance against existing mitigation strategies, and analyze the impact of difference hyperparameters choices, assessing the superiority of our proposed approach over existing alternatives.



%This paper aims to address this gap, by addressing these key questions: How can we characterize the primacy bias in DRL systems?
%Can a principled, information-geometry-based approach, informed by that characterization, effectively mitigate the primacy bias?
%How do such mitigation strategies perform when they are compared to simpler, yet widely used mitigation strategies, such as periodic network reset?
%The FGSF method adapts techniques from machine unlearning \cite{golatkar2020eternal} by leveraging the FIM structure to selectively modify the network weights. This allows us to reduce the impact of early, potentially misleading experiences while retaining valuable learned behaviors. Through experiments on multiple environments of varying complexities, we demonstrate the effectiveness of FGSF, showing that it can provide more robust and effective DRL systems. 


\section{Related Work}
The primacy bias is a critical challenge in DRL, where early experiences disproportionately influence the learning process, hindering the ability of agents to adapt to new information and achieve optimal policies \cite{nikishin2022primacy}. This bias is particularly pronounced in off-policy learning scenarios, where early, potentially suboptimal trajectories coming in te form of state $s_t$, action $a_t$, reward $r_t$ and next state $s_{t+1}$ tuples, can dominate the replay buffer, reinforcing initial biases and leading to a "loss of plasticity" \cite{abbas2023loss, d2022sample}. These early experiences disproportionately impact value function estimation \cite{lyle2022learning, lyle2022understanding, van2018deep} and can manifest in various DRL paradigms, including model-based RL \cite{qiao2023primacy} and multi-task settings \cite{chohard}.

Several strategies have been proposed to mitigate the PB. One of the earliest approaches involved periodic network resetting, where network parameters are reinitialized at regular intervals to prevent overfitting to initial experiences \cite{nikishin2022primacy}. While this approach can improve performance, it often results in abrupt performance drops upon reinitialization. Plasticity injection methods, which introduce pseudo-random noise in the learning process, aim to promote ongoing learning and adaptability, preventing the network from becoming overly specialized \cite{sokar2023dormant, nikishin2024deep}. Self-distillation strategies also aim to preserve the plasticity of the network, by transferring the knowledge from an already trained network to a randomly initialized one \cite{li2024eliminating}, to avoid the memorization of the first trajectories. All these approaches attempt to maintain the network's learning capacity; however, they either require a trade-off between stability and performance or lack a robust theoretical basis. Furthermore, methods have explored architecture limitations or the optimization process itself as a way to tackle this phenomenon \cite{obando2024value, asadi2024resetting, li2023efficient}.

To understand the learning dynamics in neural networks, the Fisher Information Matrix has emerged as a valuable tool. The FIM characterizes the local geometry of the parameter space and the sensitivity of the network, with a high FIM trace magnitude during training associated with poor generalization \cite{jastrzebski2021catastrophic}. The FIM also provides insights into the loss landscape \cite{hochreiter1997flat} and underlies techniques for approximating the FIM \cite{martens2015optimizing, george2018fast}. It also has been used to design more efficient exploration strategies \cite{kakade2001natural} and to achieve better out-of-distribution generalization \cite{pascanu2013revisiting, rame2022fishr} and to build more interpretable models \cite{luber2023structural}. More recently, the FIM has been leveraged in the field of machine unlearning \cite{Xu2023MachineUA}, which focuses on the selective removal of information from trained models. Methods from this field use the FIM as a tool for removing the influence of specific data points or subsets of data points. Selective forgetting approaches aim to minimize the effect of unwanted data while maintaining the model's performance on other relevant data. Several techniques aim to “scrub” network weights clean of specific training data \cite{golatkar2020forgetting, golatkar2020eternal} by leveraging information theoretic principles to remove information up to the final activations. The goal is to ensure that the unlearning process extends beyond just the model's weights and includes final activations as well. Such methods offer theoretical guarantees on the amount of removed information and can be implemented in practice \cite{ramkumar2024effectiveness}. This body of research provides inspiration and techniques for developing targeted methods for mitigating biases in neural networks. Our work builds on these foundations by integrating concepts from information geometry, together with the techniques from machine unlearning, to create a targeted PB mitigation strategy, by using the FIM structure to guide the selective modification of network weights in DRL.


\section{Fisher-Guided Selective Forgetting}

To effectively address the primacy bias, we introduce Fisher-Guided Selective Forgetting (FGSF), a method that combines insights from information geometry and machine unlearning. The core of our approach is based on the Fisher Information Matrix and its ability to capture the learning dynamics of neural networks.

\subsection{The Fisher Information Matrix (FIM)}

The FIM is a fundamental concept in information geometry that quantifies the amount of information a random variable carries about an unknown parameter. In the context of neural networks, the FIM provides a measure of the sensitivity of the network's output with respect to its parameters. Given a neural network with parameters $\theta$, and a probability distribution $p(x|\theta)$, the FIM, denoted as $F(\theta)$, is defined as the covariance of the score function
\begin{equation*}
F(\theta) = \mathbb{E}_{x\sim p(x|\theta)} [\nabla_\theta \log p(x|\theta) \nabla_\theta \log p(x|\theta)^T],    
\end{equation*}
where $\nabla_\theta \log p(x|\theta)$ is the gradient of the log-likelihood function, often referred to as the score function. This matrix describes the curvature of the loss surface around the current parameters and highlights which parameters are most sensitive to changes in the data. In practical deep learning applications, the empirical FIM is used, computed over a batch of data as follows:
\begin{equation*}
F(\theta) \approx \frac{1}{N} \sum_i \nabla_\theta \log p(x_i|\theta) \nabla_\theta \log p(x_i|\theta)^T,    
\end{equation*}
where $N$ is the batch size. The trace of the FIM ($\text{Tr}(F)$) is particularly relevant to our work, as it summarizes the overall sensitivity of the network's parameters.

\subsection{Characterizing the Primacy Bias with the FIM}

Our analysis reveals that the PB manifests through a characteristic two-phase pattern in the evolution of the FIM trace ($\text{Tr}(F)$) during training, as shown in the \autoref{fig:pb_example}, which represents the evolution of $\text{Tr}(F)$, the differential of $\text{Tr}(F)$, and the reward during training. This characterization of different learning periods is based on the work of \citeauthor{achille2018critical}. This pattern provides a  metric to characterize and understand how early experiences disproportionately influence learning:
\begin{itemize}
    \item \textbf{Memorization Phase:} An initial sharp increase in $\text{Tr}(F)$ during early training, characterized by a rapid exponential growth. This phase corresponds to high sensitivity to parameter updates and intensive information acquisition from initial experiences.
    \item \textbf{Reorganization Phase:} A subsequent sharp decrease in $\text{Tr}(F)$, despite continued improvement in task performance. This phase is characterized by a gradual decline of $\text{Tr}(F)$, settling at values lower than the peak, corresponding to reduced sensitivity to new information and a consolidation of learned patterns.

\begin{figure}[htbp]
  % FIRST ROW
  \begin{center}
      
    \includegraphics[scale=0.7]{figures/quad_dt.pdf}
    \hfill\quad
    \includegraphics[scale=0.7]{figures/swim_dt.pdf}

  % SECOND ROW
    \includegraphics[scale=0.7]{figures/quad_rwdt.pdf}
    \quad
    \includegraphics[scale=0.7]{figures/swim_rwdt.pdf}

  \caption{Example of the Primacy Bias characterization using the \textcolor{blue}{$\text{Tr}(F)$ (blue)} and \textcolor{black}{$\Delta \text{Tr}(F)$ (black)}.The graphs represent the learning dynamics on the \texttt{Quadruped} and \texttt{Swimmer} environment respectively. From our characterization, the PB is present in the \texttt{Quadruped} while it is not present in the \texttt{Swimmer}. \textbf{Best viewed in colors.}}
  \label{fig:pb_example}

  \end{center}
\end{figure}

\end{itemize}

These two phases indicate that initial experiences have a disproportionate impact on the model, while the following phase indicates a locking of learned patterns, which results in the PB. The differential of the trace of the FIM through training ($\Delta\text{Tr}(F)$), calculated using a Savitzky-Golay filter \cite{candan2014unified}, can highlight the transition between these two phases.

\subsection{Selective Forgetting via the FIM}

To mitigate the PB, we draw inspiration from machine unlearning and, more specifically, the selective forgetting framework introduced by \citeauthor{golatkar2020eternal}. Their approach leverages the concept of a Forgetting Lagrangian:
\begin{equation*}
L = \mathbb{E}_{S(w)}[L_{D_r}(w)] + \lambda \text{KL}(P(S(w)|D) \| P(w|D_r)),
\end{equation*}
where $L_{D_r}(w)$ represents the loss on the retained dataset $D_r$, while $P(w|D_r)$ represents the distribution of weights obtained after training on $D_r$ only. $S(w)$ indicates the weight scrubbing procedure, and $P(S(w)|D)$ is the resulting distribution of weights after scrubbing. Using a quadratic approximation of the loss function and assuming gradient flow optimization, the optimal scrubbing procedure can be derived as:
\begin{equation*}
S(w) = w - B^{-1}\nabla L_{D_r}(w) + (\lambda\sigma^2)^{1/4} B^{-1/4} \epsilon,  
\end{equation*}
where $B$ is the Hessian of the loss on the retained data, $\epsilon$ is standard Gaussian noise, and $\sigma^2$ represents the uncertainty. In practice, the Hessian $B$ is approximated with the empirical FIM \cite{martens2015optimizing}


\subsection{Fisher-Guided Selective Forgetting}
Our final algorithm, Fisher-Guided Selective Forgetting (FGSF), tailors the theoretical framework to the DRL domain. In this context, the current batch of experiences sampled from the replay buffer is treated as the set to be retained ($D_r$), while previously encountered trajectories constitute the set to be forgotten. This interpretation aligns with our goal of preventing early experiences from dominating the learning process by periodically applying a scrubbing procedure after each standard optimization step. 
The scrubbing procedure is: 
\begin{equation*}
S(w) = w + (\lambda\sigma^2)^{1/4} F^{-1/4} \epsilon,    
\end{equation*}
where $F$ is the empirical FIM, calculated from data within the current batch. 

Note that, compared to the original formulation of \citeauthor{golatkar2020eternal}, we removed the term $B^{-1}\nabla L_{D_r}(w)$. This is justified for a couple of main reasons. First, the standard optimization process, usually based on gradient descent, already performs a similar parameter update, without the Hessian term that can be added in a second moment method for optimization as natural gradient descent \cite{amari1998natural, pascanu2013revisiting}, hence making the gradient part redundant. Second, in contrast to the original formulation where scrubbing is performed once after training is done, our procedure is performed periodically during training,
making a full gradient update unrealistic since it would drastically disrupt the optimization process. 

We highlight that our proposed FGSF algorithm is compatible with any DRL algorithm that uses experience replay.
For algorithms with multiple networks, such as actor-critic methods, FGSF is applied to each network independently. The scrubbing frequency and the forgetting magnitude ($\lambda$) serve as tunable parameters for balancing PB mitigation with learning stability. A fundamental interdependence exists between the scrubbing frequency and $\lambda$: more frequent scrubbing necessitates smaller $\lambda$
values to maintain stability. This relationship directly manages the trade-off between effective information removal and the preservation of learning dynamics. For the sake of simplicity, we fixed the scrubbing frequency to 10. A detailed description of the algorithm can be found in Algorithm \ref{alg:fgsf}. 

\begin{algorithm}[tb]
  \caption{Fisher-Guided Selective Forgetting (FGSF)}
  \label{alg:fgsf}
  \begin{algorithmic}[1]
     \STATE {\bfseries Input:} Current network parameters $w$, Replay Buffer $D$,  Scrubbing Frequency $F$,  Forget Coefficient $\lambda$
      \STATE Initialize $t \gets 0$ 
      \REPEAT 
       \STATE \{$(s_t, a_t, r_t, s_{t+1})$\dots$(s_k,a_k,r_k,s_{k+1})$\} $\sim D$
       \STATE Update network parameters $w$ using the DRL algorithm's update rule
       \STATE $t \gets t + 1$
       \IF {$t \mod F = 0$}
            \STATE $FIM = \frac{1}{N} \sum_{i=1}^N \nabla_w \log p(s_i|w) \nabla_w \log p(s_i|w)^T$
             \STATE $\epsilon \sim \mathcal{N}(0, I)$
             \STATE $w \gets w + (\lambda \sigma^2)^{\frac{1}{4}}  FIM^{-\frac{1}{4}} \epsilon $
        \ENDIF
      \UNTIL{Convergence} 
  \end{algorithmic}
\end{algorithm}



\section{Experimental Setup}

To validate our approach and investigate the efficacy of FGSF in mitigating the PB, we conducted extensive experiments across a variety of environments and conditions. In this section, we will briefly describe the experimental setup we used in the paper.

\textbf{Environments}
We evaluated our proposed method on a suite of continuous control tasks from the DeepMind Control Suite (DMC) \cite{tassa2018deepmind}. The environments include:
Basic Control Tasks: \texttt{Pendulum} and \texttt{Acrobot}. Locomotion Tasks: \texttt{Humanoid}, \texttt{Quadruped}, \texttt{Walker}, \texttt{Cheetah}, \texttt{Hopper}, and \texttt{Swimmer6}. Manipulation Tasks: \texttt{Reacher} and \texttt{Finger}.

\textbf{Algorithm and Implementation Details}
For our experiments, we used the Soft Actor-Critic (SAC) algorithm \cite{haarnoja2018soft,haarnoja2018softa} as the base DRL method. SAC was selected due to its established performance in continuous control tasks and because it is the algorithm of choice in previous work investigating the PB \cite{nikishin2022primacy, sokar2023dormant, d2022sample, li2024eliminating}. We maintain the default hyperparameters of SAC, as specified in the original paper, while modifying specific parameters when explicitly studying their effects on the PB (e.g., hyperparameter study). All the experiments were performed using the Tianshou library \cite{weng2022tianshou} for the DRL implementation and, to compute the empirical FIM, we leveraged the NNGeometry library \cite{george2021nngeometry} using the Eigenvalue-corrected Kronecker-Factored Eigenbasis (EKFAC) \cite{george2018fast} approximation, which provides a computationally efficient approach for estimating the FIM and is widely used in the literature.








\section{Results}

This section presents the findings of our empirical evaluation, focusing on the performance of FGSF and its impact on various aspects of DRL. %We report the results obtained from the series of experiments designed to answer the research questions detailed in the previous section.

\subsection{Comparative Analysis of FGSF}
We evaluate the efficacy of FGSF by contrasting it with standard SAC implementations and periodic network reset methods, assessing performance, update magnitude (see Appendix \ref{weight_app}) dormant neurons (see Appendix \ref{dorm_app}),  stability, and sample efficiency to understand the advantages of our approach.
Our empirical evaluation, shown in \autoref{fig:rew_comp} and summarized in Table \ref{table:rew_comp} in Appendix \ref{sec:rew_comp}, reveals that FGSF exhibits a significant performance advantage, particularly in high-dimensional tasks. In the \texttt{Humanoid} environment, FGSF achieved a mean return of $150 \pm 15$, a $50\%$ improvement over baseline SAC ($95 \pm 10$), and a $25\%$ improvement over the reset method ($120 \pm 20$). Similarly, in the \texttt{Quadruped} environment, FGSF reached a final performance of $850 \pm 30$, compared to $650 \pm 25$ for baseline SAC and $780 \pm 35$ for the reset method. While the performance gap narrows in medium-complexity environments like \texttt{Walker} and \texttt{Cheetah}, with FGSF and baseline SAC reaching approximately $830 \pm 20$ in \texttt{Cheetah}, FGSF demonstrates superior sample efficiency, achieving $90\%$ of maximum performance approximately $2\times10^5$ steps earlier than the baseline. In simpler environments like \texttt{Pendulum} and \texttt{Reacher}, all methods attain similar final performance. Notably, FGSF shows more consistent learning without performance drops seen with the reset method. In contrast, both reset and FGSF failed to learn in the \texttt{Acrobot} environment, possibly due to hyperparameter sensitivity in this simpler environment. The \texttt{Swimmer} environment presented minimal differences, with all approaches reaching final returns of approximately $350 \pm 30$. Across all environments, the reset method introduces significant temporary performance degradation, with sharp drops every $2\times10^5$ steps, unlike FGSF which provides more stable learning trajectories. FGSF consistently requires roughly $20\%$ fewer interactions than SAC in complex environments to reach performance thresholds, particularly within the initial $2\times10^5$ steps, indicating more efficient early-stage policy identification. 

Analysis of the FIM traces, shown in \autoref{fig:trace_comp} and \ref{fig:trace2_comp}, reveals distinct patterns in how FGSF mitigates PB. In baseline SAC, both actor and critic networks show an initial sharp increase in Tr(F) during a memorization phase, reaching approximately $10^6$ for critics and $10^5$ for actors in complex environments, followed by a reorganization phase with a gradual decline. FGSF, in contrast, maintains significantly lower critic Tr(F) values (typically $10^4$-$10^5$ vs. baseline's $10^5$-$10^6$), and reduced peak magnitudes with faster stabilization in actor networks.  FGSF’s regulation of learning phases leads to enhanced performance, aligning with findings that reduced Tr(F) during early training correlates with improved generalization. The reset method exhibits discontinuities in the FIM trace every $2\times10^5$ steps, with critic networks showing faster recovery with overshoot compared to slower recovery in actor networks. In the \texttt{Humanoid} environment, baseline critic Tr(F) peaks at $2 \times 10^6$ while FGSF maintains values below $5 \times 10^5$. Based on our characterization, these FIM patterns provide evidence of FGSF's ability to mitigate the Primacy Bias.


\begin{figure}[ht]
\includegraphics[scale=0.7]{appendix_figures/reward_comparison.pdf}
    \caption{
    Learning curves showing episodic reward across different environments for \textcolor{MGOLD}{baseline SAC (gold)}, \textcolor{MTEAL}{reset method (teal)}, and \textcolor{red}{FGSF (red)}. Shaded regions represent the minimum and maximum over 5 random seeds. \textbf{Best viewed in colors.}
    }
    \label{fig:rew_comp}
\end{figure}

\begin{figure}[ht]
\includegraphics[scale=0.7]{appendix_figures/trace_comparison.pdf}
    \caption{Evolution of FIM trace (Tr(F)) for actor networks. Results compare \textcolor{MGOLD}{baseline SAC (gold)}, \textcolor{MTEAL}{reset method (teal)}, and \textcolor{red}{FGSF (red)}.  Shaded regions represent the minimum and maximum over 5 random seeds. \textbf{Best viewed in colors.}}
    \label{fig:trace_comp}
\end{figure}

\subsection{Robustness Analysis}
To assess the sensitivity of FGSF, we examine performance variations by exploring different noise injection coefficients ($\lambda$) and replay ratios, thereby determining the robustness of our approach concerning performance and stability.

\textbf{Hyperparameter Sensitivity}
Our analysis indicates that while FGSF's effectiveness depends on the scrubbing coefficient $\lambda$, it maintains robust performance across a range of values ($5 \times 10^{-6}$ to $5 \times 10^{-8}$). This can be seen in \autoref{fig:hyper_rew} and \ref{fig:trace_hyper} and is also summarized in Table \ref{table:hyper_rew}. Intermediate values, particularly $5 \times 10^{-7}$, achieve an optimal balance between learning stability and bias mitigation. Larger $\lambda$ values ($5 \times 10^{-6}$) induce aggressive forgetting and increased trajectory variability, while lower values ($5 \times 10^{-8}$) may inadequately address the PB. FIM trace analysis highlights that over-regularization (too much reduction in $\text{Tr}(F)$) can disrupt the natural transition between learning phases. Surprisingly, environment complexity exhibits minimal influence on optimal $\lambda$ values, though simpler environments often show slightly better performance with lower $\lambda$. Rapid $\text{Tr}(F)$ oscillations indicate a need for coefficient reduction, while inadequate post-memorization phase decline suggests insufficient $\lambda$ values. For practical implementation, we recommend an initial $\lambda$ value of $5 \times 10^{-7}$, monitoring both actor and critic FIM traces, and adjusting $\lambda$ based on observed learning stability.


\begin{figure}[ht]
\includegraphics[scale=0.7]{appendix_figures/reward_hyper.pdf}
    \caption{Hyperparameter sensitivity analysis showing performance across different scrubbing coefficients ($\lambda \in [5 \times 10^{-6},5 \times 10^{-8}]$ ). The lighter the color the higher the coefficient. Shaded regions represent the minimum and maximum over 5 random seeds. \textbf{Best viewed in colors.}}
    \label{fig:hyper_rew}
\end{figure}


\textbf{Replay Ratio}
To assess FGSF's robustness in the presence of increased replay ratios, which are known to exacerbate the PB, we tested ratios of 2 and 4. As depicted in \autoref{fig:rr}, our results demonstrate that while higher replay ratios drastically decrease the overall performance and robustness of SAC, FGSF performs comparatively better, retaining a more robust performance. This is because when we increase the replay ratio, we are replaying the same trajectories multiple times, and, if the model got biased in the beginning of the training, these will be amplified by the replay buffer. FGSF is able to counteract this effect by making the weights less sensible to the early, biased, experiences, which leads to a higher performance with more stable learning curves. Given the relatively high FIM traces that are observed under these conditions, this suggests the need for a stronger lambda value to regularize the trace and further mitigate the effects of amplified early biases.

\begin{figure}[ht]
\includegraphics[scale=0.7]{appendix_figures/rr.pdf}
    \caption{Performance comparison and FIM trace evolution on the \texttt{Quadruped} under different replay ratios (2 left column and 4 right column) for \textcolor{MGOLD}{baseline SAC}, \textcolor{MTEAL}{reset method}, and \textcolor{red}{FGSF}. Higher replay ratios amplify the differences between methods. \textbf{Best viewed in colors.}}
    \label{fig:rr}
\end{figure}

\subsection{Ablation Studies}
To dissect the contributions of different components to the FGSF method, we perform two ablation studies. First, we investigate the impact of network component scrubbing by selectively applying FGSF to either the critic-only, or both networks. Second, we analyze the influence of structured noise injection through a comparative evaluation against a simpler, unstructured approach. 

\textbf{Impact of Network Component Scrubbing}
Our investigation into critic-only scrubbing, shown in \autoref{fig:ac_rew}, reveals that in complex, high-dimensional locomotion tasks like \texttt{Humanoid} and \texttt{Quadruped}, it achieves comparable, and sometimes better, performance than full network scrubbing, suggesting that the critic network is more susceptible to the PB. For example, in the \texttt{Humanoid} environment, critic-only scrubbing demonstrates more stable learning with fewer performance drops. In simpler environments like \texttt{Pendulum} and \texttt{Reacher}, the difference between critic-only and full network scrubbing is minimal. However, in more complex environments like \texttt{Walker} and \texttt{Cheetah}, critic-only scrubbing shows improved stability in later stages of training,

FIM trace analysis in \autoref{fig:trace_ac} and \ref{fig:trace2_ac} validates the superior effectiveness of critic-only scrubbing, showing more effective regularization during early training, with consistently lower Tr(F) values for both critic and actor networks. Notably, critic-only scrubbing achieves comparable, and in some cases superior, regularization of Tr(F) for the actor despite not directly manipulating its parameters, further emphasizing the critic’s central role in PB development. Our analysis reveals an order-of-magnitude difference in Tr(F) values between critic and actor networks, revealing different operating regimes in parameter space, which is associated with the critic's role in value estimation. We refer the reader to Table \ref{table:ac_rew} for a full panoramic of these results.


\begin{figure}[ht]
\includegraphics[scale=0.7]{appendix_figures/reward_ac.pdf}
    \caption{Learning curves showing episodic reward between \textcolor{red}{critic-only scrubbing (red)}
and \textcolor{MTEAL}{full network scrubbing (teal)}  for the different environments. \textbf{Best viewed in colors.} }
    \label{fig:ac_rew}
\end{figure}

\textbf{Fisher vs Gaussian Noise}
To evaluate the importance of Fisher-guided noise injection, we conducted a comparative analysis between FGSF and a simpler Gaussian noise approach. The Gaussian noise variant samples perturbations from a distribution with a mean of 0 and a standard deviation equal to the mean of the network parameter values (i.e. $\mathcal{N}(0, 0.001\mu)$ where $\mu$ represents the mean of network parameter values). While multiple noise formulations were possible, this simple implementation provides a clear baseline. The results of this analysis are shown in \autoref{fig:noise_rew}
In complex environments like \texttt{Humanoid} and \texttt{Quadruped}, FGSF showed modest performance improvements over the Gaussian Noise method while achieving significantly more stable learning trajectories. Although effective, Gaussian noise exhibits higher performance variance, especially in the \texttt{Humanoid}  environment. This stability gap widens with increasing task dimensionality. In simpler environments like \texttt{Reacher} and \texttt{Pendulum}, both methods achieve similar final returns. However, FGSF maintains advantages in learning speed and stability. FGSF produces smoother learning curves than Gaussian noise injection, and learning dynamics show that FGSF achieves more consistent progress, suggesting more efficient parameter space exploration. 




\begin{figure}[ht]
\includegraphics[scale=0.7]{appendix_figures/reward_noise.pdf}
    \caption{Performance comparison between \textcolor{red}{FGSF} and \textcolor{MGOLD}{Gaussian noise injection} across different environments. Shaded regions represent the minimum and maximum over 5 random seeds. \textbf{Best viewed in colors.}}
    \label{fig:noise_rew}
\end{figure}


\section{Discussion \& Conclusion}
This paper introduced Fisher-Guided Selective Forgetting, a novel method for mitigating the primacy bias in Deep Reinforcement Learning. By leveraging the Fisher Information Matrix and adapting techniques from machine unlearning, FGSF offers a principled approach to address the PB by selectively modifying network weights and controlling the learning process. Our experiments, conducted across a diverse range of environments, demonstrate FGSF's effectiveness in several key areas. FGSF consistently achieves improved performance and stability compared to baseline SAC and the periodic network reset method, particularly in complex and high-dimensional tasks such as \texttt{Humanoid} and \texttt{Quadruped}, where we observed up to a $50\%$ increase in mean return compared to the baseline. Furthermore, our analysis highlights that the critic network is more susceptible to the PB than the actor, which aligns with previous studies \cite{lyle2022learning, lyle2022understanding, van2018deep}, and that selectively addressing the critic's bias has a stronger impact on overall performance, allowing for more efficient computation. FGSF also demonstrated robustness across various replay ratios, maintaining performance stability even at a different replay ratio, where baseline SAC degraded significantly. We also showed an improvement compared to a simple Gaussian noise injection strategy. This might indicate that the geometric properties of the FIM can indeed be exploited for better-performing models and more effective bias mitigation.


Despite these encouraging results, it is essential to acknowledge the limitations of our approach. Firstly, while FGSF shows improved sample efficiency compared to baseline methods—achieving a $20\%$ reduction in the required samples to reach $90\%$ of the final performance in complex environments—the computation of the FIM still introduces a non-negligible overhead. Although we have used an efficient approximation of the FIM (EKFAC), the additional computational cost, which is between $10$-$20\%$ in cumulative training time (\autoref{fig:time}), might be a practical concern for large-scale DRL applications or when computational resources are limited. Furthermore, while we have investigated the impact of the hyperparameter $\lambda$ and found optimal values around $5 \times  10^{-7}$, further research is needed for a more comprehensive analysis across a wider range of problems. Our observations also suggest a potential trade-off; simpler environments might not benefit as much from fine-tuning $\lambda$  and may even perform better with less regularization, indicating the need to adapt the scrubbing coefficient based on task complexity.

Our ablation study shows that even simple noise injection strategies, albeit not as effective as FGSF, can achieve significant performance improvements over the baseline SAC, indicating that the PB is indeed closely related to the optimization process itself. This resonates with the recent developments on continual backpropagation \cite{dohare2023maintaining}, which suggest that directly manipulating the optimization process may be a promising approach to address similar problems in DRL. Furthermore, it suggests that future research might explore the effects of FGSF with alternative, potentially more sophisticated, optimization algorithms like natural gradient descent \cite{kakade2001natural, pascanu2013revisiting}, which is more closely aligned with the nature of the FIM.



Despite these limitations, our work opens up several interesting avenues for future research. The integration of machine unlearning techniques into the DRL framework represents a promising direction, creating a new family of algorithms that can selectively learn and unlearn from past experiences, potentially leading to more efficient and adaptable DRL agents. While our FGSF method demonstrates the value of structured information, further research could investigate alternative ways to leverage the FIM beyond simple noise injection, exploring different techniques of performing a weight update to achieve more targeted interventions. More work also needs to be done to better understand the interplay between the FIM trace, network plasticity, and capacity, particularly with regard to the critic's role. Finally, future work should explore more complex and diverse environments to better understand the limits of FGSF's applicability in more complex training scenarios. In this regard transfer learning comes to mind, where it has been shown that DRL agents often overfit on the source task they have been pre-trained on, and fail to adapt to the target task \cite{farebrother2018generalization,sabatelli2021transferability}. 

In conclusion, this paper contributes a novel approach, FGSF, for addressing the primacy bias in DRL by exploiting the theoretical framework of information geometry and machine unlearning. Our findings demonstrate the potential of integrating FIM-based techniques for a better understanding and mitigation of biases in neural networks and open new directions for research and future work, in the continuous quest for better and more robust DRL systems

%\section{Impact Statement}
%This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.








\bibliography{bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
\newpage
\appendix
\onecolumn

\section{Supplementary Material}
The supplementary material contains the code necessary to reproduce all experiments and analyses presented in this work.  This includes scripts for data preprocessing, and model training allowing readers to independently verify our findings.



\section{Comparative Analysis of FGSF}
\label{sec:rew_comp}

\begin{table*}[ht]
\centering
\caption{Performance comparison of different algorithms (FGSF, Reset method, and Baseline SAC) across various environments.  Values represent the mean and standard deviation of the final 100 episode returns over 5 random seeds. \textcolor{myMagenta}{Magenta} represents the best performing algorithm.}
\label{table:rew_comp}
\begin{tabular}{llll}
\toprule
\cellcolor{lightgray} Environment & FGSF & Reset & Base SAC \\
\midrule
\cellcolor{lightgray} \texttt{Acrobot} & 6.481 ± 2.823 & 4.056 ± 2.449 & \textcolor{myMagenta}{145.313 ± 24.776} \\
\cellcolor{lightgray} \texttt{Humanoid} & \textcolor{myMagenta}{136.645 ± 14.360} & 91.539 ± 31.977 & 68.503 ± 21.938 \\
\cellcolor{lightgray} \texttt{Hopper} & 148.024 ± 7.215 & 149.689 ± 47.105 & \textcolor{myMagenta}{266.906 ± 18.120} \\
\cellcolor{lightgray} \texttt{Swimmer} & \textcolor{myMagenta}{326.493 ± 65.572} & 284.452 ± 59.983 & 324.661 ± 56.188 \\
\cellcolor{lightgray} \texttt{Cheetah} & \textcolor{myMagenta}{838.635 ± 24.309} & 541.672 ± 249.813 & 803.851 ± 65.473 \\
\cellcolor{lightgray} \texttt{Walker} & 746.695 ± 13.214 & 599.996 ± 148.548 & \textcolor{myMagenta}{758.397 ± 20.905} \\
\cellcolor{lightgray} \texttt{Finger} & 824.243 ± 77.102 & 279.302 ± 103.450 & \textcolor{myMagenta}{855.262 ± 66.543} \\
\cellcolor{lightgray} \texttt{Reacher} & \textcolor{myMagenta}{958.788 ± 17.978} & 899.837 ± 205.015 & 940.631 ± 33.600 \\
\cellcolor{lightgray} \texttt{Quadruped} & \textcolor{myMagenta}{873.473 ± 21.287} & 688.864 ± 152.444 & 582.909 ± 37.262 \\
\cellcolor{lightgray} \texttt{Pendulum} & 770.519 ± 51.891 & 513.362 ± 149.628 & \textcolor{myMagenta}{834.720 ± 12.865} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/trace2_comparison.pdf}
\end{center}
    \caption{ Evolution of FIM trace (Tr(F)) during training for critic networks across different environments. Results compare \textcolor{MGOLD}{baseline SAC (gold)}, \textcolor{MTEAL}{reset method (teal)}, and \textcolor{red}{FGSF (red)}. Shaded regions represent the minimum and maximum over 5 random seeds. \textbf{Best viewed in colors.} }
    \label{fig:trace2_comp}
\end{figure}
\newpage


\subsection{Weight Update Magnitude}\label{weight_app}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/KL_comparison.pdf}
\end{center}
    \caption{Local parameter update magnitudes measured by KL divergence across different environments. Lower values indicate a smaller parameter update. Spikes in the \textcolor{MGOLD}{baseline (gold)} and \textcolor{MTEAL}{reset methods (teal)} contrast with \textcolor{red}{FGSF’s (red)} more consistent update pattern. \textbf{Best viewed in colors.}}
    \label{fig:kl_comp}
\end{figure}

Our analysis of parameter update magnitudes, measured by the Kullback-Leibler (KL) divergence of weight distributions, reveals that in complex environments, FGSF maintains consistently lower update magnitudes (local delta) throughout training (typically stabilizing between $0.5$ and $0.7$), with smoother trajectories compared to the higher values and more pronounced spikes observed in baseline SAC. While \texttt{Cheetah} and \texttt{Swimmer} show periodic spikes, FGSF maintains better stability. These results suggest that FGSF’s improved performance is partly due to controlled parameter updates, preventing destabilizing policy changes. 
\newpage
\subsection{Dormant neurons}\label{dorm_app}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/dormant_comparison.pdf}
\end{center}
    \caption{Fraction of dormant neurons during training across different environments. Plots compare \textcolor{MGOLD}{baseline SAC (gold)}, \textcolor{MTEAL}{reset method (teal)}, and \textcolor{red}{FGSF (red)}. \textbf{Best viewed in colors.}}
    \label{fig:dor_comp}
\end{figure}

In baseline SAC, critic networks exhibit a consistent increase in dormant neuron fraction, particularly in complex environments. In the \texttt{Quadruped} environment, this rises from 2\% to approximately 6\% while in the \texttt{Humanoid} environment, it reaches peaks of 8\% before stabilizing around 4\%. This progressive loss of active neurons correlates strongly with Tr(F) stabilization, suggesting a link between the identified learning phases and network plasticity. FGSF, despite achieving superior performance, either matches or exceeds the baseline in terms of dormant neuron fraction, challenging the idea that dormant neuron fraction is a reliable indicator of the primacy bias. 

\newpage

\section{Impact of Network Component Scrubbing}


\begin{table*}[ht]
\centering
\caption{Comparison of learning curves between critic-only scrubbing and full network scrubbing for the different environments. Values represent the mean and standard deviation of the final 100 episode returns over 5 random seeds. \textcolor{myMagenta}{Magenta} represents the best performing algorithm}
\label{table:ac_rew}
\begin{tabular}{lll}
\toprule
\cellcolor{lightgray} Environment & Critic-only Scrubbing & Full Scrubbing \\
\midrule
\cellcolor{lightgray} \texttt{Acrobot} & 6.534 ± 3.019 & \textcolor{myMagenta}{15.105 ± 12.254} \\
\cellcolor{lightgray} \texttt{Humanoid} & \textcolor{myMagenta}{137.800 ± 13.899} & 134.148 ± 9.096 \\
\cellcolor{lightgray} \texttt{Hopper} & 150.433 ± 22.456 & \textcolor{myMagenta}{229.593 ± 34.675} \\
\cellcolor{lightgray} \texttt{Swimmer} & \textcolor{myMagenta}{333.186 ± 74.840} & 324.078 ± 78.075 \\
\cellcolor{lightgray} \texttt{Cheetah} & \textcolor{myMagenta}{842.421 ± 22.509} & 828.527 ± 27.339 \\
\cellcolor{lightgray} \texttt{Walker} & \textcolor{myMagenta}{749.690 ± 13.521} & 746.717 ± 28.791 \\
\cellcolor{lightgray} \texttt{Finger} & \textcolor{myMagenta}{833.518 ± 73.105} & 769.001 ± 104.140 \\
\cellcolor{lightgray} \texttt{Reacher} & 959.554 ± 17.695 & \textcolor{myMagenta}{962.087 ± 20.241} \\
\cellcolor{lightgray} \texttt{Quadruped} & 873.464 ± 21.803 & \textcolor{myMagenta}{889.924 ± 24.096} \\
\cellcolor{lightgray} \texttt{Pendulum} & \textcolor{myMagenta}{771.567 ± 53.413} & 660.094 ± 108.702 \\
\bottomrule
\end{tabular}
\end{table*}


\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/trace_ac.pdf}
\end{center}
    \caption{Actor network FIM trace evolution comparing \textcolor{red}{critic-only scrubbing (red)} versus \textcolor{MTEAL}{full network scrubbing (teal)} for different environments. Results demonstrate that critic-only scrubbing achieves effective regularization of actor network dynamics even without direct intervention. \textbf{Best viewed in colors.}
}
    \label{fig:trace_ac}
\end{figure}
\newpage

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/trace2_ac.pdf}
\end{center}
    \caption{Critic network FIM trace evolution under \textcolor{red}{critic-only scrubbing (red)} versus \textcolor{MTEAL}{full network scrubbing (teal)} for different environments. The traces show stronger regularization effects in critic-only scrubbing. \textbf{Best viewed in colors.}}
    \label{fig:trace2_ac}
\end{figure}
\newpage

\section{Hyperparameter Sensitivity}
\label{sec:hyper_rew}

\begin{table*}[ht]
\caption{Hyperparameter sensitivity analysis showing performance across different scrubbing coefficients ($\lambda$). Values represent the mean and standard deviation of the final 100 episode returns over 5 random seeds. \textcolor{myMagenta}{Magenta} represents the  best performing algorithm}
\label{table:hyper_rew}
\centering
\begin{tabular}{llll}
\toprule
\cellcolor{lightgray} Environment & $\lambda=5 \times 10^{-6}$ & $\lambda=5 \times 10^{-7}$ & $\lambda=5 \times 10^{-8}$ \\
\midrule
\cellcolor{lightgray} \texttt{Acrobot} & 4.915 ± 3.419 & 6.581 ± 3.033 & \textcolor{myMagenta}{7.728 ± 4.624} \\
\cellcolor{lightgray} \texttt{Humanoid} & 1.246 ± 0.100 & \textcolor{myMagenta}{137.800 ± 13.899} & 129.153 ± 17.056 \\
\cellcolor{lightgray} \texttt{Hopper} & 0.026 ± 0.058 & \textcolor{myMagenta}{148.061 ± 19.055} & 132.705 ± 16.831 \\
\cellcolor{lightgray} \texttt{Swimmer} & 316.372 ± 68.958 & \textcolor{myMagenta}{331.688 ± 74.640} & 320.555 ± 63.527 \\
\cellcolor{lightgray} \texttt{Cheetah} & 851.145 ± 16.972 & 826.530 ± 22.699 & \textcolor{myMagenta}{854.613 ± 16.708} \\
\cellcolor{lightgray} \texttt{Walker} & 729.304 ± 16.350 & \textcolor{myMagenta}{749.204 ± 13.887} & 715.562 ± 27.385 \\
\cellcolor{lightgray} \texttt{Finger} & 818.001 ± 106.650 & 835.705 ± 72.117 & \textcolor{myMagenta}{872.998 ± 73.859} \\
\cellcolor{lightgray} \texttt{Reacher} & 957.382 ± 22.751 & 959.448 ± 17.840 & \textcolor{myMagenta}{968.659 ± 18.319} \\
\cellcolor{lightgray} \texttt{Quadruped} & 775.383 ± 20.572 & \textcolor{myMagenta}{874.090 ± 21.353} & 861.912 ± 28.982 \\
\cellcolor{lightgray} \texttt{Pendulum} & 55.007 ± 58.313 & 725.280 ± 66.427 & \textcolor{myMagenta}{770.626 ± 54.307} \\
\bottomrule
\end{tabular}
\end{table*}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/trace_hyper.pdf}
\end{center}
    \caption{FIM trace of the critic network under different scrubbing coefficients ($\lambda \in [5 \times 10^{-6},5 \times 10^{-8}]$), illustrating the relationship between $\lambda$ values and the FIM trace. The lighter the color the higher the coefficient. \textbf{Best viewed in colors.}}
    \label{fig:trace_hyper}
\end{figure}
\newpage



\section{Computational Considerations}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.9]{appendix_figures/time.pdf}
\end{center}
    \caption{Comparative analysis of cumulative training time across environments. The y-axis shows total computation time in seconds, demonstrating the computational overhead of different methods. \textcolor{MGOLD}{Baseline SAC (gold)}, \textcolor{MTEAL}{reset (teal)} and \textcolor{red}{FGSF (red)}. \textbf{Best viewed in colors.}}
    \label{fig:time}
\end{figure}
FGSF shows a $15$-$20\%$ increase in cumulative update time compared to baseline SAC in high-dimensional environments like \texttt{Humanoid} and \texttt{Quadruped}. This overhead remains relatively constant throughout training, as evidenced by parallel slopes in the timing curves. The reset method has no computational overhead.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.