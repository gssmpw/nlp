\section{Related Work}
% Deep learning/supervised learning on LArTPC images
\textbf{LArTPC data analysis} largely relies on the \textsc{SPINE} framework ____ -- a state-of-the-art, end-to-end reconstruction chain for LArTPC data, relying on sparse CNNs for voxel-level feature extraction and GNNs for building higher-level particle structures, and serves as a strong fully-supervised benchmark for our study. \textsc{SPINE} contains a cascade of task-specific networks -- either sparse convolution networks or graph neural networks -- and was trained end-to-end with over 125,000 individual LArTPC events. It is performant, with dense prediction of tracks and showers achieving 97.7\% and 99.5\% respectively. A goal of this paper is to determine whether our pre-trained model can enjoy similar performance while fine-tuning on a considerably smaller labeled dataset.

% \textbf{Vision transformers (ViTs)} ____ have attracted massive attention due to their success in many vision tasks, e.g., image ____ and video ____ recognition, dense prediction ____, and object detection ____. In comparison to convolutional neural networks (CNNs) ____, where spatial bias (local neighborhood structure, translation equivariance) is baked into the architecture, the original ViT first partitions images into 16x16 pixel non-overlapping patches which are then encoded into single tokens before being used as inputs to a Transformer encoder ____, where spatial biases are learned from scratch. With the success of ViTs in image and video modalities, it naturally started being modified for use in point cloud processing. Point Cloud Transformer ____ and Point Transformer ____ showed that it was possible to adapt the transformers to point cloud tasks and showing their increased performance on shape recognition over sparse convolution networks (SCNs) ____, graph neural networks (GNNs) ____, and PointNet-based ____ approaches. \textsc{Point-BERT} ____ removed inductive biases introduced by both architectures and moved towards a more modernized Transformer encoder, the main components of which (FPS+$k$-NN patching mechanism and transformer backbone) are still used in modern architectures today. In this work, we continue to use the vanilla ViT as our backbone architecture, but choose to modify the grouping mechanism to fit with our data modality. This modification is expanded upon in Sec. \ref{sec:tok}.

\textbf{Masked modeling:} At the heart of any self-supervised learning framework is the \textit{pretext task}, i.e., the task under which the model will learn good representations of the data. For our purposes, we focus on two types of pretext tasks: masked modeling ____ and contrastive learning ____. The \textit{masked modeling task} involved masking a part of the training sample, and tasking the model with predicting what it did not see. The success of masked image ____ and language ____ modeling prompted a number of works to propose masked modeling frameworks for point cloud understanding. Point-MAE ____ and Point-BERT ____ are perhaps the most popular frameworks, of which many models today use as a baseline for comparison. Point-MAE extended image masked autoencoders (MAEs) ____ to the point-cloud regime by explicitly reconstructing point cloud groupings using the Chamfer distance. Point-BERT extended BERT ____ pre-training to point clouds. In this work, we start with the Point-MAE architecture, as it is the most conceptually simple and does not require pre-training a point cloud tokenizer as in Point-BERT. In \textit{contrast}, the \textit{contrastive learning task} involves maximizing the similarity between augmentations of the same training sample while minimizing the similarity of other samples. 
% This approach has its limitations -- requiring careful choice of data augmentations, negative sample mining and large batch sizes ____. To alleviate these limitations, student-teacher architectures ____ use joint-embeddings, whereby two copies of the same network see different views of the same sample and are trained to produce similar representations. In this framework, only the student is updated via backpropagation, while the teacher is updated as an exponential moving average of the student, in the form of \textit{self-distillation}. 
Though extremely successful in image tasks, we opt to explore contrastive learning approaches for particle trajectory representation learning in future works, just focusing on whether SSL for LArTPC images is possible in the first place.

\textbf{SSL for particle physics} has recently started gaining momentum, with several papers showing promising results for large-scale pretraining. ____ proposes a BERT-like ____ masked particle modeling strategy to learn reusable representations on unordered sets of particles by predicting the representations of masked out particles and comparing them to the output of a frozen pretrained VQ-VAE ____ encoder. Re-Simulation-based Self-Supervised Learning (R3SL) ____ utilizes a contrastive-learning approach, where the data augmentations are created by intervening mid-simulation to re-run downstream physics processes with the same initial conditions multiple times over, generating multiple consistent realizations of the same events. This work is different these approaches in our data have not been processed in any way to have coarse particle-based representations, nor do we intervene with strong augmentations as in R3SL. Instead, we work with raw energy depositions, with the only data augmentation being 3D rotations of entire events.

\begin{figure*}[ht!!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/arch.pdf}
    \vspace{-10pt}
\caption{\textit{PoLAr-MAE pre-training.} Input point clouds are partitioned into localized patches: seeds are selected via farthest point sampling (FPS), refined by centrality-based non-maximum suppression (C-NMS), and grouped via ball queries. A subset of patches is randomly masked, and visible patches are encoded into tokens using a mini-PointNet (right). The Transformer Encoder captures global relationships between tokens, while the Decoder predicts features of the missing patches using learned masked embeddings added onto learned positional encodings of the masked patch centers. Reconstructed patch coordinates are recovered via a linear projection. Additionally, an auxiliary task infills per-point energies by concatenating decoder outputs with Equivariant mini-PointNet (right) features from unmasked patches, leveraging their permutation-equivariant structure for point-wise regression.}  
\label{fig:arch}
\end{figure*}