%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{algorithmic}
\usepackage[table]{xcolor}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\definecolor{track}{RGB}{245,214,155}
\definecolor{track_border}{RGB}{174,153,114}
\definecolor{shower_border}{RGB}{94,140,127}
\definecolor{shower}{RGB}{121,181,164}
\definecolor{delta}{RGB}{186,74,9}
\definecolor{delta_border}{RGB}{147,59,7}
\definecolor{michel_border}{RGB}{18,65,106}
\definecolor{michel}{RGB}{24,88,144}


\DeclareRobustCommand{\colorsquare}[1]{\tikz{\path[draw=#1_border,fill=#1, thick] (0,0) rectangle (5pt,5pt);}}
\DeclareRobustCommand{\colordot}[2][black]{\tikz{\filldraw[color=#1, fill=#2, thick](0,0) circle (2pt);}}
\newcommand{\best}[1]{\cellcolor{gray!15}{#1}}
\newcommand{\besto}[1]{\colorbox{gray!15}{{#1}}}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\input{commands}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Particle Trajectory Representation Learning}

\begin{document}

\twocolumn[
\icmltitle{Particle Trajectory Representation Learning with Masked Point Modeling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sam Young}{stanford,slac}
\icmlauthor{Yeon-jae Jwa}{slac}
\icmlauthor{Kazuhiro Terao}{slac}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Stanford University, Stanford, CA, USA}
\icmlaffiliation{slac}{SLAC National Accelerator Laboratory, Menlo Park, CA, USA}

\icmlcorrespondingauthor{Sam Young}{youngsam@stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{self-supervised learning, high energy physics, neutrino physics, 3D computer vision, point cloud learning, open dataset}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.



\begin{abstract}
Effective self-supervised learning (SSL) techniques have been key to unlocking large datasets for representation learning. While many promising methods have been developed using online corpora and captioned photographs, their application to scientific domains, where data encodes highly specialized knowledge, remains in its early stages. We present a self-supervised \textit{masked modeling} framework for 3D particle trajectory analysis in Time Projection Chambers (TPCs). These detectors produce globally sparse ($<1$\% occupancy) but locally dense point clouds, capturing meter-scale particle trajectories at millimeter resolution. Starting with PointMAE, this work proposes volumetric tokenization to group sparse ionization points into resolution-agnostic patches, as well as an auxiliary energy infilling task to improve trajectory semantics. This approach -- which we call \textbf{Po}int-based \textbf{L}iquid \textbf{Ar}gon \textbf{M}asked \textbf{A}uto\textbf{e}ncoder (PoLAr-MAE) -- achieves 99.4\% track and 97.7\% shower classification F-scores, matching that of supervised baselines without any labeled data. While the model learns rich particle trajectory representations, it struggles with sub-token phenomena like overlapping or short-lived particle trajectories. To support further research, we release PILArNet-M -- the largest open LArTPC dataset (1M+ events, 5.2B labeled points) -- to advance SSL in high energy physics (HEP). Project site: \url{https://youngsm.com/polarmae}.

\end{abstract}
\vspace{-2mm}

\begin{figure}[t!!]
    \centering
    \includegraphics[width=1\linewidth]{figs/lartpc_image_example.pdf}
    \vspace{-10pt}
    \caption{\textit{Illustration of liquid argon time projection chamber (LArTPC) data. }\textbf{(Left.)} High energy particles enter the detector medium, depositing energy along it's trajectory. These depositions are measured by a complex particle imaging system. \textbf{(Right.)} A significant part of particle trajectory reconstruction is semantic segmentation, whereby each individual occupied voxel is classified as coming from a track-like particle~\colorsquare{track}, electromagnetic shower~\colorsquare{shower}, delta ray~\colorsquare{delta}, or Michel electron~\colorsquare{michel}. A single event can contain both overlapping trajectories and complex interactions in a small amount of space, as illustrated in the two zoomed in portions of the event.}
    \label{fig:example}
    \vspace{-5mm}
\end{figure}

\section{Introduction}

Self-supervised learning (SSL) has emerged as a cornerstone of modern computer vision, achieving state-of-the-art results across diverse domains while dramatically reducing reliance on labeled data by using pretrained models as foundation models \cite{bommasani2021opportunities}. By leveraging intrinsic data properties, SSL frameworks learn latent representations that encode rich semantic information, often rivaling fully supervised approaches in downstream tasks. This paradigm shift is especially promising in scientific domains where labeled data is scarce, costly, or impractical to obtain -- such as high-energy physics experiments. Indeed, foundation models have steadily made their way into science as an important and active area of research in areas of biology \cite{Jumper2021, Hou2024}, chemistry \cite{Irwin2022, Ross2022}, astrophysics \cite{lanusse2023astroclip, dillmann2024representation}, climate science \cite{doi:10.1126/science.adi2336,nguyen2023climax,kochkov2023neural}, mathematics \cite{Trinh2024, xin2024deepseek}, applied physics \cite{mccabe2023multiple, herde2024poseidonefficientfoundationmodels}, and others. 


Time projection chambers (TPCs) \cite{rubbia1977liquid}, a modern successor to cloud and bubble chambers, are a crucial detector technology used in a variety of domains to detect and understand the properties of neutrinos \cite{Abi2020}, perform direct dark matter detection \cite{mount2017luxzeplinlztechnicaldesign}, and understand the structure and properties of atomic nuclei. TPCs operate on the principle of tracking charged particles as they traverse a gas- and/or liquid-filled chamber. As charged particles move through the chamber, they ionize the detector medium, generating a trail of electrons. An electric field within the chamber, often created by a high-voltage cathode, propels these electrons towards an anode where their position and arrival time are precisely recorded. By analyzing this data, the TPC reconstructs the particle's trajectory and energy deposited in three dimensions, providing valuable insights into its properties and interactions from which it is a result of. An example of TPC data is visualized in Figure \ref{fig:example}.

A distinguishing feature of time projection chamber (TPC) data compared to conventional datasets is its extreme sparsity: more than 99\% of voxels in a detector volume record no signal in a single event. Particle trajectories manifest as intricate geometric structures -- cascading curved 1D manifolds embedded in 3D space. These structures, which vary by TPC type, can span lengths on the order of $\mathcal{O}(1~\text{m})$, while modern liquid argon TPCs (LArTPCs) achieve spatial resolutions of $\mathcal{O}(1~\text{mm})$. Distinct particle types produce characteristic ionization topologies: tracks, such as those from protons, pions, and muons, appear as long, nearly straight trajectories with minimal curvature. Electromagnetic particles (electrons, positrons, photons) generate shower-like topologies comprising numerous small trajectory fragments. Specific sub-structures include Michel electrons—slightly curved trajectories branching from the Bragg peak of a muon track, resulting from muon decay—and delta rays, electrons produced via interactions between track particles and the liquid argon medium, which appear exclusively along tracks. As illustrated in Fig. \ref{fig:example}, these morphologies result in a large diversity of complexity in particle imaging data. Tracks and showers dominate the signal voxels ($>$99.99\%), making their accurate discrimination critical for physics analyses. On the other hand, identifying secondary particles like Michel electrons and delta rays can provide insights into the properties of their parent track particles, are thus also important to understand.

Traditional approaches using voxel- and graph-based deep neural networks have achieved significant success in processing data generated from liquid argon TPCs (LArTPCs) \cite{drielsma2021scalableendtoenddeeplearningbaseddata}, the type of TPC employed in neutrino physics experiments. However, these models rely on labeled datasets derived from physics simulations and suffer from domain shift when applied to real data due to inherent simulation imperfections. This challenge remains a major obstacle to deploying deep learning models in experimental particle physics. Furthermore, the most effective models to date incorporate strong inductive biases rooted in domain science knowledge, and are thus not well-equipt to learn strong representations directly from data. In this paper, we explore adopting PointMAE for analyzing LArTPC image data, aiming to address two key objectives: (1) developing and demonstrating the effectiveness of self-supervised learning (SSL) for our dataset, including masked modeling \cite{vincent2010stacked,pathak2016context,chen2020generative,bao2021beit,xie2022simmim,hou2022milan,he2021maskedautoencodersscalablevision, pang2022maskedautoencoderspointcloud, yu2022pointbertpretraining3dpoint}, and (2) evaluating whether the SSL approach can compete with or surpass current state-of-the-art models that benefit from inductive bias. We select PointMAE \cite{pang2022maskedautoencoderspointcloud} as our base model due to its natural ability to handle sparsity while remaining resolution-agnostic, enabling potential applications of these methods to other science datasets.

Inspired by the success of SSL in computer vision and the supervised \textsc{SPINE} \cite{drielsma2021scalableendtoenddeeplearningbaseddata} framework for TPC data, this work investigates the potential of self-supervised representation learning for particle trajectory analysis. We aim to bridge the gap between computer vision and particle physics, showcasing how foundational CV methodologies can be adapted to high-energy physics challenges, and how new strategies must be created to deal with the idiosyncratic data modality and precision requirements of particle physics experiments.

Building upon the Point-MAE framework \cite{pang2022maskedautoencoderspointcloud}, we introduce a series of small architectural modifications tailored to our specific data modality, demonstrate that this model -- which we name PoLAr-MAE -- can effectively learn semantically meaningful representations of particle trajectories, and show that fine-tuning on a 10x less data can yield results comparable to SPINE. Notably, we propose a novel volumetric tokenization approach inspired by na\"ive voxelization, as well as an auxiliary per-point energy regression task to enhance semantic learning. Using masked modeling as the pretext task, we achieve per-token classification F-scores of 99.4\% and 97.7\% for identifying tracks and showers. These results are comparable to fully supervised approaches. Additionally, our method attains comparable performance to the state-of-the-art in fine-tuning for dense semantic segmentation of tracks and showers. However, we observe that very fine-grained semantics are not effectively captured with our approach, which poses challenges for downstream dense predictions on particle types that span only one or two tokens (Michel electrons \colorsquare{michel} and delta rays \colorsquare{delta}).

Our results demonstrate that self-supervised learning on point-cloud representations can achieve competitive performance in downstream tasks, unlocking the potential of vast, unlabeled raw TPC datasets and paving the way for scalable, cross-experiment analysis of particle trajectories. Along with this paper, we also publish the largest open simulated dataset of LArTPC images, titled PILArNet-M -- which contains more than 1,000,000 events and 5.2 billion individually labeled energy depositions -- for use in future studies.

In sum, our contributions are as follows:

\begin{itemize}
    \item The successful application of the masked modeling framework for learning rich representations of particle trajectories in LArTPCs.
    \item A volumetric approach to tokenizing point clouds, which computes a variable number of patch centers per event with empirical guarantees for minimizing missed and duplicated points within patches.
    \item An evaluation baseline for pre-training and fine-tuning LArTPC foundation models.
    \item A large dataset for use in future research and development by the community.
\end{itemize}

\section{Related Work}

% Deep learning/supervised learning on LArTPC images
\textbf{LArTPC data analysis} largely relies on the \textsc{SPINE} framework \cite{drielsma2021scalableendtoenddeeplearningbaseddata} -- a state-of-the-art, end-to-end reconstruction chain for LArTPC data, relying on sparse CNNs for voxel-level feature extraction and GNNs for building higher-level particle structures, and serves as a strong fully-supervised benchmark for our study. \textsc{SPINE} contains a cascade of task-specific networks -- either sparse convolution networks or graph neural networks -- and was trained end-to-end with over 125,000 individual LArTPC events. It is performant, with dense prediction of tracks and showers achieving 97.7\% and 99.5\% respectively. A goal of this paper is to determine whether our pre-trained model can enjoy similar performance while fine-tuning on a considerably smaller labeled dataset.

% \textbf{Vision transformers (ViTs)} \cite{dosovitskiy2020image} have attracted massive attention due to their success in many vision tasks, e.g., image \cite{dosovitskiy2020image} and video \cite{bertasius2021space, fan2021multiscale} recognition, dense prediction \cite{ranftl2021vision,kirillov2023segment}, and object detection \cite{carion2020end,misra2021end,li2022exploring}. In comparison to convolutional neural networks (CNNs) \cite{lecun1998gradient}, where spatial bias (local neighborhood structure, translation equivariance) is baked into the architecture, the original ViT first partitions images into 16x16 pixel non-overlapping patches which are then encoded into single tokens before being used as inputs to a Transformer encoder \cite{vaswani2017attention}, where spatial biases are learned from scratch. With the success of ViTs in image and video modalities, it naturally started being modified for use in point cloud processing. Point Cloud Transformer \cite{zhao2021point} and Point Transformer \cite{guo2021pct} showed that it was possible to adapt the transformers to point cloud tasks and showing their increased performance on shape recognition over sparse convolution networks (SCNs) \cite{choy20194dspatiotemporalconvnetsminkowski}, graph neural networks (GNNs) \cite{landrieu2018large,wang2019dynamicgraphcnnlearning}, and PointNet-based \cite{qi2017pointnet} approaches. \textsc{Point-BERT} \cite{yu2022pointbertpretraining3dpoint} removed inductive biases introduced by both architectures and moved towards a more modernized Transformer encoder, the main components of which (FPS+$k$-NN patching mechanism and transformer backbone) are still used in modern architectures today. In this work, we continue to use the vanilla ViT as our backbone architecture, but choose to modify the grouping mechanism to fit with our data modality. This modification is expanded upon in Sec. \ref{sec:tok}.

\textbf{Masked modeling:} At the heart of any self-supervised learning framework is the \textit{pretext task}, i.e., the task under which the model will learn good representations of the data. For our purposes, we focus on two types of pretext tasks: masked modeling \cite{devlin2018bert} and contrastive learning \cite{chen2020simple,he2020momentum,oord2018representation,tian2020contrastive,tian2020makes}. The \textit{masked modeling task} involved masking a part of the training sample, and tasking the model with predicting what it did not see. The success of masked image \cite{he2021maskedautoencodersscalablevision} and language \cite{devlin2018bert} modeling prompted a number of works to propose masked modeling frameworks for point cloud understanding. Point-MAE \cite{pang2022maskedautoencoderspointcloud} and Point-BERT \cite{yu2022pointbertpretraining3dpoint} are perhaps the most popular frameworks, of which many models today use as a baseline for comparison. Point-MAE extended image masked autoencoders (MAEs) \cite{he2021maskedautoencodersscalablevision} to the point-cloud regime by explicitly reconstructing point cloud groupings using the Chamfer distance. Point-BERT extended BERT \cite{devlin2018bert} pre-training to point clouds. In this work, we start with the Point-MAE architecture, as it is the most conceptually simple and does not require pre-training a point cloud tokenizer as in Point-BERT. In \textit{contrast}, the \textit{contrastive learning task} involves maximizing the similarity between augmentations of the same training sample while minimizing the similarity of other samples. 
% This approach has its limitations -- requiring careful choice of data augmentations, negative sample mining and large batch sizes \cite{grill2020bootstrap}. To alleviate these limitations, student-teacher architectures \cite{grill2020bootstrap,chen2021exploring,caron2021emerging,bardes2021vicreg,zhou2022ibotimagebertpretraining,baevski2022data2vecgeneralframeworkselfsupervised} use joint-embeddings, whereby two copies of the same network see different views of the same sample and are trained to produce similar representations. In this framework, only the student is updated via backpropagation, while the teacher is updated as an exponential moving average of the student, in the form of \textit{self-distillation}. 
Though extremely successful in image tasks, we opt to explore contrastive learning approaches for particle trajectory representation learning in future works, just focusing on whether SSL for LArTPC images is possible in the first place.

\textbf{SSL for particle physics} has recently started gaining momentum, with several papers showing promising results for large-scale pretraining. \citet{golling2024maskedparticlemodelingsets} proposes a BERT-like \cite{devlin2018bert} masked particle modeling strategy to learn reusable representations on unordered sets of particles by predicting the representations of masked out particles and comparing them to the output of a frozen pretrained VQ-VAE \cite{van2017neural} encoder. Re-Simulation-based Self-Supervised Learning (R3SL) \cite{r3sl} utilizes a contrastive-learning approach, where the data augmentations are created by intervening mid-simulation to re-run downstream physics processes with the same initial conditions multiple times over, generating multiple consistent realizations of the same events. This work is different these approaches in our data have not been processed in any way to have coarse particle-based representations, nor do we intervene with strong augmentations as in R3SL. Instead, we work with raw energy depositions, with the only data augmentation being 3D rotations of entire events.

\begin{figure*}[ht!!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/arch.pdf}
    \vspace{-10pt}
\caption{\textit{PoLAr-MAE pre-training.} Input point clouds are partitioned into localized patches: seeds are selected via farthest point sampling (FPS), refined by centrality-based non-maximum suppression (C-NMS), and grouped via ball queries. A subset of patches is randomly masked, and visible patches are encoded into tokens using a mini-PointNet (right). The Transformer Encoder captures global relationships between tokens, while the Decoder predicts features of the missing patches using learned masked embeddings added onto learned positional encodings of the masked patch centers. Reconstructed patch coordinates are recovered via a linear projection. Additionally, an auxiliary task infills per-point energies by concatenating decoder outputs with Equivariant mini-PointNet (right) features from unmasked patches, leveraging their permutation-equivariant structure for point-wise regression.}  
\label{fig:arch}
\end{figure*}


\section{Dataset}
\label{sec:data}

In this section, we provide an overview of the PILArNet-M dataset. This dataset is similar to the original PIPoLAr-MAE dataset \cite{adams2020pilarnetpublicdatasetparticle}, with the largest change being an order of magnitude increase in events.

Our dataset comprises of {number} simulated LArTPC images, each containing between 1 and 30,000 individual voxels, representing multiple particle interactions within a cubic detector volume of $(2.3~\text{m})^3$. The recording volume in the simulation is discretized into $768^3$ voxels, with each voxel having a pitch of $3~\text{mm}$. 

For each voxel, the dataset provides the energy deposition in {MeV along with additional metadata, including:
\vspace{-2mm}
\begin{itemize}
    \item \textit{Fragment ID}: A unique integer identifier (per image) representing the smallest unit of a ``cluster" of energy depositions.
    \item \textit{Group ID}: A unique integer identifier (within a single event) that aggregates particles with different fragment IDs into larger entities, commonly referred to as ``particles" in conventional physics analyses.
    \item \textit{Interaction ID}: A unique integer identifier (within an image) assigned to particles originating from the same physical process or creation event, often referred to as an ``interaction" or ``event."
    \item \textit{Semantic Type}: An integer identifier indicating the semantic category of the voxel. Categories include showers (ID: 0), tracks (ID: 1), Michel electrons (ID: 2), delta rays (ID: 3), and spurious low-energy depositions (ID: 4).
\end{itemize}

This comprehensive dataset enables detailed analyses of particle interactions and energy deposition patterns, supporting both semantic segmentation and clustering tasks. For more information on how the dataset was simulated, see \cite{adams2020pilarnetpublicdatasetparticle}. Further details on the composition of this dataset can be found in Appendix \ref{app:dataset}.

\paragraph{Data Preprocessing:} We remove low-energy depositions\footnote{These spurious spheroid-like deposits provide no meaningful information about the event and can be removed with unsupervised clustering methods like DBSCAN \cite{10.5555/3001460.3001507}.}, and only train on events with greater than 1024 points. The point cloud is normalized to fit within a unit sphere, and we define $\bar{E}$ as the log-transformed energy of each event, shifted and rescaled to the domain $[-1,1]^3$ using the following transformation:

\[
\bar{E} = 2\left(\frac{\log_{10}(E+\epsilon) - \log_{10}(\epsilon)}{\log_{10}(E_\text{max}+\epsilon) - \log_{10}(\epsilon)}\right) - 1,
\]

where $E$ is the deposited energy, $\epsilon = 0.01$, and $E_\text{max} = 20$. Here, $\epsilon$ and $E_\text{max}$ represent the smallest and largest energy values observed in the dataset, respectively.

We normalize events uniformly across the dataset to ensure that voxel values are processed consistently in a physically meaningful manner. Only basic 3D rotations are applied as training augmentations, for other transformations (e.g., scaling, jittering) would result in non-physical events.

\paragraph{Input Features:} The input to our point cloud architecture consists of all non-zero voxels in a single LArTPC image, represented as a list of 3D coordinates with a single energy channel. Each image is parameterized as the set $\mathcal{X} = \{(\textbf{x}_i, \bar{E}_i)\}_{i=1}^N$, where $N$ is the number of points in an event, $i$ denotes the $i$-th point, $\textbf{x}_i$ is the 3D coordinate of the $i$-th point, and $\bar{E}_i$ is the log-normalized energy as defined above.

\paragraph{Training and validation sets:} The training set consists of 1,033,377 events containing at least 1024 points. A smaller dataset of approximately 30 events (15,000 tokens) from a cache of 10,471 held-out events are used for validation metrics for pre-training, and another 30 events from the same cache are used for testing. For fine-tuning, we use a variable number of events from the training set, and use all 10,471 held-out events in the validation set for evaluation. 

\paragraph{Evaluation metrics:} To evaluate the performance of pre-training on this dataset, we follow previous works to train linear Support Vector Machines (SVMs) to classify individual point groups as related to different semantic IDs. As tokens may contain a number of points containing any number of depositions from different particles, we fit one classifier per class and perform multi-class classification with a One-vs-the-rest (OvR) strategy, i.e., our classifier classifies tokens as \textit{containing} specific semantic types or not. We report F-scores ($F_1$), which are defined as the harmonic mean of the precision and recall of the classifier, i.e. $F_1=\frac{2}{\text{recall}^{-1}+\text{precision}^{-1}}$. 
To evaluate the performance of the dense (point-wise) semantic segmentation fine-tuning, we report F-scores across classes, as well as overall precision for use in comparison to the fully supervised approach.


\section{Method}
\label{sec:method}

Figure \ref{fig:arch} outlines PoLAr-MAE's pretraining, adapted from Point-MAE with an added energy reconstruction task. The LArTPC image is converted into a point cloud, processed through a modified grouping module. Some groups are masked, while visible ones are embedded into latent tokens. A ViT-based autoencoder, with a heavy encoder and light decoder, reconstructs full patches. For energy reconstruction, a permutation-equivariant embedding module processes 3D point positions, and a linear head predicts energy using decoded masked embeddings and encoded 3D positions.

\subsection{Tokenization of Particle Trajectories}
\label{sec:tok}

\subsubsection{Patch grouping}

The common method for grouping point clouds into smaller patches involves iterative farthest point sampling (FPS) to sample group centers, followed by a $k$-nearest neighbors ($k$-NN) or ball query to find nearby neighbors for inclusion in the patch. However, in our dataset, where point density varies in regions containing signal, traditional FPS combined with $k$-NN or ball queries often proves insufficient. Specifically, these approaches result in either an excessive number of ungrouped points or excessive overlap between local patches (see Figure \ref{fig:pareto}). 

\begin{figure}[t]
    \centering
    % \vspace{-2cm}
    \includegraphics[width=0.8\linewidth]{figs/grouping_analysis_pareto_frontiers.pdf}
    \caption{\textit{Pareto frontiers for each grouping method.} This plot evaluates three methods: FPS+$k$-NN, FPS+Ball Query, and FPS+C-NMS+Ball Query, where lower values on both axes indicate better performance. Points along each curve represent optimal parameter configurations for minimizing point duplication (y-axis) and missed coverage (x-axis), and are normalized to percentage scales (0.0 = 0\%, 1.0 = 100\%). Error bars represent 1 standard deviation across 32 LArTPC events. Further analysis of this plot is given in Appendix \ref{app:cnms}.}
\label{fig:pareto}
\vspace{-3mm}
\end{figure}

\paragraph{Volumetric point cloud grouping.} To address this issue, we introduce a volumetric sampling method, \textbf{centrality-based non-maximum suppression (C-NMS)}, which extends the conventional NMS approach to operate on spherical regions. C-NMS enables efficient sampling of the minimal number of patches while maintaining low levels of ungrouped points and controlled overlap between patches. The method proceeds as follows:

\begin{enumerate}
\itemsep0em 
\item A large set of candidate group centers is sampled, e.g., via FPS or another method.
\item  Each group center is treated as the center of a sphere with a fixed radius $r$.
\item Using a greedy NMS algorithm, overlapping spheres are iteratively removed until no two remaining spheres exceed a predefined overlap factor $f$, defined as the percentage of the sphere diameter.\footnote{This is to say that $f=1$ means that sphere centers must be at least 1 entire diameter away from each other, i.e., with no overlap. A value of $f=0$ means that spheres are allowed to overlap completely.}
\item Finally, points for each group are sampled via a ball query using the same radius $r$.
\end{enumerate}

The resulting groups form a variable number $G$ of point groups, each containing a variable number $K$ of points. This approach approximates the effect of voxelizing the volume spanned by all points with a voxel pitch equal to the sphere diameter, while inherently reducing aliasing artifacts. Unlike FPS+$k$-NN, which lacks control over patch overlap, C-NMS ensures a tunable overlap between patches. We argue that $k$-NN grouping post-C-NMS is worse than a ball query, as the latter naturally constrains the domain of points within a canonical radius, effectively mitigating outlier inclusion within patches. Additional commentary on this approach, including the pseudocode implementation, can be found in Appendix \ref{app:cnms}.

\paragraph{Dynamic patch grouping.} In traditional FPS+\{$k$-NN, ball query\}, researchers often employ a fixed number of patches and a fixed number of points per patch (e.g., someone pre-training on ShapeNet \cite{chang2015shapenetinformationrich3dmodel} may produce 128 patches with 32 points each). By contrast, C-NMS dynamically determines both the number of tokens $G$ and the number of points per token $K$ based on two parameters: the sphere radius $r$ and the overlap factor $f$. The computational overhead of the additional NMS step involves an extra ball query on seed points, which can be efficiently executed using CUDA-accelerated libraries such as \textsc{pytorch3d} \cite{ravi2020pytorch3d}, but is certainly non-negligible.

\paragraph{Patch normalization.} After grouping via C-NMS, each patch is normalized by subtracting the patch center:
$
\mathbf{x}_i \rightarrow \mathbf{x}_i - \mathbf{c},
$
where $\mathbf{x}_i$ is the original point position's first three dimensions, and $\mathbf{c}$ is the patch center. Following PointNeXT \cite{qian2022pointnext}, we scale each normalized point by the sphere radius $r$, such that all patch points' positional coordinates lie within $[-1,1]^3$:
$
\mathbf{x}_i \rightarrow \frac{\mathbf{x}_i}{r}.
$ Note that we only apply these normalizations to the coordinates of each point, and not the energy value.

\paragraph{Mini-PointNet for patch embedding.} After grouping, each patch group is encoded into a single latent vector using a permutation-invariant mini-PointNet \cite{qi2017pointnet}. 

\paragraph{Transformer encoder.} The patch embeddings from the mini-PointNet form the input sequence to a vanilla Transformer encoder, which computes global contextual information across all patch tokens. To preserve spatial information lost during grouping, positional embeddings are added to the patch tokens. Each positional embedding is learned and corresponds to the 3D coordinates of the patch center. We omit the final LayerNorm \cite{ba2016layernormalization} operation in the Transformer encoder, as it resulted in better representations.

\subsection{Patch-based masked event modeling}

To facilitate effective self-supervised pretraining for point cloud data, we employ a masked autoencoder framework. A masking module selects a random subset of patch tokens, leaving the remaining unmasked tokens to be processed through the patch embedding module and Transformer encoder, producing contextually enriched embeddings. A shallow decoder then takes the encoded visible tokens along with a duplicated number of a placeholder learnable mask token. Positional embeddings corresponding to the spatial position of the patch centers are added to both visible and masked token embeddings, enabling the decoder to reconstruct the latent representations of the masked tokens. 

\subsubsection{Full Patch Reconstruction}

The Point-MAE approach takes the decoded mask tokens and applies a simple linear layer to predict the original masked point groups. Formally, for each masked token, the decoder outputs $\textbf{x}_g \in \mathbb{R}^{K_\text{max} \times (3+1)}$ predicted points, where $K_\text{max}$ is the maximum possible number of points in any group. To handle variable group sizes, we take only the first $K$ points for each masked group, where $K$ corresponds to the actual number of points in the group. The reconstruction is evaluated using the Chamfer Distance (CD) loss between the predicted points $\{\mathbf{x}_g^{\text{pred}}\}$ and the original points $\{\mathbf{x}_g^{\text{true}}\}$ of the masked patches.

\subsubsection{Per-point Energy Reconstruction}

To further facilitate trajectory representation learning, we introduce an auxiliary energy reconstruction task, whereby the network learns to predict per-point energies given their individual 3D positions.

Standard permutation-invariant PointNets struggle with per-point predictions in variable-sized groups due to unordered processing. We address this with an \textbf{Equivariant Mini-PointNet} that imposes structured order via sinusoidal positional encodings \cite{vaswani2017attention} processed through a learnable MLP (see right of Fig. \ref{fig:arch}). Unlike the original, order embeddings are injected after the first shared MLP layer, intentionally breaking permutation invariance while retaining padding invariance (when padding values match group points). A final linear projection layer concatenates positional masked tokens with the predicted mask tokens from the decoder, predicting $K_\text{max}$ energy values per group via the $L_2$ loss. This enables precise per-point energy reconstruction while handling variable group sizes.

\section{Results}
\label{sec:results}

\subsection{Pre-training}

We train our model on the training set described in \S\ref{sec:data}) using a batch size of 128 for 500,000 iterations, corresponding to about 60 epochs. We use a grouping radius of 5 voxels (15 mm) with an optimized C-NMS overlap fraction $f=0.73$, mask 60\% of input tokens randomly, and adopt ViT-S-like specifications for the encoder/decoder. With our dataset, this results in about 400 groups per event (Figure \ref{fig:group_lens}). Full architectural details are provided in Appendix \ref{app:hyperparameters}, as well as additional commentary on optimizing our grouping strategy in Appendix \ref{app:cnms}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/grouping_analysis_tokens.pdf}
    \vspace{-15pt}
    \caption{\textit{Per-event grouping statistics.} We set the context length of the model to be 512 groups, which leaves plenty of room.}
    \label{fig:group_lens}
\end{figure}


\setlength{\tabcolsep}{2pt}  % globally reduce column spacing
\begin{figure}[ht]
    \centering
    \begin{tabular}{c@{\hspace{2mm}}*{3}{c}@{}}
    & \textsc{Event 1} & \textsc{Event 2} & \textsc{Event 3} \\
    \rotatebox[origin=l]{90}{\textsc{~~~~Random}} & 
    \includegraphics[width=0.3\linewidth]{figs/latent/latent_space_random_10}&
    \includegraphics[width=0.3\linewidth]{figs/latent/latent_space_random_11} &
    \includegraphics[width=0.3\linewidth]{figs/latent/latent_space_random_7} \\
    
    \rotatebox[origin=l]{90}{\textsc{PoLAr-MAE}} & 
    \includegraphics[width=0.3\linewidth]{figs/latent/latent_space_multitask_10} &
    \includegraphics[width=0.3\linewidth]{figs/latent/latent_space_multitask_11} &
    \includegraphics[width=0.3\linewidth]{figs/latent/latent_space_multitask_7} \\
    \end{tabular}
    \caption{\textit{Visualization of learned representations after pretraining.} We use PCA to project the learned representations into RGB space. The Random Initialized model displays a strong positional bias, while the PointMAE model shows bias for individual trajectories. The PointMAE model contains visually similar embeddings as PoLAr-MAE, and is not pictured. Best viewed zoomed in. Additional examples (with embeddings for PointMAE) can be found in Appendix \ref{app:pretraining}.}
    \label{fig:latent_comparison}
    \vspace{-2mm}
\end{figure}

Table \ref{tab:svm} provides SVM F-scores (F\textsubscript{1}) -- defined as the harmonic mean of the precision and recall of the classifier, i.e. $F_1=\frac{2}{\text{recall}^{-1}+\text{precision}^{-1}}$ -- at the end of training for both the PointMAE and PoLAr-MAE. We give metrics for PointMAE from two training runs -- one with 100k training steps ($\sim$2 epochs), and one with 500k training steps ($\sim$60 epochs). These metrics make clear that the model definitively learns the semantic meaning of different types of trajectories without explicitly providing particle ID information. By just pre-training, the SVM can be taught to classify tokens as containing tracks \colorsquare{track} and showers \colorsquare{shower} with F-scores of 99.4\% and 97.7\%, respectively. Particle types that often span one to two tokens are much less discernible, and as such are harder, but not impossible, to discern, having F-scores of 51.8\% for Michels \colorsquare{michel} and 44.0\% for delta rays \colorsquare{delta}. Interestingly, the model learns to very effectively discriminate between tracks and showers just within around 10 epochs of training.

\begin{table}[t]
\centering
\caption{\textit{SVM Validation Results.} We present final SVM validation F\textsubscript{1} scores for each class—where F\textsubscript{1} is the harmonic mean of precision and recall—and an overall mean across classes. The best results are \besto{highlighted}.}
\vskip 0.15in
\label{tab:svm}
\begin{tabular}{llcccccc}
\toprule
& & & \multicolumn{4}{c}{Per-class F\textsubscript{1}}\\
\cmidrule(lr){4-7}
Method & Steps & F\textsubscript{1} & Track & Shower & Delta & Michel \\
 &  &  & \colorsquare{track} & \colorsquare{shower} & \colorsquare{delta} & \colorsquare{michel} \\
\midrule
\textsc{PointMAE}  & 100k & 0.680 & 0.990 & 0.970 & 0.436 & 0.325 \\
\textsc{PointMAE}  & 500k & 0.719 & 0.993 & 0.967 & \best{0.572} & 0.342 \\
\textsc{PoLAr-MAE}    & 500k & \best{0.732} & \best{0.994} & \best{0.977} & 0.518 &\best{ 0.440} \\
\bottomrule
\end{tabular}
\end{table}

To get a qualitative understanding of what these representations of trajectories work out, we plot out the center of each token in an event and color it according to a casting of its latent vectors to RGB space using PCA. Shown in Figure \ref{fig:latent_comparison}, the pretrained models clearly shows an understanding of semantics between tracks, showers, deltas, and Michels. A comparison to the latent representations of a model initialized with random weights is shown for comparison. Indeed, even trajectories who cross very close to one another, despite being very spatially close, have different representations. Tokens along a single particle trajectory are shown to have similar representations. Additional examples can be found in the appendix.

\paragraph{Hyperparameters.}  
The critical hyperparameters we use include a group radius of 5 voxels (15 mm) with C-NMS overlap fraction \( f = 0.73 \), oversampling size of 2048, and a context length of 512 tokens. Groups are capped at 32 points, with farthest point sampling (FPS) applied when exceeding this limit. We mask 60\% of input tokens randomly during training and adopt ViT-S-like specifications for encoder/decoder. Chamfer distance serves as the training objective. Full architectural details and secondary hyperparameters are provided in Appendix~\ref{app:hyperparameters}.

\subsection{Semantic Segmentation}


\setlength{\tabcolsep}{2pt}
\begin{figure}[t!]
    \centering
    \begin{tabular}{c@{\hspace{2mm}}*{3}{c}@{}} 
    & \textsc{Event 1} & \textsc{Event 2} & \textsc{Event 3} \\
    \rotatebox[origin=l]{90}{\textsc{\tiny ~~~~~~~~~~~~~~~~Truth}} & 
    \includegraphics[width=0.3\linewidth]{figs/segmentation/truth_10}&
    \includegraphics[width=0.3\linewidth]{figs/segmentation/truth_11} &
    \includegraphics[width=0.3\linewidth]{figs/segmentation/truth_7} \\
    
    \rotatebox[origin=l]{90}{\textsc{\tiny ~~PoLAr-MAE PEFT}} & 
    \includegraphics[width=0.3\linewidth]{figs/segmentation/mae_multitask_peft_pred_10} &
    \includegraphics[width=0.3\linewidth]{figs/segmentation/mae_multitask_peft_pred_11} &
    \includegraphics[width=0.3\linewidth]{figs/segmentation/mae_multitask_peft_pred_7} \\

    \rotatebox[origin=l]{90}{\textsc{\tiny ~~PoLAr-MAE FFT}} & 
    \includegraphics[width=0.3\linewidth]{figs/segmentation/mae_multitask_fft_pred_10} &
    \includegraphics[width=0.3\linewidth]{figs/segmentation/mae_multitask_fft_pred_11} &
    \includegraphics[width=0.3\linewidth]{figs/segmentation/mae_multitask_fft_pred_7} \\
    \end{tabular}
    \caption{\textit{Visualization of finetuned PoLAr-MAE models on the semantic segmentation task.} Classes are one of \{track \colorsquare{track}, shower \colorsquare{shower}, delta ray \colorsquare{delta}, Michel \colorsquare{michel}. Strong performance for the PoLAr-MAE models trained with parameter-efficient fine-tuning (PEFT) methods and via full fine-tuning (FFT) are shown. Most of the error is from incorrectly classifying spatially small semantic types, e.g., delta rays alongside tracks. The finetuned PointMAE predictions are visually similar to that of the PoLAr-MAE models, and are not pictured. Additional examples (with predictions from the PointMAE models) can be found in Appendix \ref{app:segsem}. Best viewed zoomed in.}
    \label{fig:segsem_examples}
    \vspace{-5mm}
\end{figure}

To evaluate the effectiveness of the representations learned by the PoLAr-MAE encoder, we fine-tune the model to perform dense classification of individual voxels as one of the four particle types: tracks \colorsquare{track}, shower \colorsquare{shower}, delta rays \colorsquare{delta}, and Michel \colorsquare{michel}.  We freeze the encoder and train simple feature upsampling and classification heads to predict per-point classes, following \cite{qi2017pointnet++}. We fine-tune both PoLAr-MAE and PointMAE pretrained models on 10,000 events, 10x less than the number of events used to train SPINE \cite{drielsma2021scalableendtoenddeeplearningbaseddata}, and use the focal loss \cite{lin2018focallossdenseobject}  along with additional loss weights proportional to the inverse frequencies of each class to balance the loss across classes. We perform full fine-tuning (optimizing all parameters of the model) as well as linear probing, i.e., freezing all components of the model except the feature upscaling and segmentation heads, to evaluate how well the representations the encoder learns actually are.

\begin{table}[t]
\centering
\caption{\textit{Semantic segmentation results.} We present F\textsubscript{1} scores for semantic segmentation of each class within the validation set -- where F\textsubscript{1} is the harmonic mean of precision and recall—and an overall mean across classes. The best results are \besto{highlighted}.}
\vskip 0.15in
\label{tab:segsem}
\begin{tabular}{llcccc}
\toprule
& & \multicolumn{4}{c}{Per-class F\textsubscript{1}}\\
\cmidrule(lr){3-6}
Model & F\textsubscript{1} & Track & Shower & Delta & Michel \\
&  & \colorsquare{track} & \colorsquare{shower} & \colorsquare{delta} & \colorsquare{michel} \\
\midrule
\textsc{PointMAE PEFT}  & 0.772 & \best{0.965} & 0.983 & \best{0.569} & 0.572 \\
\textsc{PoLAr-MAE PEFT}   & 0.798 & 0.961 & 0.990  & 0.542 & 0.698 \\
\midrule
\textsc{PointMAE FFT} & 0.831 & 0.963 & \best{0.994} & 0.561 & 0.807 \\
\textsc{PoLAr-MAE FFT} & \best{0.837} & 0.964 & \best{0.994} & \best{0.569} & \best{0.823} \\
\bottomrule
\end{tabular}
\vspace{-9mm}
\end{table}

Table \ref{tab:segsem} shows results on semantic segmentation across the two pretraining strategies and the two fine-tuning strategies using F\textsubscript{1} scores as evaluation metrics. Overall, our fine-tuned models performs exceptionally well, especially given these models were fine-tuned with just 10,000 events. PoLAr-MAE overall outperforms or is comparable to PointMAE in the FFT mdoels, however there is a clear trade-off in Michel/delta classification with the PEFT models. Qualitative examples of semantic segmentation predictions are shown in Fig. \ref{fig:segsem_examples}. A direct comparison of average precision values reported in \cite{drielsma2021scalableendtoenddeeplearningbaseddata} are shown in Figure \ref{fig:spine_comparison}. Critically, our model outperforms the fully supervised approach for shower and track classification by 2\% and 0.1\%, respectively, while performing fairly worse for Michel and delta-ray classification. A more detailed comparison between models can be found in Appendix \ref{app:segsem}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/segmentation/segsem_spine_comparison.pdf}
    \vspace{-10pt}
    \caption{\textit{Comparison to the fully supervised approach.} We compare to reported average classification precision values found in \cite{drielsma2021scalableendtoenddeeplearningbaseddata}. Notably, our fully fine-tuned PoLAr-MAE model outperforms the Sparse UResNet-based SPINE while training on 10x less labeled data.}
    \label{fig:spine_comparison}
    % \vspace{-5mm}
\end{figure}


\section{Discussion}

This work demonstrates the viability of SSL for TPC data analysis, bridging the gap between foundational methodologies in computer vision and the unique challenges of high-energy physics. By reducing dependency on labeled data and enabling scalable cross-experiment analyses, our approach represents a small step toward the development of generalized, domain-agnostic foundation models for particle physics.

An important aspect of this work is the introduction of the Centrality-Based Non-Maximum Suppression (C-NMS) tokenization approach. Unlike traditional grouping methods such as farthest point sampling (FPS) with $k$-nearest neighbors (k-NN) or ball query, C-NMS dynamically adjusts to the spatial density of TPC point clouds. This ensures minimal overlap and maximized coverage of particle trajectories, addressing key limitations of existing methods.

The high $F_1$ scores achieved for track (0.994) and shower (0.977) token classification validate the efficacy of our SSL approach, while the semantic segmentation results show a successful transfer to dense classification. These metrics are on par with state-of-the-art supervised frameworks, such as SPINE \cite{drielsma2021scalableendtoenddeeplearningbaseddata}, despite not relying on any labeled examples. This demonstrates that SSL frameworks can robustly capture the complex geometries and energy depositions characteristic of TPC data. However, the suboptimal modeling of the highly-infrequent particles, such as Michel electrons and delta rays, indicates a limitation in our current framework. Future studies should try to remedy this issue, by perhaps reducing the group radius.

Despite these successes, several avenues for improvement remain. The computational overhead introduced by C-NMS could be further optimized to enable real-time processing of large datasets. Exploring alternative SSL paradigms, such as contrastive learning and/or student-teacher distillation frameworks, may also offer benefits. In any case, we hope that the introduction of the PILArNet-M dataset into the community will spur additional research in this domain.

Throughout the development of this work, many changes were introduced to enforce the learning of better representations. We expand on several modifications we tried, but did not result in any meaningful change in token semantics, in Appendix \ref{app:failures}.


\label{sec:discussion}

% % Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

This work is supported by the U.S. Department of Energy, Office of Science, and Office of High Energy Physics under Contract No. DE-AC02-76SF00515. SY is supported by the Stanford HAI Graduate Fellowship.
% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

This work advances self-supervised learning (SSL) for 3D particle trajectory analysis in Time Projection Chambers (TPCs), a critical tool in high-energy physics (HEP). By designing SSL techniques that operate on sparse, irregular, and high-precision scientific data, we demonstrate how representation learning can be adapted to the unique constraints of experimental physics.

Beyond particle physics, this work highlights the kinds of innovations needed to make popular machine learning methods viable in hyper-constrained scientific domains. Many experimental sciences -- from astrophysics to materials science and biomedical imaging -- grapple with large-scale, highly structured, and often non-Euclidean data that traditional deep learning architectures are ill-equipped to handle. Our contributions, such as resolution-agnostic volumetric tokenization and energy-aware masked modeling, provide an example for how SSL can be molded to function in environments that demand both extreme data efficiency and precise physical interpretability.

We hope this work inspires applications of SSL in other experimental sciences that generate high-volume, high-dimensional data but lack sufficient human-labeled annotations. By reducing reliance on manual supervision, improving data efficiency, and promoting open data sharing through PILArNet-M, we aim to accelerate scientific discovery beyond particle physics, helping to bridge the gap between machine learning advances and real-world experimental constraints.

From an ethical standpoint, this research does not introduce new societal risks, as it is confined to fundamental physics research. Its consequences of failure are equivalent to those of traditional reconstruction methods -- potential inaccuracies in scientific interpretations -- without any direct impact on broader society. Nonetheless, we emphasize the importance of continued scrutiny in evaluating biases and uncertainties inherent to any machine learning model used in scientific inference.


% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Dataset}
\label{app:dataset}

The PILArNet-M dataset contains 5.2B points containing energy deposited from 28,251,859 individual particle trajectories, over 1,210,080 individual events. As described \S\ref{sec:data} of the main text, each voxel contains information corresponding to not just the energy deposited, but also a fragment ID, group ID, interaction ID, and semantic type. Though we just use the semantic type in this work, we foresee the other IDs being quite useful for other tasks, like, i.e., latent clustering for individual particle (i.e., group ID) identification. Dataset-wide statistics are shown in Figure \ref{fig:statistics}. We also offer visualizations of individual events, where voxels with the same colors correspond to sharing different IDs, in Figure \ref{fig:ids}. See \cite{adams2020pilarnetpublicdatasetparticle} for detailed information about this dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.42\linewidth]{figs/cluster_counts.pdf}
    \includegraphics[width=0.56\linewidth]{figs/voxel_counts.pdf}
    \includegraphics[width=0.5\linewidth]{figs/energy_distributions.pdf}
    \caption{\textit{PILArNet-M statistics.} Starting from top left and going clockwise: cluster frequencies, i.e., how prevalent each particle type is on a cluster (trajectory) basis; voxel frequencies, i.e., the total semantic ID makeup of all voxels in the dataset; voxel energy distributions, i.e., a histogram of the energies of any occupied voxel across all events in the dataset. The number in the bar plots represent the raw number of each quantity found in the dataset, while the bars themselves are normalized to sum to 1. Note that in the energy deposition distribution, energy values below 0.001 MeV are not present.}
    \label{fig:statistics}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tabular}{c@{\hspace{1mm}}*{8}{c}@{}} % Adjust number of columns {*{N}{c}} for N events
    & \textsc{Event 4} & \textsc{Event 5} & \textsc{Event 6} & \textsc{Event 7} & \textsc{Event 8} & \textsc{Event 9} & \textsc{Event 10} \\
    \rotatebox[origin=l]{90}{\textsc{Fragment}} & 
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_0}&
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_1} &
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_2} &
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_4} &
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_5} &
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_6} &
    \includegraphics[width=0.13\linewidth]{figs/dset/cluster_8} &

    \\\vspace{-5pt}
    
    \rotatebox[origin=l]{90}{\textsc{~~~~~Group}} & 
    \includegraphics[width=0.13\linewidth]{figs/dset/group_0}&
    \includegraphics[width=0.13\linewidth]{figs/dset/group_1} &
    \includegraphics[width=0.13\linewidth]{figs/dset/group_2} &
    \includegraphics[width=0.13\linewidth]{figs/dset/group_4} &
    \includegraphics[width=0.13\linewidth]{figs/dset/group_5} &
    \includegraphics[width=0.13\linewidth]{figs/dset/group_6} &
    \includegraphics[width=0.13\linewidth]{figs/dset/group_8} &


    \\
    \rotatebox[origin=l]{90}{\textsc{Interaction}} & 
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_0}&
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_1} &
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_2} &
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_4} &
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_5} &
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_6} &
    \includegraphics[width=0.13\linewidth]{figs/dset/interaction_8} &

    \\
    \rotatebox[origin=l]{90}{\textsc{Semantics}} & 
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_0}&
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_1} &
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_2} &
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_4} &
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_5} &
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_6} &
    \includegraphics[width=0.13\linewidth]{figs/dset/semantic_8} \\
\end{tabular}
    \caption{\textit{Visual depictions of 7 PILArNet-M events.} For each event, voxels are colored according to which integer identifiers they share, for each label type (fragment / cluster ID, group ID, interaction ID, and semantic ID). For ease of visualization, we remove low energy deposits, which would greatly degrade the ability to visualize what each identifier means.}
    \label{fig:ids}
    \vspace{-50mm}
\end{figure}


\newpage
\section{C-NMS}
\label{app:cnms}
% \subsection{Discussion}

Non-Maximum Suppression (NMS) algorithms are widely used to resolve redundancies in overlapping spatial proposals. In object detection, for instance, NMS iteratively selects the bounding box with the highest confidence score and suppresses neighboring proposals that exceed a predefined overlap threshold (e.g., Intersection-over-Union $>$ 0.5). This ensures that only the most relevant detections are retained, balancing precision and recall. 

Our \textbf{Centrality-based NMS (C-NMS)} adapts this philosophy to 3D point cloud grouping. Instead of bounding boxes, we process spherical groups centered on points selected through farthest point sampling (FPS). The algorithm prioritizes spheres whose centroids are most central within their local neighborhoods, iteratively suppressing overlapping candidates based on a tunable \textit{overlap factor} \( f \in [0, 1] \). Two spheres are considered overlapping if the distance between their centers is less than \( 2rf \), where \( r \) is the sphere radius. Lower values of \( f \) tolerate more overlap (yielding denser groups), while \( f \to 1 \) enforces strict non-overlap. 

Implementationally, C-NMS leverages CUDA-accelerated spatial queries via \textsc{pytorch3d}'s ball tree for efficient neighborhood searches, combined with a simple custom kernel to batch-process the iterative suppression across events. This balances computational efficiency with flexibility, allowing dynamic adjustment of group density without predefined cluster counts. 

The subsequent sections compare C-NMS+ball query against conventional FPS+\{\(k\)-NN, ball query\} methods, evaluating their trade-offs in coverage (\% missed points), and redundancy (\% duplicated points). By explicitly optimizing for minimal overlap, C-NMS addresses a limitation of prior grouping strategies when applied to irregular 3D geometries like particle trajectories.

The C-NMS algorithm is described in Algorithm \ref{alg:cnms}.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[H]
\caption{Centrality-Based Non-Maximum Suppression (C-NMS)}
\label{alg:cnms}
\begin{algorithmic}[1]
\REQUIRE $C \in \mathbb{R}^{P \times 3}$: Set of $P$ centroid coordinates, $r \in \mathbb{R}^{+}$: Sphere radius, $f \in [0, 1]$: Overlap factor.
\ENSURE $C' \in \mathbb{R}^{P' \times 3}$: Culled set of centroids, where $P' \leq P$.

\STATE \algcomment{Minimum overlapping distance}
\STATE $r_q \gets 2 \cdot r \cdot f$ 
\STATE \algcomment{Find neighbors within distance $r_q$}
% \STATE $N \gets \text{ComputeNeighborhoods}(C, r_q)$ 
\STATE $N \gets \emptyset$
\FOR{$c_i \in C$}
    \STATE $N_i \gets \{c_j \in C \mid \|c_i - c_j\| \leq r_q\}$
    \STATE $N \gets N \cup \{N_i\}$
\ENDFOR

\STATE \algcomment{Centrality scores for each centroid}
\STATE $S \gets \{|N_i| \;\forall i \in \{1, \dots, P\}\}$ 


\STATE \algcomment{Sort centroids by scores}
% \STATE $C_{\text{sorted}} \gets \text{SortByCentrality}(C, S)$ 
\STATE $\text{indices} \gets \text{argsort}(S, \text{descending=True})$
\STATE $C_{\text{sorted}} \gets C[\text{indices}]$


\STATE \algcomment{Perform iterative suppression}
\STATE $C' \gets \emptyset$
\STATE $\text{suppressed} \gets \text{zeros}(P)$ 
\FOR{$c_i \in C_{\text{sorted}}$}
    \IF{$\text{suppressed}[i] = 0$}
        % \STATE  \algcomment{Add centroid to retained set}
        \STATE $C' \gets C' \cup \{c_i\}$
        \FOR{$c_j \in N_i$ \textbf{and} $c_j \neq c_i$}
            % \STATE \algcomment{Mark neighbor as suppressed}
            \STATE $\text{suppressed}[j] \gets 1$ 
        \ENDFOR
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Comparison of Grouping Strategies}

\subsubsection{Evaluation Metrics}
\label{subsec:metrics}

A central challenge in adapting Masked Autoencoders (MAEs) to 3D particle trajectory reconstruction is optimizing the trade-off between two competing objectives: \textbf{comprehensive coverage} (minimizing ungrouped points) and \textbf{group distinctness} (minimizing overlap). We formalize this through two metrics:

\vspace{0.2cm}

\noindent
\textbf{1. Missed Points Ratio}  
\begin{equation}
M = \left(1 - \frac{|\mathcal{X}_{\text{grouped}} \cap \mathcal{X}|}{|\mathcal{X}|}\right), 
\end{equation}  
where $\mathcal{X}$ is the full set of points in an event, and $\mathcal{X}_{\text{grouped}} \subseteq \mathcal{X}$ represents points assigned to any group.  

\vspace{0.2cm}

\noindent
\textbf{2. Duplicated Points Ratio}  
\begin{equation}
D = \frac{|\{x \in \mathcal{X} \mid x \text{ appears in }> 1\; \text{group}\}|}{|\mathcal{X}|}. 
\end{equation}

\vspace{0.2cm}

\noindent
These metrics address critical failure modes in MAE pre-training:  
\begin{itemize}
    \vspace{-0.2cm}
    \item \textit{High $M$}: Fails to include trajectory segments in any group, degrading reconstruction quality.
    \vspace{-0.2cm}
    \item \textit{High $D$}: Allows leakage between masked and visible regions, artificially simplifying the task (unlike image MAEs, where masks are strictly non-overlapping).  
\end{itemize}

\noindent
Prior work mitigates leakage by using large masking ratios ($>$60\%), but this only \textit{reduces} overlap probability rather than eliminating it. For particle trajectory datasets—where spatial correlations are strong—even minor overlaps risk exposing the model to trivial reconstruction shortcuts. Our \textbf{C-NMS} strategy directly optimizes the Pareto frontier of $M$ vs. $D$ (see Figure \ref{fig:pareto} in the main text), ensuring groups are both \textit{complete} (low missed points) and \textit{isolated} (low duplication). This mirrors the non-overlapping structure of successful 2D image masking while adapting to the irregular geometry of particle clouds.


\subsubsection{Comparison}
\label{subsec:grouping_comparison}


\begin{figure}[h!!!]
    \centering
    % \hspace*{-0.15\linewidth}
    \includegraphics[width=0.8\linewidth]{figs/grouping_analysis_heatmaps.pdf}
    \vspace{-10pt}
    \caption{\textit{Detailed grouping hyperparameter sweep.} To find the pareto frontier of grouping methodologies, we sweep over an array of reasonable grouping parameters for each method, reporting the missed ratio $M$ and duplicated ratio $D$ heatmaps for each hyperparameter pair. Clearly, the na\"ive based approaches result in an extreme amount of duplicated points in any configuration, while the C-NMS-based grouping method empirically guarantees much smaller amounts of each quantity.}
    \label{fig:hyperparam_sweep}
\end{figure}


\paragraph{Pareto Frontier of Grouping Methodologies.} To systematically compare grouping strategies, we perform parameter sweeps for each method. Initial group centers are sampled via FPS (2048 per event). These parameter ranges are described in Table \ref{tab:param_sweep}.

\begin{table}[]
\centering
\caption{Parameter configurations for grouping method sweeps}
\vskip 0.15in
\label{tab:param_sweep}
\begin{tabular}{ll}
\toprule
\textbf{Method} & \textbf{Parameters} \\ \midrule
C-NMS + ball query & 
\begin{tabular}[t]{@{}l@{}}
Sphere radius: $r \in \text{arange}(2, 11, 1)$ voxels (1 voxel = 3~mm\textsuperscript{3}) \\
Overlap factor: $f \in \text{linspace}(0.5,\ 1.0,\ 20)$
\end{tabular} \\ \midrule 

FPS + $k$-NN & 
\begin{tabular}[t]{@{}l@{}}
Group count: $\alpha \in \text{logspace}(0.5,\ 8,\ 20,\ \text{base}=2)$ \\
Neighborhood size: $k \in \{1,\ 4,\ 8,\ 16,\ 24,\ 32,\ 40,\ 48,\ 56,\ 64,\ 128\}$
\end{tabular} \\ \midrule

FPS + ball query & 
\begin{tabular}[t]{@{}l@{}}
Group count: $\alpha \in \text{logspace}(0.5,\ 8,\ 20,\ \text{base}=2)$ \\
Query radius: $r \in \text{arange}(2,\ 11,\ 1)$ voxels \\
\end{tabular} \\
\bottomrule
\end{tabular}
\vskip -0.1in
\end{table}

\noindent
For FPS-based methods, the number of samples per event are defined as $\lceil |\mathcal{X}| / \alpha \rceil$ where $|\mathcal{X}|$ is the number of points in the event. Sweeps generate $80-220$ configurations per method, with Pareto frontiers computed from the missed/duplicated ratio metrics defined in \S\ref{subsec:metrics}. Figure \ref{fig:hyperparam_sweep} provides array of figures showing results for each metric across these sweeps.

\section{Hyperparameters}
\label{app:hyperparameters}

We provide detailed hyperparameter information in the following sections. The full implementation of these models, as well as training code, can be found at \url{https://github.com/DeepLearnPhysics/PoLAr-MAE/}.

\begin{table}[h!!!]
\centering
\caption{Model Architecture}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Parameters} \\
\midrule
\textbf{Tokenizer \& Masking} & Transformer Encoder \\
Num Channels & 4 \\
Num Seed Groups & 2048 \\
Context Length & 512 \\
Group Max Points $K_\text{max}$ & 32 \\
Group Radius $r$ &  5 voxels (15 mm) \\
Overlap factor & 0.73 \\
Group Size Reduction Method & FPS \\
Masking Method & Random \\
Masking Ratio & 0.6 \\
\midrule
\textbf{Encoder} & Transformer Encoder \\
Base Architecture & ViT-S \\
Activation Layer & GELU \\
Normalization Layer & LayerNorm \\
Normalization Type & Pre + Post \\
Embed Dim & 384 \\
Depth & 12 \\
Num Heads& 6 \\
MLP Ratio & 4 \\
Final LayerNorm & False \\
Add Position Embeddings at Every Layer & True \\
Dropout Rate & 0.0 \\
MLP / Projection Dropout Rate & 0.0 \\
Attention Dropout & 0.05 \\
Stochastic Depth Rate & 0.25 \\
\midrule
\textbf{Decoder} & Transformer Decoder \\
Base Architecture & ViT-S \\
Activation Layer & GELU \\
Normalization Layer & LayerNorm \\
Normalization Type & Pre + Post \\
Embed Dim & 384 \\
Depth & 4 \\
Num Heads& 6 \\
MLP Ratio & 4 \\
Final LayerNorm & True \\
Add Position Embeddings at Every Layer & True \\
Dropout Rate & 0.0 \\
MLP / Projection Dropout Rate & 0.0 \\
Attention Dropout & 0.05 \\
Stochastic Depth Rate & 0.25 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h!!!]
\centering
\caption{Optimizer \& Learning Rate Schedule}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & AdamW \\
Base Learning Rate & 1e-4 \\
Weight Decay & 0.05 \\
$\beta_1$ & 0.9 \\
$\beta_2$ & 0.99 \\
Distributed Training Strategy & DDP \\
Gradient Clip & 3.0 \\
LR Schedule & Linear Warmup + Cosine Decay \\
Total Steps & \{100k, 500k\} \\
Warmup Steps & 12,500 \\
Warmup Start LR & 8.6e-6 \\
Cosine Eta Min & 8.6e-6 \\
Scheduler Steps & Per Optimization Step \\
Effective Batch Size & 128 \\
Training precision & BF16-Mixed \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Grouping strategy}


\paragraph{Overlap factor $f$. }We chose a group radius equal to 5 voxels (15 mm) such that a 1m track will contain ~67 mm, and because it is approximately the characteristic distance of trajectory morphologies. To find the best value for the overlap factor $f$, we run a sweep over values of $f$, and looking at the missed points ratio $M$ and duplicated points ratio $D$ as defined in Appendix \ref{subsec:metrics}. Fig. \ref{fig:f_sweep} shows this trade-off for different values of $f$. The value of $f$ that jointly minimizes both these scores if $f=0.73,$ and hence we use this value when pretraining with this grouping strategy.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/grouping_analysis_5vox.pdf}
    \vspace{-10pt}
    \caption{\textit{C-NMS overlap parameter $f$ sweep.} We find the optimal $f$ parameter for our group radius of 5 voxels by minimizing both the missed ratio $M$ and duplicated ratio $D$. We find $f=0.73$ to be approximately optimal.}
    \label{fig:f_sweep}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/grouping_analysis_group_size.pdf}
    \vspace{-10pt}
    \caption{\textit{Group size statistics.} We show the number of points found in each group given a group radius of 5 voxels and $f=0.73$.}
    \label{fig:group_sizes}
\end{figure}

\paragraph{Group size.} We determine the number of points allowed per group based on the distribution of groups per event shown in Fig. \ref{fig:group_lens}, as well as our embedding size. This hyperparameter is extremely important, as our model is mostly bottlenecked by memory constraints as a result of the point group embedding module. Within the mini-PointNet, each individual point is lifted to a large embedding dimension (384 for this work), and as such larger and larger group sizes greatly increase the memory budget required to train the model. Fig. \ref{fig:group_sizes} shows a distribution of group sizes across a number of LArTPC events. Here, the group sizes peak around 16 points, but there is a long tail that extends to $\sim~$150 points. To keep as many points as possible without running into memory issues, we set $K_\text{max}$, the maximum number of points to take within a group, to 32. For groups with more than 32 points, we randomly sample 32 points with farthest point sampling.


\section{Pretraining Results}
\label{app:pretraining}

In Figure \ref{fig:latent_comparison_extras}, we show additional visualizations of representations learned by both PointMAE and PoLAr-MAE.

\begin{figure}[h]
    \centering
    \begin{tabular}{c@{\hspace{1mm}}*{8}{c}@{}} % Adjust number of columns {*{N}{c}} for N events
    & \textsc{Event 4} & \textsc{Event 5} & \textsc{Event 6} & \textsc{Event 7} & \textsc{Event 8} & \textsc{Event 9} & \textsc{Event 10} \\
    \rotatebox[origin=l]{90}{\textsc{~~~Random}} & 
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_0}&
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_1} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_2} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_4} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_5} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_6} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_random_8} &

    \\
    
    \rotatebox[origin=l]{90}{\textsc{~PointMAE}} & 
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_0}&
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_1} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_2} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_4} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_5} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_6} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_mae_8} &


    \\
    \rotatebox[origin=l]{90}{\textsc{PoLAr-MAE}} & 
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_0}&
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_1} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_2} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_4} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_5} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_6} &
    \includegraphics[width=0.125\linewidth]{figs/latent/latent_space_multitask_8} \\
\end{tabular}
    \caption{\textit{Additional visualizations of learned representations after pretraining.} See the caption of Fig. \ref{fig:latent_comparison} for an explanation.}
    \label{fig:latent_comparison_extras}
\end{figure}

\section{Segmentation Results}
\label{app:segsem}

In Figure \ref{fig:segsem_comparison}, we show additional examples of semantic segmentation results for both PointMAE and PoLAr-MAE fine-tuned models. In Table \ref{tab:extended_seg}, we provide additional evaluation metrics for the fine-tuned PointMAE and PoLAr-MAE models. We also supply confusion matrices for each model, normalized across predictions, in Figure \ref{fig:confusion}.

\begin{figure}[h]
    \centering
    \begin{tabular}{c@{\hspace{1mm}}*{8}{c}@{}} % Adjust number of columns {*{N}{c}} for N events
    & \textsc{Event 4} & \textsc{Event 5} & \textsc{Event 6} & \textsc{Event 7} & \textsc{Event 8} & \textsc{Event 9} & \textsc{Event 10} \\
    \rotatebox[origin=l]{90}{\textsc{~~~Truth}} & 
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_0}&
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_1} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_2} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_4} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_5} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_6} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/truth_8} &

    \\\vspace{-5pt}
    
    \rotatebox[origin=l]{90}{\textsc{~~~PL PEFT}} & 
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_0}&
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_1} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_2} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_4} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_5} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_6} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_peft_pred_8} &


    \\
    \rotatebox[origin=l]{90}{\textsc{PMAE PEFT}} & 
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_0}&
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_1} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_2} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_4} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_5} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_6} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_peft_pred_8} &

    \\
    \rotatebox[origin=l]{90}{\textsc{~~~PL FFT}} & 
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_0}&
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_1} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_2} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_4} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_5} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_6} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_multitask_fft_pred_8} &
    \\
    \rotatebox[origin=l]{90}{\textsc{PMAE FFT}} & 
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_0}&
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_1} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_2} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_4} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_5} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_6} &
    \includegraphics[width=0.13\linewidth]{figs/segmentation/mae_fft_pred_8} \\
\end{tabular}
    \caption{\textit{Further examples of dense classification.} Model types are abbreviated with PL meaning PoLAr-MAE, PMAE meaning PointMAE, PEFT meaning parameter-efficient fine-tuning, and FFT meaning full fine-tuning. Best viewed zoomed in.}
    \label{fig:segsem_comparison}
\end{figure}


\begin{table}[h]
\centering\hspace{-20pt}
\caption{\textit{Semantic segmentation results (extended).} We present detailed metrics including mean F\textsubscript{1}, precision, recall, and per-class scores. The best results are \besto{highlighted}.}
\vskip 0.15in
\label{tab:extended_seg}
\begin{tabular}{l *{3}{c c c c c}}
\toprule
& \multicolumn{5}{c}{F\textsubscript{1}} & \multicolumn{5}{c}{Precision} & \multicolumn{5}{c}{Recall} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}
Model & Mean & Track & Shower & Delta & Michel & Mean & Track & Shower & Delta & Michel & Mean & Track & Shower & Delta & Michel\\
& & \colorsquare{track} & \colorsquare{shower} & \colorsquare{delta} & \colorsquare{michel} & & \colorsquare{track} & \colorsquare{shower} & \colorsquare{delta} & \colorsquare{michel} & & \colorsquare{track} & \colorsquare{shower} & \colorsquare{delta} & \colorsquare{michel} \\
\midrule
\textsc{PoLAr-MAE PEFT}
& 0.798 &	0.961 &	0.990 &	0.542 &	0.698
& 0.730 &	0.995 &	0.997 &	0.378 &	0.548
& 0.958 &	0.929 &	0.983 &	0.956 & 0.963 \\


\textsc{PointMAE PEFT}
& 0.772 &	0.965 &	0.983 &	\best{0.569} &	0.572
& 0.702 &	0.990 &	\best{0.998} &	\best{0.413} &	0.406
& 0.948 &	\best{0.941} &	0.968 &	0.919 &	\best{0.967} \\

\textsc{PointMAE FFT}
& 0.831 &	0.963 &	\best{0.994} &	0.561 &	0.807
& 0.771 &	\best{0.996} &	0.997 &	0.395 &	0.697
& 0.962 &	0.932 &	0.990 &	0.969 &	0.957 \\

\textsc{PoLAr-MAE FFT}
& \best{0.837} &	\best{0.964} &	\best{0.994} &	\best{0.569} &	\best{0.823}
&\best{0.779} &	\best{0.996} &	0.997 &	0.402 &	\best{0.720}
& \best{0.964} &	0.934 &	\best{0.992} &	\best{0.970} &	0.959 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
    & \textsc{PointMAE} & \textsc{PoLAr-MAE} \\
    \rotatebox[origin=l]{90}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\textsc{PEFT}} &
    \includegraphics[width=0.4\linewidth]{figs/confusion/confusion_matrix_norm_pred_mae_peft_cls0123} &
    \includegraphics[width=0.4\linewidth]{figs/confusion/confusion_matrix_norm_pred_mae_multitask_peft_cls0123} \\[1em]
    \rotatebox[origin=l]{90}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\textsc{FFT}} &
    \includegraphics[width=0.4\linewidth]{figs/confusion/confusion_matrix_norm_pred_mae_fft_cls0123} &
    \includegraphics[width=0.4\linewidth]{figs/confusion/confusion_matrix_norm_pred_mae_multitask_fft_cls0123} \\
\end{tabular}
\caption{\textit{Semantic segmentation confusion matrices.} Rows denote fine-tuning strategies: PEFT and FFT. Columns denote the pre-trained model used: PointMAE and PoLAr-MAE. Each column sums to 1 such that the diagonal values represent precision metrics for each class.}
\label{fig:confusion}
\end{figure}

\newpage
\section{Failures}
\label{app:failures}

In the spirit of the computer vision community's recent increasing efforts in portraying both positive and negative results, we document the directions we explored to improve representations that ultimately resulted in either worse or similar performance.

\begin{itemize}
    \item \textbf{Energy Normalization and Embedding.} We attempted to apply centering and scaling group normalizations to the energy values and added an additional ``center energy" embedding to the tokens, similar to the positional embeddings used for coordinates. However, this approach did not yield any noticeable improvement.

    \item \textbf{Handling Stochasticity in LArTPC Events.} LArTPC events are inherently stochastic, as particle energy depositions result from a complex chain of Markov processes. Some particle types, such as tracks, are easier to predict due to their structured nature, while others, like delta rays and electromagnetic (EM) showers, are highly stochastic and challenging to predict. For example, predicting masked portions of delta rays or EM showers is nearly impossible because their presence and trajectories depend on probabilistic physical interactions. To address this, we conditioned the mask decoder with embeddings of diffused masked groups, following DiffPMAE \cite{li2024diffpmaediffusionmaskedautoencoders}. We tested both permutation-equivariant and regular PointNet-based per-point reconstruction methods. Despite these efforts, no meaningful improvements in semantic understanding were observed.

    \item \textbf{Enforcing Sub-Token Semantics.} To address the limitations in sub-token semantic representation, we added an autoencoder task on visible tokens, forcing the encoder to simultaneously encode both the original points and global context within all tokens. However, this approach introduced confusion in the encoder, resulting in degraded representations and worse performance.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
c