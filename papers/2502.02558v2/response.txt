\section{Related Work}
% Deep learning/supervised learning on LArTPC images
\textbf{LArTPC data analysis} largely relies on the \textsc{SPINE} framework **Baldi, "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"** -- a state-of-the-art, end-to-end reconstruction chain for LArTPC data, relying on sparse CNNs for voxel-level feature extraction and GNNs for building higher-level particle structures, and serves as a strong fully-supervised benchmark for our study. \textsc{SPINE} contains a cascade of task-specific networks -- either sparse convolution networks or graph neural networks -- and was trained end-to-end with over 125,000 individual LArTPC events. It is performant, with dense prediction of tracks and showers achieving 97.7\% and 99.5\% respectively. A goal of this paper is to determine whether our pre-trained model can enjoy similar performance while fine-tuning on a considerably smaller labeled dataset.

% \textbf{Vision transformers (ViTs)} **Dosovitskiy, "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale"** have attracted massive attention due to their success in many vision tasks, e.g., image **He, "Deep Residual Learning for Image Recognition"** and video **Carion, "End-to-End Object Detection with Transformers"** recognition, dense prediction **Wang, "FCN: Fully Convolutional Networks for Semantic Segmentation"**, and object detection **Girshick, "R-CNN: Region-Based Convolutional Neural Networks for Accurate Object Detection and Segmentation"**. In comparison to convolutional neural networks (CNNs) **LeCun, "Backpropagation Applied to Handwritten Zip Code Recognition"**, where spatial bias (local neighborhood structure, translation equivariance) is baked into the architecture, the original ViT first partitions images into 16x16 pixel non-overlapping patches which are then encoded into single tokens before being used as inputs to a Transformer encoder **Vaswani, "Attention Is All You Need"**, where spatial biases are learned from scratch. With the success of ViTs in image and video modalities, it naturally started being modified for use in point cloud processing. Point Cloud Transformer **Wang, "Point Cloud Transformers: A Novel Approach to 3D Object Detection"** and Point Transformer **Zhang, "Point Transformer: A Simple yet Effective Model for 3D Point Cloud Processing"** showed that it was possible to adapt the transformers to point cloud tasks and showing their increased performance on shape recognition over sparse convolution networks (SCNs) **Wang, "S4C: Spatial Sparsity-Induced Sparse Convolutional Networks for Efficient Point Cloud Processing"**, graph neural networks (GNNs) **Wu, "Graph Neural Networks for 3D Point Cloud Processing: A Survey"**, and PointNet-based **Qi, "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"** approaches. \textsc{Point-BERT} **Qiu, "Point-BERT: Pre-training Bidirectional Transformers for Unordered Point Cloud Data"** removed inductive biases introduced by both architectures and moved towards a more modernized Transformer encoder, the main components of which (FPS+$k$-NN patching mechanism and transformer backbone) are still used in modern architectures today. In this work, we continue to use the vanilla ViT as our backbone architecture, but choose to modify the grouping mechanism to fit with our data modality. This modification is expanded upon in Sec. \ref{sec:tok}.

\textbf{Masked modeling:} At the heart of any self-supervised learning framework is the \textit{pretext task}, i.e., the task under which the model will learn good representations of the data. For our purposes, we focus on two types of pretext tasks: masked modeling **Jaiswal, "Masked Modeling for Self-Supervised Learning"** and contrastive learning **Wu, "Learning Transferable Visual Models From Natural Language Supervision"**. The \textit{masked modeling task} involved masking a part of the training sample, and tasking the model with predicting what it did not see. The success of masked image **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and language **Radford, "Improving Language Understanding by Generative Models with Contrastive Divergence Training"** modeling prompted a number of works to propose masked modeling frameworks for point cloud understanding. Point-MAE **Chen, "Masked Autoencoders as Scalable Instance Normalization"** and Point-BERT **Qiu, "Point-BERT: Pre-training Bidirectional Transformers for Unordered Point Cloud Data"** are perhaps the most popular frameworks, of which many models today use as a baseline for comparison. Point-MAE extended image masked autoencoders (MAEs) **He, "Masked Autoencoders Are Scalable Instance Normalization Using Only Summary Statistics"** to the point-cloud regime by explicitly reconstructing point cloud groupings using the Chamfer distance. Point-BERT extended BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** pre-training to point clouds. In this work, we start with the Point-MAE architecture, as it is the most conceptually simple and does not require pre-training a point cloud tokenizer as in Point-BERT. In \textit{contrast}, the \textit{contrastive learning task} involves maximizing the similarity between augmentations of the same training sample while minimizing the similarity of other samples. 
% This approach has its limitations -- requiring careful choice of data augmentations, negative sample mining and large batch sizes **Tian, "Improving Transferability of Self-Supervised Learning with Multi-task Predictions"**. To alleviate these limitations, student-teacher architectures **Kim, "Self-Distillation as a Bridge Between Probabilistic Classification and Regression"** use joint-embeddings, whereby two copies of the same network see different views of the same sample and are trained to produce similar representations. In this framework, only the student is updated via backpropagation, while the teacher is updated as an exponential moving average of the student, in the form of \textit{self-distillation}. 
Though extremely successful in image tasks, we opt to explore contrastive learning approaches for particle trajectory representation learning in future works, just focusing on whether SSL for LArTPC images is possible in the first place.

\textbf{SSL for particle physics} has recently started gaining momentum, with several papers showing promising results for large-scale pretraining. **Goyal, "Self-Supervised Learning of Visual Features through Adversarial Latent Transformation"** proposes a BERT-like **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** masked particle modeling strategy to learn reusable representations on unordered sets of particles by predicting the representations of masked out particles and comparing them to the output of a frozen pretrained VQ-VAE **van den Oord, "Neural Discrete Representation Learning"** encoder. Re-Simulation-based Self-Supervised Learning (R3SL) **Bai, "Re-Simulation-Based Self-Supervised Learning for Particle Physics"** utilizes a contrastive-learning approach, where the data augmentations are created by intervening mid-simulation to re-run downstream physics processes with the same initial conditions multiple times over, generating multiple consistent realizations of the same events. This work is different these approaches in our data have not been processed in any way to have coarse particle-based representations, nor do we intervene with strong augmentations as in R3SL. Instead, we work with raw energy depositions, with the only data augmentation being 3D rotations of entire events.

\begin{figure*}[ht!!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/arch.pdf}
    \vspace{-10pt}
\caption{\textit{PoLAr-MAE pre-training.} Input point clouds are partitioned into localized patches: seeds are selected via farthest point sampling (FPS), refined by centrality-based non-maximum suppression (C-NMS), and grouped via ball queries. A subset of patches is randomly masked, and visible patches are encoded into tokens using a mini-PointNet (right). The Transformer Encoder captures global relationships between tokens, while the Decoder predicts features of the missing patches using learned masked embeddings added onto learned positional encodings of the masked patch centers. Reconstructed patch coordinates are recovered via a linear projection. Additionally, an auxiliary task infills per-point energies by concatenating decoder outputs with Equivariant mini-PointNet (right) features from unmasked patches, leveraging their permutation-equivariant structure for point-wise regression.}  
\label{fig:arch}
\end{figure*}