@article{A3,
  author       = {Tae Jun Ham and
                  Sungjun Jung and
                  Seonghak Kim and
                  Young H. Oh and
                  Yeonhong Park and
                  Yoonho Song and
                  Jung{-}Hun Park and
                  Sanghee Lee and
                  Kyoung Park and
                  Jae W. Lee and
                  Deog{-}Kyoon Jeong},
  title        = {A\({}^{\mbox{3}}\): Accelerating Attention Mechanisms in Neural Networks
                  with Approximation},
  journal      = {CoRR},
  volume       = {abs/2002.10941},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.10941},
  eprinttype    = {arXiv},
  eprint       = {2002.10941},
  timestamp    = {Tue, 03 Mar 2020 14:32:13 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-10941.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{EdgeBERT,
author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
title = {EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480095},
doi = {10.1145/3466752.3480095},
abstract = {Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 \texttimes{}, 2.5 \texttimes{}, and 53 \texttimes{} lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {830–844},
numpages = {15},
keywords = {embedded non-volatile memories, latency-aware, natural language processing, software and hardware co-design},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@article{Spatten,
  author       = {Hanrui Wang and
                  Zhekai Zhang and
                  Song Han},
  title        = {SpAtten: Efficient Sparse Attention Architecture with Cascade Token
                  and Head Pruning},
  journal      = {CoRR},
  volume       = {abs/2012.09852},
  year         = {2020},
  url          = {https://arxiv.org/abs/2012.09852},
  eprinttype    = {arXiv},
  eprint       = {2012.09852},
  timestamp    = {Sun, 03 Jan 2021 18:46:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2012-09852.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{asadi,
  title={ASADI: Accelerating Sparse Attention using Diagonal-based In-situ Computing},
  author={Li, Huize and Li, Zhaoying and Bai, Zhenyu and Mitra, Tulika},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2024},
  organization={IEEE}
}

@article{chakraborty2020resistive,
  title={Resistive crossbars as approximate hardware building blocks for machine learning: Opportunities and challenges},
  author={Chakraborty, Indranil and Ali, Mustafa and Ankit, Aayush and Jain, Shubham and Roy, Sourjya and Sridharan, Shrihari and Agrawal, Amogh and Raghunathan, Anand and Roy, Kaushik},
  journal={Proceedings of the IEEE},
  volume={108},
  number={12},
  pages={2276--2310},
  year={2020},
  publisher={IEEE}
}

@inproceedings{ham2021elsa,
  title={ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks},
  author={Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={692--705},
  year={2021},
  organization={IEEE}
}

@inproceedings{imact,
  title={In-memory computing based accelerator for transformer networks for long sequences},
  author={Laguna, Ann Franchesca and Kazemi, Arman and Niemier, Michael and Hu, X Sharon},
  booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={1839--1844},
  year={2021},
  organization={IEEE}
}

@inproceedings{kang2021framework,
  title={A framework for accelerating transformer-based language model on ReRAM-based architecture},
  author={Kang, Myeonggu and Shin, Hyein and Kim, Lee-Sup},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={41},
  number={9},
  pages={3026--3039},
  year={2021},
  publisher={IEEE}
}

@inproceedings{lu2021sanger,
  title={Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture},
  author={Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={977--991},
  year={2021}
}

@inproceedings{okazaki2022analog,
  title={Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers},
  author={Okazaki, Atsuya and Narayanan, Pritish and Ambrogio, Stefano and Hosokawa, Kohji and Tsai, Hsinyu and Nomura, Akiyo and Yasuda, Takeo and Mackin, Charles and Friz, Alexander and Ishii, Masatoshi and others},
  booktitle={2022 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={3319--3323},
  year={2022},
  organization={IEEE}
}

@inproceedings{retransformer,
  title={ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration},
  author={Yang, Xiaoxuan and Yan, Bonan and Li, Hai and Chen, Yiran},
  booktitle={Proceedings of the 39th International Conference on Computer-Aided Design},
  pages={1--9},
  year={2020}
}

@inproceedings{softermax,
author = {Stevens, Jacob R. and Venkatesan, Rangharajan and Dai, Steve and Khailany, Brucek and Raghunathan, Anand},
title = {Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DAC18074.2021.9586134},
doi = {10.1109/DAC18074.2021.9586134},
abstract = {Transformers have transformed the field of natural language processing. Their superior performance is largely attributed to the use of stacked “self-attention” layers, each of which consists of matrix multiplies as well as softmax operations. As a result, unlike other neural networks, the softmax operation accounts for a significant fraction of the total run-time of Transformers. To address this, we propose Softermax, a hardware-friendly softmax design. Softermax consists of base replacement, low-precision softmax computations, and an online normalization calculation. We show Softermax results in 2.35x the energy efficiency at 0.90x the size of a comparable baseline, with negligible impact on network accuracy.},
booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},
pages = {469–474},
numpages = {6},
location = {San Francisco, CA, USA}
}

@inproceedings{transpim,
  title={Transpim: A memory-based acceleration via software-hardware co-design for transformer},
  author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1071--1085},
  year={2022},
  organization={IEEE}
}

@article{xformer,
  title={X-former: In-memory acceleration of transformers},
  author={Sridharan, Shrihari and Stevens, Jacob R and Roy, Kaushik and Raghunathan, Anand},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={31},
  number={8},
  pages={1223--1233},
  year={2023},
  publisher={IEEE}
}

