\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{soul}
\usepackage{color}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{threeparttable}

\begin{document}

\title{HASTILY: \underline{Ha}rdware-\underline{S}oftware Co-Design for Accelerating \underline{T}ransformer \underline{I}nference \underline{L}everaging Compute-in-Memor\underline{y}}

% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

\author{Dong Eun Kim, Tanvi Sharma, and Kaushik Roy,~\IEEEmembership{Member,~IEEE,}
% \thanks{This work has been submitted to the IEEE for possible pulication.}
\thanks{Dong Eun Kim, Tanvi Sharma, and Kaushik Roy are with the Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, 47907.}
}


% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Transformers have become the backbone of neural network architecture for most machine learning applications. Their widespread use has resulted in multiple efforts on accelerating attention, the basic building block of transformers. This paper tackles the challenges associated with accelerating attention through a hardware-software co-design approach while leveraging compute-in-memory (CIM) architecture. In particular, our energy- and area-efficient CIM based accelerator, named HASTILY, aims to accelerate softmax computation, an integral operation in attention, and minimize their high on-chip memory requirements that grows quadratically with input sequence length. Our architecture consists of novel CIM units called unified compute and lookup modules (UCLMs) that integrate both lookup and multiply-accumulate functionality within the same SRAM array, incurring minimal area overhead over standard CIM arrays. Designed in TSMC 65nm, UCLMs can be used to concurrently perform exponential and matrix-vector multiplication operations. Complementing the proposed architecture, HASTILY features a fine-grained pipelining strategy for scheduling both attention and feed-forward layers, to reduce the quadratic dependence on sequence length to linear dependence. 

Further, for fast softmax computation which involves computing the maxima and sum of exponential values, such operations are parallelized across multiple cores using reduce and gather strategy. We evaluate our proposed architecture using a compiler tailored towards attention computation and a standard cycle-level CIM simulator. Our evaluation shows end-to-end throughput ($TOPS$) improvement of $4.4\times$ $- 9.8\times$ and $1.7\times$ $- 5.9\times$ over Nvidia A40 GPU and baseline CIM hardware, respectively, for BERT models with INT-8 precision. Additionally, it shows gains of $16\times$ $-36\times$ in energy-efficiency ($TOPS/W$) over A40 GPU and similar energy-efficiency as baseline CIM hardware. Code base of our evaluation setup will be open-sourced at github.
\end{abstract}

\begin{IEEEkeywords}
Compute-in-memory, accelerator, machine learning, transformers, SRAM, softmax, hardware-software co-design, encoder models.
\end{IEEEkeywords}



\begin{figure}[ht]

    \centering
    % \vspace{5mm}
    \includegraphics[width=0.9\columnwidth, trim={0 4mm 0 0 0}]{figures/intro1.png}
    % \vspace{-2mm}
    \caption{(a) Illustration of a transformer block showing multi-head attention and feed-forward blocks. \cite{AttentionIsAllYouNeed} (b) Runtime breakdown for different sequence lengths and embedding size of 512 as measured on Ampere (A40) GPU \cite{Ampere}. (c) Dependence of softmax on attention computation, limiting the overall throughput of CIM based hardware.}
    \label{fig:intro1}
\end{figure}


\section{Introduction}
\IEEEPARstart{T}{he} advent of transformers has fundamentally transformed deep neural networks (DNNs), enabling significant advancements in natural language processing (NLP) \cite{conneau2019cross, dai2019transformer, AttentionIsAllYouNeed} and computer vision \cite{swinTransformer, dosovitskiy2021imageworth16x16words, bello2020attentionaugmentedconvolutionalnetworks, chen2020generative, wu2021cvt, sun2020transtrack}. Behind the success and widespread adoption of transformers is the \textit{scaled dot-product attention} \cite{AttentionIsAllYouNeed}, which captures relationships and dependencies within long input sequences effectively. As a result, a plethora of recent research has focused on accelerating attention to meet their growing applications demands \cite{A3, Spatten, EdgeBERT, lu2021sanger, softermax, dao2022flashattention, kao2023flat, kwon2023efficient, zadeh2020gobo, softmax1_hyft, softmax2_ITA, softmax3}.

Attention computation consists of memory-intensive matrix-multiplication (MatMul) - softmax - MatMul operation as demonstrated in Fig. \ref{fig:intro1}(a). Specifically, it involves computing \textit{logits} ($QK^T$) from the MatMul between reference \textit{query} ($Q$) and context \textit{keys} ($K$). The logits are then scaled ($\sqrt{d_k}$)  and passed through softmax operation, and finally used in a MatMul with the \textit{value} ($V$) to produce the attention score, \textit{softmax($QK^T/\sqrt{d_k}$)$V$}. 
In the runtime distribution of a transformer block shown in Fig. \ref{fig:intro1}(b), MatMul and softmax operations show comparable execution times for lower sequence length ($l <= 1024$). 
However, as the sequence length ($l$) increases, the softmax runtime grows disproportionately, highlighting the need for its optimization along with MatMul. 
In other words, the presence of softmax operation at the heart of attention makes accelerating transformers distinct from other compute-intensive neural networks such as convolutional and batched fully connected layers. 
Additionally, attention computation requires storing large intermediate matrices ($QK^T$) whose size increases quadratically with the sequence length ($l^2$) \cite{kao2023flat, dai-etal-2019-transformer, choromanski2022rethinkingattentionperformers}. The intermediate matrix size imposes high on-chip memory requirements on the hardware during runtime. Therefore, accelerating transformers needs to solve two main challenges: 1)  the intricate dependency of attention on softmax, and 2) the large memory footprint of intermediate matrix.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/intro2-new.png}
    % \vspace{-2mm}
    \caption{(a) Comparison of our work, HASTILY, with other works \cite{retransformer, xformer, softmax1_hyft, softmax2_ITA} addressing the challenges associated with softmax computation. (b) Distinguishing the fine-grained pipelining technique proposed in this work compared to a prior work, ReTransformer \cite{retransformer}.}
    \label{fig:intro2}
% \vspace{-5mm}
\end{figure}


% Therefore, it becomes crucial to come up with techniques to efficiently accelerate transformer layers in the system. In this work, we particularly focus on accelerating transformer inference through hardware-software co-design approach.

 % prior works
 Owing to the above-mentioned compute complexities and high on-chip memory requirements of transformer, multiple studies have focused on algorithm-hardware co-design and dataflow optimization techniques. While co-design approaches utilize approximate softmax or pruning to reduce the compute complexity of attention, dataflow optimizations use tiling and tensor-fusion to reduce the on-chip memory footprint \cite{A3, Spatten, EdgeBERT, lu2021sanger, softermax, dao2022flashattention, kao2023flat, kwon2023efficient, zadeh2020gobo}. On the other hand, the limitations in traditional computing technology such as the slower growth rate of memory bandwidth and capacity, known as the ``memory wall", demands for new computing paradigms \cite{wulf1995hitting, gholami2024ai}. In the past, compute-in-memory (CIM) has shown great promise in accelerating convolutional and fully-connected layers by leveraging their weight stationary nature, as their feature maps are known before runtime \cite{verma2019memory, Isaac, aliCIM}. However, attention includes dynamic weights (logit/attention) which leads to an extra data loading cost in terms of latency and energy, limiting the benefits of CIM. Therefore, prior CIM works on transformer acceleration focused on solving such data loading overhead using dataflow optimizations or on accelerating softmax. For instance, ReTransformer \cite{retransformer} proposed a hybrid softmax unit using high-density resistive memory (ReRAM) and decomposes the dot-product matrix into two sub-matrices to eliminate the need to write the whole logit matrix before performing MatMul in CIM hardware. Other CIM works focused on content addressable memories for softmax, ReRAM-SRAM hybrid architecture, and processing in off-chip memory to facilitate efficient data communication during transformer inference \cite{imact, xformer, transpim}. Some research studies have also focused only on designing an efficient softmax unit for faster attention computation \cite{softmax1_hyft, softmax2_ITA, softmax3}, but that comes at an increased area cost. Moreover, the mentioned hardware accelerators face limitations in end-to-end applications due to the need of storing large intermediate matrices for softmax computation.
 
This work fills the gap in CIM based hardware for transformers by proposing a novel hardware-software co-design approach, HASTILY. Our architecture efficiently accelerates the whole transformer block, including softmax and matrix-multiplication, while minimizing area overhead, as shown in the comparison with other works (Fig. \ref{fig:intro2}). In addition, while our software techniques are devised to complement the proposed SRAM-based analog CIM hardware, we believe our compiler can be easily utilized to evaluate other CIM proposals for transformers.
 % The fundamental benefit of CIM hardware comes from its optimized circuitry in addition to its weight stationary nature. By writing the weight matrix once and reusing it multiple times, the CIM hardware performs matrix-vector multiplications with high energy-efficiency. However, the mentioned challenges for transformer inference also affect CIM based hardware and reduces its core benefit of reducing data movements. Writing of intermediate matrix increases both the latency and write energy. need to fit these large intermediate results on-chip and also write them to the CIM array before compute, reducing the benefits of CIM hardware. While the dependence on softmax computation limits the overall throughput coming from parallel CIM arrays used for matrix multiplication (Fig.\ref{fig:intro}(b)).

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\columnwidth]{figures/fig2.png}
%     \caption{Finer-granularity pipelining strategy used in HASTILY as compared to other CIM based hardware.}
%     \label{fig:compare}
% \end{figure}

% \begin{table}[t]
% \caption{Comparison of HASTILY with other Softmax Accelerators}
% \label{tab:compare}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lccr}
% \toprule
% Work & Technology & SoftmaxAcceleration & Dataflow Optimization \\
% \midrule
% ReTransformer    & ReRAM      & Hybrid architecture & Matrix Decomposition + Fine-grained pipelining\\
% X-former         & CMOS+ReRAM & N/A                   & Sequence blocking dataflow\\
% Hyft             & CMOS       & Hybrid Numeric format      & No \\
% ITA              & CMOS       & Wide dot-product INT units & Weight Stationary dataflow \\
% HASTILY(this work)& CMOS      & Dual-functionality CIM Core & Fine-grained pipelining + ReduceGather maxima\\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

 



 % This work
 First, we propose and design an area- and energy-efficient unified compute and lookup module (UCLMs) that constitute the core of our spatial hardware architecture. UCLMs consist of 8T-SRAM arrays for performing both compute and lookup operations within the same memory, whose details are provided in Section \ref{sec:core}.
 % Essentially, the same memory array acts as read-only memory during lookup operations and random access memory during compute operations.
 The additional functionality of lookup is achieved by just adding an extra source line to the memory array, resulting in no area overhead as observed from the implementation done using TSMC 65nm technology. The dual-functionality of the proposed SRAM array helps in concurrent execution of matrix-vector multiplications and exponential operations across multiple cores. Other than exponent operations, softmax computation also involves finding the maxima and reduction operations to calculate a numerically stable value, as explained in background Section \ref{sec:preliminaries}. To reduce the latency of such calculations and keep hardware changes minimal,
 % and utilize the parallel execution of exponents across multiple cores in hardware
  % we propose to implement them in software. In other words,
  computations of the local maxima and partial sums of exponential values are parallelized across multiple cores in software and then gathered to a single core in a binary tree fashion.
 

 Second, we devise a fine-grained pipelining strategy to tackle the need for writing large intermediate matrix in memory during attention. As illustrated in Fig. \ref{fig:intro2}(b), our strategy divides the attention and feed-forward computation into fine-grained pipeline where one vector of input matrix is processed in a single stage of the pipeline. The underlying concepts behind the success of this approach are the concurrent execution of logit calculation and softmax in hardware and maintaining all data dependencies between different MatMul operations in software. % using transpose operations. 
 By keeping the pipeline fed by a vector instead of a matrix during runtime, as explained in Section \ref{sec:compiler}, we are able to reduce the space complexity dependence on sequence length from quadratic to linear.
 % The concurrent operation of logit calculation and softmax in hardware assists in avoiding stalls during such pipelined scheduling of the transformer blocks.
 
In summary, the contributions of our work can be listed as follows:
\begin{itemize}
    \item We propose a novel softmax-friendly high-throughput compute-in-memory architecture with minimal area overhead for accelerating attention. It consists of 8T-SRAM arrays (TSMC65nm) acting as unified compute and lookup modules (UCLMs). 
    \item The proposed architecture solves the softmax bottleneck by enabling concurrent matrix-vector multiplication and exponential operations in hardware. Further, the maxima and reduction operations are accelerated in software to keep the time complexity of the order of $\mathcal{O}(log n)$.
    \item We further reduce the on-chip memory requirement for attention calculation by incorporating a finer-granularity pipelining strategy, processing transformer computation in smaller blocks while maintaining all data dependencies. 
    \item The evaluations, done using a compiler tailored for transformer computations and a standard CIM cycle-accurate simulator, demonstrate throughput improvements of up to 9.8$\times$ over Ampere GPU and 5.9$\times$ over the baseline CIM hardware for BERT models.  %in both throughput and energy-efficiency compared to baseline CIM hardware and the Ampere GPU.
\end{itemize}

% Our evaluation shows end-to-end throughput ($TOPS$) improvement of $4.4\times-9.8\times$ and $1.7\times-5.9\times$ over Nvidia A40 GPU and baseline CIM hardware, respectively, for BERT models with INT-8 precision. Additionally, it shows gains of $16\times-36\times$ in energy-efficiency ($TOPS/W$) over A40 GPU and similar energy-efficiency as baseline CIM hardware.


% \vspace{-4mm}
% organization of different sections.
% The paper is organized as follows: Section \ref{sec:preliminaries} provides an overview of transformer operations and CIM based hardware. The details of the proposed work are described in Section \ref{sec:core} and \ref{sec:compiler}. Further, Section \ref{sec:methodology} details the evaluation methodology for the various experiments, followed by the results and discussion in section \ref{sec:results} and \ref{sec:discussion}, respectively. The final section \ref{sec:conclusion} concludes the paper.

\begin{table*}[t!]
    \centering
    \caption{Comparison between different ways of implementing exponential function}
    \label{tab:exponent}
    % \vspace{-3mm}
    \begin{tabular}{|c|c|c|c|c|} \hline 
         \textbf{Characteristics}&  \textbf{Software}& \textbf{ LUT on-chip}&  \textbf{LUT loaded from off-chip}& \textbf{HASTILY}\\ \hline 
         Area overhead&  No&  Yes&  No& No\\ \hline 
         Power consumption&  Normal&  Low&  High& Low\\ \hline 
         Compute Latency&  High&  Low&  High (only loading)& Low\\ \hline
    \end{tabular}

\end{table*}

The details of the evaluation methodology are given in Section \ref{sec:methodology}, followed by results and discussion in Section \ref{sec:results} and Section \ref{sec:discussion} respectively. The final Section \ref{sec:conclusion} concludes the paper.



% \section{Related Works}
% \label{sec:related}
% % Previous works
% \textbf{Compute in Memory based Hardware}: There have been few studies proposing CIM based hardware for transformer acceleration. One of the first works, ReTransformer \cite{retransformer}, addressed the challenge of writing intermediate result by decomposing the matrix into sub-matrix computations. For instance, the $QK^T$ calculation was restructured as $R=QW_{K}^T$ and $RX^T$. Although ReTransformer offered a novel solution for matrix multiplication, its hybrid softmax hardware incurs significant latency, limiting overall architecture performance. 
% % For example, their Re-RAM based compare and select logic takes 13 and 8 cycles to find the $x_{max}$, which is later used to calculate exponential and logarithm using look up table circuits.
% The authors in \cite{imact} proposed a combination of content addressable memories and crossbars for transformer acceleration and implemented the multi-head self-attention mechanism. However, most studies employing CIM hardware primarily focus on emerging non-volatile memories like ReRAM \cite{kang2021framework, asadi, okazaki2022analog}. While ReRAMs provide high-density crossbars, they have limited endurance and high write latency. To mitigate the issues associated with ReRAMs, X-former \cite{xformer} utilized a hybrid architecture containing both ReRAMs and SRAMs. The SRAMs are used for storing the intermediate results to reduce the write energy and latency. Nonetheless, fabrication technology limitations and the low endurance of ReRAMs pose challenges for their practical implementation in hardware \cite{chakraborty2020resistive}. Additionally, studies such as \cite{transpim} have explored the use of CIM or processing in high-bandwidth memory (HBM) to facilitate efficient data communication during transformer inference.  

% \textbf{Algorithm-Hardware Co-Design}:
% A3\cite{A3} was one of the first works to accelerate attention, utilizing content-based search for accelerating attention and implementing online softmax where the maximum value is updated on the fly while computing exponents. Many studies have focused on modifying the attention algorithm using pruning and quantization techniques, and subsequently building a specialized hardware for it. For instance, Spatten \cite{Spatten} employed token pruning, head pruning and quantization techniques, and designed a top-k engine to support their pruning algorithm. Elsa \cite{ham2021elsa} proposed an approximate softmax calculation scheme by filtering out redundant relations along with a specialized hardware. EdgeBERT \cite{EdgeBERT} introduced an entropy-based early exit prediction for dynamic voltage-frequency scaling, combined with selective network pruning and floating-point quantization to optimize performance. Sanger \cite{lu2021sanger} developed a new software to introduce dynamic sparsity pattern and a reconfigurable systolic array design that can exploit such patterns. Softermax \cite{softermax} proposed a specialized hardware for accelerating softmax by using power of two instead of natural exponent, and designing a low cost, low precision accelerator tailored for it. %include SOLE

% \textbf{Software/System Techniques}:
% Multiple works focus on different aspects of transformer acceleration in a system. Most of them focus on addressing the auto-regressive nature of LLMs. flash attention - pioneering work to show longer sequence length computation in GPUs, idea is to tile the large matrix into smaller blocks and using online softmax for accelerating attention, have been followed up with flash-attention 2 and 3 leveraging newer GPU capabilities including asynchronous communication and quantization, pagedattention --  virtualized the allocation of large matrices involved in attention by introducing the granularity of page, thus reducing memory fragmentation and requirements. Also developed an llm serving platform named vllm implementing pagedattention, orca -- proposed iteration-level scheduling to execute requests at the granularity of iteration and suggested selective batching to serve a distributed system. deepspeed -- library including various optimizations such as ZeRO (zero Redundancy Optimizer) to reduce memory redundancies while training LLMs on hundreds of GPUs.

% Our work differs from the mentioned related works in three ways. First, we focus on reducing the softmax bottleneck using an area-efficient compute-cum-lookup module, based on an implementation designed and synthesized in TSMC 65nm. Previous works which focus on algorithm modifications, dataflow, pruning or quantization strategies are complementary to our work. Second, we propose a specialized \textit{programmable} hardware accelerator for transformer inference, paving the way for CIM based hardware exploration with transformers. Third, we employ an only CMOS based technology that overcomes the high write energy/latency issues present in ReRAM based CIM accelerators.


\section{Preliminaries and Challenges}
\label{sec:preliminaries}

This section briefly discusses the background concepts related to transformer models and CIM based hardware. We also highlight the unique challenges associated with accelerating transformers and current limitations of CIM based hardware accelerators.

\subsection{Transformer models}

% Transformer models are currently the most prevalent deep learning models for natural language processing (NLP) tasks \cite{AttentionIsAllYouNeed}. %add more citations here
% The first layer of transformers process input text by converting it into smaller units called \textit{tokens}, each represented by a unique integer based on the vocabulary size. Generally, one token can be thought of as 4 characters in English \cite{wordtoken}. The tokens are then mapped to a high-dimensional space through an embedding layer. 
A transformer network comprises multiple blocks and a final classifier, where each block can further constitute of an encoder block, decoder block, or a combination thereof. Encoder-like architectures, such as the bidirectional encoder representation (BERT) models, process the entire input sequence in one pass \cite{devlin2018bert}. As the name suggests, BERT utilizes a bi-directional attention mechanism where each token can attend to all other tokens in the sequence. Such models are used in applications like classification \cite{sun2020finetuneberttextclassification, bertTextClassification} and search engines \cite{kocián2021siamesebertbasedmodelweb}. Conversely, decoder-like architectures, such as the generative pretrained transformer (GPT) models, process input sequentially, with each token attending only to previous tokens through triangular masking during attention calculation \cite{yenduri2023gpt}. Tasks such as summarization \cite{goyal2023newssummarizationevaluationera, automatedNewsSummarization} and text prediction \cite{thoppilan2022lamdalanguagemodelsdialog, gpt-j, zhang2022optopenpretrainedtransformer} are common applications for GPT models. Recently, encoder-like transformer models have also been adopted for vision tasks, as demonstrated by the latest vision transformer models \cite{you2023vitcod, zhu2023vlgptgenerativepretrainedtransformer}. The key operations in transformers can be categorized as matrix-multiplication (or dot-product), softmax and other element-wise operations, whose details are mentioned below.
% They work in two phases - pre-fill phase and generation phase. In pre-fill phase, the whole input is processed at a time to generate one output token. In generation phase, the last output token is used as input query to generate the next token. During generation phases, the key and value matrix keeps increasing as the number of generations increase. A common example of decoder-like transformer models is . GPTs use triangular masking during attention to only attend to the words before the word in context. On the other hand, BERTs can choose to mask as per their wish.

\subsubsection{Matrix Matrix Multiplication Operations}
Matrix multiplication (\verb|matmul|) operations in a transformer layer can be categorized into dynamic and static types, based on whether the weight matrix is calculated during runtime or known beforehand. For that reason, dynamic MatMul is also commonly referred as activation - activation MatMul operation. Static MatMuls include the $Q/K/V$ projection, $Out$ projection, and $Feedforward$ layers, with a compute complexity of $O(ld^2)$ as shown in Fig. \ref{fig:intro1}(a). Dynamic MatMuls involve the $QK^T$ and $SV$ computations with a time complexity of $O(l^2d)$. Here, $l$ denotes input sequence length and $d$ is the size of embedding dimension. Since matrices are generated during runtime in dynamic MatMuls, and CIM hardware benefits heavily from its weight stationary nature, such MatMuls limit the benefits of CIM unless properly addressed. Also, the \textit{quadratically increasing size of intermediate matrix} increases the on-chip memory capacity demands and data movement costs during transformer inference.

\subsubsection{Softmax Function}
Softmax exponentiates the values of scaled logit matrix $L=QK^T/\sqrt{d_k}$ for each input sequence vector and facilitates in calculation of final attention score. Attention score decides the probability of predicting the next token in the sequence. Generally, softmax is calculated as follows to avoid overflow conditions:
\begin{equation}
    \text{softmax}(M_{i,j}) = \frac{\exp(M_{i,j}-max_j)}{\sum_{j} \exp(M_{i,j}-max_j)}
    \label{eq:softmax}
\end{equation}
Therefore, calculating the softmax includes multiple element-wise or vectorized operations:
\begin{itemize}
    \item Maxima: Calculating the maxima among all elements in a vector
    \item Subtraction: Elementwise subtraction of calculated maximum from the vector
    \item Exponent: Finding exponent values for each element in the vector
    \item Reduction: Sum of all exponents within a vector to calculate denominator
    \item Division: Elementwise division (or reciprocal multiplication) by the sum
\end{itemize} 
% Calculation of softmax faces two challenges - 1. Dealing with underflow/overflow and 2. Summing over long sequence lengths. To avoid the underflow/overflow of exponential terms, generally a logsumexp approach is used to calculate softmax. Here, instead of directly calculating the exponential of the element, first the maximum value across the vector is found. It is then used for subtracting from the original value to calculate softmax as follows:
% exp(M(i,j) - max - log(sum(exp (Mij - max))
% This helps in keeping the range of inputs to the exponential small. However, it adds extra cycles for calculating the maximum value along the sequence length dimension. For every softmax operation, it requires access to all the sequence length-wide vector. Recently, flashattention solved the second challenge by dividing the computation into smaller tiles. Each tile calculates the max for itself, and eventually all of them are added together to find the final softmax as shown below:


Other than the exponent, the element-wise operations are simple to perform using standard arithmetic and logic units (ALUs). The exponential function can be calculated as an infinite sum using the MacLaurin series expansion: 
\begin{equation}
    e^x=1+x+ x^2/2!+x^3/3!+\cdots + x^n/n! + \cdots.
\end{equation}
%Calculating the sum of infinite terms is difficult and time consuming. Therefore, 
Since calculating the sum of infinite terms is cumbersome, an approximate exponential value is calculated by dropping the higher order terms and taking only the sum of the first few terms. 
% The approximation approach works well when $x$ is small.
% However, as $x$ increases, significantly more terms are required to maintain accuracy, leading to longer computation times. 
On one hand, a fully software implementation of softmax offers a low-cost and high-latency solution where the exponential values are calculated during runtime. On the other hand, for faster exponential computation, look-up tables (LUTs) can be stored on-chip to directly access the pre-calculated values, costing extra area. It is also possible to load the LUTs on chip during runtime for limited area constraint hardware as summarized in Table \ref{tab:exponent}.
% General-purpose hardware stores these LUTs in the vector function units or multi-function units of every core. 
However, \textit{instantiating large lookup tables leads to large power overheads}, as also mentioned by \cite{softermax}. 
% talk about other works
% Softermax got rid of the max calculation step and used base 2 look up tables for calculation. There are other works as well, doing Idk what?? \hl{the limitations of current softmax approaches}



\subsubsection{Other Element-wise Operations and Functions}
Each transformer block adds the output from the previous block to the multi-head attention and feedforward layer output. Additionally, non-linear functions such as GELU (Gaussian Error Linear Unit) and LayerNorm (Layer Normalization) are employed in transformer models. GELU provides a smoother activation function compared to ReLU, maintaining a non-zero gradient at $x=0$. LayerNorm normalizes inputs using their mean and variance before adding the previous layer's output. Although such functions are essential, the runtime of GELU and LayerNorm is not as significant compared to matrix multiplications and softmax operations because they operate on comparatively smaller tensors, as shown in Fig. \ref{fig:intro1}(b).


\subsection{Compute in memory (CIM) based Hardware}\label{sec:cim_hardware}
% TODO-Final: Change the background to reflect how compute in memory works in principle using ReRAM/SRAM. After explaining its functionality, write about spatial architectures. Then, briefly mention about the challenges related to intermediate matrix sizde 


Compute in memory (CIM) enables matrix-vector multiplication operation inside a memory array by modifying the peripherals. The computational nature of CIM can be analog or digital depending on the associated peripherals and how multiply-and-accumulation (MAC) is performed. Analog CIM typically activates multiple wordlines in a memory array and accumulates current from all rows in the bit line (BL) to produce analog MAC output. The activation of wordlines is decided based on the input vector by incorporating Digital-to-Analog Converters (DACs) in the memory row decoders. Additionally, the BL analog output is converted to digital using Analog-to-Digital Converters (ADCs) and then shifted-and-added (S\&A) with partial sums from other columns to compute the final matrix-vector multiplication output \cite{analog1cim, twin8t}. Digital CIM typically employs bit-wise arithmetic logic such as a NAND gate to perform bit-wise multiplication of inputs, and then combines the partial results to compute the final output \cite{computesram, tmsc2023digital}.

In terms of memory technology, analog CIM can be implemented using either SRAM (static random access memory) or emerging memories such as resistive RAMs (ReRAMs) or magnetoresistive RAMs (MRAMs). While emerging memories offer a high-density memory solution, they typically suffer from issues such as resistance drift, write endurance and/or low on-off distinguishability \cite{chakraborty2020resistive}. In this work, we focus on SRAM based CIM hardware because of their higher feasibility in terms of implementation.

Analog CIM hardware offers a highly energy-efficient solution for accelerating neural networks due to their efficient execution of matrix-vector multiplication in analog form. The initial works on analog CIM based accelerators focused on convolutional and fully connected networks \cite{prime, Isaac, puma, song2017pipelayer}. Such spatial accelerator architectures often feature a weight-stationary dataflow and low-precision computations such as 8-bit or 16-bit datawidth to achieve higher energy efficiency. However, when analog CIM is used for transformers, the presence of dynamic MatMuls reduces their energy-efficiency because activation-activation MatMuls require writing the matrix into memory before performing \verb|matmul|, thereby reducing their weight stationary benefits. Note that SRAM based CIM will have reduced data loading overhead during dynamic MatMul operations due to their lower write energy/latency compared to emerging memory technologies \cite{agrawal2021magnetoresistive}.

To maximize the throughput, CIM based spatial accelerators constitute of a hierarchical architecture as proposed in a previous work, PUMA \cite{puma}. At the chip level, the architecture consists of multiple tiles and a Global Buffer (GB), as illustrated in Fig. \ref{fig:main_arch_cim}(a).
% , which serves as a cache bridging off-chip memory and the chip  
% Input data is loaded from off-chip memory, and resulting outputs are returned to it. Communication between tiles, and between tile and the GB occurs over the NoC. 
GB serves as a cache bridging off-chip memory and the tiles, while tiles communicate with each other via send/receive instructions.
Each tile consists of multiple cores, shared memory, and an instruction memory. 
Cores perform computations and access data stored in shared memory. \textit{The size of shared memory could limit the throughput benefits of CIM cores if it cannot accommodate the intermediate matrix}, resulting in off-chip accesses and increasing compute latency.
% Tiles fetch tile-specific instructions from their instruction memory and execute these instructions to move data between shared memory and other tiles or off-chip memory. 
% Tile instructions include operations such as 'send', 'receive', 'load', and 'store'. The 'send' and 'receive' instructions operate in pairs, ensuring data sent by one tile is received and stored in the destination tile's shared memory.

Typical CIM core constitutes of multiple Matrix-Vector Multiplication Units (MVMUs), a register file, a Vector Function Unit (VFU), and instruction memory. The core executes instructions such as loading and storing data from/to shared memory and performing matrix-vector multiplications (MVMs) within the MVMUs. MVMs operate in SIMT (Single Instruction Multiple Thread) mode, where multiple MVMUs generate output vectors concurrently. The VFU handles arithmetic operations, including element-wise addition, multiplication, division, non-linear functions (like ReLU), normalization, and transcendental functions (like exponential). However, \textit{VFUs cannot efficiently handle the high frequency and complexity of softmax operations in transformers}, limiting the overall throughput of CIM based accelerators for transformer models. Additionally, incorporating a large VFU could potentially reduce the softmax computation time, but adds significant area overhead.




\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth, trim={0 6mm 0 0 0}]{figures/core-arch-subfig.png}
    % \vspace{-2mm}
    \caption{(a) The hierarchical spatial architecture of the proposed CIM accelerator, (b) hardware architecture of each core in HASTILY, (c) micro-architecture details of each UCLM consisting of multiple SRAM arrays, (d) physical layout of each dual-functionality 8T-SRAM array implemented in TSMC65nm and (e) depiction of the two operations, compute on the left and lookup on the right, in UCLM and correspondingly in the SRAM.}
    \label{fig:main_arch_cim}

\end{figure*}

\section{Proposed CIM Core Architecture}
\label{sec:core}


The hardware architecture of HASTILY constitutes of a spatial accelerator consisting of an array of tiles, each containing multiple cores and shared memory as shown in Fig. \ref{fig:main_arch_cim}(a). 
Each core features multiple unified compute and lookup modules (UCLMs) for high-throughput execution of matrix-multiplication (\verb|matmul|) and exponent (\verb|exp|) operations. Fig. \ref{fig:main_arch_cim}(b) illustrates that each core also contains a vector functional unit (VFU) for vectorized arithmetic operations, register file (RF) for feeding data to the UCLMs and a lookup (LK) controller. The micro-architecture of UCLM shown in Fig. \ref{fig:main_arch_cim}(c) consists of dual-functionality 8T-SRAM CIM arrays, that can perform MAC operations and memory lookup operations. LK controller helps in switching the dual-functionality SRAM array mode between compute and lookup. 


\subsection{Dual-functionality 8T-SRAM arrays}
 Other than compute, the lookup functionality in 8T-SRAM array is achieved with the addition of a lookup line (LKL) at each row in the array. While the cross-coupled inverter in SRAM cell decides the weight matrix data, the lookup-table data is stored based on the connection of sourceline (SL) with LKL or ground (GND)  \cite{fong2015embedding, lee2012area}. 
 The layout of such dual-functionality array implemented in TSMC 65nm technology is shown in Fig. \ref{fig:main_arch_cim}(d), occupying same area as standard CIM array. Although the extra source line (LKL) does not add any extra area to the array, the peripheral circuits are slightly modified to enable lookup operations along with the crossbar operation. The details of how the different operations are executed in the SRAM array is explained below.

\subsubsection{Read/Write operation} For read operation, first the read bitlines (RBLs) are pre-charged, followed by activating the read wordline (RWL) to select the corresponding row. Then, RBL gets discharged depending on the data stored in the SRAM cell and the final voltage is read at the column peripheral. Similarly, for write operation, corresponding write wordline (WWL) is activated and write bitline (WBL/WBLB) is driven high or low depending on the data that needs to be written to the memory cell. Note, that LKL remains grounded during SRAM read/write operations. 

\subsubsection{Lookup operation} To utilize the SRAM array for exponential table lookup operation, first LKL is set to zero and the corresponding row of SRAM is copied to a temporary buffer by asserting the RWL. Next, the write wordline (WWL) is asserted to write '1' to the SRAM row. After these initial 2 cycles, LKL is set to high and read wordline (RWL) is asserted to read the data from the row through the read bitlines (RBL). 
If SRAM cell’s SL is connected to LKL, RBL outputs '1'; otherwise, if it is connected to GND, RBL gets discharged and outputs '0'.
Since the buffer stores the data originally present in the SRAM row, WWL is asserted again to write the data from the buffer back to the row to preserve the original information. While the data is being written back from the buffer, multiplexer sends the required lookup table value to the output register. In this manner, each SRAM array can be used for lookup operations with no area overhead and with a latency of 4 cycles. 

\subsubsection{Compute operation} During SRAM compute operation, LKL is kept low so that all SLs are grounded. The compute operation begins with pre-charging BL to the supply voltage (VDD) using an active low pulse. Multiple RWLs are activated (set to high) simultaneously, and the BL discharges proportional to the dot-product between the input vector (represented by the voltages on the word lines) and the corresponding weight column vector (stored in the memory cells). The resulting voltage on the BL, now reflecting the computed dot-product, is sampled by the sample-and-hold (S\&H) block. Analog multiplexers are used to select which columns are active for computation at any given time. The ADC converts the analog voltage into a digital signal, which is further processed by the shift and add unit (S\&A). S\&A unit manages different bit significance for inputs and weights, computing the final MAC output. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\textwidth, trim={0 7mm 0 0 0}]{figures/expo-table.png}
    \caption{(a) Each SRAM array stores a 128-entry lookup table for $2^k/128, 1\leq k\leq128$ (b) Illustration of parallel exponent operations execution in multiple UCLMs in a core of HASTILY architecture. }
    % \caption{Working mechanism of the 8T-SRAM based CIM arrays for calculating exponential. }
    \label{fig:element-wise-exp}
    % \vspace{-4mm}
\end{figure*}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth, trim={0 9mm 0 0 0}]{figures/softmaxOpt3.png}
    \caption{Computing softmax by gathering all required vectors in a single core versus parallelizing the compute across multiple cores and gathering them in a tree-like fashion.}
    \label{fig:parallel_max}
    % \vspace{-3mm}
\end{figure}
\subsection{Hardware-Software Co-design for Softmax}

As described in Section \ref{sec:preliminaries}, softmax computation involves various vectorized and/or element-wise operations. We utilize the capability of dual-functionality SRAM arrays to store lookup tables for exponent operation (\verb|exp|) and optimize other operations such as \verb|max| and \verb|add| in software. Here, we explain the details of such optimizations and the proposed micro-architecture for high-throughput execution of softmax operation.

\subsubsection{\textbf{UCLMs} for parallel exponent operations} Softmax requires finding the exponent of each element in a vector among other operations, as depicted in Equation \ref{eq:softmax}. To simplify the computation of exponential function, it can be transformed into the following form \cite{intelHarrison1999TheCO}:
\begin{equation}
    e^x = 2^n \times 2^{d/K} \times e^r  
\label{eq:exp-decomp}
\end{equation}
where $K$ is a chosen integer (representing the number of entries in a pre-computed lookup table), $n=\lfloor x/log(2) \rfloor$, $d=\lfloor (x/log(2)-n)K \rfloor$, and $r=x-n log(2)/K$. 
Therefore, the exponent value can be simply calculated with a combination of a table lookup, bit-shift operations and multiplication by a constant. Such a lookup table will contain values from $2^0$ to $2^{(K-1)/K}$ to directly fetch the value of $2^{d/K}$ and $n$ will decide the bit-shift operation. The residual term $e^r$ is approximated with MacLaurin series expansion, but due to its small range $(1 \leq e^r < 1+1/K)$, it can be approximated as 1 or 1+r with minimal error. In this work, we store a pre-computed table for storing the $2^{i/K}$ entries with $K=128$ in the SRAM array. 
The approximation of $K=128$ yields an error rate below 0.54\% and 0.0015\% with 1 and 1+r, respectively. 


Considering a bit-precision of 16-bit for each value in the 128-entry exponent table, we propose to organize the exponent table in our dual-functionality SRAM array of size $64\times64$ as illustrated in Fig. \ref{fig:element-wise-exp}(a). Since each UCLM contains multiple SRAM arrays, each core is capable of executing multiple exponent operations in parallel. Fig. \ref{fig:element-wise-exp}(b) illustrates how an \verb|exp| operation for a vector containing 64 elements is executed in parallel in the HASTILY core. 
To execute a vectorized \verb|exp| operation, the VFU first computes $n$ and $d$ in parallel, then element-wise table-lookups are performed simultaneously using multiple SRAM arrays. Finally, the VFU applies element-wise bit-shifting to complete the operation.




\subsubsection{Optimizing \textbf{Multi-Core} execution}
Moreover, HASTILY can handle longer sequence lengths by parallelizing the exponent operation over a vector to multiple cores. 
Despite parallel execution of operations such as \verb|exp|, \verb|sub|, and \verb|div| in each core, some operations such as \verb|max| and \verb|reduce| require combining partial results from each core while maintaining consistency. Such sequential gathering of partial maxima or sum could increase the total computation time due to contention in the VFU of a single core. 


To fully parallelize the softmax operation, we utilize the private VFU in each core to calculate the partial sums and local maxima. Then, we gather and operate (reduce or calculate maxima) the partial values from different cores in a tree-like hierarchy. The multi-core execution of softmax as depicted in Fig. \ref{fig:parallel_max} reduces the time complexity of \verb|max| and \verb|reduce| from $N$ to $log(N)$. While some delay is caused by extra load and store operations, it is minimal compared to fully serialized VFU computation in a single core. Additionally, it becomes a suitable choice to compute softmax across multiple cores when the size of $QK^T$ becomes large, and the $K^T$ matrix already requires multiple UCLMs in a single core to be fully mapped.

An exemplary scenario of the operations happening in two cores during softmax computation is provided in Fig. \ref{fig:parallel_max}(b). For instance, core 0 produces partial vectors $A1$ and $A2$ from MVM computations, while core 1 produces partial vectors $A3$ and $A4$. Each core independently computes the maximum value of its respective vectors, yielding $M1$ and $M2$. These maxima are then gathered to determine the overall maximum value ($M$). The overall maximum is distributed back to the cores through store/load operations in shared memory, allowing each core to perform subtraction and \verb|exp| computation simultaneously. Reduction operation is performed in a similar fashion across multiple cores to compute the denominator for the softmax output.




\section{Fine-grained Pipelining}
\label{sec:compiler}


% \subsection{Fine-grained Pipelining}
%introduce the concept
Pipelining is a standard technique that divides the overall computation into smaller stages, allowing multiple stages to process different data in parallel. In the context of transformers, computation is typically divided into blocks of matrix-multiplication, softmax, and other operations, as illustrated in Fig. \ref{fig:intro1}(a). Our focus is to further divide each of these computations into finer-grained vectors, enabling parallel processing of different vectors over various operations. 
% The fine-grained pipelining strategy proves particularly effective due to the fast softmax computation enabled by our hardware.



% Finer-granularity Scheduling, emphasis on reducing the intermediate matrix size and incorporating transpose operations.
\subsection{Pipelining Matrix-Multiplications}
% The key idea is to divide each computational stage into the number of vectors proportional to the sequence length, allowing each stage to run in parallel with other stages on a spatial architecture containing multiple tiles and cores. 
The key idea is to divide each computational stage into the number of rows of the input matrix, allowing each stage to run in parallel with other stages on a spatial architecture containing multiple tiles and cores. 
Matmul can be executed as iterative MVMs, each MVM computes the dot product between a row of the input and the weight matrix, producing a corresponding row of the output. 
For consecutive MatMuls, the output row from one MVM in the first MatMul can be directly used as input for an MVM in the second MatMul. This allows the subsequent MVM in the first MatMul to run concurrently with the second MatMul, enabling fine-grained parallelism. 
This approach enhances both speed and memory efficiency: speed is nearly doubled due to parallelized computation, while memory requirements are reduced, as storing the entire intermediate matrix for the second MatMul is unnecessary, with only temporary storage for the current vector being needed.



\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth, trim={0 4mm 0 0 0}]{figures/main_pipelined_v4.png}
    \caption{(a) Illustration of the dataflow for scheduling encoder layers of a transformer model using the fine-granularity pipelining technique and (b) the corresponding timeline of different stages shown in unique colors.}
    % - marked with unique background color.}
    \label{fig:fine-gran-timeline}
    % \vspace{-4mm}
\end{figure}
% implementation details
\subsection{Pipelining Transformer}
% Q, K, V projection

Fig. \ref{fig:fine-gran-timeline}(a) illustrates the pipelined flow of matrices and intermediate vectors  during transformer inference and (b) shows the pipelined timeline. Note that in the figure, inputs and outputs of MVMs are depicted as transposed to represent the inputs entering the word lines of the SRAM array during MVMs. However, no actual transpose operation is performed on the inputs or outputs, except for $K^T$, which is transposed and used as a weight matrix.  

\subsubsection{Q,K,V Projection}
The input is simultaneously multiplied by the three matrices, $W_Q$, $W_K$, and $W_V$, with each matrix performing $seqLen \times$ MVMs. Once the $Q$, $K$ and $V$ matrices are generated, $K$ and $V$ are written to the next cores' UCLMs as weights, labeled $K_i$ and $V_i$, where $i$ corresponds to the number of attention heads. 
% Before being written to the UCLM arrays, $K_i$ undergoes address transformation \hl{in the cores}, as it is later multiplied with $Q_i$ in the transposed state ($K_i^T$). 
Before being written to the UCLM arrays, $K_i$ is transposed ($K_i^T$) within the allocated cores by modifying the matrix's address mapping. 
%% DongEun - explain how transpose works? 
% \hl{This transformation is handled by a core that loads a chunk of $K_i$ matrix from shared memory, transposes it, and then writes it to the UCLM array.}
Each head then performs scaled dot-product attention in parallel, starting with $Q_iK_i^T$. However, the $Q_i/K_i$ projection block cannot be pipelined with the $Q_iK_i^T$ operation because the complete $K_i$ matrix must be available before the computation of $Q_iK_i^T$. This marks the end of the first stage of the process (highlighted in blue in Fig. \ref{fig:fine-gran-timeline}).


% Pipelining attention 
\subsubsection{Pipelining Attention}
The second stage (highlighted in green) operates in a fully-pipelined manner. Each column of the generated $Q_i^T$ is fed sequentially into the mapped $K_i^T$ within the UCLMs, generating the head output $(Q_iK_i^T)$, one vector at a time. This output vector is then scaled by dividing by $\sqrt{d_k}$ and processed through softmax operations within the same UCLMs using the LT mode. Once the softmax is complete, the resulting vector is immediately passed to the next UCLM to perform multiplication with $V_i$, producing $P_i$. While $P_i=SV_i$ is being computed, the next $Q_iK_i^T$ computation and its associated softmax can proceed in parallel, enabling pipelining. Since all attention heads generate their respective $P_i$ outputs concurrently, each output vector from $P$ can  undergo the linear transformation as soon as it is available, producing the final attention head output $O$ in a pipelined fashion.


% Pipelining within encoder layer
\subsubsection{Pipelining within Encoder Layer}
Next, addition of a vector from $O$ with a vector from the input can be pipelined, and the subsequent layer normalization can also be performed immediately since it operates on the result of the addition. After the Add \& LayeNorm, the two feed-forward networks, $FF1Out$ and $FF2Out$, can also be pipelined with finer granularity. Following this, another Add \& LayerNorm is performed to generate the final output of the encoder layer, again in a pipelined manner. 

% Pipelining inter-encoder layer
\subsubsection{Pipelining between Encoder Layers}
The output generated from Encoder 1 serves as the input for Encoder 2. The first output vector of Encoder 1 can be immediately used as the first input vector of Encoder 2 to compute $Q$, $K$, and $V$. Then, the same process as in Encoder 1 is repeated. This marks the end of the second stage, and the third stage (highlighted in purple) begins. To summarize, everything except for the $Q/K/V$ projection can be pipelined to finer granularity. As a result, an encoder consisting of N layers can perform the entire inference calculation in $(N+1) * seqLen \times MVM~time$. 




% \begin{scriptsize}

\begin{table}[t]
  \centering
  \scriptsize
  % \footnotesize
  \caption{Hardware Characteristics}
  % \vspace{-3mm}
  \label{table:sim-arch}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Component}&  \textbf{Power(mW)}& \textbf{Area(mm$^2$)} &\textbf{Parameter}&\textbf{Specification} \\
    \hline
    \hline
        Global Buffer&  25.35&  0.242&Capacity&256KB\\
    \hline
    Tile& 1.14K$^*$  &  2.57&\# per node&128\\\hline
    Shared Mem&  25.35&  0.242&Capacity&256 KB\\
    \hline
    Memory Bus&  6&  0.18&width&512 bit \\ \hline 
    Core& 140.3 $^{\dag}$& 0.2755& \# per Tile&8 per Tile\\
    \hline
    VFU&  1.7&  0.004&ALU width&64 \\
    \hline
    Register File&  1.14&  0.0078&Capacity&4 KB \\
    \hline 
    UCLM&  MM$^{\ddagger}$: 22.38&  0.0155&\# per Core    &16 \\
        & LT: 0.518&         & \# of CIMArray & 8 \\ 
    \hline
    DAC&  0.0035&  1.67e-7&\# per CIMArray&64 \\
           & &   & Bit-precision& 1 bit\\
    \hline
    ADC&  1.35&  0.0011&\# per CIMArray& 1 \\
           & &   & Bit-precision& 6 bit\\
    \hline
  % \multicolumn{5}{}{$^*$ Includes MVM operation, S\&H, S\&A, ADC and writing to output buffer,}
  %  $^@$ Assuming all the UCLMs in a core are performing MVMs, including the loading and storing operations to the RF.
  \end{tabular}
    \begin{tablenotes}
    \item[1] $^*$Scaled from Core total power and tile memory related instructions.
    \item[2] $^{\dagger}$Includes all UCLMs in a core performing MVM, load/store to/from Register File.
    \item[3] $^{\ddagger}$Includes MVM operation, S\&H, S\&A, ADC and writing to output buffer.
  \end{tablenotes}
  % \vspace{-2mm}
  
\end{table}
% \end{scriptsize}

% \begin{scriptsize}
\begin{table}[t!]
  \centering
  \scriptsize
  % \footnotesize
  \caption{Software Benchmarks}
  % \vspace{-3mm}
  \label{table:bert}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Embedding Size} & \textbf{\#Heads} & \textbf{\#Layers}  & \textbf{Hidden Dimension}\\
    \hline
    BERT-Base& 768& 12& 12& 3072\\
    BERT-Large& 1024& 16& 24& 4096\\
    \hline
  \end{tabular}
  % \vspace{-2mm}
\end{table}
% \end{scriptsize}

\section{Evaluation Methodology}
\label{sec:methodology}
\subsection{Simulator}
We used a cycle-level simulator based on PUMASim \cite{puma}, having a hierarchy of chip, tile, core, and MVMUs in 32nm technology. 
Since PUMA is an RRAM-based CIM architecture, we replaced its MVMUs with our UCLMs. 
In addition, we have considered DRAM and a global buffer associated with the chip for fetching and storing the transformer inputs/weights. Since transformer models produce comparable results with lower precision fixed-point representation, we consider 8 bit precision of inputs and weights for all our evaluations \cite{kim2021bert,dettmers2022llm}. The lookup table resolution is set to 16 bit as described in Section \ref{sec:core}. The energy and area characteristics for the lookup table operation are calculated from an implementation done in TSMC65nm (Fig. \ref{fig:main_arch_cim}(b), and scaled down to 32nm for our simulator. The architecture contains $16$ UCLMs per core, each having $8$ crossbars of size $64 \times 64$ for storing different bits of weights. Other architecture specifications along with the energy and area considerations are described in Table \ref{table:sim-arch}.

\subsection{Compiler}
We develop a compiler supporting BERT inference on a CIM based spatial accelerator. The new compiler supports traditional CIM operations present in a previous CIM accelerator \cite{puma}. In addition, the instruction set architecture (ISA) is extended to support operations such as transpose vector, finding maxima of a vector mapped to multiple cores, tree-sum/reduction, and single instruction multiple exponent.




\subsection{Workloads \& Baseline}
We choose two variants of transformer, BERT-Base and BERT-Large for end-to-end evaluation of throughput and energy-efficiency with the Nvidia Ampere (A40) GPU. The specifications of BERT models are shown in Table \ref{table:bert}.  Further, \verb|bitsandbytes| and \verb|transformers| library from hugging-face are used for GPU evaluations of the BERT models keeping 8-bit precision. The power consumption of GPU is measured using \verb|nvidia-smi| functionality and the idle power is subtracted from the observed power numbers to only compare the dynamic energy. For individual layer evaluations, latency is calculated by implementing multi-head attention in Pytorch and measuring runtime using the \verb|time()| library. 

We also compare the performance and energy consumption of our proposed core with PUMA \cite{puma} core across different sequence lengths and embedding sizes. To ensure fair comparison, we replaced ReRAM-based crossbar arrays in the PUMA core with 8T-SRAM-based crossbar arrays. Otherwise, the hardware architecture of PUMA shares the same configuration as HASTILY except the use of UCLMs in our accelerator. In terms of software, PUMA does not support parallelized \verb|exp| operations and relies only on VFU for softmax computation. 
% Also, since the baseline CIM accelerator (PUMA) does not support parallelized \verb|exp| operations, it retrieves the lookup table for \verb|exp| from off-chip memory during softmax computations.
For all the accelerator evaluations, we assume that the data can be stored on-chip, accessing DRAM only for loading the initial inputs and storing the layer/model output. Under the given hardware configuration, baseline CIM accelerator, PUMA, cannot accommodate a model with sequence length $l$ $> 1024$ without spilling the intermediate matrix to DRAM. Therefore, we evaluate PUMA standalone with $l<1024$ or consider fine-grained pipelining when $l > 1024$ to keep comparisons fair.


\section{Results}
\label{sec:results}

This section compares HASTILY with the Nvidia A40 (Ampere) GPU and PUMA, as baseline CIM accelerator in terms of compute latency and energy consumption. The Nvidia A40, fabricated using Samsung's 8nm process, features a die size of $628.4mm^2$ \cite{Ampere}. In contrast, our UCLM-based accelerator occupies same area as PUMA, which is approximately $330mm^2$ considering 32nm process technology. We evaluate the benefits of HASTILY at different levels -- including only softmax, single encoder layer and end-to-end model performance for different batch sizes.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth, trim={0 6mm 0 0 0}]{figures/softmax-final-sepsq_latency.png}
    \caption{Speed benefits of the proposed UCLM-based hardware and multi-core softmax computation support compared to PUMA. ALU Width refers to the number of ALUs in the VFU.}
    \label{fig:softmax-results-latency}
    % \vspace{-4mm}
\end{figure}




\subsection{Softmax Acceleration}

% chatgpt generated
%% Write reasons for these observations, check the numbers
This sub-section presents the performance of our proposed UCLM in computing softmax for a single vector of sequence length $l$. The results are divided into two parts: one for $l\leq1024$ and another for $l>1024$. 
% As expected, the latency of softmax computation increases with sequence length across all configurations. 

As shown in Fig.\ref{fig:softmax-results-latency}, the proposed UCLM consistently achieves lower latency compared to the PUMA. While multi-core softmax support results in similar latency at smaller $l$, the improvements become more pronounced at longer $l$. For instance, at $l=8192$ and ALU width of $16$, the latency of softmax for the PUMA reaches $22.13\mu s$. The proposed UCLM lowers this to $6 \mu s$, and with multi-core softmax support, it is further reduced to $1.36 \mu s$. At smaller $l$, the parallelism of \verb|exp| from UCLMs within a core is sufficient to compute softmax, and thus no difference is observed with multi-core softmax support.

In addition to multi-core support, the performance of softmax can also be enhanced by increasing the ALU width. A wider ALU width enables greater parallelism for element-wise operations such as \verb|max|, \verb|sub|, \verb|add|, and \verb|div|, which are essential components of the softmax computation. For instance, at a sequence length of $8192$, increasing ALU width from $16$ to $64$ improves the softmax computation performance by 22\% in the multi-core configuration. 

Energy consumption follows similar trends, as shown in Fig.\ref{fig:softmax-results-energy}, with the PUMA consuming the most energy that increases with the sequence length. 
For $l \leq 1024$, softmax computation is extremely cheap for all configurations. 
While the difference between PUMA and our architecture increases for $l>1024$, the ratio remains same at approximately $1.6\times$. The small energy difference between the UCLM only and the multi-core softmax support demonstrates the effectiveness of gathering maxima/partial sums in improving performance.





\subsection{Transformer Layer Acceleration}
The evaluations for a full encoder layer, including both attention and feed-forward blocks, are shown in Fig.\ref{fig:layer-results-latency}, \ref{fig:runtime-distribution}, and \ref{fig:layer-results-energy}. 
These results consider varying sequence length and embedding sizes, with a head size of $64$.
The figure highlights the impact of softmax acceleration and the fine-grained pipelining. 
For $l>1024$, only the results for GPUs and CIM cores with fine-grained pipelining are shown, as the chip is unable to accommodate the intermediate matrices that grow quadratically with the sequence length.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth, trim={0 5mm 0 0 0}]{figures/softmax-final-sepsq_energy.png}
    \caption{Energy consumption of the proposed UCLM-based hardware and multi-core softmax computation support compared to PUMA.}
    \label{fig:softmax-results-energy}
    % \vspace{-6mm}
\end{figure}



While the PUMA outperforms the GPU at $l < 1024$, it becomes slower at 1024. For $l = 2048$ and $4096$, even the PUMA with finer-granularity pipelining is worse than GPU, though it regains superiority at $8192$. The proposed UCLM with multicore softmax helps in significantly reducing the softmax computation and achieves better results than the PUMA. The effectiveness of softmax acceleration grows with increasing sequence length. At a sequence length of $8192$, the proposed architecture outperforms the PUMA with fine-grained pipelining by $3.31\times$.

At $l \leq 1024$, finer-granularity pipelining proves more effective than softmax acceleration alone, and their combination yields even greater performance gains. For instance, with an embedding size of $768$ and $l=1024$, softmax acceleration improves speed by 37\%, finer-granularity pipelining by 96\%, and their combined effect achieves a $4.47\times$ speedup over the PUMA.

Furthermore, regardless of sequence length, the proposed architecture consistently outperforms the GPU. The integration of fine-grained pipelining with our core architecture further enhances the performance, $3\times$ $-13\times$ faster than GPU. The latency with fine-grained pipelining scales linearly with the sequence length as expected from the hypothesis. Note again, smaller sequence lengths greatly benefit with all CIM architectures. At higher sequence lengths, the latency benefits of our architecture, at almost no area cost, are more evident.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth, trim={0 5mm 0 0 0}]{figures/encoder-layer-latency-synth_v2.png}
    \caption{Compute latency of an encoder layer across varying sequence length and embedding sizes, showing the benefits of softmax acceleration and fine-grained pipelining compared to PUMA and GPU.}
    \label{fig:layer-results-latency}
    % \vspace{-4mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth, trim={0 10mm 0 0 0}]{figures/runtimeDistribution_v2.png}
    \caption{Runtime distribution of compute blocks (listed in legend) in an encoder layer for embedding size of 768 and 1024 and sequence length of 512 and 1024. The x-axis shows different hardware including the Ampere GPU, PUMA, and the UCLM with multicore softmax. }
    \label{fig:runtime-distribution}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth, trim={0 6mm 0 0 0}]{figures/encoder-layer-energy-synth_v2.png}
    \caption{Energy consumption of an encoder layer across varying sequence length and embedding sizes, showing the benefits of softmax acceleration and fine-grained pipelining compared to PUMA and GPU.}
    \label{fig:layer-results-energy}
    % \vspace{-4mm}
\end{figure}



Fig. \ref{fig:runtime-distribution} contrasts the runtime distribution of compute blocks, including $Q/K/V$ projection, $QK^T$, softmax, $QK^TV$, multi-head output transformation and fully connected layers, across the proposed UCLM-based architecture, the Ampere GPU, and PUMA. 
Note, the results exclude fine-grained pipelining since runtime cannot be accurately measured when these compute blocks are pipelined.
As shown in  Fig. \ref{fig:layer-results-latency}, PUMA exhibits higher latency than the GPU for $l=1024$. The runtime breakdown reveals that this higher latency in PUMA is primarily due to its high softmax computation time, which accounts for 38\% of the total runtime. 
The graph demonstrates that the UCLM-based architecture significantly reduces the runtime percentage spent on softmax in an encoder layer. For instance, at $l=1024$, PUMA's 38\% runtime spent on softmax is reduced to 13\% in UCLM-based architecture with multicore support.



For the dynamic energy (Fig. \ref{fig:layer-results-energy}), our work achieves significantly lower energy consumption compared to GPU, ranging from $10\times$ $-68\times$ reduction. 
Between PUMA and the proposed architectures, the difference in energy consumption is negligible. This is because the instructions and the number of internal device accesses within the chip remain nearly identical. Additionally, ADC energy dominates the overall energy consumption, further minimizing the impact of architectural differences on energy usage.




\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, trim={0 5mm 0 0 0}]{figures/result_batch_bert_TOPS.png}
    \caption{Comparison of our accelerator compared to PUMA and GPU in terms of effective $TOPS$ for multiple encoder layers of BERT-Base and BERT-Large models. $TOPS$ is calculated by adding the total multiplication and addition operations from matrix-multiplications involved in transformers.}
    \label{fig:end2end-results-latency}
    % \vspace{-3mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, trim={0 5mm 0 0 0}]{figures/result_batch_bert_TOPSW.png}
    \caption{Comparison of our accelerator compared to PUMA and GPU in terms of effective $TOPS/W$ for multiple encoder layers of BERT-Base and BERT-Large models. }
    \label{fig:end2end-results-energy}
    % \vspace{-3mm}
\end{figure*}



\subsection{End-to-End performance and energy efficiency}
To comprehensively evaluate the effectiveness and scalability of our approach, we analyzed its performance across different numbers of encoder layers in BERT models and batch size, comparing it with the Ampere GPU and PUMA. The metrics assessed include compute efficiency ($TOPS$) and energy efficiency ($TOPS/W$), where $TOPS$ is tera operations per second and the number of operations is the sum of multiplication and addition operations resulting from all the matrix-multiplication computations in the given layers. 


The compute efficiency results, depicted in Fig. \ref{fig:end2end-results-latency}(a), show the superior performance of our accelerator compared to both the Ampere GPU and PUMA. With a batch size of 1, the GPU and PUMA deliver similar performance across various encoder layer configurations within a given model. For instance, the GPU achieves $19~ TOPS$, while the PUMA reaches $26~ TOPS$ for the BERT-Base model. Both are clearly under-utilized for BERT models with a single batch size. In contrast, our architecture shows improved performance as the number of encoder layers increases. This is attributed to its ability to hide the latency of computation blocks by overlapping it with other computations, as illustrated in Fig. \ref{fig:fine-gran-timeline}(b). 

In CIM architectures, utilizing multiple batch sizes can be achieved either by replicating model weight parameters across additional hardware for parallel computation or by processing batches serially. In our hardware configuration, PUMA can store weights for up to four batches within a single layer, leading to increased throughput as the batch size grows. However, our proposed fine-grained pipelining requires sufficient memory to store weights for at least two layers. This allows both BERT-Base and BERT-Large models to support finer-grained processing for batch sizes up to 2. For batch size 4, computations for batch size 2 are repeated, resulting in performance identical to that of batch size 2.
Overall, our accelerator achieves $158~ TOPS$ and $263~ TOPS$ for the BERT-Base and BERT-Large models, respectively, outperforming the GPU by up to $9.8\times$.



Energy efficiency, as illustrated in Fig.\ref{fig:end2end-results-energy}, follows the trend observed for a single encoder layer. 
The Ampere GPU exhibits low energy efficiency due to under-utilization, achieving only $0.3~TOPS/W$ for a single batch. Its efficiency improves with increasing batch size, reaching up to $0.9~TOPS/W$.
In contrast, HASTILY consistently achieves high energy efficiency of $8~TOPS/W$, regardless of the model size, number of encoder layers, or batch size. These results highlight the superior scalability and performance of our CIM-based architecture.



\section{Discussion and Future Work}
\label{sec:discussion}
% this section is for limitations, clarifying difference from previous works and future work discussions

\textbf{Scope:} Our results focus on highlighting the benefits of HASTILY for BERT models, with bigger improvements at longer sequence lengths. Since vision transformers also have an encoder-like architecture, the findings are also applicable to them \cite{dosovitskiy2021imageworth16x16words}. For decoder-like models such as GPT, our accelerator will give similar improvements for the prefill-phase. However, the decoder-phase of GPT models is dominantly memory-bound and the benefits of softmax acceleration will be limited for such use cases. 

\textbf{Accuracy:} Regarding accuracy, we primarily utilize low-precision (8-bit) for our evaluations, owing to the ongoing trend of using quantized models for inference \cite{dettmers2022llm, kim2021bert}. Further, analog CIM also introduces quantization and functional errors due to various circuit non-idealities. We assume that such errors can be reduced during transformer inference using hardware-aware training \cite{joshi2020accurate}. 

\textbf{Future work:} HASTILY uses the dual-functionality SRAM arrays for storing lookup tables for exponent operations. It can be also extended to accommodate pre-computed tables for other transcendental functions, including $log$ and $sin$ \cite{intelHarrison1999TheCO}. Our evaluation setup allows exploring other hardware configurations, model parameters and quantization, but it can be incorporated with digital CIM to expand the design space in future. Additionally, though our softmax implementation is computationally stable because we take care of the overflow/underflow conditions, it can be modified to include the logsoftmax or online implementations if needed \cite{blanchard2021accurately,dao2022flashattention}. Integrating our fine-granularity pipelining technique in GPUs could also be an interesting study to further investigate the benefits of our proposed optimization.


\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduced a scalable area-efficient compute-in-memory (CIM) accelerator, HASTILY, with unified compute and lookup modules (UCLMs) to accelerate transformer models. Leveraging parallelism at multiple levels, including UCLMs, cores and tiles, our spatial architecture efficiently computes both softmax and matrix-multiplication, solving the softmax bottleneck. The evaluations demonstrated significant contributions from both the UCLM based hardware and the software optimizations such as multi-core softmax support and fine-granularity pipelining in improving the final throughput for BERT models on CIM accelerator. Finally, we believe the compiler and cycle-level simulator tailored towards transformer inference on CIM architecture would open up new research directions, taking the field forward.

\section*{Aknowledgments}
This work was supported in part by the Center for the Co-Design of Cognitive Systems (COCOSYS), in part by the SRC/DARPA JUMP centers.



\bibliographystyle{IEEEtran}
\bibliography{refs}


% \section{Biography Section}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/DongEun.jpg}}]{Dong Eun Kim} 
received his B.S. in Electrical and Electronic Engineering from Yonsei University, Korea, in 2018. Currently, he is pursuing a Ph.D. degree in Electrical and Computer Engineering at Purdue University under the supervision of Professor Kaushik Roy. His research interests lie in hardware and software do-design for neural network. Particularly he is interested in in-memory computing based on non-volatile memories. During his PhD, he has done internship with IBM in 2021. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/Tanvi.jpg}}]{Tanvi Sharma} received her bachelor’s degree from Indian Institute of Technology, Roorkee in 2018 and worked as a Digital Design Engineer at Texas Instruments before joining Purdue University in 2019. She is in the direct PhD program at Purdue, under the guidance of Professor Kaushik Roy. She has been a recipient of Qualcomn Innovation Fellowship in 2021, and MLSys Rising Stars Award in 2023. Her research interests lie at the intersection of machine learning and systems, with focus on developing energy efficient ML hardware accelerators using compute-in-memory solutions.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/KaushikRoy.jpg}}]{Kaushik Roy} is the Edward G. Tiedemann, Jr., Distinguished Professor of Electrical and Computer Engineering at Purdue University. He received his BTech from Indian Institute of Technology, Kharagpur, PhD from University of Illinois at Urbana-Champaign in 1990 and joined the Semiconductor Process and Design Center of Texas Instruments, Dallas, where he worked for three years on FPGA architecture development and low-power circuit design. His current research focuses on cognitive algorithms, circuits and architecture for energy-efficient neuromorphic computing/ machine learning, and neuro-mimetic devices. Kaushik has supervised more than 100 PhD dissertations and his students are well placed in universities and industry. He is the co-author of two books on Low Power CMOS VLSI Design (John Wiley \& McGraw Hill). 
\end{IEEEbiography}


\vfill

\end{document}


