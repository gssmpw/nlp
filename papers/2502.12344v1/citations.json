[
  {
    "index": 0,
    "papers": [
      {
        "key": "retransformer",
        "author": "Yang, Xiaoxuan and Yan, Bonan and Li, Hai and Chen, Yiran",
        "title": "ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "imact",
        "author": "Laguna, Ann Franchesca and Kazemi, Arman and Niemier, Michael and Hu, X Sharon",
        "title": "In-memory computing based accelerator for transformer networks for long sequences"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kang2021framework",
        "author": "Kang, Myeonggu and Shin, Hyein and Kim, Lee-Sup",
        "title": "A framework for accelerating transformer-based language model on ReRAM-based architecture"
      },
      {
        "key": "asadi",
        "author": "Li, Huize and Li, Zhaoying and Bai, Zhenyu and Mitra, Tulika",
        "title": "ASADI: Accelerating Sparse Attention using Diagonal-based In-situ Computing"
      },
      {
        "key": "okazaki2022analog",
        "author": "Okazaki, Atsuya and Narayanan, Pritish and Ambrogio, Stefano and Hosokawa, Kohji and Tsai, Hsinyu and Nomura, Akiyo and Yasuda, Takeo and Mackin, Charles and Friz, Alexander and Ishii, Masatoshi and others",
        "title": "Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xformer",
        "author": "Sridharan, Shrihari and Stevens, Jacob R and Roy, Kaushik and Raghunathan, Anand",
        "title": "X-former: In-memory acceleration of transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chakraborty2020resistive",
        "author": "Chakraborty, Indranil and Ali, Mustafa and Ankit, Aayush and Jain, Shubham and Roy, Sourjya and Sridharan, Shrihari and Agrawal, Amogh and Raghunathan, Anand and Roy, Kaushik",
        "title": "Resistive crossbars as approximate hardware building blocks for machine learning: Opportunities and challenges"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "transpim",
        "author": "Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana",
        "title": "Transpim: A memory-based acceleration via software-hardware co-design for transformer"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "A3",
        "author": "Tae Jun Ham and\nSungjun Jung and\nSeonghak Kim and\nYoung H. Oh and\nYeonhong Park and\nYoonho Song and\nJung{-}Hun Park and\nSanghee Lee and\nKyoung Park and\nJae W. Lee and\nDeog{-}Kyoon Jeong",
        "title": "A\\({}^{\\mbox{3}}\\): Accelerating Attention Mechanisms in Neural Networks\nwith Approximation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Spatten",
        "author": "Hanrui Wang and\nZhekai Zhang and\nSong Han",
        "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token\nand Head Pruning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ham2021elsa",
        "author": "Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W",
        "title": "ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "EdgeBERT",
        "author": "Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon",
        "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lu2021sanger",
        "author": "Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun",
        "title": "Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "softermax",
        "author": "Stevens, Jacob R. and Venkatesan, Rangharajan and Dai, Steve and Khailany, Brucek and Raghunathan, Anand",
        "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers"
      }
    ]
  }
]