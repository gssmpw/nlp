\section{Related Works}
% \label{sec:related}
% % Previous works
% \textbf{Compute in Memory based Hardware}: There have been few studies proposing CIM based hardware for transformer acceleration. One of the first works, ReTransformer ____, addressed the challenge of writing intermediate result by decomposing the matrix into sub-matrix computations. For instance, the $QK^T$ calculation was restructured as $R=QW_{K}^T$ and $RX^T$. Although ReTransformer offered a novel solution for matrix multiplication, its hybrid softmax hardware incurs significant latency, limiting overall architecture performance. 
% % For example, their Re-RAM based compare and select logic takes 13 and 8 cycles to find the $x_{max}$, which is later used to calculate exponential and logarithm using look up table circuits.
% The authors in ____ proposed a combination of content addressable memories and crossbars for transformer acceleration and implemented the multi-head self-attention mechanism. However, most studies employing CIM hardware primarily focus on emerging non-volatile memories like ReRAM ____. While ReRAMs provide high-density crossbars, they have limited endurance and high write latency. To mitigate the issues associated with ReRAMs, X-former ____ utilized a hybrid architecture containing both ReRAMs and SRAMs. The SRAMs are used for storing the intermediate results to reduce the write energy and latency. Nonetheless, fabrication technology limitations and the low endurance of ReRAMs pose challenges for their practical implementation in hardware ____. Additionally, studies such as ____ have explored the use of CIM or processing in high-bandwidth memory (HBM) to facilitate efficient data communication during transformer inference.  

% \textbf{Algorithm-Hardware Co-Design}:
% A3____ was one of the first works to accelerate attention, utilizing content-based search for accelerating attention and implementing online softmax where the maximum value is updated on the fly while computing exponents. Many studies have focused on modifying the attention algorithm using pruning and quantization techniques, and subsequently building a specialized hardware for it. For instance, Spatten ____ employed token pruning, head pruning and quantization techniques, and designed a top-k engine to support their pruning algorithm. Elsa ____ proposed an approximate softmax calculation scheme by filtering out redundant relations along with a specialized hardware. EdgeBERT ____ introduced an entropy-based early exit prediction for dynamic voltage-frequency scaling, combined with selective network pruning and floating-point quantization to optimize performance. Sanger ____ developed a new software to introduce dynamic sparsity pattern and a reconfigurable systolic array design that can exploit such patterns. Softermax ____ proposed a specialized hardware for accelerating softmax by using power of two instead of natural exponent, and designing a low cost, low precision accelerator tailored for it. %include SOLE

% \textbf{Software/System Techniques}:
% Multiple works focus on different aspects of transformer acceleration in a system. Most of them focus on addressing the auto-regressive nature of LLMs. flash attention - pioneering work to show longer sequence length computation in GPUs, idea is to tile the large matrix into smaller blocks and using online softmax for accelerating attention, have been followed up with flash-attention 2 and 3 leveraging newer GPU capabilities including asynchronous communication and quantization, pagedattention --  virtualized the allocation of large matrices involved in attention by introducing the granularity of page, thus reducing memory fragmentation and requirements. Also developed an llm serving platform named vllm implementing pagedattention, orca -- proposed iteration-level scheduling to execute requests at the granularity of iteration and suggested selective batching to serve a distributed system. deepspeed -- library including various optimizations such as ZeRO (zero Redundancy Optimizer) to reduce memory redundancies while training LLMs on hundreds of GPUs.

% Our work differs from the mentioned related works in three ways. First, we focus on reducing the softmax bottleneck using an area-efficient compute-cum-lookup module, based on an implementation designed and synthesized in TSMC 65nm. Previous works which focus on algorithm modifications, dataflow, pruning or quantization strategies are complementary to our work. Second, we propose a specialized \textit{programmable} hardware accelerator for transformer inference, paving the way for CIM based hardware exploration with transformers. Third, we employ an only CMOS based technology that overcomes the high write energy/latency issues present in ReRAM based CIM accelerators.