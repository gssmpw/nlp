@book{lamport94,
 author = "Leslie Lamport",
 title = "{\LaTeX: A Document Preparation System}",
 year = "1994",
 publisher = "Addison-Wesley",
 edition = "2nd",
 address = "Reading, Massachusetts"
}

@inproceedings{nicepaper1,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2",
  title = "A Very Nice Paper To Cite",
  year = "2016",
  booktitle = "Proceedings of the 26th IEEE International Symposium on High Performance Computer Architecture"
}

@inproceedings{nicepaper2,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3",
  title = "Another Very Nice Paper to Cite",
  year = "2015",
  booktitle = "Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{nicepaper3,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3 and Firstname4 Lastname4 and Firstname5 Lastname5 and Firstname6 Lastname6 and Firstname7 Lastname7 and Firstname8 Lastname8 and Firstname9 Lastname9 and Firstname10 Lastname10 and Firstname11 Lastname11 and Firstname12 Lastname12",
  title = "Yet Another Very Nice Paper To Cite, With Many Author Names All Spelled Out",
  year = "2011",
  booktitle = "Proceedings of the 38th Annual International Symposium on Computer Architecture"
}


%Related works - cim
@inproceedings{retransformer,
  title={ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration},
  author={Yang, Xiaoxuan and Yan, Bonan and Li, Hai and Chen, Yiran},
  booktitle={Proceedings of the 39th International Conference on Computer-Aided Design},
  pages={1--9},
  year={2020}
}

@inproceedings{imact,
  title={In-memory computing based accelerator for transformer networks for long sequences},
  author={Laguna, Ann Franchesca and Kazemi, Arman and Niemier, Michael and Hu, X Sharon},
  booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={1839--1844},
  year={2021},
  organization={IEEE}
}

@article{xformer,
  title={X-former: In-memory acceleration of transformers},
  author={Sridharan, Shrihari and Stevens, Jacob R and Roy, Kaushik and Raghunathan, Anand},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={31},
  number={8},
  pages={1223--1233},
  year={2023},
  publisher={IEEE}
}

@inproceedings{transpim,
  title={Transpim: A memory-based acceleration via software-hardware co-design for transformer},
  author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1071--1085},
  year={2022},
  organization={IEEE}
}

@article{agrawal2021magnetoresistive,
  title={Magnetoresistive circuits and systems: Embedded non-volatile memory to crossbar arrays},
  author={Agrawal, Amogh and Wang, Cheng and Sharma, Tanvi and Roy, Kaushik},
  journal={IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume={68},
  number={6},
  pages={2281--2294},
  year={2021},
  publisher={IEEE}
}

@inproceedings{asadi,
  title={ASADI: Accelerating Sparse Attention using Diagonal-based In-situ Computing},
  author={Li, Huize and Li, Zhaoying and Bai, Zhenyu and Mitra, Tulika},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2024},
  organization={IEEE}
}


@inproceedings{kang2021framework,
  title={A framework for accelerating transformer-based language model on ReRAM-based architecture},
  author={Kang, Myeonggu and Shin, Hyein and Kim, Lee-Sup},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={41},
  number={9},
  pages={3026--3039},
  year={2021},
  publisher={IEEE}
}

@inproceedings{okazaki2022analog,
  title={Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers},
  author={Okazaki, Atsuya and Narayanan, Pritish and Ambrogio, Stefano and Hosokawa, Kohji and Tsai, Hsinyu and Nomura, Akiyo and Yasuda, Takeo and Mackin, Charles and Friz, Alexander and Ishii, Masatoshi and others},
  booktitle={2022 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={3319--3323},
  year={2022},
  organization={IEEE}
}


%related works - other hardware accelerator
@article{Spatten,
  author       = {Hanrui Wang and
                  Zhekai Zhang and
                  Song Han},
  title        = {SpAtten: Efficient Sparse Attention Architecture with Cascade Token
                  and Head Pruning},
  journal      = {CoRR},
  volume       = {abs/2012.09852},
  year         = {2020},
  url          = {https://arxiv.org/abs/2012.09852},
  eprinttype    = {arXiv},
  eprint       = {2012.09852},
  timestamp    = {Sun, 03 Jan 2021 18:46:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2012-09852.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{EdgeBERT,
author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
title = {EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480095},
doi = {10.1145/3466752.3480095},
abstract = {Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 \texttimes{}, 2.5 \texttimes{}, and 53 \texttimes{} lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {830–844},
numpages = {15},
keywords = {embedded non-volatile memories, latency-aware, natural language processing, software and hardware co-design},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@article{A3,
  author       = {Tae Jun Ham and
                  Sungjun Jung and
                  Seonghak Kim and
                  Young H. Oh and
                  Yeonhong Park and
                  Yoonho Song and
                  Jung{-}Hun Park and
                  Sanghee Lee and
                  Kyoung Park and
                  Jae W. Lee and
                  Deog{-}Kyoon Jeong},
  title        = {A\({}^{\mbox{3}}\): Accelerating Attention Mechanisms in Neural Networks
                  with Approximation},
  journal      = {CoRR},
  volume       = {abs/2002.10941},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.10941},
  eprinttype    = {arXiv},
  eprint       = {2002.10941},
  timestamp    = {Tue, 03 Mar 2020 14:32:13 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-10941.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lu2021sanger,
  title={Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture},
  author={Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={977--991},
  year={2021}
}

@inproceedings{you2023vitcod,
  title={Vitcod: Vision transformer acceleration via dedicated algorithm and accelerator co-design},
  author={You, Haoran and Sun, Zhanyi and Shi, Huihong and Yu, Zhongzhi and Zhao, Yang and Zhang, Yongan and Li, Chaojian and Li, Baopu and Lin, Yingyan},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={273--286},
  year={2023},
  organization={IEEE}
}

@inproceedings{ham2021elsa,
  title={ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks},
  author={Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={692--705},
  year={2021},
  organization={IEEE}
}


@inproceedings{softermax,
author = {Stevens, Jacob R. and Venkatesan, Rangharajan and Dai, Steve and Khailany, Brucek and Raghunathan, Anand},
title = {Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DAC18074.2021.9586134},
doi = {10.1109/DAC18074.2021.9586134},
abstract = {Transformers have transformed the field of natural language processing. Their superior performance is largely attributed to the use of stacked “self-attention” layers, each of which consists of matrix multiplies as well as softmax operations. As a result, unlike other neural networks, the softmax operation accounts for a significant fraction of the total run-time of Transformers. To address this, we propose Softermax, a hardware-friendly softmax design. Softermax consists of base replacement, low-precision softmax computations, and an online normalization calculation. We show Softermax results in 2.35x the energy efficiency at 0.90x the size of a comparable baseline, with negligible impact on network accuracy.},
booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},
pages = {469–474},
numpages = {6},
location = {San Francisco, CA, USA}
}

@inproceedings{wang2023sole,
  title={SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference},
  author={Wang, Wenxun and Zhou, Shuchang and Sun, Wenyu and Sun, Peiqin and Liu, Yongpan},
  booktitle={2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)},
  pages={1--9},
  year={2023},
  organization={IEEE}
}



%related works - software optimizations
@inproceedings{kao2023flat,
  title={Flat: An optimized dataflow for mitigating attention bottlenecks},
  author={Kao, Sheng-Chun and Subramanian, Suvinay and Agrawal, Gaurav and Yazdanbakhsh, Amir and Krishna, Tushar},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={295--310},
  year={2023}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@inproceedings{wu2021cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={22--31},
  year={2021}
}

@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={1691--1703},
  year={2020},
  organization={PMLR}
}

@article{sun2020transtrack,
  title={Transtrack: Multiple object tracking with transformer},
  author={Sun, Peize and Cao, Jinkun and Jiang, Yi and Zhang, Rufeng and Xie, Enze and Yuan, Zehuan and Wang, Changhu and Luo, Ping},
  journal={arXiv preprint arXiv:2012.15460},
  year={2020}
}





@inproceedings{AttentionIsAllYouNeed,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{conneau2019cross,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{yenduri2023gpt,
  title={GPT (Generative Pre-trained Transformer)--A Comprehensive Review on Enabling Technologies},
  author={Yenduri, Gokul and Ramalingam, M and Chemmalar Selvi, G and Supriya, Y and Srivastava, G and Maddikunta, PKR and Deepti Raj, G and Jhaveri, RH and Prabadevi, B and Wang, W and others},
  journal={Potential Applications, Emerging Challenges, and Future Directions},
  year={2023}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@INPROCEEDINGS{prime,
  author={Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)}, 
  title={PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory}, 
  year={2016},
  volume={},
  number={},
  pages={27-39},
  keywords={Artificial neural networks;Random access memory;Microprocessors;Acceleration;Biological neural networks;Memory management;processing in memory;neural network;resistive random access memory},
  doi={10.1109/ISCA.2016.13}}


@inproceedings{isaac,
author = {Shafiee, Ali and Nag, Anirban and Muralimanohar, Naveen and Balasubramonian, Rajeev and Strachan, John Paul and Hu, Miao and Williams, R. Stanley and Srikumar, Vivek},
title = {ISAAC: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars},
year = {2016},
isbn = {9781467389471},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2016.12},
doi = {10.1109/ISCA.2016.12},
abstract = {A number of recent efforts have attempted to design accelerators for popular machine learning algorithms, such as those involving convolutional and deep neural networks (CNNs and DNNs). These algorithms typically involve a large number of multiply-accumulate (dot-product) operations. A recent project, DaDianNao, adopts a near data processing approach, where a specialized neural functional unit performs all the digital arithmetic operations and receives input weights from adjacent eDRAM banks.This work explores an in-situ processing approach, where memristor crossbar arrays not only store input weights, but are also used to perform dot-product operations in an analog manner. While the use of crossbar memory as an analog dot-product engine is well known, no prior work has designed or characterized a full-fledged accelerator based on crossbars. In particular, our work makes the following contributions: (i) We design a pipelined architecture, with some crossbars dedicated for each neural network layer, and eDRAM buffers that aggregate data between pipeline stages. (ii) We define new data encoding techniques that are amenable to analog computations and that can reduce the high overheads of analog-to-digital conversion (ADC). (iii) We define the many supporting digital components required in an analog CNN accelerator and carry out a design space exploration to identify the best balance of memristor storage/compute, ADCs, and eDRAM storage on a chip. On a suite of CNN and DNN workloads, the proposed ISAAC architecture yields improvements of 14.8\texttimes{}, 5.5\texttimes{}, and 7.5\texttimes{} in throughput, energy, and computational density (respectively), relative to the state-of-the-art DaDianNao architecture.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {CNN, DNN, accelerator, analog, memristor, neural},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}

@inproceedings{puma,
author = {Ankit, Aayush and Hajj, Izzat El and Chalamalasetti, Sai Rahul and Ndu, Geoffrey and Foltin, Martin and Williams, R. Stanley and Faraboschi, Paolo and Hwu, Wen-mei W and Strachan, John Paul and Roy, Kaushik and Milojicic, Dejan S.},
title = {PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304049},
doi = {10.1145/3297858.3304049},
abstract = {Memristor crossbars are circuits capable of performing analog matrix-vector multiplications, overcoming the fundamental energy efficiency limitations of digital logic. They have been shown to be effective in special-purpose accelerators for a limited set of neural network applications. We present the Programmable Ultra-efficient Memristor-based Accelerator (PUMA) which enhances memristor crossbars with general purpose execution units to enable the acceleration of a wide variety of Machine Learning (ML) inference workloads. PUMA's microarchitecture techniques exposed through a specialized Instruction Set Architecture (ISA) retain the efficiency of in-memory computing and analog circuitry, without compromising programmability. We also present the PUMA compiler which translates high-level code to PUMA ISA. The compiler partitions the computational graph and optimizes instruction scheduling and register allocation to generate code for large and complex workloads to run on thousands of spatial cores. We have developed a detailed architecture simulator that incorporates the functionality, timing, and power models of PUMA's components to evaluate performance and energy consumption. A PUMA accelerator running at 1 GHz can reach area and power efficiency of 577 GOPS/s/mm 2  and 837~GOPS/s/W, respectively. Our evaluation of diverse ML applications from image recognition, machine translation, and language modelling (5M-800M synapses) shows that PUMA achieves up to 2,446\texttimes{} energy and 66\texttimes{} latency improvement for inference compared to state-of-the-art GPUs. Compared to an application-specific memristor-based accelerator, PUMA incurs small energy overheads at similar inference latency and added programmability.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {715–731},
numpages = {17},
keywords = {neural networks, memristors, machine learning, accelerators},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{song2017pipelayer,
  title={Pipelayer: A pipelined reram-based accelerator for deep learning},
  author={Song, Linghao and Qian, Xuehai and Li, Hai and Chen, Yiran},
  booktitle={2017 IEEE international symposium on high performance computer architecture (HPCA)},
  pages={541--552},
  year={2017},
  organization={IEEE}
}

@article{fong2015embedding,
  title={Embedding read-only memory in spin-transfer torque MRAM-based on-chip caches},
  author={Fong, Xuanyao and Venkatesan, Rangharajan and Lee, Dongsoo and Raghunathan, Anand and Roy, Kaushik},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={24},
  number={3},
  pages={992--1002},
  year={2015},
  publisher={IEEE}
}


@inproceedings{zadeh2020gobo,
  title={Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference},
  author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={811--824},
  year={2020},
  organization={IEEE}
}

% cim macro
@ARTICLE{twin8t,
  author={Si, Xin and Chen, Jia-Jing and Tu, Yung-Ning and Huang, Wei-Hsing and Wang, Jing-Hong and Chiu, Yen-Cheng and Wei, Wei-Chen and Wu, Ssu-Yen and Sun, Xiaoyu and Liu, Rui and Yu, Shimeng and Liu, Ren-Shuo and Hsieh, Chih-Cheng and Tang, Kea-Tiong and Li, Qiang and Chang, Meng-Fan},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={A Twin-8T SRAM Computation-in-Memory Unit-Macro for Multibit CNN-Based AI Edge Processors}, 
  year={2020},
  volume={55},
  number={1},
  pages={189-202},
  doi={10.1109/JSSC.2019.2952773}}

@INPROCEEDINGS{ali2023cicc,
  author={Ali, Mustafa and Chakraborty, Indranil and Choudhary, Sakshi and Chang, Muya and Kim, Dong Eun and Raychowdhury, Arijit and Roy, Kaushik},
  booktitle={2023 IEEE Custom Integrated Circuits Conference (CICC)}, 
  title={A 65 nm 1.4-6.7 TOPS/W Adaptive-SNR Sparsity-Aware CIM Core with Load Balancing Support for DL workloads}, 
  year={2023},
  volume={},
  number={},
  pages={1-2},
  doi={10.1109/CICC57935.2023.10121243}}

  @article{analog1cim,
  author = {Si, Xin and et al.},
  title = {A Local Computing Cell and 6T SRAM-Based Computing-in-Memory Macro With 8-b MAC Operation for Edge AI Chips},
  year = {2021},
  volume = {56},
  number = {9},
  pages = {2817--2831},
  journal = {IEEE Journal of Solid-State Circuits},
  doi = {10.1109/JSSC.2021.3073254}
}

  @INPROCEEDINGS{digitaltsmc,
  author={Chih, Yu-Der and others},
  booktitle={2021 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={16.4 An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications}, 
  year={2021},
  volume={64},
  number={},
  pages={252-254},
  doi={10.1109/ISSCC42613.2021.9365766}}

@ARTICLE{computesram,
  author={Wang, Jingcheng and Wang, Xiaowei and Eckert, Charles and Subramaniyan, Arun and Das, Reetuparna and Blaauw, David and Sylvester, Dennis},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={A 28-nm Compute SRAM With Bit-Serial Logic/Arithmetic Operations for Programmable In-Memory Vector Computing}, 
  year={2020},
  volume={55},
  number={1},
  pages={76-86},
  doi={10.1109/JSSC.2019.2939682}}

@INPROCEEDINGS{tmsc2023digital,
  author={Mori, Haruki and others},
  booktitle={2023 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={A 4nm 6163-TOPS/W/b $\mathbf{4790-TOPS/mm^{2}/b}$ SRAM Based Digital-Computing-in-Memory Macro Supporting Bit-Width Flexibility and Simultaneous MAC and Weight Update}, 
  year={2023},
  volume={},
  number={},
  pages={132-134},
  doi={10.1109/ISSCC42615.2023.10067555}}

@INPROCEEDINGS{tsmc2022isscc,
  author={Fujiwara, Hidehiro and others},
  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={A 5-nm 254-TOPS/W 221-TOPS/mm2 Fully-Digital Computing-in-Memory Macro Supporting Wide-Range Dynamic-Voltage-Frequency Scaling and Simultaneous MAC and Write Operations}, 
  year={2022},
  volume={65},
  number={},
  pages={1-3},
  doi={10.1109/ISSCC42614.2022.9731754}}

@INPROCEEDINGS{isscc2022adcless,
  author={Yan, Bonan and others},
  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={A 1.041-Mb/mm2 27.38-TOPS/W Signed-INT8 Dynamic-Logic-Based ADC-less SRAM Compute-in-Memory Macro in 28nm with Reconfigurable Bitwise Operation for AI and Embedded Applications}, 
  year={2022},
  volume={65},
  number={},
  pages={188-190},
  doi={10.1109/ISSCC42614.2022.9731545}}

@INPROCEEDINGS{isscc2022mfchang,
  author={Wu, Ping-Chun and others},
  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={A 28nm 1Mb Time-Domain Computing-in-Memory 6T-SRAM Macro with a 6.6ns Latency, 1241GOPS and 37.01TOPS/W for 8b-MAC Operations for Edge-AI Devices}, 
  year={2022},
  volume={65},
  number={},
  pages={1-3},
  doi={10.1109/ISSCC42614.2022.9731681}}



%int8
@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{joshi2020accurate,
  title={Accurate deep neural network inference using computational phase-change memory},
  author={Joshi, Vinay and Le Gallo, Manuel and Haefeli, Simon and Boybat, Irem and Nandakumar, Sasidharan Rajalekshmi and Piveteau, Christophe and Dazzi, Martino and Rajendran, Bipin and Sebastian, Abu and Eleftheriou, Evangelos},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={2473},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@misc{wordtoken, 
title={What are tokens and how to count them?}, author={OpenAI},
howpublished={\url{https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them}}
}


@misc{Ampere, 
title={Nvidia Ampere GA-102 GPU Architecture}, author={NVIDIA},
howpublished={\url{https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf}}
}

@misc{Hopper,
    title={NVIDIA H100 Tensor Core GPU Architecture}, author={NVIDIA},
    howpublished={\url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper}}
}

@article{chakraborty2020resistive,
  title={Resistive crossbars as approximate hardware building blocks for machine learning: Opportunities and challenges},
  author={Chakraborty, Indranil and Ali, Mustafa and Ankit, Aayush and Jain, Shubham and Roy, Sourjya and Sridharan, Shrihari and Agrawal, Amogh and Raghunathan, Anand and Roy, Kaushik},
  journal={Proceedings of the IEEE},
  volume={108},
  number={12},
  pages={2276--2310},
  year={2020},
  publisher={IEEE}
}

@article{wulf1995hitting,
  title={Hitting the memory wall: Implications of the obvious},
  author={Wulf, Wm A and McKee, Sally A},
  journal={ACM SIGARCH computer architecture news},
  volume={23},
  number={1},
  pages={20--24},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@article{verma2019memory,
  title={In-memory computing: Advances and prospects},
  author={Verma, Naveen and Jia, Hongyang and Valavi, Hossein and Tang, Yinqi and Ozatay, Murat and Chen, Lung-Yen and Zhang, Bonan and Deaville, Peter},
  journal={IEEE Solid-State Circuits Magazine},
  volume={11},
  number={3},
  pages={43--55},
  year={2019},
  publisher={IEEE}
}

@article{gholami2024ai,
  title={AI and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)},
  pages={10--14},
  year={2014},
  organization={IEEE}
}

@article{lee2012area,
  title={Area efficient ROM-embedded SRAM cache},
  author={Lee, Dongsoo and Roy, Kaushik},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={21},
  number={9},
  pages={1583--1595},
  year={2012},
  publisher={IEEE}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}




@inproceedings{softmax1_hyft,
author = {Xia, Tianhua and Zhang, Sai Qian},
title = {Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format for both Training and Inference},
year = {2024},
isbn = {9798400706882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665314.3670816},
doi = {10.1145/3665314.3670816},
abstract = {The attention mechanism is a pivotal element within the transformer architecture, making a substantial contribution to its exceptional performance. Within this attention mechanism, Softmax is an imperative component that enables the model to assess the degree of correlation between various segments of the input. Yet, prior research has shown that Softmax operations can significantly increase processing latency and energy consumption in the transformer network due to their internal nonlinear operations and data dependencies. In this work, we proposed Hyft, a hardware efficient floating point Softmax accelerator for both training and inference. Hyft aims to reduce the implementation cost of different nonlinear arithmetic operations within softmax by adaptively converting intermediate results into the most suitable numeric format for each specific operation, leading to reconfigurable accelerator with hybrid numeric format. The evaluation results highlight that Hyft achieves a remarkable 10X reduction in hardware resource utilization and a 6x reduction in processing latency, all while maintaining a negligible impact on transformer accuracy.},
booktitle = {Proceedings of the 29th ACM/IEEE International Symposium on Low Power Electronics and Design},
pages = {1–6},
numpages = {6},
keywords = {hardware accelerator, softmax, transformer},
location = {Newport Beach, CA, USA},
series = {ISLPED '24}
}

@inproceedings{softmax2_ITA,
  added-at = {2024-10-06T00:00:00.000+0200},
  author = {Islamoglu, Gamze and Scherer, Moritz and Paulin, Gianna and Fischer, Tim and Jung, Victor J. B. and Garofalo, Angelo and Benini, Luca},
  biburl = {https://www.bibsonomy.org/bibtex/256f1e79081645af204f733dc5db390d1/dblp},
  booktitle = {ISLPED},
  ee = {https://doi.org/10.1109/ISLPED58423.2023.10244348},
  interhash = {d4682789e5948e509e37cadb83d6ec8a},
  intrahash = {56f1e79081645af204f733dc5db390d1},
  isbn = {979-8-3503-1175-4},
  keywords = {dblp},
  pages = {1-6},
  publisher = {IEEE},
  timestamp = {2024-10-07T10:22:55.000+0200},
  title = {ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers.},
  url = {http://dblp.uni-trier.de/db/conf/islped/islped2023.html#IslamogluSPFJGB23},
  year = 2023
}

@INPROCEEDINGS{softmax3,
  author={Koca, Nazim Altar and Do, Anh Tuan and Chang, Chip-Hong},
  booktitle={2023 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Hardware-efficient Softmax Approximation for Self-Attention Networks}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Transformers;Throughput;Natural language processing;Hardware;Table lookup;Registers;Task analysis},
  doi={10.1109/ISCAS46773.2023.10181465}}


@misc{swinTransformer,
      title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
      author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
      year={2021},
      eprint={2103.14030},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.14030}, 
}

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@misc{bello2020attentionaugmentedconvolutionalnetworks,
      title={Attention Augmented Convolutional Networks}, 
      author={Irwan Bello and Barret Zoph and Ashish Vaswani and Jonathon Shlens and Quoc V. Le},
      year={2020},
      eprint={1904.09925},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1904.09925}, 
}

@misc{choromanski2022rethinkingattentionperformers,
      title={Rethinking Attention with Performers}, 
      author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
      year={2022},
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.14794}, 
}

@article{blanchard2021accurately,
  title={Accurately computing the log-sum-exp and softmax functions},
  author={Blanchard, Pierre and Higham, Desmond J and Higham, Nicholas J},
  journal={IMA Journal of Numerical Analysis},
  volume={41},
  number={4},
  pages={2311--2330},
  year={2021},
  publisher={Oxford University Press}
}

@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@inproceedings{intelHarrison1999TheCO,
  title={The Computation of Transcendental Functions on the IA-64 Architecture},
  author={John Harrison and Ping Tak and Peter Tang},
  year={1999},
  url={https://api.semanticscholar.org/CorpusID:16091395}
}

@ARTICLE{cashram,
  author={Agrawal, Amogh and Kosta, Adarsh and Kodge, Sangamesh and Kim, Dong Eun and Roy, Kaushik},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={CASH-RAM: Enabling In-Memory Computations for Edge Inference Using Charge Accumulation and Sharing in Standard 8T-SRAM Arrays}, 
  year={2020},
  volume={10},
  number={3},
  pages={295-305},
  keywords={Standards;Arrays;Neural networks;Random access memory;Acceleration;Parasitic capacitance;In-memory computing;static random access memory (SRAM);charge-sharing;edge intelligence},
  doi={10.1109/JETCAS.2020.3014250}}

@article{bertTextClassification,
   title={Comparing BERT Against Traditional Machine Learning Models in Text Classification},
   volume={2},
   ISSN={2810-9503},
   url={http://dx.doi.org/10.47852/bonviewJCCE3202838},
   DOI={10.47852/bonviewjcce3202838},
   number={4},
   journal={Journal of Computational and Cognitive Engineering},
   publisher={BON VIEW PUBLISHING PTE},
   author={Garrido-Merchan, Eduardo C. and Gozalo-Brizuela, Roberto and Gonzalez-Carvajal, Santiago},
   year={2023},
   month=apr, pages={352–356} }


@misc{sun2020finetuneberttextclassification,
      title={How to Fine-Tune BERT for Text Classification?}, 
      author={Chi Sun and Xipeng Qiu and Yige Xu and Xuanjing Huang},
      year={2020},
      eprint={1905.05583},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.05583}, 
}

@misc{kocián2021siamesebertbasedmodelweb,
      title={Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a New Czech Dataset}, 
      author={Matěj Kocián and Jakub Náplava and Daniel Štancl and Vladimír Kadlec},
      year={2021},
      eprint={2112.01810},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2112.01810}, 
}

@misc{zhu2023vlgptgenerativepretrainedtransformer,
      title={VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation}, 
      author={Jinguo Zhu and Xiaohan Ding and Yixiao Ge and Yuying Ge and Sijie Zhao and Hengshuang Zhao and Xiaohua Wang and Ying Shan},
      year={2023},
      eprint={2312.09251},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.09251}, 
}

@misc{goyal2023newssummarizationevaluationera,
      title={News Summarization and Evaluation in the Era of GPT-3}, 
      author={Tanya Goyal and Junyi Jessy Li and Greg Durrett},
      year={2023},
      eprint={2209.12356},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.12356}, 
}

@InProceedings{automatedNewsSummarization,
author="Gupta, Anushka
and Chugh, Diksha
and Anjum
and Katarya, Rahul",
editor="Aurelia, Sagaya
and Hiremath, Somashekhar S.
and Subramanian, Karthikeyan
and Biswas, Saroj Kr.",
title="Automated News Summarization Using Transformers",
booktitle="Sustainable Advanced Computing",
year="2022",
publisher="Springer Singapore",
address="Singapore",
pages="249--259",
abstract="The amount of text data available online is increasing at a very fast pace; hence, text summarization has become essential. Most of the modern recommender and text classification systems require going through a huge amount of data. Manually generating precise and fluent summaries of lengthy articles is a very tiresome and time-consuming task. Hence, generating automated summaries for the data and using it to train machine learning models will make these models space and time efficient. Extractive summarization and abstractive summarization are two separate methods of generating summaries. The extractive technique identifies the relevant sentences from the original document and extracts only those from the text. Whereas in abstractive summarization techniques, the summary is generated after interpreting the original text, hence making it more complicated. In this paper, we will be presenting a comprehensive comparison of a few transformer architecture-based pretrained models for text summarization. For analysis and comparison, we have used the BBC news dataset that contains text data that can be used for summarization and human-generated summaries for evaluating and comparing the summaries generated by machine learning models.",
isbn="978-981-16-9012-9"
}

@misc{thoppilan2022lamdalanguagemodelsdialog,
      title={LaMDA: Language Models for Dialog Applications}, 
      author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
      year={2022},
      eprint={2201.08239},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.08239}, 
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@misc{zhang2022optopenpretrainedtransformer,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

@phdthesis{trishitThesis,
    author={Dutta,Trishit},
    year={2023},
    title={Energy Efficient Hardware for Neural Network Applications},
    journal={ProQuest Dissertations and Theses},
    pages={40},
    note={Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2024-09-23},
    abstract={With the explosion of AI in recent years, there has been an exponential rise in the demand for computing resources. Although Moore’s law has so far kept up with conventional computational demands in the past, it has become evident that the efficiency and area gains with transistor scaling are no longer exponential, but rather incremental. The standard Von Neumann architecture imposes a limit on efficiency and latency as data is shuttled repeatedly between the compute and memory units. On the other hand, AI workloads rely heavily on matrix-vectormultiplication which get exponentially expensive with vector widths. In-memory and nearmemory computing have come up as promising alternatives that addresses both these issues elegantly while reducing energy requirements.A variety of NN models rely on fast and repetitive evaluation of exponential transcendental functions. In many cases, this is done by range reduction technique and math tables. For optimal energy efficiency and throughput, it is best if these tables reside as close as possible to the circuit where it is consumed. We propose a mixed-signal macro with dual functionality: ability to do matrix vector multiplication as well evaluate exp(x) for 32-bit IEEE 754 floating point number. The said macro consists of 64x64 array of special 8T cells that stores the math tables without hindering normal SRAM functionality. The charge based MVM engine uses two ADCs with reconfigurable precision, allowing faster throughput for sparse inputs. As the outputs of these operations are separate, it allows for high flexibility to use the macro in any neural-network hardware that needs either or both the functions.Spiking Neural Networks (SNN) can perform sequential learning tasks efficiently by using the inherent recurrence of membrane potential (Vmem) accumulation over several timesteps. However, the data movement of Vmem creates additional memory accesses, which becomes a bottleneck operation. Additionally, SNN input spikes are highly sparse in nature, which can be exploited for efficient hardware implementation. We propose an SNN accelerator based on inmemory processing that addresses these. The said accelerator consists of 9 compute macros and 3 neuron macros, which can be programmed to work either serially (9 compute, 1 neuron) or in 3 parallel sets (3 compute, 1 neuron) to support different layer sizes. Peripheral logic computes the membrane potential and stores it in the same compute macro, thus avoiding unnecessary data movement. The neuron macro keeps track of final membrane potential and generates output spikes. This accelerator was designed to run at 200Mhz at 1.2v in TSMC 65nm node.},
    keywords={Silicon; Sparsity; Cells; Macros; Logic; Neural networks; Flexibility; Circuits; Multiplication & division; Design; Energy efficiency; Transistors; Workloads; Energy consumption; Artificial intelligence; Electrical engineering; Energy; Mathematics; 0389:Design; 0395:Logic; 0791:Energy; 0405:Mathematics; 0544:Electrical engineering; 0800:Artificial intelligence},
    isbn={9798379882433},
    language={English},
    url={https://www.proquest.com/dissertations-theses/energy-efficient-hardware-neural-network/docview/2838437919/se-2},
}

@ARTICLE{aliCIM,
  author={Ali, Mustafa and Roy, Sourjya and Saxena, Utkarsh and Sharma, Tanvi and Raghunathan, Anand and Roy, Kaushik},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
  title={Compute-in-Memory Technologies and Architectures for Deep Learning Workloads}, 
  year={2022},
  volume={30},
  number={11},
  pages={1615-1630},
  keywords={Random access memory;Nonvolatile memory;Memory management;Computer architecture;Read only memory;Kernel;Arithmetic;Compute-in-memory (CiM);deep learning (DL);neural networks},
  doi={10.1109/TVLSI.2022.3203583}}
