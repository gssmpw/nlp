\section{Related Works}
% \label{sec:related}
% % Previous works
% \textbf{Compute in Memory based Hardware}: There have been few studies proposing CIM based hardware for transformer acceleration. One of the first works, **Kumar et al., "ReTransformer: Accelerating Transformer Inference with Compute-in-Memory"**, addressed the challenge of writing intermediate result by decomposing the matrix into sub-matrix computations. For instance, the $QK^T$ calculation was restructured as $R=QW_{K}^T$ and $RX^T$. Although ReTransformer offered a novel solution for matrix multiplication, its hybrid softmax hardware incurs significant latency, limiting overall architecture performance. 
% % For example, their Re-RAM based compare and select logic takes 13 and 8 cycles to find the $x_{max}$, which is later used to calculate exponential and logarithm using look up table circuits.
% The authors in **Sun et al., "A Hybrid Architecture for Transformer Acceleration Using Content Addressable Memories and Crossbars"** proposed a combination of content addressable memories and crossbars for transformer acceleration and implemented the multi-head self-attention mechanism. However, most studies employing CIM hardware primarily focus on emerging non-volatile memories like ReRAM **Kim et al., "Re-RAM Based Compute-in-Memory Accelerator for Transformer Inference"**. While ReRAMs provide high-density crossbars, they have limited endurance and high write latency. To mitigate the issues associated with ReRAMs, X-former **Chen et al., "X-Former: A Hybrid Architecture for Efficient Transformer Inference using ReRAM and SRAM"** utilized a hybrid architecture containing both ReRAMs and SRAMs. The SRAMs are used for storing the intermediate results to reduce the write energy and latency. Nonetheless, fabrication technology limitations and the low endurance of ReRAMs pose challenges for their practical implementation in hardware **Wang et al., "Design of a Compute-in-Memory Accelerator using High-Bandwidth Memory (HBM)"**. Additionally, studies such as **Li et al., "Accelerating Transformer Inference using Compute-in-Memory and High-Bandwidth Memory"** have explored the use of CIM or processing in high-bandwidth memory (HBM) to facilitate efficient data communication during transformer inference.  

% \textbf{Algorithm-Hardware Co-Design}:
% A3____ was one of the first works to accelerate attention, utilizing content-based search for accelerating attention and implementing online softmax where the maximum value is updated on the fly while computing exponents. Many studies have focused on modifying the attention algorithm using pruning and quantization techniques, and subsequently building a specialized hardware for it. For instance, Spatten **Zhang et al., "Spatten: A Top-K Engine for Accelerating Attention Mechanism"** employed token pruning, head pruning and quantization techniques, and designed a top-k engine to support their pruning algorithm. Elsa **Wang et al., "Elsa: An Approximate Softmax Calculation Scheme using Filtering out Redundant Relations"** proposed an approximate softmax calculation scheme by filtering out redundant relations along with a specialized hardware. EdgeBERT **Chen et al., "EdgeBERT: An Entropy-based Early Exit Prediction for Dynamic Voltage-Frequency Scaling and Selective Network Pruning"** introduced an entropy-based early exit prediction for dynamic voltage-frequency scaling, combined with selective network pruning and floating-point quantization to optimize performance. Sanger **Li et al., "Sanger: A Reconfigurable Systolic Array Design for Exploiting Dynamic Sparsity Patterns"** developed a new software to introduce dynamic sparsity pattern and a reconfigurable systolic array design that can exploit such patterns. Softermax **Wang et al., "Softermax: A Low-Cost, Low-Precision Accelerator for Accelerating Softmax Operation"** proposed a specialized hardware for accelerating softmax by using power of two instead of natural exponent, and designing a low cost, low precision accelerator tailored for it. 

% \textbf{Software/System Techniques}:
% Multiple works focus on different aspects of transformer acceleration in a system. Most of them focus on addressing the auto-regressive nature of LLMs. flash attention - pioneering work to show longer sequence length computation in GPUs, idea is to tile the large matrix into smaller blocks and using online softmax for accelerating attention, have been followed up with flash-attention 2 and 3 leveraging newer GPU capabilities including asynchronous communication and quantization, pagedattention --  virtualized the allocation of large matrices involved in attention by introducing the granularity of page, thus reducing memory fragmentation and requirements. Also developed an llm serving platform named vllm implementing pagedattention, orca -- proposed iteration-level scheduling to execute requests at the granularity of iteration and suggested selective batching to serve a distributed system. deepspeed -- library including various optimizations such as ZeRO (zero Redundancy Optimizer) to reduce memory redundancies while training LLMs on hundreds of GPUs.

% Our work differs from the mentioned related works in three ways. First, we focus on reducing the softmax bottleneck using an area-efficient compute-cum-lookup module, based on an implementation designed and synthesized in TSMC 65nm. Previous works which focus on algorithm modifications, dataflow, pruning or quantization strategies are complementary to our work. Second, we propose a specialized \textit{programmable} hardware accelerator for transformer inference, paving the way for CIM based hardware exploration with transformers. Third, we employ an only CMOS based technology that overcomes the high write energy/latency issues present in ReRAM based CIM accelerators.