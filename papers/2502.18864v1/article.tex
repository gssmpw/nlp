\section{Introduction}
\label{sec:introduction}

\begin{figure}[htbp!]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{figures/fig1.pdf}
\vspace{0.1cm}
\caption{\textbf{The AI co-scientist system design and experimental validation summary.} (a) Here, we illustrate the different components of the the AI co-scientist multi-agent system, and its interaction paradigm with scientists. Given a research goal in natural language, the co-scientist generates novel research hypotheses and proposals. The system employs specialized agents — Generation, Reflection, Ranking, Evolution, Proximity (which evaluates relatedness), Meta-review (which provides high level analysis) — to continuously generate, debate, and evolve research hypotheses within a tournament framework. Feedback from the tournament enables iterative improvement, creating a self-improving loop towards novel and high-quality outputs. The co-scientist leverages tools, including web search and specialized AI models to improve the grounding and quality of generated research hypotheses. Scientists can converse with the co-scientist in natural language to specify research goals, incorporate constraints, provide feedback and suggest new directions for explorations via the designated user interface. (b) We perform end-to-end validation of the co-scientist generated hypotheses in three important topics of biomedicine with varied complexity--- suggesting novel drug repurposing candidates for acute myeloid leukemia (AML) (upper panel), discovering novel epigenetic targets for liver fibrosis treatment (middle panel), and recapitulating the discovery of novel mechanism of gene transfer evolution in bacteria key to anti-microbial resistance (lower panel). The co-scientist's hypotheses for these three settings are externally, independently validated by \textit{in vitro} laboratory experiments and detailed in separate preprints co-timed with this work. In the figure, blue denotes expert scientist inputs while red denotes the co-scientist agents or outputs.
}
\label{fig:system-overview}
\end{figure}

Human ingenuity and creativity propel the advancement of fundamental research in science and medicine. However, researchers, particularly in biomedicine, are faced with a breadth and depth conundrum. The complexity of biomedical topics require increasingly deep and specific subject matter expertise, while leaps in insight may still arise from broad knowledge bridging across disciplines. With the rapid rise in scientific publications and the availability of numerous technologies for specialized high-throughput assays, mastery of both discipline-specific depth and trans-disciplinary insight can be challenging.

Despite these challenges, many modern breakthroughs have emerged from trans-disciplinary endeavours. Emmanuelle Charpentier and Jennifer Doudna won the 2020 Nobel Prize in Chemistry for their work on CRISPR~\citep{jinek2012programmable}, which combined techniques and strategies ranging from microbiology to genetics to molecular biology. These benefits of synergy have also been seen beyond experimental biomedicine in numerous other areas of science. Notably, Geoffrey Hinton and John Hopfield combined ideas from physics and neuroscience~\citep{hopfield1982neural, hinton1986learning} to develop artificial intelligence (AI) systems, which were awarded the 2024 Nobel Prize in Physics.

There has been rapid technological progress in AI towards generally intelligent and collaborative systems, which might empower scientists in creatively traversing and expertly reasoning across disciplinary domains. Such systems are capable of advanced reasoning~\citep{guo2025deepseek, jaech2024openai, team2024gemini}, multimodal understanding~\citep{team2024gemini}, and agentic behaviors~\citep{wiesinger2024agents} such as the ability to use tools to solve complex tasks over long time horizons. Further, the trends with distillation~\cite{hinton2015distilling} and inference time compute costs~\citep{team2024gemini, team2024gemma} indicate that such intelligent and general AI systems are rapidly becoming more affordable and available. Motivated by the aforementioned unmet needs in the modern discovery process in science and medicine and building on the advancements in frontier AI~\citep{leslie2024frontier}, we develop and introduce an AI co-scientist system.

The co-scientist is designed to act as a helpful assistant and collaborator to scientists and to help accelerate the scientific discovery process. The system is a compound, multi-agent AI system~\cite{chen2024more} building on Gemini 2.0 and designed to mirror the reasoning process underpinning the scientific method~\citep{gower2012scientific}. Given a research goal specified in natural language, the system can search and reason over relevant literature to summarize and synthesize prior work and build on it to propose novel, original research hypotheses and experimental protocols for downstream validations~(\cref{fig:system-overview}a). The co-scientist provides grounding for its recommendations by citing relevant literature and explaining the reasoning behind its proposals. 

This work does not aim to completely automate the scientific process with AI. Instead, the co-scientist is purpose-built for a ``scientist-in-the-loop'' collaborative paradigm, to help domain experts augment their hypothesis generation process and guide the exploration that follows. Scientists can specify their research goals in simple natural language, including informing the system of desirable attributes for the hypotheses or research proposals it should create and the constraints that the synthesized outputs should satisfy. They can also collaborate and provide feedback in a variety of ways, including directly supplying their own ideas and hypotheses, refining those generated by the system, or using natural language chat to guide the system and ensure alignment with their expertise.

The co-scientist works through a significant scaling of the test-time compute paradigm~\citep{snell2024scaling, brown2019superhuman, silver2016mastering} to iteratively reason, evolve, and improve the outputs as it gathers more knowledge and understanding. Underpinning the system are thinking and reasoning steps---notably a self-play based scientific debate step for generating novel research hypotheses; tournaments that compare and rank hypotheses via the process of finding win and loss patterns, and a hypothesis evolution process to improve their quality. Finally, the agentic nature of the system enables it to recursively self-critique its output and use tools such as web-search to provide itself with feedback to iteratively refine its hypotheses and research proposals.

While the co-scientist system is general-purpose and applicable across multiple scientific disciplines, in this study we focus our development and validation of the system to biomedicine. We validate the co-scientist's capability in three impactful areas of biomedicine with varied complexity: (1) drug repurposing, (2) novel treatment targets discovery, and (3) new mechanistic explanations for antimicrobial resistance (\cref{fig:system-overview}b).

Drug development is an increasingly time-consuming and expensive process~\citep{ringel2020breaking} in which new therapeutics require restarting many aspects of the discovery and development process for each indication or disease (roughly 70\% of drug approvals are for new drugs). In contrast, drug repurposing—--identifying novel therapeutic indications for drugs beyond their original intended use--—has emerged as a compelling strategy to address these challenges~\citep{pushpakom2019drug}. Successful examples of repurposing include Humira (adalimumab) and Keytruda (pembrolizumab), both of which have become among the most successful drugs in history.~\citep{pushpakom2019drug}. The process typically involves analyzing molecular signatures, signaling pathways, drug interactions, clinical trial results, adverse event reports, and other literature-based information~\citep{xia2024drug}, along with off-label use data and, in some cases, patient experiences. However, drug repurposing is limited by several factors: (1) the need for extensive expertise across biomedical, molecular biology, and biochemical systems, (2) the inherent complexity of mammalian biological systems, and (3) the time-intensive nature of traditional computational biology analyses required. We leverage the co-scientist to generate predictions for large-scale drug repurposing, validating the generated predictions using a combination of computational biology, expert clinician feedback, and \textit{in vitro} wet-lab validation approaches. Notably, our system has proposed novel repurposing candidates for acute myeloid leukamia (AML) that inhibit tumor viability at clinically relevant concentrations \textit{in vitro} across multiple AML cell lines.

Unlike drug repurposing, which is a combinatorial search problem through a large but constrained set of drugs and diseases, identifying novel treatment targets for diseases presents a more significant challenge, traditionally requiring extensive literature review, deep biological understanding, sophisticated hypothesis generation and complex experimental validation strategies. The uncertainty of identifying novel treatment targets is significantly greater than in drug repurposing, as it involves not only repurposing existing compounds but also uncovering entirely new components and mechanisms within biological systems. This target discovery process can be inefficient, potentially leading to suboptimal hypothesis selection and prioritization for \textit{in vitro} and \textit{in vivo} experimentation. Given the high costs and time associated with experimental validation, a more effective approach is needed. We probe the capabilities of the co-scientist to propose, rank, and provide experimental protocols for novel research hypotheses pertaining to target discovery. To demonstrate this capability, we focus on liver fibrosis, a prevalent and serious disease, showcasing the co-scientist's potential to discover novel treatment targets amenable to experimental validation. In particular, the co-scientist has suggested novel epigenetic targets demonstrating significant anti-fibrotic activity in human hepatic organoids.

As a third validation of the capabilities of our system, we focus on generation of hypotheses to explain mechanisms related to gene transfer evolution in bacteria pertaining to antimicrobial resistance (AMR) - mechanisms developed by microbes to circumvent drug applications used to fight infections. This is arguably an even more complex challenge than drug repurposing and target discovery and involves understanding of not only the molecular mechanisms of gene transfer (conjugation, transduction, and transformation) but also the ecological and evolutionary pressures that drive the spread of AMR genes: a system-level problem with many interacting variables. This is also an important healthcare challenge with increasing rates of infections and deaths worldwide~\citep{keown2014antimicrobial}. In this validation, researchers instructed the AI co-scientist to explore a topic that had already been subject to novel discovery by their independent research group. Notably, at the time of instructing the AI co-scientist system, the researchers' novel experimental insights had not yet been published or revealed in the public domain. The system was instructed to hypothesize how capsid-forming phage-inducible chromosomal islands (cf-PICIs) exist across multiple bacterial species. The system independently proposed that cf-PICIs interact with diverse phage tails to expand their host range. This \textit{in silico} discovery mirrored the novel and experimentally validated results that expert researchers had already performed, as detailed in the co-timed report~\citep{he2025chimeric, penades2025ai}.

Overall, our key contributions are summarized as follows:
\begin{itemize}[leftmargin=1.5em,rightmargin=0em]
    \item\textbf{Introducing an AI co-scientist.} We develop and introduce an AI co-scientist that goes beyond literature summarization and ``deep research'' tools to assist scientists in uncovering new knowledge, novel hypothesis generation and experimental planning.
    \item\textbf{Significant scaling of test-time compute paradigm for scientific reasoning.} The co-scientist is built on a Gemini 2.0 multi-agent architecture, utilizing an asynchronous task execution framework. This framework allows the system to flexibly allocate computational resources to scientific reasoning, mirroring key aspects of the scientific method. Specifically, the system uses self-play strategies, including a scientific debate and a tournament-based evolution process, to iteratively refine hypotheses and research proposals creating a self-improving loop. Using automated evaluations across 15 complex expert curated open scientific goals, we demonstrate the benefits of scaling the test-time compute paradigm with the AI co-scientist outperforming other state-of-the-art (SOTA) agentic and reasoning models in generating high quality hypotheses for complex problems.
    \item\textbf{Expert-in-the-loop scientific workflow.} Our system is designed for collaboration with scientists. The system can flexibly incorporate conversational feedback in natural language from scientists and co-develop, evolve and refine outputs.
    \item \textbf{End-to-end validation of the co-scientist in important topics in biomedicine.} We present end-to-end validation of novel AI-generated hypotheses through new empirical findings in three distinct and increasingly complex areas of biomedicine: drug repurposing, novel target discovery, and antimicrobial resistance. The AI co-scientist predicts novel repurposing drugs for AML, identifies novel epigenetic treatment targets grounded in preclinical evidence for liver fibrosis, and proposes novel mechanisms for gene transfer in bacterial evolution and antimicrobial resistance. These discoveries from the AI co-scientist have been validated in wet-lab settings and are detailed in separate, co-timed technical reports.
\end{itemize}


\section{Related Works}
\subsection{Reasoning models and test-time compute scaling}
The modern revolution in foundation AI models~\citep{bommasani2021opportunities} and large language models (LLMs) has been largely driven by advances in pre-training techniques~\citep{erhan2010does, radford2018improving}, leading to breakthroughs in models like the GPT and Gemini family~\citep{team2023gemini, achiam2023gpt}. These models, trained on increasingly massive internet-scale and multimodal datasets, have demonstrated impressive abilities in language understanding and generation leading to breakthrough performance in a variety of benchmarks~\citep{chowdhery2022palm, google2023palm2}.  However, a key area of ongoing development is enhancing their \textit{reasoning} capabilities. This has led to the emergence of ``reasoning models'' which go beyond simply predicting the next word and instead attempt to mimic human thought processes~\citep{wei2022chain}. One promising direction in this pursuit is the test-time compute paradigm.  This approach moves beyond solely relying on the knowledge acquired during pre-training and allocates additional computational resources during inference to enable System-2 style thinking---slower deliberate reasoning to reduce uncertainty and progress optimally towards the goal~\citep{kahneman2011thinking}. This concept emerged with early successes such as AlphaGo~\citep{silver2016mastering}, which used Monte Carlo Tree Search (MCTS) to explore game states and strategically select moves, and Libratus~\citep{brown2019superhuman}, which employed similar techniques to achieve superhuman performance in poker. This paradigm has now found applications in LLMs, where increased compute at test-time allows for more thorough exploration of possible responses, leading to improved reasoning and accuracy~\citep{wei2022chain, yao2024tree, zelikman2022star, chen2024more, snell2024scaling,4928, muennighoff2025s1, tu2024towards}. Recent advancements, like the Deepseek-R1 model~\citep{guo2025deepseek}, further demonstrate the potential of test-time compute by leveraging reinforcement learning to refine the model's ``chain-of-thought'' and enhance complex reasoning abilities over longer horizons. In this work, we propose a significant scaling of the test-time compute paradigm using inductive biases derived from the scientific method to design a multi-agent framework for scientific reasoning and hypothesis generation without any additional learning techniques.

\subsection{AI-driven scientific discovery}
AI-driven scientific discovery represents a paradigm shift in how research is conducted across various scientific domains. Recent advancements, particularly the development of large deep learning and generative models, have cemented AI's role in scientific discovery. This is best exemplified by AlphaFold 2's remarkable progress in the grand challenge of protein structure prediction, which has revolutionized structural biology and opened new avenues for drug discovery and materials science~\citep{jumper2021highly}. Other notable examples include the development of novel antibiotics, protein binder design,  and material discovery with AI~\citep{wong2024discovery, zambaldi2024novo, merchant2023scaling}. 

Building on these successes with specialized, bespoke AI models, there has been recent work exploring the even more ambitious goal of fully integrating AI, especially modern LLM-based systems, into the complete research workflow, from initial hypothesis generation all the way to manuscript writing. This end-to-end integration represents a significant shift, presenting both unprecedented opportunities and significant challenges as the field moves beyond specialized AI tools toward realizing the potential of AI as an active collaborator, or even, as some envision, a nascent ``AI scientist''~\citep{lu2024ai, schmidgall2025agent}.

As an example of this shift, Liang et al.~\cite{liang2024can} directly assessed the utility of LLMs for providing feedback on research manuscripts. Through both a retrospective analysis of existing peer reviews and a prospective user study, they demonstrated the significant concordance between LLM-generated feedback and that of human reviewers. Their study, using GPT-4~\citep{openai2023gpt4}, found that a majority of researchers perceived LLM-generated feedback as helpful, and in some instances, even more beneficial than feedback from human colleagues. However, while valuable, their work focuses solely on the feedback stage of the scientific process, leaving open the question of how LLMs might be integrated into the full research cycle, from hypothesis formation to experimental validation and manuscript writing.

Another effort embodying this shift is PaperQA2~\citep{skarlinski2024language}, an AI agent for scientific literature search and summarization. The authors claimed to surpass PhD and postdoc researchers on multiple literature research tasks, as measured both by performance on objective benchmarks and human evaluations. While the system is a useful for synthesizing information, it does not engage in scientific reasoning for novel hypothesis generation.

HypoGeniC, a system proposed by Zhou et al.~\cite{zhou2022least}, tackles hypothesis generation by iteratively refining hypotheses using LLMs and a multi-armed bandit-inspired approach. The process begins with a small set of examples, from which initial hypotheses are generated. These hypotheses are then iteratively updated through exploration and exploitation, guided by a reward function based on training accuracy.  This refined set of hypotheses is subsequently used to construct an interpretable classifier. However, the method's reliance on retrospective data for evaluation means the degree to which the system can generate truly novel hypotheses remains an open question. Furthermore, the system lacks end-to-end validation beyond subjective human evaluations.

Ifargan et al.~\cite{ifargan2025autonomous} present ``data-to-paper'', a platform that systematically guides multiple LLM and rule-based agents to generate research papers, with automated feedback mechanisms and information tracing for verification. However, the evaluations are limited to recapitulating existing peer-reviewed publications and its unclear if the system can generate truly novel, yet grounded hypothesis and research proposals.

Virtual Lab~\citep{swanson2024virtual} is another closely related work. Here, the authors propose a team of LLM agents with a ``principal investigator'' LLM guiding a team of specialized LLM agents to solve a scientific problem. The LLM team receives high level human supervision. The authors demonstrate the utility of their work by leveraging Virtual Lab to design nanobody binders to recent variants of SARS-CoV-2 with experimental validation. While similar in spirit, there are significant design differences to our approach and the generality of the system remains unclear.

Boiko et al.~\citep{boiko2023autonomous} introduced ``Coscientist'', a multi-agent system powered by GPT-4, designed for autonomous execution of complex chemical experiments. This system integrates capabilities such as web and document searching, and code execution, to facilitate independent experimental design, planning, and execution. In addition to similar sounding names, both ``Coscientist'' and our system share the overarching goal of accelerating scientific discovery through AI. However, there are several important distinctions. Notably, ``Coscientist'' is quite narrowly focused on chemical research while ours is much broadly applicable across science. Secondly, our system has important technical innovations that lead to a self-improving system that can uncover new, original knowledge while their approach is a more vanilla-stitching of GPT-4 based agents. Finally, despite the name, ``Coscientist'' prioritizes a high degree of autonomy in experimental execution, directly interfacing with laboratory hardware. Our system, instead, is explicitly designed as a collaborative tool, emphasizing a ``scientist-in-the-loop'' approach and centers on the more cognitive aspects of the research process.

Finally, Lu et al.~\cite{lu2024ai} propose ``The AI Scientist'', a fully automated system designed to conduct research using multiple collaborating LLM agents. These agents handle all stages of the research process, from defining research problems and conducting literature reviews to designing and executing experiments, and even writing up the results. The design shares similarities with our work---the key differences being our focus on the scaling of the test-time compute paradigm to generate high quality hypotheses and research proposals. Secondly, their proposed system has limited automated evaluations; in contrast, our work has a combination of automated, human expert and end-to-end wet lab validations. Finally, our goal is to not to automate scientific discovery, rather to build a helpful AI collaborator for scientists.

\subsection{AI for biomedicine}
More broadly, large AI models are increasingly demonstrating their potential in biomedical science. Both general purpose (GPT-4, Gemini) and specialized LLMs (Med-PaLM, Med-Gemini, Galactica, Tx-LLM) have shown strong performance on biomedical reasoning and question-answering benchmarks~\citep{team2023gemini, achiam2023gpt, singhal2022large, saab2024capabilities, taylor2022galactica, chaves2024tx}. Beyond benchmarks, Med-PaLM 2, was successfully applied to identify causative murine genetic factors for traits such as diabetes, cataracts, and hearing loss~\cite{tu2023genetic}---an early example of hypothesis generation and LLM-assisted discovery. We have also seen the exciting development of specialized foundation and large language models trained on DNA, RNA and protein sequences with a variety of applications~\citep{nguyen2024sequence, lin2023evolutionary, ruffolo2024design, shaw2024protex}. Although AI in biology and medicine often necessitates specialization, the rapid progress of frontier AI models has blurred the distinction. As these models grow in scale, data diversity, and complexity, they continue to achieve breakthroughs in areas once thought to require domain-specific AI. Our co-scientist system, with its modular multi-agent architecture, is flexibly designed to build on top of these advancements in general-purpose frontier AI models and leverage specialized AI models as tools to enhance the capabilities.
 
Drug repurposing is an important area of validation experiments in this work. The traditional approach to this task requires both computational and experimental approaches and a comprehensive understanding of disease-drug interactions~\cite{pushpakom2019drug, krishnamurthy2022drug}. While methods like knowledge graphs with graph convolutional networks have shown promise~\cite{zitnik2018modeling, morselli2021network}, their applicability is limited by the initial knowledge graph's scope.  TxGNN~\cite{huang2024foundation}, an example of a specialized biomedical foundation model with a graph based approach, addresses ``zero-shot'' repurposing for novel diseases but remains dependent on the underlying knowledge graph's quality and lacks sufficient scalability and explainability. Furthermore, no end-to-end validations of the model predictions were reported in the study. In contrast, our work, leveraging state-of-the-art LLMs in the co-scientist setup, is more scalable. We report a combination of expert evaluations and wet-lab experiments to validate the system predictions.

\input{method}

\clearpage
\input{results}

\section{Limitations}
\label{sec:limitations}
We are encouraged by the early promise of the AI co-scientist evaluations, which highlight its potential to augment scientific research.
However, the system has several limitations. Responsible innovation necessitates thoughtful consideration of these alongside the potential impacts to researchers and scientific research. 

\paragraph{Limitations with literature search, reviews and reasoning.} The reviews undertaken by the AI co-scientist system may miss critical prior works due to reliance on open-access literature. In the presented work, the AI co-scientist does not access the entire published literature due to compliance with license or access restrictions where applicable. The system may also omit consideration of prior work on occasions where it has incorrectly reasoned that the work is not relevant.

\paragraph{Lack of access to negative results data.} The AI co-scientist system's use of only open published literature means it likely has limited access to negative experimental results or records of failed experiments. It is known that such data may be more rarely published than positive results, yet experienced scientists working in the field may nonetheless possess and utilize this knowledge to prioritize research~\citep{brazil2024illuminating}. Strategies to overcome this phenomenon might further improve the performance of the co-scientist as a tool for scientific discovery.

\paragraph{Improved multimodal reasoning and tool-use capabilities.} Some of the most interesting data in scientific publications is not written in text but may be encoded visually in figures and charts. However, even state-of-the-art frontier models may not comprehensively utilize such data with optimal  reasoning~\citep{roberts2024scifibench} and the AI co-scientist system is unlikely to be an exception. Stronger benchmarks and evaluations are necessary to improve these capabilities. We have also not evaluated the ability of our system to reason over and integrate information from domain-specific biomedical multimodal datasets (such as large multi-omics datasets) and knowledge graphs. More work is needed to integrate the AI co-scientist system with specialized scientific tools, AI models and databases, and evaluate the ability to utilize them effectively.

\paragraph{Inherited limitations of frontier LLMs.} LLM limitations include imperfect factuality and hallucinations, which may be propagated in the co-scientist system. The system's reliance on existing LLMs and web-search, while providing immediate access to broad knowledge, may propagate errors of factuality, biases or limitations present in those resources.

\paragraph{Need for better metrics and broader evaluations.} While the current AI co-scientist evaluation includes AI auto-ratings, expert reviews and targeted \textit{in vitro} validations, the evaluation of system performance remains preliminary. A comprehensive, systematic evaluation across diverse biomedical and scientific disciplines is necessary to determine the generalizability of co-scientist. Furthermore, the system requires continued improvement to produce outputs that meet the rigor and detail of high-quality publications. Furthermore, the Elo rating implemented to help the system self-improve for hypothesis generation is a limited auto-evaluation metric. Continued investigation into alternative, more objective, less intrinsically-favored, evaluation metrics that better represent perspectives and preferences from expert scientists could strengthen future work.

\paragraph{Limitations of existing validations.} At present, the AI co-scientist focuses on identifying potential therapeutic targets and mechanisms, but many not be addressing the complexities of drug delivery systems. Pharmaceutical factors such as tissue-specific targeting, formulation requirements, and delivery efficiency—while critical for clinical translation—remain beyond the scope of the present system.

The AI co-scientist is currently also not designed to generate comprehensive clinical trial designs or to fully account for factors such as drug bioavailability, pharmacokinetics, and any complex drug interactions when applied for drug repurposing or discovery. These aspects require much deeper understanding, extensive expertise, and appropriate data beyond the scope of the current system. A dedicated translational scientific team is needed for onward clinical translation of the predictions. These limitations also highlights the need for continued development and integration of the system with more tools, such as specialized AI models and real-time databases.


\section{Safety and Ethical Implications}
\label{sec:safetyethics}
While AI systems such as the co-scientist offers the potential to accelerate scientific discovery, it also poses significant safety and ethical challenges, distinct from its impact on the scientific method itself. Safety risks center on the dual-use and the possibility that scientific breakthroughs could be exploited for harmful purposes. Ethical risks, conversely, involve research that contradicts established ethical norms and conventions within specific scientific disciplines. We review these distinct risk categories, emphasizing that further research is crucial to fully understand and mitigate them.

\paragraph{Evolving ethics frameworks, policy and regulations for advanced AI use in scientific endeavors.}
Research ethics is a central aspect of scientific endeavor and a prominent research field in its own right~\citep{shrader1994ethics, resnik2005ethics, rollin2006science, fisher2008research, edel2018science, menapace2019scientific}.  A key focus is directing research towards positive societal impact, although questions remain about potentially dual-use knowledge~\citep{miller2007ethical, selgelid2009governance, pustovit2010philosophical, forge2010note, kuhlau2013ethics}.

Core ethics principles are being complemented by emerging regulation, and formal processes involving organizational ethics reviews that are meant to provide an assessment of adherence to the code of conduct, as well as an assessment of present and future risks involved with research proposals~\citep{shaw2006research, rothstein2006risks, ludlow2015regulating, verschraegen2018regulating}.

The acceleration of science through AI, especially with advanced agentic AI systems, requires advances in science and AI ethics policy and regulation~\citep{jobin2019global, wansley2016regulation}. This adaptation is crucial to address the changing research landscape and the unique risks associated with AI agents of varying capabilities and autonomy.

Advancements in AI systems, like the co-scientist, require moving beyond the limited ethical considerations designed for earlier, specialized AI models with restricted application and action spaces~\citep{gabriel2024ethics}. Some preliminary frameworks have developed to understand the impact of LLM agents in science, specifically mapping risks across user intent, domain, and broader impact~\citep{tang2024prioritizingsafeguardingautonomyrisks}.

\paragraph{Dual-use risks and technical safeguards.}
Beyond the scientific domain, broad frameworks are being developed for evaluating the emergence of potentially dangerous capabilities in AI agents~\citep{shevlane2023model, bova2024quantifying, phuong2024evaluating}. These frameworks assess capabilities related to persuasion, deception, cybersecurity, self-proliferation, and self-reasoning. As AI agents advance, safety evaluations in science must integrate these broader assessments. A long-term risk is that agentic systems could develop intrinsic goals influencing research directions. Human susceptibility to AI manipulation, already observed in other contexts~\citep{sabour2025humandecisionmakingsusceptibleaidriven}, underscores the need for robust frameworks ensuring instruction-following and value alignment.

More immediately on a shorter time-scale, technical safeguards are needed to address unethical research queries, malicious user intent, and the potential for extracting dangerous or dual-use knowledge from scientific AI systems. Because verification is computationally `easier' than generation, significant research focuses on using advanced LLMs as `critics' or `judges' to evaluate both user queries and AI outputs acting as a scalable oversight mechanism. These critics operate based on predefined criteria, provided through direct instructions, examples (few-shot or many-shot prompting), or fine-tuning~\citep{ke2023critiquellm, vu2024foundational, wei2024systematic, lan2024criticbench, zheng2024judging, gu2024survey}. They can also leverage external tools for grounding~\citep{gou2023critic} and have shown promise in multimodal scenarios~\citep{chen2024mllm}. However, limitations remain; human expert involvement is crucial, as LLMs may not align with human judgment in specialized domains~\citep{szymanski2024limitationsllmasajudgeapproachevaluating}.

\paragraph{Adversarial robustness of scientific AI systems.}
Recognizing and mitigating adversarial attacks is a crucial, ongoing research area in the development of foundation models and advanced AI assistants~\citep{shayegani2023survey, he2023large, zhu2023promptbench, fu2023misusing, zhang2023defending, chao2024jailbreakbench, zhao2024evaluating, ma2025safetyscalecomprehensivesurvey}. While manual red teaming has identified vulnerabilities, automated approaches now allow for optimizing prompt suffixes to bypass safety measures, using techniques like greedy, gradient-based, or evolutionary methods~\citep{zou2023universal, lapid2023open}. Attacks can also exploit few-shot demonstrations, in-context learning~\citep{wang2023adversarial, qiang2023hijacking}, and multimodal inputs~\citep{qi2023visual}. Furthermore, LLMs can be used to generate and refine attacks against other LLMs~\citep{chao2023jailbreaking}, and attacks can be iterative, spanning multiple steps~\citep{wang2024footdoorunderstandinglarge}. Defenses are being developed to counter both human and automated attacks, which is increasingly important in an agentic AI future~\citep{zhang2024adversarial}.

Advances in post-training of base models will likely improve overall adversarial robustness. However, domain-specific recognition of malicious use may still require dedicated development and integration into scientific AI assistants. In AI systems employing iterative reasoning (e.g., request interpretation, hypothesis generation, internal thoughts, evaluation, user queries), each component must be tested independently.  This comprehensive testing should account for all potential failure modes, including the handling of unsafe queries, the safety of hypotheses (intermediate and final), and the accuracy of internal checks and filters.

\paragraph{Need for a comprehensive safety approach.}
Scientific AI assistants, like the co-scientist, require integrated, configurable guidelines within their safeguards. Developers should anticipate the complexity of this challenge and prioritize flexible safeguarding to rapidly incorporate community feedback. These semantic safeguards may need to be augmented by traditional software safety measures, including trusted testers, gradual feature rollouts, access controls, request logging, and flagging uncertain outputs for manual review.

Ensuring the safety of these systems, in line with existing AI safety guidelines~\citep{shneiderman2020bridging, anthropicscalingpolicy}, necessitates a multi-pronged approach. This includes:
\begin{itemize}
    \item Comprehensive threat modeling to identify potential vulnerabilities.
    \item Defense mechanisms for each identified threat.
    \item Extensive red-teaming and security testing.
    \item Rapid response procedures for quick resolution of issues including vulnerability patches.
    \item Continuous monitoring and performance tracking.
\end{itemize}

These considerations highlight the need for responsible development, governance and careful deployment of technologies designed for advancing science, appropriate safeguards and ethical guidelines and close compliance with applicable regulations. They also further underscore the need for broad community engagement and an inclusive development of best practices and recommendations around safe and ethical use for AI in science.

\paragraph{Current safeguards in the AI co-scientist.}
To mitigate these risks, the AI co-scientist currently employs the following safety mechanisms:
\begin{itemize}[leftmargin=1.5em,rightmargin=0em]
    \item\textbf{Reliance on public frontier LLMs.} The system utilizes established public Gemini 2.0 models, which already incorporate extensive safety evaluation and safeguards.
    \item\textbf{Initial research goal safety review.} Upon input, each research goal undergoes automated safety evaluation. Goals deemed potentially unsafe are rejected.
    \item\textbf{Research hypothesis safety review.} Generated hypotheses are reviewed for safety, even when the overarching research goal is deemed safe. Potentially unsafe hypotheses are excluded from the tournament, not developed any further, and are not presented to the user.
    \item\textbf{Continuous monitoring of research directions.} A meta-review agent provides an overview of research directions, enabling the AI co-scientist to continuously monitor for potential safety concerns and alert users if a research direction is detected as being potentially unsafe.
    \item\textbf{Explainability and transparency.} All system components, including the safety review, provide not only the final recommendation but also a detailed reasoning trace that can be used to justify and audit system decisions.
    \item\textbf{Comprehensive logging.} All system activities are logged and stored for future analysis and auditing.
    \item\textbf{Safety evaluations and red teaming.} A preliminary red teaming effort has been undertaken to ensure that the current implementation of unsafe research goal detection is robust and accurate. This evaluation includes an assessment of the system behavior when presented with 1,200 adversarial research goals across 40 distinct topic areas as discussed in \cref{sec:result_safety}.
    \item\textbf{Trusted tester program.} We are enthused by the early promise of the AI co-scientist system and believe it is important to more rigorously understand its strengths and limitations in many more areas of science and biomedicine; alongside making the system available to many more researchers who it is intended to support and assist. To facilitate this responsibly and with rigour, we will be enabling access to the system for scientists through a Trusted Tester Program to gather real-world feedback on the utility and robustness of the system.
\end{itemize}
Crucially, the AI co-scientist is designed to operate with continuous human expert oversight, ensuring that final decisions are always made by scientists exercising their expert judgment.


\vspace{-0.1cm}
\section{Future Work}
\vspace{-0.1cm}
\label{sec:future work}

\paragraph{Immediate improvements.}
The AI co-scientist is in its early development, with many opportunities for improvement. Immediate improvement opportunities include enhanced literature reviews, cross-checks with external tools, improved factuality checking, and increased citation recall to minimize missed relevant research. Coherence checks would also improve the system by reducing the burden of reviewing flawed hypotheses.

\paragraph{Expanded evaluations.}
Developing more objective evaluation metrics, potentially incorporating automated literature-based validation and simulated experiments, is a key area. Methods to mitigate biases or error patterns inherited from the base LLMs are also important, alongside analysis of the complementarity and optimal combination of different agent components.

A critical need is a larger-scale evaluation involving more subject matter experts with diverse, high-resolution research goals. Stress-testing the system at every level of resolution (from disease mechanisms to protein design, and expanding to other scientific disciplines) will reveal further areas for improvement. Finally, since laboratory resources are limited, improved evaluation frameworks could assist with hypothesis selection.

\paragraph{Capabilities advancements.}
Several opportunities remain to expand co-scientist's capabilities. Reinforcement learning could enhance hypothesis ranking, proposal generation, and evolutionary refinement.

Currently, the system assesses text from open-access publications but not images, data sets, or major public databases. Integrating these publicly available resources would significantly enhance the co-scientist's ability to generate and justify proposed hypotheses.

Future work will focus on handling more complex experimental designs, such as multi-step experiments and those involving conditional logic. Integrating co-scientist with laboratory automation systems could potentially create a closed-loop for validation and a grounded basis for iterative improvement. Exploring more structured user interfaces for providing feedback and insights from targeted user research studies, beyond free text, could improve the efficiency of human-AI collaboration in this paradigm.


\vspace{-0.1cm}
\section{Discussion}
\vspace{-0.1cm}
\label{sec:discussion}
Our study represents an initial foray into accelerating novel scientific discovery with agentic AI systems and here, we discuss some of the broader implications. The co-scientist iteratively refines its generated hypotheses through a generate, debate, evolve'' approach with specialized agents under the hood. This design creates a self-improving cycle for research hypothesis generation, as measured by automated evaluation metrics, and showcases the potential benefits of test-time compute scaling for scientific reasoning.

Instead of brute-force generation of a vast number of hypotheses and relying on volume to chance into a few potentially useful ones, the system is designed to mimic key aspects of the scientific reasoning method in an intelligent manner. As detailed in~\cref{sec:methods}, the co-scientist employs principled internal mechanisms, including scientific debates, tournaments, iterative refinement, and human feedback loops to progressively improve the quality of its proposals, and converge on high quality and well-reasoned hypotheses.

\paragraph{AI co-scientist novelty is grounded in prior evidence.} The AI co-scientist facilitates the generation of novel scientific hypotheses and uncovering new insights by synthesizing extensive literature and identifying latent relationships. While its primary utility in its current form may lie in enabling more incremental advancements — such as the computational repurposing of existing therapeutics — it may also be able to support exploratory, breakthrough research. When researchers define such open-ended research goals requiring complex and out-of-the box thinking, the system may produce outputs of varying confidence. Therefore, rigorous validation and critical appraisal by domain experts remain paramount. This system is intended to augment, not supplant, human scientific reasoning, empowering researchers to accelerate discovery while maintaining intellectual control over the generated insights. We further expand on the novelty aspects in the specific context of the applications considered in this work in~\cref{sec:glossary}.

\paragraph{Multiple experimental validations of novel co-scientist hypotheses.} Notably, this work demonstrates the validation of co-scientist hypotheses via experimental findings in multiple laboratories. In drug repurposing, co-scientist identifies novel candidates for AML that demonstrated \textit{in vitro} efficacy at clinically relevant concentrations, including the identification of new repurposing opportunities beyond current preclinical knowledge. For liver fibrosis, the co-scientist proposes new epigenetic treatment targets, with subsequent \textit{in vitro} experiments validating the anti-fibrotic activity of several suggested compounds, including an FDA-approved drug. In the realm of antimicrobial resistance, the co-scientist independently recapitulates a novel, unpublished finding regarding the mechanism of cf-PICI transfer between bacterial species. Early results over several queries of varying scientific complexity suggests the co-scientist has a potential to contribute to discovery within various biomedical domains.

\paragraph{Test-time compute scaling with scientific reasoning priors and inductive biases.} In the experiments reported here, the co-scientist did not require specialized pre-training, post-training, or a reinforcement learning framework. It leverages the capabilities of existing base LLMs, potentially benefiting from updates to those models without requiring retraining of the co-scientist system itself, which presents advantages of compute efficiency and generalizability. The system's architecture incorporates self-play, internal consistency checks, and tournament-based ranking, which support iterative hypothesis generation, evaluation, and refinement. This is reflected in the observed improvement in hypothesis quality over time. This self-evolution can be improved further by expanded tool use integration, including database queries, allowing the co-scientist to ground its proposals in existing knowledge and identify novel connections. In the future, we may leverage data and tournament ranking generated by the co-scientist itself as feedback to improve the whole system using reinforcement learning.

\paragraph{Frontier LLM advancements and the AI co-scientist.}
The frontier LLMs used within the co-scientist system have demonstrated a continuing trend of rapidly improved capabilities, including reasoning, logic, and also some aspects of scientific literature comprehension. As our system is designed to be model-agnostic, we hypothesize that further improvements in frontier LLMs will also result in improved co-scientist performance, and enable new avenues of research including optimal agentic use of tools.

\paragraph{Implications for drug repurposing and discovery.}
These advancements have significant implications for various biomedical and scientific domains. For example, the integration of the co-scientist into the drug candidate selection process represents a significant advancement in evidence-based drug repurposing. Beyond simple literature mining, the co-scientist maybe capable of synthesizing novel mechanistic insights by connecting molecular pathways, existing preclinical evidence, and potential therapeutic applications into structured, testable specific-aims. This capability is particularly valuable as it provides researchers with literature-supported rationales and suggests specific experimental approaches for validation. Notably, the co-scientist's structured output can be leveraged to develop comprehensive single-patient IND (Investigational New Drug) applications for compassionate use cases. By systematically presenting mechanistic evidence, relevant preclinical data, and proposed monitoring parameters, the co-scientist facilitates the development of well-reasoned treatment protocols for patients with refractory (treatment-resistant) disease who have exhausted standard therapeutic options and are ineligible for clinical trials. This application is particularly valuable in rare or aggressive diseases where traditional drug development timelines may not align with urgent patient needs. The platform's ability to rapidly generate evidence-based therapeutic hypotheses, complete with safety considerations and monitoring parameters, can help clinicians and regulatory bodies make informed decisions about compassionate use applications while maintaining scientific rigor.

The application of the co-scientist in drug repurposing presents a very compelling opportunity for orphan drugs, where extensive safety and clinical data already exist from their original rare disease indications. Given that Phase III clinical trials can cost hundreds of millions of dollars, repurposing these well-characterized therapeutics offers an efficient path to expanding treatment options across multiple diseases. This is especially relevant as orphan drugs often target fundamental biological pathways that may be relevant in other conditions, but these connections might not be immediately apparent through traditional research approaches. By systematically evaluating existing clinical data, safety outcomes, and mechanistic insights, the co-scientist can help identify promising new therapeutic applications while taking advantage of the investment already made in drug development and safety validation. This approach not only maximizes the utility of existing therapeutics but also provides a more rapid path to addressing unmet medical needs across a broader patient population.

More broadly, the co-scientist may also be potentially impactful throughout the entire drug discovery spectrum as evidenced by the early work on co-scientist assisted target discovery for liver fibrosis. 

\paragraph{Automation bias and impact on human scientific creativity.} 
Realizing the full potential of AI in biomedicine and science requires proactively addressing potential pitfalls. Over-reliance on AI-generated suggestions in collaborative AI systems could diminish critical thinking and increase homogeneity in research. Studies on AI's impact on creativity and ideation show mixed results; some suggest a risk of homogenization of ideas across populations~\citep{homogeneity_ideas}, while others are less conclusive~\citep{ashkinaze2024aiideasaffectcreativity}. The correlated success / failure modes of LLMs~\citep{wenger2025weredifferentweresame}, due to similar training data, could also artificially narrow scientific inquiry. Furthermore, AI system blind spots and performance variations across research domains must be considered. Therefore, scalable factuality and verification methods, alongside peer review and careful consideration of potential biases, are essential. Careful design and use of systems like the co-scientist are crucial to mitigate these risks.

\paragraph{AI as a catalyst for both scientific discovery and equity.}
Despite these risks, AI holds immense potential to democratize access to scientific information and accelerate discovery, particularly benefiting historically neglected and resource-constrained areas~\citep{sun2018addressing, george2023addressing}. In essence, AI can ``raise the tide'' of scientific progress, lifting all boats, especially those that have historically been left behind. Realizing this potential requires strategic investments and careful calibration of AI systems to foster ideation and innovation while minimizing false positives. This includes focusing on historically neglected research topics and addressing variations in performance across different scientific domains with varying amounts of pre-existing data. While current AI systems may tend to produce incremental ideas and research hypotheses, ongoing development aims to create systems capable of generating truly original, unorthodox and transformative scientific theories. Proactive mitigation of these challenges will ensure that AI serves as a powerful tool for all scientists, promoting a more equitable and innovative future for scientific explorations.

\clearpage

\section{Conclusion}
The AI co-scientist represents a promising step towards AI-assisted augmentation of scientists and acceleration of scientific discovery. Its ability to generate novel testable hypotheses across diverse scientific and biomedical domains, some supported by experimental findings, along with the capacity for recursive self-improvement with increasing compute, demonstrates the promise of meaningfully accelerating scientists’ endeavours to resolve grand challenges in human health, medicine and science. This innovation opens numerous questions and opportunities. Applying the empiric and responsible approach of science to the AI co-scientist system itself can thereby enable safe exploration of its undoubted potential, including how collaborative and human-centred AI systems might be able to augment human ingenuity and accelerate scientific discovery.

\vspace{12pt}
\subsubsection*{Acknowledgments}
We thank our teammates Subhashini Venugopalan, John Platt, Erica Brand, and Yun Liu for their detailed technical feedback on the manuscript. We thank Jakob T Rostoel, Cora Chmielowska and Jonasz B Patkowski from Imperial College London and Jakkapong Inchai, Weida Liu, and Wenlong Ren from Stanford University for providing expert feedback on the AI system introduced in this work, and the lab of Ravi Majeti from Stanford University for generously providing the AML cell lines used in this work. We thank Ritu Raman, Ryan Flynn, Charlie Hempstead, Lord Ara Darzi, Omar Abudayyeh, Jonathan Gootenberg, Nic Fishman, Jason Lequyer, Dan Leesman, Ravi Solanki, Dennis Gong and Ananthan Sadagopan for feedback on different aspects of the AI system and the work. We also thank Maen Abdelrahim, Ethan Burns, Preethi Prasad and Hanh Mai for their clinical expertise and expert evaluation.

We thank our teammates Thomas Wagner, Alessio Orlandi, Natasha Latysheva, Nir Kerem, Yaniv Carmel, Hussein Hassan Harrirou, Laurynas Tamulevičius and Grzegorz Glowaty for their technical support. We thank Taylor Goddu, Resham Parikh, Siyi Kou, Rachelle Sico, Amanda Ferber, Cat Kozlowski, Alison Lentz, KK Walker, Roma Ruparel, Jenn Sturgeon, Lauren Winer, Juanita Bawagan, Ed-Allt Graham, Tori Milner, MK Blake, Jack Mason, Erika Radhansson, Indranil Ghosh, Jay Nayar, Brian Cappy, Celeste Grade, Abi Jones, Laura Vardoulakis, Lizzie Dorfman, Ashmi Chakraborthy, Delia Williams-Falokun, Maggie Shiels, Kalyan Pamarthy, Sarah Brown,  Christian Wright, and S. Sara Mahdavi for their support and guidance during the course of this project. Finally, we thank Michael Brenner, Zoubin Ghahramani, Dale Webster, Joelle Barral, Michael Howell, Susan Thomas, Karen DeSalvo, Jason Freidenfelds, Ronit Levavi Morad, Vladimir Vuskovic, Ali Eslami, Anna Koivuniemi, Greg Corrado, Royal Hansen, Andy Berndt, Noam Shazeer, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean and Demis Hassabis for their support of this work.


\vspace{12pt}

\newpage
\setlength\bibitemsep{3pt}
\printbibliography
\balance
\clearpage
