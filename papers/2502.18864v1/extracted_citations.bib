@article{4928,
  author = {Tamara Williams and Marilyn Szekendi and Stephen Pavkovic and Wanda Clevenger and Julie Cerese},
  title = {The reliability of AHRQ Common Format Harm Scales in rating patient safety events.},
  year = {2015},
  journal = {J Patient Saf},
  volume = {11},
  pages = {52-59},
  month = {03/2015},
  issn = {1549-8425},
  doi = {10.1097/PTS.0b013e3182948ef9},
  language = {eng},
}

@article{achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{boiko2023autonomous,
  title={Autonomous chemical research with large language models},
  author={Boiko, Daniil A and MacKnight, Robert and Kline, Ben and Gomes, Gabe},
  journal={Nature},
  volume={624},
  number={7992},
  pages={570--578},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{brown2019superhuman,
  title={Superhuman AI for multiplayer poker},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={365},
  number={6456},
  pages={885--890},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{chaves2024tx,
  title={Tx-LLM: A Large Language Model for Therapeutics},
  author={Chaves, Juan Manuel Zambrano and Wang, Eric and Tu, Tao and Vaishnav, Eeshit Dhaval and Lee, Byron and Mahdavi, S Sara and Semturs, Christopher and Fleet, David and Natarajan, Vivek and Azizi, Shekoofeh},
  journal={arXiv preprint arXiv:2406.06316},
  year={2024}
}

@inproceedings{chen2024more,
  title={Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems},
  author={Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{chowdhery2022palm,
  title={{PaLM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{erhan2010does,
  title={Why does unsupervised pre-training help deep learning?},
  author={Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={201--208},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@misc{google2023palm2,
  author = {Google},
  title = {PaLM 2 Technical Report},
  howpublished = {\url{https://ai.google/static/documents/palm2techreport.pdf}},
  year = 2023
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{huang2024foundation,
  title={A foundation model for clinician-centered drug repurposing},
  author={Huang, Kexin and Chandak, Payal and Wang, Qianwen and Havaldar, Shreyas and Vaid, Akhil and Leskovec, Jure and Nadkarni, Girish and Glicksberg, Benjamin S and Gehlenborg, Nils and Zitnik, Marinka},
  journal={medRxiv},
  year={2024},
  publisher={Cold Spring Harbor Laboratory Preprints}
}

@article{ifargan2025autonomous,
  title={Autonomous LLM-Driven Researchâ€”from Data to Human-Verifiable Research Papers},
  author={Ifargan, Tal and Hafner, Lukas and Kern, Maor and Alcalay, Ori and Kishony, Roy},
  journal={NEJM AI},
  volume={2},
  number={1},
  pages={AIoa2400555},
  year={2025},
  publisher={Massachusetts Medical Society}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  journal={Farrar, Straus and Giroux},
  year={2011}
}

@article{krishnamurthy2022drug,
  title={Drug repurposing: a systematic review on root causes, barriers and facilitators},
  author={Krishnamurthy, Nithya and Grimshaw, Alyssa A and Axson, Sydney A and Choe, Sung Hee and Miller, Jennifer E},
  journal={BMC health services research},
  volume={22},
  number={1},
  pages={970},
  year={2022},
  publisher={Springer}
}

@article{liang2024can,
  title={Can large language models provide useful feedback on research papers? A large-scale empirical analysis},
  author={Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy Yi and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel Scott and Yin, Yian and others},
  journal={NEJM AI},
  volume={1},
  number={8},
  pages={AIoa2400196},
  year={2024},
  publisher={Massachusetts Medical Society}
}

@article{lin2023evolutionary,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{lu2024ai,
  title={The {AI} scientist: Towards fully automated open-ended scientific discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@article{merchant2023scaling,
  title={Scaling deep learning for materials discovery},
  author={Merchant, Amil and Batzner, Simon and Schoenholz, Samuel S and Aykol, Muratahan and Cheon, Gowoon and Cubuk, Ekin Dogus},
  journal={Nature},
  volume={624},
  number={7990},
  pages={80--85},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{morselli2021network,
  title={Network medicine framework for identifying drug-repurposing opportunities for COVID-19},
  author={Morselli Gysi, Deisy and Do Valle, {\'I}talo and Zitnik, Marinka and Ameli, Asher and Gan, Xiao and Varol, Onur and Ghiassian, Susan Dina and Patten, JJ and Davey, Robert A and Loscalzo, Joseph and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={19},
  pages={e2025581118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\'e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@article{nguyen2024sequence,
  title={Sequence modeling and design from molecular to genome scale with {Evo}},
  author={Nguyen, Eric and Poli, Michael and Durrant, Matthew G and Kang, Brian and Katrekar, Dhruva and Li, David B and Bartie, Liam J and Thomas, Armin W and King, Samuel H and Brixi, Garyk and others},
  journal={Science},
  volume={386},
  number={6723},
  pages={eado9336},
  year={2024},
  publisher={American Association for the Advancement of Science}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{pushpakom2019drug,
  title={Drug repurposing: progress, challenges and recommendations},
  author={Pushpakom, Sudeep and Iorio, Francesco and Eyers, Patrick A and Escott, K Jane and Hopper, Shirley and Wells, Andrew and Doig, Andrew and Guilliams, Tim and Latimer, Joanna and McNamee, Christine and others},
  journal={Nature reviews Drug discovery},
  volume={18},
  number={1},
  pages={41--58},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{ruffolo2024design,
  title={Design of highly functional genome editors by modeling the universe of CRISPR-Cas sequences},
  author={Ruffolo, Jeffrey A and Nayfach, Stephen and Gallagher, Joseph and Bhatnagar, Aadyot and Beazer, Joel and Hussain, Riffat and Russ, Jordan and Yip, Jennifer and Hill, Emily and Pacesa, Martin and others},
  journal={bioRxiv},
  pages={2024--04},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}

@article{saab2024capabilities,
  title={Capabilities of gemini models in medicine},
  author={Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and others},
  journal={arXiv preprint arXiv:2404.18416},
  year={2024}
}

@article{schmidgall2025agent,
  title={Agent laboratory: Using llm agents as research assistants},
  author={Schmidgall, Samuel and Su, Yusheng and Wang, Ze and Sun, Ximeng and Wu, Jialian and Yu, Xiaodong and Liu, Jiang and Liu, Zicheng and Barsoum, Emad},
  journal={arXiv preprint arXiv:2501.04227},
  year={2025}
}

@article{shaw2024protex,
  title={ProtEx: A Retrieval-Augmented Approach for Protein Function Prediction},
  author={Shaw, Peter and Gurram, Bhaskar and Belanger, David and Gane, Andreea and Bileschi, Maxwell L and Colwell, Lucy J and Toutanova, Kristina and Parikh, Ankur P},
  journal={bioRxiv},
  pages={2024--05},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{singhal2022large,
  title={Large Language Models Encode Clinical Knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={arXiv preprint arXiv:2212.13138},
  year={2022}
}

@article{skarlinski2024language,
  title={Language agents achieve superhuman synthesis of scientific knowledge},
  author={Skarlinski, Michael D and Cox, Sam and Laurent, Jon M and Braza, James D and Hinks, Michaela and Hammerling, Michael J and Ponnapati, Manvitha and Rodriques, Samuel G and White, Andrew D},
  journal={arXiv preprint arXiv:2409.13740},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{swanson2024virtual,
  title={The virtual lab: {AI} agents design new {SARS}-{CoV}-2 nanobodies with experimental validation},
  author={Swanson, Kyle and Wu, Wesley and Bulaong, Nash L and Pak, John E and Zou, James},
  journal={bioRxiv},
  pages={2024--11},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}

@article{taylor2022galactica,
  title={{Galactica}: A Large Language Model for Science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{tu2023genetic,
  title={Genetic Discovery Enabled by A Large Language Model},
  author={Tu, Tao and Fang, Zhouqing and Cheng, Zhuanfen and Spasic, Svetolik and Palepu, Anil and Stankovic, Konstantina M and Natarajan, Vivek and Peltz, Gary},
  journal={bioRxiv},
  year={2023},
  publisher={Cold Spring Harbor Laboratory Preprints}
}

@article{tu2024towards,
  title={Towards conversational diagnostic {AI}},
  author={Tu, Tao and Palepu, Anil and Schaekermann, Mike and Saab, Khaled and Freyberg, Jan and Tanno, Ryutaro and Wang, Amy and Li, Brenna and Amin, Mohamed and Tomasev, Nenad and others},
  journal={arXiv preprint arXiv:2401.05654},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wong2024discovery,
  title={Discovery of a structural class of antibiotics with explainable deep learning},
  author={Wong, Felix and Zheng, Erica J and Valeri, Jacqueline A and Donghia, Nina M and Anahtar, Melis N and Omori, Satotaka and Li, Alicia and Cubillos-Ruiz, Andres and Krishnan, Aarti and Jin, Wengong and others},
  journal={Nature},
  volume={626},
  number={7997},
  pages={177--185},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zambaldi2024novo,
  title={De novo design of high-affinity protein binders with AlphaProteo},
  author={Zambaldi, Vinicius and La, David and Chu, Alexander E and Patani, Harshnira and Danson, Amy E and Kwan, Tristan OC and Frerix, Thomas and Schneider, Rosalia G and Saxton, David and Thillaisundaram, Ashok and others},
  journal={arXiv preprint arXiv:2409.08022},
  year={2024}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{zitnik2018modeling,
  title={Modeling polypharmacy side effects with graph convolutional networks},
  author={Zitnik, Marinka and Agrawal, Monica and Leskovec, Jure},
  journal={Bioinformatics},
  volume={34},
  number={13},
  pages={i457--i466},
  year={2018},
  publisher={Oxford University Press}
}

