\section{Related Works}
\subsection{Reasoning models and test-time compute scaling}
The modern revolution in foundation AI models~\citep{bommasani2021opportunities} and large language models (LLMs) has been largely driven by advances in pre-training techniques~\citep{erhan2010does, radford2018improving}, leading to breakthroughs in models like the GPT and Gemini family~\citep{team2023gemini, achiam2023gpt}. These models, trained on increasingly massive internet-scale and multimodal datasets, have demonstrated impressive abilities in language understanding and generation leading to breakthrough performance in a variety of benchmarks~\citep{chowdhery2022palm, google2023palm2}.  However, a key area of ongoing development is enhancing their \textit{reasoning} capabilities. This has led to the emergence of ``reasoning models'' which go beyond simply predicting the next word and instead attempt to mimic human thought processes~\citep{wei2022chain}. One promising direction in this pursuit is the test-time compute paradigm.  This approach moves beyond solely relying on the knowledge acquired during pre-training and allocates additional computational resources during inference to enable System-2 style thinking---slower deliberate reasoning to reduce uncertainty and progress optimally towards the goal~\citep{kahneman2011thinking}. This concept emerged with early successes such as AlphaGo~\citep{silver2016mastering}, which used Monte Carlo Tree Search (MCTS) to explore game states and strategically select moves, and Libratus~\citep{brown2019superhuman}, which employed similar techniques to achieve superhuman performance in poker. This paradigm has now found applications in LLMs, where increased compute at test-time allows for more thorough exploration of possible responses, leading to improved reasoning and accuracy~\citep{wei2022chain, yao2024tree, zelikman2022star, chen2024more, snell2024scaling,4928, muennighoff2025s1, tu2024towards}. Recent advancements, like the Deepseek-R1 model~\citep{guo2025deepseek}, further demonstrate the potential of test-time compute by leveraging reinforcement learning to refine the model's ``chain-of-thought'' and enhance complex reasoning abilities over longer horizons. In this work, we propose a significant scaling of the test-time compute paradigm using inductive biases derived from the scientific method to design a multi-agent framework for scientific reasoning and hypothesis generation without any additional learning techniques.

\subsection{AI-driven scientific discovery}
AI-driven scientific discovery represents a paradigm shift in how research is conducted across various scientific domains. Recent advancements, particularly the development of large deep learning and generative models, have cemented AI's role in scientific discovery. This is best exemplified by AlphaFold 2's remarkable progress in the grand challenge of protein structure prediction, which has revolutionized structural biology and opened new avenues for drug discovery and materials science~\citep{jumper2021highly}. Other notable examples include the development of novel antibiotics, protein binder design,  and material discovery with AI~\citep{wong2024discovery, zambaldi2024novo, merchant2023scaling}. 

Building on these successes with specialized, bespoke AI models, there has been recent work exploring the even more ambitious goal of fully integrating AI, especially modern LLM-based systems, into the complete research workflow, from initial hypothesis generation all the way to manuscript writing. This end-to-end integration represents a significant shift, presenting both unprecedented opportunities and significant challenges as the field moves beyond specialized AI tools toward realizing the potential of AI as an active collaborator, or even, as some envision, a nascent ``AI scientist''~\citep{lu2024ai, schmidgall2025agent}.

As an example of this shift, Liang et al.~\cite{liang2024can} directly assessed the utility of LLMs for providing feedback on research manuscripts. Through both a retrospective analysis of existing peer reviews and a prospective user study, they demonstrated the significant concordance between LLM-generated feedback and that of human reviewers. Their study, using GPT-4~\citep{openai2023gpt4}, found that a majority of researchers perceived LLM-generated feedback as helpful, and in some instances, even more beneficial than feedback from human colleagues. However, while valuable, their work focuses solely on the feedback stage of the scientific process, leaving open the question of how LLMs might be integrated into the full research cycle, from hypothesis formation to experimental validation and manuscript writing.

Another effort embodying this shift is PaperQA2~\citep{skarlinski2024language}, an AI agent for scientific literature search and summarization. The authors claimed to surpass PhD and postdoc researchers on multiple literature research tasks, as measured both by performance on objective benchmarks and human evaluations. While the system is a useful for synthesizing information, it does not engage in scientific reasoning for novel hypothesis generation.

HypoGeniC, a system proposed by Zhou et al.~\cite{zhou2022least}, tackles hypothesis generation by iteratively refining hypotheses using LLMs and a multi-armed bandit-inspired approach. The process begins with a small set of examples, from which initial hypotheses are generated. These hypotheses are then iteratively updated through exploration and exploitation, guided by a reward function based on training accuracy.  This refined set of hypotheses is subsequently used to construct an interpretable classifier. However, the method's reliance on retrospective data for evaluation means the degree to which the system can generate truly novel hypotheses remains an open question. Furthermore, the system lacks end-to-end validation beyond subjective human evaluations.

Ifargan et al.~\cite{ifargan2025autonomous} present ``data-to-paper'', a platform that systematically guides multiple LLM and rule-based agents to generate research papers, with automated feedback mechanisms and information tracing for verification. However, the evaluations are limited to recapitulating existing peer-reviewed publications and its unclear if the system can generate truly novel, yet grounded hypothesis and research proposals.

Virtual Lab~\citep{swanson2024virtual} is another closely related work. Here, the authors propose a team of LLM agents with a ``principal investigator'' LLM guiding a team of specialized LLM agents to solve a scientific problem. The LLM team receives high level human supervision. The authors demonstrate the utility of their work by leveraging Virtual Lab to design nanobody binders to recent variants of SARS-CoV-2 with experimental validation. While similar in spirit, there are significant design differences to our approach and the generality of the system remains unclear.

Boiko et al.~\citep{boiko2023autonomous} introduced ``Coscientist'', a multi-agent system powered by GPT-4, designed for autonomous execution of complex chemical experiments. This system integrates capabilities such as web and document searching, and code execution, to facilitate independent experimental design, planning, and execution. In addition to similar sounding names, both ``Coscientist'' and our system share the overarching goal of accelerating scientific discovery through AI. However, there are several important distinctions. Notably, ``Coscientist'' is quite narrowly focused on chemical research while ours is much broadly applicable across science. Secondly, our system has important technical innovations that lead to a self-improving system that can uncover new, original knowledge while their approach is a more vanilla-stitching of GPT-4 based agents. Finally, despite the name, ``Coscientist'' prioritizes a high degree of autonomy in experimental execution, directly interfacing with laboratory hardware. Our system, instead, is explicitly designed as a collaborative tool, emphasizing a ``scientist-in-the-loop'' approach and centers on the more cognitive aspects of the research process.

Finally, Lu et al.~\cite{lu2024ai} propose ``The AI Scientist'', a fully automated system designed to conduct research using multiple collaborating LLM agents. These agents handle all stages of the research process, from defining research problems and conducting literature reviews to designing and executing experiments, and even writing up the results. The design shares similarities with our work---the key differences being our focus on the scaling of the test-time compute paradigm to generate high quality hypotheses and research proposals. Secondly, their proposed system has limited automated evaluations; in contrast, our work has a combination of automated, human expert and end-to-end wet lab validations. Finally, our goal is to not to automate scientific discovery, rather to build a helpful AI collaborator for scientists.

\subsection{AI for biomedicine}
More broadly, large AI models are increasingly demonstrating their potential in biomedical science. Both general purpose (GPT-4, Gemini) and specialized LLMs (Med-PaLM, Med-Gemini, Galactica, Tx-LLM) have shown strong performance on biomedical reasoning and question-answering benchmarks~\citep{team2023gemini, achiam2023gpt, singhal2022large, saab2024capabilities, taylor2022galactica, chaves2024tx}. Beyond benchmarks, Med-PaLM 2, was successfully applied to identify causative murine genetic factors for traits such as diabetes, cataracts, and hearing loss~\cite{tu2023genetic}---an early example of hypothesis generation and LLM-assisted discovery. We have also seen the exciting development of specialized foundation and large language models trained on DNA, RNA and protein sequences with a variety of applications~\citep{nguyen2024sequence, lin2023evolutionary, ruffolo2024design, shaw2024protex}. Although AI in biology and medicine often necessitates specialization, the rapid progress of frontier AI models has blurred the distinction. As these models grow in scale, data diversity, and complexity, they continue to achieve breakthroughs in areas once thought to require domain-specific AI. Our co-scientist system, with its modular multi-agent architecture, is flexibly designed to build on top of these advancements in general-purpose frontier AI models and leverage specialized AI models as tools to enhance the capabilities.
 
Drug repurposing is an important area of validation experiments in this work. The traditional approach to this task requires both computational and experimental approaches and a comprehensive understanding of disease-drug interactions~\cite{pushpakom2019drug, krishnamurthy2022drug}. While methods like knowledge graphs with graph convolutional networks have shown promise~\cite{zitnik2018modeling, morselli2021network}, their applicability is limited by the initial knowledge graph's scope.  TxGNN~\cite{huang2024foundation}, an example of a specialized biomedical foundation model with a graph based approach, addresses ``zero-shot'' repurposing for novel diseases but remains dependent on the underlying knowledge graph's quality and lacks sufficient scalability and explainability. Furthermore, no end-to-end validations of the model predictions were reported in the study. In contrast, our work, leveraging state-of-the-art LLMs in the co-scientist setup, is more scalable. We report a combination of expert evaluations and wet-lab experiments to validate the system predictions.

\input{method}

\clearpage
\input{results}