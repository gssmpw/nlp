\section{Related Work}

The advent of LLMs has revolutionized synthetic data generation, offering significant advantages over traditional methods. LLMs excel in generating coherent and human-like text, making them effective tools for creating high-quality datasets~\citep{survey_llmsdrivensyntheticdata}. Compared to manual data collection, LLMs provide notable benefits in flexibility, efficiency, and scalability. They can tailor datasets to specific needs by adjusting prompts and conditions~\citep{tinystories}, significantly reduce annotation costs~\citep{self_improve}, and automate the training pipeline, enabling broader application across multiple domains~\citep{sungen}.

Despite these advantages, ensuring the quality and relevance of LLM-generated datasets requires robust generation techniques and curation strategies. Below, we review key methods for data generation, focusing on prompt engineering and multi-step generation, followed by strategies for data curation and distribution alignment.

\subsection{LLM-Based Data Generation Methods}

\paragraph{Prompt Engineering.}  
Prompt engineering is critical for controlling the quality and diversity of synthetic data. Effective prompts typically define tasks clearly, specify generation conditions (e.g., themes or styles), and include in-context demonstrations, which help guide LLMs toward accurate outputs~\citep{ChatGPTOC, best_practice}. For instance, \citet{wang2023let} demonstrated how condition-based prompts improve stylistic diversity, while \citet{li2023synthetic} highlighted the role of examples in enhancing task alignment.

\paragraph{Multi-Step Generation.}  
Multi-step generation addresses complex data needs by breaking the process into sub-tasks. Sample-wise decomposition divides data into smaller parts for step-by-step generation, improving coherence, as shown by \citet{he2023annollm}. Dataset-wise decomposition dynamically adjusts generation conditions to enhance diversity and coverage~\citep{wang2023let}. Our method builds on these approaches by introducing systematic condition controls to further improve data quality and diversity.

\subsection{Data Curation and Distribution Alignment}

While LLMs excel at generating diverse data, synthetic datasets often contain noise or distributional biases that can hinder downstream performance~\citep{regen}. To address these issues, curation strategies such as sample filtering and label enhancement have been proposed. Sample filtering uses heuristic metrics like confidence scores or influence functions to identify high-quality samples~\citep{seedat2023curated, progen}. Label enhancement techniques, such as knowledge distillation~\citep{xiao2023freeal}, refine labels\citep{wan2024tnt} and reduce annotation errors\citep{li2023coannotating}. 

To align synthetic data distributions with real-world data, methods like Maximum Mean Discrepancy (MMD) have been employed~\citep{survey_llmsdrivensyntheticdata}. Our approach integrates systematic sample selection and MMD-based alignment, ensuring the synthetic data closely resembles real-world distributions while maintaining high quality.


% In summary, while LLMs provide powerful tools for synthetic data generation, their effectiveness depends on robust generation and curation strategies. Our work advances the state-of-the-art by combining prompt-based and multi-step generation with distribution alignment, enabling the creation of high-quality synthetic datasets for downstream tasks.