
\section{Introduction}
Despite the rapid development of large language models (LLMs), there remains a demand in the industry for smaller online language models tailored to high-latency scenarios~\cite{hsieh2023distilling, jia2024erase, wang2023single, du2024lightcs, du2024tutorial, zhao2024retrievable, dai2023uncovering, dai2024modeling} like search engines~\cite{khattab2020colbert}. These models are typically trained on pre-constructed datasets for online services. 
However, real-world data collection often suffers from various biases like selection bias~\cite{wang2020exposure, arora2022exposure, dai2023dually} and long-tail issues~\cite{dai2023long, wu2008information}, and expensive manual cost~\cite{daniel2018quality, dai2024cocktail}. Synthetic data generation has the potential to mitigate these biases~\cite{shahul2024bias, lyu2022semi}, enhance data diversity~\cite{feng2020genaug}, and ultimately improve model generalization and accuracy.

To fully harness the potential of synthetic data, it is essential to define how synthetic data can be high quality for a specific task? High quality can be assessed along various dimensions, such as low noise and fairness. However, in profit-driven industrial applications like search engines, we prioritize improving model accuracy as the objective. Based on Murphy’s research \cite{murphy2012machine}, we propose the following definition of high quality: \textbf{the optimal dataset is that which most closely matches the distribution under which the model will be evaluated}. Under this definition, synthetic high-quality data requires matching real data in some key attributes.

\begin{figure}[!t]
  \centering
    \includegraphics[width=1\linewidth]{figures/Motivation_fig.pdf}
  % \vspace{-0.1in}
  \caption{A case on how LLM synthetic samples misalign with human-generated samples.}
  \vspace{-0.15in}
  \label{fig.motivation}
  \Description{}
\end{figure}


Many existing methods adopted distribution matching as the objective function and used all or a subset of the real dataset to train a generative model that approximates the real data distribution, and then synthesizes data based on this model. For instance, RelGAN ~\cite{RelGAN} designs a generative adversarial network architecture for text data generation tasks, ensuring that the generated data closely aligns with the original data distribution. Similarly, FewGen~\cite{fewgen} fine-tunes an autoregressive pre-trained language model on a small sample dataset and employs it as a generator to synthesize a large number of new training samples, thereby augmenting the original training set. However, such methods require training a dedicated generative model for each scenario, incurring high manual costs. Additionally, due to the risk of catastrophic forgetting~\cite{ramasesh2021effect}, these generators may lose their in-context learning capability. 

Benefiting from massive high-quality training data and carefully designed training processes~\cite{ouyang2022training}, LLMs have developed strong instruction following and in-context learning abilities that can generate responses for diverse tasks~\cite{brown2020language}. This has prompted academia and industry to use LLMs for zero-shot data synthetic to train their domain-specific models. For example, GPT-3Mix ~\cite{GPT3Mix} utilizes large-scale language models to generate mixed samples, enhancing text datasets. Additionally, AttrPrompt ~\cite{attriprompt} generates training data through diverse attribute prompts, improving model performance while reducing data bias.

However, experiments (see section \ref{exp:datamismatch}) show two issues with the data synthesized by LLMs from the distribution matching perspective. First, LLM's zero-shot generation results cannot cover all real data's linguistic attributes (especially those ``imperfect'' ones like typos or incomplete sentences). Secondly, there is a significant discrepancy in the proportion of linguistic attributes between the LLMs synthetic data and the real data ~\cite{li2023synthetic, gao2025samplellm}. Both are primarily due to LLM’s limitations in understanding long text inputs, which prevents it from incorporating distributions of real data and previously synthetic data. These issues result in a considerable divergence between LLM-synthetic and real data, meaning that directly adding these synthetic data to the real data may not not improve model performance.

To address these challenges, we designed \textbf{SynAlign} (\textbf{Syn}thetic data generation and distribution \textbf{Align}ment framework), which incorporates three modules during LLMs' synthetic data generation process: Exploration-aware Sampling, Latent Attribute Reasoning, and Synthetic Distribution Alignment. 
Exploration-aware sampling is applied to the demonstration sampling process. It utilizes a Gaussian Process model as the samples' uncertainty-tracker. Samples with the highest uncertainty will be selected as demonstrations and their uncertainty will be updated after generation. With this process, data sampling module can efficiently explore real data distribution.
Selected demonstrations will be fed into the Latent Attribute Reasoning module to summarize key attributions covering language contents, styles, etc. Afterward, new data are synthesized based on these generalized latent attributes.
After the data synthesis process, the Synthetic Distribution Alignment module employs a post-training approach to assign sampling weights for each synthetic sample by minimizing Maximum Mean Discrepancy between synthetic data and real data. After resampling the synthetic data based on sampling weights, we obtain the final synthetic data that more closely aligns with real data distribution.

Extensive experiments demonstrate that our method synthesizes high-quality data more efficiently, requiring fewer tokens. Compared to existing data generation algorithms, the synthetic data generated by SynAlign consistently achieves superior performance. The main contributions of this paper are as follows:

$\bullet$ We designed an Exploration-aware Sampling module that can efficiently explore all language attributes in real data. Data synthesized with latent attributes extracted from these samples could cover all real data's distribution.  

$\bullet$ We designed a Synthetic Distribution Alignment module to post-align synthetic data distribution with real data distribution.

$\bullet$  Extensive experiments are conducted and demonstrate our method can generate high quality synthetic samples efficiently.
