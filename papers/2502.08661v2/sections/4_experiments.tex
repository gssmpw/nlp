\section{Experiments}
\subsection{Experimental Setup}

% We evaluate our proposed method on three widely-used text classification datasets: SST-2~\cite{sst2}, AGNEWS~\cite{agnews}, and Amazon\cite{amazon}. These datasets span different domains and tasks, including sentiment analysis, topic classification, and product review classification. Each dataset presents unique challenges, such as class imbalance and diverse linguistic styles, making them suitable for evaluating the effectiveness of synthetic data generation methods. Table~\ref{tab:data_info} summarizes the key characteristics of these datasets, including the number of classes, sample sizes, and domain-specific features. %including the original training and test set sizes, the number of generated synthetic samples, and the final sampled subsets used for training.

We evaluate our method on three widely-used text classification datasets: SST-2~\cite{sst2}, AGNEWS~\cite{agnews}, and Amazon~\cite{amazon}, covering diverse tasks such as sentiment analysis, topic classification, and product review classification. These datasets are chosen for their varying challenges, including class imbalance and linguistic diversity, making them ideal for testing synthetic data generation methods. Table~\ref{tab:data_info} summarizes their key characteristics.

To assess model performance, we adopt two standard metrics: Accuracy (Acc), which measures the proportion of correctly classified samples, and the F1 Score, calculated as the macro-average across all classes. The latter is particularly useful for datasets with imbalanced class distributions. %Additionally, to assess the efficiency of synthetic data generation, we report the \textbf{cost per 1,000 samples} generated via ChatGPT, highlighting the financial overhead associated with the generation process.

We compare it against several baselines, as summarized below.
\begin{itemize}
    \item \textbf{Gold}: Models are trained solely on the original dataset without any synthetic data. 
    \item \textbf{SimPrompt}~\cite{chen2023mixturesoftpromptscontrollable}: Augments the original dataset with $\zeta$ synthetic samples generated using simple class-conditional prompts.
    \item \textbf{AttrPrompt}~\cite{attriprompt}: Uses attribute-rich prompts to generate diverse synthetic data.
    \item \textbf{SynAlign(all)}: Trains models on the full synthetic dataset $D_{\text{gen}}$ without distribution alignment. 
    \item \textbf{SynAlign(random)}: Trains models on the original dataset combined with $\zeta$ randomly selected samples from $D_{\text{gen}}$. 
    \item \textbf{SynAlign(mmd)}: Our proposed method, which selects $\zeta$ samples from $D_{\text{gen}}$ using the MMD-based sampling approach described in Section 3.
\end{itemize}

% For implementation, we fine-tune pre-trained models for text classification tasks. We primarily use \textbf{BERT-base-uncased}\cite{bert} and \textbf{DistilBERT}~\cite{sanh2019distilbert} to examine the generalization of our method across models of different parameter scales. These models are fine-tuned with the original datasets augmented by synthetic data generated using our method. All models are optimized using the standard \textbf{cross-entropy loss}, and training follows common fine-tuning practices with hyperparameter settings consistent across datasets.% By incorporating DistilBERT, we further evaluate the effectiveness of our approach on a more lightweight architecture, demonstrating its applicability to models with varied computational requirements.

% For implementation, we fine-tune pre-trained models for text classification tasks. Specifically, we use BERT-base-uncased\cite{bert} and DistilBERT\cite{sanh2019distilbert} to examine the generalization of our method across models of different parameter scales. Synthetic data $D_{\text{gen}}$ is generated using the pipeline described in Section 3, including Exploration-aware Sampling and Latent Attribute Reasoning. All models are optimized using the Adam optimizer with a learning rate of $1e^{-5}$, batch size of 16, and a maximum of 5 epochs. For evaluation, we use early stopping based on validation accuracy. Training hyperparameters are kept consistent across datasets for fair comparison.


We fine-tune pre-trained models, including DistilBERT ~\cite{sanh2019distilbert} and BERT-base-uncased ~\cite{bert}, to evaluate the generalization of our method across different model scales. The synthetic data $D_{\text{gen}}$ is generated following the pipeline described in Section \ref{sec_method}. Details on implementation, including training hyperparameters and optimization, are provided in the \ref{apdx:imp_detail}.

\subsection{Main Experimental Results}

\begin{table*}[t]
\centering
\caption{Main results across three datasets. We report Accuracy (Acc) and F1 score for each method and model. The best results are highlighted in bold.}
\label{tab:main_result}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{llcccccc}
    \toprule
    & & \multicolumn{2}{c}{\textbf{SST-2}} & \multicolumn{2}{c}{\textbf{AGNews}} & \multicolumn{2}{c}{\textbf{Amazon}} \\
    \textbf{Method} & \textbf{Model} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} \\
    \midrule
    \textbf{LLM zero-shot} &  & 0.9351 & 0.9335 & 0.8232 & 0.8264 & 0.7556 & 0.732 \\
    \midrule\midrule
    \textbf{Gold} & BERT-base-uncased & 0.9248 & 0.9246 & 0.9286 & 0.9398 & 0.8204 & 0.7961 \\
     & DistilBERT & 0.9044 & 0.9045 & 0.9260 & 0.9248 & 0.8053 & 0.8053 \\
     \hline
    \textbf{SimPrompt} & BERT-base-uncased & 0.9264 & 0.9259 & 0.9403 & 0.9402 & 0.8274 & 0.8113 \\
     & DistilBERT & 0.9148 & 0.9164 & 0.9346 & 0.9339 & 0.8165 & 0.8165 \\
     \hline
    \textbf{AttrPrompt} & BERT-base-uncased & 0.9260 & 0.9264 & 0.9441 & 0.9417 & 0.8305 & 0.8142 \\
     & DistilBERT & 0.9231 & 0.9215 & 0.9395 & 0.9395 & 0.8262 & 0.8262 \\
     \hline
    \textbf{SynAlign(all)} & BERT-base-uncased & 0.9292 & 0.9299 & 0.9460 & 0.9441 & 0.8374 & 0.8244 \\
     & DistilBERT & 0.9252 & 0.9212 & 0.9456 & 0.9451 & 0.8351 & 0.8351 \\
     \hline
    \textbf{SynAlign(random)} & BERT-base-uncased & 0.9286 & 0.9322 & 0.9429 & 0.9455 & 0.8204 & 0.8031 \\
     & DistilBERT & 0.9203 & 0.9207 & 0.9441 & 0.9442 & 0.8296 & 0.8296 \\
     \hline
     % \rowcolor{gray!30} 
    \textbf{SynAlign(mmd)} & BERT-base-uncased & \textbf{0.9330} & \textbf{0.9352} & \textbf{0.9475} & \textbf{0.9470} & \textbf{0.8381} & \textbf{0.8266} \\
     & DistilBERT & \textbf{0.9282} & \textbf{0.9211} & \textbf{0.9464} & \textbf{0.9433} & \textbf{0.8312} & \textbf{0.8312} \\
    \bottomrule
\end{tabular}}
\end{table*}

% We evaluate the performance of our proposed method and several baselines on three datasets: \textbf{SST-2}, \textbf{AGNEWS}, and \textbf{Amazon}. 
Table~\ref{tab:main_result} reports results in terms of \textbf{Accuracy (Acc)} and \textbf{F1 score} for both BERT-base-uncased\cite{bert} and DistilBERT\cite{distilbert}.
The results show that augmenting the original dataset with synthetic data consistently improves performance over the \textbf{Gold} baseline.
% For example, on SST-2, augmenting the dataset yields an average Accuracy improvement of 0.78\% across all methods.
This trend is consistent across datasets and models, demonstrating the benefit of synthetic data augmentation. Importantly, \textbf{DistilBERT}, despite its smaller size, achieves competitive results compared to BERT-base-uncased, highlighting the scalability of our method to lightweight models.

Among baseline methods, \textbf{AttrPrompt} generally outperforms \textbf{SimPrompt}, likely due to its ability to generate more diverse and representative synthetic samples. However, both are consistently surpassed by our proposed \textbf{SynAlign} approach, which uses exploration-aware sampling and distribution alignment to select high-quality synthetic data. Within SynAlign, the MMD-based sampling strategy (\textbf{SynAlign(mmd)}) delivers the best results, outperforming \textbf{SynAlign(all)} (which uses all generated samples) and \textbf{SynAlign(random)} (which selects samples randomly). 
% For instance, on Amazon, SynAlign(mmd) improves Accuracy by 0.77\% and F1 Score by 0.24\% compared to SynAlign(all) with BERT-base-uncased.
These results highlight the importance of informed sample selection, as not all synthetic data contributes equally to performance.

\textbf{SynAlign(mmd)} consistently achieves the highest Accuracy and F1 scores across datasets and models. For example, on SST-2, SynAlign(mmd) with BERT-base-uncased achieves 93.30\% Accuracy, outperforming the Gold baseline (92.48\%) and all other augmentation methods. Similarly, on AGNEWS and Amazon, SynAlign(mmd) delivers superior results, demonstrating robustness across different tasks and domains.

Overall, the experimental results confirm the effectiveness of \textbf{SynAlign(mmd)} in leveraging synthetic data for model improvement. By selectively augmenting datasets with well-aligned samples, our method achieves consistent performance gains across datasets, domains, and model architectures, while maintaining scalability to lightweight models like DistilBERT.



% The experimental results validate the effectiveness of SynAlign(mmd) in leveraging synthetic data to improve model performance. By selectively augmenting the original dataset with well-aligned generated samples, our method achieves consistent improvements across datasets, domains, and model architectures.


\subsection{Generated Data Analysis}
\label{exp:datamismatch}
% To evaluate the quality of the generated data, we analyze two key aspects: distributional alignment with the original dataset and vocabulary diversity. These aspects are directly tied to the goals of our method, as they assess how well the synthetic data reflects the original data distribution and its linguistic richness.


% We evaluate the quality of the generated data based on two aspects: \textbf{distributional alignment} with the original dataset and \textbf{vocabulary diversity}. These metrics assess how well the synthetic data matches the original distribution and its linguistic richness.


\noindent \textbf{Distributional Alignment:}  
We measure the alignment between the original and generated data using \textbf{Wasserstein distance}, which quantifies the cost of transforming one distribution into another. Lower values indicate better alignment. Sentence embeddings are extracted using a pre-trained BERT model and visualized with t-SNE for qualitative analysis.

Figure~\ref{fig:good_distribution} illustrates the t-SNE visualizations for SST-2, comparing the original dataset (\textbf{Gold}) with data generated by different methods.
%As shown in Figure~\ref{fig:good_distribution}(a), data generated by \textbf{AttrPrompt} forms clusters far from the original data, indicating poor alignment and a significant distributional shift.
In Figure~\ref{fig:good_distribution}(a), AttrPrompt's generated data forms tight clusters far from the original data, suggesting that it captures only a limited subset of the original linguistic patterns.
In contrast, Figure~\ref{fig:good_distribution}(b) shows that \textbf{SynAlign (MMD sampling)} generates data that closely aligns with the original distribution, covering a broader and more representative area of the original data space.


Quantitatively, Table~\ref{tab:was_dis} reports the Wasserstein distances for all datasets. \textbf{SynAlign (random)} achieves lower distances compared to \textbf{SimPrompt} and \textbf{AttrPrompt}, while \textbf{SynAlign (MMD)} consistently achieves the best alignment, demonstrating its effectiveness in selecting synthetic samples that better reflect the original data distribution.



\begin{figure}[tbp]
    \centering
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/good_dis/Gold_Attrprompt.pdf}} 
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/good_dis/Gold_SynAlign_mmd.pdf}} 
    \caption{t-SNE visualization of sentence embeddings from SST-2. (a) Comparison between Gold and AttrPrompt; (b) Comparison between Gold and SynAlign (MMD). SynAlign (MMD) achieves better alignment.}
    \label{fig:good_distribution}
\end{figure}

\begin{table}[!t]
\centering
\caption{Wasserstein Distance between original and generated datasets. Lower values indicate better alignment.}
\label{tab:was_dis}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
    \hline
    \textbf{Data to compare} & \textbf{SST-2} & \textbf{AGNEWS} & \textbf{Amazon} \\
    \hline
    \textbf{Gold vs SimPrompt} & 0.1924 & 0.0468 & 0.0933 \\
    \textbf{Gold vs AttrPrompt} & 0.0366 & 0.0913 & 0.1640 \\
    \textbf{Gold vs SynAlign(random)} & 0.0082 & 0.0647 & 0.1209 \\
    \rowcolor{gray!30} 
    \textbf{Gold vs SynAlign(mmd)} & \textbf{0.0077} & \textbf{0.0516} & \textbf{0.1068} \\
    \hline
\end{tabular}
}
\end{table}

\noindent \textbf{Vocabulary Diversity:}  
We measure vocabulary diversity by calculating the vocabulary size, defined as the number of unique words in each dataset. As shown in Table~\ref{tab:voc_size}, \textbf{SynAlign (MMD)} generates datasets with higher vocabulary diversity than \textbf{SimPrompt} and comparable diversity to \textbf{AttrPrompt}. For example, on SST-2, SynAlign (MMD) achieves a vocabulary size of 7.4k, significantly larger than SimPrompt (1k) and close to AttrPrompt (7.2k).

Although the generated datasets have smaller vocabulary sizes than the original datasets, this is expected due to the limited input prompts provided to the LLM. However, combining synthetic and original data compensates for this limitation, as evidenced by the performance improvements in Section \ref{sec_method}. These results show that SynAlign not only enhances distributional alignment but also generates more linguistically diverse text, contributing to its superior performance.

\begin{table}[tbp]
\centering
\caption{Vocabulary sizes of original datasets and synthetic datasets generated by different methods.}
\label{tab:voc_size}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
    \hline
    \textbf{Data to compare} & \textbf{SST-2} & \textbf{AGNEWS} & \textbf{Amazon} \\
    \hline
    \textbf{Gold} & $13k$ & $158k$ & $90k$ \\
    \textbf{SimPrompt} & $1k$ & $27k$ & $28k$ \\
    \textbf{AttrPrompt} & $7.2k$ & $30k$ & $22k$ \\
    \textbf{SynAlign(all)} & $7.9k$ & $32k$ & $35k$ \\
    \textbf{SynAlign(mmd)} & $7.4k$ & $31k$ & $29k$ \\
    \hline
\end{tabular}
}
\end{table}

% In summary, \textbf{SynAlign (MMD)} generates data that is both better aligned with the original distribution and more diverse in vocabulary compared to other prompting methods. These qualities contribute to its superior performance in downstream tasks, as demonstrated in Section \ref{sec_method}.


\subsection{Ablation Studies}

% To understand the impact of key components in our method, we analyze three aspects: (1) exploration-aware sampling, which aims to efficiently cover the data distribution; (2) the benefits of reasoning about latent attributes during data generation; and (3) the role of MMD-based sampling in aligning synthetic data with the original distribution.

\subsubsection{Exploration-aware Sampling Coverage Speed}
\label{sec:abla_coverage}

% Efficiently covering the data distribution is critical during few-shot prompting. We compare our \textbf{exploration-aware sampling} with random sampling by evaluating the coverage rate of the original data distribution. The coverage area is measured as the convex hull formed by selected samples in the t-SNE-reduced embedding space.

Efficiently covering the original data distribution is critical for few-shot prompting. We compare our \textbf{exploration-aware sampling} with random sampling by evaluating their respective coverage rates of the original data distribution. The coverage area is measured as the convex hull formed by selected samples in the t-SNE-reduced embedding space, with coverage rates computed iteratively for 200 sampling steps.

% 这段放到附录去
% 描述一下实验做法
% To calculate the coverage rate, we first reduce the dimensionality of the sentence embeddings for both the original and generated datasets using t-SNE. A k-d tree is then constructed using the 2D t-SNE embeddings of the original dataset to enable efficient neighbor searching. The convex hull of the t-SNE embeddings of the original dataset is taken as the target distribution's total coverage area. We initialize an empty buffer set $\mathcal{B}$ to store convex hulls formed during sampling. At each sampling iteration, a single example is selected, and its embedding is combined with the $k$-nearest neighbors (identified using the k-d tree) to create a new convex hull. If this convex hull overlaps with any existing convex hulls in $\mathcal{B}$, they are merged to form a larger convex hull. The total area of all convex hulls in $\mathcal{B}$ is then calculated and compared to the total coverage area of the original dataset to compute the coverage rate. This process is repeated iteratively for 200 sampling steps.

% 对图片的描述

Figure~\ref{fig:gp_cover}(a) illustrates an example of convex hull coverage for SST-2, while Figure~\ref{fig:gp_cover}(b) compares coverage rates across AGNEWS and Amazon. Exploration-aware sampling consistently achieves faster and broader coverage of the data distribution compared to random sampling. Specifically, it avoids redundant sampling in densely populated regions and captures diverse, representative samples more effectively. This result highlights the efficiency of Gaussian Process Active Sampling in improving few-shot prompting performance.



% As shown in Figure~\ref{fig:gp_cover}(b), across all three datasets, the Exploration-aware Sampling method consistently exhibits faster coverage of the original data distribution compared to random sampling.
% Specifically, Exploration-aware sampling selects samples that more efficiently cover the diverse regions of the original data distribution. In contrast, random sampling tends to select samples from densely populated regions, leading to redundant sampling of similar points, which slows down the overall coverage. This result highlights the efficiency of Gaussian Process Active Sampling in exploring the data distribution and selecting diverse and representative samples, which is critical for improving few-shot prompting performance.

\begin{figure}[tbp]
    \centering
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/gp-cover/gp_cover.pdf}} 
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/gp-cover/coverage_plot.pdf}} 
    \caption{(a) Convex hull coverage example for SST-2. (b) Coverage rate comparison between Gaussian Process Active Sampling and random sampling on AGNEWS and Amazon. Gaussian Process achieves faster and broader coverage.}
    \label{fig:gp_cover}
\end{figure}

\subsubsection{Benefits of Latent Attribute Reasoning}

Our method employs a \textbf{two-stage generation process} that separates attribute generation (e.g., sentiment or topic) from synthetic data generation. This design encourages diversity compared to single-stage generation, which directly relies on prompt examples.

Table~\ref{tab:2stage_res} compares the performance of models trained on data generated by single-stage and two-stage approaches. The results show that the two-stage generation consistently outperforms the single-stage approach across AGNEWS and Amazon datasets. For example, on AGNEWS, SynAlign(mmd) achieves an accuracy of 83.81\% with two-stage generation, compared to 83.19\% with single-stage generation. These improvements highlight the importance of reasoning about latent attributes to improve the quality and diversity of synthetic data.

\begin{table}[!t]
\centering
\caption{Performance comparison between single-stage and two-stage generation on AGNEWS and Amazon datasets.}
\label{tab:2stage_res}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll|c|c}
    \hline
    \textbf{} & \textbf{Method} & \textbf{AGNEWS} & \textbf{Amazon} \\
    \hline
     & \textbf{Gold} & 0.8204 & 0.9353 \\
     & \textbf{AttrPrompt} & 0.8274 & 0.9441 \\
    \hline
    \multirow{3}{*}{\textbf{One Stage}} & \textbf{SynAlign(all)} & 0.8319 & 0.9430 \\
    &\textbf{SynAlign(random)} & 0.8177 & 0.9437 \\
    &\textbf{SynAlign(mmd)} & 0.8319 & 0.9439 \\
    \hline
    \multirow{3}{*}{\textbf{Two Stage}} & \textbf{SynAlign(all)} & 0.8372 & 0.9460 \\
    &\textbf{SynAlign(random)} & 0.8212 & 0.9439 \\
    \rowcolor{gray!30} 
    &\textbf{SynAlign(mmd)} & \textbf{0.8381} & \textbf{0.9475} \\
    \hline
\end{tabular}
}
\end{table}

\subsubsection{Impact of MMD Distribution Alignment}

The MMD-based sampling strategy selects synthetic samples that are closely aligned with the original data distribution. As shown in Table~\ref{tab:main_result}, SynAlign (MMD) achieves the highest performance across all datasets, outperforming both random sampling and other baselines. For example, on AGNEWS, SynAlign (MMD) achieves an accuracy of 0.9475, compared to 0.9429 for random sampling.

The Wasserstein distance results in Table~\ref{tab:was_dis} further validate the effectiveness of MMD sampling. SynAlign (MMD) consistently achieves smaller distances compared to random sampling, indicating better alignment with the original data distribution. This improved alignment explains the observed gains in downstream performance.


\subsection{Hyperparameter Analysis}

% To evaluate the sensitivity of our method to key hyperparameters, we analyze the following: (1) the length scale $\tau$ of the RBF kernel in the Gaussian Process model used during Exploration-aware Sampling, (2) the number of nearest neighbors $k$ selected at each sampling iteration in the same stage, and (3) the number of projection matrices $|\Theta|$ used during Synthetic Distribution Alignment.
% Below, we discuss the influence of these parameters on the model's performance.


\subsubsection{RBF Kernel Length Scale $\tau$ in Exploration-aware Sampling}

The RBF kernel's length scale $\tau$ is a crucial parameter in the Gaussian Process model for tracking sample uncertainty. It controls the smoothness of the covariance function and determines the range of influence of selected samples. Smaller $\tau$ values lead to highly localized effects, while larger values smooth the uncertainty estimates over broader regions.

We evaluate the impact of $\tau$ on the convex hull coverage rate across SST-2, AGNEWS, and Amazon datasets. As shown in Figure~\ref{fig:para_anal} (a), the coverage rate exhibits consistent patterns across datasets. When $\tau$ is too small(e.g., $\tau < 0.3$), the coverage rate is low due to excessive focus on densely populated regions, leading to redundant sampling. As $\tau$ increases, the coverage rate improves, reaching its peak at dataset-specific optimal values (e.g., $\tau = 0.9$ for SST-2). However, when $\tau$ becomes too large ($\tau > 1.5$), the coverage rate declines as the sampling behavior becomes overly smooth, resembling random sampling. 
% These results demonstrate the importance of tuning $\tau$ to balance local and global sampling effects, which depend on the dataset's distributional characteristics.

% 三个数据集每个点最近/最远点距离分布情况+不同rbf_length得到的coverage
% \begin{figure*}[!t]
%     \centering
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/rbf_length/k_dist_hist_sst2.pdf}} 
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/rbf_length/k_dist_hist_agnews.pdf}} 
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/rbf_length/k_dist_hist_amazon.pdf}} 
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/rbf_length/rbf_length_coverage_plot.pdf}} 
%     \caption{Coverage rate as a function of RBF kernel length scale $\tau$ for Exploration-aware Sampling across three datasets: SST-2, AGNEWS, and Amazon. Each curve represents the convex hull coverage rate at fixed sampling iterations.}
%     \label{fig:para_rbf_length}
% \end{figure*}

\begin{figure*}[htbp]
    \centering
    \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/rbf_length/rbf_length_coverage_plot.pdf}} 
    \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/num_sample/k_coverage_plot_sst2.pdf}} 
    \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/num_sample/k_coverage_plot_agnews.pdf}} 
    \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/num_sample/k_coverage_plot_amazon.pdf}}
    \caption{Coverage rate as a function of RBF kernel length scale $\tau$ for Exploration-aware Sampling across three datasets: SST-2, AGNEWS, and Amazon. Each curve represents the convex hull coverage rate at fixed sampling iterations.}
    \label{fig:para_anal}
\end{figure*}


To better understand the differences between datasets, we analyzed the distribution of pairwise Euclidean distances between sentence embeddings. These analyses, presented in the Appendix, indicate that AGNEWS has a more spread-out embedding space compared to SST-2 and Amazon, which explains why it benefits from a larger $\tau$ for optimal coverage.


\subsubsection{Number of Nearest Neighbors $k$ in Exploration-aware Sampling}

The parameter $k$, which determines the number of nearest neighbors selected during each sampling iteration, controls the trade-off between sample diversity and efficiency in Exploration-aware Sampling. Larger $k$ values enable broader coverage of the embedding space, while smaller values focus on fewer, more representative samples.


% 三个数据集的kchosen图片
% \begin{figure*}[!t]
%     \centering
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/num_sample/k_coverage_plot_sst2.pdf}} 
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/num_sample/k_coverage_plot_agnews.pdf}} 
%     \subfigure{\includegraphics[width=0.24\textwidth]{figures/para_test/num_sample/k_coverage_plot_amazon.pdf}} 
%     % \subfigure[]{\includegraphics[width=0.22\textwidth]{figures/para_test/num_sample/k_coverage_plot_amazon.pdf}} 
%     \caption{xxx}
%     \label{fig:para_num_sample}
% \end{figure*}

We evaluate the impact of $k$ on the convex hull coverage rate under the optimal RBF kernel length scale identified earlier. Figure~\ref{fig:para_anal} (b)-(d) shows that the coverage rate increases with $k$ across all datasets, but the rate of improvement diminishes as $k$ becomes large.
% For example, on SST-2, the coverage rate rises from 0.26\% to 31.41\% as $k$ increases from 3 to 23.
However, larger $k$ values incur higher computational costs, particularly for datasets with large embedding spaces like AGNEWS.
Interestingly, when $k$ is small, the performance of Exploration-aware Sampling is comparable to random sampling. 
% For instance, on SST-2, both methods achieve similar coverage at $k=3$. As $k$ increases, Exploration-aware Sampling consistently outperforms random sampling, with a widening gap at larger values (e.g., 33.06\% vs. 27.22\% on Amazon at $k=41$).
This demonstrates that our method is more effective at utilizing larger sampling budgets to achieve higher coverage.

% These results suggest that our method is robust to a wide range of $k$ values, consistently outperforming random sampling. Nevertheless, selecting an appropriate $k$ is crucial to balance computational cost and coverage efficiency, especially for datasets with higher complexity.

\subsubsection{Number of Projection Matrices $|\Theta|$ in Synthetic Distribution Alignment}

In the Synthetic Distribution Alignment stage, the number of projection matrices $|\Theta|$ plays a crucial role in aligning the original data distribution $D_{ori}$ with the generated data distribution $D_{gen}$ through MMD. This parameter determines the expressiveness of the alignment process: too few projection matrices may inadequately capture distributional differences, while too many may lead to computational overhead or overfitting.

To evaluate the effect of $|\Theta|$, we vary its value across \{10, 50, 100, 500, 1000\} and measure the Wasserstein distance between $D_{ori}$ and $D_{gen}$. Table~\ref{tab:theta_analysis} reports the results for SST-2, AGNEWS, and Amazon.
For SST-2 and Amazon, the Wasserstein distance is minimized at $|\Theta| = 50$, indicating that moderate numbers of projection matrices are sufficient for effective alignment in datasets with simpler embedding spaces. In contrast, for AGNEWS, which has a more complex embedding space due to its higher diversity, the Wasserstein distance continues to decrease as $|\Theta|$ increases, reaching its lowest value at $|\Theta| = 100$ (0.00715). Further increases in $|\Theta|$ yield diminishing returns while increasing computational costs.

% Based on these results, we recommend setting $|\Theta| = 50$ for SST-2 and Amazon, as it achieves a good trade-off between performance and efficiency. For AGNEWS, $|\Theta| = 100$ better balances alignment quality and computational cost given the dataset's complexity.



\begin{table}[htbp]
\centering
\caption{Wasserstein distance between $D_{ori}$ and $D_{gen}$ as a function of the number of projection matrices $|\Theta|$.}
\label{tab:theta_analysis}
\begin{tabular}{lccccc}
\toprule
% \hline
\textbf{Dataset} & \textbf{$10$} & \textbf{$50$} & \textbf{$100$} & \textbf{$500$} & \textbf{$1000$} \\
\midrule
SST-2 & 0.11069 & \textbf{0.10516} & 0.10735 & 0.10931 & 0.10801 \\
AGNEWS & 0.00773 & 0.00748 & \textbf{0.00715} & 0.00705 & 0.00702 \\
Amazon & 0.06055 & \textbf{0.06357} & 0.06419 & 0.06375 & 0.06310 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Online A/B Test}
% We deployed the algorithm in the pre-ranking module of industrial AppGallery's search advertising system, which filters irrelevant items based on user queries and app names. However, two issues arise: the long-tail nature of app ads results in limited search data for long-tail apps, and the functionality-abstract nature of app names creates significant query-app discrepancies (e.g., 'Presidential Election' and 'TikTok'). Thus, training relevance models with contrastive learning on exposure-click data leads to low generalization capability. To address this, we propose using LLMs to augment user queries. However, zero-shot LLMs synthesis struggles to account for these distributions when generating queries, often producing data that deviates from real-world user query preference. Mixing such synthetic queries with real ones risks distorting the data and hindering model convergence on the original distribution.

We deployed the SynAlign framework in the pre-ranking module of AppGallery's search advertising system to address two key challenges: the long-tail nature of app ads, which results in limited search data, and the significant query-app discrepancies caused by abstract app names (e.g., 'Presidential Election' vs. 'TikTok'). Training relevance models on exposure-click data using contrastive learning struggles with generalization due to these issues. While LLMs can augment user queries, zero-shot LLM synthesis often generates queries that deviate from real-world user preferences, leading to distribution mismatches and potential model convergence issues.

To address the distribution mismatch between synthetic and real queries, we utilized the SynAlign framework. During the SynAlign synthesis process, we applied uncertainty sampling to efficiently analyze all query-item pairs and identify common user query patterns (e.g., app names, substrings, typos). These patterns were then combined with app names and input into Qwen2.5 to generate hundreds of thousands of synthetic queries. Both synthetic and real queries were mapped into the embedding space, where SynAlign’s MMD-based method was used to assign a weight to each synthetic query. A new dataset was created by sampling queries based on these weights. Offline testing showed that models enhanced with distribution-aligned synthetic queries achieved a 0.26\% improvement in AUC compared to unenhanced models.

For online deployment, synthetic queries were mixed with daily updated real queries for the experimental group, while the control group used only real queries. Over a week, the experimental group achieved a 2.86\% increase in RPM and a 2.31\% increase in CPM. This SynAlign-based data synthesis approach is now fully deployed to serve all users.