\section{Method}
\label{sec_method}

\begin{figure*}[!t]
  \centering
    \includegraphics[width=0.99\linewidth]{figures/Main_fig.pdf}
  % \vspace{-0.1in}
  \caption{SynAlign comprises three modules for aligning the distribution of synthetic samples. First, the \textbf{Exploration-aware Sampling} Module selects real samples based on uncertainty to provide diverse inputs for the LLM. Next, the \textbf{Latent Attribute Reasoning} Module uses these samples as demonstrations to identify and generalize key language attributes for synthetic data generation. Finally, the \textbf{Synthetic Distribution Alignment} Module assigns sampling weights to synthetic data, which are then resampled accordingly to ensure alignment with the real data distribution.}
  % \vspace{-0.15in}
  \label{fig.model}
  \Description{}
\end{figure*}

\subsection{Overview}

% Let the real dataset be denoted as $D_{\text{ori}} = \{(x_0, y_0), (x_1, y_1), \dots\\, (x_N, y_N)\}$, where each $(x_i, y_i)$ represents a data sample and its corresponding label. The objective of data synthetic is generating a synthetic dataset $D_{\text{gen}} = \{(x'_0, y'_0), (x'_1, y'_1), \dots, (x'_M, y'_M)\}$ using LLMs, such that the synthetic data can enhance the performance of smaller, domain-specific models in downstream tasks by improving dataset diversity. To simplify the modeling of text distributions, we map the original text dataset $D_{\text{ori}}$ and $D_{\text{gen}}$ into the embedding space $E_{\text{ori}}$ and $E_{\text{gen}}$ using a Sentence-BERT~\cite{reimers2019sentence} model.

% Due to the limited capability of LLM in processing long sequences, it is infeasible to feed all real and synthesized data simultaneously. Failing to feed real data results in synthetic data may not capture the full diversity of the real data. And failing to input synthetic ones prevents the LLM from maintaining the correct proportion within diverse data attributes. Both result in a distributional gap between the synthetic and real data, which can degrade model performance when the synthetic data is directly used for training.

% \textbf{SynAlign} is designed to handle the above problems. To improve synthetic data diversity, we designed two key stages for data synthetic: \textbf{Exploration-aware Sampling} and \textbf{Latent Attribute Reasoning}. The former employs a Gaussian Process (GP) as an uncertainty tracker to actively explore heterogeneous samples. These selected samples serve as demonstrations for synthetic data generation. Then the latter apply a COT~\cite{wei2022chain} paradigm by first reasoning out demonstrations' linguistic attributes and synthetic new data based on these attributes and demonstrations. This helps LLM explicitly attend to diverse linguistic attributes.

% To ensure synthetic data's proportion of language attributes matches with the real dataset, we designed \textbf{Synthetic Distribution Alignment} module. It uses the Maximum Mean Discrepancy (MMD) ~\cite{gretton2012kernel} to align the distribution of $D_{\text{gen}}$ with  $D_{\text{ori}}$ by computing a sampling weight for each synthetic data, ensuring that the synthetic dataset closely matches the distribution of the real ones.

% This method enables SynAlign to produce diverse, high-quality synthetic data that augments downstream tasks better by ensuring minimal deviation from the real data distribution.

% 缩写
Given a real dataset $D_{\text{ori}} = \{(x_i, y_i)\}_{i=1}^N$ and a synthetic dataset $D_{\text{gen}} = \{(x'_j, y'_j)\}_{j=1}^M$ generated by a large language model (LLM), our goal is to enhance the quality and diversity of $D_{\text{gen}}$ such that it improves the performance of smaller, domain-specific models in downstream tasks. To simplify the modeling of text distributions, both $D_{\text{ori}}$ and $D_{\text{gen}}$ are mapped into embedding spaces $E_{\text{ori}}$ and $E_{\text{gen}}$ using Sentence-BERT~\cite{reimers2019sentence}.

Directly generating $D_{\text{gen}}$ often leads to a distributional gap between real and synthetic data due to LLM limitations. This gap arises from incomplete coverage of real data diversity or misaligned proportions of data attributes, which can degrade the utility of $D_{\text{gen}}$ in downstream tasks.

To address these issues, we propose \textbf{SynAlign}, comprising three key modules:
1) \textbf{Exploration-aware Sampling}: This module uses a Gaussian Process (GP) uncertainty tracker to actively select diverse and representative real samples as demonstrations for synthetic data generation.
2) \textbf{Latent Attribute Reasoning}: Demonstrations are used to reason about key linguistic attributes via a Chain-of-Thought (CoT)~\cite{wei2022chain} paradigm, guiding the LLM to explicitly attend to diverse attributes during generation.
3) \textbf{Synthetic Distribution Alignment}: Using Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}, this module aligns $D_{\text{gen}}$ with $D_{\text{ori}}$ by computing sampling weights for synthetic samples, ensuring minimal distributional deviation.

By combining these modules, SynAlign produces high-quality, diverse synthetic data that better aligns with real data distributions, resulting in improved performance in downstream tasks.


\subsection{Exploration-aware Sampling}

Our method uses a few-shot prompting approach to generate synthetic data using LLM. Selecting representative demonstrations from the real dataset is crucial for guiding the LLM in producing high-quality synthetic data. Random selection may overfit to frequently occurring patterns. We utilize an uncertainty-aware sampling strategy to ensure that the synthetic data process efficiently covers all real data's language attributes.

%The GP regression model is particularly effective because it predicts both a mean and an uncertainty estimate for each sample. When the GP is fitted to selected data points, as shown in Fig\ref{fig.model}, it reduces the uncertainty for nearby or similar data due to the influence of these fitted points. Consequently, samples that differ significantly from those already selected exhibit higher uncertainty in GP predictions. This property allows us to identify and select the most representative and diverse samples from the dataset, ensuring coverage of varied linguistic patterns and capturing the dataset's variability.

To achieve this, we use a Gaussian Process model ~\cite{williams1995gaussian} as samples' uncertainty tracker $U(E_{ori})$. When initializing $U(E_{ori})$, we assign each sample's mean value as 0 and uncertainty (variance) as 1 and covariance between samples $e_i, e_j$ as $\kappa(e_i, e_j)$.
\begin{align}
                    \left[\begin{array}{c}
                    U\left(e_0\right) \\
                    \vdots \\
                    U\left(e_n\right)
                    \end{array}\right] \sim N\left(\left[\begin{array}{c}
                    0 \\
                    \vdots \\
                    0
                    \end{array}\right],\left[\begin{array}{ccc}
                    1 & \cdots & \kappa\left(e_1, e_n\right) \\
                    \vdots & \cdots & \vdots \\
                    \kappa\left(e_n, e_1\right) & \cdots & 1
                    \end{array}\right]\right) 
\end{align}

Where $\kappa(\cdot, \cdot)$ is chosen as Radial Basis Function Kernel as below to ensure covariance between selected and unselected samples increase as text embedding's similarity decreases.
\begin{align}
\kappa(e_i, e_j) = exp(-\frac{1}{2\tau}||e_i- e_j||)
\end{align} 
Where $\tau$ is a hyper-parameter called bandwidth that controls covariance smoothness. Each demonstration selection phase includes the following two steps:

\textbf{Step 1. Demonstration Selection}. Samples with the highest uncertainty (variance) and its $k$-nearest samples $D_{dem}$ will be selected out as demonstrations according to the newest updated $U(E_{ori})$. These samples are most different to LLM already seen demonstrations regarding linguistic attributes like styles, contexts etc.
\begin{align}
D_{dem} = k\text{-}NN(D_{ori}, argmax_{i}(U(e_i|e_i \in E_{ori})), k)
\end{align} 
Where $k\text{-}NN$ is the function that returns the $k$ nearest samples of the most uncertain sample indexed by $argmax_{i}(U(e_i|e_i \in E_{ori}))$ from real dataset $D_{ori}$.

\textbf{Step 2. Uncertainty Update}. Once $D_{dem}$ are used as demonstrations for data synthetic, their embedding $E_{dem}$'s uncertainty is reduced to 0 and constructed as posterior training data $(E_{dem}, 0)$. Those samples together with historical ones $(E_{s}, 0)$ will be used to update the unselected samples' uncertainty value $U(E_{u})$ in the uncertainty tracker $U$. The updated sample uncertainty is given below:
\begin{align}
U(E_{u}) | E_{u}, E_{s}, U(E_{s}) \sim N\left(\mu^*,\Sigma^*\right) 
\end{align}                          
where $\mu^*$ equals to \textbf{0} because only uncertainty is used in the selection process. And variance(uncertainty) $\Sigma^*$ is given below:
\begin{align}
\Sigma^* = K(E_{u}, E_{u}) + I -K(E_{u}, E_{s})K(E_{s}, E_{s} + I)^{-1}K(E_{s}, E_{u}) 
\end{align} 
These two steps are repeated iteratively to select demonstrations for LLM to generate synthetic data until all samples' uncertainties are below a predefined threshold. The overall procedure is listed in Appendix Algorithm~\ref{apdx:alg}.

This uncertainty-aware sampling strategy ensures that the selected demonstrations represent the diversity of real data distribution, as measured by the uncertainty in the Gaussian Process model. These selected examples are then used as demonstrations for the few-shot prompting of the LLM in the next stage of the data generation process.

% \begin{algorithm}[!t]
% 	\caption{Exploration-Aware Sampling} 
% 	\begin{algorithmic}
% 	    \STATE \textbf{Input}: Original text dataset $D_{ori}$, embedding model $F$, demonstration size $k$, stopping threshold $\eta$.
% 	    \STATE \textbf{Output}: Demonstrations for each LLM generation stage.
%             \STATE  $E_{ori} = F(D_{ori})$ // $E_{ori}$ is training data in embedding space
%             \STATE  $
%                     \left[\begin{array}{c}
%                     G P\left(e_0\right) \\
%                     \vdots \\
%                     G P\left(e_n\right)
%                     \end{array}\right] \sim N\left(\left[\begin{array}{c}
%                     0 \\
%                     \vdots \\
%                     0
%                     \end{array}\right],\left[\begin{array}{ccc}
%                     k\left(e_1, e_1\right) & \cdots & k\left(e_1, e_m\right) \\
%                     \vdots & \cdots & \vdots \\
%                     k\left(e_m, e_1\right) & \cdots & k\left(e_m, e_m\right)
%                     \end{array}\right]\right) $ // Initializing uncertainty tracker 
% 		\WHILE{Sample B in Training Dataset}  
% 		%\IF{i \% T == 0}
% 		\STATE Conduct feed-forward computation $L(\boldsymbol{y}, f_\theta(\boldsymbol{E}))$
% 		\STATE Compute feature importance with Algorithm \ref{alg2} with $M$
%             \STATE Compute importance regularization loss with equation (\ref{eq8})
% 		\STATE Combined with cross-entropy loss and conduct optimization
% 		\ENDWHILE 
%            \STATE \textbf{Validation Phase:}
%            \WHILE{Sample B in the Validation Dataset}  
% 		%\IF{i \% T == 0}
% 		\STATE Compute feature importance with Algorithm \ref{alg2} with $n_{ac}$
%             \STATE Aggregate feature importance for each batch
% 		\ENDWHILE 
% 	\end{algorithmic} 
% \label{alg1}
% \end{algorithm}






% \textbf{Sentence Embedding Extraction:} We begin by converting all text samples from the original dataset $D_{\text{ori}}$ into sentence embeddings. Specifically, we use a pretrained DistilBERT model~\citep{distilbert} to map each data sample $x_i$ in $D_{\text{ori}} = \{(x_0, y_0), \dots, (x_{N-1}, y_{N-1})\}$ to a corresponding sentence embedding $e_i$. This results in a set of embeddings $E_{\text{ori}} = \{e_0, e_1, \dots, e_{N-1}\}$, used for Gaussian Process regression.

% \textbf{Initializing Gaussian Process Regression:} Each embedding $e_i$ is paired with an initial label of 0, forming the training dataset $X_{\text{ori}} = \{(e_0, 0), (e_1, 0), \dots, (e_{N-1}, 0)\}$. The original dataset is initialized with a standard multivariate Gaussian distribution with a squared exponential kernel.

% \begin{align}
% \left[\begin{array}{c}
% G P\left(e_0\right) \\
% \vdots \\
% G P\left(e_n\right)
% \end{array}\right] \sim N\left(\left[\begin{array}{c}
% 0 \\
% \vdots \\
% 0
% \end{array}\right],\left[\begin{array}{ccc}
% k\left(e_1, e_1\right) & \cdots & k\left(e_1, e_m\right) \\
% \vdots & \cdots & \vdots \\
% k\left(e_m, e_1\right) & \cdots & k\left(e_m, e_m\right)
% \end{array}\right]\right)
% \end{align}


% \textbf{Active Sampling Process:} We begin by randomly selecting $z$ samples from $X_{\text{ori}}$ without replacement and adding them to $X_{\text{train}}$. These initial samples are used to fit a Gaussian Process regressor. The active sampling process then proceeds iteratively as follows:
% 1) \textbf{GP Fitting:} Fit the Gaussian Process regressor to the current training set $X_{\text{train}}$.
% 2) \textbf{Prediction:} For each remaining sample $(e_i, 0) \in X_{\text{ori}} \setminus X_{\text{train}}$, predict each $\hat{y}_i$ and uncertainty $\sigma_i$ using the Gaussian Process model.
% 3) \textbf{Selection:} Identify the sample $e_i$ with the highest uncertainty $\sigma_i$, and add it to $X_{\text{train}}$.
% 4) \textbf{Neighbor Expansion:} Using the KDTree, identify the $z-1$ nearest neighbors of $e_i$ in embedding space, and add these to $X_{\text{train}}$.


% Steps 1 through 4 are repeated until $\delta = k \cdot z$ samples have been selected. The indices of the selected samples form the set $\mathcal{I} = \{i \mid e_i \in X_{\text{train}}\}$, and the corresponding dataset is denoted as $D_{\text{select}} = \{(x_i, y_i) \mid i \in \mathcal{I}\}$.

% This active sampling strategy ensures that the selected data points are both representative of the original data distribution and diverse, as measured by the uncertainty in the Gaussian Process predictions. These selected examples are then used as demonstrations for the few-shot prompting of the LLM in the next stage of the data generation process.

\subsection{Latent Attribute Reasoning}
Demonstrations selected from the previous module are used to feed and assist LLM in understanding the diverse linguistic attributes in real data so that the synthetic data produced by LLM won't be monochrome. To explicitly ensure the synthetic data captures all linguistic attributes in the real data while maintaining content diversity, a two-stage process was designed by first reasoning out linguistic attributes and then generating diverse content based on them. 

\textbf{Stage 1. Key Attribute Reasoning} stage. The goal of this stage is to identify and summarize the key linguistic attributes of the selected demonstrations, which serve as a blueprint for synthetic data generation. To ensure the synthetic data $D_{gen}$ reflects the structure and diversity of the real data $D_{ori}$, we let $D_{gen}$ mimick key attributes $\textbf{A} = \{a_1,..a_n\}$ extracted from $D_{dem}$. Referring to prior work AttrPrompt ~\cite{attriprompt}, we use LLM to identify crucial attributes. For instance, attributes in an Amazon product review dataset might include \textit{Product Info, Usage Experience,} and \textit{Writing Style}. These attributes provide a framework for summarizing the sampled examples.

Once the key attributes are identified, we construct reasoning prompts $P_1$ instructing the LLM to analyze $D_{dem}$ and extract these key attributes. This results in a JSON format \textit{Attribute Summary Set} $S$:
\begin{align}
S = LLM(D_{dem}, A, P_1) =\{(a_1,v_1), (a_2,v_2)... (a_n,v_n)\}
\end{align} 
where each tuple $(a_i,v_i)$ represents summarized attributes of selected demonstrations.

\textbf{Stage 2. Attribute-Based Data Generation} stage. We leverage the attribute summarized in Stage 1 to guide the LLM in generating synthetic data. By incorporating attribute summaries $S$ into generation prompts $P_2$, LLM can synthesize samples reflecting these key attributes, such as product information and writing style. For each attribute tuple in $\mathcal{S}$, the LLM generates a synthetic sample that adheres to the specified attributes, ensuring the generated data is both diverse and representative of the original dataset. Finally, by generating new data for each attribute summary, we construct the synthetic dataset $D_{\text{gen}}$.
\begin{align}
D_{\text{gen}} = LLM(S,P_2) =\{(x_0, y_0), (x_1, y_1), \dots, (x_M, y_M)\}
\end{align}
 The size of $D_{\text{gen}}$ depends on the number of summaries in $\mathcal{S}$ and the samples generated per summary. The generated data covers a wide range of latent attributes, maintaining alignment with the original data distribution while introducing new variations in style and content.

\subsection{Synthetic Distribution Alignment}





% To ensure that the generated dataset $D_{\text{gen}}$'s distribution aligns with the distribution of the original dataset $D_{\text{ori}}$, we propose a Maximum Mean Discrepancy (MMD)-based distribution post alignment method. The goal is to compute a set of weights $W_{\text{gen}} = \{w_0, w_1, \dots, w_M\}$ for each synthetic sample in $D_{\text{gen}}$, such that the weighted distribution of $D_{\text{gen}}$ closely matches the distribution of $D_{\text{ori}}$. This process is performed iteratively, using an MMD objective to minimize the discrepancy between the two datasets. This post-alignment process includes three steps.

% \subsubsection{Parameter Initialization}
% Let the original dataset $D_{\text{ori}}$ and the generated dataset $D_{\text{gen}}$ consist of text samples $x_i$ and their corresponding labels $y_i$. We first pass both datasets through a pre-trained model (e.g., BERT), obtaining embeddings $E_{\text{ori}} = \{e_0^{ori}, e_1^{ori}, \dots, e_N^{ori}\}$ and $E_{\text{gen}} = \{e_0^{gen}, e_1^{gen}, \dots, e_M^{gen}\}$, where $e_i^{ori}$ and $e_i^{gen}$ are the vector representations of the samples. We then initialize the weights $W_{\text{gen}} = \{w_0, w_1, \dots, w_M\}$ for $D_{\text{gen}}$, where each $w_i$ corresponds to an embedding $e_i^{gen}$.

% \subsubsection{Randomly Initialized Projection Networks}
% To map the embeddings to a scalar space for MMD computation, we introduce multiple randomly initialized projection networks $f_{pro}^i$. Each projection network maps the embeddings to a scalar value. The architecture of each $f_{pro}^i$ is a single-layer Multi-Layer Perceptron (MLP) with an output size of 1. 

% To ensure diversity in the projections, we use Gram-Schmidt orthogonalization\citep{Gram_Schmidt} to initialize the networks. The parameters of each network $f_{pro}^i$ are orthogonalized with respect to the previously initialized networks $f_{pro}^{0:i-1}$, ensuring each network captures different aspects of the data.

% \subsubsection{Iterative Optimization}
% The alignment process proceeds iteratively, with each iteration consisting of the following steps:

% \textbf{1. Projection and Weighting.}
% In each iteration, we apply each projection network $f_{pro}^i$ to the embeddings from both datasets. The projection network maps the embeddings $E_{\text{ori}}$ and $E_{\text{gen}}$ into scalar values:
% \begin{align}
% \hat{E}_{\text{ori}} = f_{pro}(E_{\text{ori}}) = \{f_{pro}(e_0^{ori}), f_{pro}(e_1^{ori}), \dots, f_{pro}(e_N^{ori})\}
% \end{align}
% \begin{align}
% \hat{E}_{\text{gen}} = f_{pro}(E_{\text{gen}}) = \{f_{pro}(e_0^{gen}), f_{pro}(e_1^{gen}), \dots, f_{pro}(e_M^{gen})\}
% \end{align}
% Each projected embedding from $D_{\text{gen}}$ is weighted by the corresponding weight $w_i \in W_{\text{gen}}$:
% \begin{align}
% \hat{E}_{\text{gen}}^{weighted} = \{w_0 \cdot f_{pro}(e_0^{gen}), w_1 \cdot f_{pro}(e_1^{gen}), \dots, w_M \cdot f_{pro}(e_M^{gen})\}
% \end{align}

% \textbf{2. MMD Calculation.}
% Once the embeddings are projected and weighted, we compute the MMD between $\hat{E}_{\text{ori}}$ and $\hat{E}_{\text{gen}}^{weighted}$. The MMD is a measure of the distance between two distributions. Specifically, the squared MMD between the two sets of samples is computed as:
% \begin{align}
% \text{MMD}^2(\hat{E}_{\text{ori}}, \hat{E}_{\text{gen}}^{weighted}) &= \frac{1}{N^2} \sum_{i,j} k(\hat{e}_i^{ori}, \hat{e}_j^{ori}) \nonumber\\
%     &+ \frac{1}{M^2} \sum_{i,j} k(\hat{e}_i^{gen}, \hat{e}_j^{gen}) \nonumber\\
%     &- \frac{2}{NM} \sum_{i,j} k(\hat{e}_i^{ori}, \hat{e}_j^{gen})
% \end{align}
% where $k(\cdot, \cdot)$ is a kernel function (e.g., Gaussian kernel), and $\hat{e}_i^{ori}$ and $\hat{e}_i^{gen}$ are the projected embeddings from $E_{\text{ori}}$ and $E_{\text{gen}}$, respectively.

% \textbf{3. Loss Computation and Weight Update.}
% The MMD value serves as the loss function, which we aim to minimize by adjusting the weights $W_{\text{gen}}$. Specifically, we update the weights using gradient-based optimization methods, such as stochastic gradient descent (SGD) or Adam. The weights are updated according to the following rule:
% \begin{align}
% w_i \leftarrow w_i - \eta \cdot \frac{\partial \text{MMD}^2}{\partial w_i}
% \end{align}
% where $\eta$ is the learning rate, and $\frac{\partial \text{MMD}^2}{\partial w_i}$ is the gradient of the loss with respect to the weight $w_i$.

% \textbf{4. Convergence Check.}
% The optimization proceeds iteratively until one of the following conditions is met: 1) The MMD loss falls below a predefined threshold, indicating that $D_{\text{gen}}$ and $D_{\text{ori}}$ are sufficiently aligned. 2) A maximum number of iterations is reached.

% After the optimization process converges, we obtain a set of optimized weights $W_{\text{gen}}$ that minimize the MMD between $D_{\text{ori}}$ and the weighted dataset $D_{\text{gen}}$. The resulting weighted dataset $D_{\text{gen}}^{weighted}$ is now closely aligned with the distribution of $D_{\text{ori}}$ and can be used for downstream tasks such as model training or evaluation.








Due to the limited input length, LLMs cannot fully account for the distribution of $D_{gen}$ and $D_{ori}$. This limitation often results in discrepancies between the linguistic attribute distributions of the synthesized and real data, potentially causing a "seesaw effect" that degrades the model's accuracy in practical applications. To enhance distribution matching, we want to learn a post-transformational function $F_\omega(\cdot)$ to minimize the distance between these two distributions:
\begin{align}
argmin_{F_\omega(\cdot)}\ Dist(D_{\text{ori}} || F_\omega(D_{\text{gen}}))
\end{align}
However, Due to the discrete and high-dimensional nature of linguistic data, measuring their distribution exactly is intractable. To address this, we adopted the Maximum Mean Discrepancy (MMD) method to approximate this matching objective. The basic idea of MMD in our application is matching the mean embedding of $E_{gen}$ and $E_{ori}$ projected in Reproducing Kernel Hilbert Space (RKHS), which is equivalent to matching these two distributions ~\cite{zhao2023dataset}.  
\begin{align}
argmin_{F_\omega(\cdot)}
sup_{||\phi_\theta||_H\leq 1} (
E\left [\phi_\theta(D_{\text{ori}})\right ] - 
E\left [\phi_\theta (F_\omega(D_{\text{gen}})\right ])
\end{align}
Where the $\phi_\theta$ is a family of functions parameterized by $\theta$ and $H$ represent the RKHS, considering the ground truth distribution is intangible, we adopt its empirical approximation by making the following changes: (1) map the text into an embedding space to obtain continuous representations; (2) simplify the transformation function as a data sampling weight $\omega$ and (3) assuming $\phi_\theta(\cdot)$ as a $R^n\rightarrow R^1$ random linear projection matrix family $\Theta$. Finally, we can derive the following objective function.
 \begin{align}
argmin_{\omega} E_{\theta\sim \Theta}||\frac{1}{N}\sum_{i=1}^{N}\theta\cdot E_{\text{ori}} - \frac{1}{M}\sum_{i=1}^{M}\theta\cdot (\omega \cdot E_{\text{gen}}))||^2
\label{equationset}
\end{align}
To ensure diversity in the projections matrix set $\Theta$, we use Gram-Schmidt orthogonalization\citep{Gram_Schmidt} to initialize these random matrices. The parameters of each projection matrix $\theta_{i}$ are orthogonalized regarding the previously initialized matrix $\theta_{1:i-1}$, ensuring each network captures different aspects of the data.
 \begin{align}
\Theta = \{\theta_i|<\theta_i,\theta_j>=\delta_{ij}\forall i, j, \delta_{ij} =1\ if\ i==j,\ otherwise\ 0\}
\end{align}
The final sample weight of each synthetic sample $\omega$ is calculated by solving the linear equation set described in formula ~\ref{equationset}. In our implementation, we use gradient descent to solve $\omega$ iteratively.

After the importance weight of each sample $\omega$ has been learned in the distribution alignment process, we can perform re-sampling on the original synthetic dataset $D_{gen}$ based on $\omega$ with replacement to construct a new synthetic dataset $D'_{gen}$ which will have similar linguistic attribute distribution with the data $D_{ori}$. Mixing up the $D'_{gen}$ with the original data $D_{ori}$ can bring higher model performance than using $D_{gen}$. The pseudocode of our algorithm is given in \ref{apdx:alg}

% 移到附录
% \begin{algorithm}[!h]
% 	\caption{The Algorithm of the Proposed SynAlign} 
% 	\label{alg2} 
% 	\begin{algorithmic}
% 	    \STATE \textbf{Input}: Real dataset $D_{ori}$, Sample uncertainty tracker $U(\cdot)$, Key Attribute Set $A$, Embedding model $F$
% 	    \STATE \textbf{Output}: Original Dataset $D_{gen}$
%         \STATE Mapping $D_{ori}$ to $E_{ori}$ with $F$
%         \STATE Initialize $U(E_{ori})$ with standard GP model with RBF kernel 
%         \STATE Initialize generated text pool $D_{gen}=\emptyset$
%         \WHILE{$max(U(E_{ori})) > \sigma$}  
%             \STATE Select demonstrations $D_{dem}$ with formula 3
%             \STATE Set $U(E_{dem})$ as 0 and update $U(E_{ori})$ with formula 4, 5
%             \STATE Extract key attribute set $S$ from $D_{dem}$ with formula 6
%             \STATE Generate $D_{gen}^{'}$ based on $S$ with formula 7
%             \STATE $D_{gen}$ = $D_{gen}^{'}\cup D_{gen}$     
%         \ENDWHILE 
%         \STATE Mapping $D_{gen}$ to $E_{gen}$ with $F$
%         \STATE Initialize Random Matrix set $\Theta$ with Gram-Schmidt algorithm
%         \STATE Initialize sampling weight $\omega$ for each embedding in $E_{gen}$
%         \STATE Train $\omega$ by minimizing MMD loss between $E_{ori}$ and $E_{gen}$
%         \STATE Resample $D_{gen}$ based on $\omega$ as the final synthetic data $D_{gen}$
%         \STATE \textbf{Return} $D_{gen}$
% 	\end{algorithmic} 
% \label{alg1}
% \end{algorithm}
% \vspace*{-0.5cm}


% \begin{algorithm}[!h]
% 	\caption{SynAlign} 
% 	\label{alg2} 
% 	\begin{algorithmic}
% 	    \STATE \textbf{Input}: Real dataset $D_{ori}$, Generation model $LLM$, Sample uncertainty tracker $U(\cdot)$, Key Attribute Set $A$
% 	    \STATE \textbf{Output}: Synthetic Dataset $D_{gen}$
%             \STATE \textbf{Training Phase:}
% 		\WHILE{Sample B in Training Dataset}  
% 		%\IF{i \% T == 0}
% 		\STATE Conduct feed-forward computation $L(\boldsymbol{y}, f_\theta(\boldsymbol{E}))$
% 		\STATE Compute feature importance with Algorithm 1 with $k_1$
%             \STATE Compute importance regularization loss with equation (\ref{eq8})
% 		\STATE Combined with cross-entropy loss and conduct optimization
% 		\ENDWHILE 
%            \STATE \textbf{Validation Phase:}
%            \WHILE{Sample B in Validation Dataset}  
% 		%\IF{i \% T == 0}
% 		\STATE Compute feature importance with Algorithm 1 with $k_2$
%             \STATE Aggregate feature importance for each batch
% 		\ENDWHILE 
% 	\end{algorithmic} 
% \label{alg1}
% \end{algorithm}
% \vspace*{-0.5cm}