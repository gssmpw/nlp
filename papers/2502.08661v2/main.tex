\PassOptionsToPackage{table}{xcolor}
\documentclass[sigconf, 9pt]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}
\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmISBN{979-8-4007-1331-6/25/04}
\acmDOI{10.1145/3701716.3715245}
% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.

\settopmatter{printacmref=true}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{enumitem}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{subfigure}
%\usepackage[table]{xcolor} % 引入 xcolor 宏包
% \usepackage{colortbl} % 引入 colortbl 宏包

\usepackage{titlesec}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% \renewcommand\cellset{\renewcommand\arraystretch{0.8}}
% \setlength\extrarowheight{0pt}}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Few-shot LLM Synthetic Data with Distribution Matching}

% \author{Jiyuan Ren$^{1}$, Zhaocheng Du$^{2}$, Zhihao Wen$^{2}$,  Qinglin Jia$^{2}$}
% \author{Sunhao Dai$^{3}$, Chuhan Wu$^{2}$,  Zhenhua Dong$^{2}$}
% \renewcommand{\authors}{Jiyuan Ren, Zhaocheng Du, Zhihao Wen,  Qinglin Jia, 
% Sunhao Dai, Chuhan Wu,  Zhenhua Dong}
% \affiliation{%
%   \institution{$^1$ Tsinghua University  \quad $^2$ Huawei Noah's Ark Lab \quad $^3$ Renmin University of China}
%   }
% \email{rjy22@mails.tsinghua.edu.cn, {zhaochengdu, wenzhihao4, jiaqinglin2, wuchuhan1, dongzhenhua}@huawei.com, sunhaodai@ruc.edu.cn}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{
% Yuang Zhao$^{2*}$, 
% Zhaocheng Du$^{1*}$\authornote{Equal contribution.},
% Qinglin Jia$^{1}$,
% Linxuan Zhang$^{2\dagger}$\authornote{Corresponding authors.},
% Zhenhua Dong$^{1\dagger}$,
% Ruiming Tang$^{1}$}
% \renewcommand{\authors}{
% Yuang Zhao,
% Zhaocheng Du,
% Qinglin Jia,
% Linxuan Zhang,
% Zhenhua Dong,
% Ruiming Tang}
% \affiliation{%
%   \institution{
%   $^1$Noah's Ark Lab,
%   Huawei \quad 
%   $^2$ Tsinghua University
%   }
%   \country{}
% }
% \email{
% zhaoya22@mails.tsinghua.edu.cn,
% lxzhang@tsinghua.edu.cn}
% \email{{
% zhaochengdu,
% jiaqinglin2,
% dongzhenhua,
% tangruiming}@huawei.com}


\author{Jiyuan Ren}
\email{rjy22@mails.tsinghua.edu.cn}
\affiliation{%
  \institution{Tsinghua University}
  \city{Beijing}
  \country{China}}
\authornote{Both authors contributed equally to this research.}

\author{Zhaocheng Du}
\email{zhaochengdu@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Shenzhen}
  \country{China}}
\authornotemark[1]

\author{Zhihao Wen}
\email{wenzhihao4@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Singapore}
  \country{Singapore}}

\author{Qinglin Jia}
\email{jiaqinglin2@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Beijing}
  \country{China}}

\author{Sunhao Dai}
\email{sunhaodai@ruc.edu.cn}
\affiliation{%
  \institution{Renmin University of China}
  \city{Beijing}
  \country{China}}

\author{Chuhan Wu}
\email{wuchuhan@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Beijing}
  \country{China}}


\author{Zhenhua Dong}
\email{dongzhenhua@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Shenzhen}
  \country{China}}
\authornote{Corresponding author.}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Ren and Du et al.}

\begin{abstract}
% 随着大语言模型训练技术的完善，大语言模型的语言数据生成能力和incontext learning的能力越来越强大。领域内的参与者很容易想到利用大语言模型生成合成数据用来辅助一些在线小模型的训练，从而进一步提升效果。但是大语言模型生成的数据存在语言风格差异过大和语言内容多样性缺乏的问题，导致直接使用LLM生成的数据并不能保证带来效果上的提升。为了解决这些问题，我们提出了一种基于分布对齐的合成数据生成与过滤框架XXX。该首先利用高斯过程主动采样与已生成数据差异较大的样本作为数据生成的参考示例，辅之以chain of latent attribution的prompt方法，生成尽可能覆盖所有语言风格的多样化合成样本。随后利用MMD的分布对齐模块为每一个样本学习出采样概率，确保通过该采样概率采样出的数据可以尽可能的和原数据分布近似。通过这两种技术的组合，XXX能够生成大量与原数据分布近似的样本，从而实现高效的数据合成。我们在多个网络文本预测任务上验证了我们的技术并且取得了显著的的提升。
As large language models (LLMs) advance, their ability to perform in-context learning and few-shot language generation has improved significantly. This has spurred using LLMs to produce high-quality synthetic data to enhance the performance of smaller models like online retrievers or weak LLMs. 
However, LLM-generated synthetic data often differs from the real data in key language attributes (e.g., styles, tones, content proportions, etc.). 
As a result, mixing these synthetic data directly with real data may distort the original data distribution, potentially hindering performance improvements. 
To solve this, we introduce \textbf{SynAlign}: a synthetic data generation and filtering framework based on key attribute distribution matching.
Before generation, SynAlign employs an uncertainty tracker surrogated by the Gaussian Process model to iteratively select data clusters distinct from selected ones as demonstrations for new data synthesis, facilitating the efficient exploration diversity of the real data. Then, a latent attribute reasoning method is employed: the LLM summarizes linguistic attributes of demonstrations and then synthesizes new data based on them. This approach facilitates synthesizing diverse data with linguistic attributes that appear in real data.
After generation, the Maximum Mean Discrepancy is used as the objective function to learn the sampling weight of each synthetic data, ensuring distribution matching with the real data. Our experiments on multiple text prediction tasks show significant performance improvements. We also conducted an online A/B test on an online retriever to demonstrate SynAlign's effectiveness. Our code is available \href{https://github.com/nighood/SynAlign}{\textcolor{blue}{here}}.
\end{abstract}


\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language generation}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Synthetic Data, Large Language Model, Data Augmentation}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\maketitle

\input{sections/1_introduction}
\input{sections/2_relatedwork}
\input{sections/3_method}
\input{sections/4_experiments}
\input{sections/5_conclusion}

\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{ref}

\clearpage
\appendix

\input{sections/6_appendix}

\end{document}

