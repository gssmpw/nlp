\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}

\usepackage{hyperref}

\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Semantic to Structure: Learning Structural Representations for Infringement Detection\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Chuanwei Huang}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% huangcw@pku.edu.cn}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Zexi Jia}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Hongyan Fei}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }
%%%%%% START：作者信息设置的核心代码
\author{\IEEEauthorblockN{
Chuanwei Huang\IEEEauthorrefmark{2},
Zexi Jia\IEEEauthorrefmark{2}, 
Hongyan Fei\IEEEauthorrefmark{2}, 
Yeshuang Zhu, 
Zhiqiang Yuan, 
Jinchao Zhang*, 
Jie Zhou* }
\IEEEauthorblockA{Peking University, Beijing, China \\  Wechat AI, Tencent, China}
}


\maketitle

\begin{abstract}
Structural information in images is crucial for aesthetic assessment, and it is widely recognized in the artistic field that imitating the structure of other works significantly infringes on creators' rights. The advancement of diffusion models has led to AI-generated content imitating artists' structural creations, yet effective detection methods are still lacking. In this paper, we define this phenomenon as "structural infringement" and propose a corresponding detection method. Additionally, we develop quantitative metrics and create manually annotated datasets for evaluation: the SIA dataset of synthesized data, and the SIR dataset of real data. Due to the current lack of datasets for structural infringement detection, we propose a new data synthesis strategy based on diffusion models and LLM, successfully training a structural infringement detection model. Experimental results show that our method can successfully detect structural infringements and achieve notable improvements on annotated test sets.
\end{abstract}

\begin{IEEEkeywords}
Image Infringement, Diffusion Models, Contrastive Learning
\end{IEEEkeywords}
% intro 和related work一起
\section{Introduction}

% % 1. AIGC 存在侵权，同时不只是语义侵权, 也可能是结构侵权，同时强调结构信息的重要性 （反映了每张图像的独特性， 是aesthetic assessment的重要组成部分）

% The rapid advancement of diffusion models~\cite{podell2023sdxl} has led to the generation of high-quality images, but it has also raised concerns about potential infringement issues. 


% While much of the focus has been on the semantic infringement of images, it is imperative to recognize that structural information is equally crucial in the evaluation of images. 

The structural information of images has long been an important aspect of aesthetic assessment~\cite{zhang2021image}. Image structure encompasses the geometric structure and the arrangement of visual elements with images, making it a critical aspect of artistic and commercial creations. Therefore, many artistic works mimic the structural composition of other works rather than directly imitating their content to avoid detection. On the other hand, the rapid development of diffusion models~\cite{ho2020denoising,podell2023sdxl} enables the synthesis of high-quality images, but it also exacerbates the issue of image infringement.
Previous studies~\cite{somepalli2023understanding, somepalli2023diffusion, he2024fantastic, wang2024evaluating} have highlighted that images produced by diffusion models may infringe on existing works from different perspectives, including both semantic and structural. These cases of image infringement greatly damage the copyright of creators, necessitating the detection of image infringement.

Commonly-used infringement detection methods predominantly focus on semantic infringement. 
These detection methods often overlook cases where images exhibit high structural similarity but low semantic similarity, as illustrated in Fig.~\ref{fig}. In this work, we refer to this phenomenon as \textit{structural infringement}.
To efficiently detect structural infringement, we introduce a novel type of image representation, termed the \textit{Image Structural Representation}, which describes the geometric and positional information within images in a fine-grained manner. 
% It holds significant potential for various applications, particularly in the domain of structural infringement detection and structure-based image retrieval.
\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.85\textwidth]{fig/intro.pdf}}
\caption{Structural infringement image pairs in SIR and SIA datasets. (a): The SIR dataset encompasses image pairs that exhibit structural infringement in the real world. (b) The SIA dataset includes image pairs with structural infringement generated by diffusion models, with real images on the left and synthetic images on the right. Despite the low content similarity, these pairs exhibit high structural similarity, indicating potential structural infringement.}
\label{fig}
\end{figure*}

Traditional image representation learning methods primarily utilize self-supervised learning methods for image representation learning. Methods like MoCo \cite{chen2021empirical} and DINO \cite{oquab2023dinov2} \cite{caron2021emerging} leverage different data augmentations to create multiple views of the same image.
By maximizing the agreement between different views, the model learns invariance to these data augmentations, thereby extracting rich semantic information.
Although these methods are effective at extracting rich semantic information, they struggle to extract meaningful structural information. This is due to the current lack of datasets that possess only similar structures, preventing the extraction of structural information. 
To tackle the issue of data scarcity, we propose a novel data synthesis pipeline that generates image pairs with similar structural information but different semantic content. These synthesized pairs are then used to train the image structural representation extractor, achieving a comprehensive understanding of images and providing a robust framework for structural infringement detection.

Previous work most closely related to our image structural representation is image layout representation~\cite{zhao2024self, 9577542, hou2020object}, which converts image layouts into concise vectors to support various downstream tasks such as image layout classification and retrieval. 
% HLAGCN~\cite{9577542} utilizes Graph Convolutional Network(GCNs) to embed image layout information, thereby achieving improved performance in the image aesthetics assessment task. 
In~\cite{zhao2024self}, primitives in the image are used to construct a heterogeneous graph and subsequently learn the image layout representation in a self-supervised manner. 
However, layout primarily describes the coarse-grained arrangement of elements within images, whereas structural infringement detection requires finer-grained structural information, including geometric and positional information.

In this work, we extend the concept of structural combination information in aesthetic assessment to AIGC images, and we decouple structural information from semantic information. We define \textit{structural infringement} as the occurrence where one image, whether human-created or AI-generated, infringes upon the structural information of another image.
To efficiently detect structural infringement, we propose to train a model dedicated to extracting structural information from images, which we term as image structural representation.
% More specifically, we utilize contrastive learning to extract the common structural information from pairs of images that differ in content but share similar structures. Due to the current lack of such data, we employ SDXL and ControlNet~\cite{zhang2023adding} to artificially synthesize data for training. 
The primary contributions of this work are as follows:
\begin{itemize}
    \item We analyze the phenomenon of infringement in realistic and synthetic images from the perspective of structural information and define this phenomenon as structural infringement.
    \item We propose to extract structural compositional information from images and design a novel data synthesis strategy for learning image structural representations.
    \item To evaluate the capabilities of different methods, we construct two manually annotated structural infringement test sets, SIA and SIR dataset. Our proposed method achieved state-of-the-art results on these datasets.
\end{itemize}
The SIA and SIR datasets are available at: \href{https://drive.google.com/drive/folders/1jQNyt5IEP2Lx5G0APCZDbK8vCLB_oijX?usp=sharing}{\textit{dataset link}}.
% The downloading link for the testsets is TODO.
\section{Methods}
% One of the major challenges in the task of extracting Image Structural Representation is the lack of annotated image datasets with structural information. The closest definition to our Image Structural Representation is Image Layout Representation learning. However, as shown in Figure~\ref{fig}(a), the latter learns the overall layout information of the image, whereas we aim to extract more detailed image structural information, including geometric and positional information as shown in Figure~\ref{fig}(b).
Due to the current lack of research on learning image structural information, there is a shortage of relevant training data and corresponding benchmarks.
To address this issue, we propose a novel data synthesis pipeline to generate image pairs that have high structural similarity but low semantic similarity, as illustrated in Fig.~\ref{fig:data}. Base on this, we train a image structural representation extractor.

With the development of diffusion models, it becomes possible to generate photo-realistic images and accept various conditions.
% Recent RPG-Diffusion~\cite{yang2024mastering} utilizes MLLMs as region planners to control the overall structure of images. While high-quality images can be generated, there remains a lack of fine-grained structural control as shown in Fig.~\ref{fig:example}. 
ControlNet~\cite{zhang2023adding} integrates guidance signals into diffusion models, enabling more precise control over the generative process and supporting various condition signals. Consequently, we choose to use "SDXL~\cite{podell2023sdxl} + ControlNet" to generate images that retain structures similar to the source image.
Given a source image $x_{\text{src}}$, we use the DPT~\cite{Ranftl_2021_ICCV} model to generate its depth map, which is then used as a control condition for ControlNet to ensure the generated image $x_{\text{syn}}$ has a similar structure. The depth map is used as the control condition because it preserves the rough geometric and positional information of the main elements within the image. Other control conditions, such as Canny edges, may include excessively detailed information, resulting in a generation quality decrease.


\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.9\textwidth]{fig/pipeline.pdf}}
\caption{Data synthesis pipeline. (a) Given a source image with a caption description, the depth map is first extracted using DPT to capture the main structural information. Subsequently, the LLM is used to modify the attributes of the main objects in the caption to change the semantic information. (b) The text and image condition are then input into SDXL and ControlNet respectively, generating images with high structural similarity but low semantic similarity to the source image.}
\label{fig:data}
\end{figure*}


However, a potential issue with the aforementioned approach is that the generated images are not only structurally similar to the source image but also semantically similar. This high degree of semantic similarity is not conducive to our subsequent training of the image structural representations. To address this, we employ a Large Language Model (LLM) to rewrite the caption corresponding to the source image. By altering the categories and attributes of the main subjects in the image, we aim to reduce the semantic similarity between the generated and source images. Furthermore, we optionally modify the style in the caption to change stylistic information. 

Training a structural representation extractor from scratch necessitates a substantial amount of labeled data and significant training time. Therefore, we opt to fine-tune pre-trained visual models to enhance their focus on the structural information.
More specifically, we fine-tune a pretrained image encoder using contrastive learning~\cite{chen2020simple, he2020momentum}. 
Contrastive learning has proven to be a powerful technique for self-supervised representation learning. The core idea involves generating two different views of the same image through various data augmentation. These views are then used to train the model such that the representations of the same image are as similar as possible, while the features from different images are pushed apart. 
In this way, the model successfully learns features invariant to various data augmentations, thereby extracting effective semantic information.

In our task, given a pair of images $[x^{\text{src}}_i, x^{\text{syn}}_i]$, generate \(x^{\text{syn}}_i\) from \(x^{\text{src}}_i\) using a diffusion model can be regarded as a complex off-line data augmentation retaining only structural information. 
Following the training paradigm of MoCo, we first input the two images into extractor $E$ and momentum extractor $E'$, obtaining l2 normalized features $\mathbf{f_i}$ and $\mathbf{f_i}'$ respectively. Unlike MoCo, which requires computing the loss after mapping through a predictor, we directly calculate the loss using the features obtained from the extractor. This approach enhances the performance in subsequent retrieval for structural infringement.
We use the InfoNCE loss to maximize the consistency between $\mathbf{f_i}$ and $\mathbf{f_i}'$, and minimize it between $\mathbf{f_i}$ and $\mathbf{f_j}'$, thereby constraining the network to extract only the structural representation. The loss is calculated as follows:
$$  \mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\mathbf{f_i} \cdot \mathbf{f_i}' / \tau)}{\exp(\mathbf{f_i} \cdot \mathbf{f_i}' / \tau) + \sum_{j} \exp(\mathbf{f_i} \cdot \mathbf{f_j}' / \tau)},$$
where the temperature parameter $\tau$ is set to 0.2.





% \subsection{Structure-based Retrieval}
% While the model fine-tuned using only synthetic dataset has already surpassed the original baseline in extracting structural representations, relying solely on synthetic data may lead to issues of insufficient generalization and robustness~\cite{singh2024synthetic}. To address this, we perform retrieval on a large-scale real dataset to construct a real training set $D_{real}$. Specifically, we first use \(E_{\text{init}}\) to extract the structural representations of the real dataset. For each anchor, we then perform KNN retrieval to find \(k\) nearest neighbor features. After removing duplicate pairs using SSCD, we filter based on the similarity between the anchor and the neighbors according to a threshold. It is important to note that image pairs with excessively high similarity may also exhibit extremely high semantic similarity, which is detrimental to learning structural representations and should be excluded from the training set. Consequently, the final similarity filtering range is set to 0.9-0.95. Finally, we combine the synthetic dataset with the real dataset to train the final feature extractor, using the same training method as shown in Fig.~\ref{fig:train}.

\section{Experiments}

\subsection{Datasets}
\textbf{Testset}: We construct two test datasets to evaluate the model's performance on structural infringement tasks: the Structural Infringement of Artworks (SIA) Dataset and the Structural Infringement of Real Images (SIR) Dataset.

\textit{SIA Dataset} is constructed using a synthetic approach. Initially, 2,000 art images of various styles are randomly selected from WikiArt. Subsequently, we employ the data synthesis pipeline in Fig.~\ref{fig:data} to generate infringing images. Since determining structural infringement ultimately requires subjective human evaluation, each pair of data is manually rated on a scale of 1 to 5, with higher scores indicating a greater degree of infringement. Pairs with average scores greater than 4 are retained, resulting in a testset of 513 pairs.

\textit{SIR Dataset} comprises images manually collected from real-world cases of alleged structural copyright infringement, encompassing various artistic styles such as photographs, comics, and posters. These images undergo a manual filtering process similar to SIA dataset, resulting in the retention of 30 image pairs. Examples from the test sets are shown in Fig.~\ref{fig}.

\textbf{Trainset}: We select 10,000 detailed-caption images from the COCO~\cite{lin2014microsoft} 2017 trainset to generate synthetic training data. GPT-4o rewrites the captions as text conditions for SDXL. We choose to use realistic data instead of artistic works to prevent overfitting due to excessive similarity with the testset.
% and maintains consistency with the pre-training data type, thereby avoiding performance drops during fine-tuning.

\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.95\textwidth]{fig/sim.pdf}}
\caption{Top-1 retrieval image on SIA datasets using DINO and our proposed image structural representation. For each pair, the left image is the query, and the right image is the retrieval result. The cosine similarity scores for each pair are shown below.}
\label{fig:sim}
\end{figure*}
\subsection{Implementation Details}
We use the pretrained ViT-L~\cite{dosovitskiy2020image} from DINOv2 as the backbone and use LoRA~\cite{hu2021lora} to fine-tune the model to improve training efficiency, with the hyperparameter \( r = 3 \). We set the initial learning rate to \( 1 \times 10^{-4} \) and utilize a cosine learning rate decay schedule. The AdamW~\cite{loshchilov2017decoupled} optimizer is employed for training the model. 
We utilize the Faiss~\cite{johnson2019billion} library to conduct the k-nearest neighbor search for testing.


\subsection{Evaluation}
We evaluate the model's ability to detect structural infringement following the image copy detection~\cite{pizzi2022self} paradigm. Given a query image, we retrieve the most likely infringing reference image from the candidate gallery, resulting in a list of \textbf{\{query, reference\}} pairs with confidence scores. We generate precision-recall curves by adjusting the confidence threshold and use average precision ($\mu AP$) to assess overall performance, similar to instance recognition~\cite{perronnin2009family}. The calculation is as follows:
$$\mu AP = \sum_{i=1}^{N} p(i) \Delta r(i),$$
where \( p(i) \) is the precision at position \( i \) of the sorted precision-recall list, \( \Delta r(i) \) is the difference between the $i$th and $(i-1)$th recall, and \( N \) is the total number of returned predictions. 

We also use features extracted from different methods for image retrieval, visualize their top-1 retrieval results, and report the mAP metrics.

\subsection{Qualitative Results}
To demonstrate that our proposed method primarily extracts structural information from images, we perform image retrieval on the SIA dataset using our learned structural representation and DINO feature. As shown in Fig.~\ref{fig:sim}, the images retrieved using DINO often contain objects of the same category as the query image (e.g., dogs, bathtubs, and vehicles), even though they are not structurally consistent.
In contrast, our proposed structural representation primarily extracts structural information from images. It yields high similarity scores for images with similar structures, whereas images with dissimilar structures will have low similarity scores even if they contain the same semantic concepts.

\subsection{Quantitative Results}
We compare the performance of our proposed image structural representation against other classical image representation learning methods for structural infringement detection DINOv2 and MoCoV3, and the traditional image copy detection method SSCD.
All methods are evaluated with ViT-L as the backbone, except for SSCD, which was tested with both ResNet50~\cite{he2016deep} and ResNeXt101~\cite{xie2017aggregated}.
We compare their performance on the SIA and SIR testsets in Table~\ref{tab:sia} and Table~\ref{tab:sir}.
Due to limited data in the SIR dataset, we add 20,000 images to expand the retrieval gallery. Results show our method significantly outperforms others in detecting structural infringement. 
The performance rankings of different methods on the SIA dataset are roughly consistent with those on the SIR dataset, indicating that our data synthesis pipeline can partially reflect real-world structural infringement phenomena. 
% Notably, all methods perform worse on SIA than on SIR, especially for other methods. This is because SIA also modifies the style of generated images during its construction, while many SIR image pairs share the same style, such as movie posters. This makes SIA more challenging than SIR and suggests that our structural representation is more robust against semantic and style interference.



% \begin{table}
% \centering
% \label{tab:exp}
% \caption{Comparisons of $\mu AP$ on SIA and SIR datasets.}
% \begin{tabular}{c|c|c|c|c|c} 
% \hline
% Datasets    & SSCD-50 & SSCD-101 & DINOv2~ & MoCov3 & Ours   \\ 
% \hline
% SIA & 0.102         & 0.110            & 0.129   & 0.120   & \textbf{0.365} \\
% SIR & 0.331         & 0.430            & 0.416   & 0.480   & \textbf{0.569}  \\
% \hline
% \end{tabular}
% \end{table}

% \usepackage{array}


\begin{table}
\centering
\caption{Performance comparison on SIA dataset}
\label{tab:sia}
\begin{tabular}{>{\centering\hspace{0pt}}m{0.15\linewidth}|>{\centering\hspace{0pt}}m{0.12\linewidth}|>{\centering\hspace{0pt}}m{0.12\linewidth}|>{\centering\hspace{0pt}}m{0.12\linewidth}|>{\centering\arraybackslash\hspace{0pt}}m{0.16\linewidth}} 
\hline
         & $\mu AP$            & mAP@1 & mAP@5 & mAP@10  \\ 
\hline
SSCD-50  & 0.102          & 0.275  & 0.328    & 0.339  \\
SSCD-101 & 0.110           & 0.279  & 0.345    & 0.355   \\
DINOv2~  & 0.129          & 0.415 & 0.496    & 0.510 \\
MoCoV3   & 0.120           & 0.359 & 0.434    & 0.445  \\
Ours     & \textbf{0.365} & \textbf{0.667} & \textbf{0.734}    & \textbf{0.743}  \\
\hline
\end{tabular}
\end{table}

% \usepackage{array}

\begin{table}
\centering
\caption{Performance comparison on SIR dataset}
\label{tab:sir}
\begin{tabular}{>{\centering\hspace{0pt}}m{0.15\linewidth}|>{\centering\hspace{0pt}}m{0.12\linewidth}|>{\centering\hspace{0pt}}m{0.12\linewidth}|>{\centering\hspace{0pt}}m{0.12\linewidth}|>{\centering\arraybackslash\hspace{0pt}}m{0.16\linewidth}} 
\hline
         & $\mu AP$            & mAP@1 & mAP@5 & mAP@10  \\ 
\hline
SSCD-50  & 0.376         & 0.400  & 0.462    & 0.466  \\
SSCD-101 & 0.450           & 0.467  & 0.511    & 0.515   \\
DINOv2~  & 0.461          & 0.533 & 0.615    & 0.632  \\
MoCoV3   & 0.496          & 0.567 & 0.646    & 0.646 \\
Ours     & \textbf{0.527} & \textbf{0.633} & \textbf{0.667}   & \textbf{0.671}  \\
\hline
\end{tabular}
\end{table}



\section{Conclusion}
In this work, we define the task of detecting structural infringement in both authentic artistic images and those generated by diffusion models. The challenge of this task lies in the lack of training data. We introduce a novel data synthesis pipeline to create image pairs with high structural similarity and low semantic similarity. Using this synthesized data, we extract image structural representations to effectively detect structural infringement. Additionally, we develop two test sets to evaluate detection capabilities. Detecting structural infringement is crucial for protecting creators' rights and advancing AIGC technologies. We hope this work provides valuable insights and inspiration for related fields.
% In this work, we addressed the overlooked issue of structural infringement in image generation by diffusion models, which traditional methods focusing on semantic content fail to detect. We introduced a novel detection method using \textit{Image Structural Representation} to capture fine-grained geometric and positional information within images. By leveraging both synthetic and real-world datasets, our approach effectively identifies structural similarities, providing a robust framework for structural infringement detection and advancing the field of image retrieval and analysis.
% By integrating fine-grained structural information, our approach extends the capabilities of existing image analysis tasks, offering new avenues for research and application.

 






% 之前的不足

%我们的贡献



% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \bibitem{b8} D. P. Kingma and M. Welling, ``Auto-encoding variational Bayes,'' 2013, arXiv:1312.6114. [Online]. Available: https://arxiv.org/abs/1312.6114
% \bibitem{b9} S. Liu, ``Wi-Fi Energy Detection Testbed (12MTC),'' 2023, gitHub repository. [Online]. Available: https://github.com/liustone99/Wi-Fi-Energy-Detection-Testbed-12MTC
% \bibitem{b10} ``Treatment episode data set: discharges (TEDS-D): concatenated, 2006 to 2009.'' U.S. Department of Health and Human Services, Substance Abuse and Mental Health Services Administration, Office of Applied Studies, August, 2013, DOI:10.3886/ICPSR30122.v2
% \bibitem{b11} K. Eves and J. Valasek, ``Adaptive control for singularly perturbed systems examples,'' Code Ocean, Aug. 2023. [Online]. Available: https://codeocean.com/capsule/4989235/tree
% \end{thebibliography}

\bibliographystyle{ieeetr}
\bibliography{main}
\end{document}
