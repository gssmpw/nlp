\section{Related Work}
\paragraph{Multi-objective planning} Multi-objective planning is gaining increasing attention. Common approaches to solve multi-objective planning problems include scalarization~\cite{wilde2024scalarizing,agarwal2022multi,van2013scalarized}, lexicographic planning~\cite{wray2015multi,saisubramanian2021multi,mouaddib2004multi,skalse2022lexicographic}, constraint optimization~\cite{deb2016multi,deb2001nonlinear}, and Pareto optimization~\cite{aydeniz2024entropy,nickelson2024redefining}. \emph{Scalarization} combines multiple objectives into a single objective value using weighted sum. While it is a popular approach, it often requires non-trivial parameter tuning~\cite{roijers2013survey}. Our work is based on the \emph{lexicographic} formulation that considers a lexicographic ordering over objectives and solves them sequentially.
While the lexicographic Markov decision process (LMDP)~\cite{wray2015multi} can support multiple state partitions, each corresponding to a different objective ordering, it suffers from the following limitations: (1) lack of a principled approach to define state partitions, especially when a state could be associated with multiple contexts such as weather, road conditions, and time of day; (2) hard-coded partitions and lack of explicit state-to-context mapping which makes it difficult to adapt to newer settings where the set of contexts may change; (3) does not support scenarios where the context influences the reward function; and (4) lack of tools to resolve conflicts in action recommendation that arise from solving partitions sequentially, with no principled approach to determine the order. 
The CLMDP framework addresses these shortcomings and facilitates smooth transitions between contexts, avoiding the risk of conflicting actions. The explicit state-to-context mapping in CLMDP offers flexibility and scalability in handling new scenarios, \emph{without} requiring manual interventions to redefine partitions or objective priorities. 

\paragraph{Constraint optimization} approach optimizes a single objective and converts other objectives into constraints~\cite{deb2016multi,deb2001nonlinear}. This approach cannot efficiently balance the trade-offs between different objectives~\cite{deb2010efficient}.
\emph{Pareto} optimization finds non-dominated solutions across objectives~\cite{aydeniz2024entropy,nickelson2024redefining}, making it unsuitable for scenarios where specific preference orderings must be satisfied.

\paragraph{Context-aware planning} 
Prior works use contextual information to determine environment dynamics and rewards~\cite{benjamins2022contextualize,kim2023context,modi2018markov}, or represent specific configurations like obstacle layouts and operational areas~\cite{bahrani2008collaborative,hvvezda2018context,ter2010context}. While many definitions of context exist for different problem settings, we focus our discussion on those most pertinent to multi-objective scenarios. In multi-objective settings, context has been used to assign scalarization weights~\cite{yang2019generalized} but this approach struggles to scale in larger state spaces. We integrate contextual information in a lexicographic setting to enable planning with different objective ordering in different regions of the state space, with associated reward functions determined by the context. 

\paragraph{Bayesian Inference} It is commonly used to infer values of unknown parameters by incorporating prior knowledge and updating beliefs as new information becomes available~\cite{belakaria2020uncertainty,zhi2020online,abdolshah2019multi}.
Bayesian inference is often used in 
inverse reinforcement learning (IRL) to estimate reward functions~\cite{ramachandran2007bayesian,pmlr-v119-brown20a}, and goal inference in multi-agent settings~\cite{ullman2009help,zhi2020online}. In multi-objective optimization problems, Bayesian methods are used to approximate the set of Pareto optimal solutions for competing objectives~\cite{belakaria2020uncertainty,pmlr-v162-daulton22a}, and identify solutions that satisfy user-defined preference ordering over competing objectives~\cite{abdolshah2019multi}.
In this work, we apply Bayesian inference to determine the most likely context of a state, using limited number of expert trajectories.

%%%%%%%%%%%%%%%%%%%%