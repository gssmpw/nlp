% \vskip -1em
\section{Experiments}
To validate the effectiveness of our proposed GECC, we compare it against classic and SOTA baselines in both non-evolving and evolving scenarios. We first detail the experimental setup, including the construction of benchmark datasets and the experimental settings of each method. Next, we present the node classification performance as a measure of condensation results, alongside a comparison of efficiency. Finally, we empirically demonstrate the effectiveness of the feature aggregation module and the importance of the specific incremental initialization design in our method.  
% Our code is provided in \url{https://anonymous.4open.science/r/GECC-F4CA}. 

\vskip -1em
\subsection{Experimental Setup}

\textbf{Datasets and Baselines.} Following most of the GC papers, we select seven datasets: five transductive datasets, i.e., Citeseer, Cora \citep{kipf2016semi}, Pubmed \citep{namata2012query}, Ogbn-arxiv, and Ogbn-products \citep{hu2020open} and two inductive datasets, Flickr and Reddit \citep{zeng2019graphsaint}. All training graphs are randomly divided into five subsets, each preserving the original class distribution. For transductive graphs, nodes in training set are split, whereas inductive graphs are partitioned into subgraphs. Subsequently, the training sets are incrementally enlarged—for example, the first subset forms $G_1$, and the first plus the second subset forms $G_2$ as formulated in Section~\ref{sec:evolving_form}.
For additional dataset details, please refer to Appendix~\ref{app:statistics}.  We compare GECC with most effective and efficient GC methods, encompassing a diverse range of optimization strategies: (1) gradient matching-based: GCond and GCondX \citep{jin2021graph}; (2) distribution matching-based: GCDM \citep{liu2022graph} and SimGC \citep{xiao2024simple}; (3) trajectory matching-based: GEOM \citep{geom}. We also include node selection baselines (Random~\cite{jin2021graph}, KCenter~\cite{sener2017activekcenter}, Herding~\cite{welling2009herding}) to better compare performance--efficiency trade-offs. The condensed graphs are evaluated using a standard GCN trained for 300 epochs with a learning rate of 0.01, as suggested by GC4NC~\cite{gong2024gc4nc}. The GCN is trained on the synthetic dataset and validated and tested on the original validation and test sets. We also list the results from training a standard GCN in whole dataset for both two settings to show the potential upper bound for graph condensation.

\input{tabs/2main_results}
\textbf{Implementation Details.} \label{sec:hyper}
To ensure a fair reproduction and comparison of baseline methods, we use the best hyperparameters reported in their original papers. For intermediate evaluation, we follow the GC4NC benchmark~\cite{gong2024gc4nc}, which restricts the number of evaluations to 10 during the whole condensation process. 
All baselines adopt the training from scratch strategy in evolving stetting, i.e., do not reuse the previous condensed graphs.
For the hyperparameters of our method, we tune them within a limited range, specifically $\alpha_0, \alpha_1, \alpha_2 \in [-0.3,0.9]$ with 0.1 interval. We introduce a negative offset to capture heterophilious properties in graphs~\cite{zhu2021graphheterophily,gong2023neighborhood}. Following prior work, we fix the maximum propagation depth in Equation~\ref{equ:prop} at $K=2$. 
In addition, learning rate, epochs and dropout of downstream GCN are all fixed as 0.01, 300 and 0.5, except the weight decay is selected from
$\{0.001, 0.0005\}$. We use soft clustering for small datasets and the repeat times are set to 50. The fuzziness are selected from $\{1.0,\; 1.1,\; 1.3\}$, respectively. For large datasets, we run standard hard $k$-means only one time. The maximum number of iterations and the $k$-means threshold are set to 300 and $1 \times 10^{-8}$, respectively.  All experiments are run ten times then we report the average. See more details in Appendix~\ref{app:exp}.

\input{tabs/transferability}

\vskip -1em
\subsection{Performance and Efficiency Comparison in the non-Evolving and Evolving Setting}

\subsubsection{Performance Comparison}
To compare the effectiveness of GECC with the baselines, we utilize the generated condensed graph data to train a standard GCN, reporting both test accuracies and standard deviations in Table \ref{tab:main}. For each dataset, we present two test accuracies: one for \textit{Non-Evolving} setting, reflects the test accuracy achieved by training the GCN on the graph at the final time step, $t = T_5$, corresponding to the largest possible graph size. Additionally, Figure \ref{fig:evolving} demonstrates the test accuracy across different time steps which compares the GECC evolving capability with the existing baselines. 
one from the \textit{Evolving} setting, which represents the average test accuracy across five time-steps as the graph evolves. This demonstrates the dynamic performance of the condensation method as the graph grows. Our analysis reveals several key insights: 

\textbf{Non-evolving setting --} GECC outperforms the baselines on almost all datasets, including the \emph{whole} dataset performance for some datasets, which highlights the effectiveness of our training-free approach in the non-evolving setting. The only exceptions are observed in the \textit{Flickr} and \textit{Ogbn-arxiv} datasets, where GECC ranks second by a minimal margin. In addition, unlike other baselines such as GCond, GECC does not fail on extremely large datasets like \textit{Ogbn-products} due to memory constraints. 

\textbf{Evolving setting --} By analyzing Table ~\ref{tab:main}, we observe that GECC consistently outperforms existing baselines across almost all datasets, often by a large margin. The only exception is \textit{Ogbn-arxiv}, where GECC ranks second. \textbf{However}, Figure~\ref{fig:evolving} highlights that GECC achieves nearly its maximum possible test accuracy even at very early time steps. For instance, on \textit{Ogbn-arxiv}, it surpasses 65\% accuracy by the second time step, a feat unmatched by existing baselines. This demonstrates GECC’s efficiency in leveraging limited data for superior generalization, whereas other methods struggle to reach comparable performance early in the training process. 
\textbf{Moreover}, we observe that GECC surpasses the performance achieved on the whole dataset for relatively smaller graphs such as \textit{Cora}, \textit{Citeseer}, and \textit{Pubmed}. This result highlights the effectiveness of our training-free condensation approach in achieving a "lossless" objective during graph evolution. \textbf{Additionally}, Figure~\ref{fig:evolving} illustrates the robustness of GECC in steadily improving performance as training size increases. This trend fails to hold for baselines, particularly for GEOM on \textit{Reddit}. We conjecture that the trajectory matching method heavily relies on the performance of the pre-trained GNN, making it susceptible to the twofold influence of data size changes: first, during the pre-training stage, and second, during the condensation stage.

% These findings collectively establish GECC as the most effective method for evolving graph scenarios, ensuring both scalability and superior condensation quality.

% \vskip -4em
\subsubsection{Efficiency Comparison}
Comparing GECC with the baselines in Figure~\ref{fig:accuracy_vs_time}, it is evident that GECC exhibits superior efficiency and scalability. It maintains stable performance while effectively managing computational resources as the graph evolves. Although certain model-based GC methods such as GEOM may slightly outperform GECC at specific time steps, GECC achieves over 100 times faster condensation time and demonstrates a significantly slower increase in computational overhead. For results in more datasets, please refer to Appendix~\ref{app:exp}.

% \begin{figure*}[t]
%     \centering
%     % Subfigure 1: Small Evolution
%     \begin{subfigure}[b]{0.8\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/Evolve_small-cropped.pdf}
%         \label{fig:evolve_small}
%     \end{subfigure}
%     \hfill
%     % Subfigure 2: Large Evolution
%     \begin{subfigure}[b]{0.6\textwidth}
\begin{figure*}[t!]
        \centering
        \includegraphics[width=\linewidth]{figs/main_merged-cropped.pdf}
    % \vskip -1em
    \caption{Comparison of test accuracy of different GC methods across five time steps.}
    \label{fig:evolving}
    % \vskip -1em
\end{figure*}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/Reddit_time_vs_accuracy-cropped.pdf}
    % \vskip -1em
    \caption{Test accuracy vs. condensation time on the \textit{Reddit} dataset (top-left is better).}
    \label{fig:accuracy_vs_time}
    \vskip -1.2em
\end{figure}

% \vskip -1em
\subsubsection{Transferability}
A crucial factor in evaluating GC methods is determining whether the condensed data can effectively train various GNNs from a data-centric perspective. Unlike GECC, which adopts a training-free condensation approach, most existing methods generate condensed graphs that are inherently dependent on the backbone GNN used during condensation, such as GCN \citep{hashemi2024comprehensive,gong2024gc4nc}. This reliance can introduce inductive biases, potentially hindering their adaptability to other GNN architectures. 

Table~\ref{tabs:transfer} shows that the condensed graphs generated by \textsc{GECC} demonstrate robust generalization across diverse architectures when compared to other SOTA baselines. Even in \textbf{Ogbn-arxiv}, \textsc{GECC} outperforms \textsc{GEOM} w.r.t consistency, as indicated by its lower standard deviation across downstream GNN models, highlighting the advantage of \textsc{GECC}'s model-agnostic design.

\subsection{Ablation Studies}
To investigate the effectiveness of each module of our method, we conduct the following ablation studies to study the impact of feature propagation, incremental $k$-means++, and balanced SSE score. 
\subsubsection{Impact of Feature Propagation}
As feature propagation (Equation~\ref{equ:prop}) is crucial for generating informative node representations, we compare \textsc{GECC} against an ablated version without feature propagation (labeled \emph{w/o propagation}) across all time steps. Specifically, for \emph{w/o propagation}, we set \(\alpha_{0}=1\) and all other \(\alpha\) coefficients to \(0\), keeping all other hyperparameters identical. Each experiment is repeated ten times, and we report the average results.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/abla_agg-cropped.pdf}
    % \vskip -1em
    \caption{Performance comparison between \textsc{GECC} and \textsc{GECC} without feature aggregation.}
    \label{fig:abla-prop}
    % \vskip -1.5em
\end{figure}

Figure~\ref{fig:abla-prop} illustrates that \textsc{GECC} with feature propagation outperforms the version without propagation. Notably, in some datasets (e.g., Flickr), the \emph{w/o propagation} approach exhibits a downward trend even as more graph data is introduced, suggesting that the raw node features may contain substantial noise. In contrast, \textsc{GECC} with feature propagation steadily improves as the dataset size increases, highlighting that propagated features effectively mitigate noise and bolster clustering performance.

\subsubsection{Impact of Incremental \(k\)-Means++}
We evaluate the effect of reusing previous condensed results when clustering at each time step. Specifically, we compare \textsc{GECC} against a variant that does \emph{not} reuse prior centroids---denoted \textit{w/o incremental \(k\)-means++}---and instead initializes centroids from scratch via a standard \(k\)-means++ procedure at every time step.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/boxplot_merged-cropped.pdf}
    % \vskip -1em
    \caption{Comparison between \textsc{GECC} and \textsc{GECC} without incremental \(k\)-means++. 
    The boxplots show the distribution (mean and quartiles) of the number of iterations required for clustering.}
    \label{fig:abla-incrementalkmeans++}
    % \vskip -1em
\end{figure}

As shown in Figure~\ref{fig:abla-incrementalkmeans++}, reusing previously learned cluster centers via incremental \(k\)-means++ significantly reduces the required number of iterations for convergence, especially as the graph grows larger. For example, on the \textit{Reddit} dataset at time step~5, incremental initialization only requires about 10\% of the iterations needed when initializing from scratch. We omit results on smaller datasets because they typically converge in fewer than 10 iterations.



\subsubsection{Relation between GC and clustering objective}
The balanced Sum of Squared Errors (SSE) is a key contribution derived from our theoretical analysis. To assess its impact, we perform an ablation study comparing the performance of GECC with and without repetitive clustering. As shown in Table~\ref{tab:comparison_sse}, for instance, on the \textit{Citeseer} dataset, applying repetitive clustering to select the lowest SSE leads to an absolute performance improvement of 2.7\%. These results clearly show that a lower SSE consistently correlates with higher test accuracy.
\input{tabs/abla_rep}