\section{Methodology} 


In this section, we outline our condensation objectives for preserving training data information and generalizing to test data in static graphs. We employ theoretical analysis to simplify these objectives and introduce a scalable condensation method that does not require training GNN models, while offering traceability by establishing clear correspondence between the original and condensed nodes. Building on this scalable approach, we further propose an efficient adaptation to the evolving scenarios.



% \subsubsection{Proposition Statements}
\subsection{A Deep Dive into Condensation Objectives}
Intuitively, the condensation process should preserve sufficient information from training data to maintain GNN training performance while ensuring model generalization to test data. Thus, we divide our discussion into two stages:  \textit{training} and \textit{test}.  To simplify our analysis, we adopt the {Simplified Graph Convolution} (SGC) model as the GNN~\cite{wu2019simplifying}, due to its simpler design, similar filtering behavior to GCN~\cite{kipf2016semi}, and its frequent use as a backbone and evaluation model in numerous graph condensation works~\cite{jin2021graph, jin2022condensing,gong2024gc4nc, xiao2024simple}.


\textbf{Training Stage Objectives.} 
During the training stage, GC aims to preserve the training data information to maintain the performance of GNNs. To reflect this, a natural way is to match the model predictions on the original graph $G$ and its condensed counterpart $G'$. Denote the predictions of an SGC model trained on $G$ as $\hat{\mathbf{Y}}\in\mathbb{R}^{N \times c}$ and those on $G'$ as $\hat{\mathbf{Y}}'\in\mathbb{R}^{N' \times c}$. We have
$\hat{\mathbf{Y}} = \mathbf{F} \mathbf{W}=\mathbf{A}^K \mathbf{X} {\bf W}$ and $\hat{\mathbf{Y}}' = \mathbf{F}' \mathbf{W}'= {\mathbf{A}'}^K \mathbf{X}'{\bf W}' \in \mathbb{R}^{N' \times d}$,
% \begin{equation}
%     \hat{\mathbf{Y}} = \mathbf{F} \mathbf{W}, \quad \hat{\mathbf{Y}}' = \mathbf{F}' \mathbf{W}',
% \end{equation}
where $K$ denotes the number of SGC layers, $ \mathbf{F}$ and $\mathbf{F}'$ represent the propagated feature matrices on $G$ and $G'$, and $ \mathbf{W}$ and $\mathbf{W}'$ are the weight matrices of the SGC model trained on $G$ and $G'$, respectively. 
% where $ \mathbf{F} = \mathbf{A}^K \mathbf{X} \in \mathbb{R}^{N \times d}$ and $\mathbf{F}' = {\mathbf{A}'}^K \mathbf{X}' \in \mathbb{R}^{N' \times d}$  on $G'$ represent the propagated feature matrices on $G$, respectively. $K$ denotes the number of SGC layers. $ \mathbf{W} \in \mathbb{R}^{d \times c}, \mathbf{W}' \in \mathbb{R}^{d \times c} $  are the weight matrices of the SGC model trained on $G$ and $G'$, respectively. 
Note that feature propagation here is more flexible than the SGC model and is not limited to the formulation \( \mathbf{A}^K \mathbf{X} \); rather, it can be expressed as any linear combination of powers of the adjacency matrix~\cite{maurya2022fsgnn}. Below, we refer to the propagated feature as the node \textit{representations}.  {We first define the distance between two matrices as the L2 norm of their difference. If matrices do not have the same shape, a projection matrix can be applied to transform them into a common shape before computing the distance.}
Then, the following theorem holds:
\begin{theorem}    \label{theo:training_stage}
    The prediction distance in the training stage is bounded by the sum of representation distance and parameter distance.
    \begin{equation}
    \| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \| 
    \leq  \| \mathcal{K}(\mathbf{F}) - \mathbf{F}' \| \cdot \| \mathbf{W}' \| + \| \mathbf{F} \| \cdot \| \mathbf{W} - \mathbf{W}' \|
    \end{equation}
    % \begin{equation}
    % dist(\hat{\mathbf{Y}}',  \hat{\mathbf{Y}}) 
    % \leq  dist(\mathbf{F}, \mathbf{F}') + dist( \mathbf{W}, \mathbf{W}')
    % \end{equation} 
    % where $dist(\cdot)$ denotes the distance calculation. 
    where $\|\cdot\|$ denotes the L2 norm and $\mathcal{K}(\cdot)$ can be any projection function that aligns the dimensions of $\hat{{\bf Y}}$ and $\hat{{\bf Y}}'$ or ${\bf F}$ and ${\bf F}'$.

\end{theorem}
We provide proof in Appendix~\ref{app:proof_training_stage}. Note that $\|\mathbf{F}\|$ is a constant, and the weight matrix $\| \mathbf{W}' \|$ is naturally constrained due to regularization techniques during model optimization to control its magnitude.  Therefore, Theorem~\ref{theo:training_stage} indicates that by minimizing the representation distance ($\| \mathcal{K}(\mathbf{F}) - \mathbf{F}' \|$) and parameter distance ($\| \mathbf{W} - \mathbf{W}' \|$) between two graphs, the predictions derived from the condensed graph can be close to those of the original graph.


\textbf{Test Stage Objectives.} At the test stage, our condensation goal is to ensure that the GNN model, trained on condensed training data $G'$, generalizes effectively to the test graph, i.e., achieving a low prediction error on the test data. We slightly abuse the notation of $\mathbf{F}$ to represent the propagated feature matrix in the test graph, and denote the test ground truth label as $\mathbf{Y}$ and the predicted test labels from the model trained on $G'$ as $\hat{\mathbf{Y}}''=\mathbf{F}\mathbf{W}'$.  Then we have the following theorem that provides understanding for the test prediction error:
% i.e., achieving low prediction error on the test data.  Given the test ground truth label  $\mathbf{Y}\in\mathbb{R}^{N\times c}$, the following theorem holds:
% In an inductive setting, the feature matrix \( \mathbf{F} \) can be replaced by \( \mathbf{F}_t \), where \( \mathbf{F}_t \) represents the propagated features in the test graph.
% For GC, the ultimate objective is to achieve effective generalization to the test graph (i.e., minimize the test error) using the model trained on $G'$. We derive that test error can be bounded by the error in parameter matching and representation matching. Specifically, the following theorem holds:
\begin{theorem}   \label{theo:test_stage} 
    The test prediction error of the GNN trained of $G'$ is bounded by the test prediction error of the GNN trained on $G$ plus the parameter distance, as formularized by $ \| \mathbf{Y} - \hat{\mathbf{Y}}'' \| \leq \| \mathbf{Y} - \mathbf{F}\mathbf{W} \| + \|\mathbf{F}\| \cdot \| \mathbf{W}- \mathbf{W}'\|$.
    % \begin{equation*}
    %     \| \mathbf{Y} - \hat{\mathbf{Y}}'' \| \leq \| \mathbf{Y} - \mathbf{F}\mathbf{W} \| + \|\mathbf{F}\| \cdot \| \mathbf{W}- \mathbf{W}'\|.
    % \end{equation*}
\end{theorem}
We provide proof in Appendix~\ref{app:proof_test_stage}. This inequality incorporates both the original test prediction error and the parameter distance. It indicates that by reducing the parameter distance $\| \mathbf{W}- \mathbf{W}'\|$, the test prediction error becomes more tightly bounded, assuming that the original test prediction error \( \| \mathbf{Y} - \mathbf{F}\mathbf{W}\| \) and the propagated feature matrix \( \mathbf{F} \) remain constant. 

\textbf{Summary - Reframing Objectives for Graph Condensation.} 
Our analysis leads us to a new approach to defining the objectives for GC. Theorems~\ref{theo:training_stage} and~\ref{theo:test_stage} suggest that both training and test stage objectives are upper bounded by the parameter distance $\|\mathbf{W} - \mathbf{W}'\|$. Furthermore, the training stage objectives are additionally upper bounded by the representation distance $dist(\mathbf{F}, \mathbf{F}')$. Thus, we introduce a new GC objective that focuses on minimizing both the \textbf{parameter and representation distances} to optimize the overall performance of the condensed graphs.



\subsection{Efficient Optimization for the New Objective}
Revolving around the new condensation objective we propose, we now discuss its efficient optimization and the establishment of a correspondence between original and condensed nodes to enhance traceability. As our objective includes representation and parameter distance, we separate their discussions as follows. 

% In this section, we try to minimize the representation distance with a graph coarsening-based strategy with more traceability and propose leveraging this strategy to bound the parameter distance by ensuring a balanced assignment of original nodes to each synthetic node.

\textbf{Optimizing Representation Distance $ \| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \|$.} As discussed in Theorem 1, $\mathcal{K}(\cdot)$ serves as a projection function that aligns the dimensions of $\hat{\bf Y}$ and $\hat{\bf Y}'$. While there are multiple choices of $\mathcal{K}(\cdot)$, we propose to implement it through a linear mapping that assigns each node in the original graph $G$ to one of the synthetic nodes in $G'$. This mapping is formalized as a function $\pi:\{1,\ldots,N\}\to\{1,\ldots N'\}$, represented by the assignment matrix $\mathbf{P} \in \mathbb{R}^{N \times N'}$. The matrix $\mathbf{P}$ is binary, where $\mathbf{P}_{ij} = 1$ if and only if node $i$ in $G$ is assigned to node $j$ in $G'$, i.e., $\pi(i) = j$.  To align $\hat{\mathbf{Y}}$ with $\hat{\mathbf{Y}}'$, we first transform $\hat{\mathbf{Y}}$ to the corresponding dimension using $\mathbf{P}^\top\hat{\mathbf{Y}}$. We then compute the prediction distance as $\|\mathbf{P}^\top \hat{\mathbf{Y}} - \hat{\mathbf{Y}}'\|$, which modifies the minimization of representation distance as  follows:
% , defined as $\pi:\{1,\ldots,N\}\to\{1,\ldots N'\}$.  Denote this linear mapping as an assignment matrix $\mathbf{P} \in \mathbb{R}^{N \times N'}$ as the \textbf{assignment matrix} of the bipartite graph between original nodes and synthetic nodes.  Specifically, $\mathbf{P}$ is a $0/1$ matrix with $\mathbf{P}_{ij} = 1$ if and only if $\pi(i)=j$. Thus, we first mapping $\hat{\mathbf{Y}}$ to the same dimension with $\hat{\mathbf{Y}}'$ by $\mathbf{P}^\top\hat{\mathbf{Y}}$. Then, we calculate the prediciton distance by $\|\mathbf{P}^\top \hat{\mathbf{Y}}' - \hat{\mathbf{Y}} \| $. The objective of minimizing representation distance satisfies
% Due to the different dimensions between $\mathbf{F}$ and $\mathbf{F}'$, it is non-trivial to calculate their distance directly. Inspired by graph coarsening methods, which groups and merges nodes into super nodes, we assume each original node can be represented by one of the synthetic nodes, and the mapping is defined as $\pi:\{1,\ldots,N\}\to\{1,\ldots N'\}$. We thus have $\mathbf{P} \in \mathbb{R}^{N \times N'}$ as the \textbf{assignment matrix} of the bipartite graph between original nodes and synthetic nodes. Specifically, $\mathbf{P}$ is a $0/1$ matrix with $\mathbf{P}_{ij} = 1$ if and only if $\pi(i)=j$. Thus, we first mapping $\hat{\mathbf{Y}}$ to the same dimension with $\hat{\mathbf{Y}}'$ by $\mathbf{P}^\top\hat{\mathbf{Y}}$. Then, we calculate the prediciton distance by $\|\mathbf{P}^\top \hat{\mathbf{Y}}' - \hat{\mathbf{Y}} \| $. The objective of minimizing representation distance satisfies
\begin{equation}
    \begin{aligned}
         \min\nolimits_{\mathbf{F}'} \|\mathcal{K}(\mathbf{F}) - \mathbf{F}'\|
        = \min\nolimits_{\mathbf{P},\mathbf{F}'} \| \mathbf{P}^\top \mathbf{F} - \mathbf{F}' \|.
    \end{aligned}
    \label{eq:representation_distance}
\end{equation}
By solving the above optimization problem, we can derive the assignment matrix ${\bf P}$ and the propagated features ${\bf F}'$ for graph $G'$. In alignment with the popular structure-free GC paradigm~\cite{geom, sfgc, jin2021graph}, we set $\mathbf{F}'$ as the condensed node features and use an identity matrix $\mathbf{I}$ for the condensed adjacency matrix to form a condensed graph that effectively minimizes the representation distance. Notably, the assignment matrix $\mathbf{P}$ and the condensed features $\mathbf{F}'$ can be efficiently derived using any Expectation-Maximization (EM)-based clustering algorithm, such as $k$-means. Since $k$-means inherently minimizes the Sum of Squared Errors (SSE), this optimization aligns with the objective $\min \|\mathbf{P} \mathbf{C} - \mathbf{F}\|$,
where $\mathbf{C}$ represents the cluster centers of feature points $\mathbf{F}$. The procedure iteratively updates the cluster centroids $\mathbf{F}'$ and the assignment matrix $\mathbf{P}$ until convergence is reached. This approach presents two major advantages: (a) \textbf{Traceability}: It provides clear insight into how each original node contributes to the condensed graph, facilitating a better understanding of the condensation process. With traceability, users can manipulate the condensed graph alongside the original one—for example, easily filtering low-quality data when the corresponding original nodes exhibit poor characteristics.
(b) \textbf{Efficiency and Versatility}: Unlike traditional GC methods that need specific GNN models to be trained, this new method does not require any GNN training or gradient calculation and is not limited to a particular GNN architecture, which greatly improves efficiency.



\textbf{Optimizing Parameter Distance $\|{\bf W} - {\bf W}'\|$.} As demonstrated in the above establishment of structure-free condensed graphs, clustering effectively bridges the gap between original and condensed graphs by minimizing representation distance. Within this framework, we further explore how the parameter distance correlates with the assignment matrix ${\bf P}$ used in clustering through the following theorem.
% \textbf{Optimizing Parameter Distance $\|{\bf W} - {\bf W}'\|$.} As discussed above, clustering  serves as an effective method to achieve a bounded error between the original and condensed graph by minimizing the representation distance. Additionally, applying constraints to the mapping matrix $\mathbf{P}$ obtained through clustering can further help bound the error introduced by parameter differences. Specifically, the following theorem holds:
\begin{theorem}
The parameter distance can be bounded by the following inequality:
    \begin{equation}
        \| \mathbf{W} - \mathbf{W}' \| \leq \mathcal{C}\cdot (\max (\text{diag}(\mathbf{\mathbf{P}^\top\mathbf{P}})))^2,
    \label{equ:balance}\end{equation}
    where $\mathcal{C}=\frac{\|\mathbf{F} \| \cdot \| \mathbf{Y}\|({\lambda_{\min}(\mathbf{F}^\top\mathbf{F})}+\|\mathbf{F} \|)}{{\lambda^2_{\min}(\mathbf{F}^\top\mathbf{F})}}$ is a constant, $\mathbf{P}^\top\mathbf{P} \in \mathbb{R}^{N' \times N'}$ is a diagonal matrix with each diagonal entry corresponding to how many original nodes are assigned to each synthetic node. 
\label{theo:bound_of_parameter}
\end{theorem}
We provide proof of Theorem~\ref{theo:bound_of_parameter} in Appendix~\ref{app:proof_bound_of_parameter}. This theorem demonstrates that the parameter distance \( \| \mathbf{W} - \mathbf{W}' \| \) is upper bounded by the maximum elements in $\mathbf{P}^\top\mathbf{P}$.  It suggests that assigning a balanced number of original nodes to each synthetic node can effectively lower the upper bound of the parameter distance. Thus, we plan to integrate this balancing constraint into our condensation process to facilitate the minimization of parameter distance.



\subsection{Graph Condensation via Clustering}\label{sec:clustering}
With clustering established as a powerful and efficient approach for achieving bounded error in GC, we now shift our focus to its practical implementation. To facilitate a clustering-based GC method, we propose the GECC process. We start with the foundational GC paradigm, which provides an efficient method for generating the condensed graph \(G'_t\) from the static original graph \(G_t\) at time step $t$. Building on this, we extend to an evolving graph setting by introducing the incremental condensation paradigm to handle dynamic graph updates efficiently. GECC takes the original graph at time step \( t \) and the condensed node representations from the previous time step \( t-1 \) as input, generating the condensed node representations for the current time step. GECC process can be divided into two main stages: \emph{(i.)} \textbf{Feature Propagation}, and \emph{(ii.)} \textbf{Representation Clustering}. The details of these steps are explained below. 

\subsubsection{Feature Propagation}

The goal of feature propagation is to propagate information from a node's neighbors to provide each node with a richer representation. To enable a GC procedure that eliminates the need for training, we develop a non-parametric feature propagation module \citep{wu2019simplifying,ma2008bringing,maurya2021improving,hamilton2017inductive} designed to produce node embeddings $\mathbf{F}_t$ in the original graph $G_t$. In our work, by following the propagation method in SGC \citep{wu2019simplifying}, the propagated node representations after \( k \) steps are computed as: $\mathbf{F}_{t,k} = \hat{\mathbf{A}}_t^k \mathbf{X}_t$,
where \( \hat{\mathbf{A}}_t \) is the normalized adjacency matrix of \( G_t \), defined as: $\hat{\mathbf{A}}_t = \tilde{\mathbf{D}}_t^{-\frac{1}{2}} \tilde{\mathbf{A}}_t \tilde{\mathbf{D}}_t^{-\frac{1}{2}}$, with \( \tilde{\mathbf{A}}_t = \mathbf{A}_t + \mathbf{I} \) being the adjacency matrix with self-loops, and \( \tilde{\mathbf{D}}_t \) being the degree matrix of \( \tilde{\mathbf{A}}_t \), where \( (\tilde{\mathbf{D}}_t)_{i,i} = \sum_j \tilde{\mathbf{A}}_{t,i,j} \). To combine information from different propagation steps, we adopt a linearization approach. The final representation for each node is given by:
$\mathbf{F}_t = \sum_{k=0}^{K} \alpha_k \mathbf{F}_{t,k} \label{equ:prop}$.
This formulation ensures that each node's representation captures multi-hop neighborhood information, which fuse the structure and feature information. Note that the weights \(\alpha\) can be chosen flexibly. In particular, if \(\alpha < 0\), it effectively introduces a ``negative offset'' that can capture heterophilic relationships in the representation~\cite{zhu2021graphheterophily}.







\subsubsection{Representation Clustering}
Once the node representations \(\mathbf{F}_t \in \mathbb{R}^{n_t \times d}\) are obtained, we discard the original graph structure and partition the node representations into groups via the following clustering technique. Each node in the condensed graph \(G'_t\) belongs to one of \(c\) classes. We denote their labels as \(\mathbf{y}'_t \in \{1, 2, \dots, c\}^{n'_t}\) or equivalently via one-hot encoding \(\mathbf{Y}'_t \in \mathbb{R}^{n'_t \times c}\). Following the approach of GCond~\citep{jin2021graph}, we predefine the labels of the condensed nodes so that their class distribution in \(\mathbf{y}'_t\) matches the original distribution in \(\mathbf{y}_t\).

\textit{\underline{A Unified Clustering Formulation.}}
By leveraging the node labels and their feature representations \(\{\mathbf{F}_{t,k}\,\mid\,k=1,\dots,c\}\), we can apply a variety of clustering methods to partition the nodes. Here, we adopt a unified formulation that accommodates both \emph{hard} \(k\)-means and \emph{soft} \(k\)-means. We first split the labeled nodes by their class label \(k\in\{1,2,\dots,c\}\).  
Within class~\(k\), we assign the nodes to \(M_k\) clusters, where \(M_k\) is chosen according to a desired \emph{reduction rate $r$}\footnote{Reduction rate $r$ is defined as (\#nodes in synthetic set)/(\#nodes in training set).}.
Let $M=\sum_{k=1}^{c} M_k=n_{t,k}\times\ r,$, where $n_{t,k}$ is number of training nodes in class $k$. 
Next, we formulate the (hard/soft) assignment matrix \(\mathbf{P}_t \in \mathbb{R}^{\,n_t \times M}\).  Each row corresponds to one labeled node, and each column corresponds to \(M\) clusters across all classes.  In more fine-grained notation, if cluster~\(j\) belongs to class~\(k\), then the entry \(\mathbf{P}_{t,i,j}\) indicates how node~\(i\) of class~\(k\) is associated with that cluster. Formally,
\begin{equation}
\mathbf{P}_{t,i,j} \;\in\;
\begin{cases}
\{0,1\},  \text{hard clustering}.\\
[0,1],  \text{\emph{soft} clustering: subject to }\sum_{j}\mathbf{P}_{t,i,j} = 1.
\end{cases}
\end{equation}
In the hard-assignment scenario, each node belongs to exactly one cluster, so \(\mathbf{P}_{t,i,j} = 1\) if node \(i\) is placed in cluster~\(j\) and \(0\) otherwise. In the soft-assignment scenario, each node has fractional memberships across the clusters in its class, with the memberships summing to 1.

Under hard \(k\)-means, each node belongs to exactly one cluster. Under soft \(k\)-means, each node has fractional memberships that sum to 1. We employ the classic algorithms of \(k\)-means~\citep{lloyd1982leastpcm} and fuzzy \(c\)-means~\citep{bezdek1984fcm} in practice.


\textit{\underline{A Balanced SSE Objective.}}
As Theorem~\ref{theo:bound_of_parameter} indicates, assigning a balanced number of original nodes to each synthetic node can effectively reduce the upper bound of the parameter distance, thereby enhancing the objectives in both the training and testing stages. Thus, to obtain balanced clusters, we add a regularization term that penalizes deviations from a uniform cluster size. Concretely, let \(\mathbf{C}_t \in \mathbb{R}^{M \times d}\) collect the centroids for the \(M\) total clusters across all classes. We denote \(\mathbf{1} \in \mathbb{R}^{n_t}\) as the all-ones vector, where \(n_t\) is the total number of labeled nodes. Suppose class \(k\) has \(n_{t,k}\) labeled nodes and is assigned \(M_k\) clusters, so that \(M = \sum_{k=1}^{c} M_k\). To encourage each class to be evenly partitioned among its \(M_k\) clusters, we set
\[
\mathbf{u} \;=\; \Bigl[\,
\underbrace{\tfrac{n_{t,1}}{M_{1}},\,\dots,\,\tfrac{n_{t,1}}{M_{1}}}_{M_{1} \text{ entries}},\ 
\underbrace{\tfrac{n_{t,2}}{M_{2}},\,\dots,\,\tfrac{n_{t,2}}{M_{2}}}_{M_{2} \text{ entries}},\ 
\dots,\ 
\underbrace{\tfrac{n_{t,c}}{M_{c}},\,\dots,\,\tfrac{n_{t,c}}{M_{c}}}_{M_{c} \text{ entries}}
\Bigr]^{\!\top},
\]
so that each block of \(\mathbf{u}\) is constant, corresponding to the perfectly balanced cluster size for each class. 
Since clustering can be stochastic, we repeat it multiple times and choose the best result. The standard metric for clustering quality is the sum of squared errors (SSE). Finally, the balanced SSE objective is then given by:
\begin{equation}
\label{eq:balanced_sse_matrix}
J(\mathbf{P}_t, \mathbf{C}_t)
\;=\;
\bigl\|\mathbf{F}_t - \mathbf{P}_t \mathbf{C}_t \bigr\|^{2}
\;+\;
\,\bigl\|\mathbf{P}_t^\top \mathbf{1} \;-\; \mathbf{u}\bigr\|^{2}.
\end{equation}


\paragraph{Centroid Computation.}
Regardless of whether the assignment is hard or soft, the cluster centroids are computed by:
\begin{equation}
\mathbf{C}_t = \mathbf{D}_{P_t}^{-1} \mathbf{P}_t^\top \mathbf{F}_{t},
\end{equation}
where \(\mathbf{D}_{P_t} \in \mathbb{R}^{n_t \times n_t}\) is a diagonal matrix satisfying $(\mathbf{D}_{P_t})_{k,k}=\sum_{i=1}^{n_t} \mathbf{P}_{t,i,k}$. Finally, as the propagated features implicitly encode structural information, and current SOTA methods~\citep{geom,sfgc} operate on structure-free graphs, we do not explicitly generate new edges. Beyond achieving strong predictive performance, this structure-free design is significantly more efficient than structure-based approaches, which typically require an additional edge-generation process with \(\mathcal{O}((n_tr)^2)\) complexity~\citep{gong2024gc4nc}.


Instead, we use an identity matrix to represent the adjacency among condensed nodes. By combining feature propagation with our representation-based clustering, we form the condensed graph as
$G'_t = \bigl(\mathbf{C}'_{t},\,\mathbf{I}'_{t}\bigr),$
where $\mathbf{I}'_{t}\in \mathbb{R}^{M\times M}$ is an identity matrix. This condensed graph preserves the essential structural and feature information of the original graph \(G_t\), thereby enabling efficient GNN training while substantially reducing computational overhead.


\subsection{Evolving Condensation via Incremental Initialization }
% \mo{not sure about this title for this section} 
To enable the GECC pipeline to effectively condense graphs that evolve over time and increase in size, we adopt a clustering approach inspired by the K-means++ initialization technique~\citep{arthur2006k}. This method improves the quality of clustering by ensuring a better spread of centroids during initialization. Specifically, for each node class \( c \), we partition the nodes into \( C \) clusters using the following process:

First, let the set of already selected centroids at step \( t-1 \) be \( \mathbf{C}_{t-1} = \{\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_k\} \), where \( \mathbf{c}_i \in \mathbb{R}^d \), and let the new coming node set in new time step is $\{\mathbf{x}_1,\mathbf{x}_{2},...,\mathbf{x}_m\}$. For the current step \( t \), we compute the distance \( D(\mathbf{x}_i, \mathbf{C}_{t-1}) \) for each node embedding \( \mathbf{x}_i \in \mathbb{R}^d \) in class \( c \) to consider how well the current centroids represent the data points in the feature space and identify regions of the space that require additional centroids. The distance is defined as the minimum Euclidean distance between \( \mathbf{x}_i \) and any centroid in \( \mathbf{C}_{t-1} \):
\[
D(\mathbf{x}_i, \mathbf{C}_{t-1}) = \min_{\mathbf{c} \in \mathbf{C}_{t-1}} \|\mathbf{x}_i - \mathbf{c}\|^2.
\]
Next, we probabilistically select a new centroid \( \mathbf{c}_{k+1} \) from the set of node embeddings \( \{\mathbf{x}_i\} \) based on the computed distances. The probability of selecting \( \mathbf{x}_i \) is proportional to \( D(\mathbf{x}_i, \mathbf{C}_{t-1})^2 \):
\[
P(\mathbf{x}_i) = \frac{D(\mathbf{x}_i, \mathbf{C}_{t-1})^2}{\sum_{\mathbf{x}_j \in \{\mathbf{x}_1,\mathbf{x}_{2},...,\mathbf{x}_m\}} D(\mathbf{x}_j, \mathbf{C}_{t-1})^2}.
\]
This step ensures that new centroids are more likely to be chosen from areas of the feature space that are underrepresented, thus improving the spread of the centroids. 
In an evolving setting, we expect the condensed graph to grow proportionally with the original graph. Therefore, we sample \(m \times r\) new centroid sets, denoted by \(\{\mathbf{C}_{\Delta}\}\). We then update the set of centroids as follows:
\[
\mathbf{C}_t = \mathbf{C}_{t-1} \cup \{\mathbf{C}_{\Delta}\}.
\]

Finally, after obtaining \( \mathbf{C}_t \), each node embedding \( \mathbf{x}_i \) in class \( c \) is assigned to the nearest centroid:
\[
\text{Cluster}(\mathbf{x}_i) = \arg\min_{\mathbf{c} \in \mathbf{C}_t} \|\mathbf{x}_i - \mathbf{c}\|^2.
\]
This step partitions the nodes into \( C \) clusters, ensuring that each cluster is represented by its respective centroid.

Once the centroids $\mathbf{C}_t$ are obtained through this process, they will be served as the condensed node representations $\mathbf{X}'_t$ in the synthetic graph \( G'_t \) at time step \( t \). By leveraging this probabilistic initialization, the clustering method ensures that new centroids are well-distributed and far from existing ones, facilitating a more effective condensation of graph representations at each time step.

\subsection{Complexity analysis}
We analyze the complexity of each component in \textsc{GECC} as follows.
\textbf{First}, feature propagation takes \(\mathcal{O}(Ke_td)\) time, where \(K\) is the propagation depth, \(e_t\) is the number of edges at time step \(t\), and \(d\) is the feature dimension.
\textbf{Second}, hard \(k\)-means with \(M\) clusters requires \(\mathcal{O}(n_tUMd)\) per iteration (\(U\) is the number of iterations), whereas soft \(k\)-means also costs \(\mathcal{O}(n_tUMd)\) for distance calculations but plus an additional \(\mathcal{O}(n_tM)\) overhead to normalize fractional memberships. 
\textbf{Third}, the balanced SSE calculation adds only \(\mathcal{O}(n_tM)\) overhead, as the repetitive clustering can be done in parallel. 
\textbf{Finally}, in an evolving setting, incremental \(k\)-means++ only considers newly arrived nodes, reducing initialization cost to \(\mathcal{O}(m|C_{t-1}|d)\), where \(m\) is the number of new nodes and \(|C_{t-1}|\) is the number of existing centroids. 
Gathering all components, the total complexity is dominated by \(\mathcal{O}(n_tUMd)\) from the clustering procedure and \(\mathcal{O}(Ke_td)\) from feature propagation, both of which scale \textbf{linearly} with the number of nodes \(n_t\) and edge count \(e_t\), given that the size of condensed graph $M$ is negligible in terms of order of magnitude compared to $n_t$.

