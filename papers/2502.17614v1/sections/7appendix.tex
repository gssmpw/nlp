% \clearpage
\appendix
\section{Proof of Theorems}
\label{app:propositions}

% \subsection{Detailed Proof of Theorem \ref{theo:1}}
% \label{app:theo1}
% % \begin{proof}[Proof of Theorem \ref{theo:1}]
%  The test error can be expressed as:
%     \[
%     \| \mathbf{Y} - \hat{\mathbf{Y}}' \| = \| \mathbf{Y} - \mathbf{F} \mathbf{W}' \|
%     \]
    
%     By adding and subtracting \( \mathbf{F} \mathbf{W} \), we have
%     \[
%     \| \mathbf{Y} - \mathbf{F} \mathbf{W} + \mathbf{F} \mathbf{W} - \mathbf{F} \mathbf{W}' \| \leq \| \mathbf{Y} - \mathbf{F} \mathbf{W} \| + \| \mathbf{F} (\mathbf{W} - \mathbf{W}') \|
%     \]

%     This demonstrates that the test error of graph condensation is bounded by the conventional GNN training error (first term) and the additional parameter matching error introduced by condensation (second term).
% \end{proof}



\subsection{Proof of Theorems~\ref{theo:training_stage}}
\label{app:proof_training_stage}
\begin{theorem-nonumber}
    The prediction distance in the training stage is bounded by the sum of representation distance and parameter distance.
    \begin{equation}
    \| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \| 
    \leq  \| \mathcal{K}(\mathbf{F}) - \mathbf{F}' \| \cdot \| \mathbf{W}' \| + \| \mathbf{F} \| \cdot \| \mathbf{W} - \mathbf{W}' \|
    \end{equation}
    where $\|\cdot\|$ denotes the L2 norm and $\mathcal{K}(\cdot)$ can be any projection function that aligns the dimensions of $\hat{{\bf Y}}$ and $\hat{{\bf Y}}'$ or ${\bf F}$ and ${\bf F}'$.
\end{theorem-nonumber}

\begin{proof}
To preserve the training data information to maintain the performance of GNNs, we focus on matching the model predictions on the original graph $G$ and its condensed counterpart $G'$. Since $\mathcal{K}(\cdot)$ only aligns the first dimensions, we have $\mathcal{K}(\hat{\mathbf{Y}}) = \mathcal{K}(\mathbf{F})\mathbf{W}$. Therefore, the expression becomes:
\begin{equation}
    \begin{aligned}
    &\| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \| \\
    = &\| \mathcal{K}(\mathbf{F})\mathbf{W} - \mathbf{F}'\mathbf{W}' \| \\
    = & \| \mathcal{K}(\mathbf{F}) \mathbf{W} - \mathcal{K}(\mathbf{F}) \mathbf{W}' + \mathcal{K}(\mathbf{F}) \mathbf{W}' - \mathbf{F}’ \mathbf{W}' \| \\
    \leq &\| \mathcal{K}(\mathbf{F}) (\mathbf{W} - \mathbf{W}') \| + \| (\mathcal{K}(\mathbf{F}) -  \mathbf{F}')\mathbf{W}' \| \\
    \leq & \| \mathcal{K}(\mathbf{F}) \| \cdot \| \mathbf{W} - \mathbf{W}' \| + \| \mathcal{K}(\mathbf{F}) - \mathbf{F}' \| \cdot \| \mathbf{W}' \|
\end{aligned}
\end{equation}
The objective in training stage is to minimizing $\| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \|$, which can be formulated as:
\begin{equation}
    \begin{aligned}
    &\arg \min_{\hat{\mathbf{Y}}'} \| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \|\\
    =&\arg \min_{\hat{\mathbf{Y}}'} \| \mathcal{K}(\mathbf{F}) \| \cdot \| \mathbf{W} - \mathbf{W}' \| + \| \mathcal{K}(\mathbf{F}) - \mathbf{F}' \| \cdot \| \mathbf{W}' \|\\
    \end{aligned}
\end{equation}
Note that $\|\mathbf{F}\|$ is a constant, and the weight matrix $\| \mathbf{W}' \|$ is naturally constrained due to regularization techniques during model optimization to control its magnitude. Then, we have:
\begin{equation}
    \begin{aligned}
    &\arg \min_{\hat{\mathbf{Y}}'} \| \mathcal{K}(\hat{\mathbf{Y}}) - \hat{\mathbf{Y}}' \|\\
    \approx&\arg \min_{\mathbf{F}'}  \underbrace{\| \mathbf{W} - \mathbf{W}' \|}_{\text{Parameter Distance}} + \underbrace{\| \mathcal{K}(\mathbf{F}) - \mathbf{F}' \|}_{\text{Representation Distance}}
    \end{aligned}
\end{equation}




Therefore, Theorem~\ref{theo:training_stage} indicates that by minimizing the \textbf{representation and parameter distances}, the predictions derived from the condensed graph can be close to those of the original graph.

This completes the proof. 
\end{proof}

% To achieve the objective, mainstream GC methods employ distribution matching, kernel ridge regression (KRR)-based matching, trajectory matching, and gradient matching~\cite{wu2020comprehensive}. However, KRR-based matching has been shown to be less effective when the condensed graph is evaluated using Graph Neural Networks (GNNs)~\cite{gcsntk, gong2024gc4nc}. Consequently, we exclude KRR-based approaches from our analysis. In contrast, both trajectory matching and gradient matching inherently aim to minimize the discrepancy between the model parameters trained on the condensed graph and those trained on the original training set. This objective can be succinctly characterized as parameter matching. Therefore, we categorize these two methodologies under the parameter matching framework. In conclusion, our theoretical framework emphasizes \textbf{distribution matching} and \textbf{parameter matching}, as we believe these approaches encompass the majority of existing GC methods.


\subsection{Proof of Theorems~\ref{theo:test_stage}}
\label{app:proof_test_stage}
\begin{theorem-nonumber}
    The test prediction error of the GNN trained of $G'$ is bounded by the test prediction error of the GNN trained on $G$ plus the parameter distance, as formularized by 
    \begin{equation*}
        \| \mathbf{Y} - \hat{\mathbf{Y}}'' \| \leq \| \mathbf{Y} - \mathbf{F}\mathbf{W} \| + \|\mathbf{F}\| \cdot \| \mathbf{W}- \mathbf{W}'\|
    \end{equation*}
\end{theorem-nonumber}

\begin{proof}
At the test stage, our condensation goal is to ensure that the GNN model, trained on condensed training data $G'$, generalizes effectively to the test graph, i.e., achieving a low prediction error on the test data. 
\begin{equation}
\begin{aligned}
    &\| \mathbf{Y} - \hat{\mathbf{Y}}'' \|  \\
    = &\| \mathbf{Y} - \mathbf{F} \mathbf{W}' \|\\
    = & \| \mathbf{Y} - \mathbf{F} \mathbf{W} + \mathbf{F} \mathbf{W} - \mathbf{F} \mathbf{W}' \| \\
    \leq &\| \mathbf{Y} - \mathbf{F} \mathbf{W} \| + \| \mathbf{F} (\mathbf{W} - \mathbf{W}') \| \\
    \leq &\| \mathbf{Y} - \mathbf{F} \mathbf{W} \| + \| \mathbf{F}\| \cdot \|\mathbf{W} - \mathbf{W}' \|
\end{aligned}
\end{equation}
This inequality incorporates both the original test prediction error and the parameter distance. The objective in testing stage is to minimizing $\| \mathbf{Y} - \hat{\mathbf{Y}}''\|$, which can be formulated as:
\begin{equation}
    \begin{aligned}
    &\arg \min_{\hat{\mathbf{Y}}''} \| \mathbf{Y} - \hat{\mathbf{Y}}'' \|\\
    \approx&\arg \min_{\hat{\mathbf{Y}}'} \| \mathbf{Y} - \mathbf{F} \mathbf{W} \| + \| \mathbf{F}\| \cdot \|\mathbf{W} - \mathbf{W}' \|\\
    \approx&\arg \min_{\hat{\mathbf{Y}}'} \underbrace{\|\mathbf{W} - \mathbf{W}'\|}_{\text{Parameter Distance}} \\
    \end{aligned}
\end{equation}
It indicates that by reducing the \textbf{parameter distance} $\| \mathbf{W}- \mathbf{W}'\|$, the test prediction error becomes more tightly bounded, assuming that the original test prediction error \( \| \mathbf{Y} - \mathbf{F}\mathbf{W}\| \) and the propagated feature matrix \( \mathbf{F} \) remain constant. 

This completes the proof. 
\end{proof}


% \begin{proof}
% We start our theoretical analysis with formulating a simple but new objective for GC. 
% GC can be seen as a process of minimizing the loss by a GNN model trained in the synthetic graph ${G'}$. The objective function can reformulated as follows: 
% \begin{equation}
% {G'}=\underset{{G'}}{\arg \min } \ \mathcal{L}(\mathbf{Y}, \operatorname{GNN}({G'})). \label{eq:graphReduction}
% \end{equation}  
%  Besides, without loss of generality, we employ a regression loss computed over the entire graph to evaluate performance during the testing phase, corresponding to a transductive setting. In an inductive setting, the feature matrix \( \mathbf{F} \) can be replaced by \( \mathbf{F}_t \), where \( \mathbf{F}_t \) represents the propagated features in the test graph.

% After training on the original training set, the prediction of the SGC model is given by:
% \begin{equation}
%     \hat{\mathbf{Y}} = \mathbf{F} \mathbf{W}
% \end{equation}
% where \( \mathbf{F} = \mathbf{A}^K \mathbf{X}\) is the propagated feature matrix of the entire graph. 
% \( \mathbf{W} \in \mathbf{R}^{d \times c} \) is the weight matrix of the trained SGC model.

% Conversely, the prediction of the model trained on the synthetic (condensed) dataset is expressed as:
% \begin{equation}
%     \hat{\mathbf{Y}}' = \mathbf{F} \mathbf{W}'
% \end{equation}
% where \( \mathbf{W}' \in \mathbf{R}^{d \times c} \) denotes the weight matrix obtained from training on the synthetic dataset. 

% The loss function $\mathcal{L}$ during the testing phase in Equ.~\ref{eq:graphReduction} can be computed using the regression loss
% \begin{equation}
%     \mathcal{L} = \| \mathbf{Y} - \hat{\mathbf{Y}}' \|
% \end{equation}
% where \( \mathbf{Y} \in \mathbf{R}^{n \times c} \) represents the true labels.
%  The test error can be expressed as:
%     \[
%     \| \mathbf{Y} - \hat{\mathbf{Y}}' \| = \| \mathbf{Y} - \mathbf{F} \mathbf{W}' \|
%     \]
    
%     By adding and subtracting \( \mathbf{F} \mathbf{W} \), we have
%     \[
%     \| \mathbf{Y} - \mathbf{F} \mathbf{W} + \mathbf{F} \mathbf{W} - \mathbf{F} \mathbf{W}' \| \leq \| \mathbf{Y} - \mathbf{F} \mathbf{W} \| + \| \mathbf{F} (\mathbf{W} - \mathbf{W}') \|
%     \]
% Finally, the regression loss can be bounded by decomposing the error into two components:
% \begin{equation}
%     \| \mathbf{Y} - \hat{\mathbf{Y}}' \| \leq \| \mathbf{Y} - \mathbf{F}\mathbf{W} \| + \| \mathbf{F} (\mathbf{W} - \mathbf{W}') \|
% \end{equation}
% This completes the proof.
% \end{proof}

\subsection{Proof of Theorem \ref{theo:bound_of_parameter}}
\label{app:proof_bound_of_parameter}
\begin{theorem-nonumber}
    The parameter distance can be bounded by the following inequality:
    \begin{equation}
        \| \mathbf{W} - \mathbf{W}' \| \leq \mathcal{C}(\max (\text{diag}(\mathbf{\mathbf{P}^\top\mathbf{P}})))^2,
    \end{equation}
    where $\mathcal{C}=\frac{\|\mathbf{F} \| \cdot \| \mathbf{Y}\|({\lambda_{\min}(\mathbf{F}^\top\mathbf{F})}+\|\mathbf{F} \|)}{{\lambda^2_{\min}(\mathbf{F}^\top\mathbf{F})}}$ is a constant, $\mathbf{P}^\top\mathbf{P} \in \mathbb{R}^{N' \times N'}$ is a diagonal matrix with each diagonal entry corresponding to how many original nodes are assigned to each synthetic node. 
\end{theorem-nonumber}

\begin{proof}
We aim to establish that clustering effectively bounds the error introduced by parameter matching and representation difference. The proofproceeds as follows:
\paragraph{Bounding the Parameter Matching Error \( \| \mathbf{W} - \mathbf{W}' \| \):}
Consider the weight matrices for the SGC and clustering-based methods
\[
\mathbf{W} = (\mathbf{F}^\top \mathbf{F})^{-1} \mathbf{F}^\top \mathbf{Y}, \quad \mathbf{W}' = (\mathbf{C}^\top \mathbf{C})^{-1} \mathbf{C}^\top \mathbf{Y}'
\]
where
\[
\mathbf{C} = (\mathbf{P}^\top\mathbf{P})^{-1} \mathbf{P}^\top \mathbf{F}, \quad \mathbf{Y}' = (\mathbf{P}^\top\mathbf{P}))^{-1} \mathbf{P}^\top \mathbf{Y}
\]

The difference between \( \mathbf{W} \) and \( \mathbf{W}' \) is
\[
\| \mathbf{W} - \mathbf{W}' \| = \left\| (\mathbf{F}^\top \mathbf{F})^{-1} \mathbf{F}^\top \mathbf{Y} - (\mathbf{C}^\top \mathbf{C})^{-1} \mathbf{C}^\top \mathbf{Y}' \right\|
\]

By substituting $\mathbf{C}$ and $\mathbf{Y}'$, we can express \( \mathbf{W}' \) in terms of \( \mathbf{F} \) and \( \mathbf{Y} \)
\[
\mathbf{W}' = \left( \mathbf{F}^\top \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \mathbf{F} \right)^{-1} \mathbf{F}^\top \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \mathbf{Y}
\]

Let \( \mathbf{A} = \mathbf{F}^\top \mathbf{F} \) and \( \mathbf{B} = \mathbf{F}^\top \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \mathbf{F} \), then:
\[
\mathbf{W} = \mathbf{A}^{-1} \mathbf{F}^\top \mathbf{Y}, \quad \mathbf{W}' = \mathbf{B}^{-1} \mathbf{F}^\top \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \mathbf{Y}
\]

The difference becomes
\[
\mathbf{W} - \mathbf{W}' = \mathbf{A}^{-1} \mathbf{F}^\top \mathbf{Y} - \mathbf{B}^{-1} \mathbf{F}^\top \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \mathbf{Y}
\]

Similar to the above two proofs, we add and subtract a term \( \mathbf{B}^{-1} \mathbf{F}^\top \mathbf{Y} \) and rewrite the difference by
\begin{align*}
        \mathbf{W} - \mathbf{W}' = &\left( \mathbf{A}^{-1} - \mathbf{B}^{-1} \right) \mathbf{F}^\top \mathbf{Y} \\
&+ \mathbf{B}^{-1} \mathbf{F}^\top \left( \mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \right) \mathbf{Y}
\end{align*}

Considering the norms, we have
    \begin{align*}
\| \mathbf{W} - \mathbf{W}' \| \leq &\| \mathbf{A}^{-1} - \mathbf{B}^{-1} \| \cdot \| \mathbf{F}^\top \mathbf{Y} \| \\
&+ \| \mathbf{B}^{-1} \| \cdot \| \mathbf{F}^\top (\mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top) \mathbf{Y} \|
\end{align*}

We will now bound each term independently.

\textbf{Bounding the First Term}
\begin{align*}
        \| \mathbf{A}^{-1} - \mathbf{B}^{-1} \| \cdot \| \mathbf{F}^\top \mathbf{Y} \|
\end{align*}


Based on the norms of matrix inequality, we have
\[
\| \mathbf{A}^{-1} - \mathbf{B}^{-1} \| \leq \| \mathbf{A}^{-1} \| \cdot \| \mathbf{B} - \mathbf{A} \| \cdot \| \mathbf{B}^{-1} \|
\]
Then, according to
\[
\| \mathbf{A}^{-1} \| = \frac{1}{\lambda_{\min}(\mathbf{A})}, \quad \| \mathbf{B}^{-1} \| = \frac{1}{\lambda_{\min}(\mathbf{B})}
\]
and assuming \( \lambda_{\min}(\mathbf{B}) \geq \frac{1}{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2} \lambda_{\min}(\mathbf{A}) \), we have
\[
\| \mathbf{A}^{-1} \| \cdot \| \mathbf{B}^{-1} \| \leq \frac{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2}{\lambda_{\min}(\mathbf{A})^2}
\]

Bounding \( \| \mathbf{B} - \mathbf{A} \| \):
\begin{align*}
    \mathbf{B} - \mathbf{A} &= \mathbf{F}^\top \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \mathbf{F} - \mathbf{F}^\top \mathbf{F} \\
    &= -\mathbf{F}^\top (\mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top) \mathbf{F}
\end{align*}
Taking norms:
\begin{align*}
\| \mathbf{B} - \mathbf{A} \| &= \| \mathbf{F}^\top (\mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top - \mathbf{I}) \mathbf{F} \| \\
&\leq \| \mathbf{F} \|^2 \cdot \| \mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \|
\end{align*}
Since \( \| \mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \| \leq 1 \), the inequality can be further simplified to
\(\| \mathbf{B} - \mathbf{A} \| \leq \| \mathbf{F} \|^2\).

Combining the above:
\[
\| \mathbf{A}^{-1} - \mathbf{B}^{-1} \| \cdot \| \mathbf{F}^\top \mathbf{Y} \| \leq \frac{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2}{\lambda_{\min}(\mathbf{A})^2} \cdot \| \mathbf{F} \|^2 \cdot \| \mathbf{Y} \|
\]


\textbf{Bounding the Second Term}
\begin{align*}
    \| \mathbf{B}^{-1} \| \cdot \| \mathbf{F}^\top (\mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top) \mathbf{Y} \|
\end{align*}


Based on the proof above, we first have
\[
\| \mathbf{B}^{-1} \| = \frac{1}{\lambda_{\min}(\mathbf{B})} \leq \frac{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2}{\lambda_{\min}(\mathbf{A})}
\]

 Since \( \| \mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top \| \leq 1 \), 
\[
\| \mathbf{F}^\top (\mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top) \mathbf{Y} \| \leq \| \mathbf{F} \| \cdot \| \mathbf{Y} \|
\]

Combining these bounds:
\begin{align*}
        &\| \mathbf{B}^{-1} \| \cdot \| \mathbf{F}^\top (\mathbf{I} - \mathbf{P} (\mathbf{P}^\top\mathbf{P})^{-2} \mathbf{P}^\top) \mathbf{Y} \| \\
        &\leq \frac{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2}{\lambda_{\min}(\mathbf{A})} \cdot \| \mathbf{F} \| \cdot \| \mathbf{Y} \|
\end{align*}


\paragraph{Combining Both Terms:}
Adding the bounds for both terms, we obtain:
\begin{align*}
\| \mathbf{W} - \mathbf{W}' \| \leq & \frac{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2}{\lambda_{\min}(\mathbf{A})^2} \cdot \| \mathbf{F} \|^2 \cdot \| \mathbf{Y} \| \\
&\quad + \frac{(\max_k (\mathbf{P}^\top\mathbf{P})_{kk})^2}{\lambda_{\min}(\mathbf{A})} \cdot \| \mathbf{F} \| \cdot \| \mathbf{Y} \| \\
=&\max_k (\mathbf{P}^\top\mathbf{P})_{kk}) \cdot (\frac{\| \mathbf{F} \|^2 \cdot \| \mathbf{Y} \|}{\lambda_{\min}(\mathbf{A})^2} + \frac{\| \mathbf{F} \| \cdot \| \mathbf{Y} \|}{\lambda_{\min}(\mathbf{A})}) \\
=&\mathcal{C}(\max (\text{diag}(\mathbf{\mathbf{P}^\top\mathbf{P}})))^2
\end{align*}
where $\mathcal{C}=\frac{\|\mathbf{F} \| \cdot \| \mathbf{Y}\|({\lambda_{\min}(\mathbf{F}^\top\mathbf{F})}+\|\mathbf{F} \|)}{{\lambda^2_{\min}(\mathbf{F}^\top\mathbf{F})}}$ is a constant, $\mathbf{P}^\top\mathbf{P} \in \mathbb{R}^{N' \times N'}$ is a diagonal matrix with each diagonal entry corresponding to how many original nodes are assigned to each synthetic node. 

Our objective is to minimize the parameter distance, which can be reformulated by:
\begin{equation}
    \begin{aligned}
    &\arg \min_{\mathbf{W}'} \| \mathbf{W} - \mathbf{W}' \|\\
    \approx&\arg \min_{\mathbf{P}} \mathcal{C}(\max (\text{diag}(\mathbf{\mathbf{P}^\top\mathbf{P}})))^2\\
    \end{aligned}
\end{equation}

This completes the proof.   
\end{proof}
    
% \paragraph{Conclusion.} \textbf{First}, larger \( \max((\mathbf{P}^\top\mathbf{P})_{kk}) \) increases the bound on parameter matching error. \textbf{Second}, lower \( \sigma_{ck}^2 \) leads to smaller representation matching error, enhancing the overall condensation quality. These two factors are directly related to clustering quality. \textbf{Third}, a larger \( \lambda_{\min}(\mathbf{A}) \) decreases the bound, indicating that better-conditioned covariance matrices lead to tighter error bounds. \textbf{Finally}, higher \( \| \mathbf{F} \| \) and \( \| \mathbf{Y} \| \) contribute to larger error bounds. These two are predefined by the original dataset.

% \section*{Incremental vs.\ Batch-wise (Streaming) kmeans++}

% We have a dataset $\mathcal{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_n\} \subset \mathbb{R}^d$ 
% and want $k$ centroids. Two procedures are common:

% \begin{enumerate}
%   \item \textbf{Standard kmeans++:} Select the first centroid $\mathbf{c}_1$ uniformly at random; 
%   for each subsequent centroid $\mathbf{c}_{t+1}$, 
%   use the distance-based probabilities 
%   $P(\mathbf{x}_i) = D(\mathbf{x}_i)^2 / \sum_{\mathbf{x}_j} D(\mathbf{x}_j)^2,$
%   where 
%   $D(\mathbf{x}_i) = \min_{1 \le s \le t}\|\mathbf{x}_i - \mathbf{c}_s\|.$
%   \vspace{0.2em}
%   \item \textbf{Incremental (or Streaming) kmeans++:} 
%   Split $\mathcal{X}$ into batches or handle data in an online manner. 
%   In each batch, we pick a subset of centroids using a similar \emph{kmeans++-like} rule, 
%   then \emph{merge} these partial centroids at the end, possibly running an extra final kmeans++ step on the merged set.
% \end{enumerate}

% \subsection*{1.\ Incremental Selection (One Centroid at a Time): Exact Equivalence}

% \paragraph{Algorithmic Description.} Instead of computing $D(\mathbf{x}_i)$ from scratch each time, we keep a \emph{running} (incremental) record of the distances:

% \begin{itemize}
%   \item Pick the first centroid $\mathbf{c}_1$ uniformly at random from $\mathcal{X}$.
%   \item Maintain for each point $\mathbf{x}_i$ a distance $D(\mathbf{x}_i) = \min_{1 \le s \le t}\|\mathbf{x}_i - \mathbf{c}_s\|^2$,
%         updated whenever a new centroid $\mathbf{c}_{t+1}$ is chosen:
%   \[
%     D(\mathbf{x}_i) \;=\; 
%       \min\Bigl(D(\mathbf{x}_i), \|\mathbf{x}_i - \mathbf{c}_{t+1}\|^2\Bigr).
%   \]
%   \item Sample the next centroid $\mathbf{c}_{t+1}$ with probability 
%   \[
%     P(\mathbf{x}_i) \;=\; 
%       \frac{D(\mathbf{x}_i)}{\sum_{\mathbf{x}_j \in \mathcal{X}} D(\mathbf{x}_j)},
%   \]
%   \item Repeat until we have $k$ centroids.
% \end{itemize}

% \paragraph{Proof of Equivalence.} 
% At each centroid-selection step $t$, the \emph{distance values} $\{D(\mathbf{x}_i)\}$ in the incremental method match exactly the corresponding distances in standard kmeans++, because in both cases 
% \[
%   D(\mathbf{x}_i) \;=\;
%   \min_{1 \le s \le t}\|\mathbf{x}_i - \mathbf{c}_s\|^2.
% \]
% Hence, both algorithms \emph{sample the next centroid} from \emph{the same probability distribution} over $\mathcal{X}$. 
% By induction on $t=1,\dots,k$, the two methods select the \emph{same} (or identically distributed) set of $k$ centroids. 
% Thus, \emph{incremental kmeans++} (one-at-a-time) is \textbf{exactly the same} as standard kmeans++ if they use the same random seed.

% \subsection*{2.\ Batch-wise (Streaming) kmeans++: Approximate Equivalence}

% In a streaming or large-scale setting, we may instead:
% \begin{enumerate}
%   \item Partition $\mathcal{X}$ into batches $\mathcal{X}_1, \mathcal{X}_2, \dots, \mathcal{X}_T$ (e.g.\ chunks or time windows).
%   \item On each batch $\mathcal{X}_t$, run a local kmeans++ to extract $k_t$ ``partial centroids'' 
%         $\{\mathbf{p}_1^{(t)}, \dots, \mathbf{p}_{k_t}^{(t)}\}$.
%   \item Merge all partial centroids $\{\mathbf{p}_j^{(t)}\}$ into one set, 
%         optionally weighting each centroid by the number of data points in its batch-cluster,
%         and run a final round of kmeans++ on this \emph{compressed} or merged set to extract $k$ global centroids.
% \end{enumerate}

% \paragraph{Why this is approximate.} In standard kmeans++, the distance probabilities are computed against \emph{all points} from \(\mathcal{X}\) at each centroid selection. 
% In the batch-wise approach, we only compute local distances within each batch and then later refine by clustering the merged centroids. 
% Hence, we generally cannot guarantee \emph{exact} equivalence: once data is partitioned, the cross-batch interactions are not fully accounted for in each local selection step. 

% However, one can show that if each batch-level selection is performed by kmeans++ and we preserve the relative weighting of partial centroids appropriately, 
% the final global clustering has a provable approximation factor relative to running a \emph{single} standard kmeans++ on all of \(\mathcal{X}\). 
% For instance, a common result in the coreset and streaming literature is that the final error (cost) of the merging approach is within a small constant factor (or $O(\log n)$ in the worst case) of the cost of a global optimum or a global kmeans++ initialization. 

% \paragraph{Sketch of Approximation Argument.}
% Suppose each batch $\mathcal{X}_t$ is of size $n_t$, with $\sum_{t=1}^\top n_t = n$. 
% Running local kmeans++ with $k_t$ partial centroids on batch $t$ yields a partial solution whose cost is close to the best $k_t$-clustering cost on that batch. 
% Collecting these partial centroids (with weights proportional to $n_t$ or the cluster sizes) forms a \emph{weighted coreset} \(\mathcal{P}\). 
% A final kmeans++ on \(\mathcal{P}\) yields $k$ global centroids. 
% By standard coreset arguments (and the properties of kmeans++ on weighted datasets), the final solution is guaranteed to be within a \emph{bounded factor} of the global kmeans++ cost on the full dataset. 
% Exact approximation ratios depend on how many centroids $k_t$ we extract per batch, the total number of batches, and the structure of the data. 
% Typical results show that for sufficiently large $k_t$ (relative to $k$) or well-chosen merges, 
% the final solution approximates a global kmeans++ solution to within constant or polylogarithmic factors.

% \subsection*{Summary}

% \begin{itemize}
%   \item \textbf{One-at-a-time incremental kmeans++ is exactly equivalent} to standard kmeans++ 
%   because the distance-based sampling distribution at each step is identical.

%   \item \textbf{Batch-wise (streaming) kmeans++ is generally an approximation} 
%   because each batch makes centroid choices without full knowledge of data in other batches. 
%   However, coreset/merging techniques plus a final global pass can ensure the final clustering 
%   is within a guaranteed approximation factor of the single-batch (standard) kmeans++ solution.
% % \end{itemize}
\section{Method Pipeline}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/main.pdf}
    \caption{Illustration of the GECC framework. The lower part of the figure depicts the two evolving settings: in the \textbf{inductive} setting, the graph structure evolves over time, while in the \textbf{transductive} setting, only the training nodes (labels) change, with the graph structure and non-training nodes remaining unchanged.}

    \label{fig:main}
\end{figure}

The proposed GECC framework is illustrated in Figure~\ref{fig:main}. GECC takes the current graph \( G_t = (\mathbf{X}_t, \mathbf{A}_t) \) along with the centroids from the previous time step to perform condensation and generate the condensed graph \( G'_t = (\mathbf{I}, \mathbf{C}'_t) \). The lower part of the figure also depicts the two evolving settings: in the \textbf{inductive} setting, the graph structure evolves over time, while in the \textbf{transductive} setting, only the training nodes (labels) change, whereas the graph structure and non-training nodes remain unchanged.



\section{Experimental Details}\label{app:exp}
\input{tabs/dataset}
\subsection{Dataset Statistics}
\label{app:statistics}
In line with most GC studies, we utilize seven datasets in total: five transductive datasets—\textit{Citeseer}, \textit{Cora} \citep{kipf2016semi}, Pubmed \citep{namata2012query}, \textit{Ogbn-arxiv}, and \textit{Ogbn-products} \citep{hu2020open}—and two inductive datasets, \textit{Flickr} and \textit{Reddit} \citep{zeng2019graphsaint}. Each graph is randomly split, ensuring a consistent class distribution. The details of the datasets statistics are shown in Table \ref{tab:statistics}. We list all evolving information in Table~\ref{tab:split_reduction}, rows above the midline correspond to smaller datasets, and rows below it correspond to larger ones.
Reduction rate $r$ is defined as (\#nodes in synthetic set)/(\#nodes in training set) while $r_w$ is (\#nodes in synthetic set)/(\#nodes of whole graph visible in training stage). The whole graph visible in training stage means the full graph dataset for transductive setting but only the training graph for inductive setting.
\input{tabs/1split_info}

\subsection{Platform and Hardware Information}
To efficiently execute the clustering algorithm, we run it on Intel(R) Xeon(R) Platinum 8260 CPUs @ 2.40GHz using NumPy~\cite{numpy}, while the downstream GNN evaluations are conducted on a cluster equipped with a mix of Tesla A100 40GB/V100 32GB GPUs for large datasets and K80 12GB GPUs for small datasets. All GNN models are implemented using the PyG package~\cite{pyg}.

\subsection{Baselines Selection}
To establish a fair benchmark, we selected recent state-of-the-art GC methods that emphasize both effectiveness and efficiency. Some recent methods, such as MCond, CGC, and GCPA, were excluded due to the unavailability of their code at the time of paper writing. For the selected approaches, we chose the best representatives from each category: GCondX for gradient matching, GCDM and SimGC for distribution matching, and GEOM for trajectory matching. We implemented these methods using the latest GraphSlim package\footnote{\url{https://github.com/Emory-Melody/GraphSlim/tree/main}}, except for SimGC\footnote{\url{https://github.com/BangHonor/SimGC}} and GEOM\footnote{\url{https://github.com/NUS-HPC-AI-Lab/GEOM/tree/main}}, for which we used their original source code. We specifically included SimGC because it is the only model-based GC method that can run on Ogbn-products without requiring any modifications.


\subsection{Implementation Details for Variants of GCond}
As mentioned in Section~\ref{sec:intro} and illustrated in Table~\ref{tab:preliminary}, adapting GCond to an evolving setting is challenging. We employ the structure-free variant of GCond, i.e., GCondX, for easier adaptation, as designing a specific growth mechanism for the condensed graph is nontrivial and requires significant effort.
In addition, to manifests the convergence speed difference between GCond and GCond-Init, we implement an early stopping criterion with a patience of 3 during intermediate evaluations. 
If no improvement in validation performance is observed over 3 consecutive evaluations, the condensation process is terminated.
\subsection{Hyperparameters}\label{app:hyper}
Compared to existing work and benchmarks in GC, we perform a moderate hyperparameter search on validation set, as detailed in Section~\ref{sec:hyper}. The final results are presented in Table~\ref{tab:hyper}. During hyperparameter optimization (HPO), we observe that inheriting clustering centroids results in an approximate 1\% absolute performance drop for \textit{Flickr} and \textit{Ogbn-arxiv}. Therefore, we also treat the use of incremental $k$-means++ as a tunable hyperparameter. Additionally, some datasets do not perform well with a single hyperparameter configuration. To address this, we employ two distinct hyperparameter sets tuned on the first and last time steps, respectively, and select the better-performing one during the evolution process. These two sets are represented using a "/" separator and are indicated as "if Dual" when this technique is applied. For all baselines, we use the best hyperparameters reported in their respective papers, as implemented in GC4NC~\cite{gong2024gc4nc}.

The optimal hyperparameters also offer meaningful insights. \textbf{First}, during the early evolution stage, graphs exhibit higher heterophily compared to later stages. For example, on the \textit{Cora} dataset, $\alpha_1=-0.3$ in the early phase contrasts with $\alpha_1=0.9$ later. This pattern likely arises because, in the early stages of a graph, groups have not yet formed; links appear more randomly, making it challenging for nodes to link to similar counterparts.
\textbf{Second}, it is noteworthy that some datasets do not rely on second-hop information. This observation is contrary to previous studies~\cite{wu2019simplifying,luo_classic_2024} that recommend using at least 2-hop propagation. We conjecture that the representation clustering process itself acts as an additional step of feature propagation.
\textbf{Finally}, weight decay emerges as a critical factor for the performance of downstream models, suggesting that future work should pay closer attention to its optimization.
\input{tabs/main_app}
\input{tabs/average_time}
\input{tabs/hyper}

\section{Additional Results}
\subsection{Performance and Efficiency}
For simplicity, Table~\ref{tab:main} omits the standard error and running time of coreset selection methods. we provide the full results here.

Figure~\ref{fig:accuracy_vs_time} presents the accuracy vs. time trade-off for Reddit. For the remaining three large datasets, we provide the corresponding results in Figure~\ref{fig:accuracy_vs_time_large_vertical}. 
The results align with our main findings, further confirming that GECC surpasses the baselines in both efficiency and scalability. It consistently maintains stable performance while effectively managing computational resources throughout graph evolution. Notably, on the large-scale Ogbn-products dataset, which contains over one million nodes, most GC methods fail, whereas GECC remains robust and continues to operate successfully.


\begin{figure*}[htbp]
  \centering
  \begin{subfigure}[b]{0.85\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/ogbn-arxiv_time_vs_accuracy-cropped.pdf}
    % \caption{Flickr Caption}
    \label{fig:flickr}
  \end{subfigure}
  % \vspace{-1em}
  \begin{subfigure}[b]{0.85\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/Ogbn-products_time_vs_accuracy-cropped.pdf}
    % \caption{Ogbn-products Caption}
    \label{fig:ogbnproducts}
  \end{subfigure}
  % \vspace{-1em}
  \begin{subfigure}[b]{0.85\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/Reddit_time_vs_accuracy-cropped.pdf}
    % \caption{Reddit Caption}
    \label{fig:redproducts}
  \end{subfigure}
  % \vskip -2em
  \caption{Test accuracy vs. condensation time on large datasets (top-left is better).}
  \label{fig:accuracy_vs_time_large_vertical}
\end{figure*}


% \begin{figure*}[htbp]
%   \centering
%   \begin{subfigure}[b]{0.33\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/ogbn-arxiv_time_vs_accuracy-cropped.pdf}
%     % \caption{Flickr Caption}
%     \label{fig:flickr}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.33\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/Ogbn-products_time_vs_accuracy-cropped.pdf}
%     % \caption{Ogbn-products Caption}
%     \label{fig:ogbnproducts}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[b]{0.33\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/Reddit_time_vs_accuracy-cropped.pdf}
%     % \caption{}
%     \label{fig:redproducts}
%   \end{subfigure}
%   \vskip -2em
%   \caption{Test accuracy vs. condensation time on large datasets (top-left is better).}
%   \label{fig:accuracy_vs_time_large}
% \end{figure*}