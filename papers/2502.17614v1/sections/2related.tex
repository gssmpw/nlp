\iffalse
\begin{figure}[t] 
  \centering
  \includegraphics[width=\linewidth]{figs/main.pdf} 
  \vspace{-1em}
  \caption{The GECC pipeline. The GECC process can be divided into two stages: \emph{(i.)} \textbf{Feature Propagation}, and \emph{(ii.)} \textbf{Representation Clustering}. At time step \( t \), it takes the original graph \( T_t = (A_t, X_t) \) and the condensed node representations from the previous time step \( X'_{t-1} \) as the initialization for clustering centroids. After encoding the graph into node representations $\mathbf{F}_t$, K-means clustering is applied class-by-class to partition the node representations into clusters, extracting new centroids as the updated condensed node representations \( X'_t \).\juntong{can you enlarge the font in the figure and reduce gaps? It would be better to put this figure in double-column format because it's too large now.}
}
  \label{fig:gecc}
\end{figure}
\fi

\vspace{-0.5em}
\section{Related Work} 

\subsection{Model-based Graph Condensation}
Model-based GC methods, which require a GNN training phase on the original graph, were the first approaches proposed for GC. In these methods, a smaller graph is synthesized to effectively represent the original for training GNNs \citep{hashemi2024comprehensive}. For example, GCond \citep{jin2021graph}, DosCond \citep{jin2022condensing}, and SGDD \citep{yang2023does} minimize a gradient matching loss that aligns the gradients of the training losses w.r.t. the GNN parameters computed on both the original and condensed graphs. Alternatively, SFGC \citep{zheng2024structure} and GEOM \citep{geom} condense a graph by aligning the parameter trajectories of the original graph, thereby eliminating the need for explicit edge generation and reducing complexity. Although these approaches are effective, they require significant computational resources due to the multiple full GNN training runs on the original graph. To mitigate this cost, GCDM \citep{liu2022graph} and SimGC~\cite{xiao2024simple} adopt a distribution matching strategy by aligning sampled original node features with synthetic nodes, thus reducing the discrepancy between gradient matrices and the final output. Other methods such as GCSNTK~\cite{gcsntk} use GNTK~\cite{du2019gntk} to bypass the gradient update in the synthetic graph; however, such approaches currently show suboptimal performance according to recent benchmarks \citep{gong2024gc4nc,sun2024gc}. Overall, these model-based GC methods share common challenges including high computational overhead, scalability, and traceability issues. 
Recent work MCond~\cite{gao_graph_2023} explicitly learns a one-to-many node mapping from original nodes to synthetic nodes, thereby facilitating inductive representation learning and enhancing traceability; however, it still operates within the framework of gradient matching and remains highly complex.



\subsection{Model-Agnostic Graph Condensation}
While model-based approaches have advanced GC, they still struggle to scale to large-scale graph datasets (e.g., graphs with over one million nodes such as Ogbn-products~\citep{hu2021ogb}). To further reduce condensation time on such large-scale graphs, recent efforts have focused on \textbf{directly matching the representations} between the original and synthetic graphs, bypassing the need for a surrogate model.. This strategy offers benefits in efficiency and generalizability to diverse downstream models. GCPA \citep{li2025a} first extracts structural and semantic information from the original graph, randomly samples the representations, and finally refines them using contrastive learning. Likewise, CGC \citep{gao2024rethinking} revisits the node feature matching paradigm by reducing the representation matching problem to a class partitioning task. In this approach, nodes within the same partition are merged, and a learning module is employed to weight different samples before they are aggregated into a synthetic node. \textbf{However}, despite its effectiveness, CGC's theoretical analysis does not establish a concrete relationship between class partitioning and the GC objective, thereby questioning the necessity of this extra step. \textbf{Moreover}, these model-agnostic methods still require an \textit{additional learning module}, which introduces challenges in optimization and hyperparameter tuning. \textbf{Finally}, none of these methods fully address the evolving nature of large-scale graphs, which typically necessitates a complete re-condensation whenever the graph changes. \textit{To our best knowledge, GECC is the first model-agnostic and training-free method with linear complexity.}



