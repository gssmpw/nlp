% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gcondx_time_vs_accuracy-cropped.pdf}
%     \vskip -1em
%     \caption{The performance and efficiency comparison among evolving variants of GCondX on the Ogbn-arxiv dataset. The vertical dashed line indicates the standard GCN training time. The Y-axis representing condensation time (right) is on a logarithmic scale.}
%     \label{fig:comp}
%     \vskip -1em
% \end{figure}

\section{Introduction} \label{sec:intro}


% The ``\verb|figure|'' environment should be used for figures. One or
% more images can be placed within a figure. If your figure contains
% third-party material, you must clearly identify it as such, as shown
% in the example below.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{sample-franklin}
%   \caption{1907 Franklin Model D roadster. Photograph by Harris \&
%     Ewing, Inc. [Public domain], via Wikimedia
%     Commons. (\url{https://goo.gl/VLCRBB}).}
%   \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}


Graph-structured data has become indispensable in various domains, including social networks~\citep{fan2019graph}, epidemiology~\citep{liu2024review}, and recommendation systems \citep{wu2020comprehensive,wang2024a}. The ability of graphs to represent complex relationships and dependencies has propelled their adoption in machine learning, especially with the advent of graph neural networks (GNNs) \citep{bronstein2017geometric}. However, the exponential growth of real-world graph datasets presents significant computational challenges, as the cost of training GNNs increases with the number of nodes and edges~\citep{hu2021ogb, complexity}. To address this, graph condensation (GC) techniques~\citep{jin2021graph, jin2022condensing, gcsntk, geom, sfgc, xiao2024simple,liu2024tinygraph} have been developed, which aim to produce significantly smaller yet information-rich graphs that accelerate GNN training.  For example,  GCond~\cite{jin2021graph}  condenses the Flickr dataset to 0.1\% of its original size while preserving 99.8\% of the original accuracy. These condensed graphs not only save significant storage space and transmission bandwidth, but also enhance the efficiency of retraining neural networks in many critical applications such as continual learning~\cite{liu_cat_2023}.

\input{tabs/preliminary}

% By retaining critical structural and feature representations of the original large graph, GC enables GNNs trained on the condensed graph to achieve performance comparable to those trained on the full graph.

Despite the promise, existing GC methods face three significant limitations that hinder their practical utility in real-world scenarios.


\textbf{(a) High Computational Overhead:} Most GC methods entail a computationally intensive condensation process, marked by frequent gradient calculations and updates.  This process closely resembles full GNN training, creating a paradox as it contradicts the primary objective of GC methods to enhance the efficiency of GNN training.
For instance, gradient matching techniques~\cite{jin2021graph,jin2022condensing} require consistent alignment of GNN gradients with each iteration. Similarly, trajectory matching methods~\cite{sfgc,geom} necessitate the alignment of GNN parameters at various stages during the training process. These methods inherently demand significant GNN training and gradient processing, thus slowing down the procedure as the size of graph datasets expands. 



\textbf{(b) Challenges with Evolving Graphs:} Existing GC methods are designed for static graphs and overlook the evolving nature of real-world graphs. In practice, graphs evolve over time, with nodes, edges, or attributes being added, removed, or modified. 
Existing graph condensation (GC) methods require re-running the entire condensation process from scratch whenever the training set changes, since modifications affect every synthetic node. Moreover, while synthetic graphs are expected to grow proportionally with the original graphs, an effective strategy for accommodating this growth is currently lacking.

In Table~\ref{tab:preliminary}, we report the relative condensation times, i.e., multiple of a standard GNN training time, for evolving graphs on the \textit{Reddit} dataset, using GCond~\cite{jin2021graph} as a representative example. We compare three variants: (i) \textbf{GCond}: Performs condensation from scratch at each time step. (ii) \textbf{GCondX-Init}: Retains synthetic nodes from the previous time step as initialization, updating all nodes during the condensation process. (iii) \textbf{GCond-Grow}: Freezes the synthetic nodes from the previous time step and concatenate new synthetic nodes to grow the condensed graph. GCond-Init demonstrates the extent to which evolving adaptation strategies can accelerate the convergence speed of GC. Meanwhile, GCond-Grow illustrates how directly reducing complexity enhances the overall efficiency of GC.
The table indicates that the condensation process requires over 100$\times$ more time than the GNN training on the uncondensed graph. This substantial computational overhead suggests that current GC methods may not be practical for evolving graphs. Even though simple evolving strategies can reduce marginal time, the cost remains significantly higher than that of GNN training, underscoring the urgent need for a novel GC method that can efficiently handle graph evolution.


\textbf{(c) Lack of Traceability:} Many GC methods synthesize condensed graphs without explicitly establishing a connection between the nodes in the condensed graph and those in the original graph. This lack of explicit connections obscures the contribution of each original node to its synthetic counterpart, diminishing the traceability of the condensation process. Although these condensed nodes might be informative to GNN models, discerning their real-world semantic meanings can be challenging to humans. This poor traceability can significantly limit the application of GC in sectors where clarity, comprehensive data explanations, and transparent decision-making are crucial. Also, with traceability, users can manipulate the condensed graph alongside the original oneâ€”for example, easily filtering low-quality data when the corresponding original nodes exhibit poor quality.

To tackle the three critical challenges altogether, we introduce a novel framework called \underline{G}raph \underline{E}volving \underline{C}lustering \underline{C}ondensation (GECC). We begin by refining our objectives for GC and providing theoretical bounds that elucidate the connection between GC and clustering. This insight allows us to perform condensation efficiently without resorting to the expensive gradient computations typically required for training GNNs on the entire dataset or other learnable modules.
Specifically, we cluster propagated node features into partitions, where centroids serve as the condensed node features at each time step. Additionally, this method adapts well to evolving, expanding graphs through incremental clustering, using centroids from previous time steps as initializations for subsequent steps. Notably, GECC enhances traceability by establishing a clear correspondence between condensed and original nodes, offering deeper insights into how condensed nodes encapsulate information from the actual graph. Our contributions are outlined as follows:

\begin{compactenum}[\textbullet]
    \item We provide a novel theoretical understanding of objectives in graph condensation from both the training and test perspectives. Our analysis reveals the connection between graph condensation and clustering, demonstrating that condensation can be effectively addressed by performing clustering with balanced clusters.
 
    \item Leveraging our theoretical insights, we introduce the first training-free framework designed to significantly accelerate condensation and adapt to the evolving nature of real-world graphs, while providing favorable traceability. 
    % This framework methodically partitions node representations to produce cluster centroids at each time step, which serve as the condensed node features.
    
    \item Comprehensive experiments demonstrate that GECC achieves state-of-the-art (SOTA) accuracy and efficiency in GC. For example, GECC can condense the evolving \textit{Reddit} dataset more than 1000 times faster than GCond, while outperforming other existing methods in terms of test accuracy.
\end{compactenum}







