\section{Preliminaries and Notations} 

 Given a node set $\mathcal{V}$ and an edge set $\mathcal{E}$, a graph is denoted as $G=({\mathcal{V},\mathcal{E}})$. In the case of attributed graphs, where nodes are associated with features, the graph can be represented as $G=({\bf{X}, \bf{A}})$, where ${\bf X}=[{\bf x}_1,{\bf x}_2,...,{\bf x}_N]$ denotes the node attributes, and ${\bf A}$ shows the adjacency matrix. The graph Laplacian matrix is $\bf L=\bf D-\bf A$, where $\bf D$ is a diagonal degree matrix with ${\bf D}_{ii}=\sum_j {\bf A}_{ij}$. Let $N=|\mathcal{V}|$ and $E=|\mathcal{E}|$ represent the number of nodes and edges, respectively.

\vskip 0.3em \noindent\textbf{Graph Condensation Formulation.}
GC aims to condense a smaller synthetic graph \( G' = (\mathbf{X}', \mathbf{A}') \), where \( \mathbf{X}' \in \mathbb{R}^{N' \times d} \), \( \mathbf{A}' \in \{0, 1\}^{N' \times N'} \), and \( N' \ll N \), from the original large graph \( G = (\mathbf{X}, \mathbf{A}) \). The objective is to ensure that GNNs trained on \( G' \) achieve performance comparable to those trained on \( G \), thereby significantly accelerating GNN training \citep{jin2021graph}. 
% A variation of GC, known as structure-free graph condensation, focuses exclusively on the node features \( \mathbf{X}' \) without utilizing the adjacency matrix \( \mathbf{A}' \) \citep{zheng2024structure}. 
The large-scale graph \( G_t = (\mathbf{X}_t, \mathbf{A}_t) \) serves as the original graph of our GC framework. Each node is associated with one of \( c \) classes, encoded as numeric labels \( \mathbf{y}_t \in \{1, \dots, c\}^{N_t} \) and one-hot labels \( \mathbf{Y}_t \in \mathbb{R}^{N_t \times c} \). Any GC method focuses on generating a condensed graph \( G'_t = (\mathbf{X}'_t, \mathbf{A}'_t) \) from the original graph \( G_t = (\mathbf{X}_t, \mathbf{A}_t) \), preserving the key structural and feature information required for downstream tasks. 
% As our method is structure-free, the adjacency matrix of the generated condensed graph is defined as \( \mathbf{A}'_t = \mathbf{I} \). 

\vskip 0.3em \noindent\textbf{Evolving Graph Condensation Formulation.}\label{sec:evolving_form} In the evolving graph scenario, we consider a sequential stream of graph batches \(\{B_1, B_2, \dots, B_m\}\), where \(m\) represents the total number of time steps. Each graph batch \(B_i = (\mathbf{X}_i, \mathbf{A}_i)\) contains newly added nodes along with their associated edges in \textit{inductive} graphs, while in \textit{transductive} graphs, it contains newly labeled nodes but retains the entire graph structure~\cite{su2023robustincremental}. 
% The differences between these two kind of evolving are illustrated in Figure~\ref{fig:main}. 
These graph batches are progressively integrated into the existing graph over time, constructing a series of incremental graphs \(\{G_1, G_2, \dots, G_t\}\). At time step \(t\), the snapshot graph \(G_t = (\mathbf{X}_t, \mathbf{A}_t)\) encompasses all nodes and edges that have appeared up to that point, with \(G_t = \bigcup_{i=1}^t B_i\), and importantly, we preserve the distribution of classes across splits as the graph evolves. At each time step \(t\), during the GC phase, we aim to generate a condensed graph \(G'_t = (\mathbf{X}'_t, \mathbf{A}'_t)\) from the snapshot graph \(G_t\). This condensed graph \(G'_t\), which retains the essential structural and feature information of \(G_t\), is then used to train GNNs efficiently for any downstream tasks. During deployment, the GNN trained on \(G'_t\) is applied to classify the nodes in \(G_t\). As new graph batches are added at time step \(t+1\), the graph expands to \(G_{t+1}\). Unlike previous approaches that require repeating the GC process from scratch, our method can effectively inherit the condensed graph from the previous time step with minimal computational overhead, ensuring that \(G'_{t+1}\) effectively represents the expanded graph \(G_{t+1}\) while maintaining high performance on the growing dataset.




