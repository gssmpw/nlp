\section{RELATED WORKS}
\subsection{Mixed CNN/RNN-based Models with GRU}
The work \cite{b14} encodes the input two-dimensional stroke trajectory information of handwritten expressions using a Gated Recurrent Unit-based Recurrent Neural Network (GRU-RNN). The decoder is also implemented with GRU-RNN and equipped with a coverage-based attention model. The effectiveness of the recognition process is demonstrated through the attention mechanism. TAP \cite{b15} architecture consists of two main components: the Tracker and the Parser. The Tracker utilizes a stacked bidirectional recurrent neural network with Gated Recurrent Units (GRUs) to model the handwriting trajectory, thereby fully leveraging the dynamic information during the handwriting process. The Parser, on the other hand, employs a GRU with a Guided Hybrid Attention (GHA) mechanism to generate mathematical symbols.  "Watch, attend and parse" \cite{b10} employs a fully convolutional encoder to extract image features and introduces a coverage attention mechanism to address inaccuracies in attention during long-distance decoding, thereby establishing the WAP model as a foundational model for most sequence-based recognition methods. The research \cite{b11} further improved upon this by proposing DenseWAP, which adopts a DenseNet structure to enhance the encoder and introduces a multi-scale attention decoding model to address the challenge of recognizing characters of different sizes in formulas. CAN \cite{b12} proposes a method that utilizes symbol counting as an auxiliary task to enhance the robustness of the encoder-decoder model in HMER, improving recognition accuracy by providing symbol-level positional information and global counting information. SAM \cite{b13} constructed a semantic graph based on statistical co-occurrence probabilities, explicitly showing the dependencies between different symbols; it also proposed a semantic-aware module that takes visual and classification features as input and maps them into semantic space. 

\subsection{Transformer-based Models}
Existing RNN-based encoder-decoder models face issues of insufficient coverage in the task of HMER, manifesting as over-parsing and under-parsing, and struggling with handling long-distance symbol relationships, especially when parsing LaTeX language. BTTR \cite{b16} introduces a transformer decoder that utilizes positional encoding to improve coverage problems and proposes a bidirectional training strategy, enabling a single decoder to perform both left-to-right and right-to-left decoding simultaneously. This enhances the modelâ€™s training efficiency and inference performance, outperforming RNN-based models. Due to the lack of a coverage attention mechanism, CoMER\cite{b17} proposes a new model that introduces an Attention Refinement Module (ARM) and self-coverage and cross-coverage mechanisms, which address the shortcomings of the Transformer decoder in handling coverage attention. As a result, CoMER achieves better performance in the HMER task compared to the standard Transformer and RNN decoders. TSDNet \cite{b28} designed a transformer-based tree decoder that can better capture complex correlations. PosFormer \cite{b18} introduces a novel Position Forest Transformer, which enhances the positioning understanding of sequence-based methods by encoding expressions into a forest structure and parsing their nested hierarchies and relative positions. This method aids expression recognition through a position identification task and incorporates an implicit attention correction module to improve the accuracy of the decoder. PosFormer has demonstrated outstanding performance on multiple benchmark datasets without incurring additional latency or computational costs. NAMER \cite{b29} proposes a novel non-autoregressive modeling approach for handwritten mathematical expression recognition that significantly outperforms state-of-the-art methods in accuracy and decoding speed.