\section{Introduction}

\input{floats/figure_one.tex}

Transformer large language models (LLMs) are computationally expensive and slow to run. To address this challenge, recent work has developed optimizations to make LLM inference and serving more efficient, such as prompt caching \citep{zheng2024sglang,gim2024prompt}. In prompt caching, reuse of the attention key-value (KV) cache across requests results in cache hits and faster response times for prompts that share a prefix with a cached prompt.


However, prompt caching results in data-dependent timing variations---cached prompts will be processed faster than non-cached prompts, introducing the risk of side-channel timing attacks and information leakage. In particular, an attacker could identify prompts that yield fast API response times; such prompts are likely cached. If the cache is shared across users, then a prompt being cached implies that another user recently sent that prompt. Figure~\ref{fig:prompt-caching} illustrates an example of prompt caching and potential privacy leakage. In general, timing differences between cache hits and cache misses have been widely exploited in computer security, such as in the infamous Meltdown \citep{lipp2018meltdown} and Spectre attacks \citep{kocher2018spectre}.



Because prompt caching may result in privacy leakage, it is important for users to know about the prompt caching policies of API providers. Some API providers have announced that they perform prompt caching, such as \citet{anthropic-caching-news} and \citet{openai-caching-news}, but other API providers may be performing prompt caching without announcing it. Also, even if a provider announces prompt caching, they may not state the level of cache sharing, i.e., per-user, per-organization, or global.


Therefore, we develop and conduct an audit to determine if an API provider is caching prompts and the precise level of cache sharing. Our audit uses statistical hypothesis testing and outputs valid p-values with respect to the null hypothesis of no caching, enabling guarantees on the false positive rate.

In our audit, we construct and sample response times from two procedures: one that attempts to produce cache hits, and one that produces cache misses. At a high level, to attempt to produce a cache hit, we send a prompt to the API to try to cache the prompt, then we send the prompt again to try to hit the cache. To produce a cache miss, we simply send a random prompt. Under the null hypothesis of no prompt caching, where only cache misses are possible, these procedures produce identical distributions of times. Accordingly, we detect caching if we find a statistically significant difference between these distributions.



We conducted audits on real-world LLM API providers in September and October 2024. We detected prompt caching in 8 out of 17 API providers. In 7 of these providers, we detected global cache sharing. On these APIs, an attacker could, in principle, detect cache hits from timing differences to infer that another user sent a prompt that shares a prefix with a given prompt.

Timing variations due to prompt caching can also result in leakage of information about a model's architecture. Cache hits between prompts that share a prefix but have different suffixes are possible only in autoregressive decoder-only Transformers, where each token attends only to previous tokens. Therefore, detecting such prompt prefix caching indicates that the model has a decoder-only architecture. Virtually all chat models are decoder-only, but embedding models can have either encoder or decoder architectures. As such, for proprietary embedding models, leakage of architecture information may represent a leakage of intellectual property. By detecting prompt prefix caching, we find evidence that \OpenAI{}'s \OpenAIEmbeddingModel{} has a decoder-only architecture, which was previously not publicly known.

\paragraph{Responsible disclosure.}
In October 2024, we disclosed our audit results with each API provider in which we detected prompt caching. We gave providers 60 days to address the vulnerabilities before publicly releasing our findings, and the actual time elapsed ended up being longer. To our knowledge, at least five providers made changes to mitigate vulnerabilities, e.g., disabling global cache sharing across organizations and updating documentation.






