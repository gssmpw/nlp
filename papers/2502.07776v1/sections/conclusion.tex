\section{Conclusion}

As LLMs and other machine learning systems become more widely deployed and used in the real world, it is increasingly important to consider security and privacy aspects of these systems. To this end, in this paper, we find that prompt caching in LLM APIs can leak private and proprietary information through timing differences. We develop and conduct rigorous statistical audits on real-world APIs, finding that multiple APIs were performing global cache sharing. We hope that future work will continue to evaluate and audit the security and privacy of machine learning systems, ensuring their robustness and trustworthiness.

