\input{floats/table_audit_results.tex}
\input{floats/table_audit_no_caching.tex}

\input{floats/figure_histograms.tex}

\section{Auditing Real-World APIs}

Next, we audit real-world LLM APIs to identify APIs that cache prompts and determine the level of cache sharing, i.e., per-user, per-organization, or global. Cache sharing results in potential privacy leakage, as an attacker could, in principle, identify cached prompts using timing data to learn information about other users' prompts.


\subsection{Audit Setup and Configuration}
\label{sec:audit-setup}

\paragraph{API providers and models.}
We audit 17 API providers: \Anthropic{}, \AmazonBedrock{}, \MicrosoftAzure{}, \Cohere{}, \DeepInfra{}, \DeepSeek{}, \FireworksAI{}, \Google{}, \Groq{}, \Hyperbolic{}, \LeptonAI{}, \Mistral{}, \OctoAI{}, \OpenAI{}, \Perplexity{}, \Replicate{}, and \TogetherAI{}. The model APIs that we audit for each provider are included in Tables~\ref{tab:audit-results} and \ref{tab:audit-no-caching}. For API providers that primarily serve open-weight models, we audit their Llama 3 or 3.1 8B Instruct API \citep{dubey2024llama}. For providers that serve proprietary models, we audit the cheapest chat model in their most recent family of models. In addition, we audit APIs for proprietary embedding models, where available. We do not audit APIs for open-weight embedding models because we did not find any APIs that served open-weight decoder-only Transformer embedding models. Prefix caching is possible in decoder-only Transformers but not encoder-only Transformers, where each token attends to all other tokens in the prompt.



\paragraph{Configuration and procedure.}
For our audits, we use $\PromptLength{} = 5000$ and $\NumSamples{} = 250$. We run four levels of audits of increasing cache sharing and privacy leakage. At each level, we only continue to audit APIs if we detect caching during the previous level. We use a significance level of $\alpha = 10^{-8}$.

To narrow down our list of providers, the first level tests for the simplest level of prompt caching:
\begin{enumerate}
    \item \textbf{Same prompt, per-user caching.} We test for prompt caching on exact prompt matches ($\tilde{x} = x$) by setting $\PrefixFraction = 1$. We set $\uvictim$ and $\uattacker$ to be the same user, and we set $\NumVictimRequests{} = 25$.
\end{enumerate}
In the remaining three levels, we test for prompt caching when $\tilde{x}$ and $x$ have the \textbf{same prefix but different suffixes} by setting $\PrefixFraction = 0.95$. We test for increasing levels of cache sharing by appropriately setting the victim and attacker users:
\begin{enumerate}
    \setcounter{enumi}{1}
    \item \textbf{Per-user caching.} $\uvictim$ and $\uattacker$ are the same user, as in the first level.
    \item \textbf{Per-organization caching.} $\uvictim$ and $\uattacker$ are different users within the same organization. For APIs without organizations, we skip this level.
    \item \textbf{Global caching.} $\uvictim$ and $\uattacker$ are different users in different organizations. For APIs without organizations, $\uvictim$ and $\uattacker$ are simply different users.
\end{enumerate}
In levels 2--4, to determine how many victim requests are needed to detect caching, we run tests using $\NumVictimRequests{} \in \{1, 5, 25\}$ in increasing order, stopping after the first significant p-value. To account for multiple testing, we perform a Bonferroni correction by dividing the significance threshold for each test by three.

In all levels, if only one timing method is available in an API (client-side or server-side timing), then we use that timing method. If both are available, we run tests using both timing methods and perform another Bonferroni correction, dividing by two this time.


\paragraph{Cost per test.}
When $\NumVictimRequests{} = 25$, one test uses roughly 34 million prompt tokens. The number of response tokens used is much smaller because we set the maximum response tokens parameter to 1. For the chat APIs we audit, the prices per million prompt tokens are 0.05--0.25 USD, resulting in a cost per test of 1.69--8.44 USD. The tests are cheaper when $\NumVictimRequests{}$ is smaller.


\subsection{Audit Results}

We conducted our audits in September and early October 2024 using clients located in California. Table~\ref{tab:audit-results} shows audit results for APIs in which we detected prompt caching, and Table~\ref{tab:audit-no-caching} shows APIs in which we did not detect prompt caching. We detected prompt caching in 8 out of 17 API providers, and we detected global cache sharing in 7 providers. This means that an attacker can potentially learn information about other users' prompts by identifying cached prompts from timing data. To assess an attacker's ability to distinguish between cache hits and cache misses, Figure~\ref{fig:precision-recall-curves-selected} contains selected precision-recall curves for classifying times from the cache hit procedure.\footnote{The cache hit procedure attempts to produce cache hits but cannot guarantee cache hits (e.g., due to server routing), so some times in the cache hit distribution may actually be cache misses.} The curves show that cache hits can be detected with near perfect precision up to moderate recall scores. Figure~\ref{fig:precision-recall-curves-full} in the appendix shows precision-recall curves for the highest level of cache sharing we detected in each API. To numerically summarize these curves, we compute the average precision \cite{zhu2004recall}, which is equal to the area under the precision-recall curve (the precision is averaged over the interval of recall scores from 0 to 1). Table~\ref{tab:audit-results} shows that the average precisions mostly lie around a moderately high value of 0.8.



Figure~\ref{fig:histograms} displays histograms of times from the cache hit and cache miss procedures. The distributions of times are clearly distinguishable, with cache hits tending to be faster. Each histogram title states the minimum \NumVictimRequests{} (denoted \textsc{v} in the titles) that resulted in a significant p-value.
In most of the APIs where we detected caching, only $\NumVictimRequests{} = 1$ was needed to detect caching. Only the \OpenAI{} and \Azure{} \OpenAIEmbeddingModel{} APIs required $\NumVictimRequests{} = 25$ to achieve a significant p-value. This may suggest that these APIs have multiple servers with separate caches and that requests are randomly routed to a server, so multiple victim requests are needed to cache the prompt in enough servers for the attacker's prompt to have a sufficient probability of producing a cache hit. In Appendix~\ref{app:audit-pvalues}, we report all the p-values from our audits. In many APIs, the p-values are many orders of magnitude smaller than our significance level of $\alpha = 10^{-8}$. In all APIs where we detected caching, all available timing methods resulted in significant p-values.



In the \Anthropic{} \AnthropicModel{} and \OpenAI{} \OpenAIChatModel{} APIs, we detected per-organization cache sharing, but not global cache sharing. This exact level of cache sharing is stated in their prompt caching documentations, confirming the efficacy of our audit procedure. Since \citet{openai-caching-docs} and \citet{anthropic-caching-docs} document per-organization cache sharing, we do not consider it a security vulnerability. Global cache sharing in the \OpenAI{} \OpenAIEmbeddingModel{} API was a potential vulnerability, but has been patched after our responsible disclosure prior to the release of this paper.

Although \citet{deepseek-caching} has a prompt caching feature and returns the number of cache hit tokens in API responses, which we used to confirm that we produced cache hits, we were unable to detect caching from response times. There was no statistically significant difference between the distributions of cache hit and cache miss times, even in two-sided tests. \DeepSeek{} states that the cache is isolated per-user, and we empirically verified that this is the case based on the number of cache hit tokens returned in the API responses.

\input{floats/precision_recall_curves_selected.tex}

\input{floats/figure_ablations.tex}

\subsection{Ablations}
\label{sec:ablations}


We run ablations to determine the effects of \PromptLength{}, \PrefixFraction{}, and model size on the average precision, shown in Figure~\ref{fig:ablations}. We use the APIs in which we detected global caching with $\NumVictimRequests{} = 1$, i.e., the Llama 3 or 3.1 8B Instruct APIs of \Fireworks{}, \Perplexity{}, and \Replicate{}.


\paragraph{Smaller \PromptLength{} decreases average precision.} In Figures~\ref{fig:ablations}a and \ref{fig:ablations}b, we vary the \PromptLength{} in the same prompt ($\PrefixFraction = 1$) and same prefix but different suffixes ($\PrefixFraction = 0.95$) settings, respectively. When the \PromptLength{} is moderately high ($\gtrapprox 1000$), the average precision is relatively high and stable. However, as the \PromptLength{} approaches zero, the average precision decreases to random chance.

\paragraph{Decreasing \PrefixFraction{} decreases average precision.} In Figure~\ref{fig:ablations}c, we vary the \PrefixFraction{} while setting $\PromptLength{} = 1000$. As the length of the matching prefix decreases, the average precision decreases to random chance.

\paragraph{No clear relationship between model size and average precision.} In Figure~\ref{fig:ablations}d, we vary the model size on the \Fireworks{} API, which supports all models in the Llama 3.1 and 3.2 families. We detected caching in all model sizes, with no clear relationship between model size and average precision.

\paragraph{Relationship to p-values.} Figure~\ref{fig:ablations-p-values} in Appendix~\ref{app:ablations-p-values} shows the effects of the ablations on the audit p-values. We observe similar patterns as above, with decreases in average precision corresponding to increases in p-values.

\subsection{Difficulty of Prompt Extraction Attacks}

Our results show that given a specific prompt $x$, an attacker could potentially detect cache hits to learn whether another user sent a prompt that shares a prefix with $x$. A natural question is whether an attacker could extract others users' prompts token-by-token. One idea is to use breadth-first search: given a partial candidate prompt, such an attack would try possible continuation tokens and determine which continuation token is cached. The cached token is appended to the candidate prompt and the process repeats.


However, we were unable to execute practical prompt extraction attacks. A successful attack requires extremely accurate detection of cached tokens, as there are many possible continuation tokens at each step. Just one incorrect token causes complete failure due to the exact prefix match required for a cache hit. In preliminary experiments, we were unable to reliably detect the presence one additional cached token. It is also difficult to make repeated measurements to boost accuracy. To detect whether a prompt is cached, the attacker must send the prompt to the API. Then, future measurements may produce a cache hit not because another user sent the prompt, but because the attacker sent it.



We do not claim that prompt extraction attacks are necessarily impossible. Such attacks face difficulties, but future work may yet develop successful, practical attacks. In addition, in more restricted sets of target prompts, e.g., known prompt templates with places for users to enter private personal information, it may be easier to overcome these difficulties.


