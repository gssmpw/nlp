\section{An Audit to Detect Prompt Caching}

Next, we propose an audit to detect whether an API provider is caching prompts and determine the level of cache sharing. Our audit uses statistical hypothesis testing and outputs valid p-values with respect to the null hypothesis of no caching, allowing for guarantees on the false positive rate.


\subsection{Audit Formulation: Statistical Hypothesis Testing}

To test for a given level of cache sharing, let $\uvictim$ and $\uattacker$ be two users that are the farthest away within that level. For example, to test for per-organization caching, $\uvictim$ and $\uattacker$ should be different users in the same organization.


We formulate our audit as a statistical hypothesis test using the following null and alternative hypotheses:
\begin{align*}
    H_0 &: \text{API does not cache prompts (at this level of sharing)}, \\
    H_1 &: \text{API caches prompts (at this level of sharing)}.
\end{align*}
The caching in $H_0$ does not refer only to prompt caching via the KV cache reuse described earlier. More verbosely, $H_0$ can be written as ``when $\uvictim$ sends a prompt $x$ to the API, the API does not store any information about $x$ that affects the TTFT $T(\tilde{x})$ for any future prompt $\tilde{x}$ sent by $\uattacker$''.


To test these hypotheses, we construct procedures that attempt to produce and measure the TTFT of cache hits and cache misses. Let $\PromptDistribution$ be a distribution of prompts. To produce a cache miss, $\uattacker$ simply sends a random prompt $x' \sim \PromptDistribution$ to the API and measures the TTFT $T(x')$.

To attempt to produce a cache hit, first, we sample a prompt $x \sim \PromptDistribution$, and $\uvictim$ sends $x$ to the API one or multiple times to try to cache $x$.\footnote{Multiple requests may be necessary in some scenarios, e.g., if API requests are randomly routed to one of several servers, and each server has a separate cache.} Next, we sample $\tilde{x} \sim \PromptDistribution$ such that $\tilde{x}$ and $x$ share a prefix of a certain length. To try to produce a cache hit, $\uattacker$ sends $\tilde{x}$ and measures the TTFT $T(\tilde{x})$.




Let $\Dhit$ and $\Dmiss$ be the distributions of TTFTs from these cache hit and cache miss procedures, respectively. Under the null hypothesis $H_0$ of no caching, $\Dhit = \Dmiss$, as both procedures will produce only cache misses. In contrast, under the alternative hypothesis $H_1$ of caching, we would expect the cache hit times to tend to be faster than the cache miss times, so $\Dhit \neq \Dmiss$. Now, we can reformulate our hypotheses as
\begin{align*}
    H_0 &: \Dhit = \Dmiss, \\
    H_1 &: \Dhit \neq \Dmiss.
\end{align*}
Given this reformulation, to perform our audit, we first sample TTFTs from the cache hit and cache miss procedures. Then, we run a statistical test for whether our samples came from the same distribution, e.g., the two-sample Kolmogorov-Smirnov test, producing a p-value with respect to the null hypothesis of no caching. 


\subsection{Audit Implementation Details}

Next, we describe the concrete implementation details of our audit. The procedure uses the following configuration parameters: \PromptLength{}, \PrefixFraction{}, \NumVictimRequests{}, and \NumSamples{}. The meanings of these parameters will become clear in the descriptions below.




\paragraph{Prompt distribution.} Our distribution $\PromptDistribution$ of prompts is a uniform distribution over all prompts consisting of \PromptLength{} English letters, lowercase and uppercase, each separated by space characters, e.g., ``\texttt{m x N j R}''. Because all commonly used byte pair encoding (BPE) tokenizers \citep{gage1994new,sennrich-etal-2016-neural} split on whitespace during pre-tokenization, all prompts in $\PromptDistribution$ will be exactly \PromptLength{} tokens long.\footnote{Many APIs add a small number of tokens to the user prompt due to the default system prompt, special tokens for prompt and role formatting, etc. However, these additional tokens are unimportant for our procedure, as the number of additional tokens is small and remains constant across prompts to a given model API.} 

\paragraph{Cache miss.}
$\uvictim$ sends a random prompt $x \sim \PromptDistribution$ to the API and measures the TTFT $T(x)$. Since the prompt consists of random letters, there is a negligible probability that a noticeable prefix has already been cached: the probability that two prompts sampled from $\PromptDistribution$ share a prefix of 15 tokens or longer is less than $10^{-25}$. Therefore, this procedure accurately measures a distribution of cache miss times.


\paragraph{Cache hit.}
First, we sample a random prompt $x \sim \PromptDistribution$. Then, $\uvictim$ sends $x$ to the API \NumVictimRequests{} times consecutively to try to cache $x$. Then, we sample $\tilde{x} \sim \PromptDistribution$ such that $\tilde{x}$ and $x$ have a shared prefix of exactly $\PrefixFraction \times \PromptLength$ tokens. To attempt to produce a cache hit, $\uattacker$ sends $\tilde{x}$ to the API and measures the TTFT $T(\tilde{x})$. When $\PrefixFraction = 1$, we test for prompt caching when $\tilde{x} = x$, i.e., exact prompt matches. When $\PrefixFraction < 1$, we test for prompt caching when $\tilde{x}$ and $x$ have the same prefix but different suffixes, e.g., ``\texttt{a b c d}'' and ``\texttt{a b c x}''.





\paragraph{Statistical testing.}
Putting these pieces together, to perform the audit, we collect \NumSamples{} timings each from the cache hit and cache miss procedures. We randomize the order in which we collect the timing samples. Then, we test for a statistically significant difference between the distributions of times from the two procedures. We use the SciPy implementation \citep{2020SciPy-NMeth} of the two-sample Kolmogorov-Smirnov (KS) test \citep{hodges1958significance}, which is a nonparametric test for equality of distributions. The test statistic is the maximum difference between the empirical cumulative distribution functions at any point. More specifically, since we expect cache hits to be faster under the alternative, we perform a one-sided test, so the test statistic is the maximum difference in the direction of cache hits being faster. The KS test outputs a p-value, which we can use to reject or not reject the null hypothesis of no prompt caching at a given significance level $\alpha$.
