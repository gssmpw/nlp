\section{Mitigations}

\paragraph{Per-user caching prevents privacy leakage.} To completely prevent any privacy leakage from prompt caching, only per-user caching should be allowed. In per-user caching, an attacker will not be able to produce cache hits on prompts sent by other users. Since it is unlikely that different users will send prompts with long matching prefixes, per-user caching should retain many of the performance benefits from global cache sharing.

\paragraph{Disclosure of cache sharing.} We believe that providers should disclose their caching policies, particularly the level of cache sharing. It is important that users know how their data is handled and who could potentially learn information about their data. This way, users can make informed decisions about how they use an LLM API. For example, if a company knows that an API uses per-organization cache sharing, the company can decide to create separate organizations for different groups of employees to prevent unauthorized information access.



\paragraph{Disabling caching prevents any information leakage.} For information leakage that only requires per-user caching, such as leakage of architecture information, the strongest mitigation is to disable prompt caching. Since per-user caching does not result in privacy leakage, but may result in leakage of the API provider's intellectual property, it is up to the provider to determine their level of risk tolerance. Another potential mitigation is to intentionally delay the response time for cache hits so that they look like cache misses. This eliminates the benefits of prompt caching for users, but API providers could still benefit, as cached prompts require less GPU processing time. 

