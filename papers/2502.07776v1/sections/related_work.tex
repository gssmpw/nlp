\section{Related Work}

\paragraph{Prompt caching.}
Many recent works have developed optimizations for inference and serving of Transformer language models. Various methods involve reuse of the attention KV cache, improving latency and throughput for shared prompt prefixes \citep{kwon2023efficient,zheng2024sglang,gim2024prompt,ye-etal-2024-chunkattention,cascade-inference,qin2024mooncake,juravsky2024hydragen}. Recall that we do not assume any particular implementation of prompt caching in our attacks. Indeed, we do not know technical details about the caching mechanisms used by the APIs we audited. Other caching methods do not preserve exact model behavior, such as retrieving cached responses for semantically similar prompts \citep{bang-2023-gptcache} or reusing the KV cache even when the prefixes do not exactly match \citep{gim2024prompt,yao2024cacheblend,hu2024epic}. We do not study such methods, but they are also likely susceptible to similar cache timing attacks, and our audit can easily be adapted to detect other types of caching.

\paragraph{Cache timing attacks.}
In computer security, many side-channel timing attacks have extracted information by using timing differences to distinguish between cache hits and cache misses, e.g., in the CPU cache or web cache. For example, cache timing attacks have been used to extract AES keys \citep{bernstein2005cache,osvik2006cache,bonneau2006cache,tromer2010efficient,gullasch2011cache,yarom2017cachebleed}, a user's private web information \citep{felten2000timing,bortz2007exposing,van2015clock}, and sensitive data from other processes on a machine \citep{percival2005cache,yarom2014flush,liu2015last}, as in the well-known Meltdown \citep{lipp2018meltdown} and Spectre attacks \citep{kocher2018spectre}.

\paragraph{Attacks on language model APIs.}
Several recent works have attacked language model APIs. \citet{carlini2024stealing} and \citet{finlayson2024logits} show that logits and logprobs leak information from an LLM API, including the model's hidden dimension size and final layer weights. \citet{weiss2024your} partially extract encrypted and streamed LLM responses by inferring and analyzing token lengths from packet sizes. \citet{carlini2024remote} and \citet{wei2024privacy} exploit speculative decoding \citep{leviathan2023fast,chen2023accelerating} and similar methods to extract LLM responses with higher success by measuring delays between packets.

Most related to our work are \citet{song2024early} and \citet{zheng2024inputsnatch}, which also study timing attacks and privacy leakages arising from prompt caching, including both KV cache reuse and semantic caching, primarily in simulated, controlled environments. Our work differs in developing an audit that is practical and provides statistical guarantees, using these audits to precisely identify different levels of cache sharing, and extracting information about model architecture. \citet{song2024early} demonstrate prompt extraction attacks in a simulated setting, but the attack is run locally without network latency, uses knowledge of the distribution of prompts, requires explicit clearing of the cache to make repeated measurements, and makes an average of over 200 measurements for each extracted token. Due to these limitations, we believe that these simulated attacks are currently unlikely to be real-world privacy threats.

