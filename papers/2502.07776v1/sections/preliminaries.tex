\section{Preliminaries and Assumptions}

First, we briefly describe prompt caching, our assumptions on how users and attackers can interact with an API, and the levels of cache sharing and privacy leakage.

\subsection{Prompt Caching}

Recent works have proposed prompt caching in Transformer \citep{vaswani2017attention} LLM serving by reusing the attention key-value (KV) cache across requests \citep{zheng2024sglang,gim2024prompt}. In these methods, a prompt is cached by storing the prompt's attention KV cache. Then, if a subsequent prompt has a matching prefix with a cached prompt, the KV cache for the matching prefix can be retrieved from the cache. As a result, cache hits will tend to have a faster time to first token (TTFT), which is the time taken to process the prompt and generate the first response token.\footnote{In embedding models, we can view the embedding output as the first and only response ``token''.} In decoder-only Transformers, where each token attends only to previous tokens, reusing the KV cache for matching prefixes exactly preserves model behavior, even when the prompt suffixes differ. Figure~\ref{fig:prompt-caching} illustrates an example of prompt caching. 

Several API providers have recently announced prompt caching features, including \citet{anthropic-caching-news}, \citet{deepseek-caching}, \citet{fireworks-caching}, and \citet{openai-caching-news}. These providers do not state technical details of prompt caching, but these providers state that cache hits occur for (and only for) exact prefix matches between prompts. For our purposes, the precise implementation of prompt caching is largely unimportant. The properties of prompt caching that we exploit are:
\begin{enumerate}
    \item Cache hits occur on prefix matches between prompts.
    \item Cache hits tend to have a faster TTFT than cache misses (after accounting for prompt length).
\end{enumerate}
To describe these properties more formally, assume that a model API takes in a prompt $x$ and has a TTFT $T(x)$. Note that $T(x)$ is a random variable due to variance from network latency, server load, etc. Assume that the API has a cache $\Cache$, which is a set of cached prompts. If $x$ has a sufficiently long matching prefix with some cached prompt $c \in \Cache$, then a cache hits occurs.


For example, let $c =$ ``\textit{The quick brown fox jumps}'' and $\Cache = \{c\}$. If $x_1 =$ ``\textit{The quick brown fox runs}'', then $x_1$ and $c$ have a matching prefix of ``\textit{The quick brown fox }'', so $x_1$ could result in a cache hit. On the other hand, if $x_2 =$ ``\textit{A quick brown fox jumps}'', then $x_2$ and $c$ do not share a prefix, so $x_2$ results in a cache miss. Since $x_1$ and $x_2$ are similar lengths but $x_1$ is a cache hit and $x_2$ is a cache miss, we would expect that $\Expectation[T(x_1)] < \Expectation[T(x_2)]$.

When a prompt $x$ is sent to the API, we assume that $x$ is added to the cache $\Cache$ and that $x$ will remain in $\Cache$ for some finite period of time. The API may use multiple servers, each with their own separate caches. We do not make assumptions about how prompts are routed to servers. A prompt may be randomly routed, or it may be intentionally routed to a server where the prompt is already cached.




\subsection{API Assumptions}

We assume that it is possible to send arbitrary prompts to the API (possibly subject to some maximum length) and measure the TTFT. The TTFT can be measured by setting the maximum tokens parameter to 1, which restricts the LLM output to only contain 1 token. Then, the overall response time is equal to the TTFT. The max tokens parameter is supported by most, if not all, real-world LLM APIs.

Either client-side or server-side timing suffices for our purposes. The client-side timing is obtained simply by measuring the time elapsed between sending the API request and receiving the API response. The server-side timing can be measured if it is contained somewhere in the API response.\footnote{We can measure server-side timing in more than half of the APIs we test, often from undocumented fields in the HTTP headers of the API response.}


\subsection{Levels of Cache Sharing and Privacy Leakage}

\input{floats/figure_users_organizations.tex}

To facilitate our discussion of prompt cache sharing and privacy leakage in APIs, we define our terminology of users and organizations. A \textbf{user} is one person that uses the API. Each user has a unique email/username and login password. An \textbf{organization} contains many users, but shares a billing system, centralized membership management, etc. Organizations can be used by companies, research groups, etc. Many, but not all, API providers support organizations, although sometimes under different terminology, such as teams or accounts. For consistency and simplicity, we refer to them all as organizations. Figure~\ref{fig:users-organizations} shows the hierarchical structure of users and organizations.

We consider three levels of cache sharing and their corresponding potential privacy leakages.
\begin{enumerate}
    \item \textbf{Per-user caching.} Each user has their own cache, i.e., when user $u$ sends a prompt, a cache hit can occur only with a cached prompt previously sent by user $u$. Therefore, there is no potential privacy leakage arising from prompt caching.
    \item \textbf{Per-organization caching.} Each organization has its own cache, i.e., when user $u$, who belongs to organization $o$, sends a prompt, a cache hit can occur only with a cached prompt previously sent by any user in organization $o$. There is a slight risk of privacy leakage if certain users in the organization have access to privileged information that other users should not, e.g., the CEO knowing sensitive business data. However, this risk can be mitigated, as the organization owner has full control over which users are members.
    \item \textbf{Global caching.} The cache is shared across all users of the API, e.g., when a user sends a prompt, a cache hit can occur with any cached prompt, regardless of who sent it. This leads to the highest risk of privacy leakage, as an attacker could potentially learn information about any other user's prompts, including users in other organizations.
\end{enumerate}



