\section{Leakage of Architecture Information}
\label{sec:architecture-leakage}


In addition to privacy implications, the detection of prompt caching can also reveal information about a model's architecture. This is because the conditions for cache hits to occur depend on model architecture.

In decoder-only Transformer models, reuse of the attention KV cache enables cache hits between prompts with matching prefixes, even if the suffixes differ, since each token attends only to previous tokens. This prefix caching is not possible in encoder-only or encoder-decoder Transformer models, where each token in the prompt attends to all other tokens in the prompt. Therefore, detecting such prompt prefix caching indicates that a model cannot have a bidirectional encoder architecture. Virtually all chat models are decoder-only, but embedding models can have either encoder or decoder architectures, as seen in the Massive Text Embedding Benchmark (MTEB) leaderboard \citep{muennighoff-etal-2023-mteb}. As such, for proprietary embedding models, leakage of architecture information may represent a leakage of intellectual property.

In our audits (Table~\ref{tab:audit-results}), we detected prompt caching in \OpenAI{}'s \OpenAIEmbeddingModel{} API when prompts had the same prefix but different suffixes. We confirm that when the prompt suffix is changed, the returned embedding also changes, indicating that the caching mechanism does not simply return cached embedding outputs from similar prompts. Assuming that \OpenAIEmbeddingModel{} is Transformer-based, this indicates that \OpenAIEmbeddingModel{} is a decoder-only Transformer. This is new information, as \OpenAI{} has not released any information about the architecture of their embedding models.

\paragraph{Floating-point precision of the cache.}
When we send the exact same prompt multiple times, when the response time is noticeably faster, indicating a cache hit, the returned embedding differs slightly from the ``normal'' embedding in most of the responses with normal response times, which indicate cache misses. This behavior is consistent across different random prompts. These differences are small, on the order of $10^{-4}$ to $10^{-5}$ in each coordinate. We hypothesize that these differences may arise if the reused KV cache is stored in a lower floating-point precision, resulting in slight discrepancies when the attention KV is computed from scratch in cache misses versus when it is retrieved from the cache in cache hits. Interestingly, in some responses, especially those that are noticeably slower, the embedding differs from both the ``normal'' and ``cache hit'' embeddings. This may be caused by some responses being processed by different GPU models, as floating point computations can differ slightly across different GPUs. Appendix~\ref{app:embeddings-openai} contains examples of response times and embeddings showing this phenomenon.

