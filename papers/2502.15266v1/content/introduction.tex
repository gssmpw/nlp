\section{Introduction}
Given an input sentence $\substring{x} = x_1, x_2 \cdots x_n$, the task of conventional Chinese spelling correction (CSC) aims to produce a new sentence of the same length, denoted as $\substring{y} = y_1, y_2 \cdots y_n$, where each misspelled character (\textit{e.g.,} $x_i$) is replaced with a correct one (\textit{e.g.,} $y_i$).
Spelling errors in text can cause misunderstandings, damage authenticity, and even lead to unnecessary financial losses, making automatic correction a crucial task in Chinese Natural Language Processing (NLP) \cite{wu-etal-2023-rethinking,zhou-etal-2024-simple,li-etal-2024-cllm,dong-etal-2024-rich,liu-etal-2024-arm}.

Spelling errors mainly arise from two sources:
The first source is typing mistakes.
Chinese characters often have multiple visually or phonetically similar variants, making it easy to select an incorrect one when using input methods \cite{hu-etal-2024-cscd}.
The second source is automatic text conversion errors.
When using automatic speech recognition (ASR) or optical character recognition (OCR), systems may also introduce incorrect characters during the conversion process \cite{wang-etal-2018-hybrid}.

\input{figures/research_scope}
Beyond spelling errors, character errors involving missing and redundant characters are also common \cite{he-etal-2023-umrspell}.
These errors can be caused by the same reasons as spelling errors.
For instance, when typing ``曲安'' (\textit{qū ān}) in ``醋酸曲安奈德'' (\textit{Triamcinolone Acetonide}), the input method might consider ``\textit{qū ān}'' as a single character ``圈'' (\textit{quān}), resulting in a misspelled and missing character error.\footnote{A real case from the Lemon \textit{Mec} subset.}
Additionally, repetitive sentence editing, such as when revising messages, social media posts, or emails, can easily introduce missing and redundant errors.
Consider this example: ``我参加的项目是\wrong{羽球}。'' (\textit{The project I participated in is \wrong{badmton}}), where ``毛'' was omitted from ``羽毛球'' (\textit{badminton}).
The missing ``毛'' likely occurred during editing: when correcting a mistyped ``求'' to ``球'', the user accidentally deleted two characters before retyping ``球''.
Such errors often go unnoticed without thorough proofreading.
For clarity, we refer to misspelling, missing, and redundant errors as \textbf{General Chinese Character Errors} (C2E).

Recent Chinese text correction competitions, CTC 2021 \cite{zhao-etal-2022-overview}, Midu-CTC\footnote{\url{https://aistudio.baidu.com/competition/detail/404/0/introduction}}, and Kingsoft-CTC\footnote{\url{https://datastudio.wps.cn/matchcenter/competition/1/introduction}}, have designed error distributions to reflect real-world scenarios.
In these competitions, C2E constitute 79.4\% to 87.3\% of errors (Table~\ref{tab:data_statistics_main}).
This indicates that C2E are more common than complex errors like grammatical mistakes, logical inconsistencies, or ambiguity.
Compared to conventional CSC, addressing C2E is more practical as it covers a broader range of common errors.

Thus, we believe that \textbf{General Chinese Character Error Correction} (C2EC) deserves more attention from the Chinese text correction community.
While \citet{he-etal-2023-umrspell} previously studied C2EC, they created synthetic ECMR-2023 by placing random errors into correct sentences.
There is a lack of a dataset specifically focusing on C2E with real-world errors.
To fill this gap, we build a new C2EC dataset using two existing datasets containing real-world errors: CCTC \cite{wang-etal-2022-cctc} and Lemon \cite{wu-etal-2023-rethinking}.
We carefully verified the data to ensure data quality and annotation consistency, resulting in 1,995 sentences for development and 5,711 for testing.

Recent work has shown the power of large language models (LLMs) for CSC \cite{dong-etal-2024-rich,li-etal-2024-cllm,zhou-etal-2024-simple}.
Notably, by combining an LLM, which evaluates fluency, with a Hamming distance to prevent over-correction, the training-free prompt-free framework (\texttt{TfPf}) \cite{zhou-etal-2024-simple} achieves strong results without any training.
We extend this approach to C2EC by using Levenshtein distance to handle missing and redundant errors, and incorporating prompt-based probability scoring for better performance.

Experiments show that our approach achieves large improvements over the \texttt{TfPf} baseline on both conventional CSC and C2EC datasets, and even outperforms the supervised fine-tuned counterparts on various domains.
It is also worth mentioning that our approach enables a 14B parameter model to achieve competitive performance with models nearly 50 times larger without any training.


Our code is available at \url{https://github.com/Jacob-Zhou/simple-csc/tree/v2.0.0}.
