\section{The Baseline TfPf Approach}
\label{sec:baseline_tfpf_approach}
The training-free prompt-free framework (\texttt{TfPf}) \cite{zhou-etal-2024-simple} combines a large language model with a distance metric:
\begin{equation}
    \begin{aligned}
        s(\substring{x}, \substring{y}) = \log p_{\mathtt{LLM}}(\substring{y}) - \mathtt{Dist}(\substring{x}, \substring{y})
        \label{eq:tfpf_score}
    \end{aligned}
\end{equation}
The first term $p_{\mathtt{LLM}}(\substring{y})$ represents a large language model that models the probability of the correct sentence $\substring{y}$, ensuring the correction is fluent from a language perspective.
The second term $\mathtt{Dist}(\substring{x}, \substring{y})$ measures the Hamming distance\footnote{The original \texttt{TfPf} paper refers to this part as the distortion model. However, from a mathematical perspective, the distortion model is equivalent to minus the weighted Hamming distance between $\substring{x}$ and $\substring{y}$. Details are in Appendix~\ref{subsec:distortion_model_of_tfpf_as_a_weighted_hamming_distance}.} between $\substring{x}$ and $\substring{y}$, preventing the model from making too many changes or inserting/deleting characters in the input sentence to achieve fluency.

\section{Our Training-free Approach for C2EC}
\label{sec:our_approach}

We extend the \texttt{TfPf} framework with the following equation:
\begin{equation}
    \begin{aligned}
        s(\substring{x}, \substring{y}) =\  & {\log p_{\mathtt{LLM}}(\substring{y}\mid \mathtt{Prompt}(\substring{x}))}                         \\
                                            & + \log p_{\mathtt{LLM}}(\substring{y}) - \mathtt{Dist}_{\mathtt{L}}(\substring{x}, \substring{y})
        \label{eq:gcsc_score}
    \end{aligned}
\end{equation}
where $\mathtt{Prompt}(\cdot)$ is the prompt template.

Our approach makes three key extensions to the \texttt{TfPf} framework:
\begin{inparaenum}[\itshape a)]
    \item We allow the input sentence $\substring{x}$ to have a different length from the output sentence $\substring{y}$.
    \item We replace the original Hamming distance metric with Levenshtein distance to handle varying input and output lengths. We use $\mathtt{Dist}_{\mathtt{L}}$ to distinguish it from the original distance metric $\mathtt{Dist}$.
    \item We incorporate an additional prompt-based probability $p_{\mathtt{LLM}}(\substring{y}\mid \mathtt{Prompt}(\substring{x}))$ in the score function to improve performance.
\end{inparaenum}

\subsection{Incremental Weighted Levenshtein Distance for Generation}
Following \texttt{TfPf}, we use a weighted distance metric to measure the distance between $\substring{x}$ and $\substring{y}$.
We classify each edit operation into different types and assign a specific weight to each type.
Table~\ref{tab:type_example} shows the weights used in our work.
For operations already defined in \texttt{TfPf}, we adopt their original weights\footnote{\url{https://github.com/Jacob-Zhou/simple-csc/blob/main/configs/default_config.yaml}}.
For the two new operation types, \texttt{Insert} and \texttt{Delete}, we set weights to 8.5 and 9.0, respectively, after grid search on development sets.

Large language models generate output incrementally, while Levenshtein distance requires solving a dynamic programming problem over the entire sentence.

To mitigate this incompatibility, we introduce an incremental Levenshtein distance.
For a partial output candidate $\substring{y}_{\le j}$, we define the partial distance as the Levenshtein distance between $\substring{y}_{\le j}$ and a corresponding prefix of the input sentence $\substring{x}_{\le b}$.
The incremental Levenshtein distance after generating a new character $y_j$ is:
\begin{equation}
    \begin{aligned}
         & \Delta_{\mathtt{Dist}_{\mathtt{L}}}(\substring{x}, a, b, \substring{y}_{<j}, y_j)                                                                               \\
         & \quad = \mathtt{Dist}_{\mathtt{L}}(\substring{x}_{\le b}, \substring{y}_{<j} \circ y_j) - \mathtt{Dist}_{\mathtt{L}}(\substring{x}_{\le a}, \substring{y}_{<j})
    \end{aligned}
\end{equation}
where $\circ$ denotes concatenation, and $a$ and $b$ are the end indices of the input prefix before and after generating $y_j$, respectively.
\input{tables/type_example.tex}

\subsection{Extra Use of Prompt-based LLM}
\paragraph{A Reinforcement Learning Perspective}
The use of two LLMs in Equation~\ref{eq:gcsc_score} can be understood from a reinforcement learning perspective.
In this view, the \texttt{TfPf} framework serves as a reward function that guides the generation process.
While the prompt-based LLM acts as a reference model that produces correction probabilities based on the prompt, the pure LLM evaluates the fluency of generated sequences.
Details on this perspective are provided in Appendix~\ref{sec:reinforcement_learning_perspective}.

\paragraph{Selection of LLM}
\label{sec:selection_of_llm}
To save GPU memory, we use the same model for both the prompt-based LLM and language model functions.
When GPU memory is sufficient, different models could be used for each function.

We use the base version of the LLM for both the prompt-based LLM and the language model, as our experiments show that it is more robust than instruction-tuned versions across different prompt templates and achieves better overall performance.

\paragraph{Prompt Template}
\label{sec:prompt_template}

\input{figures/minimal_prompt.tex}
We designed two prompt templates: a minimal prompt (\texttt{Minimal}) based on \citet{li-etal-2024-cllm} and a more sophisticated prompt (\texttt{Detailed}) based on \citet{li-etal-2023-ineffectiveness,zhou-etal-2024-simple}.
Figure~\ref{fig:minimal_prompt} shows the minimal prompt, which contains only essential task description and input sentence.
The detailed prompt, shown in Figure~\ref{fig:detail_prompt} in Appendix~\ref{sec:prompt_template}, additionally includes task-specific system prompts, format requirements, and notes.

Our experiments show that the base version of LLM performs better with the minimal prompt, while instruction-tuned LLM works better with the detailed prompt.
Based on overall performance, we adopt the minimal prompt in this work.

\subsection{Token-based Generation and Beam Search}
Large language models use multi-character tokenization for Chinese.
A token $\substring{t}_k$ may contain multiple characters.
For clarity, we define a partial output candidate at generation step $k$ as $\substring{y}_{\le j}=y_1\cdots{}y_j=\substring{t}_1\cdots\substring{t}_k$.

At each generation step $k$, we calculate the token score $\substring{t}_k$ as:
\begin{equation*}
    \begin{aligned}
         & s(\substring{x}, \substring{t}_1\dots\substring{t}_k, b) = s(\substring{x}, \substring{t}_1\dots\substring{t}_{k-1}, a)          \\
         & \qquad + \log p_{\mathtt{LLM}}(\substring{t}_k\mid \mathtt{Prompt}(\substring{x}) \circ \substring{t}_1\dots\substring{t}_{k-1}) \\
         & \qquad + \log p_{\mathtt{LLM}}(\substring{t}_k\mid \substring{t}_1\dots\substring{t}_{k-1})                                      \\
         & \qquad - \Delta_{\mathtt{Dist}_{\mathtt{L}}}(\substring{x}, a, b, \substring{t}_1\dots\substring{t}_{k-1}, \substring{t}_k)
    \end{aligned}
\end{equation*}
where $\Delta_{\mathtt{Dist}_{\mathtt{L}}}(\substring{x}, a, b, \substring{t}_1\dots\substring{t}_{k-1}, \substring{t}_k)$ is the incremental Levenshtein distance after generating multiple characters.

We use beam search to find the final output.
That is, at each generation step $k$, we expand the candidates with all possible tokens $\substring{t}_k$ and end indices $b$ and only keep the top $K$ candidates with the highest scores.
Following \texttt{TfPf}, the beam size is set to~8.

\subsection{The Full Model}
Following \texttt{TfPf}, we also adopt their length and faithfulness rewards in the score function:
\begin{equation}
    \begin{aligned}
         & s(\substring{x} ; \substring{t}_1\dots\substring{t}_k; b) = s(\substring{x}, \substring{t}_1\dots\substring{t}_{k-1}, a)                    \\
         & \qquad + \log p_{\mathtt{LLM}}(\substring{t}_k\mid \mathtt{Prompt}(\substring{x}) \circ \substring{t}_1\dots\substring{t}_{k-1})            \\
         & \qquad + \log p_{\mathtt{LLM}}(\substring{t}_k\mid \substring{t}_1\dots\substring{t}_{k-1})                                                 \\
         & \qquad + \lambda \left(\begin{array}{c}
                                          - \Delta_{\mathtt{Dist}_{\mathtt{L}}}(\substring{x}, a, b, \substring{t}_1\dots\substring{t}_{k-1}, \substring{t}_k) \\
                                          +                                                                                                                    \\
                                          \alpha\left( \mathtt{len}(\substring{t}_k)-1 \right)
                                      \end{array}\right)
    \end{aligned}
    \label{eq:final_score}
\end{equation}
where $\lambda = (1 + H_{\mathtt{LLM}}(\cdot))$ is the faithfulness reward that dynamically increases the distance impact when the language model is uncertain, and $\alpha(\mathtt{len}(\substring{t}_k)-1)$ is the length reward encouraging longer token selection.
