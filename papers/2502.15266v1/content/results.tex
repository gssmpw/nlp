\section{Main Results}
As shown in Table \ref{tab:main_results}, our method performs better than \texttt{ICL}, \texttt{ICL-RR}, and \texttt{TfPf} on both conventional CSC datasets and the C2EC dataset.
Compared to \texttt{TfPf}, from which our method is extended, we achieve improvements of 8.49 and 10.24 on average with 7B and 14B models, respectively.
Compared to \texttt{ICL-RR}, our method is shown to be a better way to combine the advantages of prompt-based LLMs and \texttt{TfPf}.
This is because \texttt{ICL-RR} can only choose from the top $K$ candidates from \texttt{ICL}.
If none of these candidates are good, reranking cannot improve the final result.

Compared to \texttt{SFT} models from \citet{li-etal-2024-cllm}, which are trained on the training set of the CSCD-NS dataset, our method shows better performance on the \textbf{out-of-domain} Lemon dataset\rlap{.}\footnote{Since the \texttt{SFT} models are trained with the \texttt{Qwen1.5} series, which may not be a fair comparison with our method, we also provide our results under the \texttt{Qwen1.5} series in Appendix~\ref{app:supervised_fine_tuning}.}
This indicates that \texttt{SFT} methods may not generalize well to new data they have not seen during training.

Compared to recent leading LLMs (e.g., the 671B parameter \texttt{DeepSeek\,V3}), our method enables much smaller 7B and 14B models to be on par with them without any training.

For a better understanding of the performance of our method, we also provide several qualitative results in Appendix~\ref{subsec:qualitative_analysis}.
\input{tables/discussion_lm}

An interesting phenomenon worth noting is that the reasoning model \texttt{DeepSeek\,R1} shows lower performance than \texttt{DeepSeek\,V3}, its non-reasoning variant, on both CSC and C2EC tasks.
We find this may be caused by incorrect reasoning.
More discussion of this is provided in Appendix~\ref{subsec:incorrect_thinking_may_lead_to_wrong_corrections}.
