\section{Experimental Setup}
\subsection{Datasets}
\label{sec:datasets}
We evaluate our approach on both conventional CSC datasets and our newly constructed dataset, \textbf{C2EC}.
For conventional CSC, following \citet{li-etal-2024-cllm}, we use two representative datasets: \textbf{CSCD-NS} \cite{hu-etal-2024-cscd} and \textbf{Lemon} \cite{wu-etal-2023-rethinking}.
Both datasets contain text written by native speakers.
CSCD-NS focuses on general domain performance, while Lemon evaluates zero-shot cross-domain capabilities.
The statistics of the datasets are shown in Table~\ref{tab:dataset_statistics}.
As we are unable to access the ECMR-2023 dataset \cite{he-etal-2023-umrspell}, we do not include it in our experiments.

\subsection{Evaluation Metrics}
Following previous works \cite{li-etal-2024-cllm,zhou-etal-2024-simple}, we use character-level correction $F_1$ as our main evaluation metric.
Since sentence-level metrics are widely used in previous works, we also report them in Appendix~\ref{subsec:sentence_level_results}.
Details of the metrics can be found in Appendix~\ref{subsec:evaluation_implementation_and_settings}.

\input{tables/spelling_check_results}
\subsection{Baselines}
We compare our approach against three training-free baselines:
\begin{inparaenum}[\itshape a)]
    \item \textbf{In-context Learning} (\texttt{ICL}): This method prompts LLMs with 10 exemplars (5 erroneous, 5 correct sentences) randomly selected and shuffled for each input. During inference, we use beam search with the same beam size as our approach;\footnote{For conventional CSC datasets, exemplars are randomly sampled from the CSCD-NS training set, while for C2EC, they come from its development set. Due to the random nature of \texttt{ICL}, we report the results averaged across 3 runs.}
    \item \textbf{Training-free Prompt-free Method} (\texttt{TfPf}, \citet{zhou-etal-2024-simple}); and
    \item \textbf{ICL with Reranking} (\texttt{ICL-RR}): This hybrid method first generates $K$ candidates using \texttt{ICL}, then reranks them using an extended \texttt{TfPf} model that supports insertion and deletion operations.
\end{inparaenum}

For reference, we also include \texttt{ICL} results from leading LLMs through API calls: \texttt{GPT4o-mini}\rlap{,}\footnote{Version: \texttt{gpt-4o-mini-2024-07-18}} \texttt{GPT4o} \cite{hurst-etal-2024-gpt4o}\rlap{,}\footnote{Version: \texttt{gpt-4o-2024-11-20}} and the 671B parameter \texttt{DeepSeek\,V3} and \texttt{R1} \cite{deepseekai-2024-deepseek-v3,deepseek-r1-2025}\rlap{.}\footnote{Due to API constraints and high cost, we use greedy decoding (\texttt{temperature=0.0}) instead of beam search, and report single-run results.}

Additionally, the supervised fine-tuning (\texttt{SFT}) results on conventional CSC datasets from \citet{li-etal-2024-cllm} are also included for better understanding.

\paragraph{Training Details of SFT Baselines}
The \texttt{SFT} models were trained on a combined dataset consisting of 271k pseudo sentence pairs \cite{wang-etal-2018-hybrid} and the \textbf{CSCD-NS} training data.
This means that while CSCD-NS is an in-domain evaluation, the Lemon dataset serves as a cross-domain dataset for evaluating the generalization capabilities.

\subsection{Model Selection}
We use the \texttt{Qwen2.5} model series \cite{yang-etal-2024-qwen25} for our experiments, as it is one of the most recent open-source LLMs with strong Chinese language capabilities.

For \texttt{ICL} experiments, we use the ``\texttt{Instruct}'' version, which is optimized for instruction following.
As for our approach, we use the ``\texttt{Base}'' version.
To reduce the computational cost, we use \texttt{Qwen2.5\,7B} for detailed analysis.

\subsection{Hyperparameters}
We largely follow the hyperparameter settings in \citet{zhou-etal-2024-simple}, using a beam size of 8 and $\alpha$ for length reward of 2.5.
Through development set tuning, we set the weights for insertion and deletion as 8.5 and 9.0, respectively, and the temperature of the prompt-based scoring as 1.5.
