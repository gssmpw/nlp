\section{Related Works}
\label{sec:related_work}

\subsection{Datasets of Chinese Spelling Correction}
Chinese Spelling Correction (CSC) has long been an important research area in NLP, with new datasets continuously being developed to address various needs.
The Sighan series \cite{wu-etal-2013-chinese,yu-etal-2014-overview,tseng-etal-2015-introduction} is one of the earliest and most influential collections of CSC datasets.
While widely used, they are criticized for their poor annotation quality, limited domain, and unrealistic error patterns \cite{yang-etal-2023-chinese,wu-etal-2023-rethinking,sun-etal-2024-two}.

While researchers like \cite{yang-etal-2023-chinese,sun-etal-2024-two} have tried to improve Sighan datasets through re-annotation, the inherent issues of unrealistic domain and limited error patterns remain.
To address these issues, researchers manually created errors across financial, official documents and medical domains \cite{lv-etal-2023-ecspell,jiang-etal-2022-mcscset}.
Instead of creating errors from correct sentences, \citet{wu-etal-2023-rethinking} collected and annotated real errors from seven different domains, building a large dataset that challenges models by not providing training data. Similarly, \citet{hu-etal-2024-cscd} created a new dataset with real errors found on social media.

Some works have tried to broaden the scope of conventional CSC datasets.
Similar to our work, \citet{he-etal-2023-umrspell} also pointed out that existing datasets mainly focus on substitution errors, overlooking two other common types: insertion and deletion errors, limiting the practicality of the task.
To address this, they created the ECMR-2023 dataset by randomly adding, removing, or replacing characters in correct sentences.
However, these artificially generated errors may not reflect real-world mistakes well.
In contrast, we build our C2EC dataset using existing Chinese text correction data, providing a more realistic benchmark for general Chinese character error correction.

\subsection{Approaches of Chinese Spelling Correction}
For many years, BERT-based models have dominated CSC research \cite{zhang-etal-2020-spelling,xu-etal-2021-read,zhu-etal-2022-mdcspell,liang-etal-2023-disentangled}.
While these models show strong performance on in-domain datasets, recent studies reveal their limitations when applied to different domains \cite{wu-etal-2023-rethinking,liu-etal-2024-rephrasing}.

With the advent of LLMs, researchers have begun exploring their potential for CSC.

Early attempts focused on prompt-based methods \cite{li-etal-2023-ineffectiveness}.
For instance, \citet{dong-etal-2024-rich} enhanced prompts with pronunciation and glyph information.
However, lightweight LLMs, such as those with 7B or 14B parameters, still struggle to achieve satisfactory performance with prompt-based methods.
Compared to prompt-based methods, supervised fine-tuning methods have shown more effectiveness.
A representative work is \citet{li-etal-2024-cllm}, which introduced a novel training paradigm that retrains LLMs at the character level to better align with CSC requirements.

Recently, \citet{zhou-etal-2024-simple} proposed \texttt{TfPf}, a training-free and prompt-free framework that achieves comparable cross-domain performance to \texttt{SFT} methods.
As detailed in \S\ref{sec:baseline_tfpf_approach}, \texttt{TfPf} combines a language model with a distance metric to balance fluency and minimal edits.
Our work builds upon this elegant framework by improving its performance and extending it to handle the C2EC task.
