\appendix

\section{Details of C2EC Dataset}
\subsection{Annotation UI for Data Verification}
\label{sec:anno_ui}
\input{figures/anno_ui.tex}
Figure~\ref{fig:anno_ui} shows our verification user interface.
To maintain verification quality, we always present the guidelines to the annotators at the top of the UI.
Additionally, we highlight the differences between the original and corrected sentences to help the annotators easily identify the changes.
We also require the annotators to carefully remove any sentences containing sensitive or offensive content.
However, we did not find any such sentences during verification.

\subsection{Discarded Cases}
\label{sec:discard_cases}
\input{tables/discard_cases.tex}
We present examples of sentences discarded during manual verification in Table~\ref{tab:discard_cases}.
The first example involves a sentence with incorrect annotation.
The input sentence incorrectly spells the word ``阻止'' (prevent, \textit{zǔ zhǐ}) as ``组织'' (organize, \textit{zǔ zhǐ}), but the annotator from the original dataset only corrected the character ``组'', leaving ``止'' unchanged.
The second example is a sentence with complex grammatical errors.
In Chinese textbooks, there is a recommended rule for arranging multiple adjectives in a sentence for a formal style.
For instance, adjectives indicating time or location should precede those of quantity, and quantity adjectives should precede attributive adjectives.
Thus, a grammatically correct phrase in this context is ``一轮巨大的秋月''.
However, correcting such sentences is beyond the scope of this work.
The third example is a sentence with multiple reasonable corrections.
The incorrect character ``人'' (person, \textit{rén}) can be corrected by either removing it or changing it to ``得'' (an auxiliary verb, \textit{dé}).
The fourth example is an ambiguous sentence that requires additional context for accurate correction.
Given the context, we cannot determine whether the term ``混沌妖族'' (\textit{Chaos Demon Clan}, \textit{hùn dùn yāo zú}) is valid within the novel, or if it should be corrected to another term, leading us to discard this sentence.

We plan to re-annotate discarded sentences in the future and share them with the original authors to improve the dataset.

\section{Distance Metrics}
\label{sec:distance_metric}

\subsection{Hamming Distance}
\label{subsec:hamming_distance}
Hamming distance measures the position-wise differences between two strings of equal length:
\begin{equation}
    \mathtt{Dist}(\substring{x}, \substring{y}) = \sum_{i=1}^{n} w(x_i, y_i)
\end{equation}
where $\substring{x}$ and $\substring{y}$ are strings of the same length $n$, and $w(x_i, y_i)$ is the weight assigned to the characters $x_i$ and $y_i$.
In the standard scenario, $w(x_i, y_i) = 1$ if $x_i \neq y_i$ and $0$ otherwise.
For instance, the Hamming distance between \texttt{a\correct{b}c\correct{de}f} and \texttt{a\correct{1}c\correct{23}f} is~3.

\paragraph{Distortion Model of \texttt{TfPf} as a Weighted Hamming Distance}
\label{subsec:distortion_model_of_tfpf_as_a_weighted_hamming_distance}
The distortion model $\log p_{\mathtt{DM}}(\boldsymbol{x}\mid \boldsymbol{y})$ in \texttt{TfPf} is character-level factorized:
\begin{equation}
    \log p_{\mathtt{DM}}(\boldsymbol{x}\mid \boldsymbol{y}) = \sum_{i=1}^{n} \log p_{\mathtt{DM}}(x_i\mid y_i)
\end{equation}
By interpreting $- \log p_{\mathtt{DM}}(x_i\mid y_i)$ as the weight for characters $x_i$ and $y_i$, the distortion model of \texttt{TfPf} can be seen as a weighted Hamming distance.

\subsection{Levenshtein Distance}
Levenshtein distance measures the difference between two strings, which may have different lengths, by calculating the minimum number of edit operations (\texttt{substitution}, \texttt{insertion}, and \texttt{deletion}) required.
For example, the Levenshtein distance between \texttt{abc\correct{d}e} and \texttt{a\correct{1}bc\correct{2}e} is 2, including an insertion of `1' between `a' and `b', and a substitution of `d' with `2'.

The Levenshtein distance can be computed using a dynamic programming algorithm with the following recurrence relation:

\begin{equation*}
    \begin{aligned}
         & \mathtt{Dist}(\substring{x}_{\le m}, \substring{y}_{\le n})                                                \\
         & \ = \min \begin{cases}
                        \mathtt{Dist}(\substring{x}_{\le m - 1}, \substring{y}_{\le n})\!+\!{}w_{\mathtt{I}} \\
                        \mathtt{Dist}(\substring{x}_{\le m}, \substring{y}_{\le n - 1})\!+\!{}w_{\mathtt{D}} \\
                        \mathtt{Dist}(\substring{x}_{\le m - 1}, \substring{y}_{\le n - 1})\!+\!{}w_{\mathtt{S}}(x_m, y_n)
                    \end{cases} %
    \end{aligned}%
\end{equation*}%
where $w_{\mathtt{I}}$, $w_{\mathtt{D}}$, and $w_{\mathtt{S}}(x_m, y_n)$ are the weights for insertion, deletion, and substitution, respectively.
For simplicity, the keep operation is treated as a special case of substitution.

\section{A Reinforcement Learning Perspective}
\label{sec:reinforcement_learning_perspective}
In this section, we provide a reinforcement learning perspective of our method.
We believe this perspective can help us understand the role of the two large language models in our method, and the relationship between our method and \texttt{TfPf}.

\subsection{KL-Regulated Reinforcement Learning}
Reinforcement learning (RL) has been widely used to improve the performance of LLMs by optimizing the following objective:
\begin{equation}
    \begin{aligned}
        \mathcal{L}(p_{\theta}) =\  & \mathbb{E}_{\boldsymbol{y}\sim p_{\theta}(\cdot\mid \boldsymbol{x})} \left[ r(\boldsymbol{x}, \boldsymbol{y}) \right]                   \\
                                    & - \beta D_{KL}\left(p_{\theta}\left(\cdot\mid \boldsymbol{x}\right) \Vert\ p_{\mathtt{ref}}\left(\cdot\mid \boldsymbol{x}\right)\right)
    \end{aligned}
\end{equation}
where $p_{\theta}$ is the model we want to optimize, $r$ is the reward function, $p_{\mathtt{ref}}$ is the reference model whose parameters are frozen, and $\beta$ is the KL-regularization coefficient that controls how much we want $p_{\theta}$ to be different from the reference model.
A larger $\beta$ means we want $p_{\theta}$ to be more similar to the reference model.

The optimal model $p^*$ of $p_{\theta}$ is unique and is given by:
\begin{equation}
    \begin{aligned}
        p^*(\boldsymbol{y}\mid \boldsymbol{x})\ \propto\ p_{\mathtt{ref}}(\boldsymbol{y}\mid \boldsymbol{x}) \exp\left(\frac{1}{\beta} r(\boldsymbol{x}, \boldsymbol{y})\right)%
    \end{aligned}%
    \label{eq:optimal_policy}
\end{equation}
This equation shows that we do not need to train the model to optimize the objective; instead, we can directly obtain the optimal policy by combining the reference model and the reward function during inference.

\subsection{\texttt{TfPf} as a Reward Model}
\label{subsec:tfpf_as_a_reward_model}
Intuitively, given an input $\substring{x}$ and two outputs $\substring{y}_{a}$ and $\substring{y}_{b}$, which output is better can be determined by the following two criteria:
\begin{asparaitem}[$\bullet$]
    \item \textbf{Fluency}: A better output $\substring{y}$ should be more fluent.
    \item \textbf{Faithfulness}: A better output $\substring{y}$ should only make the necessary changes to the input sentence $\substring{x}$. Between two sentences with the same fluency, the one with fewer changes is better.
\end{asparaitem}

Recall the score function of \texttt{TfPf} in Equation~\ref{eq:tfpf_score}.
The score function of \texttt{TfPf} is a combination of a large language model and a distance metric.
The large language model, acting as a pure language model, can be seen as a reward function measuring the fluency of the output.
On the other hand, the distance metric, acting as a reward function measuring the faithfulness of the output, penalizes outputs that change the input sentence too much.

Combining \texttt{TfPf} as a reward model with the RL framework, Equation~\ref{eq:optimal_policy} can be rewritten as:
\begin{equation}
    \begin{aligned}
        \log p^*(\substring{y}\mid \substring{x})\ \propto\  & \log p_{\mathtt{ref}}(\substring{y}\mid \substring{x})                                 \\
                                                             & + \frac{1}{\beta} \left(\begin{array}{c}\log p_{\mathtt{LLM}}(\substring{y}) \\
                                                                                               -                                       \\
                                                                                               \mathtt{Dist}(\substring{x}, \substring{y})\end{array}\right)
    \end{aligned}
\end{equation}

\paragraph{Link to the Original \texttt{TfPf} Paper}
If we set $\beta = 1$ and use a uniform distribution as the reference model $\log p_{\mathtt{ref}}(\substring{y}\mid \substring{x}) = C$, where $C$ is a constant, the above equation is equivalent to the score function of \texttt{TfPf} in Equation~\ref{eq:tfpf_score}.

\paragraph{Link to Our Method}
If we set $\beta = 1$ and use a prompt-based large language model as the reference model $\log p_{\mathtt{ref}}(\substring{y}\mid \substring{x}) = \log p_{\mathtt{LLM}}(\substring{y}\mid \mathtt{Prompt}(\substring{x}))$, the above equation becomes equivalent to our score function in Equation~\ref{eq:gcsc_score}.
By using a prompt-based large language model as the reference model instead of a uniform distribution, we obtain a more reasonable prior distribution, which enables our method to find better outputs $\substring{y}$ with a limited beam size.

\input{figures/detail_prompt.tex}
\subsection{Proof of Equation \ref{eq:optimal_policy}}
\label{subsec:proof_of_optimal_policy}
First, we rewrite the objective function of RL as follows:
\begin{equation}
    \begin{aligned}
        \mathcal{L}(p_{\theta}) & = \mathbb{E}_{\boldsymbol{y}\sim p_{\theta}(\cdot\mid \boldsymbol{x})} \left[ r(\boldsymbol{x}, \boldsymbol{y}) \right]                                                                                                                                                            \\
                                & \quad - \beta D_{KL}\left(p_{\theta}\left(\cdot\mid \boldsymbol{x}\right) \Vert\ p_{\mathtt{ref}}\left(\cdot\mid \boldsymbol{x}\right)\right)                                                                                                                                      \\
                                & = \sum_{\boldsymbol{y}} p_{\theta}(\boldsymbol{y}\mid \boldsymbol{x}) \left(\begin{array}{c} r(\boldsymbol{x}, \boldsymbol{y}) \\
                                                                                                                      -                                     \\
                                                                                                                      \beta \log \frac{p_{\theta}(\boldsymbol{y}\mid \boldsymbol{x})}{p_{\mathtt{ref}}(\boldsymbol{y}\mid \boldsymbol{x})}\end{array}\right)
    \end{aligned}
\end{equation}

Then we can compute the gradient of the objective function with respect to the policy $p_{\theta}(\boldsymbol{y}\mid \boldsymbol{x})$:
\begin{equation}
    \frac{\partial \mathcal{L}(p_{\theta})}{\partial p_{\theta}(\boldsymbol{y}\mid \boldsymbol{x})} = r(\boldsymbol{x}, \boldsymbol{y}) - \beta \left(\log \frac{p_{\theta}(\boldsymbol{y}\mid \boldsymbol{x})}{p_{\mathtt{ref}}(\boldsymbol{y}\mid \boldsymbol{x})} + 1\right)
    \label{eq:gradient_of_objective_function}
\end{equation}

Now, we can find the optimal policy $p^*$ by setting the gradient of the objective function to zero:
\begin{equation}
    r(\boldsymbol{x}, \boldsymbol{y}) - \beta \left(\log \frac{p_{\theta}(\boldsymbol{y}\mid \boldsymbol{x})}{p_{\mathtt{ref}}(\boldsymbol{y}\mid \boldsymbol{x})} + 1\right) = 0
\end{equation}



By rearranging the equation and taking the exponential of both sides, we get:
\begin{equation}
    \begin{aligned}
        p^*(\boldsymbol{y}\mid \boldsymbol{x}) =\  & p_{\mathtt{ref}}(\boldsymbol{y}\mid \boldsymbol{x}) \exp\left(\frac{r(\boldsymbol{x}, \boldsymbol{y})}{\beta} - 1\right) \\
        \propto\                                   & p_{\mathtt{ref}}(\boldsymbol{y}\mid \boldsymbol{x}) \exp\left(\frac{r(\boldsymbol{x}, \boldsymbol{y})}{\beta}\right)
    \end{aligned}
\end{equation}
where the $-1$ term inside the exponential function can be safely ignored because it is a constant.

\input{tables/data_statistics.tex}

\section{Detailed Experiment Settings}
\label{sec:detailed_experiment_settings}
\subsection{Prompt Templates}
\label{sec:prompt_templates}
\paragraph{The Detailed Prompt for Our Method}
Figure~\ref{fig:detail_prompt} shows the detailed prompt for our method, which includes task-specific system prompts, format requirements, and additional notes.

\paragraph{Prompts for In-Context Learning Baselines}
The prompts for the \texttt{ICL} baseline are shown in Figure~\ref{fig:icl_detail_prompt}.
To ensure the performance of the \texttt{ICL} baseline, we use different prompts for the \texttt{ICL} baseline on conventional CSC and C2EC.
Specifically, when evaluating conventional CSC, we do not instruct the model to correct missing and redundant characters.

\subsection{An Approximate Implementation of Our Method}
\label{subsec:approximate_implementation_of_our_method}
To reduce computational cost and speed up the generation process, we make two approximations.
First, we approximate the incremental Levenshtein distance $\Delta_{\mathtt{Dist}_{\mathtt{L}}}(\substring{x}, a, b, \substring{t}_1\dots\substring{t}_{k-1}, \substring{t}_k)$ as $\mathtt{Dist}_{\mathtt{L}}(\substring{x}_{[a:b+1]}, \substring{t}_k)$.
Then, we reduce the search space by maintaining only one best end index $b$ for each $\substring{t}_1\dots\substring{t}_k$.
While these approximations may yield suboptimal results, they work well in practice.

\subsection{Dataset Statistics}
\label{subsec:dataset_statistics}
In this work, we use three datasets to evaluate the performance of our method.
All datasets used in this work are publicly available.
Specifically, the CSCD-NS dataset is publicly available under the MIT license, while the CCTC dataset is publicly available under the Apache 2.0 license.
The specifics of these datasets are listed in Table~\ref{tab:dataset_statistics}.
The \textbf{Evaluation Sentences} row in Table~\ref{tab:dataset_statistics} shows the number of sentences actually used for evaluation.
This is because sentences where the original and corrected versions differ in length are excluded when evaluating CSC models, as done in previous works \cite{liu-etal-2023-chinese,li-etal-2024-cllm,zhou-etal-2024-simple}.

\subsection{Evaluation Implementation and Settings}
\label{subsec:evaluation_implementation_and_settings}
We use the evaluation script from \citet{zhou-etal-2024-simple} to calculate the metrics.
This script adopts a Levenshtein distance algorithm to extract edit operations, allowing for comparisons between sentences of different lengths.
We also follow their settings by ignoring whitespaces and converting all full-width punctuation to half-width.

\subsection{Hardware Setup}
Experiments were conducted on a single NVIDIA A100 40GB GPU with the Intel Xeon Gold 6248R (3.00GHz) CPU.

\section{Detailed Results}
\label{sec:detailed_results}
\subsection{Sentence-level Results}
\label{subsec:sentence_level_results}
\input{tables/spelling_check_results.sf.tex}
The sentence-level $F_1$ results are shown in Table~\ref{tab:spelling_check_results.sf}.

The results show that our method outperforms three baselines on both conventional CSC and C2EC datasets.

Additionally, our method enables a 14B model to achieve performance comparable to the leading LLM, which has 671B parameters.

\subsection{Qualitative Analysis}
\label{subsec:qualitative_analysis}
\input{tables/cases.tex}
Table~\ref{tab:qualitative_examples} shows four examples illustrating the effectiveness of our method.

In the first example, characters ``曲安'' (\textit{qū ān}) were incorrectly typed as ``圈'' (\textit{quān}).
This simple error highlights the limitation of the Hamming distance in \texttt{TfPf}, which led to an incorrect correction to ``甲'' (\textit{jiǎ}) instead of the correct insertion of a character.

The second example is the character ``调'' (\textit{diào}) in ``调方'' (\textit{prescription adjustment}), which was mistakenly entered as ``凋'' (\textit{diāo}).
Correcting this error requires subsequent contextual understanding, which pure LLM probabilities in \texttt{TfPf} failed to achieve, resulting in an incorrect high-frequency substitution to ``调换'' (\textit{exchange, diào huàn}).
This issue was mitigated by incorporating prompt-based LLM probabilities.

In the third example, the character ``乙'' (\textit{A, yǐ}) was mistyped as ``以'' (\textit{yǐ}).
The \texttt{ICL} baseline overlooked phonetic similarities, incorrectly changing it to ``甲'' (\textit{A, jiǎ}).
The \texttt{ICL-RR} baseline also failed to correct this error as the correction ``以''$\rightarrow$``乙'' was not among the top-K candidates.

The fourth example is a negative case where our method unnecessarily inserted an ``以'' into an already correct sentence.
While this change might make the sentence slightly smoother, it is an over-correction since the original sentence was error-free.

\subsection{Incorrect Thinking of Reasoning Model May Lead to Wrong Corrections}
\label{subsec:incorrect_thinking_may_lead_to_wrong_corrections}
\input{figures/overthinking_examples.tex}
The reasoning model \texttt{Deepseek\,R1} \cite{deepseek-r1-2025} shows very impressive performance on tasks like math, code, and science.
However, as discussed in the main results, \texttt{Deepseek\,R1} achieves lower performance on CSC tasks than its non-reasoning variant \texttt{Deepseek\,V3}.
Although \texttt{Deepseek\,R1} achieves higher recall than \texttt{Deepseek\,V3}, it introduces too many over-corrections.

After analyzing the reasoning process of \texttt{Deepseek\,R1}, we find that incorrect thinking may lead to wrong corrections, resulting in lower performance than \texttt{Deepseek\,V3}.

\Cref{fig:thinking_example_1,fig:thinking_example_2,fig:thinking_example_3,fig:thinking_example_4} show thinking examples from \texttt{Deepseek\,R1}.

The first two examples are correct thinking examples, showing that \texttt{Deepseek\,R1} can make reasonable thinking and correctly fix errors based on it.
Minor flaws in thinking may not affect the correctness of the correction.
For example, in the first example, \texttt{Deepseek\,R1} considers ``埋'' (\textit{mái}) and ``理'' (\textit{lǐ}) to be similar-sounding, but in fact they are look-alikes rather than sound-alikes.
However, this does not affect the correction result.

In the third example, ``刀匾'' is an uncommon but correct word.
However, \texttt{Deepseek\,R1} over-thinks about it and incorrectly changes it to ``招牌'', a more common word with a similar but slightly different meaning.

In the fourth example, \texttt{Deepseek\,R1} analyzes all words in the input sentence but fails to consider whether the ``是'' in the input sentence is correct or not.
This oversight leads to an under-correction.

Nevertheless, we believe that the potential of the reasoning model \texttt{Deepseek\,R1} is still huge.
We plan to further investigate the use of reasoning models in CSC tasks in the future.

\section{More Discussions}
\label{app:more_discussion}

\subsection{Impact of Different LLM Families}
\label{app:other_llm}
\input{tables/discussion_diff_lm}
To investigate if our method can improve the performance of other LLMs, we conducted experiments on three additional LLMs: \texttt{Qwen1.5} (Q1.5, \citet{bai-etal-2023-qwen}), \texttt{Baichuan2} (BC2, \citet{yang-etal-2023-baichuan}), and \texttt{InternLM2} (IL2, \citet{cai-etal-2024-internlm2}).
For these LLMs, we used the 7B version.

The results are shown in Table~\ref{tab:analysis:other_lm}.
Our method consistently improves the performance of these LLMs.
However, the performance gain with \texttt{TfPf} varies across different LLMs.
We observed that the performance improvement might be related to the recall value of \texttt{ICL} for different LLMs.
A higher recall value of \texttt{ICL} corresponds to a larger performance improvement with our method.
We plan to explore the underlying mechanism of this phenomenon in the future.

\subsection{Impact of Beam Size}
\label{app:beam_size}
\input{figures/beam_results}
Figure~\ref{fig:beam_size} shows the performance of our method with varying beam sizes on the CSCD-NS and C2EC datasets.
The results indicate that our method performs well even with a small beam size.
In particular, a beam size of 2 is sufficient to surpass \texttt{ICL}, \texttt{ICL-RR}, and \texttt{TfPf} on both datasets.
Interestingly, as the beam size increases, the performance of \texttt{ICL} remains almost unchanged, while the performance of \texttt{ICL-RR} steadily improves.
This suggests that while \texttt{ICL} can find a set of good candidates, it struggles to rank them properly.

\subsection{Impact of Two Rewards}
\label{app:ablation:two_reward}
\input{tables/discussion_two_reward.tex}
\input{tables/spelling_check_results.sft.tex}
In Equation \ref{eq:final_score}, we adopt both the faithfulness reward and the length reward from \texttt{TfPf} into the final scoring function.
This section investigates the effectiveness of these two rewards.
The ablation results are shown in Table~\ref{tab:ablation:two_reward}.

When the faithfulness reward is removed, our method shows an increase in recall, while the precision is reduced.
Conversely, removing the length reward results in better precision, but this comes with a decline in recall.

The faithfulness reward mainly improves precision, while the length reward mainly improves recall.
Together, they complement each other, leading to better overall performance.

\subsection{A Fair Comparison with SFT Methods}
\label{app:supervised_fine_tuning}
In main results, we compare our approach using the \texttt{Qwen2.5} series with the state-of-the-art methods that have been fine-tuned with supervision, as reported by \cite{li-etal-2024-cllm}.
It's important to note a potential discrepancy: their supervised fine-tuned methods were trained from the \texttt{Qwen1.5} series, whereas our method utilizes the \texttt{Qwen2.5} series.
To ensure a fair comparison, we also provide the results of our method on the \texttt{Qwen1.5} series in Table~\ref{tab:main_results:sft}.

\input{tables/discussion_speed.tex}
When compared to the \texttt{TfPf} method, the \texttt{SFT} methods show superior performance on the in-domain dataset CSCD-NS.
However, they perform less effectively on the out-of-domain dataset Lemon, particularly with a 7B model.
This suggests that the \texttt{SFT} methods might overfit to the in-domain dataset CSCD-NS, limiting their generalization to the out-of-domain dataset Lemon.

Our method, which requires no training, significantly outperforms the \texttt{SFT} methods on the out-of-domain dataset Lemon.
Additionally, without any training, our method achieves performance on par with the \texttt{SFT} methods on the in-domain dataset CSCD-NS, with scores of 71.53 versus 73.80 on the 14B model.

\subsection{Run-time Analysis}
\label{app:runtime}
We randomly sampled 50 sentences of varying lengths from the development set of CSCD-NS and C2EC to evaluate the running time of our method on \texttt{Qwen2.5\,7B} compared to the baselines.
The experiment was conducted on a single NVIDIA A100 40GB GPU with the Intel Xeon Gold 6248R (3.00GHz) CPU.
The batch size was set to 1 during evaluation.
The results, as shown in Table~\ref{tab:analysis:runtime}, indicate that our approach is approximately twice slower than \texttt{TfPf}.
This increased time is due to the two forward passes of the large language model at each step to obtain the final score.
However, since these two forward passes are independent, the process can be accelerated by parallelizing them if more GPUs are available.

\input{figures/icl_detail_prompt.tex}
