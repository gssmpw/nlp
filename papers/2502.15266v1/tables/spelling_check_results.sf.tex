\begin{table*}[p!]
    \centering
    \renewcommand{\arraystretch}{0.95}
    \setlength{\tabcolsep}{3.8pt}
    \scalebox{0.9}{
        \begin{NiceTabular}{lccccccccc;c;c|c}
            \toprule
            \rowcolor[gray]{.9}
                                                                                                                                             &                               &                                 & \Block[c]{1-9}{\textbf{\textsc{Conventional CSC}}} &                          &                          &                          &                          &                          &                          &                                  &                               & \textbf{\textsc{C2EC}} \\
            \Block[l]{2-1}{\textbf{Model}}                                                                                                   & \Block[c]{2-1}{\textbf{Size}} & \Block[c]{2-1}{\textbf{Method}} & \Block[c]{1-7}{\textbf{Lemon}}                     &                          &                          &                          &                          &                          &                          & \Block[c]{1-1}{\textbf{CSCD-NS}} & \Block[c]{2-1}{\textbf{Avg.}} & \textbf{C2EC}          \\
                                                                                                                                             &                               &                                 & \textit{Car}                                       & \textit{Cot}             & \textit{Enc}             & \textit{Gam}             & \textit{Mec}             & \textit{New}             & \textit{Nov}             & \textit{test}                    &                               & \textit{test}          \\
            \midrule
            \Block[c]{1-13}{\textit{Supervised Fine-tuning SoTAs} \cite{wu-etal-2023-rethinking,liu-etal-2024-rephrasing,hu-etal-2024-cscd}} &                               &                                 &                                                                                                                                                                                                                                                                                                                  \\
            \midrule
            \texttt{BERT}                                                                                                                    & \texttt{0.1B}                 & \texttt{SFT}                    & 52.3\rlap{$^\dagger$}\wz                           & 64.1\rlap{$^\dagger$}\wz & 45.5\rlap{$^\dagger$}\wz & 33.3\rlap{$^\dagger$}\wz & 50.9\rlap{$^\dagger$}\wz & 56.0\rlap{$^\dagger$}\wz & 36.0\rlap{$^\dagger$}\wz & 72.96\rlap{$^\ddagger$}          & --                            & --                     \\
            \texttt{SM-BERT}                                                                                                                 & \texttt{0.1B}                 & \texttt{SFT}                    & 52.0\rlap{$^\dagger$}\wz                           & 65.0\rlap{$^\dagger$}\wz & 44.6\rlap{$^\dagger$}\wz & 29.8\rlap{$^\dagger$}\wz & 49.3\rlap{$^\dagger$}\wz & 55.8\rlap{$^\dagger$}\wz & 37.8\rlap{$^\dagger$}\wz & \textbf{73.62}\rlap{$^\ddagger$} & --                            & --                     \\
            \texttt{ReLM}                                                                                                                    & \texttt{0.1B}                 & \texttt{SFT}                    & 53.1\rlap{$^\dagger$}\wz                           & 66.8\rlap{$^\dagger$}\wz & 49.2\rlap{$^\dagger$}\wz & 33.0\rlap{$^\dagger$}\wz & 54.0\rlap{$^\dagger$}\wz & 58.5\rlap{$^\dagger$}\wz & 37.8\rlap{$^\dagger$}\wz & --                               & --                            & --                     \\
            \Block[l]{2-1}{\texttt{Baichuan2}}                                                                                               & \texttt{7B}                   & \texttt{SFT}                    & --                                                 & --                       & --                       & --                       & --                       & --                       & --                       & 64.44\rlap{$^\ddagger$}          & --                            & --                     \\
                                                                                                                                             & \texttt{13B}                  & \texttt{SFT}                    & --                                                 & --                       & --                       & --                       & --                       & --                       & --                       & 66.10\rlap{$^\ddagger$}          & --                            & --                     \\
            \midrule
            \Block[c]{1-13}{\textit{BERT-based SFT Models cooperating with LLMs} \cite{liu-etal-2024-arm}}                                   &                               &                                 &                                                    &                          &                          &                          &                          &                          &                          &                                  &                               &                        \\
            \midrule
            \texttt{MDCSPell}$^\natural$                                                                                                     & \texttt{N/A}                  & \texttt{ARM}                    & 37.1\wz                                            & 52.7\wz                  & 35.2\wz                  & 15.3\wz                  & 33.0\wz                  & 36.4\wz                  & 15.6\wz                  & --                               &                               &                        \\
            \midrule
            \Block[c]{1-13}{\textit{Training-free Methods of LLMs}}                                                                          &                               &                                 &                                                    &                          &                          &                          &                          &                          &                          &                                  &                               &                        \\
            \midrule
            \texttt{GPT4o-mini}                                                                                                              & \texttt{N/A}                  & \texttt{ICL}                    & 29.43                                              & 30.85                    & 42.11                    & 26.67                    & 33.72                    & 33.12                    & 23.52                    & 38.78                            & 32.27                         & 29.09                  \\
            \texttt{GPT4o}                                                                                                                   & \texttt{N/A}                  & \texttt{ICL}                    & 52.91                                              & 59.98                    & 62.15                    & 39.11                    & 61.87                    & 61.82                    & 49.96                    & 63.89                            & 56.46                         & 47.66                  \\
            \texttt{DeepSeek\,V3}                                                                                                            & \texttt{671B}                 & \texttt{ICL}                    & 57.33                                              & \textbf{69.87}           & \textbf{66.07}           & \textbf{53.12}           & \textbf{69.33}           & 69.14                    & \textbf{57.43}           & 67.55                            & \textbf{63.73}                & 53.25                  \\
            \texttt{DeepSeek\,R1}                                                                                                            & \texttt{671B}                 & \texttt{ICL}                    & 54.52                                              & 63.43                    & 59.93                    & 49.27                    & 68.59                    & 67.82                    & 52.93                    & 62.39                            & 59.84                         & 45.58                  \\
            \hdashedline
            \Block[l]{4-1}{\texttt{Qwen2.5}}                                                                                                 & \Block[c]{4-1}{\texttt{7B}}   & \texttt{ICL}                    & 26.41                                              & 42.95                    & 37.97                    & 18.93                    & 38.27                    & 29.10                    & 21.85                    & 34.10                            & 31.20                         & 28.53                  \\
                                                                                                                                             &                               & \texttt{ICL-RR}                 & 38.93                                              & 56.46                    & 49.05                    & 32.40                    & 50.31                    & 44.54                    & 32.77                    & 54.40                            & 44.86                         & 40.24                  \\
                                                                                                                                             &                               & \texttt{TfPf}                   & 49.37                                              & 61.60                    & 48.48                    & 39.07                    & 55.69                    & 59.79                    & 39.09                    & 58.57                            & 51.46                         & 38.60                  \\
                                                                                                                                             &                               & \texttt{OUR}                    & 57.58                                              & 69.45                    & 61.56                    & 47.78                    & 66.59                    & 66.72                    & 47.91                    & 68.13                            & 60.72                         & 52.68                  \\
            \hdashedline
            \Block[l]{4-1}{\texttt{Qwen2.5}}                                                                                                 & \Block[c]{4-1}{\texttt{14B}}  & \texttt{ICL}                    & 35.91                                              & 52.31                    & 43.55                    & 32.86                    & 47.95                    & 40.70                    & 31.59                    & 41.54                            & 40.80                         & 31.73                  \\
                                                                                                                                             &                               & \texttt{ICL-RR}                 & 48.15                                              & 59.69                    & 53.99                    & 37.19                    & 55.60                    & 57.02                    & 42.38                    & 59.30                            & 51.67                         & 42.42                  \\
                                                                                                                                             &                               & \texttt{TfPf}                   & 50.25                                              & 59.92                    & 49.84                    & 32.88                    & 54.39                    & 60.94                    & 41.11                    & 57.95                            & 50.91                         & 37.84                  \\
                                                                                                                                             &                               & \texttt{OUR}                    & \textbf{59.83}                                     & 68.74                    & 64.37                    & 48.53                    & 66.54                    & \textbf{69.64}           & 49.65                    & 70.42                            & 62.22                         & \textbf{54.12}         \\
            \bottomrule
        \end{NiceTabular}
    }
    \caption{
        Sentence-level $F_1$ scores of our method and the baseline methods on conventional CSC datasets.
        $\dagger$ indicates the results are from models fine-tuned on 34M synthetic CSC data \cite{wu-etal-2023-rethinking,liu-etal-2024-rephrasing}.
        $\ddagger$ indicates the results are from models pre-trained on 2M synthetic data specifically designed for the CSCD-NS dataset before fine-tuning on the CSCD-NS training set \cite{hu-etal-2024-cscd}.
        $\natural$ The ARM method is based on the \texttt{GPT3.5 Turbo} model and a BERT-based model (MDCSPell), which is trained on 271k synthetic CSC data from \citet{wang-etal-2018-hybrid} and training data from the Sighan series datasets.
    }
    \label{tab:spelling_check_results.sf}
\end{table*}
