
In order to both learn and protect sensitive training data, there has been a growing interest in privacy preserving machine learning methods.  Differential privacy has emerged as an important measure of privacy.  We are interested in the federated setting where a group of parties each have one or more training instances and want to learn collaboratively without revealing their data.

In this paper, we propose strategies to compute differentially private empirical distribution functions.  While revealing complete functions is more expensive from the point of view of privacy budget, it may also provide richer and more valuable information to the learner.  We prove privacy guarantees and discuss the computational cost, both for a generic strategy fitting any security model and a special-purpose strategy based on secret sharing.  We survey a number of applications and present experiments.  


