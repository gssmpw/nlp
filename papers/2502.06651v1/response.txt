\section{Related Work}
\label{sec:related}

Strategies to compute secure averages can be built depending on the level of trust put in the aggregator or in the users, on the utility and the verifability of results and lastly on the level of communication overhead.
Local differential privacy (LDP) **McSherry, "Privacy Integrated Queries"** does not require to trust the aggregator or users but brings litte utility and no verifability. The use of cryptographic primitives combined with DP **Dwork et al., "Our Data, Ourselves: Privacy Via Distributed Noise Generation"** allows exact aggregation but is not verifiable, except **Bittau et al., "Hiding Access Patterns in Query Languages without Caching"**. It requires $\Omega(n)$ communication per party and does not reflect the impact of colluding/malicious users except **Belynda et al., "Secure Multi-Party Computation Made Simple"** which relies on two non-coluding servers. Recently the shuffle model of privacy **Wang et al., "Privacy for Low-Rate Events: Quantitative Analysis"** assumes honest-but-curious parties with high communication overhead but can match the utility of the trusted curator model. Lastly, the use of correlated Gausian noise **Balle et al., "Practical Differential Privacy: A History, Some Theory, and Some Practice (Invited Talk)"** can have utility of central models but only **Cheu et al., "RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response"** can do it with malicious users and with only $O(\log n)$ communication.