\section{Related Work}
\paragraph{Cross-Lingual ABSA.}

Research in cross-lingual ABSA (XABSA) generally falls into two categories: data alignment and embedding learning. Data alignment aims to incorporate language-specific knowledge into the target language, often using translation systems or dictionaries to convert annotated data from high-resource languages \cite{zhou-etal-2013-collective}. Techniques such as co-training \cite{zhou2015cl} and constrained SMT \cite{lambert-2015-aspect} improve data quality. Additionally, pre-trained multilingual word embeddings and methods like warm-up mechanisms \cite{Li2020UnsupervisedCA} and shared vector spaces \cite{jebbara-cimiano-2019-zero} enhance model performance in multi-lingual ABSA. \citet{zhang2021cross} adopt the translation-based methods by code-switching the aspect terms in target and source data for cross-lingual ABSA. Following this, \citet{lin2023} use contrastive learning for cross-lingual ABSA.



\paragraph{Adversarial Networks.}

Adversarial training, popular in computer vision \cite{knoester2022}, has seen limited application in ABSA. Notable work includes \citet{Miyato2017VirtualAT}, who apply domain adversarial training to ABSA, and \citet{Wang_Pan_2018}, who use adversarial networks to align feature vectors across languages. Some methods have also explored character and word-level perturbations. \citet{Mamta2022AdversarialSG} generate adversarial samples for specific aspects while maintaining semantic coherence. Adversarial training helps evaluate model resilience and identify vulnerabilities \cite{lin2023}.

\paragraph{Consistency Training.}
Consistency training regularizes a model by ensuring predictions remain similar for both original and perturbed inputs \cite{zhou-etal-2022-conner}. While widely used in NLP, its application in ABSA is still emerging. Existing work includes \citet{chen-etal-2022-unsupervised-data}, which demonstrates that simple augmentations combined with consistency training yield competitive ABSA performance. Additionally, \citet{zhang-etal-2023-span} introduces a sentiment consistency regularizer to maintain sentiment consistency across spans.