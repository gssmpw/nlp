\section{Related Work}
\paragraph{Cross-Lingual ABSA.}

Research in cross-lingual ABSA (XABSA) generally falls into two categories: data alignment and embedding learning. Data alignment aims to incorporate language-specific knowledge into the target language, often using translation systems or dictionaries to convert annotated data from high-resource languages ____. Techniques such as co-training ____ and constrained SMT ____ improve data quality. Additionally, pre-trained multilingual word embeddings and methods like warm-up mechanisms ____ and shared vector spaces ____ enhance model performance in multi-lingual ABSA. ____ adopt the translation-based methods by code-switching the aspect terms in target and source data for cross-lingual ABSA. Following this, ____ use contrastive learning for cross-lingual ABSA.



\paragraph{Adversarial Networks.}

Adversarial training, popular in computer vision ____, has seen limited application in ABSA. Notable work includes ____, who apply domain adversarial training to ABSA, and ____, who use adversarial networks to align feature vectors across languages. Some methods have also explored character and word-level perturbations. ____ generate adversarial samples for specific aspects while maintaining semantic coherence. Adversarial training helps evaluate model resilience and identify vulnerabilities ____.

\paragraph{Consistency Training.}
Consistency training regularizes a model by ensuring predictions remain similar for both original and perturbed inputs ____. While widely used in NLP, its application in ABSA is still emerging. Existing work includes ____, which demonstrates that simple augmentations combined with consistency training yield competitive ABSA performance. Additionally, ____ introduces a sentiment consistency regularizer to maintain sentiment consistency across spans.