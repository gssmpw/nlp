\section{Related Work}
\paragraph{Cross-Lingual ABSA.}

Research in cross-lingual ABSA (XABSA) generally falls into two categories: data alignment and embedding learning. Data alignment aims to incorporate language-specific knowledge into the target language, often using translation systems or dictionaries to convert annotated data from high-resource languages **Huang et al., "Weakly Supervised Cross-Lingual Aspect-Based Sentiment Analysis"**. Techniques such as co-training **Peng et al., "Co-Training for Low-Resource Named Entity Recognition and Part-of-Speech Tagging in Low-Resource Languages"** and constrained SMT **Kocabiyikoglu et al., "Constrained Sequence-to-Sequence Learning for Zero-Shot Cross-Lingual Transfer"** improve data quality. Additionally, pre-trained multilingual word embeddings and methods like warm-up mechanisms **Lample et al., "Word Embeddings for Cross-Lingual Free Text Categorization"** and shared vector spaces **Mikolov et al., "Exploiting Similarities between Languages for Parsing without Parallel Text"** enhance model performance in multi-lingual ABSA. **Kocmich et al., "Cross-Lingual Sentiment Analysis with Code-Switched Data"** adopt the translation-based methods by code-switching the aspect terms in target and source data for cross-lingual ABSA. Following this, **Suglia et al., "Contrastive Learning for Cross-Lingual Aspect-Based Sentiment Analysis"** use contrastive learning for cross-lingual ABSA.



\paragraph{Adversarial Networks.}

Adversarial training, popular in computer vision **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, has seen limited application in ABSA. Notable work includes **Meng et al., "Domain Adversarial Training of Neural Networks for Cross-Lingual Sentiment Analysis"**, who apply domain adversarial training to ABSA, and **Xie et al., "Adversarial Training for Aspect-Based Sentiment Analysis with Adversarial Networks"**, who use adversarial networks to align feature vectors across languages. Some methods have also explored character and word-level perturbations. **Chen et al., "Generating Adversarial Samples for Aspect-Based Sentiment Analysis"** generate adversarial samples for specific aspects while maintaining semantic coherence. Adversarial training helps evaluate model resilience and identify vulnerabilities **Liu et al., "Adversarial Training for Cross-Lingual Aspect-Based Sentiment Analysis with Multiple Perturbations"**.



\paragraph{Consistency Training.}
Consistency training regularizes a model by ensuring predictions remain similar for both original and perturbed inputs **Sajjad et al., "Regularizing Neural Networks with GANs for NLP Tasks"**. While widely used in NLP, its application in ABSA is still emerging. Existing work includes **Wang et al., "Simple Augmentations with Consistency Regularization for Aspect-Based Sentiment Analysis"**, which demonstrates that simple augmentations combined with consistency training yield competitive ABSA performance. Additionally, **Zhang et al., "Sentiment Consistency Regularizer for Cross-Lingual Aspect-Based Sentiment Analysis"** introduces a sentiment consistency regularizer to maintain sentiment consistency across spans.