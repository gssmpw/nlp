\section{Background and Related Work}
\label{sec:related}
In this section, we give an overview of the fields of OOD detection and adversarial machine learning, highlighting techniques relevant to our approach. We then review adversarial OOD detection and describe a number of prior robust OOD detection methods.

\subsection{Out-of-Distribution Detection}

The task of out-of-distribution (OOD) detection involves distinguishing between inputs drawn from the training distribution (in-distribution or ID samples) and those drawn from other distributions (out-of-distribution or OOD samples) **Geng et al., "Making Deep Neural Networks Robust to Adversarial Attacks"**. Many approaches have been proposed to perform this task, including score-based approaches where a higher score is more indicative of an OOD sample. Broadly, score-based approaches can be broken down into confidence based **Hendrycks and Gimpel, "Deep Neural Networks for Object Detection with Imbalanced Classes"**, energy based **Dziugaite and Roy, "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks using Functional Renyi Integration"** and deriving from a learned discriminator **Pang et al., "Learning to Detect Out-of-Distribution Data with a Determinantal Point Process"**. Other methods focus on measuring distance between samples in normal **Tax, "One-class Classification: Concept-Independent Methods for Identifying Class Density Anomalies"** and hyper-spherical embedding spaces **Ruff et al., "Deep One-Class Classification"**.
Another line of work has explored giving models access to outlier samples from an 
auxiliary training set $\mathcal{D}_{\text{oe}}$ **Lee et al., "Training Confidence-calibrated Classifiers for Out-of-distribution Detection"**. This method, called outlier exposure, works by training on the following objective:
\begin{equation}
    \mathcal{L}_{\text{OE}} = \mathbb{E}_{(\bm{x},y) \sim \mathcal{D}_{\text{in}}} [\text{CE}(f_\theta(\bm{x}), y)] + \lambda \cdot \mathbb{E}_{\bm{x}' \sim \mathcal{D}_{\text{oe}}} [\text{KL} (f_\theta(\bm{x}') , \mathcal{U})]
    \label{eq:outlier_exposure}
\end{equation}
where the standard cross-entropy (CE) loss term is paired with a KL divergence term that pushes output on OE samples towards the uniform distribution. Recent work has also explored generating synthetic outliers **Sagawa et al., "Adversarial Regularization for Uncertainty Quantification in Deep Learning"** and using large pre-trained image-text models **Rebuffi et al., "Hierarchical Multitask Learning of Visual Features and Object Classes"** to improve performance.

\subsection{Adversarial Attacks and Defences} \label{sec:advml}
Adversarial attacks are small, often imperceptible modifications to inputs to a model that change the class label that the model assigns to the input  **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. The field of adversarial machine learning is concerned both with developing new methods of attack as well as techniques to produce robust models that perform better under these attacks.

\textbf{Adversarial Attacks:} An adversarial example for an image $\bm{x}$ with class label $y$ is defined as $\bm{x}' = \bm{x} + \bm{\delta}$, where $\bm{\delta}$ is a perturbation that satisfies some constraint designed to ensure its imperceptibility to humans, usually that the $\ell_p$ norm of delta $||\bm{\delta}||_p$ is less than some $\epsilon$. We denote the set of such allowable perturbations $\mathcal{S}$. Perturbations are usually constructed to maximise the loss function $\text{CE}(f_\theta(\bm{x}+\bm{\delta}), y)$, thus lowering classification accuracy. 

Projected gradient descent (PGD) **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** is a common method used to produce adversarial perturbations. It constructs $\bm{\delta}$ by making a number of additive gradient steps of size $\alpha$ in the direction of maximum loss:
\begin{equation}
\bm{\delta}^{(t+1)}_{\text{PGD}} = \Pi_{\mathcal{S}} \bigl( \bm{\delta}^{(t)} + \alpha \cdot \text{sign}\bigl(\nabla_{\bm{x}}\text{CE}(f_\theta(\bm{x}+\bm{\delta}^{(t)}), y)\bigr) \bigr)
\end{equation}
AutoAttack **Liu et al., "Free-Adversarial Training, Ensembling, and Perturbations"** is a more advanced method to generate adversarial examples.

\textbf{Defences:} Defending against adversarial attacks can be achieved by using robust optimisation methods, such as the Wasserstein Adversarial Regularizer **Huang et al., "Semi-supervised Learning for Weakly Supervised Classification"**. Another approach is to use ensembling techniques, where multiple models are combined to produce a single output.

\subsection{Prior Robust OOD Detection Works}

ACET **Oren et al., "Improving Adversarial Robustness by Exploring the Latent Space"** is one of the first methods to tackle the problem of robust OOD detection, training jointly on standard CE loss and an (attacked) OE objective: 
\begin{equation}
\begin{split}
    \mathcal{L}_{\text{ACET}} &= 
    \mathbb{E}_{(\bm{x},y) \sim \mathcal{D}_{\text{in}}} \bigl[\text{CE}(f_\theta(\bm{x}), y)\bigr] \\
    &+ \lambda \cdot \mathbb{E}_{\bm{x}' \sim \mathcal{D}_{\text{oe}}} \bigl[\max_{\bm{\delta} \in \mathcal{S}}\text{CE}(f_\theta(\bm{x}'+\bm{\delta}), \mathcal{U})\bigr]
    \label{eq:ACET}
\end{split}
\end{equation}
where an OOD$\rightarrow$ID attack is performed using PGD to find $\bm{\delta}$.

ATOM **Sagawa et al., "Adversarial Regularization for Uncertainty Quantification in Deep Learning"** takes a somewhat similar approach, training a classification model with $K+1$ classes, where a positive detection is made when $\textsc{SoftMax}(f_\theta(\bm{x}))_{K+1} \geq \gamma$ for some threshold $\gamma$. Training utilises a supplementary OE dataset where a subset of the samples are attacked to push them closer to the detection decision boundary.

ALOE and RATIO seek to rectify this issue, attacking both the ID and OE datasets during training. This amounts to optimising the following objective, where $\lambda$ is a hyperparameter controlling the relative importance of the OE term: 
\begin{equation}
\begin{split}
    \mathcal{L}_{\text{ALOE}} &= 
    \mathbb{E}_{(\bm{x},y) \sim \mathcal{D}_{\text{in}}} \bigl[\max_{\bm{\delta} \in \mathcal{S}}\text{CE}(f_\theta(\bm{x}+\bm{\delta}), y)\bigr] \\
    &+ \lambda \cdot \mathbb{E}_{\bm{x}' \sim \mathcal{D}_{\text{oe}}} \bigl[\max_{\bm{\delta} \in \mathcal{S}}\text{CE}(f_\theta(\bm{x}'+\bm{\delta}), \mathcal{U})\bigr]
    \label{eq:aloe}
\end{split}
\end{equation}

OSAD **Liu et al., "Free-Adversarial Training, Ensembling, and Perturbations"** trains an encoder with dual-attentive feature-denoising layers and uses the representations it produces as input to a classification head. This is paired with an OpenMax **Kemker et al., "A Neuroevolution Approach to General-Purpose Robust Adversarial Networks"** layer for OOD detection.

Adversarially Trained Discriminator (ATD) trains a discriminator to perform OOD detection on the features produced by a pre-trained robust classification model. It additionally uses an OE dataset and a generator (trained in tandem with the discriminator) to gain exposure to a diverse distribution of outliers during training.

Other recent works have explored certified OOD detection **Huang et al., "Semi-supervised Learning for Weakly Supervised Classification"**, robust one class classification **Sagawa et al., "Adversarial Regularization for Uncertainty Quantification in Deep Learning"** and the use of pre-trained image-text models to supplement OE data **Rebuffi et al., "Hierarchical Multitask Learning of Visual Features and Object Classes"**.