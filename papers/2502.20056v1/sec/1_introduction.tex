\section{Introduction}
\label{sec:intro}

Chest X-ray (CXR) is a widely employed diagnostic tool in clinical practice, primarily for evaluating the lungs, heart, pleura, and skeletal structures. It is critical for diagnosing conditions, such as pneumonia, fracture, pneumothorax, pleural effusion, and cardiomegaly \cite{irvin-chexpert}. To ensure effective communication across departments and between physicians and patients, radiologists manually document detailed reports based on their interpretation of CXR images. However, this process is both expertise-dependent and time-consuming \cite{wang2024-survey-rrg,sei}. As the demand for imaging studies continues to grow, the workload associated with manual report generation may intensify, potentially impacting medical efficiency and compromising patient care quality \cite{2024-eccv-hergen,bannur2024maira2groundedradiologyreport}. To mitigate these challenges, radiology report generation (RRG) \cite{RBME-survey-rrg-2024,guo2024automaticmedicalreportgeneration-survey} has emerged as a promising solution. By automatically analyzing imaging data from X-ray \cite{liang2024divide-acmmm-24}, CT \cite{hamamci2024ct2rep}, or pathology \cite{guo2024histgen}, RRG generates clinical findings using factual terminology \cite{fse,yan2023style} and descriptive language. This automation aids radiologists by providing high-quality draft reports \cite{sei}, improving diagnostic efficiency.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig1.pdf}
    % A comparative analysis of chest X-ray report generation methods based on data inputs. 
    \caption{(A) shows medical historical data of a subject (patient) over time. (B) compares inputs for RRG, with \textit{AP} and \textit{PA} as frontal views, and \textit{Lat} and \textit{Rep} as a lateral view and its report. \textit{Ind} and \textit{MVL Data} are ``INDICATION" and multi-view longitudinal data.}
    \label{fig:1}
\end{figure}


In clinical practice, radiologists typically conduct comprehensive evaluations using multi-view images from the current visit, incorporate patient medical histories (i.e., longitudinal data) to track disease progression, and integrate patient-specific prior knowledge to assist in diagnosis and report generation. However, most existing RRG methods \cite{chen-etal-2020-generating,nicolson-improving-cvt2distilgpt2,tmm_mulview_2024-fmvp} focus solely on single images when generating reports and struggle to effectively differentiate between views, such as posteroanterior (PA), anteroposterior (AP), lateral, or left anterior oblique. These views exhibit inherent differences; for example, although both PA and AP views are frontal images, geometric variations can cause cardiac enlargement in the AP view, potentially impacting diagnostic accuracy. To address this issue, some studies \cite{chen-etal-2021-cross-modal,yang-m2kt} have introduced dual-view report generation, as illustrated in Figure \ref{fig:1}. Empirical results \cite{chen-etal-2020-generating,chen-etal-2021-cross-modal} reveal that incorporating dual views enhances the quality of generated reports. Nevertheless, these methods merely distinguish between frontal and lateral views, neglecting more subtle differences across multiple views. Moreover, both single-image and dual-view methods focus solely on the images from the current visit, disregarding the descriptions of disease progression found in radiology reports. This limitation may lead to model hallucinations. To combat this problem, some studies \cite{chexrelnet-longitudinal-miccai-2022-not-rrg,pre-fill-longitudinal-miccai-2023,2024-eccv-hergen} have sought to leverage longitudinal data to model disease progression, as shown in Figure \ref{fig:1}. However, these methods still rely on a single image to characterize the current visit, limiting diagnostic accuracy. Additionally, some patients may lack ``INDICATION", ``previous report", or ``previous image" due to their first visit or improper data storage. This variability challenges the flexible use of available data to generate accurate reports.


To mimic radiologists' diagnostic pipeline and address these challenges, we propose a two-stage MLRG for chest X-ray report generation. In Stage 1, the key part is our proposed multi-view longitudinal contrastive learning approach, which utilizes the inherent spatiotemporal information in radiology reports to supervise the pre-training of visual and textual representations. Specifically, we incorporate learnable position embeddings for each view to identify differences across varying numbers of views. We then employ a multi-view longitudinal fusion network that flexibly integrates spatial information from current multi-view images and temporal information from longitudinal data. Subsequently, we learn visual and textual representations by leveraging agreements between multi-view longitudinal data (see Figure \ref{fig:1}) and their corresponding radiology reports. In Stage 2, we introduce a tokenized absence encoding technique to handle missing patient-specific prior knowledge (i.e., ``INDICATION" and ``previous report"). This allows the multi-modal fusion network to adapt flexibly to the presence or absence of such data, improving the accuracy of the generated reports. Extensive experiments on MIMIC-CXR \cite{johnson-mimic-cxr-jpg}, MIMIC-ABN \cite{mimic-abn-ori}, and Two-view CXR \cite{mcl} datasets demonstrate the superiority of MLRG in producing clinically accurate reports. Our contributions are stated as follows:
\begin{itemize}
    \item We propose a novel multi-view longitudinal contrastive learning method that flexibly integrates multi-view longitudinal data and leverages the inherent spatiotemporal information from reports to supervise the pre-training of visual and textual representations.
    \item We introduce a tokenized absence encoding technique to handle missing patient-specific prior knowledge. This technique enables the model to adapt flexibly to scenarios with or without such data, ensuring the text generator can utilize available prior knowledge effectively.
    \item Our MLRG shows competitive results compared to various state-of-the-art methods across three public datasets: MIMIC-CXR, MIMIC-ABN, and Two-view CXR.
\end{itemize}

%-------------------------------------------------------------------------
