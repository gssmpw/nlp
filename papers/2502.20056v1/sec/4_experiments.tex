\section{Experiments}
%-------------------------------------------------------------------------

\subsection{Experimental Settings}
\textbf{Datasets.} 1) \textbf{MIMIC-CXR} \cite{johnson-mimic-cxr-jpg} is a large-scale, publicly available dataset comprising paired chest X-rays and radiology reports. Each pair contains a varying number of images compared to others, and all pairs for a patient are organized chronologically, facilitating the construction of multi-view longitudinal data. 2) \textbf{MIMIC-ABN} \cite{mimic-abn-ori} is a subset of MIMIC-CXR, focusing solely on radiology reports that describe abnormal clinical findings. 3) \textbf{Two-view CXR} \cite{mcl} aggregates visits with two current images from both MIMIC-CXR and IU X-ray \cite{demner2016preparing-iu-xray}. Notably, as the IU X-ray does not include previous visits, both previous images and reports are unavailable. We adhere to official splits for these datasets and summarize the sample counts for the training, validation, and test set in Table \ref{table:0}. In line with \cite{chen-etal-2020-generating,MMTN-aaai-2023,tanida-rgrg,sei,li-dcl}, we treat the ``FINDINGS" section in radiology reports as the reference reports.

% ablation study
\begin{table*}
\centering
\setlength{\tabcolsep}{1.8mm}
\begin{tabular}{cccccccccccccccc} 
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{M/S}} & \multirow{2}{*}{\textbf{F/R}} & \multirow{2}{*}{\textbf{PI}} & \multicolumn{2}{c}{\textbf{Stage 1}} & \multicolumn{2}{c}{\textbf{Stage 2}} & \multicolumn{6}{c}{\textbf{NLG metrics} $\uparrow$} & \multicolumn{2}{c}{\textbf{CE metrics} $\uparrow$} \\ 
\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-14}\cmidrule(r){15-16}
 &  &  &  & ${\cal{L}}_{G}$ & ${\cal{L}}_{MPC}$ & \textbf{Ind} & \textbf{PR} & \textbf{B-1} & \textbf{B-2} & \textbf{B-3} & \textbf{B-4} & \textbf{MTR} & \textbf{R-L} & \textbf{RG} & \textbf{F1} \\ 
\midrule
(a) & M & F & \ding{51} & \ding{55} & \ding{55} & \ding{51} & \ding{51} & 0.346 & 0.235 & 0.173 & 0.136 & 0.154 & 0.305 & 0.258 & 0.373 \\
(b) & M & F & \ding{51} & \ding{51} & \ding{51} & \ding{55} & \ding{55} & 0.385 & 0.239 & 0.162 & 0.118 & 0.155 & 0.283 & 0.238 & 0.479 \\
(c) & M & F & \ding{55} & \ding{51} & \ding{55} & \ding{51} & \ding{51} & 0.384 & 0.257 & 0.188 & 0.146 & 0.165 & 0.310 & 0.267 & 0.455 \\
(d) & M & F & \ding{55} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 0.395 & 0.257 & 0.183 & 0.138 & 0.167 & 0.302 &0.278~ & 0.503 \\
(e) & M & F & \ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{55} & 0.392 & 0.265 & 0.195 & 0.153 & 0.171 & 0.316 & 0.281 & 0.476 \\
(f) & M & F & \ding{51} & \ding{51} & \ding{51} & \ding{55} & \ding{51} & 0.387 & 0.240 & 0.163 & 0.118 & 0.156 & 0.281 & 0.243 & 0.484 \\
(g) & M & R & \ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 0.403 & 0.267 & 0.193 & 0.148 & 0.172 & 0.309 & 0.287 & \textbf{0.510} \\
(h) & S & F & \ding{51} & \ding{51} & \ding{55} & \ding{51} & \ding{51} & 0.400 & 0.269 & 0.196 & 0.151 & 0.173 & 0.314 & 0.289 & 0.498 \\
\textbf{MLRG} & M & F & \ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & \textbf{0.411} & \textbf{0.277} & \textbf{0.204} & \textbf{0.158} & \textbf{0.176} & \textbf{0.320} & \textbf{0.291} & 0.505 \\
\bottomrule
\end{tabular}
\caption{Ablation study on the MIMIC-CXR dataset. ``M/S" refers to methods that utilize current \textbf{M}ulti-view images or \textbf{S}ingle images as input. ``F/R" indicates alignment based on either \textbf{F}actual serialization or \textbf{R}eport. ``PI", ``PR", and ``Ind" represent \textbf{P}revious \textbf{I}mages, \textbf{P}revious \textbf{R}eports, and ``INDICATION", respectively. The best values are emphasized in \textbf{bold}.}
\label{table:2}
\end{table*}

\textbf{Evaluation metrics.} Following prior works \cite{chen-etal-2020-generating,Wang_2021_CVPR_region,wang2023metransformer,2024-eccv-hergen,Bu_Instance-level-2024_CVPR}, we evaluate the effectiveness of our MLRG using both natural language generation (NLG) and clinical efficacy (CE) metrics. NLG metrics, which assess the linguistic similarities between generated and reference reports, include BLEU-n (B-n), METEOR (MTR), and ROUGE-L (R-L). For CE metrics, we utilize CheXpert \cite{irvin-chexpert} to label the generated reports with 14 observations (see Appendix Table \ref{atable:1}) and compute the micro-average Precision (P), Recall (R), and F1 score (F1) based on ground truths. CE metrics also include F1 RadGraph (RG) \cite{jain-radgraph}, which evaluates the overlap of clinical entities and their relations, aligning more closely with radiologists than B-3 and F1 metrics \cite{yu2023evaluating}. All metrics are computed using official libraries \cite{chen2015microsoft-coco-caption,jain-radgraph,Smit2020_chexbert}, with higher values indicating better performance.

\textbf{Implementation details.} Both $\tau_1$ and $\tau_2$ are set to 0.5. The number of blocks $L_1$ and $L_2$ in Figure \ref{fig:3} are set to 3 and 1, respectively. Each dataset is configured to generate a maximum of 100 tokens. We identify the best model as the one with the highest sum of BLEU-4, F1 RadGraph, and F1 score on the validation set, and we report its performance on the test set. Additional details about epochs, learning rates, and other settings can be found in the Appendix \ref{implementation-details}.


%-------------------------------------------------------------------------




% breakdown
\begin{table}
\centering
\setlength{\tabcolsep}{1.4mm}
\begin{tabular}{ccccccc} 
\toprule
\textbf{Setting} & \textbf{\%} & \textbf{B-2 $\uparrow$} & \textbf{B-4 $\uparrow$} & \textbf{MTR $\uparrow$} & \textbf{R-L $\uparrow$} & \textbf{RG $\uparrow$} \\ 
\midrule
w/ Ind & 57.8 & \textbf{0.295} & \textbf{0.174} & \textbf{0.184} & \textbf{0.332} & \textbf{0.318} \\
w/o Ind & 42.2 & 0.253 & 0.137 & 0.166 & 0.302 & 0.254 \\
w/ MV & 70.7 & \textbf{0.282} & \textbf{0.161} & \textbf{0.179} & \textbf{0.323} & \textbf{0.301} \\
w/o MV & 29.3 & 0.264 & 0.150 & 0.171 & 0.310 & 0.266 \\
w/ MVL & 61.4 & \textbf{0.282} & \textbf{0.160} & \textbf{0.178} & \textbf{0.322} & \textbf{0.300} \\
w/o MVL & 38.6 & 0.270 & 0.155 & 0.174 & 0.316 & 0.276 \\
\bottomrule
\end{tabular}
\caption{Breakdown of MLRG's metrics on the MIMIC-CXR test set, categorized by (a) inclusion of indications (Ind), (b) inclusion of current multi-view images (MV), (c) inclusion of multi-view longitudinal data (MVL).}
\label{table:3}
\end{table}

%-------------------------------------------------------------------------
\subsection{Main Results}
We compare our MLRG with 14 state-of-the-art methods: R2Gen \cite{chen-etal-2020-generating}, CMN \cite{chen-etal-2021-cross-modal}, SA \cite{yan2023style}, MET \cite{wang2023metransformer}, KiUT \cite{huang-kiut}, CoFE \cite{cofe-eccv-24}, MAN \cite{shen2024automatic_aaai}, B-LLM \cite{aaai-liu2024bootstrapping-llm}, DCG \cite{liang2024divide-acmmm-24}, Med-LLM \cite{liu2024in-context-acmmm}, SEI \cite{sei}, FVMP \cite{tmm_mulview_2024-fmvp}, HERGen \cite{2024-eccv-hergen}, and CXRMate \cite{nicolson2023-longitudinal-multiview}. Results are presented in Table \ref{table:1}, where ``SI", ``Ind", ``MVD", ``Long", ``MVL", and ``DV" represent different input types: single images, ``INDICATION", multi-view data, longitudinal data, multi-view longitudinal data, and dual views, respectively. We observe that our MLRG achieves SOTA performance across most metrics, with particular strength in B-4, RG, and F1. This suggests that MLRG excels in generating both coherent and accurate radiology reports. Although MLRG shows slightly lower Recall than B-LLM \cite{aaai-liu2024bootstrapping-llm}, its F1 and other metrics are significantly better. In Appendix Table \ref{atable:3}, we also show our MLRG's ability to generate ``FINDINGS'' and ``IMPRESSION'' sections.


%-------------------------------------------------------------------------
\subsection{Ablation Study}
Table \ref{table:2} presents an ablation study on the MIMIC-CXR \cite{johnson-mimic-cxr-jpg} dataset, analyzing the effect of different components on model performance.

\textbf{Effect of multi-view longitudinal contrastive learning (Stage 1).} In Table \ref{table:2}, (a) represents a report generation scheme based solely on patient-specific prior knowledge, excluding Stage 1. Results reveal that our MLRG significantly exceeds (a), highlighting the critical role of multi-view longitudinal contrastive learning in enhancing the accuracy and coherence of generated reports. Additionally, we observe that both  ${\cal{L}}_{G}$ ((c) vs. (a)) and ${\cal{L}}_{MPC}$ ((d) vs. (c)) have a positive impact on model performance.

\textbf{Effect of patient-specific prior knowledge (Stage 2).} As shown in Table \ref{table:2}, MLRG significantly outperforms (b), which lacks patient-specific prior knowledge, emphasizing the importance of incorporating such knowledge to improve the coherence and clinical accuracy of generated reports. Moreover, the independent integration of ``INDICATION" ((e) vs. (b)) and ``previous report" ((f) vs. (b)) contributes positively to model performance.

\textbf{Effect of current multi-view images.} Table \ref{table:2} demonstrates that generating reports using current multi-view images outperforms those derived from single images (MLRG vs. (h)), highlighting the effectiveness of multi-view images in modeling the current disease conditions. 

\textbf{Effect of previous images.} As shown in Table \ref{table:2}, MLRG shows a clear advantage over (d), indicating that MLF network (see Figure \ref{fig:3}) effectively integrates previous images. This capability allows the model to track disease progression, thereby generating more clinically accurate reports.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig4.pdf}
    \caption{Generated reports examples on the MIMIC-CXR test set. Each ``A/B" cell refers to ``MLRG/SEI". Sentences in the reference report are highlighted in unique colors to clarify alignment with descriptions in the generated reports. Matching content in generated reports is shown in the same color, while correct temporal descriptions and failure descriptions of our MLRG are in \textbf{bold} and \underline{underlined}.}
    \label{fig:4}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig5.pdf}
    \caption{Comparison with baselines on MIMIC-CXR using LLMs. ``\#Matched Findings" denotes the number of matched findings between generated and reference reports.}
    \label{fig:5}
\end{figure}

%-------------------------------------------------------------------------
\subsection{Case Study}
\textbf{Model benefits from current multi-view images, multi-view longitudinal data, and ``INDICATION".} Table \ref{table:3} compares performance on test subsets with and without these data (multi-view images, multi-view longitudinal data, and indications). We observe that including these data significantly improves NLG and RG metrics. This suggests that the multi-view longitudinal contrastive learning method effectively integrates current multi-view images and multi-view longitudinal data, capturing semantically rich visual representations. Moreover, our multi-modal fusion network effectively leverages ``INDICATION" to help the text generator produce more accurate radiology reports.

\textbf{Clinical accuracy of 14 observations.} Appendix Tables \ref{atable:1} and \ref{atable:2} show the clinical accuracy of 14 observations labeled by CheXpert \cite{irvin-chexpert} across three datasets. Results indicate that our MLRG outperforms SEI \cite{sei} on most observations. Although not specifically tailored for imbalanced observations, MLRG still slightly exceeds the baseline on challenging observations like \textit{Pneumothorax} and \textit{Fracture}. 


\textbf{Qualitative analysis.} Figure \ref{fig:4} presents examples of generated reports using SEI \cite{sei} and our MLRG, with additional examples in Appendix Figure \ref{afig:1}. A greater number of colors in the generated report indicates broader coverage of clinical findings, while a longer color bar reflects more accurate and detailed descriptions of specific findings. Results indicate that 1) Our MLRG provides radiologists with higher-quality draft reports compared to SEI \cite{sei}; 2) Our MLRG exhibits a certain ability to describe disease progression, as evidenced in case 2 with the statement ``the patient has received a right internal jugular vein catheter". 


\textbf{Evaluation using large language models (LLMs).} Figure \ref{fig:5} illustrates the performance of our generated reports evaluated with GREEN \cite{2024-green}, a fine-tuned LLaMA 2 \cite{llama} designed to identify clinically significant errors and count matched findings. Results demonstrate that our MLRG outperforms R2Gen \cite{chen-etal-2020-generating}, CMN \cite{chen-etal-2021-cross-modal}, CGPT2 \cite{nicolson-improving-cvt2distilgpt2}, and SEI \cite{sei} in both \#Matched Findings and GREEN score, further confirming the advantage of our MLRG in generating coherent and clinically accurate radiology reports. For further details, please refer to the Appendix \ref{eva-large-language-models}.