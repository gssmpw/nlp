\section{Related Work}
\label{sec:related_work}
%-------------------------------------------------------------------------

\textbf{Radiology report generation (RRG).} RRG is akin to image captioning \cite{li2023-blip2,cvpr-2023conzic-image-captioning}, but requires generating detailed content with specialized medical terminology. Existing RRG methods consist of a vision encoder (like ResNet101 \cite{chen-etal-2021-cross-modal,yang-m2kt,huang-kiut}, CvT \cite{nicolson-improving-cvt2distilgpt2,nicolson2023-longitudinal-multiview}, or ViT \cite{wang2023metransformer,liu2024in-context-acmmm}) and a text generator (such as Memory-driven Transformer \cite{chen-etal-2020-generating,fse}, MiniGPT-4 \cite{aaai-liu2024bootstrapping-llm}, DistilGPT2 \cite{nicolson-improving-cvt2distilgpt2,liang2024divide-acmmm-24}, or LLaMA \cite{liu2024in-context-acmmm,2024-iclr-cxr-llm,wang-2023-r2gengpt}). To improve clinical accuracy in RRG, researchers have incorporated various techniques or prior knowledge, including knowledge graphs \cite{zhang-when-aaai-2020,yang-gsket}, cross-modal alignment \cite{chen2024fine-alignment-acl-24,fse}, region-guided frameworks \cite{tanida-rgrg,tmi-2024-scene-graph-rrg}, warm starting \cite{nicolson-improving-cvt2distilgpt2}, patient-specific ``INDICATION" \cite{sei,mcl}, disease labels \cite{yang-m2kt}, and disease progression \cite{recap,2024-eccv-hergen}. However, these methods rely on single-image or fixed-view data, missing the comprehensive insights supplied by multi-view longitudinal data. To address this issue, we propose the MLRG method, which flexibly captures spatiotemporal features from multi-view longitudinal data and generates radiology reports based on patient-specific prior knowledge.



% figure 2: overview
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig2.pdf}
    \caption{Overview of our proposed MLRG, including a vision encoder (RAD-DINO \cite{2024-rad-dino}), a text encoder (CXR-BERT \cite{2022-eccv-cxr-bert}), and a text generator (DistilGPT2 \cite{Sanh2019DistilBERTAD}). MLRG first learns visual features through multi-view longitudinal contrastive learning and then generates radiology reports based on patient-specific prior knowledge.}
    \label{fig:2}
\end{figure*}

%------------------------------------------------------------------------

\textbf{Medical vision-language models.} These models aim to learn generalized medical visual representations by maximizing agreements between image-report pairs. MGCA \cite{wang-mgca} presents multi-granularity cross-modal alignment, harnessing agreements at the instance, pathological region, and disease levels. KAD \cite{zhang-kad} enhances visual representation using established knowledge graphs. MedCLIP \cite{Wu-medclip} expands training sets by decoupling images and reports, reducing false negatives through semantic matching loss. BioViL-T \cite{2023-cvpr-biovit-2301} captures disease progression by analyzing longitudinal data. Despite notable improvements in tasks like medical image classification and image-text retrieval, the utilization of multi-view longitudinal data remains limited, constraining diagnostic accuracy. Therefore, we present a multi-view longitudinal contrastive learning approach that utilizes the inherent spatiotemporal information of radiology reports to guide the pre-training of visual and textual representations.



%-------------------------------------------------------------------------

\textbf{Enhancing medical image analysis via multi-view data.} Multi-view learning \cite{a_survey_multi_view} empowers models to derive shared and complementary insights from multiple views of the same subject. FMVP \cite{tmm_mulview_2024-fmvp} treats single-view images and auxiliary inputs (i.e., disease labels and medical concepts) as multi-view data to assist the text generator in producing radiology reports. However, its reliance on additional annotated disease labels limits broader applicability. CXRMate \cite{nicolson2023-longitudinal-multiview} synthesizes previous reports based on previous images and integrates them with current multi-view images to produce final reports. However, this method overlooks subtle differences across views and can introduce additional noise from previous reports. In response, we propose the MLRG approach, which captures inter-view differences and leverages spatiotemporal information in reports to guide the pre-training, all without relying on additional manual labels.

%-------------------------------------------------------------------------