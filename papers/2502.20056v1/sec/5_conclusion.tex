\section{Conclusion}
In this paper, we introduced the MLRG method for chest X-ray report generation. We first proposed a multi-view longitudinal contrastive learning approach that leveraged the inherent spatiotemporal information from radiology reports to guide the pre-training of visual and textual representations. This approach not only captured differences among views but also flexibly extracted spatial features from current multi-view images and temporal features from longitudinal data, effectively leveraging spatiotemporal information in reports for pre-training. Subsequently, we presented a tokenized absence encoding technique to handle missing patient-specific prior knowledge. This technique allowed the multi-modal fusion network to adapt flexibly to scenarios with or without such data, ensuring the text generator can utilize available prior knowledge effectively. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view datasets demonstrated that our MLRG outperforms existing SOTA methods in generating coherent and clinically accurate radiology reports, making it a strong contender for chest X-ray report generation. Future work will focus on using saliency maps \cite{2024-saliency-map} to learn region-based features and predict uncertainty \cite{uncertainty} to improve model reliability.


%In future work, we plan to enhance model reliability by predicting uncertainty \cite{uncertainty} and learn region-based features using saliency maps \cite{2024-saliency-map} to learn region-based features and predict 


%1) Our multi-view longitudinal contrastive learning approach effectively extracted spatiotemporal features from multi-view longitudinal data, significantly improving the generation of clinically accurate radiology reports. 2) Our tokenized absence encoding technique and multi-modal fusion network effectively integrated patient-specific prior knowledge, enhancing the generation of coherent and clinically accurate reports.
% our MLRG outperforms existing SOTA methods in generating coherent and clinically accurate radiology reports, making it a strong contender for chest X-ray report generation.
%Patient-specific prior knowledge is essential for enhancing the model's ability to generate coherent and clinically accurate radiology reports. 
% In future work, we plan to enhance model reliability through prediction uncertainty \cite{uncertainty} and learn region-based features using saliency map \cite{2024-saliency-map}.





%employed special tokens to simulate missing ``INDICATION" and ``previous report", allowing the multi-modal fusion network to adapt seamlessly to scenarios with or without this data, thereby providing the text generator with patient-specific prior knowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view datasets demonstrated that integrating current multi-view images, longitudinal data, and ``INDICATION" is crucial for generating coherent and clinically accurate radiology reports. In future work, we plan to enhance model reliability through prediction uncertainty \cite{uncertainty} and learn region-based features using saliency map \cite{2024-saliency-map}.This enabled the model to capture differences across varying numbers of views and flexibly integrated spatiotemporal information from multi-view longitudinal data. 