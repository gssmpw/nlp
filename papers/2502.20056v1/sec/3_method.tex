\section{Method}

%-------------------------------------------------------------------------

Figure \ref{fig:2} presents an overview of our proposed MLRG. In Stage 1, we introduce a multi-view longitudinal contrastive learning approach that leverages inherent spatiotemporal information from radiology reports to supervise the pre-training of visual and textual representations. In Stage 2, we propose a tokenized absence encoding technique to handle missing patient-specific prior knowledge, ensuring the generation of more coherent and accurate radiology reports based on available prior knowledge.


% x_{i,a}^{cur}, x_{i,\backslash a}^{cur}
%-------------------------------------------------------------------------
\subsection{Problem Formation}
Let ${\mathcal{D} _{tr}} = \{(x_i^{pri},y_i^{pri},{z_i},X_i^{cur},y_i^{cur})\} _{i=1}^{n}$ be the training set, where $n$ denotes the total number of visits. Each visit consists of a frontal previous image $x_i^{pri}$ (which may be absent), a previous report $y_i^{pri}$ (which may be absent), an ``INDICATION" ${z_i}$ (which may be absent), $m_i$ current images (views) $X_i^{cur}$, and a reference report $y_i^{cur}$. Notably, The number of current multi-view images $m_i$ may vary across visits. Our goal is to learn the function $F_{\theta}(\cdot)$ that maps $(x_i^{pri},y_i^{pri},{z_i},X_i^{cur})$ to $y_i^{cur}$ on the training set $\mathcal{D} _{tr}$, such that $F_\theta (x_i^{pri},y_i^{pri},{z_i},X_i^{cur})\to y_i^{cur}$. We then utilize the learned function $F_{\theta}(\cdot)$ to generate a radiology report based on current multi-view images, the previous image, and patient-specific prior knowledge (i.e., $y_i^{pri}$ and ${z_i}$).

%-------------------------------------------------------------------------
\subsection{Multi-view Longitudinal Contrastive Learning}
\textbf{Visual features extraction.} We employ RAD-DINO \cite{2024-rad-dino}, a vision transformer model \cite{2021-vit} trained solely on chest X-rays using DINOv2 \cite{2024-dinov2}, as the vision encoder. The feature maps from the last hidden state are treated as visual features ${\boldsymbol{V}} \in {\mathbb{R}^{M \times p \times d_1}}$, where $M = \sum\nolimits_{i = 1}^B {{m_i}}$ denotes the total number of images in the mini-batch. Here, $B$, $p$, and $d_1$ represent the batch size, the number of patches, and the feature dimension, respectively.

\textbf{Textual features extraction.} Inspired by FSE \cite{fse}, we first adopt the structural entities approach \cite{fse} to extract factual serialization, which consists exclusively of clinical descriptions from radiology reports, as shown in Figure \ref{fig:2}. This method enables the model to concentrate on alignment between images and factual serialization. We then consider CXR-BERT \cite{2022-eccv-cxr-bert}, a language model tailored for chest X-rays, as the text encoder. This is followed by a simple projection head that generates textual features ${\boldsymbol{R}}\in {\mathbb{R}^{B \times t \times d}}$, where $t$ denote the number of tokens, and $d$ is the hidden size.

\textbf{Multi-positive contrastive learning between current multi-view images to improve the consistency of visual features.} In clinical practice, radiologists often select some representative images as primary references, with other images as auxiliary support. Therefore, we treat each image from current multi-view images $X_i^{cur}$ as an anchor scan $x_{i, a}^{cur}$ while considering the remaining images as auxiliary references $x_{i,\backslash a}^{cur}=\left \{ x_{i,j}^{cur}|j \neq a, x_{i,j}^{cur} \in X_i^{cur} \right \} $, where $a \in [1,m_i]$. To identify differences among views, we incorporate learnable view positional embeddings ${\boldsymbol{E}}_v \in {\mathbb{R}^{M \times 1 \times d_1}}$ into visual features. This is followed by a simple projection head $P_v(\cdot )$ that maps the features to a specific dimension $d$. These processes are formulated as follows:
\begin{align}
{\boldsymbol{V}} = P_v\left ( {\boldsymbol{V}} + {\boldsymbol{E}}_v \right ) \in {\mathbb{R}^{M \times p \times d}}.
\end{align}
To enhance the consistency of visual features, we employ multi-positive contrastive learning \cite{NEURIPS2023_stablerep} to maximize the similarity between images from the same visit while minimizing the similarity to images from different visits. Specifically, we first exclude visits with only one image, as they do not provide positive pairs (Notably, these visits are still used for subsequent cross-modal alignment). Following this, we calculate the predicted categorical distribution ${\boldsymbol{q}} \in {\mathbb{R}^{K \times (K-1)}}$ to estimate the similarity between images:
\begin{align}
{{\boldsymbol q}_i} = \frac{{\exp \left( {{{{{\boldsymbol v}_i} \cdot {{\boldsymbol v}_{\backslash i}^T}} \mathord{\left/ \right.
 } {{\tau _1}}}} \right)}}{{\sum\nolimits_{j = 1,j \ne i}^M {\exp \left( {{{{{\boldsymbol v}_i} \cdot {{\boldsymbol v}_j^T}} \mathord{\left/ \right.
 } {{\tau _1}}}} \right)} }}, s.t. m_i \ne 1,
\end{align}
where $K = \sum\nolimits_{i = 1,m_i \ne 1}^M {{m_i}}$ represents the total number of multi-view images in the mini-batch, and $\tau_1 \in {\mathbb{R}^{+}}$ is a temperature parameter. ${{\boldsymbol v}_i} \in {\mathbb{R}^{1 \times d}}$ refers to the global visual feature of the $i^{th}$ image, while ${{\boldsymbol v}_{\backslash i}} \in {\mathbb{R}^{(K-1) \times d}}$ denotes global visual features of all multi-view images except the $i^{th}$ image. Next, we compute the ground-truth categorical distribution ${\boldsymbol{p}} \in {\mathbb{R}^{K \times (K-1)}}$ by assigning the same labels to images from the same visit, formulated as:
\begin{align}
{\boldsymbol{p}_i} = \frac{{{\mathbb{I}_{match}}\left( {{{\boldsymbol v}_{i}},{{{\boldsymbol v}}_{\backslash i}}} \right)}}{{\sum\nolimits_{j = 1,m_j \ne 1}^{M} {{\mathbb{I}_{match}}\left( {{{{\boldsymbol v}}_{i}},{{\boldsymbol v}}_{j}} \right)} }},
\end{align}
where \({\mathbb{I}_{match}}\left( {\cdot, \cdot} \right)\) is an indicator function that determines whether two visual features originate from the same visit. Although the number of current views $m_i$ may vary across visits, the different number of non-zero elements in \({\boldsymbol{p}_i}\) account for this variability. Finally, the multi-positive contrastive (MPC) loss is calculated using the cross-entropy between $\boldsymbol{q}$ and $\boldsymbol{p}$, represented as:
\begin{align}
{{{\cal L}}_{MPC}} =  - \frac{1}{K}\sum\limits_{i = 1,m_i \ne 1}^{M} {{{\boldsymbol{p}}_i}\log {{\boldsymbol{q}}_i}}.
\end{align}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig3.pdf}
    \caption{(A) represents the multi-view longitudinal fusion (MLF) network. (B) denotes the multi-modal fusion network.}
    \label{fig:3}
\end{figure}

% dataset
\begin{table*}
\centering
\setlength{\tabcolsep}{0.9mm}
\begin{tabular}{cccccccccccccccc} 
\toprule
\multirow{2}{*}{\textbf{Split} ~} & \multicolumn{5}{c}{\textbf{MIMIC-CXR}} & \multicolumn{5}{c}{\textbf{MIMIC-ABN}} & \multicolumn{5}{c}{\textbf{Two-view CXR}} \\ 
\cmidrule(lr){2-6}\cmidrule(r){7-11}\cmidrule(lr){12-16}
 & \textbf{\#Img} & \textbf{\#Rep} & \textbf{\%Ind} & \textbf{\%PI} & \textbf{\%PR} & \textbf{\#Img} & \textbf{\#Rep} & \textbf{\%Ind} & \textbf{\%PI} & \textbf{\%PR} & \textbf{\#Img} & \textbf{\#Rep} & \textbf{\%Ind} & \textbf{\%PI} & \textbf{\%PR} \\ 
\midrule
Train & 239,998 & 150,957 & 66.4 & 60.5 & 60.5 & 69,641 & 34,763 & 64.6 & 52.7 & 52.7 & 181,312 & 90,656 & 67.1 & 51.6 & 51.6 \\
Val & 2,113 & 1,182 & 65.4 & 61.3 & 0.0 & 586 & 263 & 62.7 & 50.6 & 0.0 & 1,778 & 889 & 72.1 & 36.2 & 0.0 \\
Test & 3,852 & 2,343 & 57.3 & 87.7 & 0.0 & 844 & 378 & 56.3 & 89.2 & 0.0 & 3,000 & 1500 & 68.9 & 55.8 & 0.0 \\
\bottomrule
\end{tabular}
\caption{Statistics of the three datasets for the training, validation, and test sets. ``\#Img" and ``\#Rep" denote the number of images and reports, while ``\%Ind", ``\%PI", and ``\%PR" represent the ratios of ``INDICATION", ``previous image", and ``previous report", respectively.}
\label{table:0}
\end{table*}


\textbf{Multi-view longitudinal fusion network.} Due to the varying number of current multi-view images and the absence of previous images for some patients, integrating this information flexibly presents certain challenges. To address this issue, we design the multi-view longitudinal fusion (MLF) network, as illustrated in Figure \ref{fig:3}(A). We select the most recent previous visit to model temporal information, as it typically holds the highest reference value. To distinguish different time points, we integrate temporal positional embeddings into visual features, as depicted in Figure \ref{fig:2}. Subsequently, the spatiotemporal features ${\boldsymbol V}^{st}=\left \{ {\boldsymbol v}^{st}_1, {\boldsymbol v}^{st}_2,\dots ,{\boldsymbol v}^{st}_B\right \} $ are extracted using the MLF network:
\begin{align}
{\boldsymbol v}^{st}_i=\text{MLF} ({\boldsymbol v}_{i,a}^{cur},[{\boldsymbol v}_{i, \backslash a}^{cur},{\boldsymbol v}^{pri}_i]) \in {\mathbb{R}^{p \times d}},
\end{align}
where the anchor scan ${\boldsymbol v}_{i,a}^{cur}$ functions as the query, and the concatenation of auxiliary references and previous image, $[{\boldsymbol v}_{i, \backslash a}^{cur},{\boldsymbol v}^{pri}_i]$, serves as the key and value. Although the number of images for the current visit varies and some patients may lack previous images, we process one sample at a time, allowing for flexible adaptation to these changes.


\textbf{Instance-wise cross-modal alignment.} Radiology reports not only describe the current visit's condition but may also include comparisons with the patient's medical history. Therefore, relying solely on the current multi-view images and corresponding reports for cross-modal alignment could lead to model hallucinations. To address this, We first extract spatiotemporal features from multi-view longitudinal data using the multi-view longitudinal fusion network (see Figure \ref{fig:3}(A)). Subsequently, we utilize the inherent spatiotemporal information from radiology reports to supervise the pre-training of visual and textual representations. Inspired by CLIP \cite{radford-learning-clip} and MGCA \cite{wang-mgca}, we employ instance-wise cross-modal alignment to learn uni-modal representations. Specifically, we compute the image-to-text predicted categorical distribution ${\boldsymbol{q}}^{v2r} \in {\mathbb{R}^{B \times B}}$, defined as:
\begin{align}
{{\boldsymbol q}^{v2r}} = \frac{{\exp \left( {{{{{\boldsymbol {\bar V} }} \cdot {{\boldsymbol {\bar R}}^T}} \mathord{\left/ \right.
 } {{\tau _2}}}} \right)}}{{\sum\nolimits_{j = 1}^B {\exp \left( {{{{{\boldsymbol {\bar V}}} \cdot {{{\boldsymbol {\bar R}}_j^T}}} \mathord{\left/ \right.
 } {{\tau _2}}}} \right)} }}, 
\end{align}
where ${\boldsymbol {\bar V}}$ and ${\boldsymbol {\bar R}}$ denote global features of spatiotemporal features ${\boldsymbol {V}^{st}}$ and textual features ${\boldsymbol {R}}$, respectively. Similarly, we can also obtain the symmetric text-to-image predicted categorical distribution ${\boldsymbol{q}}^{r2v}$. Given that radiology report content may be consistent across visits, we consider both spatiotemporal and textual features from the same visit as positive pairs, as well as those from different visits with identical reports. Consequently, the image-to-text ground-truth categorical distribution ${\boldsymbol{p}}^{v2r} \in {\mathbb{R}^{B \times B}}$ is defined as:
\begin{align}
{\boldsymbol{p}_{i,j}^{g}} = \frac{{{\mathbb{I}_{identical}}\left( {{y_{i}^{cur}}, {y_{j}^{cur}}} \right)}}{{\sum\nolimits_{k = 1}^{B} {{\mathbb{I}_{identical}}\left( {{y_{i}^{cur}},{y_{k}^{cur}}} \right)} }},
\end{align}
where ${{\mathbb{I}_{identical}}\left( {\cdot,\cdot} \right)}$ denotes an indicator function that determines whether two radiology reports are identical. Finally, the cross-modal alignment loss is defined as:
\begin{align}
{{{\cal L}}_{G}} =  - \frac{1}{B}\sum\limits_{i = 1}^B {\left( {{\boldsymbol{p}}_i^{g}\log {\boldsymbol{q}}_i^{v2r} + {\boldsymbol{p}}_i^{g}\log {\boldsymbol{q}}_i^{r2v}} \right)}.
\end{align}


\textbf{Overall objective in Stage 1.} We train our MLRG by jointly optimizing ${\cal L}_{MPC}$ and ${\cal L}_{G}$, 
%which encourages the model to learn discriminative visual representation. The overall training objective is defined as:
formulated as:
\begin{align}
{{{\cal L}}_{pretrain}} = {{\cal L}}_{MPC} + {{\cal L}}_{G}.
\end{align}

%-------------------------------------------------------------------------

% main results
\begin{table*}
\centering
\setlength{\tabcolsep}{1.15mm}
\begin{tabular}{c|ccccccccccccc} 
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset}}} & \multirow{2}{*}{\textbf{Input}} & \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Venue}} & \multicolumn{6}{c}{\textbf{NLG Metrics} $\uparrow$} & \multicolumn{4}{c}{\textbf{CE Metrics} $\uparrow$} \\ 
\cmidrule(r){5-10}\cmidrule(r){11-14}
\multicolumn{1}{c}{} &  &  &  & \textbf{B-1} & \textbf{B-2} & \textbf{B-3} & \textbf{B-4} & \textbf{MTR} & \textbf{R-L} & \textbf{RG} & \textbf{P} & \textbf{R} & \textbf{F1} \\ 
\midrule
\multirow{14}{*}{M-CXR} & SI & SA \cite{yan2023style} & EMNLP'23 & - & 0.184 & - & - & - & - & 0.228 & - & - & 0.394 \\
 & SI & MET \cite{wang2023metransformer} & CVPR'23 & 0.386 & 0.250 & 0.169 & 0.124 & 0.152 & 0.291 & - & 0.364 & 0.309 & 0.311 \\
 & SI & KiUT \cite{huang-kiut} & CVPR'23 & 0.393 & 0.243 & 0.159 & 0.113 & 0.160 & 0.285 & - & 0.371 & 0.318 & 0.321 \\
 & SI & CoFE \cite{cofe-eccv-24} & ECCV'24 & - & - & - & 0.125 & \textbf{0.176} & \underline{0.304} &  & 0.489 & 0.370 & 0.405 \\
 & SI & MAN \cite{shen2024automatic_aaai} & AAAI'24 & 0.396 & 0.244 & 0.162 & 0.115 & 0.151 & 0.274 & - & 0.411 & 0.398 & 0.389 \\
 & SI & B-LLM \cite{aaai-liu2024bootstrapping-llm} & AAAI'24 & \underline{0.402} & \underline{0.262} & \underline{0.180} & 0.128 & \underline{0.175} & 0.291 & - & 0.465 & \textbf{0.482} & \underline{0.473} \\
 & SI & DCG \cite{liang2024divide-acmmm-24} & ACMMM'24 & 0.397 & 0.258 & 0.166 & 0.126 & 0.162 & 0.295 & - & 0.441 & 0.414 & 0.404 \\
 & SI & Med-LLM \cite{liu2024in-context-acmmm} & ACMMM'24 & - & - & - & 0.128 & 0.161 & 0.289 & - & 0.412 & 0.373 & 0.395 \\
 & SI+Ind & SEI \cite{sei} & MICCAI'24 & 0.382 & 0.247 & 0.177 & \underline{0.135} & 0.158 & 0.299 & \underline{0.249} & \underline{0.523} & 0.410 & 0.460 \\
 & MVD & FMVP \cite{tmm_mulview_2024-fmvp} & TMM'23 & 0.389 & 0.236 & 0.156 & 0.108 & 0.150 & 0.284 & - & 0.332 & 0.383 & 0.336 \\
 & Long & HERGen \cite{2024-eccv-hergen} & ECCV'24 & 0.395 & 0.248 & 0.169 & 0.122 & 0.156 & 0.285 & - & - & - & - \\
 & MVL & CXRMate \cite{nicolson2023-longitudinal-multiview} & arXiv'23 & 0.361 & 0.223 & 0.150 & 0.108 & 0.159 & 0.263 & 0.238 & 0.495 & 0.367 & 0.422 \\
 & MVL & \textbf{MLRG(Ours)} & - & \textbf{0.411} & \textbf{0.277} & \textbf{0.204} & \textbf{0.158} & \textbf{0.176} & \textbf{0.320} & \textbf{0.291} & \textbf{0.549} & \underline{0.468} & \textbf{0.505} \\ 
\cline{2-14}
 & - & $\Delta$ (\%) $\uparrow$ & - & +0.9 & +1.5 & +2.4 & +2.3 & +0.1 & +1.6 & +4.2 & +2.6 & -1.4 & +3.2 \\ 
\cmidrule(r){1-14}
\multirow{5}{*}{M-ABN} & SI & R2Gen$^\flat$ \cite{chen-etal-2020-generating} & EMNLP'20 & 0.253 & 0.144 & 0.092 & 0.063 & 0.106 & 0.229 & 0.179 & 0.444 & 0.425 & 0.434 \\
 & SI & CMN$^\flat$ \cite{chen-etal-2021-cross-modal} & ACL'21 & 0.256 & 0.147 & 0.095 & 0.066 & 0.110 & 0.230 & 0.183 & \underline{0.466} & \underline{0.454} & \underline{0.460} \\
 & SI+Ind & SEI$^\flat$ \cite{sei} & MICCAI'24 & \underline{0.267} & \underline{0.157} & \underline{0.104} & \underline{0.073} & \underline{0.114} & \underline{0.231} & \underline{0.191} & \underline{0.466} & 0.408 & 0.435 \\
 & MVL & \textbf{MLRG(Ours)} & - & \textbf{0.332} & \textbf{0.199} & \textbf{0.132} & \textbf{0.094} & \textbf{0.136} & \textbf{0.248} & \textbf{0.219} & \textbf{0.513} & \textbf{0.517} & \textbf{0.515} \\ 
\cline{2-14}
 & - & $\Delta$ (\%) $\uparrow$ & - & +6.5 & +4.2 & +2.8 & +2.1 & +2.2 & +1.7 & +2.8 & +4.7 & +6.3 & +5.5 \\ 
\cmidrule(r){1-14}
\multirow{5}{*}{T-CXR} & DV & R2Gen$^\flat$ \cite{chen-etal-2020-generating} & EMNLP'20 & 0.346 & 0.219 & 0.153 & 0.113 & 0.141 & 0.302 & 0.267 & 0.478 & 0.329 & 0.390 \\
 & DV & CMN$^\flat$ \cite{chen-etal-2021-cross-modal} & ACL'21 & 0.387 & 0.241 & 0.166 & 0.122 & 0.151 & 0.310 & 0.268 & 0.496 & 0.336 & 0.401 \\
 & DV+Ind & SEI$^\flat$ \cite{sei} & MICCAI'24 & \underline{0.409} & \underline{0.263} & \underline{0.186} & \underline{0.140} & \underline{0.168} & \underline{0.320} & \underline{0.301} & \underline{0.522} & \underline{0.447} & \underline{0.481} \\
 & MVL & \textbf{MLRG(Ours)} & - & \textbf{0.417} & \textbf{0.276} & \textbf{0.200} & \textbf{0.154} & \textbf{0.178} & \textbf{0.331} & \textbf{0.328} & \textbf{0.532} & \textbf{0.474} & \textbf{0.501} \\ 
\cline{2-14}
 & - & $\Delta$ (\%) $\uparrow$ & - & +0.8 & +1.3 & +1.4 & +1.4 & +1.0 & +1.1 & +2.7 & +1.0 & +2.7 & +2.0  \\
\bottomrule
\end{tabular}
\caption{Comparison with SOTA methods on MIMIC-CXR (M-CXR), MIMIC-ABN (M-ABN), and Two-view CXR (T-CXR) datasets. \(\Delta\) denotes the performance difference between MLRG and the best peer methods. \({\flat}\) signifies results reproduced using official codes, while other results are sourced from original publications. The best and second-best values are emphasized in \textbf{bold} and \underline{underlined}, respectively.}
\label{table:1}
\end{table*}


\subsection{Chest X-ray Report Generation}
\textbf{Integrating patient-specific prior knowledge into the text generator.} Radiologists commonly refer to patient-specific prior knowledge, such as the ``INDICATION" (which outlines the visit reasons or symptoms) and the ``previous report" (which provides the patient's medical history), when drafting radiology reports. However, these data may be absent for some patients due to de-identification or incomplete records. To combat this issue, we propose a tokenized absence encoding technique to handle missing patient-specific prior knowledge, as shown in Figure \ref{fig:2}. Specifically,  for missing ``INDICATION" and ``previous report", we utilize special tokens, ``[NHI]" and ``[NHPR]", to simulate their presence. For existing  ``INDICATION", we apply a preprocessing strategy from SEI \cite{sei} to remove de-identification noise (e.g., \textit{-year-old}, \_\_\_, and \textbackslash\textbackslash). For available ``previous report", we employ the structural entities approach \cite{fse} to extract factual serialization, enabling the model to focus on clinically relevant details. We then combine the cleaned ``INDICATION" with factual serialization extracted from the ``previous report" into a cohesive paragraph, supplemented by segment embeddings to help the model distinguish between different sentence meanings.  Finally, we utilize the multi-modal fusion network \cite{chen-ptunifier} (illustrated in Figure \ref{fig:3}(B)) to flexibly integrate patient-specific prior knowledge into the spatiotemporal features ${\boldsymbol V}^{st}$, allowing the text generator to generate more accurate radiology reports based on available prior knowledge.
%providing the text generator with patient-specific prior knowledge.

\textbf{Report generation.} We start by initializing the projection heads, MLF network, and CXR-BERT using the model trained in Stage 1. Following prestigious works \cite{nicolson-improving-cvt2distilgpt2,2024-eccv-hergen}, we treat the DistilGPT2 \cite{Sanh2019DistilBERTAD}, initialized by CGPT2 \cite{nicolson-improving-cvt2distilgpt2}, as the text generator. We then minimize the cross-entropy loss ${\cal L}_{CE}$ to ensure that the generated reports closely align with the reference reports.




% maximizing the log-likelihood as follows:

% \begin{align}
% {{{\cal L}}_{lm_i}} = \sum\limits_{t = 1}^L {\log {{P_{\theta}}({\left. {\tilde y_{i, t}^{cur}} \right| x_i^{pri},y_i^{pri},{z_i},x_i^{cur}, \tilde y_{i,j<t}^{cur}})}}
% \end{align}
% where $L$ represents 
%-------------------------------------------------------------------------