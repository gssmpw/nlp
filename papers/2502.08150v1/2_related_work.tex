\section{Related work} \label{sec:related_work}

\paragraph{Generating Models.}
Generative models have made significant progress over the last decade, enabling diverse applications such as image synthesis, text generation, and data augmentation. One of the foundational models in this area is the Generative Adversarial Network (GAN) introduced by Goodfellow et al. (2014), which consists of a generator and a discriminator that compete in a zero-sum game, thereby leading to the generation of realistic data samples \cite{goodfellow2014generative}. GANs have inspired a variety of derivative architectures aimed at improving stability and quality, including Wasserstein GANs (WGAN), which address the instability issues of GANs by employing a different distance metric \cite{arjovsky2017wasserstein}. Conditional GANs (cGANs) extend the GAN framework to generate data conditioned on additional information, making them more controllable \cite{mirza2014conditional}. 

Variational Autoencoders (VAEs), proposed by Kingma and Welling (2013), offer another generative approach that combines variational inference with neural networks to learn a latent variable model of data \cite{kingma2013auto}. Unlike GANs, VAEs maximize a lower bound on the log-likelihood of the data, allowing for a more principled probabilistic interpretation. The introduction of the reparameterization trick was key to making VAEs feasible to train with stochastic gradient descent, which has had a considerable impact on the field of deep generative models. Recently, autoregressive models such as PixelCNN \cite{oord2016pixel} and Transformer-based models \cite{vaswani2017attention} have demonstrated impressive performance in tasks like image and text generation. These models learn to predict the next element in a sequence, thereby allowing them to generate samples one step at a time, which has proven particularly effective for generating sequential data such as text and audio. 

The development of large-scale language models like GPT-3 \cite{brown2020language} has further showcased the power of autoregressive architectures in generating coherent and contextually relevant long text, significantly advancing the state-of-the-art in natural language processing. Another line of work explores diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs), which have gained attention for their ability to generate high-quality images by modeling the process of gradually adding noise to data and then learning to reverse this process \cite{ho2020denoising}. These models provide an alternative to GANs by optimizing likelihood-based objectives, which makes training more stable. DDPMs have set new benchmarks for image generation quality, rivaling the output of GANs while avoiding some of their training difficulties. These developments collectively showcase the evolution of generative models from adversarial training with GANs to likelihood-based training with VAEs, autoregressive models, e.g, Visual autoregressive modeling (VAR) \cite{tjy+24}, and diffusion-based approaches, e.g., DDPM \cite{ho2020denoising}. Each of these methods contributes unique strengths and capabilities, advancing the scope and quality of generated data.

\paragraph{Flow Matching.}
Flow matching \cite{dlt+24,ylp+24,gdb+24,cgl+25_homo} is a key concept in fields such as optimal transport, computer vision, and machine learning, where it has been extensively studied and utilized to align two distributions effectively. The method has roots in the classic work on optimal transport theory, where Monge and Kantorovich initially laid out the foundational ideas for mapping mass distributions with minimal cost \cite{monge1781memoire, kantorovich1942transfer}. Building upon these ideas, Villani expanded the theoretical framework of optimal transport, leading to a rigorous mathematical foundation for flow matching and its related applications \cite{villani2008optimal}. 

Recent advances in machine learning have leveraged flow matching for deep generative modeling tasks. Denoising diffusion probabilistic models (DDPMs), for example, have drawn inspiration from flow-based methods to improve the stability and efficiency of training \cite{ho2020denoising}. Similarly, score-based generative models utilize a stochastic differential equation approach to approximate flows, effectively creating a flow-matching procedure for generating realistic data samples \cite{song2021score}. This approach has demonstrated considerable success in capturing complex, high-dimensional data distributions. Another relevant development in this domain is the introduction of continuous normalizing flows (CNFs) by Chen et al., which formulated generative modeling as a continuous-time flow process, further refining flow-matching techniques for density estimation and improving scalability \cite{chen2018neural}. Grathwohl et al. expanded upon this idea by demonstrating how flow matching could be combined with probability density estimation to achieve more efficient generative models \cite{grathwohl2018ffjord}. These works have collectively highlighted the flexibility of flow matching as a tool for a wide range of machine learning tasks, including unsupervised learning, density estimation, and data synthesis. Moreover, applications in computer vision often rely on flow matching to solve challenging problems such as image registration and optical flow estimation. For instance, deep learning-based approaches have integrated flow matching concepts to align images effectively, demonstrating significant improvements over traditional techniques \cite{dosovitskiy2015flownet, ilg2017flownet2}. FlowNet and its successor FlowNet2 provide compelling evidence of how flow matching can be operationalized within deep neural architectures to solve real-world vision tasks with state-of-the-art accuracy. Video Latent Flow Matching \cite{csy25,jsl+24,dsf23} incorporates flow matching for temporally coherent video generation.
Moreover, numerous recent works \cite{zcwt23,lss+24_relu,cls+24,lss+24_multi_layer,wxz+24,wcz+23,cgl+25_homo,xlc+24,wcy+23,sph+23,cxj24,fjl+24,kll+25,kll+25_tc,cll+25_var,kls+25_dpbloom,lsss24_dpntk,cll+25_deskrej,lssz24_gm,llss24_softmax,lzw+24,hwsl24,hwl+24,ssz+25_dit,ssz+25_prune} have significantly inspired and influenced our work.

