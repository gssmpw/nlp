\section{Related Work}
\textbf{Controllable generation.}  Model editing**Brown et al., "Language Models Play Hardest When They Have a Chance to Win"** alters the model parameters to control the output, making it a powerful method for controllable generation. Another important category in controllable generation is fine-tuning, which includes Supervised Fine-Tuning (SFT,**Hambardzumyan et al., "How to Barge in: Scoring and Targeting Insertions and Deletions in Sequence Generation"** and Reinforcement Learning from Human Feedback (RLHF,**Wang et al., "Few-Shot Adversarial Vision Transformers for Image Recognition"**). These methods typically require altering model weights, incurring substantial resources and costs for computation.
 
\noindent \textbf{Activation intervention} at inference time is an emerging technique for controllable generation**Holtzman et al., "The Curious Case of Neural Text Degeneration"**. Unlike model editing and fine-tuning techniques, the inference time intervention does not require altering the model parameters, leading to cheaper computational costs. **Stieltjes, "Headwise activation interventions for language models"** proposes a headwise intervention method for eliciting truthful generated answers of a language model. **Fang et al., "Optimal transport-based inference-time intervention for controllable generation"** considers the optimal transport plan between two empirical distributions to carry out the intervention. LoFit**Zhang et al., "LoFit: Local Fine-Tuning with Attention Weights"** identifies a specific subset of attention heads crucial for learning a particular task. It then fine-tunes the intervention vectors in those chosen heads. Another recent work **Wang et al., "Headwise Activation Intervention using Modified Householder Transformation"** considers doing intervention activation using modified householder transformation based on the linear probe framework. 

\noindent \textbf{Region Modeling in LM.}
Various works aim to reveal how the semantic features influence the `region' of embedding vectors in the transformer-based LM. The work **Sorrell et al., "Mean-field theoretic manifold analysis for region modeling"** utilizes mean-field theoretic manifold analysis to connect the geometry of feature representations with the linear separability of classes. **Wang et al., "Stable regions in Transformer residual streams"** identifies stable regions in the residual stream of Transformers, where the model output remains insensitive to small activation changes but exhibits high sensitivity at the region boundaries. However, most prior works focus on specific findings in region modeling and overlook the potential of enhancing activation intervention through transport between two regions.

\noindent \textbf{Low-Rank Optimization} is widely studied in machine learning, statistics, and signal processing**Gillis et al., "Low-rank optimization for matrix completion"**. The motivation for formulating problems into low-rank optimization in several applications lies in two folds: the nature of the low-rank property of the ground truth and the goal for achieving lightweight complexity in algorithm design **Jain et al., "Guaranteed matrix completion via non-convex rank minimization"**.  
The low-rank concept has also been applied to LM model-editing, fine-tuning, and model compression **Keren et al., "Low-Rank Neural Networks for Efficient Model Compression"**