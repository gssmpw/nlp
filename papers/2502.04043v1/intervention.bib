@article{ref:subramani2022extracting,
  title={Extracting latent steering vectors from pretrained language models},
  author={Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E},
  journal={arXiv preprint arXiv:2205.05124},
  year={2022}
}


@article{ref:radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford and others},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@article{ref:jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{ref:touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@article{ref:achiam2023gpt,
  title={GPT-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{ref:brown2020language,
  title={Language models are few-shot learners},
  author={Brown and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{ref:belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@inproceedings{ref:tenney2019bert,
  title={{BERT} Rediscovers the Classical {NLP} Pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}


@article{ref:alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}


@inproceedings{ref:zhang2019bertscore,
  title={{BERTScore}: Evaluating Text Generation with {BERT}},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{ref:vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@article{ref:benedict2022sigmoidf1,
  title={{sigmoidF1}: A Smooth {F1} Score Surrogate Loss for Multilabel Classification},
  author={B{\'e}n{\'e}dict, Gabriel and Koops, Hendrik Vincent and Odijk, Daan and de Rijke, Maarten},
  journal={Transactions on Machine Learning Research},
  year={2022}
}



@article{ref:singh2024mimic,
  title={Mi{M}i{C}: Minimally modified counterfactuals in the representation space},
  author={Singh, Shashwat and Ravfogel, Shauli and Herzig, Jonathan and Aharoni, Roee and Cotterell, Ryan and Kumaraguru, Ponnurangam},
  journal={arXiv preprint arXiv:2402.09631},
  year={2024}
}


@article{ref:zou2023representation,
  title={Representation engineering: A top-down approach to {AI} transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{ref:burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{ref:marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{ref:azaria2023internal,
  title={The internal state of an LLM knows when its lying},
  author={Azaria, Amos and Mitchell, Tom},
  journal={arXiv preprint arXiv:2304.13734},
  year={2023}
}

@article{ref:li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ref:hernandez2023measuring,
  title={Measuring and manipulating knowledge representations in language models},
  author={Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
  journal={arXiv preprint arXiv:2304.00740},
  year={2023}
}

@article{ref:moschella2023relative,
      title={Relative representations enable zero-shot latent space communication}, 
      author={Luca Moschella and Valentino Maiorca and Marco Fumero and Antonio Norelli and Francesco Locatello and Emanuele Rodolà},
      year={2023},
      eprint={2209.15430},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{ref:meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{ref:mallen2023eliciting,
  title={Eliciting Latent Knowledge from Quirky Language Models},
  author={Mallen, Alex and Belrose, Nora},
  journal={arXiv preprint arXiv:2312.01037},
  year={2023}
}


@InProceedings{ref:brandon2017input,
  title = 	 {Input Convex Neural Networks},
  author =       {Brandon Amos and Lei Xu and J. Zico Kolter},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {146--155},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/amos17b/amos17b.pdf},
  url = 	 {https://proceedings.mlr.press/v70/amos17b.html},
}

@inproceedings{ref:makkuva2020optimal,
  title={Optimal transport mapping via input convex neural networks},
  author={Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6672--6681},
  year={2020},
  organization={PMLR}
}

@article{ref:lin2021truthfulqa,
  title={{TruthfulQA}: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{ref:wu2024reft,
  title={{ReFT}: Representation Finetuning for Language Models},
  author={Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:2404.03592},
  year={2024}
}

@misc{ref:bhardwaj2023redteaming,
      title={Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment}, 
      author={Rishabh Bhardwaj and Soujanya Poria},
      year={2023},
      eprint={2308.09662},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{ref:prekopa1995stochastic,
  title={Stochastic Programming},
  author={Pr{\'e}kopa, Andr{\'a}s},
  year={1995},
  publisher={Springer Science \& Business Media}
}

@article{ref:taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@article{ref:chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing {GPT}-4 with 90\% {ChatGPT} quality},
  author={Chiang and others},
    journal={See https://vicuna.lmsys.org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}
@article{nguyen2025risk,
  title={Risk-Aware Distributional Intervention Policies for Language Models},
  author={Nguyen, Bao and Nguyen, Binh and Nguyen, Duy and Nguyen, Viet Anh},
  journal={arXiv preprint arXiv:2501.15758},
  year={2025}
}
@article{ref:touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models.},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{ref:peng2023instruction,
  title={Instruction tuning with {GPT}-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{ref:ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{ref:wang2023knowledge,
  title={Knowledge editing for large language models: A survey},
  author={Wang and others},
  journal={arXiv preprint arXiv:2310.16218},
  year={2023}
}

@article{ref:zhang2024comprehensive,
  title={A comprehensive study of knowledge editing for large language models},
  author={Zhang and others},
  journal={arXiv preprint arXiv:2401.01286},
  year={2024}
}

@article{ref:gunel2020supervised,
  title={Supervised contrastive learning for pre-trained language model fine-tuning},
  author={Gunel, Beliz and Du, Jingfei and Conneau, Alexis and Stoyanov, Ves},
  journal={arXiv preprint arXiv:2011.01403},
  year={2020}
}

@article{ref:griffith2013policy,
  title={Policy shaping: Integrating human feedback with reinforcement learning},
  author={Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@article{ref:zhang2023survey,
  title={A survey of controllable text generation using transformer-based pre-trained language models},
  author={Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
  journal={ACM Computing Surveys},
  volume={56},
  number={3},
  pages={1--37},
  year={2023},
  publisher={ACM New York, NY}
}

@article{ref:li2024pre,
  title={Pre-trained Language Models for Text Generation: A Survey},
  author={Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={ACM Computing Surveys},
  volume={56},
  number={9},
  pages={1--39},
  year={2024},
  publisher={ACM New York, NY}
}

@article{ref:hase2024does,
  title={Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{beck2003mirror,
	title={Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization},
	author={Beck, Amir and Teboulle, Marc},
	journal={Operations Research Letters},
	volume={31},
	number={3},
	pages={167--175},
	year={2003},
	publisher={Elsevier}
}

@inproceedings{duchi2008efficient,
  title={Efficient projections onto the $l_1$-ball for learning in high dimensions},
  author={Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
  booktitle={Proceedings of the 25th International Conference on Machine learning},
  pages={272--279},
  year={2008}
}

@inproceedings{tu2016low,
  title={Low-rank solutions of linear matrix equations via {P}rocrustes {F}low},
  author={Tu, Stephen and Boczar, Ross and Simchowitz, Max and Soltanolkotabi, Mahdi and Recht, Ben},
  booktitle={International Conference on Machine Learning},
  pages={964--973},
  year={2016},
  organization={PMLR}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013}
}

@article{tong2021accelerating,
  title={Accelerating ill-conditioned low-rank matrix estimation via scaled gradient descent},
  author={Tong, Tian and Ma, Cong and Chi, Yuejie},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={150},
  pages={1--63},
  year={2021}
}

@article{hsieh2014quic,
  title={{QUIC}: Quadratic Approximation for Sparse Inverse Covariance Estimation},
  author={Hsieh, Cho-Jui and Sustik, Máté A. and Dhillon, Inderjit S. and Ravikumar, Pradeep},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={2911--2947},
  year={2014}
}

@article{hartlap2007your,
  title={Why your model parameter confidences might be too optimistic. Unbiased estimation of the inverse covariance matrix},
  author={Hartlap, Jan and Simon, Patrick and Schneider, Peter},
  journal={Astronomy \& Astrophysics},
  volume={464},
  number={1},
  pages={399--404},
  year={2007},
  publisher={EDP Sciences}
}

@article{yin2024lofit,
  title={LoFiT: Localized Fine-tuning on {LLM} Representations},
  author={Yin, Fangcong and Ye, Xi and Durrett, Greg},
  journal={arXiv preprint arXiv:2406.01563},
  year={2024}
}

@article{chi2019nonconvex,
  title={Nonconvex optimization meets low-rank matrix factorization: An overview},
  author={Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={20},
  pages={5239--5269},
  year={2019},
  publisher={IEEE}
}

@article{hu2021lora,
  title={Lo{RA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{hsu2022language,
  title={Language model compression with weighted low-rank factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:2207.00112},
  year={2022}
}

@article{de2000mahalanobis,
  title={The {M}ahalanobis distance},
  author={De Maesschalck, Roy and Jouan-Rimbaud, Delphine and Massart, D{\'e}sir{\'e} L},
  journal={Chemometrics and Intelligent Laboratory Systems},
  volume={50},
  number={1},
  pages={1--18},
  year={2000},
  publisher={Elsevier}
}

@article{friedman2008sparse,
  title={Sparse inverse covariance estimation with the graphical lasso},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  journal={Biostatistics},
  volume={9},
  number={3},
  pages={432--441},
  year={2008},
  publisher={Oxford University Press}
}

@article{ledoit2004well,
  title={A well-conditioned estimator for large-dimensional covariance matrices},
  author={Ledoit, Olivier and Wolf, Michael},
  journal={Journal of Multivariate Analysis},
  volume={88},
  number={2},
  pages={365--411},
  year={2004},
  publisher={Elsevier}
}

@article{schafer2005shrinkage,
  title={A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics},
  author={Sch{\"a}fer, Juliane and Strimmer, Korbinian},
  journal={Statistical Applications in Genetics and Molecular Biology},
  volume={4},
  number={1},
  year={2005},
  publisher={De Gruyter}
}

@article{ref:ghandeharioun2024patchscope,
  title={Patchscope: A unifying framework for inspecting hidden representations of language models},
  author={Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  journal={arXiv preprint arXiv:2401.06102},
  year={2024}
}

@article{ref:chen2024selfie,
  title={Selfie: Self-interpretation of large language model embeddings},
  author={Chen, Haozhe and Vondrick, Carl and Mao, Chengzhi},
  journal={arXiv preprint arXiv:2403.10949},
  year={2024}
}

@article{ref:hernandez2023inspecting,
  title={Inspecting and editing knowledge representations in language models},
  author={Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
  journal={arXiv preprint arXiv:2304.00740},
  year={2023}
}


@inproceedings{ref:von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}


@inproceedings{ref:dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}


@article{ref:bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}


@inproceedings{ref:akyurek2022learning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@inproceedings{ref:yoo2022ground,
  title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author={Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-Goo and Kim, Taeuk},
  booktitle={2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022},
  pages={2422--2437},
  year={2022},
  organization={Association for Computational Linguistics (ACL)}
}


@inproceedings{ref:reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}

@article{ref:anthropic2024claude,
  title={The {C}laude 3 model family: Opus, {S}onnet, {H}aiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}


@article{ref:abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{ref:achiam2023gpt,
  title={GPT-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{ref:von2023uncovering,
  title={Uncovering mesa-optimization algorithms in transformers},
  author={von Oswald, Johannes and Niklasson, Eyvind and Schlegel, Maximilian and Kobayashi, Seijin and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and Pascanu, Razvan and others},
  journal={arXiv preprint arXiv:2309.05858},
  year={2023}
}

@article{ref:sander2024transformers,
  title={How do Transformers perform In-Context Autoregressive Learning?},
  author={Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2402.05787},
  year={2024}
}

@inproceedings{ref:wang2023label,
  title={Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning},
  author={Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9840--9855},
  year={2023}
}


@article{ref:todd2023function,
  title={Function vectors in large language models},
  author={Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David},
  journal={arXiv preprint arXiv:2310.15213},
  year={2023}
}


@inproceedings{ref:shin2020autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "4222--4235",
}


@inproceedings{ref:wen2023hard,
 author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {51008--51025},
 publisher = {Curran Associates, Inc.},
 title = {Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
 volume = {36},
 year = {2023}
}

@article{ref:zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@inproceedings{ref:hendel2023incontext,
    title = "In-Context Learning Creates Task Vectors",
    author = "Hendel, Roee  and
      Geva, Mor  and
      Globerson, Amir",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "9318--9333",
}


@article{ref:thompson2024fluent,
  title={Fluent dreaming for language models},
  author={Thompson, T Ben and Straznickas, Zygimantas and Sklar, Michael},
  journal={arXiv preprint arXiv:2402.01702},
  year={2024}
}


@article{ref:takemoto2024all,
  title={All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks},
  author={Takemoto, Kazuhiro},
  journal={Applied Sciences},
  volume={14},
  number={9},
  pages={3558},
  year={2024},
  publisher={MDPI}
}


@article{ref:fan2024not,
  title={Not all layers of {LLM}s are necessary during inference},
  author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
  journal={arXiv preprint arXiv:2403.02181},
  year={2024}
}

@inproceedings{ref:hernandez2024linearity,
    title={Linearity of Relation Decoding in Transformer Language Models}, 
    author={Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau},
    booktitle={Proceedings of the 2024 International Conference on Learning Representations},
    year={2024},
}


@article{ref:jorgensen2023improving,
  title={Improving activation steering in language models with mean-centring},
  author={Jorgensen, Ole and Cope, Dylan and Schoots, Nandi and Shanahan, Murray},
  journal={arXiv preprint arXiv:2312.03813},
  year={2023}
}

@article{ref:li2024context,
  title={In-Context Learning State Vector with Inner and Momentum Optimization},
  author={Li, Dongfang and Liu, Zhenyu and Hu, Xinshuo and Sun, Zetian and Hu, Baotian and Zhang, Min},
  journal={arXiv preprint arXiv:2404.11225},
  year={2024}
}

@article{ref:bai2024analyzing,
  title={Analyzing Task-Encoding Tokens in Large Language Models},
  author={Bai, Yu and Huang, Heyan and Piano, Cesare Spinoso-Di and Rondeau, Marc-Antoine and Chen, Sanxing and Gao, Yang and Cheung, Jackie Chi Kit},
  journal={arXiv preprint arXiv:2401.11323},
  year={2024}
}


@article{ref:liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@article{ref:ferrando2024primer,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024}
}

@article{ref:santurkar2023whose,
    title={Whose Opinions Do Language Models Reflect?},
    author={Shibani Santurkar and Esin Durmus and Faisal Ladhak and Cinoo Lee and Percy Liang and Tatsunori Hashimoto},
    year={2023},
    journal={arXiv preprint arXiv:2303.17548},
}

@article{ref:zhao2023group,
  title={Group preference optimization: Few-shot alignment of large language models},
  author={Zhao, Siyan and Dang, John and Grover, Aditya},
  journal={arXiv preprint arXiv:2310.11523},
  year={2023}
}



@article{shefi2016rate,
  title={On the rate of convergence of the proximal alternating linearized minimization algorithm for convex problems},
  author={Shefi, Ron and Teboulle, Marc},
  journal={EURO Journal on Computational Optimization},
  volume={4},
  pages={27--46},
  year={2016},
  publisher={Springer}
}


@article{yu2013decomposing,
  title={On decomposing the proximal map},
  author={Yu, Yao-Liang},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{shi2016primer,
  title={A primer on coordinate descent algorithms},
  author={Shi, Hao-Jun Michael and Tu, Shenyinying and Xu, Yangyang and Yin, Wotao},
  journal={arXiv preprint arXiv:1610.00040},
  year={2016}
}

@article{zhang2020efficient,
  title={An efficient Hessian based algorithm for solving large-scale sparse group Lasso problems},
  author={Zhang, Yangjing and Zhang, Ning and Sun, Defeng and Toh, Kim-Chuan},
  journal={Mathematical Programming},
  volume={179},
  pages={223--263},
  year={2020},
  publisher={Springer}
}
@article{xu2017globally,
  title={A globally convergent algorithm for nonconvex optimization based on block coordinate update},
  author={Xu, Yangyang and Yin, Wotao},
  journal={Journal of Scientific Computing},
  volume={72},
  number={2},
  pages={700--734},
  year={2017},
  publisher={Springer}
}

@article{pham2024householder,
  title={Householder Pseudo-Rotation: A Novel Approach to Activation Editing in {LLM}s with Direction-Magnitude Perspective},
  author={Pham, Van-Cuong and Nguyen, Thien Huu},
  journal={arXiv preprint arXiv:2409.10053},
  year={2024}
}

@article{Tong2021AcceleratingIL,
  title={Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent},
  author={Tian Tong and Cong Ma and Yuejie Chi},
  journal={Journal of Machine Learning Research},
  year={2021},
  volume={22},
  pages={1-63}
}

@article{cory2021new,
  title={A new perspective on low-rank optimization},
  author={Cory-Wright, Ryan and Pauphilet, Jean and Lodi, Andrea},
  journal={arXiv preprint arXiv:2105.05947},
  year={2021}
}

@article{olikier2022low,
  title={Low-rank optimization methods based on projected-projected gradient descent that accumulate at Bouligand stationary points},
  author={Olikier, Guillaume and Absil, P-A and Hendrickx, Julien M},
  journal={arXiv preprint arXiv:2201.03962},
  year={2022}
}

@article{zhanxuan2020low,
  title={Low Rank Regularization: A review},
  author={Zhanxuan, Hu and Feiping, Nie and Rong, Wang and Xuelong, Li},
  journal={Information Sciences},
  year={2020}
}

@article{mamou2020emergence,
  title={Emergence of separable manifolds in deep language representations},
  author={Mamou, Jonathan and Le, Hang and Del Rio, Miguel and Stephenson, Cory and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  journal={arXiv preprint arXiv:2006.01095},
  year={2020}
}

@inproceedings{bhojanapalli2016dropping,
  title={Dropping convexity for faster semi-definite optimization},
  author={Bhojanapalli, Srinadh and Kyrillidis, Anastasios and Sanghavi, Sujay},
  booktitle={Conference on Learning Theory},
  pages={530--582},
  year={2016},
  organization={PMLR}
}

@article{janiak2024characterizing,
  title={Characterizing stable regions in the residual stream of {LLM}s},
  author={Janiak, Jett and Karwowski, Jacek and Mangat, Chatrik Singh and Giglemiani, Giorgi and Petrova, Nora and Heimersheim, Stefan},
  journal={arXiv preprint arXiv:2409.17113},
  year={2024}
}

@article{hong2015unified,
  title={A unified algorithmic framework for block-structured optimization involving big data: With applications in machine learning and signal processing},
  author={Hong, Mingyi and Razaviyayn, Meisam and Luo, Zhi-Quan and Pang, Jong-Shi},
  journal={IEEE Signal Processing Magazine},
  volume={33},
  number={1},
  pages={57--77},
  year={2015},
  publisher={IEEE}
}

@article{zangwill1969nonlinear,
  title={Nonlinear programming: a unified approach},
  author={Zangwill, Willard I},
  journal={(No Title)},
  year={1969}
}

@article{zhang2024riemannian,
  title={Riemannian Preconditioned {L}o{R}{A} for Fine-Tuning Foundation Models},
  author={Zhang, Fangzhao and Pilanci, Mert},
  journal={arXiv preprint arXiv:2402.02347},
  year={2024}
}

@article{Ioffe2002OnAC,
  title={On a Characterization of ${C}^{1,1}$ Functions},
  author={A. Ioffe and T. Milosz},
  journal={Cybernetics and Systems Analysis},
  year={2002},
  volume={38},
  pages={838-842},
  publisher={Springer}
}

@article{mcrae2024benign,
  title={Benign landscapes of low-dimensional relaxations for orthogonal synchronization on general graphs},
  author={McRae, Andrew D and Boumal, Nicolas},
  journal={SIAM Journal on Optimization},
  volume={34},
  number={2},
  pages={1427--1454},
  year={2024},
  publisher={SIAM}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{ref:turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}


@article{ref:ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}


@article{ref:xu2024hallucination,
  title={Hallucination is inevitable: An innate limitation of large language models},
  author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2401.11817},
  year={2024}
}


@article{ref:rawte2023troubling,
  title={The troubling emergence of hallucination in large language models--an extensive definition, quantification, and prescriptive remediations},
  author={Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, SM and Chadha, Aman and Sheth, Amit P and Das, Amitava},
  journal={arXiv preprint arXiv:2310.04988},
  year={2023}
}


@article{ref:tonmoy2024comprehensive,
  title={A comprehensive survey of hallucination mitigation techniques in large language models},
  author={Tonmoy, SM and Zaman, SM and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  journal={arXiv preprint arXiv:2401.01313},
  year={2024}
}


@article{ref:kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{ref:achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{ref:belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{ref:alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}


@inproceedings{ref:zhang2019bertscore,
  title={{BERTScore}: Evaluating Text Generation with {BERT}},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{ref:vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@article{ref:benedict2022sigmoidf1,
  title={{sigmoidF1}: A Smooth {F1} Score Surrogate Loss for Multilabel Classification},
  author={B{\'e}n{\'e}dict, Gabriel and Koops, Hendrik Vincent and Odijk, Daan and de Rijke, Maarten},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{ref:zou2023representation,
  title={Representation engineering: A top-down approach to {AI} transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{ref:marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{ref:meng2022locating,
  title={Locating and editing factual associations in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{ref:mallen2023eliciting,
  title={Eliciting Latent Knowledge from Quirky Language Models},
  author={Mallen, Alex and Belrose, Nora},
  journal={arXiv preprint arXiv:2312.01037},
  year={2023}
}

@inproceedings{ref:makkuva2020optimal,
  title={Optimal transport mapping via input convex neural networks},
  author={Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6672--6681},
  year={2020},
  organization={PMLR}
}


@article{ref:wu2024reft,
  title={{ReFT}: Representation Finetuning for Language Models},
  author={Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:2404.03592},
  year={2024}
}

@book{ref:prekopa1995stochastic,
  title={Stochastic Programming},
  author={Pr{\'e}kopa, Andr{\'a}s},
  year={1995},
  publisher={Springer Science \& Business Media}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}
@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@article{wu2024advancing,
  title={Advancing parameter efficiency in fine-tuning via representation editing},
  author={Wu, Muling and Liu, Wenhao and Wang, Xiaohua and Li, Tianlong and Lv, Changze and Ling, Zixuan and Zhu, Jianhao and Zhang, Cenyuan and Zheng, Xiaoqing and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.15179},
  year={2024}
}
@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
@article{dubey2024llama,
  title={The {L}lama 3 herd of models},
  author={Dubey, Abhimanyu and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{joshi2017triviaqa,
  title={{TriviaQA}: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{ref:hase2024does,
  title={Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{tharwat2017linear,
  title={Linear discriminant analysis: A detailed tutorial},
  author={Tharwat, Alaa and Gaber, Tarek and Ibrahim, Abdelhameed and Hassanien, Aboul Ella},
  journal={AI Communications},
  volume={30},
  number={2},
  pages={169--190},
  year={2017},
  publisher={IOS Press}
}

@article{ref:nguyen2025risk,
      title={Risk-Aware Distributional Intervention Policies for Language Models}, 
      author={Bao Nguyen and Binh Nguyen and Duy Nguyen and Viet Anh Nguyen},
      year={2025},
      journal={arXiv preprint arXiv:2501.15758}
}
