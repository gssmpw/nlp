\section{Related Work}
\textbf{Controllable generation.}  Model editing~\citep{ref:wang2023knowledge, ref:zhang2024comprehensive} alters the model parameters to control the output, making it a powerful method for controllable generation. Another important category in controllable generation is fine-tuning, which includes Supervised Fine-Tuning (SFT,~\citep{ref:peng2023instruction, ref:gunel2020supervised}) and Reinforcement Learning from Human Feedback (RLHF,~\citep{ref:ouyang2022training, ref:griffith2013policy}). These methods typically require altering model weights, incurring substantial resources and costs for computation.
 
\noindent \textbf{Activation intervention} at inference time is an emerging technique for controllable generation~\citep{ref:li2024inference,ref:singh2024mimic,yin2024lofit}. Unlike model editing and fine-tuning techniques, the inference time intervention does not require altering the model parameters, leading to cheaper computational costs. \citet{ref:li2024inference} proposes a headwise intervention method for eliciting truthful generated answers of a language model. \citet{ref:singh2024mimic} considers the optimal transport plan between two empirical distributions to carry out the intervention. LoFit~\citep{yin2024lofit} identifies a specific subset of attention heads crucial for learning a particular task. It then fine-tunes the intervention vectors in those chosen heads. Another recent work \citep{pham2024householder} considers doing intervention activation using modified householder transformation based on the linear probe framework. 

\noindent \textbf{Region Modeling in LM.}
Various works aim to reveal how the semantic features influence the `region' of embedding vectors in the transformer-based LM. The work \citep{mamou2020emergence} utilizes mean-field theoretic manifold analysis to connect the geometry of feature representations with the linear separability of classes. \citet{janiak2024characterizing} identifies stable regions in the residual stream of Transformers, where the model output remains insensitive to small activation changes but exhibits high sensitivity at the region boundaries. However, most prior works focus on specific findings in region modeling and overlook the potential of enhancing activation intervention through transport between two regions.

\noindent \textbf{Low-Rank Optimization} is widely studied in machine learning, statistics, and signal processing \citep{olikier2022low,zhanxuan2020low,cory2021new}. The motivation for formulating problems into low-rank optimization in several applications lies in two folds: the nature of the low-rank property of the ground truth and the goal for achieving lightweight complexity in algorithm design \citep{mcrae2024benign}.  
The low-rank concept has also been applied to LM model-editing, fine-tuning, and model compression \citep{hu2021lora,hsu2022language}.