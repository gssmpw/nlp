\section{Preliminaries}
\label{Sec-Preliminaries}
% \subsection{Graph Neural Network}\label{subsec-Graph_Neural_Network}
\textbf{Graph Neural Network.} Let $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$ denote an graph, where $\mathcal{V}$ and $\mathcal{E}$ are the sets of nodes and edges, respectively. We represent the node space by $\mathbb{V}$, and $\mathcal{V} \in \mathbb{V}$. ${\boldsymbol X} \in \mathbb {R}^{|\mathcal{V}|\times d}$ denote the matrix of features of the nodes. Here, the representation of a node $v$ can be defined as $\mathbf{h}_v$. Graph neural networks (GNNs) aim to update the representation of the given graph $\mathcal{G}$ by leveraging its topological structure. For the representation $\mathbf{h}_v$ of node $v$, its propagation of the $k$-th layer GNN is represented as:
\begin{equation}\label{equa-gnns}
\begin{array}{cl}
   \mathbf{h}_v^{(k)}&=g(\mathbf{h}_v^{(k-1)};\theta)    \\
 &=\mathrm{UP}^{(k)}\big(\mathrm{AGG}^{(k)}\big( \mathbf{h}_u^{(k-1)}:\forall u \in \mathcal{N}(v)\cup v \big) \big) 
\end{array} 
\end{equation}
where $g(\cdot;\theta)$ denotes the GNN encoder, $\theta$ represents all trainable parameters of GNN encoder. $\mathrm{AGG}(\cdot)$ denotes a function that aggregates messages from the neighbors of node $v$, $\mathcal{N}(v)$ represents the set of neighbors. $\mathrm{UP}(\cdot)$ denotes a function that updates the representation of node $v$ with the current representation of $v$ and the aggregated vector.

\textbf{Problem Statement.} The node set can be divided into
$\mathcal{V} = \mathcal{V}_{\mathrm{in}} \cup \mathcal{V}_{\mathrm{out}}$, where $\mathcal{V}_{\mathrm{in}}$ and $\mathcal{V}_{\mathrm{out}}$ represents the ID node set and OOD node set, respectively. We assume ID nodes are sampled from the distribution $\mathbb{P}_{\mathrm{in}}$, and OOD nodes are sampled from distribution $\mathbb{P}_{\mathrm{out}}$. We formally define the Category-free OOD node detection task:
\begin{definition} \textbf{Category-free OOD node detection.} Given a collection of nodes sampled from $\mathbb{P}_{\mathrm{in}}$ and $\mathbb{P}_{\mathrm{out}}$, the objective is to correctly identify the source of each node, whether it is from the  $\mathbb{P}_{\mathrm{in}}$ or $\mathbb{P}_{\mathrm{out}}$.
\end{definition}

\textbf{Unlabeled Wild Node.} In this work, we incorporate unlabeled wild node $\mathcal{V}_{\mathrm{wild}} = \{ \tilde{v}_1, \cdots, \tilde{v}_m \}$ with feature $\boldsymbol{X}_{\mathrm{wild}} = \{ \tilde{\mathbf{x}}_1, \cdots, \tilde{\mathbf{x}}_m \}$ into our learning framework, leveraging the fact that such features are often accessible. We define the unlabeled wild nodes distribution as follows:
\begin{definition}\textbf{Unlabeled wild nodes.}
%A key challenge in OOD node detection is the lack of labeled OOD node data. Specifically, the potential sample space of OOD data can be extremely large, making it prohibitively expensive to collect labeled OOD samples. 
Unlabeled wild nodes typically consist of a mixture of ID nodes and OOD nodes. We use the Huber contamination model \citep{huber1992robust} to characterize the marginal distribution of the wild data:
\begin{equation}
\mathbb{P}_{\mathrm{wild}} = (1-\pi)\mathbb{P}_{\mathrm{in}} + \pi \mathbb{P}_{\mathrm{out}}
\end{equation}
where $\pi \in (0,1]$.
\end{definition}

% \subsection{Neural Collapse}\label{subsec-Neural Collapse}
% Recent works have disclosed the Neural Collapse phenomenon, which corresponds to an optimal feature classifier alignment towards a simplex equiangular tight
% frame (ETF).
% \begin{definition}
% \textbf{Simplex ETF.} A Simplex equiangular tight
% frame (ETF) refers to a collection of K equal-length and maximally-equiangular P-dimensional embedding vectors $\mathbf{E} = [e_1, \cdots, e_K] \in \mathbb{R}^{P \times K}$ which satisfies:
% \begin{equation}
%     \mathbf{E} = \sqrt{\frac{K}{K-1}}\mathbf{U}\big( \mathbf{I}_K - \frac{1}{K} \mathbf{1}_K\mathbf{1}_K^{\top} \big)
% \end{equation}
% where $\mathbf{I}_K$ is the identity matrix,$\mathbf{1}_K$ is an all-ones vector, and
% $\mathbf{U} \in \mathbb{R}^{P \times K} (P \geq K) $ allows a rotation.
% \end{definition}
% All vectors in a simplex ETF $\mathbf{E}$ have an equal $\mathit{l}_2$ norm and
% the same pair-wise maximal equiangular angle $-\frac{1}{K-1}$,
% \begin{equation}
%     e_{k_1}^{\top}e_{k_2} = \frac{K}{K-1}\delta_{k_1,k_2} - \frac{1}{K-1}, \forall k_1, k_2 \in [1, K]
% \end{equation}
% where $\delta_{k_1, k_2} = 1$ when $k_1 = k_2$ and $0$ otherwise. 

% In our work, we leverage the Neural Collapse theory to predefine a \(K\)-simplex equiangular tight frame (ETF) (\(K \geq 2\)) and arbitrarily select one vector \(e\) from it as the target vector to which the labeled ID node embeddings are aligned.















% \begin{figure}[!t]
% 	\centering
%    \subfigure[Toy Dataset]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.43\linewidth]{Figures/path/point.png}
% 	% \end{minipage}
%  }
%    \subfigure[Gradient Descent Trajectory]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.47\linewidth]{Figures/path/sample_123_cropped.png}
% 	% \end{minipage}
%  }
%     \caption{(a) We conduct a preliminary study on the changes in ID node representations and OOD node representations during training using a toy dataset.
% (b) Projections of the representations of ID and OOD nodes onto gradients: $\text{Proj}_{\nabla \mathcal{L}_1(\mathbf{W_t}, \mathbf{b_t})}\mathbf{x}_i = \frac{\mathbf{x}_i \cdot \nabla \mathcal{L}_1(\mathbf{W_t}, \mathbf{b_t}) }{\parallel \nabla \mathcal{L}_1(\mathbf{W_t}, \mathbf{b_t}) \parallel_2^2}\cdot \nabla \mathcal{L}_1(\mathbf{W_t}, \mathbf{b_t})$.}
%     \label{F-Trajectory}
% \end{figure}

