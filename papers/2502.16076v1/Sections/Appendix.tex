\section{Notations, Definitions, Assumptions and Important Constants}\label{Sec-Appendix}
\subsection{Notations}
\begin{table}[h!]
    \centering
    \begin{tabular}{c p{10cm}}
        \hline
        \textbf{Notation} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        \multicolumn{2}{c}{\cellcolor{gray!30}Spaces}\\
        \hline
        $\boldsymbol{X}$, $\boldsymbol{Y}$ & the input space and the label space. \\
    
        $\mathcal{W}$ & the hypothesis spaces. \\
        \hline
       \multicolumn{2}{c}{ \cellcolor{gray!30}Distributions}\\
        \hline
        \( \mathbb{P}_{\text{wild}}, \mathbb{P}_{\text{in}}, \mathbb{P}_{\text{out}} \) & data distribution for wild data, labeled ID data and OOD data. \\
        
        \(  \mathbb{P}_{\boldsymbol{XY}} \) & the joint data distribution for ID data.. \\
        \hline
       \multicolumn{2}{c}{ \cellcolor{gray!30}Data and Models}\\
        \hline
        \( \mathbf{w},\mathbf{x} \) & weight, input. \\
        
        \( \widehat{\nabla}, \tau \) & the average gradients on labeled ID data, uncertainty score. \\
        $e$ & randomly generated unit vector.\\
        \( y \) & target unit vector $e$ for ID node representations. \\
        \(  \widehat{y}_{\mathbf{x}} \) & predicted vector for input $\mathbf{x}$. \\  
        $h_{\theta_t}$ & predictor on labeled in-distribution\\
        $\boldsymbol{X}_{\text{wild}}^{\text{in}}, \boldsymbol{X}_{\text{wild}}^{\text{out}}$ & inliers and outliers in the wild dataset.\\
        $\boldsymbol{X}^{\text{in}}, \boldsymbol{X}_{\text{wild}}$ & labeled ID data and unlabeled wild data.\\
        $n, m$ & size of $\boldsymbol{X}^{\text{in}}$, size of $\boldsymbol{X}_{\text{wild}}$\\
        $T$ & the filtering threshold\\
        $\boldsymbol{X}_{T}$ & wild data whose uncertainty score higher than threshold $T$\\
        \hline
       \multicolumn{2}{c}{ \cellcolor{gray!30}Distances}\\
        \hline
        $r_1$ &  the radius of the hypothesis spaces $\mathcal{W}$\\
        $\parallel \cdot \parallel_2 $ & $\ell_2$ norm\\
        \hline
       \multicolumn{2}{c}{ \cellcolor{gray!30}Loss, Risk and Predictor}\\
        \hline
        $\ell(\cdot,\cdot)$ & ID loss function\\
        $R_{\boldsymbol{X}}(h_{\theta_t})$ & the empirical risk w.r.t. predictor $h_{\theta_t}$ over data $\boldsymbol{X}$\\
        $R_{\mathbb{P}_{\boldsymbol{XY}}}(h_{\theta_t})$ & the risk w.r.t. predictor $h_{\theta_t}$ over distribution $\mathbb{P}_{\boldsymbol{XY}}$.\\
        $ERR_{\text{out}}$ & the error rate of regarding OOD as ID.\\
        
        
        \hline
    \end{tabular}
    \caption{Table of Notations and Descriptions}
    \label{tab:notation}
\end{table}

\subsection{Definitions}
\begin{definition} \label{definition-beta-smooth}
($\beta$ -smooth).We say a loss function $\ell(h_{\theta_t}(\mathbf{x}),y)$ (defined over $\boldsymbol{X}\times\boldsymbol{Y})$ is $\beta$ -smooth, if forany $\mathbf{x}\in{\boldsymbol{X}}$ and $y\in\boldsymbol{Y}$

$$\begin{Vmatrix}\nabla\ell(h_{\theta_t}(\mathbf{x}),y)-\nabla\ell(h_{\theta_t}(\mathbf{x}),y)\end{Vmatrix}_2\leq\beta\|\mathbf{w}-\mathbf{w}'\|_2$$
\end{definition}

\begin{definition}\label{definition-gradient-based-discrepancy}
    (Gradient-based Distribution Discrepancy). Given distributions $\mathbb{P}$ and $\mathbb{Q}$ defined over $X$ , the Gradient-based Distribution Discrepancy w.r.t. predictor $\mathbf{f}_{\mathrm{w}}$ and loss $t$ is

$$d_{\mathbf{w}}^{\ell}(\mathbb{P},\mathbb{Q})=\left\|\nabla R_{\mathbb{P}}(h_{\theta_t},\widehat{h}_{\theta})-\nabla R_{\mathbb{Q}}(h_{\theta_t},\widehat{h}_{\theta})\right\|_{2},$$

where $\widehat{h}_{\theta}$ is a classifier which returns the closest one-hot vector of $h_{\mathrm{w}}$: $R_{\mathbb{P}}(h_{\theta_t},\widehat{h}_{\theta})=\mathbb{E}_{\mathbf{x}\sim\mathbb{P}}\ell(h_{\theta_t},\widehat{h}_{\theta})$ and $R_{\mathbb{Q}}(h_{\theta_t},\widehat{h}_{\theta})=\mathbb{E_{\mathbf{x}\sim\mathbb{Q}}}\ell(h_{\theta_t},\widehat{h}_{\theta})$
\end{definition}

\begin{definition}\label{definition-gamma-discrepancy}
$(\gamma,\xi)$ -discrepancy). We say a wild distribution $\mathbb{P}_{wild}$ has $(\gamma,\xi)$ -discrepancy w.r.t. an ID joint distribution $\mathbb{P}_{in}$ $_{n}$, if $\gamma > \min _{\mathbf{w} \in \mathcal{W} }$ $R_{\mathbb{P} _{XY}}( h_{\theta})$ and for any parameter $\mathbf{w}\in\mathcal{W}$ satisfying that $R_{\mathbb{P},\boldsymbol{X}\boldsymbol{Y}}(h_{\theta_t})\leq\gamma$ should meet the following condition

$$d_{\mathbf{w}}^{\ell}(\mathbb{P}_{in},\mathbb{P}_{wild})>\xi,$$

where $R_{\mathrm{P} _{XY}}( h_{\theta}) = \mathbb{E} _{( \mathbf{x} , y) \sim \mathbb{P} _{XY}}\ell ( h_{\theta}( \mathbf{x} ) , y)$
\end{definition}



\subsection{Assumptions}
\textbf{Assumption 1.}
\begin{itemize}
    \item The parameter space $\mathcal{W}\subset B(\mathbf{w}_{0},r_{1})\subset\mathbb{R}^{d}\left(\ell_{2}\right.$ ball of radius $r_1$ around $W_0$);
    \item $\ell(h_{\theta_t}(\mathbf{x}),y)\geq0$ and $\ell(h_{\theta_t}(\mathbf{x}),y)$ is $\beta_{1}$ -smooth;
    \item $\sup_{(\mathbf{x},y)\in\boldsymbol{X}\times\boldsymbol{Y}}\|\nabla\ell(h_{\theta_0}(\mathbf{x}),y)\|_{2}=b_{1};$
    \item $\sup_{(\mathbf{x},y)\in\boldsymbol{X}\times\boldsymbol{Y}}\ell(h_{\theta_0}(\mathbf{x}),y)=B_{1}.$
\end{itemize}

\textbf{Assumption 2.} 
$\ell(\mathbf{f}(\mathbf{x}),\widehat{y}_{\mathbf{x}})\leq\min_{y\in\boldsymbol{Y}}\ell(\mathbf{f}(\mathbf{x}),y)$ , where $\widehat{y}_{\mathbf{x}}$ returns the closest vector of the predictor $\mathbf{f}$'s output on $\mathbf{x}$

\subsection{Constants in Theory}

\begin{table}[h!]
    \centering
    \begin{tabular}{c p{10cm}}
        \hline
        \textbf{Constants} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        $M = \beta_1 r_1^2 + b_1r_1 + B_1$ & the upper bound of loss $\ell(h_{\theta_t}(\mathbf{x}),y)$.\\
        $M^{\prime} = 2(\beta_1 r_1 + b_1)^2$ & the upper bound of gradient-based filtering score \citep{du2024does} \\
        $\widehat{M}_t =  (\sqrt{M^{\prime}/2}+1)/(2t)$ & the upper bound of our resonance-based filtering score $\tau$ at the $t$-th epoch \\
        $\tilde{M} = \beta_1M$& a constant for simplified representation\\
        $d$ & the dimensions of parameter spaces $\mathcal{W}$\\
        $R_{\text{in}}^{*}$ & the optimal ID risk, i.e., $R_{in}^{*} = \mathrm{min}_{\mathbf{w}\in \mathcal{W}}\mathbb{E_{\mathbf{x}\sim\mathbb{P}_{\mathrm{in}}}}\mathcal{L}_1(\mathbf{x}, e)$\\
        $\delta(T, t)$ & the main error in \ref{equa:ERR_out^t}\\
        $\xi$ & the discrepancy between $\mathbb{P}_{\text{in}}$ and $\mathbb{P}_{\text{wild}}$\\
        $\pi$ & the ratio of OOD distribution in $\mathbb{P}_{\text{wild}}$\\
        $\alpha$ & learning rate\\
        \hline
    \end{tabular}
    \caption{Constants in theory.}
    \label{tab:constants}
\end{table}

\section{Main Theorems}
\begin{theorem}
 If Assumptions 1 and 2 hold, $\mathbb{P}_{wild}$ has $(\gamma,\xi)$ -discrepancy w.r.t. $\mathbb{P}_{xy}$ ,and there exists $\eta\in(0,1)$ s.t. $\Delta = ( 1- \eta )^{2}\xi^{2}-8\beta_{1}R_{in}^{*}>0$, then for

$$n=\Omega\big(\frac{\tilde{M}+M(r_1+1)d}{\eta^2\Delta}+\frac{M^2d}{(\gamma-R_{in}^*)^2}\big),\quad m=\Omega\big(\frac{\tilde M+M(r_1+1)d}{\eta^2\xi^2}\big),$$

with the probability at least 9/10 for any $0<T<\widehat{M}_t$ (here $\widehat{M}_t$ is the upper bound of filtering score $\tau_i$ at $t$-th epoch, i.e., $\tau_{i}\leq \widehat{M}_t$ )

% $\mathrm{ERR}_{in}\leq\frac{8\beta_{1}}{T}R_{in}^{*}+O\Big(\frac{\tilde{M}}{T}\sqrt{\frac{d}{n}}\Big)+O\Big(\frac{\tilde{M}}{T}\sqrt{\frac{d}{(1-\pi)m}}\Big)$

\begin{equation}
     \text{ERR}_{\text{out}}^t \leq \frac{\mathrm{max}\{0, 1-\Delta_{\xi}^{\eta}/\pi\}}{1-T/(\sqrt{2}/(2t\alpha - 1))^2}
     + O(\sqrt{\frac{d}{\pi^2 n}}) + O(\sqrt{\frac{\mathrm{max}\{d, \Delta_{\xi}^{\eta^2}/\pi^2\}}{\pi^2(1-\pi)m}})
\end{equation}

where $
\Delta_{\xi}^{\eta} = 0.98\eta^2\xi^2 - 8\beta_1 R_{in}^{*}
$ and  $R_{in}^{*}$ is the optimal ID risk, i.e., $R_{in}^{*} = \mathrm{min}_{\mathbf{w}\in \mathcal{W}}\mathbb{E_{\mathbf{x}\sim\mathbb{P}_{\mathrm{in}}}}\mathcal{L}_1(\mathbf{x}, e)$.
% and $\Delta_{\xi}^{\eta} = 0.98\eta^2\xi^2 - 8\beta_1 R_{in}^{*}$.
$d$ is  the dimension of the space $\mathcal{W}$, $t$ denotes the $t$-th epoch, and $\pi$ is the OOD class-prior probability in the wild.

\begin{equation}
M=\beta_{1}r_{1}^{2}+b_{1}r_{1}+B_{1},\quad\tilde{M}=M\beta_{1}
\end{equation}
\end{theorem}

\begin{theorem}
1) if $\Delta_{\xi}^{\eta} \geq (1-\epsilon)\pi$ for a small error $\epsilon \geq 0$, then the main error $\delta(T,t)$ satisfies that
    \begin{equation}
    % \begin{array}{cl}
        \delta(T, t) = \frac{\mathrm{max}\{0, 1-\Delta_{\xi}^{\eta}/\pi\}}{1-T/(\sqrt{2}/(2t\alpha - 1))^2}
        \leq \frac{\epsilon}{1-T/(\sqrt{2}/(2t\alpha - 1))^2}
    % \end{array}
    \end{equation}

2) When learning rate $\alpha$ is small sufficiently, and if $\xi \geq 2.011\sqrt{8\beta_1 R_{in}^{*} + 1.011\sqrt{\pi}}$, then there exists $\eta \in (0, 1)$ ensuring that $\Delta > 0$ and $\Delta_{\xi}^{\eta}>\pi$ hold, which implies that the main error $\delta(T, t) = 0$.
    
\end{theorem}

\section{Proofs of Main Theorems}\label{sec-appendix-proof}
\subsection{Proof of Theorem 1}
Step 1. With the probability at least $1-\frac{7}{3}\delta>0$

$$\begin{aligned}
\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim S_{\mathrm{wild}}^{\mathrm{in}}\tau_{i}}& \leq8\beta_{1}R_{\mathrm{in}}^{*}  \\
&+4\beta_{1}\Big[C\sqrt{\frac{Mr_{1}(\beta_{1}r_{1}+b_{1})d}{n}}+C\sqrt{\frac{Mr_{1}(\beta_{1}r_{1}+b_{1})d}{(1-\pi)m-\sqrt{m\log(6/\delta)/2}}} \\
&+3M\sqrt{\frac{2\log(6/\delta)}{n}}+M\sqrt{\frac{2\log(6/\delta)}{(1-\pi)m-\sqrt{m\log(6/\delta)/2}}}\Big],
\end{aligned}$$

This can be proven by Lemma 7 in \citep{du2024does} and following inequality

$$\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim\mathcal{S}_{wild}^{in}}\tau_{i}\geq\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim\boldsymbol{X}_{wild}^{m}}\left\|\nabla\ell(h_{\theta_{\boldsymbol{X}^{m}}}(\tilde{\mathbf{x}}_{i}),\widehat{{h}}_{\theta_{\boldsymbol{X}^{m}}}(\tilde{\mathbf{x}}_{i}))-\mathbb{E}_{(\mathbf{x}_{j},y_{j})\sim\boldsymbol{X}^{m}}\nabla\ell(h_{\theta_{\boldsymbol{X}^{m}}}(\mathbf{x}_{j}),y_{j})\right\|_{2}^{2},$$

Step 2.It is easy to check that

$$\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim\boldsymbol{X}_{\mathrm{wild}}}\tau_{i}=\frac{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{in}}|}{|\boldsymbol{X}_{\mathrm{wild}}|}\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{in}}}\tau_{i}+\frac{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}|}{|\boldsymbol{X}_{\mathrm{wild}}|}\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}}\tau_{i}.$$

Step 3.Let

$$\begin{gathered}
\epsilon(n,m)= 4\beta_{1}[C\sqrt{\frac{Mr_{1}(\beta_{1}r_{1}+b_{1})d}{n}}+C\sqrt{\frac{Mr_{1}(\beta_{1}r_{1}+b_{1})d}{(1-\pi)m-\sqrt{m\log(6/\delta)/2}}} \\
+3M\sqrt{\frac{2\log(6/\delta)}{n}}+M\sqrt{\frac{2\log(6/\delta)}{(1-\pi)m-\sqrt{m\log(6/\delta)/2}}}]. 
\end{gathered}$$

Under the condition in Theorem 5 in \citep{du2024does}, with the probability at least $\frac{97}{100}-\frac{7}{3}\delta>0$

$$\begin{aligned}
\mathbb{E}_{\tilde{\mathbf{x}}_{i}\sim\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}\tau_{i}}& \leq\frac{m}{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}|}\big[\frac{98\eta^{2}\xi^{2}}{100}-\frac{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{in}}|}{m}8\beta_{1}R_{\mathrm{in}}^{*}-\frac{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{in}}|}{m}\epsilon(n,m)\big]  \\
&\leq\frac{m}{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}|}\big[\frac{98\eta^{2}\xi^{2}}{100}-8\beta_{1}R_{\mathrm{in}}^{*}-\epsilon(n,m)\big] \\
&\leq[\frac{1}{\pi}-\frac{\sqrt{\log6/\delta}}{\pi^{2}\sqrt{2m}+\pi\sqrt{\log(6/\delta)}}\Big]\Big[\frac{98\eta^{2}\xi^{2}}{100}-8\beta_{1}R_{\mathrm{in}}^{*}-\epsilon(n,m)\Big].
\end{aligned}$$

In this proof, we set

$$\Delta(n,m)=\big[\frac{1}{\pi}-\frac{\sqrt{\log6/\delta}}{\pi^2\sqrt{2m}+\pi\sqrt{\log(6/\delta)}}\big]\big[\frac{98\eta^2\xi^2}{100}-8\beta_1R_{\mathrm{in}}^*-\epsilon(n,m)\big].$$

Note that $\Delta_{\xi}^{\eta}=0.98\eta^{2}\xi^{2}-8\beta_{1}R_{\mathrm{in}}^{*}$ , then

$$\Delta(n,m)=\frac{1}{\pi}\Delta_{\xi}^{\eta}-\frac{1}{\pi}\epsilon(n,m)-\Delta_{\xi}^{\eta}\epsilon(m)+\epsilon(n)\epsilon(n,m),$$

where $\epsilon(m)=\sqrt{\log6/\delta}/(\pi^{2}\sqrt{2m}+\pi\sqrt{\log(6/\delta)}).$

Step 4. Under the conditions in Theorem 5 in \citep{du2024does} and Proposition \ref{proposition-4}, with the probability at least $\frac{97}{100}-\frac{7}{3}\delta>0$

\begin{equation}
\frac{|\{\tilde{\mathbf{x}}_{i}\in\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}:\tau_{i}\leq T\}|}{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}|}\leq\frac{1-\min\{1,\Delta(n,m)\}}{1 -T/(\frac{\sqrt{2}}{2t\alpha - 1})^2},
\end{equation}


We prove this step: let $Z$ be the uniform random variable with $S_{\mathrm{wild}}^{\mathrm{out}}$ as its support and $Z(i)=$ $\tau_{i}/(\frac{\sqrt{2}}{2t\alpha - 1})^2$ , then by the Markov inequality, we have
\begin{equation}
\frac{|\{\tilde{\mathbf{x}}_{i}\in \boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}:\tau_{i}<T\}|}{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}|}=P(Z(I)<T/(\frac{\sqrt{2}}{2t\alpha - 1})^2)\geq\frac{\Delta(n,m)-T/(\frac{\sqrt{2}}{2t\alpha - 1})^2}{1-T/(\frac{\sqrt{2}}{2t\alpha - 1})^2}.
\end{equation}



Step 5. If $\pi\leq\Delta_{\xi}^{\eta}/(1-\epsilon/M^{\prime})$ , then with the probability at least $\frac{97}{100}-\frac{7}{3}\delta>0$
\begin{equation}
\frac{|\{\tilde{\mathbf{x}}_{i}\in\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}:\tau_{i}\leq T\}|}{|\boldsymbol{X}_{\mathrm{wild}}^{\mathrm{out}}|}\leq\frac{\epsilon+(\frac{\sqrt{2}}{2t\alpha - 1})^2\epsilon'(n,m)}{(\frac{\sqrt{2}}{2t\alpha - 1})^2-T},
\end{equation}


where $\epsilon^{\prime}(n,m)=\epsilon(n,m)/\pi+\Delta_{\xi}^{\eta}\epsilon(m)-\epsilon(n)\epsilon(n,m).$

Step 6. If we set $\delta=3/100$ , then it is easy to see that

$$\begin{aligned}
&\epsilon(m)\leq O({\frac{1}{\pi^{2}\sqrt{m}}}), \\
&\epsilon(n,m)\leq O(\beta_{1}M\sqrt{\frac{d}{n}})+O(\beta_{1}M\sqrt{\frac{d}{(1-\pi)m}}), \\
&\epsilon^{\prime}(n,m)\leq O(\frac{\beta_{1}M}{\pi}\sqrt{\frac{d}{n}})+O\Big((\beta_{1}M\sqrt{d}+\sqrt{1-\pi}\Delta_{\xi}^{\eta}/\pi)\sqrt{\frac{1}{\pi^{2}(1-\pi)m}}\Big).
\end{aligned}$$

Step 7. By results in Steps 4, 5 and 6, We complete this proof


\subsection{Proof of Theorem 2}
The first result is trivial. Hence,we omit it.We mainly focus on the second result in this theorem In this proof, then we set

$$\eta=\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}+0.99\pi}/(\sqrt{0.98}\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}}+\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}+\pi})$$

Note that it is easy to check that

$$\xi\geq2.011\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}}+1.011\sqrt{\pi}\geq\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}}+1.011\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}+\pi}.$$

Therefore,

$$\eta\xi\geq\frac{1}{\sqrt{0.98}}\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}+0.99\pi}>\sqrt{8\beta_{1}R_{\mathrm{in}}^{*}+\pi},$$

which implies that $\Delta_{\xi}^{\eta}>\pi$ Note that

$$(1-\eta)\xi\geq\frac{1}{\sqrt{0.98}}\big(\sqrt{0.98}\sqrt{8\beta_{1}R_{\mathrm{m}}^{*}}+\sqrt{8\beta_{1}R_{\mathrm{m}}^{*}+\pi}-\sqrt{8\beta_{1}R_{\mathrm{m}}^{*}+0.99\pi}\big)>\sqrt{8\beta_{1}R_{\mathrm{m}}^{*}},$$

which implies that $\Delta>0$ We have completed this proof

\section{Necessary Propositions.}
\subsection{Boundedness}
\begin{proposition}\label{proposition-1}
If Assumption 1 holds,

$$\begin{gathered}
\operatorname*{sup}_{\mathbf{w}\in\mathcal{W}}\operatorname*{sup}_{(\mathbf{x},y)\in\boldsymbol{X}\times\boldsymbol{Y}}\|\nabla\ell(h_{\theta_t}(\mathbf{x}),y)\|_{2}\leq\beta_{1}r_{1}+b_{1}={\sqrt{M^{\prime}/2}}, \\
% \sup_{\theta\in\Theta}\sup_{(\mathbf{x},y_{b})\in\boldsymbol{X}\times\boldsymbol{Y}_{b}}\|\nabla\ell(\mathbf{g}_{\theta}(\mathbf{x}),y_{b})\|_{2}\leq\beta_{2}r_{2}+b_{2}. \\
\sup_{\mathbf{w}\in\mathcal{W}}\sup_{(\mathbf{x},y)\in\boldsymbol{X}\times\boldsymbol{Y}}\ell(h_{\theta_t}(\mathbf{x}),y)\leq\beta_{1}r_{1}^{2}+b_{1}r_{1}+B_{1}=M, \\
% \sup_{\boldsymbol{\theta}\in\Theta}\sup_{(\mathbf{x},y_{0})\in\boldsymbol{X}\times\boldsymbol{Y}_{0}}\ell_{b}(\mathbf{g}_{\boldsymbol{\theta}}(\mathbf{x}),y_{b})\leq\beta_{2}r_{2}^{2}+b_{2}r_{2}+B_{2}=L. 
\end{gathered}$$

Proof. One can prove this by Mean Value Theorem of Integrals easily.
\end{proposition}
\begin{proposition}\label{proposition-2}
 If Assumption 1 holds, for any $\mathbf{w}\in\mathcal{W}$,

$$\left\|\nabla\ell(h_{\theta_t}(\mathbf{x}),y)\right\|_2^2\leq2\beta_1\ell(h_{\theta_t}(\mathbf{x}),y).$$

Proof. The details of the self-bounding property can be found in Appendix B of Lei Ying
\end{proposition}

\begin{proposition}\label{proposition-3}
If Assumption 1 holds, for any labeled data $\boldsymbol{X}$ and distribution $\mathbb{P}$.
\begin{gather}
\left\|\nabla R_{\boldsymbol{X}}(h_{\theta_t})\right\|_{2}^{2}\leq2\beta_{1}R_{\boldsymbol{X}}(h_{\theta_t}),\quad\forall\mathbf{w}\in\mathcal{W},\\
\left\|\nabla R_{\mathbb{P}}(h_{\theta_t})\right\|_{2}^{2}\leq2\beta_{1}R_{\mathbb{P}}(h_{\theta_t}),\quad\forall\mathbf{w}\in\mathcal{W}.
\end{gather}

Proof. Jensen's inequality implies that $R_{S}(h_{\theta_t})$ and $R_{\mathbb{P}}(\mathbf{f}_{\mathrm{w}})$ are $\beta_{1}$ -smooth.Then Proposition 2 implies the results.
\end{proposition}

\begin{proposition}\label{proposition-4}
    If Assumption 1 holds, for any $\mathbf{w}_t \in \mathcal{W}$,

    $$
    \parallel \Delta h_{\theta_t}({\mathbf{x}}) \parallel_2 \leq 
    (\sqrt{M^{\prime}/2}+1)/(2t) = \widehat{M}_t
    $$

Proof. It is trivial that
$$
\parallel \mathbf{x}^{\top} \nabla\ell(h_{\theta_t}(\mathbf{x}),y) \parallel \leq \parallel \nabla\ell(h_{\theta_t}(\mathbf{x}),y)  \parallel\leq \beta_{1}r_{1}+b_{1}={\sqrt{M^{\prime}/2}}
$$
Then

$$
% \begin{aligned}
\parallel \mathbf{x}^{\top} \nabla\ell(h_{\theta_t}(\mathbf{x}),y) \parallel  = \parallel 2(\mathbf{x}\mathbf{W}^{\top}- y)\parallel
 \geq 2 \parallel \sum_t \Delta h_{\theta_t}(\mathbf{x}) - y \parallel
 \geq 2 \parallel t \Delta h_{\theta_t}(\mathbf{x}) - y \parallel 
 \geq 2t \parallel \Delta h_{\theta_t}(\mathbf{x}) \parallel - 1
% \end{aligned}
$$
It is straightforward to verify that:
\[
\|\Delta h_{\theta_t}(\mathbf{x})\|_2 \leq \frac{\sqrt{M^{\prime}/2} + 1}{2t} \leq \alpha \sqrt{M^{\prime}/2} = \widehat{M}_t.
\]
Here, \(\alpha\) is the learning rate. From the inequality above, we establish a relationship between \(\sqrt{M^{\prime}/2}\), \(\alpha\), and \(t\) as follows:
\[
M^{\prime} \geq (\frac{\sqrt{2}}{2t\alpha - 1})^2.
\]
\end{proposition}


\section{A Straightforward Explanation of Feature Resonance}
To verify the phenomenon of Feature Resonance, we calculate the change $\Delta h_{\theta_t}(\tilde{\mathbf{x}}_i)$ in the representation \( h_{\theta_t}(\tilde{\mathbf{x}}_i) \) of an unlabeled node $i$ from the \( t \)-th ($t \geq 0$) epoch to the $( t+1 )$-th epoch, defined as follows:
\begin{equation}\label{equa-delta-f}
\begin{split}
&\Delta h_{\theta_t}(\tilde{\mathbf{x}}_i) \\
&= h_{\theta_{t+1}}(\tilde{\mathbf{x}}_i) - h_{\theta_t}(\tilde{\mathbf{x}}_i)\\
&= -\alpha \ {\tilde{\mathbf{x}}_i \  \nabla_{\theta_t}\ell({\boldsymbol X}_{\text{known}})}\\
&=2\alpha\mathbb{E}(\underbrace{\tilde{\mathbf{x}}_i{\boldsymbol X}_{\text{known}}^{\top}}_{\text{Term 1}}(\underbrace{({\boldsymbol X}_{\text{known}}\mathbf{W}^{\top}_t)-\mathbf{1}^{\top}e)}_{\text{Term 2}}) 
\end{split}
\end{equation}
where \( \alpha \) is the learning rate. 
Term 1 in the Equation \ref{equa-delta-f} illustrates that when the features of \( \tilde{\mathbf{x}}_i \) are consistent with the overall features of the labeled ID nodes ${\boldsymbol X}_{\text{known}}$, the representation of \( \tilde{\mathbf{x}}_i \) undergoes a more significant change.
% It is evident that, on a macroscopic level, the feature resonance phenomenon on the overall trajectory of the representation is accumulated from the smaller feature resonance phenomena between each epoch on a microscopic level. Correspondingly, we provide the definition of feature resonance at the microscopic level as follows:
% \begin{definition}
%     \textbf{Feature Resonance (microscopic)}: For any optimization objective $ \ell(\boldsymbol{ X}_{\text{known}},\cdot)$ applied to the known ID nodes' representations $\boldsymbol{X}_{\text{known}}$  from any model $f_{\mathbf{W}}(\cdot)$, we have $\parallel \Delta h_{\theta_t}(\tilde{\mathbf{x}}_i)\parallel_{\mathbb{P}^{\mathrm{wild}}_{\mathrm{in}}} > \parallel \Delta h_{\theta_t}(\tilde{\mathbf{x}}_i)\parallel_{\mathbb{P}^{\mathrm{wild}}_{\mathrm{out}}}$.
% \end{definition}
Meanwhile, since term 2 in the Equation \ref{equa-delta-f} and $\tilde{\mathbf{x}}_i$ are independent, the choice of the target vector can be arbitrary. It is highly suitable for category-free OOD detection scenarios, requiring no multi-category labels as ground truth. 


\section{Experiment Details}
We supplement experiment details for reproducibility. Our implementation is based on Ubuntu 20.04, Cuda 12.1, Pytorch 2.1.2, and Pytorch Geometric 2.6.1. All the experiments run with an NVIDIA 3090 with 24GB memory.

\begin{table}[!t]
\centering
\caption{Hyper-parameters for training.}\label{tabel-hyperparameters}
\scriptsize % Reduce font size
\setlength{\tabcolsep}{1.5mm} % Adjust column spacing
\begin{tabular}{c|c|c|c|c|c}
\hline
\hline
\textbf{Dataset} &\textbf{Squirrel} & \textbf{WikiCS} & \textbf{YelpChi} & \textbf{Amazon} & \textbf{Reddit}\\ 
\hline
Learning rate ($\alpha$) &0.005 &0.01 &0.005 &0.005 &0.01 \\
$h_{\theta}$ layers &1 &1 &1 &1 &1\\
$g_{\theta}(\cdot)$ layers &2 &2 &2 &2 &2\\
Hidden states &16 &16 &16 &16 &16\\
Dropout rate &0.1 &0.1 &0.1 &0.1 &0.1\\
$\mathrm{n}$ &2 &1 &2 &2 &1 \\
$\lambda$ &0.5 &0.5 &0.5 &0.5 &0.5 \\
\hline
\hline
\end{tabular}
\end{table}







\subsection{Hyperparameter}\label{subsec-appendix-hyperpm}
As shown in Table \ref{tabel-hyperparameters}.

\subsection{Metric}\label{sec-apdix-metric}
Following prior research on OOD node detection, we evaluate the detection performance using three widely recognized, threshold-independent metrics: AUROC ($\uparrow$), AUPR ($\uparrow$) and FPR95($\downarrow$). 
(1) \textbf{AUROC} measures the area under the receiver operating characteristic curve, capturing the trade-off between the true positive rate and the false positive rate across different threshold values.  
(2) \textbf{AUPR} calculates the area under the precision-recall curve, representing the balance between the precision rate and recall rate for OOD nodes across varying thresholds.  
(3) \textbf{FPR95} is defined as the probability that an OOD sample is misclassified as an ID node when the true positive rate is set at 95\%.


\subsection{Dataset Description}\label{subsec-apdix-Dataset}

To thoroughly evaluate the effectiveness of RSL, we perform experiments on five diverse and real-world OOD node detection datasets:  
\begin{itemize}[nosep, topsep=0pt, leftmargin=*]  
    \item \textbf{Squirrel} \citep{rozemberczki2021multi}: A Wikipedia network where nodes correspond to English Wikipedia articles, and edges represent mutual hyperlinks. Nodes are categorized into five classes following Geom-GCN \citep{pei2020geom} annotations, with the network exhibiting a high level of heterophily.  
    \item \textbf{WikiCS} \citep{mernyei2020wiki}: This dataset consists of nodes representing articles in the Computer Science domain. Edges are based on hyperlinks, and nodes are classified into 10 categories, each corresponding to a unique sub-field of Computer Science.  
    \item \textbf{YelpChi} \citep{rayana2015collective}: Derived from Yelp, this dataset includes hotel and restaurant reviews. Legitimate reviews are labeled as ID nodes, while spam reviews are considered OOD nodes.  
    \item \textbf{Amazon} \citep{mcauley2013amateurs}: Contains reviews from the Musical Instrument category on Amazon.com. ID nodes represent benign users, while OOD nodes correspond to fraudulent users.  
    \item \textbf{Reddit} \citep{kumar2019predicting}: A dataset comprising user posts collected from various subreddits over a month. Normal users are treated as ID nodes, while banned users are labeled as OOD nodes.  
\end{itemize}  
 We follow the same data preprocessing steps as Energy\textit{Def} \citep{gong2024energy}. Both Squirrel and WikiCS datasets are loaded using the DGL \citep{wang2019deep} package. For Squirrel, class \{1\} is selected as the OOD class, while \{0, 2, 3, 4\} are designated as ID classes. In the case of WikiCS, \{4, 5\} are chosen as OOD classes, with the remaining eight classes treated as ID. The YelpChi and Amazon datasets are processed based on the methodology described in \citep{dou2020enhancing}, and the Reddit dataset is prepared using the PyGod \citep{liu2022bond} package.





% \begin{table*}[!t]
% % \scriptsize
% \centering
% \caption{The effectiveness of feature resonance in filtering OOD nodes when using different targets as alignment objectives for known ID node representations. \textbf{True multi-label} denotes that the representations of known ID nodes are aligned with multiple target vectors based on true multi-class labels. \textbf{Multiple random vectors} denotes that the representations of known ID nodes are randomly aligned with multiple target vectors. \textbf{A random vector} denotes that the representations of known ID nodes are all aligned with a single target vector.}\label{tabel-diff-label}
% \scriptsize % Reduce font size
% \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
% \begin{tabular}{c|c|ccc|ccc}
% \hline
% \hline
% \multirow{2}*{\diagbox{\textbf{Method}}{\textbf{Dataset}}} &\multirow{2}*{\textbf{Target}}  &\multicolumn{3}{c|}{\textbf{Squirrel}} &\multicolumn{3}{c}{\textbf{WikiCS}}\\
% \cline{3-8}
% ~ &~ &\multicolumn{6}{c}{AUROC $\uparrow \ \ \ $ AUPR $\uparrow \ \ \ $ FPR@95 $\downarrow$}\\
% \hline
% $\text{Energy\textit{Def}}$& - &{64.15} &{37.40} &{91.77} &70.22 &60.10 &83.17 \\
% \hline
% RSL w/o training & True multi-label &{61.63} &{37.12} &{90.62} &71.03 &72.47 &81.96 \\

% RSL w/o training & Multiple random vectors &61.44 &37.39 &90.62 &{73.64} &{74.13} &{69.25} \\

% RSL w/o training & A random vector &{61.52} &{38.96} &{90.18} &79.15 &78.65 &70.38 \\
% \hline
% \hline
% \end{tabular}
% \end{table*}




\subsection{Baseline Description}\label{subsec-apdix-Baseline}
\begin{itemize}[nosep, topsep=0pt, leftmargin=*]
    \item \textbf{LOF-KNN} \citep{breunig2000lof} calculates the OOD scores of node attributes by assessing the deviation in local density relative to the k-nearest node attributes.
    \item \textbf{MLPAE} uses an MLP-based autoencoder, where the reconstruction error of node attributes is used as the OOD score. It is trained by minimizing the reconstruction error on ID training nodes.
    \item \textbf{GCNAE} \citep{kipf2016variational} swaps the MLP backbone for a GCN in the autoencoder. The OOD score is determined in the same way as MLPAE, following the same training process.
    \item \textbf{GAAN} \citep{chen2020generative} is a generative adversarial network for attributes that evaluates sample reconstruction error and the confidence of recognizing real samples to predict OOD nodes.
    \item \textbf{DOMINANT} \citep{ding2019deep} combines a structure reconstruction decoder and an attribute reconstruction decoder. The total reconstruction error for each node consists of the errors from both decoders.
    \item \textbf{ANOMALOUS} \citep{peng2018anomalous} is an anomaly detection method that utilizes CUR decomposition and residual analysis for identifying OOD nodes.
    \item \textbf{SL-GAD} \citep{zheng2021generative} derives OOD scores for nodes by considering two aspects: reconstruction error and contrastive scores.
    \item \textbf{GOAD} \citep{bergman2020classification} enhances training data by transforming it into independent spaces and trains a classifier to align the augmented data with the corresponding transformations. OOD scores are then calculated based on the distances between OOD inputs and the centers of the transformation spaces. For graph-structured data, we use the same GNN backbone as EnergyDef-h.
    \item \textbf{NeuTral AD} \citep{qiu2021neural} uses learnable transformations to embed data into a semantic space. The OOD score is determined by a contrastive loss applied to the transformed data.
    \item \textbf{GKDE} \citep{zhao2020uncertainty} predicts Dirichlet distributions for nodes and derives uncertainty as OOD scores by aggregating information from multiple sources.
    \item \textbf{OODGAT} \citep{song2022learning} is an entropy-based OOD detector that assumes node category labels are available. It uses a Graph Attention Network as the backbone and determines OOD nodes based on category distribution outcomes.
    \item \textbf{GNNSafe }\citep{wu2023energy} calculates OOD scores by applying the LogSumExp function over the output logits of a GNN classifier, which is trained with multi-category labels. The rationale for the OOD score is the similarity between the Softmax function and the Boltzmann distribution.
    \item \textbf{SSD} \citep{sehwag2021ssd} is an outlier detector that leverages self-supervised representation learning and Mahalanobis distance-based detection on unlabeled ID data. We use twice dropout to generate positive pairs for contrastive learning like SimCSE \citep{gao2021simcse}.
    \item \textbf{Energy\textit{Def}} \cite{gong2024energy} uses Langevin dynamics to generate synthetic OOD nodes for training the OOD node classifier.
\end{itemize}

\subsection{Implementation Details} \label{subsec-apdix-Implementation} 
We adopt the same dataset settings as Energy\textit{Def} \citep{gong2024energy}. \textit{It is worth noting that, under this dataset setup, the features of unknown nodes are accessible. Therefore, using the features of unknown nodes during the training phase to filter reliable OOD nodes is a legitimate strategy}. Specifically, for the Squirrel and WikiCS datasets, we randomly select one and two classes as OOD classes, respectively. In the case of fraud detection datasets, we categorize a large number of legitimate entities as ID nodes and fraudsters as OOD nodes. We allocate 40\% of the ID class nodes for training, with the remaining nodes split into a 1:2 ratio for validation and testing, ensuring stratified random sampling based on ID/OOD labels.

We report the average value of five independent runs for each dataset. The hyper-parameters are shown in Table \ref{tabel-hyperparameters}. 
The anomaly detection baselines are trained entirely based on graph structures and node attributes without requiring ID annotations. 
We adapt these models to the specifications of our OOD node detection tasks by minimizing the corresponding loss items solely on the ID nodes, where applicable. 

% % We optimize all models with 2-layer GNN on each dataset by selecting learning rate ($\alpha$) from \{0.001, 0.005, 0.01\} with Adam optimizer over 200 epochs, hidden states from \{16, 32, 64\} and dropout rate from \{0.0, 0.1, 0.2\}, and save the model that yields the best AUC on validation. As for the resonance-based filter, we search the number $K$ of the $K$-simplex ETF from \{5, 10, 15\} and the number $\mathrm{n}$ of candidate OOD nodes from \{1, 2, 5, 10, 15, 25\}. As for SGLD, we search the step size ($\alpha / 2$) from \{0.01, 0.1, 0.5 1.0\}, $T$ from \{10, 20, 40\} and $\lambda$ from \{0.3, 0.5, 0.7, 0.9\} in Equation \ref{equa-SGLD-cand}. Regarding the experimental environment, all experiments were conducted on Ubuntu 20.04, Cuda 12.1, Pytorch 1.12.0, and Pytorch Geometric 2.1.0.post1. All the experiments run with an NVIDIA 3090 with 24GB memory.







\section{More Experiments}

% \begin{figure*}[!t]
% 	% \centering
%   \subfigure[$\alpha$=0.1]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.1_300_line_plot.png}
% 	% \end{minipage}
%  }
%   \subfigure[$\alpha$=0.01]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.01_100_line_plot.png}
% 	% \end{minipage}
%  }
%    \subfigure[$\alpha$=0.009]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.009_100_line_plot.png}
% 	% \end{minipage}
%  }
%    \subfigure[$\alpha$=0.008]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.008_100_line_plot.png}
% 	% \end{minipage}
%  }

%    \subfigure[$\alpha$=0.007]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.007_100_line_plot.png}
% 	% \end{minipage}
%  }
%     \subfigure[$\alpha$=0.006]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.006_100_line_plot.png}
% 	% \end{minipage}
%  }
%    \subfigure[$\alpha$=0.005]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.005_100_line_plot.png}
% 	% \end{minipage}
%  }
%     \subfigure[$\alpha$=0.004]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.004_100_line_plot.png}
% 	% \end{minipage}
%  }

%    \subfigure[$\alpha$=0.003]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.003_100_line_plot.png}
% 	% \end{minipage}
%  }
%    \subfigure[$\alpha$=0.002]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.002_100_line_plot.png}
% 	% \end{minipage}
%  }
%   \subfigure[$\alpha$=0.001]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.23\linewidth]{Figures/Line/amazon_0.001_200_line_plot.png}
% 	% \end{minipage}
%  }
%     \caption{The relationship between the error of resonance-based score $\tau$ on OOD detection and the learning rate ($\alpha$) and the epoch ($t$). }
%     \label{F-appendix-alpha-t}
% \end{figure*}


% \begin{table*}[!t]
% % \scriptsize
% \centering
% \caption{The effectiveness of feature resonance in filtering OOD nodes when using different targets as alignment objectives for known ID node representations. \textbf{True multi-label} denotes that the representations of known ID nodes are aligned with multiple target vectors based on true multi-class labels. \textbf{Multiple random vectors} denotes that the representations of known ID nodes are randomly aligned with multiple target vectors. \textbf{A random vector} denotes that the representations of known ID nodes are all aligned with a single target vector.}\label{tabel-diff-label}
% \scriptsize % Reduce font size
% \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
% \begin{tabular}{c|c|ccc|ccc}
% \hline
% \hline
% \multirow{2}*{\diagbox{\textbf{Method}}{\textbf{Dataset}}} &\multirow{2}*{\textbf{Target}}  &\multicolumn{3}{c|}{\textbf{Squirrel}} &\multicolumn{3}{c}{\textbf{WikiCS}}\\
% \cline{3-8}
% ~ &~ &\multicolumn{6}{c}{AUROC $\uparrow \ \ \ $ AUPR $\uparrow \ \ \ $ FPR@95 $\downarrow$}\\
% \hline
% $\text{Energy\textit{Def}}$& - &{64.15} &{37.40} &{91.77} &70.22 &60.10 &83.17 \\
% \hline
% RSL w/o training & True multi-label &{61.63} &{37.12} &{90.62} &71.03 &72.47 &81.96 \\

% RSL w/o training & Multiple random vectors &61.44 &37.39 &90.62 &{73.64} &{74.13} &{69.25} \\

% RSL w/o training & A random vector &{61.52} &{38.96} &{90.18} &79.15 &78.65 &70.38 \\
% \hline
% \hline
% \end{tabular}
% \end{table*}




% \begin{table}[!t]
% \centering
% \caption{Time cost (s).}\label{tabel-time}
% \scriptsize % Reduce font size
% \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% \hline
% \textbf{Dataset} &\textbf{Squirrel} & \textbf{WikiCS} & \textbf{YelpChi} & \textbf{Amazon} & \textbf{Reddit}\\ 
% \hline
% Energy\textit{Def} &10.94 &27.11 &76.51 &33.81 &26.44 \\
% RSL w/o classifier &5.25 &4.03 &5.41 &5.75 &3.71\\
% RSL &11.54 &17.53 &74.83 &36.33 &38.23 \\
% \hline
% \hline
% \end{tabular}
% \end{table}



\begin{figure*}[!t]
	% \centering
  \subfigure[Reddit]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.23\linewidth]{Figures/line_new/reddit_0.01_25_line_plot.png}
	% \end{minipage}
 }
   \subfigure[Squirrel]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.23\linewidth]{Figures/line_new/squirrel_0.1_180_line_plot.png}
	% \end{minipage}
 }
   \subfigure[YelpChi]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.23\linewidth]{Figures/line_new/yelp_0.01_100_line_plot.png}
	% \end{minipage}
 }
   \subfigure[WikiCS]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.23\linewidth]{Figures/line_new/wikics_0.02_105_line_plot.png}
	% \end{minipage}
 }
    \caption{The performance of using resonance-based score $\tau$ to detect OOD nodes varies with training progress. The higher the AUROC, the better, and the lower the FPR95, the better.}
    \label{F-apdix-alpha-t}
\end{figure*}




\subsection{The Feature Resonance Phenomenon Induced by Different Target Vectors}\label{subsec-apdix-FR-diff-target}
We explore the phenomenon of feature resonance using different target vectors. Experiments are conducted on two datasets with real \( N \)-category labels, Squirrel and WikiCS (\( N \) represents the number of categories). First, based on the neural collapse theory \citep{papyan2020prevalence,zhou2022all}, we preset \( N \) target vectors, each representing a category. These \( N \) target vectors form an equiangular tight frame, maximizing the separation between them. The definition of the simplex equiangular tight frame is introduced as follows:
\begin{definition}\label{def-ETF}
\textbf{Simplex ETF.} \citep{xiao2024targeted} A simplex equiangular tight
frame (ETF) refers to a collection of K equal-length and maximally-equiangular P-dimensional embedding vectors $\mathbf{E} = [e_1, \cdots, e_K] \in \mathbb{R}^{P \times K}$ which satisfies:
\begin{equation}
    \mathbf{E} = \sqrt{\frac{K}{K-1}}\mathbf{U}\big( \mathbf{I}_K - \frac{1}{K} \mathbf{1}_K\mathbf{1}_K^{\top} \big)
\end{equation}
where $\mathbf{I}_K$ is the identity matrix,$\mathbf{1}_K$ is an all-ones vector, and
$\mathbf{U} \in \mathbb{R}^{P \times K} (P \geq K) $ allows a rotation.
\end{definition}
All vectors in a simplex ETF $\mathbf{E}$ have an equal $\mathit{l}_2$ norm and
the same pair-wise maximal equiangular angle $-\frac{1}{K-1}$,
\begin{equation}
    e_{k_1}^{\top}e_{k_2} = \frac{K}{K-1}\delta_{k_1,k_2} - \frac{1}{K-1}, \forall k_1, k_2 \in [1, K]
\end{equation}
where $\delta_{k_1, k_2} = 1$ when $k_1 = k_2$ and $0$ otherwise. 

We use MSE loss to pull the representations of known ID nodes toward their corresponding target vectors based on their labels, as follows:
\begin{equation}
    \ell(h_{\theta_t}({\boldsymbol X}_{\text{known}}),e) = \mathbb{E}(\parallel \mathbf{E}_{\text{known}} - ({\boldsymbol X}_{\text{known}}\mathbf{W}^{\top})\parallel^2_2 )
\end{equation}
where \( \mathbf{E}_{\text{known}} \) denotes the target vector matrix corresponding to the known ID nodes.

The trajectory trends and lengths of unknown ID nodes differ significantly from those of OOD nodes, with the former showing more distinct trends and longer trajectories. We refer to this as the feature resonance phenomenon and leverage it to filter OOD nodes. As shown in Table \ref{tabel-diff-label}, under the ``True multi-label" row, the experimental results demonstrate that this method is effective and performs well.
Interestingly, even with random labels for known ID nodes or aligning all known ID representations to a fixed target vector, unknown ID nodes consistently exhibit longer trajectories than unknown OOD nodes, as shown in Table \ref{tabel-diff-label}. 

The experiments above indicate that the feature resonance phenomenon is \textit{label-independent} and results from the intrinsic relationships between ID node representations. Therefore, this is highly suitable for category-free OOD detection scenarios without multi-category labels.


% \subsection{More Detailed Hyperparameter Analysis}
% We provide a more detailed error analysis in Figure \ref{F-appendix-alpha-t}. Experiments show that although using resonance-based fractions to filter OOD nodes increases errors as the number of epochs grows, we can mitigate this error growth by reducing the learning rate. This approach also extends the low-error interval, allowing for effective sampling of OOD nodes. 

% We believe the reason why the error increases with epochs is that the model primarily fits the representations of ID nodes during the early stages of training. Later, overfitting to the ID node representations may occur, leading the model to learn features that could be related to OOD node representations \citep{zhang2024feature}, thereby causing the error to increase. However, by using a lower learning rate in the initial training phase, we can slow down the model's convergence, induce the phenomenon of feature resonance, and facilitate the identification of OOD samples.






\begin{figure*}[!t]
	\centering
   \subfigure[WikiCS]{
    % \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.32\linewidth]{Figures/line_new/FPR95_AUROC_line_chart_WikiCS.png}
	% \end{minipage}
 }
   \subfigure[YelpChi]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.3\linewidth]{Figures/line_new/FPR95_AUROC_line_chart_YelpChi.png}
	% \end{minipage}
 }
    \caption{The impact of different sliding window widths on the performance of detecting OOD nodes. When the width is 1, it corresponds to the resonance-based score $\tau$.}
    \label{F-slide-window}
\end{figure*}

\begin{figure*}[!t]
	\centering
   \subfigure[Pre-training (Energy\textit{Def})]{
    % \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.22\linewidth]{Figures/tsne/without_wikics_tsne_clustering.png}
	% \end{minipage}
 }
   \subfigure[Pre-training (Ours)]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.22\linewidth]{Figures/tsne/with_wikics_tsne_clustering.png}
	% \end{minipage}
 }
   \subfigure[Post-training (Energy\textit{Def})]{
    % \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.22\linewidth]{Figures/tsne/trained_without_wikics_tsne_clustering.png}
	% \end{minipage}
 }
   \subfigure[Post-training (Ours)]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.22\linewidth]{Figures/tsne/trained_with_wikics_tsne_clustering.png}
	% \end{minipage}
 }
    \caption{T-SNE visualization of node embeddings on the dataset \textit{WikiCS}. (a) Synthetic nodes (red) generated by Energy\textit{Def} fail to accurately represent the actual features of OOD nodes (blue), whereas ours can, as shown in (b). (c) Representations of ID (green) and OOD (blue) nodes trained with synthetic nodes generated by Energy\textit{Def} are poorly separated, whereas ours can, as shown in (d).}
    \label{F-TSNE}
\end{figure*}


% \subsection{Time Efficiency}\label{subsec-time-efficiency}
% We compare the time consumption of our method, RSL, with the current SOTA method, Energy\textit{Def}. The experimental results are shown in Table \ref{tabel-time}. The experiments show that the overall time efficiency of RSL is comparable to that of Energy\textit{Def}, with similar time consumption across different datasets. However, it is worth noting that when we use the resonance-based score $\tau$ alone for OOD node detection, its efficiency improves significantly over Energy\textit{Def}, with an average reduction of 79.81\% in time consumption. This indicates that $\tau$ not only demonstrates significant effectiveness in detecting OOD nodes but also offers high efficiency.






\subsection{Variation of Microscopic Feature Resonance During Training}
We also observe the variation of the microscopic feature resonance phenomenon during the training process on other datasets, as shown in Figure \ref{F-apdix-alpha-t}. We find that the changes on Reddit, YelpChi, and WikiCS are generally consistent with Amazon, with the most significant feature resonance occurring in the middle of the training process. However, for Squirrel, the feature resonance phenomenon reaches its most pronounced level early in the training. We believe this is due to the relatively rich features in Squirrel, which allow the model to quickly identify the optimal optimization path for ID samples in the early stage of training.






\subsection{The Impact of Different Sliding Window Widths on Performance} \label{subsec-Apdix-SW}

We investigate the impact of different sliding window widths on the effectiveness of detecting OOD nodes. The experimental results in Figure \ref{F-slide-window} show that as the sliding window width increases, the detection performance for OOD nodes gradually decreases. This suggests that a more fine-grained observation leads to better detection performance.



\subsection{Node Representation Visualization}
Energy\textit{Def} generates auxiliary synthetic OOD nodes via SGLD to train an OOD classifier for category-free OOD node detection. However, we find that the synthetic OOD nodes from Energy\textit{Def} do not accurately capture the features of actual OOD nodes. As shown in Figure \ref{F-TSNE}(a), most synthetic OOD nodes are separated from actual OOD nodes and even overlap with ID nodes, limiting the classifier's performance. The severe overlap between ID and OOD node representations after training by Energy\textit{Def} (Figure \ref{F-TSNE}(c)) further highlights this issue. In contrast, we use feature resonance to identify reliable OOD nodes and synthesize new ones based on these. As seen in Figure \ref{F-TSNE}(b), our synthetic OOD nodes align more closely with the actual OOD nodes. Training with these nodes results in better separation between ID and OOD node representations, as shown in Figure \ref{F-TSNE}(d).


\section{Discussion}
\subsection{Differences from Gradient-Based Methods}
It is important to note that our method RSL differs significantly from previous gradient-based methods:

\textit{1) Originating from the Commonality of Representations.} Our method is based on the conjecture that there are inherent commonalities between the representations of the ID sample, which are independent of gradients.

 \textit{2) No Pre-trained Multi-category Classifier Required.}  
Gradient-based methods like GradNorm \citep{huang2021importance} compute the KL divergence between an unknown sample's softmax output from a multi-category classifier and a uniform distribution, using the gradient norm to distinguish OOD samples. OOD samples, with uniform softmax outputs, yield more minor gradient norms, whereas sharper outputs for ID samples produce more significant norms. Similarly, SAL \citep{du2024does} uses pseudo-labels from a multi-category classifier for unknown samples, continuing training to compute gradients, and identifies OOD samples via the gradient's principal component projection. These methods require a pre-trained multi-category classifier, making them unsuitable for category-free scenarios without labels, whereas our RSL method avoids this limitation.


 \textit{ 3) No Need to Compute Gradients for Unknown Samples.} As shown in Equation \ref{equa-delta-f}, we only need the representations of unknown samples to compute our resonance-based score. This significantly enhances the flexibility of our method, as we can detect OOD samples during any optimization of known ID representations without the need to wait until after the optimization is complete.

\section{Algorithm Pseudo-code}\label{Algorithm}

\begin{algorithm}[H]
\caption{Resonance-based Separate and Learn (RSL) Framework for Category-Free OOD Detection}
\label{alg:RSL}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Known ID nodes $\mathcal{V}_{\mathrm{known}}$, Wild nodes $\mathcal{V}_{\mathrm{wild}}$, Target vector $e$ with random initial, Validation set $\mathcal{V}_{\mathrm{val}}$
\STATE \textbf{Output:} OOD classifier $E_{\theta}$

\STATE \textbf{Phase 1: Feature Resonance Phenomenon}
\STATE Initialize model $h_{\theta}$ with random parameters $\theta$
\FOR{$t = 1$ to $\mathbb{T}$ (training epochs)}
    \STATE Optimize $h_{\theta_t}(\cdot)$ to align $\mathcal{V}_{\mathrm{known}}$ with target $e$:
    \[
    \ell(h_{\theta_t}({\boldsymbol X}_{\text{known}}), e) = \mathbb{E}(\parallel \mathbf{1}^\top e - ({\boldsymbol X}_{\text{known}}\mathbf{W}^\top) \parallel^2_2)
    \]
    \STATE Calculate the representation change of $\tilde{v}_i \in \mathcal{V}_{\mathrm{wild}}$ : $\Delta h_{\theta_t}(\tilde{\mathbf{x}}_i) = h_{\theta_{t+1}}(\tilde{\mathbf{x}}_i) - h_{\theta_t}(\tilde{\mathbf{x}}_i)$
    \STATE Compute resonance-based score $\tau_i = \parallel \Delta h_{\theta_t}(\tilde{\mathbf{x}}_i) \parallel_2$
\ENDFOR
\STATE Identify the period of feature resonance using the validation set, selecting $t$ where $\tau$ best separates ID and OOD nodes.

\STATE \textbf{Phase 2: Candidate OOD Node Selection}
\STATE Define candidate OOD set:
\[
\mathcal{V}_{\mathrm{cand}} = \{ \tilde{v}_i \in \mathcal{V}_{\mathrm{wild}} : \tau_i \leq T \}
\]

\STATE \textbf{Phase 3: Synthetic OOD Node Generation}
\FOR{each $\hat{v}_j \in \mathcal{V}_{\mathrm{syn}}$ (synthetic OOD nodes)}
    \STATE Generate $\hat{\mathbf{x}}_j^{(t+1)}$ with random initial using:
    \[
    \hat{\mathbf{x}}_j^{(t+1)} = \lambda \big (\hat{\mathbf{x}}_j^{(t)} - \frac{\alpha}{2} \nabla_{\hat{\mathbf{x}}_j^{(t)}}E_{\theta}(\hat{v}_j^{(t)}) + \epsilon \big) + (1-\lambda)\mathbb{E}_{\mathbf{x} \sim \boldsymbol{X}_{\mathrm{cand}}}(\mathbf{x} - \hat{\mathbf{x}}_j^{(t)}), , \epsilon \sim \mathcal{N}(0, \zeta)
    \]
\ENDFOR

\STATE \textbf{Phase 4: OOD Classifier Training}
\STATE Define training set $\mathcal{V}_{\mathrm{train}} = \mathcal{V}_{\text{known}} \cup \mathcal{V}_{\mathrm{cand}} \cup \mathcal{V}_{\mathrm{syn}}$
\STATE Assign labels $\boldsymbol{Y}_{\mathrm{train}}$ for ID nodes ($1$) and OOD nodes ($0$)
\STATE Train $E_{\theta}$ using binary cross-entropy loss:
\[
\ell_{\text{cls}} = \mathbb{E}_{v \sim \mathcal{V}_{\mathrm{train}}}\big( \mathrm{y}_{v}\mathrm{log}(\sigma(E_{\theta}(v))) + (1-\mathrm{y}_{v})\mathrm{log}(1-\sigma(E_{\theta}(v))) \big)
\]
\STATE \textbf{Return:} Trained OOD classifier $E_{\theta}$
\end{algorithmic}
\end{algorithm}