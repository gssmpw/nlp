




% \begin{figure*}[!t]
% 	\centering
%    \subfigure[Pre-training (Energy\textit{Def})]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.22\linewidth]{Figures/tsne/without_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%    \subfigure[Pre-training (Ours)]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.22\linewidth]{Figures/tsne/with_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%    \subfigure[Post-training (Energy\textit{Def})]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.22\linewidth]{Figures/tsne/trained_without_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%    \subfigure[Post-training (Ours)]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.22\linewidth]{Figures/tsne/trained_with_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%     \caption{T-SNE visualization of node embeddings on the dataset \textit{WikiCS}. (a) Synthetic nodes (red) generated by Energy\textit{Def} fail to accurately represent the actual features of OOD nodes (blue), whereas ours can, as shown in (b). (c) Representations of ID (green) and OOD (blue) nodes trained with synthetic nodes generated by Energy\textit{Def} are poorly separated, whereas ours can, as shown in (d).}
%     \label{F-TSNE}
% \end{figure*}



% \begin{figure*}[!t]
% 	% \centering
%   \subfigure[Energy\textit{Def}]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.3\linewidth]{Figures/kde/kde_etf_amazon_without.png}
% 	% \end{minipage}
%  }
%   \subfigure[Resonance-based Filter Score (Ours)]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.3\linewidth]{Figures/kde/kde_pure_amazon.png}
% 	% \end{minipage}
%  }
%   \subfigure[RSL (Ours)]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.3\linewidth]{Figures/kde/kde_etf_amazon.png}
% 	% \end{minipage}
%  }
%     \caption{The score distribution of ID nodes and OOD nodes on \textit{Amazon}  obtained using different methods.}
%     \label{F-Frequency}
% \end{figure*}



\section{Experiment}
\label{Sec-Experiment}
 % In Section \ref{subsec-Experimental-Setup}, we describe the experimental setup. In Section \ref{subsec-main-result}, we compare the performance of our method with current SOTA methods. In Section \ref{subsec-ablation-result}, we conduct ablation experiments to validate the effectiveness of the Feature Resonance phenomenon and synthetic OOD nodes.
 % % In Section \ref{subsec-Hyperparameter-Analysis}, we analyze the hyperparameters \(\alpha\) and \(t\) related to the error bound in Theorem \ref{theorem-2}. 
 % In Section \ref{subsec-visualization}, we perform visualization experiments on the node representations and score distributions obtained by different methods. In addition, we compare the time efficiency of the various methods in Appendix \ref{subsec-time-efficiency}.


\subsection{Experimental Setup}\label{subsec-Experimental-Setup}
\paragraph{Datasets.}
We conduct extensive experiments to evaluate RSL on five real-world OOD node detection datasets: Two multi-category datasets, Squirrel \citep{rozemberczki2021multi} and WikiCS \citep{mernyei2020wiki}, and three binary classification fraud detection datasets: YelpChi \citep{rayana2015collective}, Amazon \citep{mcauley2013amateurs}, and Reddit \citep{kumar2019predicting}.
% \begin{itemize}[nosep, topsep=0pt, leftmargin=*]
%     \item \textbf{Squirrel} is a Wikipedia page-page network where nodes represent articles from the English Wikipedia, and edges reflect mutual links between them. 
%     \item \textbf{WikiCS} comprises nodes that represent Computer Science articles, with edges based on hyperlinks. 
%     \item \textbf{YelpChi} collects hotel and restaurant reviews from Yelp, classifying legitimate (resp. spam) reviews as ID (resp. OOD) nodes.
%     \item \textbf{Amazon} includes the product reviews of the Musical Instrument category on Amazon.com, in which ID (resp. OOD) nodes are benign (resp. fraud) users.
%     \item \textbf{Reddit} comprises user posts collected from various subreddits over one month, where ID (resp. OOD)
%     nodes are normal (resp. banned) users.
% \end{itemize}
The statistics of these datasets are summarized in Table \ref{tabel-datasets}. We
provide detailed dataset description in the Appendix \ref{subsec-apdix-Dataset}.

% \paragraph{Datasets.}  
% We evaluate RSL on five diverse OOD node detection datasets. These include two datasets with multi-category labels, \textbf{Squirrel} dataset \citep{rozemberczki2021multi, pei2020geom} and \textbf{WikiCS} dataset \citep{mernyei2020wiki}, and three binary classification datasets, \textbf{YelpChi} dataset \citep{rayana2015collective}, \textbf{Amazon} dataset \citep{mcauley2013amateurs}, and \textbf{Reddit} dataset \citep{kumar2019predicting}. 
% % The \textbf{Squirrel} dataset \citep{rozemberczki2021multi, pei2020geom} is a heterophilous Wikipedia network where nodes represent articles and edges represent hyperlinks, with nodes categorized into five classes. The \textbf{WikiCS} dataset \citep{mernyei2020wiki} consists of Computer Science articles as nodes, connected by hyperlinks and classified into 10 sub-fields. The \textbf{YelpChi} dataset \citep{rayana2015collective} contains Yelp reviews, where legitimate reviews are labeled as ID nodes and spam reviews as OOD nodes. The \textbf{Amazon} dataset \citep{mcauley2013amateurs} features reviews from the Musical Instrument category, with benign users as ID nodes and fraudulent users as OOD nodes. Finally, the \textbf{Reddit} dataset \citep{kumar2019predicting} includes user posts from various subreddits, where normal users are treated as ID nodes and banned users as OOD nodes.
% The statistics of these datasets are summarized in Table \ref{tabel-datasets}. We
% provide detailed dataset description in the Appendix \ref{subsec-apdix-Dataset}.


\begin{figure}[!t]
\centering
\includegraphics[width=0.8\linewidth]{Figures/bar/FPR95_bar_chart-1.png}
    \caption{Performance of detecting OOD nodes with different metrics. $\tau$ represents the resonance-based score, the ``Overall Trajectory" represents the total cumulative length of the training trajectory $\hat{F}(\tilde{\mathbf{x}}_i) = \sum_t \tau_i$, and the ``Sliding Window" refers to the cumulative $\tau$ within a window of width 10: $\hat{F}_{10}(\tilde{\mathbf{x}}_i) = \sum^t_{t-10} \tau_i$.}
    \label{F-tau-F-SW}
    \vskip -0.1in
\end{figure}

\paragraph{Baselines.}  
We assess the performance of RSL against a diverse range of baseline methods spanning five categories:  \textit{1) Traditional outlier detection methods}, including local outlier factor \citep{breunig2000lof} with $k$-nearest neighbors (LOF-KNN) and MLP autoencoder (MLPAE). \textit{2) Graph-based outlier detection models}, including GCN autoencoder \citep{kipf2016variational}, GAAN  \citep{chen2020generative}, DOMINANT \citep{ding2019deep}, ANOMALOUS  \citep{peng2018anomalous}, and SL-GAD \citep{zheng2021generative}. \textit{3) Transformation-based outlier detection approaches}, such as GOAD \citep{bergman2020classification} and NeuTral AD  \citep{qiu2021neural}. \textit{4) Entropy-based detection techniques}, including GKDE \citep{zhao2020uncertainty}, OODGAT  \citep{song2022learning}, and GNNSafe \citep{wu2023energy}.  \textit{5) Category-free detection methods}, including Energy\textit{Def} \citep{gong2024energy} and SSD \citep{sehwag2021ssd}. 
% \begin{itemize}[nosep, topsep=0pt, leftmargin=*]  
%     \item \textit{Traditional outlier detection methods}, such as local outlier factor \citep{breunig2000lof} with $k$-nearest neighbors (LOF-KNN) and MLP autoencoder (MLPAE).  
%     \item \textit{Graph-based outlier detection models}, including GCN autoencoder \citep{kipf2016variational}, GAAN \citep{chen2020generative}, DOMINANT \citep{ding2019deep}, ANOMALOUS \citep{peng2018anomalous}, and SL-GAD \citep{zheng2021generative}.  
%     \item \textit{Transformation-based outlier detection approaches}, such as GOAD \citep{bergman2020classification} and NeuTral AD \citep{qiu2021neural}.  
%     \item \textit{Entropy-based detection techniques}, including GKDE \citep{zhao2020uncertainty}, OODGAT \citep{song2022learning}, and GNNSafe \citep{wu2023energy}.  
%     \item \textit{Category-free detection methods}, represented by Energy\textit{Def} \citep{gong2024energy}.  
% \end{itemize}  
Details of baselines and implementation are in Appendix \ref{subsec-apdix-Baseline} and \ref{subsec-apdix-Implementation}, respectively.






\begin{figure*}[!t]
	% \centering
  \subfigure[Energy\textit{Def}]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.3\linewidth]{Figures/kde/kde_etf_amazon_without.png}
	% \end{minipage}
 }
  \subfigure[Resonance-based Filter Score $\tau$ (Ours)]{
    % \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.3\linewidth]{Figures/kde/kde_pure_amazon.png}
	% \end{minipage}
 }
  \subfigure[RSL (Ours)]{
    % \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.3\linewidth]{Figures/kde/kde_etf_amazon.png}
	% \end{minipage}
 }
    \caption{The score distribution of ID nodes and OOD nodes on \textit{Amazon}  obtained using different methods.}
    \label{F-Frequency}
    \vskip -0.1in
\end{figure*}







\paragraph{Metrics.}
Following prior research on OOD node detection, we evaluate the detection performance using three widely recognized, threshold-independent metrics: AUROC ($\uparrow$), AUPR ($\uparrow$) and FPR95($\downarrow$). We provide a detailed metric description in the Appendix \ref{sec-apdix-metric}.
% Following prior research on OOD node detection, we use three widely recognized threshold-independent metrics:
% (1) \textbf{AUROC ($\uparrow$)} measures the area under the receiver operating characteristic curve, capturing the trade-off between the true positive rate and the false positive rate across different threshold values.  
% (2) \textbf{AUPR ($\uparrow$)} calculates the area under the precision-recall curve, representing the balance between the precision rate and recall rate for OOD nodes across varying thresholds.  
% (3) \textbf{FPR95 ($\downarrow$)} is defined as the probability that an OOD sample is misclassified as an ID node when the true positive rate is set at 95\%.

% \paragraph{Implementation Details}
% We adopt the same dataset settings as Energy\textit{Def} \citep{gong2024energy}. It is worth noting that, under this dataset setup, the features of unknown nodes are accessible. Therefore, using the features of unknown nodes during the training phase to filter reliable OOD nodes is a legitimate strategy. Specifically, for the Squirrel and WikiCS datasets, we randomly select one and two classes as OOD classes, respectively. In the case of fraud detection datasets, we categorize a large number of legitimate entities as ID nodes and fraudsters as OOD nodes. We allocate 40\% of the ID class nodes for training, with the remaining nodes split into a 1:2 ratio for validation and testing, ensuring stratified random sampling based on ID/OOD labels.

% We report the average value of five independent runs for each dataset. The hyper-parameters are shown in Table \ref{tabel-hyperparameters} in the Appendix. 
% The anomaly detection baselines are trained entirely based on graph structures and node attributes without requiring ID annotations. 
% We adapt these models to the specifications of our OOD node detection tasks by minimizing the corresponding loss items solely on the ID nodes, where applicable. 








\subsection{Main Results}\label{subsec-main-result}
% We discover the phenomenon of feature resonance, where, when using labeled ID samples for pretraining tasks, the representations of unlabeled ID samples exhibit more significant changes along the gradient direction. Leveraging this phenomenon, we design a simple and direct pretraining proxy task for category-free labeled ID nodes. This task involves pulling the representations of these nodes toward any fixed vector and calculating the representation change of unlabeled OOD nodes between adjacent epochs as an indication for detecting OOD nodes. The smaller the change, the more likely the node will be an OOD node. Furthermore, we use the selected candidate OOD nodes as templates to generate more realistic synthetic OOD nodes that better match the actual OOD distribution. These are then used to train the OOD classifier.

% We propose RSL (Resonance-based Separate and Learn) to detect category-free OOD nodes using the Feature Resonance phenomenon, where unknown ID nodes exhibit more significant representation changes than OOD nodes. We use the Resonance-based Filtering Score to identify OOD nodes, with more minor scores indicating higher OOD likelihood. To enhance detection, we generate synthetic OOD nodes using  SGLD and train a binary classifier with labeled ID, candidate OOD, and synthetic OOD nodes.

Table \ref{table-Main} presents the main experimental results of various methods across five public datasets.
The traditional methods like {LOF-KNN} and {MLPAE} perform poorly across most datasets, particularly with high false positive rates. Graph-based methods such as {GCNAE}, {GAAN}, and {DOMINANT} show some decent results but generally lag behind {RSL} and {Energy\textit{Def}}. Entropy-based methods like {OODGAT} and {GNNSafe} perform well on datasets with multi-category label information (e.g., WikiCS) but struggle on datasets without such labels, like {YelpChi}. Overall, these methods tend to be less robust compared to {RSL}.  Specifically, RSL achieves significant improvements on most datasets. On average, it improves by 5.24\% and \textbf{19.26}\% in AUROC and AUPR, respectively, and reduces FPR95 by \textbf{18.51}\%. Notably, when additional multi-category label information is available, GNNSafe outperforms EnergyDef on WikiCS. However, even in the absence of multi-class label information, our method still outperforms GNNSafe, achieving a remarkable improvement of 13.33\% in AUROC, 22.07\% in AUPR, and a \textbf{35.43}\% reduction in FPR95. These results powerfully demonstrate the effectiveness of our method.
% for category-free OOD node detection tasks.








\begin{table}[!t]
% \scriptsize
\centering
\caption{The effectiveness of the resonance-based filter score 
$\tau$ in filtering OOD nodes with different alignment targets for known ID node representations. \textbf{True multi-label} means aligning ID node representations with multiple target vectors based on true multi-class labels. \textbf{Multiple random vectors} means aligning ID node representations with random target vectors. \textbf{A random vector} means aligning ID node representations with a single target vector.}\label{tabel-diff-label}
% \scriptsize % Reduce font size
% \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|ccc|ccc}
\hline
\hline
\multirow{2}*{\diagbox{\textbf{Method}}{\textbf{Dataset}}} &\multirow{2}*{\textbf{Target}}  &\multicolumn{3}{c|}{\textbf{Squirrel}} &\multicolumn{3}{c}{\textbf{WikiCS}}\\
\cline{3-8}
~ &~ &\multicolumn{6}{c}{AUROC $\uparrow \ \ \ $ AUPR $\uparrow \ \ \ $ FPR@95 $\downarrow$}\\
\hline
$\text{Energy\textit{Def}}$& - &{64.15} &{37.40} &{91.77} &70.22 &60.10 &83.17 \\
\hline
RSL w/o classifier & True multi-label &{61.63} &{37.12} &{90.62} &71.03 &72.47 &81.96 \\

RSL w/o classifier & Multiple random vectors &61.44 &37.39 &90.62 &{73.64} &{74.13} &{69.25} \\

RSL w/o classifier & A random vector &{61.52} &{38.96} &{90.18} &79.15 &78.65 &70.38 \\
\hline
\hline
\end{tabular}
}
\vskip -0.1in
\end{table}


% \begin{table}[!t]
% \centering
% \caption{Time cost (s).}\label{tabel-time}
% \scriptsize % Reduce font size
% \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% \hline
% \diagbox{\textbf{Method}}{\textbf{Dataset}}&\textbf{Squirrel} & \textbf{WikiCS} & \textbf{YelpChi} & \textbf{Amazon} & \textbf{Reddit}\\ 
% \hline
% Energy\textit{Def} &10.94 &27.11 &76.51 &33.81 &26.44 \\
% RSL w/o classifier &5.25 &4.03 &5.41 &5.75 &3.71\\
% RSL &11.54 &17.53 &74.83 &36.33 &38.23 \\
% \hline
% \hline
% \end{tabular}
% \end{table}




\subsection{Ablation Study}\label{subsec-ablation-result}


\paragraph{How effective is resonance-based filter score $\tau$?} 
The experimental results in the row labeled ``RSL w/o classifier" of Table \ref{table-Main} show that using the raw resonance-based score $\tau$ to filter OOD nodes is already more effective than the SOTA method on most datasets. On the FPR95 metric, the resonance-based score achieves an average reduction of 10.93\% compared to current SOTA methods. Notably, even when compared to methods that leverage additional multi-category label information, our approach continues to demonstrate a clear performance advantage. For example, on the WikiCS dataset, the resonance-based score reduces the FPR95 metric by 7.69\% compared to GNNSafe.
% This highlights that our resonance-based score not only offers broad applicability (regardless of whether the ID nodes have multi-category labels) but also exhibits outstanding performance in OOD node detection.


\paragraph{How effective are the synthetic OOD nodes combined with the feature resonance score?}
 % To answer this, we conduct an ablation experiment to train the OOD classifier using only the candidate OOD nodes. 
 The experimental results in the row labeled ``RSL w/o $\mathcal{V}_{\mathrm{syn}}$" of Table \ref{table-Main} show that after removing the synthetic OOD nodes, the performance of the trained OOD classifier declined to varying degrees. This indicates that synthetic OOD nodes enhance the generalization ability of the OOD classifier, allowing it to detect more OOD nodes more accurately.
 It is worth noting that our synthetic OOD nodes, generated by leveraging real OOD nodes selected using $\tau$, better align with real-world OOD scenarios and, therefore, outperform Energy\textit{Def}.






% \subsection{Hyperparameter Analysis}\label{subsec-Hyperparameter-Analysis}
% Theorem \ref{theorem-2} demonstrates that the error of Feature Resonance in detecting OOD nodes is inversely proportional to the learning rate (\(\alpha\)) and the number of epochs (\(t\)). We conducted hyperparameter experiments on the Amazon dataset to reflect the error using the AUROC and FPR95 metrics. The experimental results in Figure \ref{F-alpha-t} show that as the number of epochs (\(t\)) increases, the error of FR in detecting OOD nodes also increases. By reducing the learning rate (\(\alpha\)), we can lower the error at the same \(t\) and extend the duration of the low-error interval to sample reliable OOD nodes. These experimental results align well with our theoretical findings.


\subsection{Effectiveness of Feature Resonance Score in Selecting OOD Nodes}
We aim to evaluate the performance of RSL when integrated with methods other than the resonance-based score for selecting reliable OOD nodes. To ensure fairness, we used the same parameters and selected the same number of OOD nodes. From a metric learning perspective, we computed the cosine similarity, Euclidean distance, and Mahalanobis distance between unknown nodes and the prototypes of known ID nodes, with smaller values indicating a higher likelihood of being OOD nodes. We also applied Energy\textit{Def} for OOD node selection. The results, presented in Table \ref{table-diff-select}, show that, under the same conditions, the OOD nodes selected using $\tau$ are more reliable than those selected by the other methods.

\subsection{Effectiveness of Different Scoring Strategies Based on Feature Resonance}
% We evaluate the effectiveness of different score design strategies based on feature resonance. In addition to the resonance-based score $\tau$, we design two other scores: the norm of the global trajectory and the accumulation within a sliding window of width 10. Our experiments in Figure \ref{F-tau-F-SW} show that, on most datasets, $\tau$ significantly outperforms the other two scores. At the same time, the sliding window approach performs better than the global trajectory norm, and more detailed analyses of width are provided in Appendix \ref{subsec-Apdix-SW}. This suggests that considering finer-grained information leads to more accurate detection of OOD nodes. Thus, we choose $\tau$ as the primary score for reliably filtering OOD nodes in our approach.
We evaluate the effectiveness of three score design strategies based on feature resonance: the resonance-based score $\tau$, the global trajectory norm, and the sliding window accumulation (width 10). As shown in Figure \ref{F-tau-F-SW}, $\tau$ outperforms the other two scores on most datasets. The sliding window approach performs better than the global trajectory norm, with further details on width in Appendix \ref{subsec-Apdix-SW}. This indicates that finer-grained information improves OOD node detection, so we select $\tau$ as the primary score for filtering OOD nodes in our method.


\subsection{Feature Resonance with Different Target Vectors} 
% We explore the phenomenon of feature resonance using different target vectors. Experiments are conducted on two datasets with real \( N \)-category labels, Squirrel and WikiCS (\( N \) represents the number of categories). First, based on the neural collapse theory \citep{papyan2020prevalence,zhou2022all}, we preset \( N \) target vectors, each representing a category. These \( N \) target vectors form a simplex equiangular tight frame \footnote{The definition of the simplex equiangular tight frame is introduced in Appendix \ref{def-ETF}.}, maximizing the separation between them. As shown in Table \ref{tabel-diff-label}, under the ``True multi-label" row, the experimental results demonstrate that this method is effective and performs well.
% Interestingly, even with random labels (the ``Multiple random vectors" row) for known ID nodes or aligning all known ID representations to a fixed target vector (the ``A random vector" row), unknown ID nodes consistently exhibit longer trajectories than unknown OOD nodes, as shown in Table \ref{tabel-diff-label}. 

% The experiments above indicate that the feature resonance phenomenon is \textit{label-independent} and results from the intrinsic relationships between ID node representations. Therefore, this is highly suitable for category-free OOD detection scenarios without multi-category labels.

% We investigate the phenomenon of micro-level feature resonance using different target vectors through experiments conducted on two datasets with true \( N \)-category labels, Squirrel and WikiCS. Leveraging the neural collapse theory \citep{papyan2020prevalence,zhou2022all}, we preset \( N \) target vectors, each corresponding to a category. These \( N \) target vectors form a simplex equiangular tight frame\footnote{The definition of the simplex equiangular tight frame is introduced in Appendix \ref{def-ETF}.}, which maximizes the separation between them. As shown in Table \ref{tabel-diff-label}, in the ``True multi-label" row, the results demonstrate the effectiveness of this approach. 
% Interestingly, even when random labels are assigned (the ``Multiple random vectors" row) to known ID nodes or when all known ID representations are aligned to a fixed target vector (the ``A random vector" row), unknown ID nodes consistently exhibit larger $\tau$ than unknown OOD nodes, as evidenced in Table \ref{tabel-diff-label}. 

% These findings indicate that the feature resonance phenomenon is \textit{label-independent}, arising from intrinsic relationships between ID node representations. This makes it highly suitable for category-free OOD detection scenarios without multi-category labels.


We explore micro-level feature resonance using different target vectors through experiments on Squirrel and WikiCS datasets with true \( N \)-category labels. Based on neural collapse theory \citep{papyan2020prevalence,zhou2022all}, we set \( N \) target vectors that form a simplex equiangular tight frame \footnote{The definition of the simplex equiangular tight frame is introduced in Appendix \ref{def-ETF}.}, maximizing separation. As shown in Table \ref{tabel-diff-label}, the "True multi-label" row demonstrates the effectiveness of this approach. Interestingly, even when random labels are assigned (the "Multiple random vectors" row) or when all ID representations align with a fixed vector (the "A random vector" row), unknown ID nodes still show larger $\tau$ than unknown OOD nodes, as seen in Table \ref{tabel-diff-label}. These results suggest that feature resonance is \textit{label-independent}, stemming from intrinsic relationships between ID node representations, making it suitable for category-free OOD detection.


\begin{table}[!t]
\centering
\caption{Time cost (s).}\label{tabel-time}
\scriptsize % Reduce font size
\setlength{\tabcolsep}{1.5mm} % Adjust column spacing
\begin{tabular}{c|c|c|c|c|c}
\hline
\hline
\diagbox{\textbf{Method}}{\textbf{Dataset}}&\textbf{Squirrel} & \textbf{WikiCS} & \textbf{YelpChi} & \textbf{Amazon} & \textbf{Reddit}\\ 
\hline
Energy\textit{Def} &10.94 &27.11 &76.51 &33.81 &26.44 \\
RSL w/o classifier &5.25 &4.03 &5.41 &5.75 &3.71\\
RSL &11.54 &17.53 &74.83 &36.33 &38.23 \\
\hline
\hline
\end{tabular}
\vskip -0.1in
\end{table}

\subsection{Time Efficiency}\label{subsec-time-efficiency}
We compare the time consumption of our method, RSL, with the current SOTA method, Energy\textit{Def}. The experimental results are shown in Table \ref{tabel-time}. The experiments show that the overall time efficiency of RSL is comparable to that of Energy\textit{Def}, with similar time consumption across different datasets. However, it is worth noting that when we use the resonance-based score $\tau$ alone for OOD node detection, its efficiency improves significantly over Energy\textit{Def}, with an average reduction of \textbf{79.81\%} in time consumption. This indicates that $\tau$ not only demonstrates significant effectiveness in detecting OOD nodes but also offers high efficiency.




% \subsection{Visualization Experiments}\label{subsec-visualization}
\subsection{Score Distribution Visualization}\label{subsec-visualization}
% \paragraph{Node Representation Visualization.}
% Energy\textit{Def} generates synthetic OOD nodes via SGLD, but these nodes often fail to accurately represent actual OOD nodes, leading to poor classifier performance due to overlap with ID nodes (Figure \ref{F-TSNE}(a) and \ref{F-TSNE}(c)). In contrast, our method uses Feature Resonance to identify reliable OOD nodes, generating synthetic OOD nodes that better match the actual OOD node distribution, resulting in improved representation separation between ID and OOD nodes (Figure \ref{F-TSNE}(b) and \ref{F-TSNE}(d)).
% Energy\textit{Def} generates auxiliary synthetic OOD nodes via SGLD to train an OOD classifier for category-free OOD node detection. However, we find that the synthetic OOD nodes from Energy\textit{Def} do not accurately capture the features of actual OOD nodes. As shown in Figure \ref{F-TSNE}(a), most synthetic OOD nodes are separated from actual OOD nodes and even overlap with ID nodes, limiting the classifier's performance. The severe overlap between ID and OOD node representations after training by Energy\textit{Def} (Figure \ref{F-TSNE}(c)) further highlights this issue. In contrast, we use feature resonance to identify reliable OOD nodes and synthesize new ones based on these. As seen in Figure \ref{F-TSNE}(b), our synthetic OOD nodes align more closely with the actual OOD nodes. Training with these nodes results in a better separation between ID and OOD node representations, as shown in Figure \ref{F-TSNE}(d).

% \paragraph{Score Distribution Visualization.}
We visualize the score distributions of ID and OOD nodes on the Amazon dataset obtained using different methods, as shown in Figure \ref{F-Frequency}. When using the resonance-based score (Figure  \ref{F-Frequency} (b)), the majority of unknown ID nodes show more significant representation changes compared to 
 unknown OOD nodes. This separation of OOD nodes already exceeds Energy\textit{Def} (Figure  \ref{F-Frequency} (a)). After training with synthetic OOD nodes (Figure \ref{F-Frequency} (c)), the separation between the energy scores of ID and OOD nodes still improves compared to Energy\textit{Def}, which demonstrates the effectiveness of RSL.