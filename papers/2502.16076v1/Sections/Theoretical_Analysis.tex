
\begin{table}[!t]
% \scriptsize
\centering
\caption{The statistics of the real-world OOD node detection
datasets. $\times$ denotes no available multi-category labels. Notably, even on Squirrel and WikiCS, we do not use any true label as well. }\label{tabel-datasets}
% \setlength{\tabcolsep}{2mm}{
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
\hline
\textbf{Dataset} &\textbf{Squirrel} & \textbf{WikiCS} & \textbf{YelpChi} & \textbf{Amazon} & \textbf{Reddit}\\ 
\hline
\# Nodes &5,201 &11,701 &45,954 &11,944 &10,984 \\
\# Features &2,089 &300 &32 &25 &64 \\
Avg. Degree &41.7 &36.9 &175.2 &800.2 &15.3 \\
OOD node (\%) &20.0 &29.5 &14.5 &9.5 &3.3 \\
\# Category &5 &10 &$\times$  &$\times$  &$\times$\\
\hline
\hline
\end{tabular}
}
\end{table}



\begin{table*}[!t]
% \scriptsize
\centering
\caption{Category-free OOD detection on real-world datasets. ``OOM'' indicates out-of-memory, ``TLE'' means time limit exceeded, and ``-'' denotes inapplicability. Detectors with $\clubsuit$ use only node attributes, while $\spadesuit$ share RSLâ€™s GNN backbone. Entropy-based methods with $\lozenge$ use true multi-category labels, and $\blacklozenge$ rely on K-means pseudo labels. Top results: \darkred{1st}, \darkblue{2nd}.}\label{table-Main}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc}
\hline
\hline
\multirow{2}*{\diagbox{\textbf{Method}}{\textbf{Dataset}}}  &\multicolumn{3}{c|}{\textbf{Squirrel}} &\multicolumn{3}{c|}{\textbf{WikiCS}} &\multicolumn{3}{c|}{\textbf{YelpChi}} &\multicolumn{3}{c|}{\textbf{Amazon}} &\multicolumn{3}{c}{\textbf{Reddit}}\\
\cline{2-16}	
~ &\multicolumn{15}{c}{AUROC $\uparrow \ \ \ $ AUPR $\uparrow \ \ \ $ FPR@95 $\downarrow$}\\
\hline
$\text{LOF-KNN}^{\clubsuit}$ &51.85 &29.87 &95.21 &44.06 &37.48 &96.28 &56.39 &25.98 &92.57 &45.25 &14.26 &95.10 &57.88 &6.95 &93.24\\
$\text{MLPAE}^{\clubsuit}$ &43.15 &24.81 &97.98 &70.99 &63.74 &77.76 &51.90 &24.53 &92.42 &74.54 &51.59 &57.93 &52.10 &5.80 &94.43\\
\hline
GCNAE  &37.87 &22.64 &99.08 &57.95 &46.32 &92.97 &44.20 &19.22 &97.06 &45.07 &12.38 &98.54 &51.78 &6.14 &93.75\\
GAAN  &38.01 &22.57 &98.99 &58.15 &46.60 &93.37 &44.29 &19.30 &96.91 &53.26 &6.63 &98.05 &52.21 &5.96 &94.06\\
DOMINANT  &41.78 &24.73 &95.53 &42.55 &35.43 &97.22 &52.77 &24.90 &92.86 &78.08 &35.96 &76.05 &55.89 &6.03 &96.48\\
ANOMALOUS  &51.04 &29.09 &96.39 &67.99 &54.51 &92.74 &OOM &OOM &OOM &65.12 &25.15 &85.34 &55.18 &6.40 &94.10\\
SL-GAD &48.29 &27.62 &97.19 &51.87 &44.83 &95.26 &56.11 &26.49 &93.27 &82.63 &56.27 &51.36 &51.63 &6.02 &94.27\\
\hline
$\text{GOAD}^{\spadesuit}$  &62.32 &37.51 &92.28 &50.65 &37.22 &99.78 &58.03 &28.51 &89.84 &72.92 &45.53 &66.36 &52.89 &5.36 &94.26\\
$\text{NeuTral AD}^{\spadesuit} $ &52.51 &30.04 &97.16 &53.58 &43.49 &94.30 &55.81 &25.14 &94.23 &70.01 &24.36 &92.19 &55.70 &6.45 &94.59\\
\hline
$\text{GKDE}^{\lozenge}$ &56.15 &33.41 &94.96 &70.47 &61.18 &82.71 &- &- &- &- &- &- &- &- &-\\
 $\text{OODGAT}^{\lozenge}$ &58.84 &35.13 &93.31 &{74.13} &62.47 &84.48 &- &- &- &- &- &- &- &- &-\\
 $\text{GNNSafe}^{\spadesuit \lozenge}$  &56.38 &32.22 &95.17 &73.35 &{66.47} &{76.24} &- &- &- &- &- &- &- &- &-\\
$\text{OODGAT}^{\blacklozenge}$ &57.78 &34.66 &92.61 &52.76 &44.71 &90.02 &55.97 &23.07 &97.93 &82.54 &54.94 &52.10 &54.62 &6.05 &93.85\\
$\text{GNNSafe}^{\spadesuit \blacklozenge}$ &49.52 &26.63 &97.60 &64.15 &50.85 &92.63 &55.26 &26.68 &91.40 &68.51 &25.39 &84.31 &49.63 &5.36 &95.98\\
\hline
$\text{SSD}^{\spadesuit}$ &TLE &TLE &TLE &64.29 &58.45 &87.12 &55.39 &27.88 &91.63 &72.49 &41.82 &84.27 &59.74 &6.21 &91.15\\
$\text{Energy\textit{Def}}^{\spadesuit}$ &\darkred{64.15} &{37.40} &{91.77} &70.22 &60.10 &83.17 &{62.04} &{29.71} &{90.62} &{86.57} &{74.50} &{32.43} &\darkblue{63.32} &{8.34} &\darkblue{89.34}\\
\hline
RSL w/o classifier &{61.52} &\darkblue{38.96} &\darkblue{90.18} &79.15 &78.65 &70.38 &\darkblue{65.42} &{37.08} &{83.53} &{87.43} &\darkblue{83.31} &\darkred{19.56} &52.37 &6.97 &91.39\\

RSL w/o $\mathcal{V}_{\mathrm{syn}}$ &60.46 &34.89 &93.59 &\darkblue{81.21} &\darkblue{79.93} &\darkblue{52.19} &65.15  &\darkblue{38.93}  &\darkblue{81.84}  &\darkblue{87.81} &81.10 &25.18 &{61.36} &\darkblue{8.48} &{89.43}\\

RSL &\darkblue{64.12} &\darkred{39.58} &\darkred{89.90} &\darkred{84.01} &\darkred{81.14} &\darkred{49.23} &\darkred{66.11} &\darkred{39.73} &\darkred{80.45} &\darkred{90.03} &\darkred{83.91} &\darkblue{19.60} &\darkred{64.83} &\darkred{10.18} &\darkred{85.49}\\
\hline
\hline
\end{tabular}
}
\end{table*}


\begin{table*}[!t]
% \scriptsize
\centering
\caption{The effectiveness of different OOD candidate node selection strategies.}\label{table-diff-select}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc}
\hline
\hline
\multirow{2}*{\diagbox{\textbf{Method}}{\textbf{Dataset}}}  &\multicolumn{3}{c|}{\textbf{Squirrel}} &\multicolumn{3}{c|}{\textbf{WikiCS}} &\multicolumn{3}{c|}{\textbf{YelpChi}} &\multicolumn{3}{c|}{\textbf{Amazon}} &\multicolumn{3}{c}{\textbf{Reddit}}\\
\cline{2-16}	
~ &\multicolumn{15}{c}{AUROC $\uparrow \ \ \ $ AUPR $\uparrow \ \ \ $ FPR@95 $\downarrow$}\\
\hline
% $\text{Energy\textit{Def}}$ &{64.15} &{37.40} &{91.77} &70.22 &60.10 &83.17 &{62.04} &{29.71} &{90.62} &{86.57} &{74.50} &{32.43} &{63.32} &{8.34} &{89.34}\\
% \hline

RSL w/ Cosine Similarity &{64.00} &{38.11} &{91.46} &81.61 &76.36 &70.38 &\darkblue{59.76} &\darkblue{35.03} &\darkblue{85.89} &\darkblue{83.35} &\darkblue{74.85} &\darkblue{27.63} &54.07 &7.25 &92.21\\

RSL w/ Euclidean Distance &\darkblue{64.01} &\darkblue{39.30} &\darkblue{90.45} &{78.63} &{74.28} &{63.26} &52.53  &{24.20}  &{93.53}  &{53.08} &18.29 &93.64 &\darkblue{62.19} &{8.38} &{90.90}\\

RSL w/ Mahalanobis Distance &TLE &TLE &TLE &\darkblue{83.18} &\darkblue{79.11} &\darkblue{58.03} &54.07  &{25.44}  &{92.40}  &{63.71} &30.66 &79.96 &{60.81} &{8.42} &{90.08}\\

RSL w/ Energy\textit{Def} &{63.66} &{38.29} &{91.69} &61.21 &50.41 &90.42 &{57.33} &{26.79} &{91.90} &{77.72} &{55.23} &{54.52} &61.90 &\darkblue{8.55} &\darkblue{89.51}\\

RSL w/ Resonance-based Score $\tau$ &\darkred{64.12} &\darkred{39.58} &\darkred{89.90} &\darkred{84.01} &\darkred{81.14} &\darkred{49.23} &\darkred{66.11} &\darkred{39.73} &\darkred{80.45} &\darkred{90.03} &\darkred{83.91} &\darkred{19.60} &\darkred{64.83} &\darkred{10.18} &\darkred{85.49}\\
\hline
\hline
\end{tabular}
}
\end{table*}


% \begin{table}[!t]
% \centering
% \caption{Time cost (s).}\label{tabel-time}
% \scriptsize % Reduce font size
% \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% \hline
% \diagbox{\textbf{Method}}{\textbf{Dataset}}&\textbf{Squirrel} & \textbf{WikiCS} & \textbf{YelpChi} & \textbf{Amazon} & \textbf{Reddit}\\ 
% \hline
% Energy\textit{Def} &10.94 &27.11 &76.51 &33.81 &26.44 \\
% RSL w/o classifier &5.25 &4.03 &5.41 &5.75 &3.71\\
% RSL &11.54 &17.53 &74.83 &36.33 &38.23 \\
% \hline
% \hline
% \end{tabular}
% \end{table}




% \begin{table}[!t]
% % \scriptsize
% \centering
% \caption{The effectiveness of feature resonance in filtering OOD nodes when using different targets as alignment objectives for known ID node representations. \textbf{True multi-label} denotes that the representations of known ID nodes are aligned with multiple target vectors based on true multi-class labels. \textbf{Multiple random vectors} denotes that the representations of known ID nodes are randomly aligned with multiple target vectors. \textbf{A random vector} denotes that the representations of known ID nodes are all aligned with a single target vector.}\label{tabel-diff-label}
% % \scriptsize % Reduce font size
% % \setlength{\tabcolsep}{1.5mm} % Adjust column spacing
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|ccc|ccc}
% \hline
% \hline
% \multirow{2}*{\diagbox{\textbf{Method}}{\textbf{Dataset}}} &\multirow{2}*{\textbf{Target}}  &\multicolumn{3}{c|}{\textbf{Squirrel}} &\multicolumn{3}{c}{\textbf{WikiCS}}\\
% \cline{3-8}
% ~ &~ &\multicolumn{6}{c}{AUROC $\uparrow \ \ \ $ AUPR $\uparrow \ \ \ $ FPR@95 $\downarrow$}\\
% \hline
% $\text{Energy\textit{Def}}$& - &{64.15} &{37.40} &{91.77} &70.22 &60.10 &83.17 \\
% \hline
% RSL w/o classifier & True multi-label &{61.63} &{37.12} &{90.62} &71.03 &72.47 &81.96 \\

% RSL w/o classifier & Multiple random vectors &61.44 &37.39 &90.62 &{73.64} &{74.13} &{69.25} \\

% RSL w/o classifier & A random vector &{61.52} &{38.96} &{90.18} &79.15 &78.65 &70.38 \\
% \hline
% \hline
% \end{tabular}
% }
% \end{table}




\subsection{Theoretical Analysis}
\label{subsec-Theoretical}

% Our main theorem quantifies the separability of the outliers in the wild by using the Resonance-based filter score. Let $\text{ERR}_{\text{out}}$ and  $\text{ERR}_{\text{in}}$ be the error rate of OOD data being regarded as ID and the error rate of ID data being regarded as OOD, i.e.,  $\text{ERR}_{\text{out}} = | \{\tilde{v}_i \in \mathcal{V}_{\text{wild}}^{\text{out}}: \tau_i \geq T \} | / |  \mathcal{V}_{\text{wild}}^{\text{out}} |$ and $\text{ERR}_{\text{in}} = | \{\tilde{v}_i \in \mathcal{V}_{\text{wild}}^{\text{in}}: \tau_i < T \} | / |  \mathcal{V}_{\text{wild}}^{\text{in}} |$, where $\mathcal{V}_{\text{wild}}^{\text{in}}$ and $\mathcal{V}_{\text{wild}}^{\text{out}}$ denote the sets of inliers and outliers from the wild data $\mathcal{V}_{\text{wild}}$. Then $\text{ERR}_{\text{out}} $ and $\text{ERR}_{\text{in}} $ have the following generalization bounds,

Our main theorem quantifies the separability of the outliers in the wild by using the resonance-based filter score $\tau$. We provide detailed theoretical proof in the Appendix \ref{sec-appendix-proof}.

Let $\text{ERR}_{\text{out}}^t$ be the error rate of OOD data being regarded as ID at $t$-th epoch, i.e.,  $\text{ERR}_{\text{out}}^t = | \{\tilde{v}_i \in \mathcal{V}_{\text{wild}}^{\text{out}}: \tau_i \geq T \} | / |  \mathcal{V}_{\text{wild}}^{\text{out}} |$, where $\mathcal{V}_{\text{wild}}^{\text{out}}$ denotes the set of outliers from the wild data $\mathcal{V}_{\text{wild}}$. Then $\text{ERR}_{\text{out}} $ has the following generalization bound:
\begin{theorem}\label{theorem-1}
(Informal). Under mild conditions, if $\ell(\mathbf{x}, e)$ is $\beta$-smooth w.r.t $\mathbf{w}_t$, $\mathbb{P}_{\mathrm{wild}}$ has $(\gamma, \xi)$-discrepancy w.r.t $\mathbb{P}_{\mathrm{in}}$, and there is $\eta \in (0,1)$ s.t. $\Delta = (1-\eta)^2\xi^2 - 8\beta_1 R_{in}^{*}>0$, then where $n = \Omega(d/\mathrm{min}\{ \eta^2\Delta, (\gamma-R_{in}^{*})\}), m = \Omega(d/\eta^2\xi^2)$, with the probability at least 0.9, for $0 < T < 0.9\widehat{M}_t$($\widehat{M}_t$ is the upper bound of score $\tau_i$),
\begin{equation} \label{equa:ERR_out^t}
\begin{split}
     \text{ERR}_{\text{out}}^t \leq & \frac{\mathrm{max}\{0, 1-\Delta_{\xi}^{\eta}/\pi\}}{1-T/(\sqrt{2}/(2t\alpha - 1))^2}\\
     &+ O(\sqrt{\frac{d}{\pi^2 n}}) + O(\sqrt{\frac{\mathrm{max}\{d, \Delta_{\xi}^{\eta^2}/\pi^2\}}{\pi^2(1-\pi)m}})
\end{split}
\end{equation}
where $
\Delta_{\xi}^{\eta} = 0.98\eta^2\xi^2 - 8\beta_1 R_{in}^{*}
$ and  $R_{in}^{*}$ is the optimal ID risk, i.e., $R_{in}^{*} = \mathrm{min}_{\mathbf{w}\in \mathcal{W}}\mathbb{E_{\mathbf{x}\sim\mathbb{P}_{\mathrm{in}}}}\ell(\mathbf{x}, e)$.
% and $\Delta_{\xi}^{\eta} = 0.98\eta^2\xi^2 - 8\beta_1 R_{in}^{*}$.
$d$ is  the dimension of the space $\mathcal{W}$, $t$ denotes the $t$-th epoch, and $\pi$ is the OOD class-prior probability in the wild.
\end{theorem}
\textbf{Practical implications of Therorem \ref{theorem-1}.} The above theorem states that under mild assumptions, the error $ERR_{\mathrm{out}}$ is upper bounded. If the following two regulatory conditions hold: 1) the sizes of the labeled ID $n$ and wild data $m$ are sufficiently large; 2) the optimal ID risk $R_{in}^{*}$ is small, then the upper bound is mainly depended on $T$ and $t$. We further study the main error of $T$ and $t$ which we defined as $\delta(T, t)$.
\begin{theorem}\label{theorem-2}
    (Informal). 1) if $\Delta_{\xi}^{\eta} \geq (1-\epsilon)\pi$ for a small error $\epsilon \geq 0$, then the main error $\delta(T,t)$ satisfies that
    \begin{equation}
    \begin{split}
        \delta(T, t) &= \frac{\mathrm{max}\{0, 1-\Delta_{\xi}^{\eta}/\pi\}}{1-T/(\sqrt{2}/(2t\alpha - 1))^2}\\
        &\leq \frac{\epsilon}{1 - T/(\sqrt{2}/(2t\alpha - 1))^2}
    \end{split}
    \end{equation}

2) When learning rate $\alpha$ is small sufficiently, and if $\xi \geq 2.011\sqrt{8\beta_1 R_{in}^{*} + 1.011\sqrt{\pi}}$, then there exists $\eta \in (0, 1)$ ensuring that $\Delta > 0$ and $\Delta_{\xi}^{\eta}>\pi$ hold, which implies that the main error $\delta(T, t) = 0$.
\end{theorem}
\textbf{Practical implications of Therorem \ref{theorem-2}.} Theorem \ref{theorem-2} states that when the learning rate \( \alpha \) is sufficiently small, the primary error \( \delta(T,t) \) can approach zero if the difference \( \zeta \) between the two data distributions \( \mathbb{P}_{\text{wild}} \) and \( \mathbb{P}_{\text{in}} \) is greater than a certain small value. 
Meanwhile, Theorem \ref{theorem-2} also shows that the primary error \( \delta(T,t) \) is inversely proportional to the learning rate \( \alpha \) and the number of epochs ($t$). As the $t$ increases, the primary error \( \delta(T,t) \) also increases, while a smaller learning rate \( \alpha \) leads to a minor primary error \( \delta(T,t) \). However, during training, there exists \( t \) at which the error reaches its minimum.
% This observation is experimentally verified in Section \ref{subsec-Hyperparameter-Analysis}. 
% It is worth noting that by using only our resonance-based filter score, we have already surpassed most SOTA methods in the real-world, category-free OOD node detection task. A detailed introduction will be provided in Section \ref{subsec-ablation-result}.



