\section{Introduction}
\label{Sec-Introduction}


\begin{figure}[!t]
	\centering
   \subfigure[Toy Dataset]{
    % \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.43\linewidth]{Figures/path/point.png}
	% \end{minipage}
 }
   \subfigure[Gradient Descent Trajectory]{
	% \begin{minipage}[b]{0.47\linewidth}
		\includegraphics[width=0.47\linewidth]{Figures/path/sample_123_cropped.png}
	% \end{minipage}
 }
    \caption{(a) We conduct a preliminary study on the changes in ID and OOD node representations during training using a toy dataset.
(b) Projections of the representations of ID and OOD nodes onto gradients: $\text{Proj}_{\nabla \ell(\theta_t; \cdot)}\mathbf{x}_i = \frac{\mathbf{x}_i \cdot \nabla \ell(\theta_t; \cdot) }{\parallel \nabla \ell(\theta_t; \cdot) \parallel_2^2}\cdot \nabla \ell(\theta_t; \cdot)$.}
    \label{F-Trajectory}
    \vskip -0.1in
\end{figure}




% \begin{figure}[!t]
% 	\centering
%    \subfigure[Pre-training (Energy\textit{Def})]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.47\linewidth]{Figures/tsne/without_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%    \subfigure[Pre-training (Ours)]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.47\linewidth]{Figures/tsne/with_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
 
%    \subfigure[Post-training (Energy\textit{Def})]{
%     % \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.47\linewidth]{Figures/tsne/trained_without_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%    \subfigure[Post-training (Ours)]{
% 	% \begin{minipage}[b]{0.47\linewidth}
% 		\includegraphics[width=0.47\linewidth]{Figures/tsne/trained_with_wikics_tsne_clustering.png}
% 	% \end{minipage}
%  }
%     \caption{T-SNE visualization of node embeddings on the dataset wikics. (a) Hallucinated nodes (red) generated by Energy\textit{Def} fail to accurately represent the actual features of OOD nodes, whereas ours can, as shown in (b). (c) Representations of ID and OOD nodes trained with hallucinated nodes generated by Energy\textit{Def} are poorly separated, whereas ours can, as shown in (d).}
%     \label{F-Motivation1}
% \end{figure}




Graph-based machine learning models like Graph Neural Networks (GNNs) \citep{kipf2016semi,xu2018representation,abu2019mixhop} have become increasingly prevalent in applications such as social network analysis \citep{fan2019graph}, knowledge graphs \citep{baek2020learning},  and biological networks \citep{de2018molgan}. Despite the success of GNNs, detecting out-of-distribution (OOD) nodes remains an under-explored challenge. These OOD nodes differ significantly from the in-distribution (ID) nodes used during training, and their presence can severely undermine the performance and robustness of graph models. As deploying GNNs in real-world environments becomes more common, the ability to identify and handle OOD nodes is crucial for ensuring the reliability of using these models.

To address this, most existing methods \citep{hendrycks2016baseline, liang2017enhancing, hendrycks2018deep,liu2020energy,wu2023energy} assume that ID nodes are equipped with multi-category labels.
Then, they train an in-distribution classifier and develop OOD metrics based on (i)-classifier outputs, such as Maximum Softmax Probability (MSP) \citep{hendrycks2016baseline} and Energy \citep{liu2020energy, wu2023energy}; (ii)-supervised representations, such as KNN \citep{sun2022out} and NNGuide \citep{park2023nearest}.
% Then, these methods commonly rely on entropy-based metrics derived from multi-class model predictions, such as Maximum Softmax Probability (MSP) \citep{hendrycks2016baseline} and Energy \citep{liu2020energy}, to distinguish OOD nodes. 
However, in many real-world scenarios, accessing multi-category labels for ID nodes is often highly challenging due to practical limitations such as the high cost of annotation, lack of domain expertise, or data privacy concerns, which essentially hinder the effectiveness of current OOD methods. 
To date, only a few papers \cite{gong2024energy, sun2022out, sehwag2021ssd, liu2023good} study this practical setup, and there is still a large room for improvement, especially in the graph field at the node level.
% rendering these methods ineffective and underscoring the pressing need to develop category-free OOD node detection approaches that can function independently of labeled data.

% This limitation presents significant challenges for OOD node detection because most existing methods \citep{hendrycks2016baseline, liang2017enhancing, hendrycks2018deep,liu2020energy,wu2023energy} assume that ID nodes are equipped with multi-category labels. These methods commonly rely on entropy-based metrics derived from multi-class model predictions, such as Maximum Softmax Probability (MSP) \citep{hendrycks2016baseline} and Energy \citep{liu2020energy}, to distinguish OOD nodes. 
% The underlying principle is that nodes with higher prediction entropy are more likely to belong to the OOD category.
% However, this ignores a key question: 
% {\centering \textbf{\textit{How do we detect OOD nodes when the ID nodes are without multi-category labels for multi-category training?}} \par} 
% The absence of multi-category labels disrupts this process,

% Despite the absence of label-based output space, we still have the feature space of the nodes. 
In this paper, we revisit the graph OOD task at the node level from a new perspective and turn our attention to the intrinsic similarities within the data. An intuitive idea is that the ID samples may still share some commonalities in the representation space. We hypothesize that when optimizing the representation of known ID nodes, the representation of unknown ID nodes and unknown OOD nodes will change with different trajectories.
% We argue that, regardless of the goal of the algorithm acting on the known ID nodes, the node representations obtained by the algorithm should all be affected by the nearest neighbors of the representations, i.e., assuming that the node representations form a manifold structure between them and that the changes in the local structure are always smooth. In other words, the representation transformation acting on the known ID nodes should change the representation of the unknown ID node more significantly than the change in the representation of the unknown OOD node. 
Based on the hypothesis and using a toy dataset (Figure \ref{F-Trajectory}(a)), we design an experiment where the features of labeled ID samples are aligned to an arbitrarily fixed representation vector. Interestingly, we observe a distinct behavior during this optimization process: the representations of unlabeled wild ID samples experienced more pronounced changes than wild OOD samples, as shown in Figure \ref{F-Trajectory}(b). This phenomenon closely resembles the concept of forced vibration, where resonance occurs when an external force aligns with the natural frequency of an oscillator, amplifying its oscillation to a maximum. Analogously, we refer to this phenomenon as \textbf{Feature Resonance}: \textit{during the optimization of known ID samples, the representation of unknown ID samples undergoes more significant changes compared to OOD samples.} 
This phenomenon reveals the intrinsic relationship between ID samples, highlighting their shared underlying distribution. Evidently, this feature resonance phenomenon can be leveraged for OOD detection: weaker representation changes during known ID optimization indicate a higher likelihood of being OOD.
% the weaker the change in representation during known ID sample optimization, the more likely the sample belongs to the OOD category.

% Unfortunately, despite promising results on the toy dataset, the features of samples in real-world datasets are much more complex than those in the toy dataset.  As a result, the feature resonance of the overall trajectory at the macro level is not as ideal as observed in experiments on the toy dataset. Therefore, to better reflect the feature resonance, we delve deeper into the changes in finer-grained micro-level node representations across epochs to study the feature resonance phenomenon. In our experiments on real-world datasets, we find that at the micro-level, this phenomenon is most evident in the middle of the training process, with its significance surpassing that of the macro-level overall trajectory.

% Building on the insights from the micro-level feature resonance, we propose a novel framework named \textbf{RSL} (\textbf{R}esonance-based \textbf{S}eparate and \textbf{L}earn) for category-free OOD node detection. The core idea behind RSL is to align all labeled ID representations with a predetermined fixed target vector. This target vector serves as a focal point for feature transformation, enabling the alignment of labeled ID representations and invoking the feature resonance effect. By leveraging this phenomenon, RSL allows for the reliable identification of actual OOD nodes. It is essential to note that the feature resonance phenomenon is \textit{label-independent} and results from the intrinsic relationships between ID node representations. Therefore, this is highly suitable for category-free OOD detection scenarios without multi-category labels.

% Further incorporating the identified actual OOD nodes and generating synthetic ones via stochastic gradient Langevin dynamics (SGLD) \citep{welling2011bayesian}, RSL enriches training data and trains a robust OOD classifier, setting a new standard for category-free OOD node detection. 


% In correspondence with the toy dataset, we also observe the phenomenon of feature resonance in the real-world dataset. 
% To explore this phenomenon further, we examine representation changes between epochs at a finer granularity, which we refer to as the micro-level, while viewing the overall trajectory of these changes as the macro-level. We find that micro-level representation changes exhibit feature resonance; in certain epochs, changes in unknown ID samples are more significant than those in unknown OOD samples. 
% In our experiments on real-world datasets, we find that the feature resonance phenomenon is most evident in the middle of the training process, with its significance surpassing that of the overall trajectory. 
% In real-world benchmark datasets, we also observe the feature resonance phenomenon, but the overall behavior is slightly different from the ideal setup. More concretely, it occurs most noticeably in the middle of the training process, while at other times, it may be affected by randomness or model overfitting, making the feature resonance phenomenon less prominent.
% % Therefore, from the perspective of maximizing feature resonance, we design a practical method for filtering OOD nodes: we align the representations of known ID samples with a preset target vector to induce the feature resonance phenomenon. During the alignment process, we determine the period during which feature resonance occurs by the validation set, where the magnitude of representation changes for ID samples is more significant than that for OOD samples. When this difference is maximized, we consider feature resonance to peak. At this point, the smaller the representation changes for samples in the test set, the more likely they are OOD samples. 
% Inspired by feature resonance, we propose a practical method for filtering OOD nodes: aligning the representations of known ID samples with a target vector to induce feature resonance and identifying the period of feature resonance using the validation set \citep{katz2022training, gong2024energy, du2024does, du2024haloscope}, where the representation change for ID samples is larger than for OOD samples. It is essential to note that the feature resonance phenomenon is \textit{label-independent} and results from the intrinsic relationships between ID sample representations. Therefore, this is highly suitable for category-free OOD detection scenarios without multi-category labels. 

In real-world scenarios, due to the intrinsic complex pattern in data, we find that the feature resonance phenomenon still occurs but slightly differs from the ideal conditions. 
To illustrate this, we further propose a micro-level proxy for measuring feature resonance—by computing the movement of the representation vector in one training step.
Our findings reveal that in more complex scenarios, the feature resonance phenomenon typically arises during the middle stages of the training process, whereas during other phases, it may be overwhelmed by noise or obscured by overfitting. 
In such cases, evaluating the entire trajectory often fails to yield satisfactory results. 
Fortunately, efficient OOD detection can still be achieved by calculating the micro-level feature resonance measure.
By utilizing a simple binary ID/OOD validation set\footnote{The use of the validation set is consistent with previous works \citep{katz2022training, gong2024energy, du2024does, du2024haloscope} and does not contain multi-category labels.}, we empirically show the feature resonance period can be precisely identified, and we identify more minor representation differences as OOD samples.
Notably, our new micro-level feature resonance measure is still \textit{label-independent} by fitting a randomly fixed target, making it highly compelling in category-free scenarios. Theoretical and experimental proof that micro-level feature resonance can filter a set of reliable OOD nodes with low error, e.g., on the FPR95 metric, the micro-level feature resonance achieves an average reduction of \textbf{10.93}\% compared to current state-of-the-art methods.


% In real-world benchmark datasets, we also observe the feature resonance phenomenon. Interestingly, we find that in real-world scenarios, the feature resonance phenomenon occurring during the middle of the training process is more prominent than the global trajectory. More specifically, by examining the step-wise changes in representations between consecutive epochs throughout the training process, we find that feature resonance is not significant during the early and late stages of training. We hypothesize that this is because the early stage is influenced by randomness, while the late stage is affected by model overfitting. To identify the period when feature resonance is most pronounced, we leverage a validation set \citep{katz2022training, gong2024energy, du2024does, du2024haloscope}, where we consider feature resonance to be more significant when the representation change for ID samples is more prominent than for OOD samples. It is essential to note that the feature resonance phenomenon is \textit{label-independent}, arising from the intrinsic relationships between the representations of ID samples. Therefore, it is highly suitable for category-free OOD detection scenarios that do not require multi-category labels.

% 1. further integrate with current prevalent Langevin-based xxx.
% This showing .  We call the whole framework as xxx.
% exp: 
% 1. raw feature resonance
% 2. with our new framework achieve xxx.

% To study the effectiveness of our method, we theoretically derive the error upper bound of this method for filtering OOD samples, demonstrating the existence of the feature resonance period. Under mild conditions, our method can filter a set of reliable OOD nodes with low error. 

Furthermore, we combine the micro-level feature resonance with the current Langevin-based synthetic OOD nodes generating strategy to train an OOD classifier for more effective OOD node detection performance, which we call the whole framework as \textbf{RSL}; for example, the FPR95 metric is reduced by an average of \textbf{18.51}\% compared to the current state-of-the-art methods.

% Furthermore, we combine it with a strategy of using stochastic gradient Langevin dynamics (SGLD) to generate synthetic OOD nodes for training the OOD classifier, allowing our \textbf{R}esonance-based \textbf{S}eparation and \textbf{L}earning framework (referred to as \textbf{RSL}) to achieve state-of-the-art performance in detecting category-free OOD nodes; for example, the FPR95 metric (lower is better) is reduced by an average of \textbf{18.51}\% compared to the current state-of-the-art method.

% 1. question: 
% what's feature resonance?
% how can we employ feature resonance for graph OOD?

% 2. answer:
% \textbf{a: ideal toy example
% b: practical scenario - micro proxy}
% c: +syn data, better results (-1)
% \textbf{d: +exp}
% e: theory

% 3: standard:
% a: significance solidness
% b: novelty (yourself)
% c: relevance

% In summary, the contributions of this paper are as follows: 
% \begin{enumerate}[nosep, topsep=0pt, leftmargin=*]
% \item \textbf{New Phenomenon}: To the best of our knowledge, we are the first to reveal the  \textbf{Feature Resonance} phenomenon, shedding light on a novel perspective for understanding and addressing the challenges of detecting OOD samples.
% \item \textbf{New Method}: Building upon the feature resonance phenomenon, we propose a novel framework, RSL (Resonance-based Separation and Learning), for category-free OOD node detection.  
% \item \textbf{Theoretical Analysis}: Theoretically, we provide a rigorous error bound from the perspective of separability, demonstrating that RSL can isolate candidate outliers with low error rates.
% \item \textbf{Empirical Study}: Experimentally, our experimental results on five publicly available datasets show that RSL performs well on category-free OOD node detection tasks; for example, the FPR95 metric (lower is better) is reduced by an average of 18.51\% compared to the current state-of-the-art method.  
% \end{enumerate}
