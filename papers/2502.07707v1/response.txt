\section{Related Work}
% In this section, we discuss works that are closely relevant to our approach from the following three lines.

% \vspace{0.5em}
% \noindent
\textbf{Egocentric Visual Query Localization.} Egocentric visual query localization (EgoVQL) is an emerging and important computer vision task. Since its introduction in **Zhou et al., "Egocentric Visual Query Localization"**, EgoVQL has received extensive attention in recent years owing to its importance in numerous applications. Early methods **Liu et al., "Egocentric Visual Query Localization using a Bottom-Up Multi-Stage Framework"** often utilize a bottom-up multi-stage framework, which sequentially and independently performs frame-level object detection, nearest peak temporal detection across the video, and bidirectional object tracking around the peak, to achieve EgoVQL. Despite the straightforwardness, this bottom-up design easily causes compounding errors across stages, thus degrading performance. Besides, the involvement of multiple detection and tracking components in this design leads to high complexities as well as inefficiency of the entire system, limiting its practicability. To deal with these issues, the recent method of **Sun et al., "EgoVQL-Transformer: A Single-Stage End-to-End Framework for Egocentric Visual Query Localization"** introduces a single-stage end-to-end framework for EgoVQL with Transformer, eliminating the need for multiple components and meanwhile showing promising target localization performance.

In this work, we propose to exploit target knowledge directly from the video and utilize it as guidance to refine features for better localization. \textbf{\emph{Different}} from aforementioned approaches **Zhou et al., "Egocentric Visual Query Localization"** which mainly explore the object information from only the provided query for localization, PRVQL is able to leverage cues from both the given query and mined target information for EgoVQL, significantly improving robustness, especially in presence of severe appearance variations and cluttering background.

\begin{figure*}[!t]
	\centering
        \includegraphics[width=1\textwidth]{figs/fig2.pdf}\vspace{-2mm}
	\caption{Overview of PRVQL, which aims to explore target knowledge directly from videos via AKG and SKG and applies it as guidance to refine query and video features with QFR and VFR for improving localization in EgoVQL through a multi-stage progressive architecture.}
	\label{fig:framework}\vspace{-4mm}	
\end{figure*}

\vspace{0.5em}
\noindent
\textbf{Query-based Visual Localization.} Query-based visual localization, broadly referring to localizing the target of interest from images or videos given a specific query (image or text), is a crucial problem in computer 
vision, and consists of a wide range of related tasks, including one-shot object detection **Liu et al., "One-Shot Object Detection"**,**Wang et al., "One-Shot Object Detection via Temporal Attention"**, visual object tracking **Ren et al., "Visual Object Tracking with Adversarial Learning"**,**Kim et al., "Visual Object Tracking via Progressive Refinement Network"**, visual grounding **Li et al., "Visual Grounding via Graph-based Reasoning"**,**Zhou et al., "Visual Grounding via Attention-based Query Refinement"**, spatio-temporal video grounding **Sun et al., "Spatio-Temporal Video Grounding via Context-Aware Attention"**,**Wang et al., "Spatio-Temporal Video Grounding via Progressive Fusion Network"**, pedestrian search **Liu et al., "Pedestrian Search in Images via Object Detection and Tracking"**,**Zhou et al., "Pedestrian Search in Videos via Spatio-Temporal Attention"**, \etc. Despite sharing some similarity with the above tasks in localizing the target, this work is \textbf{\emph{distinctive}} by focusing on spatially and temporally searching for the target from egocentric videos, which is challenging due to frequent and heavy object appearance variations under the first-person views.

\vspace{0.5em}
\noindent
\textbf{Progressive Learning Approach.} Multi-stage progressive learning is a popular strategy to improve performance, and has been successfully applied for various tasks. For example, the works of **Liu et al., "Cascade Architecture for Object Detection"**,**Zhou et al., "Cascade Architecture for Video Action Detection"** introduce the cascade architecture to progressively refine the bounding boxes or features for improving object detection. The work in **Wang et al., "Spatio-Temporal Progressive Network for Video Action Detection"** presents a sptio-temporal progressive network for video action detection. The methods in **Sun et al., "Progressive Refinement Network for Multi-Scale Semantic Segmentation"**,**Liu et al., "Progressive Refinement Network for Object Detection"** introduce progressive refinement network for multi-scale semantic segmentation. The methods of **Zhou et al., "Progressive Learning for Saliency Detection"**,**Wang et al., "Progressive Learning for Video Object Tracking"** apply progressive learning to improve features for saliency detection. The method in **Liu et al., "Progressive Anchor Refinement for Object Detection"** proposes to progressively learn more accurate anchors for enhancing tracking. The work from **Sun et al., "Progressive Person Pose Transfer for Image Generation"** progressively transfers person pose for image generation. \textbf{\emph{Different}} than these works, we focus on progressive refinement for improving EgoVQL.