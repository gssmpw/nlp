@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hsieh2019one,
  title={One-shot object detection with co-attention and co-excitation},
  author={Hsieh, Ting-I and Lo, Yi-Chen and Chen, Hwann-Tzong and Liu, Tyng-Luh},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{voigtlaender2020siam,
  title={Siam r-cnn: Visual tracking by re-detection},
  author={Voigtlaender, Paul and Luiten, Jonathon and Torr, Philip HS and Leibe, Bastian},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{yan2021learning,
  title={Learning spatio-temporal transformer for visual tracking},
  author={Yan, Bin and Peng, Houwen and Fu, Jianlong and Wang, Dong and Lu, Huchuan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{LoshchilovH19,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {ICLR},
  year         = {2019}
}

@article{OquabDMVSKFHMEA24,
  author       = {Maxime Oquab and
                  Timoth{\'{e}}e Darcet and
                  Th{\'{e}}o Moutakanni and
                  Huy V. Vo and
                  Marc Szafraniec and
                  Vasil Khalidov and
                  Pierre Fernandez and
                  Daniel Haziza and
                  Francisco Massa and
                  Alaaeldin El{-}Nouby and
                  Mido Assran and
                  Nicolas Ballas and
                  Wojciech Galuba and
                  Russell Howes and
                  Po{-}Yao Huang and
                  Shang{-}Wen Li and
                  Ishan Misra and
                  Michael Rabbat and
                  Vasu Sharma and
                  Gabriel Synnaeve and
                  Hu Xu and
                  Herv{\'{e}} J{\'{e}}gou and
                  Julien Mairal and
                  Patrick Labatut and
                  Armand Joulin and
                  Piotr Bojanowski},
  title        = {DINOv2: Learning Robust Visual Features without Supervision},
  journal      = {Transactions on Machine Learning Research},
  volume       = {2024},
  year         = {2024}
}

@inproceedings{DosovitskiyB0WZ21,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle    = {ICLR},
  year         = {2021}
}

@inproceedings{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{yu2022cascade,
  title={Cascade transformers for end-to-end person search},
  author={Yu, Rui and Du, Dawei and LaLonde, Rodney and Davila, Daniel and Funk, Christopher and Hoogs, Anthony and Clipp, Brian},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2017person,
  title={Person search with natural language description},
  author={Li, Shuang and Xiao, Tong and Li, Hongsheng and Zhou, Bolei and Yue, Dayu and Wang, Xiaogang},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{yang2022tubedetr,
  title={Tubedetr: Spatio-temporal video grounding with transformers},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhu2022seqtr,
  title={Seqtr: A simple yet universal network for visual grounding},
  author={Zhu, Chaoyang and Zhou, Yiyi and Shen, Yunhang and Luo, Gen and Pan, Xingjia and Lin, Mingbao and Chen, Chao and Cao, Liujuan and Sun, Xiaoshuai and Ji, Rongrong},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{liu2025grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{deng2021transvg,
  title={Transvg: End-to-end visual grounding with transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{chen2023seqtrack,
  title={Seqtrack: Sequence to sequence learning for visual object tracking},
  author={Chen, Xin and Peng, Houwen and Wang, Dong and Lu, Huchuan and Hu, Han},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{bertinetto2016fully,
  title={Fully-convolutional siamese networks for object tracking},
  author={Bertinetto, Luca and Valmadre, Jack and Henriques, Joao F and Vedaldi, Andrea and Torr, Philip HS},
  booktitle={ECCVW},
  year={2016}
}

@inproceedings{lin2025tracking,
  title={Tracking meets lora: Faster training, larger model, stronger performance},
  author={Lin, Liting and Fan, Heng and Zhang, Zhipeng and Wang, Yaowei and Xu, Yong and Ling, Haibin},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{gu2024context,
  title={Context-Guided Spatio-Temporal Video Grounding},
  author={Gu, Xin and Fan, Heng and Huang, Yan and Luo, Tiejian and Zhang, Libo},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhao2022semantic,
  title={Semantic-aligned fusion transformer for one-shot object detection},
  author={Zhao, Yizhou and Guo, Xun and Lu, Yan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yang2022balanced,
  title={Balanced and hierarchical relation learning for one-shot object detection},
  author={Yang, Hanqing and Cai, Sijia and Sheng, Hualian and Deng, Bing and Huang, Jianqiang and Hua, Xian-Sheng and Tang, Yong and Zhang, Yu},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{vu2019cascade,
  title={Cascade rpn: Delving into high-quality region proposal network with adaptive convolution},
  author={Vu, Thang and Jang, Hyunjun and Pham, Trung X and Yoo, Chang},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{zhao2018icnet,
  title={Icnet for real-time semantic segmentation on high-resolution images},
  author={Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{yang2019step,
  title={Step: Spatio-temporal progressive learning for video action detection},
  author={Yang, Xitong and Yang, Xiaodong and Liu, Ming-Yu and Xiao, Fanyi and Davis, Larry S and Kautz, Jan},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zheng2022progressive,
  title={Progressive end-to-end object detection in crowded scenes},
  author={Zheng, Anlin and Zhang, Yuang and Zhang, Xiangyu and Qi, Xiaojuan and Sun, Jian},
  booktitle={CVPR},
  year={2022}
}

@article{xu2022negative,
  title={Negative frames matter in egocentric visual query 2d localization},
  author={Xu, Mengmeng and Fu, Cheng-Yang and Li, Yanghao and Ghanem, Bernard and Perez-Rua, Juan-Manuel and Xiang, Tao},
  journal={arXiv},
  year={2022}
}

@inproceedings{xu2023my,
  title={Where is my wallet? modeling object proposal sets for egocentric visual query localization},
  author={Xu, Mengmeng and Li, Yanghao and Fu, Cheng-Yang and Ghanem, Bernard and Xiang, Tao and P{\'e}rez-R{\'u}a, Juan-Manuel},
  booktitle={CVPR},
  year={2023}
}

@article{jiang2024single,
  title={Single-stage visual query localization in egocentric videos},
  author={Jiang, Hanwen and Ramakrishnan, Santhosh Kumar and Grauman, Kristen},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{cheng2020cascadepsp,
  title={Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement},
  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{ye2023cascade,
  title={Cascade-DETR: delving into high-quality universal object detection},
  author={Ye, Mingqiao and Ke, Lei and Li, Siyuan and Tai, Yu-Wing and Tang, Chi-Keung and Danelljan, Martin and Yu, Fisher},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{cai2018cascade,
  title={Cascade r-cnn: Delving into high quality object detection},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{song2020progressive,
  title={Progressive refinement network for occluded pedestrian detection},
  author={Song, Xiaolin and Zhao, Kaili and Chu, Wen-Sheng and Zhang, Honggang and Guo, Jun},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{chen2020progressively,
  title={Progressively guided alternate refinement network for RGB-D salient object detection},
  author={Chen, Shuhan and Fu, Yun},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{zhang2018progressive,
  title={Progressive attention guided recurrent network for salient object detection},
  author={Zhang, Xiaoning and Wang, Tiantian and Qi, Jinqing and Lu, Huchuan and Wang, Gang},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{huynh2021progressive,
  title={Progressive semantic segmentation},
  author={Huynh, Chuong and Tran, Anh Tuan and Luu, Khoa and Hoai, Minh},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{fan2019siamese,
  title={Siamese cascaded region proposal networks for real-time visual tracking},
  author={Fan, Heng and Ling, Haibin},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{cheng2022masked,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{rezatofighi2019generalized,
  title={Generalized intersection over union: A metric and a loss for bounding box regression},
  author={Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{RenHGS15,
  author       = {Shaoqing Ren and
                  Kaiming He and
                  Ross B. Girshick and
                  Jian Sun},
  title        = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal Networks},
  booktitle    = {NIPS},
  year         = {2015}
}

@inproceedings{zhu2019progressive,
  title={Progressive pose attention transfer for person image generation},
  author={Zhu, Zhen and Huang, Tengteng and Shi, Baoguang and Yu, Miao and Wang, Bofei and Bai, Xiang},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {NIPS},
  year         = {2017}
}