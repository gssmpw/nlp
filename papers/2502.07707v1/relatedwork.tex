\section{Related Work}
% In this section, we discuss works that are closely relevant to our approach from the following three lines.

% \vspace{0.5em}
% \noindent
\textbf{Egocentric Visual Query Localization.} Egocentric visual query localization (EgoVQL) is an emerging and important computer vision task. Since its introduction in~\cite{grauman2022ego4d}, EgoVQL has received extensive attention in recent years owing to its importance in numerous applications. Early methods~\cite{grauman2022ego4d,xu2022negative,xu2023my} often utilize a bottom-up multi-stage framework, which sequentially and independently performs frame-level object detection, nearest peak temporal detection across the video, and bidirectional object tracking around the peak, to achieve EgoVQL. Despite the straightforwardness, this bottom-up design easily causes compounding errors across stages, thus degrading performance. Besides, the involvement of multiple detection and tracking components in this design leads to high complexities as well as inefficiency of the entire system, limiting its practicability. To deal with these issues, the recent method of~\cite{jiang2024single} introduces a single-stage end-to-end framework for EgoVQL with Transformer~\cite{VaswaniSPUJGKP17}, eliminating the need for multiple components and meanwhile showing promising target localization performance.

In this work, we propose to exploit target knowledge directly from the video and utilize it as guidance to refine features for better localization. \textbf{\emph{Different}} from aforementioned approaches~\cite{grauman2022ego4d,xu2022negative,xu2023my,jiang2024single} which mainly explore the object information from only the provided query for localization, PRVQL is able to leverage cues from both the given query and mined target information for EgoVQL, significantly improving robustness, especially in presence of severe appearance variations and cluttering background.

\begin{figure*}[!t]
	\centering
        \includegraphics[width=1\textwidth]{figs/fig2.pdf}\vspace{-2mm}
	\caption{Overview of PRVQL, which aims to explore target knowledge directly from videos via AKG and SKG and applies it as guidance to refine query and video features with QFR and VFR for improving localization in EgoVQL through a multi-stage progressive architecture.}
	\label{fig:framework}\vspace{-4mm}	
\end{figure*}

\vspace{0.5em}
\noindent
\textbf{Query-based Visual Localization.} Query-based visual localization, broadly referring to localizing the target of interest from images or videos given a specific query (image or text), is a crucial problem in computer 
vision, and consists of a wide range of related tasks, including one-shot object detection~\cite{hsieh2019one,yang2022balanced,zhao2022semantic}, visual object tracking~\cite{chen2023seqtrack,lin2025tracking,bertinetto2016fully}, visual grounding~\cite{deng2021transvg,liu2025grounding,zhu2022seqtr}, spatio-temporal video grounding~\cite{yang2022tubedetr,gu2024context}, pedestrian search~\cite{li2017person,yu2022cascade}, \etc. Despite sharing some similarity with the above tasks in localizing the target, this work is \textbf{\emph{distinctive}} by focusing on spatially and temporally searching for the target from egocentric videos, which is challenging due to frequent and heavy object appearance variations under the first-person views.

\vspace{0.5em}
\noindent
\textbf{Progressive Learning Approach.} Multi-stage progressive learning is a popular strategy to improve performance, and has been successfully applied for various tasks. For example, the works of~\cite{cai2018cascade,ye2023cascade,vu2019cascade} introduce the cascade architecture to progressively refine the bounding boxes or features for improving object detection. The work in~\cite{yang2019step} presents a sptio-temporal progressive network for video action detection. The methods in~\cite{huynh2021progressive,zhao2018icnet} introduce progressive refinement network for multi-scale semantic segmentation. The methods of~\cite{zhang2018progressive,chen2020progressively} apply progressive learning to improve features for saliency detection. The method in~\cite{fan2019siamese} proposes to progressively learn more accurate anchors for enhancing tracking. The work from~\cite{zhu2019progressive} progressively transfers person pose for image generation. \textbf{\emph{Different}} than these works, we focus on progressive refinement for improving EgoVQL.