\section{Introduction}

Large language models (LLMs) have shown impressive potential in code generation automation by converting natural language descriptions into code \cite{a:1}. In the context of front-end code generation, implementing visual designs is a complex and time-consuming task that demands skill and expertise. With LLMs, these technical barriers are lowered for people who are unfamiliar with specific programming languages \cite{a:2, a:30}. Recent studies have also started exploring the use of Visual Language Models (VLMs) to generate HTML directly from designs and sketches \cite{a:3, a:4}. These studies highlight a growing trend toward making front-end development more accessible to general users.

As AI-assisted programming systems become increasingly adopted, concerns have been raised about the practical use of their generated code, including security \cite{a:5}, correctness \cite{a:7}, web accessibility \cite{a:6} and input-output clarity \cite{a:8}. In this work, we extend these concerns with potential problematic generated code to the concept of dark patterns. \textit{Dark patterns} are designs that use knowledge of human behavior to trick users into actions that benefit an online service but go against the users' intentions or desires \cite{a:10, a:11}.

In this study, we conduct an audit of four popular LLMs --- Claude, GPT, Gemini, and CodeLlama\footnote{Specifically, we audit Claude 3.5 Sonnet \cite{a:58}, GPT-4o \cite{a:18}, Gemini-2.0-flash-exp \cite{a:19}, and CodeLlama-34b-Instruct \cite{a:20}} --- to measure how frequently %the likelihood that 
each produces dark patterns. We identify a set of components that are common in ecommerce pipelines, like search, product pages, and newsletter signups, and prompt the LLMs to produce HTML and CSS for each component. 

In addition, because prior work has noted that company incentives are an important part of the development of dark patterns \cite{a:64, a:48}, we test three experimental conditions regarding whose interests are prioritized in the prompts. The first condition explicitly prompts the model to produce a design that aligns with the company's interests, the second prompts the model to produce a design that aligns with user-centered design principles, and the third, with no goal specified, serves as a baseline. All outputs are labeled for the presence of dark patterns by a team of designers.

We find that over one-third of generated components contain at least one dark pattern. Additionally, some components, like newsletter signup and discount offers, are much \textit{more} likely to include dark patterns than others, like search and shipment tracking. Surprisingly, while the number of dark patterns decreases progressively from the company-interest condition to the baseline and then to the user-interest condition, this difference is not statistically significant. There are also notable differences in the performance of different models --- with CodeLlama producing fewer dark patterns than the others --- though this difference is also not statistically significant. %minimal difference in performance between the models in terms of the number of dark patterns generated. 
Based on these findings, we raise awareness among developers and designers about this risk and provide suggestions for identifying LLM-generated dark patterns. Furthermore, we discuss the necessary interventions and advocate for integrating user-centered design principles into LLMs to promote ethical practices and maximize their potential for ethical design.