%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{tikzmark, calc}

%\usepackage[section]{placeins}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny,disable]{todonotes}


% for appendix table of contents
\usepackage[page]{appendix}
\usepackage{etoolbox}

\renewcommand{\appendixtocname}{Appendix Contents.}
\renewcommand{\appendixpagename}{Appendices}

\makeatletter
\let\oldappendix\appendices

\renewcommand{\appendices}{%
  \clearpage
  \renewcommand{\thesection}{\Roman{section}}
  % From now, everything goes to the app - file and not to the toc
  \let\tf@toc\tf@app
  \addtocontents{app}{\protect\setcounter{tocdepth}{2}}
  \immediate\write\@auxout{%
    \string\let\string\tf@toc\string\tf@app^^J
  }
  \oldappendix
}%

\newcommand{\listofappendices}{%
  \begingroup
  \renewcommand{\contentsname}{\appendixtocname}
  \let\@oldstarttoc\@starttoc
  \def\@starttoc##1{\@oldstarttoc{app}}
  \tableofcontents% Reusing the code for \tableofcontents with different \contentsname and different file handle app
  \endgroup
}

\makeatother


\newcommand{\TFcol}{\text{TF}_{\text{col}}}
\newcommand{\TFrow}{\text{TF}_{\text{row}}}
\newcommand{\TFicl}{\text{TF}_{\text{icl}}}

\newcommand{\MEMcol}{\text{MEM}_{\text{col}}}
\newcommand{\MEMrow}{\text{MEM}_{\text{row}}}
\newcommand{\MEMicl}{\text{MEM}_{\text{icl}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{TabICL: Scalable Tabular In-context Learning}
\icmltitlerunning{TabICL: A Tabular Foundation Model for Large Data}

% notes in the margin
% \usepackage{snaptodo}

%\setlength{\marginparwidth}{1.65cm}
%\setlength{\marginparsep}{-.01cm}

% \snaptodoset{block rise=1em}
% \snaptodoset{block rise=2em}
% \snaptodoset{margin block/.style={font=\tiny}} 
% \snaptodoset{margin block/.style={font=\tiny}}

\begin{document}

\twocolumn[
\icmltitle{TabICL: A Tabular Foundation Model for In-Context Learning on Large Data}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jingang Qu}{soda}
\icmlauthor{David Holzm\"uller}{sierra,ens}
\icmlauthor{Ga\"el Varoquaux}{soda}
\icmlauthor{Marine Le Morvan}{soda}
\end{icmlauthorlist}

\icmlaffiliation{soda}{SODA team, INRIA Saclay, France}
\icmlaffiliation{sierra}{Sierra team, INRIA Paris, France}
\icmlaffiliation{ens}{Ecole Normale Sup\'erieure, PSL Research University, Paris, France}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates.
    While the very recent TabPFNv2 foundation model (2025) excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables?
    We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 56 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data. Inference code and pre-trained models are available at \href{https://github.com/soda-inria/tabicl}{https://github.com/soda-inria/tabicl}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Tabular data, structured in rows and columns, is widely used in industries like healthcare \cite{cartellaAdversarialAttacksTabular2021} and finance \cite{johnsonMIMICIIIFreelyAccessible2016}, where tabular classification problems underpin numerous real-world applications. In other data modalities, foundation models --particularly Large Language Models (LLMs) \cite{zhouComprehensiveSurveyPretrained2024}-- have significantly advanced the ability to tackle new tasks and few-shot learning. This is largely due to their remarkable in-context learning (ICL) capabilities \cite{brownLanguageModelsAre2020}, which enable them to capture patterns in prompts without requiring parameter updates. This success combined with the pervasiveness of tables have spurred interest in tabular foundation models \cite{vanbreugelWhyTabularFoundation2024}.

While LLMs are primarily designed to model natural language, attempts to fine-tune them for tabular data have emerged \citep[and references therein]{hegselmann2023tabllm, fang2024largelanguagemodelsllmstabular}. These efforts rely on table serialization, which is the process of converting table rows into text or sentences which can then be tokenized. \citet{gardnerLargeScaleTransfer2024} fine-tuned a Llama 3-8B on a corpus of serialized tables and demonstrated the effectiveness of this approach compared to tree-based models in the few-shot setting. However, such language models-based approaches are limited by the size of their context window to fit large serialized tables (e.g. up to 32 or 64 shots in \citealt{gardnerLargeScaleTransfer2024}). Moreover, it is unclear whether LLMs can effectively handle numerical values \citep{thawaniRepresentingNumbersNLP2021}. Finally, as evidence shows that LLMs are pretrained on many popular datasets \cite{bordt2024elephants}, their evaluation on tabular prediction tasks should also be conducted with care.

Taking a markedly different approach,  \citet{hollmannTabpfnTransformerThat2022} introduced TabPFN, a transformer-based tabular foundation model for classification tasks, pretrained on synthetic tabular datasets only. A key feature of TabPFN is ICL with tables, which eliminates the need for tokenization and enables efficient handling of small tables with up to 1K samples and 100 features. Very recently, the same authors introduced TabPFNv2 \yrcite{hollmannAccuratePredictionsSmall2025}, an improved version that significantly outperforms tree-based and neural network competitors on small-to-medium datasets with up to 10K samples and 500 features. The great promise of tabular ICL has spurred a new line of research (see \cref{ss:tabpfn_offsprings}), yet the quadratic cost of self attention is a threat to scalability of all these. TabPFNv2 uses a two-way attention mechanism alternating between column-wise and row-wise attentions, which limits its scalability for large datasets. In real-world scenarios, where industrial datasets can contain millions of samples \citep{rubachev2024tabred}, the high computational and memory demands of TabPFNv2 hinder its practicality.

In this paper, we introduce TabICL, a scalable and efficient tabular foundation model designed for classification tasks. Pretrained on synthetic datasets with up to 60K samples, TabICL can effectively handle datasets with up to 500K samples and 500 features, significantly expanding the scalability boundaries of ICL for tables and cementing it as a foundational technique for tabular foundation models.

To handle tables of arbitrary sizes with architectural changes, TabICL treats individual cells as basic elements: each column is seen as a set of cell values capturing feature-specific distribution and semantics, and each row consists of interdependent feature values providing a holistic view of each sample. TabICL employs a two-stage architecture to achieve efficient ICL for tabular data. First, it encodes rows (excluding target labels) into dense vector embeddings. Each embedding is designed to be capable of capturing the entire table information. This stage effectively collapses the column dimension to substantially reduce computational complexity and memory footprint for subsequent ICL. Second, it combines these compact yet informative embeddings with corresponding labels and then performs ICL. Therefore, the core of TabICL lies in its embedding strategy of the first stage, which is supposed to transform rows into semantically rich embeddings.

Words and phrases in text often carry clear semantics, and are thus naturally associated to informative embeddings \cite{mikolovEfficientEstimationWord2013}. However, tabular data lacks such an inherent structure: cell values can be ambiguous without metadata such as column names or data types. To address this challenge, TabICL adopts a well-constrained embedding strategy combining (1) distribution-aware column-wise feature embedding to capture statistical regularities within each column and (2) attention-based row-wise interaction to model dependencies across columns, thereby constructing semantically grounded representations for tabular data.

Feature embedding involves mapping scalar cell values into high-dimensional vectors for each feature, serving as a critical factor in model performance \cite{gorishniyEmbeddingsNumericalFeatures2022}. Since features often exhibit vastly different distributions, previous approaches typically use feature-specific embedding modules without parameter sharing, which, however, limits cross-table transferability. In this work, we reformulate feature embedding as a \emph{set-input} problem, where a permutation-invariant set of cell values acts as input, and the output comprises corresponding one-to-one embeddings. To achieve this, we leverage the Set Transformer \cite{leeSetTransformerFramework2019}, a model specifically designed to process sets through efficient induced self-attention. It excels in tasks such as identifying extrema and counting unique elements, enabling the discovery of distribution-related metadata within each column and enhancing the ability to distinguish between features of different data types.

The feature embeddings are then processed per-row by another transformer, and aggregated into a single vector using learnable [CLS] tokens. This effectively captures complex feature interactions and accommodates a varying number of features. Overall, this column-then-row attention-based embedding achieves efficient sparse attention across all cells by leveraging the column/row inherent structure of tabular data as a strong inductive bias.
Finally, the resulting row-wise embeddings are handled by a final transformer for ICL.

In addition to the above innovations, we introduce several improvements: (1) We refine pretraining synthetic datasets of TabPFN by adding a new tree-based data-generating model to incorporate the inductive biases of tree-based models \cite{grinsztajnWhyTreebasedModels2022,breejenWhyInContextLearning2024}; (2) We adopt curriculum learning by gradually scaling the pretraining dataset size from 1K to 60K; (3) To address classification problems with over 10 classes (the pretraining limit), we use hierarchical classification \cite{sillaSurveyHierarchicalClassification2011}, breaking them into a hierarchical structure of subproblems with $\leq$ 10 classes. The increase in the number of tasks is largely offset by the fast ICL-powered predictions of TabICL.

To summarize our contributions:
(1) We present TabICL, a novel scalable tabular foundation model for classification tasks that can accommodate any number of samples, features, and classes. In practice, TabICL handles up to 500K samples and 500 features with around 20GB of GPU memory;
(2) We introduce a distribution-aware feature embedding approach that handles features with diverse properties in a unified manner, unlocking new possibilities for cross-table transferability;
(3) TabICL performs tasks in a single forward pass and is orders of magnitude faster than tabular methods requiring hyper-parameter tuning while still providing better performance in most cases. TabICL is also consistently faster than TabPFNv2 (up to 10 times), with efficiency gains increasing as dataset size grows;
(4) We evaluate TabICL on the TALENT benchmark \cite{yeCloserLookDeep2025a}, comprising 200 classification datasets across various domains and sizes (up to 150K samples). TabICL performs comparably to TabPFNv2 on medium-sized datasets and significantly outperforms all other methods. On the 55 large datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost \cite{dorogushCatBoostGradientBoosting2018}. These results demonstrate the potential of ICL for large data.

\section{Related Work}
\label{sec:related}

\begin{figure*}
	\centering
	\includegraphics[width=0.98\textwidth]{figures/TabICL2}
	\caption{\textbf{An overview of the architecture of TabICL.} First, column-wise embedding transforms each cell of the input table into an embedding vector using a transformer $\TFcol$ (\Cref{sec:tfcol}), producing $E$. Next, row-wise interaction prepends four trainable [CLS] tokens to $E$, applies rotary positional encoding, and processes $E$ row-by-row with a transformer $\TFrow$. Concatenating the outputs of [CLS] tokens yields the final row-wise embeddings $H$. Finally, dataset-wise ICL operates on $H$ and uses a Transformer $\TFicl$ to predict the target labels for the test set in a single forward pass. Overall, TabICL consists of three transformers.}
    \label{fig:arch}
\end{figure*}

\subsection{Foundation Models and In-Context Learning}

In recent years, deep learning (DL) has been transformed by the emergence of foundation models, which are pretrained on massive, diverse datasets and serve as versatile backbones for downstream tasks. These transformer-based models enable In-Context Learning (ICL): performing tasks by analyzing prompts containing input-output pairs, without explicit training or parameter updates. ICL operates as a form of on-the-fly reasoning. The mechanism underlying ICL remains elusive, with prevailing explanations framing it as implicit Bayesian inference \cite{xieExplanationIncontextLearning2022,mullerTransformersCanBayesian2024}, gradient descent optimization \cite{vonoswaldTransformersLearnIncontext2023}, and algorithmic learning \cite{gargWhatCanTransformers2023}.

\subsection{Tabular Deep Learning Models}

Gradient-boosted decision trees (GBDTs), such as CatBoost and XGBoost \cite{chenXGBoostScalableTree2016a}, have long dominated the tabular domain. However, there are growing efforts to develop tabular DL models. Recent studies indicate a narrowing performance gap between GBDTs and tabular DL models \cite{yeModernNeighborhoodComponents2024,gorishniyTabMAdvancingTabular2024}.

As tabular DL improves, cross-table transferability emerges as an important topic. Notable efforts in this direction include XTab \cite{zhuXTabCrosstablePretraining2023} and CARTE \cite{kimCARTEPretrainingTransfer2024}, which incorporate transferable components that are typically shareable backbone networks and dataset-specific components that require fine-tuning for each new task. The advent of tabular foundation models can bring new possibilities to cross-table learning, paving the way for large-scale pretraining and transfer learning across tables.

\subsection{TabPFN and its Offsprings}
\label{ss:tabpfn_offsprings}

TabPFN, short for Tabular Prior-Data Fitted Network, is a tabular foundation model. It is a transformer pretrained on extensive synthetic datasets to perform tabular classification tasks through ICL. TabPFN interprets ICL from a Bayesian perspective as an approximate posterior predictive distribution over synthetic datasets. Several variants aim to enhance its scalability, including distilling training data into a compact learned context via prompt tuning \cite{maInContextDataDistillation2024,feuerTuneTablesContextOptimization2024}, selecting the most relevant subset of training data for each test sample \cite{xuMixtureInContextPrompters2024,thomasRetrievalFineTuningInContext2024,koshilLocalizationDataEmbedding}, replacing quadratic with linear attention \cite{zeng2024tabflex}, and generating small task-specific neural networks via an ICL-based hypernetwork \cite{mullerMotherNetFoundationalHypernetwork2023}. However, most variants do not structurally improve TabPFN but instead act as prompt engineering to reduce in-context samples.

Other approaches try to improve the quality of pre-training data, such as TabForestPFN \cite{breejenWhyInContextLearning2024} incorporating tree-based synthetic datasets and TabDPT \cite{maTabDPTScalingTabular2024} curating and using real-world datasets.

Very recently (January 2025), TabPFNv2 was released and largely improved TabPFN in terms of both prediction performance and scalability. Our contributed model, TabICL, achieves comparable performance to TabPFNv2 while being more scalable and computationally efficient.

\section{The TabICL Architecture}
\label{sec:method}

We consider a tabular classification task with an input space $\mathcal{X} \in \mathbb{R}^m$ and a target space $\mathcal{Y} \in [1, \cdots, C]$. Given a training dataset of input-output pairs $\mathcal{D}_\text{train} = \{ (x_\text{train}^{(i)}, y_\text{train}^{(i)}) \}_{i=1}^{n_\text{tr}}$ and test samples $X_\text{test} = \{ x_\text{test}^{(i)} \}_{i=1}^{n_\text{te}}$, our goal is to predict the class probabilities $p(\cdot | x_\text{test}, \mathcal{D}_\text{train})$.

\begin{figure*}[t!]
	\centering
	\begin{subfigure}{0.53\linewidth}
		\includegraphics[width=\textwidth]{figures/Induced_Self2}
		\caption{Efficient induced self-attention}
		\label{fig:ind_attn}
	\end{subfigure}
        \
	\begin{subfigure}{0.46\linewidth}
		\includegraphics[width=\textwidth]{figures/Col_Embed2}
		\caption{Column-wise embedding}
		\label{fig:col_embed}
	\end{subfigure}
	\caption{\textbf{The distribution-aware column-wise embedding block in TabICL.} The overall structure (right) relies on the induced self-attention block (middle) that employs two multi-head attention blocks (left) to process incoming embeddings.}
	\label{fig:tabicl}
\end{figure*}

\subsection{High-level Structure: Embedding then ICL}

TabICL comprises two key modules: a tabular embedding module followed by an ICL module, with labels utilized exclusively in the ICL module. The tabular embedding module encodes table rows into dense vector representations while taking the inherent column-row structure into account. This module consists of two components: distribution-aware column-wise embedding, which captures statistical characteristics of individual columns, and context-aware row-wise interaction, which models dependencies between features. The ICL module subsequently processes both training and test embeddings through a transformer, enabling the prediction of the entire test set in a single forward pass through ICL. The overall architecture is depicted in \cref{fig:arch}.

\subsection{Distribution-aware Column-wise Embedding} \label{sec:tfcol}

The column-wise embedding (or feature embedding) maps each scalar cell in a column $c_j \in \mathbb{R}^n$ into a $d$-dimensional embedding. Unlike typical approaches that assign a separate linear layer to each column \cite{gorishniyEmbeddingsNumericalFeatures2022}, we embed all columns through a shareable Set Transformer $\TFcol$, as illustrated in \cref{fig:col_embed} and formulated as follows:
\begin{align}
    W,B  &= \TFcol(c_j) &&\hspace{-4em} \in \mathbb{R}^{n \times d} \label{eq:st} \\
    e_j  &= W \odot c_j + B        &&\hspace{-4em} \in \mathbb{R}^{n \times d} \label{eq:fe6}
\end{align} 
Note that each cell in a column is assigned its own weight and bias. Essentially, feature embedding can be framed as a \emph{set-input} problem, where $\TFcol$ serves as a hypernetwork taking as input a permutation-invariant set of cell values and generating distribution-aware weights and biases.

The operations within $\TFcol$ unfold as follows:
\begin{align}
U &= \text{Lin}(c)      &&\hspace{-3em} \in \mathbb{R}^{n \times d} \label{eq:lin_X} \\
\tikzmark{start}M &= \text{MAB}_1(V_I, U_{\text{train}}, U_{\text{train}}) &&\hspace{-3em} \in \mathbb{R}^{k \times d} \label{eq:M} \\
V &= \text{MAB}_2(U, M, M) &&\hspace{-3em} \in \mathbb{R}^{n \times d} \label{eq:V}\tikzmark{end} \\
W, B &= \text{Lin}(V)    &&\hspace{-3em} \in \mathbb{R}^{n \times d} \label{eq:lin_WB}
\end{align}
\begin{tikzpicture}[remember picture, overlay]
  \coordinate (s) at ($(pic cs:start) + (-0.3, 0.4)$);
  \coordinate (e) at ($(pic cs:end) + (0.3, -0.2)$);
  \draw[thick] (s) rectangle (e);
  \coordinate (ls) at ($(s)!0.5!(s|-e)$);
  \node[left=5pt of ls, anchor=east] {ISAB};
\end{tikzpicture}
where Lin represents a linear layer, MAB denotes multi-head attention block, and IASB stands for induced self-attention block (\cref{fig:ind_attn}), introduced by \cite{leeSetTransformerFramework2019}. A column \( c \) (omitting the subscript \( j \) for simplicity) is projected into a \( d \)-dimensional space (\( d = 128 \)) via a linear layer, processed by ISAB consisting of \(\text{MAB}_1\) and \(\text{MAB}_2\), and passed through linear layers to generate $W$ and $B$.

ISAB reduces self-attention complexity to \(\mathcal{O}(n)\) while preserving the ability to capture global information through a two-stage attention mechanism. In \(\text{MAB}_1\), inducing vectors \(V_I\) act as queries and attend to training samples \(U_{\text{train}}\) to generate induced representations \(M\). In \(\text{MAB}_2\), inputs \(U\) (including both training and test samples) serve as queries and attend back to \(M\), enabling global information to propagate across all training samples. We use $k = 128$ inducing vectors, 4 attention heads in the MABs, and 3 ISAB blocks (only one ISAB block is shown in \cref{eq:M,eq:V} for clarity). Crucially, only train samples serve as keys and values in \(\text{MAB}_1\). This ensures that the outputs of ISAB depend solely on training data, thereby preventing data leakage.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/embeddings}
    \caption{\textbf{Learned column-wise embeddings encode statistical properties of feature distributions.} Visualization of these embeddings projected onto their first two principal components for 40,000 features from synthetic datasets.}
    \label{fig:embeddings}
\end{figure}

To get insights into the information captured in the learned feature embeddings, we visualize $M \in \mathbb R^{k \times d}$ (\cref{eq:M}) produced by the final ISAB block of $\TFcol$. We summarize $M$ by summing along the first dimension (i.e., aggregating the induced representations of all inducing vectors) to obtain a single vector per column, and then apply Principal Component Analysis. \Cref{fig:embeddings} reveals that columns with similar skewness (resp. kurtosis) tend to cluster together, i.e., non-symmetric distributions are differentiated from symmetrical ones (\cref{fig:embeddings} left), and heavy-tailed distributions are differentiated from light-tailed ones. This suggests that $\TFcol$ encodes distributional properties in a structured manner, and cells in a column are probably embedded to reflect their statistical role (e.g., min, max, mean, mode). Therefore, the learned feature embeddings should distinguish features based on their unique distributional properties, effectively serving as feature identifiers. This contrasts with methods that rely on semantic column names \cite{kimCARTEPretrainingTransfer2024} or learn feature identifier vectors \cite{kossenSelfattentionDatapointsGoing2021}.

\subsection{Context-aware Row-wise Interaction} \label{sec:tfrow}

After obtaining all feature embeddings \( E = [e_1, \cdots, e_m] \in \mathbb{R}^{n \times m \times d} \), a 3-layer transformer with 8 attention heads, denoted by $\TFrow$, processes \( E \) for inter-feature interactions. To aggregate the embeddings into a single vector, four learnable [CLS] tokens are prepended to each row of \( E \), and their final outputs are concatenated together. We use four tokens to provide richer representations with a total embedding size of $4 \times d = 512$ for subsequent ICL, while maintaining a lower embedding size ($d = 128$) for $\TFcol$ and $\TFrow$ to reduce memory consumption.

A defining characteristic of tabular data is that columns do not have a natural ordering. Ideally, tabular methods should be invariant to permutations of columns. Unlike TabPFN (v1), TabICL naturally incorporates this invariance through the row-wise attention. However, we experimentally observed that $\TFrow$ can suffer from a representation collapse issue. As mentioned earlier, features are identified by their distributional properties after column-wise embedding. Consequently, features originating from similar distributions thus become less distinguishable. In the extreme case where all features are drawn from the same distribution, $\TFrow$ cannot differentiate a sample from any of its column-permuted versions, leading to nearly identical representations for originally distinct samples, i.e., representation collapse. This phenomenon is exemplified by the {\tt balance scale} dataset \cite{balance_scale_12}, as shown in \cref{fig:collapse}. In this dataset, all features follow the same discrete distribution and can take only 5 values, making collapse highly probable. After processing with $\TFrow$, we observe that many samples collapse to the same representation (rightmost plot), despite being originally distinct (leftmost plot).

To break the symmetry between identically distributed features, we incorporate rotary positional embedding \citep[RoPE,][]{suRoFormerEnhancedTransformer2023} in $\TFrow$. RoPE is widely adopted in recent LLMs \citep{meta2024llama3} and directly encodes relative positional information into the attention mechanism by rotating the query and key vectors. The rotation angle is determined by the position $p$ in the sequence and the dimension index $i$, defined as \( \theta_{i} = p / ( \text{base}^{2i/d} ) \), where \(d\) is the embedding dimension and \(\text{base}\) is the frequency scaling factor. More details can be found in \cref{app:rope}.

\begin{figure}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/balance_scale}
    \caption{\textbf{An example of learning collapse without RoPE for the {\tt balance scale} dataset.} (\emph{Upper}) The histograms show that the four features follow the same discrete distribution. (\emph{Lower}) The t-SNE visualization of the input features and learned row-wise embeddings $H$ with and without RoPE demonstrates that RoPE can effectively alleviate representation collapse. The colors represent 3 classes.}
    \label{fig:collapse}
\end{figure}

While the use of RoPE breaks permutational invariance, it can mitigate the representation collapse. For example, on the balance scale dataset, RoPE preserves distinct representations for different samples (\cref{fig:collapse} middle). Following \citet{xiongEffectiveLongContextScaling2023}, we set a large scaling factor of 100,000 for RoPE to enhance generalization to a larger number of features than those seen during training (up to 100). To approximately restore permutation invariance, TabICL adopts the same strategy as other TabPFN-like models by ensembling predictions over multiple column permutations.

\subsection{Dataset-wise In-Context Learning} \label{sec:tficl}

After converting all samples into embeddings \( H \in \mathbb{R}^{n \times 4d} \), training labels are mapped to the same space as \( H \) using one-hot encoding. The embeddings of \( X \) and \( y \) for the training set are added to create the final training embeddings \( H_{\text{train}} \). We then process \( H_{\text{train}} \) and \( H_{\text{test}} \) using a 12-layer Transformer with 4 attention heads, denoted by \( \text{TF}_{\text{icl}} \). The embeddings in \( H_{\text{train}} \) can attend to one another while those in \( H_{\text{test}} \) can only attend to \( H_{\text{train}} \). Finally, a two-layer MLP converts the outputs of \( H_{\text{test}} \) into class probabilities for the test samples.

\section{Pretraining and Inference}

\subsection{Improved Pretraining Synthetic Datasets}
TabICL is pre-trained exclusively on synthetic datasets. To ensure realistic dependencies among variables, we generate these datasets using structural causal models (SCMs), following the approach proposed in TabPFN (v1). We first sample a directed acyclic graph (DAG) to define dependencies, following the structure of a fully connected MLP, where each neuron corresponds to a variable. Each feature \( c \)  is then modeled as a function \( f \) of its parent variables \( \text{Pa}(c) \) in the graph, with added independent noise \( \epsilon \), \emph{i.e.}, \( c = f(\text{Pa}(c)) + \epsilon \). Compared to previous work, we enrich the dataset generation in two ways: (i) we introduce tree-based SCMs to benefit from their inductive biases, and (ii) increase the diversity of modeling functions $f$.

\paragraph{Tree-based generation} As tree-based models excel on tabular data, we introduce tree-based SCMs to leverage their ability to model complex interactions and hierarchical dependencies between variables. We define \(f\) using an XGBoost regression model, as it is widely favored by practitioners and supports multi-output regression. At each layer of the graph, an XGBoost model is trained on fake targets drawn from Gaussian noise, taking the values of the parent variables as input. The obtained predictions then become the values of the child variables. To balance data generation, we combine SCMs (70\%) with tree-based SCMs (30\%). \cref{app:datasets} gives details and examples of generated data.

\paragraph{Diversifying activation functions}
In TabPFN (v1), \(f\) is defined as a random affine mapping (a linear layer) with an activation function chosen from \([ \text{Identity}, \text{Tanh}, \text{Leaky ReLU}, \text{ELU} ]\). We enrich this set with 15 additional activation functions to enhance the diversity of non-linear dependencies, introducing for example non-monotone or discontinuous functions. We also include random activation functions sampled from Gaussian processes with random kernels. Gaussian Process functions have been used for synthetic data generation for time series foundation models \citep{ansari2024chronos}, but not as activation functions and with different types of kernels. Finally, we added an option to use different activation functions across layers and applied standardization followed by random rescaling before each activation function. \cref{fig:acts} visualizes the employed activation functions.

\subsection{Curriculum Learning for Large-scale Pretraining}

Similar to pretraining LLMs on shorter sentences before moving to longer ones, we gradually increase the size of synthetic datasets (i.e., the number of samples) while adjusting the micro batch size \( N_{\mathcal{B}} \) used for gradient accumulation to accommodate memory constraints, as follows:
\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=0pt]
    \item \( N_{\mathcal{B}} = 4 \) with a fixed size of 1,024 for 100K steps;
    \item \( N_{\mathcal{B}} = 1 \) with the size randomly drawn from a log-uniform distribution between 1K and 40K over 2K steps. Activation checkpointing is enabled for datasets exceeding 10K samples, and we accordingly reduce the number of features to avoid out-of-memory issues;
    \item \( N_{\mathcal{B}} = 1 \) with the size uniformly sampled between 40K and 60K for 50 steps, training only \( \TFicl \) while freezing all other components.
\end{enumerate}

Each step consists of 512 datasets with the number of features ($\le 100$) and classes ($\le 10$) randomly sampled. FlashAttention and automatic mixed precision are applied globally. The pretraining took 2 weeks on three A100 GPUs with 40GB memory using PyTorch (10, 3, and 1 days for stage 1, 2, and 3, respectively). \cref{app:pretraining} gives more pretraining details.

\subsection{Hierarchical Class-extension Strategy} \label{sec:hierarchical_classification}

We tackle many-class classification problems ($>10$ classes) through hierarchical classification \cite{sillaSurveyHierarchicalClassification2011}. Specifically, we recursively and evenly partition classes into subgroups of up to 10 classes, forming a multi-level classification tree. A classification problem with $k$ classes requires a hierarchy with depth $r = \left\lceil \log_{10} k \right\rceil$. Each node in the tree corresponds to a sub-task that predicts probabilities for its subgroup. During inference, the final probability for a given class is obtained by multiplying the probabilities across all relevant nodes from root to leaf.

As noted earlier, labels are used exclusively in the final ICL block. Consequently, the hierarchical tree is constructed during dataset-wise ICL, with all sub-tasks sharing the learned row embeddings $H$ and the same $\TFicl$ for ICL predictions. This kind of sharing greatly enhances the efficiency of TabICL in hierarchical classification scenarios.

\subsection{Memory-efficient Inference}

Using FlashAttention, which offers linear memory complexity with respect to sequence length, we observed that the peak activation memory can be effectively modeled by a polynomial regression during inference: \( \alpha_1 \times \text{batch\_size} + \alpha_2 \times \text{seq\_len} + \alpha_3 \times \text{batch\_size} \times \text{seq\_len} + \alpha_4 \). This enables us to dynamically adjust the batch size based on sequence length and available GPU memory. 
The batch dimension serves different roles upon the context: it represents the number of columns for column-wise embedding, the sample size for row-wise interaction, and the number of datasets for dataset-wise ICL.
Inspired by \citet{rajbhandariZeROInfinityBreakingGPU2021}, intermediate activations can be offloaded to CPU and disk as needed to further reduce memory consumption. These optimizations enable TabICL to handle datasets with 100K samples and 500 features using only 5 GB of GPU memory and 32 GB of RAM. This suffices for most real-world applications. See \cref{app:inference} for details.

\section{Experiments}

\subsection{Benchmark}

We evaluate TabICL on the benchmark by \citet{yeCloserLookDeep2025a}, referred to as ``TALENT'' here. It comprises 200 classification datasets (120 binary and 80 multiclass datasets) and comes with results for over 30 baselines. We will first analyze the 188 datasets with at most 10 classes, since these can be handled natively by TabICL and TabPFNv2.

Datasets are split into 64\% training, 16\% validation, and 20\% test data. While most models rely on the validation set for early stopping and hyperparameter tuning, TabICL and TabPFNv2 do not. Nonetheless, we opted to train TabICL and TabPFNv2 using only the training data, which places them at a disadvantage.
On the flip side, both TabICL and TabPFNv2 leverage ensembles, unlike the other deep learning models. A fair comparison would require training the other models as an ensemble as well. However, this is computationally expensive, was not part of the original benchmark, and is not implemented in these experiments. Both TabICL and TabPFNv2 average over 32 predictions obtained by randomly shuffling columns and classes and using different pre-processors. TabICL applies $z$-normalization with or without power transformation to all input features. See \citet{hollmannAccuratePredictionsSmall2025} for details on TabPFNv2. To avoid out-of-memory issues, we subsample training sets to 30K samples for TabPFNv2, while TabICL could predict on all datasets without subsampling. In addition, we disable automatic mixed precision across all methods during inference to ensure reproducibility and consistent time measurements.

\subsection{Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/rel_imp_over_mlp2}
    \caption{\textbf{Accuracy and training/inference times on the TALENT benchmark up to 10 classes.}  For TabICL and TabPFNv2, the time reported corresponds to training+inference on an A100 GPU. For the other models, it includes both training and hyperparameter tuning. Since \citet{yeCloserLookDeep2025a} track only the training time using the best hyperparameters found after 100 tuning steps, we approximate the total training time by multiplying this value by 100. For TabPFN (v1), we do not report the time since the original time measurement does not include the inference time.}
    \label{fig:rel_imp_over_mlp}
\end{figure}

\paragraph{TabICL obtains state-of-the-art accuracy.} \Cref{fig:rel_imp_over_mlp} shows accuracies relative to the tuned MLP for all models. TabICL obtains the best median relative accuracy across all datasets while being much faster than traditional state-of-the-art models: the geometric mean training+inference time per 1K samples for TabICL is 1.1 seconds, while tuning CatBoost on a CPU takes around 3 minutes, and RealMLP and ModernNCA on a GPU take around 7 minutes. Looking at the ranks of each method based on the obtained accuracies, the critical difference diagram in \Cref{fig:cdd_all} shows that TabICL and TabPFN2 outperform competitors by a wide margin, while the difference between the two of them is not statistically significant.

\paragraph{Speedup over TabPFNv2.} \Cref{fig:speedup_vs_tabpfn2} shows that TabICL is 1.5$\times$ faster than TabPFNv2 on small datasets and 3-10$\times$ faster on large datasets. This is facilitated by the hybrid architecture of TabICL, using fewer of the expensive row-wise and column-wise attention layers with smaller embedding dimension before ICL on tokenized rows. On a dataset with 10,000 samples and 100 features, TabICL takes roughly 20 seconds versus 1 minute 40 seconds for TabPFNv2, while for a dataset of 1000 samples and 10 features, TabICL takes 1 second compared to 2 seconds for TabPFNv2. In \Cref{fig:time_fit}, we fit simple scaling laws to predict the runtime of TabICL and TabPFN2 based on the number of samples and features. These scaling laws show that the average speedup of TabICL remains around five-fold for large datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/speedup_plot.pdf}
    \caption{\textbf{Speedup of TabICL vs.\ TabPFNv2 on datasets with less than 30K samples and at most 10 classes.}}
    \label{fig:speedup_vs_tabpfn2}
\end{figure}

\paragraph{TabICL enables ICL for large datasets.} While TabPFNv2 achieves excellent performance on datasets up to 10K samples, it has only been pre-trained with up to 2048 training samples and can fail on datasets above 30K samples due to its memory usage. \Cref{fig:mf_n_samples} shows that unlike TabPFNv2, the performance of TabICL remains strong for larger datasets. While ICL is often used in few-shot settings, this demonstrates that foundation models can compete with the best deep learning or tree-based models, even in large-sample regimes where the latter have the most potential.
Additional results in \Cref{sec:appendix:experiments} demonstrate that TabICL also performs well for many classes, features, and high ratios of categorical features.

\paragraph{TabICL produces reliable probabilities.} In many practical situations, the quality of the estimated probabilities is crucial for decision-making. Therefore, we also report the log loss (a.k.a.\ cross-entropy loss), which is a proper scoring rule and therefore rewards accurate prediction of probabilities \citep{gneiting2007strictly}. Since TabICL does not leverage hyperparameter tuning, its predictions are not specifically optimized towards accuracy. The critical difference diagrams in \Cref{sec:appendix:cdd} show that TabICL and TabPFN2 significantly outperform accuracy-tuned competitors on the log loss, though the difference between the two is not significant, indicating that both produce more reliable probability estimates than the other models.

\paragraph{TabICL remains effective with more than 10 classes.} On datasets with more than 10 classes, we apply the hierarchical classification strategy from \Cref{sec:hierarchical_classification} to TabICL. Thanks to its use of label-independent row embeddings that can be shared between all sub-classifiers, TabICL scales efficiently to a large number of classes. \Cref{fig:mul_10plus_cls} shows that TabICL still achieves the second best result on these datasets in terms of mean normalized accuracy, while TabPFNv2 cannot natively handle more than 10 classes.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/mf_n_samples_pwlin}
    \caption{\textbf{Model rankings as a function of sample size.} Each point is the rank of one method on one dataset. Lower rank is better. The lines show the bootstrap median and 10\% / 90\% bootstrap confidence intervals of a piecewise linear fit.}
    \label{fig:mf_n_samples}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/mul_10plus_cls}
    \caption{\textbf{Normalized accuracy across the 12 datasets with more than 10 classes.} Colors identify tree-based models (green), deep learning models (pink), retrieval models (gray), transformers (orange), and in-context models (blue).}
    \label{fig:mul_10plus_cls}
\end{figure}

\section{Conclusion}
We introduce TabICL, a novel tabular foundation model that extends the scalability of existing tabular foundation models by an order of magnitude. Evaluated on datasets with up to 100K training samples, it delivers excellent performance without hyperparameter tuning, making it nearly two orders of magnitude faster than other tabular methods requiring hyper-parameter tuning. TabICL achieves this through a hybrid architecture and memory-saving optimizations. Compared to the newly released and leading tabular foundation model --- TabPFNv2, our TabICL achieves comparable performance while being more scalable and faster.

\paragraph{Limitations}
Like other foundation models, TabICL suffers from slow inference speed, although TabPFNv2 has demonstrated that this issue can be alleviated to some extent through caching. Currently, TabICL is limited to classification problems, but as \citet{hollmannAccuratePredictionsSmall2025} showed, regression problems can be treated with similar methodology.
Our evaluation inherits the strengths and weaknesses of the TALENT benchmark. In particular, TALENT as most other benchmarks trains single models tuned using the holdout method, while ensembling models evaluated using cross-validation can improve performance.
Bagging increases computational cost, whereas TabICL can simply be trained on the full data without needing a validation set.

\paragraph{Outlook} In-context learning for tabular data was originally introduced as a speedup for small tables \cite{hollmannTabpfnTransformerThat2022}. The expressive power of a forward pass in a transformer may seem limited, and one might wonder if in-context learning loses its edge, given sufficient data. We find that even with large data, pre-training can create implicit priors that give in-context transformers a competitive advantage.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
We thank Léo Grinsztajn for coming up with the idea to add a tree-based part to the prior. We also thank Lennart Purucker and Samuel Müller for interesting discussions.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\bibliography{reference,references_manual}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\onecolumn
% \appendix
\begin{appendices}
%
\listofappendices

\counterwithin{figure}{section}
\counterwithin{table}{section}

\crefalias{section}{appendix}
\crefalias{subsection}{appendix}

\newpage

\section{Further Experiments} \label{sec:appendix:experiments}

\paragraph{Predicting the runtime of TabICL and TabPFNv2.} To obtain rough predictions of the time for a forward pass (training and inference) in TabICL and TabPFN2, we leverage the runtime complexity of the column- and row-wise attention modules. On a table with $n$ rows and $m$ columns, intra-column attention has a time complexity of $O(n^2m)$ since it is performed separately for each column, while intra-row attention has a time complexity of $O(nm^2)$. Together, we obtain a complexity of $O(nm(n+m))$, where $nm$ is the number of cells of the table. We plot this quantity on the $x$-axis of \Cref{fig:time_fit} and fit models of the form
\begin{equation*}
    \text{time } = \alpha + \beta (nm(n+m))^\gamma
\end{equation*}
with MSLE loss, that is, a MSE loss between the log-transformed time and the log-transformed prediction. To facilitate a simpler comparison, we fix $\gamma \coloneqq 0.8$ for both models since it yields a good fit. We suspect that $\gamma = 1$ should be used asymptotically, but then more additional terms would be needed to obtain a good fit. In this model, the speedup of TabICL over TabPFNv2 for large datasets approaches $5$, while it is $1.4$ for small datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/time_fit.pdf}
    \caption{\textbf{Training+inference times of TabICL and TabPFNv2.} Each point represents the time on one dataset on an A100 GPU. We only use datasets with less than 30,000 samples because TabPFNv2 is applied with subsampling on larger datasets to avoid RAM overflow.}
    \label{fig:time_fit}
\end{figure}

\paragraph{Metafeatures.} We analyze the dependence of the ranks of TabICL, TabPFNv2, CatBoost, and ModernNCA based on different dataset metafeatures. To this end, we fit piecewise linear regression models with pre-defined nodes to predict the ranks of these models depending on a single metafeature. 

\Cref{fig:mf_n_classes} shows the scaling with the number of classes. The performance of TabICL deteriorates on datasets with three classes but not on datasets with more classes. It is unclear if this is really due to the number of classes or caused by a different characteristic present in the three-class datasets on the benchmark.

\Cref{fig:mf_n_features} shows the scaling with the number of features. These plots show that TabICL behaves well for large numbers of features even though it tokenizes entire rows in the middle of the network before seeing any labels.

\Cref{fig:mf_cat_ratio} shows the dependence on the ratio of categorical to numerical variables. In general, the performance of TabICL and TabPFNv2 deteriorates somewhat in the presence of many categorical variables. However, it is notable that TabICL performs slightly better than TabPFNv2 on such datasets even though TabPFNv2 has a more sophisticated categorical feature generation in its prior.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/mf_n_classes_pwlin.pdf}
    \caption{\textbf{Dependency of the benchmark rank on the number of classes.} Each point is the rank of one method on one dataset. Lower rank is better. The lines show the bootstrap median and 10\% / 90\% bootstrap confidence intervals of a piecewise linear fit.}
    \label{fig:mf_n_classes}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/mf_n_features_pwlin.pdf}
    \caption{\textbf{Dependency of the benchmark rank on the number of features.} Each point is the rank of one method on one dataset. Lower rank is better. The lines show the bootstrap median and 10\% / 90\% bootstrap confidence intervals of a piecewise linear fit.}
    \label{fig:mf_n_features}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/mf_cat_ratio_pwlin.pdf}
    \caption{\textbf{Dependency of the benchmark rank on the ratio of categorical to overall features.} Each point is the rank of one method on one dataset. Lower rank is better. The lines show the bootstrap median and 10\% / 90\% bootstrap confidence intervals of a piecewise linear fit.}
    \label{fig:mf_cat_ratio}
\end{figure}

\section{Synthetic Datasets for Pretraining}
\label{app:datasets}

\subsection{SCM prior with more activation functions}
\label{ss:scm_activations}

In the TabPFN prior, we replace the activation layers by the following sequence of layers:
\begin{itemize}
    \item A standardization layer that standardizes each feature across the batch (samples) dimension
    \item A random rescaling layer that, for each neuron $i$, computes $x_i \leftarrow \exp(2a)(x_i + b)$, with $a, b \sim \mathcal{N}(0, 1)$ sampled once per layer.
    \item A random activation function. With probability $1/2$, each layer uses the same type of activation function, otherwise each layer samples the type of activation function independently.
\end{itemize}

On top of the original activation functions $\{$Identity, tanh, LeakyReLU, ELU$\}$, we add the following activation functions:
\begin{itemize}
    % \item Identity $x \mapsto x$
    % \item Hyperbolic tangent (tanh)
    \item ReLU
    \item ReLU6 \citep{krizhevsky2010convolutional}
    % \item LeakyReLU with its default negative slope of $10^{-2}$ \todo{cite?}
    % \item ELU \todo{cite}
    \item SELU \citep{klambauer2017self}
    \item SiLU \citep{hendrycks2016gaussian}
    \item Softplus
    \item $\operatorname{Hardtanh}(x) = \max(-1, \min(1, x))$
    \item Signum function
    \item Sine
    \item $\mathrm{RBF}(x) = \exp(-x^2)$
    \item Exponential function
    \item $f(x) = \sqrt{|x|}$
    \item $f(x) = 1_{|x| \leq 1}$
    \item $f(x) = x^2$
    \item $f(x) = |x|$
    %\item $f(x) = \operatorname{ReLU}(x)^p$, where $p = \exp(q)$ for $q \sim \calN(0, 1)$ is sampled once per layer and dataset.
    \item A random function $f(x) = \phi(x)^\top \mathbf{z}$, where $\mathbf{z} \sim \mathcal{N}(0, 1)$ and the feature map $\phi$ is defined randomly as
    \begin{align*}
        \phi(x) & \coloneqq \frac{\mathbf{w}}{\|\mathbf{w}\|_2} \odot \sin(\mathbf{a}x + \mathbf{b}) \in \mathbb{R}^N, \\
        N &\coloneqq 256, \\
        b_i &\sim \mathcal{U}[0, 2\pi], \\
        a_i &\sim \mathcal{U}[0, N], \\
        w_i &\coloneqq a_i^{-\exp(u)}, \\
        u &\sim \mathcal{U}[0.7, 3.0]~.
    \end{align*}
    Here, all random parameters are drawn once per layer, and $\odot$ is an element-wise product. This is motivated by the fact that for fixed feature map $\phi$, the random function $f(x) = \phi(x)^\top \mathbf{z}$ is a Gaussian process with covariance kernel $k(x, x') = \phi(x)^\top \phi(x')$. The design of $\phi$ is inspired by random Fourier features \citep{rahimi2007random}. The randomly drawn exponent $-\exp(u)$ leads to different decays of coefficients with increasing frequency, which produces different levels of smoothness for the sampled function as shown in \Cref{fig:acts} (right) \citep{da2023sample}. This activation function applies standardization directly before the random function, such that the random rescaling has no effect.
%(which, unlike the other activation functions, is not an elementwise function but applied to the whole vector)
\end{itemize}
Here, the random function is sampled with a probability ten times higher than the other activation functions to account for the fact that it can represent many different functions.

\Cref{fig:acts} visualizes the used activation functions. \Cref{fig:datasets_mlp_scm} shows datasets from the resulting prior.

\begin{figure}
    \centering
    \includegraphics[width=0.48\linewidth]{figures/activations_non_random.pdf}
    \includegraphics[width=0.48\linewidth]{figures/activations_random.pdf}
    \caption{\textbf{Activation functions from the SCM prior.} Left: Non-random activation functions from the SCM prior without standardization and random rescaling. Activation functions are randomly flipped along the $x$- and/or $y$-axis to fully utilize the space in the plot. Right: Random instantiations of the random activation function from the SCM prior, including the automatic standardization of the inputs.}
    \label{fig:acts}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/datasets_mlp_scm.pdf}
    \caption{\textbf{Randomly generated 2D datasets from the SCM prior.} The color corresponds to the class label.}
    \label{fig:datasets_mlp_scm}
\end{figure}

\subsection{Tree-based SCM prior}
\label{app_ss:tree-prior}

The tree-based SCM prior replaces the linear and activation layers in the SCM prior by XGBoost models fitted on random data. More specifically, these models are generated as follows:

\begin{itemize}
    \item Sample \texttt{n\_estimators} and \texttt{max\_depth} independently as $\min\{4, 1+\operatorname{Exponential}(\lambda=0.5)\}$ and $\min\{4, 2+\operatorname{Exponential}(\lambda=0.5)\}$, respectively.
    \item For each layer with $n$ input and $m$ output neurons, fit an XGBoost multi-output regressor with the parameters above on the layer inputs $x_i$ with standard normal targets $y_i \in \mathbb{R}^m$, then use the fitted model to predict on the given inputs.
\end{itemize}

 \Cref{fig:datasets_tree_scm} shows datasets from the tree SCM prior.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/datasets_tree_scm.pdf}
    \caption{\textbf{Randomly generated 2D datasets from the tree-based SCM prior.} The color corresponds to the class label.}
    \label{fig:datasets_tree_scm}
\end{figure}



\section{Rotary Positional Embedding}
\label{app:rope}

Rotary Positional Encoding (RoPE) is a technique used in Transformer models to represent positional information in the input data. RoPE encodes positional information directly into the attention mechanism through a rotation matrix applied to the query and key vectors in self-attention. Given a position $p$, a frequency $\omega$, and a vector $x$ (query or key vector):
$$
x_p = \text{RoPE}(x, p) = R(p) x
$$
where $R(p)$ is a rotation matrix applied to the vector $x$, encoding positional information using sinusoidal functions. The rotation matrix $R(p)$ is based on a sinusoidal function and applies rotations independently to 2-dimensional subspaces of $x$. For each 2D pair of vector components $(x_{2i}, x_{2i+1})$, the rotation is defined as:
\begin{align*}
    R(p) \begin{bmatrix}
x_{2i} \\
x_{2i+1}
\end{bmatrix} &\coloneqq
\begin{bmatrix}
\cos(\theta_{i}) & -\sin(\theta_{i}) \\
\sin(\theta_{i}) & \cos(\theta_{i})
\end{bmatrix}
\begin{bmatrix}
x_{2i} \\
x_{2i+1}
\end{bmatrix}, \\
\theta_{i} &\coloneqq \frac{p}{10000^{2i/d}}
\end{align*}
% $$
% R(p) \begin{bmatrix}
% x_{2i} \\
% x_{2i+1}
% \end{bmatrix} =
% \begin{bmatrix}
% \cos(\theta_{i}) & -\sin(\theta_{i}) \\
% \sin(\theta_{i}) & \cos(\theta_{i})
% \end{bmatrix}
% \begin{bmatrix}
% x_{2i} \\
% x_{2i+1}
% \end{bmatrix},
% $$
% $$
% \theta_{i} = \frac{p}{10000^{2i/d}}
% $$
where $d$ is the dimensionality of the embedding and $\omega_{i}=10000^{2i/d}$ determines the frequency for each dimension. The above equation indicates that for each dimension pair $(2i, 2i+1)$, the vector is rotated by an angle proportional to the position $p$ and the frequency $\omega_{i}$. RoPE directly modifies the query $Q$ and key $K$ vectors in the self-attention mechanism:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{(R(p_Q)Q) \cdot (R(p_K)K)^T}{\sqrt{d}}\right) V~.
$$
Because relative positional information is preserved in the inner product, the attention scores naturally encode the relative distance between positions $p_Q$ and $p_K$.

\citet{barberoWeGoWhat2024} provides an alternative perspective on RoPE that aligns well with its use in our work. At lower indices \(i\), the rotation angle changes more rapidly with increasing \(p\), resulting in high-frequency oscillations that resemble random noise and encode positional information. Conversely, at higher indices \(i\), the rotation angle changes more slowly with increasing \(p\), producing stable values that carry semantic information. In our case, RoPE effectively introduces noise to each feature as its identifier in a controlled, predictable, and generalizable manner.

RoPE is also claimed to exhibit long-term decay, where tokens become less correlated as their relative distance increases. However, this claim is questioned by \citet{barberoWeGoWhat2024}, as it relies on an unrealistic oversimplification that queries and keys are equal.


\section{Setup of TabICL}
\subsection{Pretraining details}
\label{app:pretraining}

As outlined in the main paper, we employ curriculum learning to progressively increase the size of synthetic datasets (i.e., the number of samples) during pretraining. This process unfolds in three stages by adjusting the micro-batch size used for gradient accumulation:
\begin{enumerate}[itemsep=2pt, parsep=0pt]
    \item \( N_{\mathcal{B}} = 4 \) with a fixed size of 1,024 the first 100K steps.
    \item \( N_{\mathcal{B}} = 1 \) with the size randomly drawn from a log-uniform distribution between 1K and 40K over 2K steps. Activation checkpointing is enabled for datasets exceeding 10K samples, and we accordingly reduce the number of features to avoid out-of-memory issues.
    \item \( N_{\mathcal{B}} = 1 \) with the size uniformly sampled between 40K and 60K for 50 steps, training only \( \TFicl \) while all other components remain frozen.
\end{enumerate}

Each step comprises 512 datasets. In the first stage, all datasets contain an equal number of samples. In the second and third stages, datasets in each micro-batch have the same number of samples, but the number of samples varies between different micro-batches. We use Adam \cite{kingmaAdamMethodStochastic2014} and clip the gradient norm to 1. The learning rate schedules for pretraining are shown in \cref{fig:lrs}.

\begin{figure}
	\centering
	\begin{subfigure}{0.33\linewidth}
		\includegraphics[width=\textwidth]{figures/learning_rates/lr_stage1}
		\caption{Cosine decay with warmup for stage 1}
	\end{subfigure}
        %
	\begin{subfigure}{0.33\linewidth}
		\includegraphics[width=\textwidth]{figures/learning_rates/lr_stage2}
		\caption{Polynomial decay for stage 2}
	\end{subfigure}
        %
	\begin{subfigure}{0.33\linewidth}
		\includegraphics[width=\textwidth]{figures/learning_rates/lr_stage3}
		\caption{Constant learning rate for stage 3}
	\end{subfigure}
        \label{fig:lrs}
	\caption{\textbf{Learning rate schedules for 3 pretraining stages.}}
\end{figure}

\subsection{Memory-efficient inference}
\label{app:inference}
By employing FlashAttention, we have observed that the inference peak GPU memory consumption of the three transformers of TabICL for column-wise embedding $\TFcol$, row-wise interaction $\TFrow$, and dataset-wise ICL $\TFicl$ can be well approximated through the following polynomial regression:
$$
    \text{MEM} = \alpha_1 \times \text{batch\_size} + \alpha_2 \times \text{seq\_len} + \alpha_3 \times \text{batch\_size} \times \text{seq\_len} + \alpha_4
$$

It is important to note that the specific meanings of batch size and sequence length vary across different transformers, as outlined below:

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lcc}
        \toprule
                          & \textbf{Batch Size}        & \textbf{Sequence Length}    \\ 
        \midrule
        $\TFcol$ & Number of features        & Number of samples  \\ 
        $\TFrow$ & Number of samples        & Number of features \\ 
        $\TFicl$ & Number of datasets       & Number of samples  \\ 
        \bottomrule
    \end{tabular}
    \caption{Notions of batch size and sequence length for different transformers}
    \label{tab:transformer_notions}
\end{table}

Given an input \( X \in \mathbb{R}^{b \times n \times m} \), where \( b \), \( n \), and \( m \) represent the number of datasets, the number of samples, and the number of features, respectively, \( X \) is first reshaped to \( \mathbb{R}^{(b \times m) \times n} \) and processed by $\TFcol$ to get \( E = \mathbb{R}^{(b \times m) \times n \times d} \). Subsequently, \( E \) is reshaped to \( \mathbb{R}^{(b \times n) \times m \times d} \) and passed to $\TFrow$, which generates \( H \in \mathbb{R}^{b \times n \times 4d} \). Finally, \( H \) is fed into $\TFicl$ to predict the test set entirely through ICL. We can see that it is necessary to set appropriate batch sizes for different transformers in order to efficiently utilize GPU resources and avoid out-of-memory errors. This is precisely where the aforementioned polynomial regression comes into play.

To this end, we first systematically tracked peak GPU memory usage of different transformers by varying both batch size and sequence length on a A100 GPU with 40GB memory, and then we fit the parameters of the above polynomial regression to the tracked data, as shown below:
\begin{align}
    \MEMcol &= (0.0708 \times \text{batch\_size}) + (7.29 \times 10^{-6} \times \text{seq\_len}) + (0.00391 \times \text{batch\_size} \times \text{seq\_len}) + 137.62  \nonumber \\
    \MEMrow &= (-2.07 \times 10^{-5} \times \text{batch\_size}) + (2.27 \times 10^{-4} \times \text{seq\_len}) + (0.00537 \times \text{batch\_size} \times \text{seq\_len}) + 138.54  \nonumber \\
    \MEMicl &= (-0.260 \times \text{batch\_size}) + (4.77 \times 10^{-7} \times \text{seq\_len}) + (0.0195 \times \text{batch\_size} \times \text{seq\_len}) + 140.58  \nonumber
\end{align}
The estimated memory is measured in megabytes (MB).

In addition to adjusting the batch size, we also offload intermediate activations to the CPU or disk as needed to further alleviate GPU memory constraints. \cref{fig:mem_inference} illustrates the CPU and GPU memory consumption for a large dataset containing 100K samples and 500 features (80\% training set and  20\% test set). As shown, only 5GB of GPU memory and 25GB of CPU memory are utilized, making this a highly affordable computational setup. 

\cref{fig:huge_mem_inference} shows the CPU and GPU memory consumption for a larger dataset containing 500K samples and 500 features (80\% training set and  20\% test set). As depicted, the computation requires less than 14GB of GPU memory. The CPU memory usage reaches approximately 120GB, which, however, can be significantly reduced through optional disk offloading via memory mapping.

We can observe that GPU memory consumption exhibits periodic fluctuations during the column-wise embedding and row-wise interaction phases. This is because the entire dataset is automatically divided into multiple batches, with the batch size determined dynamically based on the polynomial regression mentioned earlier. Additionally, during column-wise embedding, the output of $\TFcol$ is progressively offloaded to CPU. As a result, we can see an incremental increase in CPU memory usage throughout this stage.

We can also see that enabling automatic mixed precision highly reduces both memory consumption and computation time.

\begin{figure}
	\centering
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width=\textwidth]{figures/cpu_gpu_memory_no_amp2}
		\caption{CPU and GPU Memory Usage without Automatic Mixed Precision}
	\end{subfigure}
        \\
        \vspace{1em}
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width=\textwidth]{figures/cpu_gpu_memory_amp}
		\caption{CPU and GPU Memory Usage with Automatic Mixed Precision}
	\end{subfigure}
	\caption{\textbf{CPU and GPU memory consumption during inference for a dataset with 100K samples and 500 features.} During dataset-wise ICL, there is only a single batch (i.e., a single dataset), which corresponds to a sharp spike in GPU usage at the final stage of computation.}
	\label{fig:mem_inference}
\end{figure}


\begin{figure}
	\centering
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width=\textwidth]{figures/huge_cpu_gpu_memory_no_amp}
		\caption{CPU and GPU Memory Usage without Automatic Mixed Precision}
	\end{subfigure}
        \\
        \vspace{1em}
	\begin{subfigure}{0.9\linewidth}
		\includegraphics[width=\textwidth]{figures/huge_cpu_gpu_memory_amp}
		\caption{CPU and GPU Memory Usage with Automatic Mixed Precision}
	\end{subfigure}
	\caption{\textbf{CPU and GPU memory consumption during inference for a dataset with 500K samples and 500 features.} The relatively high CPU memory consumption can be largely mitigated by optional disk offloading via memory mapping.}
	\label{fig:huge_mem_inference}
\end{figure}

\newpage

\section{Average Performance and Rankings} \label{sec:appendix:cdd}

In this section, we present the average rank of all methods across different dataset categories, including binary datasets, multi-class datasets ($\le$10 classes), small classification datasets ($\le$10K samples), and large classification datasets ($>$10K samples). The rankings are computed based on accuracy, AUC, and Log Loss. The average rank is given in critical difference diagrams, with Wilcoxon-Holm tests with a significance level 0.05. The lower the rank value, the better the performance.

It is important to note that accuracy is used as the objective metric for hyperparameter tuning in tabular methods.

\begin{figure*}
	\centering
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/all/cdd_accuracy}
		\caption{Accuracy}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/all/cdd_auc}
		\caption{AUC}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/all/cdd_logloss}
		\caption{Log Loss}
	\end{subfigure}
	\caption{\textbf{Critical difference diagram for all 188 classification datasets ($\leq$ 10 classes).}} \label{fig:cdd_all}
\end{figure*}

\begin{figure*}
	\centering
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/binary/cdd_bin_accuracy}
		\caption{Accuracy}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/binary/cdd_bin_auc}
		\caption{AUC}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/binary/cdd_bin_logloss}
		\caption{Log Loss}
	\end{subfigure}
	\caption{\textbf{Critical difference diagram for 120 binary classification datasets.}} \label{fig:cdd_binary}
\end{figure*}


\begin{figure*}
	\centering
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/multi/cdd_mul_accuracy}
		\caption{Accuracy}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/multi/cdd_mul_auc}
		\caption{AUC}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/multi/cdd_mul_logloss}
		\caption{Log Loss}
	\end{subfigure}
	\caption{\textbf{Critical difference diagram for 68 multi-class classification datasets ($\leq$ 10 classes).}} \label{fig:cdd_multi}
\end{figure*}

\begin{figure*}
	\centering
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/small/cdd_accuracy}
		\caption{Accuracy}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/small/cdd_auc}
		\caption{AUC}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/small/cdd_logloss}
		\caption{Log Loss}
	\end{subfigure}
	\caption{\textbf{Critical difference diagram for 132 small classification datasets ($\leq$ 10K samples).}} \label{fig:cdd_small}
\end{figure*}

\begin{figure*}
	\centering
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/large/cdd_accuracy}
		\caption{Accuracy}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/large/cdd_auc}
		\caption{AUC}
	\end{subfigure}
        \\
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/cd/large/cdd_logloss}
		\caption{Log Loss}
	\end{subfigure}
	\caption{\textbf{Critical difference diagram for 56 large classification datasets ($>$ 10K samples).}} \label{fig:cdd_large}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{appendices}

\end{document}
