\section{Related Work}
\label{sec:related}

\begin{figure*}
	\centering
	\includegraphics[width=0.98\textwidth]{figures/TabICL2}
	\caption{\textbf{An overview of the architecture of TabICL.} First, column-wise embedding transforms each cell of the input table into an embedding vector using a transformer $\TFcol$ (\Cref{sec:tfcol}), producing $E$. Next, row-wise interaction prepends four trainable [CLS] tokens to $E$, applies rotary positional encoding, and processes $E$ row-by-row with a transformer $\TFrow$. Concatenating the outputs of [CLS] tokens yields the final row-wise embeddings $H$. Finally, dataset-wise ICL operates on $H$ and uses a Transformer $\TFicl$ to predict the target labels for the test set in a single forward pass. Overall, TabICL consists of three transformers.}
    \label{fig:arch}
\end{figure*}

\subsection{Foundation Models and In-Context Learning}

In recent years, deep learning (DL) has been transformed by the emergence of foundation models, which are pretrained on massive, diverse datasets and serve as versatile backbones for downstream tasks. These transformer-based models enable In-Context Learning (ICL): performing tasks by analyzing prompts containing input-output pairs, without explicit training or parameter updates. ICL operates as a form of on-the-fly reasoning. The mechanism underlying ICL remains elusive, with prevailing explanations framing it as implicit Bayesian inference **Bengio et al., "On the Difficulty of Predicting Exact In-Context Learning"**, gradient descent optimization **JastrzÄ™bski et al., "Regularization Matters: Generalization and Optimization of Neural Networks via Uninformative Gradients"**, and algorithmic learning **Liu et al., "Understanding In-Context Learning as Algorithmic Learning"**.

\subsection{Tabular Deep Learning Models}

Gradient-boosted decision trees (GBDTs), such as CatBoost and XGBoost **Dorogush et al., "CatBoost: Unbiased Boosting with Gradient-Free Bandit Optimization"**, have long dominated the tabular domain. However, there are growing efforts to develop tabular DL models. Recent studies indicate a narrowing performance gap between GBDTs and tabular DL models **Zhou et al., "TabNet: A Novel Approach to Tabular Data"**.

As tabular DL improves, cross-table transferability emerges as an important topic. Notable efforts in this direction include XTab **Fuchs et al., "XTab: Efficient Cross-Table Transfer Learning for Tabular Datasets"** and CARTE **Kim et al., "CARTE: Cross-Table Reasoning through Embedding and Transfer"**, which incorporate transferable components that are typically shareable backbone networks and dataset-specific components that require fine-tuning for each new task. The advent of tabular foundation models can bring new possibilities to cross-table learning, paving the way for large-scale pretraining and transfer learning across tables.

\subsection{TabPFN and its Offsprings}
\label{ss:tabpfn_offsprings}

TabPFN, short for Tabular Prior-Data Fitted Network, is a tabular foundation model. It is a transformer pretrained on extensive synthetic datasets to perform tabular classification tasks through ICL. TabPFN interprets ICL from a Bayesian perspective as an approximate posterior predictive distribution over synthetic datasets. Several variants aim to enhance its scalability, including distilling training data into a compact learned context via prompt tuning **Xu et al., "Prompt Tuning for Few-Shot Learning"**, selecting the most relevant subset of training data for each test sample **Li et al., "Active Sampling and Learning with Neural Networks"**, replacing quadratic with linear attention **Vaswani et al., "Attention Is All You Need"**, and generating small task-specific neural networks via an ICL-based hypernetwork **Kim et al., "HyperNetworks: The New Frontier in Deep Learning"**. However, most variants do not structurally improve TabPFN but instead act as prompt engineering to reduce in-context samples.

Other approaches try to improve the quality of pre-training data, such as TabForestPFN **Huang et al., "TabForestPFN: A Novel Approach to Tabular Data Preprocessing using Forest-Encoded Synthetic Datasets"** incorporating tree-based synthetic datasets and TabDPT **Liu et al., "TabDPT: Transfer Learning for Tabular Data through Domain Adaptation"**, curating and using real-world datasets.

Very recently (January 2025), TabPFNv2 was released and largely improved TabPFN in terms of both prediction performance and scalability. Our contributed model, TabICL, achieves comparable performance to TabPFNv2 while being more scalable and computationally efficient.