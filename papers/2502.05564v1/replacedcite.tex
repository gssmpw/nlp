\section{Related Work}
\label{sec:related}

\begin{figure*}
	\centering
	\includegraphics[width=0.98\textwidth]{figures/TabICL2}
	\caption{\textbf{An overview of the architecture of TabICL.} First, column-wise embedding transforms each cell of the input table into an embedding vector using a transformer $\TFcol$ (\Cref{sec:tfcol}), producing $E$. Next, row-wise interaction prepends four trainable [CLS] tokens to $E$, applies rotary positional encoding, and processes $E$ row-by-row with a transformer $\TFrow$. Concatenating the outputs of [CLS] tokens yields the final row-wise embeddings $H$. Finally, dataset-wise ICL operates on $H$ and uses a Transformer $\TFicl$ to predict the target labels for the test set in a single forward pass. Overall, TabICL consists of three transformers.}
    \label{fig:arch}
\end{figure*}

\subsection{Foundation Models and In-Context Learning}

In recent years, deep learning (DL) has been transformed by the emergence of foundation models, which are pretrained on massive, diverse datasets and serve as versatile backbones for downstream tasks. These transformer-based models enable In-Context Learning (ICL): performing tasks by analyzing prompts containing input-output pairs, without explicit training or parameter updates. ICL operates as a form of on-the-fly reasoning. The mechanism underlying ICL remains elusive, with prevailing explanations framing it as implicit Bayesian inference ____, gradient descent optimization ____, and algorithmic learning ____.

\subsection{Tabular Deep Learning Models}

Gradient-boosted decision trees (GBDTs), such as CatBoost and XGBoost ____, have long dominated the tabular domain. However, there are growing efforts to develop tabular DL models. Recent studies indicate a narrowing performance gap between GBDTs and tabular DL models ____.

As tabular DL improves, cross-table transferability emerges as an important topic. Notable efforts in this direction include XTab ____ and CARTE ____, which incorporate transferable components that are typically shareable backbone networks and dataset-specific components that require fine-tuning for each new task. The advent of tabular foundation models can bring new possibilities to cross-table learning, paving the way for large-scale pretraining and transfer learning across tables.

\subsection{TabPFN and its Offsprings}
\label{ss:tabpfn_offsprings}

TabPFN, short for Tabular Prior-Data Fitted Network, is a tabular foundation model. It is a transformer pretrained on extensive synthetic datasets to perform tabular classification tasks through ICL. TabPFN interprets ICL from a Bayesian perspective as an approximate posterior predictive distribution over synthetic datasets. Several variants aim to enhance its scalability, including distilling training data into a compact learned context via prompt tuning ____, selecting the most relevant subset of training data for each test sample ____, replacing quadratic with linear attention ____, and generating small task-specific neural networks via an ICL-based hypernetwork ____. However, most variants do not structurally improve TabPFN but instead act as prompt engineering to reduce in-context samples.

Other approaches try to improve the quality of pre-training data, such as TabForestPFN ____ incorporating tree-based synthetic datasets and TabDPT ____ curating and using real-world datasets.

Very recently (January 2025), TabPFNv2 was released and largely improved TabPFN in terms of both prediction performance and scalability. Our contributed model, TabICL, achieves comparable performance to TabPFNv2 while being more scalable and computationally efficient.