@inproceedings{abadiTensorflowSystemLargescale2016,
  title = {Tensorflow: {{A}} System for Large-Scale Machine Learning},
  booktitle = {12th \$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ Symposium on Operating Systems Design and Implementation (\$\{\$\vphantom\}{{OSDI}}\$\vphantom\{\}\$ 16)},
  author = {Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  year = {2016},
  pages = {265--283}
}

@article{acunaDomainAdversarialTraining2022,
  title = {Domain {{Adversarial Training}}: {{A Game Perspective}}},
  author = {Acuna, David and Law, Marc T and Zhang, Guojun and Fidler, Sanja},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.05352},
  eprint = {2202.05352},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000022},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Domain-invariant representation learning/DOMAIN ADVERSARIAL TRAINING A GAME PERSPECTIVE.pdf}
}

@misc{agarwalNeuralAdditiveModels2021,
  title = {Neural {{Additive Models}}: {{Interpretable Machine Learning}} with {{Neural Nets}}},
  shorttitle = {Neural {{Additive Models}}},
  author = {Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey},
  year = {2021},
  month = oct,
  number = {arXiv:2004.13912},
  eprint = {2004.13912},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.13912},
  urldate = {2024-10-04},
  abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/9D7FJ7LQ/Agarwal et al. - 2021 - Neural Additive Models Interpretable Machine Learning with Neural Nets.pdf;/Users/jingang/Zotero/storage/AYTWRUFH/2004.html}
}

@misc{agarwalNeuralAdditiveModels2021a,
  title = {Neural {{Additive Models}}: {{Interpretable Machine Learning}} with {{Neural Nets}}},
  shorttitle = {Neural {{Additive Models}}},
  author = {Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey},
  year = {2021},
  month = oct,
  number = {arXiv:2004.13912},
  eprint = {2004.13912},
  publisher = {arXiv},
  urldate = {2024-11-05},
  abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/R2N8UKV6/Agarwal et al. - 2021 - Neural Additive Models Interpretable Machine Learning with Neural Nets.pdf;/Users/jingang/Zotero/storage/B52Q5FRS/2004.html}
}

@misc{ahnTransformersLearnImplement2023,
  title = {Transformers Learn to Implement Preconditioned Gradient Descent for In-Context Learning},
  author = {Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  year = {2023},
  month = nov,
  number = {arXiv:2306.00297},
  eprint = {2306.00297},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.00297},
  urldate = {2024-08-30},
  abstract = {Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with \$L\$ attention layers, we prove certain critical points of the training objective implement \$L\$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/ALZRUT4I/Ahn et al. - 2023 - Transformers learn to implement preconditioned gradient descent for in-context learning.pdf;/Users/jingang/Zotero/storage/7WFJN8DJ/2306.html}
}

@article{ahujaInvariancePrincipleMeets2021,
  title = {Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization},
  author = {Ahuja, Kartik and Caballero, Ethan and Zhang, Dinghuai and {Gagnon-Audet}, Jean-Christophe and Bengio, Yoshua and Mitliagkas, Ioannis and Rish, Irina},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {3438--3450},
  annotation = {GSCC: 0000044}
}

@article{albertyUseLegendreTransforms2001,
  title = {Use of {{Legendre}} Transforms in Chemical Thermodynamics ({{IUPAC Technical Report}})},
  author = {Alberty, Robert A.},
  year = {2001},
  journal = {Pure and Applied Chemistry},
  volume = {73},
  number = {8},
  pages = {1349--1380},
  publisher = {De Gruyter},
  isbn = {1365-3075}
}

@article{altmannPermutationImportanceCorrected2010,
  title = {Permutation Importance: A Corrected Feature Importance Measure},
  author = {Altmann, Andr{\'e} and Tolo{\c s}i, Laura and Sander, Oliver and Lengauer, Thomas},
  year = {2010},
  journal = {Bioinformatics},
  volume = {26},
  number = {10},
  pages = {1340--1347},
  publisher = {Oxford University Press},
  isbn = {1460-2059}
}

@article{andersonIterativeProceduresNonlinear1965,
  title = {Iterative Procedures for Nonlinear Integral Equations},
  author = {Anderson, Donald G.},
  year = {1965},
  journal = {Journal of the ACM (JACM)},
  volume = {12},
  number = {4},
  pages = {547--560},
  publisher = {ACM New York, NY, USA},
  isbn = {0004-5411},
  annotation = {GSCC: 0001089}
}

@article{anderssonThermoCalcDICTRAComputational2002,
  title = {Thermo-{{Calc}} \& {{DICTRA}}, Computational Tools for Materials Science},
  author = {Andersson, Jan-Olof and Helander, Thomas and H{\"o}glund, Lars and Shi, Pingfang and Sundman, Bo},
  year = {2002},
  journal = {Calphad},
  volume = {26},
  number = {2},
  pages = {273--312},
  publisher = {Elsevier},
  isbn = {0364-5916}
}

@inproceedings{appleyardAcceleratingReservoirSimulators2011,
  title = {Accelerating Reservoir Simulators Using {{GPU}} Technology},
  booktitle = {{{SPE Reservoir Simulation Symposium}}},
  author = {Appleyard, John R and Appleyard, Jeremy D and Wakefield, Mark A and Desitter, Arnaud L},
  year = {2011},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Accelerating reservoir simulators using GPU technology.pdf}
}

@article{ariClusteredLinearRegression2002,
  title = {Clustered Linear Regression},
  author = {Ari, Bertan and G{\"u}venir, H. Altay},
  year = {2002},
  journal = {Knowledge-Based Systems},
  volume = {15},
  number = {3},
  pages = {169--175},
  publisher = {Elsevier},
  isbn = {0950-7051},
  file = {/Users/jingang/Dropbox/References/Regression clustering/Clustered linear regression.pdf}
}

@inproceedings{arikTabnetAttentiveInterpretable2021,
  title = {Tabnet: {{Attentive}} Interpretable Tabular Learning},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Arik, Sercan {\"O} and Pfister, Tomas},
  year = {2021},
  volume = {35},
  pages = {6679--6687},
  isbn = {2374-3468},
  file = {/Users/jingang/Dropbox/References/Tabular learning/TabNet- Attentive Interpretable Tabular Learning.pdf}
}

@article{arjovskyInvariantRiskMinimization2019,
  title = {Invariant Risk Minimization},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2019},
  journal = {arXiv preprint arXiv:1907.02893},
  eprint = {1907.02893},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000863}
}

@misc{arpitEnsembleAveragesImproving2022,
  title = {Ensemble of {{Averages}}: {{Improving Model Selection}} and {{Boosting Performance}} in {{Domain Generalization}}},
  shorttitle = {Ensemble of {{Averages}}},
  author = {Arpit, Devansh and Wang, Huan and Zhou, Yingbo and Xiong, Caiming},
  year = {2022},
  month = oct,
  number = {arXiv:2110.10832},
  eprint = {2110.10832},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.10832},
  urldate = {2024-08-03},
  abstract = {In Domain Generalization (DG) settings, models trained independently on a given set of training domains have notoriously chaotic performance on distribution shifted test domains, and stochasticity in optimization (e.g. seed) plays a big role. This makes deep learning models unreliable in real world settings. We first show that this chaotic behavior exists even along the training optimization trajectory of a single model, and propose a simple model averaging protocol that both significantly boosts domain generalization and diminishes the impact of stochasticity by improving the rank correlation between the in-domain validation accuracy and out-domain test accuracy, which is crucial for reliable early stopping. Taking advantage of our observation, we show that instead of ensembling unaveraged models (that is typical in practice), ensembling moving average models (EoA) from independent runs further boosts performance. We theoretically explain the boost in performance of ensembling and model averaging by adapting the well known Bias-Variance trade-off to the domain generalization setting. On the DomainBed benchmark, when using a pre-trained ResNet-50, this ensemble of averages achieves an average of \$68.0{\textbackslash}\%\$, beating vanilla ERM (w/o averaging/ensembling) by \${\textbackslash}sim 4{\textbackslash}\%\$, and when using a pre-trained RegNetY-16GF, achieves an average of \$76.6{\textbackslash}\%\$, beating vanilla ERM by \$6{\textbackslash}\%\$. Our code is available at https://github.com/salesforce/ensemble-of-averages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/JKTSZRYM/Arpit et al. - 2022 - Ensemble of Averages Improving Model Selection an.pdf;/Users/jingang/Zotero/storage/VXZHZYT5/2110.html}
}

@article{arpitEnsembleAveragesImproving2022a,
  title = {Ensemble of Averages: {{Improving}} Model Selection and Boosting Performance in Domain Generalization},
  author = {Arpit, Devansh and Wang, Huan and Zhou, Yingbo and Xiong, Caiming},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {8265--8277}
}

@article{avnimelechBoostedMixtureExperts1999,
  title = {Boosted Mixture of Experts: {{An}} Ensemble Learning Scheme},
  author = {Avnimelech, Ran and Intrator, Nathan},
  year = {1999},
  journal = {Neural computation},
  volume = {11},
  number = {2},
  pages = {483--497},
  publisher = {MIT Press},
  isbn = {0899-7667},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Boosted Mixture of Experts An Ensemble Learning Scheme.pdf}
}

@article{awadSupportVectorRegression2015,
  title = {Support Vector Regression},
  author = {Awad, Mariette and Khanna, Rahul and Awad, Mariette and Khanna, Rahul},
  year = {2015},
  journal = {Efficient learning machines: Theories, concepts, and applications for engineers and system designers},
  pages = {67--80},
  publisher = {Springer},
  isbn = {1430259892}
}

@misc{bahriSCARFSelfSupervisedContrastive2022,
  title = {{{SCARF}}: {{Self-Supervised Contrastive Learning}} Using {{Random Feature Corruption}}},
  shorttitle = {{{SCARF}}},
  author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  year = {2022},
  month = mar,
  number = {arXiv:2106.15147},
  eprint = {2106.15147},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.15147},
  urldate = {2024-08-12},
  abstract = {Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/WVG3X3VZ/Bahri et al. - 2022 - SCARF Self-Supervised Contrastive Learning using .pdf;/Users/jingang/Zotero/storage/8N5TI2JH/2106.html}
}

@article{baiTransformersStatisticiansProvable2023,
  title = {Transformers as {{Statisticians}}: {{Provable In-Context Learning}} with {{In-Context Algorithm Selection}}},
  shorttitle = {Transformers as {{Statisticians}}},
  author = {Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {57125--57211},
  urldate = {2024-08-30},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/S89HJJJ3/Bai et al. - 2023 - Transformers as Statisticians Provable In-Context Learning with In-Context Algorithm Selection.pdf}
}

@article{bakerGibbsEnergyAnalysis1982,
  title = {Gibbs Energy Analysis of Phase Equilibria},
  author = {Baker, Lee E. and Pierce, Alan C. and Luks, Kraemer D.},
  year = {1982},
  journal = {Society of Petroleum Engineers Journal},
  volume = {22},
  number = {05},
  pages = {731--742},
  publisher = {OnePetro},
  isbn = {0197-7520}
}

@article{balajiMetaregDomainGeneralization2018,
  title = {Metareg: {{Towards}} Domain Generalization Using Meta-Regularization},
  author = {Balaji, Yogesh and Sankaranarayanan, Swami and Chellappa, Rama},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  annotation = {GSCC: 0000397}
}

@article{balakrishnamaLinearDiscriminantAnalysisa1998,
  title = {Linear Discriminant Analysis-a Brief Tutorial},
  author = {Balakrishnama, Suresh and Ganapathiraju, Aravind},
  year = {1998},
  journal = {Institute for Signal and information Processing},
  volume = {18},
  number = {1998},
  pages = {1--8}
}

@misc{barberoWeGoWhat2024,
  title = {Round and {{Round We Go}}! {{What}} Makes {{Rotary Positional Encodings}} Useful?},
  author = {Barbero, Federico and Vitvitskyi, Alex and Perivolaropoulos, Christos and Pascanu, Razvan and Veli{\v c}kovi{\'c}, Petar},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06205},
  eprint = {2410.06205},
  publisher = {arXiv},
  urldate = {2024-10-10},
  abstract = {Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust "positional" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/TYMNMSZZ/Barbero et al. - 2024 - Round and Round We Go! What makes Rotary Positional Encodings useful.pdf;/Users/jingang/Zotero/storage/R98P46PN/2410.html}
}

@inproceedings{barronGeneralAdaptiveRobust2019,
  title = {A General and Adaptive Robust Loss Function},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Barron, Jonathan T.},
  year = {2019},
  pages = {4331--4339}
}

@inproceedings{beazleySWIGEasyUse1996,
  title = {{{SWIG}}: {{An Easy}} to {{Use Tool}} for {{Integrating Scripting Languages}} with {{C}} and {{C}}++.},
  booktitle = {Tcl/{{Tk Workshop}}},
  author = {Beazley, David M.},
  year = {1996},
  volume = {43},
  pages = {74}
}

@inproceedings{beeryRecognitionTerraIncognita2018,
  title = {Recognition in Terra Incognita},
  booktitle = {Proceedings of the {{European}} Conference on Computer Vision ({{ECCV}})},
  author = {Beery, Sara and Van Horn, Grant and Perona, Pietro},
  year = {2018},
  pages = {456--473}
}

@inproceedings{belghaziMutualInformationNeural2018,
  title = {Mutual Information Neural Estimation},
  booktitle = {International Conference on Machine Learning},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  year = {2018},
  pages = {531--540},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mutual Information Estimation/Mutual Information Neural Estimation.pdf}
}

@inproceedings{belkadiComparisonTwoMethods2011,
  title = {Comparison of Two Methods for Speeding up Flash Calculations in Compositional Simulations},
  booktitle = {{{SPE Reservoir Simulation Symposium}}},
  author = {Belkadi, Abdelkrim and Yan, Wei and Michelsen, Michael L and Stenby, Erling H},
  year = {2011},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Comparison of Two Methods for Speeding up Flash Calculations in Compositional Simulations.pdf}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias--Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  publisher = {National Acad Sciences},
  issn = {0027-8424}
}

@article{ben-davidTheoryLearningDifferent2010,
  title = {A Theory of Learning from Different Domains},
  author = {{Ben-David}, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  year = {2010},
  journal = {Machine learning},
  volume = {79},
  number = {1},
  pages = {151--175},
  publisher = {Springer},
  isbn = {1573-0565}
}

@article{bengioConditionalComputationNeural2015,
  title = {Conditional Computation in Neural Networks for Faster Models},
  author = {Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.06297},
  eprint = {1511.06297},
  archiveprefix = {arXiv}
}

@article{bengioConditionalComputationNeural2015a,
  title = {Conditional Computation in Neural Networks for Faster Models},
  author = {Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.06297},
  eprint = {1511.06297},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Conditional computation in neural networks for faster models.pdf}
}

@article{bengioMetatransferObjectiveLearning2019,
  title = {A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  year = {2019},
  journal = {arXiv preprint arXiv:1901.10912},
  eprint = {1901.10912},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Causality learning/A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms.pdf}
}

@article{bengioRepresentationLearningReview2013,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2013},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  number = {8},
  pages = {1798--1828},
  publisher = {IEEE},
  issn = {0162-8828},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Overview/Representation Learning A Review and New Perspectives.pdf}
}

@article{bergstraAlgorithmsHyperparameterOptimization2011,
  title = {Algorithms for Hyper-Parameter Optimization},
  author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  year = {2011},
  journal = {Advances in neural information processing systems},
  volume = {24}
}

@article{bergstraAlgorithmsHyperparameterOptimization2011a,
  title = {Algorithms for Hyper-Parameter Optimization},
  author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  year = {2011},
  journal = {Advances in neural information processing systems},
  volume = {24}
}

@inproceedings{bergstraMakingScienceModel2013,
  title = {Making a Science of Model Search: {{Hyperparameter}} Optimization in Hundreds of Dimensions for Vision Architectures},
  booktitle = {International Conference on Machine Learning},
  author = {Bergstra, James and Yamins, Daniel and Cox, David},
  year = {2013},
  pages = {115--123},
  publisher = {PMLR}
}

@article{bergstraRandomSearchHyperparameter2012,
  title = {Random Search for Hyper-Parameter Optimization.},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  journal = {Journal of machine learning research},
  volume = {13},
  number = {2},
  isbn = {1532-4435}
}

@article{bezanehtakVaporLiquidEquilibrium2002,
  title = {{{Vapor}}- {{Liquid}} Equilibrium for Binary Systems of Carbon Dioxide+ Methanol, Hydrogen+ Methanol, and Hydrogen+ Carbon Dioxide at High Pressures},
  author = {Bezanehtak, K and Combes, {\relax GB} and Dehghani, F and Foster, {\relax NR} and Tomasko, {\relax DL}},
  year = {2002},
  journal = {Journal of Chemical \& Engineering Data},
  volume = {47},
  number = {2},
  pages = {161--168},
  publisher = {ACS Publications},
  issn = {0021-9568}
}

@article{bezginJAXFluidsFullydifferentiableHighorder2023,
  title = {{{JAX-Fluids}}: {{A}} Fully-Differentiable High-Order Computational Fluid Dynamics Solver for Compressible Two-Phase Flows},
  author = {Bezgin, Deniz A. and Buhendwa, Aaron B. and Adams, Nikolaus A.},
  year = {2023},
  journal = {Computer Physics Communications},
  volume = {282},
  pages = {108527},
  publisher = {Elsevier},
  isbn = {0010-4655}
}

@misc{bhattamishraUnderstandingInContextLearning2023,
  title = {Understanding {{In-Context Learning}} in {{Transformers}} and {{LLMs}} by {{Learning}} to {{Learn Discrete Functions}}},
  author = {Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03016},
  eprint = {2310.03016},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03016},
  urldate = {2024-08-30},
  abstract = {In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a teaching sequence, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement two distinct algorithms to solve a single task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/EMX7EZES/Bhattamishra et al. - 2023 - Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions.pdf;/Users/jingang/Zotero/storage/VUSB8KH4/2310.html}
}

@article{blanchardDomainGeneralizationMarginal2021,
  title = {Domain Generalization by Marginal Transfer Learning},
  author = {Blanchard, Gilles and Deshmukh, Aniket Anand and Dogan, {\"U}run and Lee, Gyemin and Scott, Clayton},
  year = {2021},
  journal = {The Journal of Machine Learning Research},
  volume = {22},
  number = {1},
  pages = {46--100},
  publisher = {JMLRORG},
  isbn = {1532-4435},
  annotation = {GSCC: 0000093}
}

@article{blanchardDomainGeneralizationMarginal2021a,
  title = {Domain Generalization by Marginal Transfer Learning},
  author = {Blanchard, Gilles and Deshmukh, Aniket Anand and Dogan, {\"U}run and Lee, Gyemin and Scott, Clayton},
  year = {2021},
  journal = {The Journal of Machine Learning Research},
  volume = {22},
  number = {1},
  pages = {46--100},
  publisher = {JMLRORG},
  isbn = {1532-4435}
}

@article{blasPredictionBinaryTernary1998,
  title = {Prediction of Binary and Ternary Diagrams Using the Statistical Associating Fluid Theory ({{SAFT}}) Equation of State},
  author = {Blas, Felipe J. and Vega, Lourdes F.},
  year = {1998},
  journal = {Industrial \& engineering chemistry research},
  volume = {37},
  number = {2},
  pages = {660--674},
  publisher = {ACS Publications},
  isbn = {0888-5885}
}

@article{blumTraining3nodeNeural1992,
  title = {Training a 3-Node Neural Network Is {{NP-complete}}},
  author = {Blum, Avrim L. and Rivest, Ronald L.},
  year = {1992},
  journal = {Neural Networks},
  volume = {5},
  number = {1},
  pages = {117--127},
  publisher = {Elsevier},
  isbn = {0893-6080}
}

@misc{bommasaniOpportunitiesRisksFoundation2022,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Finn, Chelsea and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Ho, Daniel E. and Hsu, Kyle and Icard, Thomas and Jurafsky, Dan and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2108.07258},
  eprint = {2108.07258},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07258},
  urldate = {2025-01-05},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/VY6C78PY/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Models.pdf;/Users/jingang/Zotero/storage/3C3VJ36W/2108.html}
}

@inproceedings{bonnierRevisitingMultimodalTransformers2024,
  title = {Revisiting {{Multimodal Transformers}} for {{Tabular Data}} with {{Text Fields}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics ACL}} 2024},
  author = {Bonnier, Thomas},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {1481--1500},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand and virtual meeting},
  urldate = {2024-08-28},
  abstract = {Tabular data with text fields can be leveraged in applications such as financial risk assessment or medical diagnosis prediction. When employing multimodal approaches to make predictions based on these modalities, it is crucial to make the most appropriate modeling choices in terms of numerical feature encoding or fusion strategy. In this paper, we focus on multimodal classification tasks based on tabular datasets with text fields. We build on multimodal Transformers to propose the Tabular-Text Transformer (TTT), a tabular/text dual-stream Transformer network. This architecture includes a distance-to-quantile embedding scheme for numerical features and an overall attention module which concurrently considers self-attention and cross-modal attention. Further, we leverage the two well-informed modality streams to estimate whether a prediction is uncertain or not. To explain uncertainty in terms of feature values, we use a sampling-based approximation of Shapley values in a bimodal context, with two options for the value function. To show the efficacy and relevance of this approach, we compare it to six baselines and measure its ability to quantify and explain uncertainty against various methods. Our code is available at https://github.com/thomas-bonnier/TabularTextTransformer.},
  file = {/Users/jingang/Zotero/storage/34RQ85SC/Bonnier - 2024 - Revisiting Multimodal Transformers for Tabular Data with Text Fields.pdf}
}

@article{borisovDeepNeuralNetworks2024,
  title = {Deep {{Neural Networks}} and {{Tabular Data}}: {{A Survey}}},
  shorttitle = {Deep {{Neural Networks}} and {{Tabular Data}}},
  author = {Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2024},
  month = jun,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {35},
  number = {6},
  eprint = {2110.01889},
  primaryclass = {cs},
  pages = {7499--7519},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2022.3229161},
  urldate = {2024-09-01},
  abstract = {Heterogeneous tabular data are the most commonly used form of data and are essential for numerous critical and computationally demanding applications. On homogeneous data sets, deep neural networks have repeatedly shown excellent performance and have therefore been widely adopted. However, their adaptation to tabular data for inference or data generation tasks remains challenging. To facilitate further progress in the field, this work provides an overview of state-of-the-art deep learning methods for tabular data. We categorize these methods into three groups: data transformations, specialized architectures, and regularization models. For each of these groups, our work offers a comprehensive overview of the main approaches. Moreover, we discuss deep learning approaches for generating tabular data, and we also provide an overview over strategies for explaining deep models on tabular data. Thus, our first contribution is to address the main research streams and existing methodologies in the mentioned areas, while highlighting relevant challenges and open research questions. Our second contribution is to provide an empirical comparison of traditional machine learning methods with eleven deep learning approaches across five popular real-world tabular data sets of different sizes and with different learning objectives. Our results, which we have made publicly available as competitive benchmarks, indicate that algorithms based on gradient-boosted tree ensembles still mostly outperform deep learning models on supervised learning tasks, suggesting that the research progress on competitive deep learning models for tabular data is stagnating. To the best of our knowledge, this is the first in-depth overview of deep learning approaches for tabular data; as such, this work can serve as a valuable starting point to guide researchers and practitioners interested in deep learning with tabular data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/ES3UI8SC/Borisov et al. - 2024 - Deep Neural Networks and Tabular Data A Survey.pdf;/Users/jingang/Zotero/storage/3BUV6Z8G/2110.html}
}

@misc{bradburyJAXComposableTransformations2018,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@article{brahmaHypernetworksContinualSemiSupervised2021,
  title = {Hypernetworks for {{Continual Semi-Supervised Learning}}},
  author = {Brahma, Dhanajit and Verma, Vinay Kumar and Rai, Piyush},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.01856},
  eprint = {2110.01856},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Hypernetworks for Continual Semi-Supervised Learning.pdf}
}

@misc{breejenWhyInContextLearning2024,
  title = {Why {{In-Context Learning Transformers}} Are {{Tabular Data Classifiers}}},
  author = {den Breejen, Felix and Bae, Sangmin and Cha, Stephen and Yun, Se-Young},
  year = {2024},
  month = may,
  number = {arXiv:2405.13396},
  eprint = {2405.13396},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.13396},
  urldate = {2024-06-24},
  abstract = {The recently introduced TabPFN pretrains an In-Context Learning (ICL) transformer on synthetic data to perform tabular data classification. As synthetic data does not share features or labels with real-world data, the underlying mechanism that contributes to the success of this method remains unclear. This study provides an explanation by demonstrating that ICL-transformers acquire the ability to create complex decision boundaries during pretraining. To validate our claim, we develop a novel forest dataset generator which creates datasets that are unrealistic, but have complex decision boundaries. Our experiments confirm the effectiveness of ICL-transformers pretrained on this data. Furthermore, we create TabForestPFN, the ICL-transformer pretrained on both the original TabPFN synthetic dataset generator and our forest dataset generator. By fine-tuning this model, we reach the current state-of-the-art on tabular data classification. Code is available at https://github.com/FelixdenBreejen/TabForestPFN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/MJ739C5E/Breejen et al. - 2024 - Why In-Context Learning Transformers are Tabular D.pdf;/Users/jingang/Zotero/storage/AANAUKKX/2405.html}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {2017},
  publisher = {Routledge},
  isbn = {1-315-13947-2}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  publisher = {Springer},
  isbn = {1573-0565}
}

@article{brockSmashOneshotModel2017,
  title = {Smash: One-Shot Model Architecture Search through Hypernetworks},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, James M. and Weston, Nick},
  year = {2017},
  journal = {arXiv preprint arXiv:1708.05344},
  eprint = {1708.05344},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Smash one-shot model architecture search through hypernetworks.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2024-08-30},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/RVTE33EF/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/jingang/Zotero/storage/2HGK39NJ/2005.html}
}

@inproceedings{carlucciDomainGeneralizationSolving2019,
  title = {Domain Generalization by Solving Jigsaw Puzzles},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Carlucci, Fabio M. and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
  year = {2019},
  pages = {2229--2238},
  annotation = {GSCC: 0000508}
}

@misc{cartellaAdversarialAttacksTabular2021,
  title = {Adversarial {{Attacks}} for {{Tabular Data}}: {{Application}} to {{Fraud Detection}} and {{Imbalanced Data}}},
  shorttitle = {Adversarial {{Attacks}} for {{Tabular Data}}},
  author = {Cartella, Francesco and Anunciacao, Orlando and Funabiki, Yuki and Yamaguchi, Daisuke and Akishita, Toru and Elshocht, Olivier},
  year = {2021},
  month = jan,
  number = {arXiv:2101.08030},
  eprint = {2101.08030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.08030},
  urldate = {2025-01-05},
  abstract = {Guaranteeing the security of transactional systems is a crucial priority of all institutions that process transactions, in order to protect their businesses against cyberattacks and fraudulent attempts. Adversarial attacks are novel techniques that, other than being proven to be effective to fool image classification models, can also be applied to tabular data. Adversarial attacks aim at producing adversarial examples, in other words, slightly modified inputs that induce the Artificial Intelligence (AI) system to return incorrect outputs that are advantageous for the attacker. In this paper we illustrate a novel approach to modify and adapt state-of-the-art algorithms to imbalanced tabular data, in the context of fraud detection. Experimental results show that the proposed modifications lead to a perfect attack success rate, obtaining adversarial examples that are also less perceptible when analyzed by humans. Moreover, when applied to a real-world production system, the proposed techniques shows the possibility of posing a serious threat to the robustness of advanced AI-based fraud detection procedures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/ZYFLEZ3B/Cartella et al. - 2021 - Adversarial Attacks for Tabular Data Application to Fraud Detection and Imbalanced Data.pdf;/Users/jingang/Zotero/storage/NDP3HY3R/2101.html}
}

@article{caruanaMultitaskLearning1997,
  title = {Multitask Learning},
  author = {Caruana, Rich},
  year = {1997},
  journal = {Machine learning},
  volume = {28},
  number = {1},
  pages = {41--75},
  publisher = {Springer},
  isbn = {1573-0565}
}

@misc{carusoNotAnotherImputation2024,
  title = {Not {{Another Imputation Method}}: {{A Transformer-based Model}} for {{Missing Values}} in {{Tabular Datasets}}},
  shorttitle = {Not {{Another Imputation Method}}},
  author = {Caruso, Camillo Maria and Soda, Paolo and Guarrasi, Valerio},
  year = {2024},
  month = jul,
  number = {arXiv:2407.11540},
  eprint = {2407.11540},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-17},
  abstract = {Handling missing values in tabular datasets presents a significant challenge in training and testing artificial intelligence models, an issue usually addressed using imputation techniques. Here we introduce "Not Another Imputation Method" (NAIM), a novel transformer-based model specifically designed to address this issue without the need for traditional imputation techniques. NAIM employs feature-specific embeddings and a masked self-attention mechanism that effectively learns from available data, thus avoiding the necessity to impute missing values. Additionally, a novel regularization technique is introduced to enhance the model's generalization capability from incomplete data. We extensively evaluated NAIM on 5 publicly available tabular datasets, demonstrating its superior performance over 6 state-of-the-art machine learning models and 4 deep learning models, each paired with 3 different imputation techniques when necessary. The results highlight the efficacy of NAIM in improving predictive performance and resilience in the presence of missing data. To facilitate further research and practical application in handling missing data without traditional imputation methods, we made the code for NAIM available at https://github.com/cosbidev/NAIM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/6ZAQFYBC/Caruso et al. - 2024 - Not Another Imputation Method A Transformer-based.pdf;/Users/jingang/Zotero/storage/GTE6RHB3/2407.html}
}

@inproceedings{chaDomainGeneralizationMutualinformation2022,
  title = {Domain Generalization by Mutual-Information Regularization with Pre-Trained Models},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Cha, Junbum and Lee, Kyungjae and Park, Sungrae and Chun, Sanghyuk},
  year = {2022},
  pages = {440--457},
  publisher = {Springer}
}

@article{chaikinAlgorithmHighspeedCurve1974,
  title = {An Algorithm for High-Speed Curve Generation},
  author = {Chaikin, George Merrill},
  year = {1974},
  journal = {Computer graphics and image processing},
  volume = {3},
  number = {4},
  pages = {346--349},
  publisher = {Elsevier},
  issn = {0146-664X}
}

@article{chanDataDistributionalProperties2022,
  title = {Data {{Distributional Properties Drive Emergent In-Context Learning}} in {{Transformers}}},
  author = {Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {18878--18891},
  urldate = {2024-08-30},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/875M6W5C/Chan et al. - 2022 - Data Distributional Properties Drive Emergent In-Context Learning in Transformers.pdf}
}

@inproceedings{changDATADomainAwareTaskAware2022,
  title = {{{DATA}}: {{Domain-Aware}} and {{Task-Aware Self-Supervised Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chang, Qing and Peng, Junran and Xie, Lingxi and Sun, Jiajun and Yin, Haoran and Tian, Qi and Zhang, Zhaoxiang},
  year = {2022},
  pages = {9841--9850},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/DATA Domain-Aware and Task-Aware Self-supervised Learning.pdf}
}

@article{changNewApparatusDetermination1998,
  title = {A New Apparatus for the Determination of {{P}}--x--y Diagrams and {{Henry}}'s Constants in High Pressure Alcohols with Critical Carbon Dioxide},
  author = {Chang, Chiehming J and Chiu, Kou-Lung and Day, Chang-Yih},
  year = {1998},
  journal = {The Journal of supercritical fluids},
  volume = {12},
  number = {3},
  pages = {223--237},
  publisher = {Elsevier},
  issn = {0896-8446}
}

@inproceedings{changPrincipledWeightInitialization2019,
  title = {Principled Weight Initialization for Hypernetworks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chang, Oscar and Flokas, Lampros and Lipson, Hod},
  year = {2019},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Principled weight initialization for hypernetworks.pdf}
}

@article{chapelleVicinalRiskMinimization2000,
  title = {Vicinal Risk Minimization},
  author = {Chapelle, Olivier and Weston, Jason and Bottou, L{\'e}on and Vapnik, Vladimir},
  year = {2000},
  journal = {Advances in neural information processing systems},
  volume = {13}
}

@article{chapmanNewReferenceEquation1990,
  title = {New Reference Equation of State for Associating Liquids},
  author = {Chapman, Walter G and Gubbins, Keith E and Jackson, George and Radosz, Maciej},
  year = {1990},
  journal = {Industrial \& engineering chemistry research},
  volume = {29},
  number = {8},
  pages = {1709--1721},
  publisher = {ACS Publications}
}

@article{chapmanSAFTEquationofstateSolution1989,
  title = {{{SAFT}}: {{Equation-of-state}} Solution Model for Associating Fluids},
  author = {Chapman, Walter G and Gubbins, Keith E and Jackson, George and Radosz, Maciej},
  year = {1989},
  journal = {Fluid Phase Equilibria},
  volume = {52},
  pages = {31--38},
  publisher = {Elsevier}
}

@misc{chaSWADDomainGeneralization2021,
  title = {{{SWAD}}: {{Domain Generalization}} by {{Seeking Flat Minima}}},
  shorttitle = {{{SWAD}}},
  author = {Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
  year = {2021},
  month = nov,
  number = {arXiv:2102.08604},
  eprint = {2102.08604},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.08604},
  urldate = {2024-08-03},
  abstract = {Domain generalization (DG) methods aim to achieve generalizability to an unseen target domain by using only training data from the source domains. Although a variety of DG methods have been proposed, a recent study shows that under a fair evaluation protocol, called DomainBed, the simple empirical risk minimization (ERM) approach works comparable to or even outperforms previous methods. Unfortunately, simply solving ERM on a complex, non-convex loss function can easily lead to sub-optimal generalizability by seeking sharp minima. In this paper, we theoretically show that finding flat minima results in a smaller domain generalization gap. We also propose a simple yet effective method, named Stochastic Weight Averaging Densely (SWAD), to find flat minima. SWAD finds flatter minima and suffers less from overfitting than does the vanilla SWA by a dense and overfit-aware stochastic weight sampling strategy. SWAD shows state-of-the-art performances on five DG benchmarks, namely PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large margins of +1.6\% averagely on out-of-domain accuracy. We also compare SWAD with conventional generalization methods, such as data augmentation and consistency regularization methods, to verify that the remarkable performance improvements are originated from by seeking flat minima, not from better in-domain generalizability. Last but not least, SWAD is readily adaptable to existing DG methods without modification; the combination of SWAD and an existing DG method further improves DG performances. Source code is available at https://github.com/khanrc/swad.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/VKYGXU23/Cha et al. - 2021 - SWAD Domain Generalization by Seeking Flat Minima.pdf;/Users/jingang/Zotero/storage/42V87HWZ/2102.html}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: Synthetic Minority over-Sampling Technique},
  author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
  year = {2002},
  journal = {Journal of artificial intelligence research},
  volume = {16},
  pages = {321--357},
  isbn = {1076-9757}
}

@inproceedings{chenCompoundDomainGeneralization2022,
  title = {Compound {{Domain Generalization}} via {{Meta-Knowledge Encoding}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Chaoqi and Li, Jiongcheng and Han, Xiaoguang and Liu, Xiaoqing and Yu, Yizhou},
  year = {2022},
  pages = {7119--7129},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/Domain2Vec/Compound Domain Generalization via Meta-Knowledge Encoding.pdf}
}

@misc{chenCrossTablePretrainingUniversal2024,
  title = {Cross-{{Table Pretraining}} towards a {{Universal Function Space}} for {{Heterogeneous Tabular Data}}},
  author = {Chen, Jintai and Lin, Zhen and Chen, Qiyuan and Sun, Jimeng},
  year = {2024},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-10-08},
  abstract = {Tabular data from different tables exhibit significant diversity due to varied definitions and types of features, as well as complex inter-feature and feature-target relationships. Cross-dataset pretraining, which learns reusable patterns from upstream data to support downstream tasks, have shown notable success in various fields. Yet, when applied to tabular data prediction, this paradigm faces challenges due to the limited reusable patterns among diverse tabular datasets (tables) and the general scarcity of tabular data available for fine-tuning. In this study, we fill this gap by introducing a cross-table pretrained Transformer, XTFormer, for versatile downstream tabular prediction tasks. Our methodology insight is pretraining XTFormer to establish a "meta-function" space that encompasses all potential feature-target mappings. In pre-training, a variety of potential mappings are extracted from pre-training tabular datasets and are embedded into the "meta-function" space, and suited mappings are extracted from the "meta-function" space for downstream tasks by a specified coordinate positioning approach. Experiments show that, in 190 downstream tabular prediction tasks, our cross-table pretrained XTFormer wins both XGBoost and Catboost on 137 (72\%) tasks, and surpasses representative deep learning models FT-Transformer and the tabular pre-training approach XTab on 144 (76\%) and 162 (85\%) tasks.},
  howpublished = {https://arxiv.org/abs/2406.00281v1},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/LTIH3C44/Chen et al. - 2024 - Cross-Table Pretraining towards a Universal Function Space for Heterogeneous Tabular Data.pdf}
}

@misc{chenExcelFormerNeuralNetwork2024,
  title = {{{ExcelFormer}}: {{A}} Neural Network Surpassing {{GBDTs}} on Tabular Data},
  shorttitle = {{{ExcelFormer}}},
  author = {Chen, Jintai and Yan, Jiahuan and Chen, Qiyuan and Chen, Danny Ziyi and Wu, Jian and Sun, Jimeng},
  year = {2024},
  month = jul,
  number = {arXiv:2301.02819},
  eprint = {2301.02819},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.02819},
  urldate = {2024-12-17},
  abstract = {Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a "sure bet" solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs' rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a "sure bet" solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/5I327C97/Chen et al. - 2024 - ExcelFormer A neural network surpassing GBDTs on tabular data.pdf;/Users/jingang/Zotero/storage/YGZ5RH5G/2301.html}
}

@inproceedings{chenExploringSimpleSiamese2021,
  title = {Exploring Simple Siamese Representation Learning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Xinlei and He, Kaiming},
  year = {2021},
  pages = {15750--15758},
  file = {/Users/jingang/Dropbox/References/Contrastive learning/Exploring Simple Siamese Representation Learning.pdf}
}

@misc{chenExtendingContextWindow2023,
  title = {Extending {{Context Window}} of {{Large Language Models}} via {{Positional Interpolation}}},
  author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15595},
  eprint = {2306.15595},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.15595},
  urldate = {2024-08-28},
  abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/9PV9IFC7/Chen et al. - 2023 - Extending Context Window of Large Language Models via Positional Interpolation.pdf;/Users/jingang/Zotero/storage/2UXN9K3J/2306.html}
}

@misc{chengArithmeticFeatureInteraction2024,
  title = {Arithmetic {{Feature Interaction Is Necessary}} for {{Deep Tabular Learning}}},
  author = {Cheng, Yi and Hu, Renjun and Ying, Haochao and Shi, Xing and Wu, Jian and Lin, Wei},
  year = {2024},
  month = mar,
  number = {arXiv:2402.02334},
  eprint = {2402.02334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.02334},
  urldate = {2024-09-10},
  abstract = {Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive bias for deep learning on tabular data. Code is available at https://github.com/aigc-apps/AMFormer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.4},
  file = {/Users/jingang/Zotero/storage/M3UKQT7W/Cheng et al. - 2024 - Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning.pdf;/Users/jingang/Zotero/storage/K2LENCEJ/2402.html}
}

@inproceedings{chengClubContrastiveLogratio2020,
  title = {Club: {{A}} Contrastive Log-Ratio Upper Bound of Mutual Information},
  booktitle = {International Conference on Machine Learning},
  author = {Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
  year = {2020},
  pages = {1779--1788},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mutual Information Estimation/CLUB A Contrastive Log-ratio Upper Bound of Mutual Information.pdf}
}

@article{chengDeterminingEquilibriumPartitioning2004,
  title = {Determining the Equilibrium Partitioning Coefficients of Volatile Organic Compounds at an Air--Water Interface},
  author = {Cheng, Wen-Hsi and Chou, Ming-Shean and Perng, Chih-Hao and Chu, Fu-Sui},
  year = {2004},
  journal = {Chemosphere},
  volume = {54},
  number = {7},
  pages = {935--942},
  publisher = {Elsevier},
  isbn = {0045-6535}
}

@incollection{chenGPUbasedParallelReservoir2014,
  title = {{{GPU-based}} Parallel Reservoir Simulators},
  booktitle = {Domain {{Decomposition Methods}} in {{Science}} and {{Engineering XXI}}},
  author = {Chen, Zhangxin and Liu, Hui and Yu, Song and Hsieh, Ben and Shao, Lei},
  year = {2014},
  pages = {199--206},
  publisher = {Springer},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/GPU-based Parallel Reservoir Simulators.pdf}
}

@article{chengRegressionClusteringImproved2019,
  title = {Regression Clustering for Improved Accuracy and Training Costs with Molecular-Orbital-Based Machine Learning},
  author = {Cheng, Lixue and Kovachki, Nikola B. and Welborn, Matthew and Miller III, Thomas F.},
  year = {2019},
  journal = {Journal of chemical theory and computation},
  volume = {15},
  number = {12},
  pages = {6668--6677},
  publisher = {ACS Publications},
  isbn = {1549-9618},
  file = {/Users/jingang/Dropbox/References/Regression clustering/Regression clustering for improved accuracy and training costs with molecular-orbital-based machine learning.pdf}
}

@inproceedings{chengWideDeepLearning2016,
  title = {Wide \& Deep Learning for Recommender Systems},
  booktitle = {Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
  author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  year = {2016},
  pages = {7--10}
}

@article{chenHYTRELHypergraphenhancedTabular2024,
  title = {{{HYTREL}}: {{Hypergraph-enhanced}} Tabular Data Representation Learning},
  shorttitle = {{{HYTREL}}},
  author = {Chen, Pei and Sarkar, Soumajyoti and Lausen, Leonard and Srinivasan, Balasubramaniam and Zha, Sheng and Huang, Ruihong and Karypis, George},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  urldate = {2024-06-04},
  file = {/Users/jingang/Zotero/storage/YXGMSFA6/Chen et al. - 2024 - HYTREL Hypergraph-enhanced tabular data represent.pdf}
}

@article{chenLogHyperbolicCosine2018,
  title = {Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},
  author = {Chen, Pengfei and Chen, Guangyong and Zhang, Shengyu},
  year = {2018},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Improvements/LOG HYPERBOLIC COSINE LOSS IMPROVES VARIATIONAL AUTO-ENCODER.pdf}
}

@article{chenNeuralOrdinaryDifferential2018,
  title = {Neural Ordinary Differential Equations},
  author = {Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31}
}

@inproceedings{chenSimpleFrameworkContrastive2020,
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  booktitle = {International Conference on Machine Learning},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  pages = {1597--1607},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Contrastive learning/A Simple Framework for Contrastive Learning of Visual Representations.pdf}
}

@misc{chenTromptBetterDeep2023,
  title = {Trompt: {{Towards}} a {{Better Deep Neural Network}} for {{Tabular Data}}},
  shorttitle = {Trompt},
  author = {Chen, Kuan-Yu and Chiang, Ping-Han and Chou, Hsin-Rung and Chen, Ting-Wei and Chang, Tien-Hao},
  year = {2023},
  month = may,
  number = {arXiv:2305.18446},
  eprint = {2305.18446},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The experimental results demonstrate that Trompt outperforms state-of-the-art deep neural networks and is comparable to tree-based models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/2VTJYNDF/Chen et al. - 2023 - Trompt Towards a Better Deep Neural Network for T.pdf;/Users/jingang/Zotero/storage/8HPN72J5/2305.html}
}

@article{chenUniversalApproximationNonlinear1995,
  title = {Universal Approximation to Nonlinear Operators by Neural Networks with Arbitrary Activation Functions and Its Application to Dynamical Systems},
  author = {Chen, Tianping and Chen, Hong},
  year = {1995},
  journal = {IEEE Transactions on Neural Networks},
  volume = {6},
  number = {4},
  pages = {911--917},
  publisher = {IEEE},
  isbn = {1045-9227}
}

@inproceedings{chenXgboostScalableTree2016,
  title = {Xgboost: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  pages = {785--794}
}

@inproceedings{chenXGBoostScalableTree2016a,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2025-01-05},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/MPZLUH6G/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;/Users/jingang/Zotero/storage/AQF8E9SF/1603.html}
}

@article{chevalleyInvariantCausalMechanisms2022,
  title = {Invariant {{Causal Mechanisms}} through {{Distribution Matching}}},
  author = {Chevalley, Mathieu and Bunne, Charlotte and Krause, Andreas and Bauer, Stefan},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.11646},
  eprint = {2206.11646},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Causality learning/Invariant Causal Mechanisms through Distribution Matching.pdf}
}

@article{chevalleyInvariantCausalMechanisms2022a,
  title = {Invariant Causal Mechanisms through Distribution Matching},
  author = {Chevalley, Mathieu and Bunne, Charlotte and Krause, Andreas and Bauer, Stefan},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.11646},
  eprint = {2206.11646},
  archiveprefix = {arXiv}
}

@article{chiRepresentationCollapseSparse2022,
  title = {On the {{Representation Collapse}} of {{Sparse Mixture}} of {{Experts}}},
  author = {Chi, Zewen and Dong, Li and Huang, Shaohan and Dai, Damai and Ma, Shuming and Patra, Barun and Singhal, Saksham and Bajaj, Payal and Song, Xia and Wei, Furu},
  year = {2022},
  journal = {arXiv preprint arXiv:2204.09179},
  eprint = {2204.09179},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Multi-Source Domain Adaptation with Mixture of Experts.pdf}
}

@inproceedings{choiRobustnetImprovingDomain2021,
  title = {Robustnet: {{Improving}} Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Choi, Sungha and Jung, Sanghun and Yun, Huiwon and Kim, Joanne T and Kim, Seungryong and Choo, Jaegul},
  year = {2021},
  pages = {11580--11590},
  annotation = {GSCC: 0000059},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/VAE/Robustnet Improving domain generalization in urban-scene segmentation via instance selective whitening.pdf}
}

@article{choLearningPhraseRepresentations2014,
  title = {Learning Phrase Representations Using {{RNN}} Encoder-Decoder for Statistical Machine Translation},
  author = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  journal = {arXiv preprint arXiv:1406.1078},
  eprint = {1406.1078},
  archiveprefix = {arXiv}
}

@inproceedings{clarkUnifiedScalingLaws2022,
  title = {Unified Scaling Laws for Routed Language Models},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Clark, Aidan and {de Las Casas}, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian},
  year = {2022},
  pages = {4057--4086},
  publisher = {PMLR},
  isbn = {2640-3498}
}

@article{clevertFastAccurateDeep2015,
  title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus)},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.07289},
  eprint = {1511.07289},
  archiveprefix = {arXiv}
}

@article{collinsEfficientApproachAdaptive1992,
  title = {An {{Efficient Approach}} to {{Adaptive- Implicit Compositional Simulation With}} an {{Equation}} of {{State}}},
  author = {Collins, D.A. and Nghiem, L.X. and Li, Y-K. and Grabonstotter, J.E.},
  year = {1992},
  month = may,
  journal = {SPE Reservoir Engineering},
  volume = {7},
  number = {02},
  pages = {259--264},
  issn = {0885-9248, 2469-9683},
  doi = {10.2118/15133-PA},
  urldate = {2021-11-25},
  abstract = {This paper describes a robust and efficient method for solving the equations corresponding to an equation-of-state (EOS) compositional model. The method is developed for an adaptive-implicit simulator in which only a small number of blocks need to be solved implicitly while the remaining blocks are solved explicitly. The salient features of the method are decoupling of the solution of flow equations from the flash calculations and switching from explicit to implicit formulations on the basis of a stability criterion. The performance of the adaptive-implicit approach is compared with that of a fully implicit approach and an explicit-transmissibility approach. Results show that the adaptive-implicit approach is two to three times faster than the other two methods.},
  langid = {english},
  file = {/Users/jingang/Dropbox/References/Compositional reservoir simulation/An Efficient Approach to AdaptiveImplicit Compositional Simulation_With an Equation of State.pdf}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  journal = {Machine learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  publisher = {Springer}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, George},
  year = {1989},
  journal = {Mathematics of control, signals and systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  publisher = {Springer}
}

@article{daiStableMoEStableRouting2022,
  title = {{{StableMoE}}: {{Stable}} Routing Strategy for Mixture of Experts},
  author = {Dai, Damai and Dong, Li and Ma, Shuming and Zheng, Bo and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  year = {2022},
  journal = {arXiv preprint arXiv:2204.08396},
  eprint = {2204.08396},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Stable Routing Strategy for Mixture of Experts.pdf}
}

@book{dakeFundamentalsReservoirEngineering1983,
  title = {Fundamentals of Reservoir Engineering},
  author = {Dake, Laurence Patrick},
  year = {1983},
  publisher = {Elsevier},
  isbn = {0-08-056898-X}
}

@misc{daoFlashAttention2FasterAttention2023,
  title = {{{FlashAttention-2}}: {{Faster Attention}} with {{Better Parallelism}} and {{Work Partitioning}}},
  shorttitle = {{{FlashAttention-2}}},
  author = {Dao, Tri},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08691},
  eprint = {2307.08691},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08691},
  urldate = {2025-01-05},
  abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\${\textbackslash}times\$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40{\textbackslash}\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\${\textbackslash}times\$ speedup compared to FlashAttention, reaching 50-73{\textbackslash}\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72{\textbackslash}\% model FLOPs utilization).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/9W9KE6WR/Dao - 2023 - FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning.pdf;/Users/jingang/Zotero/storage/Z98M2X7M/2307.html}
}

@misc{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14135},
  eprint = {2205.14135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14135},
  urldate = {2025-01-05},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/YK4A4FV2/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf;/Users/jingang/Zotero/storage/BU4F78QG/2205.html}
}

@article{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  journal = {Advances in neural information processing systems},
  volume = {27}
}

@article{daviesAdvancingMathematicsGuiding2021,
  title = {Advancing Mathematics by Guiding Human Intuition with {{AI}}},
  author = {Davies, Alex and Veli{\v c}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v s}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and others},
  year = {2021},
  journal = {Nature},
  volume = {600},
  number = {7887},
  pages = {70--74},
  publisher = {Nature Publishing Group},
  file = {/Users/jingang/Dropbox/References/Deep learning/Advancing mathematics by guiding human.pdf}
}

@article{dehemptinneCarnotThermodynamicLibrary2023,
  title = {Carnot: A Thermodynamic Library for Energy Industries},
  author = {{de Hemptinne}, Jean-Charles and Ferrando, Nicolas and {Hajiw-Riberaud}, Martha and Lachet, V{\'e}ronique and Maghsoodloo, Saheb and Mougin, Pascal and Ngo, Tri Dat and Pigeon, Laurent and Yanes, Jose Romero and Wender, Aur{\'e}lie},
  year = {2023},
  journal = {Science and Technology for Energy Transition},
  volume = {78},
  pages = {30}
}

@book{dehemptinneSelectThermodynamicModels2012,
  title = {Select Thermodynamic Models for Process Simulation: {{A}} Practical Guide Using a Three Steps Methodology},
  author = {De Hemptinne, Jean-Charles and Ledanois, Jean-Marie},
  year = {2012},
  publisher = {Editions Technip},
  isbn = {2-7108-0949-4}
}

@article{deitersCalculationDensitiesCubic2014,
  title = {Calculation of Densities from Cubic Equations of State: Revisited},
  author = {Deiters, Ulrich K and {Mac{\'i}as-Salinas}, Ricardo},
  year = {2014},
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {53},
  number = {6},
  pages = {2529--2536},
  publisher = {ACS Publications}
}

@article{dejongPatientreportedFactorsInfluencing2022,
  title = {Patient-Reported Factors Influencing the Choice of Their Kidney Replacement Treatment Modality},
  author = {{de Jong}, Rianne W. and Stel, Vianda S. and Rahmel, Axel and Murphy, Mark and Vanholder, Raymond C. and Massy, Ziad A. and Jager, Kitty J.},
  year = {2022},
  journal = {Nephrology Dialysis Transplantation},
  volume = {37},
  number = {3},
  pages = {477--488},
  publisher = {Oxford University Press},
  isbn = {0931-0509},
  file = {/Users/jingang/Dropbox/References/AVF/Patient-reported factors influencing the choice of their kidney.pdf}
}

@inproceedings{dengImagenetLargescaleHierarchical2009,
  title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  pages = {248--255},
  publisher = {Ieee},
  isbn = {1-4244-3992-2}
}

@article{desarboMaximumLikelihoodMethodology1988,
  title = {A Maximum Likelihood Methodology for Clusterwise Linear Regression},
  author = {DeSarbo, Wayne S. and Cron, William L.},
  year = {1988},
  journal = {Journal of classification},
  volume = {5},
  number = {2},
  pages = {249--282},
  publisher = {Springer},
  isbn = {1432-1343},
  file = {/Users/jingang/Dropbox/References/Regression clustering/A maximum likelihood methodology for clusterwise linear regression.pdf}
}

@article{deshmukhDomain2vecDeepDomain2018,
  title = {Domain2vec: {{Deep}} Domain Generalization},
  author = {Deshmukh, Aniket Anand and Bansal, Ankit and Rastogi, Akash},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.02919},
  eprint = {1807.02919},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000008},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/Domain2Vec/Domain2Vec Deep Domain Generalization.pdf}
}

@article{deshmukhGeneralizationErrorBound2019,
  title = {A Generalization Error Bound for Multi-Class Domain Generalization},
  author = {Deshmukh, Aniket Anand and Lei, Yunwen and Sharma, Srinagesh and Dogan, Urun and Cutler, James W. and Scott, Clayton},
  year = {2019},
  journal = {arXiv preprint arXiv:1905.10392},
  eprint = {1905.10392},
  archiveprefix = {arXiv}
}

@misc{deutchIncontextLearningGradient2024,
  title = {In-Context {{Learning}} and {{Gradient Descent Revisited}}},
  author = {Deutch, Gilad and Magar, Nadav and Natan, Tomer Bar and Dar, Guy},
  year = {2024},
  month = mar,
  number = {arXiv:2311.07772},
  eprint = {2311.07772},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.07772},
  urldate = {2024-08-30},
  abstract = {In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significantly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/7MIPE3DF/Deutch et al. - 2024 - In-context Learning and Gradient Descent Revisited.pdf;/Users/jingang/Zotero/storage/J6P4XLWZ/2311.html}
}

@inproceedings{dietterichEnsembleMethodsMachine2000,
  title = {Ensemble Methods in Machine Learning},
  booktitle = {International Workshop on Multiple Classifier Systems},
  author = {Dietterich, Thomas G.},
  year = {2000},
  pages = {1--15},
  publisher = {Springer}
}

@article{diltsConsistentThermodynamicDerivative2006,
  title = {Consistent Thermodynamic Derivative Estimates for Tabular Equations of State},
  author = {Dilts, Gary A.},
  year = {2006},
  journal = {Physical Review E},
  volume = {73},
  number = {6},
  pages = {066704},
  publisher = {APS},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/ML EoS/Consistent thermodynamic derivative estimates for tabular equations of state.pdf}
}

@article{dingDeepDomainGeneralization2017,
  title = {Deep Domain Generalization with Structured Low-Rank Constraint},
  author = {Ding, Zhengming and Fu, Yun},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {304--313},
  publisher = {IEEE},
  isbn = {1057-7149},
  annotation = {GSCC: 0000088}
}

@article{dingDomainGeneralizationLearning2022,
  title = {Domain Generalization by Learning and Removing Domain-Specific Features},
  author = {Ding, Yu and Wang, Lei and Liang, Bin and Liang, Shuming and Wang, Yang and Chen, Fang},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24226--24239}
}

@article{dinhDensityEstimationUsing2016,
  title = {Density Estimation Using Real Nvp},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2016},
  journal = {arXiv preprint arXiv:1605.08803},
  eprint = {1605.08803},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Normalizing Flow/density estimation using real nvp.pdf}
}

@inproceedings{dinnocenteDomainGeneralizationDomainspecific2018,
  title = {Domain Generalization with Domain-Specific Aggregation Modules},
  booktitle = {German {{Conference}} on {{Pattern Recognition}}},
  author = {D'Innocente, Antonio and Caputo, Barbara},
  year = {2018},
  pages = {187--198},
  publisher = {Springer},
  annotation = {GSCC: 0000096},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Ensemble methods/Domain generalization with domain-specific aggregation modules.pdf}
}

@inproceedings{dogruNextgenerationParallelReservoir2009,
  title = {A Next-Generation Parallel Reservoir Simulator for Giant Reservoirs},
  booktitle = {{{SPE Reservoir Simulation Symposium}}},
  author = {Dogru, Ali H and Fung, Larry Siu Kuen and Middya, Usuf and {Al-Shaalan}, Tareq and Pita, Jorge Alberto},
  year = {2009},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/A next generation of reservoir simulation.pdf}
}

@misc{dongSurveyIncontextLearning2024,
  title = {A {{Survey}} on {{In-context Learning}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Li, Lei and Sui, Zhifang},
  year = {2024},
  month = jun,
  number = {arXiv:2301.00234},
  eprint = {2301.00234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00234},
  urldate = {2024-08-30},
  abstract = {With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/RRBKDLZM/Dong et al. - 2024 - A Survey on In-context Learning.pdf;/Users/jingang/Zotero/storage/KZ75LKLC/2301.html}
}

@misc{dorogushCatBoostGradientBoosting2018,
  title = {{{CatBoost}}: Gradient Boosting with Categorical Features Support},
  shorttitle = {{{CatBoost}}},
  author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
  year = {2018},
  month = oct,
  number = {arXiv:1810.11363},
  eprint = {1810.11363},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.11363},
  urldate = {2025-01-05},
  abstract = {In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are significantly faster than other gradient boosting libraries on ensembles of similar sizes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/ALIIGS5X/Dorogush et al. - 2018 - CatBoost gradient boosting with categorical features support.pdf;/Users/jingang/Zotero/storage/L33DRJBC/1810.html}
}

@article{dosovitskiyImageWorth16x162020,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.11929},
  eprint = {2010.11929},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Miscellaneous/An image is worth 16x16 words Transformers for image recognition at scale.pdf}
}

@article{douDomainGeneralizationModelagnostic2019,
  title = {Domain Generalization via Model-Agnostic Learning of Semantic Features},
  author = {Dou, Qi and {Coelho de Castro}, Daniel and Kamnitsas, Konstantinos and Glocker, Ben},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  annotation = {GSCC: 0000356}
}

@article{douzasImprovingImbalancedLearning2018,
  title = {Improving Imbalanced Learning through a Heuristic Oversampling Method Based on K-Means and {{SMOTE}}},
  author = {Douzas, Georgios and Bacao, Fernando and Last, Felix},
  year = {2018},
  journal = {Information Sciences},
  volume = {465},
  pages = {1--20},
  publisher = {Elsevier},
  isbn = {0020-0255}
}

@article{dowdHighPerformanceComputing2010,
  title = {High Performance Computing},
  author = {Dowd, Kevin and Severance, Charles},
  year = {2010},
  publisher = {Rice University}
}

@article{dozatIncorporatingNesterovMomentum2016,
  title = {Incorporating Nesterov Momentum into Adam},
  author = {Dozat, Timothy},
  year = {2016}
}

@article{duaTricksTrainingSparse2021,
  title = {Tricks for {{Training Sparse Translation Models}}},
  author = {Dua, Dheeru and Bhosale, Shruti and Goswami, Vedanuj and Cross, James and Lewis, Mike and Fan, Angela},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.08246},
  eprint = {2110.08246},
  archiveprefix = {arXiv}
}

@inproceedings{dubeyAdaptiveMethodsRealworld2021,
  title = {Adaptive Methods for Real-World Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Dubey, Abhimanyu and Ramanathan, Vignesh and Pentland, Alex and Mahajan, Dhruv},
  year = {2021},
  pages = {14340--14349},
  annotation = {GSCC: 0000087},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Ensemble methods/Adaptive methods for real-world domain generalization.pdf}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of machine learning research},
  volume = {12},
  number = {7},
  isbn = {1532-4435}
}

@inproceedings{duGlamEfficientScaling2022,
  title = {Glam: {{Efficient}} Scaling of Language Models with Mixture-of-Experts},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan},
  year = {2022},
  pages = {5547--5569},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Glam Efficient scaling of language models with mixture-of-experts.pdf}
}

@misc{duLearningEnhancedRepresentations2022,
  title = {Learning {{Enhanced Representations}} for {{Tabular Data}} via {{Neighborhood Propagation}}},
  author = {Du, Kounianhua and Zhang, Weinan and Zhou, Ruiwen and Wang, Yangkun and Zhao, Xilong and Jin, Jiarui and Gan, Quan and Zhang, Zheng and Wipf, David},
  year = {2022},
  month = jun,
  number = {arXiv:2206.06587},
  eprint = {2206.06587},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.06587},
  urldate = {2024-08-18},
  abstract = {Prediction over tabular data is an essential and fundamental problem in many important downstream tasks. However, existing methods either take a data instance of the table independently as input or do not fully utilize the multi-rows features and labels to directly change and enhance the target data representations. In this paper, we propose to 1) construct a hypergraph from relevant data instance retrieval to model the cross-row and cross-column patterns of those instances, and 2) perform message Propagation to Enhance the target data instance representation for Tabular prediction tasks. Specifically, our specially-designed message propagation step benefits from 1) fusion of label and features during propagation, and 2) locality-aware high-order feature interactions. Experiments on two important tabular data prediction tasks validate the superiority of the proposed PET model against other baselines. Additionally, we demonstrate the effectiveness of the model components and the feature enhancement ability of PET via various ablation studies and visualizations. The code is included in https://github.com/KounianhuaDu/PET.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/72A4N4YX/Du et al. - 2022 - Learning Enhanced Representations for Tabular Data via Neighborhood Propagation.pdf;/Users/jingang/Zotero/storage/PQ366PIA/2206.html}
}

@inproceedings{duLearningLearnVariational2020,
  title = {Learning to Learn with Variational Information Bottleneck for Domain Generalization},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Du, Yingjun and Xu, Jun and Xiong, Huan and Qiu, Qiang and Zhen, Xiantong and Snoek, Cees GM and Shao, Ling},
  year = {2020},
  pages = {200--216},
  publisher = {Springer},
  annotation = {GSCC: 0000052}
}

@article{dupontAugmentedNeuralOdes2019,
  title = {Augmented Neural Odes},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32}
}

@article{dupontLearningDisentangledJoint2018,
  title = {Learning Disentangled Joint Continuous and Discrete Representations},
  author = {Dupont, Emilien},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems},
  volume = {31},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/Learning Disentangled Joint Continuous and Discrete Representations.pdf}
}

@article{durkanNeuralSplineFlows2019,
  title = {Neural Spline Flows},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  year = {2019},
  journal = {Advances in neural information processing systems},
  volume = {32},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Normalizing Flow/Neural Spline Flows.pdf}
}

@misc{duTIPTabularImagePretraining2024,
  title = {{{TIP}}: {{Tabular-Image Pre-training}} for {{Multimodal Classification}} with {{Incomplete Data}}},
  shorttitle = {{{TIP}}},
  author = {Du, Siyi and Zheng, Shaoming and Wang, Yinsong and Bai, Wenjia and O'Regan, Declan P. and Qin, Chen},
  year = {2024},
  month = jul,
  number = {arXiv:2407.07582},
  eprint = {2407.07582},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07582},
  urldate = {2024-07-15},
  abstract = {Images and structured tables are essential parts of real-world databases. Though tabular-image representation learning is promising to create new insights, it remains a challenging task, as tabular data is typically heterogeneous and incomplete, presenting significant modality disparities with images. Earlier works have mainly focused on simple modality fusion strategies in complete data scenarios, without considering the missing data issue, and thus are limited in practice. In this paper, we propose TIP, a novel tabular-image pre-training framework for learning multimodal representations robust to incomplete tabular data. Specifically, TIP investigates a novel self-supervised learning (SSL) strategy, including a masked tabular reconstruction task for tackling data missingness, and image-tabular matching and contrastive learning objectives to capture multimodal information. Moreover, TIP proposes a versatile tabular encoder tailored for incomplete, heterogeneous tabular data and a multimodal interaction module for inter-modality representation learning. Experiments are performed on downstream multimodal classification tasks using both natural and medical image datasets. The results show that TIP outperforms state-of-the-art supervised/SSL image/multimodal algorithms in both complete and incomplete data scenarios. Our code is available at https://github.com/siyi-wind/TIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/FFFXVVN9/Du et al. - 2024 - TIP Tabular-Image Pre-training for Multimodal Cla.pdf;/Users/jingang/Zotero/storage/AF34FGXT/2407.html}
}

@misc{duTIPTabularImagePretraining2024a,
  title = {{{TIP}}: {{Tabular-Image Pre-training}} for {{Multimodal Classification}} with {{Incomplete Data}}},
  shorttitle = {{{TIP}}},
  author = {Du, Siyi and Zheng, Shaoming and Wang, Yinsong and Bai, Wenjia and O'Regan, Declan P. and Qin, Chen},
  year = {2024},
  month = jul,
  number = {arXiv:2407.07582},
  eprint = {2407.07582},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07582},
  urldate = {2024-07-31},
  abstract = {Images and structured tables are essential parts of real-world databases. Though tabular-image representation learning is promising to create new insights, it remains a challenging task, as tabular data is typically heterogeneous and incomplete, presenting significant modality disparities with images. Earlier works have mainly focused on simple modality fusion strategies in complete data scenarios, without considering the missing data issue, and thus are limited in practice. In this paper, we propose TIP, a novel tabular-image pre-training framework for learning multimodal representations robust to incomplete tabular data. Specifically, TIP investigates a novel self-supervised learning (SSL) strategy, including a masked tabular reconstruction task for tackling data missingness, and image-tabular matching and contrastive learning objectives to capture multimodal information. Moreover, TIP proposes a versatile tabular encoder tailored for incomplete, heterogeneous tabular data and a multimodal interaction module for inter-modality representation learning. Experiments are performed on downstream multimodal classification tasks using both natural and medical image datasets. The results show that TIP outperforms state-of-the-art supervised/SSL image/multimodal algorithms in both complete and incomplete data scenarios. Our code is available at https://github.com/siyi-wind/TIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/WBBIWLAL/Du et al. - 2024 - TIP Tabular-Image Pre-training for Multimodal Cla.pdf;/Users/jingang/Zotero/storage/F5DSI294/2407.html}
}

@article{eastwoodProbableDomainGeneralization2022,
  title = {Probable Domain Generalization via Quantile Risk Minimization},
  author = {Eastwood, Cian and Robey, Alexander and Singh, Shashank and Von K{\"u}gelgen, Julius and Hassani, Hamed and Pappas, George J and Sch{\"o}lkopf, Bernhard},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {17340--17358}
}

@misc{edwardsNeuralStatistician2017,
  title = {Towards a {{Neural Statistician}}},
  author = {Edwards, Harrison and Storkey, Amos},
  year = {2017},
  month = mar,
  number = {arXiv:1606.02185},
  eprint = {1606.02185},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.02185},
  urldate = {2024-08-23},
  abstract = {An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/VSKTJAQ5/Edwards and Storkey - 2017 - Towards a Neural Statistician.pdf;/Users/jingang/Zotero/storage/VHDMEFLC/1606.html}
}

@article{eigenLearningFactoredRepresentations2013,
  title = {Learning Factored Representations in a Deep Mixture of Experts},
  author = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.4314},
  eprint = {1312.4314},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Learning factored representations in a deep mixture of experts.pdf}
}

@article{elbaccouchHighPressureVaporLiquid2000,
  title = {High-{{Pressure Vapor}}- {{Liquid Equilibrium}} for {{R-22}}+ {{Ethanol}} and {{R-22}}+ {{Ethanol}}+ {{Water}}},
  author = {Elbaccouch, Mohamed M and Raymond, Michael B and Elliott, J Richard},
  year = {2000},
  journal = {Journal of Chemical \& Engineering Data},
  volume = {45},
  number = {2},
  pages = {280--287},
  publisher = {ACS Publications},
  issn = {0021-9568}
}

@article{elfwingSigmoidweightedLinearUnits2018,
  title = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  year = {2018},
  journal = {Neural Networks},
  volume = {107},
  pages = {3--11},
  publisher = {Elsevier}
}

@article{elghribiHomogeneousFunctionsNew2017,
  title = {Homogeneous Functions: {{New}} Characterization and Applications},
  author = {Elghribi, Moncef and Othman, Hakeem A and {Al-Nashri}, Al-Hossain Ahmed},
  year = {2017},
  journal = {Transactions of A. Razmadze Mathematical Institute},
  volume = {171},
  number = {2},
  pages = {171--181},
  publisher = {Elsevier},
  issn = {2346-8092}
}

@article{ene-iordacheDisturbedFlowRadialcephalic2012,
  title = {Disturbed Flow in Radial-Cephalic Arteriovenous Fistulae for Haemodialysis: Low and Oscillating Shear Stress Locates the Sites of Stenosis},
  author = {{Ene-Iordache}, Bogdan and Remuzzi, Andrea},
  year = {2012},
  journal = {Nephrology Dialysis Transplantation},
  volume = {27},
  number = {1},
  pages = {358--368},
  publisher = {Oxford University Press},
  isbn = {1460-2385},
  file = {/Users/jingang/Dropbox/References/AVF/Disturbed flow in radial-cephalic arteriovenous fistulae.pdf}
}

@article{ene-iordacheEffectAnastomosisAngle2013,
  title = {Effect of Anastomosis Angle on the Localization of Disturbed Flow in `Side-to-End'Fistulae for Haemodialysis Access},
  author = {{Ene-Iordache}, Bogdan and Cattaneo, Luca and Dubini, Gabriele and Remuzzi, Andrea},
  year = {2013},
  journal = {Nephrology Dialysis Transplantation},
  volume = {28},
  number = {4},
  pages = {997--1005},
  publisher = {Oxford University Press},
  isbn = {1460-2385},
  file = {/Users/jingang/Dropbox/References/AVF/Effect of anastomosis angle on the localization of disturbed flow.pdf}
}

@inproceedings{fangUnbiasedMetricLearning2013,
  title = {Unbiased Metric Learning: {{On}} the Utilization of Multiple Datasets and Web Images for Softening Bias},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Fang, Chen and Xu, Ye and Rockmore, Daniel N.},
  year = {2013},
  pages = {1657--1664}
}

@misc{fanMultiscaleVisionTransformers2021,
  title = {Multiscale {{Vision Transformers}}},
  author = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  year = {2021},
  month = apr,
  number = {arXiv:2104.11227},
  eprint = {2104.11227},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.11227},
  urldate = {2024-09-04},
  abstract = {We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/Z6INSWB3/Fan et al. - 2021 - Multiscale Vision Transformers.pdf;/Users/jingang/Zotero/storage/JJEG8QNF/2104.html}
}

@article{fawziDiscoveringFasterMatrix2022,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and {Romera-Paredes}, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others},
  year = {2022},
  journal = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53},
  publisher = {Nature Publishing Group}
}

@article{fedusReviewSparseExpert2022,
  title = {A Review of Sparse Expert Models in Deep Learning},
  author = {Fedus, William and Dean, Jeff and Zoph, Barret},
  year = {2022},
  journal = {arXiv preprint arXiv:2209.01667},
  eprint = {2209.01667},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/A review of sparse expert models in deep learning.pdf}
}

@book{fedusSwitchTransformersScaling2021,
  title = {Switch Transformers: {{Scaling}} to Trillion Parameter Models with Simple and Efficient Sparsity},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2021}
}

@book{ferzigerComputationalMethodsFluid2002,
  title = {Computational Methods for Fluid Dynamics},
  author = {Ferziger, Joel H. and Peri{\'c}, Milovan and Street, Robert L.},
  year = {2002},
  volume = {3},
  publisher = {Springer},
  isbn = {3-540-42074-6}
}

@article{feuerTuneTablesContextOptimization2024,
  title = {{{TuneTables}}: {{Context Optimization}} for {{Scalable Prior-Data Fitted Networks}}},
  author = {Feuer, Benjamin and Schirrmeister, Robin Tibor and Cherepanova, Valeriia and Hegde, Chinmay and Hutter, Frank and Goldblum, Micah and Cohen, Niv and White, Colin},
  year = {2024},
  journal = {arXiv preprint arXiv:2402.11137},
  eprint = {2402.11137},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/TuneTables- Context Optimization for ScalablePrior-Data Fitted Networks.pdf}
}

@incollection{flachClassifierCalibration2016,
  title = {Classifier Calibration},
  booktitle = {Encyclopedia of Machine Learning and Data Mining},
  author = {Flach, Peter A},
  year = {2016},
  publisher = {Springer US}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  author = {Friedman, Jerome H.},
  year = {2001},
  journal = {Annals of statistics},
  pages = {1189--1232},
  publisher = {JSTOR},
  isbn = {0090-5364}
}

@article{fuCyclicalAnnealingSchedule2019,
  title = {Cyclical Annealing Schedule: {{A}} Simple Approach to Mitigating Kl Vanishing},
  author = {Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
  year = {2019},
  journal = {arXiv preprint arXiv:1903.10145},
  eprint = {1903.10145},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Improvements/Cyclical annealing schedule A simple approach to mitigating kl vanishing.pdf}
}

@misc{fuCyclicalAnnealingSchedule2019a,
  title = {Cyclical {{Annealing Schedule}}: {{A Simple Approach}} to {{Mitigating KL Vanishing}}},
  shorttitle = {Cyclical {{Annealing Schedule}}},
  author = {Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
  year = {2019},
  month = jun,
  number = {arXiv:1903.10145},
  eprint = {1903.10145},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.10145},
  urldate = {2022-09-16},
  abstract = {Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter {\textbackslash}beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for {\textbackslash}beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing {\textbackslash}beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Zotero/storage/5L6H8WA2/Fu et al_2019_Cyclical Annealing Schedule.pdf;/Users/jingang/Zotero/storage/545RNWGF/1903.html}
}

@article{fuSimplifiedSAFTEquation1995,
  title = {A Simplified {{SAFT}} Equation of State for Associating Compounds and Mixtures},
  author = {Fu, Yuan-Hao and Sandler, Stanley I.},
  year = {1995},
  journal = {Industrial \& engineering chemistry research},
  volume = {34},
  number = {5},
  pages = {1897--1909},
  publisher = {ACS Publications},
  isbn = {0888-5885}
}

@misc{fuTransformersLearnHigherOrder2024,
  title = {Transformers {{Learn Higher-Order Optimization Methods}} for {{In-Context Learning}}: {{A Study}} with {{Linear Models}}},
  shorttitle = {Transformers {{Learn Higher-Order Optimization Methods}} for {{In-Context Learning}}},
  author = {Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  year = {2024},
  month = may,
  number = {arXiv:2310.17086},
  eprint = {2310.17086},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17086},
  urldate = {2024-08-30},
  abstract = {Transformers excel at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they do so remains a mystery. Recent work suggests that Transformers may internally run Gradient Descent (GD), a first-order optimization method, to perform ICL. In this paper, we instead demonstrate that Transformers learn to approximate higher-order optimization methods for ICL. For in-context linear regression, Transformers share a similar convergence rate as Iterative Newton's Method; both are exponentially faster than GD. Empirically, predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations; thus, Transformers and Newton's method converge at roughly the same rate. In contrast, Gradient Descent converges exponentially more slowly. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, to corroborate our empirical findings, we prove that Transformers can implement \$k\$ iterations of Newton's method with \$k + {\textbackslash}mathcal\{O\}(1)\$ layers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/G2ZTAPRL/Fu et al. - 2024 - Transformers Learn Higher-Order Optimization Methods for In-Context Learning A Study with Linear Mo.pdf;/Users/jingang/Zotero/storage/TS8U2RFN/2310.html}
}

@article{gaballaNumericalInvestigationDroplet2022,
  title = {Numerical Investigation of Droplet Evaporation in High-Pressure Dual-Fuel Conditions Using a Tabulated Real-Fluid Model},
  author = {Gaballa, Hesham and Jafari, Sajad and Habchi, Chaouki and {de Hemptinne}, Jean-Charles},
  year = {2022},
  journal = {International Journal of Heat and Mass Transfer},
  volume = {189},
  pages = {122671},
  publisher = {Elsevier},
  issn = {0017-9310}
}

@article{gaganisIntegratedApproachRapid2014,
  title = {An Integrated Approach for Rapid Phase Behavior Calculations in Compositional Modeling},
  author = {Gaganis, Vassilis and Varotsis, Nikos},
  year = {2014},
  journal = {Journal of Petroleum Science and Engineering},
  volume = {118},
  pages = {74--87},
  publisher = {Elsevier},
  annotation = {titleTranslation:},
  file = {/Users/jingang/Dropbox/References/Accelerate flash via machine learning/An integratedapproachforrapidphasebehaviorcalculationsin.pdf}
}

@inproceedings{gaganisMachineLearningMethods2012,
  title = {Machine Learning Methods to Speed up Compositional Reservoir Simulation},
  booktitle = {{{SPE Europec}}/{{EAGE}} Annual Conference},
  author = {Gaganis, Vassilis and Varotsis, Nikos},
  year = {2012},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash via machine learning/Machine learning methods to speed up compositional reservoir simulation.pdf}
}

@article{gaganisRapidPhaseStability2018,
  title = {Rapid Phase Stability Calculations in Fluid Flow Simulation Using Simple Discriminating Functions},
  author = {Gaganis, Vassilis},
  year = {2018},
  journal = {Computers \& Chemical Engineering},
  volume = {108},
  pages = {112--127},
  publisher = {Elsevier}
}

@article{galantiModularityHypernetworks2020,
  title = {On the Modularity of Hypernetworks},
  author = {Galanti, Tomer and Wolf, Lior},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {10409--10419},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/On the Modularity of Hypernetworks.pdf;/Users/jingang/Dropbox/References/Hypernetworks/On the Modularity of Hypernetworks.pdf}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a Bayesian Approximation: {{Representing}} Model Uncertainty in Deep Learning},
  booktitle = {International Conference on Machine Learning},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  pages = {1050--1059},
  publisher = {PMLR}
}

@article{galTheoreticallyGroundedApplication2016,
  title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  journal = {Advances in neural information processing systems},
  volume = {29},
  pages = {1019--1027}
}

@misc{galTheoreticallyGroundedApplication2016a,
  title = {A {{Theoretically Grounded Application}} of {{Dropout}} in {{Recurrent Neural Networks}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  number = {arXiv:1512.05287},
  eprint = {1512.05287},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.05287},
  urldate = {2024-06-12},
  abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/NX4HCU4Q/Gal and Ghahramani - 2016 - A Theoretically Grounded Application of Dropout in.pdf;/Users/jingang/Zotero/storage/ZLVDQVPA/1512.html}
}

@article{ganinDomainadversarialTrainingNeural2016,
  title = {Domain-Adversarial Training of Neural Networks},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario and Lempitsky, Victor},
  year = {2016},
  journal = {The journal of machine learning research},
  volume = {17},
  number = {1},
  pages = {2096--2030},
  publisher = {JMLR. org},
  issn = {1532-4435},
  annotation = {GSCC: 0005606},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Domain-invariant representation learning/Domain-Adversarial Training of Neural Networks.pdf}
}

@article{ganinDomainadversarialTrainingNeural2016a,
  title = {Domain-Adversarial Training of Neural Networks},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario and Lempitsky, Victor},
  year = {2016},
  journal = {The journal of machine learning research},
  volume = {17},
  number = {1},
  pages = {2096--2030},
  publisher = {JMLR. org},
  isbn = {1532-4435},
  annotation = {GSCC: 0005606}
}

@inproceedings{ganinUnsupervisedDomainAdaptation2015,
  title = {Unsupervised Domain Adaptation by Backpropagation},
  booktitle = {International Conference on Machine Learning},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  pages = {1180--1189},
  publisher = {PMLR},
  annotation = {GSCC: 0004282},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Domain-invariant representation learning/Unsupervised Domain Adaptation by Backpropagation.pdf}
}

@inproceedings{ganinUnsupervisedDomainAdaptation2015a,
  title = {Unsupervised Domain Adaptation by Backpropagation},
  booktitle = {International Conference on Machine Learning},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  pages = {1180--1189},
  publisher = {PMLR},
  annotation = {GSCC: 0004282}
}

@inproceedings{ganLearningAttributesEquals2016,
  title = {Learning Attributes Equals Multi-Source Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gan, Chuang and Yang, Tianbao and Gong, Boqing},
  year = {2016},
  pages = {87--97},
  annotation = {GSCC: 0000195}
}

@article{garcia-escuderoRobustClusterwiseLinear2010,
  title = {Robust Clusterwise Linear Regression through Trimming},
  author = {{Garc{\'i}a-Escudero}, Luis Angel and Gordaliza, Alfonso and {Mayo-{\'I}scar}, Agust{\'i}n and San Mart{\'i}n, Roberto},
  year = {2010},
  journal = {Computational Statistics \& Data Analysis},
  volume = {54},
  number = {12},
  pages = {3057--3069},
  publisher = {Elsevier},
  isbn = {0167-9473},
  file = {/Users/jingang/Dropbox/References/Regression clustering/Robust clusterwise linear regression through trimming.pdf}
}

@misc{gardnerLargeScaleTransfer2024,
  title = {Large {{Scale Transfer Learning}} for {{Tabular Data}} via {{Language Modeling}}},
  author = {Gardner, Josh and Perdomo, Juan C. and Schmidt, Ludwig},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12031},
  eprint = {2406.12031},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12031},
  urldate = {2024-08-27},
  abstract = {Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 1.6B rows from 3.1M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/FWKTGCNE/Gardner et al. - 2024 - Large Scale Transfer Learning for Tabular Data via Language Modeling.pdf;/Users/jingang/Zotero/storage/RWNVQUPC/2406.html}
}

@misc{gargWhatCanTransformers2023,
  title = {What {{Can Transformers Learn In-Context}}? {{A Case Study}} of {{Simple Function Classes}}},
  shorttitle = {What {{Can Transformers Learn In-Context}}?},
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  year = {2023},
  month = aug,
  number = {arXiv:2208.01066},
  eprint = {2208.01066},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.01066},
  urldate = {2024-08-30},
  abstract = {In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/S4KBTQRT/Garg et al. - 2023 - What Can Transformers Learn In-Context A Case Study of Simple Function Classes.pdf;/Users/jingang/Zotero/storage/VHZX9B8P/2208.html}
}

@article{garipovLossSurfacesMode2018,
  title = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of Dnns},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31}
}

@article{gattiReviewModelingHeat2014,
  title = {Review, Modeling, {{Heat Integration}}, and Improved Schemes of {{Rectisol}}{\textregistered}-Based Processes for {{CO2}} Capture},
  author = {Gatti, Manuele and Martelli, Emanuele and Marechal, Fran{\c c}ois and Consonni, Stefano},
  year = {2014},
  journal = {Applied Thermal Engineering},
  volume = {70},
  number = {2},
  pages = {1123--1140},
  publisher = {Elsevier},
  issn = {1359-4311}
}

@article{geirhosShortcutLearningDeep2020,
  title = {Shortcut Learning in Deep Neural Networks},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  year = {2020},
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/Shortcut learning in deep neural networks.pdf}
}

@article{gemanNeuralNetworksBias1992,
  title = {Neural Networks and the Bias/Variance Dilemma},
  author = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  year = {1992},
  journal = {Neural computation},
  volume = {4},
  number = {1},
  pages = {1--58},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}}
}

@article{gernertCalculationPhaseEquilibria2014,
  title = {Calculation of Phase Equilibria for Multi-Component Mixtures Using Highly Accurate {{Helmholtz}} Energy Equations of State},
  author = {Gernert, Johannes and J{\"a}ger, Andreas and Span, Roland},
  year = {2014},
  journal = {Fluid Phase Equilibria},
  volume = {375},
  pages = {209--218},
  publisher = {Elsevier},
  issn = {0378-3812}
}

@misc{ghiasiDropBlockRegularizationMethod2018,
  title = {{{DropBlock}}: {{A}} Regularization Method for Convolutional Networks},
  shorttitle = {{{DropBlock}}},
  author = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V.},
  year = {2018},
  month = oct,
  number = {arXiv:1810.12890},
  eprint = {1810.12890},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.12890},
  urldate = {2024-06-19},
  abstract = {Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves \$78.13{\textbackslash}\%\$ accuracy, which is more than \$1.6{\textbackslash}\%\$ improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from \$36.8{\textbackslash}\%\$ to \$38.4{\textbackslash}\%\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/27ZNPKXJ/Ghiasi et al. - 2018 - DropBlock A regularization method for convolution.pdf;/Users/jingang/Zotero/storage/3I65GPY4/1810.html}
}

@inproceedings{ghifaryDomainGeneralizationObject2015,
  title = {Domain Generalization for Object Recognition with Multi-Task Autoencoders},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Ghifary, Muhammad and Kleijn, W. Bastiaan and Zhang, Mengjie and Balduzzi, David},
  year = {2015},
  pages = {2551--2559},
  annotation = {GSCC: 0000503}
}

@article{ghifaryScatterComponentAnalysis2016,
  title = {Scatter Component Analysis: {{A}} Unified Framework for Domain Adaptation and Domain Generalization},
  author = {Ghifary, Muhammad and Balduzzi, David and Kleijn, W. Bastiaan and Zhang, Mengjie},
  year = {2016},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {39},
  number = {7},
  pages = {1414--1430},
  publisher = {IEEE},
  isbn = {0162-8828},
  annotation = {GSCC: 0000312}
}

@misc{giannouLoopedTransformersProgrammable2023,
  title = {Looped {{Transformers}} as {{Programmable Computers}}},
  author = {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13196},
  eprint = {2301.13196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.13196},
  urldate = {2024-08-30},
  abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/V3HJUNNS/Giannou et al. - 2023 - Looped Transformers as Programmable Computers.pdf;/Users/jingang/Zotero/storage/P2BZTX3R/2301.html}
}

@article{gil-villegasStatisticalAssociatingFluid1997,
  title = {Statistical Associating Fluid Theory for Chain Molecules with Attractive Potentials of Variable Range},
  author = {{Gil-Villegas}, Alejandro and Galindo, Amparo and Whitehead, Paul J. and Mills, Stuart J. and Jackson, George and Burgess, Andrew N.},
  year = {1997},
  journal = {The Journal of chemical physics},
  volume = {106},
  number = {10},
  pages = {4168--4186},
  publisher = {American Institute of Physics},
  isbn = {0021-9606}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep Sparse Rectifier Neural Networks},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}}
}

@misc{golkarXValContinuousNumber2023,
  title = {{{xVal}}: {{A Continuous Number Encoding}} for {{Large Language Models}}},
  shorttitle = {{{xVal}}},
  author = {Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and Blancard, Bruno R{\'e}galdo-Saint and Tesileanu, Tiberiu and Cho, Kyunghyun and Ho, Shirley},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02989},
  eprint = {2310.02989},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.02989},
  urldate = {2025-01-03},
  abstract = {Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/LPQ8B888/Golkar et al. - 2023 - xVal A Continuous Number Encoding for Large Language Models.pdf;/Users/jingang/Zotero/storage/8ZMQPKR9/2310.html}
}

@misc{golkarXValContinuousNumerical2024,
  title = {{{xVal}}: {{A Continuous Numerical Tokenization}} for {{Scientific Language Models}}},
  shorttitle = {{{xVal}}},
  author = {Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and Blancard, Bruno R{\'e}galdo-Saint and Tesileanu, Tiberiu and Cho, Kyunghyun and Ho, Shirley},
  year = {2024},
  month = dec,
  number = {arXiv:2310.02989},
  eprint = {2310.02989},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.02989},
  urldate = {2025-01-03},
  abstract = {Due in part to their discontinuous and discrete default encodings for numbers, Large Language Models (LLMs) have not yet been commonly used to process numerically-dense scientific datasets. Rendering datasets as text, however, could help aggregate diverse and multi-modal scientific data into a single training corpus, thereby potentially facilitating the development of foundation models for science. In this work, we introduce xVal, a strategy for continuously tokenizing numbers within language models that results in a more appropriate inductive bias for scientific applications. By training specially-modified language models from scratch on a variety of scientific datasets formatted as text, we find that xVal generally outperforms other common numerical tokenization strategies on metrics including out-of-distribution generalization and computational efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/WR7GMLF5/Golkar et al. - 2024 - xVal A Continuous Numerical Tokenization for Scientific Language Models.pdf;/Users/jingang/Zotero/storage/C89MJ35G/2310.html}
}

@inproceedings{golovinGoogleVizierService2017,
  title = {Google Vizier: {{A}} Service for Black-Box Optimization},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, David},
  year = {2017},
  pages = {1487--1495}
}

@misc{golovnevaContextualPositionEncoding2024,
  title = {Contextual {{Position Encoding}}: {{Learning}} to {{Count What}}'s {{Important}}},
  shorttitle = {Contextual {{Position Encoding}}},
  author = {Golovneva, Olga and Wang, Tianlu and Weston, Jason and Sukhbaatar, Sainbayar},
  year = {2024},
  month = may,
  number = {arXiv:2405.18719},
  eprint = {2405.18719},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.18719},
  urldate = {2024-10-14},
  abstract = {The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the \$i\$-th particular word, noun, or sentence. We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/JT5DSR9N/Golovneva et al. - 2024 - Contextual Position Encoding Learning to Count What's Important.pdf;/Users/jingang/Zotero/storage/25BEFSEV/2405.html}
}

@inproceedings{gongDlowDomainFlow2019,
  title = {Dlow: {{Domain}} Flow for Adaptation and Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Gong, Rui and Li, Wen and Chen, Yuhua and Gool, Luc Van},
  year = {2019},
  pages = {2477--2486},
  annotation = {GSCC: 0000221}
}

@article{gongReshapingVisualDatasets2013,
  title = {Reshaping Visual Datasets for Domain Adaptation},
  author = {Gong, Boqing and Grauman, Kristen and Sha, Fei},
  year = {2013},
  journal = {Advances in Neural Information Processing Systems},
  volume = {26}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT press}
}

@book{goodsteinStatesMatter2014,
  title = {States of Matter},
  author = {Goodstein, David L},
  year = {2014},
  publisher = {Courier Corporation},
  isbn = {0-486-79551-9}
}

@article{gorishniyEmbeddingsNumericalFeatures2022,
  title = {On Embeddings for Numerical Features in Tabular Deep Learning},
  author = {Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24991--25004},
  file = {/Users/jingang/Dropbox/References/Tabular learning/On Embeddings for Numerical Features in Tabular Deep Learning.pdf}
}

@article{gorishniyRevisitingDeepLearning2021,
  title = {Revisiting Deep Learning Models for Tabular Data},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {18932--18943},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Revisiting deep learning models for tabular data.pdf}
}

@misc{gorishniyTabMAdvancingTabular2024,
  title = {{{TabM}}: {{Advancing Tabular Deep Learning}} with {{Parameter-Efficient Ensembling}}},
  shorttitle = {{{TabM}}},
  author = {Gorishniy, Yury and Kotelnikov, Akim and Babenko, Artem},
  year = {2024},
  month = nov,
  number = {arXiv:2410.24210},
  eprint = {2410.24210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.24210},
  urldate = {2024-12-19},
  abstract = {Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods. This study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs: namely, parameter-efficient ensembling -- a paradigm for implementing an ensemble of models as one model producing multiple predictions. We start by developing TabM -- a simple model based on MLP and our variations of BatchEnsemble (an existing technique). Then, we perform a large-scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light. Generally, we show that MLPs, including TabM, form a line of stronger and more practical models compared to attention- and retrieval-based architectures. In particular, we find that TabM demonstrates the best performance among tabular DL models. Lastly, we conduct an empirical analysis on the ensemble-like nature of TabM. For example, we observe that the multiple predictions of TabM are weak individually, but powerful collectively. Overall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency trade-off with TabM -- a simple and powerful baseline for researchers and practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/6EPXBDLV/Gorishniy et al. - 2024 - TabM Advancing Tabular Deep Learning with Parameter-Efficient Ensembling.pdf;/Users/jingang/Zotero/storage/EBXISDCU/2410.html}
}

@article{gorishniyTabRTabularDeep,
  title = {{{TabR}}: {{Tabular Deep Learning Meets Nearest Neighbors}}},
  shorttitle = {{{TabR}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Kartashev, Nikolay and Shlenskii, Daniil and Kotelnikov, Akim and Babenko, Artem},
  year = {2024},
  journal={ICLR},
   file = {/Users/jingang/Zotero/storage/3JJUBS25/Gorishniy et al. - TabR Tabular Deep Learning Meets Nearest Neighbor.pdf}
}

@misc{grattafioriLlama3Herd2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzm{\'a}n, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and {Rantala-Yeary}, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, V{\'i}tor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  year = {2024},
  month = nov,
  number = {arXiv:2407.21783},
  eprint = {2407.21783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.21783},
  urldate = {2025-01-05},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/CT89N2RE/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf;/Users/jingang/Zotero/storage/UJH4MWKX/2407.html}
}

@article{grillBootstrapYourOwn2020,
  title = {Bootstrap Your Own Latent-a New Approach to Self-Supervised Learning},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {21271--21284},
  file = {/Users/jingang/Dropbox/References/Contrastive learning/Bootstrap Your Own Latent ANewApproach to Self-Supervised Learning.pdf}
}

@misc{grinsztajnWhyTreebasedModels2022,
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = {2022},
  month = jul,
  number = {arXiv:2207.08815},
  eprint = {2207.08815},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.08815},
  urldate = {2024-09-01},
  abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\${\textbackslash}sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/jingang/Zotero/storage/LAB8EPXU/Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep learning on tabular data.pdf;/Users/jingang/Zotero/storage/TYV7BUJQ/2207.html}
}

@article{grossApplicationPerturbedchainSAFT2002,
  title = {Application of the Perturbed-Chain {{SAFT}} Equation of State to Associating Systems},
  author = {Gross, Joachim and Sadowski, Gabriele},
  year = {2002},
  journal = {Industrial \& engineering chemistry research},
  volume = {41},
  number = {22},
  pages = {5510--5515},
  publisher = {ACS Publications},
  isbn = {0888-5885}
}

@article{grossPerturbedchainSAFTEquation2001,
  title = {Perturbed-Chain {{SAFT}}: {{An}} Equation of State Based on a Perturbation Theory for Chain Molecules},
  author = {Gross, Joachim and Sadowski, Gabriele},
  year = {2001},
  journal = {Industrial \& engineering chemistry research},
  volume = {40},
  number = {4},
  pages = {1244--1260},
  publisher = {ACS Publications},
  isbn = {0888-5885}
}

@article{groveCommentsThermodynamicConsistency2019,
  title = {Some Comments on Thermodynamic Consistency for Equilibrium Mixture Equations of State},
  author = {Grove, John W},
  year = {2019},
  journal = {Computers \& Mathematics with Applications},
  volume = {78},
  number = {2},
  pages = {582--597},
  publisher = {Elsevier},
  issn = {0898-1221}
}

@article{gulrajaniSearchLostDomain2020,
  title = {In Search of Lost Domain Generalization},
  author = {Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2020},
  journal = {arXiv preprint arXiv:2007.01434},
  eprint = {2007.01434},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Dataset and Benchmark/In Search of Lost Domain Generalization.pdf}
}

@misc{guoHowTransformersLearn2023,
  title = {How {{Do Transformers Learn In-Context Beyond Simple Functions}}? {{A Case Study}} on {{Learning}} with {{Representations}}},
  shorttitle = {How {{Do Transformers Learn In-Context Beyond Simple Functions}}?},
  author = {Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10616},
  eprint = {2310.10616},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.10616},
  urldate = {2024-08-30},
  abstract = {While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with representations. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but fixed representation function, composed with a linear function that differs in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size. Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/SFNB2SEX/Guo et al. - 2023 - How Do Transformers Learn In-Context Beyond Simple Functions A Case Study on Learning with Represen.pdf;/Users/jingang/Zotero/storage/7J4JBQWC/2310.html}
}

@article{guoMultisourceDomainAdaptation2018,
  title = {Multi-Source Domain Adaptation with Mixture of Experts},
  author = {Guo, Jiang and Shah, Darsh J. and Barzilay, Regina},
  year = {2018},
  journal = {arXiv preprint arXiv:1809.02256},
  eprint = {1809.02256},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Multi-Source Domain Adaptation with Mixture of Experts.pdf}
}

@article{guptaSparselyActivatedMixtureofexperts2022,
  title = {Sparsely Activated Mixture-of-Experts Are Robust Multi-Task Learners},
  author = {Gupta, Shashank and Mukherjee, Subhabrata and Subudhi, Krishan and Gonzalez, Eduardo and Jose, Damien and Awadallah, Ahmed H. and Gao, Jianfeng},
  year = {2022},
  journal = {arXiv preprint arXiv:2204.07689},
  eprint = {2204.07689},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners.pdf}
}

@article{hafnerVerosV0Fast2018,
  title = {Veros v0. 1--{{A}} Fast and Versatile Ocean Simulator in Pure {{Python}}},
  author = {H{\"a}fner, Dion and Jacobsen, Ren{\'e} L{\o}we and Eden, Carsten and Kristensen, Mads RB and Jochum, Markus and Nuterman, Roman and Vinter, Brian},
  year = {2018},
  journal = {Geoscientific Model Development},
  volume = {11},
  number = {8},
  pages = {3299--3312},
  publisher = {Copernicus GmbH},
  isbn = {1991-959X}
}

@article{haHypernetworks2016,
  title = {Hypernetworks},
  author = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.09106},
  eprint = {1609.09106},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Hypernetworks.pdf}
}

@article{hajimiriSemisupervisedDisentanglementClassrelated2021,
  title = {Semi-Supervised Disentanglement of Class-Related and Class-Independent Factors in Vae},
  author = {Hajimiri, Sina and Lotfi, Aryo and Baghshah, Mahdieh Soleymani},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.00892},
  eprint = {2102.00892},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/Semi-Supervised Disentanglement of Class-Related and Class-Independent Factors in VAE.pdf}
}

@article{hametArtificialIntelligenceMedicine2017,
  title = {Artificial Intelligence in Medicine},
  author = {Hamet, Pavel and Tremblay, Johanne},
  year = {2017},
  journal = {Metabolism},
  volume = {69},
  pages = {S36-S40},
  publisher = {Elsevier},
  isbn = {0026-0495}
}

@article{hametArtificialIntelligenceMedicine2017a,
  title = {Artificial Intelligence in Medicine},
  author = {Hamet, Pavel and Tremblay, Johanne},
  year = {2017},
  journal = {Metabolism},
  volume = {69},
  pages = {S36-S40},
  publisher = {Elsevier},
  isbn = {0026-0495}
}

@inproceedings{hanBorderlineSMOTENewOversampling2005,
  title = {Borderline-{{SMOTE}}: A New over-Sampling Method in Imbalanced Data Sets Learning},
  booktitle = {International Conference on Intelligent Computing},
  author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
  year = {2005},
  pages = {878--887},
  publisher = {Springer}
}

@inproceedings{haugenHighlyOptimizedPhase2013,
  title = {Highly Optimized Phase Equilibrium Calculations},
  booktitle = {{{SPE Reservoir Simulation Symposium}}},
  author = {Haugen, Kjetil B and Beckner, Bret L},
  year = {2013},
  publisher = {OnePetro}
}

@inproceedings{hayderChallengesHighPerformance2012,
  title = {Challenges in High Performance Computing for Reservoir Simulation},
  booktitle = {{{SPE Europec}}/{{EAGE Annual Conference}}},
  author = {Hayder, Ehtesham M and Baddourah, Majdi},
  year = {2012},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Challenges in High Performance Computing for Reservoir Simulation.pdf}
}

@article{hazimehDselectkDifferentiableSelection2021,
  title = {Dselect-k: {{Differentiable}} Selection in the Mixture of Experts with Applications to Multi-Task Learning},
  author = {Hazimeh, Hussein and Zhao, Zhe and Chowdhery, Aakanksha and Sathiamoorthy, Maheswaran and Chen, Yihua and Mazumder, Rahul and Hong, Lichan and Chi, Ed},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {29335--29347},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Dselect-k Differentiable selection in the mixture of experts with applications to multi-task learning.pdf}
}

@inproceedings{heADASYNAdaptiveSynthetic2008,
  title = {{{ADASYN}}: {{Adaptive}} Synthetic Sampling Approach for Imbalanced Learning},
  booktitle = {2008 {{IEEE}} International Joint Conference on Neural Networks ({{IEEE}} World Congress on Computational Intelligence)},
  author = {He, Haibo and Bai, Yang and Garcia, Edwardo A. and Li, Shutao},
  year = {2008},
  pages = {1322--1328},
  publisher = {IEEE},
  isbn = {1-4244-1820-8}
}

@article{hebdenAlgorithmMinimizationUsing1973,
  title = {An Algorithm for Minimization Using Exact Second Derivatives},
  author = {Hebden, {\relax MD}},
  year = {1973},
  publisher = {Citeseer},
  file = {/Users/jingang/Dropbox/References/Flash calculations/An algorithm for minimization using exact second derivatives.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778}
}

@inproceedings{heDelvingDeepRectifiers2015,
  title = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  pages = {1026--1034}
}

@article{heFastmoeFastMixtureofexpert2021,
  title = {Fastmoe: {{A}} Fast Mixture-of-Expert Training System},
  author = {He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  year = {2021},
  journal = {arXiv preprint arXiv:2103.13262},
  eprint = {2103.13262},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Fastmoe A fast mixture-of-expert training system.pdf}
}

@article{heindelPredictingRadiocephalicArteriovenous2022,
  title = {Predicting Radiocephalic Arteriovenous Fistula Success with Machine Learning},
  author = {Heindel, Patrick and Dey, Tanujit and Feliz, Jessica D. and Hentschel, Dirk M. and Bhatt, Deepak L. and {Al-Omran}, Mohammed and Belkin, Michael and Ozaki, C. Keith and Hussain, Mohamad A.},
  year = {2022},
  journal = {NPJ digital medicine},
  volume = {5},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  isbn = {2398-6352},
  file = {/Users/jingang/Dropbox/References/AVF/Predicting radiocephalic arteriovenous fistula success.pdf}
}

@article{hendriksApplicationReductionMethod1992,
  title = {Application of a Reduction Method to Phase Equilibria Calculations},
  author = {Hendriks, Eric M and Van Bergen, {\relax ARD}},
  year = {1992},
  journal = {Fluid Phase Equilibria},
  volume = {74},
  pages = {17--34},
  publisher = {Elsevier}
}

@article{hendriksReductionTheoremPhase1988,
  title = {Reduction Theorem for Phase Equilibrium Problems},
  author = {Hendriks, Eric M},
  year = {1988},
  journal = {Industrial \& engineering chemistry research},
  volume = {27},
  number = {9},
  pages = {1728--1732},
  publisher = {ACS Publications}
}

@article{hendrycksGaussianErrorLinear2016,
  title = {Gaussian Error Linear Units (Gelus)},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.08415},
  eprint = {1606.08415},
  archiveprefix = {arXiv}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {MIT Press},
  isbn = {0899-7667}
}

@inproceedings{hoffmanDiscoveringLatentDomains2012,
  title = {Discovering Latent Domains for Multisource Domain Adaptation},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Hoffman, Judy and Kulis, Brian and Darrell, Trevor and Saenko, Kate},
  year = {2012},
  pages = {702--715},
  publisher = {Springer}
}

@article{hollmannAccuratePredictionsSmall2025,
  title = {Accurate Predictions on Small Data with a Tabular Foundation Model},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Purucker, Lennart and Krishnakumar, Arjun and K{\"o}rfer, Max and Hoo, Shi Bin and Schirrmeister, Robin Tibor and Hutter, Frank},
  year = {2025},
  month = jan,
  journal = {Nature},
  volume = {637},
  number = {8045},
  pages = {319--326},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-08328-6},
  urldate = {2025-01-27},
  abstract = {Tabular data, spreadsheets organized in rows and columns, are ubiquitous across scientific fields, from biomedicine to particle physics to economics and climate science1,2. The fundamental prediction task of filling in missing values of a label column based on the rest of the columns is essential for various applications as diverse as biomedical risk models, drug discovery and materials science. Although deep learning has revolutionized learning from raw data and led to numerous high-profile success stories3--5, gradient-boosted decision trees6--9 have dominated tabular data for the past 20\,years. Here we present the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model that outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time. In 2.8\,s, TabPFN outperforms an ensemble of the strongest baselines tuned for 4\,h in a classification setting. As a generative transformer-based foundation model, this model also allows fine-tuning, data generation, density estimation and learning reusable embeddings. TabPFN is a~learning algorithm that is itself learned across millions of synthetic datasets, demonstrating the power of this approach for algorithm development. By improving modelling abilities across diverse fields, TabPFN has the potential to accelerate scientific discovery and enhance important decision-making in various domains.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science,Scientific data,Software,Statistics},
  file = {/Users/jingang/Zotero/storage/RU3XG7N8/Hollmann et al. - 2025 - Accurate predictions on small data with a tabular foundation model.pdf}
}

@article{hollmannTabpfnTransformerThat2022,
  title = {Tabpfn: {{A}} Transformer That Solves Small Tabular Classification Problems in a Second},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.01848},
  eprint = {2207.01848},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/A transformer that solves small tabular classification problems in a second.pdf}
}

@inproceedings{hollPhiflowDifferentiablePde2020,
  title = {Phiflow: {{A}} Differentiable Pde Solving Framework for Deep Learning via Physical Simulations},
  booktitle = {{{NeurIPS Workshop}}},
  author = {Holl, Philipp and Koltun, Vladlen and Um, Kiwon and Thuerey, Nils},
  year = {2020},
  volume = {2}
}

@misc{holzmullerBetterDefaultStrong2024,
  title = {Better by {{Default}}: {{Strong Pre-Tuned MLPs}} and {{Boosted Trees}} on {{Tabular Data}}},
  shorttitle = {Better by {{Default}}},
  author = {Holzm{\"u}ller, David and Grinsztajn, L{\'e}o and Steinwart, Ingo},
  year = {2024},
  month = oct,
  number = {arXiv:2407.04491},
  eprint = {2407.04491},
  publisher = {arXiv},
  urldate = {2024-11-05},
  abstract = {For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K--500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/T6VGHSZ5/Holzmüller et al. - 2024 - Better by Default Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data.pdf;/Users/jingang/Zotero/storage/XMEEHJFR/2407.html}
}

@article{hongVaporLiquidEquilibrium1988,
  title = {Vapor---Liquid Equilibrium Studies for the Carbon Dioxide---Methanol System},
  author = {Hong, Jane H and Kobayashi, Riki},
  year = {1988},
  journal = {Fluid Phase Equilibria},
  volume = {41},
  number = {3},
  pages = {269--276},
  publisher = {Elsevier},
  issn = {0378-3812}
}

@article{hongVaporLiquidEquilibrium1988a,
  title = {Vapor---Liquid Equilibrium Studies for the Carbon Dioxide---Methanol System},
  author = {Hong, Jane H and Kobayashi, Riki},
  year = {1988},
  journal = {Fluid Phase Equilibria},
  volume = {41},
  number = {3},
  pages = {269--276},
  publisher = {Elsevier},
  issn = {0378-3812}
}

@misc{horaceheFunctorchJAXlikeComposable2021,
  title = {Functorch: {{JAX-like}} Composable Function Transforms for {{PyTorch}}},
  author = {Horace He, Richard Zou},
  year = {2021}
}

@article{hoRandomSubspaceMethod1998,
  title = {The Random Subspace Method for Constructing Decision Forests},
  author = {Ho, Tin Kam},
  year = {1998},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {20},
  number = {8},
  pages = {832--844},
  publisher = {Ieee},
  isbn = {0162-8828}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  year = {1991},
  journal = {Neural networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  publisher = {Elsevier}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  journal = {Neural networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  publisher = {Elsevier}
}

@article{hospedalesMetalearningNeuralNetworks2021,
  title = {Meta-Learning in Neural Networks: {{A}} Survey},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2021},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {44},
  number = {9},
  pages = {5149--5169},
  publisher = {IEEE},
  isbn = {0162-8828}
}

@inproceedings{huangArbitraryStyleTransfer2017,
  title = {Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Huang, Xun and Belongie, Serge},
  year = {2017},
  pages = {1501--1510},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization.pdf}
}

@article{huangEquationStateSmall1990,
  title = {Equation of State for Small, Large, Polydisperse, and Associating Molecules},
  author = {Huang, Stanley H and Radosz, Maciej},
  year = {1990},
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {29},
  number = {11},
  pages = {2284--2294},
  publisher = {ACS Publications}
}

@inproceedings{huangFsdrFrequencySpace2021,
  title = {Fsdr: {{Frequency}} Space Domain Randomization for Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Jiaxing and Guan, Dayan and Xiao, Aoran and Lu, Shijian},
  year = {2021},
  pages = {6891--6902},
  annotation = {GSCC: 0000060}
}

@inproceedings{huangSelfchallengingImprovesCrossdomain2020,
  title = {Self-Challenging Improves Cross-Domain Generalization},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Huang, Zeyi and Wang, Haohan and Xing, Eric P. and Huang, Dong},
  year = {2020},
  pages = {124--140},
  publisher = {Springer},
  annotation = {GSCC: 0000220},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Gradient operation/Self-Challenging Improves Cross-Domain.pdf}
}

@article{huangTabtransformerTabularData2020,
  title = {Tabtransformer: {{Tabular}} Data Modeling Using Contextual Embeddings},
  author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  year = {2020},
  journal = {arXiv preprint arXiv:2012.06678},
  eprint = {2012.06678},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/TabTransformer- Tabular Data Modeling Using Contextual Embeddings.pdf}
}

@misc{huangTabTransformerTabularData2020a,
  title = {{{TabTransformer}}: {{Tabular Data Modeling Using Contextual Embeddings}}},
  shorttitle = {{{TabTransformer}}},
  author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  year = {2020},
  month = dec,
  number = {arXiv:2012.06678},
  eprint = {2012.06678},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.06678},
  urldate = {2024-08-12},
  abstract = {We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0\% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1\% AUC lift over the state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/HUGHXPWF/Huang et al. - 2020 - TabTransformer Tabular Data Modeling Using Contex.pdf;/Users/jingang/Zotero/storage/9D3AWL78/2012.html}
}

@inproceedings{hutterSequentialModelbasedOptimization2011,
  title = {Sequential Model-Based Optimization for General Algorithm Configuration},
  booktitle = {International Conference on Learning and Intelligent Optimization},
  author = {Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2011},
  pages = {507--523},
  publisher = {Springer}
}

@misc{iidaTABBIEPretrainedRepresentations2021,
  title = {{{TABBIE}}: {{Pretrained Representations}} of {{Tabular Data}}},
  shorttitle = {{{TABBIE}}},
  author = {Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
  year = {2021},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-08-18},
  abstract = {Existing work on tabular representation learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model's learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.},
  howpublished = {https://arxiv.org/abs/2105.02584v1},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/5XB9PCMD/Iida et al. - 2021 - TABBIE Pretrained Representations of Tabular Data.pdf}
}

@inproceedings{ilseDivaDomainInvariant2020,
  title = {Diva: {{Domain}} Invariant Variational Autoencoders},
  booktitle = {Medical {{Imaging}} with {{Deep Learning}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M and Louizos, Christos and Welling, Max},
  year = {2020},
  pages = {322--348},
  publisher = {PMLR},
  isbn = {2640-3498},
  annotation = {GSCC: 0000105},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/VAE/DIVA Domain Invariant Variational Autoencoders.pdf}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch Normalization: {{Accelerating}} Deep Network Training by Reducing Internal Covariate Shift},
  booktitle = {International Conference on Machine Learning},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  pages = {448--456},
  publisher = {PMLR}
}

@misc{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = feb,
  number = {arXiv:1803.05407},
  eprint = {1803.05407},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.05407},
  urldate = {2024-08-03},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/GXJQV9FU/Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better.pdf;/Users/jingang/Zotero/storage/572YAYJM/1803.html}
}

@article{jacksonThermodynamicConsistencyTests1995,
  title = {Thermodynamic Consistency Tests Based on the {{Gibbs-Duhem}} Equation Applied to Isothermal, Binary Vapor-Liquid Equilibrium Data: Data Evaluation and Model Testing},
  author = {Jackson, Philip L and Wilsak, Richard A},
  year = {1995},
  journal = {Fluid Phase Equilibria},
  volume = {103},
  number = {2},
  pages = {155--197},
  publisher = {Elsevier},
  issn = {0378-3812}
}

@article{jacobsAdaptiveMixturesLocal1991,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = {1991},
  journal = {Neural computation},
  volume = {3},
  number = {1},
  pages = {79--87},
  publisher = {MIT Press},
  isbn = {0899-7667},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/History/Adaptive mixtures of local experts.pdf}
}

@article{jaegerEhrenfestClassificationPhase1998,
  title = {The {{Ehrenfest}} Classification of Phase Transitions: Introduction and Evolution},
  author = {Jaeger, Gregg},
  year = {1998},
  journal = {Archive for history of exact sciences},
  volume = {53},
  pages = {51--81},
  publisher = {Springer},
  issn = {0003-9519}
}

@inproceedings{jamiesonNonstochasticBestArm2016,
  title = {Non-Stochastic Best Arm Identification and Hyperparameter Optimization},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Jamieson, Kevin and Talwalkar, Ameet},
  year = {2016},
  pages = {240--248},
  publisher = {PMLR}
}

@misc{jaszczurSparseEnoughScaling2021,
  title = {Sparse Is {{Enough}} in {{Scaling Transformers}}},
  author = {Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, {\L}ukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  year = {2021},
  month = nov,
  number = {arXiv:2111.12763},
  eprint = {2111.12763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.12763},
  urldate = {2024-06-20},
  abstract = {Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/FJ85K43G/Jaszczur et al. - 2021 - Sparse is Enough in Scaling Transformers.pdf;/Users/jingang/Zotero/storage/V7FLXT9Z/2111.html}
}

@article{jiangMixtralExperts2024,
  title = {Mixtral of Experts},
  author = {Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian},
  year = {2024},
  journal = {arXiv preprint arXiv:2401.04088},
  eprint = {2401.04088},
  archiveprefix = {arXiv}
}

@misc{jiangProtoGatePrototypebasedNeural2024,
  title = {{{ProtoGate}}: {{Prototype-based Neural Networks}} with {{Global-to-local Feature Selection}} for {{Tabular Biomedical Data}}},
  shorttitle = {{{ProtoGate}}},
  author = {Jiang, Xiangjian and Margeloiu, Andrei and Simidjievski, Nikola and Jamnik, Mateja},
  year = {2024},
  month = jun,
  number = {arXiv:2306.12330},
  eprint = {2306.12330},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.12330},
  urldate = {2024-12-17},
  abstract = {Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size (HDLSS). Previous research has attempted to address these challenges via local feature selection, but existing approaches often fail to achieve optimal performance due to their limitation in identifying globally important features and their susceptibility to the co-adaptation problem. In this paper, we propose ProtoGate, a prototype-based neural model for feature selection on HDLSS data. ProtoGate first selects instance-wise features via adaptively balancing global and local feature selection. Furthermore, ProtoGate employs a non-parametric prototype-based prediction mechanism to tackle the co-adaptation problem, ensuring the feature selection results and predictions are consistent with underlying data clusters. We conduct comprehensive experiments to evaluate the performance and interpretability of ProtoGate on synthetic and real-world datasets. The results show that ProtoGate generally outperforms state-of-the-art methods in prediction accuracy by a clear margin while providing high-fidelity feature selection and explainable predictions. Code is available at https://github.com/SilenceX12138/ProtoGate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/MHHFSDSJ/Jiang et al. - 2024 - ProtoGate Prototype-based Neural Networks with Global-to-local Feature Selection for Tabular Biomed.pdf;/Users/jingang/Zotero/storage/SYBZ9HLG/2306.html}
}

@article{johnsonMIMICIIIFreelyAccessible2016,
  title = {{{MIMIC-III}}, a Freely Accessible Critical Care Database},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
  year = {2016},
  month = may,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160035},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.35},
  urldate = {2025-01-05},
  abstract = {MIMIC-III (`Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Health care,Medical research,Outcomes research,Prognosis},
  file = {/Users/jingang/Zotero/storage/MWZZ2IVV/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care database.pdf}
}

@article{jordanHierarchicalMixturesExperts1994,
  title = {Hierarchical Mixtures of Experts and the {{EM}} Algorithm},
  author = {Jordan, Michael I. and Jacobs, Robert A.},
  year = {1994},
  journal = {Neural computation},
  volume = {6},
  number = {2},
  pages = {181--214},
  publisher = {MIT Press},
  isbn = {0899-7667},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/History/Hierarchical mixtures of experts and the EM algorithm.pdf}
}

@article{jordanMachineLearningTrends2015,
  title = {Machine Learning: {{Trends}}, Perspectives, and Prospects},
  author = {Jordan, Michael I and Mitchell, Tom M},
  year = {2015},
  journal = {Science},
  volume = {349},
  number = {6245},
  pages = {255--260},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075}
}

@article{joungMeasurementsCorrelationHighpressure2001,
  title = {Measurements and Correlation of High-Pressure {{VLE}} of Binary {{CO2}}--Alcohol Systems (Methanol, Ethanol, 2-Methoxyethanol and 2-Ethoxyethanol)},
  author = {Joung, Seung Nam and Yoo, Chang Woo and Shin, Hun Yong and Kim, Sun Young and Yoo, Ki-Pung and Lee, Chul Soo and Huh, Wan Soo},
  year = {2001},
  journal = {Fluid Phase Equilibria},
  volume = {185},
  number = {1-2},
  pages = {219--230},
  publisher = {Elsevier},
  issn = {0378-3812}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and others},
  year = {2021},
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  file = {/Users/jingang/Dropbox/References/Deep learning/Highly accurate protein structure prediction.pdf}
}

@article{kashinathFastAlgorithmCalculating2018,
  title = {A Fast Algorithm for Calculating Isothermal Phase Behavior Using Machine Learning},
  author = {Kashinath, A. and Szulczewski, M.L. and Dogru, A.H.},
  year = {2018},
  month = jun,
  journal = {Fluid Phase Equilibria},
  volume = {465},
  pages = {73--82},
  issn = {03783812},
  doi = {10.1016/j.fluid.2018.02.004},
  urldate = {2021-11-25},
  abstract = {Compositional models are frequently used to describe fluids in petroleum reservoir simulation, particularly for simulations of enhanced oil recovery. While compositional models are more accurate than black oil models, they incur a larger computational cost, in part, due to more complex phase-equilibrium calculations and can result in longer run times. Here, we develop an algorithm to reduce the cost of phase-equilibrium calculations for compositional models by applying two machine learning techniques: relevance vector machines and artificial neural networks. We test the algorithm on three fluid data sets and find a speedup of over 20\% with an error of 0.01\%, and a speedup of over 90\% with a maximum error of 5\%. These results suggest that the algorithm can be used to reduce the overall run time of compositional reservoir simulations with a small impact on accuracy.},
  langid = {english},
  file = {/Users/jingang/Dropbox/References/Accelerate flash via machine learning/A fast algorithm for calculating isothermal phase behavior using.pdf}
}

@article{katayamaIsothermalVaporliquidEquilibria1975,
  title = {Isothermal Vapor-Liquid Equilibria of Acetone-Carbon Dioxide and Methanol-Carbon Dioxide Systems at High Pressures},
  author = {Katayama, Takashi and Ohgaki, Kazunari and Maekawa, Goro and Goto, Motojiro and Nagano, Tamon},
  year = {1975},
  journal = {Journal of Chemical Engineering of Japan},
  volume = {8},
  number = {2},
  pages = {89--92},
  publisher = {The Society of Chemical Engineers, Japan},
  issn = {0021-9592}
}

@book{kelleyIterativeMethodsLinear1995,
  title = {Iterative Methods for Linear and Nonlinear Equations},
  author = {Kelley, Carl T.},
  year = {1995},
  publisher = {SIAM},
  isbn = {0-89871-352-8}
}

@article{keskarLargebatchTrainingDeep2016,
  title = {On Large-Batch Training for Deep Learning: {{Generalization}} Gap and Sharp Minima},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.04836},
  eprint = {1609.04836},
  archiveprefix = {arXiv}
}

@inproceedings{khaitGpuoffloadedGeneralPurpose2017,
  title = {Gpu-Offloaded General Purpose Simulator for Multiphase Flow in Porous Media},
  booktitle = {{{SPE Reservoir Simulation Conference}}},
  author = {Khait, Mark and Voskov, Denis},
  year = {2017},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/GPU-Offloaded General Purpose Simulator for Multiphase Flow in Porous Media.pdf}
}

@inproceedings{khemakhemVariationalAutoencodersNonlinear2020,
  title = {Variational Autoencoders and Nonlinear Ica: {{A}} Unifying Framework},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  year = {2020},
  pages = {2207--2217},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/iVAE Variational Autoencoders and Nonlinear ICA.pdf}
}

@inproceedings{khoslaUndoingDamageDataset2012,
  title = {Undoing the Damage of Dataset Bias},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A. and Torralba, Antonio},
  year = {2012},
  pages = {158--171},
  publisher = {Springer},
  annotation = {GSCC: 0000484}
}

@misc{kimCARTEPretrainingTransfer2024,
  title = {{{CARTE}}: {{Pretraining}} and {{Transfer}} for {{Tabular Learning}}},
  shorttitle = {{{CARTE}}},
  author = {Kim, Myung Jun and Grinsztajn, L{\'e}o and Varoquaux, Ga{\"e}l},
  year = {2024},
  month = may,
  number = {arXiv:2402.16785},
  eprint = {2402.16785},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {Pretrained deep-learning models are the go-to solution for images or text. However, for tabular data the standard is still to train tree-based models. Indeed, transfer learning on tables hits the challenge of data integration: finding correspondences, correspondences in the entries (entity matching) where different words may denote the same entity, correspondences across columns (schema matching), which may come in different orders, names... We propose a neural architecture that does not need such correspondences. As a result, we can pretrain it on background data that has not been matched. The architecture -- CARTE for Context Aware Representation of Table Entries -- uses a graph representation of tabular (or relational) data to process tables with different columns, string embedding of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries. An extensive benchmark shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models. CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones. CARTE opens the door to large pretrained models for tabular data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/XVSIHTQJ/Kim et al. - 2024 - CARTE Pretraining and Transfer for Tabular Learni.pdf;/Users/jingang/Zotero/storage/V9R44ZEN/2402.html}
}

@inproceedings{kimSelfregSelfsupervisedContrastive2021,
  title = {Selfreg: {{Self-supervised}} Contrastive Regularization for Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Kim, Daehee and Yoo, Youngjun and Park, Seunghyun and Kim, Jinkyu and Lee, Jaekoo},
  year = {2021},
  pages = {9619--9628},
  annotation = {GSCC: 0000048},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Self-supervised learning/SelfReg Self-supervised Contrastive Regularization for Domain Generalization.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  archiveprefix = {arXiv}
}

@article{kingmaAutoencodingVariationalBayes2013,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P and Welling, Max},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6114},
  eprint = {1312.6114},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Auto-Encoding Variational Bayes.pdf}
}

@article{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative}} Flow with Invertible 1x1 Convolutions},
  author = {Kingma, Durk P and Dhariwal, Prafulla},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Normalizing Flow/Kingma_Dhariwal_2018_Glow.pdf}
}

@misc{kingmaVariationalDropoutLocal2015,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
  year = {2015},
  month = dec,
  number = {arXiv:1506.02557},
  eprint = {1506.02557},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02557},
  urldate = {2024-06-12},
  abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/7BJCFGI5/Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf;/Users/jingang/Zotero/storage/GDJHDS2C/1506.html}
}

@inproceedings{klambauerSelfnormalizingNeuralNetworks2017,
  title = {Self-Normalizing Neural Networks},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  pages = {972--981}
}

@book{kleinbaumLogisticRegression2002,
  title = {Logistic Regression},
  author = {Kleinbaum, David G. and Dietz, K. and Gail, M. and Klein, Mitchel and Klein, Mitchell},
  year = {2002},
  publisher = {Springer},
  isbn = {0-387-95397-3}
}

@article{kochkovMachineLearningAccelerated2021,
  title = {Machine Learning--Accelerated Computational Fluid Dynamics},
  author = {Kochkov, Dmitrii and Smith, Jamie A. and Alieva, Ayya and Wang, Qing and Brenner, Michael P. and Hoyer, Stephan},
  year = {2021},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {21},
  pages = {e2101784118},
  publisher = {National Acad Sciences},
  isbn = {0027-8424}
}

@inproceedings{kohWildsBenchmarkInthewild2021,
  title = {Wilds: {{A}} Benchmark of in-the-Wild Distribution Shifts},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena},
  year = {2021},
  pages = {5637--5664},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Dataset and Benchmark/A Benchmark of in-the-Wild Distribution Shifts.pdf}
}

@article{kontogeorgisEquationsStateThree2020,
  title = {Equations of State in Three Centuries. {{Are}} We Closer to Arriving to a Single Model for All Applications?},
  author = {Kontogeorgis, Georgios M. and Liang, Xiaodong and Arya, Alay and Tsivintzelis, Ioannis},
  year = {2020},
  journal = {Chemical Engineering Science: X},
  volume = {7},
  pages = {100060},
  publisher = {Elsevier},
  isbn = {2590-1400},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/EoS/Equations of state in three centuries.pdf}
}

@article{kontogeorgisEquationStateAssociating1996,
  title = {An Equation of State for Associating Fluids},
  author = {Kontogeorgis, Georgios M and Voutsas, Epaminondas C and Yakoumis, Iakovos V and Tassios, Dimitrios P},
  year = {1996},
  journal = {Industrial \& engineering chemistry research},
  volume = {35},
  number = {11},
  pages = {4310--4318},
  publisher = {ACS Publications}
}

@article{kontogeorgisMulticomponentPhaseEquilibrium1999,
  title = {Multicomponent Phase Equilibrium Calculations for Water--Methanol--Alkane Mixtures},
  author = {Kontogeorgis, Georgios M and Yakoumis, Iakovos V and Meijer, Henk and Hendriks, Eric and Moorwood, Tony},
  year = {1999},
  journal = {Fluid Phase Equilibria},
  volume = {158},
  pages = {201--209},
  publisher = {Elsevier}
}

@book{kontogeorgisThermodynamicModelsIndustrial2009,
  title = {Thermodynamic Models for Industrial Applications: From Classical and Advanced Mixing Rules to Association Theories},
  author = {Kontogeorgis, Georgios M and Folas, Georgios K},
  year = {2009},
  publisher = {John Wiley \& Sons}
}

@article{koshilLocalizationDataEmbedding,
  title = {Towards {{Localization}} via {{Data Embedding}} for {{TabPFN}}},
  author = {Koshil, Mykhailo and Nagler, Thomas and Feurer, Matthias and Eggensperger, Katharina},
  abstract = {In-context learning (ICL) using Prior-data fitted networks (PFNs) like TabPFN has shown significant promise in supervised tabular learning tasks. However, scalability is limited by the quadratic complexity of the transformer architecture's attention across training points provided as context. A recent theoretical analysis suggested localization to overcome this issue. In this work, we propose LETabPFN implementing a new localization method that performs nearest neighbor selection using the model's learned internal representations. We evaluate LETabPFN across six datasets, demonstrating superior performance over standard TabPFN when scaling to larger datasets. We also explore design choices and analyze the bias-variance trade-off, showing that it desirably reduces bias while maintaining manageable variance. This work opens up a pathway for scaling TabPFN and ICL methods in general to arbitrarily large tabular datasets.},
  langid = {english},
  year={2024},
  file = {/Users/jingang/Zotero/storage/B7EA28PT/Koshil et al. - Towards Localization via Data Embedding for TabPFN.pdf}
}

@article{kossenSelfattentionDatapointsGoing2021,
  title = {Self-Attention between Datapoints: {{Going}} beyond Individual Input-Output Pairs in Deep Learning},
  author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {28742--28756},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Self-Attention Between Datapoints- Going Beyond Individual Input-Output Pairs in Deep Learning.pdf;/Users/jingang/Dropbox/References/Tabular learning/Self-Attention Between Datapoints- Going Beyond Individual Input-Output Pairs in Deep Learning.pdf}
}

@inproceedings{kotelnikovTabddpmModellingTabular2023,
  title = {Tabddpm: {{Modelling}} Tabular Data with Diffusion Models},
  shorttitle = {Tabddpm},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kotelnikov, Akim and Baranchuk, Dmitry and Rubachev, Ivan and Babenko, Artem},
  year = {2023},
  pages = {17564--17579},
  publisher = {PMLR},
  urldate = {2024-06-04},
  file = {/Users/jingang/Zotero/storage/9RUPBLE8/Kotelnikov et al. - 2023 - Tabddpm Modelling tabular data with diffusion mod.pdf}
}

@book{krantzImplicitFunctionTheorem2002,
  title = {The Implicit Function Theorem: History, Theory, and Applications},
  author = {Krantz, Steven George and Parks, Harold R},
  year = {2002},
  publisher = {Springer Science \& Business Media}
}

@inproceedings{kruegerOutofdistributionGeneralizationRisk2021,
  title = {Out-of-Distribution Generalization via Risk Extrapolation (Rex)},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Le Priol, Remi and Courville, Aaron},
  year = {2021},
  pages = {5815--5826},
  publisher = {PMLR},
  isbn = {2640-3498},
  annotation = {GSCC: 0000267},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Domain-invariant representation learning/Out-of-distribution generalization via risk extrapolation (rex).pdf}
}

@inproceedings{kubatAddressingCurseImbalanced1997,
  title = {Addressing the Curse of Imbalanced Training Sets: One-Sided Selection},
  booktitle = {Icml},
  author = {Kubat, Miroslav and Matwin, Stan},
  year = {1997},
  volume = {97},
  pages = {179},
  publisher = {Nashville, USA}
}

@article{kunzGERG2008WiderangeEquation2012,
  title = {The {{GERG-2008}} Wide-Range Equation of State for Natural Gases and Other Mixtures: An Expansion of {{GERG-2004}}},
  author = {Kunz, Oliver and Wagner, Wolfgang},
  year = {2012},
  journal = {Journal of chemical \& engineering data},
  volume = {57},
  number = {11},
  pages = {3032--3091},
  publisher = {ACS Publications},
  isbn = {0021-9568},
  annotation = {GSCC: 0001157}
}

@inproceedings{kurscheidtLostTranslationModern,
  title = {Lost in {{Translation}}: {{Modern Image Classifiers}} Still Degrade Even under Simple {{Translations}}},
  booktitle = {{{ICML}} 2022 {{Shift Happens Workshop}}},
  author = {Kurscheidt, Leander and Hein, Matthias},
  annotation = {GSCC: 0000000},
  file = {/home/jingang/Documents/References/Domain generalization/problems and Failures/Lost in Translation Modern Image Classifiers still degrade even under simple Translations.pdf}
}

@article{lafitteAccurateStatisticalAssociating2013,
  title = {Accurate Statistical Associating Fluid Theory for Chain Molecules Formed from {{Mie}} Segments},
  author = {Lafitte, Thomas and Apostolakou, Anastasia and Avenda{\~n}o, Carlos and Galindo, Amparo and Adjiman, Claire S. and M{\"u}ller, Erich A. and Jackson, George},
  year = {2013},
  journal = {The Journal of chemical physics},
  volume = {139},
  number = {15},
  pages = {154504},
  publisher = {American Institute of Physics},
  isbn = {0021-9606}
}

@article{lambContextualHyperNetworksNovel2021,
  title = {Contextual {{HyperNetworks}} for {{Novel Feature Adaptation}}},
  author = {Lamb, Angus and Saveliev, Evgeny and Li, Yingzhen and Tschiatschek, Sebastian and Longden, Camilla and Woodhead, Simon and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Turner, Richard E. and Cameron, Pashmina and Zhang, Cheng},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.05860},
  eprint = {2104.05860},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Contextual HyperNetworks for Novel Feature Adaptation.pdf}
}

@inproceedings{laurikkalaImprovingIdentificationDifficult2001,
  title = {Improving Identification of Difficult Small Classes by Balancing Class Distribution},
  booktitle = {Conference on Artificial Intelligence in Medicine in {{Europe}}},
  author = {Laurikkala, Jorma},
  year = {2001},
  pages = {63--66},
  publisher = {Springer}
}

@article{laursenVLEVLLEMeasurements2002,
  title = {{{VLE}} and {{VLLE}} Measurements of Dimethyl Ether Containing Systems},
  author = {Laursen, Torben and Rasmussen, Peter and Andersen, Simon Ivar},
  year = {2002},
  journal = {Journal of Chemical \& Engineering Data},
  volume = {47},
  number = {2},
  pages = {198--202},
  publisher = {ACS Publications},
  issn = {0021-9568}
}

@article{le-khacContrastiveRepresentationLearning2020,
  title = {Contrastive Representation Learning: {{A}} Framework and Review},
  author = {{Le-Khac}, Phuc H. and Healy, Graham and Smeaton, Alan F.},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {193907--193934},
  publisher = {IEEE},
  isbn = {2169-3536}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation Applied to Handwritten Zip Code Recognition},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  year = {1989},
  journal = {Neural computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  publisher = {MIT Press},
  isbn = {0899-7667}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group}
}

@incollection{lecunEfficientBackprop2012,
  title = {Efficient Backprop},
  booktitle = {Neural Networks: {{Tricks}} of the Trade},
  author = {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  year = {2012},
  pages = {9--48},
  publisher = {Springer}
}

@misc{leeBinningPretextTask2024,
  title = {Binning as a {{Pretext Task}}: {{Improving Self-Supervised Learning}} in {{Tabular Domains}}},
  shorttitle = {Binning as a {{Pretext Task}}},
  author = {Lee, Kyungeun and Sim, Ye Seul and Cho, Hye-Seung and Eo, Moonjung and Yoon, Suhee and Yoon, Sanghyu and Lim, Woohyung},
  year = {2024},
  month = may,
  number = {arXiv:2405.07414},
  eprint = {2405.07414},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {The ability of deep networks to learn superior representations hinges on leveraging the proper inductive biases, considering the inherent properties of datasets. In tabular domains, it is critical to effectively handle heterogeneous features (both categorical and numerical) in a unified manner and to grasp irregular functions like piecewise constant functions. To address the challenges in the self-supervised learning framework, we propose a novel pretext task based on the classical binning method. The idea is straightforward: reconstructing the bin indices (either orders or classes) rather than the original values. This pretext task provides the encoder with an inductive bias to capture the irregular dependencies, mapping from continuous inputs to discretized bins, and mitigates the feature heterogeneity by setting all features to have category-type targets. Our empirical investigations ascertain several advantages of binning: capturing the irregular function, compatibility with encoder architecture and additional modifications, standardizing all features into equal sets, grouping similar values within a feature, and providing ordering information. Comprehensive evaluations across diverse tabular datasets corroborate that our method consistently improves tabular representation learning performance for a wide range of downstream tasks. The codes are available in https://github.com/kyungeun-lee/tabularbinning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/635KZ48U/Lee et al. - 2024 - Binning as a Pretext Task Improving Self-Supervis.pdf;/Users/jingang/Zotero/storage/9GELDY3I/2405.html}
}

@misc{leeSetTransformerFramework2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
  year = {2019},
  month = may,
  number = {arXiv:1810.00825},
  eprint = {1810.00825},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.00825},
  urldate = {2024-08-21},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/YS7IL7MP/Lee et al. - 2019 - Set Transformer A Framework for Attention-based Permutation-Invariant Neural Networks.pdf;/Users/jingang/Zotero/storage/8SS3KKI6/1810.html}
}

@article{leiboviciNewLookRachfordRice1992,
  title = {A New Look at the {{Rachford-Rice}} Equation},
  author = {Leibovici, {\relax ClaudeF}. and Neoschil, Jean},
  year = {1992},
  month = jul,
  journal = {Fluid Phase Equilibria},
  volume = {74},
  pages = {303--308},
  issn = {03783812},
  doi = {10.1016/0378-3812(92)85069-K},
  urldate = {2021-12-09},
  abstract = {Leibovici, C.F. and Neoschil, J., 1992. A new look at the Rachford-Rice equation. Fluid Phase Equilibria, 74: 303-308.},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/QAUFBXWW/Leibovici and Neoschil - 1992 - A new look at the Rachford-Rice equation.pdf}
}

@article{lemorvanNeuMissNetworksDifferentiable2020,
  title = {{{NeuMiss}} Networks: Differentiable Programming for Supervised Learning with Missing Values.},
  author = {Le Morvan, Marine and Josse, Julie and Moreau, Thomas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {5980--5990},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Missingness/NeuMissnetworks.pdf}
}

@article{lemorvanWhatSaGood2021,
  title = {What'sa Good Imputation to Predict with Missing Values?},
  author = {Le Morvan, Marine and Josse, Julie and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {11530--11540},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Missingness/What’sagoodimputationtopredictwithmissing values?.pdf}
}

@inproceedings{leontidisSensitivityImpuritiesCO22023,
  title = {Sensitivity of {{Impurities}} to {{CO2 Flow}} in {{Injection Wells}}},
  booktitle = {{{ARMA}}/{{DGS}}/{{SEG International Geomechanics Symposium}}},
  author = {Leontidis, V and Voronetska, K},
  year = {2023},
  pages = {ARMA-IGS},
  publisher = {ARMA}
}

@book{LeoThesis,
  title = {Leo {{Thesis}}},
  file = {/Users/jingang/Zotero/storage/PPU442QV/Leo Thesis.pdf}
}

@article{lepikhinGshardScalingGiant2020,
  title = {Gshard: {{Scaling}} Giant Models with Conditional Computation and Automatic Sharding},
  author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  year = {2020},
  journal = {arXiv preprint arXiv:2006.16668},
  eprint = {2006.16668},
  archiveprefix = {arXiv}
}

@article{leuEquilibriumPhaseProperties1991,
  title = {The Equilibrium Phase Properties of (Carbon Dioxide+ Methanol)},
  author = {Leu, Ah-Dong and Chung, Samuel Y-K and Robinson, Donald B},
  year = {1991},
  journal = {The Journal of Chemical Thermodynamics},
  volume = {23},
  number = {10},
  pages = {979--985},
  publisher = {Elsevier},
  issn = {0021-9614}
}

@misc{levinTransferLearningDeep2023,
  title = {Transfer {{Learning}} with {{Deep Tabular Models}}},
  author = {Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C. Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  year = {2023},
  month = aug,
  number = {arXiv:2206.15306},
  eprint = {2206.15306},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.15306},
  urldate = {2024-08-19},
  abstract = {Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they learn reusable features and are easily fine-tuned in new domains. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we demonstrate that upstream data gives tabular neural networks a decisive advantage over widely used GBDT models. We propose a realistic medical diagnosis benchmark for tabular transfer learning, and we present a how-to guide for using upstream data to boost performance with a variety of tabular neural network architectures. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications. Our code is available at https://github.com/LevinRoman/tabular-transfer-learning .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/5L67WH27/Levin et al. - 2023 - Transfer Learning with Deep Tabular Models.pdf;/Users/jingang/Zotero/storage/VRJQA72S/2206.html}
}

@inproceedings{lewisBaseLayersSimplifying2021,
  title = {Base Layers: {{Simplifying}} Training of Large, Sparse Models},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  year = {2021},
  pages = {6265--6274},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Base layers Simplifying training of large, sparse models.pdf}
}

@article{liAccelerationNVTFlash2019,
  title = {Acceleration of the {{NVT}} Flash Calculation for Multicomponent Mixtures Using Deep Neural Network Models},
  author = {Li, Yiteng and Zhang, Tao and Sun, Shuyu},
  year = {2019},
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {58},
  number = {27},
  pages = {12312--12322},
  publisher = {ACS Publications},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Acceleration of the NVT Flash Calculation for Multicomponent.pdf}
}

@misc{liaoTabGSLGraphStructure2023,
  title = {{{TabGSL}}: {{Graph Structure Learning}} for {{Tabular Data Prediction}}},
  shorttitle = {{{TabGSL}}},
  author = {Liao, Jay Chiehen and Li, Cheng-Te},
  year = {2023},
  month = may,
  number = {arXiv:2305.15843},
  eprint = {2305.15843},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15843},
  urldate = {2024-08-18},
  abstract = {This work presents a novel approach to tabular data prediction leveraging graph structure learning and graph neural networks. Despite the prevalence of tabular data in real-world applications, traditional deep learning methods often overlook the potentially valuable associations between data instances. Such associations can offer beneficial insights for classification tasks, as instances may exhibit similar patterns of correlations among features and target labels. This information can be exploited by graph neural networks, necessitating robust graph structures. However, existing studies primarily focus on improving graph structure from noisy data, largely neglecting the possibility of deriving graph structures from tabular data. We present a novel solution, Tabular Graph Structure Learning (TabGSL), to enhance tabular data prediction by simultaneously learning instance correlation and feature interaction within a unified framework. This is achieved through a proposed graph contrastive learning module, along with transformer-based feature extractor and graph neural network. Comprehensive experiments conducted on 30 benchmark tabular datasets demonstrate that TabGSL markedly outperforms both tree-based models and recent deep learning-based tabular models. Visualizations of the learned instance embeddings further substantiate the effectiveness of TabGSL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/jingang/Zotero/storage/Y5Z9IWJL/Liao and Li - 2023 - TabGSL Graph Structure Learning for Tabular Data Prediction.pdf;/Users/jingang/Zotero/storage/CBXVWZJ8/2305.html}
}

@article{liBranchtrainmergeEmbarrassinglyParallel2022,
  title = {Branch-Train-Merge: {{Embarrassingly}} Parallel Training of Expert Language Models},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2022},
  journal = {arXiv preprint arXiv:2208.03306},
  eprint = {2208.03306},
  archiveprefix = {arXiv}
}

@inproceedings{liDeepDomainGeneralization2018,
  title = {Deep Domain Generalization via Conditional Invariant Adversarial Networks},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
  year = {2018},
  pages = {624--639},
  annotation = {GSCC: 0000357}
}

@inproceedings{liDeeperBroaderArtier2017,
  title = {Deeper, Broader and Artier Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  pages = {5542--5550},
  annotation = {GSCC: 0000687}
}

@inproceedings{liDeeperBroaderArtier2017a,
  title = {Deeper, Broader and Artier Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  pages = {5542--5550}
}

@inproceedings{liDomainGeneralizationAdversarial2018,
  title = {Domain Generalization with Adversarial Feature Learning},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C},
  year = {2018},
  pages = {5400--5409},
  annotation = {GSCC: 0000608},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Domain-invariant representation learning/Domain Generalization with Adversarial Feature Learning.pdf}
}

@inproceedings{liDomainGeneralizationAdversarial2018a,
  title = {Domain Generalization with Adversarial Feature Learning},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C.},
  year = {2018},
  pages = {5400--5409},
  annotation = {GSCC: 0000608}
}

@misc{liDropKey2023,
  title = {{{DropKey}}},
  author = {Li, Bonan and Hu, Yinhan and Nie, Xuecheng and Han, Congying and Jiang, Xiangjian and Guo, Tiande and Liu, Luoqi},
  year = {2023},
  month = apr,
  number = {arXiv:2208.02646},
  eprint = {2208.02646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02646},
  urldate = {2024-06-12},
  abstract = {In this paper, we focus on analyzing and improving the dropout technique for self-attention layers of Vision Transformer, which is important while surprisingly ignored by prior works. In particular, we conduct researches on three core questions: First, what to drop in self-attention layers? Different from dropping attention weights in literature, we propose to move dropout operations forward ahead of attention matrix calculation and set the Key as the dropout unit, yielding a novel dropout-before-softmax scheme. We theoretically verify that this scheme helps keep both regularization and probability features of attention weights, alleviating the overfittings problem to specific patterns and enhancing the model to globally capture vital information; Second, how to schedule the drop ratio in consecutive layers? In contrast to exploit a constant drop ratio for all layers, we present a new decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. We experimentally validate the proposed schedule can avoid overfittings in low-level features and missing in high-level semantics, thus improving the robustness and stableness of model training; Third, whether need to perform structured dropout operation as CNN? We attempt patch-based block-version of dropout operation and find that this useful trick for CNN is not essential for ViT. Given exploration on the above three questions, we present the novel DropKey method that regards Key as the drop unit and exploits decreasing schedule for drop ratio, improving ViTs in a general way. Comprehensive experiments demonstrate the effectiveness of DropKey for various ViT architectures, e.g. T2T and VOLO, as well as for various vision tasks, e.g., image classification, object detection, human-object interaction detection and human body shape recovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/IICE53TV/Li et al. - 2023 - DropKey.pdf;/Users/jingang/Zotero/storage/8VTF62Q3/2208.html}
}

@inproceedings{liEpisodicTrainingDomain2019,
  title = {Episodic Training for Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Li, Da and Zhang, Jianshu and Yang, Yongxin and Liu, Cong and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2019},
  pages = {1446--1455},
  annotation = {GSCC: 0000279}
}

@article{liFourierNeuralOperator2020,
  title = {Fourier Neural Operator for Parametric Partial Differential Equations},
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.08895},
  eprint = {2010.08895},
  archiveprefix = {arXiv}
}

@misc{liHowNonlinearTransformers2024,
  title = {How {{Do Nonlinear Transformers Learn}} and {{Generalize}} in {{In-Context Learning}}?},
  author = {Li, Hongkang and Wang, Meng and Lu, Songtao and Cui, Xiaodong and Chen, Pin-Yu},
  year = {2024},
  month = jun,
  number = {arXiv:2402.15607},
  eprint = {2402.15607},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15607},
  urldate = {2024-08-30},
  abstract = {Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors on the ICL generalization performance on the remaining unseen tasks with and without data distribution shifts. We also analyze how different components in the learned Transformers contribute to the ICL performance. Furthermore, we provide the first theoretical analysis of how model pruning affects ICL performance and prove that proper magnitude-based pruning can have a minimal impact on ICL while reducing inference costs. These theoretical findings are justified through numerical experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/LLWUBJAW/Li et al. - 2024 - How Do Nonlinear Transformers Learn and Generalize in In-Context Learning.pdf;/Users/jingang/Zotero/storage/TYBXWXUI/2402.html}
}

@article{liHyperbandNovelBanditbased2017,
  title = {Hyperband: {{A}} Novel Bandit-Based Approach to Hyperparameter Optimization},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2017},
  journal = {The Journal of Machine Learning Research},
  volume = {18},
  number = {1},
  pages = {6765--6816},
  publisher = {JMLR. org},
  isbn = {1532-4435}
}

@article{likasGlobalKmeansClustering2003,
  title = {The Global K-Means Clustering Algorithm},
  author = {Likas, Aristidis and Vlassis, Nikos and Verbeek, Jakob J.},
  year = {2003},
  journal = {Pattern recognition},
  volume = {36},
  number = {2},
  pages = {451--461},
  publisher = {Elsevier},
  isbn = {0031-3203}
}

@article{liKohnShamEquationsRegularizer2021,
  title = {Kohn-{{Sham}} Equations as Regularizer: {{Building}} Prior Knowledge into Machine-Learned Physics},
  author = {Li, Li and Hoyer, Stephan and Pederson, Ryan and Sun, Ruoxi and Cubuk, Ekin D. and Riley, Patrick and Burke, Kieron},
  year = {2021},
  journal = {Physical review letters},
  volume = {126},
  number = {3},
  pages = {036401},
  publisher = {APS},
  file = {/Users/jingang/Zotero/storage/PNMVG25R/Li et al_2021_Kohn-Sham equations as regularizer.pdf}
}

@misc{liLearnableFourierFeatures2021,
  title = {Learnable {{Fourier Features}} for {{Multi-Dimensional Spatial Positional Encoding}}},
  author = {Li, Yang and Si, Si and Li, Gang and Hsieh, Cho-Jui and Bengio, Samy},
  year = {2021},
  month = nov,
  number = {arXiv:2106.02795},
  eprint = {2106.02795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.02795},
  urldate = {2024-08-25},
  abstract = {Attentional mechanisms are order-invariant. Positional encoding is a crucial component to allow attention-based deep model architectures such as Transformer to address sequences or images where the position of information matters. In this paper, we propose a novel positional encoding method based on learnable Fourier features. Instead of hard-coding each position as a token or a vector, we represent each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. The representation is particularly advantageous for a spatial multi-dimensional position, e.g., pixel positions on an image, where \$L\_2\$ distances or more complex positional relationships need to be captured. Our experiments based on several public benchmark tasks show that our learnable Fourier feature representation for multi-dimensional positional encoding outperforms existing methods by both improving the accuracy and allowing faster convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/H3SBZMA5/Li et al. - 2021 - Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding.pdf;/Users/jingang/Zotero/storage/WY36T46S/2106.html}
}

@inproceedings{liLearningGeneralizeMetalearning2018,
  title = {Learning to Generalize: {{Meta-learning}} for Domain Generalization},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy},
  year = {2018},
  volume = {32},
  isbn = {2374-3468},
  annotation = {GSCC: 0000658}
}

@article{linControllableParetoMultitask2020,
  title = {Controllable Pareto Multi-Task Learning},
  author = {Lin, Xi and Yang, Zhiyuan and Zhang, Qingfu and Kwong, Sam},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.06313},
  eprint = {2010.06313},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Controllable Pareto Multi-Task Learning.pdf}
}

@misc{linDualOperatingModes2024,
  title = {Dual {{Operating Modes}} of {{In-Context Learning}}},
  author = {Lin, Ziqian and Lee, Kangwook},
  year = {2024},
  month = aug,
  number = {arXiv:2402.18819},
  eprint = {2402.18819},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18819},
  urldate = {2025-01-02},
  abstract = {In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL. Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples. Our model offers a plausible explanation for this "early ascent" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels. Lastly, we validate our findings and predictions via experiments involving Transformers and large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/DML8VI22/Lin and Lee - 2024 - Dual Operating Modes of In-Context Learning.pdf;/Users/jingang/Zotero/storage/NH85E7DK/2402.html}
}

@inproceedings{liSemanticSegmentationGenerative2021,
  title = {Semantic Segmentation with Generative Models: {{Semi-supervised}} Learning and Strong out-of-Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Daiqing and Yang, Junlin and Kreis, Karsten and Torralba, Antonio and Fidler, Sanja},
  year = {2021},
  pages = {8300--8311},
  annotation = {GSCC: 0000049},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/VAE/Semantic segmentation with generative models Semi-supervised learning and strong out-of-domain generalization.pdf}
}

@inproceedings{liSimpleFeatureAugmentation2021,
  title = {A Simple Feature Augmentation for Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Li, Pan and Li, Da and Li, Wei and Gong, Shaogang and Fu, Yanwei and Hospedales, Timothy M.},
  year = {2021},
  pages = {8886--8895},
  annotation = {GSCC: 0000117}
}

@article{liSparseFusionMixtureofExperts2022,
  title = {Sparse {{Fusion Mixture-of-Experts}} Are {{Domain Generalizable Learners}}},
  author = {Li, Bo and Yang, Jingkang and Ren, Jiawei and Wang, Yezhen and Liu, Ziwei},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.04046},
  eprint = {2206.04046},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Others/SPARSE MIXTURE-OF-EXPERTS ARE DOMAIN GENERALIZABLE LEARNERS.pdf}
}

@article{liSystemMassivelyParallel2020,
  title = {A System for Massively Parallel Hyperparameter Tuning},
  author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and {Ben-Tzur}, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  year = {2020},
  journal = {Proceedings of Machine Learning and Systems},
  volume = {2},
  pages = {230--246}
}

@article{littwinInfinitewidthHypernetworks2020,
  title = {On Infinite-Width Hypernetworks},
  author = {Littwin, Etai and Galanti, Tomer and Wolf, Lior and Yang, Greg},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {13226--13237},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/On Infinite-Width Hypernetworks.pdf;/Users/jingang/Dropbox/References/Hypernetworks/On Infinite-Width Hypernetworks.pdf}
}

@misc{liuDynamicGroupTransformer2022,
  title = {Dynamic {{Group Transformer}}: {{A General Vision Transformer Backbone}} with {{Dynamic Group Attention}}},
  shorttitle = {Dynamic {{Group Transformer}}},
  author = {Liu, Kai and Wu, Tianyi and Liu, Cong and Guo, Guodong},
  year = {2022},
  month = may,
  number = {arXiv:2203.03937},
  eprint = {2203.03937},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.03937},
  urldate = {2024-09-04},
  abstract = {Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by each query attending to all keys/values, various methods have constrained the range of attention within local regions, where each query only attends to keys/values within a hand-crafted window. However, these hand-crafted window partition mechanisms are data-agnostic and ignore their input content, so it is likely that one query maybe attends to irrelevant keys/values. To address this issue, we propose a Dynamic Group Attention (DG-Attention), which dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group. Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention. Built on the DG-Attention, we develop a general vision transformer backbone named Dynamic Group Transformer (DGT). Extensive experiments show that our models can outperform the state-of-the-art methods on multiple common vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.0},
  file = {/Users/jingang/Zotero/storage/UVMH3UBQ/Liu et al. - 2022 - Dynamic Group Transformer A General Vision Transformer Backbone with Dynamic Group Attention.pdf;/Users/jingang/Zotero/storage/8R2P5YYU/2203.html}
}

@article{liuMachineLearningPredicting2019,
  title = {Machine Learning for Predicting Thermodynamic Properties of Pure Fluids and Their Mixtures},
  author = {Liu, Yuanbin and Hong, Weixiang and Cao, Bingyang},
  year = {2019},
  journal = {Energy},
  volume = {188},
  pages = {116091},
  publisher = {Elsevier},
  isbn = {0360-5442},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/ML EoS/Machine learning for predicting thermodynamic properties of pure fluids and their mixtures.pdf}
}

@article{liUncertaintyModelingOutofDistribution2022,
  title = {Uncertainty {{Modeling}} for {{Out-of-Distribution Generalization}}},
  author = {Li, Xiaotong and Dai, Yongxing and Ge, Yixiao and Liu, Jun and Shan, Ying and Duan, Ling-Yu},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.03958},
  eprint = {2202.03958},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/UNCERTAINTY MODELING FOR OUT-OF-DISTRIBUTION GENERALIZATION.pdf}
}

@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  pages = {10012--10022},
  file = {/Users/jingang/Dropbox/References/Miscellaneous/Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf}
}

@misc{liuSwinTransformerHierarchical2021a,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-09-05},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  howpublished = {https://arxiv.org/abs/2103.14030v2},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/6VYRR244/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf}
}

@misc{liuSwinTransformerV22021,
  title = {Swin {{Transformer V2}}: {{Scaling Up Capacity}} and {{Resolution}}},
  shorttitle = {Swin {{Transformer V2}}},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
  year = {2021},
  month = nov,
  journal = {arXiv.org},
  urldate = {2024-09-05},
  abstract = {Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\${\textbackslash}times\$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at {\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  howpublished = {https://arxiv.org/abs/2111.09883v2},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/94HAAR93/Liu et al. - 2021 - Swin Transformer V2 Scaling Up Capacity and Resolution.pdf}
}

@misc{liuTALENTTabularAnalytics2024,
  title = {{{TALENT}}: {{A Tabular Analytics}} and {{Learning Toolbox}}},
  shorttitle = {{{TALENT}}},
  author = {Liu, Si-Yang and Cai, Hao-Run and Zhou, Qi-Le and Ye, Han-Jia},
  year = {2024},
  month = jul,
  number = {arXiv:2407.04057},
  eprint = {2407.04057},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.04057},
  urldate = {2024-07-12},
  abstract = {Tabular data is one of the most common data sources in machine learning. Although a wide range of classical methods demonstrate practical utilities in this field, deep learning methods on tabular data are becoming promising alternatives due to their flexibility and ability to capture complex interactions within the data. Considering that deep tabular methods have diverse design philosophies, including the ways they handle features, design learning objectives, and construct model architectures, we introduce a versatile deep-learning toolbox called TALENT (Tabular Analytics and LEarNing Toolbox) to utilize, analyze, and compare tabular methods. TALENT encompasses an extensive collection of more than 20 deep tabular prediction methods, associated with various encoding and normalization modules, and provides a unified interface that is easily integrable with new methods as they emerge. In this paper, we present the design and functionality of the toolbox, illustrate its practical application through several case studies, and investigate the performance of various methods fairly based on our toolbox. Code is available at https://github.com/qile2000/LAMDA-TALENT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/JYWSMKV8/Liu et al. - 2024 - TALENT A Tabular Analytics and Learning Toolbox.pdf;/Users/jingang/Zotero/storage/F8XPSIPG/2407.html}
}

@misc{liuTokenizeFeaturesEnhancing2024,
  title = {Tokenize Features, Enhancing Tables: The {{FT-TABPFN}} Model for Tabular Classification},
  shorttitle = {Tokenize Features, Enhancing Tables},
  author = {Liu, Quangao and Yang, Wei and Liang, Chen and Pang, Longlong and Zou, Zhuozhang},
  year = {2024},
  month = jun,
  number = {arXiv:2406.06891},
  eprint = {2406.06891},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.06891},
  urldate = {2024-12-30},
  abstract = {Traditional methods for tabular classification usually rely on supervised learning from scratch, which requires extensive training data to determine model parameters. However, a novel approach called Prior-Data Fitted Networks (TabPFN) has changed this paradigm. TabPFN uses a 12-layer transformer trained on large synthetic datasets to learn universal tabular representations. This method enables fast and accurate predictions on new tasks with a single forward pass and no need for additional training. Although TabPFN has been successful on small datasets, it generally shows weaker performance when dealing with categorical features. To overcome this limitation, we propose FT-TabPFN, which is an enhanced version of TabPFN that includes a novel Feature Tokenization layer to better handle classification features. By fine-tuning it for downstream tasks, FT-TabPFN not only expands the functionality of the original model but also significantly improves its applicability and accuracy in tabular classification. Our full source code is available for community use and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/G3N252BD/Liu et al. - 2024 - Tokenize features, enhancing tables the FT-TABPFN model for tabular classification.pdf;/Users/jingang/Zotero/storage/B6G35PZE/2406.html}
}

@article{liuUnifiedFeatureDisentangler2018,
  title = {A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation},
  author = {Liu, Alexander H. and Liu, Yen-Cheng and Yeh, Yu-Ying and Wang, Yu-Chiang Frank},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  annotation = {GSCC: 0000219}
}

@article{liVisualizingLossLandscape2017,
  title = {Visualizing the Loss Landscape of Neural Nets},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2017},
  journal = {arXiv preprint arXiv:1712.09913},
  eprint = {1712.09913},
  archiveprefix = {arXiv}
}

@inproceedings{locatelloChallengingCommonAssumptions2019,
  title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  booktitle = {International Conference on Machine Learning},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2019},
  pages = {4114--4124},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.pdf}
}

@article{lomontIntroductionIntelAdvanced2011,
  title = {Introduction to Intel Advanced Vector Extensions},
  author = {Lomont, Chris},
  year = {2011},
  journal = {Intel white paper},
  volume = {23}
}

@article{lorraineStochasticHyperparameterOptimization2018,
  title = {Stochastic Hyperparameter Optimization through Hypernetworks},
  author = {Lorraine, Jonathan and Duvenaud, David},
  year = {2018},
  journal = {arXiv preprint arXiv:1802.09419},
  eprint = {1802.09419},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Stochastic hyperparameter optimization through hypernetworks.pdf}
}

@article{louizosVariationalFairAutoencoder2015,
  title = {The Variational Fair Autoencoder},
  author = {Louizos, Christos and Swersky, Kevin and Li, Yujia and Welling, Max and Zemel, Richard},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.00830},
  eprint = {1511.00830},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/The Variational Fair Autoencoder.pdf}
}

@inproceedings{luInvariantCausalRepresentation2021,
  title = {Invariant {{Causal Representation Learning}} for {{Out-of-Distribution Generalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lu, Chaochao and Wu, Yuhuai and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Sch{\"o}lkopf, Bernhard},
  year = {2021},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Causality learning/INVARIANT CAUSAL REPRESENTATION LEARNING FOR OUT-OF-DISTRIBUTION GENERALIZATION.pdf}
}

@article{luLearningNonlinearOperators2021,
  title = {Learning Nonlinear Operators via {{DeepONet}} Based on the Universal Approximation Theorem of Operators},
  author = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  year = {2021},
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {3},
  pages = {218--229},
  publisher = {Nature Publishing Group},
  isbn = {2522-5839}
}

@inproceedings{lvCausalityInspiredRepresentation2022,
  title = {Causality {{Inspired Representation Learning}} for {{Domain Generalization}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lv, Fangrui and Liang, Jian and Li, Shuang and Zang, Bin and Liu, Chi Harold and Wang, Ziteng and Liu, Di},
  year = {2022},
  pages = {8046--8056},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Causality learning/Causality Inspired Representation Learning for Domain Generalization.pdf}
}

@article{lyonSesameAlamosNational1978,
  title = {Sesame: The {{Los Alamos National Laboratory}} Equation of State Database},
  author = {Lyon, Stanford P},
  year = {1978},
  journal = {LANL report}
}

@inproceedings{maasRectifierNonlinearitiesImprove2013,
  title = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  booktitle = {Proc. Icml},
  author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  year = {2013},
  volume = {30},
  pages = {3},
  publisher = {Citeseer}
}

@article{mahabadiParameterefficientMultitaskFinetuning2021,
  title = {Parameter-Efficient Multi-Task Fine-Tuning for Transformers via Shared Hypernetworks},
  author = {Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  year = {2021},
  journal = {arXiv preprint arXiv:2106.04489},
  eprint = {2106.04489},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks.pdf}
}

@article{maInContextDataDistillation2024,
  title = {In-{{Context Data Distillation}} with {{TabPFN}}},
  author = {Ma, Junwei and Thomas, Valentin and Yu, Guangwei and Caterini, Anthony},
  year = {2024},
  journal = {arXiv preprint arXiv:2402.06971},
  eprint = {2402.06971},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/In-Context Data Distillation with TabPFN.pdf}
}

@misc{majmundarMETMaskedEncoding2022,
  title = {{{MET}}: {{Masked Encoding}} for {{Tabular Data}}},
  shorttitle = {{{MET}}},
  author = {Majmundar, Kushal and Goyal, Sachin and Netrapalli, Praneeth and Jain, Prateek},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08564},
  eprint = {2206.08564},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.08564},
  urldate = {2024-08-12},
  abstract = {We consider the task of self-supervised representation learning (SSL) for tabular data: tabular-SSL. Typical contrastive learning based SSL methods require instance-wise data augmentations which are difficult to design for unstructured tabular data. Existing tabular-SSL methods design such augmentations in a relatively ad-hoc fashion and can fail to capture the underlying data manifold. Instead of augmentations based approaches for tabular-SSL, we propose a new reconstruction based method, called Masked Encoding for Tabular Data (MET), that does not require augmentations. MET is based on the popular MAE approach for vision-SSL [He et al., 2021] and uses two key ideas: (i) since each coordinate in a tabular dataset has a distinct meaning, we need to use separate representations for all coordinates, and (ii) using an adversarial reconstruction loss in addition to the standard one. Empirical results on five diverse tabular datasets show that MET achieves a new state of the art (SOTA) on all of these datasets and improves up to 9\% over current SOTA methods. We shed more light on the working of MET via experiments on carefully designed simple datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/PLBDWU22/Majmundar et al. - 2022 - MET Masked Encoding for Tabular Data.pdf;/Users/jingang/Zotero/storage/SZ6VVBFH/2206.html}
}

@inproceedings{maModelingTaskRelationships2018,
  title = {Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD}} International Conference on Knowledge Discovery \& Data Mining},
  author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H.},
  year = {2018},
  pages = {1930--1939},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Modeling task relationships in multi-task learning with multi-gate mixture-of-experts.pdf}
}

@inproceedings{manciniBestSourcesForward2018,
  title = {Best Sources Forward: Domain Generalization through Source-Specific Nets},
  booktitle = {2018 25th {{IEEE}} International Conference on Image Processing ({{ICIP}})},
  author = {Mancini, Massimiliano and Bulo, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  year = {2018},
  pages = {1353--1357},
  publisher = {IEEE},
  isbn = {1-4799-7061-1},
  annotation = {GSCC: 0000085},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Ensemble methods/Best sources forward domain generalization through source-specific nets.pdf}
}

@inproceedings{manciniBoostingDomainAdaptation2018,
  title = {Boosting Domain Adaptation by Discovering Latent Domains},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Mancini, Massimiliano and Porzi, Lorenzo and Bulo, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  year = {2018},
  pages = {3771--3780}
}

@article{manciniRobustPlaceCategorization2018,
  title = {Robust Place Categorization with Deep Domain Generalization},
  author = {Mancini, Massimiliano and Bulo, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  year = {2018},
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {3},
  pages = {2093--2100},
  publisher = {IEEE},
  isbn = {2377-3766},
  annotation = {GSCC: 0000508},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Ensemble methods/Robust Place Categorization with Deep Domain Generalization.pdf}
}

@inproceedings{maniKNNApproachUnbalanced2003,
  title = {{{kNN}} Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction},
  booktitle = {Proceedings of Workshop on Learning from Imbalanced Datasets},
  author = {Mani, Inderjeet and Zhang, I.},
  year = {2003},
  volume = {126},
  pages = {1--7},
  publisher = {ICML}
}

@misc{martonGRANDEGradientBasedDecision2024,
  title = {{{GRANDE}}: {{Gradient-Based Decision Tree Ensembles}} for {{Tabular Data}}},
  shorttitle = {{{GRANDE}}},
  author = {Marton, Sascha and L{\"u}dtke, Stefan and Bartelt, Christian and Stuckenschmidt, Heiner},
  year = {2024},
  month = mar,
  number = {arXiv:2309.17130},
  eprint = {2309.17130},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.17130},
  urldate = {2024-09-10},
  abstract = {Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose \${\textbackslash}text\{GRANDE\}\$, \${\textbackslash}text\{GRA\}\$die\${\textbackslash}text\{N\}\$t-Based \${\textbackslash}text\{D\}\$ecision Tree \${\textbackslash}text\{E\}\$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We conducted an extensive evaluation on a predefined benchmark with 19 classification datasets and demonstrate that our method outperforms existing gradient-boosting and deep learning frameworks on most datasets. The method is available under: https://github.com/s-marton/GRANDE},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/I87G22QK/Marton et al. - 2024 - GRANDE Gradient-Based Decision Tree Ensembles for Tabular Data.pdf;/Users/jingang/Zotero/storage/WNG9JGLU/2309.html}
}

@misc{maTabDPTScalingTabular2024,
  title = {{{TabDPT}}: {{Scaling Tabular Foundation Models}}},
  shorttitle = {{{TabDPT}}},
  author = {Ma, Junwei and Thomas, Valentin and Hosseinzadeh, Rasa and Kamkari, Hamidreza and Labach, Alex and Cresswell, Jesse C. and Golestan, Keyvan and Yu, Guangwei and Volkovs, Maksims and Caterini, Anthony L.},
  year = {2024},
  month = oct,
  number = {arXiv:2410.18164},
  eprint = {2410.18164},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.18164},
  urldate = {2024-11-05},
  abstract = {The challenges faced by neural networks on tabular data are well-documented and have hampered the progress of tabular foundation models. Techniques leveraging in-context learning (ICL) have shown promise here, allowing for dynamic adaptation to unseen data. ICL can provide predictions for entirely new datasets without further training or hyperparameter tuning, therefore providing very fast inference when encountering a novel task. However, scaling ICL for tabular data remains an issue: approaches based on large language models cannot efficiently process numeric tables, and tabular-specific techniques have not been able to effectively harness the power of real data to improve performance and generalization. We are able to overcome these challenges by training tabular-specific ICL-based architectures on real data with self-supervised learning and retrieval, combining the best of both worlds. Our resulting model -- the Tabular Discriminative Pre-trained Transformer (TabDPT) -- achieves state-of-the-art performance on the CC18 (classification) and CTR23 (regression) benchmarks with no task-specific fine-tuning, demonstrating the adapatability and speed of ICL once the model is pre-trained. TabDPT also demonstrates strong scaling as both model size and amount of available data increase, pointing towards future improvements simply through the curation of larger tabular pre-training datasets and training larger models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/HTA3VNTW/Ma et al. - 2024 - TabDPT Scaling Tabular Foundation Models.pdf;/Users/jingang/Zotero/storage/X88D48B2/2410.html}
}

@misc{maTabPFGenTabularData2024,
  title = {{{TabPFGen}} -- {{Tabular Data Generation}} with {{TabPFN}}},
  author = {Ma, Junwei and Dankar, Apoorv and Stein, George and Yu, Guangwei and Caterini, Anthony},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05216},
  eprint = {2406.05216},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05216},
  urldate = {2024-09-24},
  abstract = {Advances in deep generative modelling have not translated well to tabular data. We argue that this is caused by a mismatch in structure between popular generative models and discriminative models of tabular data. We thus devise a technique to turn TabPFN -- a highly performant transformer initially designed for in-context discriminative tabular tasks -- into an energy-based generative model, which we dub TabPFGen. This novel framework leverages the pre-trained TabPFN as part of the energy function and does not require any additional training or hyperparameter tuning, thus inheriting TabPFN's in-context learning capability. We can sample from TabPFGen analogously to other energy-based models. We demonstrate strong results on standard generative modelling tasks, including data augmentation, class-balancing, and imputation, unlocking a new frontier of tabular data generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/XA3ULASV/Ma et al. - 2024 - TabPFGen -- Tabular Data Generation with TabPFN.pdf;/Users/jingang/Zotero/storage/DIDEEPLR/2406.html}
}

@inproceedings{matsuuraDomainGeneralizationUsing2020,
  title = {Domain Generalization Using a Mixture of Multiple Latent Domains},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Matsuura, Toshihiko and Harada, Tatsuya},
  year = {2020},
  volume = {34},
  pages = {11749--11756},
  isbn = {2374-3468},
  annotation = {GSCC: 0000144},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Domain-invariant representation learning/Domain Generalization Using a Mixture of Multiple Latent Domains.pdf}
}

@inproceedings{matsuuraDomainGeneralizationUsing2020a,
  title = {Domain Generalization Using a Mixture of Multiple Latent Domains},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Matsuura, Toshihiko and Harada, Tatsuya},
  year = {2020},
  volume = {34},
  pages = {11749--11756},
  isbn = {2374-3468},
  annotation = {GSCC: 0000144}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  journal = {The bulletin of mathematical biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  publisher = {Springer},
  isbn = {1522-9602},
  annotation = {GSCC: 0029383}
}

@article{mcelfreshWhenNeuralNets2024,
  title = {When Do Neural Nets Outperform Boosted Trees on Tabular Data?},
  author = {McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Prasad C, Vishak and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  file = {/Users/jingang/Dropbox/References/Tabular learning/When Do Neural Nets Outperform Boosted Trees onTabular Data.pdf}
}

@article{mckayComparisonThreeMethods2000,
  title = {A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code},
  author = {McKay, Michael D and Beckman, Richard J and Conover, William J},
  year = {2000},
  journal = {Technometrics},
  volume = {42},
  number = {1},
  pages = {55--61},
  publisher = {Taylor \& Francis}
}

@article{medskerRecurrentNeuralNetworks2001,
  title = {Recurrent Neural Networks},
  author = {Medsker, Larry R and Jain, {\relax LC}},
  year = {2001},
  journal = {Design and Applications},
  volume = {5},
  number = {64-67},
  pages = {2}
}

@inproceedings{meinshausenCausalityDistributionalRobustness2018,
  title = {Causality from a Distributional Robustness Point of View},
  booktitle = {2018 {{IEEE Data Science Workshop}} ({{DSW}})},
  author = {Meinshausen, Nicolai},
  year = {2018},
  pages = {6--10},
  publisher = {IEEE},
  isbn = {1-5386-4410-X}
}

@inproceedings{meinshausenCausalityDistributionalRobustness2018a,
  title = {Causality from a Distributional Robustness Point of View},
  booktitle = {2018 {{IEEE Data Science Workshop}} ({{DSW}})},
  author = {Meinshausen, Nicolai},
  year = {2018},
  pages = {6--10},
  publisher = {IEEE},
  isbn = {1-5386-4410-X}
}

@article{mentzerNeuralNetworkSurrogate2023,
  title = {Neural Network Surrogate Models for Equations of State},
  author = {Mentzer, Katherine L. and Peterson, J. Luc},
  year = {2023},
  journal = {Physics of Plasmas},
  volume = {30},
  number = {3},
  pages = {032704},
  publisher = {AIP Publishing LLC},
  isbn = {1070-664X},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/ML EoS/Neural network surrogate models for equations of state.pdf}
}

@article{michelsenComparativeStudyReducedvariablesbased2013,
  title = {A Comparative Study of Reduced-Variables-Based Flash and Conventional Flash},
  author = {Michelsen, Michael L and Yan, Wei and Stenby, Erling H},
  year = {2013},
  journal = {SPE Journal},
  volume = {18},
  number = {05},
  pages = {952--959},
  publisher = {OnePetro}
}

@article{michelsenIsothermalFlashProblem1982,
  title = {The Isothermal Flash Problem. {{Part II}}. {{Phase-split}} Calculation},
  author = {Michelsen, Michael L},
  year = {1982},
  journal = {Fluid phase equilibria},
  volume = {9},
  number = {1},
  pages = {21--40},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Flash calculations/THE ISOTHERMAL FLASH PROBLEM. PART II. PHASE-SPLIT CALCULATION.pdf}
}

@article{michelsenIsothermalFlashProblem1982a,
  title = {The Isothermal Flash Problem. {{Part I}}. {{Stability}}},
  author = {Michelsen, Michael L},
  year = {1982},
  journal = {Fluid phase equilibria},
  volume = {9},
  number = {1},
  pages = {1--19},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Flash calculations/THE ISOTHERMAL FLASH PROBLEM. PART I. STABILITY.pdf}
}

@article{michelsenMultiphaseIsenthalpicIsentropic1987,
  title = {Multiphase Isenthalpic and Isentropic Flash Algorithms},
  author = {Michelsen, Michael L.},
  year = {1987},
  journal = {Fluid phase equilibria},
  volume = {33},
  number = {1-2},
  pages = {13--27},
  publisher = {Elsevier},
  isbn = {0378-3812}
}

@article{michelsenPhaseEquilibriumCalculations1993,
  title = {Phase Equilibrium Calculations. {{What}} Is Easy and What Is Difficult?},
  author = {Michelsen, Michael L},
  year = {1993},
  journal = {Computers \& chemical engineering},
  volume = {17},
  number = {5-6},
  pages = {431--439},
  publisher = {Elsevier}
}

@article{michelsenRobustEfficientSolution2006,
  title = {Robust and Efficient Solution Procedures for Association Models},
  author = {Michelsen, Michael L.},
  year = {2006},
  journal = {Industrial \& engineering chemistry research},
  volume = {45},
  number = {25},
  pages = {8449--8453},
  publisher = {ACS Publications},
  file = {/Users/jingang/Zotero/storage/S6JKVTBG/Michelsen_2006_Robust and efficient solution procedures for association models.pdf;/Users/jingang/Zotero/storage/7G5KBBVT/ie060029x.html}
}

@article{michelsenSimplifiedFlashCalculations1986,
  title = {Simplified Flash Calculations for Cubic Equations of State},
  author = {Michelsen, Michael L},
  year = {1986},
  journal = {Industrial \& Engineering Chemistry Process Design and Development},
  volume = {25},
  number = {1},
  pages = {184--188},
  publisher = {ACS Publications},
  annotation = {titleTranslation:}
}

@article{michelsenSpeedingTwophasePTflash1998,
  title = {Speeding up the Two-Phase {{PT-flash}}, with Applications for Calculation of Miscible Displacement},
  author = {Michelsen, Michael L},
  year = {1998},
  journal = {Fluid Phase Equilibria},
  volume = {143},
  number = {1-2},
  pages = {1--12},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Speeding up the two-phase PT-flash, with applications for calculation.pdf}
}

@book{michelsenThermodynamicModellingFundamentals2004,
  title = {Thermodynamic Modelling: Fundamentals and Computational Aspects},
  author = {Michelsen, Michael Locht and Mollerup, J{\o}rgen},
  year = {2004},
  publisher = {Tie-Line Publications}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  urldate = {2025-01-05},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/QQ54T638/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf;/Users/jingang/Zotero/storage/72XKA8Y8/1301.html}
}

@article{minskyStepsArtificialIntelligence1961,
  title = {Steps toward Artificial Intelligence},
  author = {Minsky, Marvin},
  year = {1961},
  journal = {Proceedings of the IRE},
  volume = {49},
  number = {1},
  pages = {8--30},
  publisher = {IEEE},
  isbn = {0096-8390},
  annotation = {GSCC: 0003001}
}

@book{mitchellMachineLearning1997,
  title = {Machine Learning},
  author = {Mitchell, Tom M. and Mitchell, Tom M.},
  year = {1997},
  volume = {1},
  publisher = {McGraw-hill New York}
}

@book{mitchellMachineLearning1997a,
  title = {Machine Learning},
  author = {Mitchell, Tom M and Mitchell, Tom M},
  year = {1997},
  volume = {1},
  publisher = {McGraw-hill New York}
}

@article{mitrovicRepresentationLearningInvariant2020,
  title = {Representation Learning via Invariant Causal Mechanisms},
  author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.07922},
  eprint = {2010.07922},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000097}
}

@techreport{moreUserGuideMINPACK11980,
  title = {User Guide for {{MINPACK-1}}},
  author = {Mor{\'e}, Jorge J. and Garbow, Burton S. and Hillstrom, Kenneth E.},
  year = {1980},
  institution = {CM-P00068642}
}

@inproceedings{moritzRayDistributedFramework2018,
  title = {Ray: {{A}} Distributed Framework for Emerging \{\vphantom\}{{AI}}\vphantom\{\} Applications},
  booktitle = {13th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 18)},
  author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I.},
  year = {2018},
  pages = {561--577},
  isbn = {1-939133-08-4}
}

@article{morningstarSimulationQuantumManybody2022,
  title = {Simulation of Quantum Many-Body Dynamics with {{Tensor Processing Units}}: {{Floquet}} Prethermalization},
  author = {Morningstar, Alan and Hauru, Markus and Beall, Jackson and Ganahl, Martin and Lewis, Adam GM and Khemani, Vedika and Vidal, Guifre},
  year = {2022},
  journal = {PRX Quantum},
  volume = {3},
  number = {2},
  pages = {020331},
  publisher = {APS}
}

@article{morrisExploratoryDesignsComputational1995,
  title = {Exploratory Designs for Computational Experiments},
  author = {Morris, Max D. and Mitchell, Toby J.},
  year = {1995},
  journal = {Journal of statistical planning and inference},
  volume = {43},
  number = {3},
  pages = {381--402},
  publisher = {Elsevier},
  isbn = {0378-3758}
}

@inproceedings{motiianUnifiedDeepSupervised2017,
  title = {Unified Deep Supervised Domain Adaptation and Generalization},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Motiian, Saeid and Piccirilli, Marco and Adjeroh, Donald A. and Doretto, Gianfranco},
  year = {2017},
  pages = {5715--5725},
  annotation = {GSCC: 0000570}
}

@inproceedings{mouselinosMAINMultiheadAttentionImputation2021,
  title = {{{MAIN}}: {{Multihead-Attention Imputation Networks}}},
  shorttitle = {{{MAIN}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Mouselinos, Spyridon and Polymenakos, Kyriakos and Nikitakis, Antonis and Kyriakopoulos, Konstantinos},
  year = {2021},
  eprint = {2102.05428},
  primaryclass = {cs},
  doi = {10.1109/IJCNN52387.2021},
  urldate = {2024-07-09},
  abstract = {The problem of missing data, usually absent incurated and competition-standard datasets, is an unfortunate reality for most machine learning models used in industry applications. Recent work has focused on understanding the nature and the negative effects of such phenomena, while devising solutions for optimal imputation of the missing data, using both discriminative and generative approaches. We propose a novel mechanism based on multi-head attention which can be applied effortlessly in any model and achieves better downstream performance without the introduction of the full dataset in any part of the modeling pipeline. Our method inductively models patterns of missingness in the input data in order to increase the performance of the downstream task. Finally, after evaluating our method against baselines for a number of datasets, we found performance gains that tend to be larger in scenarios of high missingness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/SWG4CITD/Mouselinos et al. - 2021 - MAIN Multihead-Attention Imputation Networks.pdf;/Users/jingang/Zotero/storage/EUR9LPA2/2102.html}
}

@inproceedings{muandetDomainGeneralizationInvariant2013,
  title = {Domain Generalization via Invariant Feature Representation},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  year = {2013},
  pages = {10--18},
  publisher = {PMLR},
  annotation = {GSCC: 0000690}
}

@inproceedings{muandetDomainGeneralizationInvariant2013a,
  title = {Domain Generalization via Invariant Feature Representation},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  year = {2013},
  pages = {10--18},
  publisher = {PMLR}
}

@misc{mullerMotherNetFoundationalHypernetwork2023,
  title = {{{MotherNet}}: {{A Foundational Hypernetwork}} for {{Tabular Classification}}},
  shorttitle = {{{MotherNet}}},
  author = {M{\"u}ller, Andreas and Curino, Carlo and Ramakrishnan, Raghu},
  year = {2023},
  month = dec,
  number = {arXiv:2312.08598},
  eprint = {2312.08598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.08598},
  urldate = {2024-07-09},
  abstract = {The advent of Foundation Models is transforming machine learning across many modalities (e.g., language, images, videos) with prompt engineering replacing training in many settings. Recent work on tabular data (e.g., TabPFN) hints at a similar opportunity to build Foundation Models for classification for numerical data. In this paper, we go one step further and propose a hypernetwork architecture that we call MotherNet, trained on millions of classification tasks, that, once prompted with a never-seen-before training set generates the weights of a trained ``child'' neural-network. Like other Foundation Models, MotherNet replaces training on specific datasets with in-context learning through a single forward pass. In contrast to existing hypernetworks that were either task-specific or trained for relatively constraint multi-task settings, MotherNet is trained to generate networks to perform multiclass classification on arbitrary tabular datasets without any dataset specific gradient descent. The child network generated by MotherNet using in-context learning outperforms neural networks trained using gradient descent on small datasets, and is competitive with predictions by TabPFN and standard ML methods like Gradient Boosting. Unlike a direct application of transformer models like TabPFN, MotherNet generated networks are highly efficient at inference time. This methodology opens up a new approach to building predictive models on tabular data that is both efficient and robust, without any dataset-specific training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.6},
  file = {/Users/jingang/Zotero/storage/4FRI88T4/Müller et al. - 2023 - MotherNet A Foundational Hypernetwork for Tabular.pdf;/Users/jingang/Zotero/storage/BCZWA4IG/2312.html}
}

@misc{mullerTransformersCanBayesian2024,
  title = {Transformers {{Can Do Bayesian Inference}}},
  author = {M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  year = {2024},
  month = aug,
  number = {arXiv:2112.10510},
  eprint = {2112.10510},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10510},
  urldate = {2024-08-31},
  abstract = {Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage in-context learning in large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github.com/automl/TransformersCanDoBayesianInference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/6KQXIECT/Müller et al. - 2024 - Transformers Can Do Bayesian Inference.pdf;/Users/jingang/Zotero/storage/C9URXSEW/2112.html}
}

@article{murtaghWardHierarchicalAgglomerative2014,
  title = {Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement {{Ward}}'s Criterion?},
  author = {Murtagh, Fionn and Legendre, Pierre},
  year = {2014},
  journal = {Journal of classification},
  volume = {31},
  number = {3},
  pages = {274--295},
  publisher = {Springer},
  isbn = {1432-1343}
}

@article{mustafaMultimodalContrastiveLearning2022,
  title = {Multimodal {{Contrastive Learning}} with {{LIMoE}}: The {{Language-Image Mixture}} of {{Experts}}},
  author = {Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.02770},
  eprint = {2206.02770},
  archiveprefix = {arXiv}
}

@inproceedings{naglerStatisticalFoundationsPriordata2023,
  title = {Statistical Foundations of Prior-Data Fitted Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Nagler, Thomas},
  year = {2023},
  pages = {25660--25676},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Statistical Foundations of Prior-Data Fitted Networks.pdf}
}

@misc{naglerStatisticalFoundationsPriorData2023a,
  title = {Statistical {{Foundations}} of {{Prior-Data Fitted Networks}}},
  author = {Nagler, Thomas},
  year = {2023},
  month = may,
  number = {arXiv:2305.11097},
  eprint = {2305.11097},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.11097},
  urldate = {2024-11-11},
  abstract = {Prior-data fitted networks (PFNs) were recently proposed as a new paradigm for machine learning. Instead of training the network to an observed training set, a fixed model is pre-trained offline on small, simulated training sets from a variety of tasks. The pre-trained model is then used to infer class probabilities in-context on fresh training sets with arbitrary size and distribution. Empirically, PFNs achieve state-of-the-art performance on tasks with similar size to the ones used in pre-training. Surprisingly, their accuracy further improves when passed larger data sets during inference. This article establishes a theoretical foundation for PFNs and illuminates the statistical mechanisms governing their behavior. While PFNs are motivated by Bayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, but untrained predictors explains their behavior. A predictor's variance vanishes if its sensitivity to individual training samples does and the bias vanishes only if it is appropriately localized around the test feature. The transformer architecture used in current PFN implementations ensures only the former. These findings shall prove useful for designing architectures with favorable empirical behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/46G3BHCD/Nagler - 2023 - Statistical Foundations of Prior-Data Fitted Networks.pdf;/Users/jingang/Zotero/storage/F6GFHXS7/2305.html}
}

@article{naidooNewHighpressureVapour2008,
  title = {A New High-Pressure Vapour--Liquid Equilibrium Apparatus},
  author = {Naidoo, Paramespri and Ramjugernath, Deresh and Raal, Johan David},
  year = {2008},
  journal = {Fluid Phase Equilibria},
  volume = {269},
  number = {1-2},
  pages = {104--112},
  publisher = {Elsevier},
  issn = {0378-3812}
}

@inproceedings{nairRectifiedLinearUnits2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Icml},
  author = {Nair, Vinod and Hinton, Geoffrey E},
  year = {2010}
}

@inproceedings{namReducingDomainGap2021,
  title = {Reducing Domain Gap by Reducing Style Bias},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Nam, Hyeonseob and Lee, HyunJae and Park, Jongchan and Yoon, Wonjun and Yoo, Donggeun},
  year = {2021},
  pages = {8690--8699},
  annotation = {GSCC: 0000080},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/VAE/Reducing Domain Gap by Reducing Style Bias.pdf}
}

@misc{namSTUNTFewshotTabular2023,
  title = {{{STUNT}}: {{Few-shot Tabular Learning}} with {{Self-generated Tasks}} from {{Unlabeled Tables}}},
  shorttitle = {{{STUNT}}},
  author = {Nam, Jaehyun and Tack, Jihoon and Lee, Kyungmin and Lee, Hankook and Shin, Jinwoo},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-08-18},
  abstract = {Learning with few labeled tabular samples is often an essential requirement for industrial machine learning applications as varieties of tabular data suffer from high annotation costs or have difficulties in collecting new samples for novel tasks. Despite the utter importance, such a problem is quite under-explored in the field of tabular learning, and existing few-shot learning schemes from other domains are not straightforward to apply, mainly due to the heterogeneous characteristics of tabular data. In this paper, we propose a simple yet effective framework for few-shot semi-supervised tabular learning, coined Self-generated Tasks from UNlabeled Tables (STUNT). Our key idea is to self-generate diverse few-shot tasks by treating randomly chosen columns as a target label. We then employ a meta-learning scheme to learn generalizable knowledge with the constructed tasks. Moreover, we introduce an unsupervised validation scheme for hyperparameter search (and early stopping) by generating a pseudo-validation set using STUNT from unlabeled data. Our experimental results demonstrate that our simple framework brings significant performance gain under various tabular few-shot learning benchmarks, compared to prior semi- and self-supervised baselines. Code is available at https://github.com/jaehyun513/STUNT.},
  howpublished = {https://arxiv.org/abs/2303.00918v1},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/8HYY5S55/Nam et al. - 2023 - STUNT Few-shot Tabular Learning with Self-generated Tasks from Unlabeled Tables.pdf}
}

@article{navonLearningParetoFront2020,
  title = {Learning the Pareto Front with Hypernetworks},
  author = {Navon, Aviv and Shamsian, Aviv and Chechik, Gal and Fetaya, Ethan},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.04104},
  eprint = {2010.04104},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Learning the pareto front with hypernetworks.pdf}
}

@article{neoschilConvergePressureConcept1978,
  title = {Converge {{Pressure Concept A Key For High Pressure Equilibria}}},
  author = {Neoschil, J. and Chambrette, P.},
  year = {1978},
  publisher = {Society of Petroleum Engineers}
}

@article{nesterovGradientMethodsMinimizing2013,
  title = {Gradient Methods for Minimizing Composite Functions},
  author = {Nesterov, Yu},
  year = {2013},
  journal = {Mathematical programming},
  volume = {140},
  number = {1},
  pages = {125--161},
  publisher = {Springer},
  isbn = {1436-4646}
}

@article{ngMachineLearningYearning2017,
  title = {Machine Learning Yearning},
  author = {Ng, Andrew},
  year = {2017},
  journal = {URL: http://www. mlyearning. org/(96)},
  volume = {139}
}

@article{nguyenBorderlineOversamplingImbalanced2011,
  title = {Borderline Over-Sampling for Imbalanced Data Classification},
  author = {Nguyen, Hien M. and Cooper, Eric W. and Kamei, Katsuari},
  year = {2011},
  journal = {International Journal of Knowledge Engineering and Soft Data Paradigms},
  volume = {3},
  number = {1},
  pages = {4--21},
  publisher = {Inderscience Publishers},
  isbn = {1755-3210}
}

@article{nichitaMultiphaseEquilibriaCalculation2002,
  title = {Multiphase Equilibria Calculation by Direct Minimization of {{Gibbs}} Free Energy with a Global Optimization Method},
  author = {Nichita, Dan Vladimir and Gomez, Susana and Luna, Eduardo},
  year = {2002},
  journal = {Computers \& chemical engineering},
  volume = {26},
  number = {12},
  pages = {1703--1724},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Multiphase equilibria calculation by direct minimization.pdf}
}

@article{nichitaRapidRobustMethod2013,
  title = {A Rapid and Robust Method for Solving the {{Rachford}}--{{Rice}} Equation Using Convex Transformations},
  author = {Nichita, Dan Vladimir and Leibovici, Claude F.},
  year = {2013},
  journal = {Fluid Phase Equilibria},
  volume = {353},
  pages = {38--49},
  publisher = {Elsevier},
  isbn = {0378-3812}
}

@article{nieDensetosparseGateMixtureofexperts2021,
  title = {Dense-to-Sparse Gate for Mixture-of-Experts},
  author = {Nie, Xiaonan and Cao, Shijie and Miao, Xupeng and Ma, Lingxiao and Xue, Jilong and Miao, Youshan and Yang, Zichao and Yang, Zhi and Cui, Bin},
  year = {2021},
  journal = {arXiv preprint arXiv:2112.14397},
  eprint = {2112.14397},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Dense-to-Sparse Gate for Mixture-of-Experts.pdf}
}

@misc{niFindingPillarsStrength2023,
  title = {Finding the {{Pillars}} of {{Strength}} for {{Multi-Head Attention}}},
  author = {Ni, Jinjie and Mao, Rui and Yang, Zonglin and Lei, Han and Cambria, Erik},
  year = {2023},
  month = oct,
  number = {arXiv:2305.14380},
  eprint = {2305.14380},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14380},
  urldate = {2024-09-04},
  abstract = {Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well-established tasks while considerably compressing parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.0,I.2.7},
  file = {/Users/jingang/Zotero/storage/VZW3SVS9/Ni et al. - 2023 - Finding the Pillars of Strength for Multi-Head Attention.pdf;/Users/jingang/Zotero/storage/KX828WD9/2305.html}
}

@inproceedings{niuMultiviewDomainGeneralization2015,
  title = {Multi-View Domain Generalization for Visual Recognition},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Niu, Li and Li, Wen and Xu, Dong},
  year = {2015},
  pages = {4193--4201},
  annotation = {GSCC: 0000057}
}

@article{ohgakiIsothermalVaporliquidEquilibrium1976,
  title = {Isothermal Vapor-Liquid Equilibrium Data for Binary Systems Containing Carbon Dioxide at High Pressures: Methanol-Carbon Dioxide, n-Hexane-Carbon Dioxide, and Benzene-Carbon Dioxide Systems},
  author = {Ohgaki, Kazunari and Katayama, Takashi},
  year = {1976},
  journal = {Journal of chemical and Engineering Data},
  volume = {21},
  number = {1},
  pages = {53--55},
  publisher = {ACS Publications},
  issn = {0021-9568}
}

@article{okunoNewAlgorithmRachfordRice2010,
  title = {A New Algorithm for {{Rachford-Rice}} for Multiphase Compositional Simulation},
  author = {Okuno, Ryosuke and Johns, Russell Taylor and Sepehrnoori, Kamy},
  year = {2010},
  journal = {SPE Journal},
  volume = {15},
  number = {02},
  pages = {313--325},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/A New Algorithm for Rachford-Rice for Multiphase Compositional Simulation.pdf}
}

@misc{olssonIncontextLearningInduction2022,
  title = {In-Context {{Learning}} and {{Induction Heads}}},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11895},
  eprint = {2209.11895},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.11895},
  urldate = {2024-08-30},
  abstract = {"Induction heads" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -{$>$} [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "in-context learning" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/G2RUXEKD/Olsson et al. - 2022 - In-context Learning and Induction Heads.pdf;/Users/jingang/Zotero/storage/XUTFGW3H/2209.html}
}

@article{oordRepresentationLearningContrastive2018,
  title = {Representation Learning with Contrastive Predictive Coding},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.03748},
  eprint = {1807.03748},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mutual Information Estimation/Representation Learning with Contrastive Predictive Coding.pdf}
}

@misc{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  number = {arXiv:1609.03499},
  eprint = {1609.03499},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {/Users/jingang/Zotero/storage/F94KA2RQ/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf}
}

@article{orbachConvergencePromotionSimulation1971,
  title = {Convergence Promotion in the Simulation of Chemical Processes with Recycle-the Dominant Eigenvalue Method},
  author = {Orbach, O and Crowe, {\relax CM}},
  year = {1971},
  journal = {The Canadian Journal of Chemical Engineering},
  volume = {49},
  number = {4},
  pages = {509--513},
  publisher = {Wiley Online Library},
  annotation = {GSCC: 0000065},
  file = {/Users/jingang/Dropbox/References/Flash calculations/Dominant  Eigenvalue  Method.pdf}
}

@inproceedings{otternessAMDGPUsAlternative2020,
  title = {{{AMD GPUs}} as an Alternative to {{NVIDIA}} for Supporting Real-Time Workloads},
  booktitle = {32nd {{Euromicro Conference}} on {{Real-Time Systems}} ({{ECRTS}} 2020)},
  author = {Otterness, Nathan and Anderson, James H.},
  year = {2020},
  publisher = {Schloss Dagstuhl-Leibniz-Zentrum f{\"u}r Informatik}
}

@article{panDomainAdaptationTransfer2010,
  title = {Domain Adaptation via Transfer Component Analysis},
  author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
  year = {2010},
  journal = {IEEE transactions on neural networks},
  volume = {22},
  number = {2},
  pages = {199--210},
  publisher = {IEEE},
  isbn = {1045-9227},
  annotation = {GSCC: 0003447}
}

@article{panSurveyTransferLearning2009,
  title = {A Survey on Transfer Learning},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2009},
  journal = {IEEE Transactions on knowledge and data engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  publisher = {IEEE},
  isbn = {1041-4347}
}

@article{panSurveyTransferLearning2010,
  title = {A Survey on Transfer Learning},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  journal = {IEEE Transactions on knowledge and data engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  publisher = {IEEE},
  isbn = {1041-4347}
}

@article{papamakariosNormalizingFlowsProbabilistic2021,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}.},
  author = {Papamakarios, George and Nalisnick, Eric T and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2021},
  journal = {J. Mach. Learn. Res.},
  volume = {22},
  number = {57},
  pages = {1--64},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Normalizing Flow/Normalizing Flows for Probabilistic Modeling and Inference.pdf}
}

@article{parkAlgorithmsGeneralizedClusterwise2017,
  title = {Algorithms for Generalized Clusterwise Linear Regression},
  author = {Park, Young Woong and Jiang, Yan and Klabjan, Diego and Williams, Loren},
  year = {2017},
  journal = {INFORMS Journal on Computing},
  volume = {29},
  number = {2},
  pages = {301--317},
  publisher = {INFORMS},
  isbn = {1091-9856},
  file = {/Users/jingang/Dropbox/References/Regression clustering/Algorithms for generalized clusterwise linear regression.pdf}
}

@article{paszkePytorchImperativeStyle2019,
  title = {Pytorch: {{An}} Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  year = {2019},
  journal = {Advances in neural information processing systems},
  volume = {32},
  pages = {8026--8037}
}

@article{patelVisualDomainAdaptation2015,
  title = {Visual Domain Adaptation: {{A}} Survey of Recent Advances},
  author = {Patel, Vishal M. and Gopalan, Raghuraman and Li, Ruonan and Chellappa, Rama},
  year = {2015},
  journal = {IEEE signal processing magazine},
  volume = {32},
  number = {3},
  pages = {53--69},
  publisher = {IEEE},
  isbn = {1053-5888}
}

@inproceedings{pathakContextEncodersFeature2016,
  title = {Context Encoders: {{Feature}} Learning by Inpainting},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  year = {2016},
  pages = {2536--2544},
  annotation = {GSCC: 0004523}
}

@inproceedings{pathakContextEncodersFeature2016a,
  title = {Context Encoders: {{Feature}} Learning by Inpainting},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  year = {2016},
  pages = {2536--2544},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Improvements/Context Encoders Feature Learning by Inpainting.pdf}
}

@article{pathakFourcastnetGlobalDatadriven2022,
  title = {Fourcastnet: {{A}} Global Data-Driven High-Resolution Weather Model Using Adaptive Fourier Neural Operators},
  author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.11214},
  eprint = {2202.11214},
  archiveprefix = {arXiv}
}

@article{pavlitskayaBalancingExpertUtilization2022,
  title = {Balancing {{Expert Utilization}} in {{Mixture-of-Experts Layers Embedded}} in {{CNNs}}},
  author = {Pavlitskaya, Svetlana and Hubschneider, Christian and Struppek, Lukas and Z{\"o}llner, J. Marius},
  year = {2022},
  journal = {arXiv preprint arXiv:2204.10598},
  eprint = {2204.10598},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Balancing Expert Utilization in Mixture-of-Experts Layers Embedded in CNNs.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
  year = {2011},
  journal = {the Journal of machine Learning research},
  volume = {12},
  pages = {2825--2830},
  publisher = {JMLR. org},
  isbn = {1532-4435}
}

@inproceedings{pengDomain2vecDomainEmbedding2020,
  title = {Domain2vec: {{Domain}} Embedding for Unsupervised Domain Adaptation},
  booktitle = {European Conference on Computer Vision},
  author = {Peng, Xingchao and Li, Yichen and Saenko, Kate},
  year = {2020},
  pages = {756--774},
  publisher = {Springer},
  annotation = {GSCC: 0000019},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/Domain2Vec/Domain2Vec Domain Embedding for Unsupervised Domain Adaptation.pdf}
}

@inproceedings{pengDomainAgnosticLearning2019,
  title = {Domain Agnostic Learning with Disentangled Representations},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Peng, Xingchao and Huang, Zijun and Sun, Ximeng and Saenko, Kate},
  year = {2019},
  pages = {5102--5112},
  publisher = {PMLR},
  isbn = {2640-3498},
  annotation = {GSCC: 0000151}
}

@inproceedings{pengDomainAgnosticLearning2019a,
  title = {Domain Agnostic Learning with Disentangled Representations},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Peng, Xingchao and Huang, Zijun and Sun, Ximeng and Saenko, Kate},
  year = {2019},
  pages = {5102--5112},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Domain Agnostic Learning with Disentangled Representations.pdf}
}

@inproceedings{pengMomentMatchingMultisource2019,
  title = {Moment Matching for Multi-Source Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  year = {2019},
  pages = {1406--1415},
  annotation = {GSCC: 0000730}
}

@inproceedings{pengMomentMatchingMultisource2019a,
  title = {Moment Matching for Multi-Source Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  year = {2019},
  pages = {1406--1415}
}

@article{pengNewTwoconstantEquation1976,
  title = {A New Two-Constant Equation of State},
  author = {Peng, Ding-Yu and Robinson, Donald B},
  year = {1976},
  journal = {Industrial \& Engineering Chemistry Fundamentals},
  volume = {15},
  number = {1},
  pages = {59--64},
  publisher = {ACS Publications}
}

@inproceedings{pintoAutomaticGenerationMetafeatures2016,
  title = {Towards {{Automatic Generation}} of {{Metafeatures}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Pinto, F{\'a}bio and Soares, Carlos and {Mendes-Moreira}, Jo{\~a}o},
  editor = {Bailey, James and Khan, Latifur and Washio, Takashi and Dobbie, Gill and Huang, Joshua Zhexue and Wang, Ruili},
  year = {2016},
  pages = {215--226},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-31753-3_18},
  abstract = {The selection of metafeatures for metalearning (MtL) is often an ad hoc process. The lack of a proper motivation for the choice of a metafeature rather than others is questionable and may originate a loss of valuable information for a given problem (e.g., use of class entropy and not attribute entropy). We present a framework to systematically generate metafeatures in the context of MtL. This framework decomposes a metafeature into three components: meta-function, object and post-processing. The automatic generation of metafeatures is triggered by the selection of a meta-function used to systematically generate metafeatures from all possible combinations of object and post-processing alternatives. We executed experiments by addressing the problem of algorithm selection in classification datasets. Results show that the sets of systematic metafeatures generated from our framework are more informative than the non-systematic ones and the set regarded as state-of-the-art.},
  isbn = {978-3-319-31753-3},
  langid = {english}
}

@article{plattProbabilisticOutputsSupport1999,
  title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
  author = {Platt, John},
  year = {1999},
  journal = {Advances in large margin classifiers},
  volume = {10},
  number = {3},
  pages = {61--74},
  publisher = {Cambridge, MA}
}

@article{plimptonFastParallelAlgorithms1995,
  title = {Fast Parallel Algorithms for Short-Range Molecular Dynamics},
  author = {Plimpton, Steve},
  year = {1995},
  journal = {Journal of computational physics},
  volume = {117},
  number = {1},
  pages = {1--19},
  publisher = {Elsevier},
  isbn = {0021-9991}
}

@book{pontryaginMathematicalTheoryOptimal1987,
  title = {Mathematical Theory of Optimal Processes},
  author = {Pontryagin, Lev Semenovich},
  year = {1987},
  publisher = {CRC press},
  isbn = {2-88124-077-1}
}

@inproceedings{pooleVariationalBoundsMutual2019,
  title = {On Variational Bounds of Mutual Information},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
  year = {2019},
  pages = {5171--5180},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mutual Information Estimation/On Variational Bounds of Mutual Information.pdf}
}

@article{popovNeuralObliviousDecision2019,
  title = {Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data},
  author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.06312},
  eprint = {1909.06312},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Neural oblivious decision ensembles for deep learning on tabular data.pdf}
}

@inproceedings{prakashStructuredDomainRandomization2019,
  title = {Structured Domain Randomization: {{Bridging}} the Reality Gap by Context-Aware Synthetic Data},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Prakash, Aayush and Boochoon, Shaad and Brophy, Mark and Acuna, David and Cameracci, Eric and State, Gavriel and Shapira, Omer and Birchfield, Stan},
  year = {2019},
  pages = {7249--7255},
  publisher = {IEEE},
  isbn = {1-5386-6027-X},
  annotation = {GSCC: 0000120}
}

@incollection{precheltEarlyStoppingbutWhen1998,
  title = {Early Stopping-but When?},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the Trade},
  author = {Prechelt, Lutz},
  year = {1998},
  pages = {55--69},
  publisher = {Springer}
}

@misc{pressTrainShortTest2022,
  title = {Train {{Short}}, {{Test Long}}: {{Attention}} with {{Linear Biases Enables Input Length Extrapolation}}},
  shorttitle = {Train {{Short}}, {{Test Long}}},
  author = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  year = {2022},
  month = apr,
  number = {arXiv:2108.12409},
  eprint = {2108.12409},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.12409},
  urldate = {2024-09-03},
  abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/4XWLGUQF/Press et al. - 2022 - Train Short, Test Long Attention with Linear Biases Enables Input Length Extrapolation.pdf;/Users/jingang/Zotero/storage/W49VK8AX/2108.html}
}

@article{pudilFloatingSearchMethods1994,
  title = {Floating Search Methods in Feature Selection},
  author = {Pudil, Pavel and Novovi{\v c}ov{\'a}, Jana and Kittler, Josef},
  year = {1994},
  journal = {Pattern recognition letters},
  volume = {15},
  number = {11},
  pages = {1119--1125},
  publisher = {Elsevier},
  isbn = {0167-8655}
}

@inproceedings{qiaoLearningLearnSingle2020,
  title = {Learning to Learn Single Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Qiao, Fengchun and Zhao, Long and Peng, Xi},
  year = {2020},
  pages = {12556--12565},
  annotation = {GSCC: 0000160}
}

@inproceedings{qiaoUncertaintyguidedModelGeneralization2021,
  title = {Uncertainty-Guided Model Generalization to Unseen Domains},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Qiao, Fengchun and Peng, Xi},
  year = {2021},
  pages = {6790--6800},
  annotation = {GSCC: 0000013}
}

@misc{qiPointNetDeepHierarchical2017,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  shorttitle = {{{PointNet}}++},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = jun,
  number = {arXiv:1706.02413},
  eprint = {1706.02413},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-03},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/MU8AA6MU/Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on Point Sets in a Metric Space.pdf}
}

@book{quinonero-candelaDatasetShiftMachine2008,
  title = {Dataset Shift in Machine Learning},
  author = {{Quinonero-Candela}, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
  year = {2008},
  publisher = {Mit Press},
  isbn = {0-262-17005-1}
}

@book{quinonero-candelaDatasetShiftMachine2008a,
  title = {Dataset Shift in Machine Learning},
  author = {{Quinonero-Candela}, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
  year = {2008},
  publisher = {Mit Press},
  isbn = {0-262-17005-1}
}

@article{quPTFlashVectorizedParallel2023,
  title = {{{PTFlash}}: {{A}} Vectorized and Parallel Deep Learning Framework for Two-Phase Flash Calculation},
  author = {Qu, Jingang and Faney, Thibault and {de Hemptinne}, Jean-Charles and Yousef, Soleiman and Gallinari, Patrick},
  year = {2023},
  journal = {Fuel},
  volume = {331},
  pages = {125603},
  publisher = {Elsevier},
  isbn = {0016-2361}
}

@article{rachfordProcedureUseElectronic1952,
  title = {Procedure for Use of Electronic Digital Computers in Calculating Flash Vaporization Hydrocarbon Equilibrium},
  author = {Rachford, Henry H and Rice, {\relax JD}},
  year = {1952},
  journal = {Journal of Petroleum Technology},
  volume = {4},
  number = {10},
  pages = {19--3},
  publisher = {OnePetro},
  annotation = {GSCC: 0000632}
}

@article{raghuSurveyDeepLearning2020,
  title = {A Survey of Deep Learning for Scientific Discovery},
  author = {Raghu, Maithra and Schmidt, Eric},
  year = {2020},
  journal = {arXiv preprint arXiv:2003.11755},
  eprint = {2003.11755},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Deep learning/A Survey of Deep Learning for Scientific Discovery.pdf}
}

@misc{rahamanSpectralBiasNeural2018,
  title = {On the {{Spectral Bias}} of {{Neural Networks}}},
  author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
  year = {2018},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-10-04},
  abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with \$100{\textbackslash}\%\$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets {\textbackslash}emph\{easier\} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.},
  howpublished = {https://arxiv.org/abs/1806.08734v3},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/UCGKET8J/Rahaman et al. - 2018 - On the Spectral Bias of Neural Networks.pdf}
}

@article{raissiPhysicsInformedDeep2017,
  title = {Physics Informed Deep Learning (Part i): {{Data-driven}} Solutions of Nonlinear Partial Differential Equations},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  year = {2017},
  journal = {arXiv preprint arXiv:1711.10561},
  eprint = {1711.10561},
  archiveprefix = {arXiv}
}

@article{raissiPhysicsinformedNeuralNetworks2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E.},
  year = {2019},
  journal = {Journal of Computational physics},
  volume = {378},
  pages = {686--707},
  publisher = {Elsevier},
  isbn = {0021-9991}
}

@article{rajbhandariDeepspeedmoeAdvancingMixtureofexperts2022,
  title = {Deepspeed-Moe: {{Advancing}} Mixture-of-Experts Inference and Training to Power next-Generation Ai Scale},
  author = {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.05596},
  eprint = {2201.05596},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Deepspeed-moe Advancing mixture-of-experts inference and training to power next-generation ai scale.pdf}
}

@misc{rajbhandariZeROInfinityBreakingGPU2021,
  title = {{{ZeRO-Infinity}}: {{Breaking}} the {{GPU Memory Wall}} for {{Extreme Scale Deep Learning}}},
  shorttitle = {{{ZeRO-Infinity}}},
  author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  year = {2021},
  month = apr,
  number = {arXiv:2104.07857},
  eprint = {2104.07857},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.07857},
  urldate = {2025-01-05},
  abstract = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40\% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance},
  file = {/Users/jingang/Zotero/storage/NRZUXJCV/Rajbhandari et al. - 2021 - ZeRO-Infinity Breaking the GPU Memory Wall for Extreme Scale Deep Learning.pdf;/Users/jingang/Zotero/storage/ZRYRHQ5K/2104.html}
}

@article{ramachandranSearchingActivationFunctions2017,
  title = {Searching for Activation Functions},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.05941},
  eprint = {1710.05941},
  archiveprefix = {arXiv}
}

@article{ramachandranSwishSelfgatedActivation2017,
  title = {Swish: A Self-Gated Activation Function},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.05941},
  volume = {7},
  eprint = {1710.05941},
  pages = {1},
  publisher = {Technical report},
  archiveprefix = {arXiv}
}

@misc{rameDiverseWeightAveraging2023,
  title = {Diverse {{Weight Averaging}} for {{Out-of-Distribution Generalization}}},
  author = {Ram{\'e}, Alexandre and Kirchmeyer, Matthieu and Rahier, Thibaud and Rakotomamonjy, Alain and Gallinari, Patrick and Cord, Matthieu},
  year = {2023},
  month = jan,
  number = {arXiv:2205.09739},
  eprint = {2205.09739},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.09739},
  urldate = {2024-08-03},
  abstract = {Standard neural networks struggle to generalize under distribution shifts in computer vision. Fortunately, combining multiple networks can consistently improve out-of-distribution generalization. In particular, weight averaging (WA) strategies were shown to perform best on the competitive DomainBed benchmark; they directly average the weights of multiple networks despite their nonlinearities. In this paper, we propose Diverse Weight Averaging (DiWA), a new WA strategy whose main motivation is to increase the functional diversity across averaged models. To this end, DiWA averages weights obtained from several independent training runs: indeed, models obtained from different runs are more diverse than those collected along a single run thanks to differences in hyperparameters and training procedures. We motivate the need for diversity by a new bias-variance-covariance-locality decomposition of the expected error, exploiting similarities between WA and standard functional ensembling. Moreover, this decomposition highlights that WA succeeds when the variance term dominates, which we show occurs when the marginal distribution changes at test time. Experimentally, DiWA consistently improves the state of the art on DomainBed without inference overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/JHVAPTEQ/Ramé et al. - 2023 - Diverse Weight Averaging for Out-of-Distribution G.pdf;/Users/jingang/Zotero/storage/XM928V5H/2205.html}
}

@inproceedings{rameFishrInvariantGradient2022,
  title = {Fishr: {{Invariant}} Gradient Variances for out-of-Distribution Generalization},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Rame, Alexandre and Dancette, Corentin and Cord, Matthieu},
  year = {2022},
  pages = {18347--18377},
  publisher = {PMLR},
  isbn = {2640-3498},
  annotation = {GSCC: 0000027}
}

@inproceedings{rameModelRatatouilleRecycling2023,
  title = {Model Ratatouille: {{Recycling}} Diverse Models for out-of-Distribution Generalization},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ram{\'e}, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, L{\'e}on and {Lopez-Paz}, David},
  year = {2023},
  pages = {28656--28679},
  publisher = {PMLR},
  isbn = {2640-3498}
}

@article{raoDiscoveringNonlinearPDEs2022,
  title = {Discovering Nonlinear {{PDEs}} from Scarce Data with Physics-Encoded Learning},
  author = {Rao, Chengping and Ren, Pu and Liu, Yang and Sun, Hao},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.12354},
  eprint = {2201.12354},
  archiveprefix = {arXiv}
}

@article{rasmussenIncreasingComputationalSpeed2006,
  title = {Increasing the Computational Speed of Flash Calculations with Applications for Compositional, Transient Simulations},
  author = {Rasmussen, Claus P and Krejbjerg, Kristian and Michelsen, Michael L and Bjurstr{\o}m, Kersti E},
  year = {2006},
  journal = {SPE Reservoir Evaluation \& Engineering},
  volume = {9},
  number = {01},
  pages = {32--38},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Increasing the computational speed of flash calculations with applications for compositional, transient simulations.pdf}
}

@inproceedings{ratzlaffHyperganGenerativeModel2019,
  title = {Hypergan: {{A}} Generative Model for Diverse, Performant Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ratzlaff, Neale and Fuxin, Li},
  year = {2019},
  pages = {5361--5369},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Hypergan A generative model for diverse, performant neural networks.pdf}
}

@misc{reddyMechanisticBasisData2023,
  title = {The Mechanistic Basis of Data Dependence and Abrupt Learning in an In-Context Classification Task},
  author = {Reddy, Gautam},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03002},
  eprint = {2312.03002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03002},
  urldate = {2024-08-30},
  abstract = {Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence. In-context learning contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/U948W8FL/Reddy - 2023 - The mechanistic basis of data dependence and abrupt learning in an in-context classification task.pdf;/Users/jingang/Zotero/storage/JI87HHG6/2312.html}
}

@inproceedings{RevisitingNearestNeighbor2024,
  title = {Revisiting {{Nearest Neighbor}} for {{Tabular Data}}: {{A Deep Tabular Baseline Two Decades Later}}},
  shorttitle = {Revisiting {{Nearest Neighbor}} for {{Tabular Data}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  year = {2024},
  month = oct,
  urldate = {2024-11-05},
  abstract = {The widespread enthusiasm for deep learning has recently expanded into the domain of tabular data. Recognizing that the advancement in deep tabular methods is often inspired by classical methods, e.g., integration of nearest neighbors into neural networks, we investigate whether these classical methods can be revitalized with modern techniques. We revisit a differentiable version of \$K\$-nearest neighbors (KNN) --- Neighbourhood Components Analysis (NCA) --- originally designed to learn a linear projection to capture semantic similarities between instances, and seek to gradually add modern deep learning techniques on top. Surprisingly, our implementation of NCA using SGD and without dimensionality reduction already achieves decent performance on tabular data, in contrast to the results of using existing toolboxes like scikit-learn. Further equipping NCA with deep representations and additional training stochasticity significantly enhances its capability, being on par with the leading tree-based method CatBoost and outperforming existing deep tabular models in both classification and regression tasks on 300 datasets. We conclude our paper by analyzing the factors behind these improvements, including loss functions, prediction strategies, and deep architectures.},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/GZZY8HNY/2024 - Revisiting Nearest Neighbor for Tabular Data A Deep Tabular Baseline Two Decades Later.pdf}
}

@article{reynoldsGaussianMixtureModels2009,
  title = {Gaussian Mixture Models.},
  author = {Reynolds, Douglas A.},
  year = {2009},
  journal = {Encyclopedia of biometrics},
  volume = {741},
  number = {659-663},
  publisher = {Berlin, Springer}
}

@inproceedings{rezendeVariationalInferenceNormalizing2015,
  title = {Variational Inference with Normalizing Flows},
  booktitle = {International Conference on Machine Learning},
  author = {Rezende, Danilo and Mohamed, Shakir},
  year = {2015},
  pages = {1530--1538},
  publisher = {PMLR},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Normalizing Flow/Variational Inference with Normalizing Flows.pdf}
}

@article{riquelmeScalingVisionSparse2021,
  title = {Scaling Vision with Sparse Mixture of Experts},
  author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {8583--8595},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Scaling Vision with Sparse Mixture of Experts.pdf}
}

@inproceedings{rishEmpiricalStudyNaive2001,
  title = {An Empirical Study of the Naive {{Bayes}} Classifier},
  booktitle = {{{IJCAI}} 2001 Workshop on Empirical Methods in Artificial Intelligence},
  author = {Rish, Irina},
  year = {2001},
  volume = {3},
  pages = {41--46}
}

@inproceedings{rishEmpiricalStudyNaive2001a,
  title = {An Empirical Study of the Naive {{Bayes}} Classifier},
  booktitle = {{{IJCAI}} 2001 Workshop on Empirical Methods in Artificial Intelligence},
  author = {Rish, Irina},
  year = {2001},
  volume = {3},
  pages = {41--46}
}

@misc{rivolliCharacterizingClassificationDatasets2019,
  title = {Characterizing Classification Datasets: A Study of Meta-Features for Meta-Learning},
  shorttitle = {Characterizing Classification Datasets},
  author = {Rivolli, Adriano and Garcia, Lu{\'i}s P. F. and Soares, Carlos and Vanschoren, Joaquin and {de Carvalho}, Andr{\'e} C. P. L. F.},
  year = {2019},
  month = aug,
  number = {arXiv:1808.10406},
  eprint = {1808.10406},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1808.10406},
  urldate = {2024-08-20},
  abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/ACMD7SS2/Rivolli et al. - 2019 - Characterizing classification datasets a study of meta-features for meta-learning.pdf;/Users/jingang/Zotero/storage/I9SU53Y4/1808.html}
}

@article{rivolliMetafeaturesMetalearning2022,
  title = {Meta-Features for Meta-Learning},
  author = {Rivolli, Adriano and Garcia, Lu{\'i}s P. F. and Soares, Carlos and Vanschoren, Joaquin and {de Carvalho}, Andr{\'e} C. P. L. F.},
  year = {2022},
  month = mar,
  journal = {Knowledge-Based Systems},
  volume = {240},
  pages = {108101},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2021.108101},
  urldate = {2024-08-20},
  abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. These recommendations are made based on meta-data, consisting of performance evaluations of algorithms and characterizations on prior datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in many studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents an extensive list of meta-features and characterization tools, which can be used as a guide for new practitioners. By identifying particularities and subtle issues related to the characterization measures, this survey points out possible future directions that the development of meta-features for meta-learning can assume.},
  keywords = {Characterization measures,Classification problems,Meta-features,Meta-learning},
  file = {/Users/jingang/Zotero/storage/AB9XATC3/Rivolli et al. - 2022 - Meta-features for meta-learning.pdf;/Users/jingang/Zotero/storage/ZKF4CYFP/S0950705121011631.html}
}

@article{robbinPredictionArteriovenousFistula2018,
  title = {Prediction of Arteriovenous Fistula Clinical Maturation from Postoperative Ultrasound Measurements: Findings from the Hemodialysis Fistula Maturation Study},
  author = {Robbin, Michelle L. and Greene, Tom and Allon, Michael and Dember, Laura M. and Imrey, Peter B. and Cheung, Alfred K. and Himmelfarb, Jonathan and Huber, Thomas S. and Kaufman, James S. and Radeva, Milena K.},
  year = {2018},
  journal = {Journal of the American Society of Nephrology},
  volume = {29},
  number = {11},
  pages = {2735--2744},
  publisher = {Am Soc Nephrol},
  isbn = {1046-6673},
  file = {/Users/jingang/Dropbox/References/AVF/Prediction of Arteriovenous Fistula Clinical Maturation from Postoperative Ultrasound Measurements Findings from the Hemodialysis Fistula Maturation Study.pdf}
}

@article{robeyModelbasedDomainGeneralization2021,
  title = {Model-Based Domain Generalization},
  author = {Robey, Alexander and Pappas, George J and Hassani, Hamed},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {20210--20229},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/Model-Based Domain Generalization.pdf}
}

@article{robeyModelbasedDomainGeneralization2021a,
  title = {Model-Based Domain Generalization},
  author = {Robey, Alexander and Pappas, George J. and Hassani, Hamed},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {20210--20229},
  annotation = {GSCC: 0000039}
}

@article{rollerHashLayersLarge2021,
  title = {Hash Layers for Large Sparse Models},
  author = {Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {17555--17566}
}

@article{rosenbaumRoutingNetworksAdaptive2017,
  title = {Routing Networks: {{Adaptive}} Selection of Non-Linear Functions for Multi-Task Learning},
  author = {Rosenbaum, Clemens and Klinger, Tim and Riemer, Matthew},
  year = {2017},
  journal = {arXiv preprint arXiv:1711.01239},
  eprint = {1711.01239},
  archiveprefix = {arXiv}
}

@article{rosenbaumRoutingNetworksChallenges2019,
  title = {Routing Networks and the Challenges of Modular and Compositional Computation},
  author = {Rosenbaum, Clemens and Cases, Ignacio and Riemer, Matthew and Klinger, Tim},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.12774},
  eprint = {1904.12774},
  archiveprefix = {arXiv}
}

@article{rosenbergerMachineLearningConsistent2022,
  title = {Machine Learning of Consistent Thermodynamic Models Using Automatic Differentiation},
  author = {Rosenberger, David and Barros, Kipton and Germann, Timothy C. and Lubbers, Nicholas},
  year = {2022},
  journal = {Physical Review E},
  volume = {105},
  number = {4},
  pages = {045301},
  publisher = {APS}
}

@article{rosenbergerMachineLearningConsistent2022a,
  title = {Machine Learning of Consistent Thermodynamic Models Using Automatic Differentiation},
  author = {Rosenberger, David and Barros, Kipton and Germann, Timothy C. and Lubbers, Nicholas},
  year = {2022},
  journal = {Physical Review E},
  volume = {105},
  number = {4},
  pages = {045301},
  publisher = {APS},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/ML EoS/Machine learning of consistent thermodynamic models using automatic differentiation.pdf}
}

@misc{ruanLanguageModelingTabular2024,
  title = {Language {{Modeling}} on {{Tabular Data}}: {{A Survey}} of {{Foundations}}, {{Techniques}} and {{Evolution}}},
  shorttitle = {Language {{Modeling}} on {{Tabular Data}}},
  author = {Ruan, Yucheng and Lan, Xiang and Ma, Jingying and Dong, Yizhi and He, Kai and Feng, Mengling},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-28},
  abstract = {Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.},
  howpublished = {https://arxiv.org/abs/2408.10548v1},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/7LHZRF9G/Ruan et al. - 2024 - Language Modeling on Tabular Data A Survey of Foundations, Techniques and Evolution.pdf}
}

@misc{rubachevTabReDAnalyzingPitfalls2024,
  title = {{{TabReD}}: {{Analyzing Pitfalls}} and {{Filling}} the {{Gaps}} in {{Tabular Deep Learning Benchmarks}}},
  shorttitle = {{{TabReD}}},
  author = {Rubachev, Ivan and Kartashev, Nikolay and Gorishniy, Yury and Babenko, Artem},
  year = {2024},
  month = oct,
  number = {arXiv:2406.19380},
  eprint = {2406.19380},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.19380},
  urldate = {2024-12-22},
  abstract = {Advances in machine learning research drive progress in real-world applications. To ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature. First, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, popular tabular datasets often lack timestamp metadata to enable such evaluation. Second, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets. In this work, we aim to understand how recent research advances in tabular deep learning transfer to these underrepresented conditions. To this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. We reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on time-based data splits leads to different methods ranking, compared to evaluation on random splits, which are common in current benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/QUD23WEH/Rubachev et al. - 2024 - TabReD Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks.pdf;/Users/jingang/Zotero/storage/WA5BZXB5/2406.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year = {1986},
  journal = {nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group}
}

@article{rundelInterpretableMachineLearning2024,
  title = {Interpretable {{Machine Learning}} for {{TabPFN}}},
  author = {Rundel, David and Kobialka, Julius and {von Crailsheim}, Constantin and Feurer, Matthias and Nagler, Thomas and R{\"u}gamer, David},
  year = {2024},
  journal = {arXiv preprint arXiv:2403.10923},
  eprint = {2403.10923},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Interpretable Machine Learning for TabPFN.pdf}
}

@misc{ruossRandomizedPositionalEncodings2023,
  title = {Randomized {{Positional Encodings Boost Length Generalization}} of {{Transformers}}},
  author = {Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and {Grau-Moya}, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  year = {2023},
  month = may,
  number = {arXiv:2305.16843},
  eprint = {2305.16843},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16843},
  urldate = {2024-10-14},
  abstract = {Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0\% on average).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/9BIT3KUC/Ruoss et al. - 2023 - Randomized Positional Encodings Boost Length Generalization of Transformers.pdf;/Users/jingang/Zotero/storage/JWNXWFR9/2305.html}
}

@misc{sagawaDistributionallyRobustNeural2020,
  title = {Distributionally {{Robust Neural Networks}} for {{Group Shifts}}: {{On}} the {{Importance}} of {{Regularization}} for {{Worst-Case Generalization}}},
  shorttitle = {Distributionally {{Robust Neural Networks}} for {{Group Shifts}}},
  author = {Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
  year = {2020},
  month = apr,
  number = {arXiv:1911.08731},
  eprint = {1911.08731},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.08731},
  urldate = {2022-10-25},
  abstract = {Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.},
  archiveprefix = {arXiv}
}

@article{sagawaExtendingWildsBenchmark2021,
  title = {Extending the Wilds Benchmark for Unsupervised Adaptation},
  author = {Sagawa, Shiori and Koh, Pang Wei and Lee, Tony and Gao, Irena and Xie, Sang Michael and Shen, Kendrick and Kumar, Ananya and Hu, Weihua and Yasunaga, Michihiro and Marklund, Henrik},
  year = {2021},
  journal = {arXiv preprint arXiv:2112.05090},
  eprint = {2112.05090},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Dataset and Benchmark/Extending the WILDS Benchmark for Unsupervised Adaptation.pdf}
}

@article{samekExplainingDeepNeural2021,
  title = {Explaining Deep Neural Networks and beyond: {{A}} Review of Methods and Applications},
  author = {Samek, Wojciech and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Anders, Christopher J and M{\"u}ller, Klaus-Robert},
  year = {2021},
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {3},
  pages = {247--278},
  publisher = {IEEE},
  issn = {0018-9219}
}

@book{sandersCUDAExampleIntroduction2010,
  title = {{{CUDA}} by Example: An Introduction to General-Purpose {{GPU}} Programming},
  author = {Sanders, Jason and Kandrot, Edward},
  year = {2010},
  publisher = {Addison-Wesley Professional},
  annotation = {GSCC: 0002939}
}

@book{santnerDesignAnalysisComputer2003,
  title = {The Design and Analysis of Computer Experiments},
  author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I. and Williams, Brain J.},
  year = {2003},
  volume = {1},
  publisher = {Springer}
}

@book{satterReservoirEngineeringFundamentals2015,
  title = {Reservoir Engineering: The Fundamentals, Simulation, and Management of Conventional and Unconventional Recoveries},
  author = {Satter, Abdus and Iqbal, Ghulam M},
  year = {2015},
  publisher = {Gulf Professional Publishing}
}

@article{scarselliGraphNeuralNetwork2008,
  title = {The Graph Neural Network Model},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  year = {2008},
  journal = {IEEE transactions on neural networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  publisher = {IEEE},
  isbn = {1045-9227}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  journal = {Neural networks},
  volume = {61},
  pages = {85--117},
  publisher = {Elsevier},
  isbn = {0893-6080}
}

@article{schmitzArtificialNeuralNetworks2006,
  title = {Artificial Neural Networks for the Solution of the Phase Stability Problem},
  author = {Schmitz, Jones E. and Zemp, Roger J. and Mendes, M{\'a}rio J.},
  year = {2006},
  journal = {Fluid Phase Equilibria},
  volume = {245},
  number = {1},
  pages = {83--87},
  publisher = {Elsevier},
  isbn = {0378-3812}
}

@article{schoenholzJaxMdFramework2020,
  title = {Jax Md: A Framework for Differentiable Physics},
  author = {Schoenholz, Samuel and Cubuk, Ekin Dogus},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {11428--11441}
}

@article{schwinghammerDeterminationModellingHighpressure2006,
  title = {Determination and Modelling of the High-Pressure Vapour--Liquid Equilibrium Carbon Dioxide--Methyl Acetate},
  author = {Schwinghammer, Stephan and Siebenhofer, Matth{\"a}us and Marr, Rolf},
  year = {2006},
  journal = {The Journal of supercritical fluids},
  volume = {38},
  number = {1},
  pages = {1--6},
  publisher = {Elsevier},
  issn = {0896-8446}
}

@article{seegerGaussianProcessesMachine2004,
  title = {Gaussian Processes for Machine Learning},
  author = {Seeger, Matthias},
  year = {2004},
  journal = {International journal of neural systems},
  volume = {14},
  number = {02},
  pages = {69--106},
  publisher = {World Scientific},
  isbn = {0129-0657}
}

@article{semenovaPHASEEQUILIBRIAMETHANOLCARBON1979,
  title = {{{PHASE-EQUILIBRIA IN METHANOL CARBON DIOXIDE SYSTEM}}},
  author = {Semenova, {\relax AI} and Emelyanova, {\relax EA} and Tsimmerman, {\relax SS} and Tsiklis, {\relax DS}},
  year = {1979},
  journal = {Zhurnal Fizicheskoi Khimii},
  volume = {53},
  number = {10},
  pages = {2502--2505},
  publisher = {MEZHDUNARODNAYA KNIGA 39 DIMITROVA UL., 113095 MOSCOW, RUSSIA},
  issn = {0044-4537}
}

@article{senderaHypershotFewshotLearning2022,
  title = {Hypershot: {{Few-shot}} Learning by Kernel Hypernetworks},
  author = {Sendera, Marcin and Przewiezlikowski, Marcin and Karanowski, Konrad and Zieba, Maciej and Tabor, Jacek and Spurek, Przemyslaw},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.11378},
  eprint = {2203.11378},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Hypershot Few-shot learning by kernel hypernetworks.pdf}
}

@article{setzmannNewEquationState1991,
  title = {A New Equation of State and Tables of Thermodynamic Properties for Methane Covering the Range from the Melting Line to 625 {{K}} at Pressures up to 1000 {{MPa}}},
  author = {Setzmann, Ulrich and Wagner, Wolfgang},
  year = {1991},
  journal = {Journal of Physical and Chemical reference data},
  volume = {20},
  number = {6},
  pages = {1061--1155},
  publisher = {American Institute of Physics for the National Institute of Standards and~{\dots}},
  issn = {0047-2689},
  annotation = {GSCC: 0001286}
}

@inproceedings{shamsianPersonalizedFederatedLearning2021,
  title = {Personalized Federated Learning Using Hypernetworks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
  year = {2021},
  pages = {9489--9502},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Personalized federated learning using hypernetworks.pdf}
}

@article{shankarGeneralizingDomainsCrossgradient2018,
  title = {Generalizing across Domains via Cross-Gradient Training},
  author = {Shankar, Shiv and Piratla, Vihari and Chakrabarti, Soumen and Chaudhuri, Siddhartha and Jyothi, Preethi and Sarawagi, Sunita},
  year = {2018},
  journal = {arXiv preprint arXiv:1804.10745},
  eprint = {1804.10745},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000286}
}

@inproceedings{shaoMultiadversarialDiscriminativeDeep2019,
  title = {Multi-Adversarial Discriminative Deep Domain Generalization for Face Presentation Attack Detection},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shao, Rui and Lan, Xiangyuan and Li, Jiawei and Yuen, Pong C.},
  year = {2019},
  pages = {10023--10031},
  annotation = {GSCC: 0000127}
}

@article{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously Large Neural Networks: {{The}} Sparsely-Gated Mixture-of-Experts Layer},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.06538},
  eprint = {1701.06538},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Outrageously Large Neural Networks The Sparsely-Gated Mixture-of-Experts Layer.pdf}
}

@misc{shenPretrainedTransformersLearn2024,
  title = {Do Pretrained {{Transformers Learn In-Context}} by {{Gradient Descent}}?},
  author = {Shen, Lingfeng and Mishra, Aayush and Khashabi, Daniel},
  year = {2024},
  month = jun,
  number = {arXiv:2310.08540},
  eprint = {2310.08540},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.08540},
  urldate = {2024-08-30},
  abstract = {The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses {\textbackslash}emph\{ICL objective\} (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that {\textbackslash}emph\{the equivalence between ICL and GD remains an open hypothesis\} and calls for further studies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/FB88V29E/Shen et al. - 2024 - Do pretrained Transformers Learn In-Context by Gradient Descent.pdf;/Users/jingang/Zotero/storage/2CJIHA3Q/2310.html}
}

@article{shiGradientMatchingDomain2021,
  title = {Gradient Matching for Domain Generalization},
  author = {Shi, Yuge and Seely, Jeffrey and Torr, Philip HS and Siddharth, N. and Hannun, Awni and Usunier, Nicolas and Synnaeve, Gabriel},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.09937},
  eprint = {2104.09937},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000056}
}

@article{shiGradientMatchingDomain2021a,
  title = {Gradient Matching for Domain Generalization},
  author = {Shi, Yuge and Seely, Jeffrey and Torr, Philip HS and Siddharth, N and Hannun, Awni and Usunier, Nicolas and Synnaeve, Gabriel},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.09937},
  eprint = {2104.09937},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/GRADIENT MATCHING FOR DOMAIN GENERALIZATION.pdf}
}

@misc{shmuelComprehensiveBenchmarkMachine2024,
  title = {A {{Comprehensive Benchmark}} of {{Machine}} and {{Deep Learning Across Diverse Tabular Datasets}}},
  author = {Shmuel, Assaf and Glickman, Oren and Lazebnik, Teddy},
  year = {2024},
  month = aug,
  number = {arXiv:2408.14817},
  eprint = {2408.14817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.14817},
  urldate = {2024-08-28},
  abstract = {The analysis of tabular datasets is highly prevalent both in scientific research and real-world applications of Machine Learning (ML). Unlike many other ML tasks, Deep Learning (DL) models often do not outperform traditional methods in this area. Previous comparative benchmarks have shown that DL performance is frequently equivalent or even inferior to models such as Gradient Boosting Machines (GBMs). In this study, we introduce a comprehensive benchmark aimed at better characterizing the types of datasets where DL models excel. Although several important benchmarks for tabular datasets already exist, our contribution lies in the variety and depth of our comparison: we evaluate 111 datasets with 20 different models, including both regression and classification tasks. These datasets vary in scale and include both those with and without categorical variables. Importantly, our benchmark contains a sufficient number of datasets where DL models perform best, allowing for a thorough analysis of the conditions under which DL models excel. Building on the results of this benchmark, we train a model that predicts scenarios where DL models outperform alternative methods with 86.1\% accuracy (AUC 0.78). We present insights derived from this characterization and compare these findings to previous benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/BFYDK955/Shmuel et al. - 2024 - A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets.pdf;/Users/jingang/Zotero/storage/B3DDCYBD/2408.html}
}

@inproceedings{shuOpenDomainGeneralization2021,
  title = {Open Domain Generalization with Domain-Augmented Meta-Learning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shu, Yang and Cao, Zhangjie and Wang, Chenyu and Wang, Jianmin and Long, Mingsheng},
  year = {2021},
  pages = {9624--9633},
  annotation = {GSCC: 0000032}
}

@article{sillaSurveyHierarchicalClassification2011,
  title = {A Survey of Hierarchical Classification across Different Application Domains},
  author = {Silla, Carlos N. and Freitas, Alex A.},
  year = {2011},
  month = jan,
  journal = {Data Mining and Knowledge Discovery},
  volume = {22},
  number = {1},
  pages = {31--72},
  issn = {1573-756X},
  doi = {10.1007/s10618-010-0175-9},
  urldate = {2025-01-27},
  abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
  langid = {english},
  keywords = {Artificial Intelligence,DAG-structured class hierarchies,Hierarchical classification,Tree-structured class hierarchies},
  file = {/Users/jingang/Zotero/storage/SP3A7P37/Silla and Freitas - 2011 - A survey of hierarchical classification across different application domains.pdf}
}

@article{simonyanVeryDeepConvolutional2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.1556},
  eprint = {1409.1556},
  archiveprefix = {arXiv}
}

@misc{singhTransientNatureEmergent2023,
  title = {The {{Transient Nature}} of {{Emergent In-Context Learning}} in {{Transformers}}},
  author = {Singh, Aaditya K. and Chan, Stephanie C. Y. and Moskovitz, Ted and Grant, Erin and Saxe, Andrew M. and Hill, Felix},
  year = {2023},
  month = dec,
  number = {arXiv:2311.08360},
  eprint = {2311.08360},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.08360},
  urldate = {2024-08-30},
  abstract = {Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to "overtrain" transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/XPJXDX7R/Singh et al. - 2023 - The Transient Nature of Emergent In-Context Learning in Transformers.pdf;/Users/jingang/Zotero/storage/5PAR38KR/2311.html}
}

@article{sivaprasadReappraisingDomainGeneralization2021,
  title = {Reappraising {{Domain Generalization}} in {{Neural Networks}}},
  author = {Sivaprasad, Sarath and Goindani, Akshay and Garg, Vaibhav and Gandhi, Vineet},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.07981},
  eprint = {2110.07981},
  archiveprefix = {arXiv}
}

@inproceedings{smithCyclicalLearningRates2017,
  title = {Cyclical Learning Rates for Training Neural Networks},
  booktitle = {2017 {{IEEE}} Winter Conference on Applications of Computer Vision ({{WACV}})},
  author = {Smith, Leslie N},
  year = {2017},
  pages = {464--472},
  publisher = {IEEE}
}

@article{smithInstanceLevelAnalysis2014,
  title = {An Instance Level Analysis of Data Complexity},
  author = {Smith, Michael R. and Martinez, Tony and {Giraud-Carrier}, Christophe},
  year = {2014},
  journal = {Machine learning},
  volume = {95},
  number = {2},
  pages = {225--256},
  publisher = {Springer},
  isbn = {1573-0565}
}

@article{smithIntroductionChemicalEngineering1950,
  title = {Introduction to Chemical Engineering Thermodynamics},
  author = {Smith, Joseph Mauk},
  year = {1950},
  publisher = {ACS Publications},
  issn = {0021-9584},
  annotation = {GSCC: 0007595}
}

@article{smithNoMorePesky2015,
  title = {No More Pesky Learning Rate Guessing Games},
  author = {Smith, Leslie N},
  year = {2015},
  journal = {CoRR, abs/1506.01186},
  volume = {5},
  eprint = {1506.01186},
  archiveprefix = {arXiv}
}

@inproceedings{smithSuperconvergenceVeryFast2019,
  title = {Super-Convergence: {{Very}} Fast Training of Neural Networks Using Large Learning Rates},
  booktitle = {Artificial {{Intelligence}} and {{Machine Learning}} for {{Multi-Domain Operations Applications}}},
  author = {Smith, Leslie N and Topin, Nicholay},
  year = {2019},
  volume = {11006},
  pages = {1100612},
  publisher = {{International Society for Optics and Photonics}}
}

@article{smukalaNewEquationState2000,
  title = {New Equation of State for Ethylene Covering the Fluid Region for Temperatures from the Melting Line to 450 {{K}} at Pressures up to 300 {{MPa}}},
  author = {Smukala, J and Span, Roland and Wagner, Wolfgang},
  year = {2000},
  journal = {Journal of Physical and Chemical Reference Data},
  volume = {29},
  number = {5},
  pages = {1053--1121},
  publisher = {American Institute of Physics for the National Institute of Standards and~{\dots}},
  issn = {0047-2689},
  annotation = {GSCC: 0000156}
}

@article{snoekPracticalBayesianOptimization2012,
  title = {Practical Bayesian Optimization of Machine Learning Algorithms},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  year = {2012},
  journal = {Advances in neural information processing systems},
  volume = {25}
}

@inproceedings{snoekScalableBayesianOptimization2015,
  title = {Scalable Bayesian Optimization Using Deep Neural Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  year = {2015},
  pages = {2171--2180},
  publisher = {PMLR}
}

@article{soaveEquilibriumConstantsModified1972,
  title = {Equilibrium Constants from a Modified {{Redlich-Kwong}} Equation of State},
  author = {Soave, Giorgio},
  year = {1972},
  journal = {Chemical engineering science},
  volume = {27},
  number = {6},
  pages = {1197--1203},
  publisher = {Elsevier}
}

@article{somepalliSaintImprovedNeural2021,
  title = {Saint: {{Improved}} Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training},
  author = {Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  year = {2021},
  journal = {arXiv preprint arXiv:2106.01342},
  eprint = {2106.01342},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Improved neural networks for tabular data via row attention and contrastive pre-training.pdf}
}

@inproceedings{songAutointAutomaticFeature2019,
  title = {Autoint: {{Automatic}} Feature Interaction Learning via Self-Attentive Neural Networks},
  booktitle = {Proceedings of the 28th {{ACM}} International Conference on Information and Knowledge Management},
  author = {Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
  year = {2019},
  pages = {1161--1170},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Autoint- Automatic feature interaction learning via self-attentive neural networks.pdf}
}

@article{sonodaNeuralNetworkUnbounded2017,
  title = {Neural Network with Unbounded Activation Functions Is Universal Approximator},
  author = {Sonoda, Sho and Murata, Noboru},
  year = {2017},
  journal = {Applied and Computational Harmonic Analysis},
  volume = {43},
  number = {2},
  pages = {233--268},
  publisher = {Elsevier},
  isbn = {1063-5203}
}

@article{spanMultiparameterEquationsState2001,
  title = {Multiparameter Equations of State---Recent Trends and Future Challenges},
  author = {Span, R and Wagner, Wolfgang and Lemmon, {\relax EW} and Jacobsen, {\relax RT}},
  year = {2001},
  journal = {Fluid Phase Equilibria},
  volume = {183},
  pages = {1--20},
  publisher = {Elsevier},
  issn = {0378-3812},
  annotation = {GSCC: 0000101}
}

@article{spanNewEquationState1996,
  title = {A New Equation of State for Carbon Dioxide Covering the Fluid Region from the Triple-point Temperature to 1100 {{K}} at Pressures up to 800 {{MPa}}},
  author = {Span, Roland and Wagner, Wolfgang},
  year = {1996},
  journal = {Journal of physical and chemical reference data},
  volume = {25},
  number = {6},
  pages = {1509--1596},
  publisher = {American Institute of Physics for the National Institute of Standards and~{\dots}},
  issn = {0047-2689},
  annotation = {GSCC: 0005558}
}

@article{spanReferenceQualityEquation1998,
  title = {A Reference Quality Equation of State for Nitrogen},
  author = {Span, R and Lemmon, {\relax EW} and Jacobsen, {\relax RT} and Wagner, Wolfgang},
  year = {1998},
  journal = {International Journal of Thermophysics},
  volume = {19},
  pages = {1121--1132},
  publisher = {Springer},
  issn = {0195-928X},
  annotation = {GSCC: 0000939}
}

@article{spathAlgorithm39Clusterwise1979,
  title = {Algorithm 39 Clusterwise Linear Regression},
  author = {Sp{\"a}th, Helmuth},
  year = {1979},
  journal = {Computing},
  volume = {22},
  number = {4},
  pages = {367--373},
  publisher = {Springer},
  isbn = {1436-5057},
  file = {/Users/jingang/Dropbox/References/Regression clustering/Algorithm 39 Clusterwise linear regression.pdf}
}

@misc{spinaciPORTALScalableTabular2024,
  title = {{{PORTAL}}: {{Scalable Tabular Foundation Models}} via {{Content-Specific Tokenization}}},
  shorttitle = {{{PORTAL}}},
  author = {Spinaci, Marco and Polewczyk, Marek and Hoffart, Johannes and Kohler, Markus C. and Thelin, Sam and Klein, Tassilo},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13516},
  eprint = {2410.13516},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13516},
  urldate = {2024-11-05},
  abstract = {Self-supervised learning on tabular data seeks to apply advances from natural language and image domains to the diverse domain of tables. However, current techniques often struggle with integrating multi-domain data and require data cleaning or specific structural requirements, limiting the scalability of pre-training datasets. We introduce PORTAL (Pretraining One-Row-at-a-Time for All tabLes), a framework that handles various data modalities without the need for cleaning or preprocessing. This simple yet powerful approach can be effectively pre-trained on online-collected datasets and fine-tuned to match state-of-the-art methods on complex classification and regression tasks. This work offers a practical advancement in self-supervised learning for large-scale tabular data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/K2VHKL2N/Spinaci et al. - 2024 - PORTAL Scalable Tabular Foundation Models via Content-Specific Tokenization.pdf;/Users/jingang/Zotero/storage/XNSUC5IJ/2410.html}
}

@article{srivastavaBayesianQuadraticDiscriminant2007,
  title = {Bayesian Quadratic Discriminant Analysis.},
  author = {Srivastava, Santosh and Gupta, Maya R. and Frigyik, B{\'e}la A.},
  year = {2007},
  journal = {Journal of Machine Learning Research},
  volume = {8},
  number = {6},
  isbn = {1532-4435}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {The journal of machine learning research},
  volume = {15},
  number = {1},
  pages = {1929--1958},
  publisher = {JMLR. org}
}

@inproceedings{standleyWhichTasksShould2020,
  title = {Which Tasks Should Be Learned Together in Multi-Task Learning?},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Standley, Trevor and Zamir, Amir and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  year = {2020},
  pages = {9120--9132},
  publisher = {PMLR},
  isbn = {2640-3498}
}

@techreport{stevensAIScienceReport2020,
  title = {{{AI}} for {{Science}}: {{Report}} on the {{Department}} of {{Energy}} ({{DOE}}) {{Town Halls}} on {{Artificial Intelligence}} ({{AI}}) for {{Science}}},
  author = {Stevens, Rick and Taylor, Valerie and Nichols, Jeff and Maccabe, Arthur Barney and Yelick, Katherine and Brown, David},
  year = {2020},
  institution = {Argonne National Lab.(ANL), Argonne, IL (United States)}
}

@article{subbaswamyUnifyingCausalFramework2022,
  title = {A Unifying Causal Framework for Analyzing Dataset Shift-Stable Learning Algorithms},
  author = {Subbaswamy, Adarsh and Chen, Bryant and Saria, Suchi},
  year = {2022},
  journal = {Journal of Causal Inference},
  volume = {10},
  number = {1},
  pages = {64--89},
  publisher = {De Gruyter},
  isbn = {2193-3685}
}

@inproceedings{sunDeepCoralCorrelation2016,
  title = {Deep Coral: {{Correlation}} Alignment for Deep Domain Adaptation},
  booktitle = {European Conference on Computer Vision},
  author = {Sun, Baochen and Saenko, Kate},
  year = {2016},
  pages = {443--450},
  publisher = {Springer},
  annotation = {GSCC: 0001803}
}

@misc{sunLengthExtrapolatableTransformer2022,
  title = {A {{Length-Extrapolatable Transformer}}},
  author = {Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10554},
  eprint = {2212.10554},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10554},
  urldate = {2024-08-28},
  abstract = {Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available at https://aka.ms/LeX-Transformer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/I8NZPII5/Sun et al. - 2022 - A Length-Extrapolatable Transformer.pdf;/Users/jingang/Zotero/storage/A8EBKTWF/2212.html}
}

@misc{suRoFormerEnhancedTransformer2023,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.09864},
  urldate = {2024-08-28},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/Z7T9K7G8/Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Position Embedding.pdf;/Users/jingang/Zotero/storage/TQ47KISD/2104.html}
}

@misc{suTableGPT2LargeMultimodal2024,
  title = {{{TableGPT2}}: {{A Large Multimodal Model}} with {{Tabular Data Integration}}},
  shorttitle = {{{TableGPT2}}},
  author = {Su, Aofeng and Wang, Aowen and Ye, Chao and Zhou, Chen and Zhang, Ga and Zhu, Guangcheng and Wang, Haobo and Xu, Haokai and Chen, Hao and Li, Haoze and Lan, Haoxuan and Tian, Jiaming and Yuan, Jing and Zhao, Junbo and Zhou, Junlin and Shou, Kaizhe and Zha, Liangyu and Long, Lin and Li, Liyao and Wu, Pengzuo and Zhang, Qi and Huang, Qingyi and Yang, Saisai and Zhang, Tao and Ye, Wentao and Zhu, Wufang and Hu, Xiaomeng and Gu, Xijun and Sun, Xinjie and Li, Xiang and Yang, Yuhang and Xiao, Zhiqing},
  year = {2024},
  month = nov,
  number = {arXiv:2411.02059},
  eprint = {2411.02059},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.02059},
  urldate = {2024-11-07},
  abstract = {The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI applications, presenting vast new opportunities across industries. Yet, the integration of tabular data remains notably underdeveloped, despite its foundational role in numerous real-world domains. This gap is critical for three main reasons. First, database or data warehouse data integration is essential for advanced applications; second, the vast and largely untapped resource of tabular data offers immense potential for analysis; and third, the business intelligence domain specifically demands adaptable, precise solutions that many current LLMs may struggle to provide. In response, we introduce TableGPT2, a model rigorously pre-trained and fine-tuned with over 593.8K tables and 2.36M high-quality query-table-output tuples, a scale of table-related data unprecedented in prior research. This extensive training enables TableGPT2 to excel in table-centric tasks while maintaining strong general language and coding abilities. One of TableGPT2's key innovations is its novel table encoder, specifically designed to capture schema-level and cell-level information. This encoder strengthens the model's ability to handle ambiguous queries, missing column names, and irregular tables commonly encountered in real-world applications. Similar to visual language models, this pioneering approach integrates with the decoder to form a robust large multimodal model. We believe the results are compelling: over 23 benchmarking metrics, TableGPT2 achieves an average performance improvement of 35.20\% in the 7B model and 49.32\% in the 72B model over prior benchmark-neutral LLMs, with robust general-purpose capabilities intact.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/KLPMPDGY/Su et al. - 2024 - TableGPT2 A Large Multimodal Model with Tabular Data Integration.pdf;/Users/jingang/Zotero/storage/NB9NUM45/2411.html}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {International Conference on Machine Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year = {2013},
  pages = {1139--1147},
  publisher = {PMLR}
}

@article{suzukiIsothermalVaporliquidEquilibrium1990,
  title = {Isothermal Vapor-Liquid Equilibrium Data for Binary Systems at High Pressures: Carbon Dioxide-Methanol, Carbon Dioxide-Ethanol, Carbon Dioxide-1-Propanol, Methane-Ethanol, Methane-1-Propanol, Ethane-Ethanol, and Ethane-1-Propanol Systems},
  author = {Suzuki, Kazuhiko and Sue, Haruhusa and Itou, Masahiro and Smith, Richard L and Inomata, Hiroshi and Arai, Kunio and Saito, Shozaburo},
  year = {1990},
  journal = {Journal of chemical and Engineering Data},
  volume = {35},
  number = {1},
  pages = {63--66},
  publisher = {ACS Publications},
  issn = {0021-9568}
}

@article{swestyThermodynamicallyConsistentInterpolation1996,
  title = {Thermodynamically Consistent Interpolation for Equation of State Tables},
  author = {Swesty, F Douglas},
  year = {1996},
  journal = {Journal of Computational Physics},
  volume = {127},
  number = {1},
  pages = {118--127},
  publisher = {Elsevier},
  issn = {0021-9991}
}

@inproceedings{TabFlexScalingTabular2024,
  title = {{{TabFlex}}: {{Scaling Tabular Learning}} to {{Millions}} with {{Linear Attention}}},
  shorttitle = {{{TabFlex}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  year = {2024},
  month = oct,
  urldate = {2024-11-13},
  abstract = {Recent advances in the field of in-context learning (ICL) have demonstrated impressive performance for tabular classification, exemplified by TabPFN's success on small datasets. However, the quadratic complexity of the attention mechanism limits its applicability to larger datasets. To address this issue, we conduct a comprehensive comparison of popular scalable attention alternatives, including state-space models (SSMs) and linear attention mechanisms, revealing that the inherent causality of SSMs hinders ICL performance for large datasets, while linear attention preserves effectiveness. Leveraging these insights, we introduce TabFlex, a model based on linear attention that supports thousands of features and hundreds of classes, capable of handling datasets with millions of samples. Extensive experiments demonstrate that TabFlex is significantly faster than most existing methods while achieving top-two performance on small datasets among 25 baselines, with a 2\${\textbackslash}times\$ speedup over TabPFN and a 1.5\${\textbackslash}times\$ speedup over XGBoost. On large datasets, TabFlex remains efficient (e.g., approximately 5 seconds on the `poker-hand` dataset, which consists of millions of samples), while achieving relatively solid performance.},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/AXN9NTH5/2024 - TabFlex Scaling Tabular Learning to Millions with Linear Attention.pdf}
}

@misc{tancikFourierFeaturesLet2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  year = {2020},
  month = jun,
  number = {arXiv:2006.10739},
  eprint = {2006.10739},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.10739},
  urldate = {2024-08-25},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/DE2GPU35/Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains.pdf;/Users/jingang/Zotero/storage/FY2G6IFT/2006.html}
}

@article{tayHypergridTransformersSingle2021,
  title = {Hypergrid Transformers: {{Towards}} a Single Model for Multiple Tasks},
  author = {Tay, Yi and Zhao, Zhe and Bahri, Dara and Metzler, Don and Juan, Da-Cheng},
  year = {2021},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Hypergrid transformers Towards a single model for multiple tasks.pdf}
}

@misc{teamGemma2Improving2024,
  title = {Gemma 2: {{Improving Open Language Models}} at a {{Practical Size}}},
  shorttitle = {Gemma 2},
  author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and Ferret, Johan and Liu, Peter and Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos, Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur, Behnam and Bachem, Olivier and Walton, Alanna and Severyn, Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison, Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and Brock, Andy and Coenen, Andy and Laforge, Anthony and Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo and Royal, Brandon and Chen, Charlie and Kumar, Chintu and Perry, Chris and Welty, Chris and {Choquette-Choo}, Christopher A. and Sinopalnikov, Danila and Weinberger, David and Vijaykumar, Dimple and Rogozi{\'n}ska, Dominika and Herbison, Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and Visin, Francesco and Rasskin, Gabriel and Wei, Gary and Cameron, Glenn and Martins, Gus and Hashemi, Hadi and {Klimczak-Pluci{\'n}ska}, Hanna and Batra, Harleen and Dhand, Harsh and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou, Jin Peng and Carrasqueira, Joana and Iljazi, Joana and Becker, Jocelyn and Fernandez, Joe and van Amersfoort, Joost and Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji, Ju-yeong and Mohamed, Kareem and Badola, Kartikeya and Black, Kat and Millican, Katie and McDonell, Keelin and Nguyen, Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund, Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann, Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins, Luciano and Reid, Machel and Singh, Manvinder and Iverson, Mark and G{\"o}rner, Martin and Velloso, Mat and Wirth, Mateo and Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson, Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan, Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and Barnes, Parker and Barham, Paul and Michel, Paul and Jin, Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala, Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana, Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and Cogan, Sarah and Perrin, Sarah and Arnold, S{\'e}bastien M. R. and Krause, Sebastian and Dai, Shengyang and Garg, Shruti and Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan, Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav, Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao, Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Sculley, D. and Banks, Jeanine and Dragan, Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel, Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi, Robert and Andreev, Alek},
  year = {2024},
  month = oct,
  number = {arXiv:2408.00118},
  eprint = {2408.00118},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.00118},
  urldate = {2024-11-22},
  abstract = {In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/5564TK32/Team et al. - 2024 - Gemma 2 Improving Open Language Models at a Practical Size.pdf;/Users/jingang/Zotero/storage/CVBZUYUW/2408.html}
}

@article{tegelerNewEquationState1999,
  title = {A New Equation of State for Argon Covering the Fluid Region for Temperatures from the Melting Line to 700 {{K}} at Pressures up to 1000 {{MPa}}},
  author = {Tegeler, Ch and Span, Roland and Wagner, Wolfgang},
  year = {1999},
  journal = {Journal of Physical and Chemical Reference Data},
  volume = {28},
  number = {3},
  pages = {779--850},
  publisher = {American Institute of Physics for the National Institute of Standards and~{\dots}},
  isbn = {0047-2689},
  annotation = {GSCC: 0000644}
}

@article{tengUnderstandingGeneralizationDecomposing2021,
  title = {Towards {{Understanding Generalization}} via {{Decomposing Excess Risk Dynamics}}},
  author = {Teng, Jiaye and Ma, Jianhao and Yuan, Yang},
  year = {2021},
  journal = {arXiv preprint arXiv:2106.06153},
  eprint = {2106.06153},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/TOWARDS UNDERSTANDING GENERALIZATION VIA DECOMPOSING EXCESS RISK DYNAMICS.pdf}
}

@misc{thawaniRepresentingNumbersNLP2021,
  title = {Representing {{Numbers}} in {{NLP}}: A {{Survey}} and a {{Vision}}},
  shorttitle = {Representing {{Numbers}} in {{NLP}}},
  author = {Thawani, Avijit and Pujara, Jay and Szekely, Pedro A. and Ilievski, Filip},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13136},
  eprint = {2103.13136},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.13136},
  urldate = {2025-01-05},
  abstract = {NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by 18 previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/NZ6PG8RP/Thawani et al. - 2021 - Representing Numbers in NLP a Survey and a Vision.pdf;/Users/jingang/Zotero/storage/SH4QRY42/2103.html}
}

@misc{thielmannMambularSequentialModel2024,
  title = {Mambular: {{A Sequential Model}} for {{Tabular Deep Learning}}},
  shorttitle = {Mambular},
  author = {Thielmann, Anton Frederik and Kumar, Manish and Weisser, Christoph and Reuter, Arik and S{\"a}fken, Benjamin and Samiee, Soheila},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-08-28},
  abstract = {The analysis of tabular data has traditionally been dominated by gradient-boosted decision trees (GBDTs), known for their proficiency with mixed categorical and numerical features. However, recent deep learning innovations are challenging this dominance. We introduce Mambular, an adaptation of the Mamba architecture optimized for tabular data. We extensively benchmark Mambular against state-of-the-art models, including neural networks and tree-based methods, and demonstrate its competitive performance across diverse datasets. Additionally, we explore various adaptations of Mambular to understand its effectiveness for tabular data. We investigate different pooling strategies, feature interaction mechanisms, and bi-directional processing. Our analysis shows that interpreting features as a sequence and passing them through Mamba layers results in surprisingly performant models. The results highlight Mambulars potential as a versatile and powerful architecture for tabular data analysis, expanding the scope of deep learning applications in this domain. The source code is available at https://github.com/basf/mamba-tabular.},
  howpublished = {https://arxiv.org/abs/2408.06291v1},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/HYKWVJAM/Thielmann et al. - 2024 - Mambular A Sequential Model for Tabular Deep Learning.pdf}
}

@article{tholEquationStateLennardJones2016,
  title = {Equation of State for the {{Lennard-Jones}} Fluid},
  author = {Thol, Monika and Rutkai, Gabor and K{\"o}ster, Andreas and Lustig, Rolf and Span, Roland and Vrabec, Jadran},
  year = {2016},
  journal = {Journal of Physical and Chemical Reference Data},
  volume = {45},
  number = {2},
  pages = {023101},
  publisher = {{AIP Publishing LLC for the National Institute of Standards and Technology}},
  isbn = {0047-2689}
}

@misc{thomasRetrievalFineTuningInContext2024,
  title = {Retrieval \& {{Fine-Tuning}} for {{In-Context Tabular Models}}},
  author = {Thomas, Valentin and Ma, Junwei and Hosseinzadeh, Rasa and Golestan, Keyvan and Yu, Guangwei and Volkovs, Maksims and Caterini, Anthony},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05207},
  eprint = {2406.05207},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05207},
  urldate = {2024-12-17},
  abstract = {Tabular data is a pervasive modality spanning a wide range of domains, and the inherent diversity poses a considerable challenge for deep learning. Recent advancements using transformer-based in-context learning have shown promise on smaller and less complex datasets, but have struggled to scale to larger and more complex ones. To address this limitation, we propose a combination of retrieval and fine-tuning: we can adapt the transformer to a local subset of the data by collecting nearest neighbours, and then perform task-specific fine-tuning with this retrieved set of neighbours in context. Using TabPFN as the base model -- currently the best tabular in-context learner -- and applying our retrieval and fine-tuning scheme on top results in what we call a locally-calibrated PFN, or LoCalPFN. We conduct extensive evaluation on 95 datasets curated by TabZilla from OpenML, upon which we establish a new state-of-the-art with LoCalPFN -- even with respect to tuned tree-based models. Notably, we show a significant boost in performance compared to the base in-context model, demonstrating the efficacy of our approach and advancing the frontier of deep learning in tabular data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/TWYWMT7I/Thomas et al. - 2024 - Retrieval & Fine-Tuning for In-Context Tabular Models.pdf;/Users/jingang/Zotero/storage/C4HAHVEP/2406.html}
}

@article{tielemanLecture5rmspropDivide2012,
  title = {Lecture 6.5-Rmsprop: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  year = {2012},
  journal = {COURSERA: Neural networks for machine learning},
  volume = {4},
  number = {2},
  pages = {26--31}
}

@article{timmesAccuracyConsistencySpeed1999,
  title = {The Accuracy, Consistency, and Speed of Five Equations of State for Stellar Hydrodynamics},
  author = {Timmes, {\relax FX} and Arnett, Dave},
  year = {1999},
  journal = {The Astrophysical Journal Supplement Series},
  volume = {125},
  number = {1},
  pages = {277},
  publisher = {IOP Publishing},
  issn = {0067-0049}
}

@article{tippingSparseBayesianLearning2001,
  title = {Sparse {{Bayesian}} Learning and the Relevance Vector Machine},
  author = {Tipping, Michael E},
  year = {2001},
  journal = {Journal of machine learning research},
  volume = {1},
  number = {Jun},
  pages = {211--244}
}

@article{tochigiMeasurementPredictionHighpressure2010,
  title = {Measurement and Prediction of High-Pressure Vapor--Liquid Equilibria for Binary Mixtures of Carbon Dioxide+ n-Octane, Methanol, Ethanol, and Perfluorohexane},
  author = {Tochigi, Katsumi and Namae, Tooru and Suga, Tooru and Matsuda, Hiroyuki and Kurihara, Kiyofumi and {dos Ramos}, M Carolina and McCabe, Clare},
  year = {2010},
  journal = {The Journal of Supercritical Fluids},
  volume = {55},
  number = {2},
  pages = {682--689},
  publisher = {Elsevier},
  issn = {0896-8446}
}

@article{tomczakImprovingVariationalAutoencoders2016,
  title = {Improving Variational Auto-Encoders Using Householder Flow},
  author = {Tomczak, Jakub M and Welling, Max},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.09630},
  eprint = {1611.09630},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Normalizing Flow/Improving Variational Auto-Encoders using Householder Flow.pdf}
}

@inproceedings{tsaiMiceMixtureContrastive2020,
  title = {Mice: {{Mixture}} of Contrastive Experts for Unsupervised Image Clustering},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Tsai, Tsung Wei and Li, Chongxuan and Zhu, Jun},
  year = {2020},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Mice Mixture of contrastive experts for unsupervised image clustering.pdf}
}

@article{tschannenRecentAdvancesAutoencoderbased2018,
  title = {Recent Advances in Autoencoder-Based Representation Learning},
  author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  year = {2018},
  journal = {arXiv preprint arXiv:1812.05069},
  eprint = {1812.05069},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Overview/Recent Advances in Autoencoder-Based Representation Learning.pdf}
}

@article{tyskiewiczApplicationSupercriticalFluid2018,
  title = {The Application of Supercritical Fluid Extraction in Phenolic Compounds Isolation from Natural Plant Materials},
  author = {Ty{\'s}kiewicz, Katarzyna and Konkol, Marcin and R{\'o}j, Edward},
  year = {2018},
  journal = {Molecules},
  volume = {23},
  number = {10},
  pages = {2625},
  publisher = {MDPI},
  issn = {1420-3049}
}

@article{tzengDeepDomainConfusion2014,
  title = {Deep Domain Confusion: {{Maximizing}} for Domain Invariance},
  author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.3474},
  eprint = {1412.3474},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0002058}
}

@misc{ucarSubTabSubsettingFeatures2021,
  title = {{{SubTab}}: {{Subsetting Features}} of {{Tabular Data}} for {{Self-Supervised Representation Learning}}},
  shorttitle = {{{SubTab}}},
  author = {Ucar, Talip and Hajiramezanali, Ehsan and Edwards, Lindsay},
  year = {2021},
  month = oct,
  number = {arXiv:2110.04361},
  eprint = {2110.04361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.04361},
  urldate = {2024-08-12},
  abstract = {Self-supervised learning has been shown to be very effective in learning useful representations, and yet much of the success is achieved in data types such as images, audio, and text. The success is mainly enabled by taking advantage of spatial, temporal, or semantic structure in the data through augmentation. However, such structure may not exist in tabular datasets commonly used in fields such as healthcare, making it difficult to design an effective augmentation method, and hindering a similar progress in tabular data setting. In this paper, we introduce a new framework, Subsetting features of Tabular data (SubTab), that turns the task of learning from tabular data into a multi-view representation learning problem by dividing the input features to multiple subsets. We argue that reconstructing the data from the subset of its features rather than its corrupted version in an autoencoder setting can better capture its underlying latent representation. In this framework, the joint representation can be expressed as the aggregate of latent variables of the subsets at test time, which we refer to as collaborative inference. Our experiments show that the SubTab achieves the state of the art (SOTA) performance of 98.31\% on MNIST in tabular setting, on par with CNN-based SOTA models, and surpasses existing baselines on three other real-world datasets by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/LMFGWCKP/Ucar et al. - 2021 - SubTab Subsetting Features of Tabular Data for Se.pdf;/Users/jingang/Zotero/storage/HEWV2XRI/2110.html}
}

@inproceedings{uddinHarmonyGenericUnsupervised2022,
  title = {Harmony: {{A Generic Unsupervised Approach}} for {{Disentangling Semantic Content From Parameterized Transformations}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Uddin, Mostofa Rafid and Howe, Gregory and Zeng, Xiangrui and Xu, Min},
  year = {2022},
  pages = {20646--20655},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/A Generic Unsupervised Approach for Disentangling Semantic Content from Parameterized Transformations.pdf}
}

@article{ulyanovInstanceNormalizationMissing2016,
  title = {Instance Normalization: {{The}} Missing Ingredient for Fast Stylization},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2016},
  journal = {arXiv preprint arXiv:1607.08022},
  eprint = {1607.08022},
  archiveprefix = {arXiv}
}

@article{valderramaOverviewThermodynamicConsistency2019,
  title = {An Overview of a Thermodynamic Consistency Test of Phase Equilibrium Data. {{Application}} of the Versatile {{VPT}} Equation of State to Check Data of Mixtures Containing a Gas Solute and an Ionic Liquid Solvent},
  author = {Valderrama, Jos{\'e} O and Fa{\'u}ndez, Claudio A and Campusano, Richard},
  year = {2019},
  journal = {The Journal of Chemical Thermodynamics},
  volume = {131},
  pages = {122--132},
  publisher = {Elsevier},
  issn = {0021-9614}
}

@article{valderramaThermodynamicConsistencyTest2006,
  title = {Thermodynamic Consistency Test for High Pressure Gas--Solid Solubility Data of Binary Mixtures Using Genetic Algorithms},
  author = {Valderrama, Jos{\'e} O and Zavaleta, Jack},
  year = {2006},
  journal = {The Journal of supercritical fluids},
  volume = {39},
  number = {1},
  pages = {20--29},
  publisher = {Elsevier},
  issn = {0896-8446}
}

@misc{vanbreugelWhyTabularFoundation2024,
  title = {Why {{Tabular Foundation Models Should Be}} a {{Research Priority}}},
  author = {{van Breugel}, Boris and {van der Schaar}, Mihaela},
  year = {2024},
  month = jun,
  number = {arXiv:2405.01147},
  eprint = {2405.01147},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.01147},
  urldate = {2024-08-27},
  abstract = {Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources. In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data. Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power. We believe the time is now to start developing tabular foundation models, or what we coin a Large Tabular Model (LTM). LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets. The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery. We intend to excite reflections on the modalities we study, and convince some researchers to study large tabular models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/MSQZMINJ/van Breugel and van der Schaar - 2024 - Why Tabular Foundation Models Should Be a Research Priority.pdf;/Users/jingang/Zotero/storage/R39LDND3/2405.html}
}

@article{vandenoordNeuralDiscreteRepresentation2017,
  title = {Neural Discrete Representation Learning},
  author = {Van Den Oord, Aaron and Vinyals, Oriol},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/VQVAE Neural Discrete Representation Learning.pdf}
}

@article{vandermaatenVisualizingDataUsing2008,
  title = {Visualizing Data Using T-{{SNE}}.},
  author = {{Van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of machine learning research},
  volume = {9},
  number = {11},
  isbn = {1532-4435}
}

@book{vanderwaalsContinuityGaseousLiquid2004,
  title = {On the Continuity of the Gaseous and Liquid States},
  author = {Van Der Waals, Johannes Diderik and Rowlinson, John Shipley},
  year = {2004},
  publisher = {Courier Corporation}
}

@book{vanrossumPythonLanguageReference2011,
  title = {The Python Language Reference Manual},
  author = {Van Rossum, Guido and Drake, Fred L},
  year = {2011},
  publisher = {Network Theory Ltd.}
}

@article{vanschorenMetalearningSurvey2018,
  title = {Meta-Learning: {{A}} Survey},
  author = {Vanschoren, Joaquin},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.03548},
  eprint = {1810.03548},
  archiveprefix = {arXiv}
}

@book{vapnikNatureStatisticalLearning1999,
  title = {The Nature of Statistical Learning Theory},
  author = {Vapnik, Vladimir},
  year = {1999},
  publisher = {Springer science \& business media},
  isbn = {0-387-98780-0}
}

@article{vapnikOverviewStatisticalLearning1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, Vladimir N},
  year = {1999},
  journal = {IEEE transactions on neural networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  publisher = {IEEE},
  issn = {1045-9227}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  urldate = {2024-06-05},
  file = {/Users/jingang/Zotero/storage/98HYWICC/Vaswani et al. - 2017 - Attention is all you need.pdf}
}

@inproceedings{venkateswaraDeepHashingNetwork2017,
  title = {Deep Hashing Network for Unsupervised Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  year = {2017},
  pages = {5018--5027}
}

@misc{vermaDomainAgnosticContrastiveLearning2021,
  title = {Towards {{Domain-Agnostic Contrastive Learning}}},
  author = {Verma, Vikas and Luong, Minh-Thang and Kawaguchi, Kenji and Pham, Hieu and Le, Quoc V.},
  year = {2021},
  month = jul,
  number = {arXiv:2011.04419},
  eprint = {2011.04419},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.04419},
  urldate = {2024-08-14},
  abstract = {Despite recent success, most contrastive self-supervised learning methods are domain-specific, relying heavily on data augmentation techniques that require knowledge about a particular domain, such as image cropping and rotation. To overcome such limitation, we propose a novel domain-agnostic approach to contrastive learning, named DACL, that is applicable to domains where invariances, and thus, data augmentation techniques, are not readily available. Key to our approach is the use of Mixup noise to create similar and dissimilar examples by mixing data samples differently either at the input or hidden-state levels. To demonstrate the effectiveness of DACL, we conduct experiments across various domains such as tabular data, images, and graphs. Our results show that DACL not only outperforms other domain-agnostic noising methods, such as Gaussian-noise, but also combines well with domain-specific methods, such as SimCLR, to improve self-supervised visual representation learning. Finally, we theoretically analyze our method and show advantages over the Gaussian-noise based contrastive learning approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/I4FUJZUA/Verma et al. - 2021 - Towards Domain-Agnostic Contrastive Learning.pdf;/Users/jingang/Zotero/storage/JSLJS9PB/2011.html}
}

@article{vilaltaPerspectiveViewSurvey2002,
  title = {A Perspective View and Survey of Meta-Learning},
  author = {Vilalta, Ricardo and Drissi, Youssef},
  year = {2002},
  journal = {Artificial intelligence review},
  volume = {18},
  number = {2},
  pages = {77--95},
  publisher = {Springer},
  isbn = {1573-7462}
}

@inproceedings{vladymyrovLinearTransformersAre2024,
  title = {Linear {{Transformers}} Are {{Versatile In-Context Learners}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{In-Context Learning}}},
  author = {Vladymyrov, Max and Oswald, Johannes Von and Sandler, Mark and Ge, Rong},
  year = {2024},
  month = jun,
  urldate = {2024-08-30},
  abstract = {Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that each layer of a linear transformer maintains a weight vector for an implicit linear regression problem and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/N95DTJS9/Vladymyrov et al. - 2024 - Linear Transformers are Versatile In-Context Learners.pdf}
}

@article{volkExamplebasedHypernetworksOutofdistribution2022,
  title = {Example-Based Hypernetworks for out-of-Distribution Generalization},
  author = {Volk, Tomer and {Ben-David}, Eyal and Amosy, Ohad and Chechik, Gal and Reichart, Roi},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.14276},
  eprint = {2203.14276},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Example-based hypernetworks for out-of-distribution generalization.pdf}
}

@article{volpiGeneralizingUnseenDomains2018,
  title = {Generalizing to Unseen Domains via Adversarial Data Augmentation},
  author = {Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John C. and Murino, Vittorio and Savarese, Silvio},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  annotation = {GSCC: 0000415}
}

@article{vonluxburgTutorialSpectralClustering2007,
  title = {A Tutorial on Spectral Clustering},
  author = {Von Luxburg, Ulrike},
  year = {2007},
  journal = {Statistics and computing},
  volume = {17},
  number = {4},
  pages = {395--416},
  publisher = {Springer},
  isbn = {1573-1375}
}

@article{vonoswaldContinualLearningHypernetworks2019,
  title = {Continual Learning with Hypernetworks},
  author = {Von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~a}o and Grewe, Benjamin F.},
  year = {2019},
  journal = {arXiv preprint arXiv:1906.00695},
  eprint = {1906.00695},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Continual learning with hypernetworks.pdf}
}

@misc{vonoswaldTransformersLearnIncontext2023,
  title = {Transformers Learn In-Context by Gradient Descent},
  author = {{von Oswald}, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  year = {2023},
  month = may,
  number = {arXiv:2212.07677},
  eprint = {2212.07677},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.07677},
  urldate = {2024-08-30},
  abstract = {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers\_learn\_icl\_by\_gd .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/WXLF2WQ6/von Oswald et al. - 2023 - Transformers learn in-context by gradient descent.pdf;/Users/jingang/Zotero/storage/23INXU7U/2212.html}
}

@article{vonsolmsComputationalPhysicalPerformance2003,
  title = {Computational and Physical Performance of a Modified {{PC-SAFT}} Equation of State for Highly Asymmetric and Associating Mixtures},
  author = {{von Solms}, Nicolas and Michelsen, Michael L. and Kontogeorgis, Georgios M.},
  year = {2003},
  journal = {Industrial \& engineering chemistry research},
  volume = {42},
  number = {5},
  pages = {1098--1105},
  publisher = {ACS Publications},
  isbn = {0888-5885}
}

@article{voskovComparisonNonlinearFormulations2012,
  title = {Comparison of Nonlinear Formulations for Two-Phase Multi-Component {{EoS}} Based Simulation},
  author = {Voskov, Denis V. and Tchelepi, Hamdi A.},
  year = {2012},
  month = feb,
  journal = {Journal of Petroleum Science and Engineering},
  volume = {82--83},
  pages = {101--111},
  issn = {09204105},
  doi = {10.1016/j.petrol.2011.10.012},
  urldate = {2021-11-25},
  abstract = {We analyze several widely used nonlinear formulations for general-purpose compositional reservoir simulation. All the formulations are implemented using a unified computational framework based on automatic differentiation. The numerical behaviors using different variable sets, including the so-called natural and mass variables, are investigated. The fully implicit method (FIM) is used throughout this work. In the FIM framework, the full Jacobian matrix with all the equations and constraint relations as a function of the primary and secondary variables is computed. For a given formulation, rigorous construction of the Schurcomplement, in which the full Jacobian is reduced to a system for the primary equations in terms of the primary unknowns, is performed purely algebraically. We study multi-component multiphase displacements with significant pressure variation in both space and time, and we focus on challenges posed by nonlinear phenomena, including the appearance and disappearance of phases, and crossing into and out of the critical region. We analyze the behavior of the Newton-based nonlinear solver as a function of timestep size for different variable sets and for different nonlinear updating strategies.},
  langid = {english},
  file = {/Users/jingang/Dropbox/References/Compositional reservoir simulation/Comparison of nonlinear formulations for two-phase multi-component EoS.pdf}
}

@inproceedings{voskovCompositionalSpaceParameterization2007,
  title = {Compositional Space Parameterization for Flow Simulation},
  booktitle = {{{SPE Reservoir Simulation Symposium}}},
  author = {Voskov, Denis and Tchelepi, Hamdi A},
  year = {2007},
  publisher = {OnePetro},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Compositional space parameterization for flow simulation.pdf}
}

@article{wagnerIAPWSFormulation19952002,
  title = {The {{IAPWS}} Formulation 1995 for the Thermodynamic Properties of Ordinary Water Substance for General and Scientific Use},
  author = {Wagner, Wolfgang and Pru{\ss}, Andreas},
  year = {2002},
  journal = {Journal of physical and chemical reference data},
  volume = {31},
  number = {2},
  pages = {387--535},
  publisher = {American Institute of Physics for the National Institute of Standards and~{\dots}},
  isbn = {0047-2689},
  annotation = {GSCC: 0004996}
}

@book{walasPhaseEquilibriaChemical2013,
  title = {Phase Equilibria in Chemical Engineering},
  author = {Walas, Stanley M.},
  year = {2013},
  publisher = {Butterworth-Heinemann},
  isbn = {1-4831-4508-5}
}

@article{wangAcceleratingStabilizingVaporliquid2019,
  title = {Accelerating and Stabilizing the Vapor-Liquid Equilibrium ({{VLE}}) Calculation in Compositional Simulation of Unconventional Reservoirs Using Deep Learning Based Flash Calculation},
  author = {Wang, Shihao and Sobecki, Nicolas and Ding, Didier and Zhu, Lingchen and Wu, Yu-Shu},
  year = {2019},
  month = oct,
  journal = {Fuel},
  volume = {253},
  pages = {209--219},
  issn = {00162361},
  doi = {10.1016/j.fuel.2019.05.023},
  urldate = {2021-11-25},
  abstract = {The flash calculation with large capillary pressure often turns out to be time-consuming and unstable. Consequently, the compositional simulation of unconventional oil/gas reservoirs, where large capillary pressure exists on the vapor-liquid phase interface due to the narrow pore channel, becomes a challenge to traditional reservoir simulation techniques. In this work, we try to resolve this issue by combining deep learning technology with reservoir simulation. We have developed a compositional simulator that is accelerated and stabilized by stochastically-trained proxy flash calculation.},
  langid = {english},
  file = {/Users/jingang/Dropbox/References/Accelerate flash via machine learning/Wang et al. - 2019 - Accelerating and stabilizing the vapor-liquid equilibrium (VLE)calculation in compositional simulation of unconvent.pdf}
}

@inproceedings{wangDeepMixtureExperts2020,
  title = {Deep Mixture of Experts via Shallow Embedding},
  booktitle = {Uncertainty in Artificial Intelligence},
  author = {Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E.},
  year = {2020},
  pages = {552--562},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Deep Mixture of Experts via Shallow Embedding.pdf}
}

@article{wangDeepVisualDomain2018,
  title = {Deep Visual Domain Adaptation: {{A}} Survey},
  author = {Wang, Mei and Deng, Weihong},
  year = {2018},
  journal = {Neurocomputing},
  volume = {312},
  pages = {135--153},
  publisher = {Elsevier},
  isbn = {0925-2312}
}

@article{wangGeneralizingUnseenDomains2022,
  title = {Generalizing to Unseen Domains: {{A}} Survey on Domain Generalization},
  author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  publisher = {IEEE},
  issn = {1041-4347},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Overview/A Survey on domain generalization.pdf}
}

@inproceedings{wangHeterogeneousDomainGeneralization2020,
  title = {Heterogeneous Domain Generalization via Domain Mixup},
  booktitle = {{{ICASSP}} 2020-2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Yufei and Li, Haoliang and Kot, Alex C.},
  year = {2020},
  pages = {3622--3626},
  publisher = {IEEE},
  isbn = {1-5090-6631-4},
  annotation = {GSCC: 0000061},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Mixup/Heterogeneous domain generalization via domain mixup.pdf}
}

@article{wangLargeLanguageModels2023,
  title = {Large {{Language Models Are Latent Variable Models}}: {{Explaining}} and {{Finding Good Demonstrations}} for {{In-Context Learning}}},
  shorttitle = {Large {{Language Models Are Latent Variable Models}}},
  author = {Wang, Xinyi and Zhu, Wanrong and Saxon, Michael and Steyvers, Mark and Wang, William Yang},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {15614--15638},
  urldate = {2024-08-30},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/JH5UZK7D/Wang et al. - 2023 - Large Language Models Are Latent Variable Models Explaining and Finding Good Demonstrations for In-.pdf}
}

@article{wangNoniterativeFlashCalculation1994,
  title = {Non-Iterative Flash Calculation Algorithm in Compositional Reservoir Simulation},
  author = {Wang, Peng and Stenby, Erling H},
  year = {1994},
  journal = {Fluid Phase Equilibria},
  volume = {95},
  pages = {93--108},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Non-iterative Flash Calculation Algorithm in Compositional Reservoir Simulation.pdf}
}

@misc{wangSharpnessAwareGradientMatching2023,
  title = {Sharpness-{{Aware Gradient Matching}} for {{Domain Generalization}}},
  author = {Wang, Pengfei and Zhang, Zhaoxiang and Lei, Zhen and Zhang, Lei},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10353},
  eprint = {2303.10353},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10353},
  urldate = {2024-08-03},
  abstract = {The goal of domain generalization (DG) is to enhance the generalization capability of the model learned from a source domain to other unseen domains. The recently developed Sharpness-Aware Minimization (SAM) method aims to achieve this goal by minimizing the sharpness measure of the loss landscape. Though SAM and its variants have demonstrated impressive DG performance, they may not always converge to the desired flat region with a small loss value. In this paper, we present two conditions to ensure that the model could converge to a flat minimum with a small loss, and present an algorithm, named Sharpness-Aware Gradient Matching (SAGM), to meet the two conditions for improving model generalization capability. Specifically, the optimization objective of SAGM will simultaneously minimize the empirical risk, the perturbed loss (i.e., the maximum loss within a neighborhood in the parameter space), and the gap between them. By implicitly aligning the gradient directions between the empirical risk and the perturbed loss, SAGM improves the generalization capability over SAM and its variants without increasing the computational cost. Extensive experimental results show that our proposed SAGM method consistently outperforms the state-of-the-art methods on five DG benchmarks, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Codes are available at https://github.com/Wang-pengfei/SAGM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/BNTMUMT6/Wang et al. - 2023 - Sharpness-Aware Gradient Matching for Domain Gener.pdf;/Users/jingang/Zotero/storage/W9MU63LZ/2303.html}
}

@article{wangTensorflowSimulationFramework2022,
  title = {A Tensorflow Simulation Framework for Scientific Computing of Fluid Flows on Tensor Processing Units},
  author = {Wang, Qing and Ihme, Matthias and Chen, Yi-Fan and Anderson, John},
  year = {2022},
  journal = {Computer Physics Communications},
  volume = {274},
  pages = {108292},
  publisher = {Elsevier},
  isbn = {0010-4655}
}

@article{wangTransferLearningDynamic2020,
  title = {Transfer Learning with Dynamic Distribution Adaptation},
  author = {Wang, Jindong and Chen, Yiqiang and Feng, Wenjie and Yu, Han and Huang, Meiyu and Yang, Qiang},
  year = {2020},
  journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume = {11},
  number = {1},
  pages = {1--25},
  publisher = {ACM New York, NY, USA},
  isbn = {2157-6904},
  annotation = {GSCC: 0000106}
}

@article{wangTranstabLearningTransferable2022,
  title = {Transtab: {{Learning}} Transferable Tabular Transformers across Tables},
  author = {Wang, Zifeng and Sun, Jimeng},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {2902--2915},
  file = {/Users/jingang/Dropbox/References/Tabular learning/TransTab- Learning Transferable Tabular Transformers Across Tables.pdf}
}

@inproceedings{wangTutaTreebasedTransformers2021,
  title = {Tuta: {{Tree-based}} Transformers for Generally Structured Table Pre-Training},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei},
  year = {2021},
  pages = {1780--1790},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Tuta- tree-based transformers for generally structured table pre-training.pdf}
}

@article{wangUnderstandingMitigatingGradient2021,
  title = {Understanding and Mitigating Gradient Flow Pathologies in Physics-Informed Neural Networks},
  author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  year = {2021},
  journal = {SIAM Journal on Scientific Computing},
  volume = {43},
  number = {5},
  pages = {A3055-A3081},
  publisher = {SIAM},
  isbn = {1064-8275},
  file = {/Users/jingang/Dropbox/References/PINN/UNDERSTANDING AND MITIGATING GRADIENT PATHOLOGIES.pdf}
}

@article{wangVariationalDisentanglementDomain2021,
  title = {Variational Disentanglement for Domain Generalization},
  author = {Wang, Yufei and Li, Haoliang and Cheng, Hao and Wen, Bihan and Chau, Lap-Pui and Kot, Alex C},
  year = {2021},
  journal = {arXiv preprint arXiv:2109.05826},
  eprint = {2109.05826},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Feature disentanglement/VAE/variational_disentanglement_fo.pdf}
}

@inproceedings{wangVisualDomainAdaptation2018,
  title = {Visual Domain Adaptation with Manifold Embedded Distribution Alignment},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  author = {Wang, Jindong and Feng, Wenjie and Chen, Yiqiang and Yu, Han and Huang, Meiyu and Yu, Philip S.},
  year = {2018},
  pages = {402--410},
  annotation = {GSCC: 0000378}
}

@book{wankatSeparationProcessEngineering2022,
  title = {Separation Process Engineering: Includes Mass Transfer Analysis},
  author = {Wankat, Phillip C},
  year = {2022},
  publisher = {Pearson},
  isbn = {0-13-746812-1}
}

@article{weissSurveyTransferLearning2016,
  title = {A Survey of Transfer Learning},
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  year = {2016},
  journal = {Journal of Big data},
  volume = {3},
  number = {1},
  pages = {1--40},
  publisher = {SpringerOpen},
  isbn = {2196-1115}
}

@article{wenAcceleratingCarbonCapture2022,
  title = {Accelerating {{Carbon Capture}} and {{Storage Modeling}} Using {{Fourier Neural Operators}}},
  author = {Wen, Gege and Li, Zongyi and Long, Qirui and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M.},
  year = {2022},
  journal = {arXiv preprint arXiv:2210.17051},
  eprint = {2210.17051},
  archiveprefix = {arXiv}
}

@article{wenUFNOEnhancedFourier2022,
  title = {U-{{FNO}}---{{An}} Enhanced {{Fourier}} Neural Operator-Based Deep-Learning Model for Multiphase Flow},
  author = {Wen, Gege and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M.},
  year = {2022},
  journal = {Advances in Water Resources},
  volume = {163},
  pages = {104180},
  publisher = {Elsevier},
  isbn = {0309-1708}
}

@article{werbosBackpropagationTimeWhat1990,
  title = {Backpropagation through Time: What It Does and How to Do It},
  author = {Werbos, Paul J},
  year = {1990},
  journal = {Proceedings of the IEEE},
  volume = {78},
  number = {10},
  pages = {1550--1560},
  publisher = {IEEE},
  issn = {0018-9219}
}

@article{wertheimFluidsHighlyDirectional1984,
  title = {Fluids with Highly Directional Attractive Forces. {{II}}. {{Thermodynamic}} Perturbation Theory and Integral Equations},
  author = {Wertheim, Michael S},
  year = {1984},
  journal = {Journal of statistical physics},
  volume = {35},
  number = {1},
  pages = {35--47},
  publisher = {Springer}
}

@article{wertheimFluidsHighlyDirectional1984a,
  title = {Fluids with Highly Directional Attractive Forces. {{I}}. {{Statistical}} Thermodynamics},
  author = {Wertheim, {\relax MS}},
  year = {1984},
  journal = {Journal of statistical physics},
  volume = {35},
  number = {1},
  pages = {19--34},
  publisher = {Springer}
}

@article{wertheimFluidsHighlyDirectional1986,
  title = {Fluids with Highly Directional Attractive Forces. {{IV}}. {{Equilibrium}} Polymerization},
  author = {Wertheim, {\relax MS}},
  year = {1986},
  journal = {Journal of statistical physics},
  volume = {42},
  number = {3},
  pages = {477--492},
  publisher = {Springer}
}

@article{wertheimFluidsHighlyDirectional1986a,
  title = {Fluids with Highly Directional Attractive Forces. {{III}}. {{Multiple}} Attraction Sites},
  author = {Wertheim, {\relax MS}},
  year = {1986},
  journal = {Journal of statistical physics},
  volume = {42},
  number = {3},
  pages = {459--476},
  publisher = {Springer}
}

@article{whitsonNegativeFlash1989,
  title = {The Negative Flash},
  author = {Whitson, Curtis H. and Michelsen, Michael L.},
  year = {1989},
  journal = {Fluid phase equilibria},
  volume = {53},
  pages = {51--71},
  publisher = {Elsevier},
  isbn = {0378-3812},
  file = {/Users/jingang/Dropbox/References/Flash calculations/THE NEGATIVE FLASH.pdf}
}

@article{wiesLearnabilityInContextLearning2023,
  title = {The {{Learnability}} of {{In-Context Learning}}},
  author = {Wies, Noam and Levine, Yoav and Shashua, Amnon},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {36637--36651},
  urldate = {2024-08-30},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/GQBBTBAB/Wies et al. - 2023 - The Learnability of In-Context Learning.pdf}
}

@article{wilhelmsenEvaluationSPUNGOther2012,
  title = {Evaluation of {{SPUNG}}* and Other Equations of State for Use in Carbon Capture and Storage Modelling},
  author = {Wilhelmsen, {\O}ivind and Skaugen, Geir and J{\o}rstad, Oddvar and Li, Hailong},
  year = {2012},
  journal = {Energy Procedia},
  volume = {23},
  pages = {236--245},
  publisher = {Elsevier},
  issn = {1876-6102},
  annotation = {GSCC: 0000059}
}

@article{wilhelmsenThermodynamicModelingEquations2017,
  title = {Thermodynamic Modeling with Equations of State: Present Challenges with Established Methods},
  author = {Wilhelmsen, {\O}ivind and Aasen, Ailo and Skaugen, Geir and Aursand, Peder and Austegard, Anders and Aursand, Eskil and Gjennestad, Magnus Aa and Lund, Halvor and Linga, Gaute and Hammer, Morten},
  year = {2017},
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {56},
  number = {13},
  pages = {3503--3515},
  publisher = {ACS Publications},
  issn = {0888-5885},
  annotation = {GSCC: 0000928}
}

@book{williamsGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Williams, Christopher KI and Rasmussen, Carl Edward},
  year = {2006},
  volume = {2},
  publisher = {MIT press Cambridge, MA}
}

@article{wolpertNoFreeLunch1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, David H. and Macready, William G.},
  year = {1997},
  journal = {IEEE transactions on evolutionary computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  publisher = {IEEE},
  isbn = {1089-778X}
}

@article{wolpertStackedGeneralization1992,
  title = {Stacked Generalization},
  author = {Wolpert, David H.},
  year = {1992},
  journal = {Neural networks},
  volume = {5},
  number = {2},
  pages = {241--259},
  publisher = {Elsevier},
  isbn = {0893-6080}
}

@misc{wuDeepFeatureEmbedding2024,
  title = {Deep {{Feature Embedding}} for {{Tabular Data}}},
  author = {Wu, Yuqian and Luo, Hengyi and Lee, Raymond S. T.},
  year = {2024},
  month = aug,
  number = {arXiv:2408.17162},
  eprint = {2408.17162},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.17162},
  urldate = {2024-09-02},
  abstract = {Tabular data learning has extensive applications in deep learning but its existing embedding techniques are limited in numerical and categorical features such as the inability to capture complex relationships and engineering. This paper proposes a novel deep embedding framework with leverages lightweight deep neural networks to generate effective feature embeddings for tabular data in machine learning research. For numerical features, a two-step feature expansion and deep transformation technique is used to capture copious semantic information. For categorical features, a unique identification vector for each entity is referred by a compact lookup table with a parameterized deep embedding function to uniform the embedding size dimensions, and transformed into a embedding vector using deep neural network. Experiments are conducted on real-world datasets for performance evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/VKRN8XZZ/Wu et al. - 2024 - Deep Feature Embedding for Tabular Data.pdf;/Users/jingang/Zotero/storage/C34EKPGI/2408.html}
}

@article{wuFastformerAdditiveAttention2021,
  title = {Fastformer: {{Additive}} Attention Can Be All You Need},
  author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing},
  year = {2021},
  journal = {arXiv preprint arXiv:2108.09084},
  eprint = {2108.09084},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Transformers/Fastformer.pdf}
}

@misc{wuPointTransformerV22022,
  title = {Point {{Transformer V2}}: {{Grouped Vector Attention}} and {{Partition-based Pooling}}},
  shorttitle = {Point {{Transformer V2}}},
  author = {Wu, Xiaoyang and Lao, Yixing and Jiang, Li and Liu, Xihui and Zhao, Hengshuang},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05666},
  eprint = {2210.05666},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.05666},
  urldate = {2024-09-03},
  abstract = {As a pioneering work exploring transformer architecture for 3D point cloud understanding, Point Transformer achieves impressive results on multiple highly competitive benchmarks. In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work. In particular, we first propose group vector attention, which is more effective than the previous version of vector attention. Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer. We also strengthen the position information for attention by an additional position encoding multiplier. Furthermore, we design novel and lightweight partition-based pooling methods which enable better spatial alignment and more efficient sampling. Extensive experiments show that our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and 3D point cloud classification on ModelNet40. Our code will be available at https://github.com/Gofinge/PointTransformerV2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/VSUUKF5U/Wu et al. - 2022 - Point Transformer V2 Grouped Vector Attention and Partition-based Pooling.pdf;/Users/jingang/Zotero/storage/QM9DWDNG/2210.html}
}

@misc{wuPointTransformerV32024,
  title = {Point {{Transformer V3}}: {{Simpler}}, {{Faster}}, {{Stronger}}},
  shorttitle = {Point {{Transformer V3}}},
  author = {Wu, Xiaoyang and Jiang, Li and Wang, Peng-Shuai and Liu, Zhijian and Liu, Xihui and Qiao, Yu and Ouyang, Wanli and He, Tong and Zhao, Hengshuang},
  year = {2024},
  month = mar,
  number = {arXiv:2312.10035},
  eprint = {2312.10035},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10035},
  urldate = {2024-09-03},
  abstract = {This paper is not motivated to seek innovation within the attention mechanism. Instead, it focuses on overcoming the existing trade-offs between accuracy and efficiency within the context of point cloud processing, leveraging the power of scale. Drawing inspiration from recent advances in 3D large-scale representation learning, we recognize that model performance is more influenced by scale than by intricate design. Therefore, we present Point Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the accuracy of certain mechanisms that are minor to the overall performance after scaling, such as replacing the precise neighbor search by KNN with an efficient serialized neighbor mapping of point clouds organized with specific patterns. This principle enables significant scaling, expanding the receptive field from 16 to 1024 points while remaining efficient (a 3x increase in processing speed and a 10x improvement in memory efficiency compared with its predecessor, PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that span both indoor and outdoor scenarios. Further enhanced with multi-dataset joint training, PTv3 pushes these results to a higher level.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jingang/Zotero/storage/E43YNPNJ/Wu et al. - 2024 - Point Transformer V3 Simpler, Faster, Stronger.pdf;/Users/jingang/Zotero/storage/P4APZGMA/2312.html}
}

@misc{wuSwitchTabSwitchedAutoencoders2024,
  title = {{{SwitchTab}}: {{Switched Autoencoders Are Effective Tabular Learners}}},
  shorttitle = {{{SwitchTab}}},
  author = {Wu, Jing and Chen, Suiyao and Zhao, Qi and Sergazinov, Renat and Li, Chen and Liu, Shengjie and Zhao, Chongchao and Xie, Tianpei and Guo, Hanqing and Ji, Cheng and Cociorva, Daniel and Brunzel, Hakan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02013},
  eprint = {2401.02013},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.02013},
  urldate = {2024-11-12},
  abstract = {Self-supervised representation learning methods have achieved significant success in computer vision and natural language processing, where data samples exhibit explicit spatial or semantic dependencies. However, applying these methods to tabular data is challenging due to the less pronounced dependencies among data samples. In this paper, we address this limitation by introducing SwitchTab, a novel self-supervised method specifically designed to capture latent dependencies in tabular data. SwitchTab leverages an asymmetric encoder-decoder framework to decouple mutual and salient features among data pairs, resulting in more representative embeddings. These embeddings, in turn, contribute to better decision boundaries and lead to improved results in downstream tasks. To validate the effectiveness of SwitchTab, we conduct extensive experiments across various domains involving tabular data. The results showcase superior performance in end-to-end prediction tasks with fine-tuning. Moreover, we demonstrate that pre-trained salient embeddings can be utilized as plug-and-play features to enhance the performance of various traditional classification methods (e.g., Logistic Regression, XGBoost, etc.). Lastly, we highlight the capability of SwitchTab to create explainable representations through visualization of decoupled mutual and salient features in the latent space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/Y32L4TF2/Wu et al. - 2024 - SwitchTab Switched Autoencoders Are Effective Tabular Learners.pdf;/Users/jingang/Zotero/storage/5QBEWYA8/2401.html}
}

@misc{xieExplanationIncontextLearning2022,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  year = {2022},
  month = jul,
  number = {arXiv:2111.02080},
  eprint = {2111.02080},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.02080},
  urldate = {2024-08-31},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/FSFSXAB8/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit Bayesian Inference.pdf;/Users/jingang/Zotero/storage/Q5MRJ5TN/2111.html}
}

@inproceedings{xieGeneralIncrementalLearning2022,
  title = {General {{Incremental Learning}} with {{Domain-aware Categorical Representations}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xie, Jiangwei and Yan, Shipeng and He, Xuming},
  year = {2022},
  pages = {14351--14360},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Others/General Incremental Learning with Domain-aware Categorical Representations.pdf}
}

@misc{xingAnytimeNeuralArchitecture2024,
  title = {Anytime {{Neural Architecture Search}} on {{Tabular Data}}},
  author = {Xing, Naili and Cai, Shaofeng and Luo, Zhaojing and Ooi, Beng Chin and Pei, Jian},
  year = {2024},
  month = may,
  number = {arXiv:2403.10318},
  eprint = {2403.10318},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {The increasing demand for tabular data analysis calls for transitioning from manual architecture design to Neural Architecture Search (NAS). This transition demands an efficient and responsive anytime NAS approach that is capable of returning current optimal architectures within any given time budget while progressively enhancing architecture quality with increased budget allocation. However, the area of research on Anytime NAS for tabular data remains unexplored. To this end, we introduce ATLAS, the first anytime NAS approach tailored for tabular data. ATLAS introduces a novel two-phase filtering-and-refinement optimization scheme with joint optimization, combining the strengths of both paradigms of training-free and training-based architecture evaluation. Specifically, in the filtering phase, ATLAS employs a new zero-cost proxy specifically designed for tabular data to efficiently estimate the performance of candidate architectures, thereby obtaining a set of promising architectures. Subsequently, in the refinement phase, ATLAS leverages a fixed-budget search algorithm to schedule the training of the promising candidates, so as to accurately identify the optimal architecture. To jointly optimize the two phases for anytime NAS, we also devise a budget-aware coordinator that delivers high NAS performance within constraints. Experimental evaluations demonstrate that our ATLAS can obtain a good-performing architecture within any predefined time budget and return better architectures as and when a new time budget is made available. Overall, it reduces the search time on tabular data by up to 82.75x compared to existing NAS approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/BE5HSYII/Xing et al. - 2024 - Anytime Neural Architecture Search on Tabular Data.pdf;/Users/jingang/Zotero/storage/RXAILUJI/2403.html}
}

@misc{xiongEffectiveLongContextScaling2023,
  title = {Effective {{Long-Context Scaling}} of {{Foundation Models}}},
  author = {Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and Khabsa, Madian and Fang, Han and Mehdad, Yashar and Narang, Sharan and Malik, Kshitiz and Fan, Angela and Bhosale, Shruti and Edunov, Sergey and Lewis, Mike and Wang, Sinong and Ma, Hao},
  year = {2023},
  month = nov,
  number = {arXiv:2309.16039},
  eprint = {2309.16039},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16039},
  urldate = {2025-01-05},
  abstract = {We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/A25X2C52/Xiong et al. - 2023 - Effective Long-Context Scaling of Foundation Models.pdf;/Users/jingang/Zotero/storage/SMNU4528/2309.html}
}

@inproceedings{xiongLatentDomainsModeling2014,
  title = {Latent Domains Modeling for Visual Domain Adaptation},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Xiong, Caiming and McCloskey, Scott and Hsieh, Shao-Hang and Corso, Jason},
  year = {2014},
  volume = {28},
  isbn = {2374-3468}
}

@inproceedings{xiongLayerNormalizationTransformer2020,
  title = {On Layer Normalization in the Transformer Architecture},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  year = {2020},
  pages = {10524--10533},
  publisher = {PMLR},
  urldate = {2024-06-05},
  file = {/Users/jingang/Zotero/storage/L3YFDBTT/Xiong et al. - 2020 - On layer normalization in the transformer architec.pdf}
}

@inproceedings{xuAdversarialDomainAdaptation2020,
  title = {Adversarial Domain Adaptation with Domain Mixup},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Xu, Minghao and Zhang, Jian and Ni, Bingbing and Li, Teng and Wang, Chengjie and Tian, Qi and Zhang, Wenjun},
  year = {2020},
  volume = {34},
  pages = {6502--6509},
  isbn = {2374-3468},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Mixup/Adversarial Domain Adaptation with Domain Mixup.pdf}
}

@misc{xuBiSHopBiDirectionalCellular2024,
  title = {{{BiSHop}}: {{Bi-Directional Cellular Learning}} for {{Tabular Data}} with {{Generalized Sparse Modern Hopfield Model}}},
  shorttitle = {{{BiSHop}}},
  author = {Xu, Chenwei and Huang, Yu-Chao and Hu, Jerry Yao-Chieh and Li, Weijian and Gilani, Ammar and Goan, Hsi-Sheng and Liu, Han},
  year = {2024},
  month = jul,
  number = {arXiv:2404.03830},
  eprint = {2404.03830},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.03830},
  urldate = {2024-10-07},
  abstract = {We introduce the {\textbackslash}textbf\{B\}i-Directional {\textbackslash}textbf\{S\}parse {\textbackslash}textbf\{Hop\}field Network ({\textbackslash}textbf\{BiSHop\}), a novel end-to-end framework for deep tabular learning. BiSHop handles the two major challenges of deep tabular learning: non-rotationally invariant data structure and feature sparsity in tabular data. Our key motivation comes from the recent established connection between associative memory and attention mechanisms. Consequently, BiSHop uses a dual-component approach, sequentially processing data both column-wise and row-wise through two interconnected directional learning modules. Computationally, these modules house layers of generalized sparse modern Hopfield layers, a sparse extension of the modern Hopfield model with adaptable sparsity. Methodologically, BiSHop facilitates multi-scale representation learning, capturing both intra-feature and inter-feature interactions, with adaptive sparsity at each scale. Empirically, through experiments on diverse real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods with significantly less HPO runs, marking it a robust solution for deep tabular learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/QKAULAD9/Xu et al. - 2024 - BiSHop Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Mo.pdf;/Users/jingang/Zotero/storage/B6I8QWWN/2404.html}
}

@article{xueJAXFEMDifferentiableGPUaccelerated2022,
  title = {{{JAX-FEM}}: {{A}} Differentiable {{GPU-accelerated 3D}} Finite Element Solver for Automatic Inverse Design and Mechanistic Data Science},
  author = {Xue, Tianju and Liao, Shuheng and Gan, Zhengtao and Park, Chanwook and Xie, Xiaoyu and Liu, Wing Kam and Cao, Jian},
  year = {2022},
  journal = {arXiv preprint arXiv:2212.00964},
  eprint = {2212.00964},
  archiveprefix = {arXiv}
}

@inproceedings{xuFourierbasedFrameworkDomain2021,
  title = {A Fourier-Based Framework for Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, Qinwei and Zhang, Ruipeng and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  year = {2021},
  pages = {14383--14392},
  annotation = {GSCC: 0000077}
}

@misc{xuMixtureInContextPrompters2024,
  title = {Mixture of {{In-Context Prompters}} for {{Tabular PFNs}}},
  author = {Xu, Derek and Cirit, Olcay and Asadi, Reza and Sun, Yizhou and Wang, Wei},
  year = {2024},
  month = may,
  number = {arXiv:2405.16156},
  eprint = {2405.16156},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.16156},
  urldate = {2024-06-06},
  abstract = {Recent benchmarks found In-Context Learning (ICL) outperforms both deep learning and tree-based algorithms on small tabular datasets. However, on larger datasets, ICL for tabular learning cannot run without severely compromising performance, due to its quadratic space and time complexity w.r.t. dataset size. We propose MIXTUREPFN, which both extends nearest-neighbor sampling to the state-of-the-art ICL for tabular learning model and uses bootstrapping to finetune said model on the inference-time dataset. MIXTUREPFN is the Condorcet winner across 36 diverse tabular datasets against 19 strong deep learning and tree-based baselines, achieving the highest mean rank among Top-10 aforementioned algorithms with statistical significance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/38YQBQPL/Xu et al. - 2024 - Mixture of In-Context Prompters for Tabular PFNs.pdf;/Users/jingang/Zotero/storage/6U3E4RAW/2405.html}
}

@article{yangClassDisentanglementApplicationsAdversarial2021,
  title = {Class-{{Disentanglement}} and {{Applications}} in {{Adversarial Detection}} and {{Defense}}},
  author = {Yang, Kaiwen and Zhou, Tianyi and Zhang, Yonggang and Tian, Xinmei and Tao, Dacheng},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {16051--16063},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/Class-Disentanglement and Applications in Adversarial Detection and Defense.pdf}
}

@misc{yangTableFormerRobustTransformer2022,
  title = {{{TableFormer}}: {{Robust Transformer Modeling}} for {{Table-Text Encoding}}},
  shorttitle = {{{TableFormer}}},
  author = {Yang, Jingfeng and Gupta, Aditya and Upadhyay, Shyam and He, Luheng and Goel, Rahul and Paul, Shachi},
  year = {2022},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-08-18},
  abstract = {Understanding tables is an important aspect of natural language understanding. Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias. Such spurious biases make the model vulnerable to row and column order perturbations. Additionally, prior work has not thoroughly modeled the table structures or table-text alignments, hindering the table-text understanding ability. In this work, we propose a robust and structurally aware table-text encoding architecture TableFormer, where tabular structural biases are incorporated completely through learnable attention biases. TableFormer is (1) strictly invariant to row and column orders, and, (2) could understand tables better due to its tabular inductive biases. Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA, WTQ and TabFact table reasoning datasets, and achieves state-of-the-art performance on SQA, especially when facing answer-invariant row and column order perturbations (6\% improvement over the best baseline), because previous SOTA models' performance drops by 4\% - 6\% when facing such perturbations while TableFormer is not affected.},
  howpublished = {https://arxiv.org/abs/2203.00274v2},
  langid = {english},
  file = {/Users/jingang/Zotero/storage/7PDVVR22/Yang et al. - 2022 - TableFormer Robust Transformer Modeling for Table-Text Encoding.pdf}
}

@inproceedings{yangUniTabEUniversalPretraining2023,
  title = {{{UniTabE}}: {{A Universal Pretraining Protocol}} for {{Tabular Foundation Model}} in {{Data Science}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Yang, Yazheng and Wang, Yuqi and Liu, Guang and Wu, Ledell and Liu, Qi},
  year = {2023},
  file = {/Users/jingang/Dropbox/References/Tabular learning/A Universal Pretraining Protocol for Tabular Foundation Model in Data Science.pdf}
}

@article{yanImproveUnsupervisedDomain2020,
  title = {Improve Unsupervised Domain Adaptation with Mixup Training},
  author = {Yan, Shen and Song, Huan and Li, Nanxiang and Zou, Lincan and Ren, Liu},
  year = {2020},
  journal = {arXiv preprint arXiv:2001.00677},
  eprint = {2001.00677},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Mixup/IMPROVE UNSUPERVISED DOMAIN ADAPTATION WITH MIXUP TRAINING.pdf}
}

@misc{yeCloserLookDeep2024,
  title = {A {{Closer Look}} at {{Deep Learning}} on {{Tabular Data}}},
  author = {Ye, Han-Jia and Liu, Si-Yang and Cai, Hao-Run and Zhou, Qi-Le and Zhan, De-Chuan},
  year = {2024},
  month = jul,
  number = {arXiv:2407.00956},
  eprint = {2407.00956},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-02},
  abstract = {Tabular data is prevalent across various domains in machine learning. Although Deep Neural Network (DNN)-based methods have shown promising performance comparable to tree-based ones, in-depth evaluation of these methods is challenging due to varying performance ranks across diverse datasets. In this paper, we propose a comprehensive benchmark comprising 300 tabular datasets, covering a wide range of task types, size distributions, and domains. We perform an extensive comparison between state-of-the-art deep tabular methods and tree-based methods, revealing the average rank of all methods and highlighting the key factors that influence the success of deep tabular methods. Next, we analyze deep tabular methods based on their training dynamics, including changes in validation metrics and other statistics. For each dataset-method pair, we learn a mapping from both the meta-features of datasets and the first part of the validation curve to the final validation set performance and even the evolution of validation curves. This mapping extracts essential meta-features that influence prediction accuracy, helping the analysis of tabular methods from novel aspects. Based on the performance of all methods on this large benchmark, we identify two subsets of 45 datasets each. The first subset contains datasets that favor either tree-based methods or DNN-based methods, serving as effective analysis tools to evaluate strategies (e.g., attribute encoding strategies) for improving deep tabular models. The second subset contains datasets where the ranks of methods are consistent with the overall benchmark, acting as a probe for tabular analysis. These ``tiny tabular benchmarks'' will facilitate further studies on tabular data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/AL6NDXDP/Ye et al. - 2024 - A Closer Look at Deep Learning on Tabular Data.pdf;/Users/jingang/Zotero/storage/W7T2CU5E/2407.html}
}

@misc{yeCloserLookDeep2025a,
  title = {A {{Closer Look}} at {{Deep Learning Methods}} on {{Tabular Datasets}}},
  author = {Ye, Han-Jia and Liu, Si-Yang and Cai, Hao-Run and Zhou, Qi-Le and Zhan, De-Chuan},
  year = {2025},
  month = jan,
  number = {arXiv:2407.00956},
  eprint = {2407.00956},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.00956},
  urldate = {2025-01-19},
  abstract = {Tabular data is prevalent across diverse domains in machine learning. While classical methods like tree-based models have long been effective, Deep Neural Network (DNN)-based methods have recently demonstrated promising performance. However, the diverse characteristics of methods and the inherent heterogeneity of tabular datasets make understanding and interpreting tabular methods both challenging and prone to unstable observations. In this paper, we conduct in-depth evaluations and comprehensive analyses of tabular methods, with a particular focus on DNN-based models, using a benchmark of over 300 tabular datasets spanning a wide range of task types, sizes, and domains. First, we perform an extensive comparison of 32 state-of-the-art deep and tree-based methods, evaluating their average performance across multiple criteria. Although method ranks vary across datasets, we empirically find that top-performing methods tend to concentrate within a small subset of tabular models, regardless of the criteria used. Next, we investigate whether the training dynamics of deep tabular models can be predicted based on dataset properties. This approach not only offers insights into the behavior of deep tabular methods but also identifies a core set of "meta-features" that reflect dataset heterogeneity. The other subset includes datasets where method ranks are consistent with the overall benchmark, acting as a reliable probe for further tabular analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/NANZTSH5/Ye et al. - 2025 - A Closer Look at Deep Learning Methods on Tabular Datasets.pdf;/Users/jingang/Zotero/storage/VWVKLDFX/2407.html}
}

@misc{yeDifferentialTransformer2024,
  title = {Differential {{Transformer}}},
  author = {Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.05258},
  eprint = {2410.05258},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.05258},
  urldate = {2024-10-09},
  abstract = {Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/9JDYY3IW/Ye et al. - 2024 - Differential Transformer.pdf;/Users/jingang/Zotero/storage/UB5SCUI3/2410.html}
}

@misc{yeModernNeighborhoodComponents2024,
  title = {Modern {{Neighborhood Components Analysis}}: {{A Deep Tabular Baseline Two Decades Later}}},
  shorttitle = {Modern {{Neighborhood Components Analysis}}},
  author = {Ye, Han-Jia and Yin, Huai-Hong and Zhan, De-Chuan},
  year = {2024},
  month = jul,
  number = {arXiv:2407.03257},
  eprint = {2407.03257},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.03257},
  urldate = {2024-12-17},
  abstract = {The growing success of deep learning in various domains has prompted investigations into its application to tabular data, where deep models have shown promising results compared to traditional tree-based methods. In this paper, we revisit Neighborhood Component Analysis (NCA), a classic tabular prediction method introduced in 2004, designed to learn a linear projection that captures semantic similarities between instances. We find that minor modifications, such as adjustments to the learning objectives and the integration of deep learning architectures, significantly enhance NCA's performance, enabling it to surpass most modern deep tabular models. Additionally, we introduce a stochastic neighbor sampling strategy that improves both the efficiency and predictive accuracy of our proposed ModernNCA -- sampling only a subset of neighbors during training, while utilizing the entire neighborhood during inference. Extensive experiments demonstrate that our ModernNCA achieves state-of-the-art results in both classification and regression tasks across various tabular datasets, outperforming both tree-based and other deep tabular models, while also reducing training time and model size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/RTWMRZ88/Ye et al. - 2024 - Modern Neighborhood Components Analysis A Deep Tabular Baseline Two Decades Later.pdf;/Users/jingang/Zotero/storage/NQ4SBIQP/2407.html}
}

@article{yinTaBERTPretrainingJoint2020,
  title = {{{TaBERT}}: {{Pretraining}} for Joint Understanding of Textual and Tabular Data},
  author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  year = {2020},
  journal = {arXiv preprint arXiv:2005.08314},
  eprint = {2005.08314},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/TaBERT- Pretraining for joint understanding of textual and tabular data.pdf}
}

@misc{yiWhyNotUse2020,
  title = {Why {{Not}} to {{Use Zero Imputation}}? {{Correcting Sparsity Bias}} in {{Training Neural Networks}}},
  shorttitle = {Why {{Not}} to {{Use Zero Imputation}}?},
  author = {Yi, Joonyoung and Lee, Juhyuk and Kim, Kwang Joon and Hwang, Sung Ju and Yang, Eunho},
  year = {2020},
  month = feb,
  number = {arXiv:1906.00150},
  eprint = {1906.00150},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.00150},
  urldate = {2024-06-19},
  abstract = {Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output of a predictive model largely varies with respect to the rate of missingness in the given input, and show that it adversarially affects the model performance. We first theoretically analyze this phenomenon and propose a simple yet effective technique to handle missingness, which we refer to as Sparsity Normalization (SN), that directly targets and resolves the VSP. We further experimentally validate SN on diverse benchmark datasets, to show that debiasing the effect of input-level sparsity improves the performance and stabilizes the training of neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/V9WMRLKS/Yi et al. - 2020 - Why Not to Use Zero Imputation Correcting Sparsit.pdf;/Users/jingang/Zotero/storage/B5YVNAQY/1906.html}
}

@inproceedings{yoonVIMEExtendingSuccess2020,
  title = {{{VIME}}: {{Extending}} the {{Success}} of {{Self-}} and {{Semi-supervised Learning}} to {{Tabular Domain}}},
  shorttitle = {{{VIME}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and {van der Schaar}, Mihaela},
  year = {2020},
  volume = {33},
  pages = {11033--11043},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-08-12},
  abstract = {Self- and semi-supervised learning frameworks have made significant progress in training machine learning models with limited labeled data in image and language domains. These methods heavily rely on the unique structure in the domain datasets (such as spatial relationships in images or semantic relationships in language). They are not adaptable to general tabular data which does not have the same explicit structure as image and language data. In this paper, we fill this gap by proposing novel self- and semi-supervised learning frameworks for tabular data, which we refer to collectively as VIME (Value Imputation and Mask Estimation). We create a novel pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning. We also introduce a novel tabular data augmentation method for self- and semi-supervised learning frameworks. In experiments, we evaluate the proposed framework in multiple tabular datasets from various application domains, such as genomics and clinical data. VIME exceeds state-of-the-art performance in comparison to the existing baseline methods.},
  file = {/Users/jingang/Zotero/storage/76854DI2/Yoon et al. - 2020 - VIME Extending the Success of Self- and Semi-supe.pdf}
}

@article{yousefiApplicationEquationState2013,
  title = {Application of Equation of State and Artificial Neural Network to Prediction of Volumetric Properties of Polymer Melts},
  author = {Yousefi, F. and Karimi, H.},
  year = {2013},
  journal = {Journal of Industrial and Engineering Chemistry},
  volume = {19},
  number = {2},
  pages = {498--507},
  publisher = {Elsevier},
  isbn = {1226-086X},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/ML EoS/Application of equation of state and artificial neural network to prediction of volumetric properties of polymer melts.pdf}
}

@inproceedings{yueDomainRandomizationPyramid2019,
  title = {Domain Randomization and Pyramid Consistency: {{Simulation-to-real}} Generalization without Accessing Target Domain Data},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yue, Xiangyu and Zhang, Yang and Zhao, Sicheng and {Sangiovanni-Vincentelli}, Alberto and Keutzer, Kurt and Gong, Boqing},
  year = {2019},
  pages = {2100--2110},
  annotation = {GSCC: 0000159}
}

@article{yukselTwentyYearsMixture2012,
  title = {Twenty Years of Mixture of Experts},
  author = {Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
  year = {2012},
  journal = {IEEE transactions on neural networks and learning systems},
  volume = {23},
  number = {8},
  pages = {1177--1193},
  publisher = {IEEE},
  isbn = {2162-237X},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Twenty years of mixture of experts.pdf}
}

@article{yuModelingFluidPhase2021,
  title = {Modeling Fluid Phase Equilibria of Carbon Dioxide-Methanol Binary System},
  author = {Yu, Cheng-Hsiu and Lin, Yu-Jeng and Wong, David Shan-Hill and Bruno, Juan Carles and Chen, Chau-Chyun},
  year = {2021},
  journal = {Fluid Phase Equilibria},
  volume = {529},
  pages = {112866},
  publisher = {Elsevier},
  issn = {0378-3812},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/NNEoS/Modeling fluid phase equilibria of carbon dioxide-methanol binary system.pdf}
}

@misc{zaheerDeepSets2018,
  title = {Deep {{Sets}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  year = {2018},
  month = apr,
  number = {arXiv:1703.06114},
  eprint = {1703.06114},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.06114},
  urldate = {2024-08-20},
  abstract = {We study the problem of designing models for machine learning tasks defined on {\textbackslash}emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics {\textbackslash}cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams {\textbackslash}cite\{Jung15Exploration\}, to cosmology {\textbackslash}cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/HQ9MPKZI/Zaheer et al. - 2018 - Deep Sets.pdf;/Users/jingang/Zotero/storage/QMGGJPT3/1703.html}
}

@article{zarembaRecurrentNeuralNetwork2014,
  title = {Recurrent Neural Network Regularization},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.2329},
  eprint = {1409.2329},
  archiveprefix = {arXiv}
}

@article{zeilerAdadeltaAdaptiveLearning2012,
  title = {Adadelta: An Adaptive Learning Rate Method},
  author = {Zeiler, Matthew D.},
  year = {2012},
  journal = {arXiv preprint arXiv:1212.5701},
  eprint = {1212.5701},
  archiveprefix = {arXiv}
}

@article{zhangAcceleratingFlashCalculations2020,
  title = {Accelerating Flash Calculations in Unconventional Reservoirs Considering Capillary Pressure Using an Optimized Deep Learning Algorithm},
  author = {Zhang, Tao and Li, Yiteng and Sun, Shuyu and Bai, Hua},
  year = {2020},
  journal = {Journal of Petroleum Science and Engineering},
  volume = {195},
  pages = {107886},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/Accelerating flash calculations in unconventional reservoirs considering.pdf}
}

@article{zhangAdaptiveRiskMinimization2021,
  title = {Adaptive Risk Minimization: {{Learning}} to Adapt to Domain Shift},
  author = {Zhang, Marvin and Marklund, Henrik and Dhawan, Nikita and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {23664--23678}
}

@article{zhangBetterBenchmarkingDomain2007,
  title = {Towards Better Benchmarking for Domain Generalization},
  author = {Zhang, X and He, Y and Xu, R and Yu, H and Shen, Z and Cui, P Nico+},
  year = {2007},
  journal = {arXiv preprint arXiv:2204.08040},
  eprint = {2204.08040},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Dataset and Benchmark/Towards Better Benchmarking for Domain Generalization.pdf}
}

@article{zhangFoundationModelsLearning2023,
  title = {Towards Foundation Models for Learning on Tabular Data},
  author = {Zhang, Han and Wen, Xumeng and Zheng, Shun and Xu, Wei and Bian, Jiang},
  year = {2023},
  journal = {arXiv preprint arXiv:2310.07338},
  eprint = {2310.07338},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Tabular learning/Towards Foundation Models for Learning on Tabular Data.pdf}
}

@article{zhangMixupEmpiricalRisk2017,
  title = {Mixup: {{Beyond}} Empirical Risk Minimization},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and {Lopez-Paz}, David},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.09412},
  eprint = {1710.09412},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0004866},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Mixup/mixup- Beyond Empirical Risk Minimization.pdf}
}

@article{zhangOverviewMultitaskLearning2018,
  title = {An Overview of Multi-Task Learning},
  author = {Zhang, Yu and Yang, Qiang},
  year = {2018},
  journal = {National Science Review},
  volume = {5},
  number = {1},
  pages = {30--43},
  publisher = {Oxford University Press},
  isbn = {2095-5138},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/An Overview of Multi-Task Learning.pdf}
}

@inproceedings{zhangPrincipledDisentanglementDomain2022,
  title = {Towards Principled Disentanglement for Domain Generalization},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Hanlin and Zhang, Yi-Fan and Liu, Weiyang and Weller, Adrian and Sch{\"o}lkopf, Bernhard and Xing, Eric P},
  year = {2022},
  pages = {8024--8034},
  annotation = {GSCC: 0000008},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Feature disentanglement/VAE/Towards Principled Disentanglement for Domain Generalization.pdf}
}

@inproceedings{zhangRegressionClustering2003,
  title = {Regression Clustering},
  booktitle = {Third {{IEEE International Conference}} on {{Data Mining}}},
  author = {Zhang, Bin},
  year = {2003},
  pages = {451--458},
  publisher = {IEEE},
  isbn = {0-7695-1978-4},
  file = {/Users/jingang/Dropbox/References/Regression clustering/Regression clustering.pdf}
}

@article{zhangSelfadaptiveDeepLearning2020,
  title = {A Self-Adaptive Deep Learning Algorithm for Accelerating Multi-Component Flash Calculation},
  author = {Zhang, Tao and Li, Yu and Li, Yiteng and Sun, Shuyu and Gao, Xin},
  year = {2020},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {369},
  pages = {113207},
  publisher = {Elsevier},
  isbn = {0045-7825}
}

@article{zhangSelfadaptiveDeepLearning2020a,
  title = {A Self-Adaptive Deep Learning Algorithm for Accelerating Multi-Component Flash Calculation},
  author = {Zhang, Tao and Li, Yu and Li, Yiteng and Sun, Shuyu and Gao, Xin},
  year = {2020},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {369},
  pages = {113207},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Accelerate flash calculations/A self-adaptive deep learning algorithm for accelerating multi-component flash calculation.pdf}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2021},
  journal = {Communications of the ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  publisher = {ACM New York, NY, USA},
  issn = {0001-0782}
}

@misc{zhangWhatHowDoes2023,
  title = {What and {{How}} Does {{In-Context Learning Learn}}? {{Bayesian Model Averaging}}, {{Parameterization}}, and {{Generalization}}},
  shorttitle = {What and {{How}} Does {{In-Context Learning Learn}}?},
  author = {Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  year = {2023},
  month = oct,
  number = {arXiv:2305.19420},
  eprint = {2305.19420},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19420},
  urldate = {2024-08-30},
  abstract = {In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned by large language models? (b) What is a proper performance metric for ICL and what is the error rate? (c) How does the transformer architecture enable ICL? To answer these questions, we adopt a Bayesian view and formulate ICL as a problem of predicting the response corresponding to the current covariate, given a number of examples drawn from a latent variable model. To answer (a), we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm, which is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a \${\textbackslash}mathcal\{O\}(1/T)\$ regret bound for perfectly pretrained ICL, where \$T\$ is the number of examples in the prompt. To answer (c), we show that, in addition to encoding Bayesian model averaging via attention, the transformer architecture also enables a fine-grained statistical analysis of pretraining under realistic assumptions. In particular, we prove that the error of pretrained model is bounded by a sum of an approximation error and a generalization error, where the former decays to zero exponentially as the depth grows, and the latter decays to zero sublinearly with the number of tokens in the pretraining dataset. Our results provide a unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization, which deepens our knowledge of these essential aspects of modern language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jingang/Zotero/storage/26F4WZKA/Zhang et al. - 2023 - What and How does In-Context Learning Learn Bayesian Model Averaging, Parameterization, and General.pdf;/Users/jingang/Zotero/storage/98S34CYA/2305.html}
}

@article{zhangWhyGradientClipping2019,
  title = {Why Gradient Clipping Accelerates Training: {{A}} Theoretical Justification for Adaptivity},
  author = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  year = {2019},
  journal = {arXiv preprint arXiv:1905.11881},
  eprint = {1905.11881},
  archiveprefix = {arXiv}
}

@inproceedings{zhaoInfovaeBalancingLearning2019,
  title = {Infovae: {{Balancing}} Learning and Inference in Variational Autoencoders},
  booktitle = {Proceedings of the Aaai Conference on Artificial Intelligence},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2019},
  volume = {33},
  pages = {5885--5892},
  isbn = {2374-3468},
  file = {/Users/jingang/Dropbox/References/Variational Autoencoder/Disentangle Representations/InfoVAE Balancing Learning and Inference in Variational Autoencoders.pdf}
}

@inproceedings{zhaoLearningGeneralizeUnseen2021,
  title = {Learning to Generalize Unseen Domains via Memory-Based Multi-Source Meta-Learning for Person Re-Identification},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhao, Yuyang and Zhong, Zhun and Yang, Fengxiang and Luo, Zhiming and Lin, Yaojin and Li, Shaozi and Sebe, Nicu},
  year = {2021},
  pages = {6277--6286},
  annotation = {GSCC: 0000053}
}

@article{zhaoMetalearningHypernetworks2020,
  title = {Meta-Learning via Hypernetworks},
  author = {Zhao, Dominic and {von Oswald}, Johannes and Kobayashi, Seijin and Sacramento, Jo{\~a}o and Grewe, Benjamin F.},
  year = {2020},
  publisher = {IEEE},
  file = {/Users/jingang/Dropbox/References/Hypernetworks/Meta-Learning via Hypernetworks.pdf;/Users/jingang/Dropbox/References/Hypernetworks/Meta-Learning via Hypernetworks.pdf}
}

@misc{zhengDAPEDataAdaptivePositional2024,
  title = {{{DAPE}}: {{Data-Adaptive Positional Encoding}} for {{Length Extrapolation}}},
  shorttitle = {{{DAPE}}},
  author = {Zheng, Chuanyang and Gao, Yihang and Shi, Han and Huang, Minbin and Li, Jingyao and Xiong, Jing and Ren, Xiaozhe and Ng, Michael and Jiang, Xin and Li, Zhenguo and Li, Yu},
  year = {2024},
  month = oct,
  number = {arXiv:2405.14722},
  eprint = {2405.14722},
  publisher = {arXiv},
  urldate = {2024-10-12},
  abstract = {Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be data-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that DAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/AH6ISIWU/Zheng et al. - 2024 - DAPE Data-Adaptive Positional Encoding for Length Extrapolation.pdf;/Users/jingang/Zotero/storage/5HVYW952/2405.html}
}

@misc{zhengDAPEV2Process2024,
  title = {{{DAPE V2}}: {{Process Attention Score}} as {{Feature Map}} for {{Length Extrapolation}}},
  shorttitle = {{{DAPE V2}}},
  author = {Zheng, Chuanyang and Gao, Yihang and Shi, Han and Xiong, Jing and Sun, Jiankai and Li, Jingyao and Huang, Minbin and Ren, Xiaozhe and Ng, Michael and Jiang, Xin and Li, Zhenguo and Li, Yu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04798},
  eprint = {2410.04798},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.04798},
  urldate = {2024-10-14},
  abstract = {The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens, in contrast to earlier feed-forward neural networks. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem. The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/CSQ8TN76/Zheng et al. - 2024 - DAPE V2 Process Attention Score as Feature Map for Length Extrapolation.pdf;/Users/jingang/Zotero/storage/32R3KYVR/2410.html}
}

@misc{zhengRethinkingPositionalEncoding2021,
  title = {Rethinking {{Positional Encoding}}},
  author = {Zheng, Jianqiao and Ramasinghe, Sameera and Lucey, Simon},
  year = {2021},
  month = oct,
  number = {arXiv:2107.02561},
  eprint = {2107.02561},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.02561},
  urldate = {2025-01-03},
  abstract = {It is well noted that coordinate based MLPs benefit -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/EASSVEVP/Zheng et al. - 2021 - Rethinking Positional Encoding.pdf;/Users/jingang/Zotero/storage/WAGC47NX/2107.html}
}

@article{zhiFallibilityAnalyticRoots2002,
  title = {Fallibility of Analytic Roots of Cubic Equations of State in Low Temperature Region},
  author = {Zhi, Yun and Lee, Huen},
  year = {2002},
  journal = {Fluid phase equilibria},
  volume = {201},
  number = {2},
  pages = {287--294},
  publisher = {Elsevier},
  file = {/Users/jingang/Dropbox/References/Flash calculations/Fallibility of analytic roots of cubic equationsof state in low temperature region.pdf}
}

@article{zhongMetaDMoEAdaptingDomain2022,
  title = {Meta-{{DMoE}}: {{Adapting}} to {{Domain Shift}} by {{Meta-Distillation}} from {{Mixture-of-Experts}}},
  author = {Zhong, Tao and Chi, Zhixiang and Gu, Li and Wang, Yang and Yu, Yuanhao and Tang, Jin},
  year = {2022},
  journal = {arXiv preprint arXiv:2210.03885},
  eprint = {2210.03885},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/Domain generalization/Others/Meta-DMoE.pdf}
}

@article{zhouComprehensiveSurveyPretrained2024,
  title = {A Comprehensive Survey on Pretrained Foundation Models: A History from {{BERT}} to {{ChatGPT}}},
  shorttitle = {A Comprehensive Survey on Pretrained Foundation Models},
  author = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
  year = {2024},
  month = nov,
  journal = {International Journal of Machine Learning and Cybernetics},
  issn = {1868-808X},
  doi = {10.1007/s13042-024-02443-6},
  urldate = {2025-01-05},
  abstract = {Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks across different data modalities. A PFM (e.g., BERT, ChatGPT, GPT-4) is trained on large-scale data, providing a solid parameter initialization for a wide range of downstream applications. In contrast to earlier methods that use convolution and recurrent modules for feature extraction, BERT learns bidirectional encoder representations from Transformers, trained on large datasets as contextual language models. Similarly, the Generative Pretrained Transformer (GPT) method employs Transformers as feature extractors and is trained on large datasets using an autoregressive paradigm. Recently, ChatGPT has demonstrated significant success in large language models, utilizing autoregressive language models with zero-shot or few-shot prompting. The remarkable success of PFMs has driven significant breakthroughs in AI, leading to numerous studies proposing various methods, datasets, and evaluation metrics, which increases the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, and other data modalities. It covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning, while also exploring advanced PFMs for different data modalities and unified PFMs that address data quality and quantity. Additionally, the review discusses key aspects such as model efficiency, security, and privacy, and provides insights into future research directions and challenges in PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and user-friendly interactive ability for artificial general intelligence.},
  langid = {english},
  keywords = {Artificial Intelligence,BERT,ChatGPT,Computer vision,GPT-4,Graph learning,Natural language processing,Pretrained foundation models}
}

@inproceedings{zhouDeepDomainadversarialImage2020,
  title = {Deep Domain-Adversarial Image Generation for Domain Generalisation},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  year = {2020},
  volume = {34},
  pages = {13025--13032},
  isbn = {2374-3468},
  annotation = {GSCC: 0000125}
}

@article{zhouDomainAdaptiveEnsemble2021,
  title = {Domain Adaptive Ensemble Learning},
  author = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  year = {2021},
  journal = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {8008--8018},
  publisher = {IEEE},
  isbn = {1057-7149},
  annotation = {GSCC: 0000079},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Learning strategy/Ensemble methods/Domain adaptive ensemble learning.pdf}
}

@article{zhouDomainGeneralizationMixstyle2021,
  title = {Domain Generalization with Mixstyle},
  author = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.02008},
  eprint = {2104.02008},
  archiveprefix = {arXiv},
  annotation = {GSCC: 0000187}
}

@article{zhouDomainGeneralizationSurvey2022,
  title = {Domain Generalization: {{A}} Survey},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  issn = {0162-8828},
  file = {/Users/jingang/Dropbox/References/Domain generalization/Overview/Domain Generalization A Survey.pdf}
}

@inproceedings{zhouImprovingAdversarialRobustness2022,
  title = {Improving {{Adversarial Robustness}} via {{Mutual Information Estimation}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zhou, Dawei and Wang, Nannan and Gao, Xinbo and Han, Bo and Wang, Xiaoyu and Zhan, Yibing and Liu, Tongliang},
  year = {2022},
  pages = {27338--27352},
  publisher = {PMLR},
  isbn = {2640-3498},
  file = {/Users/jingang/Dropbox/References/Mutual Information Estimation/Improving Adversarial Robustness via Mutual Information Estimation.pdf}
}

@inproceedings{zhouLearningGenerateNovel2020,
  title = {Learning to Generate Novel Domains for Domain Generalization},
  booktitle = {European Conference on Computer Vision},
  author = {Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  year = {2020},
  pages = {561--578},
  publisher = {Springer},
  annotation = {GSCC: 0000147}
}

@article{zhouMixtureofExpertsExpertChoice2022,
  title = {Mixture-of-{{Experts}} with {{Expert Choice Routing}}},
  author = {Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew and Chen, Zhifeng and Le, Quoc and Laudon, James},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.09368},
  eprint = {2202.09368},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Mixture-of-Experts with Expert Choice Routing.pdf}
}

@misc{zhouValueResidualLearning2024,
  title = {Value {{Residual Learning For Alleviating Attention Concentration In Transformers}}},
  author = {Zhou, Zhanchao and Wu, Tianyi and Jiang, Zhiyun and Lan, Zhenzhong},
  year = {2024},
  month = oct,
  number = {arXiv:2410.17897},
  eprint = {2410.17897},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.17897},
  urldate = {2024-11-14},
  abstract = {Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50\%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jingang/Zotero/storage/WFCLQ657/Zhou et al. - 2024 - Value Residual Learning For Alleviating Attention Concentration In Transformers.pdf;/Users/jingang/Zotero/storage/BYM5LNAX/2410.html}
}

@article{zhuangComprehensiveSurveyTransfer2020,
  title = {A Comprehensive Survey on Transfer Learning},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {1},
  pages = {43--76},
  publisher = {IEEE},
  isbn = {0018-9219}
}

@article{zhuGeneratingMachinelearnedEquation2020,
  title = {Generating a Machine-Learned Equation of State for Fluid Properties},
  author = {Zhu, Kezheng and M{\"u}ller, Erich A.},
  year = {2020},
  journal = {The Journal of Physical Chemistry B},
  volume = {124},
  number = {39},
  pages = {8628--8639},
  publisher = {ACS Publications},
  isbn = {1520-6106}
}

@article{zhuGeneratingMachinelearnedEquation2020a,
  title = {Generating a Machine-Learned Equation of State for Fluid Properties},
  author = {Zhu, Kezheng and M{\"u}ller, Erich A.},
  year = {2020},
  journal = {The Journal of Physical Chemistry B},
  volume = {124},
  number = {39},
  pages = {8628--8639},
  publisher = {ACS Publications},
  isbn = {1520-6106},
  file = {/Users/jingang/Library/CloudStorage/Dropbox/References/ML EoS/Generating a Machine-Learned Equation of State for Fluid Properties.pdf}
}

@misc{zhuXTabCrosstablePretraining2023,
  title = {{{XTab}}: {{Cross-table Pretraining}} for {{Tabular Transformers}}},
  shorttitle = {{{XTab}}},
  author = {Zhu, Bingzhao and Shi, Xingjian and Erickson, Nick and Li, Mu and Karypis, George and Shoaran, Mahsa},
  year = {2023},
  month = may,
  number = {arXiv:2305.06090},
  eprint = {2305.06090},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.06090},
  urldate = {2024-08-18},
  abstract = {The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce XTab, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and quantities among tables by utilizing independent featurizers and using federated learning to pretrain the shared component. Tested on 84 tabular prediction tasks from the OpenML-AutoML Benchmark (AMLB), we show that (1) XTab consistently boosts the generalizability, learning speed, and performance of multiple tabular transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior performance than other state-of-the-art tabular deep learning models on various tasks such as regression, binary, and multiclass classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jingang/Zotero/storage/46CD6WWI/Zhu et al. - 2023 - XTab Cross-table Pretraining for Tabular Transformers.pdf;/Users/jingang/Zotero/storage/CUJLY888/2305.html}
}

@article{zophDesigningEffectiveSparse2022,
  title = {Designing Effective Sparse Expert Models},
  author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.08906},
  eprint = {2202.08906},
  archiveprefix = {arXiv},
  file = {/Users/jingang/Dropbox/References/Mixture of experts/Designing effective sparse expert models.pdf}
}

@article{zophNeuralArchitectureSearch2016,
  title = {Neural Architecture Search with Reinforcement Learning},
  author = {Zoph, Barret and Le, Quoc V.},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.01578},
  eprint = {1611.01578},
  archiveprefix = {arXiv}
}

@article{zouRegularizationVariableSelection2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  year = {2005},
  journal = {Journal of the royal statistical society: series B (statistical methodology)},
  volume = {67},
  number = {2},
  pages = {301--320},
  publisher = {Wiley Online Library}
}
