\begin{figure}
 \centering
 \includegraphics[width=\linewidth]{figures/introfigure.pdf}
 \caption{Critical difference diagram computed with the Friedman test followed by a post-hoc Wilcoxon test (with $\alpha=0.1$) for the (a) F-score and (b) range-based F-score over $250$ time series in KDD21 \cite{kdd}. Bold lines indicate insignificant differences of connected methods.}
 \label{fig:introf}
\end{figure}



\section{Introduction}

Massive collections of time-varying measurements, commonly referred to as \textit{time series}, have become a reality in virtually every scientific and industrial domain ~\cite{DBLP:journals/dagstuhl-reports/BagnallCPZ19,Palpanas2019,paparrizos_k-shape_2016,paparrizos2019grail,paparrizos2020debunking,dziedzic2019band,bariya2021k,paparrizos22fast,paparrizos2018fast}. Notably, there is an increasingly pressing need for developing techniques for efficient and effective analysis of zettabytes of time series produced by millions of Internet-of-Things (IoT) devices \cite{iotstats,hung2017leading,paparrizos2021vergedb,jiang2020pids,jiang2021good,liu2021decomposed}. 
IoT deployments empower diverse data science applications in environmental sciences, astrophysics, neuroscience, and engineering, among others~\cite{Palpanas2015,fulfillingtheneed}, and have revolutionized many industries, including automobile, healthcare, manufacturing, and utilities~\cite{ng2017internet}. 
However, rare events, or imperfections and inherent complexities in the data generation and measurement pipelines, often introduce abnormalities that appear as \textit{anomalies} in time-series databases, impacting the effectiveness of downstream tasks and analytics.


Consequently, \textit{anomaly detection} (AD) becomes a fundamental problem with broad applications sharing the same goal~\cite{statisticaloutliers,DBLP:conf/vldb/SubramaniamPPKG06,DBLP:conf/icdm/YehZUBDDSMK16}: analyzing time series to identify observations that do not conform to some notion of expected behavior based on previously observed data. During the past decades, a multitude of AD methods have been proposed and compared~\cite{yeh_time_2018,DBLP:journals/datamine/LinardiZPK20,DBLP:conf/icde/BoniolLRP20a,DBLP:conf/edbt/Gao0B20,boniol_unsupervised_2021,boniol2021sandaction,Series2GraphPaper,DBLP:journals/pvldb/BoniolPPF21,DBLP:conf/icdm/YehZUBDDSMK16,Liu:2008:IF:1510528.1511387,breunig_lof_2000,DBLP:journals/csur/Blazquez-Garcia21,DBLP:journals/pvldb/PaparrizosKBTPF22,theseus}. Different from other domains that principally focus on \textit{point-based} anomalies (i.e., outliers in standalone observations), AD for time series is also concerned with \textit{range-based} anomalies (i.e., outliers spanning multiple observations). 
Unfortunately, it has become common practice to use traditional point-based information retrieval (IR) accuracy measures, such as Precision, Recall, and F-score, to quantify the effectiveness of different anomaly detectors.



In addition, the previously mentioned IR evaluation measures suffer from a significant limitation: a threshold is necessary over the anomaly score produced by AD methods to mark each time-series point as an anomaly or not. The most common approach to set a threshold value is to use the average score plus three times the standard deviation of the anomaly score. However, this popular choice might not suit every AD method, use case, and domain, leading to significant variations in the quality values of the evaluation measures. Therefore, these IR measures are difficult to trust and complicate evaluating different AD methods on heterogeneous benchmarks. To eliminate the need to set a threshold, another standard measure for binary classification is used: the receiver operator characteristic (ROC) curve and the Area Under the Curve (AUC), which is the area below the ROC curve (AUC-ROC). The ROC curve is generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings (instead of only one threshold used in Precision, Recall, and F-score measures). Another variant, the Precision-Recall (PR) curve, represents the relation between Precision and Recall, and the Area under the PR curve (AUC-PR) is the area below PR \cite{10.1145/1143844.1143874}.



Unfortunately, all previous measures, Precision, Recall, F-Score, AUC-ROC, and AUC-PR, are ideal for point-based anomalies but cannot adequately evaluate ubiquitous range-based contextual and collective anomalies \cite{blazquez2021review}. Remarkably, the mapping of discrete labels into continuous data introduces unavoidable shortcomings (e.g., difficulty in marking precisely the range of the anomalies and handling misalignments between the human labels and the anomaly range produced by thresholding the anomaly score). To address these shortcomings, a range-based definition of Precision and Recall has been proposed by extending the traditional definitions \cite{tatbul_precision_2018}. Range-based Precision, Recall, and F-Score consider several factors: (i) whether a subsequence is detected or not; (ii) how many points in the subsequence are detected; (iii) which part of the subsequence is detected; and (iv) how many fragmented regions correspond to one real subsequence outlier. This definition is detailed and comprehensive; however, several parameters require tuning and, importantly, a threshold over the anomaly score is still required.

A recent study~\cite{DBLP:journals/corr/abs-2303-01272} listed AD evaluation measures for time series, describing their advantages and shortcomings measured on synthetic time series. However, there has never been (to the best of our knowledge) a large-scale systematic quantitative and qualitative analysis of time-series AD evaluation measures on real time series. Notably, the choice of evaluation measure may significantly bias the experimental outcome. To understand the implications of choosing an appropriate measure, Figure \ref{fig:introf} depicts the critical diagrams of the F-score and range-based F-score computed with the Friedman test followed by a Wilcoxon test~\cite{10.2307/3001968} over several AD methods (see Section \ref{sec:exp} for details) across the $250$ time series of the KDD21 dataset~\cite{kdd}. Figure~\ref{fig:introf} demonstrates that not only the ranking is changing, but also some methods shift from insignificantly to significantly different from one measure to the other. 



In this paper, we extensively evaluate quality measures for time-series AD to assess their robustness under noise, misalignments, and different anomaly cardinality ratios. Specifically, our study includes $9$ previously proposed quality measures, computed over the anomaly scores of $10$ AD methods across $10$ diverse datasets that contain $900$ time series with marked anomalies. Our analysis assesses the robustness of quality measures both qualitatively and quantitatively by studying the influence of threshold, lag, noise, and normal-abnormal anomaly ratio to identify robust measures that better separate accurate from inaccurate methods. 

Our results indicate that measures producing quality values independently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for time-series AD. This is surprising considering that we include the range-based Precision, Recall, and F-score measures, which highlights the strong influence the thresholding of anomaly scores has in assessing the quality of methods. 

Motivated by this observation and to address the limitations of existing measures, we propose \textit{four} new accuracy evaluation measures. We first present Range-AUC-ROC and Range-AUC-PR, threshold-independent (for the anomaly score) evaluation measures that use a continuous buffer region in the labels to increase the robustness to potential misalignments with the human labels. Then, we propose the Volume Under the Surface (VUS) family of measures that extend the traditional AUC measures to consider all buffer sizes (in addition to all thresholds). Therefore, VUS-ROC and VUS-PR are parameter-free, threshold-independent, and robust to lags, noise, and anomaly cardinality ratios. 
Our analysis demonstrates that VUS-ROC and VUS-PR are the most reliable accuracy quality measures for both point-based and range-based anomaly evaluation. Table~\ref{methodTable} summarizes the accuracy evaluation measures analyzed in this paper based on their independence to four critical characteristics.

In addition to the accuracy evaluation, we perform an extensive execution time evaluation. 
VUS requires computing accuracy measures (i.e., ROC, Precision, and Recall) for different values of buffer sizes.
As this buffer size changes the labels of the time series, the naive implementation of VUS computes accuracy measures over the entire labels as many times as the number of buffer sizes we consider. However, the buffer size affects only small sections of the labels, leaving the vast majority unchanged. Therefore, we introduce two optimized versions of the VUS computation algorithm that compute accuracy measures over the sections affected by the buffer length. We demonstrate theoretically and empirically the execution time improvement of the optimized implementations over the naive implementation of VUS while remaining exact (i.e., providing the same values as the naive implementation). Overall, our optimized implementations is up to 10 times faster than the naive implementation for large time series, and render the VUS measures easier to use in practice.


\begin{table}[tp]
\caption{Analysis of quality measures based on: (i) independence to the number of anomalies; (ii) independence to the threshold; (iii) adaptation to continuous sequences; and (iv) independence to setting parameters.}
\label{methodTable}
\centering
\scalebox{0.70}{
\begin{tabular}{|c||c|c|c|c|}
\hline
{\bf Acc. Measure} & \# of anom. & Score Thres. & Sequence-adapted & Param-free \\
\hline
Precision@k & \redmark & \greencheck & \redmark & \redmark \\
Precision & \greencheck & \redmark & \redmark & \redmark \\
Recall & \greencheck & \redmark & \redmark & \redmark \\
F-Score & \greencheck & \redmark & \redmark & \redmark\\
Rprecision & \greencheck & \redmark & \greencheck & \redmark \\
Rrecall & \greencheck & \redmark & \greencheck & \redmark \\
RF-Score & \greencheck & \redmark & \greencheck & \redmark \\
AUC-PR & \greencheck & \greencheck & \redmark &\greencheck \\
AUC-ROC & \greencheck & \greencheck & \redmark & \greencheck \\
\hline
\rowcolor{Gray}
\multicolumn{5}{|c|}{{\bf \emph{Proposed measures}}}\\
\hline
R-AUC-PR & \greencheck & \greencheck & \greencheck & \redmark \\
R-AUC-ROC & \greencheck & \greencheck & \greencheck & \redmark \\
VUS-PR & \greencheck & \greencheck & \greencheck & \greencheck \\
VUS-ROC & \greencheck & \greencheck & \greencheck & \greencheck \\
\hline
\end{tabular}
} % scalebox
\end{table}

Interestingly, even though outside of the scope of this paper, the flexibility of VUS measures in evaluating methods while varying parameters of choice may have implications beyond time-series AD. Specifically, VUS measures are applicable across binary classification tasks for evaluating methods with a single quality value while considering different parameter choices (e.g., learning rates, batch sizes, and other critical varying parameters). 

\noindent{\bf (Sec.~\ref{sec:background})} We start with a detailed discussion of the relevant background and related work. Then, we present our contributions\footnote{A preliminary version has appeared elsewhere~\cite{DBLP:journals/pvldb/PaparrizosBPTEF22}.}:

\noindent{\bf (Sec.~\ref{sec:problem})} We discuss the limitations of existing evaluation measures, resulting in a formal definition of the necessary principles of time-series AD quality measures.

\noindent{\bf (Sec.~\ref{sec:range-auc})} We present R-AUC (ROC and PR) that rely on a new label transformation for a more robust and reliable score for contextual and collective anomalies.

\noindent{\bf (Sec.~\ref{sec:vus})} We introduce VUS (ROC and PR), parameter-free measures that formally extend AUC-based measures to consider more varying parameters.

\noindent{\bf (Sec.~\ref{sec:fasterimpl})} We introduce $VUS_{opt}$ and $VUS^{mem}_{opt}$, two optimized versions for the computation of both VUS-ROC and VUS-PR, with significantly better time complexity properties. These two optimized versions prune the sections of the time series in which the anomaly score does not change regardless of the threshold and the buffer length. The $VUS^{mem}_{opt}$ algorithm further improves time-complexity by using more memory.

\noindent{\bf (Sec.~\ref{exp:qual} and~\ref{exp:quant})} We extensively evaluate, both qualitatively and quantitatively, $13$ quality measures ($9$ previously proposed and our $4$ new measures) across $10$ AD methods over $10$ diverse datasets containing $900$ time series with marked anomalies.

\noindent{\bf (Sec.~\ref{exp:separability})} We analyze the separability of the measures by comparing pairs of accurate and inaccurate methods.

\noindent{\bf (Sec.~\ref{sec:entropy})} We assess the consistency of the measures by evaluating changes in methods' ranks across measures.

\noindent{\bf (Sec.~\ref{sec:exectime})} We evaluate the scalability of the VUS-based measures on different time series characteristics, and we measure the speed-up of $VUS_{opt}$ and $VUS^{mem}_{opt}$ compared to the naive implementation of VUS.

\noindent{\bf (Sec.~\ref{sec:conclusions})} Finally, we conclude with the implications of our work and discuss future research directions.




