\section{Background and Related Work}
\label{sec:background}

We first introduce formal notations useful for the rest of the paper (Section \ref{sec:notation}). Then, we review in detail previously proposed evaluation measures for time-series AD methods (Section \ref{acc_measure}). 


\subsection{Time-Series and Anomaly Score Notations}
\label{sec:notation}
We review notations for the time series and anomaly score sequence.
\newline \textbf{Time Series: } A time series $T \in \mathbb{R}^n $ is a sequence of real-valued numbers $T_i\in\mathbb{R}$ $[T_1,T_2,...,T_n]$, where $n=|T|$ is the length of $T$, and $T_i$ is the $i^{th}$ point of $T$. We are typically interested in local regions of the time series, known as subsequences. A subsequence $T_{i,\ell} \in \mathbb{R}^\ell$ of a time series $T$ is a continuous subset of the values of $T$ of length $\ell$ starting at position $i$. Formally, $T_{i,\ell} = [T_i, T_{i+1},...,T_{i+\ell-1}]$.	
\newline \textbf{Anomaly Score Sequence: } For a time series $T \in \mathbb{R}^n $, an AD method $A$ returns an anomaly score sequence $S_T$. For point-based approaches (i.e., methods that return a score for each point of $T$), we have $S_T \in \mathbb{R}^n$. For range-based approaches (i.e., methods that return a score for each subsequence of a given length $\ell$), we have $S_T \in \mathbb{R}^{n-\ell}$. Overall, for range-based (or subsequence-based) approaches, we define $S_T = [{S_T}_1,{S_T}_2,...,{S_T}_{n-\ell}]$ with ${S_T}_i \in [0,1]$.


\subsection{Accuracy Evaluation Measures for AD}
\label{acc_measure}

We present previously proposed quality measures for evaluating the accuracy of an AD method, given its anomaly score. We first discuss threshold-based and then threshold-independent measures.

\subsubsection{Threshold-based AD Evaluation Measures} \hfill\\
The anomaly score $S_T$ produced by an AD method $A$ highlights the parts of the time series $T$ considered as abnormal. The highest values in the anomaly score correspond to the most abnormal points. Threshold-based measures require setting a threshold to mark each point as an anomaly or not. Usually, this threshold is set to $\mu(S_T) + \alpha*\sigma(S_T)$, with $\alpha$ set to 3~\cite{statisticaloutliers}, where $\mu(S_T)$ is the mean and $\sigma(S_T)$ is the standard deviation $S_T$. Given a threshold $Thres$, we compute the $pred \in \{0,1\}^n$ as follows:


\begin{equation}
\begin{split}
&\forall i \in [1,|S_T|], pred_i = \left.
\begin{cases}
0,& \text{if: } {S_T}_i < Thres \\
1,& \text{if: } {S_T}_i \geq Thres 
\end{cases}
\right.
\end{split}
\end{equation}

Threshold-based measures compare $pred$ to $label \in \{0,1\}^n$, which indicates the true (human provided) labeled anomalies. Given the identity vector $I=[1,1,...,1]$, the points detected as anomalies or not fall into the following four categories:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=0.5cm]
	\item {\bf True Positive (TP)}: Number of points that have been correctly identified as anomalies. Formally: $TP = label^\top \cdot pred$.
	\item {\bf True Negative (TN)}: Number of points that have been correctly identified as normal. Formally: $TN = (I-label)^\top \cdot (I-pred)$.
	\item {\bf False Positive (FP)}: Number of points that have been wrongly identified as anomalies. Formally: $FP = (I-label)^\top \cdot pred$.
	\item {\bf False Negative (FN)}: Number of points that have been wrongly identified as normal. Formally: $FN = label^\top \cdot (I-pred)$.
\end{itemize}
Given these categories, several quality measures have been proposed to assess the accuracy of AD methods.
\newline \textbf{Precision: } We define Precision (or positive predictive value) as the number of correctly identified anomalies over the total number of points detected as anomalies by the method:

\begin{equation}
Precision = \frac{TP}{TP+FP}
\end{equation}
\newline \textbf{Recall: } We define Recall (or True Positive Rate (TPR), $tpr$) as the number of correctly identified anomalies over all anomalies:

\begin{equation}
Recall = \frac{TP}{TP+FN}
\end{equation}

\noindent \textbf{False Positive Rate (FPR): } A supplemental measure to the Recall is the FPR, $fpr$, defined as the number of points wrongly identified as anomalies over the total number of normal points:

\begin{equation}
fpr = \frac{FP}{FP+TN}
\end{equation}
\newline \textbf{F-Score: } Precision and Recall evaluate two different aspects of the AD quality. A measure that combines these two aspects is the harmonic mean $F_{\beta}$, with non-negative real values for $\beta$:
\begin{equation}
F_{\beta} = \frac{(1+\beta^2)*Precision*Recall}{\beta^2*Precision+Recall}
\end{equation}
\noindent Usually, $\beta$ is set to 1, balancing the importance between Precision and Recall. In this paper, $F_1$ is referred to as F or F-score.
\newline \textbf{Precision@k: } All previous measures require an anomaly score threshold to be computed. An alternative approach is to measure the Precision using a subset of anomalies corresponding to the $k$ highest value in the anomaly score $S_T$. This is equivalent to setting the threshold such that only the $k$ highest values are retrieved. 

To address the shortcomings of the point-based measures, a range-based definition was proposed, extending the traditional Precision and Recall \cite{tatbul_precision_2018}. This definition considers several factors: (i) whether a subsequence is detected or not (ExistenceReward or ER); (ii) how many points in the subsequence are detected (OverlapReward or OR); (iii) which part of the subsequence is detected (position-dependent weight function); and (iv) how many fragmented regions correspond to one real subsequence outlier (CardinalityFactor or CF). Formally, we define $R=\{R_1,...R_{N_r}\}$ as the set of anomaly ranges, with $R_k=\{pos_i,pos_{i+1}, ..., pos_{i+j}\}$ and $\forall pos \in R_k, label_{pos} = 1$, and $P=\{P_1,...P_{N_p}\}$ as the set of predicted anomaly ranges, with $P_k=\{pos_i,pos_{i+1}, ..., pos_{i+j}\}$ and $\forall pos \in R_k, pred_{pos} = 1$. Then, we define ER, OR, and CF as follows:

%\vspace{-0.3cm}
\begin{equation}
\footnotesize{
\begin{split}
&ER(R_i,P) = \left.
\begin{cases}
1, &\text{if } \sum_{j=1}^{N_p} |R_i \cap P_j| \geq 1\\
0, &\text{otherwise}
\end{cases}
\right. \\
&CF(R_i,P) = \left.
\begin{cases}
1, &\text{if } \exists P_i \in P, |R_i \cap P_i| \geq 1\\
\gamma(R_i,P), &\text{otherwise}
\end{cases}
\right. \\
&OR(R_i,P) = CF(R_i,P)*\sum_{j=1}^{N_p} \omega(R_i,R_i \cap P_j, \delta)
\end{split}
}
%\vspace{-0.1cm}
\end{equation}

\noindent The $\gamma(),\omega()$, and $\delta()$ are tunable functions that capture the cardinality, size, and position of the overlap respectively.
The default parameters are set to $\gamma()=1,\delta()=1$ and $\omega()$ to the overlap ratio covered by the predicted anomaly range~\cite{tatbul_precision_2018}.
\newline \textbf{Rprecision and Rrecall~\cite{tatbul_precision_2018}: } Based on the above, we define:

%\vspace{-0.2cm}
\begin{equation}
\footnotesize{
\begin{split}
Rprecision(R,P) &= \frac{\sum_{i=1}^{N_p} Rprecision_s(R,P_i)}{N_p}\\
Rprecision_s(R,P_i) &= CF(P_i,R)*\sum_{j=1}^{N_r} \omega(P_i,P_i \cap R_j, \delta)
\end{split}
}
%\vspace{-0.2cm}
\end{equation}

%%\vspace{-0.2cm}
\begin{equation}
\footnotesize{
\begin{split}
Rrecall(R,P) &= \frac{\sum_{i=1}^{N_r} Rrecall_s(R_i,P)}{N_r} \\
Rrecall_s(R_i,P) &= \alpha*ER(R_i,P) + (1-\alpha)*OR(R_i,P)
\end{split}
}
%%\vspace{-0.1cm}
\end{equation}
\noindent The parameter $\alpha$ is user defined. The default value is $\alpha=0$.
\newline \textbf{Range F-score (RF)~\cite{tatbul_precision_2018}: } As described previously, the F-score combines Precision and Recall. Similarly, we define $RF_{\beta}$, for $\beta>0$ as follows:

%\vspace{-0.3cm}
\begin{equation}
RF_{\beta} = \frac{(1+\beta^2)*Rprecision*Rrecall}{\beta^2*Rprecision+Rrecall}
%\vspace{-0.1cm}
\end{equation}

\noindent As before, $\beta$ is set to 1. In this paper, $RF_1$ is referred to as RF-score.

\subsubsection{Threshold-independent AD Evaluation Measures} \hfill\\
Until now, we introduced accuracy measures requiring to threshold the produced anomaly score of AD methods. However, the accuracy values vary significantly when the threshold changes. To evaluate a method holistically using its corresponding anomaly score, two measures from the AUC family of measures are used.
\newline \textbf{AUC-ROC~\cite{FAWCETT2006861}: } The Area Under the Receiver Operating Characteristics curve (AUC-ROC) is defined as the area under the curve corresponding to TPR on the y-axis and FPR on the x-axis when we vary the anomaly score threshold. The area under the curve is computed using the trapezoidal rule. For that purpose, we define $Th$ as an ordered set of thresholds between 0 and 1. Formally, we have $Th=[Th_0,Th_1,...Th_N]$ with $0=Th_0<Th_1<...<Th_N=1$. Therefore, $AUC\text{-}ROC$ is defined as follows:
%\vspace{-0.2cm}
\begin{equation}
\begin{split}
&AUC\text{-}ROC = \frac{1}{2}\sum_{k=1}^{N} \Delta^{k}_{TPR}*\Delta^{k}_{FPR}\\
&\text{with: } \left.
\begin{cases}
\Delta^{k}_{FPR} &= FPR(Th_{k})-FPR(Th_{k-1})\\
\Delta^{k}_{TPR} &= TPR(Th_{k-1})+TPR(Th_{k})
\end{cases}
\right. 
\end{split}
\label{equAUCROC}
\end{equation}
\newline \textbf{AUC-PR~\cite{10.1145/1143844.1143874}: } The Area Under the Precision-Recall curve (AUC-PR) is defined as the area under the curve corresponding to the Recall on the x-axis and Precision on the y-axis when we vary the anomaly score threshold. 
As before, the area under the curve can be calculated using the trapezoidal rule, defined as follows:

{\footnotesize
\begin{equation}
\begin{split}
&AUC\text{-}PR = \frac{1}{2}\sum_{k=1}^{N} \Delta^{k}_{Precision}*\Delta^{k}_{Recall}\\
&\text{with: } \left.
\begin{cases}
\Delta^{k}_{Recall} &= Recall(Th_{k})-Recall(Th_{k-1})\\
\Delta^{k}_{Precision} &= Precision(Th_{k-1})+Precision(Th_{k})
\end{cases}
\right. 
\end{split}
%\vspace{-0.1cm}
\label{equAUCPR}
\end{equation}
}

\noindent As discussed in~\cite{10.1145/1143844.1143874}, linear interpolation in PR space may result in an overly optimistic estimate of performance. Therefore, we adopt an alternative interpolation method, Stepwise Interpolation, to approximate the area under the curve by calculating the average precision of the PR curve:

%\vspace{-0.2cm}
\begin{equation}
AUC\text{-}PR = \sum_{k=1}^{N} Precision(Th_{k})*\Delta^{k}_{Recall}
%\vspace{-0.2cm}
\end{equation}

\noindent For consistency, we use the above equation in this paper to compute AUC-PR.


\section{Problem motivation and limitations}
\label{sec:problem}

\begin{figure}
 \centering
 \includegraphics[height=13cm,width=\linewidth]{figures/param_influence_intro.pdf}
 %\vspace{-0.7cm}
 \caption{Evaluation measures when we vary the (a) threshold, (b) lag, (c) noise, and (d) normal/abnormal ratio. Example with Isolation Forest methods over a snippet of an ECG time series~\cite{goldberger_physiobank_2000}.}
 \label{fig:limitation_robust}
 \vspace{-0.1cm}
\end{figure}

Having introduced existing measures to assess the quality of range-based anomalies, we now elaborate on their critical limitations.

%\vspace{-0.1cm}
\subsection{Limitations of Threshold-based Measures}
%\vspace{-0.1cm}

The need to threshold the anomaly score severely impacts the accuracy measures. First, Figure~\ref{fig:limitation_robust}(a) depicts an electrocardiogram time series with an arrhythmia in red (Figure~\ref{fig:limitation_robust}(a.1)) and the corresponding anomaly score computed with Isolation Forest~\cite{liu_isolation_2008} (Figure~\ref{fig:limitation_robust}(a.2)) for one threshold equal to $\mu(score) + \sigma(score)$ and for another threshold $\mu(score) + 0.6*\sigma(score)$ (Figures~\ref{fig:limitation_robust}(a.3) and (a.4)). We compute the different accuracy measures for the first threshold (blue bars in Figure~\ref{fig:limitation_robust}(a.5)) and the second threshold (orange bars in Figure~\ref{fig:limitation_robust}(a.5)) and their differences (Figure~\ref{fig:limitation_robust}(a.6)). We observe that the threshold choice has a substantial impact on Precision, Rprecision, F and RF scores. On the contrary, the threshold-independent measures (i.e., measures computing all possible thresholds), namely, AUC-ROC and AUC-PR, show a clear advantage.

Overall, the threshold choice depends on the application and the type of input time series. Setting the threshold automatically is hard and almost impossible when we compare different categories of AD methods across heterogeneous benchmarks. To illustrate this point, we consider two transformations of the anomaly score that correspond to practical cases we observed (e.g., different methods introduce different lag and noise levels to the anomaly score).
\newline \textbf{Influence of Noise: } Some AD methods applied to some specific time series might result in a noisy anomaly score. In addition, due to manufacturing issues or external causes, a sensor can inject noise into the time series, which then propagates on the anomaly score. Figure~\ref{fig:limitation_robust}(c) depicts two cases: the first corresponds to an anomaly score without any noise (Figure~\ref{fig:limitation_robust}(c.2)). The second corresponds to an anomaly score with noise (Figure~\ref{fig:limitation_robust}(c.2)). We applied on both cases the same threshold $\mu(score) + \sigma(score)$. We observe in Figure~\ref{fig:limitation_robust}(c.6) that most of the threshold-based measures are strongly impacted by noise. This is caused by the fact that the score fluctuates around the threshold, making threshold-based measures less robust to noise. On the contrary, AUC-ROC and AUC-PR are much less influenced by noise, returning approximately the same value.
\newline \textbf{Influence of Normal/Abnormal Ratio: } Depending on the domain and the task, the number of anomalies and, consequently, the normal/abnormal ratio changes drastically. A variation in this ratio might cause a variation in the threshold, which leads to variations in threshold-based accuracy measure values. This is explained by the fact that if an anomaly score detects the anomalies correctly, the standard deviation of that score will be higher for a time series with more anomalies. Figure~\ref{fig:limitation_robust}(d) depicts two cases: one time series snippet with a $0.2$ ratio (Figure~\ref{fig:limitation_robust}(d.2)) and one time series snippet with a $0.05$ ratio (Figure~\ref{fig:limitation_robust}(d.4)). We observe that this change implies a larger variation for several threshold-based measures. Thus, the latter confirms the limitations and the non-robustness of threshold-based measures to the anomaly cardinality ratio.

%\vspace{-0.2cm}
\subsection{Limitations of Point-based Measures}
%\vspace{-0.1cm}

In the previous section, we illustrated the limitations of threshold-based measures. By design and because of their independence from the threshold choice, AUC-ROC and AUC-PR measures are robust to those limitations. However, those measures are designed for point-based outliers. Each point is considered independently and the detection of each point has an equivalent contribution to AUC. In contrast, we need to consider two factors, the range detection and the existence detection, for the subsequence AD problem.

The range detection has the same methodology as point detection. We prefer that the algorithm detects every point in the subsequence anomaly. The existence detection is a loose but crucial estimation for the anomaly subsequence detector: detecting a tiny segment of one subsequence outlier is still of great value. 
\newline \textbf{Mismatch between the anomaly score and labels: } Compared to point-based AD, range-based AD encourages accurate capturing of each subsequence anomaly, but the existence detection is good enough to be partially rewarded. Two other reasons support the application of this coarse estimation. 

First, there is no consistent labeling tradition among datasets. Some may label the whole period as an anomaly if this period does not repeat the typical pattern, while others may only mark a partial period. Figure~\ref{fig:influence_labeling}(3) depicts different labeling strategies. Figures~\ref{fig:influence_labeling}(ex1), (ex2), and (ex3) depict three real examples corresponding to three different labeling strategies that we observed in existing datasets (see Table~\ref{table:charac}). Even if we specify that each period should share the same label, the next question is how to define the starting and end points of a period. Given accurate starting or end points, it is also challenging to label a small segment in one period. Unlike a point outlier, which is an evident deviation from the trend line of the time series, range-based anomalies may not have outrageous values. This difficulty of labeling is inevitable when we assign the discrete labels to a continuous time series. There may be a transition region between the two statuses, but we have to decide on a discontinuous jumping point artificially.

Second, many algorithms, for instance, LOF~\cite{breunig_lof_2000} and iForest~\cite{liu_isolation_2008}, would first apply a sliding window to convert a 1-D time series to a set of high-dimensional data points. We denote the original time series as ($T_1$, $T_2$, $\dots$, $T_n$), and suppose the length of window is $\ell$, then the converted data set is $\{(T_i, \dots, T_{i+\ell-1})|i \in \{1, \dots T-\ell+1\}\}$. The label of point $T_k$ in the time series is defined as the label of high-dimensional point ($T_{k-\ell/2}$, $\dots$, $T_{k+\ell/2-1}$) in the transformed dataset. The conversion from a time series to a dataset has one consequence: every dimension in the high-dimensional point is equally important. So, an abnormal value at the middle or end of this point has the same ability to make it an outlier in the high-dimensional space. Usually, if the sliding window covers more anomaly points, a good algorithm should give a higher anomaly score to the converted data point. However, there are some exceptions, such as that one abnormal value at the beginning or the end of sliding windows is enough to make the converted point an outlier. 
To summarize, an anomaly subsequence $(T_s, \dots, T_e)$ may induce a high anomaly score for the range [$T_{s-\ell/2}, T_{e+\ell/2}$]. A perfect result is that the peak of the anomaly score is slightly broader than the whole abnormal region. The latter is illustrated in Figure~\ref{fig:influence_labeling}(2). However, the anomaly score is not perfect. A high score may be assigned at the range [$T_{s-\ell/2}, T_s$], which fails to reveal the entire range of the outlier but succeeds in indicating the starting region. AUC-based measures will give a low value since there is no overlap between the peak and the outlier.
\newline \textbf{Overall Limitations due to Lag: } A lag can be injected into the anomaly score depending on the choice of AD methods. Overall, such a lag may also exist due to the approximation made during the labeling phase. As illustrated in Figure~\ref{fig:limitation_robust}(b), such a lag (even small) has a substantial impact on \textit{all} existing evaluation measures. For example, in Figure~\ref{fig:limitation_robust}(b) AUC-PR fluctuates between $0.75$ and $0.50$ for a lag of $0.25$ of the labeled section length. Among all measures, only the AUC-ROC measure demonstrates to be less sensitive to such lag (as well as noise and normal/abnormal ratio).


\begin{figure}[tb]
 \centering
 \includegraphics[height=5.5cm,width=\linewidth]{figures/influence_eval_short.pdf}
 %\vspace{-0.7cm}
 \caption{Influence of the anomaly detection method score (2) and labeling strategy (3), illustrated with three examples.}
 \label{fig:influence_labeling}
 %\vspace{-0.2cm}
\end{figure}

%\vspace{-0.2cm}
\subsection{Problem Definition}
%\vspace{-0.1cm}

In summary, our goal is to develop a new anomaly score threshold-independent evaluation measure based on the robust principles of AUC. A promising direction is an extension of AUC for the range-based AD with the following desired properties: 


\noindent{\bf Robust to Lag}: Two similar anomaly scores with a slight lag difference should return approximately the same accuracy measures. For example, a high anomaly score near the border of the anomaly should be rewarded as close as a high anomaly score in the middle of the range-based anomaly.
	%\item 

\noindent{\bf Robust to Noise}: Two similar anomaly scores with and without noise should return similar accuracy.
	%\item 

\noindent{\bf Robust to the Anomaly Cardinality Ratio}: This ratio should not impact the accuracy measures.
	%\item 

\noindent{\bf High Separability between Accurate and Inaccurate Methods}: The accuracy measure should well separate accurate from inaccurate methods.
	%\item 

\noindent{\bf Consistent}: Finally, an appropriate accuracy measure should produce consistent scores for similar time series (i.e., rank different anomaly detection methods in a consistent manner).
%\end{itemize}

\noindent Next, we present new accuracy measures to satisfy these properties.