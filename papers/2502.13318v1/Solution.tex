
\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figures/porposed_approaches.pdf}
  %\vspace{-0.7cm}
  \caption{Illustration of previous quality measures compared to our proposed measures. By varying the buffer window, VUS constructs a surface of TPR, FPR, and window. The volume under the surface is a measure of AUC for various windows. }
  \label{fig:auc_volume}
  %\vspace{-0.1cm}
\end{figure*}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/label_extension.pdf}
  %\vspace{-0.7cm}
  \caption{\commentRed{Illustration of proposed label extension strategy.}}
  \label{fig:label_extension}
  %\vspace{-0.1cm}
\end{figure}

%\vspace{-0.1cm}
\section{Our Measures: Range-AUC and VUS}
%\vspace{-0.1cm}

We first present new range-based extensions for ROC and PR curves by introducing a new continuous label to enable more flexibility in measuring detected anomaly ranges. We then present the Volume Under the Surface (VUS) for ROC and PR curves. VUS extends the mathematical model of Range-AUC measures by varying the buffer length. \commentRed{An alternative solution is to learn the necessary parameters and thresholds. However, such a solution works only under supervised settings and may impact the generalizability to new datasets. For the specific case of unsupervised learning, the threshold selection can only be achieved using statistical heuristics. The most common strategy to set the threshold unsupervisely is to set it to $\mu(S_T) + \alpha*\sigma(S_T)$, with $\alpha=3$~\cite{statisticaloutliers}. We will use this strategy when comparing our proposed measures to threshold-based measures.}

%\vspace{-0.2cm}
\subsection{Range-AUC-ROC and Range-AUC-PR}
\label{sec:range-auc}
%\vspace{-0.1cm}

To compute the ROC curve and PR curve for a subsequence, we need to extend to definitions of TPR, FPR, and Precision. 
The first step is to add a buffer region at the boundary of outliers. The idea is that there should be a transition region between the normal and abnormal subsequences to accommodate the false tolerance of labeling in the ground truth (as discussed, this is unavoidable due to the mapping of discrete data to continuous time series). An extra benefit is that this buffer will give credit to the high anomaly score in the vicinity of the outlier boundary, which is what we expected with the application of a sliding window originally. 

Figure ~\ref{fig:auc_volume}(b) shows the original binary labels (in blue), and Figure ~\ref{fig:auc_volume}(c) the new label with buffer region (in orange). By default, the width of the buffer region at each side is half of the period $w$ of the time series (the period is an intrinsic characteristic of the time series). Differently, this parameter can be set into the average length of anomaly sizes or can be set to a desired value by the user.

The traditional binary label is extended to a continuous value. Formally, for a given buffer length $\ell$, the positions $s,e \in [0,|label|]$ the beginning and end indexes of a labeled anomaly (i.e., sections of continuous $1$ in $label$), we define the continuous $label_r$ as follows:
%\vspace{-0.1cm}
\begin{equation}
\footnotesize{
\begin{split}
&\forall i \in [0,|label|], \quad label_{\ell i} \\
& = \begin{cases}
\bigg(1-\frac{|s-i|}{\ell}\bigg)^{\frac{1}{2}}, & \text{if } s-\frac{\ell}{2} \leq i < s \text{ and } {pred}_i = 1, \\
1, & \text{if } s \leq i < e, \\
\bigg(1-\frac{|e-i|}{\ell}\bigg)^{\frac{1}{2}}, & \text{if } e \leq i < e+\frac{\ell}{2} \text{ and } {pred}_i = 1, \\
0, & \text{else}.
\end{cases}
\end{split}
\label{label_equation}
}
%\vspace{-0.1cm}
\end{equation}

\commentRed{
\noindent Specifically, if no predicted anomaly exists within the extended buffer region, we set ${label_{\ell}}_i$ to $0$ to prevent unnecessary false negatives caused by excessive label extension, as illustrated in Figure~\ref{fig:label_extension}.
}
\noindent When the buffer regions of two discontinuous outliers overlap, the label will be the superposition of these two orange curves with one as the maximum value. Using this new continuous label, one can compute $TP_\ell$, $FP_\ell$, $TN_\ell$ and $FN_\ell$ similarly as follows:
%\vspace{-0.2cm}
\begin{equation}
{\small
\begin{split}
&TP_{\ell} = label_{\ell}^\top \cdot pred &FP_{\ell} = (I- label_{\ell})^\top \cdot pred \\
&TN_{\ell} = (I- label_{\ell})^\top \cdot (I-pred) &FN_{\ell} = label_{\ell}^\top \cdot (I-pred) \\
\end{split}
} % font size
%\vspace{-0.2cm}
\end{equation}
\noindent The total number of positive points P in this case naively should be $P_{{\ell}_0} = TP_{\ell}+ FN_{\ell} = label_{\ell}^\top \cdot I$. Here, we define it as:
%\vspace{-0.2cm}
\begin{equation}
%\begin{split}
P_{\ell} = (label+label_{\ell})^\top \cdot \frac{I}{2} \text{, } N_{\ell} = |label_{\ell}|-P_{\ell}
%\end{split}
%\vspace{-0.2cm}
\end{equation}
\noindent The reason is twofold. When the length of the outlier is several periods, $P_{{\ell}_0}$ and $P_{\ell}$ are similar because the ratio of the buffer region to the whole anomaly region is small. When the length of the outlier is only half-period, the size of the buffer region is nearly two times the original abnormal region. In other words, to pursue false tolerance, the relative change we make to the ground truth is too significant. We use the average of $label$ and $label_{\ell}$ to limit this change.

We finally generalize the point-based $Recall$, $Precision$, and $FPR$ to the range-based variants. Formally, following the definition of $R$ and $P$ as the set of anomalies range and detected predicted anomaly range (see Section~\ref{acc_measure}), we define $TPR_{\ell}$, $FPR_{\ell}$, and $Precision_{\ell}$:
%\vspace{-0.2cm}
\begin{equation}
{\small
\begin{split}
TPR_\ell&=Recall_{\ell}=\frac{TP_{\ell}}{P_{\ell}}*\sum_{R_i \in R} \frac{ExistenceR(R_i,P)}{|R|} \\
FPR_{\ell}&=\frac{FP_{\ell}}{N_{\ell}} \text{, } Precision_{\ell}=\frac{TP_{\ell}}{TP_{\ell}+FP_{\ell}} \\
\end{split}
} % font size
%\vspace{-0.2cm}
\label{eqution_constant}
\end{equation}
\noindent Note that $TPR_r=Recall_r$. Moreover, for the recall computation, we incorporate the idea of Existence Reward \cite{tatbul_precision_2018}, which is the ratio of the number of detected subsequence outliers to the total number of subsequence outliers. However, consistent with their work \cite{tatbul_precision_2018}, we do not include the Existence ratio in the definition of range-precision. We can then compute R-AUC-ROC and R-AUC-PR using Equation~\ref{equAUCROC} and Equation~\ref{equAUCPR}.
\newline \textbf{Relation between Range-ROC and Range-PR: } PR curve is a supplement to the ROC curve. In a highly unbalanced dataset, because the number of positive points is too small, at the same level of FPR, it is easy to have a high TPR (or $TPR_{\ell}$) at the cost of low precision.  There are deep connections between ROC and PR \cite{10.1145/1143844.1143874}. First, ROC and PR have one-to-one mapping for a given dataset because the confusion matrix is uniquely determined given TPR and FPR. This relation is broken for the range method because we include an extra Existence factor for range-TPR. Therefore, the confusion matrix cannot be decided in the range-ROC space. Secondly, for a point-based version, if one ROC curve \textit{dominates} another ROC curve, its corresponding PR curve would also dominate another one. Here, dominate means the curve is always higher or equal to another one. Because of the Existence factor, this rule is also lifted for the range definition. This is true only if both of the methods have the same existence ratio. However, this is not always guaranteed. Finally, a maximized AUC does not necessarily correspond to a maximized AP. This holds for the range definition.

\subsection{VUS: Volume Under the Surface}
\label{sec:vus}

Our range-AUC family of measures chooses the width of the buffer region to be half of a subsequence length $\ell$ of the time series. Such buffer length can be either set based on the knowledge of an expert (e.g., the usual size of arrhythmia in an electrocardiogram) or set automatically using the time series's period. \commentRed{The latter can be computed using different strategies: (I) using the Fourier transform to identify the most relevant period of the time series, or (ii) computing the cross-correlation and retrieving the lag value (i.e., subsequence length) that locally maximize the correlation. In practice, we observe that computing the cross-correlation of a time series and selecting the length corresponding to the first local maximal is accurate. In addition, the latter allows users to consider the shortest period length, significantly limiting the execution time of most of the AD methods and the range-AUC measures.} 

Since the period is an intrinsic property of the time series, we can compare various algorithms on the same basis. However, a different approach may get a slightly different period. In addition, there are multi-period time series. So other groups may get different range-AUC because of the difference in the period. As a matter of fact, the parameter $\ell$, if not well set, can strongly influence range-AUC measures. To eliminate this influence, we introduce two generalizations of range-AUC measures.

The solution is to compute ROC and PR curves for different buffer lengths from 0 to $\ell$ as shown in Figure~\ref{fig:auc_volume}(d). Therefore, ROC and PR curves become a surface in a three-dimensional space. Then, the overall accuracy measure corresponds to the Volume Under the Surface (VUS) for either the ROC surface (VUS-ROC) or PR surface (VUS-PR). As the R-AUC-ROC and R-AUC-PR are measures independent of the threshold on the anomaly score, the VUS-ROC and VUS-PR are independent of both the threshold and buffer length. Formally, given $Th=[Th_0,Th_1,...Th_N]$ with $0=Th_0<Th_1<...<Th_N=1$, and $\mathcal{L}=[\ell_0,\ell_1,...,\ell_L]$ with $0=\ell_0<\ell_1< ... < \ell_L = \ell$, we have:
%\vspace{-0.1cm}
\begin{equation}
\footnotesize{
\begin{split}
&VUS\text{-}ROC = \frac{1}{4}\sum_{w=1}^{L} \sum_{k=1}^{N} \Delta^{(k,w)} * \Delta^{w} \text{, with: }\\
&\left.
\begin{cases}
\Delta^{(k,w)} &= \Delta^{k}_{TPR_{\ell_w}}*\Delta^{k}_{FPR_{\ell_w}}+\Delta^{k}_{TPR_{\ell_{w-1}}}*\Delta^{k}_{FPR_{\ell_{w-1}}} \\
\Delta^{k}_{FPR_{\ell_w}} &= FPR_{\ell_w}(Th_{k})-FPR_{\ell_w}(Th_{k-1}) \\
\Delta^{k}_{TPR_{\ell_w}} &= TPR_{\ell_w}(Th_{k-1})+TPR_{\ell_w}(Th_{k}) \\
\Delta^{w} &= |\ell_w - \ell_{w-1}|
\end{cases}
\right. 
\end{split}
\label{equVUSROC}
}
%\vspace{-0.1cm}
\end{equation}

%\vspace{-0.1cm}
\begin{equation}
\footnotesize{
\begin{split}
&VUS\text{-}PR = \frac{1}{2}\sum_{w=1}^{L} \sum_{k=1}^{N} \Delta^{(k,w)} * \Delta^{w} \text{, with: }\\
&\left.
\begin{cases}
\Delta^{(k,w)} &= {Precision_{\ell_w}(Th_k)}*\Delta^{k}_{Re_{\ell_w}}\\  &\quad+{Precision_{\ell_{w-1}}(Th_k)}*\Delta^{k}_{Re_{\ell_{w-1}}} \\
\Delta^{k}_{Re_{\ell_w}} &= Recall_{\ell_w}(Th_{k})-Recall_{\ell_w}(Th_{k-1}) \\
% \Delta^{k}_{Pr_{\ell_w}} &= Precision_{\ell_w}(Th_{k-1})+Precision_{\ell_w}(Th_{k}) \\
\Delta^{w} &= |\ell_w - \ell_{w-1}|
\end{cases}
\right. 
\end{split}
\label{equVUSPR}
}
%\vspace{-0.1cm}
\end{equation} 

From the above equations, VUS measures are more expensive to compute than range-AUC measures.
Thus, the application of VUS versus range-AUC depends on our knowledge of which buffer length to set. If one user knows which would be the most appropriate buffer length, range-AUC-based measures are preferable compared to VUS-based measures.
However, if there exists an uncertainty on $\ell$, then setting a range and using VUS increases the flexibility of the usage and the robustness of the evaluation. Finally, more parameters than $\ell$ can be included in VUS-based measures. If, in addition to $\ell$, there is a need to define a range for another parameter (such as the normal model length $\ell_{N_M}$ of NormA), the two-dimensional surface is transformed into a three-dimensional hyper-surface. In general, for $P$ parameters, the value is the volume under a $|P|-1$ hyper-surface. 






\subsubsection{{\bf Complexity Analysis}}\hfill\\

This section analyzes the complexity of the VUS-based measures. 
We take into account both computation time, and memory usage.

\begin{algorithm}[tb]
{\small
    \caption{\textbf{VUS algorithm}}\label{alg:VUS}
    \label{alg:vus}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{Original Labels $label$, anomaly score $S_{T}$, maximum Buffer Length $L$, thresholds $N$}
    \Output{VUS\_ROC, VUS\_PR}
    \BlankLine
    $Th$ $\leftarrow$ $Thresholds(N)$\;
    $\mathcal{L}$ $\leftarrow$ $Buffer\_Lengths(L)$\;
    AUC $\leftarrow$ [],
    AP $\leftarrow$ []\;
    \tcp{Iterate through the buffer Lengths}
    \ForEach{$\ell \in \mathcal{L}$ }
    {
        $Create$ $label_\ell$ from $label$ and $\ell$\;
        $seq$= $Anomaly\_Index(label_\ell)$\;
        $list\_TPR_{\ell}$ $\leftarrow$ [],
        $list\_FPR_{\ell}$ $\leftarrow$ [],
        $list\_Prec_{\ell}$ $\leftarrow$ []\;
        \tcp{Iterate through the thresholds}
        \ForEach{$threshold \in Th$}
        {   
            $pred$ $\leftarrow$ $S_{T}>threshold$\;
            $Change$ $label_\ell$ to $label_\ell^{thres}$ based on $pred$\;
            $product$ $\leftarrow$ $label_\ell^{thres}*pred$\;
            $SumPred$ $\leftarrow$ $\sum_{p\in pred} p$\;
            $SumLabel$ $\leftarrow$ $\sum_{p\in label_\ell^{thres}} p$\;
            $TP_\ell$ $\leftarrow$ 0\;
            \ForEach{$seg \in seq_L$}
            {
                $TP_\ell$ $\leftarrow$ $TP_\ell$ + $\sum_{p\in product[seg[0]:(seg[1]+1)]}p$
            }
            $Compute$ $FP_\ell$, $P_\ell$, $N_\ell$\ from $TP_\ell$, $SumPred$, $SumLabel$\;% $\leftarrow$ $\sum_{p\in product}p$\;
            %$FP_\ell$ $\leftarrow$ $\sum_{p\in product}p$\;
            %$P_\ell$ $\leftarrow$ $\sum_{l_1,l_2\in label,label_\ell} \frac{(l_1+l_2)}{2}$
            
            $Existence_{seq}$ $\leftarrow$ 0\;
            \tcp{Iterate through the anomalies}
            \ForEach{$seg \in seq$}
            {
                \If{$\sum_{p\in product[seg[0]:(seg[1]+1)]}p>0$}
                {
                    $Existence_{seq}$ $\leftarrow$ $Existence_{seq}$ + 1
                }
                $Existence$ $\leftarrow$ $\frac{Existence_{seq}}{|seq|}$
                  
            }
            $Append$ $\frac{TP_\ell*Existence}{P_\ell}$ in $list\_TPR_{\ell}$\;
            $Append$ $\frac{FP_\ell}{N_\ell}$ in $list\_FPR_{\ell}$\;
            $Append$ $\frac{TP_\ell}{TP_\ell+FP_\ell}$ in $list\_Prec_{\ell}$\;
        }
        $Compute$ AUC\_r, AP\_r $from$ $list\_TPR_{\ell}$,$list\_FPR_{\ell}$ and $list\_Prec_{\ell}$\;
        $Append$ AUC\_r, AP\_r $in$ AUC, AP\;
    }
    \tcp{Avg. across thresholds and buffer lengths}
    VUS\_ROC $\leftarrow$ $\frac{\sum_{a\in AUC}a}{|\mathcal{L}|}$,
    VUS\_PR $\leftarrow$ $\frac{\sum_{a\in AP}a}{|\mathcal{L}|}$\;
 % font size
 }
\end{algorithm}

{\bf [Time Complexity]}
The time complexity of VUS (both VUS-ROC and VUS-PR) is determined by varying two parameters, namely the buffer length $\ell \in \mathcal{L}$ and the number of thresholds $N$.
This is further illustrated in Algorithm~\ref{alg:vus}, which breaks down the computation steps. 
It comprises a nested loop that demonstrates the variation of the parameters buffer length \commentRed{($\mathcal{L}$ lengths in total)} and number of thresholds \commentRed{($N$ thresholds in total)}. \commentRed{Therefore, VUS complexity is quadratic to $N$ and $L$. Then, for each threshold and length (inside the loop) the computational complexity is $O(\alpha \ell_a + T_1 + T_2)$}, where $\alpha$ is the number of anomalies, $\ell_a$ refers to the mean length of anomalies, and $T_1, T_2$ refer to computations in the order of length of the time series $T$ involved in the anomaly detection. 
There is a distinction between $T_1$ and $T_2$ because their practical implementations are optimized to different extents, producing very different execution times. 
Here, $O(T_1)$ is the complexity of the calculations involving the entire time series, such as $pred$ (i.e., a boolean sequence indicating if a point of the anomaly score $S_T$ is above a given threshold), and $label_\ell$ (i.e., the modified label sequence with buffer regions). $O(T_2)$ refers to the complexity of the computation of $product$, $TP_\ell$, $FP_\ell$, $P_\ell$, and $N_\ell$, which has a cost of $|T|$, but is less optimized than the previously described computation. 
Moreover, $\alpha \ell_a$ corresponds to the computation of $Existence$. Thus, the total complexity of the algorithm is $O(NL(\alpha \ell_a+T_1+T_2))$. 
In practice, $\alpha \ell_a$ is negligible compared to $T_1$ or $T_2$, and VUS complexity can be approximated to $O(NL(T_1+T_2))$.

{\bf [Space Complexity]}
The space complexity can be obtained from the pseudo-code in Algorithm~\ref{alg:vus}. 
The computation of VUS-ROC and VUS-PR is performed by iterating over the set of buffer lengths ($\mathcal{L}$) and the set of thresholds ($N$). 
Thus, the space complexity of VUS is $O(NL)$.

\subsection{A faster Implementation of VUS}
\label{sec:fasterimpl}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/Static_Dyn.pdf}
  %\vspace*{-0.5cm}
  \caption{Synthetic illustration of an anomaly score (a) and labels (b) of a given time series. We differentiate \textbf{static sections} that are invariant to the change of threshold and buffer length, and \textbf{dynamic sections} that have an impact on the accuracy.}
  \label{fig:static_dyn}
\end{figure}

As theoretically explained in the previous section, VUS's computation heavily depends on the time series length, as well as on the number of buffer lengths considered. In this section, we propose a novel implementation that significantly reduces the theoretical computation of the VUS measures.
 

\begin{algorithm}[tb]
{\small
    \caption{\textbf{\textbf{VUS}$_{opt}$}}\label{alg:VUS_opt}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{Original Labels $T$, anomaly score $S_{T}$, maximum Buffer Length $L$, thresholds $N$}
    \Output{VUS-ROC, VUS-PR}
    \BlankLine
    $Th$ $\leftarrow$ $Thresholds(N)$,
    $\mathcal{L}$ $\leftarrow$ $Buffer\_Lengths(L)$\;
    $Create$ $label_L$ from $label$ and $L$\;
    \tcp{Extract anomalies positions for buffer length L}
    $seq_L$ $\leftarrow$ $Anomaly\_Index(label_L)$\;
    $AUC$ $\leftarrow$ [], 
    $AP$ $\leftarrow$ []\;
    \tcp{Static Part}
    \tcp{Iterate through thresholds only}
    \ForEach{$threshold \in Th$}
    {\label{line_vus:static_b}
        $pred$ $\leftarrow$ $S_{T}>threshold$\;
        $SumPred$ $\leftarrow$ $\sum_{p\in pred} p$\;
    }\label{line_vus:static_e}
    \tcp{Dynamic Part}
    \tcp{Iterate through the buffer Lengths}
    \ForEach{$\ell \in \mathcal{L}$ }
    {\label{line_vus:dyn_b}
        $Create$ $label_\ell$ from $label$ and $\ell$\;
        $seq$= $Anomaly\_Index(label_\ell)$\;
        $l\_TPR_{\ell}$ $\leftarrow$ [], 
        $l\_FPR_{\ell}$ $\leftarrow$ [], 
        $l\_Prec_{\ell}$ $\leftarrow$ []\;
        \tcp{Iterate through the thresholds}
        \ForEach{$threshold \in Th$}
        {   
            $pred$ $\leftarrow$ $S_{T}>threshold$\;
            $Change$ $label_\ell$ to $label_\ell^{thres}$ based on $pred$\;
            $product$ $\leftarrow$ $label_\ell^{thres}$*$pred$\;
            $SumLabel$ $\leftarrow$ $\sum_{p\in label_\ell^{thres}} p$\;
            $TP_\ell$ $\leftarrow$ 0\;
            \ForEach{$seg \in seq_L$}
            {
                $TP_\ell$ $\leftarrow$ $TP_\ell$ + $\sum_{p\in product[seg[0]:(seg[1]+1)]}p$
            }
            $Compute$ $FP_\ell$, $P_\ell$, $N_\ell$\ from $TP_\ell$, $SumPred$, $SumLabel$\;
            
            $Existence_{seq}$ $\leftarrow$ 0\;
            \tcp{Iterate through the anomalies}
            \ForEach{$seg \in seq$}
            {
                \If{$\sum_{p\in product[seg[0]:(seg[1]+1)]}p>0$}
                {
                    $Existence_{seq}$ $\leftarrow$ $Existence_{seq}$ + 1
                }
                $Existence$ $\leftarrow$ $\frac{Existence_{seq}}{|seq|}$
                  
            }
            $Append$ $\frac{TP_\ell*Existence}{P_\ell}$ in $l\_TPR_{\ell}$\;
            $Append$ $\frac{FP_\ell}{N_\ell}$ in $l\_FPR_{\ell}$\;
            $Append$ $\frac{TP_\ell}{TP_\ell+FP_\ell}$ in $l\_Prec_{\ell}$\;
        }
        $Compute$ $AUC_r$, $AP_r$ $from$ $l\_TPR_{\ell}$,$l\_FPR_{\ell}$ and $l\_Prec_{\ell}$\;
        $Append$ $AUC_r$, $AP_r$ $in$ $AUC$, $AP$\;
    } \label{line_vus:dyn_e}
    \tcp{Avg. across thresholds and buffer lengths}
    VUS-ROC $\leftarrow$ $\frac{\sum_{a\in AUC}a}{|\mathcal{L}|}$, 
    VUS-PR $\leftarrow$ $\frac{\sum_{a\in AP}a}{|\mathcal{L}|}$\;
 % font size
 }
\end{algorithm}

\begin{algorithm}
{\small
    \caption{\textbf{VUS$_{opt}^{mem}$}}\label{alg:VUS_opt^{mem}}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{Original Labels $T$, anomaly score $S_{T}$, maximum Buffer Length $L$, thresholds $N$}
    \Output{VUS-ROC, VUS-PR}
    \BlankLine
    $Th$ $\leftarrow$ $Thresholds(N)$,
    $\mathcal{L}$ $\leftarrow$ $Buffer\_Lengths(L)$\;
    $Create$ $label_L$ from $label$ and $L$\;
    \tcp{Extract anomalies positions for buffer length L}
    $seq_L$ $\leftarrow$ $Anomaly\_Index(label_L)$\;
    $AUC$ $\leftarrow$ [], 
    $AP$ $\leftarrow$ []\; 
    $Pred_{Th}$ $\leftarrow$ []\;
    \tcp{Static Part}
    \tcp{Iterate only through thresholds}
    \ForEach{$threshold \in Th$}
    {
        $pred$ $\leftarrow$ $S_{T}>threshold$\;
        $Pred_{Th}$ $\leftarrow$ Append with $pred$\;
        $SumPred$ $\leftarrow$ $\sum_{p\in pred} p$\;
    }
    \tcp{Dynamic Part}
    \tcp{Iterate through the buffer Lengths}
    \ForEach{$\ell \in \mathcal{L}$ }
    {
        $Create$ $label_\ell$ from $label$ and $\ell$\;
        $seq$= $Anomaly\_Index(label_\ell)$\;
        $l\_TPR_{\ell}$ $\leftarrow$ [],
        $l\_FPR_{\ell}$ $\leftarrow$ [],
        $l\_Prec_{\ell}$ $\leftarrow$ []\;
        \tcp{Iterate through the thresholds}
        count $\leftarrow$ 0\;
        \ForEach{$threshold \in Th$}
        {  
            $Change$ $label_\ell$ to $label_\ell^{thres}$ based on $Pred_{Th}[threshold]$\;
            $product$ $\leftarrow$ $label_\ell^{thres}*Pred_{Th}[threshold]$\;
            $SumLabel$ $\leftarrow$ $\sum_{p\in label_\ell^{thres}} p$\;
            $TP_\ell$ $\leftarrow$ 0\;
            \ForEach{$seg \in seq_L$}
            {
                $TP_\ell$ $\leftarrow$ $TP_\ell$ + $\sum_{p\in product[seg[0]:(seg[1]+1)]}p$
            }
            $Compute$ $FP_\ell$, $P_\ell$, $N_\ell$\ from $TP_\ell$, $SumPred$, $SumLabel$\;
            $Existence_{seq}$ $\leftarrow$ 0\;
            \tcp{Iterate through the anomalies}
            \ForEach{$seg \in seq$}
            {
                \If{$\sum_{p\in product[seg[0]:(seg[1]+1)]}p>0$}
                {
                    $Existence_{seq}$ $\leftarrow$ $Existence_{seq}$ + 1
                }
                $Existence$ $\leftarrow$ $\frac{Existence_{seq}}{|seq|}$
                  
            }
            $Append$ $\frac{TP_\ell*Existence}{P_\ell}$ in $l\_TPR_{\ell}$\;
            $Append$ $\frac{FP_\ell}{N_\ell}$ in $l\_FPR_{\ell}$\;
            $Append$ $\frac{TP_\ell}{TP_\ell+FP_\ell}$ in $l\_Prec_{\ell}$\;   
        }
        $Compute$ AUC\_r, AP\_r $from$ $l\_TPR_{\ell}$,$l\_FPR_{\ell}$ and $l\_Prec_{\ell}$\;
        $Append$ AUC\_r, AP\_r $in$ AUC, AP\;
    }
    \tcp{Avg. across thresholds and buffer lengths}
    VUS\_ROC $\leftarrow$ $\frac{\sum_{a\in AUC}a}{|\mathcal{L}|}$,
    VUS\_PR $\leftarrow$ $\frac{\sum_{a\in AP}a}{|\mathcal{L}|}$\;
 % font size
 }
\end{algorithm}










\subsubsection{Dynamic versus Static sections}\hfill\\

The variations of thresholds and buffer length affect the modified labels (i.e., $label_\ell$) and $pred$, which cause changes in the values of True and False Positive Rates ($TPR$ and $FPR$). 
However, not all sections of the time series are affected by these variations. 
The data points, whose labels are not affected by the change in the buffer length for a given threshold, have the same $TPR$ and $FPR$ (i.e., data points that lie outside the maximum possible buffer length of an anomaly). 

As a result, we can segment the time series into two categories: $Dynamic$ and $Static$. The first category corresponds to sections of the time series containing labels affected by the variation of buffer length. The second category corresponds to sections that are not affected by these changes. Figure~\ref{fig:static_dyn} illustrates this segmentation, enabling us to compute the same measures with significantly reduced computational costs.





\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/dyn_stat_2.pdf}
  %\vspace*{-0.5cm}
  \caption{Synthetic illustration of the labels evolution with $L$. In contrast to dynamic sections (in green), the buffer length has no impact on VUS within the static sections (in grey).}
  \label{fig:static_dyn_2}
\end{figure}





\subsubsection{Algorithmic Implementation}\hfill\\

The optimization described above can be performed following two different strategies:

\begin{itemize}
\item {\bf VUS$_{opt}$}: In this version, we split the time series anomaly scores $S_T$ and $label_\ell$ into static and dynamic sections. We compute the constant required to calculate VUS only once for the static sections, and once for each buffer length and threshold value for the dynamic sections.
\item {\bf VUS$_{opt}^{mem}$}: This version is an extension of the previous, with a code-wise modification that leads to a further decrease in time complexity at the expense of increased space complexity.
Given the large main memory sizes of modern servers (and even desktops and laptops), VUS$_{opt}^{mem}$ represents a very attractive solution in practice.
\end{itemize}

Due to the consideration of splitting data into static and dynamic regions, the implementation has the following advantages:

\begin{itemize}
\item The static split avoids repetitive calculations that would have involved the same values repeatedly in a nested loop (i.e., computing only the accuracy values for each threshold for the static region, since buffer size does not affect static regions).
\item The calculations of $TP$ and $N$ in Equation~\ref{eqution_constant} essentially add up to zero in the above consideration of the static part, and do not need to be computed. 
\item The overall computational time is similar to that of the Range-AUC measures for a relatively small number of anomalies, but even more importantly, it does not increase when the number of anomalies gets significantly larger.
\end{itemize}

The computational steps of $VUS_{opt}$ and $VUS_{opt}^{mem}$ are shown in Algorithm~\ref{alg:VUS_opt} and Algorithm~\ref{alg:VUS_opt^{mem}}, respectively.
These two algorithms are divided into two different sections: (i) the static part in which we compute VUS for sections of the time series without anomalies, and (ii) the dynamic part in which we compute VUS only for the time series sections that contain anomalies.
In the following sections, we analyze in detail the theoretical complexity (space and time).

\noindent{\bf [VUS$_{opt}$ Time and Space Complexity]}: The VUS$_{opt}$ computation is similar to the original VUS computation (cf. Algorithm~\ref{alg:vus}) for the calculations of the dynamic part. 
However, it differs in the static part, as its calculations that involve predictions and labels are unaffected by buffer length. 
The static part computation (Lines~\ref{line_vus:static_b}-\ref{line_vus:static_e}) involves the predictions (according to all possible thresholds in $Th$) and their summation. 
Thus, the complexity for the static sections is $O(N(T_1+T_2))$. 
For the dynamic part (Lines~\ref{line_vus:dyn_b}-\ref{line_vus:dyn_e}), the computations (for each threshold and buffer length) are only performed for the sections containing anomalies (i.e., dynamic sections in Figure~\ref{fig:static_dyn}). Thus, the complexity of the dynamic part computation is $O(\alpha \ell_a)$.
We also have to compute the predictions (score values above a given threshold) for each dynamic section, which have a complexity of $O(T_2)$. 
Thus the total complexity adds up to $O(N(T_1+T_2))+O(NL(\alpha \ell_a+T_2))$.
In addition, the space complexity of the dynamic computation with the nested loop of thresholds and buffer length is $O(NL)$, and $O(N)$ for the static part. Therefore, the overall space complexity of VUS$_{opt}$ is $O(NL)$.

\noindent{\bf [VUS$_{opt}^{mem}$ Time and Space Complexity]}
As shown in Algorithm~\ref{alg:VUS_opt^{mem}}, the complexity of the static sections remains unchanged compared to VUS$_{opt}$. However, the complexity related to the dynamic sections is reduced by reusing the saved predictions calculated in the static part (as illustrated in Figure~\ref{fig:static_dyn_2}, it is not affected by buffer lengths).
This reduces the dynamic complexity to  $O(\alpha \ell_a)$, adding up to a total complexity of $O(N(T_1+T_2)+ NL\alpha \ell_a)$. 
For VUS$_{opt}^{mem}$, similarly to VUS$_{opt}$, the space complexity of the dynamic computation containing the nested loop of thresholds and buffer length is $O(NL)$. However, due to the storage and indexing of predictions, the computations related to the static sections result in a space complexity of $O(NT)$. Thus, the total space complexity of VUS$_{opt}^{mem}$ is $O(N(L+T))$. 
The time and space complexity of all three VUS implementations are listed in Table~\ref{tab:complexity_summary}.

\begin{table}[tb]
    \centering
    \caption{{Space and time complexity of VUS implementations}}
    \scalebox{0.88}{
    \begin{tabular}{|c|c|c|}
    \hline
         Version & Time & Space \\
         \hline
         $VUS$ & $O(NL(\alpha \ell_a+T_1+T_2))$ & $O(NL)$\\ 
 VUS$_{opt}$ & $O(N(T_1+T_2+L(\alpha \ell_a+T_2)))$ &  $O(NL)$\\
 VUS$_{opt}^{mem}$ & $O(N(T_1+T_2+L\alpha \ell_a))$ & $O(N(L+T))$\\ 
 \hline
    \end{tabular}
    }
    \label{tab:complexity_summary}
\end{table}


