\section{Experimental Analysis}
\label{sec:exp}
We now describe in detail our experimental analysis. The experimental section is organized as follows:
%\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=0.5cm]
%\item 

\noindent In {\bf 
Section~\ref{exp:setup}}, we introduce the datasets and methods to evaluate the previously defined accuracy measures.

%\item
\noindent In {\bf 
Section~\ref{exp:qual}}, we illustrate the limitations of existing measures with some selected qualitative examples.

%\item 
\noindent In {\bf 
Section~\ref{exp:quant}}, we continue by measuring quantitatively the benefits of our proposed measures in terms of {\it robustness} to lag, noise, and normal/abnormal ratio.

%\item 
\noindent In {\bf 
Section~\ref{exp:separability}}, we evaluate the {\it separability} degree of accurate and inaccurate methods, using the existing and our proposed approaches.

%\item
\noindent In {\bf 
Section~\ref{sec:entropy}}, we conduct a {\it consistency} evaluation, in which we analyze the variation of ranks that an AD method can have with an accuracy measures used.

%\item 
\noindent In {\bf 
Section~\ref{sec:exectime}}, we conduct an {\it execution time} evaluation, in which we analyze the impact of different parameters related to the accuracy measures and the time series characteristics. 
We focus especially on the comparison of the different VUS implementations.
%\end{enumerate}

\begin{table}[tb]
\caption{Summary characteristics (averaged per dataset) of the public datasets of TSB-UAD (S.: Size, Ano.: Anomalies, Ab.: Abnormal, Den.: Density)}
\label{table:charac}
%\vspace{-0.2cm}
\footnotesize
\begin{center}
\scalebox{0.82}{
\begin{tabular}{ |r|r|r|r|r|r|} 
 \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Dataset \end{tabular}} & 
\textbf{\begin{tabular}[c]{@{}c@{}}S. \end{tabular}} & 
\textbf{\begin{tabular}[c]{c@{}} Len.\end{tabular}} & 
\textbf{\begin{tabular}[c]{c@{}} \# \\ Ano. \end{tabular}} &
\textbf{\begin{tabular}[c]{c@{}c@{}} \# \\ Ab. \\ Points\end{tabular}} &
\textbf{\begin{tabular}[c]{c@{}c@{}} Ab. \\ Den. \\ (\%)\end{tabular}} \\ \hline
Dodgers \cite{10.1145/1150402.1150428} & 1 & 50400   & 133.0     & 5612.0  &11.14 \\ \hline
SED \cite{doi:10.1177/1475921710395811}& 1 & 100000   & 75.0     & 3750.0  & 3.7\\ \hline
ECG \cite{goldberger_physiobank_2000}   & 52 & 230351  & 195.6     & 15634.0  &6.8 \\ \hline
IOPS \cite{IOPS}   & 58 & 102119  & 46.5     & 2312.3   &2.1 \\ \hline
KDD21 \cite{kdd} & 250 &77415   & 1      & 196.5   &0.56 \\ \hline
MGAB \cite{markus_thill_2020_3762385}   & 10 & 100000  & 10.0     & 200.0   &0.20 \\ \hline
NAB \cite{ahmad_unsupervised_2017}   & 58 & 6301   & 2.0      & 575.5   &8.8 \\ \hline
NASA-M. \cite{10.1145/3449726.3459411}   & 27 & 2730   & 1.33      & 286.3   &11.97 \\ \hline
NASA-S. \cite{10.1145/3449726.3459411}   & 54 & 8066   & 1.26      & 1032.4   &12.39 \\ \hline
SensorS. \cite{YAO20101059}   & 23 & 27038   & 11.2     & 6110.4   &22.5 \\ \hline
YAHOO \cite{yahoo}  & 367 & 1561   & 5.9      & 10.7   &0.70 \\ \hline 
\end{tabular}}
\end{center}
\end{table}











\subsection{Experimental Setup and Settings}
\label{exp:setup}
%\vspace{-0.1cm}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=1\linewidth]{figures/quality.pdf}
  %\vspace{-0.7cm}
  \caption{Comparison of evaluation measures (proposed measures illustrated in subplots (b,c,d,e); all others summarized in subplots (f)) on two examples ((A)AE and OCSM applied on MBA(805) and (B) LOF and OCSVM applied on MBA(806)), illustrating the limitations of existing measures for scores with noise or containing a lag. }
  \label{fig:quality}
  %\vspace{-0.1cm}
\end{figure*}

We implemented the experimental scripts in Python 3.8 with the following main dependencies: sklearn 0.23.0, tensorflow 2.3.0, pandas 1.2.5, and networkx 2.6.3. In addition, we used implementations from our TSB-UAD benchmark suite.\footnote{\scriptsize \url{https://www.timeseries.org/TSB-UAD}} For reproducibility purposes, we make our datasets and code available.\footnote{\scriptsize \url{https://www.timeseries.org/VUS}}
\newline \textbf{Datasets: } For our evaluation purposes, we use the public datasets identified in our TSB-UAD benchmark. The latter corresponds to $10$ datasets proposed in the past decades in the literature containing $900$ time series with labeled anomalies. Specifically, each point in every time series is labeled as normal or abnormal. Table~\ref{table:charac} summarizes relevant characteristics of the datasets, including their size, length, and statistics about the anomalies. In more detail:

\begin{itemize}
    \item {\bf SED}~\cite{doi:10.1177/1475921710395811}, from the NASA Rotary Dynamics Laboratory, records disk revolutions measured over several runs (3K rpm speed).
	\item {\bf ECG}~\cite{goldberger_physiobank_2000} is a standard electrocardiogram dataset and the anomalies represent ventricular premature contractions. MBA(14046) is split to $47$ series.
	\item {\bf IOPS}~\cite{IOPS} is a dataset with performance indicators that reflect the scale, quality of web services, and health status of a machine.
	\item {\bf KDD21}~\cite{kdd} is a composite dataset released in a SIGKDD 2021 competition with 250 time series.
	\item {\bf MGAB}~\cite{markus_thill_2020_3762385} is composed of Mackey-Glass time series with non-trivial anomalies. Mackey-Glass data series exhibit chaotic behavior that is difficult for the human eye to distinguish.
	\item {\bf NAB}~\cite{ahmad_unsupervised_2017} is composed of labeled real-world and artificial time series including AWS server metrics, online advertisement clicking rates, real time traffic data, and a collection of Twitter mentions of large publicly-traded companies.
	\item {\bf NASA-SMAP} and {\bf NASA-MSL}~\cite{10.1145/3449726.3459411} are two real spacecraft telemetry data with anomalies from Soil Moisture Active Passive (SMAP) satellite and Curiosity Rover on Mars (MSL).
	\item {\bf SensorScope}~\cite{YAO20101059} is a collection of environmental data, such as temperature, humidity, and solar radiation, collected from a sensor measurement system.
	\item {\bf Yahoo}~\cite{yahoo} is a dataset consisting of real and synthetic time series based on the real production traffic to some of the Yahoo production systems.
\end{itemize}


\textbf{Anomaly Detection Methods: }  For the experimental evaluation, we consider the following baselines. 

\begin{itemize}
\item {\bf Isolation Forest (IForest)}~\cite{liu_isolation_2008} constructs binary trees based on random space splitting. The nodes (subsequences in our specific case) with shorter path lengths to the root (averaged over every random tree) are more likely to be anomalies. 
\item {\bf The Local Outlier Factor (LOF)}~\cite{breunig_lof_2000} computes the ratio of the neighbor density to the local density. 
\item {\bf Matrix Profile (MP)}~\cite{yeh_time_2018} detects as anomaly the subsequence with the most significant 1-NN distance. 
\item {\bf NormA}~\cite{boniol_unsupervised_2021} identifies the normal patterns based on clustering and calculates each point's distance to normal patterns weighted using statistical criteria. 
\item {\bf Principal Component Analysis (PCA)}~\cite{aggarwal_outlier_2017} projects data to a lower-dimensional hyperplane. Outliers are points with a large distance from this plane. 
\item {\bf Autoencoder (AE)} \cite{10.1145/2689746.2689747} projects data to a lower-dimensional space and reconstructs it. Outliers are expected to have larger reconstruction errors. 
\item {\bf LSTM-AD}~\cite{malhotra_long_2015} use an LSTM network that predicts future values from the current subsequence. The prediction error is used to identify anomalies.
\item {\bf Polynomial Approximation (POLY)} \cite{li_unifying_2007} fits a polynomial model that tries to predict the values of the data series from the previous subsequences. Outliers are detected with the prediction error. 
\item {\bf CNN} \cite{8581424} built, using a convolutional deep neural network, a correlation between current and previous subsequences, and outliers are detected by the deviation between the prediction and the actual value. 
\item {\bf One-class Support Vector Machines (OCSVM)} \cite{scholkopf_support_1999} is a support vector method that fits a training dataset and finds the normal data's boundary.
\end{itemize}

\subsection{Qualitative Analysis}
\label{exp:qual}



We first use two examples to demonstrate qualitatively the limitations of existing accuracy evaluation measures in the presence of lag and noise, and to motivate the need for a new approach. 
These two examples are depicted in Figure~\ref{fig:quality}. 
The first example, in Figure~\ref{fig:quality}(A), corresponds to OCSVM and AE on the MBA(805) dataset (named MBA\_ECG805\_data.out in the ECG dataset). 

We observe in Figure~\ref{fig:quality}(A)(a.1) and (a.2) that both scores identify most of the anomalies (highlighted in red). However, the OCSVM score points to more false positives (at the end of the time series) and only captures small sections of the anomalies. On the contrary, the AE score points to fewer false positives and captures all abnormal subsequences. Thus we can conclude that, visually, AE should obtain a better accuracy score than OCSVM. Nevertheless, we also observe that the AE score is lagged with the labels and contains more noise. The latter has a significant impact on the accuracy of evaluation measures. First, Figure~\ref{fig:quality}(A)(c) is showing that AUC-PR is better for OCSM (0.73) than for AE (0.57). This is contradictory with what is visually observed from Figure~\ref{fig:quality}(A)(a.1) and (a.2). However, when using our proposed measure R-AUC-PR, OCSVM obtains a lower score (0.83) than AE (0.89). This confirms that, in this example, a buffer region before the labels helps to capture the true value of an anomaly score. Overall, Figure~\ref{fig:quality}(A)(f) is showing in green and red the evolution of accuracy score for the 13 accuracy measures for AE and OCSVM, respectively. The latter shows that, in addition to Precision@k and Precision, our proposed approach captures the quality order between the two methods well.

We now present a second example, on a different time series, illustrated in Figure~\ref{fig:quality}(B). 
In this case, we demonstrate the anomaly score of OCSVM and LOF (depicted in Figure~\ref{fig:quality}(B)(a.1) and (a.2)) applied on the MBA(806) dataset (named MBA\_ECG806\_data.out in the ECG dataset). 
We observe that both methods produce the same level of noise. However, LOF points to fewer false positives and captures more sections of the abnormal subsequences than OCSVM. 
Nevertheless, the LOF score is slightly lagged with the labels such that the maximum values in the LOF score are slightly outside of the labeled sections. 
Thus, as illustrated in Figure~\ref{fig:quality}(B)(f), even though we can visually consider that LOF is performing better than OCSM, all usual measures (Precision, Recall, F, precision@k, and AUC-PR) are judging OCSM better than AE. On the contrary, measures that consider lag (Rprecision, Rrecall, RF) rank the methods correctly. 
However, due to threshold issues, these measures are very close for the two methods. Overall, only AUC-ROC and our proposed measures give a higher score for LOF than for OCSVM.

\subsection{Quantitative Analysis}
\label{exp:case}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/eval_case_study.pdf}
  %\vspace*{-0.7cm}
  \caption{\commentRed{
  Comparison of evaluation measures for synthetic data examples across various scenarios. S8 represents the oracle case, where predictions perfectly align with labeled anomalies. Problematic cases are highlighted in the red region.}}
  %\vspace*{-0.5cm}
  \label{fig:eval_case_study}
\end{figure}
\commentRed{
We present the evaluation results for different synthetic data scenarios, as shown in Figure~\ref{fig:eval_case_study}. These scenarios range from S1, where predictions occur before the ground truth anomaly, to S12, where predictions fall within the ground truth region. The red-shaded regions highlight problematic cases caused by a lack of adaptability to lags. For instance, in scenarios S1 and S2, a slight shift in the prediction leads to measures (e.g., AUC-PR, F score) that fail to account for lags, resulting in a zero score for S1 and a significant discrepancy between the results of S1 and S2. Thus, we observe that our proposed VUS effectively addresses these issues and provides robust evaluations results.}

%\subsection{Quantitative Analysis}
%\subsection{Sensitivity and Separability Analysis}
\subsection{Robustness Analysis}
\label{exp:quant}


\begin{figure}[tb]
  \centering
  \includegraphics[width=1\linewidth]{figures/lag_sensitivity_analysis.pdf}
  %\vspace*{-0.7cm}
  \caption{For each method, we compute the accuracy measures 10 times with random lag $\ell \in [-0.25*\ell,0.25*\ell]$ injected in the anomaly score. We center the accuracy average to 0.}
  %\vspace*{-0.5cm}
  \label{fig:lagsensitivity}
\end{figure}

We have illustrated with specific examples several of the limitations of current measures. 
We now evaluate quantitatively the robustness of the proposed measures when compared to the currently used measures. 
We first evaluate the robustness to noise, lag, and normal versus abnormal points ratio. We then measure their ability to separate accurate and inaccurate methods.
%\newline \textbf{Sensitivity Analysis: } 
We first analyze the robustness of different approaches quantitatively to different factors: (i) lag, (ii) noise, and (iii) normal/abnormal ratio. As already mentioned, these factors are realistic. For instance, lag can be either introduced by the anomaly detection methods (such as methods that produce a score per subsequences are only high at the beginning of abnormal subsequences) or by human labeling approximation. Furthermore, even though lag and noises are injected, an optimal evaluation metric should not vary significantly. Therefore, we aim to measure the variance of the evaluation measures when we vary the lag, noise, and normal/abnormal ratio. We proceed as follows:

\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=0.5cm]
\item For each anomaly detection method, we first compute the anomaly score on a given time series.
\item We then inject either lag $l$, noise $n$ or change the normal/abnormal ratio $r$. For 10 different values of $l \in [-0.25*\ell,0.25*\ell]$, $n \in [-0.05*(max(S_T)-min(S_T)),0.05*(max(S_T)-min(S_T))]$ and $r \in [0.01,0.2]$, we compute the 13 different measures.
\item For each evaluation measure, we compute the standard deviation of the ten different values. Figure~\ref{fig:lagsensitivity}(b) depicts the different lag values for six AD methods applied on a data series in the ECG dataset.
\item We compute the average standard deviation for the 13 different AD quality measures. For example, figure~\ref{fig:lagsensitivity}(a) depicts the average standard deviation for ten different lag values over the AD methods applied on the MBA(805) time series.
\item We compute the average standard deviation for the every time series in each dataset (as illustrated in Figure~\ref{fig:sensitivity_per_data}(b to j) for nine datasets of the benchmark.
\item We compute the average standard deviation for the every dataset (as illustrated in Figure~\ref{fig:sensitivity_per_data}(a.1) for lag, Figure~\ref{fig:sensitivity_per_data}(a.2) for noise and Figure~\ref{fig:sensitivity_per_data}(a.3) for normal/abnormal ratio).
\item We finally compute the Wilcoxon test~\cite{10.2307/3001968} and display the critical diagram over the average standard deviation for every time series (as illustrated in Figure~\ref{fig:sensitivity}(a.1) for lag, Figure~\ref{fig:sensitivity}(a.2) for noise and Figure~\ref{fig:sensitivity}(a.3) for normal/abnormal ratio).
\end{enumerate}

%height=8.5cm,

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/sensitivity_per_data_long.pdf}
%  %\vspace*{-0.3cm}
  \caption{Robustness Analysis for nine datasets: we report, over the entire benchmark, the average standard deviation of the accuracy values of the measures, under varying (a.1) lag, (a.2) noise, and (a.3) normal/abnormal ratio. }
  \label{fig:sensitivity_per_data}
\end{figure}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/sensitivity_analysis.pdf}
  %\vspace*{-0.7cm}
  \caption{Critical difference diagram computed using the signed-rank Wilkoxon test (with $\alpha=0.1$) for the robustness to (a.1) lag, (a.2) noise and (a.3) normal/abnormal ratio.}
  \label{fig:sensitivity}
\end{figure*}

The methods with the smallest standard deviation can be considered more robust to lag, noise, or normal/abnormal ratio from the above framework. 
First, as stated in the introduction, we observe that non-threshold-based measures (such as AUC-ROC and AUC-PR) are indeed robust to noise (see Figure~\ref{fig:sensitivity_per_data}(a.2)), but not to lag. Figure~\ref{fig:sensitivity}(a.1) demonstrates that our proposed measures VUS-ROC, VUS-PR, R-AUC-ROC, and R-AUC-PR are significantly more robust to lag. Similarly, Figure~\ref{fig:sensitivity}(a.2) confirms that our proposed measures are significantly more robust to noise. However, we observe that, among our proposed measures, only VUS-ROC and R-AUC-ROC are robust to the normal/abnormal ratio and not VUS-PR and R-AUC-PR. This is explained by the fact that Precision-based measures vary significantly when this ratio changes. This is confirmed by Figure~\ref{fig:sensitivity_per_data}(a.3), in which we observe that Precision and Rprecision have a high standard deviation. Overall, we observe that VUS-ROC is significantly more robust to lag, noise, and normal/abnormal ratio than other measures.




\subsection{Separability Analysis}
\label{exp:separability}

%\newline \textbf{Separability Analysis: } 
We now evaluate the separability capacities of the different evaluation metrics. 
\commentRed{The main objective is to measure the ability of accuracy measures to separate accurate methods from inaccurate ones. More precisely, an appropriate measure should return accuracy scores that are significantly higher for accurate anomaly scores than for inaccurate ones.}
We thus manually select accurate and inaccurate anomaly detection methods and verify if the accuracy evaluation scores are indeed higher for the accurate than for the inaccurate methods. Figure~\ref{fig:separability} depicts the latter separability analysis applied to the MBA(805) and the SED series. 
The accurate and inaccurate anomaly scores are plotted in green and red, respectively. 
We then consider 12 different pairs of accurate/inaccurate methods among the eight previously mentioned anomaly scores. 
We slightly modify each score 50 different times in which we inject lag and noises and compute the accuracy measures. 
Figure~\ref{fig:separability}(a.4) and Figure~\ref{fig:separability}(b.4) are divided into four different subplots corresponding to 4 pairs (selected among the twelve different pairs due to lack of space). 
Each subplot corresponds to two box plots per accuracy measure. 
The green and red box plots correspond to the 50 accuracy measures on the accurate and inaccurate methods. 
If the red and green box plots are well separated, we can conclude that the corresponding accuracy measures are separating the accurate and inaccurate methods well. 
We observe that some accuracy measures (such as VUS-ROC) are more separable than others (such as RF). We thus measure the separability of the two box-plots by computing the Z-test. 

\begin{figure*}[tb]
  \centering
  \includegraphics[width=1\linewidth]{figures/pairwise_comp_example_long.pdf}
  %\vspace*{-0.5cm}
  \caption{Separability analysis applied on 4 pairs of accurate (green) and inaccurate (red) methods on (a) the MBA(805) data series, and (b) the SED data series.}
  %\vspace*{-0.3cm}
  \label{fig:separability}
\end{figure*}

We now aggregate all the results and compute the average Z-test for all pairs of accurate/inaccurate datasets (examples are shown in Figures~\ref{fig:separability}(a.2) and (b.2) for accurate anomaly scores, and in Figures~\ref{fig:separability}(a.3) and (b.3) for inaccurate anomaly scores, for the MBA(805) and SED series, respectively). 
Next, we perform the same operation over three different data series: MBA (805), MBA(820), and SED. 
Then, we depict the average Z-test for these three datasets in Figure~\ref{fig:separability_agg}(a). 
Finally, we show the average Z-test for all datasets in Figure~\ref{fig:separability_agg}(b). 


We observe that our proposed VUS-based and Range-based measures are significantly more separable than other current accuracy measures (up to two times for AUC-ROC, the best measures of all current ones). Furthermore, when analyzed in detail in Figure~\ref{fig:separability} and Figure~\ref{fig:separability_agg}, we confirm that VUS-based and Range-based are more separable over all three datasets. 

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/agregated_sep_analysis.pdf}
  %\vspace*{-0.5cm}
  \caption{Overall separability analysis (averaged z-test between the accuracy values distributions of accurate and inaccurate methods) applied on 36 pairs on 3 datasets.}
  \label{fig:separability_agg}
\end{figure}


\noindent \textbf{Global Analysis: } Overall, we observe that VUS-ROC is the most robust (cf. Figure~\ref{fig:sensitivity}) and separable (cf. Figure~\ref{fig:separability_agg}) measure. 
On the contrary, Precision and Rprecision are non-robust and non-separable. 
Among all previous accuracy measures, only AUC-ROC is robust and separable. 
Popular measures, such as, F, RF, AUC-ROC, and AUC-PR are robust but non-separable.

In order to visualize the global statistical analysis, we merge the robustness and the separability analysis into a single plot. Figure~\ref{fig:global} depicts one scatter point per accuracy measure. 
The x-axis represents the averaged standard deviation of lag and noise (averaged values from Figure~\ref{fig:sensitivity_per_data}(a.1) and (a.2)). The y-axis corresponds to the averaged Z-test (averaged value from Figure~\ref{fig:separability_agg}). 
Finally, the size of the points corresponds to the sensitivity to the normal/abnormal ratio (values from Figure~\ref{fig:sensitivity_per_data}(a.3)). 
Figure~\ref{fig:global} demonstrates that our proposed measures (located at the top left section of the plot) are both the most robust and the most separable. 
Among all previous accuracy measures, only AUC-ROC is on the top left section of the plot. 
Popular measures, such as, F, RF, AUC-ROC, AUC-PR are on the bottom left section of the plot. 
The latter underlines the fact that these measures are robust but non-separable.
Overall, Figure~\ref{fig:global} confirms the effectiveness and superiority of our proposed measures, especially of VUS-ROC and VUS-PR.


\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/final_result.pdf}
  \caption{Evaluation of all measures based on: (y-axis) their separability (avg. z-test), (x-axis) avg. standard deviation of the accuracy values when varying lag and noise, (circle size) avg. standard deviation of the accuracy values when varying the normal/abnormal ratio.}
  \label{fig:global}
\end{figure}




\subsection{Consistency Analysis}
\label{sec:entropy}

In this section, we analyze the accuracy of the anomaly detection methods provided by the 13 accuracy measures. The objective is to observe the changes in the global ranking of anomaly detection methods. For that purpose, we formulate the following assumptions. First, we assume that the data series in each benchmark dataset are similar (i.e., from the same domain and sharing some common characteristics). As a matter of fact, we can assume that an anomaly detection method should perform similarly on these data series of a given dataset. This is confirmed when observing that the best anomaly detection methods are not the same based on which dataset was analyzed. Thus the ranking of the anomaly detection methods should be different for different datasets, but similar for every data series in each dataset. 
Therefore, for a given method $A$ and a given dataset $D$ containing data series of the same type and domain, we assume that a good accuracy measure results in a consistent rank for the method $A$ across the dataset $D$. 
The consistency of a method's ranks over a dataset can be measured by computing the entropy of these ranks. 
For instance, a measure that returns a random score (and thus, a random rank for a method $A$) will result in a high entropy. 
On the contrary, a measure that always returns (approximately) the same ranks for a given method $A$ will result in a low entropy. 
Thus, for a given method $A$ and a given dataset $D$ containing data series of the same type and domain, we assume that a good accuracy measure results in a low entropy for the different ranks for method $A$ on dataset $D$.

\begin{figure*}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/entropy_long.pdf}
  %\vspace*{-0.5cm}
  \caption{Accuracy evaluation of the anomaly detection methods. (a) Overall average entropy per category of measures. Analysis of the (b) averaged rank and (c) averaged rank entropy for each method and each accuracy measure over the entire benchmark. Example of (b.1) average rank and (c.1) entropy on the YAHOO dataset, KDD21 dataset (b.2, c.2). }
  \label{fig:entropy}
\end{figure*}

We now compute the accuracy measures for the nine different methods (we compute the anomaly scores ten different times, and we use the average accuracy). 
Figures~\ref{fig:entropy}(b.1) and (b.2) report the average ranking of the anomaly detection methods obtained on the YAHOO and KDD21 datasets, respectively. 
The x-axis corresponds to the different accuracy measures. We first observe that the rankings are more separated using Range-AUC and VUS measures for these two datasets. Figure~\ref{fig:entropy}(b) depicts the average ranking over the entire benchmark. The latter confirms the previous observation that VUS measures provide more separated rankings than threshold-based and AUC-based measures. We also observe an interesting ranking evolution for the YAHOO dataset illustrated in Figure~\ref{fig:entropy}(b.1). We notice that both LOF and MatrixProfile (brown and pink curve) have a low rank (between 4 and 5) using threshold and AUC-based measures. However, we observe that their ranks increase significantly for range-based and VUS-based measures (between 2.5 and 3). As we noticed by looking at specific examples (see Figure~\ref{exp:qual}), LOF and MatrixProfile can suffer from a lag issue even though the anomalies are well-identified. Therefore, the range-based and VUS-based measures better evaluate these two methods' detection capability.


Overall, the ranking curves show that the ranks appear more chaotic for threshold-based than AUC-, Range-AUC-, and VUS-based measures. 
In order to quantify this observation, we compute the Shannon Entropy of the ranks of each anomaly detection method. 
In practice, we extract the ranks of methods across one dataset and compute Shannon's Entropy of the different ranks. 
Figures~\ref{fig:entropy}(c.1) and (c.2) depict the entropy of each of the nine methods for the YAHOO and KDD21 datasets, respectively. 
Figure~\ref{fig:entropy}(c) illustrates the averaged entropy for all datasets in the benchmark for each measure and method, while Figure~\ref{fig:entropy}(a) shows the averaged entropy for each category of measures.
We observe that both for the general case (Figure~\ref{fig:entropy}(a) and Figure~\ref{fig:entropy}(c)) and some specific cases (Figures~\ref{fig:entropy}(c.1) and (c.2)), the entropy is reducing when using AUC-, Range-AUC-, and VUS-based measures. 
We report the lowest entropy for VUS-based measures. 
Moreover, we notice a significant drop between threshold-based and AUC-based. 
This confirms that the ranks provided by AUC- and VUS-based measures are consistent for data series belonging to one specific dataset. 


Therefore, based on the assumption formulated at the beginning of the section, we can thus conclude that AUC, range-AUC, and VUS-based measures are providing more consistent rankings. Finally, as illustrated in Figure~\ref{fig:entropy}, we also observe that VUS-based measures result in the most ordered and similar rankings for data series from the same type and domain.










\subsection{Execution Time Analysis}
\label{sec:exectime}

In this section, we evaluate the execution time required to compute different evaluation measures. 
In Section~\ref{sec:synthetic_eval_time}, we first measure the influence of different time series characteristics and VUS parameters on the execution time. In Section~\ref{sec:TSB_eval_time}, we  measure the execution time of VUS (VUS-ROC and VUS-PR simultaneously), R-AUC (R-AUC-ROC and R-AUC-PR simultaneously), and AUC-based measures (AUC-ROC and AUC-PR simultaneously) on the TSB-UAD benchmark. \commentRed{As demonstrated in the previous section, threshold-based measures are not robust, have a low separability power, and are inconsistent. 
Such measures are not suitable for evaluating anomaly detection methods. Thus, in this section, we do not consider threshold-based measures.}


\subsubsection{Evaluation on Synthetic Time Series}\hfill\\
\label{sec:synthetic_eval_time}

We first analyze the impact that time series characteristics and parameters have on the computation time of VUS-based measures. 
to that effect, we generate synthetic time series and labels, where we vary the following parameters: (i) the number of anomalies {\bf$\alpha$} in the time series, (ii) the average \textbf{$\mu(\ell_a)$} and standard deviation $\sigma(\ell_a)$ of the anomalies lengths in the time series (all the anomalies can have different lengths), (iii) the length of the time series \textbf{$|T|$}, (iv) the maximum buffer length \textbf{$L$}, and (v) the number of thresholds \textbf{$N$}.


We also measure the influence on the execution time of the R-AUC- and AUC- related parameter, that is, the number of thresholds ($N$).
The default values and the range of variation of these parameters are listed in Table~\ref{tab:parameter_range_time}. 
For VUS-based measures, we evaluate the execution time of the initial VUS implementation, as well as the two optimized versions, VUS$_{opt}$ and VUS$_{opt}^{mem}$.

\begin{table}[tb]
    \centering
    \caption{Value ranges for the parameters: number of anomalies ($\alpha$), average and standard deviation anomaly length ($\mu(\ell_a)$,$\sigma(\ell_a)$), time series length ($|T|$), maximum buffer length ($L$), and number of thresholds ($N$).}
    \begin{tabular}{|c|c|c|c|c|c|c|} 
 \hline
 Param. & $\alpha$ & $\mu(\ell_a)$ & $\sigma(\ell_{a})$ & $|T|$ & $L$ & $N$ \\ [0.5ex] 
 \hline\hline
 \textbf{Default} & 10 & 10 & 0 & $10^5$ & 5 & 250\\ 
 \hline
 Min. & 0 & 0 & 0 & $10^3$ & 0 & 2 \\
 \hline
 Max. & $2*10^3$ & $10^3$ & $10$ & $10^5$ & $10^3$ & $10^3$ \\ [1ex] 
 \hline
\end{tabular}
    \label{tab:parameter_range_time}
\end{table}


Figure~\ref{fig:sythetic_exp_time} depicts the execution time (averaged over ten runs) for each parameter listed in Table~\ref{tab:parameter_range_time}. 
Overall, we observe that the execution time of AUC-based and R-AUC-based measures is significantly smaller than VUS-based measures.
In the following paragraph, we analyze the influence of each parameter and compare the experimental execution time evaluation to the theoretical complexity reported in Table~\ref{tab:complexity_summary}.

\vspace{0.2cm}
\noindent {\bf [Influence of $\alpha$]}:
In Figure~\ref{fig:sythetic_exp_time}(a), we observe that the VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$ execution times are linearly increasing with $\alpha$. 
The increase in execution time for VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$ is more pronounced when we vary $\alpha$, in contrast to $l_a$ (which nevertheless, has a similar effect on the overall complexity). 
We also observe that the VUS$_{opt}^{mem}$ execution time grows slower than $VUS_{opt}$ when $\alpha$ increases. 
This is explained by the use of 2-dimensional arrays for the storage of predictions, which use contiguous memory locations that allow for faster access, decreasing the dependency on $\alpha$.

\vspace{0.2cm}
\noindent {\bf [Influence of $\mu(\ell_a)$]}:
As shown in Figure~\ref{fig:sythetic_exp_time}(b), the execution time variation of VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$ caused by $\ell_a$ is rather insignificant. 
We also observe that the VUS$_{opt}$ and VUS$_{opt}^{mem}$ execution times are significantly lower when compared to VUS. 
This is explained by the smaller dependency of the complexity of these algorithms on the time series length $|T|$. 
Overall, the execution time for both VUS$_{opt}$ and VUS$_{opt}^{mem}$ is significantly lower than VUS, and follows a similar trend. 

\vspace{0.2cm}
\noindent {\bf [Influence of $\sigma(\ell_a)$]}: 
As depicted in Figure~\ref{fig:sythetic_exp_time}(d) and inferred from the theoretical complexities in Table~\ref{tab:complexity_summary}, none of the measures are affected by the standard deviation of the anomaly lengths.

\vspace{0.2cm}
\noindent {\bf [Influence of $|T|$]}:
For short time series (small values of $|T|$), we note that O($T_1$) becomes comparable to O($T_2$). 
Thus, the theoretical complexities approximate to $O(NL(T_1+T_2))$, $O(N*(T_1+T_2))+O(NLT_2)$ and $O(N(T_1+T_2))$ for VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$, respectively. 
Indeed, we observe in Figure~\ref{fig:sythetic_exp_time}(c) that the execution times of VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$ are similar for small values of $|T|$. However, for larger values of $|T|$, $O(T_1)$ is much higher compared to $O(T_2)$, thus resulting in an effective complexity of $O(NLT_1)$ for VUS, and $O(NT_1)$ for VUS$_{opt}$, and VUS$_{opt}^{mem}$. 
This translates to a significant improvement in execution time complexity for VUS$_{opt}$ and VUS$_{opt}^{mem}$ compared to VUS, which is confirmed by the results in Figure~\ref{fig:sythetic_exp_time}(c).

\vspace{0.2cm}
\noindent {\bf [Influence of $N$]}: 
Given the theoretical complexity depicted in Table~\ref{tab:complexity_summary}, it is evident that the number of thresholds affects all measures in a linear fashion.
Figure~\ref{fig:sythetic_exp_time}(e) demonstrates this point: the results of varying $N$ show a linear dependency for VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$ (i.e., a logarithmic trend with a log scale on the y axis). \commentRed{Moreover, we observe that the AUC and range-AUC execution time is almost constant regardless of the number of thresholds used. The latter is explained by the very efficient implementation of AUC measures. Therefore, the linear dependency on the number of thresholds is not visible in Figure~\ref{fig:sythetic_exp_time}(e).}

\vspace{0.2cm}
\noindent {\bf [Influence of $L$]}: Figure~\ref{fig:sythetic_exp_time}(f) depicts the influence of the maximum buffer length $L$ on the execution time of all measures. 
We observe that, as $L$ grows, the execution time of VUS$_{opt}$ and VUS$_{opt}^{mem}$ increases slower than VUS. 
We also observe that VUS$_{opt}^{mem}$ is more scalable with $L$ when compared to VUS$_{opt}$. 
This is consistent with the theoretical complexity (cf. Table~\ref{tab:complexity_summary}), which indicates that the dependence on $L$ decreases from $O(NL(T_1+T_2+\ell_a \alpha))$ for VUS to $O(NL(T_2+\ell_a \alpha)$ and $O(NL(\ell_a \alpha))$ for $VUS_{opt}$, and $VUS_{opt}^{mem}$.





\begin{figure*}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/synthetic_res.pdf}
  %\vspace*{-0.5cm}
  \caption{Execution time of VUS, R-AUC, AUC-based measures when we vary the parameters listed in Table~\ref{tab:parameter_range_time}. The solid lines correspond to the average execution time over 10 runs. The colored envelopes are to the standard deviation.}
  \label{fig:sythetic_exp_time}
\end{figure*}


\vspace{0.2cm}
In order to obtain a more accurate picture of the influence of each of the above parameters, we fit the execution time (as affected by the parameter values) using linear regression; we can then use the regression slope coefficient of each parameter to evaluate the influence of that parameter. 
In practice, we fit each parameter individually, and report the regression slope coefficient, as well as the coefficient of determination $R^2$.
Table~\ref{tab:parameter_linear_coeff} reports the coefficients mentioned above for each parameter associated with VUS, VUS$_{opt}$, and VUS$_{opt}^{mem}$.



\begin{table}[tb]
    \centering
    \caption{Linear regression slope coefficients ($C.$) for VUS execution times, for each parameter independently. }
    \begin{tabular}{|c|c|c|c|c|c|c|} 
 \hline
 Measure & Param. & $\alpha$ & $l_a$ & $|T|$ & $L$ & $N$\\ [0.5ex] 
 \hline\hline
 \multirow{2}{*}{$VUS$} & $C.$ & 21.9 & 0.02 & 2.13 & 212 & 6.24\\\cline{2-7}
 & {$R^2$} & 0.99 & 0.15 & 0.99 & 0.99 & 0.99 \\   
 \hline
  \multirow{2}{*}{$VUS_{opt}$} & $C.$ & 24.2  & 0.06 & 0.19 & 27.8 & 1.23\\\cline{2-7}
  & $R^2$& 0.99 & 0.86 & 0.99 & 0.99 & 0.99\\ 
 \hline
 \multirow{2}{*}{$VUS_{opt}^{mem}$} & $C.$ & 21.5 & 0.05 & 0.21 & 15.7 & 1.16\\\cline{2-7}
  & $R^2$ & 0.99 & 0.89 & 0.99 & 0.99 & 0.99\\[1ex] 
 \hline
\end{tabular}
    \label{tab:parameter_linear_coeff}
\end{table}

Table~\ref{tab:parameter_linear_coeff} shows that the linear regression between $\alpha$ and the execution time has a $R^2=0.99$. Thus, the dependence of execution time on $\alpha$ is linear. We also observe that VUS$_{opt}$ execution time is more dependent on $\alpha$ than VUS and VUS$_{opt}^{mem}$ execution time.
Moreover, the dependence of the execution time on the time series length ($|T|$) is higher for VUS than for VUS$_{opt}$ and VUS$_{opt}^{mem}$. 
More importantly, VUS$_{opt}$ and VUS$_{opt}^{mem}$ are significantly less dependent than VUS on the number of thresholds and the maximal buffer length. 







\subsubsection{Evaluation on TSB-UAD Time Series}\hfill\\
\label{sec:TSB_eval_time}

In this section, we verify the conclusions outlined in the previous section with real-world time series from the TSB-UAD benchmark. 
In this setting, the parameters $\alpha$, $\ell_a$, and $|T|$ are calculated from the series in the benchmark and cannot be changed. Moreover, $L$ and $N$ are parameters for the computation of VUS, regardless of the time series (synthetic or real). Thus, we do not consider these two parameters in this section.

\begin{figure*}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/TSB2.pdf}
  \caption{Execution time of VUS, R-AUC, AUC-based measures on the TSB-UAD benchmark, versus $\alpha$, $\ell_a$, and $|T|$.}
  \label{fig:TSB}
\end{figure*}

Figure~\ref{fig:TSB} depicts the execution time of AUC, R-AUC, and VUS-based measures versus $\alpha$, $\mu(\ell_a)$, and $|T|$.
We first confirm with Figure~\ref{fig:TSB}(a) the linear relationship between $\alpha$ and the execution time for VUS, VUS$_{opt}$ and VUS$_{opt}^{mem}$.
On further inspection, it is possible to see two separate lines for almost all the measures. 
These lines can be attributed to the time series length $|T|$. 
The convergence of VUS and $VUS_{opt}$ when $\alpha$ grows shows the stronger dependence that $VUS_{opt}$ execution time has on $\alpha$, as already observed with the synthetic data (cf. Section~\ref{sec:synthetic_eval_time}). 

In Figure~\ref{fig:TSB}(b), we observe that the variation of the execution time with $\ell_a$ is limited when compared to the two other parameters. We conclude that the variation of $\ell_a$ is not a key factor in determining the execution time of the measures.
Furthermore, as depicted in Figure~\ref{fig:TSB}(c), $VUS_{opt}$ and $VUS_{opt}^{mem}$ are more scalable than VUS when $|T|$ increases. 
We also confirm the linear dependence of execution time on the time series length for all the accuracy measures, which is consistent with the experiments on the synthetic data. 
The two abrupt jumps visible in Figure~\ref{fig:TSB}(c) are explained by significant increases of $\alpha$ in time series of the same length. 

\begin{table}[tb]
\centering
\caption{Linear regression slope coefficients ($C.$) for VUS execution time, for all time series parameters all-together.}
\begin{tabular}{|c|ccc|c|} 
 \hline
Measure & $\alpha$ & $|T|$ & $l_a$ & $R^2$ \\ [0.5ex] 
 \hline\hline
 \multirow{1}{*}{${VUS}$} & 7.87 & 13.5 & -0.08 & 0.99  \\ 
 %\cline{2-5} & $R^2$ & \multicolumn{3}{c|}{ 0.99}\\
 \hline
 \multirow{1}{*}{$VUS_{opt}$} & 10.2 & 1.70 & 0.09 & 0.96 \\
 %\cline{2-5} & $R^2$ & \multicolumn{3}{c|}{0.96}\\
\hline
 \multirow{1}{*}{$VUS_{opt}^{mem}$} & 9.27 & 1.60 & 0.11 & 0.96 \\
 %\cline{2-5} & $R^2$ & \multicolumn{3}{c|}{0.96} \\
 \hline
\end{tabular}
\label{tab:parameter_linear_coeff_TSB}
\end{table}



We now perform a linear regression between the execution time of VUS, VUS$_{opt}$ and VUS$_{opt}^{mem}$, and $\alpha$, $\ell_a$ and $|T|$.
We report in Table~\ref{tab:parameter_linear_coeff_TSB} the slope coefficient for each parameter, as well as the $R^2$.  
The latter shows that the VUS$_{opt}$ and VUS$_{opt}^{mem}$ execution times are impacted by $\alpha$ at a larger degree than $\alpha$ affects VUS. 
On the other hand, the VUS$_{opt}$ and VUS$_{opt}^{mem}$ execution times are impacted to a significantly smaller degree by the time series length when compared to VUS. 
We also confirm that the anomaly length does not impact the execution time of VUS, VUS$_{opt}$, or VUS$_{opt}^{mem}$.
Finally, our experiments show that our optimized implementations VUS$_{opt}$ and VUS$_{opt}^{mem}$ significantly speedup the execution of the VUS measures (i.e., they can be computed within the same order of magnitude as R-AUC), rendering them practical in the real world.











\subsection{Summary of Results}


Figure~\ref{fig:overalltable} depicts the ranking of the accuracy measures for the different tests performed in this paper. The robustness test is divided into three sub-categories (i.e., lag, noise, and Normal vs. abnormal ratio). We also show the overall average ranking of all accuracy measures (most right column of Figure~\ref{fig:overalltable}).
Overall, we see that VUS-ROC is always the best, and VUS-PR and Range-AUC-based measures are, on average, second, third, and fourth. We thus conclude that VUS-ROC is the overall winner of our experimental analysis.

\commentRed{In addition, our experimental evaluation shows that the optimized version of VUS accelerates the computation by a factor of two. Nevertheless, VUS execution time is still significantly slower than AUC-based approaches. However, it is important to mention that the efficiency of accuracy measures is an orthogonal problem with anomaly detection. In real-time applications, we do not have ground truth labels, and we do not use any of those measures to evaluate accuracy. Measuring accuracy is an offline step to help the community assess methods and improve wrong practices. Thus, execution time should not be the main criterion for selecting an evaluation measure.}
