\section{Background and Related Work}
\label{sec:background}

We first introduce formal notations useful for the rest of the paper (Section \ref{sec:notation}). Then, we review in detail previously proposed evaluation measures for time-series AD methods (Section \ref{acc_measure}). 


\subsection{Time-Series and Anomaly Score Notations}
\label{sec:notation}
We review notations for the time series and anomaly score sequence.
\newline \textbf{Time Series: } A time series $T \in \mathbb{R}^n $ is a sequence of real-valued numbers $T_i\in\mathbb{R}$ $[T_1,T_2,...,T_n]$, where $n=|T|$ is the length of $T$, and $T_i$ is the $i^{th}$ point of $T$. We are typically interested in local regions of the time series, known as subsequences. A subsequence $T_{i,\ell} \in \mathbb{R}^\ell$ of a time series $T$ is a continuous subset of the values of $T$ of length $\ell$ starting at position $i$. Formally, $T_{i,\ell} = [T_i, T_{i+1},...,T_{i+\ell-1}]$.	
\newline \textbf{Anomaly Score Sequence: } For a time series $T \in \mathbb{R}^n $, an AD method $A$ returns an anomaly score sequence $S_T$. For point-based approaches (i.e., methods that return a score for each point of $T$), we have $S_T \in \mathbb{R}^n$. For range-based approaches (i.e., methods that return a score for each subsequence of a given length $\ell$), we have $S_T \in \mathbb{R}^{n-\ell}$. Overall, for range-based (or subsequence-based) approaches, we define $S_T = [{S_T}_1,{S_T}_2,...,{S_T}_{n-\ell}]$ with ${S_T}_i \in [0,1]$.


\subsection{Accuracy Evaluation Measures for AD}
\label{acc_measure}

We present previously proposed quality measures for evaluating the accuracy of an AD method, given its anomaly score. We first discuss threshold-based and then threshold-independent measures.

\subsubsection{Threshold-based AD Evaluation Measures} \hfill\\
The anomaly score $S_T$ produced by an AD method $A$ highlights the parts of the time series $T$ considered as abnormal. The highest values in the anomaly score correspond to the most abnormal points. Threshold-based measures require setting a threshold to mark each point as an anomaly or not. Usually, this threshold is set to $\mu(S_T) + \alpha*\sigma(S_T)$, with $\alpha$ set to 3____, where $\mu(S_T)$ is the mean and $\sigma(S_T)$ is the standard deviation $S_T$. Given a threshold $Thres$, we compute the $pred \in \{0,1\}^n$ as follows:


\begin{equation}
\begin{split}
&\forall i \in [1,|S_T|], pred_i = \left.
\begin{cases}
0,& \text{if: } {S_T}_i < Thres \\
1,& \text{if: } {S_T}_i \geq Thres 
\end{cases}
\right.
\end{split}
\end{equation}

Threshold-based measures compare $pred$ to $label \in \{0,1\}^n$, which indicates the true (human provided) labeled anomalies. Given the identity vector $I=[1,1,...,1]$, the points detected as anomalies or not fall into the following four categories:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=0.5cm]
	\item {\bf True Positive (TP)}: Number of points that have been correctly identified as anomalies. Formally: $TP = label^\top \cdot pred$.
	\item {\bf True Negative (TN)}: Number of points that have been correctly identified as normal. Formally: $TN = (I-label)^\top \cdot (I-pred)$.
	\item {\bf False Positive (FP)}: Number of points that have been wrongly identified as anomalies. Formally: $FP = (I-label)^\top \cdot pred$.
	\item {\bf False Negative (FN)}: Number of points that have been wrongly identified as normal. Formally: $FN = label^\top \cdot (I-pred)$.
\end{itemize}
Given these categories, several quality measures have been proposed to assess the accuracy of AD methods.
\newline \textbf{Precision: } We define Precision (or positive predictive value) as the number of correctly identified anomalies over the total number of points detected as anomalies by the method:

\begin{equation}
Precision = \frac{TP}{TP+FP}
\end{equation}
\newline \textbf{Recall: } We define Recall (or True Positive Rate (TPR), $tpr$) as the number of correctly identified anomalies over all anomalies:

\begin{equation}
Recall = \frac{TP}{TP+FN}
\end{equation}

\noindent \textbf{False Positive Rate (FPR): } A supplemental measure to the Recall is the FPR, $fpr$, defined as the number of points wrongly identified as anomalies over the total number of normal points:

\begin{equation}
fpr = \frac{FP}{FP+TN}
\end{equation}
\newline \textbf{F-Score: } Precision and Recall evaluate two different aspects of the AD quality. A measure that combines these two aspects is the harmonic mean $F_{\beta}$, with non-negative real values for $\beta$:
\begin{equation}
F_{\beta} = \frac{(1+\beta^2)*Precision*Recall}{\beta^2*Precision+Recall}
\end{equation}
\noindent Usually, $\beta$ is set to 1, balancing the importance between Precision and Recall. In this paper, $F_1$ is referred to as F or F-score.
\newline \textbf{Precision@k: } All previous measures require an anomaly score threshold to be computed. An alternative approach is to measure the Precision using a subset of anomalies corresponding to the $k$ highest value in the anomaly score $S_T$. This is equivalent to setting the threshold such that only the $k$ highest values are retrieved. 

To address the shortcomings of the point-based measures, a range-based definition was proposed, extending the traditional Precision and Recall ____. This definition considers several factors: (i) whether a subsequence is detected or not (ExistenceReward or ER); (ii) how many points in the subsequence are detected (OverlapReward or OR); (iii) which part of the subsequence is detected (position-dependent weight function); and (iv) how many fragmented regions correspond to one real subsequence outlier (CardinalityFactor or CF). Formally, we define $R=\{R_1,...R_{N_r}\}$ as the set of anomaly ranges, with $R_k=\{pos_i,pos_{i+1}, ..., pos_{i+j}\}$ and $\forall pos \in R_k, label_{pos} = 1$, and $P=\{P_1,...P_{N_p}\}$ as the set of predicted anomaly ranges, with $P_k=\{pos_i,pos_{i+1}, ..., pos_{i+j}\}$ and $\forall pos \in R_k, pred_{pos} = 1$. Then, we define ER, OR, and CF as follows:

%\vspace{-0.3cm}
\begin{equation}
\footnotesize{
\begin{split}
&ER(R_i,P) = \left.
\begin{cases}
1, &\text{if } \sum_{j=1}^{N_p} |R_i \cap P_j| \geq 1\\
0, &\text{otherwise}
\end{cases}
\right. \\
&CF(R_i,P) = \left.
\begin{cases}
1, &\text{if } \exists P_i \in P, |R_i \cap P_i| \geq 1\\
\gamma(R_i,P), &\text{otherwise}
\end{cases}
\right. \\
&OR(R_i,P) = CF(R_i,P)*\sum_{j=1}^{N_p} \omega(R_i,R_i \cap P_j, \delta)
\end{split}
}
%\vspace{-0.1cm}
\end{equation}

\noindent The $\gamma(),\omega()$, and $\delta()$ are tunable functions that capture the cardinality, size, and position of the overlap respectively.
The default parameters are set to $\gamma()=1,\delta()=1$ and $\omega()$ to the overlap ratio covered by the predicted anomaly range____.
\newline \textbf{Rprecision and Rrecall____: } Based on the above, we define:

%\vspace{-0.2cm}
\begin{equation}
\footnotesize{
\begin{split}
Rprecision(R,P) &= \frac{\sum_{i=1}^{N_p} Rprecision_s(R,P_i)}{N_p}\\
Rprecision_s(R,P_i) &= CF(P_i,R)*\sum_{j=1}^{N_r} \omega(P_i,P_i \cap R_j, \delta)
\end{split}
}
%\vspace{-0.2cm}
\end{equation}

%%\vspace{-0.2cm}
\begin{equation}
\footnotesize{
\begin{split}
Rrecall(R,P) &= \frac{\sum_{i=1}^{N_r} Rrecall_s(R_i,P)}{N_r} \\
Rrecall_s(R_i,P) &= \alpha*ER(R_i,P) + (1-\alpha)*OR(R_i,P)
\end{split}
}
%%\vspace{-0.1cm}
\end{equation}
\noindent The parameter $\alpha$ is user defined. The default value is $\alpha=0$.
\newline \textbf{Range F-score (RF)____: } As described previously, the F-score combines Precision and Recall. Similarly, we define $RF_{\beta}$, for $\beta>0$ as follows:

%\vspace{-0.3cm}
\begin{equation}
RF_{\beta} = \frac{(1+\beta^2)*Rprecision*Rrecall}{\beta^2*Rprecision+Rrecall}
%\vspace{-0.1cm}
\end{equation}

\noindent As before, $\beta$ is set to 1. In this paper, $RF_1$ is referred to as RF-score.

\subsubsection{Threshold-independent AD Evaluation Measures} \hfill\\
Until now, we introduced accuracy measures requiring to threshold the produced anomaly score of AD methods. However, the accuracy values vary significantly when the threshold changes. To evaluate a method holistically using its corresponding anomaly score, two measures from the AUC family of measures are used.
\newline \textbf{AUC-ROC____: } The Area Under the Receiver Operating Characteristics curve (AUC-ROC) is defined as the area under the curve corresponding to TPR on the y-axis and FPR on the x-axis when we vary the anomaly score threshold. The area under the curve is computed using the trapezoidal rule. For that purpose, we define $Th$ as an ordered set of thresholds between 0 and 1. Formally, we have $Th=[Th_0,Th_1,...Th_N]$ with $0=Th_0<Th_1<...<Th_N=1$. Therefore, $AUC\text{-}ROC$ is defined as follows:
%\vspace{-0.2cm}
\begin{equation}
\begin{split}
&AUC\text{-}ROC = \frac{1}{2}\sum_{k=1}^{N} \Delta^{k}_{TPR}*\Delta^{k}_{FPR}\\
&\text{with: } \left.
\begin{cases}
\Delta^{k}_{FPR} &= FPR(Th_{k})-FPR(Th_{k-1})\\
\Delta^{k}_{TPR} &= TPR(Th_{k-1})+TPR(Th_{k})
\end{cases}
\right. 
\end{split}
\label{equAUCROC}
\end{equation}
\newline \textbf{AUC-PR____: } The Area Under the Precision-Recall curve (AUC-PR) is defined as the area under the curve corresponding to the Recall on the x-axis and Precision on the y-axis when we vary the anomaly score threshold. 
As before, the area under the curve can be calculated using the trapezoidal rule, defined as follows:

{\footnotesize
\begin{equation}
\begin{split}
&AUC\text{-}PR = \frac{1}{2}\sum_{k=1}^{N} \Delta^{k}_{Precision}*\Delta^{k}_{Recall}\\
&\text{with: } \left.
\begin{cases}
\Delta^{k}_{Recall} &= Recall(Th_{k})-Recall(Th_{k-1})\\
\Delta^{k}_{Precision} &= Precision(Th_{k-1})+Precision(Th_{k})
\end{cases}
\right. 
\end{split}
%\vspace{-0.1cm}
\label{equAUCPR}
\end{equation}
}

\noindent As discussed in____, linear interpolation in PR space may result in an overly optimistic estimate of performance. Therefore, we adopt an alternative interpolation method, Stepwise Interpolation, to approximate the area under the curve by calculating the average precision of the PR curve:

%\vspace{-0.2cm}
\begin{equation}
AUC\text{-}PR = \sum_{k=1}^{N} Precision(Th_{k})*\Delta^{k}_{Recall}
%\vspace{-0.2cm}
\end{equation}

\noindent For consistency, we use the above equation in this paper to compute AUC-PR.