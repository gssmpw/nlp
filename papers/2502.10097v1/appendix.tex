\maketitle


\clearpage
\appendix
% \begin{spacing}{0.95}
\tableofcontents
% \end{spacing}
\clearpage


\section{Broader Impact}
\label{Broader Impact}
To avoid blind exploration and improve sample efficiency, we propose $\texttt{\textbf{CIP}}$ for efficient reinforcement learning. $\texttt{\textbf{CIP}}$ leverages the causal relationships among states, actions, and rewards to prioritize causal information for efficient policy learning. 
$\texttt{\textbf{CIP}}$ first learns a causal matrix between states and rewards to execute counterfactual data augmentation, prioritizing important state features without additional environmental interactions. Subsequently, it learns a causal reweight matrix between actions and rewards to prioritize causally-informed behaviors. We then introduce a causal action empowerment term into the learning objective to enhance the controllability. By prioritizing the causal information, $\texttt{\textbf{CIP}}$ enables agents to focus on behaviors that have causally significant effects on their tasks. 
$\texttt{\textbf{CIP}}$ offers substantial broader impact by prioritizing causal information through individual assessment of how different factors contribute to rewards. Our novel empowerment learning objective achieves efficient policy optimization by leveraging entropy via the policy and learned inverse dynamics model. This approach shows promise for extension into research frameworks centered on maximum entropy algorithms. 

Despite its strengths, $\texttt{\textbf{CIP}}$ has limitations beyond its reliance on the method DirectLiNGAM. There's potential to explore alternative causal discovery techniques for more robust relationship mapping. Moreover, analyzing inter-entity causal connections could lead to better disentanglement of diverse behaviors. 
Our future work will investigate a range of causal discovery methods to refine our approach. We aim to extend $\texttt{\textbf{CIP}}$ to model-based RL frameworks, focusing on building causal world models to enhance generalization.


\section{Assumptions and Propositions}
\label{sec:app_ass}
% \subsection{Definitions and Proofs}
\begin{assumption}
(d-separation~\citep{pearl2009causality}) d-separation is a graphical criterion used to determine, from a given causal graph, if a set of variables X is conditionally independent of another set Y, given a third set of variables Z. 
In a directed acyclic graph (DAG) $\mathcal{G}$, a path between nodes $n_1$ and $n_m$ is said to be blocked by a set $S$ if there exists a node $n_k$, for $k = 2, \cdots, m-1$, that satisfies one of the following two conditions:

(i) $n_k \in S$, and the path between $n_{k-1}$ and $n_{k+1}$ forms ($n_{k-1} \rightarrow n_k \rightarrow n_{k+1}$), ($n_{k-1} \leftarrow n_k \leftarrow n_{k+1}$), or ($n_{k-1} \leftarrow n_k \rightarrow n_{k+1}$). 

(ii) Neither $n_k$ nor any of its descendants is in $S$, and the path between $n_{k-1}$ and $n_{k+1}$ forms ($n_{k-1} \rightarrow n_k \leftarrow n_{k+1}$). 

In a DAG, we say that two nodes $n_a$ and $n_b$ are d-separated by a third node $n_c$ if every path between nodes $n_a$ and $n_b$ is blocked by $n_c$, denoted as $n_a  \! \perp \!\!\! \perp n_b|n_c$. 
\end{assumption}

\begin{assumption}
    (Global Markov Condition~\citep{spirtes2001causation, pearl2009causality}) The state is fully observable and the dynamics is Markovian. The distribution $p$ over a set of variables $\mathcal{V}=(s^1_{t},\cdots,s^d_{t},a^1_{t},\cdots,a^d_{t},r_t)^T$ satisfies the global Markov condition on the graph if for any partition $(\mathcal{S, A, R})$ in $\mathcal{V}$ such that if $\mathcal{A}$ d-separates $\mathcal{S}$ from $\mathcal{R}$, then $p(\mathcal{S},\mathcal{R}|\mathcal{A}) = p(\mathcal{S}|\mathcal{A})\cdot p(\mathcal{R}|\mathcal{A})$
\end{assumption}

\begin{assumption}
    (Faithfulness Assumption~\citep{spirtes2001causation, pearl2009causality}) 
For a set of variables $\mathcal{V}=(s^1_{t},\cdots,s^d_{t},a^1_{t},\cdots,a^d_{t},r_t)^T$, there are no independencies between variables that are not implied by the Markovian Condition.
\end{assumption}

\begin{assumption}
Under the assumptions that the causal graph is Markov and faithful to the observations, the edge $s^i_t \to  s^i_{t+1}$ exists for all state variables $s^i$.
\end{assumption}

\begin{assumption}
No simultaneous or backward edges in time.
\end{assumption}

\paragraph{Proposition 1}
\textit{Under the assumptions that the causal graph is Markov and faithful to the observations, there exists an edge from $a^i_t \to r_t$ if and only if $a^i_t \not \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$.}

\textit{Proof:} We proceed by proving both directions of the if and only if statement.

($\Rightarrow$) Suppose there exists an edge from $a^i_t$ to $r_t$. We prove that $a^i_t \not \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$ by contradiction. Assume $a^i_t \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$. By the faithfulness assumption, this independence must be reflected in the graph structure. However, this implies the absence of a directed path from $a^i_t$ to $r_t$, contradicting the existence of the edge. Thus, $a^i_t \not \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$.

($\Leftarrow$) Now, suppose $a^i_t \not \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$. We prove the existence of an edge from $a^i_t$ to $r_t$ by contradiction. Assume no such edge exists. By the Markov assumption, the absence of this edge implies $a^i_t  \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$, contradicting our initial supposition. Therefore, an edge from $a^i_t$ to $r_t$ must exist.
Thus, we have shown that an edge from $a^i_t$ to $r_t$ exists if and only if $a^i_t \not \! \perp \!\!\! \perp r_t | { a_t \setminus a^i_t, s_t }$, completing the proof.
\paragraph{Proposition 2} 
\textit{Under the assumptions that the causal graph is Markov and faithful to the observations, there exists an edge from $s^i_t \to r_t$ if and only if $s^i_t \not \! \perp \!\!\! \perp r_t | \{a_t, s_t  \setminus r_t\}$.}

The proof of Proposition 2 follows a similar line of reasoning as that of Proposition 1.

\textbf{Theorem 1} \textit{Based on above $5$ assumptions and $2$ propositions, suppose $s_t, a_t, s_t$ follow the factored MDP reward function Eq.~\ref{eq:gen}, the causal matrices $M^{s \to r}$ and $M^{a \to r}$ are identifiable}.

\section{Extended Related Work}
\label{sec:appendix_related_work}
We categorize existing causal RL approaches based on problem domains and task types, providing a systematic analysis of how different methods explore causal relationships between states, actions, and rewards, as illustrated in Table~\ref{tab:causal_rl}.

In the single-task learning domain, methods such as ACE~\citep{ji2024ace} and IFactor~\citep{liu2024learning} have shown success in learning policies for manipulation and locomotion tasks. However, both approaches are limited by focusing on a single reward-guided causal relationship.
Regarding generalization, AdaRL~\citep{huangadarl} effectively leverages both state-reward and action-reward causal relationships. 
However, AdaRL focuses primarily on applying causal inference to address generalization challenges in locomotion tasks. 
Its application is limited to locomotion tasks, leaving more complex manipulation tasks unaddressed. Since our work focuses on the single-task problem domain, we do not provide a direct comparison with AdaRL. 
Conversely, CBM~\citep{wang2024building} considers the causal relationship between states and rewards but overlooks the causal link between actions and rewards.
In the problem domain of counterfactual data augmentation, current causal RL methods~\citep{urpicausal,pitis2020counterfactual,pitis2022mocoda} have not yet explored the inference and utilization of both causal relationships.

In summary, current research on reward-guided causal discovery remains incomplete and lacks validation across a broader spectrum of tasks. This gap underscores the need for more comprehensive investigation and application in the field of causal reinforcement learning. 

\textcolor{black}{
\subsection{Extended Discussion on object-centric RL and 3D world models}
\label{ocrl}The main similarity lies between  our framework and object-centric RL is both are learning and using factored MDPs~\citep{kearns1999efficient}, but they differ in granularity: our framework operates at the component level (e.g., raw state variables), whereas object-centric RL factors states based on objects.}

\textcolor{black}{Although our work is orthogonal to object-centric RL, we believe certain elements of object-centric RL could complement our framework in specific applications, particularly in real-world robotic manipulation tasks. Potential future work include:
\begin{itemize}
    \item \textbf{Using object-centric representation as input}: Object-centric models can help identify object-factored variables, such as object attributes, geometry, and physical states, which are useful for planning~\citep{jiang2019scalor,lin2020improving,kossen2019structured, mambelli2022compositional,feng2023learning, choi2024unsupervised, zadaianchuk2022self, park2021object, zadaianchuk2020self, yuan2022sornet, li2020towards, mitash2024scaling, haramati2024entity, li2024manipllm}. In this case, states are factored as objects, and we can learn causal graphs over these variables. This is useful in robotic environments involving numerous objects. We will leave this as a future work for adapting our current framework to the applications of the object-centric robotic task. 
    \item \textbf{Learning more compact factored object representations with our framework}: Our structure learning approach could benefit object-centric RL by disentangling the internal representations of individual objects to the reward-relevant and reward-irrelevant groups by learning the causal structures. This can enhance the compactness and interpretability of object-centric representations.
    \item \textbf{Using object-aware 3D world models for applications}: In 3D environments, object-aware 3D world models~\citep{li2024objectaware} can provide essential representations of objects. Our framework could then build causal structures on top of these factored 3D-object representations.
\end{itemize}
}

\textcolor{black}{While these directions are promising and could advance the applicability of our framework in certain domains, they are outside the primary focus of this work. We plan to explore these ideas as part of future work.}


\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{1pt}
\caption{Categorization of different causal RL methods with two different causal relationship of state-to-reward (state-reward) and action-to-reward (action-reward).}
\label{tab:causal_rl}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{\textbf{Problem domain}} & \multirow{2}{*}{\textbf{Task type}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Causal relationship}} \\ \cline{4-5} 
                         &                            &                         & \textbf{state-reward}       & \textbf{action-reward}      \\ \toprule
\textbf{Single-task}              & manipulation; locomotion    & ACE~\citep{ji2024ace}                     & \xmark                  & \cmark                  \\
                         & manipulation; locomotion    & IFactor~\citep{liu2024learning}                 & \cmark                 & \xmark                   \\ 
                          & manipulation    & CAI~\citep{seitzer2021causal}                 & \xmark                 & \xmark                   \\\hline
\textbf{Generalization}           & manipulation               & CDL~\citep{wang2022causal}                     & \xmark                   & \xmark                   \\
                         & locomotion                 & AdaRL~\citep{huangadarl}                   & \cmark                  & \cmark                  \\ 
                          & manipulation; locomotion                 & CBM~\citep{wang2024building}                   & \cmark                  & \xmark                  \\ \hline
\textbf{Augmentation}             & manipulation               & CAIAC~\citep{urpicausal}                   & \xmark                   & \xmark                   \\
             & manipulation               & CoDA~\citep{pitis2020counterfactual}                   & \xmark                   & \xmark                   \\  & manipulation               & MoCoDA~\citep{pitis2022mocoda}                   & \xmark                   & \xmark                   \\ \bottomrule
\end{tabular}
\end{table}

\section{Details on Experimental Design and Results}
\label{Details on Experimental Design and Results}

\subsection{Experimental setup}
\label{sec:experimental_setup_appendix}
We present the detailed hyperparameter settings of the proposed method $\texttt{\textbf{CIP}}$ across all $5$ environments in Table~\ref{tab:hps}. Additionally, the Q-value and V-value networks are used MLP with 512 hidden size. And the policy network is the Gaussian MLP with 512 hidden size. Moreover, we set the target update interval of $2$. For fair comparison, the hyperparameters of the baseline methods (SAC~\citep{haarnoja2018soft}, BAC~\citep{ji2023seizing}, ACE~\citep{ji2024ace}) follow the same settings in the experiments.

For pixel-based DMControl environments, we employ IFactor~\citep{liu2024learning} to encode latent states and integrate the \texttt{\textbf{CIP}} framework for policy learning. We utilize the $s_{t}^{\bar{r}}$ state features in IFactor as uncontrollable states unrelated to rewards to execute counterfactual data augmentation. Furthermore, for simplicity, we maximize the mutual information between future states and actions to facilitate empowerment. All parameter settings in these three tasks adhere to those specified in IFactor. Additionally, We use the same background video for the comparison. 

\begin{table}[h]
 \centering
% \footnotesize
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\caption{Hyperparameter settings of $\texttt{\textbf{CIP}}$ in $5$ environments}
\label{tab:hps}
% 
\begin{tabular}{cccccc}
\hline
\multirow{2}{*}{\textbf{Hyperparameter}} & \multicolumn{5}{c}{\textbf{Environment}}                        \\ \cline{2-6} 
                                & \textbf{Meta-World} & \textbf{Sparse} & \textbf{MuJoCo} & \textbf{DMControl} & \textbf{Adroit Hand} \\ \hline
\textbf{batch size }                     & 512        & 512    & 256    & 512       & 256         \\
\textbf{hidden size  }                   & 1024       & 1024   & 256    & 1024      & 256         \\
\textbf{Q-value network hidden size}                       & \multicolumn{5}{c}{512}                            \\
\textbf{V-value network hidden size}                       & \multicolumn{5}{c}{512}                            \\
\textbf{policy network hidden size}                       & \multicolumn{5}{c}{512}                            \\
\textbf{learning step}                       & \multicolumn{5}{c}{1000000}                            \\
\textbf{replay size}                     & \multicolumn{5}{c}{1000000}                            \\
\textbf{causal sample size}              & \multicolumn{5}{c}{10000}                              \\
\textbf{gamma}                           & \multicolumn{5}{c}{0.99}                               \\
\textbf{learning rate}                   & \multicolumn{5}{c}{0.0003}                             \\ 
\textbf{update interval}                   & \multicolumn{5}{c}{2}                             \\
\hline
\end{tabular}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/meta/meta_1_success.pdf}
    \includegraphics[width=1\textwidth]{figs/meta/meta_2_success.pdf}
    \includegraphics[width=1\textwidth]{figs/meta/meta_3_success.pdf}
    \includegraphics[width=1\textwidth]{figs/meta/meta_4_success.pdf}
    \includegraphics[width=0.3\textwidth]{figs/meta/pick_place.pdf} 
    \caption{Experimental results across $17$ manipulation skill learning tasks in Meta-World.}
    \label{fig:appendix_manipulation}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/aug/heatmap.pdf}
    \caption{Visualization of the trajectories in soccer task.}
    \label{fig:aug}
    \vspace{-3mm}
\end{figure}
% \clearpage

\subsection{Full Results}
\label{sec:full_results_appendix}

\subsubsection{Effectiveness in robot arm manipulation}
Figure~\ref{fig:appendix_manipulation} presents the learning curves for all 17 manipulation skill tasks within the Meta-World environment. The \(\texttt{\textbf{CIP}}\) framework demonstrates superior learning outcomes and efficiency compared to the three baseline methods, despite exhibiting minor instabilities in the basketball and dial-turn tasks. Notably, \(\texttt{\textbf{CIP}}\) achieves a 100\% success rate in more complex tasks, such as pick-place-wall and assembly. 
The visualization results presented in Figures~\ref{fig:appendix_meta_vis} and \ref{fig:appendix_adroit_vis} further demonstrate \(\texttt{\textbf{CIP}}\)'s ability to effectively and efficiently complete tasks, even in high-dimensional action spaces such as the Adroit Hand environment. 

In the hammer task, \(\texttt{\textbf{CIP}}\) allows the robot arm to execute reach and pick actions with precision, enabling it to accurately identify the nail's position and successfully perform the hammering action.
In the Adroit Hand door task, \(\texttt{\textbf{CIP}}\) effectively controls the complex joints to grasp the doorknob and applies the appropriate force to twist it, thereby opening the door. 

These findings affirm the effectiveness of \(\texttt{\textbf{CIP}}\) in robot arm manipulation skill learning, highlighting its capacity to enhance sample efficiency while mitigating the risks associated with blind exploration.

\vspace{-3mm}
\paragraph{Visualization.} 
We employ trajectory visualization to comparatively validate the efficacy of our method. As depicted in Figure~\ref{fig:aug}, the light-shaded regions delineate the policy exploration space, while the point clustering area indicates the area of frequent interaction. Our analysis reveals that $\texttt{\textbf{CIP}}$, leveraging counterfactual data augmentation, achieves substantially broader exploration compared to ACE and SAC. Concurrently, the causal information prioritization framework facilitates more focused execution in critical state regions. These visual findings provide robust empirical support for the effectiveness of our proposed augmentation framework. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/visualization/meta.pdf}
    \caption{Visualization trajectories of $4$ manipulation skill learning tasks in Meta-World environment.}
    \label{fig:appendix_meta_vis}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/visualization/adroit.pdf}
    \caption{Visualization trajectory of Adroit Hand door open task.}
    \label{fig:appendix_adroit_vis}
\end{figure}

\subsubsection{Effectiveness in spare reward settings}

Figure~\ref{fig:appendix_meta_sparse} presents the learning curves for all three sparse reward setting tasks within the Meta-World environment, while Figure~\ref{fig:appendix_meta_sparse_vis} showcases their corresponding visualization trajectories. These findings reveal that \(\texttt{\textbf{CIP}}\) not only achieves superior learning efficiency but also adeptly executes critical actions necessary for task completion, such as opening the door and window and maneuvering the node to the target place.

These results substantiate the effectiveness of \(\texttt{\textbf{CIP}}\) in sparse reward scenarios. The counterfactual data augmentation process prioritizes salient state information, effectively filtering out irrelevant factors that could hinder learning. Meanwhile, causal action empowerment enhances policy controllability by focusing on actions that are causally linked to desired outcomes. This dual approach not only accelerates the learning process but also fosters a more robust policy capable of navigating the complexities inherent in sparse reward settings. Overall, these findings underscore \(\texttt{\textbf{CIP}}\)'s potential to significantly improve performance in challenging environments characterized by limited feedback.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/sparse/appendix/sparse_success_appendix.pdf}
    \caption{Experimental results across $3$ manipulation skill learning tasks in sparse reward settings of Meta-World environment.}
    \label{fig:appendix_meta_sparse}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/visualization/sparse.pdf}
    \caption{Visualization trajectories of $3$ manipulation skill learning tasks in sparse reward settings of Meta-World environment.}
    \label{fig:appendix_meta_sparse_vis}
\end{figure}

\subsubsection{Effectiveness in locomotion}
We further evaluate \(\texttt{\textbf{CIP}}\) in $15$ locomotion tasks in DMControl and MuJoCo environments. Figure~\ref{fig:appendix_locomotion} presents the learning curves, while Figure~\ref{fig:appendix_loco_vis} showcases the corresponding visualization trajectories in $4$ specific tasks. 
A comprehensive analysis indicates that \(\texttt{\textbf{CIP}}\) achieves faster learning efficiency and greater stability compared to ACE and SAC, while demonstrating comparable policy learning performance to BAC, which is known for its proficiency in control tasks. The visualization results reveal that \(\texttt{\textbf{CIP}}\) effectively executes running and walking actions in complex humanoid scenarios.

These findings collectively underscore the efficacy of \(\texttt{\textbf{CIP}}\) in locomotion tasks, highlighting its potential to advance the state-of-the-art in reinforcement learning for intricate motor control problems. The method's success across varied environments suggests a robust framework that could generalize effectively to other challenging domains within robotics and control systems, paving the way for future research and applications in these areas.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/dmc/appendix/dmcontrol_reward_1.pdf}
    \includegraphics[width=1\textwidth]{figs/dmc/appendix/dmcontrol_reward_2.pdf}
    \includegraphics[width=0.75\textwidth]{figs/dmc/appendix/dmcontrol_reward_3.pdf}
    \caption{Experimental results across $11$ locomotion tasks in DMControl environment.}
    \label{fig:appendix_locomotion}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/visualization/dmc.pdf}
    \caption{Visualization trajectories of $4$ locomotion tasks in DMControl environment.}
    \label{fig:appendix_loco_vis}
\end{figure}
\clearpage

\subsubsection{Effectiveness in pixel-based tasks}
\label{sec:vis_pixel}
To further validate the effectiveness of our proposed framework in pixel-based environments, we evaluated $\texttt{\textbf{CIP}}$ on three DMControl pixel-based tasks. We leverage IFactor for latent state processing and differentiation of uncontrollable state features to execute counterfactual data augmentation, alongside maximizing the mutual information between future states and actions for empowerment. 

Figure~\ref{fig:appendix_pixel} presents the learning curves, while Figure~\ref{fig:appendix_pixel_vis} shows the visualization trajectories. The proposed framework exhibits enhanced policy learning performance and effectively mitigates interference from background video, facilitating efficient locomotion. These findings reinforce the effectiveness and extensibility of our causal information prioritization framework, highlighting its potential to improve learning in complex, pixel-based environments. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.24\textwidth]{figs/pixel/envs/walker.jpg}
    \includegraphics[width=0.24\textwidth]{figs/pixel/envs/reacher.jpg}
    \includegraphics[width=0.24\textwidth]{figs/pixel/envs/cheetah.jpg}
    \includegraphics[width=0.24\textwidth]{figs/pixel/envs/cartpole.png}
    \caption{The DMControl environment of $3$ pxiel-based tasks (Walker Walk, Cheetah Run, Reacher Easy) and $1$ task in Cartpole environment~\citep{liu2024learning}.}
    \label{fig:appendix_pixel_env}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/pixel/pixel.pdf}
    \caption{Visualization trajectories in $3$ pixel-based locomotion tasks of DMControl environment with video backgrounds as distractors.}
    \label{fig:appendix_pixel_vis}
\end{figure}


\subsection{Property Analysis}
\label{sec:pro_res_appendix}

\subsubsection{Analysis for replacing counterfactual data augmentation}
\label{sec:appendix_cda_replace}

In $\texttt{\textbf{CIP}}$, we exploit the causal relationship between states and rewards to perform counterfactual data augmentation on irrelevant state features, thus prioritizing critical state information. We compare this approach with an alternative method: masking irrelevant state features to achieve state abstraction for subsequent causal action empowerment and policy learning. To evaluate the efficacy of both approaches, we conduct experiments with $\texttt{\textbf{CIP}}$ with counterfactual data augmentation ($\texttt{\textbf{CIP}}$ w/i Cda) and $\texttt{\textbf{CIP}}$ with causally-informed states ($\texttt{\textbf{CIP}}$ w/i Cs) across three distinct environments.

Figure \ref{fig:cda_meta} illustrates comparative results for four manipulation skill learning tasks in the Meta-World environment. Both $\texttt{\textbf{CIP}}$ variants achieve 100\% task success rates with high sample efficiency, validating their effectiveness. Notably, $\texttt{\textbf{CIP}}$ w/i Cda exhibits superior learning efficiency compared to $\texttt{\textbf{CIP}}$ w/i Cs, underscoring the value of our counterfactual data augmentation approach in enhancing training data without additional environmental interactions. 
In three sparse reward setting tasks (Figure \ref{fig:cda_sparse}), $\texttt{\textbf{CIP}}$ w/i Cda demonstrates superior policy performance. Further experiments across four locomotion environmetal tasks corroborate these findings, consistently favoring the counterfactual data augmentation approach.
These comprehensive experimental results strongly support the effectiveness and significance of incorporating counterfactual data augmentation in $\texttt{\textbf{CIP}}$, highlighting its potential to enhance reinforcement learning across diverse task domains.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/cda/cda-meta.pdf}
    \caption{Experimental results in $4$ manipulation skill learning tasks of Meta-World environment. w/i stands for with.}
    \label{fig:cda_meta}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/cda/sparse_success_cda.pdf}
    \caption{Experimental results in $3$ manipulation skill learning tasks of Meta-World environment with sparse reward settings.}
    \label{fig:cda_sparse}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/cda/cda-dmc.pdf}
    \caption{Experimental results in $4$ locomotion tasks of DMControl environment.}
    \label{fig:cda_dmc}
\end{figure}

\subsubsection{Extensive ablation study}
\paragraph{Robot arm manipulation}

The ablation study results in the Meta-World and Adroit Hand environments are presented in Figure~\ref{fig:appendix_abl_manipulation}. The findings indicate that $\texttt{\textbf{CIP}}$ without counterfactual data augmentation exhibits reduced learning efficiency and is unable to successfully complete tasks such as pick-and-place. This underscores the importance of incorporating counterfactual data augmentation, which prioritizes causal state information, to enhance learning efficiency by mitigating the influence of irrelevant state information and preventing policy divergence.

Furthermore, $\texttt{\textbf{CIP}}$ without causal action empowerment demonstrates a significant decline in policy performance across robot arm manipulation tasks. In complex scenarios, such as Adroit Hand door opening and assembly, it fails to learn effective strategies for task completion. This outcome further corroborates the efficacy of the proposed causal action empowerment mechanism, as prioritizing causally informed actions facilitates more efficient exploration of the environment, ultimately enabling successful policy learning.

\paragraph{Sparse reward settings}
Figure~\ref{fig:appendix_abl_manipulation} presents the results of the ablation study conducted across three sparse reward setting tasks. These findings underscore the substantial influence of causal action empowerment on the efficacy of policy learning, demonstrating its critical role in enhancing performance in challenging environments. Additionally, the incorporation of counterfactual data augmentation proves effective in mitigating the need for additional environmental interactions, thereby significantly improving sample efficiency. This approach not only facilitates more rapid learning but also ensures that the agent can effectively navigate sparse reward scenarios by focusing on the most relevant causal information. 

\paragraph{Locomotion}
We further conducted ablation experiments on locomotion tasks. The experimental results in the MuJoCo environment are shown in Figure~\ref{fig:appendix_abl_mujoco}, where it is evident that the performance of $\texttt{\textbf{CIP}}$ without causal action empowerment declines significantly. Similarly, $\texttt{\textbf{CIP}}$ without counterfactual data augmentation also exhibits reduced learning efficiency. Notably, in the 11 DMControl tasks, the decline in performance for $\texttt{\textbf{CIP}}$ without causal action empowerment is particularly pronounced. 

These experimental results further validate the effectiveness of our proposed method, which systematically analyzes the causal relationships between states, actions, and rewards. This analysis enables the execution of counterfactual data augmentation to avoid interference from irrelevant factors while prioritizing important state information. Subsequently, by leveraging the causal relationships between actions and rewards, we reweight actions to prioritize causally informed actions, thereby enhancing the agent’s controllability and overall learning efficacy.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/ablation/abl_meta_1_success.pdf}
    \includegraphics[width=1\textwidth]{figs/ablation/abl_meta_2_success.pdf}
    \includegraphics[width=1\textwidth]{figs/ablation/abl_meta_3_success.pdf}
    \includegraphics[width=1\textwidth]{figs/ablation/abl_meta_4_success.pdf}
    \includegraphics[width=1\textwidth]{figs/ablation/abl_meta_5_success.pdf} 
    \caption{Ablation results across $21$ manipulation skill learning tasks in Meta-World including sparse reward settings and adroit hand.}
    \label{fig:appendix_abl_manipulation}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/ablation/abl_mujoco_reward.pdf}
    \caption{Ablation results across $4$ locomotion tasks in MuJoCo environment.}
    \label{fig:appendix_abl_mujoco}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/ablation/abl_dmc_1_reward.pdf}
    \includegraphics[width=1\textwidth]{figs/ablation/abl_dmc_2_reward.pdf}
    \includegraphics[width=0.75\textwidth]{figs/ablation/abl_dmc_3_reward.pdf}
    \caption{Ablation results across $11$ locomotion tasks in DMControl environment.}
    \label{fig:appendix_abl_dmc}
\end{figure}


\subsubsection{Hyperparameter analysis}
We conduct a detailed analysis of the hyperparameters associated with the causal update interval (\(I\)) and sample size within the $\texttt{\textbf{CIP}}$ framework. The experimental results for four distinct tasks are illustrated in Figure~\ref{fig:hps}. Across all tasks,  $\texttt{\textbf{CIP}}$ demonstrates optimal performance with a causal update interval of \(I = 2\) and a sample size of 10,000. 

Our findings suggest that while a reduction in the causal update interval can lead to improved performance, it may also result in heightened computational costs. Additionally, we observe that higher update frequencies and increased sample sizes introduce greater instability, which significantly raises computational demands. This analysis underscores the importance of carefully balancing hyperparameter settings to optimize both performance and efficiency within the $\texttt{\textbf{CIP}}$.

\textcolor{black}{Furthermore, we analyze the performance under different settings of the temperature factor $\alpha$ proposed in Eq.~\ref{eq:alpha}. The results across $3$ tasks are shown in Figure~\ref{fig:appendix_hps_a}. Our analysis reveals that $\texttt{\textbf{CIP}}$ demonstrates robust performance across different values of $\alpha$ in manipulation tasks, while showing some instability in locomotion tasks when $\alpha$ is either too small or too large. Moreover, we observe that setting $\alpha$ to 0.2 yields optimal performance across all tasks, which motivated our choice of $\alpha = 0.2$ for all experiments.}

\textcolor{black}{Finally, we analyze the performance under different settings of the batch size and hidden size. The results across $3$ tasks are shown in Figure~\ref{fig:appendix_hps_size}. Our experimental results demonstrate that $\texttt{\textbf{CIP}}$ exhibits robust performance across various parameter settings in coffee push and sparse hand insert tasks, while maintaining strong performance in hopper stand task. 
Based on these experimental results, we configure the hyperparameters as follows: for manipulation tasks, we set the batch size to 512 and hidden size to 1024, while for locomotion tasks, we use a batch size of 256 and hidden size of 256. All other hyperparameters remain constant across all tasks, as detailed in Table~\ref{tab:hps}.}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/hps/hps_results.pdf}
    \caption{Hyperparameter study. Learning curves of $\texttt{\textbf{CIP}}$ with different hyperparameter settings. The shaded regions are the standard deviation of each policy.}
    \label{fig:hps}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{rebuttal_fig/hps/hps_a_results.pdf}
    \caption{Hyperparameter analysis of temperature factor $\alpha$ across $3$ task.}
    \label{fig:appendix_hps_a}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{rebuttal_fig/hps/hps_size_results.pdf}
    \caption{Hyperparameter analysis of batch size and hidden size across $3$ task.}
    \label{fig:appendix_hps_size}
\end{figure}

\textcolor{black}{
\subsubsection{Computation cost analysis}
We analyze the computational cost of the proposed framework. The computation time for all methods across 36 tasks is shown in Figure~\ref{fig:appendix_time}. Our experimental results demonstrate that CIP achieves its performance improvements with minimal additional computational burden - specifically less than $10\%$ increase compared to SAC, less than $5\%$ increase compared to ACE, and actually requiring less computation time than BAC. All experiments were conducted on the same computing platform with the same computational resources detailed in Appendix~\ref{exp}. 
These experimental results verify that our proposed method achieves performance improvements without incurring significant additional computational costs.}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{rebuttal_fig/time/time_1.pdf}
    \includegraphics[width=1\textwidth]{rebuttal_fig/time/time_2.pdf}
    \includegraphics[width=1\textwidth]{rebuttal_fig/time/time_3.pdf}
    \includegraphics[width=1\textwidth]{rebuttal_fig/time/time_4.pdf}
    \includegraphics[width=1\textwidth]{rebuttal_fig/time/time_5.pdf} 
    \includegraphics[width=1\textwidth]{rebuttal_fig/time/time_6.pdf} 
    \caption{Computation time in $36$ tasks.}
    \label{fig:appendix_time}
\end{figure}

\textcolor{black}{
\subsubsection{Statistical performance analysis}
\label{sec:appendix_statis}
To further validate the statistical significance of the performance, we select $3$ statistical metrics~\citep{agarwal2021deep} - IQM, Mean, and Median - for analysis across $8$ locomotion tasks. The results are shown in Figure~\ref{fig:appendix_IQM_1} and~\ref{fig:appendix_IQM_2}. Our findings indicate that \texttt{\textbf{CIP}} achieves notably superior performance across all tasks, with the sole exception of the ant task where it performs slightly below BAC.
}

\textcolor{black}{
\vspace{-5mm}
\subsubsection{Generalization analysis}
We conduct multi-task experiments in the Meta-World environment~\citep{yu2020meta} to validate the generalizability. We establish MT1 and MT10 tasks for generalization validation:
}

\textcolor{black}{
\textbf{Multi-Task 1 (MT1)}: Learning one multi-task policy that generalizes to $5$ tasks belonging to the same environment. MT1 uses single Meta-World environments, with the training “tasks” corresponding to $5$ random initial object and goal positions. The goal positions are provided in the observation and are a fixed set, as to focus on the ability of algorithms in acquiring a distinct skill across multiple goals, rather than generalization and robustness.
}

\textcolor{black}{
\textbf{Multi-Task 10 (MT10)}: This task involves learning a single multi-task policy that generalizes to 50 tasks across 10 training environments, totaling 500 training tasks. A crucial step towards rapid adaptation to distinctly new tasks is the ability to train a single policy capable of solving multiple distinct training tasks. The multi-task evaluation in Meta-World tests the ability to learn multiple tasks simultaneously, without accounting for generalization to new tasks. The MT10 evaluation encompasses 10 environments: reach, push, pick and place, open door, open drawer, close drawer, press button top-down, insert peg side, open window, and close window.
}

\textcolor{black}{We adapt our proposed $\texttt{\textbf{CIP}}$ to multi-task learning  by incorporating a one-hot task ID as input, comparing $\texttt{\textbf{MT-CIP}}$ with MT-SAC. The results in Figure~\ref{fig:gen} show that $\texttt{\textbf{MT-CIP}}$ outperforms MT-SAC in both MT1 (soccer) and MT10 tasks, achieving average success rates above $50\%$ and $40\%$ respectively. Notably, $\texttt{\textbf{MT-CIP}}$ exhibits strong performance in specific MT10 tasks like drawer close and window open. 
The superior performance of $\texttt{\textbf{MT-CIP}}$ stems from its effective learning of causal information, enabling robust task transfer across diverse domains. While these results are promising, future work will focus on causal state abstraction for enhanced generalization and sample efficiency. All experiments were conducted under the same hyperparameter settings, and the implementation will be made publicly available.}

\textcolor{black}{
\vspace{-5mm}
\subsubsection{Causal discovery analysis}
In $\texttt{\textbf{CIP}}$, we use the linear causal discovery method DirectLiNAM for causal structure learning. To explore alternative approaches, we compare it with two other causal discovery methods: score-based GES~\citep{chickering2002optimal} and constraint-based PC~\citep{spirtes2001causation}. The experimental results in Figure~\ref{fig:appendix_causal} across three tasks demonstrate that our chosen DirectLiNAM method exhibits superior performance compared to both alternatives. 
During experimentation, we also observe that both GES and PC methods incur significant computational overhead and frequently encounter memory constraints. In contrast, our proposed method $\texttt{\textbf{CIP}}$, which is fundamentally reward-guided, efficiently discovers causal relationships between dimensional factors in states and actions with respect to rewards. This approach better aligns with the requirements of policy learning while maintaining minimal computational costs.
}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/CheetahRun_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/CheetahRun_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/CheetahRun_median.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/HopperStand_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/HopperStand_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/HopperStand_median.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/QuadrupedWalk_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/QuadrupedWalk_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/QuadrupedWalk_median.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/ReacherHard_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/ReacherHard_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/ReacherHard_median.pdf}
    \caption{statistical metrics of IQM, Mean, and Median (higher values are better) on 4 DMControl tasks.}
    \label{fig:appendix_IQM_1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Ant_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Ant_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Ant_median.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/HalfCheetah_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/HalfCheetah_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/HalfCheetah_median.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Hopper_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Hopper_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Hopper_median.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Walker2d_iqm.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Walker2d_mean.pdf}
    \includegraphics[width=0.32\textwidth]{rebuttal_fig/statistical/Walker2d_median.pdf}
    \caption{statistical metrics of IQM, Mean, and Median (higher values are better) on 4 MuJoCo tasks.}
    \label{fig:appendix_IQM_2}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.91\textwidth]{rebuttal_fig/statistical/t_test_1.pdf}
%     \includegraphics[width=0.91\textwidth]{rebuttal_fig/statistical/t_test_2.pdf}
%      \caption{statistical metrics of t test (lower value indicates significant difference with CIP) on $8$ tasks.}
%     \label{fig:appendix_t_test}
% \end{figure}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.24\textwidth]{rebuttal_fig/generalize/MT1.pdf}
    \includegraphics[width=0.24\textwidth]{rebuttal_fig/generalize/mt10.pdf}
    \includegraphics[width=0.24\textwidth]{rebuttal_fig/generalize/drawer_close.pdf}
    \includegraphics[width=0.24\textwidth]{rebuttal_fig/generalize/window_open.pdf}
    \caption{Generalization results in MT1 and MT10 tasks.}
    \label{fig:gen}
\end{figure}

\clearpage

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{rebuttal_fig/causal.pdf}
    \caption{Compared performance with $2$ different causal discovery methods across $3$ task.}
    \label{fig:appendix_causal}
\end{figure}

\section{Details on the Proposed Framework}
\label{Details on the Proposed Framework}

Algorithm~\ref{alg:algorithm1} lists the full pipeline of \texttt{\textbf{CIP}} below. 

\begin{algorithm}[h]
\footnotesize
    \caption{Causal information prioritization for efficient RL}
    \label{alg:algorithm1}
    \textbf{Input}: $Q$ network $Q_{\pi_c}$, policy network $\pi_{c}$, inverse dynamics model $\phi_c$ with $Q$ network $Q_{\phi_c}$, replay buffer $\mathcal{D}$, local causal buffer $\mathcal{D}_c$, causal update interval $I$, causal matrix $M^{a \to s}$ and $M^{a \to r}$. 
    
    \begin{algorithmic}[]
    \FOR{each environment step $t$}
    \STATE Collect data with $\pi_{\theta}$ from real environment
    \STATE Add to replay buffer $\mathcal{D}$ and local buffer $\mathcal{D}_c$ 

    \ENDFOR
    \begin{tcolorbox}[colback=black!0!white,colframe=white!50!black,title=Step 1: Counterfactual data augmentation]
     \IF{every $I$ environment step}
     \STATE Sample transitions $\mathcal{D}_s$ from local buffer $\mathcal{D}_c$
     \STATE Learn causal mask matrix $M^{a \to r}$ with $\left\{(s, a, r, s')\right\}^{|\mathcal{D}_s|}$ for causal state prioritization
     \STATE Compute uncontrollable set $\mathcal{U}_s$ followed by Eq.~\ref{cda}
     \STATE Sample $(s, a, r, s') \in \mathcal{D}_s$
     \FOR{$s^i \in \mathcal{U}_s$}
     \STATE Sample $(\hat{s}, \hat{a}, \hat{r}, \hat{s}') \sim \mathcal{D}_s$
     \IF{state $\hat{s}^i$ $\in$ $\mathcal{U}_{\hat{s}}$ }
     \STATE Construct a counterfactual transition $(\tilde{s}, \tilde{a}, \tilde{r}, \tilde{s}')$ by swapping $(s^i, s'^{i})$ with $(\hat{s}^i, \hat{s}'^{i})$
     \STATE Add $(\tilde{s}, \tilde{a}, \tilde{r}, \tilde{s}')$ to local buffer $\mathcal{D}_c$
     \ENDIF
     \ENDFOR
     \ENDIF
     \end{tcolorbox} 

      \begin{tcolorbox}[colback=orange!0!white,colframe=orange!60!black,title=Step 2: Causal weighted matrix learning]
       \IF{every $I$ environment step}
     \STATE Sample transitions $\mathcal{D}_a$ from local buffer $\mathcal{D}_c$  
     \STATE Learn causal weighted matrix $M^{a \to r}$ with $\left\{(s, a, r, s')\right\}^{|\mathcal{D}_a|}$ for causal action prioritization
     \ENDIF
    \end{tcolorbox}

      \begin{tcolorbox}[colback=green!0!white,colframe=green!50!black,title=Step 3: Policy optimization with causal action empowerment]
     \FOR{each gradient step}
     \STATE Sample $N$ transitions $(s,a,r,s')$ from $\mathcal{D}$
     \STATE Compute causal action empowerment followed by Eq.~\ref{eq:emp_comp}.
     \STATE Calculate the target $Q_{\phi_c}$ value
     \STATE Update $Q_{\phi_c}$ by $\min_{\phi_c}(\mathcal{T}Q_{\phi_c}-Q_{\phi_c})^2$
     \STATE Update $\phi_{c}$ by $\max(Q_{\phi_c}(s,a)$)
     \STATE Calculate the target $Q_{\pi_c}$ value
     \STATE Update $Q_{\pi_c}$ by $\min_{\pi_c}(\mathcal{T}_cQ_{\pi_c}-Q_{\pi_c})^2$
     \STATE Update $\pi_{c}$ by $\max_{c}(Q_{\pi_c}(s,a) + \mathcal{E}_{\pi_c}(s)$)
     \ENDFOR
     \end{tcolorbox}
    \end{algorithmic} 
\end{algorithm}

\section{Experimental Platforms and Licenses}
\label{exp}
\subsection{Experimental platforms}
All experiments of this approach are implemented on 2 Intel(R) Xeon(R) Gold 6430 and 2 NVIDIA Tesla A800 GPUs.

\subsection{Licenses}
In our code, we have utilized the following libraries, each covered by its respective license agreements:
\begin{itemize}
    \item PyTorch (BSD 3-Clause "New" or "Revised" License)
    \item Numpy (BSD 3-Clause "New" or "Revised" License)
    \item Tensorflow (Apache License 2.0)
    \item Meta-World (MIT License)
    \item MuJoCo (Apache License 2.0)
    \item Deep Mind Control (Apache License 2.0)
    \item Adroit Hand (Creative Commons License 3.0)
\end{itemize}


