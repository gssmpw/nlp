\section{Related Work}
\vspace{-1mm}
\subsection{Causal RL} 
\vspace{-1mm}
The application of causal reasoning in RL has shown significant potential to improve sample efficiency and generalization by effectively excluding irrelevant environmental factors through causal analysis~\citep{huangadarl,feng2023learning,mutti2023provably, sun2024acamda, sun2022toward}. Wang~\citep{wang2021task} introduces a novel regularization-based method for causal dynamics learning, which explicitly identifies causal dependencies by regulating the number of variables used to predict each state variable. 
CDL~\citep{wang2022causal} takes an innovative approach by using conditional mutual information to compute causal relationships between different dimensions of states and actions. 
IFactor~\citep{liu2024learning} is a general framework to model four distinct categories of latent state variables, capturing various aspects of information. ACE~\citep{ji2024ace}, an off-policy actor-critic method, integrates causality-aware entropy regularization. Table~\ref{tab:causal_rl} provides a categorization of various causal RL methods, highlighting their focus on different reward-guided causal relationships. 
Existing approaches do not fully account for the causal relationships between both states and actions with rewards. Our goal is to explore these causal relationships from a reward-guided perspective to enhance sample efficiency across a broader range of tasks.
\vspace{-3mm}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

% \vspace{-1mm}
\subsection{Empowerment in RL}
\vspace{-2mm}
Empowerment, an information theory-based concept of intrinsic motivation, has emerged as a powerful paradigm for enhancing an agent's environmental controllability \citep{mohamed2015variational,klyubin2005empowerment,cao2024towards}. This framework conceptualizes actions and future states as information transmission channels, offering a novel perspective on agent-environment interactions. 
In RL, empowerment has been applied to uncover more controllable associations between states and actions, as well as to develop robust skill~\citep{salge2014empowerment,bharadhwaj2022information,choi2021variational, eysenbach2018diversity,leibfried2019unified,seitzer2021causal}. Empowerment, expressed as maximizing mutual information $\max_{\pi} I$, serves as a learning objective in various RL frameworks, providing intrinsic motivation for exploration and potentially yielding more efficient and generalizable policies. Our approach extends empowerment in RL by examining the influence of state, actions, and rewards through a causal lens, integrating causal understanding with empowerment to enhance exploration strategy and learning efficiency.
\vspace{-3mm}


\vspace{-1mm}
\subsection{Object-centric RL and Object-Oriented RL}
\vspace{-2mm}
Recent advances in object-centric representation learning focus on acquiring and leveraging structured, object-wise representations from high-dimensional observations. Foundational works include Slot Attention~\citep{locatello2020object} and AIR~\citep{eslami2016attend, kosiorek2018sequential}, establishing basis for this field. Subsequent follow-ups have worked on these concepts by employing state-of-the-art architectures, including DINO-based approaches~\cite{zadaianchuk2023objectcentric}, transformer-based models~\citep{wu2022slotformer}, diffusion models~\citep{jiang2023object}, and state-space models~\citep{jiang2024slot}. Notably, learning object-centric representations can enable compositional generalization across various domains, such as video and scene generation~\citep{wu2023slotdiffusion, wu2024neural}. Moreover, several theoretical studies have explored the mechanisms underlying compositional generalization and the causal identifiability~\citep{kori2024identifiable, brady2023provably, lachapelle2024additive}.

Object-centric representations have been effectively employed in world models to capture multi-object dynamics, as demonstrated by works~\citep{jiang2019scalor,lin2020improving,kossen2019structured}. Building on these object-centric world models, various studies use them in RL by better modeling complex object-centric structures in partially observable MDPs~\citep{kossen2019structured, mambelli2022compositional, feng2023learning, choi2024unsupervised}, identifying critical objects ~\citep{zadaianchuk2022self, park2021object}, and learning object-centric policies~\citep{zadaianchuk2020self, yuan2022sornet} and applications in robotic manipulation tasks~\citep{li2020towards, mitash2024scaling, haramati2024entity, li2024manipllm}, as well as in learning intrinsic or curiosity-driven policies based on objects and their interactions~\citep{watters2019cobra, hu2023elden, wang2024skild}. 

Another research direction explores object-oriented MDPs, with the homomorphic object-oriented world model being a notable example that leverages MDP homomorphism to model object dynamics and enable efficient planning through symmetric equivalence in MDPs~\citep{diuk2008object, scholz2014physics, wandzel2019multi, van2020plannable, rezaei2022continuous, zhao2022toward}, provides a powerful foundation for learning object-oriented MDPs and facilitates efficient planning~\citep{wolfe2006defining}.   Our work, which focuses on uncovering general causal relationships among components in MDPs and empowerment optimization, is orthogonal to object-centric RL. However, object-centric RL could provide useful abstract object-based variables that could be useful for causal structure learning in complex environments~\footnote{We provide a detailed discussion in Appendix~\ref{ocrl}.}.

\vspace{-2mm}