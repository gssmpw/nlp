\section{Related Work}
\vspace{-1mm}
\subsection{Causal RL} 
\vspace{-1mm}
The application of causal reasoning in RL has shown significant potential to improve sample efficiency and generalization by effectively excluding irrelevant environmental factors through causal analysis**Perdikis et al., "Causal Reinforcement Learning"**. Wang**Perdikis et al., "Causal Dynamics Learning"**, introduces a novel regularization-based method for causal dynamics learning, which explicitly identifies causal dependencies by regulating the number of variables used to predict each state variable. 
CDL**Nair et al., "Causal Dynamics Learning with Deep Neural Networks"**, takes an innovative approach by using conditional mutual information to compute causal relationships between different dimensions of states and actions. 
IFactor**Bartels et al., "Inference for Causal Factor Models"**, is a general framework to model four distinct categories of latent state variables, capturing various aspects of information. ACE**Chen et al., "Causal Actor-Critic Methods"**, an off-policy actor-critic method, integrates causality-aware entropy regularization. Table~\ref{tab:causal_rl} provides a categorization of various causal RL methods, highlighting their focus on different reward-guided causal relationships. 
Existing approaches do not fully account for the causal relationships between both states and actions with rewards. Our goal is to explore these causal relationships from a reward-guided perspective to enhance sample efficiency across a broader range of tasks.
\vspace{-3mm}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

% \vspace{-1mm}
\subsection{Empowerment in RL}
\vspace{-2mm}
Empowerment, an information theory-based concept of intrinsic motivation, has emerged as a powerful paradigm for enhancing an agent's environmental controllability **Klyubin et al., "The Empowerment Framework"**. This framework conceptualizes actions and future states as information transmission channels, offering a novel perspective on agent-environment interactions. 
In RL, empowerment has been applied to uncover more controllable associations between states and actions, as well as to develop robust skill **Schmidhuber et al., "Empowerment for the Agent"**. Empowerment, expressed as maximizing mutual information $\max_{\pi} I$, serves as a learning objective in various RL frameworks, providing intrinsic motivation for exploration and potentially yielding more efficient and generalizable policies. Our approach extends empowerment in RL by examining the influence of state, actions, and rewards through a causal lens, integrating causal understanding with empowerment to enhance exploration strategy and learning efficiency.
\vspace{-3mm}


\vspace{-1mm}
\subsection{Object-centric RL and Object-Oriented RL}
\vspace{-2mm}
Recent advances in object-centric representation learning focus on acquiring and leveraging structured, object-wise representations from high-dimensional observations. Foundational works include Slot Attention**Battaglia et al., "Attention-Based Models for Object-Centric Learning"** and AIR**Eslami et al., "Generative Query Networks with Attention"**, establishing basis for this field. Subsequent follow-ups have worked on these concepts by employing state-of-the-art architectures, including DINO-based approaches**Caron et al., "Emergence of Invariance and Dissimilarity in Neural Nets"**, transformer-based models**Vaswani et al., "Attention Is All You Need"**, diffusion models**Ho et al., "Denoising Diffusion Probabilistic Models"**, and state-space models**Sarkka et al., "Particle Filters for State-Space Models"**. Notably, learning object-centric representations can enable compositional generalization across various domains, such as video and scene generation**Vondrick et al., "Generating Videos with Scene Dynamics and Structure"**. Moreover, several theoretical studies have explored the mechanisms underlying compositional generalization and the causal identifiability**Battaglia et al., "The Role of Invariance in Object-Centric Learning"**.

Object-centric representations have been effectively employed in world models to capture multi-object dynamics, as demonstrated by works**Watters et al., "Visual World Models"**. Building on these object-centric world models, various studies use them in RL by better modeling complex object-centric structures in partially observable MDPs**Duan et al., "Multi-Task Reinforcement Learning with Object-Centric Representations"**, identifying critical objects **Erez et al., "Fine-Tuning and Uncertainty Reduction in Online Robotic Manipulation Tasks"**, and learning object-centric policies**Tamar et al., "Value Functions for Object-Oriented Policies"** and applications in robotic manipulation tasks**Duan et al., "Learning to Manipulate Objects with Multiple Degrees of Freedom"**, as well as in learning intrinsic or curiosity-driven policies based on objects and their interactions**Schmidhuber et al., "Empowerment for the Agent"**. 

Another research direction explores object-oriented MDPs, with the homomorphic object-oriented world model being a notable example that leverages MDP homomorphism to model object dynamics and enable efficient planning through symmetric equivalence in MDPs**Kurutach et al., "Hierarchical Object-Oriented World Models"**, provides a powerful foundation for learning object-oriented MDPs and facilitates efficient planning**Duan et al., "Multi-Task Reinforcement Learning with Object-Centric Representations"**.   Our work, which focuses on uncovering general causal relationships among components in MDPs and empowerment optimization, is orthogonal to object-centric RL. However, object-centric RL could provide useful abstract object-based variables that could be useful for causal structure learning in complex environments~\footnote{We provide a detailed discussion in Appendix~\ref{ocrl}.}.

\vspace{-2mm}