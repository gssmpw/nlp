
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}     
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
% \usepackage[usenames,dvipsnames]{xcolor}
\usepackage{colortbl}
% colors
\usepackage{graphicx}
\usepackage{amsmath}
% \D\texttt{\textbf{ECL}}areMathOperator*{\argmax}{argmax} %为了使用 \argmax
% \D\texttt{\textbf{ECL}}areMathOperator*{\argmin}{argmin} %为了使用 \argmin
\usepackage{multirow}
\usepackage{subcaption} % 导入 subcaption 包
% \usepackage{natbib}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{xcolor}
\newtheorem{definition}{\bf Definition}
\newtheorem{assumption}{\bf Assumption}  % assumptions
\newtheorem{thm}{\bf Theorem}        % theorems
\newtheorem{corollary}{\bf Corollary}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{royalpurple}{rgb}{0.47, 0.32, 0.66}
\colorlet{mylinkcolor}{royalpurple} %violet
\colorlet{mycitecolor}{royalpurple}
\colorlet{myurlcolor}{MyDarkRed}
\hypersetup{
  citecolor  = mycitecolor,
  linkcolor = mylinkcolor,
  urlcolor = myurlcolor,
  colorlinks = true
}
\usepackage{setspace}
\newcommand{\codesite}{\url{https://sites.google.com/view/rl-cip/}}
\newenvironment{compactitemize}{\begin{itemize}[nosep,leftmargin=*]}{\end{itemize}}
\newcommand{\tianpei}[1]{{\color{red}{[tp: #1]}}}
\newcommand{\rebuttal}[1]
{{\color{myurlcolor}{[rebuttal: #1]}}}
\newcommand{\cmark}{\textcolor{green}{\ding{51}}} % 绿勾
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}   % 红叉


\title{Causal Information Prioritization for \\ Efficient Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Hongye Cao$^{1}$ \quad Fan Feng$^{2,3}$ \quad Tianpei Yang$^{1,4}$\thanks{Corresponding to Tianpei Yang (\texttt{tianpei.yang@nju.edu.cn}).} \quad Jing Huo$^{1}$ \quad \textbf{Yang Gao}$^{1,4}$\\
$^{1}$National Key Laboratory for Novel Software Technology, Nanjing University\\
$^{2}$University of California, San Diego \quad $^{3}$MBZUAI\\
$^{4}$School of Intelligence Science and Technology, Nanjing University\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}
% Reinforcement Learning (RL) has gained prominence in intelligent decision-making due to its effective trial-and-error learning capabilities. 
Reinforcement Learning (RL) methods often suffer from sample inefficiency, one of the underlying reasons is that blind exploration strategies may neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization ($\texttt{\textbf{CIP}}$) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, $\texttt{\textbf{CIP}}$ identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, $\texttt{\textbf{CIP}}$ integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. 
To fully assess the effectiveness of $\texttt{\textbf{CIP}}$, we conduct extensive experiments across $39$ tasks in $5$ diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that $\texttt{\textbf{CIP}}$ consistently outperforms existing RL methods across a wide range of scenarios. The project page is \codesite. 
% \tianpei{Should we mention However, current RL methods suffer from sample-inefficiency. Recent research shows that discovering and taking advantage of causal relationships among states, actions, and rewards is one effective way to improve RL sample efficiency. However, previous causal RL methods overlook... To address this issue, we propose..} 
% However, most RL methods overlook crucial causal relationships among states, actions, and rewards.
% This oversight cripples their ability to discern signal\tianpei{what signal?} from noise—a capability intrinsic \tianpei{what's capability intrinsic} for human learning—thereby severely impairing learning efficiency. 


\end{abstract}
\vspace{-1em}
\section{Introduction} 
\vspace{-1em}

Reinforcement Learning (RL) has emerged as a powerful paradigm for training intelligent decision-making agents to learn optimal behaviors by interacting with their environments, receiving reward feedback, and iteratively optimizing their decision-making policies~\citep{haarnoja2018soft,ze2024h,sutton2018reinforcement,silver2017mastering,cao2023enhancing}. 
Despite its notable successes, most RL approaches are faced with the sample-inefficiency problem, which means they typically necessitate an enormous number of interactions with the environment to learn policies, which can be impractical or costly in real-world scenarios~\citep{savva2019habitat, kroemer2021review}. 
Inefficient policy learning often results from blind exploration strategies that neglect causal relationships, leading to spurious correlations and suboptimal solutions with high exploration costs~\citep{zeng2023survey,liu2024learning}. 

Causal reasoning captures essential information by analyzing causal relationships between different factors, filtering out irrelevant information, and avoiding interference from spurious correlations~\citep{wang2022causal,pitis2022mocoda, zhang2024interpretable, huang2022action}. 
These approaches build internal causal structural models, enabling agents to strategically focus their exploration on the most pertinent aspects of the environment. They significantly reduce the number of samples required and demonstrate remarkable performance in single-task learning, generalization, and counterfactual reasoning~\citep{richens2024robust,urpicausal,deng2023causal,huangadarl,feng2023learning}. 
However, most of these works overlook the reward-relevant causal relationships among different factors, or only partially consider the causal connections between states, actions, and rewards~\citep{liu2024learning,ji2024ace}, thus hindering efficient exploration. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/example.pdf}
    \caption{(a). An example of a robot manipulation soccer task with three trajectories, where the objective is to move the ball into the goal. (b). Underlying causal structure of this example in a factored MDP. Different nodes represent different dimensional states and actions. }
    \label{fig:example}
    \vspace{-8mm}
\end{figure}

In this work, we aim to identify and exploit task-specific causal relationships between states, actions, and rewards, enabling agents to discern relevant states and select actions that maximize rewards, ultimately facilitating precise and goal-oriented behaviors. Here we provide a motivating example in Figure~\ref{fig:example}, showing three trajectories for executing a manipulation soccer task, along with the underlying causal structure in a factored Markov Decision Process (MDP)~\citep{kearns1999efficient}. In the first trajectory (row 1), when the agent fails to distinguish states with more intricate causal relationships of the task, the robotic arm exhibits chaotic moving and receives no rewards. The second trajectory (row 2) shows that even without chaotic movements, uncontrollable actions unrelated to the reward lead to an inability to guide the ball towards the goal. Only by filtering out irrelevant state features and executing more controllable actions can we guarantee that the ball is kicked into the goal like row 3. 
Quantifying the contribution of different factors to the reward can effectively help analyzing important causal relationships. 

To address the limitation of sample-inefficiency and leverage the potential of causal reasoning, we propose a novel approach named Causal Information Prioritization ($\texttt{\textbf{CIP}}$) for efficient RL, improving learning efficiency from the perspective of rewards. 
Building upon the factored MDPs, $\texttt{\textbf{CIP}}$ infers causal relationships between states, actions, and rewards across different dimensions, respectively. $\texttt{\textbf{CIP}}$ employs counterfactual data augmentation based on the causality between states and rewards to generate transitions, prioritizing critical state transitions. Furthermore, $\texttt{\textbf{CIP}}$ leverages the causality between actions and rewards to reweight actions, while utilizing empowerment to maximize mutual information between causally informed actions and future states, thereby enabling better control. 
% causal action empowerment quantifies the agent's capacity to influence its environment using the causal knowledge between action and reward, thereby prioritizing causally informed behavior for goal achievement. 
% By prioritizing causal state and action information pertinent to reward, $\texttt{\textbf{CIP}}$ birdge the gap between causal reasoning and reward maximization.

Specifically, $\texttt{\textbf{CIP}}$ leverages collected data to construct a reward-guided structural model that explicitly reasons about state-reward causal influences, enabling the swapping of causally independent state features across observed trajectories to generate synthetic transitions without additional environment interactions. By swapping independent state features across different transitions (i.e., irrelevant state dimensions of chaotic movements in the soccer task), $\texttt{\textbf{CIP}}$ accentuates causally dependent state information (i.e., relevant states to reach the ball), facilitating focused learning of critical state transitions. 
Subsequently, $\texttt{\textbf{CIP}}$ constructs another structural model that incorporates actions and rewards to reweight actions of dimensions.
To enhance the exploration efficiency, $\texttt{\textbf{CIP}}$ integrates a causality-aware empowerment term, quantifying the agent's capacity to exert controlled influence over its environment through the mutual information. 
This empowerment term, combined with causally weighted actions, is integrated into the learning objective, prioritizing actions with high causal influence. The synthesis of causal reasoning and action empowerment enables agents to focus on behaviors that are causally relevant to the task, leading to more efficient and effective policy learning. 
The main contributions of this work can be summarized as follows. 
\begin{compactitemize}
    \item 
    To address limitations of blind exploration and sample-inefficiency, we introduce $\texttt{\textbf{CIP}}$, a novel efficient RL framework that prioritizes causal information through the lens of reward. $\texttt{\textbf{CIP}}$ bridges the gap between causal reasoning and empowerment to facilitate efficient exploration. 
    \item  $\texttt{\textbf{CIP}}$ constructs reward-guided structural models to uncover causal relationships between states, actions, and rewards across dimensions. By leveraging state-reward causality, it performs counterfactual data augmentation, eliminating the need for additional environment interactions, and enabling learning on critical state transitions. Exploiting action-reward causality, it reweights actions to enhance exploration efficiency through empowerment. By prioritizing causal information, $\texttt{\textbf{CIP}}$ enables agents to focus on behaviors that have causally significant effects on their tasks. 
    \item  To validate the effectiveness of $\texttt{\textbf{CIP}}$, we conduct extensive experiments in $39$ tasks across $5$ diverse continuous control environments, including manipulation and locomotion. These comprehensive evaluations demonstrate the effectiveness of $\texttt{\textbf{CIP}}$ in pixel-based and sparse reward settings, underscoring its versatility and reliability. 
\end{compactitemize}
\vspace{-3mm}
\section{Related Work}
\vspace{-1mm}
\subsection{Causal RL} 
\vspace{-1mm}
The application of causal reasoning in RL has shown significant potential to improve sample efficiency and generalization by effectively excluding irrelevant environmental factors through causal analysis~\citep{huangadarl,feng2023learning,mutti2023provably, sun2024acamda, sun2022toward}. Wang~\citep{wang2021task} introduces a novel regularization-based method for causal dynamics learning, which explicitly identifies causal dependencies by regulating the number of variables used to predict each state variable. 
CDL~\citep{wang2022causal} takes an innovative approach by using conditional mutual information to compute causal relationships between different dimensions of states and actions. 
IFactor~\citep{liu2024learning} is a general framework to model four distinct categories of latent state variables, capturing various aspects of information. ACE~\citep{ji2024ace}, an off-policy actor-critic method, integrates causality-aware entropy regularization. Table~\ref{tab:causal_rl} provides a categorization of various causal RL methods, highlighting their focus on different reward-guided causal relationships. 
Existing approaches do not fully account for the causal relationships between both states and actions with rewards. Our goal is to explore these causal relationships from a reward-guided perspective to enhance sample efficiency across a broader range of tasks.
\vspace{-3mm}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

% \vspace{-1mm}
\subsection{Empowerment in RL}
\vspace{-2mm}
Empowerment, an information theory-based concept of intrinsic motivation, has emerged as a powerful paradigm for enhancing an agent's environmental controllability \citep{mohamed2015variational,klyubin2005empowerment,cao2024towards}. This framework conceptualizes actions and future states as information transmission channels, offering a novel perspective on agent-environment interactions. 
In RL, empowerment has been applied to uncover more controllable associations between states and actions, as well as to develop robust skill~\citep{salge2014empowerment,bharadhwaj2022information,choi2021variational, eysenbach2018diversity,leibfried2019unified,seitzer2021causal}. Empowerment, expressed as maximizing mutual information $\max_{\pi} I$, serves as a learning objective in various RL frameworks, providing intrinsic motivation for exploration and potentially yielding more efficient and generalizable policies. Our approach extends empowerment in RL by examining the influence of state, actions, and rewards through a causal lens, integrating causal understanding with empowerment to enhance exploration strategy and learning efficiency.
\vspace{-3mm}


\vspace{-1mm}
\subsection{Object-centric RL and Object-Oriented RL}
\vspace{-2mm}
Recent advances in object-centric representation learning focus on acquiring and leveraging structured, object-wise representations from high-dimensional observations. Foundational works include Slot Attention~\citep{locatello2020object} and AIR~\citep{eslami2016attend, kosiorek2018sequential}, establishing basis for this field. Subsequent follow-ups have worked on these concepts by employing state-of-the-art architectures, including DINO-based approaches~\cite{zadaianchuk2023objectcentric}, transformer-based models~\citep{wu2022slotformer}, diffusion models~\citep{jiang2023object}, and state-space models~\citep{jiang2024slot}. Notably, learning object-centric representations can enable compositional generalization across various domains, such as video and scene generation~\citep{wu2023slotdiffusion, wu2024neural}. Moreover, several theoretical studies have explored the mechanisms underlying compositional generalization and the causal identifiability~\citep{kori2024identifiable, brady2023provably, lachapelle2024additive}.

Object-centric representations have been effectively employed in world models to capture multi-object dynamics, as demonstrated by works~\citep{jiang2019scalor,lin2020improving,kossen2019structured}. Building on these object-centric world models, various studies use them in RL by better modeling complex object-centric structures in partially observable MDPs~\citep{kossen2019structured, mambelli2022compositional, feng2023learning, choi2024unsupervised}, identifying critical objects ~\citep{zadaianchuk2022self, park2021object}, and learning object-centric policies~\citep{zadaianchuk2020self, yuan2022sornet} and applications in robotic manipulation tasks~\citep{li2020towards, mitash2024scaling, haramati2024entity, li2024manipllm}, as well as in learning intrinsic or curiosity-driven policies based on objects and their interactions~\citep{watters2019cobra, hu2023elden, wang2024skild}. 

Another research direction explores object-oriented MDPs, with the homomorphic object-oriented world model being a notable example that leverages MDP homomorphism to model object dynamics and enable efficient planning through symmetric equivalence in MDPs~\citep{diuk2008object, scholz2014physics, wandzel2019multi, van2020plannable, rezaei2022continuous, zhao2022toward}, provides a powerful foundation for learning object-oriented MDPs and facilitates efficient planning~\citep{wolfe2006defining}.   Our work, which focuses on uncovering general causal relationships among components in MDPs and empowerment optimization, is orthogonal to object-centric RL. However, object-centric RL could provide useful abstract object-based variables that could be useful for causal structure learning in complex environments~\footnote{We provide a detailed discussion in Appendix~\ref{ocrl}.}.

\vspace{-2mm}
\section{Preliminaries}
\vspace{-2mm}
% \subsection{MDP with Causal Structures}
\subsection{Markov Decision Process}
\vspace{-1mm}
In RL, the agent-environment interaction is formalized as an MDP. The standard MDP is defined by the tuple $ \mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mu_0, r, \gamma \rangle $, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ represents the action space, $\mathcal{P}(s' | s, a)$ is the transition dynamics, $r(s, a)$ is the reward function, and $\mu_0$ is the distribution of the initial state $s_0$. The discount factor $\gamma \in [0, 1)$ is also included. The objective of RL is to learn a policy $\pi: \mathcal{S} \times \mathcal{A} \to [0, 1]$ that maximizes the expected discounted cumulative reward ${\eta _\mathcal{M}}(\pi) := \mathbb{E}_{s_0 \sim \mu_0, s_t \sim \mathcal{P}, a_t \sim \pi} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t}r(s_t, a_t)\right]$. 
\vspace{-1mm}
\subsection{Structural Causal Model}
\vspace{-1mm}
A Structural Causal Model (SCM)~\citep{pearl2009causality} is defined by a distribution over random variables, defined as $\mathcal{V}=\{s_t^1, \cdots, s_t^d, a_t^1, \cdots, a_t^n, r_t, s_{t+1}^1, \cdots, s_{t+1}^d \}$ and a Directed Acyclic Graph (DAG) $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with a conditional distribution $\mathcal{P}(v_i|\mathrm{PA}(v_i))$ for node $v_i \in \mathcal{V}$. Then the distribution can be specified as: 
\begin{equation}
    p(v_1, \dots, v_{|\mathcal{V}|})= \prod_{i=1}^{|\mathcal{V}|}p(v_i|\mathrm{PA}(v_i) ) ,
\end{equation}
where $\mathrm{PA}(v_i)$ is the set of parents of the node $v_i$ in the graph $\mathcal{G}$. 
\paragraph{Causal Structures in MDP}
We use a factored MDP~\citep{kearns1999efficient,guestrin2003efficient, guestrin2001multiagent} to model the MDP and the underlying causal structures between states, actions, and rewards. In the factored MDP, nodes represent system variables (rewards and different dimensions of the states and actions), while the edges denote their relationships within the MDP. We employ causal discovery methods to learn the structures of $\mathcal{G}$. 

We can identify the graph structure in $\mathcal{G}$, which can be represented as the adjacency matrix $M$. To integrate such relationships in MDP, we explicitly encode the causal mask over variables into the reward function. Hence, the reward function in MDP with the causal structure is defined as follows: 
\begin{equation}
r_t = R(M^{s \to r} \odot s_t, M^{a \to r} \odot a_t, \epsilon_{r,t})
\label{eq:gen}
\end{equation}
where \( \odot \) denotes the element-wise product. $ M^{s \to r} \in \mathbb{R}^{|s|\times 1}$ and $ M^{a \to r} \in \mathbb{R}^{|a|\times 1}$ are the adjacency matrices indicating the influence of current states and actions on the reward, respectively, and \( \epsilon_{r,t} \) represents i.i.d. Gaussian noise. Under the Markov condition and faithfulness assumption~\citep{pearl2009causality,spirtes2001causation}, the structural vectors are identifiable. The detailed assumptions and propositions can be found in Appendix~\ref{sec:app_ass}. 
In this work, our objective is to discover and leverage these two causal matrices to prioritize causal information for efficient RL. 
\vspace{-2mm}
\subsection{Empowerment in RL}
\vspace{-2mm}
Empowerment quantifies an agent's capacity to influence its environment and perceive the consequences of its actions~\citep{klyubin2005empowerment,bharadhwaj2022information,jung2011empowerment}. In our framework, the empowerment is defined as the mutual information between the agent state ${s}_{t+1}$ and action ${a}_t$, conditioned on the present state $s_t$ and causal mask $M$, as shown follows: 
\begin{equation}
    \mathcal{E} := \max_{\pi} \mathcal{I}(a_{t}; s_{t+1} \mid s_t, M),
\end{equation}
where $\mathcal{E}$ denotes the channel capacity from actions to states. Unlike \citep{cao2024towards}, which focuses on action-to-state empowerment effects, we leverage causal understanding and more accurate entropy calculation to analyze state-to-action influences, facilitating the development of more controllable behavioral policies. 

\vspace{-2mm}
\section{Causal Information Prioritization}
\label{sec:app}
\vspace{-2mm}
In this section, we introduce the proposed framework $\texttt{\textbf{CIP}}$, which implements causal information prioritization based on the causal relationships between states, actions, and rewards (as shown in Figure~\ref{fig:framework}). First, we train a structural model based on the causal discovery method, DirectLiNGAM~\citep{shimizu2011directlingam} using collected trajectories to obtain a causal matrix $M^{s \to r}$. Utilizing this matrix, $\texttt{\textbf{CIP}}$ executes the swapping of causally independent state features, generating synthetic transitions (Section~\ref{sec:aug}). This process of swapping independent state information accentuates causally dependent state information, enabling focused learning on critical state transitions. 
Subsequently, $\texttt{\textbf{CIP}}$ constructs another structural model to get a weight matrix $M^{a \to r}$ that incorporates actions and rewards to reweight actions (Section~\ref{sec:emp}). Furthermore, $\texttt{\textbf{CIP}}$ integrates a causality-aware empowerment term $\mathcal{E}_{\pi_c}(s)$ combined with causally weighted actions into the learning objective to promote efficient exploration. This integration encourages the agent's policy $\pi_c$ to prioritize actions with high causal influence, thereby enhancing its goal-achievement capabilities.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/framework.pdf}
    \caption{(a) Underlying causal 
 structure of $\texttt{\textbf{CIP}}$. (b) The whole learning process of $\texttt{\textbf{CIP}}$ includes counterfactual data augmentation, causal action reweight and causal action empowerment.}
    \label{fig:framework}
    \vspace{-3mm}
\end{figure}

\vspace{-1mm}
\subsection{Counterfactual Data Augmentation}
\label{sec:aug}
\vspace{-1mm}
% \paragraph{causal matrix learning}
To discover the causal relationships between states and rewards, we initially collect trajectories to train a structural model by the DirectLiNGAM method, denoted as $q_\mu^s$, to obtain the causal matrix $M^{s \to r}$. Subsequently, we infer the local factorization, which is utilized to generate counterfactual transitions. 
For each state $s$ in the trajectories, we compute the uncontrollable set, defined as the set of variables in $s$ for which the agent has no causal influence on rewards: 
\begin{equation}
\mathcal{U}_s = \{ s^i \mid M^{s \to r} \cdot(s^i_t, r_t) < \theta; i \in [1,N] \},
\label{cda}
\end{equation}
where $\theta$ is a fixed threshold and $N$ is the dimension of the state space. The set $\mathcal{U}_s$ encompasses all dimensional state variables for which the causal relationship $s^i_t \to r_t$ does not exist in the causal matrix of states and rewards.
Utilizing the learned causal matrix $M^{s \to r}$, we partition all state variables in the factored MDP into controllable and uncontrollable sets. These uncontrollable sets are then leveraged for counterfactual data augmentation, thereby prioritizing the causally-informed state information to improve learning efficiency. 

To generate counterfactual samples, we perform a swap of variables that fall under the uncontrollable category (i.e., in set $\mathcal{U}_s$) sampled from the collect trajectories. Specifically, given two transitions $(s_t, a_t, s_{t+1}, r_t)$ and $(\hat{s}_j, \hat{a}_j, \hat{s}_{j+1}, \hat{r_j})$ sampled from trajectories, which share at least one uncontrollable sub-graph structure (i.e., $\mathcal{U}_s \cap \mathcal{U}_{\hat{s}} \neq \emptyset$), we construct a counterfactual transition $(\tilde{s_t}, \tilde{a_t}, \tilde{s}_{t+1}, \tilde{r_t})$ by swapping the irrelevant state variables $(s^i_t, s^i_{t+1})$ with $(\hat{s}^i_j, \hat{s}^{i}_{j+1})$ for each $i \in \mathcal{U}_s \cap \mathcal{U}_{\hat{s}}$. 
The augmented transitions will be added to the training data for causal reasoning during subsequent action empowerment, thus eliminating the need for additional environment interactions to prioritize causal information. Furthermore, we also consider directly using controllable state sets combined with causal action empowerment to replace counterfactual data augmentation for policy learning. The comparative experimental results validating this approach are presented in Appendix~\ref{sec:appendix_cda_replace}. 

\vspace{-1mm}
\subsection{Causal Action Prioritization Through Empowerment}
\label{sec:emp}
\vspace{-1mm}

\paragraph{Causal action reweight} 
Having analyzed the causal relationships between states and rewards to achieve efficient data augmentation, in this section, we further discover the causal relationships between actions and rewards to prioritize causally-informed decision-making behaviors. 
$\texttt{\textbf{CIP}}$ constructs a reward-guided structural model, incorporating states (including augmented states), actions, and rewards. This model forms the foundation for action prioritization in policy learning, enabling action reweighting based on causality. Leveraging this structural model to delineate relationships between policy decisions and rewards, we evaluate the causal impact of different actions on reward outcomes. In this way, the agent focuses on pivotal actions with demonstrable causal links to desired reward outcomes, potentially accelerating learning and optimizing performance in complex environments. 

Specifically, in $\texttt{\textbf{CIP}}$, we employ DirectLiNGAM method to train a causal structural model $q^a_{\omega}$, which yields a weight matrix $M^{a \to r}$, delineating the relationships between actions and rewards, conditioned on the states. 
For a given set of actions $(a^1_t, a^2_t, a^3_t, \dots)$, we utilize the weight matrix $M^{a \to r}$ to reweight them as $(\omega_1 a^1_t, \omega_2 a^2_t, \omega_3 a^3_t, \dots)$, where $\omega$ represents the causal weights derived from the matrix $M^{a \to r}$. 
By leveraging this causal structure, we can prioritize the most pivotal actions, potentially leading to more efficient policy exploration and targeted policy improvements. 

\paragraph{Causal action empowerment} 
Based on the learned causal structure, we propose the causal action empowerment to incorporate the reweighted actions into the learning objective for efficient exploration in a controllable manner. 
To this end, we design a causality-aware empowerment term $\mathcal{E}_{\pi_c}(s)$ for policy optimization. We maximize the empowerment gain of the policy $\pi_c$, where $\pi_c$ incorporates the learned causal structure. This approach allows us to quantify and maximize the empowerment that can be achieved by explicitly considering causal relationships, thereby bridging the gap between causal reasoning and empowerment. 

We denote the empowerment of the causal policy as $\mathcal{E}_{\pi_c}(s) = \max_a  \mathcal{I}\left(a_t; s_{t+1} \mid s_t; M\right)$. 
We then formulate the following objective empowerment function: 
\begin{equation}
\begin{aligned}
   \mathcal{E}_{\pi_c}(s) & = \max_a  \mathcal{I}\left(a_t; s_{t+1} \mid s_t; M\right) \\
   & = \max_{a_t \sim \pi_c(\cdot|s)} \mathcal{H}(\pi_c(a_t|s_t)) - \mathcal{H}(\pi_c(a_t|s_t; s_{t+1})), 
    \end{aligned}
    \label{eq:emp_comp}
\end{equation}
where $\pi_c$ is the policy under the causal weighted matrix $M^{a \to r}$. 
The first entropy term $\mathcal{H}(\pi_c(a_t|s_t))$ promotes action diversity within the constraints of the causal structure. It encourages the agent to explore a wide range of actions that are causally informed, while the second entropy term $- \mathcal{H}(\pi_c(a_t|s_t; s_{t+1}))$ enhances the action predictability in state transitions. It encourages the selection of actions that lead to predictable outcomes, given the current and subsequent states, thereby promoting controlled and goal-oriented behaviors. 
We train an inverse dynamics model to represent the policy $\pi_c(\cdot|s_t;s_{t+1})$. 
The detailed derivation proceeds as follows: 
\begin{equation}
    \mathcal{H}(\pi_c(\cdot|s_t)) = -\mathbb{E}_{a_t \in  \mathcal{A}} \left[
    \sum_{i=1}^{d_\mathcal{A}} {M^{a^i \to r}} \odot \pi_c(a_t^i|s_t)\log \pi(a_t^i|s_t) 
    \right], 
\end{equation}
    and 
\begin{equation}
    \mathcal{H}(\pi_c(\cdot|s_t;s_{t+1})) = -\mathbb{E}_{a_t \in  \mathcal{A}} \left[
    \sum_{i=1}^{d_\mathcal{A}} {M^{a^i \to r}} \odot \pi_c(a_t^i|s_t;s_{t+1})\log \pi(a_t^i|s_t;s_{t+1}) 
    \right],
\end{equation}
where $d_\mathcal{A}$ is the dimension of the action space. Hence, the learning objective of the causal action empowerment can be defined as follows: 
\begin{equation}
    \begin{aligned}
   \mathcal{E}_{\pi_c}(s)  &  = \max_{a_t \sim \pi_c(\cdot|s)} \mathcal{H}(\pi_c(a_t|s_t)) - \mathcal{H}(\pi_c(a_t|s_t; s_{t+1}))
    \\ & = \max_{a_t \sim \pi_c(\cdot|s)} \mathbb{E}_{\pi_c(a_t|s_t) p_{\pi_c}( a_t|s_t,s_{t+1})} \left[\log \mathcal{P}_{\phi_c}(a_t \mid s_t, s_{t+1}; M) - \log \mathcal{P}_{\pi_c}(a_{t}|s_t;M) \right],
    \end{aligned}
    \label{eq:emp_comp}
\end{equation}
where $ \mathcal{P}_{\pi_c}(a_t|s_t; M)$ is the action distribution given current state of policy $\pi_c$ with the causal structure, which can be denoted as $\pi_c(a_t|s_t)$. $\mathcal{P}_{\phi_c}(a_t|s_{t+1},s_t; M)$ represents an inverse dynamics model trained on the collected transitions of state variables. 
Hence, we update the target policy $\pi_c$ by maximizing the empowerment objective function derived in Eq.~\ref{eq:emp_comp}.

Adhering to the maximum entropy paradigm~\citep{haarnoja2018soft},
we calculate $\mathcal{E}_{\pi_c}(s)$ for maximization instead of standard entropy, thus prioritizing exploration of pivotal actions that are more likely to have significant causal effects on the reward. This targeted exploration strategy has the potential to accelerate learning by focusing on the most influential actions in current controllable states. 
Based on the causality-aware empowerment, the Q-value for policy $\pi_c$ could be computed iteratively by applying a modified Bellman operator $\mathcal{T}^{\pi}_c$ with $\mathcal{E}_{\pi_c}(s)$ term as stated below: 
\begin{equation}
\fontsize{8.5}{1}
\begin{aligned}
    \mathcal{T}^{\pi}_c Q(s_t,a_t) & =r(s_t,a_t)+\gamma \mathbb{E}_{s_{t+1}\sim \mathcal{P}} \left[
    \mathbb{E}_{a_t \sim \pi_c}\left[Q(s_{t+1},a_{t+1})
    +\alpha \mathcal{E}_{\phi_c}(s)
    \right]
    \right] \\
    & =r(s_t,a_t)+\gamma \mathbb{E}_{s_{t+1}\sim \mathcal{P}} \left[
    \mathbb{E}_{a_t \sim \pi_c}\left[Q(s_{t+1},a_{t+1})
    +\alpha (\mathcal{H}(\pi_c(a_t|s_t)) - \mathcal{H}(\pi_c(a_t|s_t; s_{t+1})))
    \right]
    \right]. \\
\end{aligned}
\label{eq:alpha}
\end{equation}Hence, we integrate the causality-aware empowerment term into the policy optimization objective function, $\hat{\eta}_\mathcal{M}(\pi_c)= 
\mathbb{E}_{s_0 \sim \mu_0, s_t \sim \mathcal{P}, a_t \sim \pi_c} \left[\sum\nolimits_{t = 0}^\infty {\gamma^t} (r(s_t, a_t) +\alpha \mathcal{E}_{\pi_c}(s))\right]
$. 

In summary, $\texttt{\textbf{CIP}}$ harnesses empowerment to integrate the causal understanding into decision-making. By maximizing the empowerment gain of the causally-informed policy, we guide the agent to prioritize actions that align with the environment's underlying causal relationships. This approach enhances the agent's exploration efficiency, focusing on actions with meaningful causal impacts and correlated with desired outcomes. Algorithm~\ref{alg:algorithm1} illustrates the complete $\texttt{\textbf{CIP}}$ pipeline. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/environment.pdf}
    \caption{The $36$ experimental tasks in $5$ continuous control environments} 
    \label{fig:environment}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/main_result/main.pdf}
    \caption{Experimental results with normalized score across all $36$ tasks in $5$ environments.}
    \label{fig:main_results}
    \vspace{-3mm}
\end{figure}
% \clearpage
\vspace{-3mm}
\section{Experiments}
\vspace{-3mm}
Our experiments aim to address the following questions:
(i) How does the performance of $\texttt{\textbf{CIP}}$ compare to other RL approaches in diverse continuous control tasks, including manipulation and locomotion with sparse rewards, high-dimensional action spaces, and pixel-based challenges?
(ii) Can $\texttt{\textbf{CIP}}$, through data augmentation and empowerment, improve sample efficiency and learn reliable policies?
(iii) What are the effects of the components and hyperparameters in $\texttt{\textbf{CIP}}$? 



\vspace{-3mm}
\subsection{Experimental setup} 
\vspace{-1mm}
\textbf{Environments.} We evaluate $\texttt{\textbf{CIP}}$ on $5$ continuous control environments, including MuJoCo~\citep{todorov2012mujoco}, DMControl~\citep{tassa2018deepmind}, Meta-World~\citep{yu2020meta}, Adroit Hand~\citep{rajeswaran2017learning}, and sparse reward setting environments in Meta-World. 
This comprehensive evaluation encompasses 36 tasks, spanning both locomotion and manipulation skill learning, as illustrated in Figure~\ref{fig:environment}. We also conduct experiments in $4$ pixel-based tasks of the DMControl and Cartpole environment as shown in Figure~\ref{fig:appendix_pixel_env}.
Our experimental tasks incorporate a wide range of challenges, including high-dimensional state and action spaces, sparse reward settings, pixel-based scenarios, and locomotion. 
For extensive experimental settings, please refer to Appendix~\ref{sec:experimental_setup_appendix}. 

\textbf{Baselines.} 
We compare $\texttt{\textbf{CIP}}$ with three popular RL baselines across all $36$ tasks and against IFactor~\citep{liu2024learning} in $3$ pixel-based tasks: 
(1) SAC~\citep{haarnoja2018soft}, an off-policy actor-critic algorithm featuring maximum entropy regularization. 
(2) ACE~\citep{ji2024ace}, a method employing causality-aware entropy regularization. 
(3) BAC~\citep{ji2023seizing}. a method that balances sufficient exploitation of past successes with exploration optimism. 
(4) IFactor~\citep{liu2024learning}. a causal framework modeling four distinct categories of latent state variables for pixel-based tasks. 
To ensure robustness and statistical significance, we conduct each experiment using 4 random seeds. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/meta/meta_1_success.pdf}
    \includegraphics[width=1\linewidth]{figs/sparse/sparse_success.pdf}
    \caption{Experimental results of $8$ manipulation skill learning tasks in Meta-World and adroit hand environments including sparse reward settings. For all tasks results, please refer to Appendix~\ref{sec:full_results_appendix}.}
    \label{fig:manipulation}
    \vspace{-2mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.24\textwidth]{figs/pixel/cheetah.pdf}
    \includegraphics[width=0.24\textwidth]{figs/pixel/reacher.pdf}
    \includegraphics[width=0.24\textwidth]{figs/pixel/walker.pdf}
    \includegraphics[width=0.24\textwidth]{rebuttal_fig/cartpole/cartpole.pdf}
    \caption{Experimental results of $4$ pixel-based tasks in DMControl and Cartpole environments.}
    \label{fig:appendix_pixel}
    \vspace{-3mm}
\end{figure}

\vspace{-3mm}
\subsection{Main Results}
\vspace{-3mm}
Figure~\ref{fig:main_results} presents the normalized scores of $\texttt{\textbf{CIP}}$ compare to other methods across $36$ tasks in $5$ environments. In $17$ Meta-World robot-arm tasks, $\texttt{\textbf{CIP}}$ achieves a near-perfect score of $100$, showcasing its exceptional performance in manipulation tasks. For locomotion tasks in DMControl and MuJoCo, $\texttt{\textbf{CIP}}$ consistently attains scores exceeding $80$, indicating robust performance across diverse locomotion challenges. Notably, $\texttt{\textbf{CIP}}$ exhibits significant performance improvements in challenging scenarios, such as adroit hand manipulation and $3$ tasks with the sparse reward setting. 
These results underscore the effectiveness in tackling complex, high-dimensional control problems. In next sections, we present a comprehensive analysis of $\texttt{\textbf{CIP}}$'s performance across diverse tasks. 

\textbf{Robot-arm manipulation.} 
Figure~\ref{fig:manipulation} presents the success rates across $7$ Meta-World robot-arm manipulation tasks including sparse reward settings. $\texttt{\textbf{CIP}}$ consistently outperforms all other methods across these tasks, demonstrating both faster policy learning and enhanced stability. 
In challenging tasks, such as disassemble, $\texttt{\textbf{CIP}}$ achieves an impressive 100\% success rate. 
The effectiveness of $\texttt{\textbf{CIP}}$ can be attributed to focus on causally relevant information within the state and action spaces. In sparse reward settings, the efficient extraction of causal state information and the prioritization of controllable actions enable effective task completion. 
By systematically eliminating noise from non-causal factors, $\texttt{\textbf{CIP}}$ allows the agent to construct a more controllable and efficient policy. 

\textbf{High-dimensional Adroit hand manipulation.}
To rigorously evaluate our method's efficacy in high-dimensional tasks, we conduct comparative experiments in the Adroit Hand environment of door open task. This challenging setup involves controlling a robotic hand with up to 28 actuated degrees of freedom ($\mathcal{A} \in \mathbb{R}^{28}$).
Figure~\ref{fig:manipulation} illustrates the success rates achieved across all methods. Notably, while the three other comparative methods fail to demonstrate significant progress on this challenging task, $\texttt{\textbf{CIP}}$ achieves a near $100\%$ success rate after $700k$ environment steps. 

\textbf{Locomotion.}
We further evaluate $\texttt{\textbf{CIP}}$ in another important category: locomotion. The part experimental results of average return in MuJoCo and DMControl environments are presented in Table~\ref{tab1:loco}. Learning curves are illustrated in Figure~\ref{fig:loco}. 
We observe that $\texttt{\textbf{CIP}}$ achieves the best performance in six tasks and sub-optimal in other tasks, and shows statistically significant improvements in 5 out of 8 tasks. Moreover, compared to the traditional method SAC, $\texttt{\textbf{CIP}}$ demonstrates significant performance improvements in more challenging tasks such as CheetahRun and Hopper. 
Compared to the causality-based method ACE, $\texttt{\textbf{CIP}}$ demonstrates improvements in all tasks. 
Overall, in locomotion tasks, $\texttt{\textbf{CIP}}$ achieves superior performance and attains high sample efficiency. \textcolor{black}{Detailed performance and statistical analyses are provided in Appendix~\ref{sec:full_results_appendix} and~\ref{sec:appendix_statis}. }
% \textcolor{MyDarkRed}

\begin{table}[t]
\footnotesize
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{2.1pt} % 设置列间距为4pt
\caption{The experimental results of average return in $8$ locomotion tasks. We bold the best scores, and underline second-best results, $\pm$ is the standard deviation, w/o represents without. \textcolor{black}{$\bullet$ indicates $\texttt{\textbf{CIP}}$ is statistically superior to compared method (pairwise \textit{t}-test at $95\%$ confidence interval).}} 
\label{tab1:loco}
\begin{tabular}{cccccccccc}
\toprule
\textbf{Method}        & \textbf{Ant}  & \textbf{HalfCheetah} & \textbf{Hopper} & \textbf{Walker2d} & \multicolumn{1}{c}{\textbf{\makecell{Cheetah \\ Run}}}
 & 
 \multicolumn{1}{c}{\textbf{\makecell{Hopper \\ Stand}}}
 & 
 \multicolumn{1}{c}{\textbf{\makecell{Quadruped \\ Walk}}}
 & 
 \multicolumn{1}{c}{\textbf{\makecell{Reacher \\ Hard}}}

  \\ 
\hline
\textbf{$\texttt{\textbf{CIP}}$}           & \underline{6418$\pm$81}    & \textbf{12594$\pm$210  }       & \textbf{2846$\pm$882}      & \textbf{5624$\pm$91}     & \textbf{893$\pm$12}        & \textbf{936$\pm$17}            & \underline{948$\pm$54}          & \textbf{991$\pm$11}              
\\
 % \multicolumn{1}{c}{\textbf{\makecell{CAP \\ w/o CS}}}
 \textbf{w/o Aug}
 & 6231$\pm$81    & \underline{12225$\pm$102}       & 2308$\pm$785      & 5294$\pm$41     & \underline{885$\pm$13}        & 931$\pm$22            & 945$\pm$35           & \underline{989$\pm$13}            
\\
% \multicolumn{1}{c}{\textbf{\makecell{CAP \\ w/o Prior}}}
\textbf{w/o Emp}
 & 6295$\pm$210          & 10986$\pm$572          & 2270$\pm$904           & \underline{5547$\pm$91}       & 876$\pm$21                & 785$\pm$114                  & 924$\pm$23                    & 971$\pm$13                     
\\ 
\hline
\textbf{SAC}           & 6062$\pm$105$\bullet$          & 10888$\pm$240$\bullet$                & 2266$\pm$981            & 5251$\pm$106              & 767$\pm$16$\bullet$                 & \textbf{936$\pm$8}         & 930$\pm$19 $\bullet$                   & 980$\pm$8$\bullet$         
\\
\textbf{BAC}           & \textbf{6511$\pm$30} & 10276$\pm$34$\bullet$                & 2263$\pm$1063$\bullet$   & 3316$\pm$702 $\bullet$             & 665$\pm$6$\bullet$                 & 932$\pm$4$\bullet$                  & \textbf{962$\pm$24}              & 974$\pm$16               
\\
\textbf{ACE}           & 5922$\pm$106 $\bullet$         & 9390$\pm$25$\bullet$                 & \underline{2312$\pm$673$\bullet$}            & 4922$\pm$96$\bullet$              & 863$\pm$23$\bullet$                 & 912$\pm$16                  & 933$\pm$57                    & 973$\pm$17               
\\ \bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}


\begin{figure}[h]
    \centering
     \includegraphics[width=1\linewidth]{figs/mujoco/mujoco_reward.pdf}
    \includegraphics[width=1\linewidth]{figs/dmc/dmcontrol_reward_1.pdf}
    \caption{Experimental results with average return across $8$ tasks in locomotion tasks.}
    \label{fig:loco}
    \vspace{-3mm}
\end{figure}

\textbf{Pixel-based task learning} 
To further validate the performance in pixel-based tasks, we use $3$ complex pixel-based DMControl tasks for evaluation, where video backgrounds serve as distractors. We apply the proposed counterfactual data augmentation and causal action empowerment to IFactor for comparison. 
As shown in Figure~\ref{fig:appendix_pixel}, \texttt{\textbf{CIP}} surpasses IFactor in terms of average return. These results underscore \texttt{\textbf{CIP}}'s efficacy in pixel-based tasks and its capacity to better overcome spurious correlations arising from video backgrounds, focusing on locomotion. \textcolor{black}{Moreover, the result of Cartpole task in Figure~\ref{fig:appendix_pixel} demonstrate the effectiveness in discrete action space environment.}
For visualization trajectories in pixel-based results, please refer to Appendix~\ref{sec:vis_pixel}. 

\vspace{-4mm}
\subsection{Analysis}
\vspace{-2mm}
\paragraph{Ablation study.}
We conduct ablation experiments involving $\texttt{\textbf{CIP}}$, $\texttt{\textbf{CIP}}$ without (w/o) counterfactual data augmentation (Aug), and $\texttt{\textbf{CIP}}$ w/o Empowerment (Emp). 
The results in $8$ locomotion tasks are shown in Table~\ref{tab1:loco}. And the learning curves of all tasks are depicted in Appendix~\ref{sec:pro_res_appendix}. 
The experimental results reveal that the variant without the empowerment learning objective performs poorly, underscoring the critical role of empowerment maximization in enhancing control capabilities. Additionally, $\texttt{\textbf{CIP}}$ without counterfactual data augmentation is less sample efficient than $\texttt{\textbf{CIP}}$, highlighting the importance of augmentation. 

\vspace{-3mm}
\paragraph{Reliability evaluation.}
We evaluate $\texttt{\textbf{CIP}}$'s reliability across 35 tasks in 4 environments, excluding the Adroit Hand door task due to $\texttt{\textbf{CIP}}$'s exceptional performance there. Figure \ref{fig:rel} illustrates the experimental results using the Optimality Gap metric~\citep{agarwal2021deep}. $\texttt{\textbf{CIP}}$ consistently achieves the lowest values across all tasks in four environments, with lower values indicating superior performance. This consistent excellence across diverse scenarios underscores the robustness and reliability of our proposed method. 


\begin{figure}[t]
    \centering
     \includegraphics[width=0.245\linewidth]{figs/robust/og_meta.pdf}
    \includegraphics[width=0.245\linewidth]{figs/robust/og_dm.pdf}
    \includegraphics[width=0.245\linewidth]{figs/robust/og_mujoco.pdf}
     \includegraphics[width=0.245\linewidth]{figs/robust/og_sparse.pdf}
    \caption{Experimental results of reliability evaluation by the metric Optimality Gap (lower values are better) on $4$ diverse environments across $35$ tasks. }
    \label{fig:rel}
    \vspace{-3mm}
\end{figure}



\vspace{-3mm}
\section{Conclusion}
\vspace{-3mm}
This study introduces an efficient RL framework, designed to enhance sample efficiency. This approach begins by counterfactual data augmentation using the causality between states and rewards, effectively mitigating interference from irrelevant states without additional environmental interactions. We then develop a reward-guided structural model that leverages causal awareness to prioritize causal actions through empowerment. 
We conduct extensive experiments across $39$ tasks spanning $5$ diverse continuous control environments which demonstrate the exceptional performance of our proposed method, showcasing its robustness and adaptability across challenging scenarios. 

\textbf{Limitation and Future Work}
\quad \textcolor{black}{The current limitations of our work are twofold. First, $\texttt{\textbf{CIP}}$ has not yet been extended to complex scenarios, such as real-world 3D robotics tasks. Potential approaches to address this limitation include leveraging object-centric models~\citep{wu2023slotdiffusion}, 3D perception models~\citep{wang2024rise}, and robotic foundation models~\citep{team2024octo, firoozi2023foundation} to construct essential variables for causal world modeling. Second, $\texttt{\textbf{CIP}}$ does not adequately consider non-stationarity and heterogeneity, which are critical challenges in causal discovery. Future work could integrate method designed to handle such complexities, such as CD-NOD~\citep{huang2020causal}.}

\vspace{-3mm}
\section*{Reproducibility Statement}
\vspace{-3mm}
We provide the core code of $\texttt{\textbf{CIP}}$ in the supplementary material. 
The implementation details are shown in Appendix~\ref{sec:experimental_setup_appendix}.

\section*{Acknowledgment}
This work was supported in part by the National Natural Science Foundation of China under Grant 62276128, Grant 62192783; in part by the Jiangsu Science and Technology Major Project BG2024031; in part by the Natural Science Foundation of Jiangsu Province under Grant BK20243051; the Fundamental Research Funds for the Central Universities(14380128); in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\input{appendix}


\end{document}
