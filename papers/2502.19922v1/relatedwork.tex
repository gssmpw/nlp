\section{Related Work}
\label{sec:related}
Class-incremental learning (CIL) addresses the challenge of training a model sequentially on a series of tasks, without access to previous or future data~\cite{icarl}. When training without any constraints, models fail to retain knowledge from previous tasks -- a problem known as \emph{catastrophic forgetting}~\cite{french1999catastrophic, empirical_catastrophic}. Usually, each incremental task contains a disjoint set of new classes, which increases the difficulty of discriminating between those which have not been learned together under the same task~\cite{marc_survey2}.
A key challenge in incremental learning lies in keeping the balance of the stability-plasticity dilemma~\cite{stability_plasticity}, critical for mitigating catastrophic forgetting while ensuring the adaptability of the model to new tasks.

Incremental learning approaches include: weight regularization~\cite{ewc, mas}, which preserves important weights by estimating their importance; knowledge distillation~\cite{lwf, hinton2015distilling}, which focuses on protecting task representations rather than weights; rehearsal~\cite{rolnick2019experience}, which replay stored exemplars from previous tasks; mask-based approaches~\cite{masana2020ternary}, which use task-specific masks to isolate parameters that can be updated; and dynamic network structures~\cite{plastil, ensembles}, which expand the model architecture by adding new or contracting existing modules for each task. In this work, we concentrate on weight-regularization, knowledge distillation and dynamic network structure-based methods. These are the approaches that work on task-agnostic scenarios (do not require a task-ID during inference) and promote privacy preservation (do not store samples).

\minisection{Incremental learning with repetition.}
In many practical applications (automated failure inspection, medical imaging, robotics), pattern repetition naturally arises, yet traditional CIL approaches assume that each class is encountered only once, imposing a strict no-repetition constraint~\cite{incremental_repetition}. This constraint focuses on the prevention of catastrophic forgetting but also diverges from real-world scenarios where classes may reappear or shift over time. To address this, Hemati et. al~\cite{hemati2023class, HEMATI2025106920} propose an extension to the class-incremental learning scenario which models the repetition of individual classes outside of a single task. Unlike joint incremental or rehearsal-based learning, this repetition is innate to the learning scenario and cannot be adjusted. This emphasizes an experience-based scenario~\cite{experience_based_il}, which favours shorter training tasks that can sometimes only cover a part of the class distribution. Moreover, covering scenarios that lie between the classic offline incremental and the online ones.

\begin{figure}[t]
    \centering
    \vspace{1em}
    \includegraphics[width=0.95\linewidth]{figures/horde_overview_concat_v2.png}
    \caption{Overview of our proposed method (Horde). Each data sample is processed by an ensemble of independent feature extractors. The features from all extractors are concatenated before being passed into a unified head that can accommodate the dynamic input size through pseudo-feature projection.}
    \label{fig:horde_overview}
\end{figure}

Class-incremental learning with repetition has received increased interest in the research community, being a central element in the challenge tracks of the last two CLVISION challenge tracks at CVPR 2023 and 2024~\cite{HEMATI2025106920, clvision_challenge_2024}. In the 2023 edition, we competed with a base variant of our proposed method, although without elements for controlling ensemble growth (see \cref{sec:ensemble_fe}), self-supervision (see \cref{sec:abl_fe_training}) or applicability to variable network architectures.

\minisection{Class prototypes and pseudo-features.}
To enforce stability and alleviate class-recency biases in the classifier~\cite{marc_survey1}, Exemplar-Free Class Incremental \mbox{Learning~(EFCIL)} methods~\cite{il2a, pass, praka, ssre, fetril} utilize class prototypes to simulate unavailable classes. These prototypes capture statistical properties of embedding representations of each class, which are usually modeled as a multivariate Gaussian distribution~\cite{pass, il2a, praka}. Specifically, the statistics typically include the mean and covariance of feature representations for each class, allowing to generate pseudo-features when class data is not available. To extract representations, the neural network is divided into two modules. A feature extractor~(FE) that projects the input samples into their corresponding embedding representation; and a classifier head that uses these embeddings to solve the classification task.
Therefore, prototype-based methods can generate embeddings even when no samples from past classes are available during subsequent tasks by sampling the stored distributions of each class.
The sampled embedding representations are rehearsed alongside the current task data, thus promoting stability and mitigating class-recency bias. However, in order to maintain valid approximations of class distributions, the feature extractor needs to be either frozen or heavily regularized to prevent changes or drifts in the extracted features. Unlike rehearsal-based approaches, the use of prototypes does not violate data privacy due to the non-linearly projected representation in the embedding space~\cite{survey_class_incremental, pass}.
 
\minisection{Feature translation.} 
Instead of sampling the distribution approximated by class prototypes, FeTrIL~\cite{fetril} proposes to translate the features of available data classes to unavailable ones directly. Given a feature extractor $\mathbf{f}(x; \theta)$ being trained on current data $\{(x_{i},y_{i})\}$, its output embedding $F$ is efficiently translated from one of the current classes to the desired previously learned class $c\in \mathcal{Y}$ as
\begin{equation}
\hat{F}_{c} = \mathbf{f}(\bm{x}_{i}; \theta) + \mu_{c} - \mu_{\bm{y}_{i}}\ ,
\label{eq:fetril-proj}
\end{equation}

\noindent where $\mu_{c}$ and $\mu_{\bm{y}_{i}}$ represent the means of the old and current classes, respectively. The feature translation modifies the classifier, however, the feature extractor is requires to be frozen after the initial training so that the class means can be reliably extracted. This limits the continual learning process as the initial task constrains the diversity and robustness of the features that can be learned for new classes~\cite{belouadah2021comprehensive, fetril}. In our proposed approach, we relax this restriction by allowing an ensemble of smaller feature extractors to be learned. This allows for unknown class prototypes to be estimated through pseudo-feature projection until the repetition of classes allows for an accurate extraction of class prototypes.