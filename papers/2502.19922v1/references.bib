@String(CVPR  = {Proc. CVPR})
@String(ICCV  = {Proc. ICCV})
@String(ECCV  = {Proc. ECCV})
@String(NIPS  = {Proc. NeurIPS})
@String(ICPR  = {Proc. ICPR})
@String(BMVC  =	{Proc. BMVC})
@String(ICASSP=	{Proc. ICASSP})
@String(ICIP  = {Proc. ICIP})
@String(ACCV  = {Proc. ACCV})
@String(ICLR  = {Proc. ICLR})
@String(IJCAI = {Proc. IJCAI})
@String(AAAI = {Proc. AAAI})
@String(CVPRW= {Proc. CVPRW})
@String(CSVT = {IEEE TCSVT})

@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(PR = {PR})


@misc{Authors14,
 author = {Lastname, Firstname},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Lastname, Firstname},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {Alpher, Firstname},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {Alpher, Firstname and Fotheringham-Smythe, Firstname},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {Alpher, Firstname and Fotheringham-Smythe, Firstname and Gamow, Firstname Middle},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {Alpher, Firstname and Gamow, Firstname Middle},
title = {Can models frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Datasets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{mnist,
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine}, 
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]}, 
  year={2012},
  volume={29},
  number={6},
  pages={141-142},
  doi={10.1109/MSP.2012.2211477}}


@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{tiny_imagenet,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}


@article{caltech,
  title={Caltech-256 object category dataset},
  author={Griffin, Gregory and Holub, Alex and Perona, Pietro},
  year={2007},
  publisher={California Institute of Technology}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% General Computer Vision Papers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{lenet,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@InProceedings{vgg,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}


@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{nlp_transformer_attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{
vision_transformer,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}


@article{cv_trend,
  title={Deep learning—a first meta-survey of selected reviews across scientific disciplines, their commonalities, challenges and research impact},
  author={Egger, Jan and Pepe, Antonio and Gsaxner, Christina and Jin, Yuan and Li, Jianning and Kern, Roman},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e773},
  year={2021},
  publisher={PeerJ Inc.}
}

@inproceedings{revconv,
  title={Reversible Column Networks},
  author={Cai, Yuxuan and Zhou, Yizhuang and Han, Qi and Sun, Jianjian and Kong, Xiangwen and Li, Jun and Zhang, Xiangyu},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=Oc2vlWU0jFY}
}

@inproceedings{convnext,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{swintransformer,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Important CIL Papers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{marc_survey1,
  author={Masana, Marc and Liu, Xialei and Twardowski, Bartłomiej and Menta, Mikel and Bagdanov, Andrew D. and van de Weijer, Joost},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Class-Incremental Learning: Survey and Performance Evaluation on Image Classification}, 
  year={2023},
  volume={45},
  number={5},
  pages={5513-5533},
  doi={10.1109/TPAMI.2022.3213473}
}

@ARTICLE{marc_survey2,
  author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Aleš and Slabaugh, Gregory and Tuytelaars, Tinne},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Continual Learning Survey: Defying Forgetting in Classification Tasks}, 
  year={2022},
  volume={44},
  number={7},
  pages={3366-3385},
  doi={10.1109/TPAMI.2021.3057446}
}

@ARTICLE{stability_plasticity,
AUTHOR={Mermillod, Martial and Bugaiska, Aurélia and BONIN, Patrick},   
TITLE={The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects},      
JOURNAL={Frontiers in Psychology},      
VOLUME={4},           
YEAR={2013},      
URL={https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00504},       
DOI={10.3389/fpsyg.2013.00504},      
ISSN={1664-1078}   
}

@article{goodfellow2013empirical,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2014}
}

@article{hemati2023class,
  title={Class-Incremental Learning with Repetition},
  author={Hemati, Hamed and Cossu, Andrea and Carta, Antonio and Hurtado, Julio and Pellegrini, Lorenzo and Bacciu, Davide and Lomonaco, Vincenzo and Borth, Damian},
  journal={arXiv preprint arXiv:2301.11396},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Normal Benchmarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{ewc,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@inproceedings{liu2018rotate,
  title={Rotate your networks: Better weight consolidation and less catastrophic forgetting},
  author={Liu, Xialei and Masana, Marc and Herranz, Luis and Van de Weijer, Joost and Lopez, Antonio M and Bagdanov, Andrew D},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)},
  pages={2262--2268},
  year={2018},
  organization={IEEE}
}

@article{lwf,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}

@inproceedings{icarl,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}

@inproceedings{mas,
  title={Memory aware synapses: Learning what (not) to forget},
  author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={139--154},
  year={2018}
}

@inproceedings{bic,
  title={Large scale incremental learning},
  author={Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={374--382},
  year={2019}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SotA Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@InProceedings{fetril,
    author    = {Petit, Gr\'egoire and Popescu, Adrian and Schindler, Hugo and Picard, David and Delezoide, Bertrand},
    title     = {FeTrIL: Feature Translation for Exemplar-Free Class-Incremental Learning},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {1},
    year      = {2023},
    pages     = {3911-3920}
}

@InProceedings{plastil,
    author    = {Petit, Gr\'egoire and Popescu, Adrian and Belouadah, Eden  and Picard, David and Delezoide, Bertrand},
    title     = {PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning},
    booktitle = {Second Conference on Lifelong Learning Agents (CoLLAs)},
    month     = {8},
    year      = {2023}
}

@article{il2a,
  title={Class-Incremental Learning via Dual Augmentation},
  author={Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-lin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14306--14318},
  year={2021}
}

@InProceedings{pass,
    author    = {Zhu, Fei and Zhang, Xu-Yao and Wang, Chuang and Yin, Fei and Liu, Cheng-Lin},
    title     = {Prototype Augmentation and Self-Supervision for Incremental Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {6},
    year      = {2021},
    pages     = {5871-5880}
}

@inproceedings{weigth_align,
  title={Maintaining discrimination and fairness in class incremental learning},
  author={Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shu-Tao},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13208--13217},
  year={2020}
}

@inproceedings{ssre,
  title={Self-sustaining representation expansion for non-exemplar class-incremental learning},
  author={Zhu, Kai and Zhai, Wei and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9296--9305},
  year={2022}
}

@inproceedings{praka,
  title={Prototype Reminiscence and Augmented Asymmetric Knowledge Aggregation for Non-Exemplar Class-In\-cremental Learning},
  author={Shi, Wuxuan and Ye, Mang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1772--1781},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Other references for context or completeness
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@inproceedings{masana2020ternary,
  title={Ternary Feature Masks: continual learning without any forgetting},
  author={Masana, Marc and Tuytelaars, Tinne and van de Weijer, Joost},
  booktitle={Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  year={2021}
}

@inproceedings{hardnegativepair,
  author       = {Joshua David Robinson and
                  Ching{-}Yao Chuang and
                  Suvrit Sra and
                  Stefanie Jegelka},
  title        = {Contrastive Learning with Hard Negative Samples},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=CR1XOQ0UTh-},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/RobinsonCSJ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{temperature_scaling,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{batchnorm,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@inproceedings{autoaugment,
  title={Autoaugment: Learning augmentation strategies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={113--123},
  year={2019}
}

@inproceedings{trivialaugment,
  title={Trivialaugment: Tuning-free yet state-of-the-art data augmentation},
  author={M{\"u}ller, Samuel G and Hutter, Frank},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={774--782},
  year={2021}
}

@article{mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}


@book{understanding_deep_learning,
 author = "Simon J.D. Prince",
 title = "Understanding Deep Learning",
 publisher = "MIT Press",
 year = 2023,
 url = "http://udlbook.com"
}

@inproceedings{xavier_init,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{kaiming_he_init,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{learning_rate_1,
  title={An empirical study of learning rates in deep neural networks for speech recognition},
  author={Senior, Andrew and Heigold, Georg and Ranzato, Marc'Aurelio and Yang, Ke},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6724--6728},
  year={2013},
  organization={IEEE}
}

@article{learning_rate2,
title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimizationb},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {1},
pages = {26-40},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@inproceedings{cyclic_lr,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE winter conference on applications of computer vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@inproceedings{
batch_size_regularisation,
title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=H1oyRlYgg}
}


@article{ensembles,
  title={Ensemble deep learning: A review},
  author={Ganaie, Mudasir A and Hu, Minghui and Malik, AK and Tanveer, M and Suganthan, PN},
  journal={Engineering Applications of Artificial Intelligence},
  volume={115},
  pages={105151},
  year={2022},
  publisher={Elsevier}
}

@article{adam_optimizer,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}

@article{medical_il,
  title={Distributed weight consolidation: a brain segmentation case study},
  author={McClure, Patrick and Zheng, Charles Y and Kaczmarzyk, Jakub and Rogers-Lee, John and Ghosh, Satra and Nielson, Dylan and Bandettini, Peter A and Pereira, Francisco},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{robotics_il,
  title={Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges},
  author={Lesort, Timoth{\'e}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and D{\'\i}az-Rodr{\'\i}guez, Natalia},
  journal={Information fusion},
  volume={58},
  pages={52--68},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{edge_il,
  title={RILOD: Near real-time incremental learning for object detection at the edge},
  author={Li, Dawei and Tasci, Serafettin and Ghosh, Shalini and Zhu, Jingwen and Zhang, Junting and Heck, Larry},
  booktitle={Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
  pages={113--126},
  year={2019}
}

@misc{carbon_il,
  title={Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author={Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  howpublished={ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
  month={July},
  note={arXiv:2007.03051},
  year={2020}}

@article{gpt3_paper,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{empirical_catastrophic,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@article{incremental_repetition,
  title={Is class-incremental enough for continual learning?},
  author={Cossu, Andrea and Graffieti, Gabriele and Pellegrini, Lorenzo and Maltoni, Davide and Bacciu, Davide and Carta, Antonio and Lomonaco, Vincenzo},
  journal={Frontiers in Artificial Intelligence},
  volume={5},
  pages={829842},
  year={2022},
  publisher={Frontiers Media SA}
}

@misc{adoption_dataset,
  title={Learning without forgetting In European Conference on Computer Vision},
  author={Li, Zhizhong and Hoiem, Derek},
  year={2016},
  publisher={Springer}
}


@article{experience_based_il,
  title={Three scenarios for continual learning},
  author={Van de Ven, Gido M and Tolias, Andreas S},
  journal={arXiv preprint arXiv:1904.07734},
  year={2019}
}

@article{ISDA,
  title={Regularizing deep networks with semantic data augmentation},
  author={Wang, Yulin and Huang, Gao and Song, Shiji and Pan, Xuran and Xia, Yitong and Wu, Cheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={7},
  pages={3733--3748},
  year={2021},
  publisher={IEEE}
}

@article{augmix,
  title={{AugMix}: A Simple Data Processing Method to Improve Robustness and Uncertainty},
  author={Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{herding,
  title={Herding dynamical weights to learn},
  author={Welling, Max},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={1121--1128},
  year={2009}
}


@InProceedings{activation_func1,
  title = 	 {Noisy Activation Functions},
  author = 	 {Gulcehre, Caglar and Moczulski, Marcin and Denil, Misha and Bengio, Yoshua},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {3059--3068},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {6},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gulcehre16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gulcehre16.html},
  abstract = 	 {Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradients. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to explore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps optimization in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.}
}

@article{activation_func2,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{survey_dl_theory,
  title={A state-of-the-art survey on deep learning theory and architectures},
  author={Alom, Md Zahangir and Taha, Tarek M and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and Van Essen, Brian C and Awwal, Abdul AS and Asari, Vijayan K},
  journal={electronics},
  volume={8},
  number={3},
  pages={292},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}


@InProceedings{train_val_test_1,
  title = 	 {Generalization Guarantees for Neural Architecture Search with Train-Validation Split},
  author =       {Oymak, Samet and Li, Mingchen and Soltanolkotabi, Mahdi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8291--8301},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/oymak21a/oymak21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/oymak21a.html},
  abstract = 	 {Neural Architecture Search (NAS) is a popular method for automatically designing optimized deep-learning architectures. NAS methods commonly use bilevel optimization where one optimizes the weights over the training data (lower-level problem) and hyperparameters - such as the architecture - over the validation data (upper-level problem). This paper explores the statistical aspects of such problems with train-validation splits. In practice, the lower-level problem is often overparameterized and can easily achieve zero loss. Thus, a-priori, it seems impossible to distinguish the right hyperparameters based on training loss alone which motivates a better understanding of train-validation split. To this aim, we first show that refined properties of the validation loss such as risk and hyper-gradients are indicative of those of the true test loss and help prevent overfitting with a near-minimal validation sample size. Importantly, this is established for continuous search spaces which are relevant for differentiable search schemes. We then establish generalization bounds for NAS problems with an emphasis on an activation search problem and gradient-based methods. Finally, we show rigorous connections between NAS and low-rank matrix learning which leads to algorithmic insights where the solution of the upper problem can be accurately learned via spectral methods to achieve near-minimal risk.}
}

@article{splitting_dataset,
  title={On splitting training and validation set: a comparative study of cross-validation, bootstrap and systematic sampling for estimating the generalization performance of supervised learning},
  author={Xu, Yun and Goodacre, Royston},
  journal={Journal of analysis and testing},
  volume={2},
  number={3},
  pages={249--262},
  year={2018},
  publisher={Springer}
}


@article{data_augmentation,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of big data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={SpringerOpen}
}

@article{data_augment_2,
  title={The art of data augmentation},
  author={Van Dyk, David A and Meng, Xiao-Li},
  journal={Journal of Computational and Graphical Statistics},
  pages={1--50},
  year={2001},
  publisher={JSTOR}
}

@article{data_augment_medical,
  title={A review of medical image data augmentation techniques for deep learning applications},
  author={Chlap, Phillip and Min, Hang and Vandenberg, Nym and Dowling, Jason and Holloway, Lois and Haworth, Annette},
  journal={Journal of Medical Imaging and Radiation Oncology},
  volume={65},
  number={5},
  pages={545--563},
  year={2021},
  publisher={Wiley Online Library}
}

@article{cnn_receptive_field,
  title={Understanding the effective receptive field in deep convolutional neural networks},
  author={Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{cnn_receptive_field2,
  title={Computing receptive fields of convolutional neural networks},
  author={Araujo, Andr{\'e} and Norris, Wade and Sim, Jack},
  journal={Distill},
  volume={4},
  number={11},
  pages={e21},
  year={2019}
}

@article{mobilenet,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:\-1704.04861},
  year={2017}
}

@article{bias_variance,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{backprop,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}

@book{dl_theorie,
  title={Theorie der neuronalen Netze: eine systematische Einf{\"u}hrung},
  author={Rojas, Ra{\'u}l},
  year={2013},
  publisher={Springer-Verlag}
}

@article{gradient_descent_orig,
  title={Cauchy and the gradient method},
  author={Lemar{\'e}chal, Claude},
  journal={Doc Math Extra},
  volume={251},
  number={254},
  pages={10},
  year={2012}
}

@book{convex_opt,
  title={Convex optimization},
  author={Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@book{loss_func,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}

@book{regularization,
  title={Statistics for high-dimensional data: methods, theory and applications},
  author={B{\"u}hlmann, Peter and Van De Geer, Sara},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@inproceedings{group_norm,
  title={Group normalization},
  author={Wu, Yuxin and He, Kaiming},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}

@article{layer_norm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@book{goodfellow_deeplearning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@inproceedings{unsupervised_transfer,
  title={Deep learning of representations for unsupervised and transfer learning},
  author={Bengio, Yoshua},
  booktitle={Proceedings of ICML workshop on unsupervised and transfer learning},
  pages={17--36},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{foundation_models,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{metric_learning,
  title={Metric learning: A survey},
  author={Kulis, Brian and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={5},
  number={4},
  pages={287--364},
  year={2013},
  publisher={Now Publishers, Inc.}
}

@inproceedings{il2m,
  title={Il2m: Class incremental learning with dual memory},
  author={Belouadah, Eden and Popescu, Adrian},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={583--592},
  year={2019}
}

@online{clvision_challenge,
  title = {{4th CLVISION CVPR} Workshop Challenge},
  year = 2023,
  url = {https://sites.google.com/view/clvision2023/challenge},
  urldate = {2024-01-01}
}

@online{input_structure_img,
  title = {Linear/Fully-Connected Layers User's Guide},
  year = 2024,
  url = {https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html},
  urldate = {2024-01-01}
}


@inproceedings{acc_metric,
  title={Basic principles of ROC analysis},
  author={Metz, Charles E},
  booktitle={Seminars in nuclear medicine},
  volume={8},
  number={4},
  pages={283--298},
  year={1978},
  organization={Elsevier}
}

@inproceedings{rwalk,
  title={Riemannian walk for incremental learning: Understanding forgetting and intransigence},
  author={Chaudhry, Arslan and Dokania, Puneet K and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={532--547},
  year={2018}
}

@article{atari_games,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}


@online{openai_dota,
  title = {OpenAI Five defeats Dota 2 world champions},
  year = 2024,
  url = {https://openai.com/research/openai-five-defeats-dota-2-world-champions},
  urldate = {2024-01-01}
}

@article{alpha_go,
  title={Google's AlphaGo defeats Chinese go master in win for AI},
  author={Mozur, Paul},
  journal={International New York Times},
  pages={NA--NA},
  year={2017},
  publisher={International Herald Tribune}
}

@misc{alpha_star,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojtek and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}


@article{deep_learning_application,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{animal_species,
    doi = {10.1371/journal.pbio.1001127},
    author = {Mora, Camilo AND Tittensor, Derek P. AND Adl, Sina AND Simpson, Alastair G. B. AND Worm, Boris},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {How Many Species Are There on Earth and in the Ocean?},
    year = {2011},
    month = {08},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pbio.1001127},
    pages = {1-8},
    number = {8},
}

@online{species_discovery,
  title = {Why Thousands of New Animal Species Are Still Discovered Each Year},
  year = 2015,
  url = {https://www.atlasobscura.com/articles/new-animal-species},
  urldate = {2024-01-01}
}

@article{van2020brain,
  title={Brain-inspired replay for continual learning with artificial neural networks},
  author={Van de Ven, Gido M and Siegelmann, Hava T and Tolias, Andreas S},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={4069},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{forward_transfer,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{diaz2018don,
  title={Don't forget, there is more than forgetting: new metrics for Continual Learning},
  author={D{\'\i}az-Rodr{\'\i}guez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
  journal={arXiv preprint arXiv:1810.13166},
  year={2018}
}

@article{cosine_annealing,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@book{beta_distribution,
  title={Continuous univariate distributions, volume 2},
  author={Johnson, Norman L and Kotz, Samuel and Balakrishnan, Narayanaswamy},
  volume={289},
  year={1995},
  publisher={John wiley \& sons}
}

@inproceedings{gdumb,
  title={Gdumb: A simple approach that questions our progress in continual learning},
  author={Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={524--540},
  year={2020},
  organization={Springer}
}

@article{rolnick2019experience,
  title={Experience replay for continual learning},
  author={Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@InProceedings{semantic_freeze,
author = {Michieli, Umberto and Zanuttigh, Pietro},
title = {Incremental Learning Techniques for Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
month = {10},
year = {2019}
} 

@inproceedings{domain_incremental_freeze,
  title={An efficient domain-incremental learning approach to drive in all weather conditions},
  author={Mirza, M Jehanzeb and Masana, Marc and Possegger, Horst and Bischof, Horst},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3001--3011},
  year={2022}
}

@inproceedings{freeze_other,
  title={Self-promoted prototype refinement for few-shot class-incremental learning},
  author={Zhu, Kai and Cao, Yang and Zhai, Wei and Cheng, Jie and Zha, Zheng-Jun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6801--6810},
  year={2021}
}

@inproceedings{zhang2021few,
  title={Few-shot incremental learning with continually evolved classifiers},
  author={Zhang, Chi and Song, Nan and Lin, Guosheng and Zheng, Yun and Pan, Pan and Xu, Yinghui},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12455--12464},
  year={2021}
}

@inproceedings{schwarz2018progress,
  title={Progress \& compress: A scalable framework for continual learning},
  author={Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle={International conference on machine learning},
  pages={4528--4537},
  year={2018},
  organization={PMLR}
}

@article{parisi2019continual,
  title={Continual lifelong learning with neural networks: A review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={Neural networks},
  volume={113},
  pages={54--71},
  year={2019},
  publisher={Elsevier}
}

@article{three_decades_activation,
  title={Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks},
  author={Kunc, Vladim{\'\i}r and Kl{\'e}ma, Ji{\v{r}}{\'\i}},
  journal={arXiv preprint arXiv:2402.09092},
  year={2024}
}

@online{data_augmentation_example,
  title = {Stanford Education},
  year = 2023,
  url = {https://ai.stanford.edu/blog/data-augmentation/},
  urldate = {2023-10-05}
}

@online{mixup_example_ref,
  title = {Label Mixup vs Loss Mixup},
  year = 2020,
  url = {https://crazyoscarchang.github.io/2020/09/27/revisiting-mixup/},
  urldate = {2024-03-24}
}

@article{mauricio2023comparing,
  title={Comparing vision transformers and convolutional neural networks for image classification: A literature review},
  author={Maur{\'\i}cio, Jos{\'e} and Domingues, In{\^e}s and Bernardino, Jorge},
  journal={Applied Sciences},
  volume={13},
  number={9},
  pages={5521},
  year={2023},
  publisher={MDPI}
}

@online{over_underfit_figure,
  title = {ML | Underfitting and Overfitting},
  year = 2024,
  url = {https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/},
  urldate = {2024-03-31}
}

@article{li2023comprehensive,
  title={A comprehensive survey on design and application of autoencoder in deep learning},
  author={Li, Pengzhi and Pei, Yan and Li, Jianqiang},
  journal={Applied Soft Computing},
  volume={138},
  pages={110176},
  year={2023},
  publisher={Elsevier}
}

@article{task_incremental,
  title={A continual learning survey: Defying forgetting in classification tasks},
  author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={7},
  pages={3366--3385},
  year={2021},
  publisher={IEEE}
}

@inproceedings{NEURIPS2019_fa7cdfad,
 author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Experience Replay for Continual Learning},
 volume = {32},
 year = {2019}
}

@inproceedings{serra2018overcoming,
  title={Overcoming catastrophic forgetting with hard attention to the task},
  author={Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros},
  booktitle={International conference on machine learning},
  pages={4548--4557},
  year={2018},
  organization={PMLR}
}

@inproceedings{mallya2018packnet,
  title={Packnet: Adding multiple tasks to a single network by iterative pruning},
  author={Mallya, Arun and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={7765--7773},
  year={2018}
}

@article{hogea2024fetril++,
  title={FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing},
  author={Hogea, Eduard and Popescu, Adrian and Onchis, Darian and Petit, Gr{\'e}goire},
  journal={arXiv preprint arXiv:2403.07406},
  year={2024}
}

@article{HEMATI2025106920,
title = {Continual learning in the presence of repetition},
journal = {Neural Networks},
volume = {183},
pages = {106920},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106920},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024008499},
author = {Hamed Hemati and Lorenzo Pellegrini and Xiaotian Duan and Zixuan Zhao and Fangfang Xia and Marc Masana and Benedikt Tscheschner and Eduardo Veas and Yuxiang Zheng and Shiji Zhao and Shao-Yuan Li and Sheng-Jun Huang and Vincenzo Lomonaco and Gido M. {van de Ven}},
keywords = {Continual learning, Class-incremental learning, Repetition, Competition},
abstract = {Continual learning (CL) provides a framework for training models in ever-evolving environments. Although re-occurrence of previously seen objects or tasks is common in real-world problems, the concept of repetition in the data stream is not often considered in standard benchmarks for CL. Unlike with the rehearsal mechanism in buffer-based strategies, where sample repetition is controlled by the strategy, repetition in the data stream naturally stems from the environment. This report provides a summary of the CLVision challenge at CVPR 2023, which focused on the topic of repetition in class-incremental learning. The report initially outlines the challenge objective and then describes three solutions proposed by finalist teams that aim to effectively exploit the repetition in the stream to learn continually. The experimental results from the challenge highlight the effectiveness of ensemble-based solutions that employ multiple versions of similar modules, each trained on different but overlapping subsets of classes. This report underscores the transformative potential of taking a different perspective in CL by employing repetition in the data stream to foster innovative strategy design.}
}

@online{clvision_challenge_2023,
  title = {{4th CLVISION CVPR} Workshop Challenge},
  year = 2023,
  url = {https://sites.google.com/view/clvision2023/challenge},
  urldate = {2024-12-18}
}

@online{clvision_challenge_2024,
  title = {{5th CLVISION CVPR} Workshop Challenge},
  year = 2023,
  url = {https://sites.google.com/view/clvision2024},
  urldate = {2024-12-18}
}

@article{ovadia2019can,
  title={Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Li02012019,
author = {He Li, Lu Yu and Wu He},
title = {The Impact of GDPR on Global Technology Development},
journal = {Journal of Global Information Technology Management},
volume = {22},
number = {1},
pages = {1--6},
year = {2019},
publisher = {Routledge},
doi = {10.1080/1097198X.2019.1569186},
URL = {
        https://doi.org/10.1080/1097198X.2019.1569186
},
eprint = {https://doi.org/10.1080/1097198X.2019.1569186}

}

@article{verwimp2023continual,
  title={Continual learning: Applications and the road forward},
  author={Verwimp, Eli and Aljundi, Rahaf and Ben-David, Shai and Bethge, Matthias and Cossu, Andrea and Gepperth, Alexander and Hayes, Tyler L and H{\"u}llermeier, Eyke and Kanan, Christopher and Kudithipudi, Dhireesha and others},
  journal={arXiv preprint arXiv:2311.11908},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{MASCHLER2021452,
title = {Regularization-based Continual Learning for Anomaly Detection in Discrete Manufacturing},
journal = {Procedia CIRP},
volume = {104},
pages = {452-457},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.076},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009744},
author = {Benjamin Maschler and Thi Thu {Huong Pham} and Michael Weyrich},
keywords = {Anomaly Detection, Continual Learning, Deep Learning, Discrete Manufacturing, Elastic Weight Consolidation, Industrial Transfer Learning, Learning Without Forgetting, Regularization Strategies, Synaptic Intelligence},
abstract = {The early and robust detection of anomalies occurring in discrete manufacturing processes allows operators to prevent harm, e.g. defects in production machinery or products. While current approaches for data-driven anomaly detection provide good results on the exact processes they were trained on, they often lack the ability to flexibly adapt to changes, e.g. in products. Continual learning promises such flexibility, allowing for an automatic adaption of previously learnt knowledge to new tasks. Therefore, this article discusses different continual learning approaches from the group of regularization strategies, which are implemented, evaluated, and compared based on a real industrial metal forming dataset.}
}

@InProceedings{survey_class_incremental,
author="Yang, Dejie
and Zheng, Minghang
and Wang, Weishuai
and Li, Sizhe
and Liu, Yang",
editor="Lu, Huchuan
and Ouyang, Wanli
and Huang, Hui
and Lu, Jiwen
and Liu, Risheng
and Dong, Jing
and Xu, Min",
title="Recent Advances in Class-Incremental Learning",
booktitle="Image and Graphics",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="212--224",
abstract="A large number of deep learning models have been applied in a wide range of fields nowadays. However, most existing models can only generalize to the categories in the training set and are unable to learn new categories incrementally. In practical applications, new categories or tasks will constantly emerge, which requires models to continuously learn new category knowledge like humans while maintaining existing category knowledge. Such the learning process, i.e., class-incremental learning (CIL), abstracts more attention from the research community. CIL faces several challenges, such as imbalanced data distribution, limited model memory capacity, and the catastrophic forgetting of category representation. Therefore, we provide an up-to-date and detailed overview of CIL methods in this survey, including data-based, model-based, and representation-based approaches. We also discuss the impact of pre-trained models on CIL and compare the latest methods on widely-used benchmarks. Finally, we summarize the challenges and future directions of CIL.",
isbn="978-3-031-46308-2"
}

@article{belouadah2021comprehensive,
  title={A comprehensive study of class incremental learning algorithms for visual tasks},
  author={Belouadah, Eden and Popescu, Adrian and Kanellos, Ioannis},
  journal={Neural Networks},
  volume={135},
  pages={38--54},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{belouadah2020active,
  title={Active class incremental learning for imbalanced datasets},
  author={Belouadah, Eden and Popescu, Adrian and Aggarwal, Umang and Saci, L{\'e}o},
  booktitle={European Conference on Computer Vision},
  pages={146--162},
  year={2020},
  organization={Springer}
}

@inproceedings{masana2018metric,
author = {Masana, Marc and Ruiz, Idoia and Serrat, Joan and van de Weijer, Joost and Lopez, Antonio M},
title = {Metric Learning for Novelty and Anomaly Detection},
booktitle = {British Machine Vision Conference (BMVC)},
year = {2018}
}

@unpublished{astonpr524,
       booktitle = {Proceedings International Conference on Artificial Neural Networks ICANN'95},
           title = {Regularization and complexity control in feed-forward networks},
          author = {Christopher M. Bishop},
       publisher = {EC2 et Cie},
            year = {1995},
           pages = {141--148},
            note = {International Conference on Artificial Neural Networks ICANN'95.},
        keywords = {NCRG complexity control feed-forward networks architecture selection regularization early stopping training with noise},
             url = {https://publications.aston.ac.uk/id/eprint/524/},
        abstract = {In this paper we consider four alternative approaches to complexity control in feed-forward networks based respectively on architecture selection, regularization, early stopping, and training with noise. We show that there are close similarities between these approaches and we argue that, for most practical applications, the technique of regularization should be the method of choice.}
}
