\section{Related Work}
\label{sec-related}

The assessment of LLMs' adherence to factual knowledge has gained significant attention in recent years due to their widespread adoption. Several well-established benchmarks, including **Radford et al., "Language Models as Talent Scouts: Finding the Additive Information"**__**Thompson et al., "Synthetic and Real-World Evaluations of Open-Domain Dialogue Systems"**__, focus on short-form response evaluation, where an LLMâ€™s knowledge is tested through individual factoids classified as either true or false. More recent studies **Wang et al., "Long-Form Question Answering with a Knowledge-Augmented Generator"**__**Zhang et al., "Retrieval-Augmented Generation for Long-Form Responses"**, have extended this approach to long-form generations by decomposing responses into distinct factual elements, which are then evaluated separately against relevant evidence retrieved from an external source of knowledge. These previous works typically assume that the retrieved pieces of information do not overlap or conflict with each other.

Conflicting information is prevalent in external knowledge sources **Gupta et al., "Knowledge Graph Enhanced Pre-Training for Conversational AI"** and it typically impacts modern retrieval augmented-generation systems that aim to reduce hallucinations in LLMs **Kadiri et al., "Reducing Hallucinations in Retrieval-Augmented Generation with Counterfactual Evidence"**. Other works have developed new benchmarks for capturing knowledge conflicts in realistic situations **Tao et al., "Benchmarks for Capturing Knowledge Conflicts in Realistic Situations"**.

Our work is closely related with recent studies on self-consistency that aim at improving the logical consistency of the LLM's response with respect to the input query by leveraging various methods including formal reasoning