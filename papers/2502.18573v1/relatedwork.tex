\section{Related Work}
\label{sec-related}

The assessment of LLMs' adherence to factual knowledge has gained significant attention in recent years due to their widespread adoption. Several well-established benchmarks, including TruthfulQA \cite{lin2022truthfulqa}, FreshQA \cite{vu2023freshllms}, HaluEval \cite{li2023halueval}, HalluQA \cite{cheng2023evaluating}, and FELM \cite{chen2023felm}, focus on short-form response evaluation, where an LLMâ€™s knowledge is tested through individual factoids classified as either true or false. More recent studies \cite{factscore2023emnlp,wei2024longform,bayat2025factbench,song2024veriscore} have extended this approach to long-form generations by decomposing responses into distinct factual elements, which are then evaluated separately against relevant evidence retrieved from an external source of knowledge. These previous works typically assume that the retrieved pieces of information do not overlap or conflict with each other. 

Conflicting information is prevalent in external knowledge sources \cite{xu2024knowledge} and it typically impacts modern retrieval augmented-generation systems that aim to reduce hallucinations in LLMs \cite{lewis2021retrievalaugmented}. Other works have developed new benchmarks for capturing knowledge conflicts in realistic situations \cite{hou2024wiki,marjanovic2024dynamicqa,su2024conflictbank,pham2024s}. 

Our work is closely related with recent studies on self-consistency that aim at improving the logical consistency of the LLM's response with respect to the input query by leveraging various methods including formal reasoning
\cite{wang2023selfconsistency,dohan2022language,mitchell2022concord}.