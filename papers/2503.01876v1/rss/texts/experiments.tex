\section{Experiments}
\label{sec:exp}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{rss/figs/deployment-issues.pdf}
    \caption{\textbf{Simulated environments.} We considers three major problems during policy deployment. (a) Distribution shift; (b) Partial observability (c) Action multi-modality. }
    \label{fig:envs}
\end{figure}

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{rss/figs/real-tasks.pdf}
    \caption{Real robot experiments.}
    \label{fig:real-tasks}
\end{figure*}

We validate our method on the three types of deployment problems discussed above, conducting experiments in both simulated environments and real-world robotic setups. For most of our test cases, full teleoperation is generally able to successfully complete the task; therefore, with sufficient human intervention and assistance, the success rate can reach 100\%. However, a critical aspect of human-in-the-loop deployment is the efficiency of human intervention -- the robot should request assistance conservatively, minimizing unnecessary interruptions.

To evaluate the effectiveness of our method, we thus focus on two key aspects across all experiments. First, we assess the efficiency of human-robot interaction by measuring the frequency of human interventions. Second, we evaluate the improvement in task performance achieved through human assistance and policy fine-tuning, quantifying the benefits of integrating human feedback into the system.

We note that most offline policy learning methods do not consider HitL deployment during training, and are thus not comparable to our method. To benchmark our method, we compare it against two state-of-the-art baselines for incorporating uncertainty estimation into HitL operation.  The baselines considered in this work are as follows:
\begin{compactitem}
    \item \textbf{Gaussian ResNet18 \cite{he2016deep} Multi-Layer-Perception (MLP).} This baseline uses Gaussian MLP as the policy class for imitation learning. While this approach is not necessarily developed with a HitL framework, it is one of the few learning approaches that offers a natural uncertainty metric, which could be used in our contect.   
    During deployment, we use the entropy of the policy output as an uncertainty measurement for HitL polices. 
    % We explore several implementations (e.g. vision transformers[cite], ResNet18[cite]) of this baseline to integrate vision observations and only report the best performent version using ResNet18. 
    We also add an entropy term to regularize the training.
    \item \textbf{HULA-offline~\cite{he2024hula}.} HULA is a HitL policy learning method based on reinforcement learning. It uses an explicit uncertainty metric based on the variance of the estimated reward at any given time step. However, the original HULA method is designed for online RL. In order to make it directly comparable to our method, where only offline datasets are available during training, we adapt it to offline RL by implementing an offline variant using Conservative Q-Learning (CQL)~\cite{kumar2020conservative}.
    We also augment the dataset by labeling rewards if they are not available from the original environment: each step receives a reward of $1$ if it is the final step of the trajectory and $0$ otherwise. 
\end{compactitem}

\subsection{Environments}
In this section, we briefly summarize the environments that we test our method on in both simulated and real world setting. Details about each environment and statistics of collected data from them are included in the supplementary materials [ref].

% For simulated tasks, we test our method on each deployment problem.
% lets figure out better font format for this..
\textbf{Distribution shift: Lift-sim.} we first consider the deployment problem of distribution shift. In this task, we ask the robot to grasp and lift objects in a table-top setting. To emulate distribution shift, demonstration data is collected using only a single object (red cube - see Fig.\ref{fig:envs}), while for testing we roll out the pretrained policy to a set of unseen objects (round nuts, hammers, and hooks).

% this goes to supp
% This setup evaluates the generalization capability of the diffusion policy to unseen dynamics. Observations include the sceneâ€™s side and wrist views, along with the robot's end-effector (e.e.) pose. The training dataset consists of 200 trajectories spanning 9,666 steps.

\textbf{Partial observability: Cup Stacking.} we then test our method on problems with partially observable environments. In this task, we ask the robot to grasp a green cup and place it inside a red cup. In this task, we use three views as our visual observation: a front view, a side view and a wrist view. Successful execution requires the robot to infer object alignment based on its observations. Misalignment can lead to unintended collisions, resulting in failures such as the red cup tipping over or becoming unstable. To introduce variability and train a robust policy, cup positions are randomized during data collection. 
% The training dataset has $n$ trajectories.

\textbf{Incorrect choice of action mode: Open drawer.} Here, the robot is tasked with opening one of three drawers in the scene. The collected dataset includes trajectories for opening each drawer, with approximately $33.3\%$ of the data corresponding to each drawer. However, the dataset does not specify which drawer is to be opened in a given trajectory, introducing under-specification in the dataset. 

\subsection{Real robot experiments}

Finally, we validate our method on real robot data collected using tele-operation. In this work, we use a tele-operation system to collect human demonstration data. The robot is controlled by a trakSTAR electromagnetic 6DoF pose tracker and a gripper control unit that provides continuous commands. To meet the need for real robot deployment, we use de-noising diffusion implicit models (DDIM) that allow for high-frequency action generation. Since DDIM can be used with a DDPM, our method can be directly applied to a DDIM model. Unlike DDPM, our vector field sampling can be parallelized and batched feed-forward with the model. Hence, this additional computation does not add a big overhead during policy deployment. Details about hyper-parameters used with DDIM can be found in Appendix.\ref{sec:appendix}.

We evaluate our method on 4 real robot tasks (see Fig.~\ref{fig:real-tasks}). As in the simulated experiments, we show an example in each of the deployment problems.

\textbf{Lift-real.} This task extends the lift-sim setup to the real world, where the robot is trained to grasp a set of objects (Fig.~\ref{fig:lift-objects}) and tested on unseen objects to evaluate generalization.
Although some objects from train and test set are visually similar, the robot needs to learn different strategies to grasp them. For example, the cup we use for testing has a higher radius than the one in the training set, making it difficult to cage with a gripper. The robot needs to learn to grip the rim of the cup to achieve stable grasps.

\textbf{3-Mode Pushing.} In this task, the robot is required to push a cup to three designated locations on the table, marked with blue tape. The dataset used for training includes equal proportions of pushing trajectories for each of the three locations, with each mode a third of the full dataset. Notably, during training, the specific target location (mode) for the push is not explicitly provided to the robot.

\textbf{Ramekin Stacking.} In this task, the robot is required to pick up one ramekin and place it on top of the other. The success criterion for this task is achieving stable placement, where the top ramekin is horizontally aligned with the bottom one. This requires precise alignment between the two ramekins. 
As shown in Fig.~\ref{fig:real-cup-stacking-obs}, a key challenge arises during stacking because the bottom ramekin becomes occluded in the wrist camera view, making it difficult for the robot to achieve proper alignment for stable placement.

\textbf{Nut Pick-and-Place.} The final task evaluates our method on a pick-and-place scenario where the robot must pick up a nut and place it onto a lug. The success criterion for this task is achieving a stable placement of the nut when the gripper is open, ensuring it is properly aligned for threading. This task demands high placement precision, which is particularly challenging to achieve using visual observations alone.


% \subsection{Evaluation}

% For each task, we train a diffusion policy and evaluate its performance by executing the closed-loop policy $k$ times to measure success rates. Human operators assist the robot during deployment by intervening and taking control when necessary. In the simulated environments, operators use keyboard inputs to control the robot, with $n$ discrete actions available for intervention.

% In this work, to evaluate the effectiveness of our method, we measure the success rate of the policy deployment with assistance from human. For each task, we roll out the policy in an environment for $k$ times and record its success rate. It is also important to understand whether our method can reduce human operators' workload. Hence, the number of human intervention is used for measuring human intervention. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{rss/figs/real-robot-setup.pdf}
    \caption{Data collection pipeline.}
    \label{fig:envs}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{rss/figs/lift-real-objects.pdf}
    \caption{Objects used for the Lift-real task. The left and right images show training and testing objects respectively.}
    \label{fig:lift-objects}
\end{figure}








