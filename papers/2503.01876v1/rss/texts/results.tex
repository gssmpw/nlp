\section{Results}
\label{sec:results}

\begin{table*}[tbh]
    \centering
    \caption{Average \# of human assistance steps for simulated tasks}
    \begin{tabular}{c|c c c}
        & Lift-sim & Cup stacking & Open drawer  \\ 
        \hline
        Gaussian ResNet18-MLP & 74 & 42 & 56 \\
        \hline
        HULA-offline & 56 &54 & 22 \\
        \hline
        Denoising Uncertainty (ours) & 17 & 5 & 8\\
        \hline
        Avg. Full-trajectory length & 77 & 147 & 115 \\
    \end{tabular}
    \label{tab:human-steps-sim}
\end{table*}


\begin{table*}[tbh]
    \centering
    \caption{HitL control efficiency for real robot tasks:  Average \# of human assisted steps during policy deployment}
    \begin{tabular}{c| c c c c}
        & Lift-real & Ramekin stacking & 3-Mode Pushing & Nut PnP \\ 
        \hline
        Denoising Uncertainty (Ours) & 7.2 & 6.8 & 6.5 & 8.4\\
        \hline
        Avg. Full-trajectory length & 80 & 112 & 99 & 49\\
        % 
    \end{tabular}
    \label{tab:human-steps-real}
\end{table*}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{rss/figs/uncertainty-sim.pdf}
    \caption{Qualitative results for uncertainty estimation of the learned policy with the simulated task. The color of the trajectory represents the uncertainty estimation of the agent. Each dot represents an action prediction from the policy. }
    \label{fig:cup-variance}
\end{figure}



\subsection{Task performance}
As a sanity check, we first evaluate the task performance of the diffusion policy on each task under the training data distribution.
For the Lift-sim task, diffusion policy achieves a $100\%$ success rate with the training object but fails completely ($0\%$ success rate) when tested on unseen objects. 

In the Cup Stacking task, the robot consistently picks up the first cup (100\% grasp success rate) but fails to place it into the second cup due to alignment difficulties, resulting in an overall success rate of $0\%$ without human assistance. We also note that this task is sensitive to observation selectionâ€”training with only side and front views causes the robot to fail at grasping the green cup.

For the Open Drawer task, the diffusion policy successfully learns to open a drawer with $100\%$ success if the task description does not specify which particular drawer should be opened. Interestingly, despite the under-specified training (i.e., no conditioning on which drawer to open), the policy captures the multi-modality of the training distribution. As shown in Fig.~\ref{fig:open-drawer}, during 100 rollouts with random sampling, the robot opens the middle and bottom drawers in 15\% and 85\% of trials, respectively, but never opens the top drawer.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{rss/figs/open-drawer-dataset-vis.pdf}
    \caption{Multi-modal demonstrations and learned modes from diffusion policy in the Open Drawer task. }
    \label{fig:open-drawer}
\end{figure}

\subsection{Efficiency of human interaction}
We then evaluate human-in-the-loop deployment performance of these tasks. We note again that 100\% success rate is always possible with sufficient human assistance. Thus, we focus here on achieving high success rates with as few human assistance steps as possible, which is a critical aspect for real-world scalability of HitL systems. 

As shown in Table~\ref{tab:human-steps-sim}, for all simulated tasks, our method allows the policy to achieve high task success rates with a small number of teleoperation steps. 
For the Lift-sim task, the robot only seeks human assistance when its gripper is close to the object, allowing the human to pose the gripper to locations that leads to successful grasp. After the human assists the robot for grasping, the robot can lift the object without any human intervention. 
In the Cup Stacking task, our method identifies states where the agent fails to align the two cups as having high uncertainty. In contrast, states where the robot needs to pick up the green cup, and can rely on a full observation to complete the grasp process, are marked as having low uncertainty. 
Finally, for the Open Drawer task, experiments show that our method allow users to choose which drawer to open, or, in other words, which of the learned modes to execute. The policy asks for assistance when it is deciding which drawer to reach to, and, once the human operator steers it to a certain height, the robot autonomously completes the rest of the task.
Compared to the baselines, our method can consistently complete the task with fewer human interventions. The Gaussian ResNet18-MLP baseline requires frequent human assistance due to its reliance on entropy minimization for effective action generation. Striking a balance between the entropy term and the log-likelihood objective is challenging, making it difficult to be used as a HitL policy.

\subsection{Fine-tuning performance}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{rss/figs/fine-tuning-lift.pdf}
    \caption{Average success rate of fine-tuning the Lift-sim task with different number of human demonstration samples.}
    \label{fig:finetune-lift}
\end{figure}

Our method requests operator assistance in states where the policy exhibits high uncertainty. We posit that these states are particularly valuable as they highlight areas where the policy can benefit from additional data collection for fine-tuning. We test this hypothesis by checking if leveraging our uncertainty metrics reduces the amount of data required for fine-tuning, while still achieving significant performance improvements. 

As shown in Fig.~\ref{fig:finetune-lift}, our policy performance improves by $63.3\%$ on average if we fine-tune the policy with $170$ teleoperator-led time steps obtained specifically when uncertainty is high. In contrast, the policy success rate only improves by $28\%$ if we fine-tune using full trajectories comprising $383$ teleoperator time steps.
The HULA-offline baseline only improves its success rate by $8\%$ on average. We also observe that HULA is more prone to over-fitting in both pretraining and fine-tuning. 
It can complete the task in locations seen in the training set, but fails when the object locations are different, even if close to locations in the training set. This shows that our method can identify states that the policy needs more information. 

Compared to the baselines, our method also consistently achieves higher success rates with the similar amount of fine-tuning data. For example, while using approximately $100$ time steps of fine-tuning data, our method achieves $45.3\%$ success rate improvement while all baselines achieve less than $10\%$ improvement. 
We note that the data used for each fine-tuning experiment is collected independently (i.e. the human-in-the-loop fine-tuning data set is not a part of the full-trajectory data set). For the HitL fine-tuning, the fine-tuning dataset only consists of actions when the robot is operated by human operators, instead of full trajectories.

\begin{table}[tb]
    \centering
    \caption{Fine-tuning performance of the Lift-real task. Results are success rates derived by 20 policy rollouts per object.}
    \begin{tabular}{c|c c c}
         & Train & Test ($\uparrow$) & $||\mathcal{D}_{ft}||$ ($\downarrow$)  \\ 
        \hline
        Zero-shot  & 1 & 0.16 & 0 \\
        \hline
        HitL fine-tuning & 1 & 0.63 & 80 \\
        \hline
        Full-trajectory fine-tuning & 1 & 0.31  & 132  \\
        % \hline
    \end{tabular}
    \label{tab:lift-real-finetune}
\end{table}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{rss/figs/real-cup-stacking-obs.pdf}
    \caption{Examples of observations for Ramekin Stacking in different stages. The top row shows image frames from one of the side cameras and the bottom row shows the view of the wrist camera.}
    \label{fig:real-cup-stacking-obs}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{rss/figs/nut-pnp.pdf}
    \caption{Two examples of states with high uncertainty for the Nut PnP task: when the robot is posing for grasping the nut (left) and trying to alighn the nut with the lug (right). The image highlighted in red shows rotation of the gripper around z axis while rolling out the policy with the same initial pose of the nut. The background nut image is used only for  visualization, not observations.}
    \label{fig:z-rotation}
\end{figure}

\subsection{Evaluation on real robot experiments}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{rss/figs/push-cup-trajectories.pdf}
    \caption{Object trajectories visualization: policy random rollout vs policy steering.}
    \label{fig:push-rollout-compare}
\end{figure}

Finally, our results on real robot experiments shows that our method can be used with policy trained with real world data. Again, with human-assisted deployment, the robot can complete all four tasks. On average, our method only requests help from the human for approximately 8.3\% of time steps during policy execution (see Table~\ref{tab:human-steps-real}). Since we are using action chunking during real robot deployment, we allow the human to control the robot four steps, the same as the diffusion policy. For all tasks, the robot asks for assistance for less than 6 times (each assistance with 3 human control steps). Details about the evaluations protocol (e.g. the number of rollouts for each evaluation) are included in supplementary materials.

Qualitatively, our method identifies crucial states during policy execution. 
For example, in the Lift-real task, the robot asks for assistance when the gripper is close the the object. Using human-collected data with uncertainty, we can fine-tune the diffusion policy to improve 47\% success rate on average (shown in Table~\ref{tab:lift-real-finetune}), outperforming fine-tuning with full-trajectories of data. In the 3-Mode Pushing task, the robot autonomously reaches to the side of the object and then transfers control to the human operator, who poses the gripper in the correct location depending on the intended target. Once the pose of the gripper is indicative of the desired target, uncertainty drops, and the robot takes over and completes the task autonomously. In the Ramekin Stacking task, our method identifies high-uncertainty states when the robot is attempting to align the ramekins and the bottom one is visually occluded (see Fig.~\ref{fig:real-cup-stacking-obs}). In contrast, grasping the first ramekin -- where visual observations suffice -- is marked as low-uncertainty and thus performed autonomously.

Finally, in the Nut Pick-and-Place task, our method assigns high uncertainty to two critical stages of execution. The first stage occurs during the positioning phase for grasping, where the dataset contains diverse strategies for aligning the gripper with the nut's edge. Interestingly, when rolling out the policy multiple times with identical initial nut configurations, the robot exhibits varying rotations around the z-axis to grasp the nut (see Fig.~\ref{fig:z-rotation}). This highlights the ability of our uncertainty metrics to capture critical decision points during execution. The second stage of high uncertainty arises during the placement phase, where precise positioning of the nut is required. The visual observations from the wrist camera and the two side cameras fail to reliably determine the stability of the placement, resulting in elevated uncertainty during this stage. The agent thus requests operator assistance for task completion.


\subsection{Hyperparameter exploration}

\begin{table}[t]
    \centering
    \caption{Effect of Sampling Radius on Fine-tuning Performance of the Lift-sim task.}
    \begin{tabular}{c|c c c c}
        Radius of sampling & 0.01 & 0.03 & 0.05 & 0.1\\ 
        \hline
        $\#$ of human steps ($\downarrow$)  & 60.3 & 31.6 & \textbf{20} & 46.3\\
        \hline
        Success rate ($\uparrow$) & 0.46 & 0.55 & \textbf{0.63} & 0.53\\
    \end{tabular}
    \label{tab:ablation-radius}
\end{table}


\begin{table}[t]
    \centering
    \caption{Effect of scaling constant $\alpha$ on Fine-tuning Performance of the Lift-sim task.}
    \begin{tabular}{c|c c c c c}
        $\alpha$ & 0.01 & 0.05 & 0.1 & 0.3 & 0.5\\ 
        \hline
        $\#$ of human steps & 5 & 5 & 5 & 7  & 7\\
        \hline
        index of max uncertainty  & 139 & 139 & 139 & 137 & 137\\
    \end{tabular}
    \label{tab:ablation-radius}
\end{table}
In this section, we investigate how hyper-parameters affect the performance of our HitL agent. The choice of these hyper-parameters plays an important role of our uncertainty estimation, and hence can affect how the robot ask for human assistance.

\subsubsection{Sampling Radius}
The radius parameter $r$ defines the neighborhood size for collecting denoising vectors in uncertainty estimation. It is crucial to consider the scale of $r$ relative to the action distribution that a diffusion policy aims to recover. 
If $r$ is too large, the uncertainty metric becomes similar across all states, as it incorporates a broad range of them. Conversely, if $r$ is too small, the metric may be overly influenced by local uncertainty. In this work, we normalize each dimension of the action space to $[0,1]$, simplifying the selection of $r$.


In HitL fine-tuning for Lift-sim (see Table~\ref{tab:ablation-radius}), we test radii ranging from 0.01 to 0.1. 
A radius of 0.05 achieves the best balance between intervention steps and success rate, requiring only 20 interventions while improving the success rate by 0.63. This efficiency results from accurate uncertainty detection when the gripper approaches unseen targets but fails to grasp them. Smaller radii  and larger radii yield less precise estimations, leading to interventions that are either premature or delayed, reducing success rates despite more interventions.


Overall, a radius of 0.05 consistently achieves optimal performance by balancing local and global uncertainty estimations. Smaller radii miss key trajectory patterns, while larger radii incorporate irrelevant vectors, reducing the accuracy of uncertainty estimation in robotic manipulation tasks.


\subsubsection{Scaling constant $\alpha$ in uncertainty calculation}
The alpha parameter serves as a scaling factor in our uncertainty calculation, balancing two components: mode divergence and overall variance. Mode divergence captures directional differences between action modes, while overall variance measures spread within each mode.

Directional differences typically provide stronger uncertainty signals, and small $\alpha$ values (0.01 -- 0.1) emphasize mode divergence, effectively identifying critical occlusion phases (e.g., step 139). 
This highlights mode divergence as a reliable indicator of uncertain states. 
As $\alpha$ increases, the overall variance term gains more weight, raising both maximum and minimum variance values. 
However, this added emphasis on within-mode spread does not significantly enhance uncertain state detection, supporting the dominance of mode divergence as the more informative component.
The variance term remains essential for distinguishing states with single action mode (e.g. reaching for grasping), where mode divergence alone would yield identical scores. 
While $\alpha$ influences absolute uncertainty values, it minimally affects the identification of critical steps (consistently around steps 137â€“139 in cup stacking). 

