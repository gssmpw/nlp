\section{Method}
\label{sec:method}

We start from the basic premise of HitL robotic agents: the robot is expected to generally act autonomously, but a human operator is available to provide teleoperation commands should the robot require them. Our method is designed to determine when the agent should request such expert assistance, making effective use of a limited number of such calls during deployment. Furthermore, since we use a learning agent, we aim to eliminate the need for expert intervention during the training phase, as that would likely place a large burden on the operator. This means that the agent has no quantitative knowledge about the effect of assistance, except for the assumption that it effective for task completion. 

To achieve this, we utilize diffusion models~\cite{chi2023diffusion} as our policy class. Diffusion policies offer two key advantages: (1) they have demonstrated robust performance in imitation learning tasks, and (2) their generative process involves an iterative denoising mechanism which we can leverage for insight into the agent's decision-making process. Specifically, we use the denoising information to compute an uncertainty metric for the policy, which is then used during deployment to determine when human intervention is most beneficial. Importantly, the policy is trained using an offline dataset and does not rely on human assistance during the training phase. Finally, we show that data collected during the operator interventions can be incorporated back into training through a fine tuning process that further improves policy performance.

In this section, we first re-iterate general diffusion policies concepts, describe how our method utilizes their generative process to compute an uncertainty metric, and finally discuss how this metric is applied to improve policy deployment performance.

\subsection{Background: diffusion policies}
\label{sec:background}
Diffusion policies generate actions through an action-denoising process, leveraging denoising diffusion probabilistic models (DDPM). A DDPM models a continuous data distribution $p(x^0)$ as reversing a forward noising process from $x^0$ to $x^K$, which is defined as a Markovian chain with Gaussian transition. Here, $x^K$ represents a sample of $\mathcal{N}(0, \sigma^2\mathcal{I})$.
To generate actions, the model learns to predict the transition (i.e. noises added during each step) and map the data from $x^K$ back to $x^0$. Sampling begins with a random Gaussian noise and iteratively refines it to produce a denoised output.

Specifically, the generative process of a diffusion policy $\pi(\mathrm{A}|\mathrm{O})$, where $\mathrm{O}$ and $\mathrm{A}$ are observations and actions, starts by sampling a random noise $a^{K}_t$ and iteratively remove noises by:
\[
a^{k-1}_t = \beta(a^{k}_t - \gamma \epsilon_{\theta}(o_t, a^{k}_t, k) + \mathcal{N}(0, \sigma^2\mathcal{I})),
\]
where $\beta$, $\sigma$ and $\gamma$ are functions of iteration step $k$.
To train a diffusion policy, we learn a score function $\epsilon_{\theta}$ that predicts the noises used during forward noising:
\begin{align}
    \centering
    \label{eq:loss}
    \mathcal{L} = MSE(\epsilon^k, \epsilon_{\theta}(o_t, a^{k}_t + \epsilon^k, k))
\end{align}

\subsection{Denoising-based uncertainty metric}

To estimate the uncertainty of a diffusion-based agent, our method leverages the generative process inherent in the diffusion policy. In particular, we note that, when operating in task-space control  (where the action space consists of desired end-effector poses), the noise predicted during the generative process can be interpreted as a vector field pointing toward the target distribution in the next state. We can thus leverage this vector field to analyze whether a diffusion-based agent is confident about its generative target. 

We assume that our policy is operating on task space control and a diffusion policy outputs absolute end-effector (e.e.) poses and manipulator state. We also assume that the current e.e. pose is available as an input for action denoising. Our goal is thus to estimate an uncertainty metric $\texttt{Uncertainty}(o_{t})$ where $o_t$ is the observation at time step $t$.

To compute this metric, we first sample a set of points $A_{sampled}$ within a distance $r$ from the current pose. For each sampled point, we feed the diffusion policy forward and predict the noise vectors required to sample actions: 
\begin{align}
    v = \epsilon_{\theta}(o_t, a_{sampled}, 0)
\end{align}
These predicted noise vectors represent the directions toward the data distribution that the policy aims to recover. In this work, we use these denoising vectors to estimate uncertainty, defined as $\texttt{Uncertainty}(o_{t}) = f(V_t)$, where $V_t$ represents the set of denoising vectors at time step $t$.

A critical characteristic of diffusion policies is their ability to capture multi-modality in the underlying human demonstrations (starting from a given state, there might be multiple distinct action trajectories that accomplish the desired task). Thus, at any given moment, the denoising vector field could reflect the multi-modal nature of the data, and naive variance estimation of the vector field may fail to capture this effect. 

To address this, we use Gaussian Mixture Models (GMMs) to capture the potentially multi-modal nature of action generation. Our method, outlined in Algorithm \ref{alg:uncertain}, starts by fitting the collected denoising vectors with $N$ GMMs, each using a different number of modes. We then select the best-fit GMM for uncertainty estimation via maximum likelihood estimation:
\[
\max_{n, \theta_{\mathrm{gmm}}} P(V_t; n, \theta_{\mathrm{gmm}}),
\]
where $n$ is the number of modes and  $\theta_{\mathrm{gmm}}$ is the parameters of a Gaussian mixture model. With the best-fit GMM, we then can estimate an agent's uncertainty at state $s$. We first evaluate the divergence between each mode:
\[
D(V_t) = \frac{1}{n(n-1)}\sum_{i, j} 1 - S_{c}(g_i, g_j)
\]
where,
\[
S_c(g_i, g_j) = \frac{g_i \cdot g_j}{||g_i|| \cdot ||g_j||}
\]
and $g_i$ represents the mean of the $i^{th}$ mode and $S_c$ represents cosine similarity between two vectors.
% \begin{align}
%         D(v) &= \frac{1}{k}\sum_{i, j} 1 - S_{c}(g_i, g_j) \\
%         \text{where,}& \nonumber \\
%         S_c(g_i, g_j) &= \frac{g_i \cdot g_j}{||g_i|| \cdot ||g_j||} 
% \end{align}
We also evaluate the GMM variance as part of the uncertainty estimation:
\begin{align}
    \texttt{Var}_g(V_t) = \sum_{i} p(v_{i})\texttt{Var}(v_{i})
\end{align}
where \texttt{Var} represents the variance of vector data.
Putting them together, we can estimate the overall uncertainty as:
\begin{align}
    \label{eq:uncertainty}
    \texttt{Uncertainty}(o_t) = D(V_t) + \alpha \texttt{Var}_{g}(V_t), 
\end{align}
where $\alpha$ is a constant.
This uncertainty estimation considers two aspects during denoising: how diverged the target distributions are, and how much entropy there is in each of the modes.

\subsection{Uncertainty-based intervention and policy fine-tuning}
\label{sec:deployment-method}

\begin{algorithm}[t]
\caption{HitL Policy Deployment}
\label{alg:uncertain}
\begin{algorithmic}[1]
    \WHILE {rollout not done}
        \STATE Sample a set of points uniformly within the radius of $r$\\
        \STATE Feed forward the diffusion policy $\pi$ to collect a set of denoising vectors $V_t$\\
        \STATE Estimate uncertainty in this state using Eq.\ref{eq:uncertainty}\\
        \IF {$\texttt{Uncertainty}(o_t) \ge \texttt{threshold}$}
            \STATE Execute $m$ steps of human input actions $a_{human}$
            \STATE Save intervention data $\{o, a_{human}\}^m$ to $\mathcal{D}_{ft}$
        \ELSE
            \STATE Execute an action $a_t$ from the policy $\pi(a_t | o_t)$
        \ENDIF
    \ENDWHILE
    \IF{fine-tune}
        \WHILE {fine-tuning not done}
            \STATE Sample a batch of data  from $\mathcal{D}_{ft}$ and $\mathcal{D}_{train}$
            \STATE Optimize $\pi$ using Eq.~\ref{eq:loss}
        \ENDWHILE
    \ENDIF
\end{algorithmic}
\end{algorithm}

Having defined our uncertainty metric, we can use it during deployment by setting a threshold to determine whether we to request human assistance. At every state, the agent computes its own uncertainty and, if the level of uncertainty exceeds the threshold, the agent requests that the operator take control and teleoperate the system for several steps, until uncertainty returns below the threshold.

In addition, our method can also be used to collect data to further fine-tune the policy. This allows for better performance in the next policy execution. To fine-tune a policy, we save the observation and action pairs $\{\mathrm{O}, \mathrm{A}\}$ when a human operator is intervening with the robot and use this data set to fine-tune the underlying diffusion policy. To avoid catastrophic forgetting \cite{ball2023efficient}, we sample from both the fine-tuning dataset $\mathcal{D}_{ft}$ and pretraining dataset $\mathcal{D}_{train}$. For each mini-batch, we ensure $50\%$ are from $\mathcal{D}_{ft}$. Our approach implicitly means that this fine-tuning data specifically addresses the areas of state space where the agent's uncertainty is high, since that is where operator assistance is requested.

Putting all components together, the final pipeline contains three main steps: 1. train a diffusion policy; 2. deploy the policy, and request operator control whenever policy uncertainty estimated by our metric exceeds a pre-set threshold; 3. (optional) use the human intervention data to fine-tune the diffusion policy.

To envision the intended applicability of this framework, let us consider three types of deployment issues that typically cause uncertainty for learning-based agents:
\begin{compactitem}
    \item Case 1: Data distribution shift. For example, visual observation distribution shift can be caused by change of lighting condition. A special case for robotics is the change of dynamics that is caused by interacting with novel objects.
    \item Case 2: Incomplete state observability. The common approach to address this can include redesigning, adding or moving sensors. However, in the general case, it is very difficult to have a sensor setup that guarantees full observations for all the tasks.
    \item Case 3: Incorrect choice between different action modes. In cases where the agent is presented with a discrete choice between two possible action trajectory modes equally represented during training, diffusion policies are naturally well-equipped to make such choices. However, it is possible that, due to task under-specification, the choice will be incorrect for the given goal.
\end{compactitem}

During policy execution, these problems may not be present in all states. In fact, many states are easy to make decisions. For example, moving the arm in free space is often easy and does not require human attention to help the robot. Our first goal is to identify when the issues arise, and request help at that time. We posit that, by combining uncertainty due to variance within a mode with uncertainty due to presence of multiple modes, our framework is positioned to recognize all three cases above. Furthermore, we believe that a HitL framework is particularly well equipped to handle Case 3, since a few steps under teleoperator control can ``steer" the policy towards the desired mode. Case 1 lends itself well to fine-tuning based on the novel data collected during teleoperation. Finally, Case 2 is the most difficult, since, by definition, correct decision making is impossible without changing the available observation. We design our experiment set to test a range of scenarios covering these situations.
