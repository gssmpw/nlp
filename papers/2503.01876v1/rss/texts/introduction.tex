\section{Introduction}
\label{sec:intro}

Human-in-the-Loop (HitL) robot deployment is a paradigm where a human operator can intervene and assist the robot during deployment. 
This paradigm is seeing increasing adoption in cases where robots must continue to operate adequately even in corner cases not foreseen before deployment. 

In parallel, even as recent advances in policy learning have shown significant improvements in robustness at deployment time, current methods can still fail due to problems such as data distribution shift or incomplete state observability. 
This issue is particularly pronounced in robotics, where available datasets are far smaller compared to other domains. 

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{rss/figs/hitl-teasor-v2.pdf}
    \caption{\textbf{Human-in-the-loop policies:} We assume a  human-in-the-loop robotic agent that actively requests human assistance when necessary. In our method, the robot uses an underlying diffusion policy for autonomous operation. During deployment, we leverage the denoising process inherent to diffusion models in order to identify states with high uncertainty, where the robot seeks operator intervention. We show that this approach allows for task improvement with a small number of such operator calls. Furthermore, the human-operated data can also be used to fine-tune the policy, improving the robot's autonomous execution performance in future deployment.}
    \label{fig:candy}
\end{figure}

To address this issue, HitL methods can be a natural fit for learning agents. 
The robot can operate autonomously when possible, leveraging the ability of policy learning to execute complex motor control tasks. An expert operator can take over to deal with corner cases, ensuring overall task success. However, deploying human-in-the-loop policies can be labor-intensive if it implies constant monitoring of the robot's behavior by the human operator, or frequent, interruptive interventions. Such limitations can make HitL deployment both impractical and costly in many applications. 

In this work, we propose a data-driven approach for generating HitL policies. Our method is based on diffusion models, and does not require any additional computation or human intervention during training. Instead, we utilize the intrinsic properties of diffusion models, specifically the denoising process, to allow the agent to compute its internal uncertainty during deployment. When this uncertainty exceeds a certain threshold, the agent proactively seeks human assistance. Furthermore, a key feature of our method is its dual utility: it can be used not only for HitL deployment, but also for collecting additional data to fine-tune the policy. 

We test our method on experimental setups where uncertainty can arise from partial observability (where our uncertainty metrics can enable effective supervision), data distribution shifts (where targeted data collection can also improve policy performance) and action multi-modality (where human guidance can help steer the robot into states where mode selection becomes unambiguous). Our main contributions are as follows:
\begin{itemize}
    \item To the best of our knowledge, we are the first to propose using uncertainty estimation with diffusion policies for efficient HitL deployment of a diffusion-based learning agent. Furthermore, our approach does not require human-robot interactions during training, and incurs minimal computational overhead during deployment. 
    \item We validate our method across multiple types of deployment challenges, in both simulated and real environment. Experimental results show that our approach requires fewer human interventions to and achieves higher task performance compares to alternative learning-based HitL agents.     
    \item We also show that our uncertainty-based state identification method can be utilized to collect targeted fine-tuning data, yielding performance improvements with smaller datasets compared to collecting additional full-trajectory demonstrations.  
\end{itemize}

We see our work as complementary to approaches aiming to improve the robustness of fully autonomous learning-based agents. Until corner cases can be fully eliminated and such agents can generalize to any situation that can be encountered during deployment, HitL methods have the potential to bridge the gap to applications, and help collect critical training data for further policy improvements. We believe this work is a step in this direction.