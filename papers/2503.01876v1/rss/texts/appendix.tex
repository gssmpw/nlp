\begin{table*}
\centering
\begin{tabular}{c|c|c|c|c|c|c|c}
 & \multicolumn{3}{c|}{Simulated tasks} & \multicolumn{4}{c}{Real tasks} \\ \hline
 & Lift-sim & Cup stacking &  Open drawer & Lift-real & Ramekin stacking & 3-mode pushing & Nut PnP \\ 
 \hline
 Pretrain dataset size & 9666 & 41670 & 33617 & 9600 & 3360 & 5940 & 2450 \\
 \hline
 Observations & WFL & SWFL & FL & \multicolumn{4}{c}{Two side cameras views, one wrist view and low-dim obs} \\
 \hline
 Action chunk Size & \multicolumn{3}{c|}{1}  & \multicolumn{4}{c}{8} \\
 \hline
 Obs. history size & \multicolumn{3}{c|}{1}  & \multicolumn{4}{c}{2} \\
 \hline
 Image crop size & 76 x 76 & \multicolumn{2}{c|}{96 x 96} & \multicolumn{4}{c}{216 x 288} \\
 \hline
 Initial object position & \multicolumn{2}{c|}{randomized} & fixed & \multicolumn{2}{c|}{randomize}  & \multicolumn{2}{c}{fixed}
 
 \end{tabular}
 \label{tab:training_details}
\caption{Training details: S, W, F represents side, wrist, front camera views receptively. L represents low-dimensional observations that contains end-effector pose and gripper state.}
\end{table*}

\appendix
\label{sec:appendix}
\subsection{Training details and data statistics}
In this section, we detail the training and implementation setup for each experiment. The hyperparameters used during pretraining are provided in Table~\ref{tab:training_details}. We employ a transformer-based diffusion policy (DP-T) \cite{chi2023diffusion}, which yields stable performance across the tasks tested. 
For each experiment, we use the best-performing policy from pretraining. 
We also note that the selection of specific observations is crucial for task learning. For instance, in the Cup Stacking task, the policy fails to learn how to grasp the green cup without including the wrist view. 
Additionally, we observe that policies with lower performance typically require more human assistance. Parameters for policy rollouts are also listed in Table~\ref{tab:training_details}.

\subsection{Human-in-the-loop policy deployment}

In this work, a human operator controls the robot via keyboard inputs during high-uncertainty states. Specifically, the operator provides a delta pose and gripper control, with each assistance event allowing control for $3$ or $4$ steps. 
The results presented in this paper reflect human assistance commands rather than the number of expert calls, although these can be easily converted to the amount of expert calls if needed.


\subsection{Instability of HULA}
In this work, we extensively test the performance of our baselines to ensure fair comparisons. 
We find that the training of HULA is sensitive to hyper-parameters. The underlying cause is the requirement of training a good Q-function in the base RL algorithm CQL. 
For example, Fig.~\ref{fig:cql} shows an example of training curves of one of our failed HULA training. 
Since the uncertainty estimation is dependent on the estimated Q function, the uncertainty estimation can be instable too.

\begin{figure}[th]
    \centering
    \includegraphics[width=0.8\linewidth]{rss/figs/cql_instable.pdf}
    \caption{Example of the training curve for an unstable training with HULA-offline. }
    \label{fig:cql}
\end{figure}

