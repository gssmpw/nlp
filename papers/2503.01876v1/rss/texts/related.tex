\section{Related Work}
\label{sec:related}

Despite recent advances in generalist robot manipulation policies~\cite{open_x_embodiment_rt_x_short_ref_2023, octo_2023}, particularly in domestic settings~\cite{liu2024okrobot, wu2024tidybot}, deploying these policies in real-world scenarios remains a significant challenge. Robots often face substantial data distribution shifts when introduced to new environments. 
In SIMPLER~\cite{li2024evaluating}, Li \etal show the sensitivity of policies to minor changes, such as variations in robot arm texture, which can cause performance drops exceeding 20\%. 
Additionally, real-world deployment is hampered by incomplete and noisy observations due to environmental factors such as lighting conditions, occlusions, clutter, and weak texture features~\cite{cong2021comprehensive}. The inherent complexity of modeling multi-modal distributions in human demonstrations further exacerbates these issues, particularly in behavior cloning, where stochastic sampling and initialization procedures introduce additional challenges~\cite{chi2023diffusion, florence2022implicit, mandlekar2020human, shafiullah2022behavior}.
Our work addresses these deployment difficulties by incorporating human assistance at critical moments. 
By identifying high-uncertainty states during deployment, our method facilitates timely human intervention, enabling robots to overcome distribution shifts, handle incomplete observations, and navigate complex multi-modal action spaces more effectively.

Human-in-the-loop approaches have been widely explored to improve robot manipulation policies by leveraging human feedback in various forms, including interventions~\cite{mandlekar2020human, spencer2020learning}, preferences~\cite{lee2021pebble}, rankings~\cite{pmlr-v97-brown19a}, scalar feedback~\cite{macglashan2017interactive}, and human gaze~\cite{zhang2020human}. Recent works~\cite{liu2022robot, luo2024hilserl} have demonstrated the effectiveness of human assistance. For example, HIL-SERL achieves state-of-the-art performance in vision-based real-world reinforcement learning, while Sirius integrates human trust signals to enhance behavioral cloning. 
The HitL systems is also extensively applied in autonomous driving. For example re-routing systems from ZOOX~\cite{zoox2024} enable human operators to intervene when vehicles encounter challenging scenarios.

However, these approaches often focus on incorporating human feedback during training while neglecting the critical question of when human assistance is most necessary during deployment. They typically require continuous human supervision, which is inefficient and impractical in real-world settings. 
In contrast, our work introduces an uncertainty-aware diffusion model that actively identifies deployment scenarios where human expertise is most beneficial. By requesting assistance only during high-uncertainty states, we enable a more efficient and scalable HitL framework that minimizes the reliance on constant human oversight. The closest work to ours is HULA \cite{he2024hula}, a reinforcement learning approach that explicitly models the uncertainty of an agent. However, HULA has limited application in the setting of online reinforcement learning, while our works focus on deriving HitL policies from offline dataset.

In this work, we leverage diffusion models to produce HitL policies.
Diffusion models have recently achieved remarkable success in robotics and decision-making tasks~\cite{ankile2024juicer, chi2023diffusion, pearce2023imitating, reuss2023goal, sridhar2024nomad, wang2024poco, ze20243d}. These models are particularly adept at capturing complex behaviors and multi-modal trajectory distributions when trained on high-quality demonstration data. 
However, collecting perfect demonstration datasets is often infeasible due to limitations in data quality and the prevalence of suboptimal demonstrations. To address this, researchers have proposed methods such as guiding the denoising process with external objectives like reward signals or goal conditioning~\cite{ajay2022conditional, chen2024diffusion, janner2022planning, liang2023adaptdiffuser, venkatraman2023reasoning}. 
Other strategies include integrating Q-learning and weighted regression in offline~\cite{chen2022offline, wang2022diffusion} or online~\cite{hansen2023idql, kang2024efficient, psenka2023learning} settings.
Our work takes a novel approach by leveraging the inherent distribution modeling capabilities of diffusion models to quantify uncertainty in action modes for each state. This allows us to identify high-uncertainty states where human intervention is most impactful. By targeting these critical states, we create a more efficient and focused human-in-the-loop policy deployment framework, enhancing both performance and scalability.
