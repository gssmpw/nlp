\section{Related Work}
\label{sec:related_work}

%\subsection{Long Document Summarization}

The literature is rife with proposals aiming to  mitigate LLM difficulties with processing inputs that exceed their context lengths. A significant challenge in overcoming the context limit is addressing the quadratic computational
burden of the self-attention mechanism. Previous work has 
attempted to reduce this cost by architectural modifications such as introducing sparse attention \cite{child2019generatinglongsequencessparse,beltagy2020longformerlongdocumenttransformer}, linearized attention \cite{katharopoulos:ea:2020},  changes to positional encoding \cite{peng2023yarnefficientcontextwindow,chen2023extendingcontextwindowlarge},  and special-purpose fine-tuning methods which directly modify the attention mechanism \cite{chen2024longloraefficientfinetuninglongcontext}  or incorporate discourse-level information. %\cite{liu2024rstloradiscourseawarelowrankadaptation} in LoRA fine-tuning \cite{hu2021loralowrankadaptationlarge}. 

Another line of work adopts a divide-and-conquer approach, splitting the long input into manageable chunks. The chunks can be processed independently \cite{wang2023augmentinglanguagemodelslongterm,bertsch2023unlimiformer} or through progressively merging adjacent chunks as they
are processed along the transformer layers \cite{song2024hierarchicalcontextmergingbetter}. 
\citet{Wu2021RecursivelySB} propose a dive-and-conquer approach for long document summarization in which an LLM is fine-tuned via reinforcement learning to summarize each chunk and then hierarchically merge
chunk-level summaries  into a final summary. Their method has since been adapted to use zero-shot prompting for summary generation \cite{chang2024booookscore, fables-2024-kim-et-al}, without further training (as shown in Figure~\ref{fig:hm}). 

Our proposal modifies hierarchical merging to consider evidence from the source document at the intermediate summary generation stage. It draws inspiration from retrieval augmented generation (RAG) techniques which have proven particularly effective in long-form question answering \cite{lewis2020rag, izacard-grave-2021-leveraging,xu2024retrievalmeetslongcontext,edge2024localglobalgraphrag}, as a means to integrate up-to-date information, mitigate hallucinations, and enhance response quality. Our work explores various proposals for context integration, e.g.,~as a replacement of  intermediate summaries or additional supporting evidence, as well as retrieval augmentation methods. In addition to traditional query-based retrieval which we implement using the intermediate summaries as queries, we also turn to extractive summarization to select and rank source document sentences that best represent its content.  

Various methods have been proposed for long document extractive
summarization, focusing on reinforcement learning
\cite{gu-etal-2022-memsum, bian2023gosumextractivesummarizationlong}
and adapting LLMs for sentence extraction \cite{Lu2023HybridLD,
  Hemamou2024ScalingUS}. However, these methods were primarily
evaluated on datasets with relatively short inputs, such as arXiv
(\citealt{cohan-etal-2018-discourse}; ~5K tokens) and GovReport
(\citealt{huang-etal-2021-govreport}; ~10K tokens), making their
effectiveness on inputs exceeding 100K tokens uncertain. As we
identify salient sentences within each chunk, we expect extractive
methods to be able to isolate supporting evidence for ultimately
generating more factual \emph{abstractive} summaries.

Our work also  has connections to recent efforts in creating verifiable systems that generate responses to queries by incorporating citations to source material \cite{fierro2024learningplangeneratetext,nakano2022webgptbrowserassistedquestionansweringhuman}. Most existing models learn to generate citations \cite{menick2022teachinglanguagemodelssupport,nakano2022webgptbrowserassistedquestionansweringhuman} or implement a post-processing step \cite{Bohnet:Ea:2022,gao-etal-2023-enabling} where the text is first generated and then edited to be made attributable (e.g.,~to retrieved web content).  For hierarchical merging, generating intermediate summaries with citations is advantageous as it  eschews the need for additional retrieval or extraction mechanisms. However, we experimentally find that generating \emph{accurate} citations is challenging requiring models to understand and execute complex instructions. 

%The advanced instruction-following capabilities of LLMs have also made prompting-based methods effective for long document summarization. 

%\citet{Wu2021RecursivelySB} introduced a hierarchical approach that segments the input into chunks, generates summaries for each chunk, and recursively combines them into a final summary. This method was later adapted to use zero-shot prompting for summary generation  proving effective in producing coherent summaries.

%researchers can fine-tune them with specialized methods that overcome transformer limitations. For example, LongLoRA \cite{chen2024longloraefficientfinetuninglongcontext} modifies the attention mechanism in LoRA fine-tuning \cite{hu2021loralowrankadaptationlarge} by implementing shifted sparse attention, which efficiently extends LLMs' context length while maintaining performance. Another variant, RST-LoRA \cite{liu2024rstloradiscourseawarelowrankadaptation}, specifically targets summarization by incorporating discourse information into the LoRA model, demonstrating strong results on long document summarization.



\begin{figure}[t]
  \includegraphics[width=0.45\textwidth, trim=0cm 0cm 0cm 0cm, clip]{proposed.pdf}
  \caption{Illustration of proposed pipeline for the first two
    levels. The Incorporate Context (IC) module is applied at the
    first-level to obtain relevant input contexts~$p^1_i$ alongside
    summaries~$s^1_i$. At the \mbox{second-level}, we either use
    $p^1_1 \dots p^1_m$ as supporting evidence (\textbf{Support}), or
    replace $s^1_1 \dots s^1_m$ entirely with $p^1_1 \dots p^1_m$
    (\textbf{Replace}) for generating~$s^2_1$ and subsequently
    obtaining~$p^2_1$.}
  \label{fig:proposed}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=\textwidth, trim=0cm 0cm 0cm 0cm, clip]{ic.pdf}
 \caption{\label{fig:ic} Incorporate Context (IC)
   module from Figure~\ref{fig:proposed} with three context
   augmentation methods: Extractive Summarization (Extract),
   Retrieval-augmented Generation (Retrieve), and Generating text with
   citations (Cite).} 
\end{figure*}

%\begin{figure}[t]
%\caption{
%Illustration of the Incorporate Context (IC) module in Figure \ref{fig:proposed} with three context augmentation methods: Extractive Summarization (Extract), Retrieval-augmented Generation (Retrieve) and Generating text with citations (Cite).}
 %   \end{figure}
    

%\subsection{Incorporating Input Context into Hierarchical Merging}

%This section reviews prior work related to our methods of incorporating input context into hierarchical merging.

%\paragraph{Extractive summarization} involves ranking and selecting input sentences that best represent the document's content. 

%\paragraph{Information Retrieval} enhances model performance by retrieving relevant information from external knowledge bases \cite{lewis2020rag, izacard-grave-2021-leveraging}. While retrieval was widely adopted for question answering, research has shown its effectiveness in summarization \cite{xu2024retrievalmeetslongcontext, edge2024localglobalgraphrag} and general knowledge integration \cite{zhao2024retrievalaugmentedgenerationrag}. Retrieval performance depends on various factors, including retriever selection \cite{xu2024retrievalmeetslongcontext} and query quality \cite{shi2024enhancingretrievalmanagingretrieval}.

%\paragraph{Generating text with citations} focuses on producing verifiable content by incorporating citations to source material after generated sentences or paragraphs \cite{fierro2024learningplangeneratetext, gao2023enablinglargelanguagemodels}. This approach requires models to generate accurate citations corresponding to input contexts and understand complex citation-generation instructions \cite{menick2022teachinglanguagemodelssupport, nakano2022webgptbrowserassistedquestionansweringhuman}.