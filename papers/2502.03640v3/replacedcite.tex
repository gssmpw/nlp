\section{Related work}
\noindent\textbf{Constructing Decentralized CBFs. }
The challenge of applying CBFs for MAS has been explored via the construction of distributed CBFs ____, which only take local observations as input.
This simplifies the big centralized QP problem into small QP problems to be solved per agent.
However, the construction is either limited to the case of unbounded control ____
or only for a specific dynamics model (e.g., double integrator) ____.
Recent advances in \textit{learning} CBFs using neural networks ____ has resulted in works that investigate learning distributed CBFs ____.
Nevertheless, these approaches assume \textit{known} dynamics and are only applicable to \textit{continuous-time} dynamics and hence cannot be applied to our problem setting. Moreover, it is assumed that a performant nominal policy is available, which we do not consider in this work.

\noindent\textbf{CBF in RL. }
Originally inspired by the prospect of safety during training, recent works have integrated CBFs into the RL training process via the safety filter ____ for both single-agent ____ and multi-agent ____ cases.
Although both continuous-time ____ and discrete-time ____ dynamics have been considered,
a major limitation is the requirement of affine (D)CBFs and control-affine dynamics up to a constant disturbance term to be learned.
In contrast, the problem we tackle in this work does not make any such assumptions about the safety specifications or the structure of the dynamics. 

\noindent\textbf{Safe Multi-agent RL. }
The problem of constructing safe policies for MAS has also been studied in the RL community ____.
Early works achieved safety via reward function design ____.
However, these approaches do not guarantee the satisfaction of the safety constraints even for the optimal policy ____.
More recently, in the single-agent case, methods work with constraints in the form of the constrained Markov decision process (CMDP) problem and apply techniques from constrained optimization, including 
primal methods ____,
primal-dual methods using Lagrange multipliers ____,
and via trust-region-based approaches ____.
Of these, Lagrange-multiplier-based approaches are the most popular due to their simplicity,
leading to multi-agent extensions ____.
However, Lagrangian methods for CMDPs have been observed to have unstable training and convergence to poor policies when the constraint threshold is \textit{zero} 
____, which is the setting we target in this work.