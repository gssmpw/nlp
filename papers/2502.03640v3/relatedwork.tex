\section{Related work}
\noindent\textbf{Constructing Decentralized CBFs. }
The challenge of applying CBFs for MAS has been explored via the construction of distributed CBFs \citep{borrmann2015control,glotfelter2017nonsmooth,wang2017safety,lindemann2019control,black2023adaptation}, which only take local observations as input.
This simplifies the big centralized QP problem into small QP problems to be solved per agent.
However, the construction is either limited to the case of unbounded control \citep{lindemann2019control}
or only for a specific dynamics model (e.g., double integrator) \citep{borrmann2015control,glotfelter2017nonsmooth,wang2017safety}.
Recent advances in \textit{learning} CBFs using neural networks \citep{saveriano2019learning,srinivasan2020synthesis,lindemann2021learning,peruffo2021automated,dawson2022safe,so2024train,knoedler2024rpcbf} has resulted in works that investigate learning distributed CBFs \citep{qin2021learning,zhang2023neural,zhang2024gcbf+,zinage2024decentralized}.
Nevertheless, these approaches assume \textit{known} dynamics and are only applicable to \textit{continuous-time} dynamics and hence cannot be applied to our problem setting. Moreover, it is assumed that a performant nominal policy is available, which we do not consider in this work.

\noindent\textbf{CBF in RL. }
Originally inspired by the prospect of safety during training, recent works have integrated CBFs into the RL training process via the safety filter \citep{tearle2021predictive,hsu2023safety,garg2024learning} for both single-agent \citep{cheng2019end,emam2022safe,hailemichael2023optimal} and multi-agent \citep{pereira2021safe,pereira2022decentralized} cases.
Although both continuous-time \citep{emam2022safe,hailemichael2023optimal} and discrete-time \citep{cheng2019end} dynamics have been considered,
a major limitation is the requirement of affine (D)CBFs and control-affine dynamics up to a constant disturbance term to be learned.
In contrast, the problem we tackle in this work does not make any such assumptions about the safety specifications or the structure of the dynamics. 

\noindent\textbf{Safe Multi-agent RL. }
The problem of constructing safe policies for MAS has also been studied in the RL community \citep{garg2024learning}.
Early works achieved safety via reward function design \citep{chen2017decentralized,chen2017socially,long2018towards,everett2018motion,semnani2020multi}.
However, these approaches do not guarantee the satisfaction of the safety constraints even for the optimal policy \citep{massiani2022safe,everett2018motion,long2018towards}.
More recently, in the single-agent case, methods work with constraints in the form of the constrained Markov decision process (CMDP) problem and apply techniques from constrained optimization, including 
primal methods \citep{xu2021crpo},
primal-dual methods using Lagrange multipliers \citep{borkar2005actor,tessler2018reward,he2023autocost,huang2024safedreamer},
and via trust-region-based approaches \citep{achiam2017constrained,he2023autocost}.
Of these, Lagrange-multiplier-based approaches are the most popular due to their simplicity,
leading to multi-agent extensions \citep{gu2023safe,liu2021cmix,ding2023provably,lu2021decentralized,geng2023reinforcement,zhao2024multi}.
However, Lagrangian methods for CMDPs have been observed to have unstable training and convergence to poor policies when the constraint threshold is \textit{zero} 
\citep{zanon2020safe,he2023autocost,so2023solving,ganai2024iterative}, which is the setting we target in this work.