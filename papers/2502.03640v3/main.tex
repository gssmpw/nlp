\documentclass{article} \usepackage{iclr2025_conference,times}

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}

\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}

\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\vnu{{\bm{\nu}}}
\def\vpi{{\bm{\pi}}}

\def\va{{\mathbf{a}}}
\def\vb{{\mathbf{b}}}
\def\vc{{\mathbf{c}}}
\def\vd{{\mathbf{d}}}
\def\ve{{\mathbf{e}}}
\def\vf{{\mathbf{f}}}
\def\vg{{\mathbf{g}}}
\def\vh{{\mathbf{h}}}
\def\vi{{\mathbf{i}}}
\def\vj{{\mathbf{j}}}
\def\vk{{\mathbf{k}}}
\def\vl{{\mathbf{l}}}
\def\vm{{\mathbf{m}}}
\def\vn{{\mathbf{n}}}
\def\vo{{\mathbf{o}}}
\def\vp{{\mathbf{p}}}
\def\vq{{\mathbf{q}}}
\def\vr{{\mathbf{r}}}
\def\vs{{\mathbf{s}}}
\def\vt{{\mathbf{t}}}
\def\vu{{\mathbf{u}}}
\def\vv{{\mathbf{v}}}
\def\vw{{\mathbf{w}}}
\def\vx{{\mathbf{x}}}
\def\vy{{\mathbf{y}}}
\def\vz{{\mathbf{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}

\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\usepackage[hypertexnames=false]{hyperref}
\usepackage{url}

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{cleveref}
\usepackage{bbm}
\usepackage{xcolor-material}
\usepackage[inline]{enumitem}
\usepackage{soul}

\usepackage[all=normal,paragraphs=tight,floats=tight,tracking=normal,trackingfraction=0.985]{savetrees}

\Crefname{appendix}{Appendix}{appendices}

\makeatletter
\def\section{\@startsection {section}{1}{\z@}{-1.0ex plus
    -0.5ex minus -.2ex}{0.5ex plus 0.3ex
minus0.2ex}{\large\sc\raggedright}}
\def\subsection{\@startsection{subsection}{2}{\z@}{-0.8ex plus
-0.5ex minus -.2ex}{0.3ex plus .2ex}{\normalsize\sc\raggedright}}
\makeatother

\newcommand{\subjto}{\mathrm{s.t.}}
\renewcommand{\eqref}[1]{(\ref{#1})}
\newtheorem{Definition}{Definition}
\newtheorem{Remark}{Remark}
\newtheorem{Theorem}{Theorem}
\newtheorem{InformalTheorem}[Theorem]{Informal Theorem}
\newtheorem{Corollary}{Corollary}
\newtheorem{Lemma}{Lemma}
\newtheorem{Proposition}{Proposition}
\newtheorem{Assumption}{Assumption}

\DeclareMathOperator{\SafetyFilter}{SafetyFilter}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator*{\esssup}{ess\,sup}

\definecolor{stopgradBlue}{HTML}{065380}
\newcommand*\stopgrad{\textcolor{stopgradBlue}{\psi}}

\newcommand*\dif{\mathop{}\!\mathrm{d}}

\definecolor{ggGreen}{HTML}{8EBA42}
\newcommand{\claim}[1]{#1}

\definecolor{colorStoch}{HTML}{465480}
\definecolor{colorDet}{HTML}{74A6A1}

\newcommand{\chuchu}[1]{\textcolor{red}{[Chuchu: #1]}}

\definecolor{newColor}{HTML}{D80000}
\newcommand{\diff}[1]{\textcolor{newColor}{#1}}
\newcommand{\remove}[1]{\textcolor{newColor}{\st{#1}}}

\newcommand{\ourname}[0]{\textcolor{MaterialBlue900}{\small\textsf{\algname{}}}}
\newcommand{\baselinename}[1]{\textcolor{MaterialPurple900}{\small\textsf{#1}}}

\newcommand{\algname}[0]{DGPPO}

\newcommand{\ind}[1]{\mathbbm{1}_{\{#1\}}}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\newlength{\ucht}
\AtBeginDocument{\settoheight{\ucht}{A}}

\title{Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control}

\author{Songyuan Zhang$^\dagger$ \hskip1em Oswin So$^\dagger$ \hskip1em Mitchell Black$^*$ \hskip1em Chuchu Fan$^\dagger$ \\
$^\dagger$Department of Aeronautics and Astronautics, MIT \hskip1em $^*$MIT Lincoln Laboratory \\
$^\dagger$\texttt{\{szhang21,oswinso,chuchu\}@mit.edu} \hskip1em $^*$\texttt{mitchell.black@ll.mit.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\vspace*{-1ex}
\begin{abstract}
Control policies that can achieve high task performance and satisfy safety constraints are desirable for any system, including multi-agent systems (MAS). One promising technique for ensuring the safety of MAS is distributed control barrier functions (CBF). However, it is difficult to design distributed CBF-based policies for MAS that can tackle unknown discrete-time dynamics, partial observability, changing neighborhoods, and input constraints, especially when a distributed high-performance nominal policy that can achieve the task is unavailable. To tackle these challenges, we propose \textbf{\algname{}}, a new framework that \textit{simultaneously} learns both a \claim{\textit{discrete} graph CBF which handles neighborhood changes and input constraints,} and a \claim{distributed high-performance safe policy for MAS with unknown discrete-time dynamics}.
We empirically validate our claims on a suite of multi-agent tasks spanning three different simulation engines. The results suggest that, compared with existing methods, our \algname{} framework obtains policies that \claim{achieve high task performance (matching baselines that ignore the safety constraints), and high safety rates (matching the most conservative baselines), with a \textit{constant} set of hyperparameters across all environments.\footnote{DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited.}\footnote{Project website: \url{https://mit-realm.github.io/dgppo/}}}
\end{abstract}

\section{Introduction}\label{sec: intro}

Multi-agent systems (MAS) have gained significant attention in recent years due to their potential applications in various domains, such as warehouse robotics \citep{kattepur2018distributed}, autonomous vehicles \citep{shalev2016safe}, traffic routing \citep{wu2020multi}, and power systems \citep{biagioni2022powergridworld}. However, a big challenge for MAS is designing distributed control policies that can achieve high task performance while ensuring safety, especially when the two are conflicting.
In the single-agent continuous-time case, control barrier functions (CBF) are an effective tool to resolve the conflict via the solution of a safety filter quadratic program (QP) \citep{xu2015robustness,ames2017control}, minimally modifying a given performance-oriented nominal policy to be safe.
While distributed CBFs have been proposed for multi-agent \citep{wang2017safety} and partially observable cases \citep{zhang2024gcbf+}, they have a limitation of requiring \textit{known} continuous-time dynamics and a nominal policy that can achieve high task performance (albeit not necessarily safely).

While the assumptions above are reasonable for many applications, they do not apply
when the dynamics are \textit{unknown} and a performance-oriented nominal policy is unavailable.
The challenge of requiring a nominal policy has been addressed by approaches that combine CBFs with reinforcement learning (RL) \citep{cheng2019end,emam2022safe}, where the nominal policy is learned via an unconstrained RL algorithm to maximize task performance while the CBF is used as a safety filter to ensure safety.
However, these works have only been applied to the single-agent case, and require known control-affine dynamics to ensure the resulting safety filter QP is computationally tractable, which is too strict for most systems, e.g., with contact dynamics, especially in discrete time.

A third challenge is that CBF-based methods require a CBF to be known.
This can be challenging in the case of input constraints since not every function satisfies the CBF conditions \citep{chen2021backup}.
Constructing a CBF is even more challenging in the case of \claim{MAS with changing neighborhoods and limited sensing} \citep{zhang2024gcbf+}.
\citet{zhang2024gcbf+} proposed a learning framework for constructing a graph CBF (GCBF) for MAS that guarantees safety 
while satisfying input constraints. However, they assume known continuous-time dynamics and require an existing nominal policy.

In this work, we address these challenges by proposing a novel framework that \claim{simultaneously learns a discrete graph CBF and a high-performance safe policy for a MAS under unknown discrete-time dynamics, changing neighborhoods, and input constraints}. We summarize our contributions below.

\begin{itemize}[leftmargin=1em,topsep=0pt,partopsep=0pt,parsep=0pt]
\item We propose a method of learning discrete CBFs (DCBF) for unknown discrete-time dynamics and with input constraints.
\item We propose the discrete GCBF (DGCBF), a discrete-time extension of the GCBF, for \claim{ensuring safety under varying neighborhoods in the limited sensing setting} and \claim{extend the DCBF learning above to the case of DGCBF}.
    \item We propose \textbf{D}iscrete \textbf{G}raph CBF \textbf{P}roximal \textbf{P}olicy \textbf{O}ptimization (\textbf{\algname{}}), a framework combining RL and DGCBF for \claim{solving discrete-time multi-agent safe optimal control problems for MAS with unknown dynamics and limiting sensing without a known performant nominal policy}.
\item Through extensive simulations, we demonstrate that \claim{\algname{} outperforms existing methods and is not sensitive to hyperparameters}.
    Specifically, compared to existing methods that require different choices of hyperparameters per environment, \algname{} achieves the lowest cost compared to baselines with near $100\%$ safety rate using a single set of hyperparameters.
\end{itemize}

\section{Related work}

\noindent\textbf{Constructing Decentralized CBFs. }
The challenge of applying CBFs for MAS has been explored via the construction of distributed CBFs \citep{borrmann2015control,glotfelter2017nonsmooth,wang2017safety,lindemann2019control,black2023adaptation}, which only take local observations as input.
This simplifies the big centralized QP problem into small QP problems to be solved per agent.
However, the construction is either limited to the case of unbounded control \citep{lindemann2019control}
or only for a specific dynamics model (e.g., double integrator) \citep{borrmann2015control,glotfelter2017nonsmooth,wang2017safety}.
Recent advances in \textit{learning} CBFs using neural networks \citep{saveriano2019learning,srinivasan2020synthesis,lindemann2021learning,peruffo2021automated,dawson2022safe,so2024train,knoedler2024rpcbf} has resulted in works that investigate learning distributed CBFs \citep{qin2021learning,zhang2023neural,zhang2024gcbf+,zinage2024decentralized}.
Nevertheless, these approaches assume \textit{known} dynamics and are only applicable to \textit{continuous-time} dynamics and hence cannot be applied to our problem setting. Moreover, it is assumed that a performant nominal policy is available, which we do not consider in this work.

\noindent\textbf{CBF in RL. }
Originally inspired by the prospect of safety during training, recent works have integrated CBFs into the RL training process via the safety filter \citep{tearle2021predictive,hsu2023safety,garg2024learning} for both single-agent \citep{cheng2019end,emam2022safe,hailemichael2023optimal} and multi-agent \citep{pereira2021safe,pereira2022decentralized} cases.
Although both continuous-time \citep{emam2022safe,hailemichael2023optimal} and discrete-time \citep{cheng2019end} dynamics have been considered,
a major limitation is the requirement of affine (D)CBFs and control-affine dynamics up to a constant disturbance term to be learned.
In contrast, the problem we tackle in this work does not make any such assumptions about the safety specifications or the structure of the dynamics. 

\noindent\textbf{Safe Multi-agent RL. }
The problem of constructing safe policies for MAS has also been studied in the RL community \citep{garg2024learning}.
Early works achieved safety via reward function design \citep{chen2017decentralized,chen2017socially,long2018towards,everett2018motion,semnani2020multi}.
However, these approaches do not guarantee the satisfaction of the safety constraints even for the optimal policy \citep{massiani2022safe,everett2018motion,long2018towards}.
More recently, in the single-agent case, methods work with constraints in the form of the constrained Markov decision process (CMDP) problem and apply techniques from constrained optimization, including 
primal methods \citep{xu2021crpo},
primal-dual methods using Lagrange multipliers \citep{borkar2005actor,tessler2018reward,he2023autocost,huang2024safedreamer},
and via trust-region-based approaches \citep{achiam2017constrained,he2023autocost}.
Of these, Lagrange-multiplier-based approaches are the most popular due to their simplicity,
leading to multi-agent extensions \citep{gu2023safe,liu2021cmix,ding2023provably,lu2021decentralized,geng2023reinforcement,zhao2024multi}.
However, Lagrangian methods for CMDPs have been observed to have unstable training and convergence to poor policies when the constraint threshold is \textit{zero} 
\citep{zanon2020safe,he2023autocost,so2023solving,ganai2024iterative}, which is the setting we target in this work.

\section{Problem setting and preliminaries}
\subsection{Multi-agent constrained optimal control problem}

Consider an $N$ agent MAS. We aim to solve distributed control policies that minimize a joint cost describing a desired task while staying safe. Let the state and control of agent $i$ at timestep $k$ be $x_i^k\in\mathcal X$ and $u_i^k\in\mathcal U$, where $x_i^k$ contains agent $i$'s position $p_i^k\in\mathcal P$.
The joint state is defined as $\vx^k \coloneqq [x_1^k;\dots;x_N^k;y^k]\in \bm{\mathcal{X}}$, where $y^k\in\mathcal Y$ is non-agent states (e.g., obstacles, goals).
The joint action is defined with $\vu^k \coloneqq [u_1^k;\dots,u_N^k]\in\bm{\mathcal{U}}$. We assume $\vx$ follows the general discrete-time dynamics
\begin{equation}\label{eq: discrete dyn}
    \vx^{k+1} = f(\vx^k, \vu^k),
\end{equation}
where $f: \bm{\mathcal{X}}\times\bm{\mathcal{U}}\to\bm{\mathcal{X}}$ describes the joint dynamics and is \textit{unknown}.
We consider the setting where agents only observe objects within their sensing radius $R>0$.
Let $\mathcal N_i(\vx) = \{j\mid \norm{p_j - p_i} \leq R\}$ denote the \textit{neighborhood} of agent $i$. 
At timestep $k$, agent $i$ only has access to local observation $o_i^k \coloneqq O_i(\vx^k) \in \mathcal{O}$ for observation function $O_i$:
\begin{equation} \label{eq: obs_fn}
    O_i(\vx^k) = \Big( \{o_{ij}\}_{j\in\mathcal N_i},\; o_i^y \Big),\qquad o_{ij}\coloneqq O^a(x_i, x_j), \quad o_i^y\coloneqq O^y(x_i, y)
\end{equation}
for inter-agent $O^a : \mathcal{X} \times \mathcal{X} \to \mathcal{O}_a$ and non-agent $O^y : \mathcal{X} \times \mathcal{Y} \to \mathcal{O}_y$ observation functions.
Let the joint cost function describing the desired task be denoted as $l:\bm{\mathcal{X}}\times\bm{\mathcal U}\to\mathbb R$
\footnote{The cost function $l$ here is \textbf{not} the \textit{cost} in CMDP. Rather, it corresponds to the \textit{negative reward} in CMDP.}.
Each agent has an avoid set defined as $\mathcal A_i \coloneqq \{o_i\in\mathcal O\mid h_{i}^{(m)}(o_i)>0, \forall m\}$ for the avoid functions $h_i^{(m)}:\mathcal O\to\mathbb R$, $m \in \{1, \dots, M\}$, such that the agent is \textit{unsafe} it it enters $\mathcal A_i$ any time in the trajectory.
We want to learn distributed policies $\mu_i:\mathcal{O}\to\mathcal{U}$ that minimize the joint cost $l$ while avoiding the avoid set $\mathcal A_i$ at all times.
Formally, denoting the joint policy by $\vmu(\vx) = [\mu_1(o_1);\, \dots;\, \mu_N(o_N)]$, we want to solve the following discrete-time distributed multi-agent safe optimal control problem (MASOCP):
\begin{subequations}\label{eq: macocp}
\begin{align}
    \min_{\mu_1, \dots, \mu_N} \quad& \sum_{k=0}^\infty l(\vx^k,\vmu(\vx^k)) \label{eq: macocp:cost}\\
    \subjto \quad& \vx^{k+1} = f(\vx^k, \vmu(\vx^k)),  & \forall i\in\{1,\dots,N\},\, k\geq 0, \\
    & h_i^{(m)}(o_i^k) \leq 0, \quad o_i^k = O_i(\vx^k), & \forall i\in\{1,\dots,N\},\, \forall m \in \{1, \dots, M\},\, k\geq 0.\label{eq: hbar}
\end{align}
\end{subequations}
The main challenge in solving \eqref{eq: macocp} is satisfying the safety constraints \eqref{eq: hbar}, especially in the multi-agent case with changing neighborhood, under unknown discrete-time dynamics with input constraints.
We propose to tackle this challenge using the framework of DCBFs, which we review next.

\subsection{Discrete CBF} \label{sec: prelim:dcbf}
To tackle the safety constraint of Problem \eqref{eq: macocp}, we review the notion of DCBF. Considering the discrete-time dynamics, we take the following definition of a DCBF from \citet{ahmadi2019safe}:
\begin{Definition} \label{def:dcbf}
    A function $B : \bm{\mathcal{X}} \to \mathbb{R}$ is a discrete CBF (DCBF) for \eqref{eq: discrete dyn} if there exists an extended class-$\kappa$ function $\alpha$ satisfying $\alpha(-r) > -r$ for all $r > 0$ such that $B$ satisfies the following property:
    \begin{equation}\label{eq: dcbf-condition}
        B(\vx)\leq 0 \implies \inf_{\vu \in \bm{\mathcal{U}}} B(f(\vx, \vu)) - B(\vx) + \alpha(B(\vx)) \leq 0.
    \end{equation}
\end{Definition}
As shown in \citet{ahmadi2019safe}, the following theorem holds.
\begin{Theorem} \label{thm:dcbf_invariant}
    The set $\mathcal{C} \coloneqq\{ \vx \mid B(\vx) \leq 0 \}$ is control invariant under any policy $\vmu$ that satisfies
    \begin{equation}\label{eq: dcbf-descent}
        B( f(\vx, \vmu(\vx)) ) - B(\vx) + \alpha(B(\vx)) \leq 0, \quad \forall \vx \in \mathcal{C}
    \end{equation}
\end{Theorem}
Thus, if $\mathcal{C} \cap \mathcal{A} = \emptyset$ for the avoid set $\mathcal{A}$,
then the $\vmu$ from \Cref{thm:dcbf_invariant} renders the system safe.

\noindent\textbf{Safe and Performant Policies via Safety Filtering. }
Given DCBFs $B^{(m)}$ for $m = 1, \dots, M$, we can construct a safe and performant policy using the safety filter framework. Assuming a nominal policy $\vmu_\mathrm{nom}$ that is performant, e.g., minimizes the cost $l$, but not necessarily safe, we can obtain a safe \textit{and} performant policy by solving the following nonlinear optimization problem:
\begin{subequations} \label{eq: dcbf_safety_filter}
\begin{align}
    \min_{\vu \in \bm{\mathcal{U}}}\quad & \norm{\vu - \vmu_\mathrm{nom}(\vx)}^2, \label{eq: dcbf_safety_filter:obj}\\
    \mathrm{s.t.}\quad & C^{(m)}(\vx, \vu) \leq 0, \quad C^{(m)}(\vx, \vu) \coloneqq B^{(m)}(f(\vx, \vu)) - B^{(m)}(\vx) + \alpha(B^{(m)}(\vx)), \quad \forall m. \label{eq: cbf-program-constraint}
\end{align}
\end{subequations}
In the general case, even if the dynamics $f$ are known, \eqref{eq: dcbf_safety_filter} is potentially a nonlinear program that could be difficult to solve.
In our problem setting, we assume $f$ to be \textit{unknown}, which renders this approach infeasible.
Moreover, applying the safety filter framework assumes access to a performant, \textit{distributed} nominal policy $\vmu_\mathrm{nom}$, which we do not assume is available.
Even if it were available, the safety filtering only minimizes the instantaneous deviation in control \eqref{eq: dcbf_safety_filter:obj} and can be \textit{myopic}, potentially leading to liveness problems \citep{reis2020control} and deadlocks \citep{jankovic2023multiagent}.

Note that not every function satisfies \eqref{eq: dcbf-descent} and is a DCBF.
In continuous time with control-affine dynamics $f$, unbounded inputs are sufficient for any continuously differentiable function to be a CBF \citep{xiao2019control} since the CBF condition will be \textit{linear} in the controls.
However, in discrete time, the DCBF condition \eqref{eq: dcbf-descent} is potentially \textit{nonlinear} in $\vu$, making it difficult to construct a DCBF even with unbounded controls.
While solutions have been proposed for the continuous-time case to construct valid CBFs under bounded inputs \citep{chen2021backup,so2024train}, the same is not true for DCBFs, potentially due to the requirement of solving a nonlinear program \eqref{eq: dcbf_safety_filter} even after such a DCBF has been found.

\section{Tackling Challenges of DCBFs for MASOCP with DGPPO} \label{sec: challenges}
We now address four challenges of extending a DCBF-based approach to our problem setting and propose \algname{}, our framework for solving \eqref{eq: macocp}.
\textbf{All proofs can be found in \Cref{app: proofs}.}

\subsection{Constraint-value function is DCBF}\label{sec: value-function-cbf}
To construct a DCBF, we show
that the constraint-value function of a policy \textit{is} a DCBF, extending the insights of \citet{so2024train} to discrete-time.
This allows learning a DCBF with policy evaluation.

For an arbitrary function $\zeta : \bm{\mathcal{X}} \to \mathbb{R}$, let the avoid set be $\mathcal A \coloneqq \{ \vx\in \bm{\mathcal{X}} \mid \zeta(\vx)>0\}$.
For a fixed deterministic policy $\vmu$, consider the constraint-value functions $V^{\zeta, \vmu}$, defined as
\begin{equation}
    V^{\zeta, \vmu}(\vx) = \max_{k\geq0} \zeta(\vx^k),\qquad \mathrm{s.t.}\quad \vx^0 = \vx,\quad \vx^{k+1} = f(\vx^k, \vmu(\vx^k)).
\end{equation}
Then, $V^{\zeta,\vmu}$ is a DCBF, which we show in the following theorem.
\begin{Theorem}[Discrete Policy CBF] \label{thm:dpcbf}
    For a given $\vmu$, the constraint-value function $V^{h,\vmu}$ is a DCBF for any extended class-$\kappa$ function $\alpha$ satisfying $\alpha (-r) > -r$ for all $r>0$.
Moreover, given $h^{(m)}$, $\vmu$ satisfies $V^{h^{(m)},\vmu}(f(\vx,\vu)) - V^{h^{(m)},\vmu}(\vx) + \alpha( V^{h^{(m)},\vmu}(\vx) ) \leq 0$ for all $m \in \{1, \dots, M\}$.
\end{Theorem}
\Cref{thm:dpcbf} enables the construction of a DCBF by choosing any policy $\vmu$ and evaluating its constraint-value function. 
Consequently, we can construct a DCBF by learning the value function.

\subsection{Removing the Nominal Policy with Explicit Cost Optimization}\label{sec: cost-optimization}
We next address the challenge of requiring a performant nominal policy in the safety filter \eqref{eq: dcbf_safety_filter} by instead directly learning a policy using RL that minimizes the joint cost function \eqref{eq: macocp:cost}. 
This has been done previously in the single-agent setting via the framework of shielding for RL, where an unconstrained policy $\vmu_\theta$ with parameters $\theta$ is learned using existing unconstrained RL techniques, and the (D)CBF safety filter is incorporated into the environment dynamics \citep{cheng2019end,emam2022safe,hailemichael2023optimal}. Formally, the following problem is considered:
\begin{equation} \label{eq: cbfrl}
    \min_{\vmu_{\theta}}\quad \sum_{k=0}^\infty l(\vx^k,\vu^k), \qquad \mathrm{s.t.}\quad \vu^k = \SafetyFilter(\vmu_{\theta}(\vx^k)),
\end{equation}
where $\SafetyFilter$ computes the minimizer of \eqref{eq: dcbf_safety_filter}.
However, solving \eqref{eq: dcbf_safety_filter} in the discrete case is a nonlinear program that is difficult unless $B^{(m)}$ is linear and the dynamics are control-affine,
an assumption that we, unlike previous works \citep{cheng2019end,emam2022safe,hailemichael2023optimal}, do not impose.
To work around this,
we constrain the learned policy to satisfy the DCBF conditions instead of using the safety filter framework. 
\begin{subequations}\label{eq: dcbf_ocp}
\noindent\begin{minipage}[t]{.32\linewidth}
    \vspace{-1.1em}
\begin{equation}
    \hspace{-.8em}\min_{\theta} \sum_{k=0}^\infty l(\vx^k,\vmu_\theta(\vx^k)), \label{eq: dcbf_ocp:obj}
\end{equation}
    \vspace{.0em}
\end{minipage}\begin{minipage}[t]{.68\linewidth}
    \vspace{-1.1em}
\begin{equation}
    \vphantom{\sum_{k=0}^\infty}
    \mathrm{s.t.}\mspace{10mu} C^{(m)}(\vx^k, \vmu_\theta(\vx^k)) \leq 0, \mspace{10mu} \forall m = \{ 1, \dots, M \}, \mspace{10mu} k \geq 0, \label{eq: dcbf_ocp:constraint}
\end{equation}
    \vspace{.0em}
\end{minipage}
\end{subequations}
where $C^{(m)}$ is defined in \eqref{eq: cbf-program-constraint}.
This removes the need for a nominal policy and for solving the nonlinear safety filter program \eqref{eq: dcbf_safety_filter}.

\subsection{Constrained Policy Optimization using DCBF Under Unknown Dynamics}\label{sec: cbf-model-free}
We now tackle the challenge of performing constrained policy optimization using DCBFs in \eqref{eq: dcbf_ocp}. 
We choose to use a purely primal method inspired by constraint-rectified policy optimization (CRPO) \citep{xu2021crpo} due to its simplicity (it does not have parameters related to the dual problem but simply chooses to take different gradient steps based on the current constraint satisfaction).
Specifically, if the constraint \eqref{eq: dcbf_ocp:constraint} is satisfied, our algorithm takes one gradient step to minimize the objective \eqref{eq: dcbf_ocp:obj}. Otherwise, if the constraint is violated, our algorithm takes one gradient step to minimize the DCBF constraint violation $C$.

This procedure still cannot be implemented as-is since the gradient of $C$ cannot be computed directly without knowledge of the dynamics $f$.
To this end,
we propose to use score function gradients \citep{williams1992simple}
to compute gradients of \eqref{eq: dcbf_ocp:constraint} without knowing the dynamics $f$.
Since this requires a \textit{stochastic} policy, we modify \eqref{eq: dcbf_ocp}
accordingly.
For clarity, let $\vpi_\theta(\cdot|\vx)$ denote the probability density function of the stochastic policy with parameters $\theta$ conditioned on state $\vx$. It can be tempting to consider the following stochastic version of the problem, which resembles the CMDP setting.
\begin{equation} \label{eq: dcbf_safety_filter:stoch_ver1}
\min_{\theta}\quad \mathbb E_{\vx \sim\rho_{0}, \vu \sim \vpi_\theta(\cdot|\vx)} \big[ Q^{\vpi_\theta}(\vx,\vu) \big], \qquad
\mathrm{s.t.}\quad \mathbb E_{\vx\sim\rho^{\vpi_\theta}, \vu\sim\vpi_\theta(\cdot|\vx)} \left[ C^{(m)}(\vx, \vu) \right] \leq 0,
\end{equation}
where $Q^{\vpi_\theta}$ is the Q-function and $\rho_0$, $\rho^{\vpi_\theta}$ are the initial and stationary state distributions.
Here, we constrain the \textit{expectation}
of the DCBF constraint. However, this formulation is \textit{not} sufficient for safety,
as the expectation does not guarantee
satisfaction for all states
as in \eqref{eq: dcbf_safety_filter}, leading to an unsafe policy.
To tackle this, we modify the constraint, leading to the following problem.
\begin{subequations} \label{eq: dcbf_safety_filter:stoch_ver2}
\begin{align}
\min_{\theta}\quad & \mathbb E_{\vx \sim \rho_{0}, \vu\sim\vpi_\theta(\cdot|\vx)}
    \big[ Q^{\vpi_\theta}(\vx, \vu) \big], \label{eq: dcbf_safety_filter:stoch_ver2:objective} \\[-2pt]
\mathrm{s.t.}\quad & \mathbb E_{\vx\sim\rho^{\vpi_\theta}}\underbrace{\E_{\vu\sim\vpi_\theta(\cdot \mid \vx)} \left[ \max\big\{0, C^{(m)}(\vx, \vu) \big\} \right]}_{\coloneqq \tilde{C}^{(m)}_\theta(\vx)} \leq 0, \quad \forall m. \label{eq: dcbf_safety_filter:stoch_ver2:constraint}
\end{align}
\end{subequations}
Here, the satisfaction of \eqref{eq: dcbf_safety_filter:stoch_ver2:constraint} guarantees that the DCBF constraint is satisfied almost surely (\Cref{app: pf:almost_sure}). Applying gradient-manipulation style primal optimization as in CRPO \citep{xu2021crpo} gives us the following expression for the gradient $\nabla_\theta L$ of the policy loss $L$.
\begin{equation} \label{eq: crpo_ver2}
    \nabla_\theta L(\theta) = \begin{dcases}
\nabla_\theta \mathbb E_{\vx \sim \rho_{0}, \vu \sim \vpi_\theta(\cdot|\vx)}
    \big[ Q^{\vpi_\theta}(\vx, \vu) \big], & \mathbb E_{\vx \sim \rho^{\vpi_\theta}}\big[ \tilde{C}^{(m)}_\theta(\vx) \big] \leq 0,\; \forall m, \\
\nu\, \mathbb{E}_{\vx \sim \rho^{\vpi_\theta}}\big[ \nabla_\theta \tilde{C}^{(m)}_\theta(\vx) \big]\text{ for any $m$ that violates}, & \text{otherwise},
    \end{dcases}
\end{equation}
where $\nu > 0$ is a hyperparameter scaling the size of constraint minimization steps.
The gradient of $\tilde{C}_\theta^{(m)}$ can be computed using score function gradients (see \Cref{app: policy-gradient}, discussion on $\rho^{\vpi_\theta}$ in \Cref{app: pol_loss_details:pol_grad}).

\noindent\textbf{Improving sample efficiency with Gradient Projection. }
One drawback of \eqref{eq: crpo_ver2} is that this scheme is sample-inefficient in the sense that if only a single state $\bar{\vx}$ violates the DCBF constraint, 
then the gradient information of the total cost from all other safe states are thrown away. 
We propose to use this thrown-away gradient information by \textit{projecting} the cost gradient to be \textit{orthogonal} to the gradient direction of the violating constraints,
similar to techniques from multi-objective optimization \citep{yu2020gradient,liu2021conflict}.
However, this requires computing the gradient $M+1$ times,
which is expensive when $M$ is large.
Instead, we propose to use the following informal theorem:

\begin{InformalTheorem}[Approximate Gradient Projection for Decoupled Policy Parameters] \label{thm: grad_proj}
    Let $\sigma^{(m)} \coloneqq \nabla_\theta \E_{\vx \sim \rho}[ \tilde{C}_\theta^{(m)}(\vx) ]$ denote the gradient of the $m$-th DCBF constraint violation for any state distribution $\rho$.
    Under suitable assumptions on the policy parametrization $\vpi_\theta$, modifying the gradient of the objective \eqref{eq: dcbf_safety_filter:stoch_ver2:objective} from
    $g_{\text{orig}} \coloneqq \mathbb{E}_{\vx \sim \rho^{\vpi_\theta}}
    \mathbb{E}_{
        \vu \sim \vpi_\theta(\cdot|\vx)}
        \left[ 
        \nabla_{\theta} \log \vpi(\vx, \vu)\,
        Q^{\vpi_\theta}(\vx, \vu)
        \right]
    $
    to
    \begin{equation}
        g \coloneqq
        \mathbb{E}_{\vx \sim \rho^{\vpi_\theta}}
        \mathbb{E}_{
        \vu \sim \vpi_\theta(\cdot|\vx)}
        \left[ 
        \nabla_{\theta} \log \vpi(\vx, \vu)\,
        \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0}\,
        Q^{\vpi_\theta}(\vx, \vu)
        \right],
    \end{equation}
    by multiplying the $Q^{\vpi_\theta}$ with an indicator function
    gives an approximate projection $g$ of $g_{\text{orig}}$ such that $g \cdot \sigma^{(m)} = 0\; \forall m$, i.e., it lies in the orthogonal complement of the constraint gradients $\sigma^{(m)}$.
\end{InformalTheorem}

We state this more formally in \Cref{thm: grad_proj_formal}. By \Cref{thm: grad_proj}, we can treat $g$ as an approximate projection of the gradient of the objective \eqref{eq: dcbf_safety_filter:stoch_ver2:objective} so that it does not interfere with the gradients from the safety constraints.
Combining $\sigma^{(m)}$ (with $\rho = \rho^{\pi_\theta}$) and $g$ from \Cref{thm: grad_proj} gives the following gradient $\nabla_\theta L$ of the policy loss $L$,
where $\stopgrad$ denotes the \textit{stop gradient} operation 
(see \Cref{app: pol_loss_details} for a detailed derivation and discussion about important details).
\begin{align} \label{eq: dcbf_safety_filter:decoupled}
    L(\theta)
    &= \E_{\vx \sim \stopgrad(\rho^{\vpi_\theta})}
    \E_{\vu \sim \stopgrad(\vpi_\theta(\cdot|\vx))}
    \big[ \log \vpi_\theta(\vx, \vu) \stopgrad\big( \tilde{Q}(\vx, \vu, \theta) \big) \big], \\
\tilde{Q}(\vx, \vu, \theta) &\coloneqq
    \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0} \stopgrad(Q^{\vpi_\theta}(\vx, \vu)) + \nu \max_m \tilde{C}^{(m)}(\vx, \vu).
\end{align}
Although we cannot guarantee that the conditions of \Cref{thm: grad_proj} hold,
we find that adding this gradient projection improves performance.
We provide empirical comparisons between this gradient projection method \eqref{eq: dcbf_safety_filter:decoupled} and methods without projection \eqref{eq: dcbf_safety_filter:stoch_ver1}, \eqref{eq: dcbf_safety_filter:stoch_ver2} in \Cref{app: decouple-vs-couple}.

\noindent\textbf{Scheduling the weight $\nu$. }
Empirically, different values of $\nu$ result in a tradeoff between safety and cost minimization (potentially due to not satisfying \Cref{thm: grad_proj}).
To remedy this, we schedule $\nu$ by taking $\nu=1$ initially to encourage exploration at the beginning, then doubling it after $50\%$ and $75\%$ of the total update steps to emphasize safety.
We investigate this further in \Cref{sec: ablation}.

\subsection{Extending to the Multi-Agent Case with DGCBF} \label{sec:method:multi_agent}

Having proposed three solutions to tackle the difficulty of using CBFs in RL for unknown discrete-time dynamics, 
we now \claim{tackle the final challenge of changing neighborhoods due to the limited sensing radius of each agent}.
For this, we draw inspiration from GCBF \citep{zhang2024gcbf+}, which provides a theoretical framework for constructing distributed CBFs that can handle varying neighborhood sizes, albeit for the case of continuous-time dynamics.

To construct a distributed DCBF $B$, 
we need $B$ to be a function of each agent's local observation $o_i$ as opposed to the joint state $\vx$. We make this concrete in the following definition.
\begin{Definition}[Discrete GCBF] \label{def: dgcbf}
    A function $\tilde{B} : \mathcal{O} \to \mathbb{R}$ is a Discrete Graph CBF (DGCBF) if there exists a class-$\kappa$ function $\alpha$ with $\alpha(-r) > -r$ for all $r > 0$ and a control policy $\mu : \mathcal{O} \to \mathcal{U}$ satisfying 
    \begin{equation} \label{eq: dgcbf}
        \tilde{B}( o_j(\vx) ) \leq 0,\; \forall j \implies \tilde{B}( o_i^+ (\vx) ) - \tilde{B}( o_i(\vx) ) + \alpha( \tilde{B}( o_i(\vx) ) ) \leq 0,
        \quad\forall \vx \in \bm{\mathcal{X}},\; \forall i,
    \end{equation}
    where $o_i^+(\vx) = O_i( f(\vx, \vmu(\vx) ))$ and $\vmu$ denotes the resulting joint policy from each agent using $\mu$.
\end{Definition}

Since a DCBF $B$ is defined for a MAS with a \textit{fixed} size $N$, $B$ can not be used when $N$ changes.
Given a DGCBF $\tilde{B}$, we can construct a DCBF $B$ as $B(\vx) \coloneqq \max_i \tilde{B}(o_i(\vx))$ (\Cref{app: dgcbf_dcbf}),
thus $\tilde{B}$ guarantees safety.
However, the \textit{same} DGCBF $\tilde{B}$ can \textit{also} be used to guarantee safety for \textit{any} $N$, which we show in \Cref{app: dgcbf_safety_proof}.

\begin{Remark}[Discontinuity due to neighborhood changes]
    Unlike the continuous-time case \citep{zhang2024gcbf+}, which assumes that the GCBF is unaffected by agents at the sensing-radius boundary, DGCBF does not have this requirement.
This is because the proof of safety in GCBF relies on continuity (w.r.t. time) during neighborhood changes.
    However, in the discrete-time case, the proof of safety only looks at finite differences and hence does not require the continuity of the DGCBF. 
\end{Remark}

Finding a function $\tilde B$ that satisfies \eqref{eq: dgcbf} is nontrivial, 
especially in the case of neighborhood changes due to the limited sensing radius.
Though we can learn a DGCBF using \Cref{sec: value-function-cbf},
it is unclear how the learned function satisfies \eqref{eq: dgcbf} when the neighborhood of agents changes.
One sufficient way for this to hold is to take advantage of the attention mechanism to place zero weights on the features corresponding to agents far enough away such that the value of $\tilde{B}$ is not affected too much by such agents.

We state this informally in the following theorem (see \Cref{app: pf:attn_dgcbf} for the formal version).
\begin{InformalTheorem}[Satisfying \eqref{eq: dgcbf} during neighborhood changes]\label{thm:attn_dgcbf}
    Let $\xi_1$, $\xi_2$, and $\xi_3$ be functions that encode 
    the input observations into some feature space.
    Suppose $\tilde{B}$ is of the form
    \begin{equation} \label{eq: tilde_B_attn_form}
        \tilde{B}(O_i(\vx)) = \xi_1\left( \sum_{j \in \mathcal{N}_i} w(o_{ij}) \xi_2(o_{ij}),\; \xi_3(o_i^{y}) \right),
    \end{equation}
    where $w : \mathcal{O}_a \to \mathbb{R}$ is a weighting function that goes to $0$ for observations $o_{ij}$ of agents that are far enough away.
    Then, under technical conditions on the dynamics, $\tilde{B}$ satisfies \eqref{eq: dgcbf} and is a DGCBF.
\end{InformalTheorem}

We encourage $\tilde{B}$ to satisfy \eqref{eq: dgcbf} during neighborhood changes by parameterizing the value functions using graph neural networks (GNN) with graph attention \citep{velivckovic2017graph}, which takes the form of \eqref{eq: tilde_B_attn_form} and hence is amenable to \Cref{thm:attn_dgcbf}.
Similar to \citet{zhang2024gcbf+}, the attention mechanism naturally learns to place zero weights on the features corresponding to neighboring agents that are far enough away, enabling the learned $\tilde{B}$ to satisfy \eqref{eq: dgcbf} during neighborhood changes. 

Finally, \Cref{thm:dpcbf} can be extended to DGCBF, as shown in the following Corollary.
\begin{Corollary}[Discrete Policy GCBF] \label{thm:dpgcbf}
    Suppose
$V^{h^{(m)},\vmu}_i : \bm{\mathcal{X}} \to \mathbb{R}$ for agent $i$ can be expressed using only local observations $o_i$, i.e., there exists some $\tilde{V}^{h^{(m)}, \vmu} : \mathcal{O} \to \mathbb{R}$ such that
    \begin{equation}
        \tilde{V}^{h^{(m)},\vmu}(o^0_i) = V^{h^{(m)}, \vmu}_i(\vx^0) \coloneqq \max_{k \geq 0} h^{(m)}(o^k_i).
    \end{equation}
    Then, $\tilde{V}^{h^{(m)}}$ is a DGCBF.
\end{Corollary}

\begin{figure}[t]
    \centering
    \vspace{-1.5ex}
    \includegraphics[width=.9\columnwidth]{figs/algorithm_v2.pdf}
    \vspace*{-1.5ex}
    \caption{\textbf{\algname{} algorithm.} In addition to the normal MAPPO path (top) using \textcolor{colorStoch}{\textbf{stochastic rollouts}}, we introduce a second path (bottom) that uses \textcolor{colorDet}{\textbf{deterministic rollouts}} to learn a DGCBF.
    }
    \label{fig: algorithm}
\vspace*{-2ex}
\end{figure}
\subsection{DGPPO: Putting everything together} \label{sec: final_method}
Combining the proposed solutions from the previous subsections, we present \textbf{\algname{}}, a framework for solving the discrete-time multi-agent safe optimal control problem \eqref{eq: macocp}.
\algname{} follows the basic structure of on-policy MARL algorithms such as MAPPO \citep{yu2022surprising}.
\begin{enumerate}[leftmargin=1em, topsep=0pt,itemsep=-0.9ex,partopsep=1ex,parsep=1ex]
    \item We perform a $T$-step stochastic rollout with the policy $\vpi_\theta$. However, unlike MAPPO, we additionally perform a $T$-step \textit{deterministic} rollout using a deterministic version of $\vpi_\theta$ (by taking the mode), which we denote $\vmu$, to learn the DGCBF (per \Cref{thm:dpcbf}).
\item We update the value functions via regression on the corresponding targets computed using GAE \citep{schulman2015high}, where the targets for 
    the cost-value function $V^l$ uses the stochastic rollout, and the targets for the constraint-value functions $V^{h^{(m)},\vmu}$ use the deterministic rollout.
\item We update the policy $\vpi_\theta$ by replacing the $Q$-function with its GAE \citep{schulman2015high}, then combining the CRPO-style \textit{decoupled} policy loss \eqref{eq: dcbf_safety_filter:decoupled} with the PPO clipped loss \citep{schulman2017proximal} using the learned constraint-value functions $V^{h^{(m)},\vmu}$ as the DGCBFs $\tilde{B}^{(m)}$.
    Specifically, we treat the expression within the expectation in \eqref{eq: dcbf_safety_filter:decoupled} as a pseudo-advantage $\tilde{A}_i$ for agent $i$
    and use a single-sample estimator $\hat{C}^{(m)}_{\theta,i}$ of $\tilde{C}^{(m)}_{\theta,i}$ in \eqref{eq: dcbf_safety_filter:decoupled}, 
    giving us
\begin{align}
\hat{C}^{(m)}_{\theta,i} &\coloneqq \max\left\{ 0,\, V^{h^{(m)},\vmu}(o_i^+) - V^{h^{(m)},\vmu}(o_i) + \alpha(V^{h^{(m)},\vmu}(o_i)) \right\}, \\
\tilde{A}_i & \coloneqq A^{\mathrm{GAE}} \ind{\max_m \hat{C}_{\theta,i}^{(m)} \leq 0} + 
        \nu \max_m \hat{C}_{\theta,i}^{(m)}
        \ind{\max_m \hat{C}_{\theta,i}^{(m)} > 0}
\end{align}
where $A^{\mathrm{GAE}}$ denotes the GAE \citep{schulman2015high} for agent $i$.
    We then use $\tilde{A}_i$ in the PPO policy loss \citep{schulman2017proximal} as done in MAPPO \citep{yu2022surprising}.
\end{enumerate}
We summarize our \algname{} algorithm in \Cref{fig: algorithm}.

\section{Experiments}\label{sec: experiments}

In this section, we design experiments to answer the following research questions:
\textbf{(Q1)} Does \algname{} learn a safe policy that also achieves low costs without hyperparameter tuning in different environments?
\textbf{(Q2)} How stable is the training of \algname{}?
\textbf{(Q3)} Can \algname{} maintain its performance with an increasing number of agents?
\textbf{(Q4)} Is \algname{} sensitive to the hyperparameters?

To compare the methods, we look at the cost and safety rate.
The \textbf{cost} is the trajectory cumulative cost $\sum_{k=0}^{T}l(\vx^k,\vu^k)$. The \textbf{safety rate} is the ratio of agents that are safe over the \textit{entire} trajectory. 
Details on implementation, tasks, hyperparameters, and additional experiments are in \Cref{app: experiments}.

\begin{figure}[t]
    \centering
    {
    \newcommand{\fwidth}{.22\linewidth}
    \newcommand{\fwidthA}{.02\linewidth}
    \newcommand{\fwidthB}{.04\linewidth}
    \begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=\columnwidth]{figs/env/LidarNav.pdf}
        \vspace*{-4ex}
        \caption{\small\textsc{Target}}
        \label{fig: lidarnav}
    \end{subfigure}\hspace{\fwidthA{}}\begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=\columnwidth]{figs/env/LidarSpread.pdf}
        \vspace*{-4ex}
        \caption{\small\textsc{Spread}}
        \label{fig: lidarspread}
    \end{subfigure}\hspace{\fwidthA{}}\begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=\columnwidth]{figs/env/LidarLine.pdf}
        \vspace*{-4ex}
        \caption{\small\textsc{Line}}
        \label{fig: lidarline}
    \end{subfigure}\hspace{\fwidthA{}}\begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=\columnwidth]{figs/env/LidarBicycle.pdf}
        \vspace*{-4ex}
        \caption{\small\textsc{Bicycle}}
        \label{fig: lidarbicycle}
    \end{subfigure}

    \vspace{0.9ex}
    \begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=\columnwidth]{figs/env/Transport.pdf}
        \vspace*{-3.1ex}
        \caption{\small\textsc{Transport}}
        \label{fig: transport}
    \end{subfigure}\hspace{\fwidthB{}}\begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=0.98\columnwidth]{figs/env/VmasWheel.pdf}
        \vspace*{-3.1ex}
        \caption{\small\textsc{Wheel}}
        \label{fig: wheel}
    \end{subfigure}\hspace{\fwidthB{}}\begin{subfigure}{\fwidth{}}
        \centering
        \includegraphics[width=0.98\columnwidth]{figs/env/VmasTransport2.pdf}
        \vspace*{-3.1ex}
        \caption{\small\textsc{Transport2}}
        \label{fig: transport2}
    \end{subfigure}
    }
    \vspace*{-1.5ex}
    \caption{\textbf{Environments. } We test on (top) LiDAR, (bottom) MuJoCo, and VMAS environments.}
    \vspace*{-.4ex}
\end{figure}

\subsection{Setup}

\noindent\textbf{Environments. }
We evaluate \algname{} in a wide range of environments including four LiDAR environments (\textsc{Target}, \textsc{Spread}, \textsc{Line}, \textsc{Bicycle}) where the agents use LiDAR to detect obstacles \citep{keyumarsi2023lidar}, one MuJoCo environment \textsc{Transport} \citep{mujoco}, and two VMAS environments (\textsc{Transport2}, \textsc{Wheel}) \citep{bettini2022vmas,bettini2024benchmarl}.

\noindent\textbf{Baselines. }
We compare \ourname{} against baseline methods that can solve MASOCP under unknown discrete-time dynamics, including the state-of-the-art MARL algorithm InforMARL \citep{nayak2023scalable} and the safe MARL algorithm MAPPO-Lagrangian \citep{gu2021multi,gu2023safe}. 
For InforMARL, we add the constraint violations $\max\{0, \max_m h^{(m)} \}$ weighted by $\beta$ to the cost function for different $\beta$s (\baselinename{Penalty($\beta$)}).
We also try a weight-scheduling scheme where $\beta$ starts at $0.01$ and increases at $50\%$ and $75\%$ of the total steps (\baselinename{Schedule}).
For MAPPO-Lagrangian, we use a GNN backbone for fair comparison.
We notice the official implementation \citep{gu2023safe} uses a tiny learning rate on the Lagrange multipliers ($10^{-7}$),
so we consider two different initialization $\lambda_0$ (\baselinename{Lagr($\lambda_0$)}).
We also increase the learning rate of $\lambda$ to $0.1$\footnote{This is the smallest learning rate of $\lambda$ that does not make the algorithm ignore the safety constraints.} (\baselinename{Lagr(lr)}). 
We run each method for the same number of update steps, chosen to be large enough such that all methods converge.\footnote{We also test baselines with double environment steps similar to \algname{} for fairness (\Cref{app: more-data}).}

\begin{figure}[t]
    \centering
    \includegraphics[width=.995\columnwidth]{figs/experiment/safe_cost_main.pdf}
    \vspace*{-.4em}
\caption{\textbf{Comparison on $N = 3$ agents.}
    \protect\includegraphics[height=1.05\ucht]{figs/err_bars.pdf} denotes the mean $\pm$ standard deviation.
    Methods closer to the top left yield lower costs and higher safety rates.}
\label{fig: safe_cost_main}
\end{figure}

\subsection{Main results}\label{sec: results}
For each environment, we run each algorithm with $3$ different seeds and evaluate each run on $32$ different initial conditions.
We draw the following conclusions. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.995\columnwidth]{figs/experiment/train_main.pdf}
    \vspace*{-0.8em}
    \caption{\textbf{Training stability.} \ourname{} yields smoother training curves compared to the baselines.}
    \label{fig: train_main}
    \vspace{-.4em}
\end{figure}

\noindent\textbf{(Q1): \ourname{} has the best performance and is hyperparameter insensitive. } 
We first compare the converged policies of all algorithms (\Cref{fig: safe_cost_main}).
\ourname{} is closest to the top left corner in all environments, indicating that it performs the best.
For \baselinename{Penalty} and \baselinename{Lagr},
different choices of hyperparameters result in either focusing only on safety or focusing only on performance.
Even with a fixed hyperparameter, the performance of these two baselines also varies between environments.
On the other hand, using the same set of hyperparameters in all environments, \ourname{} consistently achieves the lowest cost among methods with a safety rate close to $100\%$. 

\noindent\textbf{(Q2): Training of \ourname{} is more stable. }
Next, we compare the training stability of \ourname{} with \baselinename{Schedule}, due to having a weight scheduled, and \baselinename{Lagr(lr)}, due to having a non-negligible learning rate (\Cref{fig: train_main}).
\baselinename{Schedule} experiences an increase in cost as $\beta$ increases throughout training. \baselinename{Lagr(lr)} experiences high variance and many spikes in both cost and safety rate throughout training. This is similar to previous results obtained in the single-agent when the cost threshold is zero \citep{so2023solving,he2023autocost}.
\ourname{} has a much smoother training curve than both.
We provide training curves on other algorithms and environments in \Cref{app: training-curve}. 

\noindent\textbf{(Q3): \ourname{} scales well with more agents. }
Finally, we test the scalability of the methods on \textsc{Line} by increasing the $N$ from $3$ to $5$ and $7$ (\Cref{fig: train_increase_n}).
The same trends from before still hold, with \ourname{} achieving the best performance and high safety rates.
We also see that \ourname{} performs well even with more agents, but the baseline methods are more inconsistent (e.g., \baselinename{Schedule} is mostly safe with $N=5$ but not so for $N=7$), possibly due to their hyperparameter sensitivity.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.995\columnwidth]{figs/experiment/train_increase_n.pdf}
    \vspace*{-0.8em}
    \caption{\textbf{Scaling to $N=5, 7$.} Unlike other methods, \ourname{} performs similarly with more agents.}
    \label{fig: train_increase_n}
    \vspace{-1em}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}{.495\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/experiment/train_alpha.pdf}
        \vspace*{-1.7em}
        \caption{Varying $\alpha$ in \algname{}.}
        \label{fig: train_alpha}
    \end{subfigure}
    \begin{subfigure}{.495\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/experiment/train_gcbf_weight.pdf}
        \vspace*{-1.7em}
        \caption{Varying $\nu$ in \algname{}.}
        \label{fig: train_nu}
    \end{subfigure}

    \vspace{.2em}
    \begin{subfigure}{.495\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/experiment/train_vh_stochastic.pdf}
        \vspace{-1.7em}
\caption{The necessity of deterministic rollouts to learn $V^h$.}
        \label{fig: train_vh_stochastic}
        \vspace{-.5em}
    \end{subfigure}
    \begin{subfigure}{.495\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/experiment/train_hcbf.pdf}
        \vspace{-1.7em}
\caption{The necessity of learning a CBF.}
        \label{fig: train_hcbf}
        \vspace{-.5em}
    \end{subfigure}
    \caption{\textbf{Ablations.} We vary hyperparameters (top) and verify our design decisions (bottom).}
    \vspace{-1em}
\end{figure}

\subsection{Ablation studies}\label{sec: ablation}

We now study hyperparameter sensitivity \textbf{(Q4)} by varying different hyperparameters in \ourname{}.

\noindent\textbf{Class-$\kappa$ function $\alpha$. }
For the class-$\kappa$ function in \eqref{eq: dgcbf}, we use a linear $\alpha(r)=a r$ with $a=0.3$.
We test the sensitivity of \ourname{} to this by varying $a$ (\Cref{fig: train_alpha}) on \textsc{Spread}.
We observe that $a=0$ leads to conservative behavior with a high cost. $a=1.2$ leads to an unsafe policy, which is to be expected since the $\alpha(-r) > -r$ condition is violated.
For the other values that satisfy this condition, there is no significant difference in either cost or safety. We can thus choose any $a \in (0, 1)$.

\noindent\textbf{Weight $\nu$ on the gradient of $\tilde C$. }
We introduced a schedule for $\nu$, which weights the constraint minimization step (\Cref{sec: cbf-model-free}). We test the sensitivity to $\nu$ on \textsc{Spread} with different static schedules (\Cref{fig: train_nu}).
The safety rate is lower with $\nu=1$, while the convergence in cost for $\nu=4, 8$ is slower.
The proposed schedule leads to faster cost convergence and a high safety rate.

\noindent\textbf{Learning $V^h$ with a stochastic policy. }
\Cref{thm:dpcbf}, used to learn the DGCBF $V^{h^{(m)}, \vmu}$, requires \textit{deterministic} rollouts.
Consequently, \ourname{} uses double the environment samples by performing both a stochastic and deterministic rollout.
We verify whether this is necessary by seeing how the type of rollout (deterministic vs stochastic) used to learn the DGCBF affects performance (\Cref{fig: train_vh_stochastic}),
which shows that using a stochastic rollout degrades both the cost and safety rate.
Thus, the use of a deterministic rollout to learn $V^{h^{(m)}, \vmu}$ is necessary despite the increased data use.

\noindent\textbf{Using a hand-crafted DGCBF. }
One motivation for \ourname{} is that it is difficult to construct a DGCBF with changing neighborhoods and input constraints.
We test this by using $h^{(m)}$ \textit{directly} as the DGCBF (instead of the learned $V^{h^{(m)}, \vmu}$,), as is commonly done for CBFs, on \textsc{Transport2} (\Cref{fig: train_hcbf}).
Using this hand-crafted ``DGCBF'' results in a decreased safety rate ($\sim\!15\%$), validating the need to learn a DGCBF. If no DGCBF is used, it performs even worse ({\Cref{app: no-cbf}}).

\section{Conclusion}\label{sec: conclusion}
We propose \algname{} to learn distributed safe policies for discrete-time MAS with unknown dynamics under a limited sensing range.
We extend CBFs to this problem setting with DGCBFs,
propose a construction using constraint-value functions,
and apply CBFs to the case of unknown dynamics using score function gradients.
Experimental results across three simulation engines suggest that \algname{} is robust to hyperparameters and performs well, achieving a safety rate matching conservative baselines while matching the performance of the performant but unsafe baselines.

\noindent\textbf{Limitations. }
\algname{} uses both stochastic and deterministic rollouts,
decreasing the sample efficiency.
Moreover, safety under stochastic dynamics has not been considered.
Finally, although safety is guaranteed when the DGCBF constraints are satisfied at \textit{all} states, achieving this in practice using learning is hard.
We leave these limitations to future work.

\section*{acknowledgement}
This work was partly supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. In addition, Zhang, So, and Fan are supported by the MIT-DSTA program. 
Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and dont necessarily reflect the views of the sponsors.

 2025 Massachusetts Institute of Technology.

Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\setcounter{Theorem}{0}
\setcounter{Lemma}{0}

\renewcommand{\theTheorem}{A\arabic{Theorem}}

\section{Proofs} \label{app: proofs}

\subsection{Proof of \texorpdfstring{\Cref{thm:dpcbf}}{Theorem~\ref{thm:dpcbf}}}\label{app: pf1}

\begin{proof}
    Dynamic programming on $V^{h,\mu}$ gives
    \begin{equation}
        V^{h,\mu}(x) = \max\Big\{ h(x),\; V^{h,\mu}(f(x, \mu(x))) \Big\}.
    \end{equation}
    Let $V^{h,\mu}(x)\leq 0$. This gives us two cases depending on which argument in the $\max$ is larger.

    \noindent\textbf{Case 1: $h(x) \leq V^{h,\mu}(f(x, \mu(x)))$: }
    Here, we have that $V^{h,\mu}(x) = V^{h,\mu}( f(x, \mu(x) ) )$, which implies
    \begin{equation}
        V^{h, \mu}(f(x, \mu(x))) - V^{h,\mu}(x) = 0 \leq 0 \leq -\alpha(V^{h, \mu}(x)).
    \end{equation}

    \noindent\textbf{Case 2: $h(x) > V^{h,\mu}(f(x, \mu(x)))$: }
    Here, we have that $V^{h,\mu}(x) = h(x) > V^{h,\mu}( f(x, \mu(x)) )$, which implies
    \begin{equation}
        V^{h,\mu}( f(x, \mu(x) ) ) - V^{h,\mu}(x) < 0 \leq 0\leq -\alpha(V^{h, \mu}(x)).
    \end{equation}

    Thus, $V^{h, \mu}(f(x, \mu(x))) - V^{h,\mu}(x) + \alpha(V^{h, \mu}(x)) \leq 0$ if $V^{h,\mu}(x)\leq 0$, and $V^{h,\mu}$ is a DCBF. 
\end{proof}

\subsection{Proof that \texorpdfstring{\Cref{eq: dcbf_safety_filter:stoch_ver2:constraint}}{Equation \eqref{eq: dcbf_safety_filter:stoch_ver2:constraint} } implies $C^{(m)}(\vx, \vu) \leq 0$ almost surely} \label{app: pf:almost_sure}

\begin{Theorem}
    Suppose
    \begin{equation} \label{eq: almost_sure_thing}
      \E_{\vx\sim\rho^{\vpi_\theta}}\underbrace{\E_{\vu\sim\vpi_\theta(\cdot \mid \vx)} \left[ \max\big\{0, C^{(m)}(\vx, \vu) \big\} \right]}_{\coloneqq \tilde{C}^{(m)}_\theta(\vx)} \leq 0.
    \end{equation}
    Then, $C^{(m)}(\vx, \vu) \leq 0$ almost surely.
\end{Theorem}

\begin{proof}
    Note that \eqref{eq: almost_sure_thing} can only be satisfied when the expectation equals $0$ since $\max\{0, \cdot\}$ is non-negative. 

    Assume for contradiction that $P( C^{(m)}(\vx, \vu) \leq 0 ) \leq 1 - \epsilon$ for $\epsilon > 0$, i.e., $P( C^{(m)}(\vx, \vu) > 0 ) \geq \epsilon$. Then,
    \begin{align}
        0
        &= \E_{\vx \sim \rho^{\vpi_\theta}} \E_{\vu \sim \vpi_\theta(\cdot \mid \vx)}\left[ \max\big\{0, C^{(m)}(\vx, \vu) \big\} \right] \\
        &\geq P\Big(C^{(m)}(\vx, \vu) > 0 \Big) \E_{\vx \sim \rho^{\vpi_\theta}, \vu \sim \vpi_\theta(\cdot \mid \vx)}\left[ \max\big\{0, C^{(m)}(\vx, \vu) \big\} \mid C^{(m)}(\vx, \vu) > 0 \right] \\
        &= \epsilon\, \E_{\vx \sim \rho^{\vpi_\theta}, \vu \sim \vpi_\theta(\cdot \mid \vx)}\left[ \max\big\{0, C^{(m)}(\vx, \vu) \big\} \mid C^{(m)}(\vx, \vu) > 0 \right] \\
        &> 0.
    \end{align}
    which is a contradiction. Thus, $P( C^{(m)}(\vx, \vu) \leq 0 ) = 1$, and $C^{(m)}(\vx, \vu) \leq 0$ almost surely.
\end{proof}

\subsection{Formal Statement and Proof of \texorpdfstring{\Cref{thm: grad_proj}}{Informal Theorem~\ref{thm: grad_proj}}} \label{app: pf:grad_proj}

We first formally state \Cref{thm: grad_proj} below.

\begin{Theorem}[Approximate Gradient Projection for Decoupled Policy Parameters] \label{thm: grad_proj_formal}
    Suppose that for all $\vx_1 \not= \vx_2$, the parameters $\theta$ of the stochastic policy $\vpi_\theta$ are orthogonal, i.e.,
    $\left( \nabla_{\theta} \vpi_\theta(\vu_1 \mid \vx_1) \right) \cdot \left( \nabla_{\theta} \vpi_\theta(\vu_2 \mid \vx_2) \right) = 0$ for all $\vu_1, \vu_2 \in \bm{\mathcal{U}}$
    (e.g., a finite state-space $\bm{\mathcal{X}}$ with independent distribution at each state).
    Let $\sigma^{(m)} \coloneqq \nabla_\theta \E_{\vx \sim \rho}[ \tilde{C}_\theta^{(m)}(\vx) ]$ denote the gradient of the $m$-th DCBF constraint violation for any state distribution $\rho$.
Then, the gradient of the objective \eqref{eq: dcbf_safety_filter:stoch_ver2:objective}, modified with an extra indicator as follows:
\begin{equation}
        g \coloneqq
        \mathbb \mathbb{E}_{\vx \sim \rho^{\vpi_\theta}}
        \left[
        \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0}\,
        \mathbb{E}_{
        \vu \sim \vpi_\theta(\cdot|\vx)}
        \left[ 
        \nabla_{\theta} \log \vpi(\vx, \vu)\,
        Q^{\vpi_\theta}(\vx, \vu)
        \right] \right],
    \end{equation}
    satisfies $g \cdot \sigma^{(m)} = 0\; \forall m$, i.e., it lies in the orthogonal complement of the constraint gradients $\sigma^{(m)}$.
\end{Theorem}

\begin{proof}
    For $\vx \in \bm{\mathcal{X}}$, define the set $\Theta_\vx$ as the column space of the gradient of the policy $\vpi$ at $\vx$, i.e.,
    \begin{equation}
        \Theta_\vx \coloneqq \spn\{ \nabla_\theta \vpi_\theta(\vu \mid \vx) : \vu \in \bm{\mathcal{U}} \}.
    \end{equation}
    By assumption, this implies that for $\vx_1 \not= \vx_2$,
    \begin{equation}
        \theta_1 \in \Theta_{\vx_1},\, \theta_2 \in \Theta_{\vx_2} \implies \theta_1 \cdot \theta_2 = 0.
    \end{equation}
    Now, note that
    \begin{align}
\sigma^{(m)}
        &\coloneqq \nabla_\theta \E_{\vx \sim \rho}[ \tilde{C}_\theta^{(m)}(\vx) ] \\
        &= \nabla_\theta \E_{\vx \sim \rho} \E_{\vu \sim \vpi_\theta(\cdot \mid \vx)}[ \max(0, C_\theta^{(m)}(\vx, \vu)) ] \\
        &= \E_{\vx \sim \rho} \E_{\vu \sim \vpi_\theta(\cdot \mid \vx)}\left[ \nabla_\theta \log \vpi_\theta(\vu \mid \vx) \max(0, C_\theta^{(m)}(\vx, \vu)) \right] \\
        &= \E_{\vx \sim \rho} \E_{\vu \sim \vpi_\theta(\cdot \mid \vx)}\left[ \ind{C_\theta^{(m)}(\vx, \vu) > 0} \nabla_{\theta} \log \vpi_\theta(\vu \mid \vx) C_\theta^{(m)}(\vx, \vu) \right] \\
        &\subseteq \bigcup_{\vx : \mathcal{E}^{(m)}} \Theta_\vx, \qquad \mathcal{E}^{(m)} \coloneqq \left\{ \vx \in \bm{\mathcal{X}} : \esssup_{\vu \in \bm{\mathcal{U}}} C_\theta^{(m)}(\vx, \vu) > 0 \right\},
    \end{align}
    where we have used the score function gradient estimator of $\tilde{C}_\theta$ (\Cref{app: policy-gradient}). Similarly,
    \begin{align}
        g
        &\coloneqq \mathbb E_{\vx \sim \rho^{\vpi_\theta}, \vu \sim \vpi_\theta(\cdot|\vx)} \big[ \nabla_\theta \log \vpi_\theta(\vu \mid \vx) \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0} Q^{\vpi_\theta}(\vx, \vu) \big], \\
        &= \mathbb E_{\vx \sim \rho^{\vpi_\theta}, \vu \sim \vpi_\theta(\cdot|\vx)} \big[ \nabla_\theta \log \vpi_\theta(\vu \mid \vx) \ind{\max_m \esssup_{\vu \in \bm{\mathcal{U}}} C^{(m)}_\theta(\vx) \leq 0} Q^{\vpi_\theta}(\vx, \vu) \big], \\
        &\subseteq \bigcup_{\vx : \mathcal{F}} \Theta_\vx,
    \end{align}
    where
    \begin{align}
        \mathcal{F}
        &\coloneqq \left\{ \vx \in \bm{\mathcal{X}} : \max_m \esssup_{\vu \in \bm{\mathcal{U}}} C_\theta^{(m)}(\vx, \vu) \leq 0 \right\}, \\
        &= \left\{ \vx \in \bm{\mathcal{X}} : \esssup_{\vu \in \bm{\mathcal{U}}} C_\theta^{(m)}(\vx, \vu) \leq 0,\; \forall m \right\}.
    \end{align}
    Since $\mathcal{E}^{(m)} \cap \mathcal{F} = \emptyset$ for all $m$, we have that $\sigma^{(m)} \cdot g = 0$ for all $m$.
\end{proof}

\subsection{Score Function Gradient Estimator of $\tilde{C}_\theta$}
\label{app: policy-gradient}

\begin{proof}
    Using the log trick,
    \begin{equation}
    \begin{aligned}
        \nabla_\theta \mathbb{E}_{\vu \sim \vpi_\theta(\vx)}\left[ C(\vx, f(\vx, \vu)) \right]
        &= \nabla_\theta \int C(\vx, f(\vx, \vu)) \, \vpi_\theta(\vx,\vu)\, d\vu, \\
        &=  \int C(\vx, f(\vx, \vu)) \left(\nabla_\theta \log \vpi_\theta(\vx, \vu) \right) \vpi_\theta(\vx,\vu)\, d\vu, \\
        &= \mathbb{E}_{\vu \sim \vpi_\theta(\vx)}\left[ \nabla \log \vpi_\theta(\vx, \vu)\, C(\vx, f(\vx, \vu)) \right].
    \end{aligned}
    \end{equation}
\end{proof}

\subsection{Formal Statement and Proof of \texorpdfstring{\Cref{thm:attn_dgcbf}}{Informal Theorem~\ref{thm:attn_dgcbf}}} \label{app: pf:attn_dgcbf}

We first formally state \Cref{thm:attn_dgcbf} below.
\begin{Theorem}[Satisfying \eqref{eq: dgcbf} during neighborhood changes]\label{thm:attn_dgcbf_formal}
    Suppose that the maximum distance an agent can travel in a time step is $\bar{d}$. Let $\tilde{B}$ be of the following form.
    \begin{equation} \label{eq: tilde_B_attn_form_formal}
        \tilde{B}(O_i(\vx)) = \xi_1\left( \sum_{j \in \mathcal{N}_i} w(o_{ij}) \xi_2(o_{ij}),\; \xi_3(o_i^{y}) \right),
    \end{equation}
    where $\xi_1 : \mathbb{R}^{\rho_1} \times \mathbb{R}^{\rho_2} \to \mathbb{R}$, $\xi_2 : \mathcal O_a\to \mathbb{R}^{\rho_1}$, and $\xi_3:\mathcal O_y\to\mathbb R^{\rho_2}$ encode the observations into some feature space, and $w : \mathcal{O}_a \to \mathbb{R}$ is a weighting function such that
    \begin{equation} \label{eq: attn_weight_cond}
        w(o_{ij}) = 0, \quad \text{ for all } x_i, x_j \text{ such that } \norm{p_i - p_j} \geq R - 2 \bar{d}.
    \end{equation}
    If 1) $\tilde{B}$ satisfies \eqref{eq: dgcbf} for all transitions where the neighborhood does not change,
    and 2) for any transition $\vx \to \vx^+$ with a neighborhood change $\mathcal{N}_i(\vx) \not= \mathcal{N}_i(\vx^+)$, there exists a transition $\bar{\vx} \to \bar{\vx}^+$ where all agents that either enter or leave the neighborhood (i.e., the complement of $\mathcal{N}_i(\vx) \cap \mathcal{N}_i(\vx^+)$) are moved outside the sensing radius, and all the remaining agents move identically in $\vx$ and $\bar{\vx}$,
then $\tilde{B}$ is a DGCBF.
\end{Theorem}

\begin{proof}
Let $\vx$ and $\vx^+$ be consecutive states such that the neighborhood of agent $i$ changes, i.e., $\mathcal{N}_i(\vx) \not= \mathcal{N}_i(\vx^+)$. Let $E \coloneqq \mathcal{N}_i(\vx) \setminus \mathcal{N}_i(\vx^+)$ and $F \coloneqq \mathcal{N}_i(\vx^+) \setminus \mathcal{N}_i(\vx)$ denote the set of agents that leave and enter the neighborhood of agent $i$, respectively.
Since the maximum distance agents can travel in one timestep is $\bar{d}$, the distance between agents can change by at most $2\bar{d}$ in one timestep.
Hence, all agents exiting the neighborhood are at least $R - 2\bar{d}$ away agent $i$, i.e.,
\begin{equation}
    j \in E \implies \norm{p_i - p_j} \geq R - 2\bar{d}.
\end{equation}
Similarly, all agents entering the neighborhood are at least $R - 2\bar{d}$ away from agent $i$ at the $\vx^+$, i.e.,
\begin{equation}
    j \in F \implies \norm{p_i^+ - p_j^+} \geq R - 2\bar{d}.
\end{equation}
Hence, by \eqref{eq: attn_weight_cond}, we have that $w(o_{ij}) = 0$ for all $j \in E$ and $w(o^+_{ij}) = 0$ for all $j \in F$.

Now, by asumption, there exists consecutive states $\bar{\vx}$ and $\bar{\vx}^+$ with the same neighborhood $\mathcal{N}_i(\bar{\vx}) = \mathcal{N}_i(\bar{\vx}^+) = \mathcal{N}_i(\vx) \cap \mathcal{N}_i(\vx^+)$, such that $\bar{x}_j = x_j$ and $\bar{x}^+_j = x^+_j$ for $j \in \mathcal{N}_i(\bar{\vx})$, and similarly for non-agent states $y=\bar{y}$, $y^+ = \bar{y}^+$.
By definition of the observation function $O_i$, $\vx$ and $\bar{\vx}$ share the same observation except for the $o_{ij}$ for $j \in E$, and similarly for $\vx^+$ and $\bar{\vx}^+$ for $j \in F$.
Since $w_{ij} = 0$ for all $j \in E \cup F$, the form of $\tilde{B}$ \eqref{eq: tilde_B_attn_form_formal} implies that
\begin{align}
    \sum_{j \in \mathcal{N}_i} w(o_{ij}) \xi_2(o_{ij}) &= \sum_{j \in \mathcal{N}_i} w(\bar{o}_{ij}) \xi_2(\bar{o}_{ij}) \\
    \sum_{j \in \mathcal{N}_i} w(o^+_{ij}) \xi_2(o_{ij}) &= \sum_{j \in \mathcal{N}_i} w(\bar{o}_{ij}) \xi_2(\bar{o}^+_{ij})
\end{align}
Hence, we must have that
\begin{align}
    \tilde{B}(O_i(\vx)) = \tilde{B}(O_i(\bar{\vx})), \quad \tilde{B}(O_i(\vx^+)) = \tilde{B}(O_i(\bar{\vx}^+)).
\end{align}
By assumption, $\tilde{B}$ satisfies the DGCBF condition \eqref{eq: dgcbf} for all transitions where the neighborhood does not change, which includes $\bar{\vx} \to \bar{\vx}^+$. Hence,
\begin{align}
    \tilde{B}(o_i^+) - \tilde{B}(o_i) + \alpha\left( \tilde{B}(o_i) \right)
    &= \tilde{B}(\bar{o}_i^+) - \tilde{B}(\bar{o}_i) + \alpha\left( \tilde{B}(\bar{o}_i) \right) \\
    &\leq 0,
\end{align}
and $\tilde{B}$ also satisfies the DGCBF condition \eqref{eq: dgcbf} for transitions where the neighborhood changes.
Thus, $\tilde{B}$ is a DGCBF.
\end{proof}

\subsection{Proof that a DGCBF can be used to construct a DCBF} \label{app: dgcbf_dcbf}
\begin{Theorem} \label{thm: dgcbf_dcbf}
    For a $N$-agent MAS, define $B : \bm{\mathcal{X}} \to \mathbb{R}$ as
    \begin{equation} \label{eq: dgcbf_safety:dcbf_def}
        B(\vx) \coloneqq \max_i \tilde{B}(O_i(\vx)).
    \end{equation}
    Then, $B$ is a DCBF.
\end{Theorem}

\begin{proof}
    Let $\mu : \mathcal{O} \to \mathbb{R}$ denote the per-agent control policy corresponding to the DGCBF $\tilde{B}$ in \Cref{def: dgcbf},
    and let $\vmu$ denote the resulting joint control policy.
    Then, under $\mu$, \eqref{eq: dgcbf} implies that for any $i$,
    \begin{equation} \label{eq: dgcbf_safety:tmp0}
        \tilde{B}( o_i^+ (\vx) ) - \tilde{B}( o_i(\vx) ) + \alpha( \tilde{B}( o_i(\vx) ) ) \leq 0,
        \quad\forall \vx \in \bm{\mathcal{X}}.
    \end{equation}

    Now, for a given $\vx \in \bm{\mathcal{X}}$,
    let $\vx^+ = f(\vx, \vmu(\vx))$ denote the next state following $\vmu$.
    Let $i_1$ and $i_2$ denote the index that maximizes \eqref{eq: dgcbf_safety:dcbf_def} at $\vx$ and $\vx^+$ respectively, i.e.,
    \begin{align}
        i_1 &\coloneqq \argmin_i \tilde{B}(O_i(\vx)) \quad \implies B(\vx) = \tilde{B}(O_{i_1}(\vx)), \\
        i_2 &\coloneqq \argmin_i \tilde{B}(O_i(\vx^+)) \quad \implies B(\vx^+) = \tilde{B}(O_{i_2}(\vx^+)). \\
    \end{align}
    Then, using \eqref{eq: dgcbf_safety:tmp0} and the fact that $(1 - \alpha)$ is also an extended class-$\kappa$ function and thus is a monotonic function \citep{ahmadi2019safe}:
    \begin{align}
        0
        &\geq \tilde{B}(O_{i_2}(\vx^+)) - \tilde{B}(O_{i_2}(\vx)) + \alpha(\tilde{B}(O_{i_2}(\vx))), \\
        &= \tilde{B}(O_{i_2}(\vx^+)) - (1 - \alpha) \circ \tilde{B}(O_{i_2}(\vx)), \\
        &\geq \tilde{B}(O_{i_2}(\vx^+)) - (1 - \alpha) \circ \tilde{B}(O_{i_1}(\vx)), \\
        &= B(\vx^+) - (1 - \alpha) \circ B(\vx), \\
        &= B(\vx^+) - B(\vx) + \alpha(B(\vx)).
    \end{align}
    and \eqref{eq: dcbf-condition} holds.
    Thus, $B$ is a DCBF.
\end{proof}
Since $\tilde{B}$ enables the construction of a DCBF, this implies that $\mathcal{C}$, the zero sub-level set of $B$, i.e.,
\begin{equation}
 \mathcal{C} \coloneqq \{ \vx \mid B(\vx) \leq 0 \} = \bigcap_{i}\, \{ \vx \mid \tilde{B}( o_i(\vx) ) \leq 0 \},
\end{equation}
is forward-invariant under $\vmu$ and hence control invariant.

\subsection{DGCBF has Generalizable Safety Guarantees}
\label{app: dgcbf_safety_proof}
In this subsection, we prove that the same DGCBF $\tilde{B}$ from \Cref{def: dgcbf} can guarantee the safety of a MAS with any number of agents $N$.
We will do this by showing that there exists an $\bar{N}$ such that if $\tilde{B}$ satisfies the DGCBF conditions \eqref{eq: dgcbf} for $\bar{N}$ agents,
then the \textbf{same} DGCBF $\tilde{B}$ will also satisfy the DGCBF conditions \eqref{eq: dgcbf}.

Let the maximum distance an agent can travel in a time step is $\bar{d}$.
Given a sensing radius $R$,
define $\bar{N}$ as the maximum number of agents that can be located within a ball of radius $2R + 2\bar{d}$.
For a $N$-agent MAS, let $\bm{\mathcal{X}}_N$ and $\bm{\mathcal{U}}_N$ denote the joint state and control space respectively,
and let $f_N$ denote the corresponding dynamics function. 
For generalizability to an arbitrary number of agents,
we make the additional assumption that the dynamics $f_N$ are decoupled for each agent, i.e.,
\begin{equation}
    x^{k+1}_i = f_0(x^k, u^k),
\end{equation}
for per-agent dynamics function $f_0$.

For a state $\vx \in \bm{\mathcal{X}}_N$, let $\vx_{i,\bar{N}} \in \bm{\mathcal{X}}_{\bar{N}}$ denote the restriction of $\vx$ to that of agent $i$ and its $\bar{N}-1$ closest neighbors.
We then have the following theorem.
\begin{Theorem} \label{thm:dgcbf_generalize_thm}
    Let $\tilde{B}$ satisfy the DGCBF conditions \eqref{eq: dgcbf} for $\bar{N}$ agents, i.e.,
    there exists
    a class-$\kappa$ function $\alpha$ with $\alpha(-r) > -r$ for all $r > 0$ and
    a control policy $\mu : \mathcal{O} \to \mathcal{U}$ satisfying 
    \begin{equation} \label{eq: dgcbf_safety_proof:Nbar}
        \tilde{B}( O_j(\tilde{\vx}) ) \leq 0,\; \forall j \implies \tilde{B}( O_i^+ (\tilde{\vx}) ) - \tilde{B}( O_i(\tilde{\vx}) ) + \alpha( \tilde{B}( O_i(\tilde{\vx}) ) ) \leq 0,
        \quad\forall \tilde{\vx} \in \bm{\mathcal{X}}_{\bar{N}},\; \forall i,
    \end{equation}
    Then, $\tilde{B}$ also satisfies the DGCBF conditions \eqref{eq: dgcbf} for any $N > \bar{N}$ agents, i.e.,
    \begin{equation} \label{eq: dgcbf_safety_proof:N}
        \tilde{B}( O_j(\vx) ) \leq 0,\; \forall j \implies \tilde{B}( O_i^+ (\vx) ) - \tilde{B}( O_i(\vx) ) + \alpha( \tilde{B}( O_i(\vx) ) ) \leq 0,
        \quad\forall \vx \in \bm{\mathcal{X}}_{N},\; \forall i,
    \end{equation}
\end{Theorem}

We will prove \Cref{thm:dgcbf_generalize_thm} by showing that \eqref{eq: dgcbf_safety_proof:N} holds because it can be reduced to the case of \eqref{eq: dgcbf_safety_proof:Nbar}.
For convenience, define $B$ as in \Cref{thm: dgcbf_dcbf} so that
\begin{equation}
    \tilde{B}( o_j(\vx) ) \leq 0,\; \forall j \quad \iff \quad B(\vx) \leq 0.
\end{equation}
Before we prove \Cref{thm:dgcbf_generalize_thm}, we first prove a few helpful lemmas.

\begin{Lemma}[Restriction leaves observations of all agents within $R+2\bar{d}$ invariant] \label{lem: cur_obs_invariant}
    For any $\vx \in \bm{\mathcal{X}}_N$ such that $B(\vx) \leq 0$,
    the restriction of $\vx$ to that of agent $i$ and its $\bar{N}-1$ closest neighbors leaves the observation of agent $i$
    and all agent within $R+2\bar{d}$ of $i$ invariant, i.e., for any $i$,
    \begin{equation}
        \norm{p_j - p_i} \leq R+2\bar{d} \quad \implies \quad O_j(\vx) = O_j(\vx_{i, \bar{N}})
    \end{equation}
\end{Lemma}

\begin{proof}
    Since $B(\vx) \leq 0$, by definition of $\bar{N}$ there can be no more than $\bar{N} - 1$ agents within radius of $2R + \bar{d}$ of agent $i$.
    Hence, $\vx_{i, \bar{N}}$ will include all agents within a radius of $2R + \bar{d}$ of agent $i$.

    By definition of the observation function $O_i$ \eqref{eq: obs_fn},
    \begin{equation}
        O_i(\vx) = \Big( \{o_{ij}\}_{j\in\mathcal N_i(\vx)},\; o_i^y \Big).
    \end{equation}

$O_j(\vx)$ depends only on all agents $l$ that are at most $R$ away from $j$.
    Since $\vx$ and $\vx_{i, \bar{N}}$ agree on all agents up to $2R + 2\bar{d}$ away from $i$,
    this implies that for all $j$ that are $R + 2\bar{d}$ away from $i$,
\begin{equation}
      \mathcal{N}_j(\vx) = \mathcal{N}_j(\vx_{i,\bar{N}}),
    \end{equation}
    thus the observation is unchanged as well.
\end{proof}

Next, we show that the observation of the \textit{next} state for agent $i$ is also left unchanged after restriction.

\begin{Lemma}[Restriction leaves the next observation for agent $i$ unchanged] \label{lem: next_obs_invariant}
    For any $\vx \in \bm{\mathcal{X}}_N$ such that $B(\vx) \leq 0$,
    let $\vx^+ = f_N(\vx, \vmu(\vx))$ denote the next state under $\vmu$ for $\vx$,
    and $\tilde{\vx}^+ = f_{\bar{N}}(\vx_{i, \bar{N}}, \vmu(\vx_{i, \bar{N}}))$ the next state under $\vmu$ starting from $\vx_{i, \bar{N}}$.
    Then, for any $i$,
    \begin{equation}
         O_i( \vx^+ ) = O_i( \tilde{\vx}^+ ).
    \end{equation}
\end{Lemma}

\begin{proof}
    We will prove this by showing that the states of all neighbors $\mathcal{N}_i(\vx^+)$ agree.
    For each new neighbor $j \in \mathcal{N}_i(\vx^+)$, since each agent can travel at most $\bar{d}$ in one step, this implies that
    \begin{equation}
        \norm{p_j - p_i} \leq R + 2 \bar{d}.
    \end{equation}
    Applying \Cref{lem: cur_obs_invariant} gives us that agent $j$'s observation is equal in both cases, i.e.,
    \begin{equation}
        O_j( \vx ) = O_j( \vx_{i, \bar{N}} ).
    \end{equation}
    Hence, the controls $\mu( O_j( \vx ))$, and thus the new states match, i.e.,
    \begin{equation}
        x_j^+ = f_0( x_j, \mu( O_j( \vx )) ) = f_0( x_j, \mu( O_j( \vx_{i, \bar{N}} )) ) = \tilde{x}^+_j.
    \end{equation}
    Since the new states for all agents in $\mathcal{N}_i(\vx^+)$ agree, this implies that the new observation for agent $i$ must also agree.
\end{proof}

We are now ready to prove \Cref{thm:dgcbf_generalize_thm}.
\begin{proof}[Proof of \Cref{thm:dgcbf_generalize_thm}]
    Let $\vx \in \bm{\mathcal{X}}_N$ such that $B(\vx) \leq 0$.
    Then, applying \Cref{lem: cur_obs_invariant} and \Cref{lem: next_obs_invariant} implies that
    the current and next observations for agent $i$ remain unchanged even when considering only the $\bar{N}$ closest agents, i.e.,
    \begin{align}
        O_i(\vx_{i, \bar{N}}) &= O_i(\vx), \\
        O_i^+(\vx_{i, \bar{N}}) &= O_i^+(\vx).
    \end{align}
    Hence, taking $\tilde{x} = \vx_{i, \bar{N}} \in \bm{\mathcal{X}}_{\bar{N}}$ and using 
    \eqref{eq: dgcbf_safety_proof:Nbar},
    \begin{align}
        \tilde{B}( O_i^+ (\vx) ) - \tilde{B}( O_i(\vx) ) + \alpha( \tilde{B}( O_i(\vx) ) )
        &= \tilde{B}( O_i^+ (\vx_{i, \bar{N}}) ) - \tilde{B}( O_i(\vx_{i, \bar{N}}) ) + \alpha( \tilde{B}( O_i(\vx_{i, \bar{N}}) ) ) \\
        &= \tilde{B}( O_i^+ ( \tilde{\vx} ) ) - \tilde{B}( O_i( \tilde{\vx} ) ) + \alpha( \tilde{B}( O_i( \tilde{\vx} ) ) ) \\
        &\leq 0.
    \end{align}
\end{proof}

\Cref{thm:dgcbf_generalize_thm} implies that finding a \textbf{single} DGCBF $\tilde{B}$ that satisfies the DGCBF condition for $\bar{N}$ agents enables the \textbf{same} DGCBF $\tilde{B}$ to \textit{also} be applied to larger numbers of agents $N > \bar{N}$.

\subsection{Proof of \texorpdfstring{\Cref{thm:dpgcbf}}{Corollary~\ref{thm:dpgcbf}}} \label{app: pf:dpgcbf}

\begin{proof}
    Since $\tilde{V}^{h^{(m)},\vmu}(o^0_i) = V^{h^{(m)}, \vmu}_i(\vx^0) \coloneqq \max_{k \geq 0} h^{(m)}(o^k_i)$, applying \Cref{thm:dpcbf} gives us that
    \begin{align}
        &\mathrel{\phantom{=}}\tilde{V}^{h^{(m)},\vmu}(o^+_i) - \tilde{V}^{h^{(m)},\vmu}(o_i) + \alpha\left( \tilde{V}^{h^{(m)},\vmu}(o_i) \right) \\
        &= V^{h^{(m)}, \vmu}_i(\vx^+) - V^{h^{(m)}, \vmu}_i(\vx) + \alpha\left( V^{h^{(m)}, \vmu}_i(\vx) \right) \\
        &\leq 0.
    \end{align}
    Thus, $\tilde{V}^{h^{(m)},\vmu}$ is a DGCBF.
\end{proof}

\section{Policy Loss Details} \label{app: pol_loss_details}
\subsection{Derivation of \eqref{eq: dcbf_safety_filter:decoupled}}
We first start from \eqref{eq: dcbf_safety_filter:stoch_ver2}, which we repeat below for convenience.
\begin{subequations}
\begin{align}
    \min_{\theta}\quad & \mathbb E_{\vx \sim \rho_{0}, \vu\sim\vpi_\theta(\cdot|\vx)}
    \big[ Q^{\vpi_\theta}(\vx, \vu) \big], \label{eq: dcbf_safety_filter:stoch_ver2:repeat:objective} \\
\mathrm{s.t.}\quad & \mathbb E_{\vx\sim\rho^{\vpi_\theta}}\underbrace{\E_{\vu\sim\vpi_\theta(\cdot \mid \vx)} \Big[ \underbrace{\max\big\{0, C^{(m)}(\vx, \vu) \big\}}_{\coloneqq \tilde{C}^{(m)}(\vx, \vu)} \Big]}_{\coloneqq \tilde{C}^{(m)}_\theta(\vx)} \leq 0, \quad \forall m. 
    \label{eq: dcbf_safety_filter:stoch_ver2:repeat:constraint}
\end{align}
\end{subequations}
In the above, we additionally define $\tilde{C}^{(m)}_\theta(\vx) \coloneqq \max\{ 0, C^{(m)}(\vx, \vu) \}$ for convenience.

Borrowing the ideas of gradient projection from multi-objective optimization \citep{yu2020gradient,liu2021conflict},
we combine the gradient from the objective minimization \eqref{eq: dcbf_safety_filter:stoch_ver2:repeat:objective}, and the gradient from constraint satisfaction
\eqref{eq: dcbf_safety_filter:stoch_ver2:repeat:constraint} (equivalent to constraint minimization due to the constraints being non-negative) by projecting the gradient of 
\eqref{eq: dcbf_safety_filter:stoch_ver2:repeat:objective}
such that it is orthogonal to the gradient of \textit{all} $m$ constraints \eqref{eq: dcbf_safety_filter:stoch_ver2:repeat:constraint}.
We can do this in a \textbf{single} backward pass by using \Cref{thm: grad_proj}.

Let $\stopgrad$ denote the \textit{stop gradient} function, such that the gradient of $\stopgrad$ is equal to zero.
Then, for a state distribution $\rho$,
\begin{align}
    \sigma
    &\coloneqq
    \mathbb{E}_{\vx \sim \rho}[ \nabla_\theta \max_m \tilde{C}_\theta^{(m)}(\vx)] \label{eq: app:pg:sigma_inside} \\
&= \mathbb{E}_{\vx \sim \rho}\, \mathbb{E}_{\vu \sim \vpi_\theta(\vx)}\left[ \nabla_\theta \log \vpi_\theta(\vx, \vu)\, \max_m \tilde{C}^{(m)}(\vx, \vu) \right], \\
&= \mathbb{E}_{\vx \sim \stopgrad(\rho)}\, \mathbb{E}_{\vu \sim \stopgrad(\vpi_\theta(\vx))}\left[ \nabla_\theta \log \vpi_\theta(\vx, \vu)\, \max_m \tilde{C}^{(m)}(\vx, \vu) \right], \\
&= \nabla_\theta\, \mathbb{E}_{\vx \sim \stopgrad(\rho)}\, \mathbb{E}_{\vu \sim \stopgrad(\vpi_\theta(\vx))}\left[ \log \vpi_\theta(\vx, \vu)\, \max_m \tilde{C}^{(m)}(\vx, \vu) \right], \label{eq: app:pg:sigma}
\end{align}
where the second line uses the score function gradient (\Cref{app: policy-gradient}), and
\begin{align}
    g
    &= \mathbb{E}_{\vx \sim \rho^{\vpi_\theta}} 
    \left[
    \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0}\,
    \mathbb{E}_{
    \vu \sim \vpi_\theta(\cdot|\vx)}
    \left[  \nabla_\theta \log \vpi_\theta(\vx, \vu) Q^{\vpi_\theta}(\vx, \vu)
    \right] \right], \label{eq: app:pg:g_inside} \\
&= \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})} 
    \left[
    \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0}\,
    \mathbb{E}_{
    \vu \sim \stopgrad(\vpi_\theta(\cdot|\vx))}
    \left[ \nabla_\theta \log \vpi_\theta(\vx, \vu) \stopgrad(Q^{\vpi_\theta}(\vx, \vu))
    \right] \right], \\
&= \nabla_\theta \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})} 
    \left[
    \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0}\,
    \mathbb{E}_{
    \vu \sim \stopgrad(\vpi_\theta(\cdot|\vx))}
    \left[ \log \vpi_\theta(\vx, \vu) \stopgrad(Q^{\vpi_\theta}(\vx, \vu))
    \right] \right],  \label{eq: app:pg:g}
\end{align}
where the second line follows from the policy gradient theorem.
Let $\tilde{g} \coloneqq \nu \sigma^{(m)} + g$.
, where we take the $\rho$ in the definition of $\sigma^{(m)}$ to be equal to $\rho^{\vpi_\theta}$ (we discuss the implications of this in the next subsection \Cref{app: pol_loss_details:pol_grad}):
\begin{Theorem} \label{thm: g_plus_sigma_gradient}
    The combined gradient
    $\tilde{g}$ can be computed as the gradient of the loss function $L$ defined in \eqref{eq: dcbf_safety_filter:decoupled}, i.e.,
    \begin{equation}
        \tilde{g}
        = \nabla_\theta L(\theta), 
    \end{equation}
    where
    \begin{align}
        L(\theta) &\coloneqq \E_{\vx \sim \stopgrad(\rho^{\vpi_\theta})} \E_{\vu \sim \stopgrad(\vpi_\theta(\cdot|\vx))} \big[ \log \vpi_\theta(\vx, \vu)\, \stopgrad\big(\tilde{Q}(\vx, \vu, \theta) \big) \big], \\
\tilde{Q}(\vx, \vu, \theta) &\coloneqq
        \begin{dcases}
            Q^{\vpi_\theta}(\vx, \vu), & \tilde{C}^{(m)}_\theta(\vx) \leq 0,\; \forall m, \\
            \nu \max_m \tilde{C}^{(m)}(\vx, \vu), & \text{otherwise}.
        \end{dcases} \\[2pt]
        &= \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0} \stopgrad(Q^{\vpi_\theta}(\vx, \vu)) + \nu \max_m \tilde{C}^{(m)}(\vx, \vu).
    \end{align}
\end{Theorem}
\begin{proof}
    Taking $\rho = \rho^{\vpi_\theta}$ for $\sigma$ and summing the expressions for $\sigma$ and $g$ in \eqref{eq: app:pg:sigma} and \eqref{eq: app:pg:g} respectively gives
    \begin{align}
    \begin{split}
    \hspace{-.9em}
    \nu \sigma + g
        &= \nu \nabla_\theta\, \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})}\, \mathbb{E}_{\vu \sim \stopgrad(\vpi_\theta(\cdot | \vx))}\left[ \log \vpi_\theta(\vx, \vu)\, \max_m \tilde{C}^{(m)}(\vx, \vu) \right] \\
        &\quad + \nabla_\theta \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})} 
    \left[
    \ind{\max_m \tilde{C}^{(m)}_\theta(\vx) \leq 0}\,
    \mathbb{E}_{
    \vu \sim \stopgrad(\vpi_\theta(\cdot|\vx))}
    \left[ \log \vpi_\theta(\vx, \vu) \stopgrad(Q^{\vpi_\theta}(\vx, \vu))
    \right] \right]
    \end{split} \\[3pt]
    &=
    \nabla_\theta\, \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})}\, \mathbb{E}_{\vu \sim \stopgrad(\vpi_\theta(\cdot | \vx))}\left[ \log \vpi_\theta(\vx, \vu)
    \left(
        \nu \max_m \tilde{C}^{(m)}(\vx, \vu)
        + \stopgrad(Q^{\vpi_\theta}(\vx, \vu))
    \right)
    \right] \raisetag{3.5ex}\\
    &= 
    \nabla_\theta\, \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})}\, \mathbb{E}_{\vu \sim \stopgrad(\vpi_\theta(\cdot | \vx))}\Big[ \log \vpi_\theta(\vx, \vu)
    \stopgrad\big(
        \underbrace{\nu \max_m \tilde{C}^{(m)}(\vx, \vu)
        + Q^{\vpi_\theta}(\vx, \vu)
        }_{\coloneqq \tilde{Q}(\vx, \vu, \theta)}
    \big)
    \Big] \\
    &= 
    \nabla_\theta\, \mathbb{E}_{\vx \sim \stopgrad(\rho^{\vpi_\theta})}\, \mathbb{E}_{\vu \sim \stopgrad(\vpi_\theta(\cdot | \vx))}\left[ \log \vpi_\theta(\vx, \vu)
    \stopgrad\big( \tilde{Q}(\vx, \vu, \theta) \big)
    \right]
    \end{align}
\end{proof}
In other words, \Cref{thm: g_plus_sigma_gradient} implies that $\tilde{g}$ can be computed using a single backward pass.

\subsection{Implications of minimizing the constraint violation over $\rho^{\pi_\theta}$} \label{app: pol_loss_details:pol_grad}
Note that a key step that allows us to move the gradient operator between the inside \eqref{eq: app:pg:sigma_inside} and outside \eqref{eq: app:pg:sigma} for the expression of $\sigma$ is our use of $\stopgrad$, because this allows this equivalent holding even when we take $\rho = \rho^{\pi_\theta}$.

\textit{Without} the $\stopgrad$ and taking $\rho = \rho^{\vpi_\theta}$, we would obtain that \eqref{eq: app:pg:sigma} is \textit{instead} equivalent to
\begin{align}
\hspace{-0.8em}
    \nabla_\theta \E_{\vx \sim \rho^{\vpi_\theta}} \E_{\vu \sim \vpi_\theta(\cdot | \vx)} [ \max_m \tilde{C}^{(m)}(\vx, \vu)]
    &= \nabla_\theta \E_{\vx_0 \sim \rho_0, \vu^k \sim \vpi_\theta(\cdot | \vx^k)} \left[\, \underbrace{\sum_{k=0}^\infty \max_m \tilde{C}^{(m)}(\vx^k, \vu^k)}_{\coloneqq Q^{C,\vpi_\theta}(\vx_0, \vu)} \right] \label{eq: tmp:cc} \\
    &\propto
    \E_{\vx \sim \rho^{\vpi_\theta}} \E_{\vu \sim \vpi_\theta(\cdot | \vx)} \Big[
        \nabla_\theta \log \vpi_\theta(\vx, \vu)\; Q^{C,\vpi_\theta}(\vx, \vu)
    \Big], \label{eq: tmp:bb}\\
    &\not= \mathbb{E}_{\vx \sim \rho^{\vpi_\theta}}\, \mathbb{E}_{\vu \sim \vpi_\theta(\vx)}\left[ \nabla_\theta \log \vpi_\theta(\vx, \vu)\, \max_m \tilde{C}^{(m)}(\vx, \vu) \right]. \label{eq: tmp:aa}
\end{align}
Note that the second line comes from the use of the \textbf{policy gradient theorem}, while the expression in \eqref{eq: tmp:aa} comes from the application of the \textbf{score function gradient} \Cref{app: policy-gradient}.
Consequently, while \eqref{eq: tmp:aa} will minimize the maximum DCBF violation $\tilde{C}^{(m)}$,
\eqref{eq: tmp:bb} minimizes the \textit{sum} of \textbf{future} DCBF violations as well.

Another way to see this is to note that \eqref{eq: tmp:cc} can be viewed as an optimal control problem with \textit{cost} equal to the DCBF violation.
In other words, in \eqref{eq: tmp:cc} the policy will additionally try to move to states where the DCBF violation is small as opposed to changing only the control $\vu$ such that it satisfies the DCBF conditions in \eqref{eq: tmp:aa}.

\paragraph{Experimental Validation.}
To validate our intuition above, we conduct experiments in the \textsc{Transport2} environment to compare using \eqref{eq: tmp:cc} with \eqref{eq: tmp:aa}, and plot the training curves in \Cref{fig:cbf_reward}.
The results show that using \eqref{eq: tmp:cc} converges much slower in cost, which matches the intuition above.
Namely, \eqref{eq: tmp:cc} \textit{additionally} tries to avoid \textit{states} where the DCBF constraint violation is high, which leads to unnecessary conservatism.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/experiment/cbf_reward.pdf}
    \caption{
    In comparison to DGPPO which uses \eqref{eq: tmp:aa},
    using \eqref{eq: tmp:cc} converge much slower in cost,
    which matches the intuition that \eqref{eq: tmp:cc} is optimizing for the wrong objective due to unnecessarily avoiding states where the DCBF constraint violation is high.
}
    \label{fig:cbf_reward}
\end{figure}

\section{Experiments}\label{app: experiments}

\subsection{Computation resources}

The experiments are run on a 13th Gen Intel(R) Core(TM) i7-13700KF CPU with 64GB RAM and an NVIDIA GeForce RTX 4090 GPU. The training time is around $12$ hours for $2\times 10^5$ steps for \ourname{}, $14$ hours for \baselinename{Lagr} and $10$ hours for \baselinename{Penalty}. 

\subsection{Environments}\label{app: experiments-environments}

\subsubsection{LiDAR environments}

In the LiDAR environments, we assume that the agents have a radius of $r=0.05$ and a local sensing radius $R=0.5$ such that one agent can observe other agents or obstacles only when they are within its sensing radius. Agents use LiDAR to detect obstacles. For each agent, there are $32$ evenly-spaced LiDAR rays originating from each agent measuring the relative location of obstacles. To reduce the size of the multi-agent graph, the $8$ shortest LiDAR rays are returned. 

We use directed graphs $\mathcal G = (\mathcal V, \mathcal E)$ to represent the LiDAR environments. $\mathcal V$ is the set of nodes containing the objects in the environments, including agents $\mathcal V_a$, goals/landmarks $V_g$, and the hitting points of LiDAR rays (obstacles) $\mathcal V_o$. The edges $\mathcal E\subseteq \{(i, j)\;|\;i\in \mathcal V_a, j\in \mathcal V\}$ denote the information flow from a sender node $j$ to a receiver node (agent) $i$. An edge $(i,j)$ exists only if the distance between node $i$ and $j$ are smaller than the sensing radius $R$. The neighbor nodes of agent $i$ is defined as $\mathcal N_i\coloneqq\{j\;|\;(i,j)\in\mathcal E\}$, so that the information flow happens between the agents and their neighbors. The node features $v_i$ include the state of the node $x_i$ and a one-hot encoding of the type of the node $i$ (e.g., agent, goal/landmark, LiDAR hitting points). The edge features $e_{ij}$ include the information passed from node $j$ to node $i$, including the relative positions and velocities. 

In all LiDAR environments, we include $3$ rectangle-shaped obstacles, and the agents need to avoid inter-agent collision and agent-obstacle collisions.

We consider $4$ LiDAR environments: \textsc{Target}, \textsc{Spread}, \textsc{Line}, and \textsc{Bicycle}:
\paragraph{\textsc{Target}:} The agents need to reach their pre-assigned goals (\Cref{fig: lidarnav}).

\paragraph{\textsc{Spread}:} The agents need to collectively cover a set of goals without having access to an assignment (\Cref{fig: lidarspread}).

\paragraph{\textsc{Line}:} The agents need to form a line between two given landmarks (\Cref{fig: lidarline}).

\paragraph{\textsc{Bicycle}:} The agents follow the more difficult bicycle dynamics. The task here is the same as \textsc{Target} (\Cref{fig: lidarbicycle}).

The agents in the first $3$ environments follow the double integrator dynamics. The state of agent $i$ is $x_i = [p^x_i, p^y_i, v^x_i, v^y_i]^\top$, where $[p^x_i,p^y_i]^\top \coloneqq p_i\in\mathbb R^2$ is the position of agent $i$, and $[v^x_i, v^y_i]$ is its velocity. The control inputs are $u_i = [a^x_i, a^y_i]^\top$, which are the acceleration along the x-axis and y-axis. The agents follow the dynamics
\begin{equation}
    \dot x_i = \begin{bmatrix}
        v^x_i & v^y_i & a^x_i & a^y_i
    \end{bmatrix}^\top.
\end{equation}
We limit the control inputs of the agents to be within $[-1, 1]$, and also the velocities to be within $[-10, 10]$. In the \textsc{Bicycle} environment, the state of agent $i$ is defined with $x_i = [p^x_i, p^y_i, \cos\theta, \sin\theta, v]^\top$, where $\theta$ is the heading and $v$ is the speed. The control inputs are $u_i = [\delta, a]^\top$, including the steering angle $\delta$ and the acceleration $a$. The agents follow the bicycle dynamics given by 
\begin{equation}
    \dot x_i = \begin{bmatrix}
        v \cos\theta & v \sin\theta & -v\sin\theta\tan\delta & v\cos\theta\tan\delta & a
    \end{bmatrix}^\top.
\end{equation}
The control inputs are limited by $v\in[-10, 10]$ and $\delta\in[-1.47, 1.47]$. In all LiDAR environments, we use a simulation time step of $0.03$ seconds and a total horizon of $128$ time steps. For all LiDAR environments, the edge features are defined as the relative positions and relative velocities between the nodes. 

In LiDAR environments, the constraint function $h$ contains two parts: agent-agent collisions and agent-obstacle collisions. The agent-agent collisions $h^{(1)}$ function is defined as 
\begin{align}
    h^{(1)}(o_i) = 2r - \min_{j\in\mathcal N_i} \|p_i - p_j\|, 
\end{align}
and the agent-obstacle collision $h^{(2)}$ function is 
\begin{align}
    h^{(2)}(o_i) = r - \min_{j\in\mathcal N_i} \|p_i - p_j\|.
\end{align}

For the cost functions $l$, we consider two types of them. The first type is used in the \textsc{Target} and the \textsc{Bicycle} environments, where the agents need to \textit{reach} their pre-assigned goals. We define this type of cost function with
\begin{align}
    l(\vx, \vu) = \frac{1}{N}\sum_{i=1}^N \left(0.01\|p_i - p_i^\mathrm{goal}\| + 0.001\mathrm{sign}\left(\mathrm{ReLU}(\|p_i - p_i^\mathrm{goal}\| - 0.01)\right) + 0.0001\|u_i\|^2\right),
\end{align}
where the first term penalizes the agents if they cannot reach the goal, the second term penalizes the agents if they cannot reach the goal exactly, and the third term encourages small controls. The second type of the cost functions is used in the \textsc{Spread} and the \textsc{Line} environments, where the agents need to \textit{cover} some goals/landmarks. We define this type of cost function with 
\begin{align}
    l(\vx, \vu) = \frac{1}{N}\sum_{j=1}^N \min_{i\in\mathcal V_a}&\left(0.01\|p_i - p_j^\mathrm{goal}\| + 0.001\mathrm{sign}\left(\mathrm{ReLU}(\|p_i - p_j^\mathrm{goal}\| - 0.01)\right)\right. \nonumber\\
    & \left.+ 0.0001\|u_j\|^2\right).
\end{align}
Here, each goal finds its nearest agent and penalizes the whole team with the distance between them. In this way, the optimal policy of the agents is to cover all goals collaboratively. 

\subsubsection{MuJoCo environments}
For the \textsc{Transport} environment, we model the agents as double integrators and control the forces applied to each agent using the MuJoCo simulator \citep{mujoco}. We limit the control inputs of the agents to be within $[-1, 1]$. The agents need to collaboratively push a box from the inside so that the box reaches a given goal while avoiding colliding with each other.

We use a similar cost function $l(\vx, \vu)$ as the \textsc{Target} environment but only for the box, i.e.,
\begin{equation}
    l(\vx, \vu) = 0.01\|p_{\text{box}} - p_{\text{box}}^\mathrm{goal}\| + 0.001\mathrm{sign}\left(\mathrm{ReLU}(\|p_{\text{box}} - p_{\text{box}}^\mathrm{goal}\| - 0.01)\right)
\end{equation}

We usee a single constraint function $h^{(1)}$ for the agent-agent collision, defined as 
\begin{align}
    h^{(1)}(o_i) = 2r - \min_{j\in\mathcal N_i} \|p_i - p_j\|, 
\end{align}
where $p_i$ denotes the position of agent $i$, and $r$ is the radius of the agent.

\subsubsection{VMAS environments}
We use the 
\textsc{ReverseTransport} (which we call \textsc{Transport2}) and \textsc{Wheel}
environments from VMAS \citep{bettini2022vmas,bettini2024benchmarl}. Note that the obstacles in the VMAS environments are not represented using LiDAR but using states including their positions and sizes.
In both environments, the agents are modeled as double integrators and control the forces applied to each agent. We limit the control inputs of the agents to be within $[-1, 1]$.

\paragraph{\textsc{Transport2}:} In this environment, $N$ agents are placed in a square red package and must push the package from the inside to a goal location.
The red package and the goal location are randomly sampled.
Unlike the original \textsc{ReverseTransport} environment, we include the following two additional safety constraints:
\begin{itemize}[leftmargin=2em]
    \item \textbf{Inter-agent collision avoidance:} The agents must avoid colliding with each other.
    \item \textbf{Package collision avoidance:} The center of the package must avoid colliding with three randomly placed circular obstacles.
\end{itemize}

We use a similar cost function $l(\vx, \vu)$ as the \textsc{Target} environment but only for the package, i.e.,
\begin{equation}
    l(\vx, \vu) = 0.01\|p_{\text{package}} - p_{\text{package}}^\mathrm{goal}\| + 0.001\mathrm{sign}\left(\mathrm{ReLU}(\|p_{\text{package}} - p_{\text{package}}^\mathrm{goal}\| - 0.01)\right)
\end{equation}

We usee a two constraint functions $h^{(1)}$, $h^{(2)}$ for the agent-agent collision and package-obstacle collisions respectively. The agent-agent collision function $h^{(1)}$ is defined as 
\begin{align}
    h^{(1)}(o_i) = 2r - \min_{j\in\mathcal N_i} \|p_i - p_j\|, 
\end{align}
where $p_i$ denotes the position of agent $i$, and $r$ is the radius of the agent.
The package-obstacle collision function $h^{(2)}$ is defined as
\begin{align}
    h^{(2)}(o_i) = r_{\text{obs}} - \min_{q \in \{1, 2, 3\} } \|p_{\text{package}} - p_{q}\|.
\end{align}

\paragraph{\textsc{Wheel}:} In this environment, $N$ agents must collectively rotate a line anchored to the origin with a large mass.
Unlike the original \textsc{Wheel} environment, we modify the goal to be a target angle that the line must rotate to.
We also include the following two additional safety constraints:
\begin{itemize}[leftmargin=2em]
    \item \textbf{Inter-agent collision avoidance:} The agents must avoid colliding with each other.
    \item \textbf{Line collision avoidance:} The angle of the line must stay outside of a certain range of angles. 
\end{itemize}

We use the same cost function $l(\vx, \vu)$ as the \textsc{Target} environment but on the angle of the line, i.e.,
\begin{equation}
    l(\vx, \vu) = \frac{1}{N}\sum_{i=1}^N \left(0.01\|p_i - p_i^\mathrm{goal}\| + 0.001\mathrm{sign}\left(\mathrm{ReLU}(\|p_i - p_i^\mathrm{goal}\| - 0.01)\right) + 0.0001\|u_i\|^2\right).
\end{equation}

We usee a two constraint functions $h^{(1)}$, $h^{(2)}$ for the agent-agent collision and line-obstacle collisions respectively. The agent-agent collision function $h^{(1)}$ is defined as 
\begin{align}
    h^{(1)}(o_i) = 2r - \min_{j\in\mathcal N_i} \|p_i - p_j\|, 
\end{align}
where $p_i$ denotes the position of agent $i$, and $r$ is the radius of the agent.
The package-obstacle collision function $h^{(2)}$ is defined as
\begin{align}
    h^{(2)}(o_i) = r_{\text{obs}} - \abs{\theta_{\text{line}} - \theta_{\text{obs}}},
\end{align}
where the absolute value on the angle $\abs{\cdot}$ is defined as the minimum angle between the two angles.

\subsection{Implement details and hyperparameters}

In our experiment, we let all agents share the same parameters for their policies and value functions. More specifically, we parameterize the agent's policy with $\pi_\theta:\mathcal O \to \mathcal U$, cost-value function with $V^l_\phi:\bm{\mathcal X}\to\mathbb R$, and constraint-value function $V^{h^{(m)}}_\psi: \mathcal O\to\mathbb R^m$ using graph transformers \citep{shi2020masked} with parameters $\theta$, $\phi$, and $\psi$, respectively. Since the cost-value function $V^l_\phi$ is centralized, after the graph transformer, we compute the average of all node features and pass that to a final layer (multi-layer perceptron) to obtain the global cost value for the MAS. 

In \Cref{tab: params-shared}, we provide the value of the common hyperparameters for \algname{} and the baselines. Besides these common hyperparameters, the value of the unique hyperparameters of \algname{} are provided in \Cref{tab: params-alg}. 

\begin{table}[t]
    \centering
    \caption{Common hyperparameters of \algname{} and the baselines.}
    \label{tab: params-shared}
    \begin{tabular}{lc|lc}
        \toprule
        Hyperparameter & Value & Hyperparameter & Value \\
        \midrule
        policy GNN layers & 2 & RNN type & GRU \\
        massage passing dimension & 32 & RNN data chunk length & 16\\
        GNN output dimension & 64 & RNN layers & 1 \\
        number of attention heads & 3 & number of sampling environments & 128 \\
        activation functions & ReLU & gradient clip norm & 2 \\
        GNN head layers & (32, 32) & entropy coefficient & 0.01 \\
        optimizer & Adam & GAE $\lambda$ & 0.95 \\
        discount $\gamma$ & 0.99 & clip $\epsilon$ & 0.25 \\
        policy learning rate & 3e-4 & PPO epoch & 1 \\
        $V^l$ learning rate & 1e-3 & batch size & 16384 \\
        network initialization & Orthogonal & layer normalization & True \\
        $V^l$ GNN layers & 2 &  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \caption{Unique hyperparameters of \algname{}}
    \label{tab: params-alg}
    \begin{tabular}{l|c}
        \toprule
        Hyperparameter & Value \\
        \midrule
        $V^h$ GNN layers & 1 \\
        $a$ & 0.3 \\
        $\nu$ & Scheduled. Initialized at $\nu=1$, then doubled at $0.5$ and $0.75$ of the total update steps. \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Implementation of the baselines}\label{app: implementation-baseline}

We implement a JAX \citep{jax2018github} version of the baselines following their original implementations:

\begin{itemize}[leftmargin=0.5cm]
    \item InforMARL: \href{https://github.com/nsidn98/InforMARL}{https://github.com/nsidn98/InforMARL} (MIT license)
    \item MAPPO-L: \href{https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation}{https://github.com/chauncygu/Multi-Agent-Constrained-Policy-Optimisation} (MIT License)
\end{itemize}

\subsection{Training curves}\label{app: training-curve}

Here, we provide the cost and safety rate during training for all algorithms in \Cref{fig: train_all}. We can observe that \ourname{} achieves stable training in all environments with only one constant set of hyperparameters. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.995\columnwidth]{figs/experiment/train_all.pdf}
    \caption{Costs and safety rates of \algname{} and the baselines during training.}
    \label{fig: train_all}
\end{figure}

\subsection{Additional ablation studies}\label{app: ablation}

\subsubsection{Comparison between the decoupling method and the coupled methods}\label{app: decouple-vs-couple}

In \Cref{sec: cbf-model-free}, we have introduced a decoupling method to performance gradient descent update following \Cref{eq: dcbf_safety_filter:decoupled}. Here, we empirically compare the decoupling method \eqref{eq: dcbf_safety_filter:decoupled} with the CRPO-style coupling method \eqref{eq: crpo_ver2} and the CRPO-style coupling method with the constraint being on $C(x, u)$ instead of $\max\{0, C(x, u)\}$ (\Cref{eq: dcbf_safety_filter:stoch_ver1}). We conduct experiments in the \textsc{Transport2} environment and compare the safety rate and cost of the converged policies with different methods in \Cref{fig: crpo-ablation}. We can observe that DGPPO achieves a much lower cost compared with the coupling methods. This is because the coupling methods perform gradient descent to minimize $Q^{\vpi_\theta}$ only when the \textit{whole trajectory} is safe, i.e., $\mathbb E_{\vx\sim\rho^{\vpi_\theta}}[\tilde C_\theta(\vx)] \leq 0$. This is much more conservative than the safety requirement on the per-transition level used by the decoupling method, especially in the multi-agent case. Therefore, the coupling method has little chance to minimize $Q^{\vpi_\theta}$ during training but focuses on safety, resulting in a safe policy with poor performance. On the other hand, if the coupling method is performed with $C(x, u)$ instead of $\max\{0, C(x, u)\}$ (\Cref{eq: dcbf_safety_filter:stoch_ver1}), the learned policy is no longer safe. This matches our discussion in \Cref{sec: cbf-model-free}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\columnwidth]{figs/experiment/safe_cost_crpo.pdf}
    \caption{Comparison between \algname{} with the decoupling method and the CRPO-style update with $\mathbb E[\max\{0, C\}]$ and with $\mathbb E[C]$.}
    \label{fig: crpo-ablation}
\end{figure}

\subsubsection{Provide the baselines with more data}\label{app: more-data}

In \Cref{sec: final_method}, we introduced that \algname{} requires sampling with both a stochastic and a deterministic policy. This means that \algname{} requires twice as much data per update step compared with the baselines, although the update steps are the same. Here, we answer the question that \textit{what if the baselines are provided with double data}? We choose the \textsc{Wheel} environment and select the three best baselines in \Cref{fig: safe_cost_main}, namely \baselinename{Penalty($0.02$)}, \baselinename{Penalty($0.1$)}, and \baselinename{Schedule}. To double the data, we consider two situations: doubling the batch size and keeping the training steps (similar to \algname{}), and doubling the training steps and keeping the batch size. The results are shown in \Cref{fig: safe_cost_more_data}, where the semi-transparent colors show the original performance of the baselines, and the non-transparent colors show the performance of baselines after doubling the data. We can observe that after doubling the data, it is uncertain how the performance of the baselines changes. For example, \baselinename{Penalty($0.02$)} performs better than before with doubling the batch size in \Cref{fig: safe_cost_2bs}, but performs worse with doubling training steps in \Cref{fig: safe_cost_2step}. On the contrary, \ourname{} consistently outperforms all baselines. 

\begin{figure}[t]
    \centering
    \begin{subfigure}{.495\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/experiment/safe_cost_2bs.pdf}
        \caption{Doubling the batch size.}
        \label{fig: safe_cost_2bs}
    \end{subfigure}
    \begin{subfigure}{.495\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/experiment/safe_cost_2step.pdf}
        \caption{Doubling the training steps.}
        \label{fig: safe_cost_2step}
    \end{subfigure}
    \caption{Costs and safety rates of \algname{} and three best baselines in the \textsc{Wheel} environment. We also plot the original performance of the baselines in semi-transparent colors.}
    \label{fig: safe_cost_more_data}
\end{figure}

\subsubsection{Comparison with constrained optimization without a CBF} \label{app: no-cbf}

As the proposed algorithm \algname{} is based on CBF, one natural question to ask is \textit{what if we do not learn the CBF} but directly perform constrained optimization? To answer the question, we consider another baseline which changes the constraint in \Cref{eq: dcbf_safety_filter:stoch_ver2:constraint} to $\mathbb E_{\vx\sim\rho^{\vpi_\theta}}\left[\max\{0, h^{(m)}(\vx)\}\right] \leq 0$ while keep all other parts the same as \algname{}. We compare \algname{} (\baselinename{Learned CBF}) with this new baseline (\baselinename{No CBF}) in the \textsc{Transport2} environment. The results are shown in \Cref{fig: no-cbf}, which suggests that \baselinename{No CBF} cannot achieve a safety rate that is as high as \baselinename{Learned CBF}. Intuitively, it is because CBF not only constrains entering the avoid set, but also constrains the rate at which the agent can approach the safe-unsafe boundary. Therefore, it is more robust to estimation errors than directly doing constrained optimization. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/experiment/train_nocbf.pdf}
    \caption{Comparison between \algname{} with doing constrained optimization without a CBF.}
    \label{fig: no-cbf}
\end{figure}

\subsubsection{Sensitivity analysis of $\nu$}

In \Cref{sec: ablation}, we show that our proposed $\nu$ scheduling method achieves the best result compared with other choices of $\nu$.
In particular, $\nu>2$ results in slower convergence in the cumulative cost.
Here, we further perform experiments to demonstrate the influence of $\nu$ on \algname{}. We consider the \textsc{Spread} environment with $\nu = 1, 2, 4, 6$, and train \algname{} until convergences. The results are shown in \Cref{fig: nu_long}, where the mean cost and standard deviation of \ourname{} and the baseline with the best performance (\baselinename{Schedule}) are shown in dashed lines and shades. We can observe that although the convergence of \algname{} becomes slower with larger $\nu$, \textit{the converged costs are the same}, and are much lower than \baselinename{Schedule}. This phenomenon is different from the baselines, where different choices of hyperparameters directly affect the converged costs (See \Cref{fig: safe_cost_main}). 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/experiment/train_nu_long.pdf}
    \caption{Influence of different $\nu$ on the convergence speed and the converged result of \algname{}. The dashed lines show the mean of \ourname{} and the baseline with the best performance (\baselinename{Schedule}), and the shades show the standard deviation.}
    \label{fig: nu_long}
\end{figure}

\subsection{Additional environments in the safe multi-agent MuJoCo environments}

To further demonstrate the ability of \algname{} in environments with complex discrete-time dynamics, here we consider another benchmark named safe multi-agent MuJoCo \citep{gu2023safe}. We use the Safe \textsc{HalfCheetah(2x3)} and the Safe \textsc{Coupled HalfCheetah(4x3)} tasks, where the agents control different subsets of joints of one or two cheetahs. The first number in the parenthesis denotes the number of agents, while the second number shows the number of joints that each agent controls. The agents need to work collaboratively to maximize the forward velocity but avoid colliding with a wall that moves forward at a predefined velocity. The results are shown in \Cref{fig: cheetah}, which suggests the same results as the main experiments in \Cref{sec: results}, that \ourname{} has the best performance with a fixed set of hyperparameters. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/experiment/cheetah_train.pdf}
    \caption{Cost and safety rates of DGPPO and the baselines during training in the Safe multi-agent MuJoCo environments.}
    \label{fig: cheetah}
\end{figure}

\subsection{Scalability and Generalizability}

Here we test the scalability and generalizability of \algname{}. Following \citet{zhang2024gcbf+}, we define scalability as the number of agents during training, and generalizability as the ability to be deployed with more agents during test time.

Considering scalability, \algname{} has a similar performance as its based method GCBF+ \citep{zhang2024gcbf+} as we have shown in \Cref{sec: results} that \algname{} can be trained on $7$ agents, and GCBF+ is trained with $8$ in its original paper. In addition to \Cref{fig: train_increase_n} in the main pages, here we also provide the comparison of the costs and safety rates of the converged policies of each algorithm in \Cref{fig: safe-cost-increase-n}. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/experiment/safe_cost_increase_n.pdf}
    \caption{Costs and safety rates of the converged policies of \algname{} and the baselines in environments with $N=5, 7$.}
    \label{fig: safe-cost-increase-n}
\end{figure}

Considering generalizability, GCBF+ can be deployed on $512$ agents without significant performance loss after training. Here, we perform a new experiment in the \textsc{Target} environment where \algname{} is also trained with $8$ agents. In \Cref{tab: generalize}, we show the test results of \algname{} deployed with larger numbers of agents. We observe that \algname{} maintains high safety rates and low costs when deployed on up to $512$ agents.

However, the above results are obtained in environments with the same agent density as the training environment. We cannot deploy DGPPO in environments with significantly higher agent density than the training environment because RL algorithms are sensitive to distribution shifts. We leave handling large distribution shifts to future work.

\begin{table}[t]
    \centering
    \caption{Generalizability test results of \algname{}.}
    \label{tab: generalize}
    \begin{tabular}{c|cc}
    \toprule
    Number of agents & Safety rate & Normalized cost \\
    \midrule
    8 & $1.000\pm0.000$ & $1.673\pm0.430$ \\
    16 & $0.992\pm0.088$ & $1.784\pm0.316$
    \\
    32 & $0.987\pm0.112$ & $1.748\pm0.235$ \\
    64 & $0.986\pm0.118$ & $1.799\pm0.418$ \\
    128 & $0.982\pm0.133$ & $1.839\pm0.323$ \\
    256 & $0.985\pm0.122$ & $1.823\pm0.366$ \\
    512 & $0.985\pm0.123$ & $1.821\pm0.390$ \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Code}

The code of our algorithm and the baselines are provided in the `dgppo.zip' file in the supplementary materials and online at \url{https://github.com/MIT-REALM/dgppo}.

\section{More discussion about \algname{}}

\subsection{Advantages on not depending on a nominal policy}

As discussed in \Cref{sec: intro}, one of the drawbacks of the CBF-based methods \citep{wang2017safety,zhang2024gcbf+} is that they require a nominal policy that can achieve high task performance. Here we further discuss why relying on a nominal performant policy is \textit{not} a good idea. 

\paragraph{Requirement of Simple or Known Dynamics.}
Controller design usually requires the dynamics to be simple or requires knowledge of the dynamics.
The PID controllers in \citet{zhang2024gcbf+} are constructed for the unicycle dynamics. More generally, PID controllers are usually only used with single-input single-output systems. For more complicated systems, one could use LQR or MPC, but this requires full knowledge of the dynamics. In addition, PID controllers are much more difficult to apply in environments with complex \textit{contact dynamics}, for example, our \textsc{Transport}, \textsc{Wheel}, and \textsc{Transport2} environments. 

\paragraph{Deadlocks.} 
Another drawback is that the CBF-QP approach of \citet{zhang2024gcbf+} leads to deadlocks, as discussed in Section VIII of \citet{zhang2024gcbf+} or theoretically in e.g. \citet{grover2021deadlock}.
This is because the safety filter approach of \citet{zhang2024gcbf+} only minimizes deviations from the nominal policy at the current time step, even if this leads to a deadlock at a future time step. In contrast, minimizing the cumulative cost directly takes future behavior into account and hence will try to avoid deadlocks. Here we perform an additional experiment to demonstrate this. We apply the converged controller trained with GCBF+ and \algname{} respectively in the \textsc{Target} environment shown in \Cref{fig: deadlock-comparison}, where the agent needs to get around a large static obstacle and reach the goal. In the figure, we can observe that the GCBF+ policy gets in a deadlock behind the obstacle because the GCBF+ policy is myopic and only considers safety safe and minimizes the deviation from the reference controller at the \textit{current} timestep. Therefore, stopping behind the obstacle is the optimal solution for GCBF+. On the contrary, the \algname{} policy successfully reaches the goal because it minimizes the \textit{long-horizon} cumulative cost. 

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[trim={1.3cm 0 1.3cm 0},clip,angle=90,width=\columnwidth]{figs/deadlock/gcbf+_deadlock.pdf}
        \caption{GCBF+ \citep{zhang2024gcbf+}}
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[trim={1.3cm 0 1.3cm 0},clip,angle=90,width=\columnwidth]{figs/deadlock/dgppo_no_deadlock.pdf}
        \caption{\algname{}}
    \end{subfigure}
    \caption{Comparison of the converged policies learned using \algname{} and GCBF+ \citep{zhang2024gcbf+} in a \textsc{Target} environment, where the agent (blue) needs to avoid the large obstacle (red) and reach the goal (green). The GCBF+ policy is myopic and gets stuck in a deadlock, while the \algname{} policy avoids the obstacle and reaches the goal to minimize the cumulative cost.}
    \label{fig: deadlock-comparison}
\end{figure}

\end{document}
