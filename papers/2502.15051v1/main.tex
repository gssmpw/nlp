%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

%\usepackage{pdf}

\usepackage{todonotes}
\usepackage{adjustbox} % To help with adjusting the size of the figure

\usepackage{standalone}  % Allows standalone files to be included
\usepackage{graphicx}    % For any other images you may need

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\crefname{equation}{}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{problem}[theorem]{Problem}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% my packages
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage[acronym]{glossaries}
\glsdisablehyper
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{natbib}
\setcitestyle{round}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumerate}
\usepackage{comment}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{positioning, decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
%\usepackage{subcaption}





% added acronyms
\newacronym{oavi}{OAVI}{Oracle Approximate Vanishing Ideal Algorithm}
\newacronym{abm}{ABM}{Approximate Buchberger-Möller Algorithm}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{cka}{CKA}{Centered Kernel Alignment}
\newacronym{cca}{CCA}{Canonical Correlation analysis}
\newacronym{gs}{GS}{geometric score}
\newacronym{nn}{NN}{Neural Network}
\newacronym{pnn}{PNN}{Polynomial Neural Network}
\newacronym{gmdh}{GMDH}{group method of data handling}
\newacronym{spnn}{SPNN}{sigma-pi neural network}
\newacronym{spsnn}{SPSNN}{sigma-pi-sigma neural network}
\newacronym{avinn}{VI-Net}{Vanishing Ideal Network}
\newacronym{avinns}{VI-Nets}{Vanishing Ideal Networks}  % We redefine the plural to introduce this as well, since there seems to be no plural equivalent of \glsfirst
\newacronym{svm}{SVM}{support vector machine}
\newacronym{vgg}{VGG}{very deep convolutional networks}
\newacronym{resnet}{ResNet}{Residual Network}


\makeglossaries

% Define \X as \mathbf{X} and \x as \mathbf{x}
\newcommand{\X}{\mathbf{X}} 
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\norm}[1]{\left\|#1\right\|}
% operators
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\relu}{ReLU}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\spann}{span}
\DeclareMathOperator{\inex}{InEx}
\DeclareMathOperator{\argmax}{argmax}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage{todonotes}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Approximating Latent Manifolds in Neural Networks via Vanishing Ideals}

\begin{document}

    \twocolumn[
        \icmltitle{Approximating Latent Manifolds in Neural Networks via Vanishing Ideals}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
        \icmlsetsymbol{equal}{*}

        \begin{icmlauthorlist}
            \icmlauthor{\centering \textbf{Nico Pelleriti \quad Max Zimmer \quad Elias Wirth \quad Sebastian Pokutta}
            \\
            \small{Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany\\
            Institute of Mathematics, Technische Universität Berlin, Germany}\\
            \texttt{\{pelleriti,zimmer,wirth,pokutta\}@zib.de}}{}
        \end{icmlauthorlist}

        \vskip 0.3in
    ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
    Deep neural networks have reshaped modern machine learning by learning powerful latent representations that often align with the manifold hypothesis: high-dimensional data lie on lower-dimensional manifolds. In this paper, we establish a connection between manifold learning and computational algebra by demonstrating how vanishing ideals can characterize the latent manifolds of deep networks. To that end, we propose a new neural architecture that (i) truncates a pretrained network at an intermediate layer, (ii) approximates each class manifold via polynomial generators of the vanishing ideal, and (iii) transforms the resulting latent space into linearly separable features through a single polynomial layer. The resulting models have significantly fewer layers than their pretrained baselines, while maintaining comparable accuracy, achieving higher throughput, and utilizing fewer parameters. Furthermore, drawing on spectral complexity analysis, we derive sharper theoretical guarantees for generalization, showing that our approach can in principle offer tighter bounds than standard deep networks. Numerical experiments confirm the effectiveness and efficiency of the proposed approach.
\end{abstract}


    \section{Introduction}
    
    Deep \glspl{nn} have revolutionized various fields by learning powerful feature representations from data \citep{7797053, alzubaidi_review_2021, shi2022theoretical}. A key theoretical foundation of their success is the manifold hypothesis \citep{cayton_algorithms_2008, fefferman_testing_2013, brahma_why_2016}, which suggests that high-dimensional real-world data lies on lower-dimensional manifolds, making many learning tasks tractable. 

    \begin{figure}[H]
        \centering
        % Resize the TikZ diagram to fit the column width (for single-column format)
        \resizebox{0.8\columnwidth}{!}{\input{Figures/nn_corporate}}
    
       \caption{Comparison between a baseline \gls{nn} (left) and a truncated \gls{nn} (right). The baseline \gls{nn} output is interpreted as class probabilities. The truncated \gls{nn} output is interpreted using the manifold hypothesis, suggesting that classes lie on distinct manifolds. We characterize these manifolds using a set of polynomials, capturing their underlying structure. The depicted manifold is a visualization of a Calabi–Yau manifold, licensed under the Creative Commons Attribution-Share Alike 2.5 Generic license \citep{calabi_yau_wikipedia}.}
        \label{fig:nn_diagram}
    \end{figure}
    Such manifolds can often be described as the zero set of 
    a system of polynomial equations \citep{a03bacc3-7240-3944-a9aa-c74ef7db94fd}. Formally, this system gives rise to an \emph{algebraic variety}, which is defined as
\begin{align*}
\mathcal{Z} = \{\, \mathbf{z} \in \mathbb{R}^n : p_i(\mathbf{z}) = 0 \text{ for all } i \in \{1,\dots,k\}\},
\end{align*}
for some set of polynomials ${p_1,\dots,p_k}$. Given a sample ${\mathbf{Z}} = \{\mathbf{z}_1, \dots, \mathbf{z}_m \} \subseteq \mathcal{Z}$ drawn from this variety, we can in turn estimate the polynomials $p_1, \dots, p_k$ that characterize $\mathcal{Z}$ by computing the \emph{vanishing ideal} of this sample, i.e., the set of all polynomials that vanish over $\mathbf{Z}$, defined as 
\begin{align*}
    \mathcal{I}(\mathbf{Z}) = \{p \in \mathcal{P} : p(\mathbf{z}_i) = 0 \text{ for all } \z_i \in \mathbf{Z}\},
\end{align*}
where $\mathcal{P}$ is the set of all $n$-variate polynomials. While the vanishing ideal is infinite, there exist finitely many polynomials that generate it \citep{10.5555/1951670}. To construct such a set of \emph{generators}, one typically employs \emph{vanishing ideal algorithms} \citep{fassino_almost_2008, heldt_approximate_2009, livni_vanishing_2013, limbeck_computation_2014, zhang_improvement_2018, wirth_conditional_2024}: For a given set of points $\mathbf{Z}$, these algorithms construct the finite set of generators, thereby characterizing $\mathcal{I}(\mathbf{Z})$. These sets of polynomials have been leveraged in downstream tasks, such as hand posture recognition, principal variety analysis for nonlinear data modeling and solution selection using genetic programming \citep{iraji_principal_2017, 10.1109/CEC.2016.7748325, wang_towards_2018}.


In this work, we aim to use vanishing ideal algorithms to construct polynomials that describe the \emph{latent space} of an \gls{nn}, which serves as a compressed, structured representation of the input data within the \gls{nn}. Prior work \citep{brahma_why_2016} has shown that this latent space often behaves like a manifold, where the network disentangles complex, high-dimensional manifolds into simpler, lower-dimensional ones.

A key observation, and central motivation for our work, is that if the latent space is a manifold, it can be algebraically characterized as the zero set of certain polynomials. Further, if the data classes correspond to disjoint algebraic varieties, then the generators of their vanishing ideals can be used as discriminative features to separate classes in the latent space, making them linearly separable \citep{livni_vanishing_2013}. 

In consequence, computing generators of the vanishing ideals on such latent representations is highly desirable. Unfortunately however, current vanishing ideal algorithms are incapable of processing high-dimensional data, such as images or even their representations in the latent space of \glspl{nn}. While recent advancements (\citealt{wirth_conditional_2024}) have made it feasible to compute approximate generators for data with dimensionalities up to 67, beyond these scales the computational cost grows prohibitively. Already at this scale, vanishing ideal algorithms can produce millions of polynomials, making subsequent evaluation computationally infeasible. Hence, identifying a compact and meaningful subset of generators that effectively characterizes the underlying manifold remains an open challenge.

    Addressing these challenges, we introduce a set of techniques that make vanishing ideal computations feasible for high-dimensional latent spaces. Building on these advancements, we propose a novel neural network architecture, which we term {\glspl{avinn}}. \glspl{avinn} leverage the per-class latent representations of a pretrained \gls{nn} at a layer $L'$, constructing generators of their corresponding vanishing ideals, as illustrated in \cref{fig:nn_diagram}. This approach yields a feature transformation that renders linearly-inseparable data linearly-separable, which is then used for simple discrimination and classification. Notably, while discarding all layers beyond layer $L'$, \glspl{avinn} still achieve competitive performance while using significantly fewer parameters. We summarize our key contributions:
 
 
    
    \paragraph{1. Characterizing Manifolds in \gls{nn} Latent Spaces.} Vanishing ideal algorithms require low-dimensional input points and often produce an excessive number of generators, each with numerous monomial terms needing individual evaluation. This makes their application to the high-dimensional latent space of \glspl{nn} infeasible. Furthermore, the complete set of generators often fails to accurately represent the algebraic varieties of the latent space. We address these issues by: \textbf{a)} introducing dimensionality reduction and data rescaling techniques to enable the application of vanishing ideal algorithms in \gls{nn} latent spaces; \textbf{b)} leveraging and adapting stochastic formulations of vanishing ideal algorithms to derive relatively sparse polynomials that can be efficiently evaluated; \textbf{c)} proposing pruning methods to significantly reduce the number of generators, preserving only those that meaningfully capture the underlying manifold.
    \paragraph{2. Building Efficient \glspl{avinn}.} We propose \glspl{avinn}, a novel \gls{nn} architecture that truncates a pretrained \gls{nn} and replaces its final layers with a single polynomial layer. This layer embeds the generators of each class's vanishing ideal, transforming the linearly-inseparable but polynomially-separable features of the latent space into linearly-separable ones. Unlike methods that approximate manifolds through indirect means like simplicial complexes \citep{khrulkov2018geometry} or k-nearest neighbor graphs \citep{tsitsulin2020shape}, our approach directly captures their algebraic structure through polynomial roots. 

    \paragraph{3. Providing Theoretical Learning Guarantees.} We prove learning guarantees for \glspl{avinn} by leveraging the spectral complexity framework \citep{bartlett2017spectrallynormalizedmarginboundsneural}. Specifically, we exploit the sparsity of the constructed generators to derive better generalization error bounds when comparing to the corresponding pretrained \gls{nn}.

    \paragraph{4. Training \glspl{avinn}.} We perform extensive experiments that showcase that \glspl{avinn} can achieve performance competitive to the pretrained baseline \glspl{nn} while using much fewer parameters. In particular, we analyze the trade-off between model accuracy and parameter count, when truncating more and more layers from the pretrained \gls{nn}. 

    To the best of our knowledge, leveraging tools from Computational Algebra in the context of Deep Learning is a relatively underexplored field. By demonstrating the feasibility of using vanishing ideal computations to characterize data manifolds in \gls{nn} latent spaces, we aim to encourage further research in this direction.



\section{Algebraic Characterization of Data Manifolds}\label{sec:methodology}
In this section, we present our methodology for approximating data manifolds using generators of the vanishing ideal, specifically addressing the challenges of applying vanishing ideal algorithms to \gls{nn} latent spaces. Our approach constructs class-specific polynomial sets that characterize the underlying manifold structure, which we then leverage to build efficient polynomial layers. We begin by formally stating our core problem:



\subsection{Preliminaries}
Let $l, k, m, n, q, K \in \mathbb{N}$. Vectors and matrices are denoted in \textbf{bold}. Define $\mathcal{T}$ and $\mathcal{P}$ as the sets of monomials and polynomials, respectively. A polynomial $g \in \mathcal{P}$ is expressed as $g = \sum_{i=1}^{q} c_i t_i$, with coefficients $\mathbf{c} = (c_1, \dots, c_q)$ and monomials $t_i \in \mathcal{T}$ for $i \in [q]$. We focus on the latent representations of \glspl{nn}, specifically the output activations of a specific layer $L'$. These vectors, denoted as $\z \in \mathbb{R}^n$, are derived from passing a sample $\x$ through an \gls{nn} $\phi$. We focus on \glspl{nn} for classification tasks. For notational simplicity, functions are often applied to sets, e.g., $\phi(\X) = \{\phi(\x) : \x \in \X\}$ for a set $\X$. Superscript indices denote class membership, while subscript indices denote specific elements in a set, e.g., $\x^k_1, \x^k_2, \x^k_3$ are three distinct input samples (interpreted as vectors) of class $k$.


Given some set $\Z = \{\z_1, \dots, \z_m\} \subseteq \mathbb{R}^n$, we follow \citet{10.5555/1951670} and define the \emph{vanishing ideal} as follows.

\begin{definition}
    Given sample $\Z$, the \emph{vanishing ideal} $\mathcal{I}(\Z) \subseteq \mathcal{P}$ is defined as
    \[
        \mathcal{I}(\Z) = \{g \in \mathcal{P} : g(\z_i) = 0 \text{ for all } i \in [m]\}.
    \]
    Conversely, given a set of polynomials $\mathcal{F} \subseteq \mathcal{P}$, the corresponding \emph{algebraic variety} $\mathcal{V}(\mathcal{F}) \subseteq \mathbb{R}^n$ is defined as
    \[
        \mathcal{V}(\mathcal{F}) = \{\z \in \mathbb{R}^n : g(\z) = 0 \text{ for all } g \in \mathcal{F}\}.
    \]
\end{definition}
Despite $\mathcal{I}(\Z)$ being an infinite set, there exist finitely many polynomials that generate it \citep{10.5555/1951670}. Further, such generators can be computed using a variety of algorithms \citep{livni_vanishing_2013, limbeck_computation_2014, wirth_conditional_2024}.



\subsection{Class Separation in the Latent Space} 
A key insight is that data points from different classes in the latent space, while not linearly separable, can be distinguished through polynomial separability. This concept is central to \glspl{avinn}, which we will define later. Consider a pretrained \gls{nn} $\phi$ that we modify by truncating a certain number of its final layers, resulting in $\hat \phi$ (see \cref{fig:nn_diagram}). Let $\X^k$ represent input data points for class $k \in [K]$, such as those from a validation dataset. Correspondingly, $\Z^k$ denotes the latent space representations after truncation, i.e., $\Z^k = \hat \phi(\X^k)$. For each class $k$, we anticipate that the set of output activations $\Z^k$ resides on a manifold that can be approximated using generators of the vanishing ideal of $\Z^k$.

Suppose now that for each class $k$, we are given $n_k$ generators $p^k_1, \dots, p^k_{n_k}$ of the vanishing ideal of $\Z^k$. Then, we define the feature transformation for a latent space sample $\z \in \mathbb{R}^n$ as:
\begin{align*}
    g^k(\z) = \left(|p^k_1(\z)|, \dots, |p^k_{n_k}(\z)|\right) \in \mathbb{R}^{n_k}.
\end{align*}

Aggregating these feature transformations for all classes yields the mapping:
\begin{align*}
    G(\z) = \left(g^1(\z), \dots, g^K(\z)\right) \in \mathbb{R}^{n_1 + \dots + n_K}.
    \tag{POLY}
    \label{polylayer}
\end{align*}
Typically, the latent space subsets $\Z^k$ are not linearly separable. While additional non-linear layers might achieve linear separability, this is not assured at intermediate layers. By definition of the vanishing ideal, the generators $p^k_1, \dots, p^k_{n_k}$ for class $k$ vanish on $\Z^k$ but most likely not on $\Z^i$ for $i \neq k$. Thus, for a point $\z \in \Z^k$, $G(\z)$ is zero at coordinates for class $k$ generators and positive elsewhere, enabling a linear classifier to separate the classes. For an input $\x$, the mapping
\begin{align*}
    \x \mapsto \mathbf{W}G(\hat \phi(\x))
\end{align*}
allows linear classification, where $\mathbf{W} \in \mathbb{R}^{K \times (n_1 + \dots + n_K)}$ is the weight matrix mapping to the $K$ class logits.


\subsection{Practical Data Manifold Approximation}
Approximating class data manifolds with algebraic varieties provides a structured method to capture data relationships. As we have shown, this approach can further be used to construct a classifier from a truncated \gls{nn}. However, real-world data is noisy, making it impractical to find polynomials that vanish exactly on the data. Furthermore, we argue that computing the \emph{exact} vanishing ideal of a sample $\Z$ is not only computationally intractable but also lacks generalization properties. Since $\Z$ is finite, it itself forms a variety (e.g. as the roots of an appropriate polynomial), and by the ideal-variety correspondence \citep{10.5555/1951670}, $\mathcal{V}(\mathcal{I}({\Z})) = \Z$. Thus, the algebraic variety defined by the vanishing ideal's generators includes only the points in $\Z$, offering no generalization beyond the sample. However, for class $k$, our aim is to approximate the underlying manifold $U^k$ from which $\Z^k$ is drawn, not just to describe $\Z^k$. In general, recovering $U^k$ exactly from a finite sample is impossible without structural information, as $U^k$ could range from being as minimal as $\Z^k$ to as broad as $\mathbb{R}^n$.

Hence, research has mainly focused on constructing approximate generators for the vanishing ideal, i.e., polynomials that \emph{$\psi$-approximately vanish} over the data allowing a small error tolerance $\psi \geq 0$ \citep{ heldt_approximate_2009, livni_vanishing_2013, limbeck_computation_2014, wirth_conditional_2024}. We define \emph{$\psi$-approximately vanishing} polynomials as follows:
\begin{definition}\label{def:approx_vanishing}
    For $\psi \geq 0$, a polynomial $g = \sum_{i=1}^{k}c_it_i$ is \emph{$\psi$-approximately vanishing} over $\Z$ if MSE$(g, \Z) = \frac{1}{m}\sum_{i = 1}^mg(\z_i)^2 \leq \psi$ and its leading coefficient equals $1$.
\end{definition}

Setting the leading coefficient (i.e., the highest degree monomial's coefficient) to $1$ is essential for defining approximately vanishing polynomials. Without this constraint, any polynomial $g \in \mathcal{P}$ can be rescaled to approximately vanish for all $\z \in \Z$, causing the \emph{spurious vanishing problem} \citep{kera2019spurious}, where polynomials fail to capture meaningful data structure. 
Unfortunately, the ideal generated by all $\psi$-approximately vanishing polynomials, known as the \emph{approximate vanishing ideal}, also fails to provide the desired structure \citep{heldt_approximate_2009}, as it typically becomes the unit ideal, encompassing the entire polynomial ring.

{To summarize:} In contrast to the \emph{exact} vanishing ideal, which is too restrictive and only captures the points in $\Z$, the \emph{approximate} vanishing ideal generates the entire polynomial ring and does not depend on $\Z$ at all. Thus, we focus on identifying a subset $\mathcal{
G}$ of its constructed generators, approximating the underlying variety and therefore manifold effectively by $\mathcal{V}(\mathcal{
G})$.

\section{Constructing practical \glspl{avinn}}\label{sec:pol_layer}
We introduced the core concept of \glspl{avinn}, using generators of the vanishing ideal to approximate per-class data manifolds. Now, we address the identified challenges: We start with our approach for finding polynomial generators that meaningfully describe the underlying manifolds, followed by adaptations of existing vanishing ideal algorithms to being able to compute the generators in the first place.

\subsection{Selecting Meaningful Polynomials}
We derived that computing the exact vanishing ideal is too restrictive, only capturing points in $\Z$, while the approximate vanishing ideal generates the entire polynomial ring. To meaningfully approximate the underlying manifold $U_k$ for each class $k \in [K]$, we select a \emph{subset} of the constructed generators of the approximate vanishing ideal with some tolerance $\psi > 0$, based on two key criteria: \textbf{a)} \emph{low complexity} and \textbf{b)} \emph{discriminative power}. We detail these approaches below and provide theoretical justification in \cref{sec:theoretical_analysis} from the perspective of statistical learning theory.

\paragraph{Low Complexity: Finding coefficient-sparse generators.}
The number of monomial terms increases exponentially with the polynomial's maximal degree, making evaluation inefficient. A polynomial's complexity is determined by the sparsity of its coefficient vector $\mathbf{c}$; greater sparsity implies fewer non-zero coefficients and thus fewer monomial terms to compute. To achieve this, we use the meta-algorithm \gls{oavi} \citep{wirth_approximate_2023, wirth_conditional_2024} for vanishing ideal generation, adopting the Frank-Wolfe algorithm \citep{https://doi.org/10.1002/nav.3800030109, pmlr-v28-jaggi13} as its oracle to produce sparse coefficients. We also employ the \gls{abm} \citep{limbeck_computation_2014}, which similarly yields sparse generators \citep{wirth_conditional_2024}, and limit the maximal degree to avoid overfitting. While both \gls{oavi} and \gls{abm} yield coefficient-sparse polynomials, they can still generate many that do not meaningfully approximate $U^k$, an issue addressed next.

\paragraph{Discriminative Power: Finding distinctive generators.}
We ensure selecting polynomials that effectively differentiate $U^k$ from other class regions by eliminating those that also vanish for other classes. Our experiments demonstrate that most generators capture the dataset's global structure rather than class-specific features, so we introduce a pruning score to rank each generator based on its non-vanishing ability on other classes. Let $\Z^k = \{\z^k_1, \dots, \z^k_{m_k}\}$ represent the latent representations for class $k$. For a generator $p$ associated with class $j$, its score $s^j(p)$ is defined as
\[
s^j(p) = \min_{k \neq j} \frac{1}{m_k}\sum_{i=1}^{m_k}\left|p(\z^k_i)\right|.
\]
This score measures the minimal average vanishing of generator $p$ for class $j$ on other classes. Low scores indicate that a generator nearly vanishes on some class $k$, despite being a generator for class $i$, showing its lack of distinctiveness. We rank generators using $s^j(g)$ and select the top  $p \ \%$ for each of the class $j$, as the manifolds for different classes likely exhibit varying complexity \citep{brown2023verifyingunionmanifoldshypothesis}. The remaining $N$ generators are the most discriminative and are used to form the polynomial layer, while others are discarded. In \Cref{sec:experiments}, we validate our method, demonstrating that a subset of generators suffices for classification. 

Evaluating many sparse polynomials can still be computationally intensive due to the lack of sparsity overlap in their coefficient vectors: different polynomials might have entirely different monomials. Our two-fold approach addresses this: First, we use \gls{oavi} or \gls{abm} to generate sparse coefficient vectors. Second, we prune the set of generators to enhance discriminative power, significantly reducing the number of polynomials and indirectly also the number of monomial terms that need to be evaluated, allowing us to drastically shrink the matrix of coefficients. The remaining coefficients are then fine-tuned via gradient descent to recover any lost accuracy. This process, visualized in \cref{fig:pruning}, focuses on identifying a subset of generators that effectively approximate the data manifold.
\begin{figure}
    \centering
    \resizebox{0.9\columnwidth}{!}{\input{Figures/pruning_python}}

   \caption{Illustration of our method for obtaining parameter-efficient representations of the most discriminatory vanishing ideal generators. The process begins by pruning columns corresponding to polynomials with the lowest pruning scores, such as  $p_2$  and  $p_4$  in this example. Exploiting the sparsity of the generated polynomials, rows with only zero entries—indicating monomials that do not contribute—do not need to be evaluated. As a result, only the essential monomials ($y ,  x^2 $ and  $y^2$  in this case) need to be evaluated during the final computation. Experiments confirm that this approach effectively reduces the number of evaluated monomials (cf. \cref{fig:impact_pruning_c100_monomials} in the appendix).}
    \label{fig:pruning}
\end{figure}

\subsection{Adapting Vanishing Ideal Algorithms to the Latent Space}
In the previous section, our pruning approach assumed access to the full set of generators for each class’s vanishing ideal. While \gls{abm} and \gls{oavi} (both fully described in \cref{sec:vanishing_ideal_algos}) are known for constructing sparse polynomials \citep{wirth_conditional_2024}, directly applying these methods to high-dimensional latent spaces with large sample sizes is computationally infeasible. Recent advances demonstrate their effectiveness on moderate-dimensional datasets \citep{wirth_approximate_2023}, but making them tractable in large-scale settings requires further adaptation.

We propose three key adaptations. First, we adopt \emph{stochastic} variants of \gls{oavi} or \gls{abm}, enforcing vanishing on random subsets of each class' rather than on the entire class dataset. This significantly reduces computational overhead and memory usage. Second, we perform \emph{dimensionality reduction} (e.g., via PCA) in the latent space, effectively reducing the dimensionality of input features of \gls{oavi} and \gls{abm} and thereby controlling the growth of monomial terms. Third, we employ \emph{lower-precision arithmetic} (e.g., float16) in the vanishing ideal computations and a \emph{tanh-based rescaling} \citep{hampel2011robust} (cf. \cref{sec:tanh-rescaling}) of latent features to stabilize numerical computations. Although these choices can slightly reduce initial accuracy, they preserve the crucial monomial structure, which we later refine by \emph{retraining} coefficients via gradient descent. This pipeline thus retains the principal strengths of \gls{oavi} and \gls{abm}—namely, the ability to discover sparse, class-specific polynomials—while ensuring improved scalability in the setting of latent spaces within neural networks.



   



\subsection{The \gls{avinn} Pipeline}
\cref{alg:pipeline} describes the \gls{avinn} pipeline. It starts by training or using a pretrained network $\phi$, truncating it at layer $L'$, and extracting features $\mathbf{Z}^k$ for each class $k \in [K]$. Generators $p_1^{k}, \dots, p_{n_k}^{k}$ of the vanishing ideal of $\mathbf{Z}^k$ are computed using stochastic variants of \gls{oavi} and \gls{abm}, followed by pruning. The pruned generators are aggregated to construct the transformation $G$ from \cref{polylayer}. Finally, the coefficients of the polynomials in $G$ and the weights $\mathbf{W}_F$ of the appended linear layer are jointly retrained to optimize classification performance.
\begin{algorithm}
\caption{\gls{avinn} Pipeline}
\label{algo:avinn}
\begin{algorithmic}[1]
\label{alg:pipeline}
\STATE \textbf{Input:} Dataset $\mathbf{X} = \{\x^k_i\}_{i, k}$, \gls{nn} $\phi$, intermediate layer $L'$, pruning function $\text{PRUNE}$, vanishing ideal algorithm $\text{VANISH}$.
\STATE \textbf{Output:} Trained \gls{avinn} $\Tilde{\phi}_{L'}$.
\STATE $\phi \leftarrow \text{TRAIN}(\phi, \mathbf{X})$ 
\STATE $\phi_{L'} \leftarrow \text{TRUNCATE}(\phi, L')$
\FOR{class $k \in [K]$}
    \STATE $\mathbf{Z}^k \leftarrow \phi_{L'}(\X^k)$
    \STATE $p_1^{k}, \dots, p_{n_k}^{k} \leftarrow \text{VANISH}(\mathbf{Z}^k)$
\ENDFOR
\FOR{class $k \in [K]$}
    \STATE  $p_1^{k}, \dots, p_{n'_k}^{k} \leftarrow \text{PRUNE}((p_1^{k}, \dots, p_{n_k}^{k}), \mathbf{Z})$
    \STATE $g^k \leftarrow (|p_1^{k}|, \dots, |p_{n'_k}^{k}|)$
\ENDFOR
\STATE $G  \leftarrow ({g}^1, \dots, {g}^K), \mathbf{W}_F \leftarrow \text{INIT}(K, n'_1 + \dots, n'_K)$
\STATE $\W_F \leftarrow \text{TRAIN}\left (\mathbf{W}_F, G (\mathbf{Z})\right )$
\STATE $\Tilde \Phi_{L'} \leftarrow \W_F \circ G \circ \Phi_{L'}$
\end{algorithmic}
\end{algorithm}








\section{Theoretical Analysis}\label{sec:theoretical_analysis}
We establish learning guarantees for \glspl{avinn}, leveraging the sparsity of the coefficient matrix obtained through pruning (cf. \cref{fig:pruning}). We demonstrate that \glspl{avinn} can achieve lower \emph{spectral complexity} \citep{bartlett2017spectrallynormalizedmarginboundsneural} than baseline \glspl{nn}, resulting in a tighter bound on the generalization error.


\subsection{Spectral Complexity of \gls{avinn}}
Following \citet{bartlett2017spectrallynormalizedmarginboundsneural}, we formally introduce the spectral complexity of an \gls{nn}. To that end, let $ \norm{\W}_2$ and $\norm{\W}_{p,q} := \norm{\left( \|\W_{:,1}\|_p, \ldots, \|\W_{:,m}\|_p \right)}_q$ denote the spectral norm and the $(p,q)$-norm of matrix $\W$, respectively.

\begin{definition}[Spectral Complexity] \label{def:spectral_complexity}
    Let $\phi$ be an \gls{nn} of depth $L$ with $\rho_i$-Lipschitz activation functions $\{\sigma_i\}_{i=1}^L$ and weight matrices $\{\W_i\}_{i=1}^L$, i.e., $\phi$ is given as the function $\phi \equiv \sigma_L \circ \W_{L} \circ \sigma_{L-1} \circ \W_{L-1} \circ \dots \circ \sigma_1 \circ \W_1$. The spectral complexity $R_{\phi}$ of $\phi$ is defined as 
\begin{align*}
    R_{\phi} \coloneqq \left( \prod_{i=1}^L \rho_i \|\mathbf{W}_i\|_{2} \right) \left( \sum_{i=1}^L \frac{\|\mathbf{W}_i^T\|_{2,1}^{2/3}}{\|\mathbf{W}_i\|_2^{2/3}} \right)^{3/2}.
\end{align*}
\end{definition}
In the following, we will show that we can encode \glspl{avinn} in the format of \cref{def:spectral_complexity}, which allows us to provide learning guarantees, since by \citet[][Theorem 1.1]{bartlett2017spectrallynormalizedmarginboundsneural}, the generalization error of $\phi$ is proportional to its spectral complexity $R_{\phi}$. To that end, we require the following assumption:

\begin{assumption}
Let the polynomial layer $G: \mathbb{R}^n \rightarrow \mathbb{R}^{N}$ be as in \cref{polylayer}, where $N \coloneqq n_1 + \dots + n_K$. Let further $S \in \mathbb{N}$ be the number of monomial terms needed to evaluate $G$ and $d \in \mathbb{N}$ the highest degree of any polynomial in $G$. We assume that there exists $\tau > 0$ such that $\|\mathbf{c}_i^k\|_1 \leq \tau$ i.e., the coefficient vectors $\mathbf{c}_i^k$ of each polynomial $p_i^k$ are bounded in the $\ell_1$-norm by $\tau$.
\label{assump:G}
\end{assumption}

This assumption is justified as bounded coefficient vectors can be ensured either directly by the vanishing ideal algorithm (e.g., \gls{oavi}, where $\tau$ is a hyperparameter) or indirectly through our pruning and fine-tuning approach, which adjusts and sparsifies the polynomials post hoc. Next, we introduce an activation function, that allows us to encode the polynomial layer in the format of \cref{def:spectral_complexity}.

\begin{definition}[InEx-activation function]\label{def:inex}
    Let $d \in \mathbb{N}$ as in \cref{assump:G}. Abbreviating $j_k \coloneqq \sum_{i = 1}^k \binom{i}{d}$, the \emph{InEx-activation function} of degree $d$ is defined as
   \begin{align*}
    \alpha^d: \mathbb{R}^{2^d}\rightarrow \mathbb{R} \quad \z \mapsto \frac{1}{d!} \sum_{k = 1}^d (-1)^{d-k} \sum_{j = j_{k-1}}^{j_k} \left( z_j \right)^d.
\end{align*}
\end{definition}

The following lemma leverages that the $\inex$-activation function, based on the \emph{inclusion-exclusion principle}, computes monomial terms by selecting the appropriate variables using the rows in the matrix $\mathbf{W}_M$. The vector $\sigma(\mathbf{W}\mathbf{x})$ thus represents the $S$ monomial terms needed for evaluating $G$. These terms are then multiplied by the coefficient matrix $\mathbf{W}_C$ to complete the evaluation. 


\begin{lemma}\label{lem:vinet_as_nn}
Let $G$ and $S$ as in \cref{assump:G} and define the stacked $\inex$-activation function $\mathbf{\sigma} = (\alpha^{d_1}, \dots, \alpha^{d_S}): \mathbb{R}^s \rightarrow \mathbb{R}^S$ for $s \in \mathbb N$. Then, $G$ can be represented as the two layer \gls{nn} $\left[\W_C \circ \mathbf{\sigma} \circ \W_M\right]$, where $\W_M \in \mathbb{R}^{s \times n}$ and $\mathbf{W}_C \in \mathbb{R}^{N \times S}$ such that $s \in \mathcal{O}(S)$. 
\end{lemma}


The entire \gls{avinn} consists of the $L'$ remaining truncated layers of the \gls{nn}, the matrices $\mathbf{W}_M, \mathbf{W}_C$ from \cref{lem:vinet_as_nn} and a final, linear layer $\mathbf{W}_F$. The spectral complexity of $\Tilde \phi_{L'}$ can hence be computed analogously to \cref{def:spectral_complexity}, albeit with $L'+3$ layers instead of $L$.

\subsection{Learning Guarantees for \glspl{avinn}}
We now analyze the spectral properties of matrices $\W_M$ and $\W_C$ to establish an upper bound on the spectral complexity of the \gls{avinn}, enabling the spectral complexity comparison with the baseline \gls{nn}. Consider an \gls{nn} $\phi$ with $L$ layers, defined by weight matrices $\{\W_i\}_{i=1}^L$ and $\rho_i$-Lipschitz activation functions $\{\sigma_i\}_{i=1}^L$, as per \cref{def:spectral_complexity}. Let $\phi_{L'} \equiv \sigma_{L'} \circ \W_{L'} \circ \sigma_{L'-1} \circ \W_{L'-1} \circ \dots \circ \sigma_1 \circ \W_1$ represent the truncated \gls{nn} of depth $L' < L$. The corresponding encoded \gls{avinn} $\Tilde \phi_{L'} \equiv \W_C \circ \sigma \circ \W_M \circ \phi_{L'}$ has $L'+3$ layers and follows the format of \cref{def:spectral_complexity}, incorporating the polynomial layer $G$ from \cref{polylayer}, with $G$ satisfying \cref{assump:G}.

To simplify notation, we denote the weight matrices of $\Tilde \phi_{L'}$ as $(\Tilde \W_1, \ldots, \Tilde \W_{L'+3})$ with corresponding Lipschitz constants $(\Tilde \rho_1, \ldots, \Tilde \rho_{L'+3})$. Note that $\Tilde \W_i = \W_i$ for $i \in \{1, \ldots, L'\}$ and $\left(\Tilde \W_{L'+1}, \Tilde \W_{L'+2}, \Tilde \W_{L'+3}\right) = \left(\W_M, \W_C, \W_F\right)$, analogously for the Lipschitz constants, however noting that for the new layers, we have $(\Tilde \rho_{L'+1}, \Tilde \rho_{L'+2}, \Tilde \rho_{L'+3}) = (d, 1, 1)$.



\begin{theorem}\label{thm:spectral_complexity}
    Let the networks $\phi, \phi_{L'}$ and $\Tilde \phi_{L'}$ as above. Then, the spectral complexities of $\phi$ and $\Tilde \phi_{L'}$ can be related as $R_{\Tilde \phi_{L'}} = \kappa \cdot R_{\phi}$, where $\kappa$ is given by
    \begin{align*}
        \kappa = \frac{\prod\limits_{i = L'+1}^{L'+3} \Tilde \rho_i \norm{\Tilde \W_i}_{2}}{\prod\limits_{i = L' + 1}^{L} \rho_i \norm{\W_i}_{2}} 
            \left(
            \frac{
            \sum\limits_{i=1}^{L'+3} \norm{\Tilde \W_i^T}_{2,1}^{\frac{2}{3}}\norm{\Tilde \W_i}_{2}^{-\frac{2}{3}} 
            }{
            \sum\limits_{i = 1}^L \norm{\W_i^T}_{2,1}^{\frac{2}{3}}\norm{\W_i}_{2}^{-\frac{2}{3}}
            }
            \right)^{\frac{3}{2}}.
    \end{align*}

If there further exist $\lambda_1, \lambda_2 > 0$, such that $\norm{\Tilde \W_{L'+3}}_{2} \leq \lambda_1$ and $\norm{\Tilde \W_{L'+3}}_{2,1} \leq \lambda_2 \norm{\Tilde \W_{L'+3}}_{2}$, then it holds that 
\begin{align*}
    \prod\limits_{i=L'+1}^{L'+3} \norm{\Tilde \W_i}_{2} &\leq 2^d d \tau \lambda_1 \sqrt{NS},\\
    \sum\limits_{i=L'+1}^{L'+3}\frac{\norm{\Tilde \W_i^T}_{2,1}^{\frac{2}{3}}}{\norm{\Tilde \W_i}_{2}^{\frac{2}{3}}} &\leq 2^{\frac{2d}{3}}S^{\frac{2}{3}} + N^{\frac{2}{3}}S^{\frac{1}{3}}+\lambda_2^{\frac{2}{3}}.
\end{align*}
\end{theorem}

The theorem now allows application of statistical learning theory results to \glspl{avinn}. Specifically, if the product of the spectral norms of the truncated layers is smaller than the bound for the newly added layers—and the same holds for the corresponding ratios—then the spectral norm of the \gls{avinn} is \emph{provably} lower than that of the base \gls{nn}.

As a corollary, we derive a bound on the generalization error of a \gls{avinn}, leveraging the spectral complexity (\cref{thm:spectral_complexity}) and the margin-based generalization framework of \citet{bartlett2017spectrallynormalizedmarginboundsneural}. The margin measures the gap between the network's output logit for the correct label and the closest output for any incorrect label, i.e., $\Tilde \phi_{L'}(\mathbf{x})_y - \max_{k \neq y} \Tilde \phi_{L'}(\mathbf{x})_k$. 


\begin{corollary}
    Let $\Tilde \phi_{L'}$ be a \gls{avinn} of width $\omega$ and let $R_{\Tilde{\phi}_{L'}}$ be given as in \cref{thm:spectral_complexity}. Then, for $(x, y), (x_1, y_1), \ldots, (x_m, y_m)$ drawn i.i.d. from any probability distribution over $\mathbb{R}^d \times [K]$, we have with probability at least $1 - \delta$ over $((x_i, y_i))_{i=1}^m$, every margin $\gamma > 0$ satisfies
    \begin{align*}
    &\mathbb P\left[ \arg\max_k\Tilde{\phi}_{L'}(x)_k \neq y\right] \\
    &\leq \mathcal L_\gamma(\Tilde{\phi}_{L'}) + O\left(\frac{\norm{X}_{2,2} R_{\Tilde{\phi}_{L'}}}{\gamma \sqrt{m}}\ln(\omega) + \sqrt{\frac{\ln(1/\delta)}{m}}\right),
    \end{align*}
    where
    \[
    \mathcal L_\gamma(\Tilde \phi_{L'}) = \frac{1}{m} \sum_{i=1}^m \mathbb{I}\left[\Tilde \phi_{L'}(x_i)_{y_i} - \max_{k \neq y_i} \Tilde \phi_{L'}(x_i)_k \leq \gamma\right],
    \]
    $\mathbb{I}$ the indicator function, and $X \coloneqq (x_1, \dots, x_m) \in \mathbb{R}^{d \times m}$.
\end{corollary}




\section{Numerical Experiments}
\label{sec:experiments}

We assess the capability of \gls{avinn} to replace entire convolutional and linear layers in standard CNNs, aiming to maintain or enhance accuracy while improving parameter efficiency. Our experiments use a pretrained baseline neural network, leveraging only its latent outputs, and apply approximate vanishing ideal computations to these features. The primary goal is to determine if the network's final layers can be replaced with a polynomial layer from vanishing ideal generators, without retraining the entire model.

Specifically, we \textbf{(i)} demonstrate that \gls{avinn} achieves competitive accuracy, and \textbf{(ii)} examine the impact of network truncation and polynomial pruning on accuracy, parameter efficiency, and throughput (measured in images per second). Inspired by recent findings that deeper layers in large language models are often less effective than earlier ones \citep{gromov2024unreasonableineffectivenessdeeperlayers}, we intend to replace deeper layers with a polynomial layer for a more compact yet equally expressive model.


\subsection{General Experimental Setup}
All experiments use PyTorch \citep{paszke2019pytorchimperativestylehighperformance}. We evaluate classification on CIFAR-10/-100~\citep{Krizhevsky09learningmultiple} with ResNet models~\citep{he2015deep}. \gls{avinn} replaces the final layers of a pretrained network with polynomial generators (Algorithm~\ref{alg:pipeline}), where we use the following parameters: vanishing tolerance $\psi=0.1$ (cf. \cref{{def:approx_vanishing}}), maximal polynomial degree $d=5$, and feature reduction to 128 dimensions using PCA. Fine-tuning of the linear classifier and coefficient matrix uses SGD (learning rate $0.05$, momentum $0.9$) with standard data augmentations.

\subsection{Impact of Truncation}
We first analyze the impact of removing varying fractions of the final convolutional layers on accuracy and the number of constructed generators for CIFAR-100, with CIFAR-10 results deferred to \cref{exp:truncation_cifar10}. We compare three setups, all truncating the network at the same layer but differing in the layers they append and retrain: \textbf{i)} \gls{avinn}, which appends the polynomial layer followed by a linear classifier, \textbf{ii)} \emph{Linear Head}, which appends only a linear classifier to the truncated network, and \textbf{iii)} \emph{Random Monomials}, which appends a polynomial layer with the same number of monomials and an identically shaped coefficient matrix as \gls{avinn}, but with randomly sampled monomials instead of those found by the vanishing ideal algorithms.



\paragraph{The effect of truncation on performance.}
Figure~\ref{fig:impact_truncation_c100} shows how truncating ResNet-34 layers affects CIFAR-100 test accuracy. Removing layers and adding only a linear layer reduces network performance due to non-linearly separable latent features. In contrast, \gls{avinn} maintains competitive accuracy even after removing half of the layers. It consistently outperforms the random monomials layer, highlighting that \gls{avinn}'s strength lies not just in the non-linearity of the monomials but in their discriminative power when determined by the vanishing ideal algorithms. At the start of the residual blocks, \gls{avinn} and the monomials layer show similar accuracy, likely because more monomials are generated in this region (cf. \cref{exp:pruning_monomial_count}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{Figures/clean/truncation_vs_accuracy_cifar100_random_clean.pdf}
    \caption{{CIFAR-100 (ResNet-34):} Test accuracy vs.\ percentage of removed convolutional layers. As more layers are removed, a classifier with only a linear head after truncation experiences a sharp performance drop, whereas \gls{avinn} maintains accuracy.}
    \label{fig:impact_truncation_c100}
\end{figure}


\paragraph{The effect of truncation on the generators.}
Table~\ref{tab:lay} shows the number of monomials and polynomials \gls{abm} constructed after removing different fractions of layers in ResNet-18 and ResNet-34. Truncating earlier layers increases the \emph{intrinsic dimensionality}~\citep{ansuini_intrinsic_2019} of data manifolds, resulting in more monomial terms (cf. \cref{exp:intrinisic_dimension}). Despite significant truncation, the total number of generated terms remains well below the theoretical limit for degree-5 monomials in 128 dimensions, indicating that the constructed generators remain sparse.


\begin{table}[h]
\caption{Elapsed time, number of polynomials and monomials generated by the vanishing ideal algorithm vs.\ percentage of removed Layers: We compare the effect of removing different fractions of the final layers on the vanishing ideal algorithm.}
\vspace{0.1in}
\centering
\resizebox{\linewidth}{!}{%

\begin{tabular}{c cc c}
\toprule
\multicolumn{4}{c}{\textbf{CIFAR-10 (ResNet-18)}} \\
\midrule
\textbf{\% of Layers Removed} &  \textbf{Monomials} &  \textbf{Polynomials} & \textbf{Time (s)} \\
\midrule
64.5\% & 8,386 & 53,132 & 6,468.36 \\
53.3\% & 4,462 & 14,297 & 245.09 \\
46.7\% & 1561   & 4,346  & 11.73 \\
26.7\% & 167   & 1,319  & 7.31 \\
6.7\%  & 130   & 1,280  & 6.94 \\

\midrule

\multicolumn{4}{c}{\textbf{CIFAR-100 (ResNet-34)}} \\
\midrule
\textbf{\% of Layers Removed} &  \textbf{Monomials} & \textbf{Polynomials} & \textbf{Time (s)} \\
\midrule
75.8\% & 7,625 & 188,743 & 3368.76 \\
50.1\% & 5,196 & 53,675 & 82.46 \\
43.9\% & 1,990 & 30,004 & 18.38 \\
25.1\% & 196 & 12,978 & 7.46\\
7.1\% & 237 & 12,997 & 8.56 \\
\bottomrule
\end{tabular}%
}

\label{tab:lay}
\end{table}

 



\subsection{Pruning the generators}
\label{sec:pruning}
Figure~\ref{fig:impact_pruning_c100} demonstrates that pruning a large fraction of polynomials can significantly degrade accuracy, especially in early layers. On the other hand, moderate pruning largely preserves performance. Figure~\ref{fig:impact_pruning_c100_monomials} further confirms the validity of the approach outlined in Figure~\ref{fig:pruning}: pruning polynomials suffices to reduce the number of monomials significantly.



\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{Figures/clean/3d_cifar100_abm_4_3.pdf}
    \caption{{CIFAR-100 (ResNet-34):} Test accuracy vs.\ percentage of removed layers and the pruning fraction of polynomials. We compare the performance of different pruning ratios for removing different fractions of layers.}
    \label{fig:impact_pruning_c100}
\end{figure}



\subsection{The parameter-efficiency of \glspl{avinn}}
We show that \gls{avinn} achieves parameter efficiency by pruning up to a moderate number of polynomials. We create four variants by truncating increasing portions of the trailing layers, substituting them with the polynomial expansions: \text{VI-Net–Tiny} (retains roughly 30\% of base layers), \text{VI-Net–Small} (roughly 50\%), \text{VI-Net–Medium} (roughly 80\%), and \text{VI-Net–Large} (removes only the last layer). Table~\ref{tab:layer_performance} shows that \glspl{avinn} often achieve both higher throughput and competitive performance.

\begin{table}[h]
\caption{Performance Comparison for \gls{avinn}. Total Parameters is the sum of Baseline Parameters (truncated ResNet) and Polynomial Parameters (generated expansions), measured in millions (M). Throughput is measured on the test split (batch size 256), averaged across 5 random seeds with standard deviation indicated.}
\vspace{0.1in}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l ccc cc}
\toprule
\multicolumn{6}{c}{\textbf{CIFAR-10 (ResNet-18)}} \\
\midrule
\textbf{Model} & \textbf{Total} & \textbf{Base} & \textbf{Poly} & \textbf{Acc (\%)} & \textbf{Throughput (img/s)} \\
\midrule
VI-Net18-Tiny   & 1.86M & 0.68M & 1.18M & 88.62 & 100798 $\pm$ 20506 \\
VI-Net18-Small  & 2.84M & 2.18M & 0.66M & 92.66 & 79307  $\pm$ 15576 \\
VI-Net18-Medium & 4.35M & 3.96M & 0.39M & 92.92 & 71851  $\pm$ 13987 \\
VI-Net18-Large  & 9.20M & 8.81M & 0.39M & 93.02 & 62086  $\pm$ 11291 \\
\cmidrule{1-6}
ResNet18        & 11.24M & 11.24M & -    & 92.89 & 66533  $\pm$ 12577 \\

\midrule

\multicolumn{6}{c}{\textbf{CIFAR-100 (ResNet-34)}} \\
\midrule
\textbf{Model} & \textbf{Total} & \textbf{Base} & \textbf{Poly} & \textbf{Acc (\%)} & \textbf{Throughput (img/s)} \\
\midrule
VI-Net34-Tiny   & 3.52M  & 2.85M  & 0.67M & 71.94 & 42064  $\pm$ 7247 \\
VI-Net34-Small  & 5.88M  & 5.21M  & 0.67M & 74.03 & 37611  $\pm$ 6502 \\
VI-Net34-Medium & 14.60M & 14.20M & 0.40M & 74.66 & 29285  $\pm$ 5791 \\
VI-Net34-Large  & 19.32M & 18.92M & 0.40M & 74.78 & 27862  $\pm$ 5358 \\
\cmidrule{1-6}
ResNet34        & 20.35M & 20.35M & -    & 74.75 & 28253  $\pm$ 4290 \\
\bottomrule
\end{tabular}%
}
\label{tab:layer_performance}
\end{table}



\section{Conclusion and Limitations}
\label{sec:conclusion}
We establish a connection between manifold learning and computational algebra, demonstrating how vanishing ideal algorithms can be used to accurately discriminate different classes in the latent spaces of neural networks, despite said spaces being of much higher dimensionality than what is typically considered in the context of vanishing ideal algorithms. By proposing \gls{avinn}, we showed that the found generators of the respective vanishing ideals can be used to construct a polynomial layer that, when appended to a the truncated smaller network, can match the performance of the full network with higher parameter-efficiency.

However, limitations remain. If the latent-space class manifolds overlap significantly or have high intrinsic dimensionality, the polynomials may fail to capture them accurately. Exploring higher-dimensional latent spaces, such as those in transformer models, could be a promising direction for future work.

\newpage

\section*{Acknowledgements}
This research was partially supported by the DFG Cluster of Excellence MATH+ (EXC-2046/1, project id 390685689) funded by the Deutsche Forschungsgemeinschaft (DFG) as well as by the German Federal Ministry of Education and Research (fund number 01IS23025B).




    \bibliography{bibliography}
    \bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage
    \appendix
    \onecolumn

    \section{Appendix: Additional details, experiments and figures}
   \subsection{Implementation Details}
For the parameter-efficient \gls{avinn} implementation, we prune to a fixed number of polynomials and monomial terms. Each variant includes a PCA layer reducing to 128 components, encoded monomials ({512} $\times$ {128}), polynomial coefficients ({1280} $\times$ {512} or {12800} $\times$ {512}), and a final linear layer.

\begin{table}[h]
    \centering
    \caption{Name of the layer at which we truncate the baseline NN.}
    \label{tab:truncation}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Variant} & \textbf{ResNet18 (CIFAR10)} & \textbf{ResNet34 (CIFAR100)} \\
        \midrule
        \textbf{Tiny}   & \texttt{layer2.1.bn2}  & \texttt{layer3.1.bn1}  \\
        \textbf{Small}  & \texttt{layer3.1.bn1}  & \texttt{layer3.3.bn1}  \\
        \textbf{Medium} & \texttt{layer4.0.bn1}  & \texttt{layer4.1.bn1}  \\
        \textbf{Large}  & \texttt{layer4.1.bn1}  & \texttt{layer4.2.bn1}  \\
        \bottomrule
    \end{tabular}
\end{table}

All ResNet18-based variants use {512} monomials and {1280} polynomials, while ResNet34-based variants use {512} monomials and {12800} polynomials.

    \newpage
    \subsection{Additional Experiments}
    We list additional experiments.
    \subsubsection{Impact of Truncation on CIFAR-10}\label{exp:truncation_cifar10}
    We apply truncations to a standard ResNet-18, retaining 512 training images per class for constructing approximate vanishing ideals. We do not prune polynomials in this experiment. Both the linear layer and the coefficients were retrained for 20 epochs.

   \cref{fig:impact_truncation_c10} shows that removing up to $20\%$ of the final convolutional layers does not degrade the performance of the linear head compared to \gls{avinn}, likely because the data is already linearly separable at these layers. Additionally, we observe that a network with randomly generated monomials performs significantly worse than the linear head when removing between $0\%$ and $30\%$ of the final layers. However, as more layers are removed, the non-linearity introduced by the monomials becomes beneficial. Notably, \gls{avinn} maintains higher accuracy and consistently outperforms both the linear head and the randomly sampled monomials.

\begin{figure}[h]
    \centering   
    \includegraphics[width=0.9\linewidth]{Figures/clean/truncation_vs_accuracy_cifar10_final.pdf}
    \caption{{CIFAR-10 (ResNet-18):} Test accuracy vs.\ percentage of removed convolutional layers. The dashed gray line indicates the fully trained baseline. A linear layer retains accuracy only with minimal truncation, whereas \gls{avinn} maintains high accuracy even with substantial truncation.}
    \label{fig:impact_truncation_c10}
\end{figure}
\newpage   

\subsubsection{Pruning and Monomial Count}\label{exp:pruning_monomial_count}
In this experiment, we examine how the number of monomials required to evaluate the generators varies with different pruning ratios. \cref{fig:impact_pruning_c100_monomials} shows that pruning polynomials significantly reduces the number of monomials to be evaluated. Notably, when removing between $20\%$ and $40\%$ of the layers, the number of remaining monomials falls below the latent dimension of 128 (after PCA). This explains the increased susceptibility of these layers to accuracy degradation: excessive pruning of polynomials leads to the loss of too many monomial terms, reducing accuracy. Furthermore, this result suggests that ABM efficiently constructs representations in this range, as it retains relatively few monomial terms without requiring explicit pruning.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/clean/3d_monomial_pruning_final.pdf}
    \caption{CIFAR-100 (ResNet-34): Impact of pruning on monomial count. Removing a large fraction of polynomials reduces the number of remaining monomials, sometimes below the latent dimension of 128. This reduction explains the observed accuracy drop at high pruning ratios, particularly when removing $20\%$ to $50\%$ of the layers (see \cref{fig:impact_pruning_c100}).}
    \label{fig:impact_pruning_c100_monomials}
\end{figure}
\newpage
\subsubsection{Comparison of Different Vanishing Ideal Algorithms}
We construct \gls{avinn} following the pipeline in \cref{alg:pipeline}, using different vanishing ideal algorithms. Specifically, we compare \gls{abm} to \gls{oavi} with two oracle variants: Frank-Wolfe and Accelerated Gradient Descent. The baseline network is a ResNet34, trained and tested on CIFAR-100.

\cref{fig:impact_truncation_c100_comparison} shows that all vanishing ideal algorithms yield similar performance for the resulting \glspl{avinn}, with \gls{abm} slightly outperforming the \gls{oavi} variants in intermediate layers. This similarity suggests that the algorithms capture comparable structural properties of the data. Additionally, since we retrain the coefficients of the constructed generators, the primary difference lies in the selection of monomial terms by each algorithm.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/clean/truncation_vs_accuracy_comparison_cifar100.pdf}
    \caption{{CIFAR-100 (ResNet-34):} Test accuracy vs.\ percentage of removed convolutional layers. The dashed gray line indicates the fully trained baseline. We compare different \glspl{avinn}, constructing polynomials using \gls{abm}, as well as \gls{oavi} with Accelerated Gradient Descent and Frank-Wolfe.}
    \label{fig:impact_truncation_c100_comparison}
\end{figure}



    
    \newpage 
    \subsubsection{Effect of Pruning on CIFAR-10 (ResNet18)}
In this experiment, we examine the effect of polynomial pruning at different fractions on ResNet18 trained on CIFAR-10. \cref{fig:impact_pruning_c10} shows that removing up to $40\%$ of all layers has a negligible impact on accuracy, suggesting that many polynomials are not essential for classification and can be discarded. A significant accuracy drop is observed only at extreme pruning levels ($> 90\%$). In earlier layers, pruning reduces performance more noticeably, likely due to the loss of expressivity, which the polynomial layer would otherwise compensate for.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/clean/3d_cifar10_abm_final.pdf}
    \caption{{CIFAR-10 (ResNet18):} Impact of polynomial pruning on test accuracy. Accuracy remains stable up to moderate pruning levels, but excessive pruning—especially in early layers—leads to a significant drop due to reduced expressivity.}
    \label{fig:impact_pruning_c10}
\end{figure}
\newpage

   \subsubsection{Effect of Pruning on \gls{oavi}-CG Based \gls{avinn}}
We analyze the effect of pruning the generator set for \gls{oavi} using Conditional Gradients (Frank-Wolfe) as the oracle. The results exhibit a similar trend to \cref{fig:impact_pruning_c100}, though the accuracy shows a slightly weaker dependence on the pruning ratio. This is likely due to the higher number of constructed generators, which provides additional redundancy.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/clean/3d_cifar100_cg_final.pdf}
    \caption{{CIFAR-100 (ResNet34):} Impact of polynomial pruning on test accuracy for \gls{oavi}-CG. While the general trend mirrors \cref{fig:impact_pruning_c100}, the higher number of generators reduces sensitivity to pruning.}
    \label{fig:impact_pruning_cg_c100}
\end{figure}
    \newpage

    \subsubsection{Effect of Pruning on \gls{oavi}-AGD Based \gls{avinn}}
We analyze the effect of pruning the generator set for \gls{oavi} using Accelerated Gradient Descent as the oracle. The results exhibit a similar trend to \cref{fig:impact_pruning_c100}, though the accuracy shows a slightly weaker dependence on the pruning ratio. This is likely due to the higher number of constructed generators, which provides additional redundancy.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/clean/3d_cifar100_agd_final.pdf}
    \caption{{CIFAR-100 (ResNet-34):} Impact of polynomial pruning on test accuracy for \gls{oavi}-AGD. The overall trend resembles \cref{fig:impact_pruning_c100}, though the higher number of generators reduces sensitivity to pruning.}
    \label{fig:impact_pruning_agd}
\end{figure}

    \newpage

   \subsubsection{Truncation vs. Number of Generators}
We analyze the number of polynomials and monomials constructed by \gls{abm} when removing different layers of ResNet34 on CIFAR-100. A clear trend emerges: earlier layers produce more monomials and polynomials, increasing computational cost. However, at the start of residual blocks, we observe an unusually high number of constructed generators, suggesting a shift in the algebraic structure of the data at these layers.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/clean/layer_vs_monomials_polynomials.pdf}
    \caption{{CIFAR-100 (ResNet34):} Number of constructed monomials and polynomials across network layers. Earlier layers generate more terms, increasing computation time, while residual block boundaries show an unexpected increase in generators, indicating structural changes in the data.}
    \label{fig:impact_pruning_monomials_extended}
\end{figure}
    
    \newpage

    \subsubsection{Intrinsic Dimension vs. Vanishing Ideal}\label{exp:intrinisic_dimension}
In this experiment, we analyze the intrinsic dimension of the data manifolds at different layers of ResNet34 and its effect on the number of constructed monomials. We estimate the intrinsic dimension using the Levina-Bickel estimator \cite{NIPS2004_74934548} with the MacKay-Ghahramani extension \citep{MacKay2005}. 

\cref{fig:impact_pruning_monomials_id} suggests a strong correlation between intrinsic dimension and the number of constructed monomial terms. This aligns with intuition: a more complex underlying manifold requires a more intricate algebraic variety, necessitating a larger number of monomial terms.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/clean/id_vs_monomials.pdf}
    \caption{{CIFAR-100 (ResNet34):} Correlation between intrinsic dimension and number of monomial terms. Higher intrinsic dimensionality corresponds to a greater number of constructed monomials, reflecting increased algebraic complexity.}
    \label{fig:impact_pruning_monomials_id}
\end{figure}


\newpage
    \section{Appendix: Theoretical Analysis and Learning Guarantees}
    \label{app:technical_details}

    \subsection{Proofs}
    We provide the proof for the theoretical results.
    \subsubsection{$\inex$-activation function}
    The $\inex$ activation function is a theoretical tool that we introduce to encode a \gls{avinn} in the form of \cref{def:spectral_complexity}. For illustration, we compute several monomials.
\begin{example}\label{example:monomial}
    Consider the degree $2$ monomial $xy$. This can be expressed as 
    \[
    xy = \frac{1}{2}\left((x+y)^2 - x^2 - y^2\right),
    \]
    which requires only addition and squaring operations. We generalize this approach to degree $3$ monomials using the inclusion-exclusion principle. For example, the monomial $xyz$ can be computed by considering all subsets of the variables $\{x, y, z\}$ and their contributions to the cube expansion:
    \[
    (x+y+z)^3 = x^3 + y^3 + z^3 + 3x^2y + 3x^2z + 3y^2x + 3y^2z + 3z^2x + 3z^2y + 6xyz.
    \]

    To isolate $xyz$, we subtract terms involving lower-order monomials:
    \[
    xyz = \frac{1}{6} \left((x+y+z)^3 - (x+y)^3 - (x+z)^3 - (y+z)^3 + x^3 + y^3 + z^3\right).
    \]

    This approach also applies to monomials where variables appear with higher powers, such as $x^2y$. To handle such cases, we replace one variable with a duplicate of another. For instance, setting $z = x$ allows us to rewrite $x^2y$ in terms of the cube expansion above. 
\end{example}
The following lemmas formalize \cref{example:monomial}. 
\begin{lemma}[Multiplication with $\inex$-activation]\label{lem:computing_monomials_inex}
    Let $\alpha^d$ be given as in \cref{def:inex}. Let $x_1, \dots, x_d \in \mathbb{R}$ and let $(I_1, \dots, I_{2^d})$ be the tuple of all subsets of $[d]$, ordered by cardinality. Then, for $\mathbf{z}$ with $z_j = \sum_{i \in I_j}x_i$ it holds 
    \begin{align*}
        \alpha^d(\mathbf{z}) = \prod_{i = 1}^d x_i. 
    \end{align*}
\end{lemma}
\begin{proof}
    This is an application of the inclusion-exclusion-principle. Note, that by the multinomial theorem, it holds
    \begin{align*}
        \left(\sum_{i \in I_j}x_i\right)^d = n! \sum_{\substack{\sum\limits_{i \in I_j} k_i= n}}\left(\prod_{i \in I_j}\frac{x_i^{k_i}}{k_i!}\right). 
    \end{align*}
    Therefore a monomial of degree $d$ is in the support of $\left(\sum_{i \in I_j}x_i\right)^d$ if and only if all variable indices are in $I_j$. Further, the coefficient does not depend on the cardinality of $I_j$, which allows to apply the inclusion-exclusion principle to the sum. Thus, after rescaling by $\frac{1}{n!}$ only the term $\prod_{i = 1}^d x_i$ survives. 
\end{proof}
\begin{lemma}[Computing monomials with $\inex$-activation]\label{lem:matrix_monomial}
   Let $x_1, \dots, x_k \in \mathbb{R}$ and $\alpha_i \geq 1$ be given such that $\sum_{i = 1}^{k}\alpha_i = d$. Then, there exists a matrix $\mathbf{W}$ with $k$ columns and $2^d$ rows, such that 
   \begin{align*}
       \alpha^d(\mathbf{W}\mathbf{x}) = \prod_{i = 1}^k x_i^{\alpha_i}. 
   \end{align*}
\end{lemma}
\begin{proof}
    First introduce artificial variables $x_{i,1}, \dots, x_{i, \alpha_i}$, which we all set to $x_i$. Define the set $I = \{(i, q) : i \in [k], q \in [\alpha_i]\}$, which has cardinality $d$, since $\sum_{i}^k\alpha_i = d$. Then, we can rewrite this product as $\prod_{i = 1}^k x_i^{\alpha_i} = \prod_{(i,q) \in I}x_{i,q}$. Let $I_j \subseteq I$ be one of the $2^d$ subsets of $I$ and define $\mathbf{w}_j^T := \sum_{(i,q) \in I_j} e_i$, where $e_i$ denotes the $i$-th basic vector in $k$ dimensions. Since $x_{i, q} = x_{i}$ for all $q \leq \alpha_i$ it holds that $\mathbf{w}^T_j\mathbf{x} = \sum_{(i,q) \in I_j} x_{i,j}$. Therefore, if we order the sets $I_j$ by cardinality, we can apply \cref{lem:computing_monomials_inex} to obtain for $\mathbf{W} = (\mathbf{w}_1, \dots, \mathbf{w}_{2^d})^T$ that $\alpha^d(\mathbf{W}\mathbf{x}) = \prod_{(i,q) \in I}x_{i,q} = \prod_{i = 1}^k x_i^{\alpha_i}$. 
\end{proof}

\begin{lemma}[Lipschitz property of $\inex$-activation]
    Let $\alpha^d$ be given as in \cref{def:inex}. Then, $\alpha^d$ is $d$-Lipschitz on the image of $\mathbf{W}$ for $\mathbf{x} \in [-1,1]^k$ from \cref{lem:matrix_monomial}.
\end{lemma}
\begin{proof}
    From \cref{lem:matrix_monomial}, we know that the mapping $\mathbf{x} \rightarrow \alpha^d(\mathbf{W}(\mathbf{x}))$ is equivalent to the mapping of the corresponding monomial, which is $d$-Lipschitz on $[-1,1]^k$. Next, for each $i \in [k]$, it holds $e_i^T$ is a row in $\mathbf{W}$, therefore for $\mathbf{x}, \mathbf{x}' \in [-1,1]^k$ it holds $\|\mathbf{W}\mathbf{x}-\mathbf{W}\mathbf{x}'\|_2 \geq \|\mathbf{x}-\mathbf{x}'\|_2$. Since arbitrary points in the image of $\mathbf{W}$ can be expressed as $\mathbf{W}\mathbf{x}$ with $\mathbf{x} \in [-1,1]^k$ we obtain 
    \begin{align*}
        \frac{\|\alpha^d(\mathbf{W}\mathbf{x})-\alpha^d(\mathbf{W}\mathbf{x}')\|_2}{\|\mathbf{W}\mathbf{x}-\mathbf{W}\mathbf{x}'\|_2} &\leq \frac{\|\alpha^d(\mathbf{W}\mathbf{x})-\alpha^d(\mathbf{W}\mathbf{x}')\|_2}{\|\mathbf{x}-\mathbf{x}'\|_2}\\
        &\leq \frac{d\|\mathbf{x}-\mathbf{x}'\|_2}{\|\mathbf{x}-\mathbf{x}'\|_2} \\
        &= d. 
    \end{align*}
    Here, the second inequality is due to the Lipschitz property of the map $\alpha^d(\mathbf{W}(\cdot))$. 
\end{proof}
\subsubsection{Encoding a \gls{avinn} as \cref{def:spectral_complexity}}
By the previous section, we know that we can compute monomials employing the $\inex$ activation function and a matrix $\mathbf{W}_M$. 
\begin{lemma}
Assume $G$ is given as in \cref{assump:G}. Then, $G$ can be encoded as the two layer \gls{nn} 
\begin{align*}
    \mathbf{W}_C(\sigma(\mathbf{W}_M(\mathbf{x}))),
\end{align*}
where $\mathbf{W}_M \in \mathbb{R}^{s \times n}$, and we stack $\inex$-activation functions such that $\sigma = (\alpha^{d_1}, \dots, \alpha^{d_S}): \mathbb{R}^s \rightarrow \mathbb{R}^S$ 
and $\mathbf{W}_C \in \mathbb{R}^{N \times S}$ such that $s \in \mathcal{O}(S)$. 
\end{lemma}
\begin{proof}
    Construct the matrix $\mathbf{W}_M$ by employing the matrices $\mathbf{W}$ from \cref{lem:matrix_monomial} for each monomial to compute and stacking the respective $\inex$ activation functions, such that $\sigma = (\alpha^{d_1}, \dots, \alpha^{d_S})$. Thus, the $i$-th component of the vector $\sigma(\mathbf{W}_M \mathbf{x})$ will be the $i$-th monomial of the $S$ polynomials we have to compute to evaluate $G$. Finally, we construct $\mathbf{W}_C$ by choosing the rows as the coefficient vectors of the polynomials in $G$. This concludes the proof. 
\end{proof}

\subsubsection{Spectral Properties of \gls{avinn} encoding}
In this section, we obtain bounds for the norm of the matrices that are used to encode a \gls{avinn} in the format of \cref{def:spectral_complexity}. We begin my analysing the properties of $\mathbf{W}_M$
\begin{lemma}[Spectral properties of $\mathbf{W}_M$]\label{lem:bound_Wm}
    Let \cref{assump:G} hold, and let $\mathbf{W}_M$ be constructed as in \cref{lem:computing_monomials_inex}. Then, it holds that:
    \begin{align*}
    \|\mathbf{W}_M\|_2 \leq 2^\frac{d}{2}d\sqrt{S} \quad  \text{and} \quad \frac{\|\mathbf{W}_M^T\|_{2,1}}{\|\mathbf{W}_M\|_2} \leq 2^dS,
        \end{align*}
    where \(S\) is the number of monomials, $n$ is the number of variables and $d$ the highest degree of any one monomial.
\end{lemma}

\begin{proof}
We bound the Frobenius norm of the matrix $\mathbf{W}_M$. By construction, $\mathbf{W}_M$ has at most $S\cdot 2^d$ rows. For each row, the squared euclidean norm is at most $d^2$. Therefore, it holds

    \begin{align}
        \|\mathbf{W}_M\|_F &\leq \sqrt{S 2^d \cdot d^2} = 2^\frac{d}{2}d\sqrt{S}.
    \end{align}
    As $\|\mathbf{W}_M\|_2 \leq \|\mathbf{W}_M\|_F$, it follows that
    \begin{align}
        \|\mathbf{W}_M\|_2 \leq 2^\frac{d}{2}d\sqrt{S}.
    \end{align}

    Next, for the \(\ell_{2,1}\) norm, recall that
    \begin{align}
        \|\mathbf{W}^T_M\|_{2,1} &= \sum_{i=1}^m \|\mathbf{W}^T_{M,i}\|_2,
    \end{align}
    where \(\mathbf{W}^T_{M,i}\) is the \(i\)-th column of \(\mathbf{W}^T_M\), i.e., the $i$-th row of $\mathbf{W}_M$. Let $\mathbf{w}$ denote the row with the largest euclidean norm in $\mathbf{W}_M$. This row is part of a block required for computing a specific degree $d$ monomial $\prod_{i = 1}^k x_i^{\alpha_i}$ as in \cref{lem:matrix_monomial}. Let $j$ denote the index with the largest entry in $\mathbf{w}$, i.e., $j \in \argmax_{i \in [k]}\alpha_i$. For each of the other variable $i'$ there is a row of the form $\alpha_{i'}e_{i'}+\alpha_je_j$. Thus, there are $k-1$ such rows, i.e, in the $j$-th column of $\mathbf{W}$, the term $\alpha_j$ occurs $k-1$ times, one for each of the remaining variables. By our choice of $j$ it holds 
    \begin{align*}
        \sum_{i = 1}^k \alpha_i^2 \leq (k-1)\alpha_j^2 + \alpha_j^2 \leq \|\mathbf{W}_{M, j}\|^2_2. 
    \end{align*}
    Therefore, it holds that the largest euclidean norm of any row is bounded by the largest euclidean norm of any column. Note that the spectral norm of a matrix is bounded from below by the column with the largest euclidean norm,i.e., 
    \begin{align}
        \|\mathbf{W}_M\|_2 &\geq \max_j \|\mathbf{W}_{M, :,j}\|_2. 
    \end{align}
    Thus, we conclude that $\|\mathbf{W}_{M}\|_{2,1} \leq 2^dS\|\mathbf{W}_{M}\|_{2}$ and therefore $\frac{\|\mathbf{W}_{M}\|_{2,1}}{\|\mathbf{W}_{M}\|_{2}} \leq 2^dS$. 
\end{proof}
\begin{lemma}[Norm properties of $\mathbf{W}_C$]\label{lem:bound_Wc}
    Let $\mathbf{W}_C \in \mathbb{R}^{N \times S}$ have rows bounded in the $\ell_1$ norm by $\tau$. Then, it holds 
    \begin{align*}
        \|\mathbf{W}_C\|_{2} \leq \sqrt{N}\tau \quad \text{and} \quad \frac{\|\mathbf{W}_C^T\|_{2,1}}{\|\mathbf{W}_C\|_{2}} \leq N\sqrt{S},   
    \end{align*}
    where $S$ is the number of monomials and $N$ is the number of polynmials. 
\end{lemma}

\begin{proof}
We have
    \begin{align}
        \|\mathbf{W}_C\|_2 &\leq \sqrt{N}\|\mathbf{W}_C\|_{\infty} \\
        &\leq \sqrt{N}\tau. 
    \end{align}
    For the ratio, it holds
    \begin{align*}
        \frac{1}{\sqrt{S}}\|\mathbf{W}_C\|_{\infty} \leq\|\mathbf{W}_C\|_{2}. 
    \end{align*}
    For any vector, its euclidean norm is always bounded by the $\ell_1$-norm. Therefore, it holds
    \begin{align*}
        N\|\mathbf{W}_C\|_{\infty} \geq\|\mathbf{W}_C^T\|_{2,1}.
    \end{align*}
    Combining these results, we obtain
    \begin{align*}
        \frac{\|\mathbf{W}_C^T\|_{2,1}}{\|\mathbf{W}_C\|_{2}} &\leq \frac{N\|\mathbf{W}_C\|_{\infty} }{\frac{1}{\sqrt{S}}\|\mathbf{W}_C\|_{\infty} }\\
        &= N\sqrt{S}
    \end{align*}
\end{proof}
\subsubsection{Proof of \cref{thm:spectral_complexity}}
Finally, we are ready to prove \cref{thm:spectral_complexity}. 
\begin{theorem}%\label{thm:spectral_complexity}
    Let $\phi$ be an \gls{nn} with $L$ layers in the form \cref{def:spectral_complexity}. Let $G$ be a polynomial layer as in \cref{polylayer} satisfying \cref{assump:G}, and $\Tilde{\phi}$ the corresponding \gls{avinn} based on the truncated \gls{nn} $\hat{\phi}$ with $L'$ layers. Then, the spectral complexity of $\Tilde{\phi}$ can be bounded as follows
\begin{align*}
    R_{\Tilde{\phi}} = \frac{d\prod\limits_{i \in \{M, C, F\}} \|\mathbf{W}_i\|_{2}}{\prod\limits_{i = L' + 1}^{L} \rho_i \|\mathbf{W}_i\|_{2}} 
    \left(
    \frac{
    \sum\limits_{\substack{i = 1 \\ i \in \{M, C, F\}}}^{L'} \frac{\|\mathbf{W}_i^T\|_{2,1}^{\frac{2}{3}}}{\|\mathbf{W}_i\|_{2}^{\frac{2}{3}}} 
    }{
    \sum\limits_{i = 1}^L \frac{\|\mathbf{W}_i^T\|_{2,1}^{\frac{2}{3}}}{\|\mathbf{W}_i\|_{2}^{\frac{2}{3}}}
    }
    \right)^{\frac{3}{2}} R_{\phi}.
\end{align*}
Further, assume the linear layer satisfies $\|\mathbf{W}_{F}\|_{2} \leq \lambda_1$ and $\frac{\|\mathbf{W}_{F}\|_{2,1}}{\|\mathbf{W}_{F}\|_{2}} \leq \lambda_2$. Then, it holds 
\begin{align*}
    \prod\limits_{i \in \{M, C, F\}} \|\mathbf{W}_i\|_{2} &\leq 2^d d \tau \lambda_1 \sqrt{NS},\\
    \sum\limits_{\substack{i \in \{M, C, F\}}}\frac{\|\mathbf{W}_i^T\|_{2,1}^{\frac{2}{3}}}{\|\mathbf{W}_i\|_{2}^{\frac{2}{3}}} &\leq 2^{\frac{2d}{3}}S^{\frac{2}{3}} + N^{\frac{2}{3}}S^{\frac{1}{3}}+\lambda_2^{\frac{2}{3}}.
\end{align*}
\end{theorem}
\begin{proof}
    We employ the absolute value function in $G$ as an activation function, which is $1$-Lipschitz and the $\inex$ activation function, which is $d$-Lipschitz on its domain, while the last linear layer has no activation function. Using our encoding from \cref{lem:vinet_as_nn}, we get that the spectral complexity of this encoding is given by 
    \begin{align*}
    R_{\Tilde{\phi}} &= \left(d\prod_{i \in \{M, C, F\}}\|\mathbf{W}_i\|_{2}\right)\left( \prod_{i=1}^{L'} \rho_i \|\mathbf{W}_i\|_{2} \right) \\
    &\cdot \left( \sum_{\substack{i\in\{1, \dots, L'\} \\ i \in  \{M, C, F\}}}\frac{\|\mathbf{W}_i^T\|_{2,1}^{2/3}}{\|\mathbf{W}_i\|_2^{2/3}}\right)^{3/2}.
\end{align*}
The spectral complexity of the baseline \gls{nn} is given by the definition as
\begin{align*}
    R_{\phi} = \left( \prod_{i=1}^L \rho_i \|\mathbf{W}_i\|_{2} \right) \left( \sum_{i=1}^L \frac{\|\mathbf{W}_i^T\|_{2,1}^{2/3}}{\|\mathbf{W}_i\|_2^{2/3}} \right)^{3/2}.
\end{align*}
Therefore it holds 
\begin{align*}
    \frac{R_{\Tilde{\phi}}}{R_{\phi} } = \frac{d\prod\limits_{i \in \{M, C, F\}} \|\mathbf{W}_i\|_{2}}{\prod\limits_{i = L' + 1}^{L} \rho_i \|\mathbf{W}_i\|_{2}} 
    \left(
    \frac{
    \sum\limits_{\substack{i = 1 \\ i \in \{M, C, F\}}}^{L'} \frac{\|\mathbf{W}_i^T\|_{2,1}^{\frac{2}{3}}}{\|\mathbf{W}_i\|_{2}^{\frac{2}{3}}} 
    }{
    \sum\limits_{i = 1}^L \frac{\|\mathbf{W}_i^T\|_{2,1}^{\frac{2}{3}}}{\|\mathbf{W}_i\|_{2}^{\frac{2}{3}}}
    }
    \right)^{\frac{3}{2}}.
\end{align*}
Multiplication by $R_{\phi}$ yields the first result.\\
For the second part, we apply \cref{lem:bound_Wm} and \cref{lem:bound_Wc} and obtain 
\begin{align*}
    \prod\limits_{i \in \{M, C, F\}} \|\mathbf{W}_i\|_{2} &\leq 2^{\frac{d}{2}}d\sqrt{S} \cdot \sqrt{N} \tau \cdot \lambda_1. 
\end{align*}
Reordering yields the result. For the sum, we proceed the same way. 
\end{proof}
\newpage
\section{Vanishing Ideal Algorithms}\label{sec:vanishing_ideal_algos}
In this section, we provide an overview of the vanishing ideal algorithms considered in our setting: \gls{abm} and \gls{oavi}. Both algorithms take a set of points and construct generators of the (approximate) vanishing ideal. They extend noise-free generator construction algorithms, which require exact vanishing, by allowing polynomials to vanish up to a tolerance $\psi$. Structurally, they follow the same principles as exact algorithms but introduce robustness to noisy data.

Beyond returning a set of generators, denoted as $\mathcal{G}$ in this section, these algorithms also provide a set $\mathcal{O}$, known as the \emph{order ideal}. An order ideal is a subset of monomial terms that is closed under division; that is, if a monomial such as $x^2y$ belongs to $\mathcal{O}$, then all its divisors (e.g., $1, x, y, x^2$) must also be in $\mathcal{O}$. A fundamental concept related to order ideals is the \emph{border}, which captures monomials that are one step outside the order ideal.

\begin{definition}[Border]\label{def:border}
    Let $\mathcal{O} \subseteq \mathcal{T}$ be an order ideal. Then, the \emph{d-border} of $\mathcal{O}$ is defined as:
    \begin{align*}
        \partial_{d}\mathcal{O} := \{u \in \mathcal{T}_{d} \mid t \in \mathcal{O}_{\leq d}, t \neq u, \text{ and } t \mid u\}. 
    \end{align*}
\end{definition}

A fundamental result from computational algebra states that every zero-dimensional ideal (such as the vanishing ideal of a finite set of points) admits a generating set known as a \emph{border basis}. A border basis provides a structured way to represent the ideal while ensuring computational efficiency in polynomial interpolation and approximation.

\begin{definition}[Border Basis]\label{def:border_basis}
    Let $\mathcal{G} = \{g_1, \dots, g_m\}$ be a set of generators for a zero-dimensional ideal $I$, and let $\mathcal{O}$ be an order ideal. Then, $\mathcal{G}$ is a \emph{border basis} of $I$ if:
    \begin{enumerate}
        \item $\mathcal{G}$ consists of polynomials of the form
        \begin{align*}
            g_u = u - \sum_{t \in \mathcal{O}} c_{u,t} t, \quad u \in \partial \mathcal{O},
        \end{align*}
        where each $g_u$ is a polynomial whose leading term belongs to the border $\partial \mathcal{O}$.
        \item The polynomials in $\mathcal{G}$ form a basis for the ideal $I$.
    \end{enumerate}
\end{definition}

Intuitively, a border basis expresses each polynomial in the vanishing ideal in terms of monomials from the order ideal, ensuring that computations remain well-structured and stable. For a more detailed overview and the algebraic background we refer to \citet{KEHREIN2006279}.\\\\
For this section, note that we think of sample of points $\mathbf{X}$ as a tuple, rather than a set, as we require that the order of points is fixed. Then, for a given monomial $u \in \mathcal{T}$ we write the \emph{evaluation vector} $(u(\mathbf{x}_1), \dots, u(\mathbf{x}_m))$ as $u(\mathbf{X})$. For a set of monomials, such as $\mathcal{O}$, we write the evaluation matrix $\mathcal{O}(\mathbf{X})$, where the entry indexed by $i$ and $j$ corresponds to the $i$-th monomial in $\mathcal{O}$ evaluated at the $j$-th point. 
\subsection{Approximate Buchberger Möller Algorithm}
The Approximate Buchberger-Möller Algorithm (ABM) constructs an approximate border basis, as described in \citep{limbeck_computation_2014}. Given a term $u_i$ in the border, ABM computes the smallest eigenvalue and corresponding eigenvector of the matrix $A$, which contains the evaluations of all monomials at every point in the dataset. This process identifies the best-fitting polynomial relations while ensuring numerical stability. For a detailed explanation, see \citet{limbeck_computation_2014}.
\begin{algorithm}[H]
\caption{Approximate Buchberger Möller Algorithm (ABM) \citep{limbeck_computation_2014}}\label{alg:abm}
\textbf{Input:} Dataset $\mathbf{X}$, a vanishing parameter $\psi \geq 0$ \\
\textbf{Output:} An approximate $\mathcal{O}$-border basis
\hrule
\begin{algorithmic}[1]
\STATE $\mathcal{O} := \{1\}, \mathcal{G} := \emptyset, d := 1,  M := (1, \dots, 1)^\top \in \mathbb{R}^{s,1}$
\WHILE{$\partial_d \mathcal{O} = \{u_1, \dots, u_l\} \neq \emptyset$}
\FOR{$i = 1$ to $l$}
    \STATE $A := (u_i({\mathbf{X}}) , M)$
    \STATE $B := A^T A$
    \STATE $\lambda :=$ smallest eigenvalue of $B$
    \IF{$\sqrt{\lambda} \geq \epsilon$}
        \STATE $m := |\mathcal{O}|$
        \STATE $\mathbf{s} := (s_{m+1}, s_m, \dots, s_1) :=$ the norm-one eigenvector of $B$ w.r.t. $\lambda$
        \STATE // Assume that $\mathcal{O} = [o_m, \dots, o_1]$
        \STATE $g := s_{m+1}t_i + s_m o_m + \dots + s_1 o_1$
        \STATE $\mathcal{G} \leftarrow \mathcal{G} \cup \{ g \}$
    \ELSE
        \STATE $\mathcal{O} \leftarrow \mathcal{O} \cup \{u_i\}$
        \STATE $M := A$
    \ENDIF
\ENDFOR
\STATE $d := d + 1$
\ENDWHILE
\STATE \textbf{return} $(\mathcal{G}, \mathcal{O})$
\end{algorithmic}
\end{algorithm}



\subsection{Oracle Approximate Vanishing Ideal Algorithm}
The Oracle Approximate Vanishing Ideal (OAVI) algorithm, introduced by \citet{wirth_conditional_2024}, incorporates a constrained convex optimization approach. Instead of computing the eigenvector corresponding to the smallest eigenvalue of $A^T A$, OAVI solves a constrained convex optimization problem at line 5 to determine the coefficient vector $\mathbf{c}$. This requires a convex optimization oracle and a constraint region, chosen as the $\ell_1$-ball of radius $\tau$. Like ABM, OAVI maintains a record of the evaluated monomials in the order ideal $\mathcal{O}$ within the matrix $A$. In \citet{wirth_conditional_2024} the oracle was (among others) choosen as Conditional Gradient, which is known for constructing sparse solutions. 
\begin{algorithm}[H]
\caption{Oracle Approximate Vanishing Ideal algorithm (OAVI) \citep{wirth_conditional_2024}}\label{alg:oavi}
\textbf{Input:} Dataset $\mathbf{X}$, vanishing parameter $\psi \geq 0$, coefficient bound $\tau \geq 2$\\
\textbf{Output:} The set of generators $\mathcal{G} \subseteq \mathcal{P}$ and the set $\mathcal{O} \subseteq \mathcal{T}$
\hrule-
\begin{algorithmic}[1]
\STATE $\mathcal{O} := \{1\}, \mathcal{G} := \emptyset, d := 1$
\WHILE{$\partial_d \mathcal{O} = \{u_1, \dots, u_l\} \neq \emptyset$}
\FOR{$i = 1, \dots, l$}
\STATE $\ell \leftarrow |\mathcal{O}|, A \leftarrow \mathcal{O}(\mathbf{X}), \mathbf{b} = u_i(\mathbf{X}), P = \{\mathbf{y} \in \mathbb{R}^{\ell}:\|\mathbf{y}\|_1 \leq \tau - 1\}$    
\STATE $\mathbf{c} \in \text{argmin}_{\mathbf{y}\in P}\frac{1}{m}\|\mathbf{Ay}+\mathbf{b}\|_2^2$
\STATE $g \leftarrow u_i + \sum_{j = 1}^{\ell}c_j t_j$
\IF{$\text{MSE}(g, \mathbf{X}) \leq \psi$}
\STATE $\mathcal{G} \leftarrow \mathcal{G} \cup \{g\}$ 
\ELSE
\STATE $\mathcal{O} \leftarrow \mathcal{O} \cup \{u_i\}$ 
\ENDIF
\ENDFOR
\STATE $d \leftarrow d + 1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Stochastic Versions of ABM and OAVI}
In our experiments, a key bottleneck of the vanishing ideal algorithms was \emph{memory}, primarily due to the matrix $A$ used in both methods. Given a dataset $\mathbf{X}$ with $m$ samples, the matrix $A$ has dimensions $m \times |\mathcal{O}|$, making storage and computation costly for large $m$. 

To mitigate this, we employ a stochastic approach by subsampling the dataset, using only $m' \ll m$ samples to construct $A$. This significantly reduces memory usage while maintaining approximation quality. Additionally, we leverage lower precision arithmetic (e.g., float16) to further reduce the memory footprint without compromising stability.



\subsection{Addressing Data-Sensitivity of Vanishing Ideal Algorithms}
\label{sec:tanh-rescaling}

Vanishing ideal algorithms are highly sensitive to the scale of the data. Recall that $\psi > 0$ specifies the extent to which polynomials are required to vanish on the training data (cf. \cref{def:approx_vanishing}). This parameter depends on the scale of the data. Further, vanishing ideal algorithms typically require the data to be contained in $[-1, 1]^n$. Thus, we have to ensure that both the training data and, during inference, the input of the polynomial layer is properly scaled. To that end, we propose to employ a variant of tanh-rescaling \citep{hampel2011robust}. Given the training dataset, we collect $\mu$ and $\sigma$ as the mean and standard deviation of the hidden activations at truncation, respectively. These can be easily calculated just by using forward passes and are collected similarly to the running averages in batch normalization. Then, a tanh-rescaling layer is applied right after truncation, mapping $z$ as 
\begin{align*}
    z \mapsto \tanh\left(\frac{z - \mu}{\sigma}\right).
\end{align*}
This mapping ensures that the data is scaled to a reasonable range.



    



\end{document}


