

\section{Introduction}\label{sec_introduction}

Wireless networks, \eg WiFi and 5G, support both communication and sensing~\cite{ma2019wifi, abedi2020witag, okubo2024integrated}, but commonly encounter issues such as unreliable communication and insufficient training data. 
Reliable communication requires network planning to optimize transceiver placement using received signal data at various locations~\cite{zhu2021network, ahamed20215g, zelaya2021lava, krijestorac2021spatial}.  
Similarly, sensing applications~\cite{ding2024exploring, ding2024multi}, such as fingerprinting-based localization~\cite{ayyalasomayajula2020deep, lu2023millimeter, yang2024orchloc}, rely on received signal data to train machine learning~(ML) models.
Site surveys~\cite{kar2018site}, typically conducted to collect such data, are labor-intensive and time-consuming~\cite{site_survey_cisco}.


An alternative is RF signal spatial propagation modeling, which computes the received signal at a fixed receiver location given a transmitter emitting RF signals from any position in a scene.
However, accurate modeling is challenging due to complex interactions between RF signals and objects like reflection, diffraction, and scattering~\cite{1451581, na2022huygens, maxwell1873treatise}.
Existing methods include simulations~\cite{wirelessinsite_web, orekondy2022winert, RayTracingToolbox}, empirical models~\cite{rappaport1996wireless, parsons2012mobile, hata1980empirical}, and ML models~\cite{parralejo2021comparative, liu2021fire, malmirchegini2012spatial}, but all suffer from low modeling fidelity due to notable limitations. 
Simulations require accurate scene priors, \eg Computer-Aided Design~(CAD) models, which are rarely available.
Empirical models oversimplify propagation with limited parameters, predicting only a single signal power.
ML models map inputs to labels but fail to capture underlying propagation physics.



\begin{figure}[tp]
\centering
	\subfigure[NeRF-based: \nerft~\cite{zhao2023nerf}]{
	\includegraphics[width=.23\textwidth]{figs/nerf2_view.pdf}\label{fig_over_nerft}}
	\subfigure[Ours: \ourSystem]{
\includegraphics[width=.23\textwidth]{figs/our_view.pdf}\label{fig_over_our}}
\caption{Both systems include scene representation \& ray tracing. $\left\{x, y, z\right\}$: voxel position, $\left\{x_{\text{tx}}, y_{\text{tx}}, z_{\text{tx}}\right\}$: transmitter position, $\left\{\alpha, \beta\right\}$: azimuth and elevation of ray $\gamma$.}
\Description[]{}
\end{figure}



\nerft~\cite{zhao2023nerf} addresses these limitations by adapting Neural Radiance Field~(NeRF)~\cite{mildenhall2021nerf} to the RF domain, achieving state-of-the-art modeling fidelity.
As depicted in Figure~\ref{fig_over_nerft}, the 3D space is divided into a dense grid of voxels.
Each voxel has two attributes representing its impact on RF signal propagation: emission~\(\boldsymbol{\psi}\) and attenuation~\(\boldsymbol{\rho}\).
Attributes of each voxel are computed by querying a trained Multilayer Perceptron~(MLP) with input~\(\{x, y, z, \alpha, \beta, x_{\text{tx}}, y_{\text{tx}}, z_{\text{tx}}\}\).
A ray tracing algorithm emits a ray \(\gamma\) from the receiver in a direction \(\left\{\alpha, \beta\right\}\), intersecting voxels along its path.  
The intersected voxels' attributes are aggregated to compute the received signal in that direction.  
For hemispherical coverage, \nerft emits \(360*90\) rays spanning directions \((360^\circ, 90^\circ)\) at one-degree resolution.
For a specific receiver location, an MLP is trained on signals received from various transmitter positions using mean squared error~(MSE) loss.


However, \nerft's high modeling fidelity comes at the cost of extensive data, substantial training GPU-hours, and high inference latency, resulting in a scalability-fidelity tradeoff~(more experimental results in ยง\ref{sec_nerf2_tradeoff}).  
\textit{First,} the vast number of voxels~(\(\mathord{\sim}31\) million in a conference room) demands immense data to learn their attributes.  
\textit{Second,} repeatedly querying the large MLP~(\(\mathord{\sim}0.7\) million parameters) during ray tracing is time- and computation-intensive.


This paper proposes \ourSystem, a scalable RF signal spatial propagation modeling framework that matches \nerft's high fidelity while minimizing data requirements, GPU-hours, and inference latency, termed \textbf{S1- High Efficiency}.
Furthermore, \ourSystem offers two additional scalable features, addressing the limitations of conventional methods:  
\textbf{S2- Prior-Free:} Does not require scene priors, such as CAD models.  
\textbf{S3- Signal Versatility:} Supports diverse RF signal types, including signal strength, complex values, and spectrum.



Recently, in computer graphics, 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} is developed to enable real-time image synthesis for 3D scenes.  
3DGS represents a scene as a collection of 3D Gaussian distributions. Each Gaussian has four attributes: mean~(position), covariance matrix~(defining size, shape, and orientation), color, and opacity.  
A pixel in an image is rendered by tracing a ray from the camera through the scene, intersecting 3D Gaussians along the ray.
To identify which Gaussians the ray intersects, termed ray-Gaussian intersections, 3D Gaussians are projected~(or "splatted") onto the 2D image.
The pixel value is computed by aggregating the color and opacity contributions from all the intersected Gaussians.



Inspired by 3DGS, \ourSystem represents a scene using 3D Gaussian distributions to offer high efficiency.
As illustrated in Figure~\ref{fig_over_our}, each Gaussian is parameterized by its mean \(\boldsymbol{\mu}\) and covariance \(\boldsymbol{\Sigma}\), which flexibly control its position, size, shape, and orientation.  
This flexibility enables a single Gaussian to efficiently encapsulate information that would otherwise require numerous voxels in \nerft's voxel grid representation.  
By adjusting the density of Gaussians, \ourSystem assigns more Gaussians to regions with objects, while sparsely populating free spaces or homogeneous areas.  
Additionally, each Gaussian explicitly stores emission \(\boldsymbol{\psi}\) and attenuation \(\boldsymbol{\rho}\), eliminating queries to large MLPs.
The sparsity of Gaussians and direct attribute retention drastically reduce data requirements and ray tracing costs.
However, transforming \ourSystem into a practical system is challenging for three reasons.

\textbf{C1- Directional and Phase Modeling:}
In 3DGS, a Gaussian's color is parameterized by spherical harmonics~(SH) coefficients~\cite{schonefeld2005spherical, kerbl20233d} to capture directional variations caused by optical effects such as reflections and shading. 
In contrast, RF signals with centimeter-scale wavelengths involve complex effects such as diffraction~\cite{1451581}, along with phase impacts on constructive and destructive interference, none of which are well captured by SH coefficients~\cite{schonefeld2005spherical, schmitz2012using}.


\textbf{C2- Prior-Free Initialization:}
In 3DGS, the initialization of 3D Gaussians depends on a scene's point cloud, which is generated using the Structure-from-Motion~(SfM) algorithm~\cite{snavely2006photo} applied to input images.  
However, in the RF domain, no equivalent algorithm exists to derive a point cloud from RF measurements.



\textbf{C3- Customized Ray Tracing:}  
In 3DGS, ray tracing involves splatting, aggregation, and parallelization.  
Splatting relies on transformation matrices to project 3D Gaussians onto the 2D image plane, identifying ray-Gaussian intersections.  
Aggregation combines the color and opacity of intersected Gaussians to compute each pixel's value.
Since splatting requires traversing all Gaussians for each ray, 3DGS incorporates custom Compute Unified Device Architecture~(CUDA) kernels~\cite{cuda_programming} for efficient parallel GPU computation. 
However, in the RF domain, signals are received by antennas, lacking established transformation matrices for Gaussian splatting.  
Second, aggregation in RF signal modeling requires consideration of both amplitude and phase.  
Third, RF-specific splatting and aggregation computations necessitate redesigned CUDA kernels.



By tackling the three challenges above, we make the following three contributions.



\textbf{i) Gaussian-Based RF Scene Representation:}
A scene is represented as customized 3D Gaussians. 
Each Gaussian is defined by four attributes: geometry-related, including the mean and covariance matrix; and RF-related, including emission and attenuation.
Each Gaussian's directional emission attribute is encoded by a dedicated small MLP instead of SH coefficients.
Additionally, RF-related attributes include amplitude and phase channels to capture phase information.


\textbf{ii) Gradient-Guided Attribute Learning:}
Instead of initializing Gaussians with a point cloud or randomly, we partition the scene into equal-sized cubes and assign each cube's center as a Gaussian's mean.  
The covariance matrix is initialized based on the average distance to the three nearest cube centers, while attenuation and emission are assigned randomly.  
This cube-based strategy ensures that the initialized Gaussians cover the entire scene.
All four Gaussian attributes are optimized using stochastic gradient descent~(SGD)~\cite{amari1993backpropagation}, with the number of Gaussians dynamically adjusted via a gradient-threshold-based mechanism.



\textbf{iii) RF-Customized CUDA for Ray Tracing:}  
Rays are emitted from the receiver in their respective directions.  
Ray-Gaussian intersections are identified through a proposed orthographic projection-based splatting module.  
A customized complex-valued blending algorithm aggregates RF-related attributes from the intersected Gaussians along each ray to compute the received signal.  
The computations for $360*90$ rays are parallelized using RF-customized CUDA kernels.

We evaluate \ourSystem through three field studies and two applications~(gateway coverage estimation and localization) across diverse frequency bands, antennas, signals, and scenes: RFID spatial spectrum synthesis~\cite{zhao2023nerf}, BLE and LoRa signal strength prediction.  
Results demonstrate that \ourSystem matches the modeling fidelity of \nerft~\cite{zhao2023nerf} while delivering significantly higher efficiency.  
For instance, in RFID spatial spectrum synthesis, \ourSystem matches \nerft's fidelity while reducing data requirements from 7.8 to 0.8\,\(\text{measurements}/\text{ft}^3\), GPU-hours from 5.0\,hours to 16.2 minutes, and inference time from 352.7\,ms to 4.2\,ms, achieving 9.8\,$\times$, 18.6\,$\times$, and 84.4\,$\times$ reductions, respectively.



These improvements offer substantial practical benefits. 
For example, localization tasks in a conference room~(\(26.2\,\textit{ft} \times 16.4\,\textit{ft} \times 9.8\,\textit{ft}\))~\cite{matlab_conference_room}, \ourSystem can save up to 200\,hours of data collection.
With rapid training, it adapts to dynamic environments within minutes.  
Its low latency enables real-time applications like cellular downlink channel prediction, requiring inference within the 50\,ms coherence time~\cite{liu2021fire, marzetta2016fundamentals}.


