

\section{Implementation}\label{sec_appendix_implementation}

\textbf{Training:}
Table~\ref{table_parameter} presents the hyperparameter settings, defined in Section~\ref{sec_design}. 
These values are determined through extensive empirical studies.
The attributes of all 3D Gaussians are updated using SGD~\cite{amari1993backpropagation}. 
The learning rates are set as follows: $\eta_{\boldsymbol{\rho}} = 0.01$ for attenuation, $\eta_{\boldsymbol{\psi}} = 0.0025$ for emission, $\eta_{\boldsymbol{S}} = 0.01$ for the scaling matrix, and $\eta_{\boldsymbol{R}} = 0.005$ for the rotation matrix. 
The learning rate for the mean, \(\eta_{\boldsymbol{\mu}}\), starts at 0.00016 and decreases exponentially to \(1.6 \times 10^{-6}\) over 30,000 iterations.
For Gaussian count optimization, the number of Gaussians is optimized only during the first half of the total iterations.
After that, only the attributes of the Gaussians are updated.

\begin{table}[t]
\centering
\caption{Hyperparameter settings.}
\begin{tabular}{lL{1.8in}l}
\toprule
Symbol & Meaning & Value \\ 
\midrule
\(\epsilon_{\boldsymbol{\mu}}\) & Threshold for mean gradient & 0.0002 \\ 
\(\epsilon_{r}\) & Threshold for radius & 10.0 \\ 
\(\epsilon_{\boldsymbol{\rho}}\) & Threshold for attenuation & 0.004 \\ 
\(N_{\boldsymbol{\mu}}\) & Gradient check frequency & 100 \\ 
\(N_{\boldsymbol{\rho}}\) & Attenuation check frequency & 100 \\ 
\(r_{\text{rx}}\) & Radius of the RESS & 1.0 \\
\(\phi\) & Scaling matrix reduction factor & 1.6 \\
\(\lambda\) & Balance two losses & 0.2 \\
\bottomrule
\end{tabular}
\label{table_parameter}
\end{table}


\textbf{CUDA Kernel:}  
Each grid contains 16 rays in both azimuth and elevation angles, totaling \(N_{\text{rays}} = 16 \times 16\) rays per grid.  
Gaussians intersecting each grid are sorted using the CUDA built-in \texttt{cub::DeviceRadixSort} API~\cite{cuda_sort}.  
Each splatting instance (a Gaussian intersecting a grid) is assigned a 64-bit key: the lower 32 bits store the distance to the receiver, while the upper 32 bits encode the grid index.  
This structure enables efficient parallel sorting of all splats by distance with a single invocation of the \texttt{cub::DeviceRadixSort} API.  


To integrate PyTorch with CUDA execution, we implement a custom PyTorch extension using C++ and CUDA, enabling efficient GPU-accelerated computations.  
The forward and backward computations are encapsulated within a subclass of \texttt{torch.autograd.Function}, ensuring seamless differentiation and gradient propagation within PyTorch’s computational graph. 
The Python interface, implemented via PyTorch’s C++ API, facilitates interaction between PyTorch tensors and CUDA kernels, handling memory layout conversions and efficient CPU-GPU data transfers.









