%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}  %PREPRINT 

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usetikzlibrary{calc}
\usetikzlibrary{patterns}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks}

\begin{document}
\input{macros}
\twocolumn[
\icmltitle{From Counterfactuals to Trees:\linebreak Competitive Analysis of Model Extraction Attacks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Awa Khouna}{polymtl}
\icmlauthor{Julien Ferry}{polymtl}
\icmlauthor{Thibaut Vidal}{polymtl}
\end{icmlauthorlist}

\icmlaffiliation{polymtl}{CIRRELT \& SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematics and Industrial Engineering, Polytechnique Montréal, Canada}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Thibaut Vidal}{thibaut.vidal@polymtl.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadat a in the PDF but will not be shown in the document
\icmlkeywords{Machine learning, Model extraction attacks, Competitive analysis, Counterfactual explanations}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%PREPRINT 
\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution 
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models.  In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the ``oracle'' queries required for faithfully inferring the underlying prediction function.
We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment.
\end{abstract}

\section{Introduction}
Recent research has shown that sharing trained machine learning (ML) models can lead to the reconstruction of sensitive training data, posing significant privacy risks~\citep[see, e.g.,][]{Boenisch2023, Carlini2024, DBLP:conf/icml/FerryFPV24}. Applications in fields such as medical diagnostics, financial services, and personalized advertising often handle large amounts of private data, making them attractive targets for data reconstruction attacks. These attacks exploit vulnerabilities in the model to recover confidential information from the training dataset, thereby undermining the privacy guarantees that organizations seek to uphold. Consequently, organizations may prefer to utilize Machine Learning as a Service (MLaaS) to leverage powerful models without directly exposing them, balancing the benefits of advanced analytics with the need to protect sensitive information.

While MLaaS platforms provide accessible and scalable ML solutions, they must address the growing demand for explainability in their decision-making processes. Regulatory frameworks such as the EU AI Act's Article 13\footnote{\href{https://artificialintelligenceact.eu/article/13/}{https://artificialintelligenceact.eu/article/13/}} further mandate greater transparency across a wide range of applications. In response, MLaaS providers increasingly incorporate explainability techniques to elucidate model behavior and ensure fairness. However, studies have shown that querying a model’s explanations can enable attackers to replicate its hyperparameters and architecture, effectively copying the original model~\citep{Florian2016StealingMachineLearningModels, wang2022dualcf, aivodji2020model, oksuz2023autolycus}. This reveals a critical tension between the need for transparency and the protection of model integrity and intellectual property.

Model extraction attacks were proposed against a variety of ML models in recent years~\citep{DBLP:journals/csur/OliynykMR23}. While very few of them are \emph{functionally equivalent} (i.e., they provably reconstruct the black-box model's decision boundary), they often rely on strong assumption, such as access to a leaf identifier in the case of decision tree models~\citep{Florian2016StealingMachineLearningModels}. Moreover, the majority of the literature focuses solely on empirically evaluating the fidelity of the extracted model w.r.t. the target black-box, lacking a rigorous framework for analyzing attack complexities and thoroughly characterizing their worst-case scenarios.
Finally, while counterfactual explanations constitute a promising attack surface and were exploited to conduct model extraction attacks~\citep{aivodji2020model, wang2022dualcf, dissanayake2024model}, existing approaches rely on training surrogate models without functional equivalence guarantees. 

\input{figs/fig_online_discovery}

In this study, we address these limitations through the following key contributions: 
\begin{itemize}
    \item We define a rigorous framework to characterize the complexity of model extraction attacks, utilizing competitive analysis (a notion from online optimization) to evaluate the difficulty of reconstructing models under various conditions 
    \item We introduce a novel algorithm (TRA) specifically designed to efficiently extract axis-parallel decision boundary models (including, but not limited to, tree ensemble models) through optimal counterfactual explanations. %, using the OCEAN framework \citep{parmentier2021optimal}.
    \item We provide a comprehensive theoretical analysis of our proposed method, offering guarantees on query complexity and demonstrating 100\% fidelity in the extracted models
    \item We conduct extensive experiments to validate our theoretical findings, presenting an average-case and anytime performance analysis of TRA compared to state of the art reconstruction methods. These experiments not only confirm out theoretical results, but also provide practical insights into the effectiveness and limitations of our approach
\end{itemize}

These contributions collectively highlight and permit us to better characterize security vulnerabilities in deploying explainable tree ensembles. 

\section{Online Discovery, Model Extraction Attacks and Competitive Analysis}\label{sec:online_analysis_extraction_attacks}
Online discovery problems have long been a focus of research in theoretical computer science, where the goal is to uncover the structure of an unknown environment through a sequence of queries or observations~\citep{Ghosh10, Deng91}. A classic example arises in map exploration: an agent (e.g., a robot) navigates a space cluttered with obstacles, with only limited ``line of sight'' at each position. The agent’s objective is to construct a complete representation (e.g., map) of its surroundings while minimizing resources such as travel distance or exploration time.

Model extraction attacks on MLaaS platforms exhibit striking parallels to these online exploration tasks. In a typical model extraction attack, an adversary queries a predictive model (the ``black box'') to gain information about its internal decision boundaries, effectively learning the decision function through a limited set of inputs and outputs. Drawing an analogy to the map exploration scenario, each query in a model extraction attack can be likened to a ``probe'' in the space of features that reveals partial information about the region—namely, the predicted label or a counterfactual explanation identifying the closest boundary capable of changing the prediction. Figure~\ref{fig:Illustration} illustrates this connection by contrasting a rover’s sensor sweep in a polygon exploration task with a query to locate a counterfactual explanation in a machine-learning model.

\paragraph{Online discovery problems \& Model extraction attacks.} Online discovery typically assumes an agent that can move freely in the physical world while receiving feedback about obstacles in its vicinity. In model extraction, the ``environment'' is the model’s input space, and the queries return a point that lies on the nearest decision boundary (or provides the counterfactual boundary itself). Thus, while map exploration may allow richer geometric observations (e.g., an entire sensor sweep of obstacles), counterfactual-based model extraction often yields more constrained information (e.g., only the nearest boundary for a given input). Despite these differences, both problems share a common hallmark: the true structure (environment or decision boundaries) is unknown \emph{a priori} and must be inferred \emph{online} via carefully chosen queries.

\paragraph{Competitive Analysis.} A central tool for analyzing online problems is \emph{competitive analysis}~\citep{Karlin86}, which compares the performance of an \emph{online} algorithm — one that adapts its decisions based solely on information acquired so far — to an optimal \emph{offline} algorithm that has full prior knowledge of the underlying structure. Formally, we measure the ratio between: (i) the complexity (e.g., number of queries, computational cost) incurred by the online algorithm to achieve its goal and (ii) the minimal complexity that an offline algorithm, with complete foresight, would require to accomplish the same task or verify its reconstruction. 
When this ratio remains bounded by a constant, we say that the online algorithm is \emph{constant-competitive}. Otherwise, the ratio might grow with the size or complexity of the problem, revealing fundamental limitations of online approaches.

By applying competitive analysis to model extraction attacks, we link these two domains in a natural way. We quantify how many queries (i.e., counterfactual explanations or label predictions) are required to guarantee perfect fidelity in reconstructing a target model under worst-case scenarios. Moreover, competitive analysis encourages us to ask: \emph{How many queries, relative to an all-knowing attacker, does one need in order to prove with certainty that a specific model has been recovered?} In this paper, we adapt these notions of worst-case online complexity to create a principled measure of the difficulty of extracting tree-based models, paralleling traditional results in online map discovery~\citep{Deng91, Hoffmann01, Ghosh10, Fekete10}.

This perspective paves the way for a unified view: model extraction attacks can be seen as online exploration in feature space. Our work not only provides new theoretical results for tree-based model extraction but also invites cross-pollination between the literature on online discovery algorithms and emerging threats in machine learning security.

\section{Method}

\subsection{Problem Statement}

We consider an input as an \(m\)-dimensionall vector in the input space \(\mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_m \subseteq \mathbb{R}^m\), and the output belongs to the categorical space \(\mathcal{Y}\). Let \(\mathcal{F}\) denote the set of all axis-parallel decision boundary models, including decision trees and their ensembles, such as random forests \citep{RF}. A machine learning (ML) classification model \(f \in \mathcal{F}\) is defined as a function \(f : \mathcal{X} \mapsto \mathcal{Y}\). For simplicity and without loss of generality, we focus our discussion on decision trees, as any axis-parallel decision boundary model can be represented as a decision tree \citep{vidal2020bornagaintreeensembles}. The input space \(\mathcal{X}\) may comprise both categorical and continuous features.
\begin{definition}
\label{def:Oracle}
    For a distance function \(d\), an optimal counterfactual explanation oracle \(\mathcal{O}_d\) is defined as a function \(\mathcal{O}_d : \mathcal{F} \times \mathcal{X} \times \mathcal{P}(\mathcal{X}) \mapsto \mathcal{X}\) that returns an optimal counterfactual explanation using the distance \(d\) for a given instance \(x \in \mathcal{X}\), a model \(f \in \mathcal{F}\), and an input subset \(\mathcal{E} \subseteq \mathcal{X}\):
    \begin{align*}
        \mathcal{O}_d (f, x, \mathcal{E}) \in \text{argmin}_{x' \in \mathcal{E}} &\quad d(x, x') \\
        \text{s.t.} \quad & f(x') \neq f(x)
    \end{align*}
    where \(\mathcal{P}(\mathcal{X})\) is the set of all possible subsets $\mathcal{E} \subseteq \mathcal{X}$. 
\end{definition}
Formally, an optimal counterfactual explanation, also coined as closest counterfactual, can be interpreted as the nearest input to \(x\) that yields a different prediction under model \(f\).


For an adversary with black-box access to a target model~\(f\) (\emph{i.e.,} through a prediction API), the objective of a \emph{model extraction attack} is to retrieve the exact model's parameters~\cite{Florian2016StealingMachineLearningModels}. A tractable weakening, coined \emph{functionally equivalent extraction} (Definition~\ref{def:func_eq}), is to reconstruct a model encoding the exact same function over the input space~\citep{DBLP:conf/uss/JagielskiCBKP20}. Note that with such relaxation, the model's parameters (or even hypothesis class) may differ as long as the prediction function is exactly the same. 
A common way to empirically evaluate such attacks is through the \emph{fidelity}~\citep{aivodji2020model} of the model reconstructed by the attacker, coined the \emph{surrogate model}. 
It is defined as the proportion of examples (from a given dataset) for which the surrogate agrees with the target model. 

\begin{definition}
\label{def:func_eq}
    A functionally equivalent attack aims to reconstruct a model \(\hat{f} \in \mathcal{F}\) such that it is functionally identical to the target model \(f \in \mathcal{F}\) across the entire input space~\(\mathcal{X}\). Formally, the attack seeks to find \(\hat{f}\) satisfying:
    \[
        \forall x \in \mathcal{X}, \quad \hat{f}(x) = f(x)
    \]
    The objective is to achieve this equivalence with the minimum number of queries to the target model.
\end{definition}

\begin{definition}
\label{def:compet}
    Let \(\mathcal{A}\) denote an online model extraction attack algorithm. Define \(Q_{\mathcal{A}}^f\) as the number of queries required by \(\mathcal{A}\) to extract the decision boundary model \(f\), and let \(Q_{opt}^f\) represent the minimal (optimal) number of queries necessary to extract \(f\) by an omniscient offline algorithm. The algorithm \(\mathcal{A}\) is said to be \emph{c-competitive} if, for any model \(f \in \mathcal{F}\):
    \[
        Q_{\mathcal{A}}^f \leq c \cdot Q_{opt}^f
    \]
\end{definition}


In this work, we address the problem of executing a functionally equivalent attack to reconstruct the decision function~\(f\) (i.e., decision boundary) using a minimal number of queries. To this end, our approach leverages access to an optimal counterfactual explanation oracle \(\mathcal{O}_d\). %Consequently, the objective is to develop a functionally equivalent attack leveraging such counterfactual explanations. 

\subsection{Tree Reconstruction Attack algorithm}

We now introduce our primary algorithm, the \textbf{Tree Reconstruction Attack (TRA)}, detailed in \Cref{alg:TRA}. TRA is a divide-and-conquer based algorithm that aims to reconstruct a decision tree \( f_n \) with \( n \) split levels by leveraging a counterfactual explanation oracle \( \mathcal{O}_d \) and systematically exploring the input space \( \mathcal{X} \). A split level divides the input space into two subspaces based on a particular value for a given feature. Note that multiple nodes within different branches of a decision tree can share the same split level.

\paragraph{Algorithm Overview.}
TRA operates by maintaining a query list \( \mathcal{Q} \) that initially contains the entire input space~\(\mathcal{X}\). The algorithm iteratively processes each input subset \( \mathcal{E} \subseteq \mathcal{X}\) from \( \mathcal{Q} \), until \( \mathcal{Q} \) is empty, ensuring that all decision boundaries of the target model \( f \) are identified and replicated in the reconstructed tree. More precisely, at each iteration, TRA first retrieves the subset \( \mathcal{E} \) on top of the priority queue \( \mathcal{Q} \). It computes its geometric center~\(x\) using the \( center \) function (line 5). It then queries the oracle \( \mathcal{O}_d \) with the current model \( f \), input \( x \), and subset \( \mathcal{E} \) to obtain a counterfactual explanation \( x' = \mathcal{O}_d(f, x, \mathcal{E}) \) (line 8). The set of feature indices where \( x' \) differs from \( x \), i.e., \( \{ i \mid x'_i \neq x_i \} \) is consequently identified. For each differing feature \( i \), TRA splits the input subset \( \mathcal{E} \) into two subspaces based on the split value \( x'_i \) (\( split \) function). The resulting subspaces are added to \( \mathcal{Q} \) for further exploration (line 9). If no counterfactual explanation \( x' \) exists within \( \mathcal{E} \) 
, TRA assigns the label \( y = f(x) \) to the entire subset \( \mathcal{E} \), indicating that it corresponds to a leaf node in the reconstructed tree (line 11). % line 11

\paragraph{Illustrative Example.}
To demonstrate the execution of TRA, consider the axis-parallel decision boundary illustrated on the right side of Figure \ref{fig:decision_bound_TRA}. Initially, TRA begins with the entire input space \(\mathcal{X} = [0,1]^2\). In the first iteration, the algorithm selects the center point \(x^{(1)} = (0.5,0.5)\) of \(\mathcal{X}\) and queries the counterfactual explanation oracle \( \mathcal{O}_{\lVert.\rVert_2} \), which returns a counterfactual \(x'^{(1)} = (0.5,0.4)\) that differs from \(x^{(1)}\) in the second feature ($x_2$). This results in the first split of the input space (to $\mathcal{E}_1 = [0,1]\times]0.4,1]$ and $\mathcal{E}_2 = \mathcal{X}\setminus\mathcal{E}_1$) based on the condition \(x_2 \leq 0.4\), as shown on the left side of Figure \ref{fig:decision_bound_TRA}. In the subsequent iterations, TRA focuses on the resulting subspaces. For example, within the subset where \(x_2 \geq 0.4\), TRA identifies another split at \(x_1 \leq 0.7\), further partitioning the space. After three iterations, as shown in Figure~\ref{fig:decision_tree_TRA}, the reconstructed decision tree accurately captures part of the decision boundaries of the target model, effectively distinguishing between different regions in the input space. The gray hatched zones (or ``\texttt{?}'' nodes) represent regions that have not yet been explored and remain in the query list \(\mathcal{Q}\). 

\input{figs/tra_algorithm}
\begin{figure}[h!]
    \centering
     \begin{subfigure}{0.45\textwidth}
        \centering
        \input{results/Tree_example.tikz}
        \caption{Decision tree reconstructed by TRA after 3 iterations.}
        \label{fig:decision_tree_TRA}
    \end{subfigure}   
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \input{results/DB_example.tikz}
        \caption{Left : Decision boundary of the model extracted by TRA after 3 iterations. Right : Decision boundary of the target model \smash{$f : [0,1]^2 \to \{\colorbox{class1}{\classB},\colorbox{class2}{\textcolor{white}{\classA}}\}$}.} %``Green",``Purple"
        \label{fig:decision_bound_TRA}
    \end{subfigure}
    \caption{Illustrative example of the execution of TRA.}
\end{figure}

\begin{proposition}\label{prop:TRComplexity}
    Let \( f_n \) be a decision tree with \( n \) split levels across a $m$-dimensional input space \( \mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_m \). Denote \( s_i \) as the number of split levels in \( f_n \) over the \( i \)-th feature, such that \( \sum_{i=1}^{m} s_i = n \). The worst-case complexity of \Cref{alg:TRA} is \( O\left(\prod_{\substack{i = 1 \\ s_i \neq 0}}^{m} s_i\right) \).
\end{proposition}

\begin{corollary}\label{cor:TRComplexity}
    Let \( p \leq m \) denote the number of features with at least one split level. The worst-case complexity of \Cref{alg:TRA} is \( O\left(\left({n}/{p}\right)^p\right) \).
\end{corollary}

The proofs of Proposition \ref{prop:TRComplexity} and Corollary \ref{cor:TRComplexity} are provided in the Appendix \ref{proof:prop_1}.
Proposition \ref{prop:TRComplexity} establishes a first simple upper bound on the complexity of \Cref{alg:TRA}, indicating that the algorithm's efficiency is constrained by this theoretical limit. To illustrate, consider a two-dimensional decision tree that partitions the space in a chessboard-like pattern. A comprehensive mapping of such a space necessitates at least \( s_1 \times s_2 \) queries (one in each sub-square), a requirement that holds for multi-class classification scenarios and in high-dimension (\( m \geq 2 \)).



\paragraph{Query Selection Analysis.}
The primary hyperparameter in the TRA algorithm, as outlined in \Cref{alg:TRA}, pertains to the strategy for selecting query points within an input space. By default, TRA selects the geometrical center of the current input subset as the query point. Alternatively, one could choose other points such as the lower/upper left/right corners, or even a random point. In the following, we present theoretical results analyzing the impact of different query selection strategies on the algorithm's performance. To this end, we leverage the notion of competitive analysis for online discovery problems discussed in Section~\ref{sec:online_analysis_extraction_attacks}.




\begin{proposition}\label{prop:TRA_compet}
    For $(n,m) \in \mathbb{N}^2$, the TRA Algorithm \ref{alg:TRA} achieves a competitive ratio of $C_{TRA}^{(n,m)}$, defined as:
    \begin{align*}
    C_{TRA}^{(n,m)} &= \frac{\sum_{i=1}^m s_i \prod_{j=1}^{i-1} (s_j + 1)  + \prod_{j=1}^m (s_j + 1)}{2n+1} \\
    &\leq \frac{2\left(1 + \frac{n}{m}\right)^m - 1}{2n + 1},
    \end{align*}
    where $s_i$ is the number of splits along the $i$-th feature.
\end{proposition}

\begin{proposition}\label{prop:DC_compet}
    For all $n > 0$ and $m \geq 2$, no divide-and-conquer-based algorithm can achieve a competitive ratio better than $C^{(n,m)}_{TRA}$.
\end{proposition}

The proofs of Propositions \ref{prop:TRA_compet} and \ref{prop:DC_compet} are provided in Appendix \ref{proof:prop_1}.
Proposition \ref{prop:TRA_compet} provides the competitive ratio achieved by the TRA algorithm, while Proposition \ref{prop:DC_compet} establishes that the choice of query position does not affect the competitive ratio for any divide-and-conquer algorithm iteratively partitioning the input space. Together, these propositions demonstrate that TRA not only offers a competitive approach to tree reconstruction under various query selection strategies but also sets a theoretical benchmark that cannot be surpassed by other divide-and-conquer-based methods. This underscores the effectiveness of TRA in efficiently reconstructing axis-parallel decision boundary models.

\paragraph{Anytime Behavior.} Since the query budget may not be known in advance, and may vary depending on the application, the ability of an extraction attack to operate in an anytime manner is crucial. It means that it can be stopped at any iteration while still producing a decision tree classifier—though it may not yet be fully functionally equivalent. Specifically, whenever we split the input space (line 9 of Algorithm~\ref{alg:TRA}), we assign provisional labels to the resulting subspaces: one inherits the query’s label, and the other adopts the counterfactual’s label. This design ensures the algorithm can output a classifier at any stage of execution. Note that the exploration strategy of Algorithm~\ref{alg:TRA} depends on the ordering of the priority queue $\mathcal{Q}$. In practice, preliminary experiments suggested good anytime performance using breadth-first search (BFS), which distributes the exploration evenly. % Alternative strategies such as prioritizing the largest subspace can improve the anytime performance by distributing exploration more evenly. In our experiments, we adopt BFS. 
While the design of alternative best-first search strategies is a promising direction to enhance TRA's anytime performance, it is worth noting that the exploration strategy does not affect the number of queries needed by TRA to fully reconstruct the target model. Indeed, it does not modify the iterative partitioning of the input space performed by the divide-and-conquer algorithm.

\section{Experiments}

We now empirically evaluate the efficiency and effectiveness of our proposed TRA extraction attack and benchmark it against existing model extraction techniques. We first introduce the experimental setup, before discussing the results. 

\subsection{Experimental Setup}\label{sec:exp_setup}

\paragraph{Datasets.} We use six binary classification datasets, selected from related works on model extraction attacks~\citep{aivodji2020model,wang2022dualcf,Florian2016StealingMachineLearningModels} and encompassing a variety of feature types, dimensionalities, and classification tasks, as summarized in Table \ref{tab:datasets}. More precisely, we consider the COMPAS dataset~\citep{angwin2016machine}, as well as the Adult Income (Adult), Default of Credit Card Clients (Credit Card), German Credit and Student Performance (SPerformance) 
datasets from the UCI repository~\citep{Dua:2019}. Categorical features are one-hot encoded, while numerical, discrete (ordinal) and binary ones are natively handled by both tree building procedures and reconstruction attacks. Each dataset is partitioned into training, validation, and test sets with proportions of 60\%, 20\%, and 20\%, respectively. 
\begin{table}[b]
    \caption{Summary of the datasets used in our experiments. For each dataset, $m$ is the number of features after pre-processing, encompassing $m_N$ numerical, $m_B$ binary, $m_C$ categorical (before one-hot encoding) and $m_D$ discrete (ordinal) ones.}
    \label{tab:datasets}
    \begin{center}
        \begin{small}
            \begin{tabular}{lcccccc}
                \toprule
                \textbf{Dataset} & \#Samples & $m$ & $m_N$ & $m_B$ & $m_C$ & $m_D$  \\ 
                \midrule
                Adult & 45222 & 41 & 2 & 2 & 4 & 3 \\
                COMPAS & 5278 & 5 & 0 & 3 & 0 & 2 \\
                Credit Card & 29623 & 14 & 0 & 3 & 0 & 11 \\
                German Credit & 1000 & 19 & 1 & 0 & 3 & 5 \\
                SPerformance & 395 & 43 & 0 & 13 & 4 & 13\\
                \bottomrule
            \end{tabular}
        \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\paragraph{Training the target tree-based models.} 
We train two types of tree-based target models implemented in the scikit-learn library~\citep{scikit-learn}: decision trees and random forests. For decision trees, we experiment with varying $\texttt{max\_depth}$ parameters ranging from 4 to 10, as well as unbounded trees ($\texttt{max\_depth}$ set to $\texttt{None}$). The experiments on random forests focus on the COMPAS dataset, employing different numbers of estimators to assess scalability and robustness. To prevent overfitting, we utilize the validation set for hyperparameter tuning and apply cost-complexity pruning where applicable. Detailed training procedures and hyperparameters configurations are elaborated in Appendix~\ref{appendix:training_details}.

\paragraph{Baselines.}
We benchmark TRA against three state-of-the-art model extraction attacks.
First, \PathFinding{} is the only functionally equivalent model extraction attack against decision trees. While it does not rely on counterfactual examples, it assumes access to a leaf identifier indicating in which leaf of the target decision tree the query example falls~\cite{Florian2016StealingMachineLearningModels}. It is thus not applicable to random forests.
Second, \CF{} \citep{aivodji2020model} leverages counterfactual explanations to train a surrogate model mimicking the target one.
Third, \DualCF{} \citep{wang2022dualcf} enhances the \CF{} approach by additionally computing the counterfactuals of the counterfactuals themselves, which improves fidelity \cite{wang2022dualcf}. 
For each baseline, we adapt the number of queries relative to the complexity of the target model. Specifically,~\CF~and~\DualCF~are allocated a query budget of $50$ times the number of nodes in the target decision tree, while~\PathFinding~is configured with an $\epsilon = 10^{-5}$ to achieve approximate functional equivalence. 

We evaluate three surrogate model variants for the two surrogate-based baselines \CF{} and \DualCF{}: a multilayer perceptron (MLP), and two models from the same hypothesis class as the target model (i.e., a decision tree or a random forest) -- one of them sharing the exact same hyperparameters, and the other one using default hyperparameter values. 
These three surrogates correspond to three different knowledge for the adversary: knowledge of the hypothesis class of the target model, of its hyperparameters, or none of them. Both \CF{} and \DualCF{} were originally tested against heuristic-based counterfactual explanations using the DiCE~\citep{Mothilal_2020} algorithm. To assess the impact of explanation optimality on the attack's performance, and to ensure fair comparisons, we run \CF{} and \DualCF{} using both DiCE and the OCEAN framework~\citep{parmentier2021optimal}, the later providing optimal counterfactual explanations. In a nutshell, OCEAN formulates the search of counterfactual examples as an optimization problem, modeled and solved using a mixed-integer linear programming solver. Our implementation of TRA relies on OCEAN as the counterfactual oracle \(\mathcal{O}_d\).

\paragraph{Evaluation.} We employ two metrics to quantify the effectiveness of each model extraction attack. First, \emph{fidelity} quantifies the success of the performed extraction attack, by measuring the proportion of inputs on a given set for which the extracted model's predictions match those of the target model. 
Second, the \emph{number of queries} indicates the attack's efficiency, as it counts the total number of queries made to the target model's oracle during the extraction process.  

All experiments are run on a computing platform, over a set of homogeneous nodes using Intel Platinum 8260 Cascade Lake @ 2.4GHz CPU. Each run is allocated 4 threads using up to $4$GB of RAM each (multi-threading is mainly used by the OCEAN optimal counterfactual oracle). We repeat each experiment five times, using five different random seeds, and report average values. The source code to reproduce all our experiments and figures is accessible at \url{https://github.com/vidalt/Tree-Extractor}, under a MIT license.

\subsection{Results}\label{sec:expes_results}

\input{figs/figs_anytime_paper}
\input{figs/figs_anytime_rfs_paper}
\input{figs/figs_func_equiv_paper}

\paragraph{Fidelity measurements.} We consider two fidelity values, measured either on a dataset of $3,000$ examples uniformly sampled over the input space or on a test set. Since both measurements provide the same trends, we hereafter focus on the later, which better characterizes the ability of the extracted models to mimic the target ones over the whole input space. 
For the sake of completeness, we also report fidelity results measured on a test set for our experiments using random forest target models in Appendix~\ref{appendix:fidelity_test_set}.

\paragraph{Tuning surrogate-based model extraction attacks.} 
In Appendix~\ref{appendix:surrogate_based_attacks}, we provide the detailed results of our model extraction experiments using the \CF{} and \DualCF{} attacks. We perform a comprehensive comparison of the impact of the type of surrogate and counterfactual oracle used. Our results demonstrate that fitting surrogate models of the same hypothesis class facilitates the extraction of both decision trees and random forests. However, knowledge of their hyperparameters does not provide any advantage to either attack. Finally, the non-optimal counterfactuals provided by DiCE lead to better fidelity results than the optimal ones computed by OCEAN for these two attacks. This can be explained by their heuristic nature, which leads to the building of more diverse counterfactuals, not necessarily lying next to a decision boundary.
For the remaining of this section, we retain only the best performing configuration for both \CF{} and \DualCF{}, which consists in training a surrogate model of the same hypothesis class using DiCE counterfactuals.

\paragraph{TRA outperforms existing approaches in terms of anytime fidelity to extract tree-based models.}
Figure~\ref{fig:MFvsQ_adultcompas} presents the average fidelity of surrogate models obtained from the four studied model extraction attacks on decision trees, as a function of the number of queries, for the Adult and COMPAS datasets. Results for additional datasets, provided in Figure~\ref{fig:MFvsQ_full} (Appendix~\ref{appendix:all_results}), exhibit the same trends. Across all cases, TRA consistently achieves higher fidelity for any fixed query budget and converges orders of magnitude faster to perfect fidelity, while also certifying functional equivalence—unlike \CF{} and \DualCF{}. The anytime performance of the considered model extraction attacks against random forests (excluding \PathFinding{}, which is not applicable to ensemble models) is shown in Figure~\ref{fig:MFvsQ_adultcompas_RFs} for the COMPAS dataset. Again, TRA outperforms existing methods, producing more accurate surrogate models with fewer queries and converging to perfect fidelity significantly faster.

\paragraph{TRA exhibits state-of-the-art performances for functionally equivalent extraction of decision trees.}
Figure~\ref{fig:QvsN_adultcompas} shows the number of queries required by \PathFinding{} and TRA to achieve functionally equivalent model extraction, plotted against the number of nodes in the target models. Results for the Adult and COMPAS datasets are presented, with additional datasets provided in Figure~\ref{fig:QvsN_full} (Appendix~\ref{appendix:all_results}). TRA consistently requires orders of magnitude fewer queries than \PathFinding{} to reconstruct the target models with perfect fidelity.

\paragraph{TRA theoretically and empirically outperforms existing approaches to extract random forests.}
Figure~\ref{fig:MFvsQ_adultcompas_RFs} presents the average fidelity of the three considered model extraction attacks against random forests, plotted against the number of performed queries for the COMPAS dataset. The results show that TRA achieves higher fidelity with fewer queries to the counterfactual oracle and converges significantly faster to perfect fidelity. Moreover, TRA is the only attack that certifies functional equivalence for tree ensemble target models.
Additional results in Figure~\ref{fig:tra_queries_rf_size} (Appendix~\ref{appendix:all_results}) indicate that TRA scales efficiently with the size of the target random forest, as the number of required queries grows sub-linearly with the total number of nodes. This behavior arises because large forests with many trees may introduce redundancies, allowing the extracted model to be represented with perfect fidelity as a more compact decision tree \citep{vidal2020bornagaintreeensembles}. 

\section{Related Works}

The flourishing literature on privacy in machine learning encompasses a wide variety of inference attacks, each considering different setups and objectives~\citep{DBLP:journals/corr/abs-2005-08679,DBLP:journals/corr/abs-2007-07646}. 
This paper focuses on model extraction attacks~\citep{Florian2016StealingMachineLearningModels}, which aim at reconstructing the decision boundary of a black-box target model as accurately as possible, given a prediction API. As highlighted in recent surveys~\cite{DBLP:journals/cm/GongWCYJ20,DBLP:journals/csur/OliynykMR23}, numerous attacks have been proposed in recent years, targeting a broad spectrum of hypothesis classes. Hereafter, we focus on those specifically targeting tree-based models or exploiting counterfactual explanations. 

\citet{Florian2016StealingMachineLearningModels} propose a functionally equivalent model extraction attack specifically targeting regression or decision trees: \PathFinding{}. Their approach assumes that each query reply contains a unique identifier for the associated leaf. In a nutshell, \PathFinding{} aims at identifying the decision boundaries of each leaf in the target tree by varying the values of each feature. While effective, this method requires a large number of queries, though partial input queries can sometimes mitigate this overhead.

While many recent have focused on generating counterfactual explanations~\citep{DBLP:journals/datamine/Guidotti24}, they have also been shown to facilitate privacy attacks~\citep{DBLP:conf/aistats/PawelczykLN23}. \citet{aivodji2020model} introduce \CF{}, a model extraction attack that leverages counterfactual explanations. Their approach constructs a labeled dataset by querying both predictions and counterfactuals from the target model, which is then used to train a surrogate. \citet{wang2022dualcf} extend this method with \DualCF{}, which improves fidelity by additionally querying the counterfactuals of the counterfactual explanations.
\citet{dissanayake2024model} employ polytope theory to show that a sufficient number of optimal counterfactual explanations can approximate convex decision boundaries. They propose a model extraction attack against locally Lipschitz continuous models, with fidelity guarantees dependent on the Lipschitz constants of the target and surrogate models. However, functional equivalence cannot be strictly certified, and as the authors acknowledge, these guarantees do not apply to decision tree-based models, which lack local Lipschitz continuity and convexity.
Finally, while beyond the scope of this paper, other explanation-based model extraction attacks have been explored, including those relying on gradient-based~\citep{DBLP:conf/fat/MilliSDH19,DBLP:conf/sectl/MiuraSY24} and other feature-based methods~\citep{oksuz2023autolycus}.

\section{Conclusions and Discussion}

We introduced the first functionally equivalent model extraction attack against decision trees and tree ensembles, leveraging optimal counterfactual explanations. In addition to its rigorous functional equivalence guarantee, the proposed method achieves higher fidelity than prior approaches while requiring fewer queries. We also leveraged well-established tools from online discovery to enable a formal analysis of model extraction, drawing an analogy between the two fields. We illustrated the applicability and relevance of this analysis by providing bounds on our attack's efficiency compared to the best achievable strategy, relying on the notion of competitive ratio. This perspective is essential for formally characterizing and comparing model extraction attacks.

Our study demonstrates that optimal counterfactual explanations can be systematically exploited to reconstruct tree ensembles via query APIs, as they inherently reveal decision boundaries. This raises significant concerns, especially as explainability is increasingly mandated by regulations. In many real-world applications, counterfactual explanations serve as a natural mechanism to meet transparency requirements by providing recourse information.

The research perspectives connected to our work are numerous. First, we believe that competitive analysis provides a valuable foundation for studying model extraction attacks, and future work should adopt the same lenses to evaluate other target models. Besides this, both the algorithms and their theoretical bounds could be refined. Improving input space exploration while mitigating worst-case query complexity is a key direction, including strategies such as dynamically reordering TRA’s priority queue to avoid worst-case scenarios, or adopting completely different exploration methods. Finally, investigating the impacts of privacy-preserving mechanisms for counterfactual explanations~\citep{10.1145/3580305.3599343} on attack's success is a crucial direction towards conciliating trustworthiness and privacy through ML explainability APIs.


\section*{Impact Statement}

To meet ethical and legal transparency requirements, machine learning explainability techniques have been extensively studied in recent years. Among them, counterfactual explanations provide a natural and effective approach by identifying how an instance could be modified to receive a different classification. In credit granting applications, for example, they can provide recourse to individuals whose credit was denied.

As a result, MLaaS platforms increasingly integrate such explainability tools into their APIs. While these explanations enhance user trust, they also expose a new attack surface to malicious entities by revealing additional model information. In this work, we theoretically and empirically demonstrate that optimal counterfactual explanations of decision trees and tree ensembles can be exploited to conduct efficient model extraction attacks. These results highlight the critical tension between transparency and the protection of model integrity and intellectual property. By identifying and quantifying these vulnerabilities, we highlight the risks of releasing model explanations without a thorough security assessment. Our research establishes a benchmark for evaluating method safety, advocating for the development of privacy-preserving approaches to explainability.

Finally, while previous evaluations of model extraction attacks have been predominantly empirical, we show that tools from online discovery provide a principled framework for characterizing attack efficiency. This perspective paves the way for more structured approaches to assessing model attack budgets and risks.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proofs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of Proposition \ref{prop:TRComplexity}]
    \label{proof:prop_1}
    We prove the proposition by induction on the number of split levels \( n \). For clarity and precision, we denote the number of splits in the \(i\)-th dimension for a decision tree with \(n\) split levels as \(s^{(n)}_i\), rather than simply using~\(s_i\).
    
    \begin{itemize}
        \item \textbf{Base Case (\( n = 1 \)):} 
            For \( n = 1 \), there exists a single feature \( j \) with \( s^{(1)}_j = 1 \) and \( s^{(1)}_i = 0 \) for all \( i \neq j \). The number of queries required is at most \( 3 = 2s^{(1)}_j + 1 = O(s^{(1)}_j) \).
        
        \item \textbf{Inductive Step:} 
            Assume the statement holds for all trees with up to \( n \) split levels. Consider a tree \( f_{n+1} \) with \( n+1 \) split levels. Let \(1 \leq  j \leq m\) be the feature index of the first detected split ($x_j \leq \alpha$) where $\alpha \in \mathbb{R}$, dividing \( \mathcal{X} \) into two subspaces:
            \[
                \mathcal{X}_j^1 = \{x \in \mathcal{X} \mid x_j \leq \alpha\}, \quad \mathcal{X}_j^2 = \{x \in \mathcal{X} \mid x_j > \alpha\}.
            \]
            Each subspace contains subtrees \( f_{n_1} \) and \( f_{n_2} \) with \( n_1, n_2 \leq n \) split levels, respectively. By the inductive hypothesis, the number of queries for each subtree is respectively \(O\left(\prod_{\substack{i = 1 \\ s^{(n_1)}_i \neq 0}}^{m} s^{(n_1)}_i\right) \)  and \(O\left(\prod_{\substack{i = 1 \\ s^{(n_2)}_i \neq 0}}^{m} s^{(n_2)}_i\right) \). \\
            Since \( s^{(n+1)}_j = s^{(n_1)}_j + s^{(n_2)}_j + 1 \) and \( s^{(n_q)}_i = s^{(n+1)}_i \) for \( i \neq j \) (and $q \in \{1, 2\}$), the total number of queries for \( f_{n+1} \) is:
            \[
                O\left(\prod_{\substack{i = 1 \\ s^{(n+1)}_i \neq 0}}^{m} s^{(n+1)}_i\right).
            \]
    \end{itemize}

    Therefore, the query count for \( f_n \) is \( O\left(\prod_{\substack{i = 1 \\ s^{(n)}_i \neq 0}}^{m} s^{(n)}_i\right) \).
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:TRComplexity}]
    \label{proof:cor_1}
    We build on the worst-case complexity demonstrated in Proposition \ref{prop:TRComplexity}, and we solve the following optimization problem:
    \[
        \max_{s^{(n)}_1, \ldots, s^{(n)}_p} \prod_{i=1}^{p} s^{(n)}_i \quad \text{s.t.} \quad \sum_{i=1}^{p} s^{(n)}_i = n \quad \text{and} \quad s^{(n)}_i \geq 1 \ \forall i \in \{1, \ldots, p\}.
    \]
    Since maximizing a positive value is equivalent to maximizing its logarithm, we transform the objective into:
    \[
        \max_{s^{(n)}_1, \ldots, s^{(n)}_p} \sum_{i=1}^{p} \log(s^{(n)}_i).
    \]
    Applying the Karush-Kuhn-Tucker (KKT) conditions, we find that the maximum occurs when \( s^{(n)}_i = \frac{n}{p} \) for all \( i \). Substituting back, the worst-case complexity becomes \(O\left(\left({n}/{p}\right)^p\right)\).
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop:TRA_compet} ]
     Let $n>0$, $m\geq1$, denote $\alpha_1, ..., \alpha_n$ the tree split levels (decision thresholds) and for each feature $ j = 1, ...,m$, let $s_j$ represent the number of splits in the $j$-th dimension, ordered such that \( s_1 \geq s_2 \geq \dots \geq s_m \). Without loss of generality, assume that the split levels are grouped by dimension. Specifically, splits \( \alpha_1 \) to \( \alpha_{s_1} \) occur in the first dimension, splits \( \alpha_{s_1 + 1} \) to \( \alpha_{s_1 + s_2} \) in the second dimension, and so on. Additionally, within each dimension, the split levels are sorted in increasing order, i.e.,
    \[
    \forall 1 \leq j \leq m, \quad \sum_{p=1}^{j-1} s_p + 1 \leq i \leq \sum_{p=1}^{j} s_p, \quad \alpha_i < \alpha_{i+1}.
    \]
\begin{enumerate}
    \item \textit{Proof of Upper Bound:}

    In the best-case scenario, where the decision tree follows a single branch (i.e., all splits occur along one feature), any algorithm, including the optimal one, would require at least \( 2n + 1 \) queries to reconstruct the tree (i.e. $Q_{opt}^f \geq 2n + 1$). This includes one query for each split and additional queries to verify the leaf nodes.
    
    Conversely, in the worst-case scenario, such as a chessboard-like decision tree where splits are evenly distributed across multiple features, the TRA algorithm must explore all possible regions created by these splits. For a two-dimensional tree, this results in \( s_1 s_2 + s_1 + s_2 + 1 \) queries, where \( s_1 \) and \( s_2 \) are the number of splits along each feature. Extending this to \( m \) dimensions, the number of queries grows multiplicatively with the number of splits per feature, leading to:
    \[
    Q_{TRA}^f \leq \sum_{i=1}^m s_i \prod_{j=1}^{i-1} (s_j + 1) + \prod_{j=1}^m (s_j + 1)
    \]
    Therefore, the competitive ratio \( C_{TRA}^{(n,m)} \) is bounded above by:
    \[
    C_{TRA}^{(n,m)} = \text{sup}_{f \in \mathcal{F}} \left(\frac{Q_{TRA}^f}{Q_{opt}^f}\right) \leq \frac{\sum_{i=1}^m s_i \prod_{j=1}^{i-1} (s_j + 1) + \prod_{j=1}^m (s_j + 1)}{2n + 1}
    \]
    
     
    \item \textit{Proof of Lower Bound:}
    
    Consider a tree with \( n \) splits. An adversary (dynamically building the worst-case instance the online algorithm is run on) can arrange the splits such that the first split detected by TRA is the last decision node in the tree. Specifically, the adversary ensures that the initial split does not reduce the complexity of identifying the remaining \( n \) splits. 
    
    Consider the following adversarial example: for each \( 1 \leq i \leq n \) and \( 1 \leq j \leq m \), let the dimension that \( \alpha_i \) splits on be~\( j \), and set
    \[
     \alpha_i = \begin{cases}
         \frac{i}{(s_1 + 1)}, & \text{if } j = 1, \\
         \frac{i - \sum_{p=1}^{j-1} s_p}{2(s_j + 1)} + \frac{1}{2} + \epsilon, & \text{otherwise},
     \end{cases} 
    \]
    where \( \epsilon > 0 \). This adversarial example ensures that within any hyper-rectangle defined by split level boundaries, for \(j = 1, ..., m-1\) if there are splits in both the \( j \)-th and \( j+1 \)-th dimensions, then there exists a split in the \( j \)-th dimension that is closer to the center of the hyper-rectangle than any split in the \( j+1 \)-th dimension.
  
    
    Therefore, in this adversarial example, TRA will always detect the splits of the \( j \)-th dimension before those of the \( j+1 \)-th dimension. Therefore, the adversary can design a decision tree with a single branch (as illustrated in the right tree of Figure \ref{proof_figure}) that begins by splitting on the split levels in reverse (decreasing) order of dimensions (see a 2D example in Figure \ref{proof_figure}). For this specific example, TRA will require
    \[\sum_{i=1}^m s_i \prod_{j=1}^{i-1} (s_j + 1) + \prod_{j=1}^m (s_j + 1)\]
    queries, whereas the optimal offline algorithm only needs \( 2n + 1 \) queries.
    
    Therefore, by the definition of competitive ratio:
    \[
    C_{TRA}^{(n,m)} \geq \frac{\sum_{i=1}^m s_i \prod_{j=1}^{i-1} (s_j + 1) + \prod_{j=1}^m (s_j + 1)}{2n + 1}
    \]
\end{enumerate}
     
    
    Hence, 
    \[C_{TRA}^{(n,m)} = \frac{\sum_{i=1}^m s_i \prod_{j=1}^{i-1} (s_j + 1) + \prod_{j=1}^m (s_j + 1)}{2n + 1}\]
     
    
    \begin{figure}[!ht]
        \centering
        % Minipage for the graph
        \begin{minipage}{0.4\textwidth}
            \centering
            \input{results/compet_ex.tikz}
        \end{minipage}
        \hfill
        % Minipage for the tree
        \begin{minipage}{0.4\textwidth}
            \centering
            \input{results/tree_proof.tikz}
        \end{minipage}
        \caption{Adversarial example for TRA. The classes are \colorbox{class1}{\classB},\colorbox{class2}{\textcolor{white}{\classA}} and \colorbox{class3}{\textcolor{white}{\classC}}. For simplicity, we denote \( s_1 = s^{(n)}_1 \) and \( s_2 = s^{(n)}_2 \). Here, the instance is dynamically built so that the number of queries required by TRA is \( s_2(s_1 + 1) + s_1 + (s_1 + 1)(s_2 + 1) \), whereas the optimal offline algorithm only needs \( 2n + 1 \), as shown in the right tree figure.}
        \label{proof_figure}
    \end{figure}

\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:DC_compet}]
    \textbf{Key Idea.} A pure divide-and-conquer (D\&C) algorithm discovers a split along a specific feature dimension upon querying a point. This split divides the input space into two subproblems. An adversary can strategically arrange the splits so that the feature splits detected by the algorithm early on are the ``least helpful'' ones, meaning they occur as the last decisions along their respective feature branches. By doing this, the adversary ensures that these initial splits do not simplify the identification of the remaining splits. We demonstrate that this construction forces the D\&C algorithm to perform poorly compared to an optimal strategy.
    
    We define a pure D\&C-based algorithm as one that divides the input space (problem) into subspaces (sub-problems) based on counterfactual explanations and recursively continues this process within each subspace until no counterfactuals are found. This class of algorithms encompasses all types of querying strategies, such as selecting the top-left corner, bottom-right corner, or a random point within the input space, among others.

    \textbf{Proof.}  Let \( m > 1 \) be the number of dimensions and \( n \geq m \) be the number of split levels. We prove this proposition by induction on the number of split levels \( n > 0 \).
    \begin{itemize}
        \item \textbf{Base Case} (\( n=2 \), \( m=2 \)): Consider a two-dimensional tree with split levels \( \alpha_1 \) and \( \alpha_2 \). Let \( q = (q_1, q_2) \) be the query made by the D\&C algorithm. The adversary chooses \( \alpha_1 = q_1 + \epsilon_1 \) and \( \alpha_2 = q_2 + \epsilon_2 \), where \( \epsilon_2 > \epsilon_1 > 0 \). Consequently, the first counterfactual explanation returned by the oracle is \( q' = (\alpha_1, q_2) \). The algorithm then splits the input space into two subspaces, both containing the split at \( \alpha_2 \), as depicted in Figure \ref{fig:treeDCproofBase}.
    
    \begin{figure}
        \centering
        \begin{subfigure}{0.4\textwidth}
            \centering
            \begin{tikzpicture}[
                sibling distance=5em,
                level distance=3em,
                every node/.style = {shape=rectangle, rounded corners, draw, align=center, top color=white},
                blue node/.style = {bottom color=class2!70, shape=circle},
                red node/.style = {bottom color=class1!70, shape=circle},
                green node/.style = {bottom color=class3!70, shape=circle}
            ]       
                \node {\(x_2 \leq \alpha_2\)}
                    child { node[xshift=-1em] {\(x_1 \leq \alpha_1\)} 
                        child {node[blue node] {\classA}}
                        child {node[red node, xshift=-0.3em] {\classB}}
                    }
                    child {node[green node] {\classC}};
            \end{tikzpicture}
            \caption{Adversarial decision tree.}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.4\textwidth}
            \centering
            \begin{tikzpicture}[scale=5]
                % Draw axes
                \draw[->] (0,0) -- (1.05,0) node[right] {$x_1$};
                \draw[->] (0,0) -- (0,1.05) node[above] {$x_2$};
            
                % Add split lines
                \draw[thick] (0,0.7) -- (1,0.7);
                \node at (-0.05,-0.05) {$0$};
                \node at (-0.05,1.0) {$1$};
                \node at (1.0,-0.06) {$1$};
                
                % Color regions
                \fill[class3!70] (0,0.7) rectangle (0.6,1);
                \fill[class1!70] (0.6,0) rectangle (1,0.7);
                \fill[class2!70] (0,0) rectangle (0.6,0.7);
                \fill[class3!70] (0.6,0.7) rectangle (1,1);
            
                % Draw vertical split
                \draw[thick] (0.6,0) -- (0.6,1);
                \draw (-0.02,0.7) -- (0.02,0.7) node[left, xshift=-0.1cm] {$\alpha_2$};
                \draw (-0.02,1.0) -- (0.02,1.0);
                \draw (0.6,-0.02) -- (0.6,0.02) node[below, yshift=-0.1cm] {$\alpha_1$};
                \draw (1.0,-0.02) -- (1.0,0.02);
            \end{tikzpicture}
            \caption{Decision Boundary of the adversarial decision tree.}
        \end{subfigure}
        \caption{An adversarial example for \( n=2 \), \( m=2 \), triggering the worst-case competitive ratio of our algorithm.}
        \label{fig:treeDCproofBase}
    \end{figure}
    
    In this adversarial example, the D\&C algorithm requires at least 7 queries to reconstruct the exact decision tree, whereas an optimal algorithm can achieve this with only 5 queries. Therefore, for this adversarial example, no D\&C-based algorithm can attain a competitive ratio better than \( C_{TRA}^{n,m} = C_{TRA}^{2,2} = \frac{7}{5} \).
    
    
    
    \item \textbf{Inductive Step}: Assume the proposition holds for all trees with up to \( n \) split levels. Consider a tree with \( n+1 \) split levels, with split levels \( (\alpha_1, \ldots, \alpha_{n+1}) \). Let \( q = (q_1, q_2, \ldots, q_m) \in \mathcal{X} \) be the first query made by the D\&C algorithm. The adversary sets \( \alpha_1 = q_1 + \epsilon_1 \) where \( \epsilon_1 > 0 \) and returns the counterfactual explanation \( q' = (\alpha_1, q_2, \ldots, q_m) \). The adversary places this split as the last decision node in the tree. Consequently, the D\&C algorithm splits the input space into two subspaces, each containing all splits of the remaining dimensions, thereby containing at most \( n \) splits each.
    
    By the induction hypothesis, the algorithm will require at least:
    \[
    \mathcal{Q}_1 = s_1^{(1)} + \sum_{i=2}^m s_i (s_1^{(1)} + 1) \prod_{j=2}^{i-1} (s_j + 1) + (s_1^{(1)} + 1) \prod_{j=2}^m (s_j + 1))
    \]
    for the first subspace, and
    \[
    \mathcal{Q}_2 = s_1^{(2)} + \sum_{i=2}^m s_i (s_1^{(2)} + 1) \prod_{j=2}^{i-1} (s_j + 1) + (s_1^{(2)} + 1) \prod_{j=2}^m (s_j + 1))
    \]
    for the second subspace, where \( s_1^{(1)} \) and \( s_1^{(2)} \) are the remaining splits along the first dimension in the first and second subspaces, respectively.
    
    Therefore, the total number of queries is:
    \[
    1 + \mathcal{Q}_1 + \mathcal{Q}_2 = (2n + 1) C_{TRA}^{(n,m)}
    \]
    given that \( s_1^{(1)} + s_1^{(2)} + 1 = s_1 \). Hence, the best competitive ratio $C^{(n,m)}_{D\&C}$ achievable by any D\&C-based algorithm satisfies:
    \[
    C^{(n,m)}_{D\&C} \geq \frac{(2n + 1) C_{TRA}^{(n,m)}}{2n + 1} = C_{TRA}^{(n,m)}
    \]
    \end{itemize}
\end{proof}


\section{Additional Experimental Results}

\subsection{Experimental Setup Details}
\label{appendix:training_details}
% All datasets results

\paragraph{Target Model Training.} During the training process of both decision trees and random forests, we conduct a grid search with 50 steps over the range $[0, 0.2]$ to determine the optimal cost-complexity pruning parameter \texttt{ccp\_alpha} that maximizes accuracy on the validation dataset.

\paragraph{Surrogate Model Training.} For surrogate models that do not utilize the target model's hyperparameters, we employ the default parameters provided by scikit-learn. Specifically for MLPs, we configure a scikit-learn MLP with two hidden layers, each consisting of 20 neurons, while keeping all other parameters at their default values.

\paragraph{Anytime Fidelity.} The anytime fidelity was calculated each 20 queries during all attacks execution, except for \PathFinding{} which is not an anytime attack.  

\subsection{Configuration of Surrogate-Based Attacks}\label{appendix:surrogate_based_attacks}

We report in Figure~\ref{fig:CF_full} (respectively, Figure~\ref{fig:DualCF_full}) the anytime performance of the \CF{} (respectively, \DualCF{}) model extraction attack against decision tree models, for the three considered types of surrogates and the two counterfactual oracles, on all considered datasets.
More precisely, as depicted in Section~\ref{sec:exp_setup}, we run these two attacks using three different assumptions on adversarial knowledge, namely the hypothesis class of the target model, its hyperparameters, and none of them. In the first case, the adversary trains a decision tree surrogate with default parameters (DT). In the second case, he trains a surrogate decision tree with the exact same hyperparameters as the target model (DT+). Finally, in the third case, a multi-layer perceptron (MLP) is used as surrogate model. 
Both \CF{} and \DualCF{} were originally tested using the DiCE~\citep{Mothilal_2020} counterfactual oracle, which provides heuristic-based (non-optimal) counterfactual explanations. To assess the impact of explanation optimality on the attack’s performance, and to ensure fair comparisons, we run \CF{} and \DualCF{} both using DiCE and using optimal counterfactual explanations computed with the OCEAN framework~\citep{parmentier2021optimal}.

We also report in
Figure~\ref{fig:CF_DualCF_RFs_full} the anytime performance of the \CF{} and \DualCF{} model extraction attacks against random forest models, for the three considered types of surrogates and the two counterfactual oracles, on the COMPAS dataset.

We hereafter highlight the key trends of these results, on two dimensions: the adversarial knowledge (regarding the target model's architecture) and the type of counterfactual oracle.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/CF/CF_Adult.pdf}
         \caption{Adult dataset}
         \label{fig:CF_adult}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/CF/CF_COMPAS.pdf}
         \caption{COMPAS dataset}
         \label{fig:CF_compas}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/CF/CF_CreditCard.pdf}
         \caption{Credit Card dataset}
         \label{fig:CF_CC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/CF/CF_GCredit.pdf}
         \caption{German Credit dataset}
         \label{fig:CF_GC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/CF/CF_SPerformanceMAT.pdf}
         \caption{Student Performance dataset}
         \label{fig:CF_SP}
     \end{subfigure}
     
     \hspace{10pt}
     
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/CF/legend.pdf}
         \label{fig:CF_legend}
     \end{subfigure}
        \caption{Anytime performance of the \CF{} model extraction attack against decision trees. We report results for all datasets and studied configurations, including adversarial knowledge regarding the target model architecture (DT, DT+, and MLP) and counterfactual oracles (DiCE and OCEAN).}
        \label{fig:CF_full}
\end{figure}
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/DualCF/DualCF_Adult.pdf}
         \caption{Adult dataset}
         \label{fig:DualCF_adult}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/DualCF/DualCF_COMPAS.pdf}
         \caption{COMPAS dataset}
         \label{fig:DualCF_compas}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/DualCF/DualCF_CreditCard.pdf}
         \caption{Credit Card dataset}
         \label{fig:DualCF_CC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/DualCF/DualCF_GCredit.pdf}
         \caption{German Credit dataset}
         \label{fig:DualCF_GC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/DualCF/DualCF_SPerformanceMAT.pdf}
         \caption{Student Performance dataset}
         \label{fig:DualCF_SP}
     \end{subfigure}
     
     \hspace{10pt}
     
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/DualCF/legend.pdf}
         \label{fig:DualCF_legend}
     \end{subfigure}
        \caption{Anytime performance of the \DualCF{} model extraction attack against decision trees. We report results for all datasets and studied configurations, including adversarial knowledge regarding the target model architecture (DT, DT+, and MLP) and counterfactual oracles (DiCE and OCEAN).}
    \label{fig:DualCF_full}
\end{figure}


%FIGURE (RF)
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.49\linewidth]{figs/RF_CF.pdf}
    \includegraphics[width=0.49\linewidth]{figs/RF_DualCF.pdf}
    \caption{Anytime performance of the \CF{} (left) and \DualCF{} (right) model extraction attacks against random forests. We report results for the COMPAS dataset and all studied configurations, including adversarial knowledge regarding the target model architecture (DT, DT+, and MLP) and counterfactual oracles (DiCE and OCEAN).}
    \label{fig:CF_DualCF_RFs_full}
\end{figure}
% DualCF et CF : DT - DT+ - MLP 

\paragraph{Knowledge of Target Model Architecture and Hyperparameters.} One first important trend that is consistent across both \CF{} and \DualCF{}, and for both decision trees and random forests, is that knowledge of the hypothesis class of the target model helps fitting a surrogate with high fidelity. Indeed, as can be observed in Figures~\ref{fig:CF_full}, \ref{fig:DualCF_full} and \ref{fig:CF_DualCF_RFs_full}, the MLP surrogate always under-performs compared to the decision trees or random forests ones. Indeed, fitting a surrogate model of the same type is facilitated by the fact that the shapes of its decision boundary are the same as the target model, e.g., axis-parallel splits for tree-based models. Interestingly, knowledge of the hyperparameters of the target decision tree or random forests does not seem to help fitting the surrogate. Indeed, in most experiments, the surrogate sharing the same hypothesis class as the target model (i.e., DT or RF) and the surrogate sharing both the hypothesis class and the hyperparameters (i.e., DT+ or RF+) have very close performances. Furthermore, imposing the target model's hyperparameters to the trained surrogate can even be counterproductive, as can be seen in Figures~\ref{fig:CF_adult} and~\ref{fig:CF_compas} for instance. In such cases, the fact that surrogate learning is more constrained due to the enforced hyperparameters (e.g., maximum depth) seems to slow its convergence towards very high fidelity values. This is consistent with previous findings: for instance, \citet{aivodji2020model} observed that knowledge of the architecture of a target neural network did not provide a significant advantage to the model extraction attacker.

\paragraph{Optimality of Counterfactuals.} The results in Figures~\ref{fig:CF_full}, \ref{fig:DualCF_full} and \ref{fig:CF_DualCF_RFs_full} suggest that the non-optimal counterfactual examples returned by DiCE helped fitting the extracted surrogate models better than the optimal ones provided by OCEAN. Indeed, for a fixed query budget and surrogate type, the performances of the extracted model are often better when fitted with DiCE counterfactuals than with OCEAN ones. Although some variations appear, this finding is generally verified for all the studied types of surrogates, for both decision trees and random forests target models, and for both the \CF{} and \DualCF{} extraction attacks. Intuitively, this can be attributed to a greater diversity in the non-optimal counterfactuals, which do not necessarily lie close to a decision boundary, unlike optimal ones. This also highlights a crucial insight: optimal counterfactuals only give an advantage to a model extraction attack if the attack is able to leverage the information it carries as a whole (including both the counterfactual example and its optimality) through a structured approach, as demonstrated by TRA.

% Dice vs OCEAN
\newpage
\subsection{Test Set Fidelity Results}
\label{appendix:fidelity_test_set}

The results provided in Section~\ref{sec:expes_results} measure fidelity on a dataset uniformly sampled over the input space, which accurately quantifies how well the extracted models fit the decision boundaries of the target ones over the whole input space. Another approach consists in evaluating fidelity on a test set. In such cases, the results indicate how well the extracted models mimic the target one for examples drawn from the actual data distribution. We report in Table~\ref{tab:results_test_set_fidelity} the results of our extraction attacks against random forests. More precisely, for random forests of varying sizes, we report the average fidelity (measured on the uniformly sampled dataset or on a test set) achieved by all considered methods along with the required number of queries. For \DualCF{} and \CF{}, these values are arbitrarily fixed to allow their surrogates to converge towards (near) perfect fidelity. For our proposed TRA, functional equivalence is achieved using the reported number of queries, hence fidelity on both considered datasets is always $1.0$. For \CF{} and \DualCF{}, we report results for the random forest surrogate using default parameters, for both studied counterfactual oracles. Indeed, we observed in Section~\ref{appendix:surrogate_based_attacks} that DiCE counterfactuals led to better anytime performances (in terms of uniform dataset fidelity) than OCEAN ones within the \CF{} model extraction attack, in most experiments. However, this is not always the case, with a few setups in which the difference between both approaches becomes very small or shifts in favor of the runs using OCEAN counterfactuals after sufficiently many iterations. This is the case on the COMPAS dataset (Figure~\ref{fig:CF_compas}), and although the difference remains very subtle, it is also visible on the test set fidelity, illustrating the fact that the two values are often very well aligned. 

Overall, the superiority of TRA is clear, both in terms of (uniform or test) fidelity and in terms of required numbers of queries, confirming the observation of Section~\ref{sec:expes_results}.


\begin{table}[htb]
    \caption{Summary of our model extraction experiments against random forests, on the COMPAS dataset. For random forests with varying numbers of trees, we report their total number of nodes and the average performances of the different considered model extraction attacks. FU and FTD denote respectively the Fidelity over the Uniform and Test Data.}
    \label{tab:results_test_set_fidelity}
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lll|lcc|lcc|lcc|lcc|lcc}
    \toprule
    & & & \multicolumn{3}{c|}{TRA} & \multicolumn{6}{c|}{DualCF} & \multicolumn{6}{c}{CF} \\
    \cline{7-18}
    & & & \multicolumn{3}{c|}{} &  \multicolumn{6}{c|}{RF} &  \multicolumn{6}{c}{RF}  \\
    \cline{4-18}
    & & & \multicolumn{3}{c|}{OCEAN}  & \multicolumn{3}{c|}{DiCE} & \multicolumn{3}{c|}{OCEAN} & \multicolumn{3}{c|}{DiCE} & \multicolumn{3}{c}{OCEAN}  \\
    \cline{4-18}
    Dataset & \#Trees & Nodes &  \#Queries & FU & FTD & \#Queries & FU & FTD & \#Queries & FU & FTD & \#Queries & FU & FTD & \#Queries & FU & FTD \\
    \hline \multirow{4}{*}{COMPAS}  & 5 & 486.60 & 73.60 & 1.00 & 1.00 & 3000 & 1.00 & 0.99 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 \\
     & 25 & 4569.00 & 138.80 & 1.00 & 1.00 & 3000 & 0.99 & 0.98 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 \\
     & 50 & 9151.20 & 147.60 & 1.00 & 1.00 & 3000 & 0.99 & 0.98 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 \\
     & 75 & 5691.33 & 95.20 & 1.00 & 1.00 & 3000 & 1.00 & 0.99 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 \\
     & 100 & 6855.07 & 129.60 & 1.00 & 1.00 & 3000 & 1.00 & 0.98 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 & 3000 & 1.00 & 1.00 \\
    \bottomrule
    \end{tabular}}
\end{table}


\clearpage 

\subsection{Detailed Experimental Results}\label{appendix:all_results}

We hereafter report all the results of our main experiments over the six considered datasets. 

First, Figure~\ref{fig:MFvsQ_full} provides the anytime performances (in terms of average surrogate fidelity as a function of the number of performed queries) of the four considered model extraction attacks when applied on decision tree target models. The findings highlighted in Section~\ref{sec:expes_results} are consistent across all considered datasets: TRA exhibits higher anytime fidelity than \CF{} and \DualCF{} for all query budgets, while also providing functional equivalence guarantees. While \PathFinding{} also provides these guarantees, it necessitates orders of magnitudes more queries to fit its surrogate.

Figure~\ref{fig:QvsN_full} focuses on functionally equivalent model extraction attacks, and relates the number of queries they require to fully recover the target model to its size (quantified as its number of nodes). The logarithmic scale of the y-axis highlights that TRA usually requires orders of magnitudes less queries than \PathFinding{} to entirely extract a given decision tree.
Interestingly, this trend is more subtle when reconstructing large trees trained on the datasets with the highest numbers of features (i.e., Adult and SPerformance).

Finally, Figure~\ref{fig:tra_queries_rf_size} reports the number of counterfactual queries required by TRA to conduct a functionally equivalent extraction of random forests of various sizes, as a function of the total number of nodes to be recovered within the target forest. 
Importantly, as quantified through the performed power-law regression, the number of queries required by TRA to entirely extract the target forests grows sub-linearly -- in $\Theta(\text{\#Nodes}^{0.38})$ -- with the total number of nodes to be retrieved. This empirically demonstrates the very good scalability of TRA with respect to the size of the target random forests. Note that this behavior arises because large forests with many trees introduce redundancies, allowing the extracted model to be represented with perfect fidelity as a more compact decision tree \citep{vidal2020bornagaintreeensembles}. 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MFvsQ/MFvsQ_Adult.pdf}
         \caption{Adult dataset}
         \label{fig:MFvsQ_adult}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MFvsQ/MFvsQ_COMPAS.pdf}
         \caption{COMPAS dataset}
         \label{fig:MFvsQ_compas}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MFvsQ/MFvsQ_CreditCard.pdf}
         \caption{Credit Card dataset}
         \label{fig:MFvsQ_CC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MFvsQ/MFvsQ_GCredit.pdf}
         \caption{German Credit dataset}
         \label{fig:MFvsQ_GC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/MFvsQ/MFvsQ_SPerformanceMAT.pdf}
         \caption{Student Performance dataset}
         \label{fig:MFvsQ_SP}
     \end{subfigure}
     
     \vspace{10pt}
     \begin{subfigure}[b]{0.7\textwidth}
         \centering     
         \includegraphics[width=0.7\textwidth]{figs/MFvsQ/legend.pdf}
         \label{fig:MFvsQ_legend}
     \end{subfigure}
     
        \caption{Anytime performance of all the considered model extraction attacks against decision trees. We report results for all datasets and retain the best configuration for the surrogate-based attacks \CF{} and \DualCF{}.}
    \label{fig:MFvsQ_full}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/QvsN/QvsN_Adult.pdf}
         \caption{Adult dataset}
         \label{fig:QvsN_adult}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/QvsN/QvsN_COMPAS.pdf}
         \caption{COMPAS dataset}
         \label{fig:QvsN_compas}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/QvsN/QvsN_CreditCard.pdf}
         \caption{Credit Card dataset}
         \label{fig:QvsN_CC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/QvsN/QvsN_GCredit.pdf}
         \caption{German Credit dataset}
         \label{fig:QvsN_GC}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/QvsN/QvsN_SPerformanceMAT.pdf}
         \caption{Student Performance dataset}
         \label{fig:QvsN_SP}
     \end{subfigure}
     
     \vspace{10pt}
     \begin{subfigure}[b]{0.7\textwidth}
         \centering     
         \includegraphics[width=0.4\textwidth]{figs/QvsN/legend.pdf}
         \label{fig:QvsN_legend}
     \end{subfigure}
     
        \caption{Performance of the \PathFinding{} and TRA functionally equivalent model extraction attacks against decision trees. We report results for all datasets where each point represents the number of queries needed to fully reconstruct the trees.}
    \label{fig:QvsN_full}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figs/RF_QvsN_Estimators.pdf}
    \caption{Number of queries required by TRA to perform a functionally equivalent extraction of target random forests of various sizes on the COMPAS dataset, as a function of the total number of nodes to be reconstructed within the target forest. As illustrated through the performed power-law regression, the number of required queries grows sub-linearly -- in $\Theta(\text{\#Nodes}^{0.38})$ -- with the total number of nodes to be retrieved, suggesting good scalability of the extraction attack with respect to the forest size.
}
    \label{fig:tra_queries_rf_size}
\end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
