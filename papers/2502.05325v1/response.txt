The flourishing literature on privacy in machine learning encompasses a wide variety of inference attacks, each considering different setups and objectives**Srivastava et al., "Model Invariant Attacks"**. 
This paper focuses on model extraction attacks**Fredrikson et al., "Model Inversion Attacks"**, which aim at reconstructing the decision boundary of a black-box target model as accurately as possible, given a prediction API. As highlighted in recent surveys**Shokri et al., "Membership Inference Attacks"**, numerous attacks have been proposed in recent years, targeting a broad spectrum of hypothesis classes. Hereafter, we focus on those specifically targeting tree-based models or exploiting counterfactual explanations. 

**Fredrikson et al. propose a functionally equivalent model extraction attack specifically targeting regression or decision trees: \PathFinding{}. Their approach assumes that each query reply contains a unique identifier for the associated leaf. In a nutshell, \PathFinding{} aims at identifying the decision boundaries of each leaf in the target tree by varying the values of each feature. While effective, this method requires a large number of queries, though partial input queries can sometimes mitigate this overhead.

While many recent have focused on generating counterfactual explanations**Ross et al., "Counterfactual Explanations"**, they have also been shown to facilitate privacy attacks**Carlini et al., "Model Extraction Attacks"**. **Jagielski et al. introduce \CF{}, a model extraction attack that leverages counterfactual explanations. Their approach constructs a labeled dataset by querying both predictions and counterfactuals from the target model, which is then used to train a surrogate.  extend this method with \DualCF{}, which improves fidelity by additionally querying the counterfactuals of the counterfactual explanations.
**Carlini et al. employ polytope theory to show that a sufficient number of optimal counterfactual explanations can approximate convex decision boundaries. They propose a model extraction attack against locally Lipschitz continuous models, with fidelity guarantees dependent on the Lipschitz constants of the target and surrogate models. However, functional equivalence cannot be strictly certified, and as the authors acknowledge, these guarantees do not apply to decision tree-based models, which lack local Lipschitz continuity and convexity.
Finally, while beyond the scope of this paper, other explanation-based model extraction attacks have been explored, including those relying on gradient-based**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** and other feature-based methods**Sinha et al., "Stealthy Attacks against Black-Box Machine Learning Models"**.