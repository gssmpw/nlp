\section{Related Works}
The flourishing literature on privacy in machine learning encompasses a wide variety of inference attacks, each considering different setups and objectives~\citep{DBLP:journals/corr/abs-2005-08679,DBLP:journals/corr/abs-2007-07646}. 
This paper focuses on model extraction attacks~\citep{Florian2016StealingMachineLearningModels}, which aim at reconstructing the decision boundary of a black-box target model as accurately as possible, given a prediction API. As highlighted in recent surveys~\cite{DBLP:journals/cm/GongWCYJ20,DBLP:journals/csur/OliynykMR23}, numerous attacks have been proposed in recent years, targeting a broad spectrum of hypothesis classes. Hereafter, we focus on those specifically targeting tree-based models or exploiting counterfactual explanations. 

\citet{Florian2016StealingMachineLearningModels} propose a functionally equivalent model extraction attack specifically targeting regression or decision trees: \PathFinding{}. Their approach assumes that each query reply contains a unique identifier for the associated leaf. In a nutshell, \PathFinding{} aims at identifying the decision boundaries of each leaf in the target tree by varying the values of each feature. While effective, this method requires a large number of queries, though partial input queries can sometimes mitigate this overhead.

While many recent have focused on generating counterfactual explanations~\citep{DBLP:journals/datamine/Guidotti24}, they have also been shown to facilitate privacy attacks~\citep{DBLP:conf/aistats/PawelczykLN23}. \citet{aivodji2020model} introduce \CF{}, a model extraction attack that leverages counterfactual explanations. Their approach constructs a labeled dataset by querying both predictions and counterfactuals from the target model, which is then used to train a surrogate. \citet{wang2022dualcf} extend this method with \DualCF{}, which improves fidelity by additionally querying the counterfactuals of the counterfactual explanations.
\citet{dissanayake2024model} employ polytope theory to show that a sufficient number of optimal counterfactual explanations can approximate convex decision boundaries. They propose a model extraction attack against locally Lipschitz continuous models, with fidelity guarantees dependent on the Lipschitz constants of the target and surrogate models. However, functional equivalence cannot be strictly certified, and as the authors acknowledge, these guarantees do not apply to decision tree-based models, which lack local Lipschitz continuity and convexity.
Finally, while beyond the scope of this paper, other explanation-based model extraction attacks have been explored, including those relying on gradient-based~\citep{DBLP:conf/fat/MilliSDH19,DBLP:conf/sectl/MiuraSY24} and other feature-based methods~\citep{oksuz2023autolycus}.