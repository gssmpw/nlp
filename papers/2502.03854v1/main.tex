
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage[pagebackref]{hyperref}
\hypersetup{%
  setpagesize=false,%
  bookmarks=true,%
  % bookmarksdepth=tocdepth,%
  bookmarksnumbered=true,%
  colorlinks=true,%
  citecolor=teal,
  pdftitle={},%
  pdfsubject={},%
  pdfauthor={},%
  pdfkeywords={}
}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{preprint_from_iclr2025_conference}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[textsize=tiny]{todonotes}

\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{aligned-overset}
\usepackage{notations}



\title{
  Mirror Descent Actor Critic via \\
  Bounded Advantage Learning
  % Mirror Descent Actor Critic via Bounded Advantage Learning
}
\author{%
  Ryo Iwaki \\
  IBM Research - Tokyo \\
  \texttt{Ryo.Iwaki@ibm.com} \\
}


\begin{document}

\maketitle

\vspace{-1.5\baselineskip}

\begin{abstract}
Regularization is a core component of recent Reinforcement Learning (RL) algorithms. Mirror Descent Value Iteration (MDVI) uses both Kullback-Leibler divergence and entropy as regularizers in its value and policy updates. Despite its empirical success in discrete action domains and strong theoretical guarantees, the performance of a MDVI-based method does not surpass an entropy-only-regularized method in continuous action domains. In this study, we propose Mirror Descent Actor Critic (MDAC) as an actor-critic style instantiation of MDVI for continuous action domains, and show that its empirical performance is significantly boosted by bounding the actor's log-density terms in the critic's loss function, compared to a non-bounded naive instantiation. Further, we relate MDAC to Advantage Learning by recalling that the actor's log-probability is equal to the regularized advantage function in tabular cases, and theoretically discuss when and why bounding the advantage terms is validated and beneficial. We also empirically explore a good choice for the bounding function, and show that MDAC perfoms better than strong non-regularized and entropy-only-regularized methods with an appropriate choice of the bounding function.
\end{abstract}

\vspace{-0.5\baselineskip}


\section{Introduction}

Model-free reinforcement learning (RL) is a promising approach to
obtain reasonable controllers in unknown environments.
In particular, actor-critic (AC) methods are appealing
because they can be naturally applied to continuous control domains.
AC algorithms have been applied in a range of challenging domains
including robot control \citep{Smith2023},
% magnetic control of tokamak plasmas \citep{Degrave2022},
tokamak plasma control \citep{Degrave2022},
and alignment of large language models \citep{Stiennon2020}.

Regularization is a core component of,
not only such AC methods,
but also value-based reinforcement learning algorithms \citep{Peters2010,Azar2012,Schulman2015,Schulman2017,Haarnoja2017,Haarnoja2018a,Abdolmaleki2018}.
Kullback-Leibler (KL) divergence and entropy are two major regularizers
that have been adopted to derive many successful algorithms.
In particular, Mirror Descent Value Iteration (MDVI) uses both KL divergence
and entropy as regularizers in its value and policy updates \citep{Geist2019,Vieillard2020a}
and enjoys strong theoretical guarantees \citep{Vieillard2020a,Kozuno2022}.
However, despite its empirical success in discrete action domains \citep{Vieillard2020b},
the performance of a MDVI-based algorithm does not surpass an entropy-only-regularized method
in continuous action domains \citep{Vieillard2022}.

In this study, we propose Mirror Descent Actor Critic (MDAC)
as a model-free actor-critic instantiation of MDVI for continuous action domains,
and show that its empirical performance is significantly boosted
by bounding the actor's log-density terms in the critic's loss function, compared to a non-bounded naive instantiation.
To understand the impact of bounding beyond just as an ``implementation detail",
we relate MDAC to Advantage Learning (AL) \citep{Baird1999,Bellemare2016}
by recalling that the policy's log-probability is equal to
the regularized soft advantage function in tabular case,
and theoretically discuss when and why bounding the advantage terms is validated and beneficial.
Our analysis indicates that it is beneficial to bound the log-policy term of
not only the current state-action pair but also the successor pair in the TD target signal.


\paragraph{Related Works.}

The key component of our actor-critic algorithm is to bound the log-policy terms in the critic loss,
which can be also understood as bounding the regularized advantages.
Munchausen RL clips the log-policy term for the current state-action pair,
which serves as an augumented reward, as an implementation issue \citep{Vieillard2020b}.
Our analysis further supports the empirical suceess of Munchausen algorithms.
\citet{Zhang2022} extended AL by introducing a clipping strategy,
which increases the action gap
only when the action values of suboptimal actions exceed a certain threshold.
Our bounding strategy is different from theirs in the way that
the action gap is increased for all state-action pairs but with bounded amounts.
\citet{Vieillard2022} proposed a sound parameterization of Q-function that uses log-policy.
By consruction, the regularized greedy step of MDVI can be performed exactly
even in actor-critic settings with their parameterization.
Our study is orthogonal to theirs since our approach modifies
not the parameterization of the critic but its loss function.

MDVI and its variants are instances of mirror descent (MD) based RL.
There are substantial research efforts in this direction
\citep{Wang2019,Vaswani2022,Kuba2022,Yang2022,Tomar2022,Lan2023,Alfano2023}.
The MD perspective enables to
understand the existing algorithms in a unified view,
analyze such methods with strong theoretical tools,
and propose a novel and superior one.
Further discussion on MD based methods are provided in Appendix \ref{ss:appendix:md_methods}.
This paper focuses on a specific choice of mirror, i.e. adopting KL divegence and entropy as regularizers,
and provides a deeper understanding in this specific scope
via a notion of \emph{gap-increasing} operators.

It is well known that the log-policy terms in AC algorithms often cause instability,
since the magnitude of log-policy terms grow large naturally in MDP,
where a deterministic policy is optimal.
Recent RL implementations handle this problem by bounding the range of
the standard deviation for Gaussian policies \citep{SpinningUp2018,Huang2022CleanRL}.
Beyond such an implementation detail,
\citet{Silver2014} proposed to use deterministic policy gradient,
which is a foundation of the recent actor-critic algorithms such as TD3 \citep{Fujimoto2018}.
\citet{Iwaki2019} proposed an implicit iteration method
to stably estimate the natural policy gradient \citep{Kakade2001_NPG},
which also can be viewd as a MD-based RL method \citep{Thomas2013}.



\paragraph{Contibutions.}

Our contributions are summarized as follows:
(1) we proposed MDAC, a model-free actor-critic instantiation of MDVI for continuous action domains,
and
showed that its empirical performance is significantly boosted by bounding the actor's log-density terms in the critic's loss function, compared to a non-bounded naive instantiation.
(2) We theoretically analyzed the validity and the effectivness of the bounding strategy by relating MDAC to AL with bouded advantage terms.
To be specific,
(2-1) we provided sufficient conditions under which the bounding strategy results in asymptotic convergence, which also suggests that Munchausen RL is convergent even when the ad-hoc clipping is employed,
and (2-1) we showed that the bounding strategy reduces \emph{inherent errors} of gap-increasing Bellman operators.
(3) We empirically explored what types of bounding functions are effective.
(4) We demonstrated that MDAC performs better than strong non-regularized and entropy-only-regularized baseline methods in simulated benchmarks.


% \newpage
\section{Preliminary}

\paragraph{MDP and Approximate Value Iteration.}

A Markov Decision Process (MDP) is specified by a tuple $(\cS,\cA,P,R,\gamma)$, where
$\cS$ is a state space,
$\cA$ is an action space,
$P$ is a Markovian transition kernel,
$R$ is a reward function bounded by $\Rmax$, and
$\gamma\in(0,1)$ is a discount factor.
For $\cEnt\ge 0$, we write $V^\cEnt_{\rm max} = \frac{\Rmax + \cEnt\log|\cA|}{1-\gamma}$
(assuming $\cA$ is finite)
and $V_{\rm max} = V^{0}_{\rm max}$.
We write $\one\in\bR^{\cS\times\cA}$ the vector whose components are all equal to one.
A policy $\pi$ is a distribution over actions given a state.
Let $\Pi$ denote a set of Markovian policies.
The state-action value function associated with a policy $\pi$ is defined as
$%
    Q^\pi(s,a) = \bE_\pi \left[ \sum_{t=0}^\infty \gamma^t R(S_t,A_t)\middle \sep S_{0}=s, A_{0}=a\right]
$, %
where $\bE_\pi$ is the expectation over trajectories generated under $\pi$.
An optimal policy satisfies $\pi^{*} \in \argmax_{\pi\in\Pi} Q^\pi$
with the understanding that operators are point-wise, and $Q^{*}=Q^{\pi^{*}}$.
For $f_1,f_2\in\bR^{\cS\times\cA}$, we define a component-wise dot product
$%
    \langle f_1,f_2 \rangle = \left(\sum_{a} f_1(s,a) f_2(s,a)\right)_{s}\in\bR^\cS
$. %
Let $P_\pi$ denote the stochastic kernel induced by $\pi$.
For $Q\in\mathbb{R}^{\cS\times\cA}$, let us define
$%
  P_\pi Q = \left(\sum_{s'}P(s'|s,a) \sum_{a'} \pi(a'|s') Q(s',a')\right)_{s,a}\in\bR^{\cS\times\cA}
$. %
Furthermore, for $V\in\bR^\cS$ let us define
$%
  P V = \left(\sum_{s'} P(s'|s,a) V(s')\right)_{s,a}\in\bR^{\cS\times\cA}
$ %
and
$%
  P^{\pi}V = \left(\sum_{a} \pi(a|s) \sum_{s'}P(s'|s,a) V(s')\right)_{s}\in\bR^{\cS}
$. %
It holds that $P_\pi Q = P\langle \pi, Q\rangle$.
The Bellman operator is defined as $\cT_\pi Q = R + \gamma P_\pi Q$, whose unique fixed point is $Q^\pi$.
The set of greedy policies w.r.t. $Q\in\bR^{\cS\times\cA}$ is written as
$\cG(Q) = \argmax_{\pi\in\Pi} \langle Q,\pi\rangle$.
Approximate Value Iteration (AVI) \citep{Bellman1959} is a classical approach
to estimate an optimal policy.
Let $Q_{0}\in\bR^{\cS\times\cA}$ be initialized as $\nInf{Q_{0}}\le V_{\rm max}$
and $\epsilon_{k} \in \bR^{\cS\times\cA}$ represent approximation/estimation errors.
Then, AVI can be written as the following abstract form:
\begin{align*}
  \begin{cases}
    \pi_{k+1} \in \cG(Q_{k})
    \\
    Q_{k+1} = \cT_{\pi_{k+1}} Q_{k} + \epsilon_{k+1}
  \end{cases}
  .
\end{align*}


% \clearpage
\paragraph{Regularized MDP and MDVI.}

In this study, we consider the Mirror Descent Value Iteration (MDVI) scheme \citep{Geist2019,Vieillard2020a}.
Let us define the entropy $\cH(\pi) = - \langle \pi,\log \pi\rangle \in\bR^\cS$ and
the KL divergence $\kl(\pi_1 \| \pi_2) = \langle \pi_1, \log \pi_1 - \log \pi_2 \rangle \in\bR^\cS_{\ge 0}$.
For $Q\in\bR^{\cS\times\cA}$ and a reference policy $\mu \in\Pi$,
we define the regularized greedy policy as
$%
    \cG_\mu^{\cKL,\cEnt}(Q)
    = \argmax_{\pi\in\Pi} \left(\langle \pi, Q\rangle + \cEnt \cH(\pi) - \cKL \kl(\pi\|\mu)\right)
$.
We write $\cG^{0,\cEnt}$ for $\cKL=0$ and $\cG^{0,0}(Q) = \cG(Q)$.
We define the soft state value function $V(s)\in\bR^\cS$ as
$
  V(s) = \langle \pi, Q\rangle + \cEnt \cH(\pi) - \cKL \kl(\pi\|\mu)
$, where $\pi=\cG_\mu^{\cKL,\cEnt}(Q)$.
Furthermore, we define the regularized Bellman operator as
$
  \cT_{\pi\sep\mu}^{\cKL,\cEnt} Q
  = R + \gamma P \left(\langle \pi, Q\rangle + \cEnt \cH(\pi) - \cKL \kl(\pi\|\mu)\right)
$.
Given these notations, MDVI scheme is defined as
\begin{align}
  \begin{cases}
    \pi_{k+1} = \cG_{\pi_k}^{\cKL,\cEnt}(Q_{k})
    \\
    Q_{k+1} = \cT^{\cKL,\cEnt}_{\pi_{k+1}\sep\pi_k} Q_{k} + \epsilon_{k+1}
  \end{cases}
  ,
  \label{eq:mdvi}
\end{align}
where $\pi_{0}$ is initialized as the uniform policy.

\citet{Vieillard2020b} proposed a reparameterization $\Psi_{k} = Q_{k} + \rcKL\rcEnt\log\pi_{k}$.
Then, defining
$\rcEnt = \cEnt + \cKL$ and $\rcKL = \cKL/(\cEnt + \cKL)$,
the recursion \eqref{eq:mdvi} can be rewritten as
\begin{align}
  \begin{cases}
    &\pi_{k+1} = \cG^{0,\rcEnt}(\Psi_{k})
    \\
    &\Psi_{k+1} = R + \gamma P \left\langle\pi_{k+1}, \Psi_{k} - \rcEnt\log\pi_{k+1} \right\rangle
    % \\
    % &\qquad\qquad
    + \, \rcKL\rcEnt\log\pi_{k+1} + \epsilon_{k+1}
  \end{cases}
  % ,
  .
  \label{eq:imdvi}
\end{align}
We refer \eqref{eq:imdvi} as Munchausen Value Iteration (M-VI).
In the recursion \eqref{eq:imdvi},
KL regularization is implicitly applied through $\Psi_{k}$ and
there is no need to store $\pi_{k}$ for explicit computation of the KL term.
Notice that the regularized greedy policy $\pi_{k+1} = \cG^{0,\rcEnt}(\Psi_{k})$ can be obtained analytically in discrete action spaces as
$
  \bigl(\cG^{0,\rcEnt}(\Psi_{k})\bigr)(s,a)
  = \frac{\exp \Psi_{k}(s,a)/\rcEnt}{\dprod{\one}{\exp \Psi_{k}(s,a)/\rcEnt}}
  \eqdef \bigl(\sm{\rcEnt}{\Psi_{k}}\bigr)(s,a)
$.



\section{Mirror Descent Actor Critic with Bounded Bonus Terms}
\label{ss:motivating_the_bounding_functions}

In this section,
we introduce a model-free actor-critic instantiation of MDVI for continuous action domains,
and show that a naive implementation results in poor performance.
Then, we demonstrate that its performance is improved significantly
by a simple modification to its loss function.

Now we derive Mirror Descent Actor Critic (MDAC).
Let $\pi_{\theta}$ be a tractable stochastic policy such as a Gaussian with a parameter $\theta$.
Let $Q_{\psi}$ be a value function with a parameter $\psi$.
The functions $\pi_{\theta}$ and $Q_{\psi}$ approximate $\pi_{k}$ and $\Psi_{k}$
in the recursion \eqref{eq:imdvi}, respectively.
Further, let $\bar{\psi}$ be a target parameter that is updated slowly, that is,
$\bar{\psi} \leftarrow (1-\polyak) \bar{\psi} + \polyak \psi$ with $\polyak\in(0,1)$.
% Now, we derive the losses for the actor $\pi_{\theta}$ and the critic $Q_{\psi}$.
Let $\cD$ be a replay buffer that stores past experiences $\{(s,a,r,s')\}$.
We can derive model-free and off-policy losses from the recursion \eqref{eq:imdvi}
for the actor $\pi_{\theta}$ and the critic $Q_{\psi}$
by (i) letting the parameterized policy $\pi_{\theta}$ be represent
the information projection of $\pi_{k}$ in terms of the KL divergence,
and (ii) approximating the expectations using the transition samples drawn from $\cD$:
\begin{align}
  L^{Q} \!\left( \psi \right)
  % &=
  % \!
  % \underset{\substack{(s,a,r,s^{\prime})\sim\cD,\\ a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})}}{\mathbb{E}}
  % \!
  % \Bigl[
  %   \bigl(
  %   \underbrace{
  %   r + \rcKL\rcEnt\log\pi_{\theta}(a|s) + \gamma \!\left( Q_{\bar{\psi}}(s^{\prime},a^{\prime}) \!-\! \rcEnt\log\pi_{\theta}(a^{\prime}|s^{\prime})\right)
  %   }_{y(s,a,r,s',a')}
  %   - Q_{\psi}(s,a)
  %   \bigr)^2
  % \Bigr]
  &=
  \!
  \underset{\substack{(s,a,r,s^{\prime})\sim\cD,\\ a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})}}{\mathbb{E}}
  \!
  \Bigl[
    \bigl(
    y - Q_{\psi}(s,a)
    \bigr)^2
  \Bigr]
  ,
  \label{eq:mdac:critic}
  \\
  y &= r + \rcKL\rcEnt\log\pi_{\theta}(a|s)
  % \nonumber\\
  % &\quad
  + \gamma \!\left( Q_{\bar{\psi}}(s^{\prime},a^{\prime}) \!-\! \rcEnt\log\pi_{\theta}(a^{\prime}|s^{\prime})\right)
  ,
  \label{eq:mdac:critic_target}
  \\
  L^{\pi} (\theta)
  % &= \mathbb{E}_{s\sim\cD}
  &= \underset{s\sim\cD}{\mathbb{E}}
  \Bigl[
    D_{\rm KL} \bigl(
      \pi_{\theta}(a|s) \bigm\|
      % \frac{1}{Z(s)} \exp \left(\frac{Q_{\psi}(s,a)}{\rcEnt} \right)
      \sm{\rcEnt}{Q_{\psi}}(s,a)
    \bigr)
  \Bigr]
  % \nonumber
  % \\
  % &
  = \underset{\substack{s\sim\cD,\\ a\sim\pi_{\theta}(\cdot|s)}}{\mathbb{E}}
  \Bigl[
      \rcEnt \log \pi_{\theta}(a|s) - Q_{\psi}(s,a)
  \Bigr]
  .
  \label{eq:mdac:actor}
\end{align}
Though $\pi_{\theta}$ can be any tractable distribution,
we choose commonly used Gaussian policy in this paper.
We lower-bound its standard deviation by a common hyperparameter $\log\sigma_{\rm min}$, which is typically fixed to
$\log\sigma_{\rm min}\!=\!-20$ \citep{Huang2022CleanRL}
or $\log\sigma_{\rm min}\!=\!-5$ \citep{SpinningUp2018}.
Although there are two hyperparameters $\rcEnt$ and $\rcKL$ originated from KL and entropy regularization,
these hyperparameters need not to be tuned manually.
We fixed $\rcKL=1-(1-\gamma)^2$ as the theory of MDVI suggests \citep{Kozuno2022}.
For $\rcEnt$, we perform an optimization process similar to SAC \citep{Haarnoja2018b}.
Noticing that the strength of the entropy regularization is governed by
$\cEnt = (1-\rcKL)\rcEnt$, we optimize the following loss in terms of $\rcEnt$
with $\bar{\cH}=-{\rm dim}(\cA)$:
\begin{align}
  % \vspace{-0.5cm}
  % $
  L(\rcEnt)
  % = \mathbb{E}_{\substack{s\sim\cD,\\ a\sim\pi_{\theta}(\cdot|s)}}
  % &= \underset{\substack{s\sim\cD,\\ a\sim\pi_{\theta}(\cdot|s)}} {\mathbb{E}}
  % \left[
  %   - (1-\rcKL)\rcEnt \log \pi_{\theta}(a|s) - (1-\rcKL)\rcEnt \bar{\cH}
  % \right]
  % \nonumber
  % \\
  % &=
  % (1-\rcKL)\rcEnt
  % \underset{s\sim\cD} {\mathbb{E}}
  % \left[
  %   \cH\left(\pi_{\theta}(\cdot|s)\right) - \bar{\cH}
  % \right]
  % \vspace{-0.5cm}
  &= (1-\rcKL)\rcEnt
  \underset{\substack{s\sim\cD,\\ a\sim\pi_{\theta}(\cdot|s)}} {\mathbb{E}}
  \left[
    - \log \pi_{\theta}(a|s) - \bar{\cH}
  \right]
  \label{eq:mdac:entropy}
  .
  % $
  % \vspace{-0.5cm}
\end{align}


\begin{figure*}[t]
 \begin{center}
   \includegraphics[width=0.45\linewidth]{"fig/tanh_works_w_lsm2_iqm_curve_mdac.v7_kappaFixed_gauss_None_mdac.v7_identity_lsm2_mdac.v7_kappaFixed_gauss_rsquash_naive_10.pdf"}
   \caption{
     Effect of bounding $\rcEnt\log\pi_{\theta}$ terms.
   }
   \label{fig:tanh_works}
 \end{center}
 \vspace{-\baselineskip}
\end{figure*}
The reader may notice that \eqref{eq:mdac:critic} and \eqref{eq:mdac:actor}
are nothing more than SAC losses \citep{Haarnoja2018a, Haarnoja2018b}
with the Munchausen augumented reward \citep{Vieillard2020b},
and expect that optimizing these losses results in good performance.
However, a naive implementation of these losses leads to poor performance.
The gray learning curve in Figure \ref{fig:tanh_works} is an aggregated learning result for 6 Mujoco environments with $\log\sigma_{\rm min}\!=\!-5$
\footnote{Details on the setup and the metrics can be found in Section \ref{ss:experiment}, and Figure \ref{fig:tanh_works_all} in Appendix \ref{ss:appendix:experimental_details:mujoco} shows the per-environment results.}.
The left column of Figure \ref{fig:scales} compares
the quantities in the loss functions
% the naive critc
% loss \eqref{eq:mdac:critic}
for the initial learning phase in %\texttt{Walker2d-v4} and
\texttt{HalfCheetah-v4}.
% and \texttt{Ant-v4}.
% For all the featured
Clearly, the magnitude of
% the log-density terms
$\log\pi_{\theta}$ terms
gets much larger than the reward quickly.
We hypothesized that the poor performance of the naive implementation is due to this scale difference; the information of the reward is erased by the bonus terms.
This explosion is more severe in the Munchausen bonus $\rcKL\rcEnt\log\pi_{\theta}(a|s)$ than
the entropy bonus $\rcEnt\log\pi_{\theta}(a^{\prime}|s^{\prime})$,
because while $a^{\prime}$ is an \emph{on-policy} sample from the current actor $\pi_{\theta}$, $a$ is an old \emph{off-policy} sample from the replay buffer $\cD$.
Careful readers may wonder if the larger $\log\sigma_{\rm min}$ resolves this issue.
The yellow learning curve in Figure \ref{fig:tanh_works} is the learning result for $\log\sigma_{\rm min}=-2$, which still fails to learn.
The middle column of Figure \ref{fig:scales} shows that the bonus terms are still divergent, and it is caused by the exploding behavior of $\rcEnt$.
A naive update of $\rcEnt$ using the loss \eqref{eq:mdac:entropy}
and SGD with a step-size $\rho>0$ is expressed as
\begin{align*}
 % \vspace{\baselineskip}
 % $
 \rcEnt \leftarrow \rcEnt
 + \frac{\rho (1-\rcKL)}{N} \sum_{n=1}^{N}
   \bigl(
     \log \pi_{\theta}(a_n|s_n) - {\rm dim}(\cA)
   \bigr)
 % $
 ,
 % \vspace{\baselineskip}
\end{align*}
% }
where
$N$ is a mini-batch size,
$s_n$ is a sampled state in a mini-batch
and
$a_n\sim\pi_{\theta}(\cdot|s_n)$.
This expression indicates that,
if the averages of $\log\pi_{\theta}(a|s)$ over the sampled mini-batches
are bigger than ${\rm dim}(\cA)$ over the iterations,
$\rcEnt$ keeps growing.
The bottom row of left and middle plots in Figure \ref{fig:scales} indicates that this phenomenon is indeed happening.
We argue that, an unstable behavior of a single component ruins the other learning components through the actor-critic structure.
Through the loss \eqref{eq:mdac:actor},
$\log\pi_{\theta}$ concentrates to high value, which makes $\rcEnt$ grow.
Then, $\rcEnt\log\pi_{\theta}$ terms explode and hinder $Q_\psi$, and $\log\pi_{\theta}$ stays ruined.

We found that ``bounding'' $\rcEnt\log\pi_{\theta}$ terms improves the performance significantly.
To be precise, by replacing the target
% $y(s,a,r,s',a')$
$y$
in the critic's loss \eqref{eq:mdac:critic} with
the following,
the agent succeeds to reach reasonable performance
(the green learning curve in Figure \ref{fig:tanh_works};
$\log\sigma_{\rm min}\!=\!-5$ is used):
\begin{align}
 % \begin{cases}
 %   % L^{Q}_{\rm sq} \left( \psi \right)
 % L^{Q} \left( \psi \right)
 % % &
 % =
 % \mathbb{E}_{\substack{(s,a,r,s^{\prime})\sim\cD,\\ a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})}} \Bigl[
 % \left(
 %   y(s,a,r,s',a') - Q_{\psi}(s,a)
 %   \right)^2
 % \Bigr]
 % % ,
 % % \label{eq:mdac:critic:tanh}
 % \\
 y%(s,a,r,s',a')
 &= r + \rcKL\dgreen{\tanh}\left(\rcEnt\log\pi_{\theta}(a|s)\right)
 % \nonumber
 % \\
 % &\quad
 + \gamma \left( Q_{\bar{\psi}}(s^{\prime},a^{\prime}) - \dgreen{\tanh}\left(\rcEnt\log\pi_{\theta}(a^{\prime}|s^{\prime})\right)\right)
 .
 \label{eq:mdac:critic:tanh}
\end{align}
The right column of Figure \ref{fig:scales} shows that
with this target \eqref{eq:mdac:critic:tanh},
$\rcEnt\log\pi_{\theta}$ terms do not explode since
$\log\pi_{\theta}$ does not concentrate to high value
and
$\rcEnt$ does not grow,
and $Q_\psi$ is not ruined.
In the next section, we analyze what happens under the hood by
theoretically investigating the effect of bounding $\rcEnt\log\pi_{\theta}$ terms.
% and show that
We argue that bounding $\rcEnt\log\pi_{\theta}$ terms is not just an ad-hoc implementation issue,
but it changes the property of the underlying Bellman operator.
We quantify the amount of ruin caused by $\rcEnt\log\pi_{\theta}$ terms,
and show how this negative effect is mitigated by the bounding.
% \vspace{-0.5\baselineskip}
\begin{figure*}[t]
 \begin{center}
   \includegraphics[width=0.99\linewidth]{"fig/joint_tidy_scales_identity_tanh_4HcQ_TD_ent_HalfCheetah-v4.pdf"}
   \caption{
     Scale comparison of the quantities in loss functions.
     The means of the quantities over the multiple sampled minibatchs are plotted.
     Left: $\log\sigma_{\rm min}\!=\!-5$,
     Middle: $\log\sigma_{\rm min}\!=\!-2$,
     Right: $\log\sigma_{\rm min}\!=\!-5$ with bounding by $\tanh$.
     Top: comparison in critic loss \eqref{eq:mdac:critic},
     Bottom: comparison in actor and entropy losses \eqref{eq:mdac:actor} and  \eqref{eq:mdac:entropy}.
     $\rcEnt$ is indicated by the right y-axis.
     Blue shaded areas indicate standard deviations.
     Light blue shaded areas indicate minimum and maximum values.
   }
   \label{fig:scales}
 \end{center}
 \vspace{-\baselineskip}
\end{figure*}
% \vspace{-0.5\baselineskip}



% \clearpage
\section{Analysis}

In this section, we theoretically investigate the properties of
the log-policy-bounded target \eqref{eq:mdac:critic:tanh} in tabular settings.
Rather than analyzing a specific choice of bounding, e.g. $\tanh(x)$,
we characterize the conditions for bounding functions that are validated and effective.
For the sake of analysis, we provide an abstract dynamic programming scheme of
the log-policy-bounded target \eqref{eq:mdac:critic:tanh} and
relate it to Advantage Learning \citep{Baird1999,Bellemare2016}
in Section \ref{ss:analysis:bal}.
In Section \ref{ss:analysis:convergence}, we show that
carefully chosen bounding function ensures asymptotically convergence.
In Section \ref{ss:analysis:epa}, we show that such bouding is indeed beneficial in terms of inherent error reduction property.
All the proofs will be found in Appendix \ref{ss:appendix:proofs}.


\clearpage
\subsection{Bounded Advantage Learning}
\label{ss:analysis:bal}


\begin{figure}[t]
% \begin{wrapfigure}{l}{0.45\linewidth}
 \vspace{-2\baselineskip}
 \centering
 \includegraphics[keepaspectratio, scale=0.4]{"fig/fg.pdf"}
 % \\[0.5\baselineskip
 % ]
 % \vspace{-0.5\baselineskip}
 % \parbox{\linewidth}{
 %   Examples of $f,g$.\label{fig:fg}}
  \caption{Examples of $f,g$.}
  \label{fig:fg}
 % \vspace{-2\baselineskip}
\end{figure}
% \end{wrapfigure}
Let $\bALc$ and $\bALs$ be non-decreasing functions over $\bR$ such that,
for both $h\in\{\bALc,\bALs\}$,
(i) $h(x) > 0$ for $x > 0$, $h(x) < 0$ for $x < 0$ and $h(0)=0$,
(ii) $x-h(x)\ge 0$ for $x\ge0$ and $x-h(x)\le 0$ for $x\le0$, and
% (iii) their codomains are connected subsets of $[-c_{\bALc},c_{\bALc}]$ and $[-c_{\bALs},c_{\bALs}]$.
(iii) their codomains are connected subsets of $[-c_h,c_h]$.
The functions $\tanh(x)$ and ${\rm clip}(x, -1, 1)$ satisfy these conditions.
We understand that the identity map $\Id$ also satisfies these conditions with $c_{h}\to\infty$.
Roughly speaking, we require the functions $\bALc$ and $\bALs$
to lie in the shaded area in Figure \ref{fig:fg}.
Then, the loss \eqref{eq:mdac:critic}, \eqref{eq:mdac:actor} and
\eqref{eq:mdac:critic:tanh}
can be seen as an instantiation of the following abstract VI scheme:
\begin{align}
% \!\!\!
 \begin{cases}
   \pi_{k+1} = \cG^{0,\rcEnt}(\Psi_{k})
   \\
   \Psi_{k+1} = R + \rcKL\blue{\bALc}\left(\rcEnt\log\pi_{k+1}\right)
   % \\
   % \qquad\qquad
   + \, \gamma P  \left\langle\pi_{k+1},  \Psi_{k} - \blue{\bALs}\left(\rcEnt\log\pi_{k+1}\right) \right\rangle
   + \epsilon_{k+1}
 \end{cases}
 % \!\!\!\!\!\!\!\!
 .
 % \!\!\!
 \label{eq:bounded_imdvi}
\end{align}
Notice that Munchausen-DQN and its variants are instantiations of this scheme,
since their implementations clip the Munchausen bonus term by $\bALc(x)=[x]^{0}_{l_{0}}$
with $l_{0}=-1$ typically, while $\bALs=\Id$.
Furthermore, if we choose $\bALc=\bALs\equiv0$,
\eqref{eq:bounded_imdvi} reduces to Expected Sarsa \citep{vanSeijen2009}.

Now, from the basic property of regularized MDPs,
the soft state value function $V\in\bR^{\cS}$ satisfies
% \begin{align}
$
 V
 = \rcEnt \log \left\langle\mu^{\rcKL}, \exp \frac{Q}{\rcEnt}\right\rangle
 % = \rcEnt \log \left\langle\mu^{\rcKL}, \exp Q/\rcEnt\right\rangle
 = \rcEnt \log \left\langle\one, \exp \frac{\Psi}{\rcEnt}\right\rangle
 % = \rcEnt \log \left\langle\one, \exp \Psi/\rcEnt\right\rangle
$,
% \end{align}
where $\Psi = Q + \rcKL\rcEnt\log\mu$.
We write $\bL^{\rcEnt}\Psi = \rcEnt \log \dprod{\one}{\exp\frac{\Psi}{\rcEnt}}$ for convention.
The basic properties of $\bL^{\rcEnt}$ are summarized in Appendix \ref{ss:appendix:proofs:L}.
In the limit $\rcEnt\to 0$, it holds that $V(s)=\max_{a\in\cA}\Psi(s,a)$.
Furthermore, for a policy $\pi = \cG^{0,\rcEnt}(\Psi)$,
$\rcEnt\log\pi$ equals to the soft advantage function $A\in\bR^{\cS\times\cA}$:
\begin{align*}
 % \rcEnt\log\pi_{k+1}
 % = \rcEnt \log \frac{\exp\frac{\Psi_{k}}{\rcEnt}}{\langle \one, \exp\frac{\Psi_{k}}{\rcEnt}\rangle}
 % = \rcEnt \log \exp\left(\frac{\Psi_{k} - V_{k}}{\rcEnt}\right)
 % = \Psi_{k} - V_{k}
 % = A_{k}
 \rcEnt\log\pi
 &= \rcEnt \log \frac{\exp\frac{\Psi}{\rcEnt}}{\langle \one, \exp\frac{\Psi}{\rcEnt}\rangle}
 = \rcEnt \log \exp\left(\frac{\Psi - V}{\rcEnt}\right)
 % \\
 % &
 = \Psi - V
 \eqdef A
 ,
\end{align*}
thus we have that $\alpha\log\pi_{k+1}=A_k$.
% Reviewer{DZWM}
Therefore, as discussed by \citet{Vieillard2020a},
the recursion \eqref{eq:imdvi} is written as
a soft variant of Advantage Learning (AL):
\begin{align*}
 \Psi_{k+1}
 &= R + \rcKL A_{k} + \gamma P \left\langle\pi_{k+1}, \Psi_{k} - A_{k} \right\rangle + \epsilon_{k+1}
 \\
 &= R + \gamma P V_{k} - \rcKL (V_{k} - \Psi_{k}) + \epsilon_{k+1}
 .
\end{align*}

Given these observations,
we introduce a \emph{bounded gap-increasing Bellman operator} $\cT^{\bALc\bALs}_{\pi_{k+1}}$:
\begin{align}
 \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}
 &= R + \rcKL {\bALc}(A_{k}) + \gamma P \left\langle\pi_{k+1}, \Psi_{k} \!-\! {\bALs}(A_{k}) \right\rangle
 .
 \label{eq:bal_operator}
\end{align}
Then, the DP scheme \eqref{eq:bounded_imdvi} is equivalent to
the following \emph{Bounded Advantage Learning} (BAL):
\begin{align}
 \begin{cases}
   \pi_{k+1} = \cG^{0,\rcEnt}(\Psi_{k})
   \\
   \Psi_{k+1} = \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k} + \epsilon_{k+1}
 \end{cases}
 \label{eq:bal}
 .
\end{align}
By construction, the operator $\cT^{\bALc\bALs}_{\pi_{k+1}}$ pushes-down the value of actions.
To be precise,
since $\max_{a\in\cA}\Psi(s,a) \le \left(\bL^{\rcEnt}\Psi\right)(s)$,
the soft advantage $A_{k}$ is always non-positive.
Thus, the re-parameterized action value $\Psi_k$ is decreased by adding the term $\rcKL {\bALc}(A_{k})$.
The decrement is smallest at the optimal action $\arg\max_{a}\Psi_{k}(s,a)$.
Therefore, the operator $\cT^{\bALc\bALs}_{\pi_{k+1}}$ increases the action gaps with bounded magnitude dependent on ${\bALc}$.
The increased action gap is advantageous in the presence of approximation or estimation errors $\epsilon_{k}$ \citep{Farahmand2011,Bellemare2016}.
In addition, as the term $- \gamma P \left\langle\pi_{k+1}, {\bALs}(A_{k}) \right\rangle$ in Eq. \eqref{eq:bal_operator} indicates,
the entropy bonus for the successor state action pair
$(s^{\prime},a^{\prime})\sim P_{\pi}(\cdot|s,a)$
is decreased by ${\bALs}$.

We remark that BAL preserves the original mirror descent structure of MDVI \eqref{eq:mdvi}.
Noticing that
$Q_{k} = \Psi_{k} - \rcKL\rcEnt\log\pi_{k}$,
$(1-\rcKL)\rcEnt = \cEnt$ and
$\rcKL\rcEnt = \cKL$,
and following some steps similar to the derivation of Munchausen RL
in Appendix A.2 of \citep{Vieillard2020b},
the bounded gap-increasing operator \eqref{eq:bal_operator}
can be rewritten in terms of $Q$ as
\begin{align*}
 % &\tilde{\cT}^{\bALc\bALs}_{\pi_{k+1}\sep\pi_k} Q_{k}
 % \\
 % &=
 % R + \gamma P \left(
 %     \left<\pi_{k+1},Q_{k}\right>
 %     % + (1-\rcKL) \rcEnt \mathcal{H}(\pi_{k+1})
 %     % - \rcEnt\rcKL D_{KL}(\pi_{k+1}\|\pi_{k})
 %     \!+\! \cEnt \mathcal{H}(\pi_{k+1})
 %     \!-\! \cKL D_{KL}(\pi_{k+1}\|\pi_{k})
 % \right)
 % \\
 % &\quad- \rcKL \left(A_{k} - {f}(A_{k})\right)
 % + \gamma P
 %     \left<\pi_{k+1},A_{k} - g(A_{k})\right>
 % \\
 % \tilde{\cT}^{\bALc\bALs}_{\pi_{k+1}\sep\pi_k} Q_{k}
 \cT^{\bALc\bALs}_{\pi_{k+1}\sep\pi_k} \Psi_{k}
 % \\
 &=
 \cT^{\cKL,\cEnt}_{\pi_{k+1}\sep\pi_k} Q_{k}
   - \rcKL \left(A_{k} \!-\! {f}(A_{k})\right)
   % \\
   % &\qquad\qquad\qquad~
   + \gamma P
       \left<\pi_{k+1},A_{k} \!-\! g(A_{k})\right>
 .
\end{align*}
Therefore, BAL still aligns the the original mirror descent structure of MDVI,
but with additional modifications to the Bellman backup term.


\subsection{Convergence of BAL}
\label{ss:analysis:convergence}

First, we investigate the \emph{asymptotic} converegnce property of BAL scheme.
Since gap-increasing operators are \emph{not contraction maps} in general,
we need an argument similar to the analysis provided by \citet{Bellemare2016}.
Indeed, for
the case where $\rcEnt\to 0$ while keeping $\rcKL$ constant,
which corresponds to KL-only regularization and hard gap-increasing,
their asymptotic result directly applies and
it is guaranteed that BAL is \emph{optmiality-preserving}
(please see Appendix \ref{ss:appendix:proofs:hard_bal}).
On the other hand, however,
we need tailored analyses for the case $\rcEnt > 0$.
The following theorem characterizes the possibly biased convergence of
soft gap-increasing operators under KL-entropy regularization.
\begin{theorem}
 \label{thm:BAL_fI_is_convergent}
 Let $\Psi \in \bR^{\cS\times\cA}$, $V=\bL^{\rcEnt}\Psi$,
 $\cT^{\rcEnt}\Psi = R + \gamma P \bL^{\rcEnt}\Psi$
 and $\cT'$ be an operator with the properties that
 $\cT' \Psi \le \cT^{\rcEnt} \Psi$
 and $
   \cT' \Psi
   \ge \cT^{\rcEnt} \Psi - \rcKL \left( V - \Psi \right)
 $.
 Consider the sequence $\Psi_{k+1} \defeq \cT' \Psi_{k}$ with $\Psi_0 \in \bR^{\cS\times\cA}$,
 and let $V_{k} = \bL^{\rcEnt} \Psi_{k}$.
 Further, with an abuse of notation, we write
 $V_{\cEnt}^{*}\in\bR^{\cS}$
 as the unique fixed point of the operator
 $
   \cT^{\cEnt} V
   = \bL^{\cEnt}(R + \gamma P V)
 $.
 Then, the sequence $(V_{k})_{k \in \bN}$ converges,
 and the limit
 $
 \tilde{V} = \lim_{k \to \infty} V_{k}
 $
 satisfies
 $
  V^{*}_{\cEnt} \le \tilde{V} \le V^{*}_{\rcEnt}
 $
 .
 Furthermore, $\limsup_{k \to \infty} \Psi_{k} \le Q_{\rcEnt}^{*}$ and
 $\liminf_{k \to \infty} \Psi_{k}
   \ge \frac{1}{1 - \rcKL} \left(\tilde{Q} - \rcKL \tilde{V} \right)
 $,
 where $\tilde{Q} = R + \gamma P \tilde{V}$.
\end{theorem}
Since $
 \cT^{\rcEnt} \Psi_{k} \ge \cT^{\bALc\Id}_{\pi_{k+1}} \Psi_{k}
 = \cT^{\rcEnt} \Psi_{k} + \rcKL f(A_{k})
 \ge \cT^{\rcEnt} \Psi_{k} + \rcKL A_{k}
$,
from Theorem \ref{thm:BAL_fI_is_convergent}
we can assure that BAL is convergent and
$\Psi_{k}$ remains in a bounded range if $\bALs=\Id$,
even though $\tilde{V} \ne V^{*}_\cEnt$ in general.
Furthermore, this result suggests that
\emph{Munchausen RL is convergent even when the ad-hoc clipping is employed}.
However, Theorem \ref{thm:BAL_fI_is_convergent} does not support the convergence for $\bALs\!\ne\!\Id$,
even though $\bALs\!\ne\!\Id$ is empirically beneficial as seen in Section \ref{ss:motivating_the_bounding_functions}.
The following Proposition
\ref{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg}
offers a sufficient condition for the asymptotic convergence when $\bALs\!\ne\!\Id$, and characterizes the limiting behavior of BAL.
\begin{prop}
  \label{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg}
  Consider the sequence $\Psi_{k+1} \defeq \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}$
  produced by the BAL operator \eqref{eq:bal_operator}
  with $\Psi_0 \in \bR^{\cS\times\cA}$,
  and let $V_{k} = \bL^{\rcEnt} \Psi_{k}$.
  Assume that for all $k \in \bN$ it holds that
  \begin{align}
    % &\cKL \kl(\pi_{k+1}\|\pi_{k})
    % - \gamma P^{\pi_{k+1}} \!\bigl(
    %  \rcEnt \cH(\pi_{k+1}) \!+\! \dprod{\pi_{k+1}}{\bALs(A_{k})}
    % \bigr)
    % \nonumber\\
    % &\quad\ge 0
    % % \cKL \kl(\pi_{k}\|\pi_{k-1})
    % % - \gamma P^{\pi_{k}} \!\bigl(
    % %   \rcEnt \cH(\pi_{k}) \!+\! \dprod{\pi_{k}}{\bALs(A_{k-1})}
    % % \bigr)
    % % \ge 0
    % .
    \!\!\!
    \cKL D_{k+1}
    - \gamma P^{\pi_{k+1}} \!\bigl(
     \rcEnt
     % \cH_{k+1}
     \cH(\pi_{k+1})
     \!+\!
     \dprod{\pi_{k+1}}{\bALs(A_{k})}
    \bigr)
    \ge 0
    ,
    \label{eq:sufficient_condition_for_bal_fg_to_converge}
  \end{align}
  where $D_{k+1} \!=\! \kl(\pi_{k+1}\|\pi_{k})$.
  % and $\cH_{k+1} \!=\! \cH(\pi_{k+1})$.
  Then, the sequence $(V_{k})_{k \in \bN}$ converges,
  and the limit
  $
  \tilde{V} = \lim_{k \to \infty} V_{k}
  $
  satisfies
  $
    V^{*}_{\rcEnt} - \frac{1}{1-\gamma}\left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
    \le
    \tilde{V}
    \le V^{*}_{\rcEnt}
  $.
  Furthermore, $\limsup_{k \to \infty} \Psi_{k} \le Q_{\rcEnt}^{*}$ and
  % \begin{align*}
    $\liminf_{k \to \infty} \Psi_{k}
      \ge \tilde{Q} - \left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
    $,
    where $\tilde{Q} = R + \gamma P \tilde{V}$.
\end{prop}
We remark that the lower bound
of $\tilde{V}$
is reasonable.
Since
$
  V^\rcEnt_{\rm max}
  = V_{\rm max} + \frac{\rcEnt\log|\cA|}{1-\gamma}
$,
the magnutide of the lower bound roughly matches %the upper bound of
the un-regularized value, which appears because $\bALs$ decreases the entropy bonus in the Bellman backup.
One way to satisfy \eqref{eq:sufficient_condition_for_bal_fg_to_converge} for all $k \in \bN$ is to use an adaptive strategy to determine $\bALs$.
Since $\pi_{k+1}$ is obtained \emph{before} the update $\Psi_{k+1} = \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}$ in BAL scheme \eqref{eq:bal},
it is possible that we first compute $\kl(\pi_{k+1}\|\pi_{k})$ and $\cH(\pi_{k+1})$, and then adaptively find $\bALs$ that satisfies \eqref{eq:sufficient_condition_for_bal_fg_to_converge}, with additional computational efforts.
In the following, however, we provide an error propagation analysis and argue that a fixed $\bALs\ne\Id$ is indeed beneficial.


\subsection{Bounding Decreases The Inherent Errors}
\label{ss:analysis:epa}

Theorem \ref{thm:BAL_fI_is_convergent} indicates that BAL is convergent but possibly biased even when $\bALs=\Id$.
However, we can still upper-bound the error between
the optimal entropy-regularized state value $V_{\cEnt}^{*}$,
which is the unique fixed point of the operator
$
 % \cT_{\pi^{*}}^{0,\cEnt} V
 % = \dprod{\pi^{*}}{R + \gamma P V} + \rcEnt \cH(\pi^{*})
 \cT^{\cEnt} V
 = \bL^{\cEnt}(R + \gamma P V)
$,
and the entropy-regularized state value $V_{\cEnt}^{\pi_{k}}$
for the sequence of the policies $(\pi_{k})_{k\in\bN}$ generated by BAL.
Theorem \ref{thm:BAL_soft_advantage_error} below,
which generalizes Theorem 1 by \citet{Zhang2022} to KL-entropy-regularized settings with the bounding functions $\bALc$ and $\bALs$,
provides such a bound and
highlights the advantage of BAL for both $\bALc\ne\Id$ and $\bALs\ne\Id$.
\begin{theorem}
 \label{thm:BAL_soft_advantage_error}
 Let $(\pi_{k})_{k\in\bN}$ be a sequence of the policies obtained by BAL.
 Defining
 $\Delta^{\bALc\bALs}_{k} = \dprod{\pi^{*}}{
   \rcKL \left(A_{\cEnt}^{*} - {\bALc}(A_{k-1})\right) - \gamma P \dprod{
     \pi_{k}}{A_{k-1} - {\bALs}(A_{k-1})}}
 $,
 it holds that:
 \begin{align}
  &\nInf{V_{\cEnt}^{*}-V_{\cEnt}^{\pi_{K+1}}}
  % \nonumber\\
  %  &
   \le \frac{2\gamma}{1-\gamma} \! \left[
     2 \gamma^{K-1} V^\cEnt_{\rm max}
     +\!
     \sum_{k=1}^{K-1}\gamma^{K-k-1}\nInf{\Delta_{k}^{\bALc\bALs}}\right]
   \label{eq:bal_suboptimality}
   .
 \end{align}
\end{theorem}
Since the suboptimality of BAL is characterize by Theorem \ref{thm:BAL_soft_advantage_error}, we can discuss its convergence property as in previous researches \citep{Kozuno2019,Vieillard2020a}.
The bound \eqref{eq:bal_suboptimality} resembles the standard suboptimality bounds in the literature \citep{Munos2005,Munos2007,Antos2008,Farahmand2010},
which consists of the horizon term $2\gamma/(1-\gamma)$,
initialization error $2 \gamma^{K-1} V^\cEnt_{\rm max}$ that goes to zero as $K\to\infty$,
and the accumulated error term.
However, our error terms do not represent the Bellman backup errors,
but capture \emph{the misspecifications of the optimal policy} as we discuss later.
We note that, the error term $\Delta^{\bALc\bALs}_{k}$ does not contain the error $\epsilon_k$, because we simply omitted it in our analysis as done by \citet{Zhang2022}.
Our interest here is
\emph{not} in the effect of the approximation/estimation error $\epsilon_k$,
but in the effect of \emph{the ruin caused by the soft advantage} $A_k=\rcEnt\log\pi_{k+1}$,
that is, the error inherent to the soft-gap-increasing nature of M-VI and BAL in model-based tabular settings without any approximation.
In the following,
we consider a decompostion of the error
$
 \Delta^{\bALc\bALs}_{k}
 % \le
 =
 % \Delta^{\bALc}_{k} + \Delta^{\bALs}_{k}
 \Delta^{\rX\bALc}_{k} + \Delta^{\cH\bALs}_{k}
 % \dprod{\pi^{*}}{
 % \rcKL \left(A_{\cEnt}^{*} - {\bALc}(A_{k-1})\right) - \gamma P \dprod{
 %   \pi_{k}}{A_{k-1} - {\bALs}(A_{k-1})}}
$
and argue that
(1) the cross term
$\Delta^{\rX\bALc}_{k} = - \rcKL \dprod{\pi^{*}}{{\bALc}(A_{k-1})}$
has major effect on the sub-optimality and is \emph{always} decreased by $\bALc\ne\Id$,
and (2) the entropy terms
$
 \Delta^{\cH\bALs}_{k} = \dprod{\pi^{*}}{
   \rcKL A_{\cEnt}^{*} - \gamma P \dprod{
   \pi_{k}}{A_{k-1} - {\bALs}(A_{k-1})}}
$ are decreased by $\bALs\ne\Id$,
although which is \emph{not always} true.

To ease the exposition, first let us consider the case
$\rcEnt\to 0$ while keeping $\rcKL>0$ constant,
which corresponds to KL-only regularization.
Then, noticing that we have $\cG^{0,0}(\Psi) = \cG(\Psi)$,
$\bL^{\rcEnt}\Psi(s) \to \max_{b\in\cA} \Psi(s,b)$ and $\bALs(0)\!\!=\!\! 0$,
it follows that the entropy terms are equal to zero:
$
  \!
  \dprod{\pi^{*}}{A^{*}}
  \!\!=\!\! \dprod{\pi_{k+1}}{A_{k}}
  \!\!=\!\! \dprod{\pi_{k+1}}{\bALs(A_{k})}
  \!\!=\!\! 0
$.
Thus, $\Delta^{\bALc\bALs}_{k}$ reduces to
% it holds that
$\Delta^{\rX\bALc}_{k} = - \rcKL \dprod{\pi^{*}}{{\bALc}(A_{k-1})}$
and $
 \Delta^{\rX\bALc}_{k}(s)
 = - \rcKL \bALc \left(\Psi_{k-1}(s,\pi^{*}(s)) - \Psi_{k-1}(s, \pi_{k}(s))\right)
$.
Therefore, $\Delta_{k}$ represents \emph{the error incurred by the misspecification of the optimal policy}.
For AL, the error is $
 \Delta^{\rX\Id}_{k}(s)
 = \rcKL\left(\Psi_{k-1}(s, \pi_{k}(s)) - \Psi_{k-1}(s,\pi^{*}(s))\right)
$.
Since both AL and BAL are optimality-preserving for $\rcEnt\to 0$,
we have $\|{\Delta^{\rX\Id}_{k}}\|_{\infty} \to 0$ and
$\|{\Delta^{\rX\bALc}_{k}}\|_{\infty} \to 0$ as $k\to\infty$.
Howerver, their convergence speed is governed by the magnitude of
$\|{\Delta^{\rX\Id}_{k}}\|_{\infty}$ and $\|{\Delta^{\rX\bALc}_{k}}\|_{\infty}$
at finite $k$, respectively.
We remark that for all $k$ it holds that
$|\Delta^{\rX\bALc}_{k}| \le |\Delta^{\rX\Id}_{k}|$ point-wise.
Indeed, from the non-positivity of $A_k$ and the requirement to $\bALc$,
we always have $A_k = \Id(A_k) \le \bALc(A_k)$ point-wise
and then $-\rcKL \Id(A_k(s,a)) \ge - \rcKL \bALc(A_k(s,a))$ for all $(s,a)$ and $k$,
both sides of which are non-negative.
Thus, we have
$\left< \pi^\ast, - \rcKL \bALc(A_{k-1}) \right>
\le \left< \pi^\ast, - \rcKL I(A_{k-1}) \right>$
point-wise and then $|\Delta_k^{\rX\bALc}| \le |\Delta_k^{\rX\Id}|$.
Further, we have
$\|{\Delta^{\rX\Id}_{k}}\|_{\infty} \le \frac{2 R_{\rm \max}}{1-\gamma}$ for AL
while
$\|{\Delta^{\rX\bALc}_{k}}\|_{\infty} \le c_{\bALc}$ for BAL.
Therefore, BAL has better convergence property than AL
by a factor of the horizon $1/(1-\gamma)$
when $\Psi_{k}$ is far from optimal.

For the case $\rcEnt>0$,
$\|{\Delta^{\bALc\bALs}_{k}}\|_{\infty} \to 0$ does not hold in general.
Further, the entropy terms are no longer equal to zero.
However, the cross term, which is an order of $1/(1-\gamma)$,
is much larger unless the action space is extremely large
since the entropy is an order of $\log |\cA|$ at most,
and is always decreased by $\bALc\ne\Id$.
Furthermore,
we can expect that
$\bALs\ne\Id$ decreases the error
$
 \Delta^{\cH\bALs}_{k}
 % =
 % \dprod{\pi^{*}}{
 %   \rcKL A_{\cEnt}^{*} - \gamma P \dprod{
 %   \pi_{k}}{A_{k-1} - {\bALs}(A_{k-1})}}
$,
though it does \emph{not always} true.
If $\bALs\ne\Id$, the entropy terms reduce to
$\Delta_k^{\cH\Id} = \left< \pi^\ast, \rcKL A^\ast \right>$.
Since $A_{k-1}$ is non-positive,
we have $A_{k-1} - \bALs(A_{k-1})\le 0$ from the requirements to $\bALs$.
Since the stochastic matrix $P$ is non-negative,
we have $P \left<\pi_k, A_{k-1} - \bALs(A_{k-1}) \right> \le 0$,
where the l.h.s. represents the decreased negative entropy of the successor state
and its absolute value is again an order of $\log |\cA|$ at most.
Since $A^\ast \le 0$ also, whose absolute value is an order of  $1/(1-\gamma)$,
it holds that
$
 % \rcKL A^\ast(s,a)
 % \le
 % \rcKL A^\ast(s,a) - \gamma \left(P \left<\pi_k, A_{k-1} - \bALs(A_{k-1}) \right>\right)(s,a)
 %  % | \le |  |
 \rcKL A^\ast
 \le
 \rcKL A^\ast - \gamma P \left<\pi_k, A_{k-1} - \bALs(A_{k-1}) \right>
$
% point-wise
and thus
$
 \Delta_k^{\cH\Id}
 = \left< \pi^\ast, \rcKL A^\ast \right>
 \le
 \dprod{\pi^{*}}{
   \rcKL A_{\cEnt}^{*} - \gamma P \dprod{
   \pi_{k}}{A_{k-1} - {\bALs}(A_{k-1})}}
  =
  \Delta^{\cH\bALs}_{k}
$
.
When $\Delta^{\cH\bALs}_{k}$ is non-positive, it is guaranteed that
$\bigl|\Delta^{\cH\bALs}_{k}\bigr| \le \bigl|\Delta_k^{\cH\Id}\bigr|$.
In addition, we can expect that this error is largely decreased by zero function ${\bALs}(x)\equiv 0$, though it makes harder to satisfy the inequality
\eqref{eq:sufficient_condition_for_bal_fg_to_converge}.
However, this inequality does not always hold because it depends on the actual magnitude of $A^\ast$ and $P \left<\pi_k, A_{k-1} - g(A_{k-1}) \right>$.


Overall, there is a trade-off in the choice of $\bALs$;
$\bALs=\Id$ always satisfies the sufficient condition of asymptotic convergence \eqref{eq:sufficient_condition_for_bal_fg_to_converge},
but the entropy term is not decreased.
On the other hand, ${\bALs}(x)\equiv 0$ is expected to decrease the entroy term,
though which possibly violates \eqref{eq:sufficient_condition_for_bal_fg_to_converge} and might hinder the asymptotic performance.
In the next section, we examine how the choice of $\bALc$ and $\bALs$ affects the empirical performance.


\section{Experiment}
\label{ss:experiment}


\subsection{BAL on Grid World}
\label{ss:experiment:toy}

First, we compare the model-based tabular M-VI \eqref{eq:imdvi} and BAL \eqref{eq:bal}.
As discussed by \citet{Vieillard2020a}, the larger the value of $\rcKL$ is, the slower the initial convergence of MDVI gets, and thus M-VI as well.
Since the inherent error reduction by BAL is effective when $\Psi_k$ is far from optimum, it is expected
that BAL is effective especially in earlier stage.
We vaidate this hypothesis by a gridworld environment,
where transition kernel $P$ and reward function $R$ are accessible.
We performed $100$ independent runs with random initialization of $\Psi_{0}$.
Figure \ref{fig:vi_curve} compares the normalized value of the suboptimality $\|V^{\pi_{k}}-V^{*}_{\cEnt}\|_{\infty}$, where the interquatile mean (IQM) is reported as suggested by \citet{Agarwal2021}.
The result suggests that BAL outperforms M-VI initially.
Furthermore, $\bALs\ne\Id$ performs slightly better than $\bALs=\Id$ in the earlier stage, even in this toy problem.
More experimental details are found in Appendix \ref{ss:appendix:experimental_details:toy}.



\subsection{MDAC on Mujoco Locomoation Environments}
\label{ss:experiment:mujoco}


\paragraph{Setup and Metrics.}

Next, we empirically evaluate the effectiveness of MDAC on 6 Mujoco environments
(\texttt{Hopper-v4}, \texttt{HalfCheetah-v4}, \texttt{Walker2d-v4}, \texttt{Ant-v4}, \texttt{Humanoid-v4} and \texttt{HumanoidStandup-v4})
from Gymnasium \citep{Gymnasium2023}.
We evaluate our algorithm and baselines on 3M environmental steps,
except for easier \texttt{Hopper-v4} on 1M steps.
For the reliable benchmarking,
we again report the aggregated scores over all 6 environments as suggested by \citet{Agarwal2021}.
To be precise, we train 10 different instances of each algorithm with different random seeds
and calculate baseline-normalized scores along iterations for each task as
$
 {\rm score} =
   \frac{
     {\rm score_{algorithm}} - {\rm score_{random}}
     }{
     {\rm score_{baseline}} - {\rm score_{random}}
   }
$,
where the baseline is the mean SAC score after 3M steps (1M for \texttt{Hopper-v4}).
Then, we calculate the IQM score by aggregating the learning results over all 6 environments.
We also report pointwise 95\% percentile stratified bootstrap confidence intervals.
We use Adam optimizer \citep{Kingma2015} for all the gradient-based updates.
The discount factor is set to $\gamma=0.99$.
All the function approximators, including those for baseline algorithms,
are fully-connected feed-forward networks with two hidden layers
and each hidden layer has $256$ units with ReLU activations.
We use a Gaussian policy with mean and standard deviation provided by the neural network.
We fixed $\log\sigma_{\rm min}\!=\!-5$.
More experimental details, including a full list of the hyperparameters and per-environment results,
will be found in Appendix \ref{ss:appendix:experimental_details:mujoco}.

% \vspace{-0.5cm}

\paragraph{Effect of bounding functions $f$ and $g$.}

We start from evaluating how the performance of MDAC is affected by
the choice of the bounding functions.
First, we evaluate whether bounding both $\log \pi(a|s)$ terms is beneficial.
We compare 3 choices:
(i) $\bALc\!=\!\bALs\!=\!\Id$,
(ii) $\bALc(x)\!=\!{\rm tanh}(x/10), \bALs\!=\!\Id$ and
(iii) $\bALc(x)\!=\!\bALs(x)\!=\!{\rm tanh}(x/10)$.
Figure \ref{fig:ablation_fg_use_both} compares the learning results for these choices
and it indicates that bounding both $\rcEnt\log\pi$ terms is indeed beneficial.

\clearpage
Next, we compare 5 choices under $\bALc\!=\!\bALs\!\ne\!\Id$:
(i) ${\rm clip}(x, -1, 1)$,
(ii) ${\rm clip}(x/10, -1, 1)$,
(iii) ${\rm tanh}(x)$,
(iv) ${\rm tanh}(x/10)$,
and
(v) ${\rm sign}(x)$.
Notice that the last choice (v) violates our requirement to the bounding functions.
Figure \ref{fig:ablation_fg_type} compares the learning curves for these choices.
The result indicates that the performence difference between ${\rm clip}(x)$ and ${\rm tanh}(x)$ is small.
On the other hand, the performance is better if the slower saturating functions are used.
Furthermore, ${\rm sign}(x)$ resulted in the worst performance among these choices.
Figure \ref{fig:clipping_frequency} compares the frequencies of clipping $\rcEnt\log\pi$ terms
by ${\rm clip}(x, -1, 1)$ and ${\rm clip}(x/10, -1, 1)$
in the sampled minibatchs for the initial learning phase in
\texttt{HalfCheetah-v4}.
For ${\rm clip}(x, -1, 1)$,
the clipping occurs frequently especially for the current $(s,a)$ pairs
and the information of relative $\rcEnt\log\pi$ values between different state-actions are lost.
In contrast,
for ${\rm clip}(x/10, -1, 1)$, the clipping rarely happens and the information of relative $\rcEnt\log\pi$ values are leveraged in the learning.
These results suggest that the relative values of $\rcEnt\log\pi$ terms between different state-actions are beneficial,
even though the raw values (by $\bALc\!=\!\bALs\!=\!\Id$) are harmful.

\begin{figure*}[t]
  \begin{tabular}{ccc}
    \begin{minipage}[b]{0.3\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.3]{"fig/lc_random_imdvi_bal_frtanh_gidentity_bal_frtanh_grtanh_iqm_tidy.pdf"}
      \caption{
        Comparison of
        M-VI and BAL.
      }
      \label{fig:vi_curve}
    \end{minipage}
    &
    % \quad
    \begin{minipage}[b]{0.3\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.3]{"fig/bound_both_works_iqm_curve_mdac.v7_kappaFixed_gauss_None_mdac.v7_kappaFixed_gauss_squash_mdac.v7_kappaFixed_gauss_rsquash_10.pdf"}
      % \hspace{5mm}
      \captionsetup{justification=raggedleft, singlelinecheck=off}
      \caption{
        Effect of $\bALc\ne\Id$ and $\bALs\ne\Id$.
      }
      \label{fig:ablation_fg_use_both}
    \end{minipage}
    &
    % \quad
    \begin{minipage}[b]{0.3\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.3]{"fig/compare_fg_iqm_curve_mdac.v7_kappaFixed_gauss_None_mdac.v7_kappaFixed_gauss_rclip_mdac.v7_kappaFixed_gauss_rclip_0.1_mdac.v7_kappaFixed_gauss_rsquash_naive_mdac.v7_kappaFixed_gauss_rsquash_mdac.v7_binary_10.pdf"}
      % \hspace{5mm}
      \captionsetup{justification=raggedleft, singlelinecheck=off}
      \caption{
        Comparison of $f$ and $g$.
      }
      \label{fig:ablation_fg_type}
    \end{minipage}
  \end{tabular}
  \vspace{-\baselineskip}
\end{figure*}
\begin{figure*}[t]
  \begin{tabular}{ccc}
    \begin{minipage}[b]{0.3\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.3]{"fig/tidy_freqs_seed42_envHalfCheetah-v4_until100_fs20.pdf"}
      \caption{
        Clipping frequencies.}
      \label{fig:clipping_frequency}
    \end{minipage}
    &
    % \quad
    \begin{minipage}[b]{0.3\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.3]{"fig/benchmarking_iqm_curve_sac_clean.v2_lrQ3e-4_td3_official.v1_mdac.v7_kappaFixed_gauss_rclip_0.1_10.pdf"}
      % \hspace{5mm}
      \captionsetup{justification=raggedleft, singlelinecheck=off}
      \caption{Comparison on Mujoco.}
      \label{fig:benchmarking}
    \end{minipage}
    &
    % \quad
    \begin{minipage}[b]{0.3\hsize}
      \centering
      \includegraphics[keepaspectratio, scale=0.3]{"fig/dmc_iqm_curve_sac_clean.lr3e-4_10M_full_mdac.lr3e-4_10M_full_dSdWdTdRdF_30_until5000.pdf"}
      % \hspace{5mm}
      \captionsetup{justification=raggedleft, singlelinecheck=off}
      \caption{
        Comparison on \texttt{dog} domains.}
      \label{fig:benchmarking:dmc}
    \end{minipage}
  \end{tabular}
  \vspace{-\baselineskip}
\end{figure*}


% \newpage
\paragraph{Comparison to baseline algorithms.}
We compare MDAC against SAC \citep{Haarnoja2018b},
an entropy-only-regularized method,
and TD3 \citep{Fujimoto2018}, a non-regularized method.
% We used Clean RL \citep{Huang2022CleanRL} for their implementations.
We adopted $\bALc(x)\!=\!\bALs(x)\!=\!{\rm clip}(x/10, -1, 1)$.
Figure \ref{fig:benchmarking} compares the learning results.
Notice that the final IQM score of SAC does not match $1$,
because the scores are normalized by the mean of all the SAC runs,
whereas IQM is calculated by middle 50\% runs.
The results show that MDAC overtakes both SAC and TD3.



\subsection{MDAC on DeepMind Control Suite}
\label{ss:experiment:dmc}

Finally, we compare MDAC and SAC on the challenging \texttt{dog} domain from DeepMind Control Suite \citep{DMC2020}.
We adopted \texttt{stand}, \texttt{walk}, \texttt{trot}, \texttt{run} and \texttt{fetch} tasks.
We train 30 different instances of each algorithm for 5M environmental steps,
and report SAC normalized IQM scores.
We adopted $\bALc(x)\!=\!\bALs(x)\!=\!{\rm clip}(x/10, -1, 1)$ for MDAC again.
Hyperparameters are set to equivalent values as in Mujoco experiments.
Figure \ref{fig:benchmarking:dmc} compares the aggregated learning results.
Though the aggregated result is not very strong statistically,
MDAC tends to reach better performace than SAC.
 % especially in \texttt{walk} and \texttt{run}.
Figure \ref{fig:benchmarking_all:dmc_25} shows per-environment results with 25$\%$ and 75$\%$ percentile scores.
While the performances of SAC often degrade during the learning
due to the difficulty of the \texttt{dog} domain,
this degradation is lessly observed for MDAC.
% (please refer Fig \ref{fig:benchmarking_all:dmc_25} in Appendix \ref{ss:appendix:experimental_details:mujoco}).
We conjecture that this effect is due to the implicit KL-regularized nature of MDAC.
\begin{figure}[t]
  \begin{center}
    \includegraphics[width=\linewidth]{"fig/dmc_score_test_mean25_pe10_sac_clean.lr3e-4_10M_full_mdac.lr3e-4_10M_full_30_until5000.pdf"}
    \caption{
      Per-environment performances in \texttt{dog} domain from DeepMind Control Suite.
      The mean scores of 30 independent runs are reported.
      The shaded region corresponds to 25$\%$ and 75$\%$ percentile scores over the 30 runs.
    }
    \label{fig:benchmarking_all:dmc_25}
  \end{center}
  % \vspace{-\baselineskip}
\end{figure}



\section{Conclusion}
\label{ss:conclusion}

In this study, we proposed MDAC,
a model-free actor-critic instantiation of MDVI for continuous action domains.
We showed that its empirical performance is significantly boosted
by bounding the values of log-density terms in the critic loss.
By relating MDAC to AL, we theoretically showed that
the inherent error of gap-increaing operators is decreased by bounding the soft advantage terms, as well as provided the convergence analyses.
Our analyses indicated that bounding both of the log-policy terms is beneficial.
Lastly, we evaluated the effectiveness of MDAC empirically in simulated environments.


\paragraph{Limitations.}
This study has three major limitations.
Firstly, our theoretical analyses are valid only for fixed $\rcEnt$.
Thus, its exploding behavior observed in Section \ref{ss:motivating_the_bounding_functions} for $\bALc=\bALs=\Id$ is not captured.
Secondly, our theoretical analyses apply only to tabular cases in the current forms.
To extend our analyses to continuous state-action domains,
we need measure-theoretic considerations as explored in Appendix B of \citep{Puterman1994}.
Lastly, our analyses and experiments do not offer
the optimal design of the bounding functions $\bALc$ and $\bALs$.
We leave these issues as open questions.


\bibliography{references}
\bibliographystyle{preprint}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\appendix


\section{Additional Discussion on MD-based RL Methods}
\label{ss:appendix:md_methods}

\citet{Wang2019} explores off-policy policy gradients in MD view and proposes an off-policy variant of PPO.
\citet{Tomar2022} considers a MD structure with the advantage function and the KL divergence, and proposes variants of SAC and PPO.
\citet{Yang2022} incorporates a variance reduction method into MD based RL.
\citet{Vaswani2022} and \citet{Alfano2023} try to generalize the existing MD based approaches to general policy parameterizations.
\citet{Kuba2022} proposes a further generalization that unify even non-regularized RL methods such as DDPG and A3C.
\citet{Lan2023} proposes a MD method that resembles MDVI, which incorporates both the (Bregman/KL) divergence and an additional convex regularizer, and show that it achieves fast linear rate of convergence.
Munchausen RL is distinct from the above literature in the sense that,
it is \emph{implicit} mirror descent due to the sound reparameterization by \citet{Vieillard2020b}.
Though this makes it very easy to implement, the control of the policy change is vague, particularly when combined with function approximations.
Thus, we argue that (1) Munchausen RL based methods are very good starting point to use, and (2) if a precise control of policy change is demanded, another MD methods could be tried.


\section{Proofs}
\label{ss:appendix:proofs}

\subsection{Basic Properties of $\bL^{\rcEnt}$}
\label{ss:appendix:proofs:L}

In this section, we omit $\Psi$'s dependency to state $s$,
and let $\Psi\in\bR^\cA$ for brevity.
For $\rcEnt > 0$, we write
$\bL^{\rcEnt}\Psi = \rcEnt \log \dprod{\one}{\exp\frac{\Psi}{\rcEnt}} \in \bR$.
\begin{lemma}
  \label{lem:L_is_continuous_and_increasing}
  $\bL^{\rcEnt}$ is continuous and strictly increasing.
\end{lemma}
\begin{proof}
  Continuity follows from the fact that $\bL^{\rcEnt}\Psi = \rcEnt \log \dprod{\one}{\exp\frac{\Psi}{\rcEnt}}$ is a composition of continuous functions.
  We also have that
  \begin{align*}
    \frac{\partial}{\partial \Psi(a)} \bL^{\rcEnt}\Psi
    = \frac{\exp\frac{\Psi(a)}{\rcEnt}}
    {\dprod{\one}{\exp\frac{\Psi}{\rcEnt}}}
    > 0,
  \end{align*}
  from which we conclude that $\bL^{\rcEnt}$ is strictly increasing.
\end{proof}
\begin{lemma}
  \label{lem:L_is_no_less_than_max}
  It holds that
  \begin{align*}
    \max_{a\in\cA} \Psi(a)
    \le \bL^{\rcEnt} \Psi
    \le \max_{a\in\cA} \Psi(a) + \rcEnt \log |\cA|
    .
  \end{align*}
\end{lemma}
\begin{proof}
  Let $y=\max_{a\in\cA} \Psi(a)$.
  We have that
  \begin{align*}
    \exp \frac{y}{\rcEnt}
    \le \dprod{\one}{\exp\frac{\Psi}{\rcEnt}}
    = \sum_{a\in\cA} \exp\frac{\Psi(a)}{\rcEnt}
    \le |\cA| \exp \frac{y}{\rcEnt}
    .
  \end{align*}
  Applying the logarithm to this inequality, we have
  \begin{align*}
    \frac{y}{\rcEnt}
    \le \log\dprod{\one}{\exp\frac{\Psi}{\rcEnt}}
    \le \frac{y}{\rcEnt} + \log|\cA|
    ,
  \end{align*}
  and thus the claim follows.
\end{proof}

\begin{lemma}
  \label{lem:L_converges_to_max}
  It holds that
  % \begin{align*}
    $
    \lim_{\rcEnt\to0} \bL^{\rcEnt} \Psi \to \max_{a\in\cA} \Psi(a)
    $
    .
  % \end{align*}
\end{lemma}
\begin{proof}
  Let $y=\max_{a\in\cA} \Psi(a)$
  and $\cB=\left\{ a\in\cA \middle| \Psi(a) = y \right\}$.
  It holds that
  \begin{align*}
    \bL^{\rcEnt} \Psi
    &=
    \rcEnt \log \sum_{a\in\cA} \exp\frac{\Psi(a)}{\rcEnt}
    \\
    &=
    \rcEnt \log \left(
      \exp\frac{y}{\rcEnt} \sum_{a\in\cA} \exp\frac{\Psi(a)-y}{\rcEnt}
    \right)
    \\
    &=
    y + \rcEnt \log \left(
      \sum_{a\in\cB} \underbrace{\exp\frac{\Psi(a)-y}{\rcEnt}}_{= 1}
      + \sum_{a\not\in\cB} \exp\frac{\Psi(a)-y}{\rcEnt}
    \right)
    \\
    &=
    y + \rcEnt \log \left(
      |\cB|
      + \sum_{a\not\in\cB} \exp\frac{\Psi(a)-y}{\rcEnt}
    \right)
    .
  \end{align*}
  Since $\Psi(a)-y < 0$ for $a\not\in\cB$, we have
  $\exp\frac{\Psi(a)-y}{\rcEnt} \to 0$ as $\rcEnt\to0$ for $a\not\in\cB$,
  % which concludes the proof.
  thus it holds that
  $\lim_{\rcEnt\to0} \bL^{\rcEnt} \Psi \to y = \max_{a\in\cA} \Psi(a)$.
\end{proof}

\begin{lemma}
  \label{lem:identity_if_action_agnostic}
  Let $v$ be independent of actions.
  Then it holds that
  $\bL^{\rcEnt}(\Psi + v) = \bL^{\rcEnt}(\Psi) + v$.
\end{lemma}
\begin{proof}
  \begin{align*}
    \bL^{\rcEnt} (\Psi + v)
    % &
    = \rcEnt \log \dprod{\one}{\exp \frac{\Psi + v}{\rcEnt}}
    % \\
    % &
    = \rcEnt \log \dprod{\one}{\exp \frac{\Psi}{\rcEnt}}
     + \rcEnt \log \exp \frac{v}{\rcEnt}
    % \\
    % &
    = \bL^{\rcEnt} \Psi + v
    .
  \end{align*}
\end{proof}

\begin{lemma}
  \label{lem:tau_is_recovered_from_L_alpha}
  It holds that
  $
    \bL^{\rcEnt} \frac{1}{1-\rcKL}\Psi = \frac{1}{1-\rcKL} \bL^{\cEnt} \Psi
  $
  .
\end{lemma}
\begin{proof}
  Noticing $\cEnt=(1-\rcKL)\rcEnt$, we have
  \begin{align*}
    \cG^{0,\rcEnt}\left(\frac{\Psi}{1-\rcKL}\right)
    = \frac{\exp \frac{1}{\rcEnt} \frac{\Psi}{1-\rcKL}}{\dprod{\one}{\exp \frac{1}{\rcEnt} \frac{\Psi}{1-\rcKL}}}
    = \frac{\exp \frac{\Psi}{\cEnt}}{\dprod{\one}{\exp \frac{\Psi}{\cEnt}}}
    =
    \cG^{0,\cEnt}\left(\Psi\right)
    \eqdef \pi_{\cEnt}
    ,
  \end{align*}
  and thus
  \begin{align*}
    \bL^{\rcEnt} \frac{\Psi}{1-\rcKL}
    = \dprod{\pi_{\cEnt}}{\frac{\Psi}{1-\rcKL}} + \rcEnt \cH(\pi_{\cEnt})
    = \frac{1}{1-\rcKL} \bigl(\dprod{\pi_{\cEnt}}{\Psi} + (1-\rcKL)\rcEnt \cH(\pi_{\cEnt})\bigr)
    = \frac{1}{1-\rcKL} \bL^{\cEnt} \Psi
    .
  \end{align*}
\end{proof}

\begin{lemma}
  \label{lem:inequality_of_limsup_and_liminf}
  Let $(\Psi_{k})_{k \in \bN}$ be a bounded sequence.
  Then it holds that, for pointwise,
  \begin{align*}
    \limsup_{k \to \infty} \bL^{\rcEnt} \Psi_{k}
    \le
    \bL^{\rcEnt} \limsup_{k \to \infty} \Psi_{k}
  \end{align*}
  and
  \begin{align*}
    \bL^{\rcEnt} \liminf_{k \to \infty} \Psi_{k}
    \le
    \liminf_{k \to \infty} \bL^{\rcEnt} \Psi_{k}
    .
  \end{align*}
\end{lemma}
\begin{proof}
  % https://math.stackexchange.com/questions/2220211/sum-of-limit-inferior-liminf-s-n-liminf-t-n-le-liminf-s-nt-n
  % https://math.stackexchange.com/questions/1812486/prove-lim-sup-f-x-n-f-lim-sup-x-n-and-same-for-inf
  Since $\log$ and $\exp$ are continuous and strictly increasing,
  $\limsup$ and $\liminf$ are both commute with these functions \citep{Basu2019}.
  Furthermore, for real valued bounded sequences $x_k$ and $y_k$,
  we have
  $
    \limsup_{k \to \infty} (x_k + y_k)
    \le
    \limsup_{k \to \infty} x_k + \limsup_{k \to \infty} y_k
  $
  and
  $
    \liminf_{k \to \infty} x_k + \liminf_{k \to \infty} y_k
    \le
    \liminf_{k \to \infty} (x_k + y_k)
  $.
  Since $\bL^{\rcEnt}$ is a composition of $\exp$, summation and $\log$, the claim follows.
\end{proof}


\subsection{
Asymptotic Property of BAL with $\rcEnt\to0$
}
\label{ss:appendix:proofs:hard_bal}

If an action-value function is updated using an operator $\cT'$ that is \emph{optimality-preserving},
at least one optimal action remains optimal,
and suboptimal actions remain suboptimal.
Further, if the operator $\cT'$ is also \emph{gap-increasing},
the value of suboptimal actions are pushed-down,
which is advantageous in the presence of approximation or estimation errors \citep{Farahmand2011}.

Now, we provide the formal definitions of
\emph{optimality-preserving}
and \emph{gap-increasing}.
\begin{definition}[Optimality-preserving]
  An operator $\cT'$ is \emph{optimality-preserving} if,
  for any $Q_{0} \in \bR^{\cS\times\cA}$ and $s \in \cS$,
  letting $Q_{k+1} \defeq \cT' Q_{k}$,
  % \begin{align*}
  $
    \tilde{V}(s) \defeq \lim_{k \to \infty} \max_{b\in\cA} Q_{k}(s,b)
  $
  % \end{align*}
  exists, is unique, $\tilde{V}(s) = V^\ast(s)$, and for all $a \in \cA$,
  % \begin{align*}
  $
    Q^\ast(s,a) < V^\ast(s,a) \hspace{-0.3em} \implies \hspace{-0.3em} \limsup_{k \to \infty} Q_{k}(s,a) < V^\ast(s)
    .
  $
  % \end{align*}
\end{definition}
\begin{definition}[Gap-increasing]
  An operator $\cT'$ is \emph{gap-increasing}
  if for all $Q_{0} \in \bR^{\cS\times\cA}$, $s \in \cS, a \in \cA$,
  letting $Q_{k+1} \defeq \cT' Q_{k}$ and $V_{k}(x) \defeq \max_b Q_{k}(s,b)$,
  $
    \liminf_{k \to \infty} \big [ V_{k}(s) - Q_{k}(s,a) \big ] \ge V^\ast(s) - Q^\ast(s,a)
    .
  $
\end{definition}

The following lemma characterizes the conditions when an operator is optimality-preserving and gap-increasing.
\begin{lemma}[Theorem 1 in \citep{Bellemare2016}]
  \label{lem:optimality_achieving_operators}
  Let $V(s) \defeq \max\nolimits_{b} Q(s,b)$ and
  let $\cT$ be the Bellman optimality operator $\cT Q = R + \gamma P V$.
  Let $\cT'$ be an operator with the property that there exists an $\cGI \in [0, 1)$ such that for all $Q \in \bR^{\cS\times\cA}$, $s \in \cS, a \in \cA$,
  $\cT' Q \le \cT Q$, and $\cT' Q \ge \cT Q - \cGI \left(V - Q \right)$.
  Then $\cT'$ is both optimality-preserving and gap-increasing.
\end{lemma}

% Now, we state our Theorem \ref{lem:Hard_BAL_is_OP_and_GI} again.
Notably, our operator $\cT^{\bALc\bALs}_{\pi_{k+1}}$ is
both optimality-preserving and gap-increasing in the limit $\rcEnt\to 0$.
\begin{theorem}
  In the limit $\rcEnt\to 0$,
  the operator $\cT^{\bALc\bALs}_{\pi_{k+1}}$ satisfies
  $\cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k} \le \cT \Psi_{k}$ and
  $\cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k} \ge \cT \Psi_{k} - \rcKL \left(V_{k} - \Psi_{k} \right)$
  and thus is both optimality-preserving and gap-increasing.
\end{theorem}

\begin{proof}
  From Lemma \ref{lem:L_converges_to_max},
  we have $\bL^{\rcEnt}(s) \Psi \to \max_{a\in\cA} \Psi(s,a)$ as $\rcEnt\to0$ for $\Psi\in\bR^{\cS\times\cA}$.
  Observe that, for $h\in\{\bALc,\bALs\}$,
  it holds that $h(A_{k}) = h(\Psi_{k}-V_{k}) \le 0$
  since $A_{k}(s,a)=\Psi_{k}(s,a) - \max_{b\in\cA} \Psi_{k}(s,b) \le 0$ and
  $h$ does not flip the sign of argument.
  Additionally, for $\pi_{k+1}\in\cG(\Psi_{k})$ it follows that $\left\langle\pi_{k+1}, h(A_{k}) \right\rangle=0$
  since $h(0)=0$.
  It holds that
  \begin{align*}
    \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k} - \cT \Psi_{k}
    &= R + \rcKL\bALc(A_{k}) + \gamma P \left\langle\pi_{k+1}, \Psi_{k} - \bALs(A_{k}) \right\rangle
     - R - \gamma P \left\langle\pi_{k+1}, \Psi_{k}\right\rangle
    \\
    &=
    \rcKL\underbrace{\bALc(A_{k})}_{\le 0} - \gamma P \underbrace{\left\langle\pi_{k+1}, \bALs(A_{k}) \right\rangle}_{=0}
    \le 0
    .
  \end{align*}
  Furthermore, observing that $x-f(x)\le 0$ for $x\le0$,
  it follows that
  \begin{align*}
    \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k} - \cT \Psi_{k} + \rcKL \left(V_{k} - \Psi_{k}\right)
    &=
    - \rcKL \bigl(\underbrace{A_{k} - \bALc(A_{k})}_{\le 0}\bigr)
    - \gamma P \underbrace{\bigl\langle\pi_{k+1}, \bALs(A_{k}) \bigr\rangle}_{= 0}
    \ge 0
    .
  \end{align*}
  Thus, the operator $\cT^{\bALc\bALs}_{\pi_{k+1}}$ satisfies
  the conditions of Lemma \ref{lem:optimality_achieving_operators}.
  Therefore we conclude that $\cT^{\bALc\bALs}_{\pi_{k+1}}$ is
  both optimality-preserving and gap-increasing.
\end{proof}


\subsection{Proof of Theorem \ref{thm:BAL_fI_is_convergent}}

We provide several lemmas that are used to prove Theorem \ref{thm:BAL_fI_is_convergent}.

\begin{lemma}
  \label{lem:convergence_of_sgi_operators}
  Let $\Psi \in \bR^{\cS\times\cA}$, $V=\bL^{\rcEnt}\Psi$
  and $\cT'$ be an operator with the properties that
  $\cT' \Psi \le \cT^{\rcEnt} \Psi$
  and $
  % \begin{align*}
    \cT' \Psi
    \ge \cT^{\rcEnt} \Psi - \rcKL \left( V - \Psi \right)
    = \cT^{\rcEnt} \Psi + \rcKL \left( A \right)
  $.
  Consider the sequence $\Psi_{k+1} \defeq \cT' \Psi_{k}$ with $\Psi_0 \in \bR^{\cS\times\cA}$,
  and let $V_{k} = \bL^{\rcEnt} \Psi_{k}$.
  Then the sequence $(V_{k})_{k \in \bN}$ converges.
\end{lemma}
\begin{proof}
  \begin{align*}
    V_{k+1}
    &= \bL^{\rcEnt}\Psi_{k+1}
     = \dprod{\pi_{k+2}}{\Psi_{k+1}} + \rcEnt\cH(\pi_{k+2})
    \\
    &\ge \dprod{\pi_{k+1}}{\Psi_{k+1}} + \rcEnt\cH(\pi_{k+1})
    \\
    &= \dprod{\pi_{k+1}}{\cT' \Psi_{k}} + \rcEnt\cH(\pi_{k+1})
    \\
    &\ge \dprod{\pi_{k+1}}{\cT^{\rcEnt} \Psi_{k} + \rcKL A_{k}} + \rcEnt\cH(\pi_{k+1})
    \\
    \overset{(a)}&{=}
      \dprod{\pi_{k+1}}{\cT^{\rcEnt} \Psi_{k}} + (1 - \rcKL)\rcEnt\cH(\pi_{k+1})
    \\
    \overset{(b)}&{=}
      \dprod{\pi_{k+1}}{Q_{k} + \gamma P(V_{k} - V_{k-1})
      } + (1 - \rcKL)\rcEnt\cH(\pi_{k+1})
    \\
    \overset{(c)}&{=}
      \dprod{\pi_{k+1}}{Q_{k} + \gamma P(V_{k} - V_{k-1})
      } + \cEnt\cH(\pi_{k+1})
      - \cKL \kl(\pi_{k+1}\|\pi_{k}) + \cKL \kl(\pi_{k+1}\|\pi_{k})
    \\
    \overset{(d)}&{=}
      V_{k}
      + \dprod{\pi_{k+1}}{\gamma P(V_{k} - V_{k-1})}
      + \cKL \kl(\pi_{k+1}\|\pi_{k})
    \\
    &\ge V_{k} + \dprod{\pi_{k+1}}{\gamma P(V_{k} - V_{k-1})}
    ,
  \end{align*}
  where (a) follows from $
    \dprod{\pi_{k+1}}{A_{k}} = \dprod{\pi_{k+1}}{\rcEnt\log\pi_{k+1}} = -\rcEnt\cH(\pi_{k+1})
  $,
  (b) follows from $
    \cT^{\rcEnt} \Psi_{k} = R + \gamma P \bL^{\rcEnt}\Psi_{k} = R + \gamma P V_{k} = Q_{k+1}
  $,
  (c) follows from $(1 - \rcKL)\rcEnt=\cEnt$,
  and (d) follows from
  % \begin{align*}
  $
    V_{k}
    = \bL^{\rcEnt}\Psi_{k}
    = \dprod{\pi_{k+1}}{Q_{k}} + \cEnt \cH(\pi_{k+1}) - \cKL \kl(\pi_{k+1}\|\pi_{k})
  $.
  % \end{align*}
  Thus we have
  \begin{align*}
    V_{k+1} - V_{k} \ge \gamma P^{\pi_{k+1}}(V_{k} - V_{k-1})
  \end{align*}
  and by induction
  \begin{align*}
    V_{k+1} - V_{k} \ge \gamma^{k} P_{k+1:2}(V_{1} - V_{0})
    ,
  \end{align*}
  where $P_{k+1:2} = P^{\pi_{k+1}} P^{\pi_{k}} \cdots P^{\pi_{2}}$.
  From the conditions on $\cT'$, if $V_{0}$ is bounded
  then $V_{1}$ is also bounded, and thus $\nInf{V_{1} - V_{0}} < \infty$.
  By definition, for any $\delta > 0$ and $n \in \bN$, $\exists k \ge n$
  such that $V_{k} > \tilde{V} - \delta$.
  Since $P_{k+1:2}$ is a nonexpansion in $\infty$-norm, we have
  \begin{align*}
    V_{k+1} - V_{k}
    \ge -\gamma^k \nInf{V_{1} - V_{0}}
    \ge -\gamma^n \nInf{V_{1} - V_{0}} \eqdef -\epsilon
    ,
  \end{align*}
  and for all $t \in \bN$,
  \begin{align*}
    V_{k+t} - V_{k}
    \ge - \sum_{i=0}^{t-1} \gamma^i \epsilon
    \ge \frac{-\epsilon}{1-\gamma}
    .
  \end{align*}
  Thus, we have
  \begin{equation*}
    \inf_{t \in \bN} V_{k+t}
    \ge V_{k} - \frac{\epsilon}{1-\gamma}
    > \tilde{V} - \delta - \frac{\epsilon}{1-\gamma}
    .
  \end{equation*}
  It follows that for any $\delta' > 0$,
  we can choose an $n \in \bN$ to make $\epsilon$ small enough such that for all $k \ge n$,
  $V_{k} > \tilde{V} - \delta'$. Hence
  \begin{equation*}
    \liminf_{k \to \infty} V_{k} = \tilde{V},
  \end{equation*}
  and thus $V_{k}$ converges.
\end{proof}


\begin{lemma}
  \label{lem:V_is_bounded}
  Let $\cT'$ be an operator satisfying the conditions of Lemma \ref{lem:convergence_of_sgi_operators}.
  Then for all $k \in \bN$,
  \begin{align}
    \label{eq:V_is_bounded}
    |V_{k}|
    \le \frac{1}{1-\gamma} \Big(
      \Rmax + 3 \nInf{V_0} + \rcEnt \log|\cA|
    \Big)
    \eqdef V_{\rm max}^{\rm SGI}
    .
  \end{align}
\end{lemma}
\begin{proof}
  Following the derivation of Lemma \ref{lem:convergence_of_sgi_operators}, we have
  \begin{align}
    V_{k+1} - V_{0}
    &\ge -\sum_{i=1}^{k} \gamma^{i} \nInf{V_{1} - V_{0}}
    \ge \frac{-1}{1-\gamma} \nInf{V_{1} - V_{0}}
    .
    \label{eq:V_diff_LB}
  \end{align}
  We also have
  \begin{align*}
    V_{1}
    = \bL^{\rcEnt} \cT' \Psi_{0}
    \le \bL^{\rcEnt} \cT^{\rcEnt} \Psi_{0}
    = \max \dprod{\pi}{R+\gamma P V_{0}} + \rcEnt \cH(\pi)
    \le \nInf{R+\gamma P V_{0}} + \rcEnt\log|\cA|
  \end{align*}
  and then for pointwise
  \begin{align*}
    V_{1} - V_{0} \le \Rmax + 2 \nInf{V_{0}} + \rcEnt\log|\cA|
    .
  \end{align*}
  Combining above and \eqref{eq:V_diff_LB},
  we have
  \begin{align}
    V_{k+1}
    &\ge V_{0} - \frac{1}{1-\gamma} \left(
      \Rmax + 2 \nInf{V_{0}} + \rcEnt\log|\cA|
    \right)
    \\
    &\ge - \frac{1-\gamma}{1-\gamma} \nInf{V_{0}} - \frac{1}{1-\gamma} \left(
      \Rmax + 2 \nInf{V_{0}} + \rcEnt\log|\cA|
    \right)
    \\
    &\ge
    - \frac{1}{1-\gamma} \bigl( 3 \nInf{V_0} + \Rmax + \rcEnt \log|\cA| \bigr)
    .
  \end{align}
  Now assume that the upper bound of \eqref{eq:V_is_bounded} holds up to $k \in \bN$.
  Then we have
  \begin{align*}
    V_{k+1}
    &= \bL^{\rcEnt} \cT' \Psi_{k} \le \bL^{\rcEnt} \cT^{\rcEnt} \Psi_{k}
    \\
    &= \max \dprod{\pi}{R+\gamma P V_{k}} + \rcEnt \cH(\pi)
    \\
    &\le \Rmax + \gamma\nInf{V_{k}} + \rcEnt\log|\cA|
    \\
    &\le \Rmax + \frac{\gamma}{1-\gamma} \bigl( 3 \nInf{V_0} + \Rmax + \rcEnt \log|\cA| \bigr) + \rcEnt\log|\cA|
    \\
    &\le \frac{\gamma}{1-\gamma} 3 \nInf{V_0}
      + \left(\frac{1-\gamma}{1-\gamma} + \frac{\gamma}{1-\gamma}\right)\left(
        \Rmax + \rcEnt \log|\cA|
      \right)
    \\
    &\le \frac{1}{1-\gamma} \bigl( 3 \nInf{V_0} + \Rmax + \rcEnt \log|\cA| \bigr)
  \end{align*}
  Since \eqref{eq:V_is_bounded} holds for $k = 0$ also
  from $1 \le \frac{3}{1-\gamma}$,
  the claim follows.
\end{proof}

\begin{lemma}
  \label{lem:Psi_is_bounded}
  Let $\nInf{\Psi_0} < \infty$ and
  $\cT'$ be an operator satisfying the conditions of Lemma \ref{lem:convergence_of_sgi_operators}.
  Then for all $k \in \bN$,
  \begin{align}
    \label{eq:Psi_is_bounded}
    \Psi_{k} \le \frac{1}{1-\gamma} \bigl(
      \Rmax + \nInf{\Psi_0} + \gamma \rcEnt \log|\cA|
    \bigr)
    % .
  \end{align}
  and
  \begin{align*}
    % \label{eq:Psi_is_bounded}
    \Psi_{k} \ge
      - \frac{1}{(1-\rcKL)(1-\gamma)}\left(
        (1+\rcKL) \Rmax + (\gamma + \rcKL) \Big(
          3 \nInf{V_0} + \rcEnt \log|\cA|
        \Big)
      \right)
      - \nInf{\Psi_0}
    .
  \end{align*}
\end{lemma}
\begin{proof}
  Assume that, the inequality \eqref{eq:Psi_is_bounded} holds up to $k \in \bN$.
  Then, it holds that
  \begin{align*}
    \Psi_{k}
    &= \cT' \Psi_{k}
    \\
    &\le \cT^{\rcEnt} \Psi_{k}
    \\
    &= R + \gamma P \bL^{\rcEnt} \Psi_{k}
    \\
    &= R + \gamma P \left(\dprod{\pi_{k+1}}{\Psi_{k}} + \rcEnt \cH(\pi_{k+1})\right)
    \\
    &\le \Rmax + \gamma \nInf{\Psi_{k}} + \gamma \rcEnt\log|\cA|
    \\
    &\le \Rmax + \frac{\gamma}{1-\gamma} \bigl(
      \Rmax + \nInf{\Psi_0} + \gamma \rcEnt \log|\cA|
    \bigr) + \gamma \rcEnt\log|\cA|
    \\
    &= \left(\frac{1-\gamma}{1-\gamma} + \frac{\gamma}{1-\gamma}\right) \left(
      \Rmax + \gamma \rcEnt\log|\cA|
      \right) + \frac{\gamma}{1-\gamma} \nInf{\Psi_0}
    \\
    &\le \frac{1}{1-\gamma} \bigl(
      \Rmax + \nInf{\Psi_0} + \gamma \rcEnt\log|\cA|
      \bigr)
    .
  \end{align*}
  Since $\Psi_0$ satisfies \eqref{eq:Psi_is_bounded} also
  from $1 \le \frac{1}{1-\gamma}$,
  the upper bound \eqref{eq:Psi_is_bounded} holds for all $k \in \bN$.
  Now, we also have
  \begin{align*}
    \Psi_{k+1}
    &= \cT' \Psi_{k}
    \\
    &\ge \cT^{\rcEnt} \Psi_{k} - \rcKL \left( V_{k} - \Psi_{k} \right)
    \\
    &= R + \gamma P V_{k} - \rcKL V_{k} + \rcKL \Psi_{k}
    \\
    \overset{(a)}&{\ge} - \Rmax - (\gamma + \rcKL) V_{\rm max}^{\rm SGI} + \rcKL \Psi_{k}
    \\
    &= - c_{\rm max} + \rcKL \Psi_{k}
    ,
  \end{align*}
  where
  (a) follows from Lemma \ref{lem:V_is_bounded} and
  $c_{\rm max} = \Rmax + (\gamma + \rcKL) V_{\rm max}^{\rm SGI} > 0$.
  Using the above recursively, we obtain
  \begin{align*}
    \Psi_{k+1}
    &\ge
      - (1 + \rcKL + \rcKL^2 + \cdots + \rcKL^k) c_{\rm max}
      + \rcKL^{k+1} \Psi_{0}
    \\
    &\ge
      - \frac{1}{1-\rcKL}c_{\rm max}
      - \nInf{\Psi_0}
    \\
    &=
      - \frac{1}{1-\rcKL}\left(
        \Rmax + \frac{\gamma + \rcKL}{1-\gamma} \Big(
          \Rmax + 3 \nInf{V_0} + \rcEnt \log|\cA|
        \Big)
      \right)
      - \nInf{\Psi_0}
    \\
    &=
      - \frac{1}{(1-\rcKL)(1-\gamma)}\left(
        (1+\rcKL) \Rmax + (\gamma + \rcKL) \Big(
          3 \nInf{V_0} + \rcEnt \log|\cA|
        \Big)
      \right)
      - \nInf{\Psi_0}
    .
  \end{align*}
\end{proof}

\begin{theorem}[Theorem \ref{thm:BAL_fI_is_convergent} in the main text]
  \label{thm:BAL_fI_is_convergent:appendix}
  Let $\Psi \in \bR^{\cS\times\cA}$, $V=\bL^{\rcEnt}\Psi$,
  $\cT^{\rcEnt}\Psi = R + \gamma P \bL^{\rcEnt}\Psi$
  and $\cT'$ be an operator with the properties that
  $\cT' \Psi \le \cT^{\rcEnt} \Psi$
  and $
    \cT' \Psi
    \ge \cT^{\rcEnt} \Psi - \rcKL \left( V - \Psi \right)
  $.
  Consider the sequence $\Psi_{k+1} \defeq \cT' \Psi_{k}$ with $\Psi_0 \in \bR^{\cS\times\cA}$,
  and let $V_{k} = \bL^{\rcEnt} \Psi_{k}$.
  Further, with an abuse of notation, we write
  $V_{\cEnt}^{*}\in\bR^{\cS}$
  as the unique fixed point of the operator
  $
    \cT^{\cEnt} V
    = \bL^{\cEnt}(R + \gamma P V)
  $.
  Then, the sequence $(V_{k})_{k \in \bN}$ converges,
  and the limit
  $
  \tilde{V} = \lim_{k \to \infty} V_{k}
  $
  satisfies
  $
   V^{*}_{\cEnt} \le \tilde{V} \le V^{*}_{\rcEnt}
  $
  .
  Furthermore, $\limsup_{k \to \infty} \Psi_{k} \le Q_{\rcEnt}^{*}$ and
  % \begin{align*}
    $\liminf_{k \to \infty} \Psi_{k}
      \ge \frac{1}{1 - \rcKL} \left(\tilde{Q} - \rcKL \tilde{V} \right)
    $,
    where $\tilde{Q} = R + \gamma P \tilde{V}$.
  % \end{align*}
\end{theorem}

\begin{proof}
  {\bfseries Upper Bound.}
  From $\cT' \Psi \le \cT^{\rcEnt} \Psi$ and
  observing that $\cT^{\rcEnt}$ has a unique fixed point, we have
  \begin{align}
    \limsup_{k \to \infty} \Psi_{k}
    = \limsup_{k \to \infty} (\cT')^k \Psi_0
    \le \limsup_{k \to \infty} (\cT^{\rcEnt})^k \Psi_0
    = Q_{\rcEnt}^{*}
    .
    \label{eq:upper_bound_of_lim_sup_psi}
  \end{align}
  We know that $V_{k}=\bL^{\rcEnt} \Psi_{k}$ converges to $\tilde{V}=\lim_{k \to \infty} \bL^{\rcEnt} \Psi_{k}$ by Lemma \ref{lem:convergence_of_sgi_operators}.
  Since Lemma \ref{lem:Psi_is_bounded} assures that
  the sequence $(\Psi_{k})_{k \in \bN}$ is bounded,
  we have that $
    \limsup_{k \to \infty} \bL^{\rcEnt} \Psi_{k}
    \le \bL^{\rcEnt} \limsup_{k \to \infty} \Psi_{k}
  $
  from Lemma \ref{lem:inequality_of_limsup_and_liminf}.
  Thus, it holds that
  \begin{align}
    \tilde{V}
    = \lim_{k \to \infty} V_{k}
    = \limsup_{k \to \infty} V_{k}
    = \limsup_{k \to \infty} \bL^{\rcEnt} \Psi_{k}
    \le \bL^{\rcEnt} \limsup_{k \to \infty} \Psi_{k}
    \le \bL^{\rcEnt} Q_{\rcEnt}^{*}
    = V_{\rcEnt}^{*}
    .
    \label{eq:tilde_V_is_UB_Valpha}
  \end{align}

  {\bfseries Lower Bound.}
  Now, it
  holds that
  \begin{align}
    \Psi_{k+1} &= \cT' \Psi_{k}
    \nonumber \\
    &\ge \cT^{\rcEnt} \Psi_{k} - \rcKL\left( V_{k} - \Psi_{k} \right)
    \nonumber \\
    &= R + \gamma P V_{k} - \rcKL V_{k} + \rcKL \Psi_{k}.
    \label{eq:Psi_inequality}
  \end{align}
  From Lemma \ref{lem:V_is_bounded} and Lebesgue's dominated convergence theorem,
  we have
  \begin{equation}
    \lim_{k \to \infty} P V_{k} = P \tilde{V}
    \label{eq:dominated_convergence}
    .
  \end{equation}
  Let $\bar{\Psi} \defeq \liminf_{k \to \infty} \Psi_{k}$.
  Taking the $\liminf$ of both sides of \eqref{eq:Psi_inequality}
  and
  from the fact $
    \liminf_{k \to \infty} V_{k}
    = \lim_{k \to \infty} V_{k}
    = \tilde{V}
  $
  we obtain
  \begin{align*}
    \bar{\Psi}
    &\ge R + \gamma P \tilde{V} - \rcKL \tilde{V} + \rcKL \bar{\Psi}
    \\
    &= \tilde{Q} - \rcKL \tilde{V} + \rcKL \bar{\Psi}
    ,
  \end{align*}
  where $
    \tilde{Q}
    = R + \gamma P \tilde{V}
  $.
  Thus it holds that
  \begin{align}
    \bar{\Psi}
      &\ge \frac{1}{1 - \rcKL} \left(\tilde{Q} - \rcKL \tilde{V} \right)
    .
    \label{eq:bar_Psi_lower_bound}
  \end{align}
  Now, from Lemma \ref{lem:inequality_of_limsup_and_liminf} and \ref{lem:Psi_is_bounded},
  it holds that $
    \bL^{\rcEnt} \liminf_{k \to \infty} \Psi_{k}
    \le
    \liminf_{k \to \infty} \bL^{\rcEnt} \Psi_{k}
  $.
  Thus, applying $\bL^{\rcEnt}$ to the both sides of \eqref{eq:bar_Psi_lower_bound}
  and from Lemma \ref{lem:identity_if_action_agnostic} and \ref{lem:tau_is_recovered_from_L_alpha},
  it follows that
  \begin{align*}
    \tilde{V}
      \ge \bL^{\cEnt} \tilde{Q}
      = \bL^{\cEnt} \left( R + \gamma P \tilde{V} \right)
      = \cT^{\cEnt} \tilde{V}
    .
  \end{align*}
  Using the above recursively, we have
  \begin{align}
    \tilde{V}
    \ge \lim_{k \to \infty} (\cT^{\cEnt})^k \tilde{V}
    = V^{*}_{\cEnt}
    .
    \label{eq:tilde_V_is_LB_Vtau}
  \end{align}
  Combining \eqref{eq:tilde_V_is_LB_Vtau} and \eqref{eq:tilde_V_is_UB_Valpha},
  we have
  \begin{align*}
    V^{*}_{\cEnt} \le \tilde{V} \le V^{*}_{\rcEnt}
    .
  \end{align*}
\end{proof}



\subsection{Proof of Proposition \ref{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg}}

We provide several lemmas that are used to prove Proposition \ref{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg}.
\begin{lemma}
  \label{lem:BGI_is_bounded}
  The bounded gap-increasing operator satisfies
  $
    \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}
    \leq
    \cT^{\rcEnt} \Psi_{k}
  $.
\end{lemma}
\begin{proof}
  From the non-positivity of $A_k$ and the property of $\bALc$ and $\bALs$, it holds that
  \begin{align*}
    \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}
    &=
      R + \rcKL {\bALc}(A_{k}) + \gamma P \left\langle\pi_{k+1}, \Psi_{k} - {\bALs}(A_{k}) \right\rangle
    \\
    &\leq
      R + \gamma P \left\langle\pi_{k+1}, \Psi_{k} - {\bALs}(A_{k}) \right\rangle
    \\
    &\leq
      R + \gamma P \left\langle\pi_{k+1}, \Psi_{k} - A_{k} \right\rangle
    \\
    &=
      R + \gamma P \bL^{\rcEnt} \Psi_{k}
    \\
    &=
      \cT^{\rcEnt} \Psi_{k}
    .
  \end{align*}
\end{proof}

\begin{lemma}
  \label{lem:V_fg_is_convergent}
  Consider the sequence $\Psi_{k+1} \defeq \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}$
  produced by the BAL operator \eqref{eq:bal_operator}
  with $\Psi_0 \in \bR^{\cS\times\cA}$,
  and let $V_{k} = \bL^{\rcEnt} \Psi_{k}$.
  Then the sequence $(V_{k})_{k \in \bN}$ converges,
  if it holds that
  \begin{align}
    \cKL \kl(\pi_{k+1}\|\pi_{k})
    - \gamma P^{\pi_{k+1}} \left(
      \rcEnt \cH(\pi_{k+1}) + \dprod{\pi_{k+1}}{\bALs(A_{k})}
    \right) \ge 0
    % \label{eq:sufficient_condition_for_bal_fg_to_converge}
  \end{align}
  for all $k \in \bN$.
\end{lemma}
\begin{proof}
  We follow similar steps as in the proof of Lemma \ref{lem:convergence_of_sgi_operators}.
  Let $\tilde{V} \defeq \limsup_{k \to \infty} V_{k}$.
  It holds that
  \begin{align*}
    V_{k+1}
    &= \bL^{\rcEnt}\Psi_{k+1}
     = \dprod{\pi_{k+2}}{\Psi_{k+1}} + \rcEnt\cH(\pi_{k+2})
    \\
    &\ge
      \dprod{\pi_{k+1}}{\Psi_{k+1}} + \rcEnt\cH(\pi_{k+1})
    \\
    &=
      \dprod{\pi_{k+1}}{\cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}} + \rcEnt\cH(\pi_{k+1})
    \\
    &=
      \dprod{\pi_{k+1}}{\cT_{\pi_{k+1}} \Psi_{k} - \gamma P \dprod{\pi_{k+1}}{\bALs(A_{k})} + \rcKL \bALc(A_{k})} + \rcEnt\cH(\pi_{k+1})
    \\
    % &\ge
    \overset{(a)}&{\ge}
      \dprod{\pi_{k+1}}{\cT_{\pi_{k+1}} \Psi_{k} - \gamma P \dprod{\pi_{k+1}}{\bALs(A_{k})} + \rcKL A_{k}} + \rcEnt\cH(\pi_{k+1})
    \\
    \overset{(b)}&{=}
    % &=
      \dprod{\pi_{k+1}}{\cT_{\pi_{k+1}} \Psi_{k}} + \cEnt\cH(\pi_{k+1})
      - \gamma \dprod{\pi_{k+1}}{P \dprod{\pi_{k+1}}{\bALs(A_{k})}}
    \\
    \overset{(c)}&{=}
    % &=
      \dprod{\pi_{k+1}}{
        R + \gamma P \left(
          V_{k} - \rcEnt\cH(\pi_{k+1})
        \right)
      } + \cEnt\cH(\pi_{k+1})
      - \gamma P^{\pi_{k+1}} \dprod{\pi_{k+1}}{\bALs(A_{k})}
    \\
    \overset{(d)}&{=}
    % &=
      \dprod{\pi_{k+1}}{Q_{k} + \gamma P(V_{k} - V_{k-1})
      } + \cEnt \cH(\pi_{k+1})
      - \gamma P^{\pi_{k+1}} \left(
        \rcEnt \cH(\pi_{k+1}) + \dprod{\pi_{k+1}}{\bALs(A_{k})}
      \right)
    \\
    \overset{(e)}&{=}
    % &=
      V_{k}
      + \gamma P^{\pi_{k+1}}(V_{k} - V_{k-1})
      + \cKL \kl(\pi_{k+1}\|\pi_{k})
      - \gamma P^{\pi_{k+1}} \left(
        \rcEnt \cH(\pi_{k+1}) + \dprod{\pi_{k+1}}{\bALs(A_{k})}
      \right)
    ,
  \end{align*}
  where (a) follows from the non-negativity of the advantage $A_{k}$ and $x-\bALc(x)\le 0$,
  where (b) follows from $
    \dprod{\pi_{k+1}}{A_{k}} = \dprod{\pi_{k+1}}{\rcEnt\log\pi_{k+1}} = -\rcEnt\cH(\pi_{k+1})
  $ and $(1 - \rcKL)\rcEnt=\cEnt$,
  (c) follows from $
    V_{k} = \bL^{\rcEnt} \Psi_{k} = \dprod{\pi_{k+1}}{\Psi_{k}} + \rcEnt\cH(\pi_{k+1})
  $,
  (d) follows from $
    \cT^{\rcEnt} \Psi_{k} = R + \gamma P \bL^{\rcEnt}\Psi_{k} = R + \gamma P V_{k} = Q_{k+1}
  $,
  and (e) follows from
  % \begin{align*}
  $
    V_{k}
    = \bL^{\rcEnt}\Psi_{k}
    = \dprod{\pi_{k+1}}{Q_{k}} + \cEnt \cH(\pi_{k+1}) - \cKL \kl(\pi_{k+1}\|\pi_{k})
  $.
  % \end{align*}
  Thus, if it holds that
  \begin{align*}
    \cKL \kl(\pi_{k+1}\|\pi_{k})
    - \gamma P^{\pi_{k+1}} \left(
      \rcEnt \cH(\pi_{k+1}) + \dprod{\pi_{k+1}}{\bALs(A_{k})}
    \right) \ge 0
  \end{align*}
  for all $k$, we have
  \begin{align*}
    V_{k+1} - V_{k}
    \ge \gamma P^{\pi_{k+1}}(V_{k} - V_{k-1})
    .
  \end{align*}
  Therefore, by following the steps equivalent to the proof of Lemma \ref{lem:convergence_of_sgi_operators},
  we have that $\liminf_{k \to \infty} V_{k} = \tilde{V}$ and $V_{k}$ converges.
\end{proof}
\begin{lemma}
  \label{lem:V_and_Psi_are_bounded_for_BAL_fg}
  Let the conditions of Lemma \ref{lem:V_fg_is_convergent} holds.
  Then for all $k \in \bN$, the sequences
  $(V_{k})_{k \in \bN}$ and $(\Psi_{k})_{k \in \bN}$ are both bounded.
\end{lemma}
\begin{proof}
  Since the proof of Lemma \ref{lem:V_is_bounded} relies on the two inequalities
  $\cT' \Psi \le \cT^{\rcEnt} \Psi$ and
  $
    V_{k+1} - V_{k}
    \ge \gamma P^{\pi_{k+1}}(V_{k} - V_{k-1})
  $,
  the boundedness of $(V_{k})_{k \in \bN}$ follows from the identical steps given Lemma \ref{lem:BGI_is_bounded} and
  Lemma \ref{lem:V_fg_is_convergent}.
  Furthermore, following the proof of Lemma \ref{lem:Psi_is_bounded},
  we can show that the sequence $(\Psi_{k})_{k \in \bN}$ is also bounded,
  where its lower bound has dependencies to $c_{\bALc}$ and $c_{\bALs}$.
\end{proof}

We are ready to prove Proposition \ref{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg}.
We also have an improved lower bound with an explicit dependency to $c_\bALc$.
\begin{prop}[Proposition \ref{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg} in the main text]
  \ref{pp:sufficient_condition_for_bal_fg_to_converge_with_c_fg}
  Consider the sequence $\Psi_{k+1} \defeq \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}$
  produced by the BAL operator \eqref{eq:bal_operator}
  with $\Psi_0 \in \bR^{\cS\times\cA}$,
  and let $V_{k} = \bL^{\rcEnt} \Psi_{k}$.
  Assume that for all $k \in \bN$ it holds that
  \begin{align}
    \cKL \kl(\pi_{k+1}\|\pi_{k})
    - \gamma P^{\pi_{k+1}} \left(
      \rcEnt \cH(\pi_{k+1}) + \dprod{\pi_{k+1}}{\bALs(A_{k})}
    \right) \ge 0
    .
    % \label{eq:sufficient_condition_for_bal_fg_to_converge}
  \end{align}
  Then, the sequence $(V_{k})_{k \in \bN}$ converges,
  and the limit
  $
  \tilde{V} = \lim_{k \to \infty} V_{k}
  $
  satisfies
  $
    V^{*}_{\rcEnt} - \frac{1}{1-\gamma}\left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
    \le
    \tilde{V}
    \le V^{*}_{\rcEnt}
  $.
  Furthermore, $\limsup_{k \to \infty} \Psi_{k} \le Q_{\rcEnt}^{*}$ and
  $\liminf_{k \to \infty} \Psi_{k}
    \ge \tilde{Q} - \left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
  $,
  where $\tilde{Q} = R + \gamma P \tilde{V}$.
\end{prop}
\begin{proof}
  {\bfseries Upper Bound.}
  Following the identical steps in the proof of Theorem \ref{thm:BAL_fI_is_convergent:appendix},
  we obtain the upper bounds
  $
    \tilde{\Psi}
    \defeq \limsup_{k \to \infty} \Psi_{k}
    \le Q_{\rcEnt}^{*}$
  and
  $
    \tilde{V}
    = \lim_{k \to \infty} V_{k}
    = \limsup_{k \to \infty} V_{k}
    \le V_{\rcEnt}^{*}
  $ again from Lemma \ref{lem:BGI_is_bounded}.

  {\bfseries Lower Bound.}
  It holds that
  \begin{align}
    \Psi_{k+1}
    &= \cT^{\bALc\bALs}_{\pi_{k+1}} \Psi_{k}
    \nonumber\\
    &= \cT_{\pi_{k+1}} \Psi_{k} - \gamma P \dprod{\pi_{k+1}}{\bALs(A_{k})} + \rcKL \bALc(A_{k})
    \nonumber\\
    \overset{(a)}&{\ge}
      \cT_{\pi_{k+1}} \Psi_{k} - \rcKL c_\bALc
    \nonumber\\
    &= R + \gamma P V_{k} - \rcKL c_\bALc - \gamma \rcEnt P \cH(\pi_{k+1})
    \nonumber\\
    &\ge R + \gamma P V_{k} - \rcKL c_\bALc - \gamma \rcEnt \log|\cA|
    ,
    \label{eq:Psi_inequality_BAL_fg}
  \end{align}
  where (a) follows from the non-positivity of the soft advantage
  and the property of $\bALc$ and $\bALs$.
  Following the proof of Lemma \ref{lem:Psi_is_bounded},
  we can show that the sequence $(\Psi_{k})_{k \in \bN}$ is bounded again,
  Now, $V_{k}$ converges to $\tilde{V}$ by Lemma \ref{lem:V_fg_is_convergent}.
  Furthermore, by Lemma \ref{lem:V_and_Psi_are_bounded_for_BAL_fg} and Lebesgue's dominated convergence theorem,
  we have $\lim_{k \to \infty} P V_{k} = P \tilde{V}$.
  Let $\bar{\Psi} \defeq \liminf_{k \to \infty} \Psi_{k}$.
  Taking the $\liminf$ of both sides of \eqref{eq:Psi_inequality_BAL_fg},
  we obtain
  \begin{align*}
    \bar{\Psi}
    &\ge R + \gamma P \tilde{V} - \rcKL c_\bALc - \gamma \rcEnt \log|\cA|
    \\
    &= \tilde{Q} - \left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
    ,
  \end{align*}
  where $
    \tilde{Q}
    = R + \gamma P \tilde{V}
  $.
  Now,
  from Lemma \ref{lem:inequality_of_limsup_and_liminf} and \ref{lem:Psi_is_bounded},
  it holds that $
    \bL^{\rcEnt} \liminf_{k \to \infty} \Psi_{k}
    \le
    \liminf_{k \to \infty} \bL^{\rcEnt} \Psi_{k}
  $.
  Thus, applying $\bL^{\rcEnt}$ to the both sides
  and from Lemma \ref{lem:identity_if_action_agnostic},
  we have
  \begin{align*}
    \tilde{V}
      \ge \bL^{\rcEnt} \tilde{Q} - \left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
      = \cT^{\rcEnt} \tilde{V} - \left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
    .
  \end{align*}
  Therefore, using this expression recursively we obtain
  \begin{align*}
    \tilde{V}
    \ge
    V^{*}_{\rcEnt} - \frac{1}{1-\gamma}\left(\rcKL c_\bALc + \gamma \rcEnt \log|\cA|\right)
    .
  \end{align*}
\end{proof}


\subsection{Proof of Theorem \ref{thm:BAL_soft_advantage_error}}

\begin{theorem}[Theorem \ref{thm:BAL_soft_advantage_error} in the main text]
  Let $(\pi_{k})_{k\in\bN}$ be a sequence of the policies obtained by BAL.
  Defining
  $\Delta^{\bALc\bALs}_{k} = \dprod{\pi^{*}}{
    \rcKL \left(A_{\cEnt}^{*} - {\bALc}(A_{k-1})\right) - \gamma P \dprod{
      \pi_{k}}{A_{k-1} - {\bALs}(A_{k-1})}}
  $,
  it holds that:
  \begin{align}
   \nInf{V_{\cEnt}^{*}-V_{\cEnt}^{\pi_{K+1}}}
    &\le \frac{2\gamma}{1-\gamma}\left[
      2 \gamma^{K-1} V^\cEnt_{\rm max} +
      \sum_{k=1}^{K-1}\gamma^{K-k-1}\nInf{\Delta_{k}^{\bALc\bALs}}\right]
    .
  \end{align}
\end{theorem}

\begin{proof}
  For the policy $\pi_{k+1} = \cG^{0,\rcEnt}(\Psi_{k})$,
  the operator $\cT_{\pi_{k+1}}^{0,\cEnt}$ is a contraction map.
  Let $V_{\cEnt}^{\pi_{K+1}}$ denote the fixed point of
  $\cT_{\pi_{K+1}}^{0,\cEnt}$, that is,
  $V_{\cEnt}^{\pi_{K+1}} = \cT_{\pi_{K+1}}^{0,\cEnt} V_{\cEnt}^{\pi_{K+1}}$.
  Observing that $
  \pi_{k+1} = \cG_{\pi_k}^{\cKL,\cEnt}(Q_{k}) = \cG_{\pi_k}^{\cKL,\cEnt}(R + \gamma P V_{k-1})
  $,
  we have for $K \ge 1$,
  \begin{align}
    V_{\cEnt}^{*} - V_{\cEnt}^{\pi_{K+1}}
    &= \cT_{\pi^{*}}^{0,\cEnt} V_{\cEnt}^{*}
     - \cT_{\pi^{*}}^{0,\cEnt} V_{K-1}
     + \cT_{\pi^{*}}^{0,\cEnt} V_{K-1}
     - \cT^{\cEnt} V_{K-1}
     + \cT^{\cEnt} V_{K-1}
     - \cT_{\pi_{K+1}}^{0,\cEnt} V_{\cEnt}^{\pi_{K+1}}
    \nonumber\\
    \overset{(a)}&{\le}
    \gamma P^{\pi^{*}} (V_{\cEnt}^{*} - V_{K-1})
    + \gamma P^{\pi_{K+1}} (V_{K-1} - V_{\cEnt}^{\pi_{K+1}})
    \nonumber\\
    &= \gamma P^{\pi^{*}} (V_{\cEnt}^{*} - V_{K-1})
    + \gamma P^{\pi_{K+1}} (V_{K-1} - V_{\cEnt}^{*} + V_{\cEnt}^{*} - V_{\cEnt}^{\pi_{K+1}})
    \nonumber\\
    &= \left(\Id - \gamma P^{\pi_{K+1}}\right)^{-1}\bigl(
      \gamma P^{\pi^{*}} - \gamma P^{\pi_{K+1}}
      \bigr) \left(V_{\cEnt}^{*} - V_{K-1}\right)
    \label{eq:BAL_telescopic_bound}
    ,
  \end{align}
  where (a) follows from
  $\cT_{\pi^{*}}^{0,\cEnt} V_{K-1} \le \cT^{\cEnt} V_{K-1} = \cT_{\pi_{K+1}}^{0,\cEnt} V_{K-1}$
  and the definition of $\cT_{\pi}^{0,\cEnt}$.

  We proceed to bound the term $V_{\cEnt}^{*} - V_{K-1}$:
  \begin{align*}
    V_{\cEnt}^{*} - V_{K-1}
    &= \cT_{\pi^{*}}^{0,\cEnt} V_{\cEnt}^{*}
     - \cT_{\pi^{*}}^{0,\cEnt} V_{K-2}
     + \cT_{\pi^{*}}^{0,\cEnt} V_{K-2}
     - \bL^{\rcEnt} \Psi_{K-1}
    \\
    &= \gamma P^{\pi^{*}} \left(V_{\cEnt}^{*} - V_{K-2}\right)
     + \Delta_{K-1}
    ,
  \end{align*}
  where $\Delta_{K-1} = \cT_{\pi^{*}}^{0,\cEnt} V_{K-2} - \bL^{\rcEnt} \Psi_{K-1}$.
  Observing that
  \begin{align*}
    \bL^{\rcEnt} \Psi_{K-1}
    &= \dprod{\pi_{K}}{\Psi_{K-1}} + \rcEnt \cH(\pi_{K})
    \\
    &= \max_{\pi} \dprod{\pi}{\Psi_{K-1}} + \rcEnt \cH(\pi)
    \\
    &\ge \dprod{\pi^{*}}{\Psi_{K-1}} + \rcEnt \cH(\pi^{*})
    \\
    &= \dprod{\pi^{*}}{
      R + \rcKL {\bALc}(A_{K-2}) + \gamma P \dprod{\pi_{K-1}}{\Psi_{K-2} - {\bALs}(A_{K-2})}
    } + (\cEnt + \rcKL\rcEnt) \cH(\pi^{*})
    ,
  \end{align*}
  we have
  \begin{align*}
    \Delta_{K-1}
    &= \dprod{\pi^{*}}{R + \gamma P V_{K-2}} + \cEnt \cH(\pi^{*}) - \bL^{\rcEnt} \Psi_{K-1}
    \\
    &\le
      \dprod{\pi^{*}}{\gamma P V_{K-2}} - \dprod{\pi^{*}}{
      \rcKL {\bALc}(A_{K-2}) + \gamma P \dprod{\pi_{k-1}}{\Psi_{K-2} - {\bALs}(A_{K-2})}
      } - \rcKL\rcEnt \cH(\pi^{*})
    \\
    &=
    \dprod{\pi^{*}}{
      \rcKL \left(A_{\cEnt}^{*} - {\bALc}(A_{K-2})\right) - \gamma P \dprod{\pi_{K-1}}{A_{K-2} - {\bALs}(A_{K-2})}
    }
    \\
    &\eqdef \Delta^{\bALc\bALs}_{K-1}
    .
  \end{align*}
  Thus, it follows that
  \begin{align*}
    V_{\cEnt}^{*} - V_{K-1}
    &\le \gamma P^{\pi^{*}} \left(V_{\cEnt}^{*} - V_{K-2}\right) + \Delta^{\bALc\bALs}_{K-1}
    \\
    &\le
      \bigl(\gamma P^{\pi^{*}}\bigr)^{K-1} \left(V_{\cEnt}^{*} - V_{0}\right)
      + \sum_{k=1}^{K-1} \bigl(\gamma P^{\pi^{*}}\bigr)^{K-k-1} \Delta^{\bALc\bALs}_{k}
    .
 \end{align*}
 Plugging the above into \eqref{eq:BAL_telescopic_bound} and
 taking $\nInf{\cdot}$ on both sides, we obtain
 \begin{align}
   \nInf{V_{\cEnt}^{*}-V_{\cEnt}^{\pi_{K+1}}}
    &\le \frac{2\gamma}{1-\gamma}\left[
      2 \gamma^{K-1} V^\cEnt_{\rm max} +
      \sum_{k=1}^{K-1}\gamma^{K-k-1}\nInf{\Delta_{k}^{\bALc\bALs}}\right]
    .
 \end{align}
\end{proof}


% \clearpage

\section{Additional Experimental Details.}
\label{ss:appendix:experimental_details}

\subsection{BAL on Grid World.}
\label{ss:appendix:experimental_details:toy}

Figure \ref{fig:maze} shows the grid world environment used in Section \ref{ss:experiment:toy}. The reward is $r=1$ at the top-right and botom left corners, $r=2$ at the bottom-right corner and $r=0$ otherwise.
The action space is $\cA=\{{\rm North}, {\rm South}, {\rm West}, {\rm East}\}$. An attempted action fails with probability $0.1$ and random action is performed uniformly.
We set $\gamma=0.99$.
We chose $\rcEnt=0.02$ and $\rcKL=0.99$,
thus $\cEnt = (1 - \rcKL) \rcEnt = 0.0002$ and $\cKL = \rcKL \rcEnt = 0.0198$.
Since the transition kernel $P$ and the reward function $R$ are directly available for this environment,
we can perform the model-based M-VI \eqref{eq:imdvi} and BAL \eqref{eq:bal} schemes.
We performed $100$ independent runs with random initialization of $\Psi$ by
$\Psi_{0}(s,a) \sim {\rm Unif}(-V_{\rm max}^{\tau},V_{\rm max}^{\tau})$.
Figure \ref{fig:vi_curve} compares the normalized value of the suboptimality $\|V^{\pi_{k}}-V^{*}_{\cEnt}\|_{\infty}$,
where we computed $V^{*}_{\cEnt}$ by the recursion
$
V_{k+1} = \cT^{\cEnt} V_{k} = \bL^{\cEnt}(R + \gamma P V_{k})
$ with $V_{0}(s)=0$ for all state $s\in\cS$.
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.4\linewidth]{"fig/maze.pdf"}
    \caption{
      Grid world environment for model-based experiment.
    }
    \label{fig:maze}
  \end{center}
\end{figure}


\subsection{MDAC on Mujoco and DMC Control Suite.}
\label{ss:appendix:experimental_details:mujoco}

We used
PyTorch\footnote{\url{https://github.com/pytorch/pytorch}}
and
Gymnasium\footnote{\url{https://github.com/Farama-Foundation/Gymnasium}}
for all the experiments.
We used rliable\footnote{\url{https://github.com/google-research/rliable}}
to calculate the IQM scores.
MDAC is implemented based on SAC agent from
CleanRL\footnote{\url{https://github.com/vwxyzjn/cleanrl}}.
Each trial of MDAC run was performed by a single NVIDIA V100 with 8 CPUs
and took approximately 8 hours for 3M environment steps.
For the baselines, we used SAC agent from CleanRL
with default parameters from the original paper.
We used author's implementation\footnote{\url{https://github.com/sfujim/TD3}} for TD3 with default parameters.

Table \ref{tab:shared_params} summarizes the hyperparameter values for MDAC,
which are equivalent to the values for SAC except the additional $\rcKL$.
\begin{table}[h]
  \renewcommand{\arraystretch}{1.1}
  \centering
  \caption{MDAC Hyperparameters}
  \label{tab:shared_params}
  \vspace{1mm}
  \begin{tabular}{l| l }
    \toprule
    Parameter &  Value \\
    \midrule
    optimizer &Adam \citep{Kingma2015} \\
    learning rate & $3 \cdot 10^{-4}$ \\
    discount factor $\gamma$ &  0.99 \\
    replay buffer size & $10^6$ \\
    number of hidden layers (all networks) & 2 \\
    number of hidden units per layer & 256 \\
    number of samples per minibatch & 256 \\
    nonlinearity & ReLU \\
    target smoothing coefficient by polyack averaging ($\polyak$) & 0.005 \\
    target update interval & 1 \\
    gradient steps per environmental step & 1 \\
    reparameterized KL coefficient $\rcKL$ & $1-(1-\gamma)^2$ \\
    entropy target $\bar{\cH}$ to optimize $\cEnt = (1-\rcKL)\rcEnt$ & $-{\rm dim}(\cA)$ \\
    \bottomrule
  \end{tabular}
\end{table}




\paragraph{Per-environment results.}

Here, we provide per-environment results for ablation studies.
Figure
\ref{fig:ablation_fg_use_both_all},
\ref{fig:ablation_fg_type_all},
\ref{fig:benchmarking_all} and
\ref{fig:benchmarking_all:dmc}
show the per-environment results for Figure
\ref{fig:ablation_fg_use_both},
\ref{fig:ablation_fg_type},
\ref{fig:benchmarking} and.
\ref{fig:benchmarking:dmc}, respectively.


\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\linewidth]{"fig/tanh_works_w_lsm2_score_test_meanmM_pe10_mdac.v7_kappaFixed_gauss_None_mdac.v7_identity_lsm2_mdac.v7_kappaFixed_gauss_rsquash_naive_10.pdf"}
    \caption{
      Per-environment performances for Figure \ref{fig:tanh_works}.
      The mean scores of 10 independent runs are reported.
      The shaded region corresponds to the minimum and maximum scores over the 10 runs.
    }
    \label{fig:tanh_works_all}
  \end{center}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\linewidth]{"fig/bound_both_works_score_test_meanmM_pe10_mdac.v7_kappaFixed_gauss_None_mdac.v7_kappaFixed_gauss_squash_mdac.v7_kappaFixed_gauss_rsquash_10.pdf"}
    \caption{
      Per-environment performances for Figure \ref{fig:ablation_fg_use_both}.
      The mean scores of 10 independent runs are reported.
      The shaded region corresponds to the minimum and maximum scores over the 10 runs.
    }
    \label{fig:ablation_fg_use_both_all}
  \end{center}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\linewidth]{"fig/compare_fg_score_test_meanmM_pe10_mdac.v7_kappaFixed_gauss_None_mdac.v7_kappaFixed_gauss_rclip_mdac.v7_kappaFixed_gauss_rclip_0.1_mdac.v7_kappaFixed_gauss_rsquash_naive_mdac.v7_kappaFixed_gauss_rsquash_mdac.v7_binary_10.pdf"}
    \caption{
      Per-environment performances for Figure \ref{fig:ablation_fg_type}.
      The mean scores of 10 independent runs are reported.
      The shaded region corresponds to the minimum and maximum scores over the 10 runs.
    }
    \label{fig:ablation_fg_type_all}
  \end{center}
\end{figure}

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=\linewidth]{"fig/benchmarking_score_test_meanmM_pe10_sac_clean.v2_lrQ3e-4_td3_official.v1_mdac.v7_kappaFixed_gauss_rclip_0.1_10.pdf"}
    \caption{
      Per-environment performances.
      The mean scores of 10 independent runs are reported.
      The shaded region corresponds to the minimum and maximum scores over the 10 runs.
    }
    \label{fig:benchmarking_all}
  \end{center}
  % \vspace{-\baselineskip}
\end{figure}

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=\linewidth]{"fig/dmc_score_test_meanmM_pe10_sac_clean.lr3e-4_10M_full_mdac.lr3e-4_10M_full_30_until5000.pdf"}
    \caption{
      Per-environment performances in \texttt{dog} domain from DeepMind Control Suite.
      The mean scores of 30 independent runs are reported.
      The shaded region corresponds to the minimum and maximum scores over the 30 runs.
    }
    \label{fig:benchmarking_all:dmc}
  \end{center}
  % \vspace{-\baselineskip}
\end{figure}




\paragraph{Quantities in TD target under clipping.}

Figure \ref{fig:clipping_frequency_WHA} compares the clipping frequencies for
$f=g={\rm clip}(x, -1, 1)$ and $f=g={\rm clip}(x/10, -1, 1)$.
Figure \ref{fig:scales_for_clip} compares the the quantities in TD target.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{"fig/tidy_freqs_until_seed43_100_fs20.pdf"}
    \caption{
      Comparison of clipping frequencies.
      Left: \texttt{Walker2d-v4},
      Middle: \texttt{HalfCheetah-v4}.
      Right: \texttt{Ant-v4}.
    }
    \label{fig:clipping_frequency_WHA}
  \end{center}
\end{figure}
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{"fig/joint_tidy_scales_clip_twin.pdf"}
    \caption{
      Scale comparison of the quantities in TD target.
      Top row: ${\rm clip}(x, -1, 1)$,
      Bpttom row: ${\rm clip}(x/10, -1, 1)$,
      Left column: \texttt{Walker2d-v4},
      Middle column: \texttt{HalfCheetah-v4}.
      Right column: \texttt{Ant-v4}.
    }
    \label{fig:scales_for_clip}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
