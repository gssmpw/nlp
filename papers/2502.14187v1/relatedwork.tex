\section{Related Work}
\label{sec:related}

FL has gained significant attention as a privacy-preserving paradigm for training AI models across decentralized data sources. Notably, Ye et al. \cite{ye2024openfedllm, ye2024fedllm} have designed realistic benchmarks for FL tailored to LLMs. These benchmarks incorporate a diverse range of aggregation methods, including FedAvg \cite{mcmahan2017communication}, FedProx \cite{li2020federated}, SCAFFOLD \cite{karimireddy2020scaffold}, FedAvgM \cite{hsu2019measuring}, FedYogi \cite{reddiadaptive}, FedAdagrad \cite{reddiadaptive}, and FedAdam \cite{reddiadaptive}, each of which offers unique approaches to combining model updates from distributed clients at a central server.

A key strength of these benchmarks is their use of real-world data from Chatbot-arena Conversations \cite{zheng2023judging}. This dataset consists of authentic human-chatbot interactions, organized by User ID to simulate clients in a federated learning setting, thereby addressing the heterogeneity and sparsity challenges typical in FL scenarios. 
In particular, Ye et al.'s benchmarks evaluate performance across three critical metrics: 
\begin{enumerate}
    \item \textbf{MT-Bench-1:} A benchmark for assessing 1-turn conversational abilities \cite{zheng2023judging}.
    \item \textbf{Vicuna:} A benchmark designed to evaluate instruction-following capabilities \cite{chiang2023vicuna}.
    \item \textbf{AdvBench:} A benchmark focused on AI safety, measuring a modelâ€™s robustness to adversarial prompts \cite{zou2023universal}.
\end{enumerate}

Their results demonstrate decent performance for DPO across these benchmarks. However, DPO's reliance on paired responses and its susceptibility to data heterogeneity limit its applicability in real-world FL scenarios. In our work, we build upon these benchmarks and demonstrate that KTO, a simpler and more flexible fine-tuning method, outperforms DPO in terms of both effectiveness and adaptability. 

%