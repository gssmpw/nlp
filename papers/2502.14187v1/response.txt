\section{Related Work}
\label{sec:related}

FL has gained significant attention as a privacy-preserving paradigm for training AI models across decentralized data sources. Notably, Ye et al. **Ye, "Federated Learning Benchmarks"** have designed realistic benchmarks for FL tailored to LLMs. These benchmarks incorporate a diverse range of aggregation methods, including FedAvg ____**, FedProx ____**, SCAFFOLD ____**, FedAvgM ____**, FedYogi ____**, FedAdagrad ____**, and FedAdam ____**, each of which offers unique approaches to combining model updates from distributed clients at a central server.

A key strength of these benchmarks is their use of real-world data from Chatbot-arena Conversations ____**. This dataset consists of authentic human-chatbot interactions, organized by User ID to simulate clients in a federated learning setting, thereby addressing the heterogeneity and sparsity challenges typical in FL scenarios. 
In particular, Ye et al.'s benchmarks evaluate performance across three critical metrics: 
\begin{enumerate}
    \item \textbf{MT-Bench-1:} A benchmark for assessing 1-turn conversational abilities ____**.
    \item \textbf{Vicuna:} A benchmark designed to evaluate instruction-following capabilities ____**.
    \item \textbf{AdvBench:} A benchmark focused on AI safety, measuring a modelâ€™s robustness to adversarial prompts ____**.
\end{enumerate}

Their results demonstrate decent performance for DPO across these benchmarks. However, DPO's reliance on paired responses and its susceptibility to data heterogeneity limit its applicability in real-world FL scenarios. In our work, we build upon these benchmarks and demonstrate that KTO, a simpler and more flexible fine-tuning method, outperforms DPO in terms of both effectiveness and adaptability.