\section{Related Work}
\subsection{Protecting Intellectual Property of Deep Neural Networks}
The protection of intellectual property (IP) in deep neural networks (DNNs) has become a critical area of research due to the increasing deployment of DNNs in commercial and sensitive applications. DNN models require substantial computational resources, proprietary datasets, and extensive fine-tuning, making them valuable assets prone to unauthorized duplication and misuse \cite{uchida2017embedding, adi2018turning}.

Existing approaches to protecting the IP of DNNs can be broadly categorized into invasive and non-invasive methods \cite{fan2019rethinking}. Invasive methods involve embedding unique identifiers into the model itself, such as watermarks \cite{zhang2018protecting} or cryptographic signatures \cite{wang2021riga}. These methods have demonstrated resilience against model theft but often introduce overhead and may degrade performance. Non-invasive methods, including fingerprinting and behavioral authentication, focus on capturing unique model characteristics without modifying the model’s architecture or weights \cite{guan2020are}.

A prominent line of research focuses on model fingerprinting, which leverages decision boundary analysis to detect stolen models \cite{peng2022fingerprinting}. While effective in black-box settings, fingerprinting methods face challenges in scalability and robustness against fine-tuning and adversarial attacks \cite{korus2022computational}. Additionally, watermarking-based methods such as DeepMarks \cite{chen2018deepmarks} and DeepSigns \cite{rouhani2018deepsigns} have been proposed to embed resilient identifiers within model parameters. However, these approaches are susceptible to removal via model pruning and parameter tuning \cite{rouhani2019deepmarks}.

A critical gap in existing IP protection methods is the lack of standardization and legal enforcement mechanisms. Most frameworks assume the presence of a trusted verification authority, which may not always be feasible \cite{li2022persistent}. Moreover, current solutions struggle with balancing robustness, imperceptibility, and computational efficiency \cite{lou2022reversible}.

\subsection{Watermarking Deep Neural Networks}
Watermarking is a widely explored technique for embedding unique identifiers within DNN models to assert ownership. It can be categorized into black-box and white-box watermarking. Black-box watermarking involves embedding trigger-based patterns in the model’s decision-making process, which can be detected using specific queries. White-box watermarking, on the other hand, modifies internal parameters to encode ownership information in an imperceptible manner \cite{zhang2021robustness}.

A seminal work by Uchida et al. \cite{uchida2017embedding} introduced a method for embedding watermarks into model weights, demonstrating resilience against model compression and fine-tuning. Other approaches, such as adversarial frontier stitching \cite{lemerrer2019adversial}, leverage backdoor triggers to verify ownership without affecting model performance. However, these techniques have been shown to be vulnerable to model distillation and adversarial countermeasures \cite{kuribayashi2021survey}.

Recent advances in persistent and unforgeable watermarking techniques have addressed some of these vulnerabilities. Li et al. \cite{li2022persistent} introduced wonder filters, a novel watermarking primitive that embeds a persistent bit-sequence into a model during its initial training phase. Unlike previous watermarking schemes, wonder filters offer strong resilience against fine-tuning and adversarial attacks by leveraging out-of-bound values and null-embedding techniques. This ensures that the watermark cannot be removed or forged without destroying the model’s functionality. Experimental results demonstrate that wonder filters achieve high levels of persistence and piracy resistance, making them a promising direction for future watermarking techniques \cite{li2022persistent}.

Reversible watermarking techniques, which allow watermark extraction without degrading model performance, have gained traction \cite{lou2022reversible}. Methods like Greedy Residuals Watermarking \cite{liu2021greedy} have been proposed to enhance robustness while maintaining fidelity. Despite these advancements, challenges persist in ensuring watermark detectability under extensive model modifications \cite{zhang2021robustness}.

One major research gap in DNN watermarking is the lack of a theoretical framework for assessing watermark capacity and robustness. Existing solutions rely on empirical evaluations without formal guarantees on the detectability and permanence of embedded marks \cite{sun2023deepintellectualpropertyprotection}. Additionally, watermarking techniques primarily target classification models, with limited applicability to generative and reinforcement learning models \cite{fan2021ownership}.

\subsection{Watermarking EEG-Based Neural Networks for IP Protection}
The protection of EEG-based deep learning models has gained increasing importance due to their unique privacy and security concerns. EEG models, widely used in brain-computer interface (BCI) applications, encode highly sensitive neurological data, making them valuable assets in need of robust IP protection \cite{xu2023protecting}.

Despite the extensive research on watermarking standard DNNs, the application of watermarking for EEG-based models remains relatively unexplored. The work by Xu et al. \cite{xu2023protecting} represents the first attempt to integrate watermarking into EEG-based neural networks for IP protection. Their method employs a trigger set specifically designed for EEG data, ensuring that watermarks can be embedded without significantly affecting model performance. By leveraging neuroscientific insights, they propose three key constraints that EEG-based watermarks should satisfy: symbolic representation, consistency with EEG input characteristics, and uniqueness.

Their study demonstrates the robustness of their watermarking approach against common anti-watermarking attacks such as fine-tuning, transfer learning, and pruning. The proposed watermarking framework embeds distinct patterns into EEG models while maintaining classification accuracy, making it resistant to piracy \cite{xu2023protecting}. However, their work focuses exclusively on convolutional neural networks (CNNs), limiting its generalizability to other deep learning architectures. Additionally, a significant limitation of their approach is the lack of authentication mechanisms. Their method verifies watermark presence but does not establish a direct link between the model and its rightful owner, making it vulnerable to ownership disputes.

\subsection{Novelty of Our Work}
While significant progress has been made in protecting the intellectual property (IP) of deep neural networks (DNNs), existing methods fall short when applied to EEG-based models. Current approaches either lack robust mechanisms for establishing definitive ownership claims or fail to address the unique challenges posed by EEG data \cite{xu2023protecting}. Our work bridges this gap by introducing the first framework for protecting EEG-based neural networks using unforgeable watermarks. By integrating cryptographic techniques, we ensure a definitive and undisputable link between the model and its owner, addressing critical limitations in ownership verification and robustness against attacks. This represents a significant advancement in IP protection for EEG-based applications.
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.