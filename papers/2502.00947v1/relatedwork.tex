\section{Related Work}
\label{sec:related-work}

\textit{Perturbation bounds}.\quad A detailed sensitivity analysis of classical scaling was first studied in a series of works by \cite{sibson1978studies,sibson1979studies,sibson1981studies}. For a small local perturbation {$\tilde{\Del} \asymp \Del + t\Phi$}, they show that {$\smallnorm{\tilde\X - \X}_2 \asymp C t \smallnorm{\Phi}_2$} via local first-order approximations. \cite{de2004sparse} subsequently extended the generality of these first order perturbations. More recently, \cite{arias2020perturbation} provide refined perturbation bounds and improve on earlier results in two ways. First, they show that the output of \cref{alg:cs} is stable with respect to arbitrary perturbations in the Schatten $r$-norm, i.e., ${\min_{g \in \euc{p}}\smallnorm{\tilde\X - g(\X)}_{r} \lesssim \smallnorm{\Delc(\tilde\X) - \Delc(\X)}_r^2}$. Second, by characterizing the stability of the orthogonal Procrustes problem, their  perturbation bounds explicitly characterize the arbitrary rigid transformation $g \in \euc{p}$ that minimizes the reconstruction error. As we shall see later on, this will play a key role in our analyses. However, the perturbation bounds in these works operate in the deterministic setting, and do not account for the randomness arising from $\X$ or the noise $\Eps$.

\textit{Noisy dissimilarities}.\quad When $\Eps$ is random, \cite{javanmard2013localization} study the problem of localization from noisy measurements, i.e., where the observed dissimilarity $D$ is both corrupted by noise and incomplete measurements. Their method considers a semidefinite relaxation of the objective in \cref{eq:cs-objective}, and establish conditions under which their solution recovers the true configuration with the $\loss\rmse$ loss. A similar problem is also considered by \cite{chatterjee2015matrix} in the context of matrix completion. Both of these works focus on the case where the noise is bounded, and provide guarantees on the reconstruction error when this bound is known \textit{a priori}. \cite{zhang2016distance} consider the case where the dissimilarities are completely observed but corrupted by \iid{} sub-Gaussian noise; in this setting (and, although the authors don't explicitly make the connection to the work of  \citealp{javanmard2013localization}), they show that the output from the same semidefinite relaxation is consistent in the $\loss\rmse$ loss.

\textit{Classical scaling with noisy dissimilarities.}\quad To the best of our knowledge, detailed statistical analyses of classical scaling in the presence of noise have only been considered recently. \cite{peterfreund2021multidimensional} and \cite{little2023analysis} consider the spiked model where: $d_{ij} = \norm{\tilde{x}_i - \tilde{x}_j}^2$ for ${x}'_i = x_i + z_i$ where $z_i$ are some (possibly high dimensional) noise. In other words, { the dissimilarities are the Euclidean distances associated with a noisy (low-dimensional) configuration} $\tilde{\X} = \X + \mathrm{Z}$. 
In the high-dimensional regime, and when the eigenvalues used in classical scaling are appropriately thresholded, \cite{peterfreund2021multidimensional} establish consistency in the $\loss\rmse$ loss. In a similar vein, \cite{little2023analysis} shed light on the signal-to-noise thresholds for optimal clustering using the output of classical scaling in the high-dimensional regime by establishing the convergence rates in $\loss_{\max}$ ({{where the matrix $\norm{\cdot}_{\max}$-norm is used in the reconstruction error from \cref{eq:reconstruction-error}}}). Notably, in both of these works, the noise is assumed to manifest in the configuration which generates the dissimilarities and not in the observed dissimilarities themselves. To this end, and, perhaps most relevant to the present work, \cite{li2020central} provide the first detailed analysis of the consistency of classical scaling when $D = \Del + \Eps$; they consider three types of noise models---all of which, in some way, rely on the noise being \iid{} and sub-Gaussian. Under these assumptions and for an appropriate rigid transformation $g \in \euc{p}$, they show that in the $\loss\avg$ loss, the output of classical scaling is $\sqrt{n}$-consistent. This allows them to establish a central limit theorem for the output of classical scaling: the embedding produced by classical scaling asymptotically converges to a Gaussian distribution centered on $g(\X)$---the true configuration after a rigid transformation. These contributions form the main backdrop for the present work.