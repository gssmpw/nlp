\section{Related work}
Neural Architecture Search (NAS) is a technique designed to automate the creation of neural network architectures. Its primary goal is to enhance model performance and simplify the design process by algorithmically identifying optimal network structures. Traditional NAS methods **Zoph, "Learning Transferable Architectures for Scalable Image Recognition"** often involve training and evaluating a large number of candidate architectures from scratch, making them both computationally expensive and time-consuming.
To address these challenges and improve efficiency, researchers have proposed several advancements in recent years. Among these, {\bf One-Shot NAS} and the integration of {\bf LLMs for NAS} have emerged as two prominent and promising directions.

\subsection{One-Shot NAS}
One-Shot NAS **Pham, "Efficient Neural Architecture Search via Parameter Sharing"** has revolutionized NAS by introducing the concept of a supernet, enabling the performance evaluation of multiple architectures without the need for individually training them. Early works such as **Liu, "DARTS: Differentiable Architecture Search"** laid the groundwork for this approach, demonstrating the effectiveness of weight-sharing techniques. However, these methods faced several challenges, including weight entanglement and suboptimal fairness in architecture evaluation **Brock, "SMASH: One-Shot Model Architectures via Meta Learning"**. Recent research has sought to address these limitations through various innovations. For instance, **Elthuzayel, "One-Shot Neural Architecture Search with Weight Sharing"** introduced an angle-based metric to simplify the search space by pruning unpromising candidates, thereby reducing the difficulty of identifying high-quality architectures. Similarly, **Liu, "P-DARTS: Partial Channel and Resolution Down-sampling for Efficient Network Design"** employed incremental learning to bridge the gap between the search and evaluation phases. In **Chen, "Progressive Neural Architecture Search"**, node normalization and decorrelation discretization strategies were proposed to improve generality and stability. Additionally, **Wu, "NAS-FPN: Learning Long-Range Spatiotemporal Dependencies for Single-View Depth Estimation"** utilized the Shapley value to assess the importance of operations, while **Guo, "One-Shot Neural Architecture Search with Graph Attention Networks"** introduced a hybrid approach that combined evolutionary strategies with gradient descent, effectively mitigating local issues through global optimization while maintaining efficiency. Despite these advancements, One-Shot NAS methods continue to struggle with several key challenges. They are particularly prone to local optima, especially in complex search spaces. Moreover, gradient-based One-Shot NAS approaches are highly sensitive to initial conditions and often encounter difficulties in thoroughly exploring non-smooth or multi-modal search spaces. Additionally, %the approach lacks interpretability regarding the underlying design principles. 
most existing NAS methods still depend on expert knowledge to design the search space, search algorithms, and performance evaluation systems **Zhang, "Understanding the Darts Method for Model Compression"**. These limitations highlight the need for further innovation to fully unlock the potential of One-Shot NAS.


% One-Shot NAS **Pham, "Efficient Neural Architecture Search via Parameter Sharing"** has transformed NAS by introducing a supernet that allows performance evaluation of multiple architectures without individually training them. Early works such as **Liu, "DARTS: Differentiable Architecture Search"** established the foundation of this approach, demonstrating the viability of weight-sharing techniques. However, these methods faced challenges including weight entanglement and suboptimal fairness in architecture evaluation **Brock, "SMASH: One-Shot Model Architectures via Meta Learning"**. Recent research has sought to address these issues. For example, **Elthuzayel, "One-Shot Neural Architecture Search with Weight Sharing"** proposed an angle-based metric to simplify the search space by eliminating unpromising candidates, reducing the challenges in finding high-quality architectures; **Liu, "P-DARTS: Partial Channel and Resolution Down-sampling for Efficient Network Design"** adopted incremental learning to bridge the gap between search and evaluation; **Chen, "Progressive Neural Architecture Search"** introduced node normalization and decorrelation discretization strategies to enhance generality and stability; **Wu, "NAS-FPN: Learning Long-Range Spatiotemporal Dependencies for Single-View Depth Estimation"** applied the Shapley value to evaluate operation importance; and **Guo, "One-Shot Neural Architecture Search with Graph Attention Networks"** proposed a hybrid approach of evolutionary strategies and gradient descent, effectively alleviating local issues through global strategies while maintaining efficiency. 
% However, these One-shot NAS methods are still primarily affected by local optima, especially in complex search spaces. Moreover, gradient-based One-shot NAS methods are highly sensitive to initial conditions and often struggle to explore the entire search space in non-smooth or multi-modal problems.

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.95\textwidth]{p1.pdf}
    % \captionsetup{font={footnotesize}}
  \caption{Prompt framework for Self-Evolution.}
  \label{fig:p1}
  \vspace{-4mm}
\end{figure}

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.95\textwidth]{p2.pdf}
    % \captionsetup{font={footnotesize}}
  \caption{Prompt framework for Knowledge Inspiration.}
  \label{fig:p2}
  \vspace{-4mm}
\end{figure}

\subsection{LLM for NAS}

Recent advances have introduced LLMs into the NAS field, aiming to enhance the automation of architecture design. With their exceptional performance across various domain-specific tasks, LLMs have unlocked new opportunities in this area. For example, studies such as **Hoang, "Neural Architecture Search with Transformers"** have successfully employed LLMs, including GPT-4, to generate architectures for Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), achieving promising results. Beyond architecture generation, LLMs have been utilized as performance predictors to accelerate the NAS search process **Li, "Efficient Neural Architecture Search with Reinforcement Learning"**. They have also been applied to optimize search spaces and design architectures, significantly improving the automation and interpretability of the overall process **Zhou, "Neural Architecture Search with Deep Reinforcement Learning"**. Despite their promising capabilities, the use of LLMs for NAS is still relatively new and comes with certain limitations, such as lower performance or a reliance on relevant data to achieve high-quality results. We believe the issue lies in the fact that the capabilities of LLMs have yet to be fully explored. To address these challenges, our solution, SEKI, fully harnesses the potential of LLMs through self-evolution and knowledge inspiration, and delivers both high efficiency and performance without the need for any data/ prior knowledge on architectures.