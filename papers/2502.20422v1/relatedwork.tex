\section{Related work}
Neural Architecture Search (NAS) is a technique designed to automate the creation of neural network architectures. Its primary goal is to enhance model performance and simplify the design process by algorithmically identifying optimal network structures. Traditional NAS methods~\cite{2017Learning, 2018Regularized} often involve training and evaluating a large number of candidate architectures from scratch, making them both computationally expensive and time-consuming.
To address these challenges and improve efficiency, researchers have proposed several advancements in recent years. Among these, {\bf One-Shot NAS} and the integration of {\bf LLMs for NAS} have emerged as two prominent and promising directions.

\subsection{One-Shot NAS}
One-Shot NAS \cite{brock2017smashoneshotmodelarchitecture} has revolutionized NAS by introducing the concept of a supernet, enabling the performance evaluation of multiple architectures without the need for individually training them. Early works such as \cite{2018Efficient,liu2018darts,xu2019pc} laid the groundwork for this approach, demonstrating the effectiveness of weight-sharing techniques. However, these methods faced several challenges, including weight entanglement and suboptimal fairness in architecture evaluation \cite{chen2019progressive,zela2019understanding}. Recent research has sought to address these limitations through various innovations. For instance, \cite{hu2020angle} introduced an angle-based metric to simplify the search space by pruning unpromising candidates, thereby reducing the difficulty of identifying high-quality architectures. Similarly, \cite{DBLP:journals/corr/abs-2006-10355} employed incremental learning to bridge the gap between the search and evaluation phases. In \cite{DBLP:journals/corr/abs-2108-11014}, node normalization and decorrelation discretization strategies were proposed to improve generality and stability. Additionally, \cite{xiao2022shapley} utilized the Shapley value to assess the importance of operations, while \cite{Cai_Chen_Liu_Ling_Lai_2024} introduced a hybrid approach that combined evolutionary strategies with gradient descent, effectively mitigating local issues through global optimization while maintaining efficiency. Despite these advancements, One-Shot NAS methods continue to struggle with several key challenges. They are particularly prone to local optima, especially in complex search spaces. Moreover, gradient-based One-Shot NAS approaches are highly sensitive to initial conditions and often encounter difficulties in thoroughly exploring non-smooth or multi-modal search spaces. Additionally, %the approach lacks interpretability regarding the underlying design principles. 
most existing NAS methods still depend on expert knowledge to design the search space, search algorithms, and performance evaluation systems \cite{chen2023evopromptinglanguagemodelscodelevel}. These limitations highlight the need for further innovation to fully unlock the potential of One-Shot NAS.


% One-Shot NAS \cite{brock2017smashoneshotmodelarchitecture} has transformed NAS by introducing a supernet that allows performance evaluation of multiple architectures without individually training them. Early works such as \cite{2018Efficient,liu2018darts,xu2019pc} established the foundation of this approach, demonstrating the viability of weight-sharing techniques. However, these methods faced challenges including weight entanglement and suboptimal fairness in architecture evaluation \cite{chen2019progressive,zela2019understanding}. Recent research has sought to address these issues. For example, \cite{hu2020angle} proposed an angle-based metric to simplify the search space by eliminating unpromising candidates, reducing the challenges in finding high-quality architectures; \cite{DBLP:journals/corr/abs-2006-10355} adopted incremental learning to bridge the gap between search and evaluation; \cite{DBLP:journals/corr/abs-2108-11014} introduced node normalization and decorrelation discretization strategies to enhance generality and stability; \cite{xiao2022shapley} applied the Shapley value to evaluate operation importance; and \cite{Cai_Chen_Liu_Ling_Lai_2024} proposed a hybrid approach of evolutionary strategies and gradient descent, effectively alleviating local issues through global strategies while maintaining efficiency. 
% However, these One-shot NAS methods are still primarily affected by local optima, especially in complex search spaces. Moreover, gradient-based One-shot NAS methods are highly sensitive to initial conditions and often struggle to explore the entire search space in non-smooth or multi-modal problems.

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.95\textwidth]{p1.pdf}
    % \captionsetup{font={footnotesize}}
  \caption{Prompt framework for Self-Evolution.}
  \label{fig:p1}
  \vspace{-4mm}
\end{figure}

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.95\textwidth]{p2.pdf}
    % \captionsetup{font={footnotesize}}
  \caption{Prompt framework for Knowledge Inspiration.}
  \label{fig:p2}
  \vspace{-4mm}
\end{figure}

\subsection{LLM for NAS}

Recent advances have introduced LLMs into the NAS field, aiming to enhance the automation of architecture design. With their exceptional performance across various domain-specific tasks, LLMs have unlocked new opportunities in this area. For example, studies such as \cite{chen2023evopromptinglanguagemodelscodelevel,zhang2023automlgptautomaticmachinelearning,nasir2024llmatic,qin2024flnasfairnessnasresource,wang2024graphneuralarchitecturesearch,dong2023heterogeneous} have successfully employed LLMs, including GPT-4, to generate architectures for Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), achieving promising results. Beyond architecture generation, LLMs have been utilized as performance predictors to accelerate the NAS search process \cite{zhang2023automlgptautomaticmachinelearning,jawahar2024llmperformancepredictorsgood,chen2024large,10448075}. They have also been applied to optimize search spaces and design architectures, significantly improving the automation and interpretability of the overall process \cite{zhou2024designprincipletransferneural}. Despite their promising capabilities, the use of LLMs for NAS is still relatively new and comes with certain limitations, such as lower performance or a reliance on relevant data to achieve high-quality results. We believe the issue lies in the fact that the capabilities of LLMs have yet to be fully explored. To address these challenges, our solution, SEKI, fully harnesses the potential of LLMs through self-evolution and knowledge inspiration, and delivers both high efficiency and performance without the need for any data/ prior knowledge on architectures.