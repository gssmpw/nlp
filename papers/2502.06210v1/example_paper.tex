%%%%%%%% hideinfo 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{hideinfo2025} with \usepackage[nohyperref]{hideinfo2025} above.
\usepackage{hyperref}

\usepackage{colortbl}
\usepackage{xcolor}         % colors
\definecolor{ac_gray}{gray}{.2}
\definecolor{forestgreen}{RGB}{47, 159, 87}%
\definecolor{forestred}{RGB}{255,70,70}%

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{hideinfo2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{hideinfo2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{float}

\usepackage{booktabs}
\usepackage{array,multirow,graphicx}
\usepackage{pifont} % http://ctan.org/pkg/pifont
\newcommand{\cmark}{{\ding{51}}}%
\newcommand{\xmark}{{\ding{55}}}%

\usepackage{colortbl}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \hideinfotitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\hideinfotitlerunning{Position: Continual Learning Benefits from An Evolving Population over An Unified Model}

\begin{document}

\twocolumn[
\hideinfotitle{Position: Continual Learning Benefits from An Evolving Population over An Unified Model}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the hideinfo2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\hideinfosetsymbol{equal}{*}

\begin{hideinfoauthorlist}
\hideinfoauthor{Aojun Lu}{scu}
\hideinfoauthor{Junchao Ke}{scu}
\hideinfoauthor{Chunhui Ding}{scu}
\hideinfoauthor{Jiahao Fan}{scu}
\hideinfoauthor{Yanan Sun}{scu}
\end{hideinfoauthorlist}

\hideinfoaffiliation{scu}{College of Computer Science, Sichuan University, Chengdu, China}

\hideinfocorrespondingauthor{Yanan Sun}{ysun@scu.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\hideinfokeywords{Machine Learning, hideinfo}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \hideinfoEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\hideinfoEqualContribution} % otherwise use the standard text.

\begin{abstract}

%%%%%%%%%%%%%%
Deep neural networks have demonstrated remarkable success in machine learning; however, they remain fundamentally ill-suited for Continual Learning (CL). Recent research has increasingly focused on achieving CL without the need for rehearsal. Among these, parameter isolation-based methods have proven particularly effective in enhancing CL by optimizing model weights for each incremental task. Despite their success, they fall short in optimizing architectures tailored to distinct incremental tasks. To address this limitation, updating a group of models with different architectures offers a promising alternative to the traditional CL paradigm that relies on a single unified model. Building on this insight, this study introduces a novel Population-based Continual Learning (PCL) framework. PCL extends CL to the architectural level by maintaining and evolving a population of neural network architectures, which are continually refined for the current task through NAS. Importantly, the well-evolved population for the current incremental task is naturally inherited by the subsequent one, thereby facilitating forward transfer, a crucial objective in CL. Throughout the CL process, the population evolves, yielding task-specific architectures that collectively form a robust CL system. Experimental results demonstrate that PCL outperforms state-of-the-art rehearsal-free CL methods that employs a unified model, highlighting its potential as a new paradigm for CL.
\end{abstract}


\section{Introduction}
Natural intelligence possess the remarkable ability to continually learn and update knowledge without erasing previously acquired information. This capability is essential for humans to not only master new tasks but also to retain the skills or knowledge related to earlier tasks. Consequently, it is natural to expect that artificial intelligences should exhibit a similar ability, which has motivated the study of Continual Learning (CL)~\cite{van2022three, survey_1}. Unfortunately, current research reveals that deep neural networks, the cornerstone of modern visual models, tend to largely ``forget'' previously learned knowledge when trained on new tasks, a phenomenon known as \textit{catastrophic forgetting}~\cite{mccloskey1989catastrophic,goodfellow2013empirical}. This issue is a facet of the trade-off between model plasticity and stability: an excess of the
former interferes with the latter, and vice versa, known as \textit{stability-plasticity dilemma}~\cite{grossberg2013adaptive}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.96\linewidth]{./images/fig1}
    \caption{\textbf{Left.} Performance results on the 10 split tasks of CIFAR100 indicate that the more advanced ResNet-50 does not outperform ResNet-18 across all tasks when using independent models for each task. \textbf{Right.} This work optimizes the architecture for each incremental task to enhance CL through an evolving population. Notably, the well-evolved population for the current task is inherited by the next one (see blue arrows), thereby facilitating forward transfer.}
    \label{fig:fig1}
\end{figure*}

Perhaps the strongest solution for the stability-plasticity dilemma is storing a small subset of data from previous tasks, and then using it in the form of experience replay with the new task~\cite{icarl}. 
However, considering these methods may not be suitable for scenarios where data privacy is strictly concerned~\cite{efc,ldc}, recent studies have proposed various rehearsal-free CL methods. These methods include imposing regularization on network parameter changes and employing specialized network components for each task~\cite{survey_masana}. Among these, parameter isolation-based methods have particularly excelled in CL. These methods~\cite{li2019learn,bns} allocate a distinct parameter subspace for each task within the network to minimize the conflicts between old and new tasks. By isolating parameter subspaces, these methods prevent the overwriting of parameters containing old knowledge with new knowledge, thereby maintaining stability. Simultaneously, the specialized parameters are optimized for each task, thus benefiting plasticity. Nonetheless, the potential benefits of a specialized architecture for each task remain underexplored.

To motivate, certain existing studies~\cite{mirzadeh2022architecture,CL_design} have demonstrated that architecture plays an important role in CL. Notably, a recent study~\cite{CL_design} highlights that a suitable architecture can significantly enhance CL performance, with the improvement being comparable to that achieved through the superior CL methods. However, these investigations have not considered that the optimal architecture for each incremental task may vary substantially and a good generic architecture does not necessarily guarantee promising performance across all tasks. To illustrate this, we conduct an experiment in which two distinct architectures are employed to learn incremental tasks separately, and the results are depicted in Figure~\ref{fig:fig1}. Our observations reveal that although ResNet-50~\cite{he2016deep} generally exhibits superior performance across most tasks, it still lags behind ResNet-18 in certain instances (\textit{i.e.} tasks 3, 6, 8). These results not only demonstrate the significant impact of architecture but also suggest that it is suboptimal to employ the same architecture for all tasks in CL. 

Therefore, \textbf{we state our position that updating a group of models with diverse architectures presents a viable and promising alternative to the conventional CL paradigm, which relies on a single unified model}. 
Building on this insight, we propose a Population-based Continual Learning (PCL) framework, which leverages Neural Architecture Search (NAS)~\cite{zoph2016neural,nas_survey} to optimize the architecture for each task. This approach entails the iterative enhancement of the architecture to maximize performance on each incremental task. As shown in Figure~\ref{fig:fig1}, the cornerstone of this framework is a population of neural network architectures that continually evolve to facilitate the CL process. When a new task arises, the optimal architecture for the current task, along with its learned parameters, is archived within the CL system. Subsequently, the population is inherited by the next task and continues to evolve, with the focus shifting to accommodate the requirements of the new task. 

Throughout the CL process, the population of architectures evolves across multiple generations, yielding a specialized architecture for each task. Ultimately, the PCL approach results in a CL system that comprises a collection of task-specific architectures. In this way, PCL inherently incorporates the advantages of the parameter isolation method, which is characterized by enhanced stability due to the isolation of parameter subspaces. Furthermore, the PCL system provides not only specialized parameters but also a dedicated architecture for each task, thereby enhancing the plasticity of CL systems. Moreover, as the well-evolved population for the current incremental task is inherited by the next one, PCL naturally facilitates forward transfer, a crucial objective in CL. 

In summary, the contributions of this study can be outlined as follows:

\begin{itemize}

    \item We broaden the scope of existing CL techniques by proposing a novel framework which employs an evolving population of models with specialized network architectures to perform CL, \textit{i.e.}, PCL. Extensive experimental results demonstrate that PCL can achieve better CL performance than existing state-of-the-art methods that employs a unified model.

    \item We present a NAS strategy tailored for the automatic and efficient generation of task-specific networks for CL with multi models.
    
    \item Our proposed PCL method is rehearsal-free, and thus can be applied to scenarios where access to past data is strictly prohibited or impractical.

\end{itemize}

\section{Preliminaries}

Prior to further elaboration, key definitions related to CL are introduced. In CL, a dynamic data stream is partitioned into \(\mathcal{N}\) independent tasks \(\{\mathcal{T}_i\}_{i=0}^{N-1}\), where the data across tasks are non-overlapping (i.e., \(\mathcal{T}_i \cap \mathcal{T}_j = \varnothing\) for \(i \neq j\)). Each task \(\mathcal{T}_i\) is characterized by a dataset \(\mathcal{D}_i = (\mathcal{X}_i, \mathcal{Y}_i)\), where \(\mathcal{X}_i\) represents the input data and \(\mathcal{Y}_i\) denotes the corresponding labels. Specifically, the objective of CL at phase \(k\) is to train a model on the training data \(\mathcal{D}^{train}_k = (\mathcal{X}^{train}_k, \mathcal{Y}^{train}_k)\). And its performance is evaluated on the joint test dataset \(\mathcal{D}^{test}_{0:k}\), which encompasses all test data from phase \(0\) to phase \(k\). This paper focuses on a rehearsal-free CL setting, where access to data from previous tasks \(\{\mathcal{T}_0, \mathcal{T}_1, \dots, \mathcal{T}_{k-1}\}\) is strictly prohibited during the learning phase of \(\mathcal{T}_k\).

Based on whether the task identity is provided or must be inferred, CL can be categorized into three typical scenarios: Task/Class/Domain Incremental Learning (IL)~\cite{van2022three}. In this study, we mainly focus on two primary CL scenarios: Class and Task IL~\cite{survey_1}. \textbf{Class IL} is a challenging setting where the model must classify data across all classes encountered up to task \(n\) without access to task-specific labels during inference. Formally, given a test sample \(\mathbf{x}\), the model must predict its label \(\hat{y}\) from the union of all classes seen so far, i.e., \(\hat{y} = \arg\max_{y \in \bigcup_{i=0}^n \mathcal{Y}_i} P(y \mid \mathbf{x})\). This setting is particularly difficult because the model must distinguish between an expanding set of classes without explicit task information~\cite{survey_masana,survey_2}.
\textbf{Task IL} is a simpler multi-task setting where task labels \(k\) are provided during both training and inference. In this scenario, the model can leverage the task label to restrict the classification problem to the subset of classes relevant to the specific task. Formally, given a test sample \(\mathbf{x}\) and its associated task label \(k\), the model predicts \(\hat{y} = \arg\max_{y \in \mathcal{Y}_k} P(y \mid \mathbf{x}, k)\). This reduces the complexity of the problem, as the model only needs to discriminate among classes within the current task.

\section{Alternative Views}

Current research in CL predominantly focuses on the use of a single unified model to achieve CL objectives. While some CL methods incorporate additional models, these are typically employed in an auxiliary capacity to support the primary model~\cite{lwf, mind}. This is due to two primary concerns that the use of multiple models in CL raises. First, the increased memory consumption associated with maintaining multiple models can be prohibitive~\cite{zhou2023model}. Second, in Class IL, selecting an appropriate model for inference becomes challenging due to the absence of task identity information during inference~\cite{van2022three}. 

In this study, we address these concerns by demonstrating that an evolving population of models can achieve superior CL performance compared to a single unified model with less memory consumption. Furthermore, we show that this enhanced performance extends to Class IL scenarios through the use of a straightforward inference strategy. These results suggest that leveraging an evolving population of models, can offer a promising alternative relying on a single model for CL.

\section{Related Works}

This section presents a comprehensive review of existing CL methods, encompassing a range of established approaches. Furthermore, the NAS, a technique utilized for facilitating CL in the proposed PCL, is discussed.

\subsection{Continual Learning}

CL involves letting models sequentially learn a series of tasks without or with limited access to previous data. Neural networks have achieved remarkable success in the CV fields~\cite{he2016deep,vaswani2017attention}, but are ill-equipped for CL due to catastrophic forgetting~\cite{goodfellow2013empirical}. To address catastrophic forgetting, various approaches have been proposed to balance stability and plasticity. These methods encompass a range of techniques, including memory replay, parameter regularization, and dynamic architecture. It should be noted that many approaches may incorporate techniques from multiple categories. Based on whether memory replay is used, these approaches can be roughly divided into two types, i.e., rehearsal-based and rehearsal-free.

\textbf{Rehearsal-based} approaches maintain previous knowledge by explicitly storing or generating past data, which are subsequently replayed during the learning of new tasks. A pioneering method in this domain is experience replay~\cite{er}, which randomly selects samples from previously encountered tasks for replay in future learning stages. Following this, several studies have integrated experience replay with parameter regularization and dynamic architecture, resulting in notable performance results, such as WA~\cite{zhao2020maintaining} and DER~\cite{yan2021dynamically}. Besides experience replay, pseudo-rehearsal approaches employ an auxiliary generative model to produce synthetic data for replay, as exemplified by FearNet~\cite{fearnet} and DDGR~\cite{ddgr}. These methods are highly effective when storing past data or continually training a generative model is feasible. However, in many application scenarios, long-term storage of training data poses significant challenges due to data privacy concerns. With respect to the pseudo-rehearsal, since CL of generative models is extremely difficult and requires significant resource overhead, such approaches are typically limited to relatively simple datasets~\cite{van2020brain}. These limitations have motivated the CL community to explore \textbf{rehearsal-free} CL methods, which can be divided into regularization-based and architecture-based methods.

\textit{Regularization-based} approaches focus on introducing explicit regularization terms to retain knowledge acquired from previous tasks. Depending on the target of regularization, these methods can be divided into two main subcategories~\cite{survey_1}. The first subcategory is weight regularization, which aims to preserve previous knowledge by constraining the plasticity of network parameters. For instance, EWC~\cite{ewc} achieves this by penalizing changes to parameters that are crucial for previously learned tasks, as determined by the Fisher information. Alternative methodologies for assessing parameter importance include synaptic saliency~\cite{si}, gradient inspection~\cite{mas}, and their combination~\cite{rwalk}. The second subcategory is function regularization, which employs Knowledge Distillation~\cite{kd} to ensure that the model does not deviate excessively from the representations learned in previous tasks. As a pioneer work, LwF~\cite{lwf} computes the distillation loss by utilizing the output logits of past tasks to transfer knowledge from the old model to the new. Some works also propose different distillation targets, such as attention heatmaps~\cite{lwm}. 

\textit{Architecture-based} approaches mitigate inter-task interference by developing task-specific parameters. This type of approach can be further categorized into three main subcategories~\cite{survey_1}. The first subcategory is parameter allocation, which involves dedicating isolated parameter subspaces to each task throughout the network, such as WSN~\cite{wsn}.  The second subcategory is model decomposition, which explicitly separates a model into task-sharing and expandable task-specific components, such as APD~\cite{APD}.  The third subcategory is modular networks, which leverages parallel sub-networks or sub-modules to learn incremental tasks in a differentiated manner, such as RPSNet~\cite{rps}. 

\subsection{Neural Architecture Search}

NAS~\cite{zoph2016neural} is a burgeoning research field that aims to develop automated techniques for designing neural network architectures that are specifically tailored to perform a given task~\cite{nas_survey}. In essence, NAS works by using a search strategy to explore a predefined search space, thereby generating a collection of candidate architectures. Subsequently, these candidates are then evaluated using performance estimation methods to guide the search strategy. The above process typically operates iteratively, ultimately identifying the optimal architecture. Based on the search strategy, NAS can be categorized into gradient-based, reinforcement learning-based, and evolution-based NAS~\cite{nas_survey}. This paper focuses on evolution-based NAS, also known as Evolutionary NAS (ENAS)~\cite{enas_survey}, which simulates natural evolutionary processes to generate architectures. Specifically, ENAS employs genetic operations such as mutation and crossover to evolve a population of architectures across successive generations. We argue that ENAS is particularly well-suited for CL compared to other NAS strategies. This suitability is evident as the well-evolved population for the current incremental task can be naturally inherited by the next one in ENAS, thereby facilitating forward transfer, a crucial objective in CL.

\textbf{NAS and CL}
NAS has previously been applied to the design of network architectures for CL. For instance, certain works~\cite{li2019learn,wang2023task,I-DARTS} have leveraged NAS to refine the architecture-based CL methods. Specifically, these methods typically employ NAS to identify the optimal strategy for incrementally expanding the CL network, thereby mitigating catastrophic forgetting. Moreover, ArchCraft~\cite{CL_design} utilizes NAS to uncover CL-friendly and efficient basic network architectures, thereby improving CL performance. This demonstrates that optimizing the entire architecture, rather than just the expansion strategy, can enhance CL performance in a distinct manner. It is important to highlight that the core insight of our proposed method diverges from both. Specifically, unlike the first category, which focuses on expansion strategies, our work concentrates on the entire architecture. Moreover, while the second category aims to discover a generic architecture for all CL tasks, our approach is dedicated to crafting specialized architectures tailored to individual CL tasks. This distinction is crucial because a good generic architecture does not necessarily guarantee promising performance across all tasks.

\section{Method}

In this section, we elaborate on the procedures of PCL and demonstrate its implementation in Task and Class IL scenarios. We begin by presenting the overall framework of the PCL. Subsequently, we define the search space and performance evaluation strategy used in the NAS process of PCL. Finally, we discuss the inference phase, describing how the designed expert sub-networks are employed for task-specific predictions. These components collectively ensure an efficient methodology that can automatically optimize the architectures for each CL task.

\subsection{Overall Framework}
\label{Overall framework}

Algorithm~\ref{Pseudo code of PCL} outlines the procedure of PCL, including architecture search for each task and subsequent learning. The process begins with randomly generating a population of architectures within the defined search space (line 3). Throughout the learning of CL tasks, this population evolves iteratively to yield specialized architectures, each of which is further trained to obtain optimal weights for the corresponding task.

For each task, the population undergoes cycles of crossover, mutation, and environment selection, resulting in a new generation of individuals with improved performance. Initially, the fitness of individuals within the population is assessed on the current task (line 7). Before the new task comes, the population continues to evolve, optimizing its performance for the current task. Specifically, a selection operator is employed to choose parent individuals with high fitness (line 9). Then, PCL employs crossover and mutation operators to these parents to generate offspring (lines 10). The offspring population, denoted as $Q$, is subsequently trained and evaluated to determine their fitness (line 11). The next population is then generated by selecting individuals with high fitness from both the parent and offspring populations (line 12). Moreover, it should be noted that during fitness evaluation, the model is trained for only a few epochs, thereby reducing the computational consumption.

This evolutionary process for the current task persists until a new task is introduced in principle. For simplicity, we assume that a new task arrives when the generation counter $g$ reaches a predefined maximum, $Max Generation$. At this point, the architecture individual with the highest performance across all generations is chosen as the final design for the current task. This architecture is then fully trained and archived along with its parameters. After that, the population evolves continually for the next task. In scenarios where new data is continuously presented, this process can continue uninterrupted. Ultimately, PCL concludes with a CL system composed of various archived models, each tailored to a specific learned task.

\begin{algorithm}
\caption{Overall process of PCL}
\label{Pseudo code of PCL}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} Incremental datasets $D$ with $T$ tasks
\STATE {\bfseries Output:} An optimal model population for CL tasks
\STATE $P \gets $ Initialize a population
\FOR{$t= 1, 2, \dots, T$}
    \STATE Train and evaluate the individuals in $P$
    \FOR{$g = 1, 2, \dots, MaxGeneration$}
        \STATE $P_{parent} \gets$ Select parents from $P$
        \STATE $Q \gets$ Generate new offspring based on $P_{parent}$
        \STATE Evaluate the individuals with few epochs in $Q$
        \STATE Update $P$ by selecting individuals with high fitness from the previous $P$ and $Q$
        \STATE $Solution^{g}_t \gets$ Preserve a solution with the best fitness in $P$
    \ENDFOR
    \STATE Train best solution candidates in each generation $Solution^{i}_t$ ($i=0,...,MaxGeneration$) on $D^t_{train}$
    \STATE $Solution^{}_t \gets$ evaluate the best solution candidates on $D^t_{valid}$ to find the best solution
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Search Space}
\label{subs:Search Space}

Designing an effective search space is pivotal to achieving optimal performance in NAS~\cite{radosavovic2020designing,wan2022redundancy}. Among various types of search spaces, the cell-based space~\cite{zoph2018learning,liu2018darts} has gained considerable popularity in recent years, thanks to its excellent scalability and efficiency. Drawing inspiration from this, we have crafted a search space for PCL that is also based on the cell structure, with its overall design aligned with those of DARTS~\cite{liu2018darts}. As illustrated in Figure~\ref{f:search_space}, the network architecture is constructed by stacking small, learnable building blocks known as cells. These cells are categorized into normal and reduction cells, with the structure of cells of the same type defined as the same. Reduction cells are designed to halve the feature map's spatial dimensions while doubling the number of channels, whereas normal cells preserve the original dimensions of the feature map. In this study, we set $N=1$ by default, resulting in 3 normal cells and 2 reduction cells in each network.
A cell is a directed acyclic graph in which each node represents an operation (e.g., convolution). And if there is a connection between nodes $i$ and $j$ ($i < j$), it means that the output of node $i$ is fed into node $j$. The NAS process is primarily concerned with the seamless exploration of architectural configurations within these cells, searching for the optimal operation type and connections for each node.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{./images/search_space.pdf}
    \caption{\emph{Search Space within PCL.} \textbf{Left:} A cell is a small network represented by a directed acyclic graph. \textbf{Right:} The entire architecture consists of several normal and reduction cells, which is a common structure in cell-based space.}
    \label{f:search_space}
\end{figure}

In light of the distinct objectives of CL compared to conventional machine learning, it is imperative to tailor the existing search space to the nature of CL. A recent study~\cite{mirzadeh2022architecture} has indicated that skip connections may not have a substantial effect on model performance in CL benchmarks. Inspired by this, we have omitted skip connections from our search space, which reduces the search space complexity and enhances the efficiency of the search process. Consequently, our refined search space encompasses seven types of operations within each cell. These operations consist of $3\times3$ dilated convolution, $5\times5$ dilated convolution, $3\times3$ separable convolution, $5\times5$ separable convolution, max pooling, average pooling, and the identity operation. Furthermore, the network depth significantly influences the performance of CL models, and the optimal depth cannot be predetermined due to the absence of prior knowledge in CL scenarios~\cite{mirzadeh2022architecture, CL_design}. Therefore, we use a variable rather than a fixed number of nodes within each cell, allowing for the automatic exploration of appropriate network depth. Specifically, the number of neural nodes in both the normal and reduction cells is allowed to range between 4 and 7, enabling the search process to determine the most suitable depth for each task.

\subsection{Performance Evaluation Strategy}
\label{Selective evaluation strategy}

An efficient performance evaluation strategy is also important for NAS to explore distinct architectures within specified resource constraints efficiently. Consequently, we propose a selective evaluation strategy to accelerate the performance evaluation process. It involves conducting brief, low-fidelity evaluations for the majority of architectures. Such a strategy allows PCL to allocate computational resources more effectively to the most promising individuals, facilitating the CL system to learn new knowledge.

A recent study~\cite{PEPNAS} has indicated a moderate correlation between the performance rankings of individuals during early training epochs and their final rankings. Motivated by this, we adopt a straightforward strategy that assigns different training epochs for low and high fidelity evaluations. Specifically, during the search process, individuals in the population are trained with early stopping to determine their fitness. At the end of the evolutionary phase, the top-performing individual is fully trained and evaluated to identify the optimal architecture for the current task.

\subsection{Inference}
\label{subs:infer}

Upon completion of the PCL process, a CL system containing multiple sub-networks is obtained. Considering that the samples may come from any of the learned tasks during the test phase, it is necessary to select a suitable model for inference. In request of it, two distinct inference strategies are used for Task IL and Class IL. In the Task IL scenario, the task ID of each sample is known, allowing for a straightforward selection of the corresponding expert sub-network. In contrast, the Class IL scenario lacks task IDs for each sample, necessitating the inference of the task ID in advance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.96\columnwidth]{./images/inference.pdf}
    \caption{\emph{Inference strategy for class IL.} For a given input sample, PCL collects logit vectors from all expert networks. And the classification result with the highest probability (see red bounding box) is considered as the final result.}
    \label{fig:inference}
\end{figure}

To better demonstrate the effectiveness of a population of models for CL, we simply use a straightforward strategy to infer the task IDs for Class IL, which is depicted in Figure~\ref{fig:inference}. Specifically, we input the sample $x$ into all designed expert networks and compute the outputs $z_i$ of each expert network. Notably, the expert networks are stored on disk rather than kept in memory when inactive to conserve resources. From the logits $z_i$ for $i = 0, \ldots, K-1$, where $K$ represents the current number of tasks, we determine the predicted class $c_i$ by selecting the class with the highest likelihood according to the current model. Subsequently, we perform an additional selection step in which all $c_i$ are ranked based on their predicted confidence scores. The final prediction is the predicted class with the highest confidence score across all sub-networks.

\section{Experiments}

\subsection{Experimental Setting}
\label{subs:setting}

\textbf{Datasets.} Following convention~\cite{rebuffi2017icarl}, we have selected CIFAR-100~\cite{cifar100} and Tiny-ImageNet~\cite{le2015tiny} as the datasets for evaluating PCL. Both datasets are partitioned into tasks consisting of 10 classes each for a total of 10 tasks. 

\textbf{Baselines.} To ensure a fair and equitable comparison, we evaluate PCL against various existing state-of-the-art rehearsal-free methods. For Task IL, we select EWC~\cite{ewc}, SI~\cite{si}, UCL~\cite{ucl}, TAG~\cite{tag}, SupSup~\cite{supsup}, WSN~\cite{wsn}, SPG~\cite{spg} as the baselines. For Class IL, we select PASS~\cite{pass}, FeTrIL~\cite{fetril}, FeCAM~\cite{fecam}, NCM~\cite{icarl}, SDC~\cite{sdc}. Following convention~\cite{wsn, ldc}, we employ AlexNet as the backbone for all Task IL methods and ResNet-18 as the backbone for Class IL methods. In Task IL, We also conduct a comparison with ArchCraft~\cite{CL_design}, a state-of-the-art method that focuses on designing a generic architecture for all incremental tasks.

\textbf{Implementation Details.} We train networks utilizing stochastic gradient descent with momentum, initializing the learning rate at 0.1 and employing a single-period cosine decay learning rate schedule. During fitness evaluation, the models are only trained for 10 epochs. To optimize the weights of the final selected network for each task, we train it for 300 epochs. In line with common practice~\cite{liu2018darts}, the number of channels in all architectures is set to 16. Moreover, the maximum generation for the NAS process for each task is set at 10, and the population size is set at 10.

\textbf{Evaluation Metrics.} The average classification accuracy after learning the $b$-th task, say $AA_{b}$, is defined as:
\begin{equation}
    AA_{b}=\frac{1}{b} \sum_{i=1}^{b} a_{i,b}
\end{equation}
where $a_{i,b}$ is the classification accuracy evaluated on the test set of the $i$-th task after learning the $b$-th task ($i \le b$). In both Task and Class IL scenarios, the performance of CL is mainly measured by the {\em Last Accuracy} (LA). The LA is the average classification accuracy after the last task, i.e., $LA=A_{K}$, where $K$ is the total number of tasks. LA reflects the overall accuracy among all classes. The higher LA, the better CL performance.

\subsection{Experimental Results}
\label{subs:results}

\paragraph{Evaluation in Task IL} 

Table~\ref{tab:result_til} details a comparative analysis between PCL and existing state-of-the-art rehearsal-free methods in Task IL. In this context, PCL consistently and significantly outperforms existing methods across datasets. Specifically, PCL exhibits a 12.5\% and 29.9\% improvement in LA relative to the second-best method on CIFAR100 and Tiny-ImageNet, respectively. In particular, PCL outperforms ArchCraft, well demonstrating the superiority of our method over using general architecture for all incremental tasks. 

\begin{table}[ht]
    \centering
    \resizebox{1\linewidth}{!}{
    \input{./tables/pcl_table1}
    }
\caption{Comparison of LA on CIFAR100 and Tiny-ImageNet in Task IL.  \textbf{Bolded} indicates the best performance. \underline{Underline} indicates the second best. 
}
\label{tab:result_til}
\end{table}

\paragraph{Evaluation in Class IL}

Table~\ref{tab:result_cil} reports the performance of PCL and the baselines in \textit{Class IL}. It can be observed that the PCL method demonstrates superior performance over current rehearsal-free techniques across both datasets. In particular, PCL surpasses the second best method by a margin of 5.0\% and 0.4\% in LA on CIFAR100 and Tiny-ImageNet, respectively. These results strongly emphasize the superiority of our proposed method. 

\begin{table}[ht]
    \centering
    \resizebox{1\linewidth}{!}{
    \input{./tables/pcl_table2}
    }
\caption{Comparison of LA on CIFAR100 and Tiny-ImageNet in Class IL. \textbf{Bolded} indicates the best performance. \underline{Underline} indicates the second best.}
\label{tab:result_cil}
\end{table}

\subsection{Effectiveness of Specialized Architectures}

To further assess the efficacy of the specialized architectures put forth by PCL, we conduct a comparative study with a strong baseline that employs an independent ResNet-50 model for each task. To simplify, we select CIFAR100 as the representative dataset. The results of this experiment are presented in Figure~\ref{fig:comparison_pcl_res50}. We observed that employing the expert networks crafted by PCL for each task achieves higher performance than the ResNet-50 across all incremental stages. In light of these findings, it can be concluded that task-specific expert architectures cannot be replaced by a more generalized network design. The findings underscore the importance of our proposed method in optimizing network architectures tailored to training data, thereby markedly enhancing the plasticity of CL systems.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.96\linewidth]{./images/compare_pcl_res50_cil.pdf}
    \caption{Comparison of average accuracy between PCL and using an independent ResNet-50 model for each incremental task of CIFAR100 in Class IL.}
    \label{fig:comparison_pcl_res50}
\end{figure}

To further investigate the necessity of task-specialized architectures, we explored the cross-task performance of architectures initially designed for a single task. Specifically, we select the best architectures designed for the first five tasks and evaluate their performance on all five tasks. The results of this investigation are presented in Figure~\ref{fig:comparison_irreplaceability}. It can be observed that while the architecture tailored for task $t$ exhibits outstanding performance on that specific task, it does not maintain comparable effectiveness when applied to other tasks. These findings indicate that there are notable differences in the performance of architectures when applied to specific incremental tasks. It would appear that no single, generic architecture exists that can optimize performance across all tasks. Therefore, to achieve the greatest possible performance in the context of CL, it is essential to identify the optimal architecture for each task. These findings substantiate the necessity for PCL.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.96\linewidth]{./images/irreplaceability.png}
    \caption{The accuracy of PCL-designed architectures on different incremental tasks of CIFAR100. Note that Arch-$t$ denotes the architecture specialized for task $t$.}
    \label{fig:comparison_irreplaceability}
\end{figure}

\subsection{Analysis on Bias-correction}

In class IL, there is a clear bias towards tasks that have been recently learned when the model performs inference. This phenomenon, which has been termed the task-recency bias~\cite{survey_masana,zhao2020maintaining}, represents one of the underlying causes of catastrophic forgetting. Specifically, CL models tend to misclassify instances from earlier tasks as belonging to the classes of newly introduced tasks. In this subsection, we conduct a further assessment of the efficacy of our proposed methods in alleviating task-recency bias. To this end, we present the task confusion matrices for the PCL method and the baseline which employs independent ResNet-50 models for each task. As illustrated in Figure~\ref{f:distribution}, PCL enables more accurate determination of the correct task ID, leading to a reduction in inter-task classification errors. In particular, PCL significantly alleviates the phenomenon of misclassifying data from earlier tasks (such as task 1) as belonging to subsequent tasks. These findings suggest that PCL can effectively reduce task-recency bias, thereby mitigating catastrophic forgetting in CL systems. 

\begin{figure}[ht]
    \centering
    \subcaptionbox{ResNet-50}{\includegraphics[width = 0.49\linewidth]{./images/res50_heatmap.pdf}}
    \hfill
    % \hspace{.4cm}
    \subcaptionbox{PCL}{\includegraphics[width = 0.49\linewidth]{./images/pcl_heatmap.pdf}}
    \caption{Task confusion matrices for CIFAR100.}
    \label{f:distribution}
\end{figure}

\subsection{Analysis on Resource Consumption}

NAS is often considered resource-intensive. To address this potential concern, in this subsection, we will discuss how our method's resource consumption is fully acceptable compared to existing approaches. To simplify, we calculated all data based on the CIFAR-100 dataset.

\textbf{Memory Consumption}. Our method necessitates maintaining a sub-network for each task to optimize the architecture. However, it does not result in additional memory consumption compared to existing CL methods. This is attributed to the fact that each sub-network can be highly efficient while maintaining superior performance. Specifically, in our experiments on CIFAR100, the average parameter count of the architectures is 0.145M. This results in a total parameter count of 1.45M for all 10 tasks, which is less than those of widely-used architectures such as ResNet-18 (11.2M).

\textbf{Computation Consumption}. Our method incorporates an additional search stage, requiring the training of 100 networks per task. Despite this, each network is trained for only 1/30 of the duration compared to the learning stage. Consequently, the total computational overhead is merely 4.3 times that of the standard CL paradigm. Furthermore, in our experiments on CIFAR100, the average FLOPs of these networks is only 15.9M. These values are significantly lower than those of widely used architectures such as ResNet-18 (558M). These results demonstrate that the computation consumption of our method is acceptable.

\section{Conclusion}

In this study, we present PCL, a population-based CL framework that aims to optimize network architectures for each CL task. PCL broadens the scope of existing CL techniques by employing an evolving population of models with specialized network architectures to perform CL. Extensive experiment results indicate that PCL can achieve better performance than state-of-the-art rehearsal-free CL methods that using a single unified model in both Task and Class IL, without additional memory consumption. The superior performance of PCL demonstrate that \textit{an evolving population can outperform than a single unified model in CL}, indicating its potential as a new paradigm for CL. We hope that this work will inspire further exploration of enhancing the capabilities of CL systems via multi models.

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{hideinfo2025}



\end{document}