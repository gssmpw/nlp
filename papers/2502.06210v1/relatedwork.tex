\section{Related Works}
This section presents a comprehensive review of existing CL methods, encompassing a range of established approaches. Furthermore, the NAS, a technique utilized for facilitating CL in the proposed PCL, is discussed.

\subsection{Continual Learning}

CL involves letting models sequentially learn a series of tasks without or with limited access to previous data. Neural networks have achieved remarkable success in the CV fields~\cite{he2016deep,vaswani2017attention}, but are ill-equipped for CL due to catastrophic forgetting~\cite{goodfellow2013empirical}. To address catastrophic forgetting, various approaches have been proposed to balance stability and plasticity. These methods encompass a range of techniques, including memory replay, parameter regularization, and dynamic architecture. It should be noted that many approaches may incorporate techniques from multiple categories. Based on whether memory replay is used, these approaches can be roughly divided into two types, i.e., rehearsal-based and rehearsal-free.

\textbf{Rehearsal-based} approaches maintain previous knowledge by explicitly storing or generating past data, which are subsequently replayed during the learning of new tasks. A pioneering method in this domain is experience replay~\cite{er}, which randomly selects samples from previously encountered tasks for replay in future learning stages. Following this, several studies have integrated experience replay with parameter regularization and dynamic architecture, resulting in notable performance results, such as WA~\cite{zhao2020maintaining} and DER~\cite{yan2021dynamically}. Besides experience replay, pseudo-rehearsal approaches employ an auxiliary generative model to produce synthetic data for replay, as exemplified by FearNet~\cite{fearnet} and DDGR~\cite{ddgr}. These methods are highly effective when storing past data or continually training a generative model is feasible. However, in many application scenarios, long-term storage of training data poses significant challenges due to data privacy concerns. With respect to the pseudo-rehearsal, since CL of generative models is extremely difficult and requires significant resource overhead, such approaches are typically limited to relatively simple datasets~\cite{van2020brain}. These limitations have motivated the CL community to explore \textbf{rehearsal-free} CL methods, which can be divided into regularization-based and architecture-based methods.

\textit{Regularization-based} approaches focus on introducing explicit regularization terms to retain knowledge acquired from previous tasks. Depending on the target of regularization, these methods can be divided into two main subcategories~\cite{survey_1}. The first subcategory is weight regularization, which aims to preserve previous knowledge by constraining the plasticity of network parameters. For instance, EWC~\cite{ewc} achieves this by penalizing changes to parameters that are crucial for previously learned tasks, as determined by the Fisher information. Alternative methodologies for assessing parameter importance include synaptic saliency~\cite{si}, gradient inspection~\cite{mas}, and their combination~\cite{rwalk}. The second subcategory is function regularization, which employs Knowledge Distillation~\cite{kd} to ensure that the model does not deviate excessively from the representations learned in previous tasks. As a pioneer work, LwF~\cite{lwf} computes the distillation loss by utilizing the output logits of past tasks to transfer knowledge from the old model to the new. Some works also propose different distillation targets, such as attention heatmaps~\cite{lwm}. 

\textit{Architecture-based} approaches mitigate inter-task interference by developing task-specific parameters. This type of approach can be further categorized into three main subcategories~\cite{survey_1}. The first subcategory is parameter allocation, which involves dedicating isolated parameter subspaces to each task throughout the network, such as WSN~\cite{wsn}.  The second subcategory is model decomposition, which explicitly separates a model into task-sharing and expandable task-specific components, such as APD~\cite{APD}.  The third subcategory is modular networks, which leverages parallel sub-networks or sub-modules to learn incremental tasks in a differentiated manner, such as RPSNet~\cite{rps}. 

\subsection{Neural Architecture Search}

NAS~\cite{zoph2016neural} is a burgeoning research field that aims to develop automated techniques for designing neural network architectures that are specifically tailored to perform a given task~\cite{nas_survey}. In essence, NAS works by using a search strategy to explore a predefined search space, thereby generating a collection of candidate architectures. Subsequently, these candidates are then evaluated using performance estimation methods to guide the search strategy. The above process typically operates iteratively, ultimately identifying the optimal architecture. Based on the search strategy, NAS can be categorized into gradient-based, reinforcement learning-based, and evolution-based NAS~\cite{nas_survey}. This paper focuses on evolution-based NAS, also known as Evolutionary NAS (ENAS)~\cite{enas_survey}, which simulates natural evolutionary processes to generate architectures. Specifically, ENAS employs genetic operations such as mutation and crossover to evolve a population of architectures across successive generations. We argue that ENAS is particularly well-suited for CL compared to other NAS strategies. This suitability is evident as the well-evolved population for the current incremental task can be naturally inherited by the next one in ENAS, thereby facilitating forward transfer, a crucial objective in CL.

\textbf{NAS and CL}
NAS has previously been applied to the design of network architectures for CL. For instance, certain works~\cite{li2019learn,wang2023task,I-DARTS} have leveraged NAS to refine the architecture-based CL methods. Specifically, these methods typically employ NAS to identify the optimal strategy for incrementally expanding the CL network, thereby mitigating catastrophic forgetting. Moreover, ArchCraft~\cite{CL_design} utilizes NAS to uncover CL-friendly and efficient basic network architectures, thereby improving CL performance. This demonstrates that optimizing the entire architecture, rather than just the expansion strategy, can enhance CL performance in a distinct manner. It is important to highlight that the core insight of our proposed method diverges from both. Specifically, unlike the first category, which focuses on expansion strategies, our work concentrates on the entire architecture. Moreover, while the second category aims to discover a generic architecture for all CL tasks, our approach is dedicated to crafting specialized architectures tailored to individual CL tasks. This distinction is crucial because a good generic architecture does not necessarily guarantee promising performance across all tasks.