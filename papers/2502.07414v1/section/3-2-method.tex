

\subsection{Sample Weight Averaging}

To mitigate the issue of low effective sample size and high variance in previous independence-based sample reweighting methods, we turn to bagging for inspiration. 
As a conventional ensemble learning strategy, bagging can decrease the estimation variance by averaging models trained on bootstrap-sampled data from the original dataset. 
Thus we consider designing a similar ensemble procedure. 
In order to generate diverse weighting functions, we note that in DWR, since the sample size, i.e. the number of parameters for sample weight learning, is much larger than the feature dimension, it could bear resemblance to the overparameterization characteristics of neural networks, e.g. one may anticipate the existence of multiple local minima when optimizing with gradient descent. The same is true for SRDO since the number of MLP parameters is much larger than the feature dimension. 
Consequently, we are likely to obtain diverse solutions even applying the same algorithm, as long as we vary elements of randomness like initialization. 
We theoretically substantiate this intuition in \Cref{prop:dwr} and \ref{prop:srdo}, and empirically confirm it in \Cref{fig:dist-comp} and \ref{fig:sim-comp} of \Cref{sec:synthetic}. 



Thus we propose SAmple Weight Averaging (SAWA) to improve covariate-shift generalization ability of independence-based sample reweighting algorithms. It learns multiple sets of sample weights by varying the random initialization of parameters $\boldtheta$ in weight learning. 
For DWR, we adopt standard normal distribution to initialize sample weights. For SRDO, we use Xavier Glorot Initialization \citep{glorot2010understanding} for the MLP-structured weighting function. 
Then we directly average the set of sample weights to yield the ensemble result. The entire procedure is described in \Cref{alg:sawa}.


\begin{algorithm}[t]
\caption{SAmple Weight Averaging (SAWA)} \label{alg:sawa}
\begin{algorithmic}
    \STATE {\bfseries Input:} 
    \item Dataset $[\boldsymbol{x}, \boldsymbol{y}]$, where $\boldsymbol{x}\in \mathbb{R}^{n\times p}, \boldsymbol{y}\in \mathbb{R}^{n\times 1}$. 
    \item Weight learning algorithm $\mathbb{A}$.  
    \item Number of averaged sets of sample weights $K$. 
    
    \STATE {\bfseries Output:} Sample weights $\bar{\boldsymbol{W}}$. 
    \STATE Initialize $\tilde{\boldsymbol{W}}$ as an empty list.
    \FOR {$k=1$ to $K$}
        \item Generate a random initialization $\boldtheta_0^{(k)}$. 
        \item Execute the weight learning algorithm to get the weighting function $w^{(k)}=\mathbb{A}(\boldsymbol{x}, \boldtheta_0^{(k)})$. 
        \item Calculate discrete sample weights $\boldsymbol{W}^{(k)}=w^{(k)}(\boldsymbol{x})$. 
        \item Add $\boldsymbol{W}^{(k)}$ to $\tilde{\boldsymbol{W}}$. 
    \ENDFOR
    \STATE Average sets of sample weights in $\tilde{\boldsymbol{W}}$ for the ensemble result $\bar{\boldsymbol{W}}$. 
    \STATE {\bfseries return: $\bar{\boldsymbol{W}}$} 
\end{algorithmic}
\end{algorithm}



Since the learning processes of these sets of sample weights can be easily parallelized, SAWA exhibits a low time cost in contrast to SVI \citep{yu2023stable}, which incurs a high time cost due to its iterative framework that can hardly be parallelized. 
Meanwhile, this strategy does not require information from outcome labels \citep{yu2023stable} or environment labels \citep{shen2020stable2}, and can be flexibly incorporated into any existing independence-based sample reweighting methods, since the weight learning algorithm $\mathbb{A}$ in Algorithm \ref{alg:sawa} can be DWR, SRDO, SVI or any other ones. 
Next, we provide theoretical results from two perspectives. Detailed proofs can be referred to in Appendix. 

\subsubsection{Validity of averaged sample weights}
\label{sec:validity}
We prove that the average of possible solutions is also a valid solution for weight learning. 

\begin{proposition}
\label{prop:dwr}
For a stronger version of DWR that constrains both weighted covariance and weighted mean equal to zero, when $n>\frac{p(p+1)}{2}+1$, it will have infinite solutions if solvable. Furthermore, the solution space is a convex set. 
\end{proposition}

\begin{proposition}
\label{prop:srdo}
For SRDO, when using the LSIF loss $\mathbb{E}_{\tilde{P}}[-w(\boldx)]+\mathbb{E}_{P}[w(\boldx)^2/2]$ to directly learn the density ratio, i.e. the weighting function $w$, if restricting $w$ coming from the linear parameterized weighting function family $\mathcal{W}_{\rm lin}=\{w_{\boldtheta}(\boldx)=a(\boldx)^T \boldtheta+b(\boldx)\ | \ a:\mathcal{X}\mapsto \mathbb{R}^p,b:\mathcal{X}\mapsto \mathbb{R}\}$, then the minima constitute a convex set. 
\end{proposition}

For \Cref{prop:srdo}, the weighting function family $\mathcal{W}_{\rm lin}$ is rich because functions $a$ and $b$ can arbitrarily change and the dimension of $\boldtheta$ can be very high. In our implementation of SRDO, we use MLP-structured weighting functions. With proper assumptions, we can use Neural Tangent Kernel (NTK) approximation \citep{lee2019wide} to include wide MLPs into $\mathcal{W}_{\rm lin}$. 
As the possible solutions constitute a convex set for both DWR and SRDO, the average of multiple optimization results also belongs to the set, thus also a possible optimization outcome of the corresponding weight learning algorithm. So we confirm the validity and rationality of sample weight averaging. Note that other reweighting algorithms like SVI are based on DWR and SRDO, to which \Cref{prop:dwr} and \ref{prop:srdo} can also be applied. 

\subsubsection{Benefits of decreasing error of weight learning and model parameter estimation}
\label{sec:benefits}
Following the theory of bagging \citep{ghojogh2019theory} and existing theoretical analyses for model parameter averaging \citep{rame2022diverse}, we come up with the following proposition. 
\begin{proposition}    
Denote $w$ as some desired weighting function in $\mathcal{W}_{\perp}$. 
Denote $w^E$ as the expected weighting function outputted by a single weight learning procedure over the joint distribution $P^g$ of training data $\boldsymbol{x}$ and random initialization $\boldtheta_0$, calculated as $w^E(\boldx)=\expect{g\sim P^g}{w^g(\boldx)}$, where $g=(\boldsymbol{x}, \boldtheta_0)$. 
Denote $\bar{w}$ as the average of the $K$ learned weighting functions, calculated as $\bar{w}(\boldx)=\frac1K \sum_{k=1}^K w^{(k)}(\boldx)$, where $w^{(k)}=\mathbb{A}(g^{(k}))$, $\{g^{(k)}\}_{k=1}^K$ are identically sampled from $P^g$, and all pairs of elements in $\{g^{(k)}\}_{k=1}^K$ shares the same covariance. 
Then expected estimation error of the averaged weighting function over $P^{te}$ and $P^g$ can be decomposed into the following three parts:
\begin{equation} \label{eq:decomp}
\small
\begin{aligned}
&\expect{\left\{g^{(k)}\right\}_{k=1}^K}{\expect{\boldx\sim P^{te}}{(\bar{w}(\boldx)-w(\boldx))^2}}\\
=& \mathop{\mathbb{E}}\limits_{\boldx\sim P^{te}}\Bigg[\left(w^E(\boldx)-w(\boldx)\right)^2\\
+&\frac1K \expect{g^{(k)}}{\left(w^{(k)}(\boldx)-w^E(\boldx)\right)^2}\\
+&\frac{K-1}{K}\expect{g^{(l)},g^{(m)} 
\atop 
l\neq m}{\left(w^{(l)}(\boldx)-w^E(\boldx)\right)\left(w^{(m)}(\boldx)-w^E(\boldx)\right)}\Bigg]
\end{aligned}
\end{equation}
\label{prop:decomp}
\end{proposition}

The first term of the right-hand side is $(w^E(\boldx)-w(\boldx))^2$, the squared bias of weight learning, solely related to the weight learning algorithm itself. It remains constant irrespective of the averaging strategy. 
The second term can be interpreted as the variance of weight learning, inversely proportional to $K$. Therefore, when we apply SAWA, an increase of $K$ results in a reduction of this variance component. 
The third term characterizes the degree of diversity present in sample weights. It depicts the correlation between two distinct weighting functions. By enhancing the diversity among weighting functions used for averaging, this term can be mitigated. This can elucidate the superiority of averaging sample weights from different initializations, as compared with moving average from the same initialization, which is popular in current DG research \citep{cha2021swad,arpit2022ensemble}. This finding aligns with the conclusion drawn by~\citet{rame2022diverse}. Relevant empirical analyses are in \Cref{fig:sim-comp} of \Cref{sec:synthetic}. 



Finally, following~\citet{xu2021stable}, we connect weight learning error with regression coefficient estimation error. 
\begin{proposition}
    Denote $\hat{\boldb}_{\bar{w}}$ as the model coefficient estimated by WLS using $\bar{w}$ with sample size $n$. 
    Denote $\boldb_w$ as the model coefficient estimated by WLS using some $w\in \mathcal{W}_{\perp}$ with infinite samples. 
    Denote $\Lambda_w$ as the smallest eigenvalue of the population-level weighted covariance matrix. 
    Then with mild assumptions, we have:
    \begin{equation}
        \left\|\hat{\boldb}_{\bar{w}}-\boldb_w\right\|\leq \frac{4\epsilon^2 M_w}{\left(\Lambda_w-\epsilon\sqrt{\expect{}{\|\boldx\|_2^4}}\right)^2}+O\left(\frac1n\right)
    \end{equation}
    where $\epsilon^2=\expect{\boldx\sim P^{tr}}{(\bar{w}(\boldx)-w(\boldx))^2}$ is the weight learning error, $M_w$ is a term only related to $w$. 
\label{prop:error}
\end{proposition}
\Cref{prop:error} reveals that as $n$ grows large enough, the dominated term in the bound of coefficient estimation error is positively related to the weight learning error. 
Notably, coefficients associated with unstable variables $\boldv$ in $\boldb_w$ are almost surely zero almost according to~\citet{xu2021stable}. 
Consequently, by refining the learning of sample weights, we can achieve improved estimations for the model coefficients on $\bolds$ and drive coefficients on $\boldv$ towards zero. Such refinement will lead to a stronger covariate-shift generalization ability and more stable prediction. 


