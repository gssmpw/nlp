\section{Method}

\subsection{Notations and Problem}
\label{sec:problem}

Throughout the paper, regular letters signify scalars, while bold letters denote vectors or matrices. 
For variables, uppercase letters are used. 
The input covariates are denoted as $\boldsymbol{X}\in \mathbb{R}^p$ with a dimension $p$, where $X_d$ represents the d$^{th}$ covariate. The outcome $Y\in \mathbb{R}$ corresponds to a scalar in the context of regression tasks. 
When referring to a dataset with a sample size $n$, lowercase letters are used: $\boldsymbol{x}\in \mathbb{R}^{n\times p}$ denotes the input data, also the design matrix, and $\boldsymbol{y}\in\mathbb{R}^n$ denotes the outcome data, where $(\boldsymbol{x}_i,y_i)$ denotes the i$^{th}$ sample. Training distribution is denoted as $P^{tr}(\boldsymbol{X}, Y)$ while test distribution is denoted as $P^{te}(\boldsymbol{X}, Y)$. 
The notation $\boldsymbol{A}\perp \boldsymbol{B}$ is employed to indicate the statistical independence between $\boldsymbol{A}$ and $\boldsymbol{B}$. 
Expectations are represented by $\mathbb{E}_{Q(\cdot)}[\cdot]$, where $Q$ can be chosen as $P^{tr}$, $P^{te}$ or any other proper distributions. 

We use $w(\boldsymbol{X})\in \mathcal{W}$ to denote the weighting function whose input is $\boldx$, formally defined below:
\begin{definition} [Weighting function] \label{def:weighting}
    Let $\mathcal{W}$ be the set of weighting functions that satisfy
    \begin{equation}
    \small
        \mathcal{W} = \left\{w: \mathcal{X} \rightarrow \mathbb{R}^{+} \mid \mathbb{E}_{P^{tr}(\boldsymbol{X})}[w(\boldsymbol{X})] = 1 \right\}.
    \end{equation}
    Then $\forall w \in \mathcal{W}$, the corresponding weighted distribution is $\tilde{P}_w(\boldsymbol{X}, Y) = w(\boldsymbol{X})P^{tr}(\boldsymbol{X}, Y)$. $\tilde{P}_w$ is well defined with the same support of $P^{tr}$.
\end{definition}
Regarding independence-based sample reweighting, $\mathcal{W}_{\perp}$ designates the subset of $\mathcal{W}$ where $\boldsymbol{X}$ are mutually independent in the weighted distribution $\tilde{P}_w$.
The parameters for weight learning are denoted as $\boldtheta\in \mathbb{R}^n$ while the parameters (coefficients) for linear regression are represented as $\boldsymbol{\beta}\in \mathbb{R}^p$ to establish a clear distinction. 
For a weight learning algorithm $\mathbb{A}$, it takes data $\boldsymbol{x}$ and parameter initialization $\boldtheta_0$ as input, and produces a weighting function parameterized by $\boldtheta$, expressed as $w_{\boldtheta}=\mathbb{A}(\boldsymbol{x},\boldtheta_0)\in \mathcal{W}$. 
In the scenarios of finite samples, $W_i$ is employed as an abbreviation for $w(\boldsymbol{x}_i)$, and $\boldsymbol{W}\in \mathbb{R}^n$ is utilized in place of $w$ without ambiguity. 


Generally speaking, OOD generalization focuses on the setting where $P^{tr}(\boldsymbol{X}, Y)\neq P^{te}(\boldsymbol{X}, Y)$. For covariate-shift generalization, its definition is presented below. 



\begin{problem}[Covariate-Shift Generalization]
    Given samples $\{(\boldsymbol{x}_i, y_i)\}_{i=1}^{n}$ drawn from training distribution $P^{tr}$, the goal of covariate-shift generalization is to learn a prediction model so that it performs stably on predicting $Y$ in agnostic test distribution where $P^{te}(\boldsymbol{X}, Y)=P^{te}(\boldsymbol{X})P^{tr}(Y|\boldsymbol{X})$.
\end{problem}
We can see $P^{te}$ differs from $P^{tr}$ in the shift of covariate distribution only, while keeping the conditional distribution fixed. 
To address this covariate-shift generalization problem, the concept of a minimal stable variable set is defined below.

\begin{definition} [Minimal stable variable set] 
    A minimal stable variable set of predicting $Y$ under training distribution $P^{tr}$ is any subset $\boldsymbol{S}$ of $\boldsymbol{X}$ satisfying the following equation, and none of its proper subsets satisfies it.
    \begin{equation}
    \small
        \mathbb{E}_{P^{tr}}[Y | \boldsymbol{S}] = \mathbb{E}_{P^{tr}}[Y | \boldsymbol{X}]. \label{eq:stable-set}
    \end{equation}
\end{definition}
\citet{xu2021stable} theoretically prove that under the assumption of strictly positive density, which is common in causal inference~\citep{imbens2015causal}, the minimal stable variable set $\bolds$ is equivalent to the minimal and optimal predictor under $P^{te}$. Thus we aim to capture the minimal stable variable set $\bolds$, i.e. stable variables, and get rid of unstable variables $\boldv=\boldx\backslash\bolds$. 



\subsection{Independence-Based Sample Reweighting}

In different environments, correlations between $\bolds$ and $\boldv$ tend to vary, leading to the spurious correlations between $Y$ and $\boldv$ which could be easily learned by traditional machine learning algorithms. Thus previous stable learning methods try to decorrelate $\bolds$ and $\boldv$ via sample reweighting~\citep{kuang2020stable,shen2020stable,yu2023stable,xu2021stable}. Under the infinite-sample setting, \citet{xu2021stable} prove that by conducting weighted least squares (WLS) via $w\in \mathcal{W}_{\perp}$, where $\boldsymbol{X}$ are mutually independent after reweighting, coefficients on unstable variables $\boldsymbol{V}$ are zero almost surely even when the data generation function of $Y$ is nonlinear, serving as the foundation of independence-based sample reweighting against covariate shift. Two representative sample reweighting techniques are introduced below. 

\paragraph{DWR}~\citep{kuang2020stable} aims to remove pairwise linear correlations, i.e.
\begin{equation} \label{eq:DWR}
\small
    \hat{w}(\boldsymbol{X}) = \arg \min_{w(\boldsymbol{X})} \sum_{1\le i,j \le p, i\ne j}\left(Cov(X_i, X_j; w)\right)^2,
\end{equation}
where $Cov(X_i, X_j; w)$ represents the covariance of $X_i$ and $X_j$ after being reweighted to $\tilde{P}_{w}$. 
DWR is well fitted for the case where the data generation process is dominated by a linear function, since it focuses on linear decorrelation only. 

\paragraph{SRDO}~\citep{shen2020stable} conducts sample reweighting by density ratio estimation. 
It simulates the target distribution $\tilde{P}$ via random resampling on each covariate so that $\tilde{P}(X_1, X_2, \dots, X_p) = \prod_{i=1}^p P^{tr}(X_i)$. 
Then the weighting function can be estimated by
\begin{equation} \label{eq:SRDO}
\small
    \hat{w}(\boldsymbol{X}) = \frac{\tilde{P}(\boldsymbol{X})}{P^{tr}(\boldsymbol{X})} =  \frac{P^{tr}(X_1)P^{tr}(X_2)\dots P^{tr}(X_p)}{P^{tr}(X_1, X_2, \dots, X_p)}.
\end{equation}
To estimate such density ratio, with the help of MLP, we can learn a binary classifier to predict the probability of which distribution a sample belongs to, or employ LSIF loss for direct estimation~\citep{menon2016linking}. 
Compared with DWR, SRDO can not only decrease linear correlations among covariates, but weaken the nonlinear dependence. 

It is worth noting that both DWR and SRDO suffer from the over-reduced effective sample size and variance inflation~\citep{shen2020stable2} since they conduct global decorrelation between all covariates while strong correlations commonly exist inside stable variables. Therefore, they both require an enormous sample size to work. Although~\citet{yu2023stable} attempt to address this issue via an iterative framework, they require the utilization of the outcome $Y$ when learning sample weights and suffer from a high time cost considering the iterative process that cannot be parallelized. 
