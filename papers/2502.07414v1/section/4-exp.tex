
\begin{table*}[!ht]
\centering
\caption{Under linear settings, results with different strengths of collinearity in $\bolds$ and $\boldv$.
We highlight the better result w or w/o SAWA with the bold type, and underline the best result across all methods. Vary $n$ and $r$ while fixing $V_b=0.2*p$.}
\label{table:basic}
\resizebox{0.87\textwidth}{!}{%
\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
\multicolumn{13}{c}{Scenario 1: Strong collinearity in $\bolds$ (Fixing $\rho_s=0.9, \rho_v=0.1, r=2.1$, varying $n$)}                                                                                                                                                                                                                                                                               \\ \midrule
\multicolumn{1}{c|}{$n$}       & \multicolumn{4}{c|}{$n=1000$}                                                                                  & \multicolumn{4}{c|}{$n=2000$}                                                                                  & \multicolumn{4}{c}{$n=15000$}                                                             \\ \midrule
\multicolumn{1}{c|}{Methods}   & $\beta$\_Error       & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & $\beta$\_Error       & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & $\beta$\_Error       & Mean\_Error          & Std\_Error           & Max\_Error           \\ \midrule
\multicolumn{1}{c|}{OLS}       & 0.895                & 0.393                & 0.057                & \multicolumn{1}{c|}{0.502}                & 0.815                & 0.388                & 0.048                & \multicolumn{1}{c|}{0.469}                & 0.765                & 0.382                & 0.045                & 0.461                \\
\multicolumn{1}{c|}{Ridge}     & 1.326                & 0.445                & 0.100                & \multicolumn{1}{c|}{0.612}                & 1.250                & 0.440                & 0.091                & \multicolumn{1}{c|}{0.572}                & 1.174                & 0.432                & 0.086                & 0.559                \\
\multicolumn{1}{c|}{Lasso}     & 1.761                & 0.494                & 0.138                & \multicolumn{1}{c|}{0.712}                & 1.657                & 0.483                & 0.124                & \multicolumn{1}{c|}{0.654}                & 1.603                & 0.472                & 0.116                & 0.635                \\
\multicolumn{1}{c|}{STG}       & -                    & 0.420                & 0.077                & \multicolumn{1}{c|}{0.557}                & -                    & 0.407                & 0.061                & \multicolumn{1}{c|}{0.503}                & -                    & 0.393                & 0.053                & 0.480                \\
\multicolumn{1}{c|}{DRO}       & 1.319                & 0.477                & 0.098                & \multicolumn{1}{c|}{0.626}                & 1.201                & 0.457                & 0.111                & \multicolumn{1}{c|}{0.582}                & 1.100                & 0.427                & 0.082                & 0.549                \\
\multicolumn{1}{c|}{JTT}       & 1.420                & 0.489                & 0.102                & \multicolumn{1}{c|}{0.656}                & 1.033                & 0.425                & 0.078                & \multicolumn{1}{c|}{0.519}                & 0.894                & 0.410                & 0.061                & 0.499                \\ \midrule
\multicolumn{1}{c|}{DWR}       & 1.432                & 0.600                & 0.054                & \multicolumn{1}{c|}{0.701}                & 0.693                & 0.464                & 0.038                & \multicolumn{1}{c|}{0.537}                & 0.614                & 0.391                & 0.038                & \textbf{0.441}       \\
\multicolumn{1}{c|}{DWR+SAWA}  & \textbf{1.308}       & \textbf{0.507}       & \textbf{0.039}       & \multicolumn{1}{c|}{\textbf{0.586}}       & {\ul \textbf{0.667}} & \textbf{0.407}       & \textbf{0.035}       & \multicolumn{1}{c|}{\textbf{0.472}}       & {\ul \textbf{0.557}} & \textbf{0.375}       & \textbf{0.026}       & 0.445                \\ \midrule
\multicolumn{1}{c|}{SRDO}      & 0.781                & 0.405                & \textbf{0.034}       & \multicolumn{1}{c|}{0.486}                & 0.961                & 0.410                & 0.028                & \multicolumn{1}{c|}{0.465}                & 0.644                & 0.428                & 0.030                & 0.485                \\
\multicolumn{1}{c|}{SRDO+SAWA} & {\ul \textbf{0.702}} & \textbf{0.392}       & 0.036                & \multicolumn{1}{c|}{\textbf{0.475}}       & \textbf{0.822}       & \textbf{0.407}       & \textbf{0.022}       & \multicolumn{1}{c|}{\textbf{0.445}}       & \textbf{0.577}       & \textbf{0.418}       & \textbf{0.030}       & \textbf{0.476}       \\ \midrule
\multicolumn{1}{c|}{SVI}       & -                    & 0.376                & 0.028                & \multicolumn{1}{c|}{0.441}                & -                    & 0.380                & 0.028                & \multicolumn{1}{c|}{0.429}                & -                    & 0.409                & 0.064                & 0.510                \\
\multicolumn{1}{c|}{SVI+SAWA}  & -                    & {\ul \textbf{0.359}} & {\ul \textbf{0.016}} & \multicolumn{1}{c|}{{\ul \textbf{0.400}}} & -                    & {\ul \textbf{0.376}} & {\ul \textbf{0.015}} & \multicolumn{1}{c|}{{\ul \textbf{0.353}}} & -                    & {\ul \textbf{0.340}} & {\ul \textbf{0.012}} & {\ul \textbf{0.362}} \\ \midrule
\multicolumn{1}{c|}{SVI'}      & -                    & 0.399                & 0.055                & \multicolumn{1}{c|}{0.506}                & -                    & 0.383                & 0.036                & \multicolumn{1}{c|}{0.448}                & -                    & 0.353                & 0.017                & 0.387                \\
\multicolumn{1}{c|}{SVI'+SAWA} & -                    & \textbf{0.373}       & \textbf{0.038}       & \multicolumn{1}{c|}{\textbf{0.453}}       & -                    & \textbf{0.384}       & \textbf{0.028}       & \multicolumn{1}{c|}{\textbf{0.435}}       & -                    & \textbf{0.341}       & \textbf{0.012}       & \textbf{0.363}       \\ \midrule
\multicolumn{13}{c}{Scenario 2: Strong collinearity in both $\bolds$ and $\boldv$ (Fixing $\rho_s=\rho_v=0.7$, varying $r$ and $n$)}                                                                                                                                                                                                                                                     \\ \midrule
\multicolumn{1}{c|}{$r$}       & \multicolumn{4}{c|}{$n=1000, r=2.1$}                                                                           & \multicolumn{4}{c|}{$n=1000, r=2.5$}                                                                           & \multicolumn{4}{c}{$n=2000, r=2.5$}                                                       \\ \midrule
\multicolumn{1}{c|}{Methods}   & $\beta$\_Error       & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & $\beta$\_Error       & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & $\beta$\_Error       & Mean\_Error          & Std\_Error           & Max\_Error           \\ \midrule
\multicolumn{1}{c|}{OLS}       & 1.414                & 0.532                & 0.158                & \multicolumn{1}{c|}{0.732}                & 1.661                & 0.602                & 0.218                & \multicolumn{1}{c|}{0.860}                & 1.622                & 0.602                & 0.217                & 0.877                \\
\multicolumn{1}{c|}{Ridge}     & 1.655                & 0.583                & 0.202                & \multicolumn{1}{c|}{0.828}                & 1.894                & 0.659                & 0.267                & \multicolumn{1}{c|}{0.969}                & 1.867                & 0.660                & 0.266                & 0.990                \\
\multicolumn{1}{c|}{Lasso}     & 1.911                & 0.653                & 0.261                & \multicolumn{1}{c|}{0.960}                & 2.283                & 0.783                & 0.370                & \multicolumn{1}{c|}{1.203}                & 2.296                & 0.787                & 0.372                & 1.231                \\
\multicolumn{1}{c|}{STG}       & -                    & 0.531                & 0.158                & \multicolumn{1}{c|}{0.729}                & -                    & 0.600                & 0.217                & \multicolumn{1}{c|}{0.857}                & -                    & 0.601                & 0.217                & 0.876                \\
\multicolumn{1}{c|}{DRO}       & 1.392                & 0.522                & 0.130                & \multicolumn{1}{c|}{0.653}                & 1.591                & 0.599                & 0.213                & \multicolumn{1}{c|}{0.849}                & 1.582                & 0.579                & 0.218                & 0.661                \\
\multicolumn{1}{c|}{JTT}       & 1.381                & 0.524                & 0.134                & \multicolumn{1}{c|}{0.698}                & 1.535                & 0.570                & 0.198                & \multicolumn{1}{c|}{0.794}                & 1.441                & 0.552                & 0.220                & 0.649                \\ \midrule
\multicolumn{1}{c|}{DWR}       & 1.342                & 0.519                & 0.055                & \multicolumn{1}{c|}{0.613}                & 2.259                & 0.861                & 0.146                & \multicolumn{1}{c|}{1.070}                & 1.364                & 0.682                & 0.040                & 0.772                \\
\multicolumn{1}{c|}{DWR+SAWA}  & \textbf{1.340}       & \textbf{0.516}       & {\ul \textbf{0.051}} & \multicolumn{1}{c|}{\textbf{0.606}}       & \textbf{2.244}       & \textbf{0.853}       & \textbf{0.127}       & \multicolumn{1}{c|}{\textbf{1.041}}       & \textbf{1.234}       & \textbf{0.563}       & \textbf{0.035}       & \textbf{0.643}       \\ \midrule
\multicolumn{1}{c|}{SRDO}      & 0.899                & 0.460                & 0.072                & \multicolumn{1}{c|}{0.568}                & 1.077                & 0.488                & 0.089                & \multicolumn{1}{c|}{0.618}                & 0.997                & 0.496                & 0.049                & 0.581                \\
\multicolumn{1}{c|}{SRDO+SAWA} & {\ul \textbf{0.868}} & {\ul \textbf{0.455}} & \textbf{0.057}       & \multicolumn{1}{c|}{{\ul \textbf{0.553}}} & {\ul \textbf{0.870}} & {\ul \textbf{0.465}} & {\ul \textbf{0.073}} & \multicolumn{1}{c|}{{\ul \textbf{0.578}}} & {\ul \textbf{0.992}} & \textbf{0.488}       & {\ul \textbf{0.026}} & \textbf{0.544}       \\ \midrule
\multicolumn{1}{c|}{SVI}       & -                    & 0.518                & 0.133                & \multicolumn{1}{c|}{0.690}                & -                    & 0.706                & 0.263                & \multicolumn{1}{c|}{0.991}                & -                    & 0.458                & \textbf{0.050}       & 0.543                \\
\multicolumn{1}{c|}{SVI+SAWA}  & -                    & \textbf{0.456}       & \textbf{0.067}       & \multicolumn{1}{c|}{\textbf{0.558}}       & -                    & \textbf{0.653}       & \textbf{0.222}       & \multicolumn{1}{c|}{\textbf{0.904}}       & -                    & {\ul \textbf{0.439}} & 0.056                & {\ul \textbf{0.533}} \\ \midrule
\multicolumn{1}{c|}{SVI'}      & -                    & 0.500                & 0.120                & \multicolumn{1}{c|}{0.657}                & -                    & 0.594                & 0.202                & \multicolumn{1}{c|}{0.835}                & -                    & 0.493                & 0.104                & 0.638                \\
\multicolumn{1}{c|}{SVI'+SAWA} & -                    & \textbf{0.472}       & \textbf{0.099}       & \multicolumn{1}{c|}{\textbf{0.608}}       & -                    & \textbf{0.534}       & \textbf{0.153}       & \multicolumn{1}{c|}{\textbf{0.728}}       & -                    & \textbf{0.490}       & \textbf{0.100}       & \textbf{0.631}       \\ \bottomrule
\end{tabular}%
}
\centering
\caption{Under nonlinear settings, results with varying $n$ and $r$. We highlight the better result w or w/o SAWA with the bold type, and underline the best result across all methods. Vary $n$ and $r$ while fixing $\rho_s=0.9, \rho_v=0.1, V_b=0.1*p$. }
\label{table:nonlinear}
\resizebox{0.81\textwidth}{!}{%
\begin{tabular}{@{}cccccccccc@{}}
\toprule
\multicolumn{10}{c}{Scenario 1: Varying sample size $n$ (Fixing $r=2.0$)}                                                                                                                                                                                                              \\ \midrule
\multicolumn{1}{c|}{$n$}       & \multicolumn{3}{c|}{$n$=15000}                                                          & \multicolumn{3}{c|}{$n=20000$}                                                          & \multicolumn{3}{c}{$n=25000$}                                      \\ \midrule
\multicolumn{1}{c|}{Methods}   & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & Mean\_Error          & Std\_Error           & Max\_Error           \\ \midrule
\multicolumn{1}{c|}{MLP}       & 0.221                & 0.080                & \multicolumn{1}{c|}{0.331}                & 0.262                & 0.113                & \multicolumn{1}{c|}{0.416}                & 0.249                & 0.104                & 0.389                \\
\multicolumn{1}{c|}{STG}       & 0.177                & 0.049                & \multicolumn{1}{c|}{0.243}                & 0.176                & 0.048                & \multicolumn{1}{c|}{0.241}                & 0.176                & 0.048                & 0.243                \\
\multicolumn{1}{c|}{DRO}       & 0.289                & 0.178                & \multicolumn{1}{c|}{0.402}                & 0.281                & 0.142                & \multicolumn{1}{c|}{0.477}                & 0.301                & 0.132                & 0.434                \\
\multicolumn{1}{c|}{JTT}       & 0.209                & 0.075                & \multicolumn{1}{c|}{0.298}                & 0.241                & 0.100                & \multicolumn{1}{c|}{0.295}                & 0.167                & 0.033                & 0.210                \\ \midrule
\multicolumn{1}{c|}{SRDO}      & 0.244                & 0.123                & \multicolumn{1}{c|}{0.380}                 & 0.288                & 0.133                & \multicolumn{1}{c|}{0.469}                & 0.231                & 0.090                & 0.373                \\
\multicolumn{1}{c|}{SRDO+SAWA} & \textbf{0.163}       & \textbf{0.031}       & \multicolumn{1}{c|}{\textbf{0.211}}       & \textbf{0.169}       & \textbf{0.037}       & \multicolumn{1}{c|}{\textbf{0.235}}       & \textbf{0.151}       & \textbf{0.026}       & \textbf{0.198}       \\ \midrule
\multicolumn{1}{c|}{SVI'}      & 0.130                & 0.002                & \multicolumn{1}{c|}{0.133}                & 0.125                & 0.001                & \multicolumn{1}{c|}{0.128}                & 0.126                & {\ul \textbf{0.002}} & 0.129                \\
\multicolumn{1}{c|}{SVI'+SAWA} & {\ul \textbf{0.126}} & {\ul \textbf{0.002}} & \multicolumn{1}{c|}{{\ul \textbf{0.130}}} & {\ul \textbf{0.122}} & {\ul \textbf{0.001}} & \multicolumn{1}{c|}{{\ul \textbf{0.126}}} & {\ul \textbf{0.109}} & 0.003                & {\ul \textbf{0.127}} \\ \midrule
\multicolumn{10}{c}{Scenario 2: Varying bias rate $r$ (Fixing $n=25000$)}                                                                                                                                                                                                              \\ \midrule
\multicolumn{1}{c|}{$r$}       & \multicolumn{3}{c|}{$r=1.8$}                                                            & \multicolumn{3}{c|}{$r=2.0$}                                                            & \multicolumn{3}{c}{$r=2.2$}                                        \\ \midrule
\multicolumn{1}{c|}{Methods}   & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & Mean\_Error          & Std\_Error           & \multicolumn{1}{c|}{Max\_Error}           & Mean\_Error          & Std\_Error           & Max\_Error           \\ \midrule
\multicolumn{1}{c|}{MLP}       & 0.188                & 0.051                & \multicolumn{1}{c|}{0.259}                & 0.249                & 0.104                & \multicolumn{1}{c|}{0.389}                & 0.498                & 0.312                & 0.901                \\
\multicolumn{1}{c|}{STG}       & 0.150                & 0.026                & \multicolumn{1}{c|}{0.186}                & 0.176                & 0.048                & \multicolumn{1}{c|}{0.243}                & 0.208                & 0.076                & 0.308                \\
\multicolumn{1}{c|}{DRO}       & 0.212                & 0.098                & \multicolumn{1}{c|}{0.345}                & 0.272                & 0.137                & \multicolumn{1}{c|}{0.422}                & 0.569                & 0.435                & 1.300                \\
\multicolumn{1}{c|}{JTT}       & 0.149                & 0.027                & \multicolumn{1}{c|}{0.195}                & 0.160                & 0.032                & \multicolumn{1}{c|}{0.256}                & 0.219                & 0.074                & 0.295                \\ \midrule
\multicolumn{1}{c|}{SRDO}      & 0.200                & 0.071                & \multicolumn{1}{c|}{0.297}                & 0.236                & 0.089                & \multicolumn{1}{c|}{0.353}                & 0.469                & 0.203                & 0.717                \\
\multicolumn{1}{c|}{SRDO+SAWA} & \textbf{0.144}       & \textbf{0.023}       & \multicolumn{1}{c|}{\textbf{0.182}}       & \textbf{0.154}       & \textbf{0.023}       & \multicolumn{1}{c|}{\textbf{0.210}}       & \textbf{0.201}       & \textbf{0.046}       & \textbf{0.272}       \\ \midrule
\multicolumn{1}{c|}{SVI'}      & 0.132                & 0.007                & \multicolumn{1}{c|}{0.144}                & 0.126                & 0.002                & \multicolumn{1}{c|}{0.129}                & 0.126                & 0.002                & 0.129                \\
\multicolumn{1}{c|}{SVI'+SAWA} & {\ul \textbf{0.120}} & {\ul \textbf{0.006}} & \multicolumn{1}{c|}{{\ul \textbf{0.135}}} & {\ul \textbf{0.121}} & {\ul \textbf{0.002}} & \multicolumn{1}{c|}{{\ul \textbf{0.126}}} & {\ul \textbf{0.121}} & {\ul \textbf{0.002}} & {\ul \textbf{0.125}} \\ \bottomrule
\end{tabular}%
}
\end{table*}





\begin{figure*}[t]
	\centering
	\subfigure[RMSE when varying the selection bias rate of test environments, fixing $n=1000$. ]  {
	\label{fig:linear}
	    \includegraphics[width=0.3\linewidth]{figures/linear.pdf}
	}
	\subfigure[Bias and variance of model parameter estimation for DWR w or w/o SAWA.] {
	\label{fig:bv}
	    \includegraphics[width=0.3\linewidth]{figures/bv.pdf}
	}
	\subfigure[RMSE when varying the number of averaged sets of sample weights.]   {
	\label{fig:ens}
	    \includegraphics[width=0.3\linewidth]{figures/ens.pdf}
	}
	\caption{Results on synthetic data when fixing $r_{train}=3.0,\ \rho_s=0.7,\ \rho_v=0.1$. 
    Subscript $_s$ represents combination with SAWA, drawn in solid lines while baselines are drawn in dashed lines. 
    In Figure \ref{fig:linear}, we can see that SAWA helps every sample reweighting method decrease the prediction error. 
    In Figure \ref{fig:bv}, we can see that SAWA greatly mitigates the variance for DWR while keeping the bias in a moderate range. 
    In Figure \ref{fig:ens}, we can see that the reduction of RMSE becomes marginal after the number of ensemble exceeds 10, so we set it as the constant value when we apply SAWA. 
    }
	\label{fig:basic}
\end{figure*}


\section{Experiments}
\label{sec:exp}

We have carried out comprehensive experiments on both synthetic and real-world datasets to demonstrate the superior performance of the proposed SAWA. We have also conducted abundant extra experiments to facilitate deeper and more thorough analyses. 

\subsection{Baselines}

Here we list the baseline methods we compare to. 
\begin{itemize}
    \item OLS: Minimizing the residual sum of squares (RSS). 
    \item Ridge~\citep{tikhonov1963solution}: Minimizing RSS and $\ell_2$ norm. 
    \item Lasso~\citep{tibshirani1996regression}: Minimizing RSS and $\ell_1$ norm. 
    \item STG~\citep{yamada2020feature}: Feature selection via a probablitic and continuous approximation of $\ell_0$ norm, exhibiting strong performance. 
    \item DRO~\citep{sinha2018certifying}: Searching for the worst-case distribution and optimizing for it. 
    \item JTT~\citep{liu2021just}: Training with ERM in the first round, and upweighting samples with high losses in the second round. 
    \item DWR~\citep{kuang2020stable}: Optimizing weighted covariance to learn sample weights and conduct WLS. 
    \item SRDO~\citep{shen2020stable}: Employing density ratio estimation to learn a weighting function and conduct WLS. 
    \item SVI~\citep{yu2023stable}: An iterative procedure combining sample reweighting and sparsity constraint.  
    \item SVI'~\citep{yu2023stable}: The nonlinear version of SVI. 
\end{itemize}
For linear settings, we exclude MLP. For nonlinear settings, we do not report linear algorithms (OLS, Ridge, Lasso) or reweighting algorithms of linear decorrelation (DWR and SVI). 

\subsection{Evaluation Metrics}

We evaluate our strategy across multiple environments to assess its covariate-shift generalization ability. The metrics adopted are listed below:
\begin{itemize}
    \item $\beta\_{\rm Error}=\|\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\|_1$; 
    \item ${\rm Mean\_Error} = \frac{1}{|\varepsilon_{te}|}\sum_{e\in \varepsilon_{te}}\mathcal{L}^e$;
    \item ${\rm Std\_Error} = \sqrt{\frac{1}{|\varepsilon_{te}|-1} \sum_{e\in \varepsilon_{te}}(\mathcal{L}^e-Mean\_Error)^2}$;
    \item ${\rm Max\_Error} = \max_{e\in \varepsilon_{te}}\mathcal{L}^e$;
\end{itemize}
Here $\varepsilon_{te}$ refers to test environments. $\mathcal{L}^e$ is empirical error in the environment $e$. 
Among the metrics, $\beta$\_Error is only adopted for linear settings. 
For STG, SVI, and SVI', they conduct a feature selection procedure and then apply OLS on the selected features, so we do not report $\beta$\_Error for them. 




\begin{figure*}[t]
	\centering
	\subfigure[Comparing sample weights corresponding to MA and SAWA.] {
	\label{fig:dist-comp}
	    \includegraphics[width=0.3\linewidth]{figures/dist_comp.pdf}
	}
	\subfigure[Similarity of sample weights generated by MA and SAWA.] {
	\label{fig:sim-comp}
	    \includegraphics[width=0.3\linewidth]{figures/sim_comp.pdf}
	}
	\subfigure[Comparing prediction error of MA, CA and SAWA.] {
	\label{fig:avg-comp}
	    \includegraphics[width=0.3\linewidth]{figures/avg_comp.pdf}
	}
	\caption{Comparison with moving average (MA) and coefficient average (CA) when fixing $r_{train}=3.0,\ \rho_s=0.7,\ \rho_v=0.1$. 
    In Figure \ref{fig:dist-comp}, SAWA generates a very different set of sample weights from the original one, while MA nearly overlaps with the original one. 
    In Figure \ref{fig:sim-comp}, there is a lower similarity among the sets of sample weights that SAWA generates than MA. 
    In Figure \ref{fig:avg-comp}, SAWA achieves lower prediction error than other parameter averaging strategies. 
    }
	\label{fig:comp}
\end{figure*}

\subsection{Experiments on Synthetic Data}
\label{sec:synthetic}

\subsubsection{Data generation}
Before simulating different environments via selection bias, we first generate covariates $\boldx=\{\bolds, \boldv\}$ from $N(\boldsymbol{0}, \boldsigma)$, where $\boldsigma={\rm Diag}\left( \boldsigma^{(S)}, \boldsigma^{(V)} \right)$, assuming block structure. 
For $\boldsigma^{(S)} \in \mathbb{R}^{p_s \times p_s}$, we have: $\boldsigma^{(S)}_{jk}=\rho_s$ for $j \neq k$ and $\boldsigma^{(S)}_{jk}=1$ for $j=k$. 
For $\boldsigma^{(V)} \in \mathbb{R}^{p_v \times p_v}$, we have: $\boldsigma^{(V)}_{jk}=\rho_v$ for $j \neq k$ and $\boldsigma^{(V)}_{jk}=1$ for $j=k$. 
Thus we can control the strength of collinearity in stable variables $\bolds$ and unstable variables $\boldv$. 
Then we generate the outcome $Y$. 
For linear settings, we add a polynomial term to the linear term which still dominates the data generation:  
$
    Y=f(\boldsymbol{S})+\epsilon=[\boldsymbol{S}, \boldsymbol{V}]\cdot [\boldsymbol{\beta}_s, \boldsymbol{\beta}_v]^T + \boldsymbol{S}_{\cdot, 1}\boldsymbol{S}_{\cdot, 2}\boldsymbol{S}_{\cdot, 3} + \epsilon
$ and later use linear model to fit the data. 
For nonlinear settings, we generate data in a totally nonlinear way. We employ random initialized MLP as the data generation function: 
$
    Y=f(\boldsymbol{S})+\epsilon = MLP(\boldsymbol{S})+\epsilon 
$ and later use MLP with smaller capacity to fit the data. 
Note that a certain degree of model misspecification is needed, otherwise the model will be good enough to capture the stable variables already. 

To create covariate shift, we try to simulate various environments by changing $P(\boldv_b|\bolds)$, where $\boldv_b\in \boldv$. In this way, we can generate spurious correlations in $P(Y|\boldv)$ through a process of data selection following~\citet{kuang2020stable}: 
Given a bias rate $r\in[-3, -1)\cup(1,3]$, we select each sample with a probability of $Pr=\Pi_{V_i \in \boldsymbol{V}_b}|r|^{-5D_i}$, where $D_i=|f(\boldsymbol{S})-{\rm sign}(r)V_i|$ and ${\rm sign}$ represents the sign function.
We can find that $r>1$ corresponds to a positive correlation between $Y$ and $\boldsymbol{V}_b$, and $r<-1$ refers to a negative correlation between $Y$ and $\boldsymbol{V}_b$. 
A larger value of $|r|$ implies a stronger correlation between $\boldsymbol{V}_b$ and $Y$. By varying the value of the bias rate $r$, we can simulate different environments. 




\subsubsection{Experimental settings}
In our experiments, for linear settings, we simulate two scenarios: strong collinearity in $\bolds$ but weak collinearity in $\boldv$, and strong collinearity in both $\bolds$ and $\boldv$. The second one is more challenging, and is beyond the structural assumption made in~\citep{yu2023stable}. 
For nonlinear settings, we assume strong collinearity in $\bolds$ and weak collinearity in $\boldv$. 
For the hyperparameter introduced by our strategy, i.e. the number of averaged sets of sample weights, we fix it as $10$, which will be analyzed and explained later. 
Other hyperparameters are selected according to the performance on validation data which is split from training data. 
In each specific experimental setting, we employ a single environment corresponding to the bias rate $r_{train}$ for training, and test on multiple environments with $r_{test}$ ranging in $[-3, -1)\cup(1,3]$. 
We run all the experiments $10$ times and report the averaged outcome of these 10 times unless otherwise specified. 
More details regarding data generation and synthetic experimental settings are included in Appendix. We leave analyses of time and memory in Appendix as well. 






\subsubsection{Basic results} \Cref{table:basic} and \ref{table:nonlinear} display the estimation errors and prediction performance of baselines and our strategy. For each sample reweighting scheme and its combination with SAWA, superior results (indicated by lower values) are highlighted in bold typeface. The optimal result among all methods is underlined for each setting. 
As we can see, the integration with SAWA yields improvements when combined with four different independence-based sample reweighting algorithms, enhancing model parameter estimation and the average OOD prediction performance across all settings. 
Meanwhile, SAWA also reduces Std\_Error and Max\_Error in most settings, implying that it improves the stability of test performance. 
Such improvements are also witnessed in nonlinear settings. 
We can refer to Figure~\ref{fig:linear} as well, where improvements are made across all test environments when paired with established reweighting techniques (solid lines), especially when $r_{test}<0$, signifying a reversed correlation compared with $r_{train}$. 
It is worth noting that in linear settings SAWA brings improvement in both scenarios, where the scenario of strong collinearity in both $\bolds$ and $\boldv$ has never been well addressed before. 
Overall, the flexibility of seamlessly integrating with any reweighting method, coupled with improvements observed across all settings, collectively demonstrates the efficacy and universality of our strategy against covariate shift. 


\subsubsection{Bias-variance analysis} Early reweighting methods suffer from variance inflation due to inefficient utilization of samples, an inherent vulnerability of the reweighting operation itself. 
In \Cref{fig:bv}, it is evident that after applying SAWA, variance of model parameter estimation diminishes sharply while the bias increases by a tiny margin or decreases, leading to better bias-variance trade-off. 

\subsubsection{Analysis for the size of ensemble} The size of sample weights ensemble is a hyperparameter in our strategy. We conduct experiments to assess its influence on the performance. From Figure \ref{fig:ens}, we can see that after exceeding $10$, the reduction in RMSE becomes negligible. Thus we fix it to a constant value of $10$ across all conducted experiments. 




\begin{figure*}[!t]
	\centering
	\subfigure[Prediction error for house price.] {
	\label{fig:house}
	    \includegraphics[width=0.3\linewidth]{figures/house.pdf}
	}
	\subfigure[Classification error for Adult.] {
	\label{fig:adult}
	    \includegraphics[width=0.3\linewidth]{figures/adult.pdf}
	}
	\subfigure[Classification error for C-MNIST] {
	\label{fig:cmnist}
	    \includegraphics[width=0.3\linewidth]{figures/cmnist.pdf}
	}
 	\subfigure[Classification error for ACS PUMS data] {
	\label{fig:acs}
	    \includegraphics[width=\linewidth]{figures/ACS.pdf}
	}
	\caption{Results of experiments on real-world data. The subscript $_s$ represents a combination with SAWA. We use similar colors for a certain reweighting method w or w/o SAWA (darker or lighter). 
    For the convenience of plotting, we only plot 7 methods. The detailed results of other methods are in Appendix. We can see that after being combined with SAWA, all sample reweighting methods gain a decrease in prediction error against distribution shifts. 
 }
 \label{fig:real}
\end{figure*}

\subsubsection{Comparison with other averaging strategies}
Previous OOD works employ moving average (MA) \citep{arpit2022ensemble} and model parameter averaging from multiple runs (DiWA) \citep{rame2022diverse}. The latter averages model parameters learned with the same initialization but different hyperparameters, of which a direct adaptation to our linear setting is model coefficient averaging (CA). 
In \Cref{fig:dist-comp}, we plot the sample-wise distribution of sample weights of MA and SAWA along with the original weights. 
The result of MA almost overlaps with the original one, while that of SAWA exhibits remarkable differences. 
In \Cref{fig:sim-comp}, we plot the similarity of sample weights generated by MA and SAWA. Here we adopt the logarithmically transformed result of the third term in \Cref{eq:decomp} as the metric, which describes the correlation of different weighting functions. 
As observed, the similarity metric of SAWA is much lower. Combining these findings, we confirm that SAWA indeed produces significantly more diverse sets of sample weights. This conforms to theoretical analyses of \Cref{eq:decomp}, where the third term is more greatly decreased by SAWA than MA. 
As for CA, it also assembles results from multiple runs, but its estimation lacks theoretical properties for validity, while the sample weights acquired through SAWA can be guaranteed by \Cref{prop:dwr} and \ref{prop:srdo} to be valid and reasonable. Moreover, \Cref{fig:avg-comp} reveals that despite CA improving upon MA, it still falls behind SAWA by a large margin. 




\subsection{Experiments on Real-World Data}
\label{sec:real}

We apply our strategy to multiple tabular data tasks and an image recognition task, following previous works \citep{arjovsky2019invariant, shen2020stable, liu2021heterogeneous}. 
Detailed information of the datasets are left in Appendix. 

\subsubsection{House price prediction}
It is a regression task predicting house prices based on attributes like the number of bedrooms or bathrooms. 
We partition the dataset into six distinct periods, each spanning two decades, treated as 6 environments. To examine the covariate-shift generalization ability, we train on the first period and test on the other five. 
As depicted in \Cref{fig:house}, in most cases SAWA brings improvement to current reweighting methods. This demonstrates the benefits of SAWA in real-world tasks. 

\subsubsection{People income prediction}
\label{exp:adult}
It is binary classification for predicting if the annual income of an adult surpasses 50k based on UCI Adult \citep{kohavi1996scaling}. We split the dataset into ten environments according to combination of race and sex. We train on the environment of (White, Female) and test on the others. From \Cref{fig:adult}, when testing on the last five environments, characterized by a male sex attribute, we observe a large improvement after applying SAWA. 

\subsubsection{Colored MNIST}
Following~\citet{arjovsky2019invariant}, we convert the labels of MNIST into binary format: assigning $0$ to $0\sim 4$, and $1$ to $5\sim 9$. Then we assign a color id of each image identical to the digit label but with a probability of $R$ to flip it. We also introduce label noise by flipping the digit label with a probability $C$. 
We set $C=0.25$ across all environments, and set $R=0.25$ for the training environment. For test environments, we generate via $R\in\{0.5,0.7,0.9\}$ for a reverse correlation compared with training data. From Figure \ref{fig:cmnist}, SAWA still decreases the error when applied to existing methods. 
This reveals the potential of our strategy for extension to vision tasks. 


\subsubsection{ACS PUMS data} \citet{ding2021retiring} provide five binary classification tasks from US-wide ACS PUMS data in terms of employment, income, mobility, public coverage, and travel time. For each task, we randomly select 2 states 'AL' and 'SC' to establish 2 settings. For each setting, We treat 'AL' or 'SC' as the training environment and use the other 49 states as the test environments. Although ACS PUMS is a superset of the UCI Adult dataset used in Sec \ref{exp:adult}, we generate different distribution shifts for them. For UCI Adult, the shift comes from race and sex. For ACS PUMS, the shift comes from states, i.e. geographical locations. 
The results are in Figure \ref{fig:acs}. We observe increases in the test performance for each sample reweighting algorithm across all tasks after applying SAWA, which demonstrates the benefits of SAWA in terms of stable prediction. 
