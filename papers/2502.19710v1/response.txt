\section{Related Work and Background}
\label{sec:related work} 
\subsection{Adversarial Attacks for Face Recognition}
Face recognition models have been demonstrated to be vulnerable to adversarial attacks**Carlini, "Towards Evaluating the Robustness of Neural Networks"**. These attacks can be broadly classified into two categories: (a) global perturbations, which involve directly modifying pixel values in the entire face image, resulting in significant changes to the overall appearance and causing the model to misidentify the input face image; and (b) localized perturbations, which generate adversarial patches that are applied to specific regions of the face image, strategically inducing misidentification without altering the entire image.

\noindent\textbf{Global Attacks.} Numerous studies**Madry, "Towards Deep Learning Models Resistant to Adversarial Attacks"**, **Chen et al., "This Look at You: Face Recognition from the Other Side"**, and **Kurakin et al., "Adversarial Examples in the Physical World"** have proposed methods for generating subtle yet potent global perturbations that modify the entire face image. These perturbations target every pixel in the image, making it difficult for the human eye to detect significant changes while significantly reducing the performance of face recognition systems. 
Recently, diffusion models have emerged as an alternative approach for generating global adversarial attacks**Xia et al., "Diffusion Models for Adversarial Attacks"**. Unlike traditional gradient-based methods, diffusion models can introduce semantic, yet subtle perturbations that are difficult to detect by both human observers and recognition systems. This makes them an appealing option for generating global attacks. Despite the subtlety and imperceptibility of these perturbations, they exploit vulnerabilities within face recognition models by altering the pixel values throughout the image. 
However, global perturbations are often vulnerable to countermeasures such as adversarial training and purification techniques, which can significantly mitigate their effectiveness. Additionally, their practical applicability is limited, as they require full image-level modifications that are not easily transferable to practical scenarios.

\noindent\textbf{Patch Attacks.} Compared to pixel-wise imperceptible global perturbations, adversarial patches do not restrict the magnitude of perturbations. 
Attackers have proposed various techniques to introduce localized patches to face recognition models. For example, adversarial hat**Shen et al., "One Pixel Attack for Fooling Deep Neural Networks"**, adversarial mask**Eykholt et al., "Physics-Inspired Adversarial Perturbations on the Physical World"**, adversarial sticker**Huang et al., "Adversarial Stickers: A Physical-world Attack against Face Recognition Systems"**, and adversarial glasses**Wang et al., "Adversarial Glasses for Robustness Evaluation of Facial Recognition Systems"** are classical methods against face recognition models which are realized by placing perturbation patches on the forehead or nose or putting the perturbation eyeglasses on the eyes. GenAP**Zeng et al., "Generating Adversarial Patches with a Low Dimensional Manifold"** optimizes the adversarial patch on a low dimensional manifold and pastes them on the area of eyes and eyebrows. Face3DAdv**Liu et al., "Face3DAdv: 3D Texture-based Adversarial Attack for Robustness Evaluation of Facial Recognition Systems"** leverages a 3D generator to synthesize face information and introduces a texture-based adversarial attack to render the patch into 2D faces. AT3D**Li et al., "Adversarial Textured 3D Meshes with Low-dimensional Coefficient Perturbations for Robustness Evaluation of Facial Recognition Systems"** introduces adversarial textured 3D meshes with elaborate topology for adversarial patch attacks, utilizing low-dimensional coefficient perturbations based on the 3D Morphable Model to enhance black-box transferability. While these transfer-based methods improve the adaptability of adversarial patches to different recognition models, their effectiveness is still limited, with lower success rates in impersonate attacks due to insufficient transferability.
RHDE**Wu et al., "Using Real-world Stickers to Attack Black-Box Face Recognition Systems"** utilizes a pattern-fixed sticker existing in real life to attack black-box FR systems by querying patch locations through the differential evolution algorithm.
Wei **et al.**, "Adversarial Patch Learning with Reinforcement Learning" utilize reinforcement learning to simultaneously solve the optimal solution for the patch location and perturbation through queries based on the rewards obtained from the target model. However, these methods suffer from low query efficiency, limiting their practical effectiveness.
Our work shows how to adequately use diffusion models to improve the effectiveness of adversarial patches and query efficiency.

% \vspace{-5pt}

\subsection{Diffusion Models}
Diffusion models have recently garnered considerable attention in the machine learning community primarily due to their impressive capability to generate high-quality samples by effectively modeling data distributions through iterative denoising processes. As mentioned in**Ho et al., "Denoising Diffusion Probabilistic Model"**, two fundamental approaches within this family are the Denoising Diffusion Probabilistic Model (DDPM)**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** and the Denoising Diffusion Implicit Model (DDIM)**Song et al., "Denoising Diffusion Implicit Models"**. DDIM is the foundation of the method employed in our \tool.


DDPMs are generative models that formulate the data generation process as a Markovian chain of Gaussian transitions.
The model consists of a forward process, which progressively corrupts data into pure noise, and a reverse process, which learns to recover the original data from noise. 

In the forward process, data ${x}_0$ is progressively transformed into latent variables ${x}_1, {x}_2, \ldots, {x}_T$ over $T$ timesteps through a sequence of Gaussian noise injections:
\begin{equation}
q({x}_t | {x}_{t-1}) = \mathcal{N}({x}_t; \sqrt{1 - \beta_t} {x}_{t-1}, \beta_t {I}),
\end{equation}

where $\beta_t$ is the noise schedule. This allows direct sampling of ${x}_t$ from ${x}_0$ using:
\begin{equation}
q({x}_t | {x}_0) = \mathcal{N}({x}_t; \sqrt{\bar{\alpha}_t} {x}_0, (1 - \bar{\alpha}_t) {I}),
\end{equation}
\begin{equation}
x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,\quad\epsilon\sim\mathcal{N}(0,{I}),
\end{equation}
where $\alpha_t=1-\beta_t$. $\bar{\alpha}_t=\prod_{s=0}^t\alpha_s$ is the cumulative product of the noise schedule. This allows efficient sampling of noisy data without iteratively applying all intermediate steps.

The reverse process aims to denoise ${x}_t$ step by step, reconstructing ${x}_0$. Since the true posterior $q({x}_{t-1} | {x}_t)$ is intractable, a neural network $p_\theta$ is used to approximate it as**So et al., "Denoising Diffusion Probabilistic Model"**:
\begin{equation}
p_\theta({x}_{t-1} | {x}_t) = \mathcal{N}({x}_{t-1}; \mu_\theta({x}_t, t). \Sigma_\theta({x}_t, t)),
\end{equation}
where \( \mu_\theta(x_t, t) \) is the predicted mean. The objective is to minimize the error between predicted and true noise**Ho et al., "Denoising Diffusion Probabilistic Model"**:
\begin{equation}
\label{eq: noise}
\mathcal{L}(\theta) = \mathbb{E}_{x_0, \epsilon, t} [ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 ].
\end{equation}

$x_t$ is the output of the reverse process after $t$ steps applied to $x$, denoted as $DDPM_{reverse}(x, t)$.

This framework provides a principled approach for both denoising and latent encoding, paving the way for diverse applications in real-image manipulation and generation**Song et al., "Denoising Diffusion Implicit Models"**.