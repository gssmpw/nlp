\section{Related Work and Background}
\label{sec:related work} 
\subsection{Adversarial Attacks for Face Recognition}
Face recognition models have been demonstrated to be vulnerable to adversarial attacks~\cite{szegedy2013intriguing, dong2019efficient, zhong2020towards, wang2024survey}. These attacks can be broadly classified into two categories: (a) global perturbations, which involve directly modifying pixel values in the entire face image, resulting in significant changes to the overall appearance and causing the model to misidentify the input face image; and (b) localized perturbations, which generate adversarial patches that are applied to specific regions of the face image, strategically inducing misidentification without altering the entire image.

\noindent\textbf{Global Attacks.} Numerous studies \cite{dong2019efficient, zhong2020towards, yang2021attacks, deb2020advfaces, jia2022adv, goswami2018unravelling, zhong2019adversarial, chatzikyriakidis2019adversarial, dabouei2019fast} have proposed methods for generating subtle yet potent global perturbations that modify the entire face image. These perturbations target every pixel in the image, making it difficult for the human eye to detect significant changes while significantly reducing the performance of face recognition systems. 
Recently, diffusion models have emerged as an alternative approach for generating global adversarial attacks~\cite{couairon2023diffedit, chen2023advdiffuser, sun2024diffam, chen2024diffusion}. Unlike traditional gradient-based methods, diffusion models can introduce semantic, yet subtle perturbations that are difficult to detect by both human observers and recognition systems. This makes them an appealing option for generating global attacks. Despite the subtlety and imperceptibility of these perturbations, they exploit vulnerabilities within face recognition models by altering the pixel values throughout the image. 
However, global perturbations are often vulnerable to countermeasures such as adversarial training and purification techniques, which can significantly mitigate their effectiveness. Additionally, their practical applicability is limited, as they require full image-level modifications that are not easily transferable to practical scenarios.

\noindent\textbf{Patch Attacks.} Compared to pixel-wise imperceptible global perturbations, adversarial patches do not restrict the magnitude of perturbations. 
Attackers have proposed various techniques to introduce localized patches to face recognition models. For example, adversarial hat~\cite{komkov2021advhat}, adversarial mask~\cite{xiao2021improving, yang2022controllable, yang2023towards}, adversarial sticker~\cite{wei2022adversarial, wei2022simultaneously, pautov2019adversarial, xiao2021improving} and adversarial glasses~\cite{sharif2019general, sharif2016accessorize} are classical methods against face recognition models which are realized by placing perturbation patches on the forehead or nose or putting the perturbation eyeglasses on the eyes. GenAP~\cite{xiao2021improving} optimizes the adversarial patch on a low dimensional manifold and pastes them on the area of eyes and eyebrows. Face3DAdv~\cite{yang2022controllable} leverages a 3D generator to synthesize face information and introduces a texture-based adversarial attack to render the patch into 2D faces. AT3D~\cite{yang2023towards} introduces adversarial textured 3D meshes with elaborate topology for adversarial patch attacks, utilizing low-dimensional coefficient perturbations based on the 3D Morphable Model to enhance black-box transferability. While these transfer-based methods improve the adaptability of adversarial patches to different recognition models, their effectiveness is still limited, with lower success rates in impersonate attacks due to insufficient transferability.
RHDE~\cite{wei2022adversarial} utilizes a pattern-fixed sticker existing in real life to attack black-box FR systems by querying patch locations through the differential evolution algorithm.
Wei \textit{et al.}~\cite{wei2022simultaneously} utilize reinforcement learning to simultaneously solve the optimal solution for the patch location and perturbation through queries based on the rewards obtained from the target model. However, these methods suffer from low query efficiency, limiting their practical effectiveness.
Our work shows how to adequately use diffusion models to improve the effectiveness of adversarial patches and query efficiency.

% \vspace{-5pt}

\subsection{Diffusion Models}
Diffusion models have recently garnered considerable attention in the machine learning community primarily due to their impressive capability to generate high-quality samples by effectively modeling data distributions through iterative denoising processes. As mentioned in~\cite{chen2024diffusion}, two fundamental approaches within this family are the Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising} and the Denoising Diffusion Implicit Model (DDIM)~\cite{song2021denoising}. DDIM is the foundation of the method employed in our \tool.


DDPMs are generative models that formulate the data generation process as a Markovian chain of Gaussian transitions.
The model consists of a forward process, which progressively corrupts data into pure noise, and a reverse process, which learns to recover the original data from noise. 

In the forward process, data ${x}_0$ is progressively transformed into latent variables ${x}_1, {x}_2, \ldots, {x}_T$ over $T$ timesteps through a sequence of Gaussian noise injections:
\begin{equation}
q({x}_t | {x}_{t-1}) = \mathcal{N}({x}_t; \sqrt{1 - \beta_t} {x}_{t-1}, \beta_t {I}),
\end{equation}

where $\beta_t$ is the noise schedule. This allows direct sampling of ${x}_t$ from ${x}_0$ using:
\begin{equation}
q({x}_t | {x}_0) = \mathcal{N}({x}_t; \sqrt{\bar{\alpha}_t} {x}_0, (1 - \bar{\alpha}_t) {I}),
\end{equation}
\begin{equation}
x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,\quad\epsilon\sim\mathcal{N}(0,{I}),
\end{equation}
where $\alpha_t=1-\beta_t$. $\bar{\alpha}_t=\prod_{s=0}^t\alpha_s$ is the cumulative product of the noise schedule. This allows efficient sampling of noisy data without iteratively applying all intermediate steps.

The reverse process aims to denoise ${x}_t$ step by step, reconstructing ${x}_0$. Since the true posterior $q({x}_{t-1} | {x}_t)$ is intractable, a neural network $p_\theta$ is used to approximate it as~\cite{sohl2015deep}:
\begin{equation}
p_\theta({x}_{t-1} | {x}_t) = \mathcal{N}({x}_{t-1}; \mu_\theta({x}_t, t). \Sigma_\theta({x}_t, t)),
\end{equation}
where \( \mu_\theta(x_t, t) \) is the predicted mean. The objective is to minimize the error between predicted and true noise~\cite{ho2020denoising}:
\begin{equation}
\label{eq: noise}
\mathcal{L}(\theta) = \mathbb{E}_{x_0, \epsilon, t} [ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 ].
\end{equation}
Sampling is then done with the trained $\theta({x}_t, t)$ :
\begin{equation}
x_{t-1}=\mu_\theta\left(x_t, t\right)+\sigma_t z, \quad z \sim N(0, I)
\end{equation}

DDPM employs a stochastic reverse process, whereas DDIM introduces a deterministic alternative to enhance sampling efficiency while still maintaining sample quality. 
Instead of a Markovian process, DDIMs use a fixed transformation to map noise to data~\cite{song2021denoising}:
\begin{equation}
{x}_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{{x}_t - \sqrt{1 - \alpha_t} \epsilon_\theta({x}_t, t)}{\sqrt{\alpha_t}} \right) + \sqrt{1 - \alpha_{t-1}} \epsilon_\theta({x}_t, t),
\end{equation}
where $\alpha_t$ is a cumulative product of $(1 - \beta_t)$ over timesteps, and $\epsilon_\theta$ represents the predicted noise.

The deterministic nature of DDIM can be interpreted as Euler integration for solving ordinary differential equations (ODEs)~\cite{song2021denoising}. This perspective enables the reverse process to effectively map a real image to its corresponding latent representation, facilitating a more stable transformation. This operation referred to as DDIM Inversion, facilitates subsequent manipulations of real images~\cite{song2021denoising}. Conversely, the reverse process in DDIM can be expressed as:
\begin{equation}
    \begin{aligned}x_{t+1}-x_{t}=&\sqrt{\bar{\alpha}_{t+1}}\left[\left(\sqrt{1/\bar{\alpha}_{t}}-\sqrt{1/\bar{\alpha}_{t+1}}\right)x_{t}\right.\\&+\left(\sqrt{1/\bar{\alpha}_{t+1}-1}-\sqrt{1/\bar{\alpha}_{t}-1}\right)\epsilon_{\theta}(x_{t},t)\Big]\end{aligned}
\end{equation}

$x_t$ is the output of $x$ after $t$ timesteps of DDIM Inversion, denoted as $DDIM_{inverse}(x, t)$.

The denoising process of DDIM (denoted as ${DDIM_{denoise}}$) operates as follows:
\begin{equation}
    \begin{aligned}x_{t-1}-x_{t}=&\sqrt{\bar{\alpha}_{t-1}}\left[\left(\sqrt{1/\bar{\alpha}_{t}}-\sqrt{1/\bar{\alpha}_{t-1}}\right)x_{t}\right.\\&\left.+\left(\sqrt{1/\bar{\alpha}_{t-1}-1}-\sqrt{1/\bar{\alpha}_{t}-1}\right)\epsilon_{\theta}(x_{t},t)\right]\end{aligned}
\end{equation}

$x_t$ is the output of the DDIM denoising process  after $t$ timesteps applied to $x$, denoted as $DDIM_{denoise}(x, t)$.


This framework provides a principled approach for both denoising and latent encoding, paving the way for diverse applications in real-image manipulation and generation~\cite{mokady2023null}~\cite{couairon2023diffedit}.