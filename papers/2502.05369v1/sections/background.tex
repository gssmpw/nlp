\section{Background}
\label{sec:background}

\subsection{Data Management and LSM-tree-based KV Stores}
\label{sec:rocksdb}
In order to formally define the interpretation of the data management system as random variables illustrated in Fig.~\ref{fig:prev-designs}, it is essential to express these variables through a mathematical formulation~\cite{2024limousine}. Conducting this first, we know that there are two primary methods to reduce the complexity of big data management: (1) \textit{Data Partitioning ($Par(.)$)}: divides given data into smaller partitions. (2) \textit{Indexing ($I(.,.)$)}: given partitioned data and a query key, it directly returns a subset of partitions where the target key is stored.

$Par(.)$ is unchanged by the key query, but it is possible to assume its integration with $I(.,.)$ for a given key $k$. Consequently, for simplicity in notation during the construction phase, we can represent it as $I(.)$. This approach lays the foundation for the formation of a comprehensive set of data structures with indexed entries relevant to any dataset $D$. A general combination form is generated by a set of $\mathcal{P}=\{ Par_1, Par_2, \dots, Par_n \}$ and $\mathcal{I}=\{ I_1, I_2, \dots, I_m \}$, denoted $\mathcal{F}=f_1(f_2(\dots f_k(D)))=f_1\circ f_2\circ\dots\circ f_k(D)$, where $f_i$ belongs to $\mathcal{P}$ for $i$ within the range $[1:n]$ or to $\mathcal{I}$ for $i$ within the range $[1:m]$. Recognize that this general structure may imply an infeasible data structure because particular partitions $Par_i$ or indexing $I_i$ presume certain data property assumptions about their input arguments. Once the construction is complete, $\mathcal{F}(k)$ acts as the top-level index for the newly built data structure.

In the context of an LSM-tree, the data structure is constructed from four distinct partitions defined as $\mathcal{P}_{LSM}=\{ Par_{TreeLevel}, Par_{SST}, Par_{Block}, Par_{KV} \}$. Its indexing mechanism is represented by $\mathbf{I}_{LSM}=\{ I_{LevelBloomFilter}, I_{SST}, I_{IndexBlock}, I_{KV} \}$. The composite function is articulated as $\mathcal{F}_{LSM}= I_{KV} \circ Par_{KV}\circ I_{IndexBlock}\circ Par_{Block}\circ I_{SST}\circ Par_{SST}\circ I_{LevelBloomFilter}\circ Par_{TreeLevel}(D)$.

Breaking down $\mathcal{F}_{LSM}$, initially, the given data $D$ is partitioned into a hierarchical structure through tree levels ($Par_{TreeLevel}$), employing a level bloom filter ($I_{LevelBloomFilter}$) that generates a Boolean index indicating the presence of a key at a particular level. Within each level, data are segregated into distinct SST files ($Par_{SST}$) with an index that maintains the key range within each SST and uses linear search, acting as the SST index ($I_{SST}$). Each SST arranges data in a sorted manner and is further divided into uniform blocks ($Par_{Block}$) containing key-value pairs ($Par_{KV}$). Furthermore, each SST contains an index block as part of its metadata, which includes an index entry for every data block ($I_{IndexBlock}$) facilitated by binary search, along with KV pairs indexed by $I_{KV}$ that are subjected to linear search in RocksDB. Fig.~\ref{fig:prev-designs} demonstrates four different $Par_{Block}$ configurations \{a,b,c,d\}, each generating a unique block-level division within the same SST file; further details are provided in \S~\ref{sec:design:overview}.

\begin{figure}[t]
  \centering
  \makebox{\includegraphics[width=\columnwidth]{figs/rocksdb-arch.pdf}}
  \vspace{-1.5em}
  \caption{\small{RocksDB Architecture.}}
  \label{fig:sst-format}
  \vspace{-1.5em}
\end{figure}


\subsection{Learned Index (LI)}
\label{sec:li-storage}

Learned indexes \cite{ding2020alex, ferragina2020pgm, li2020lisa, tang2020xindex, kipf2020radixspline} aim to improve the efficiency of data retrieval in database systems by employing machine learning models to map keys to their respective locations. These indexes can involve intricate models, such as neural networks, or simpler hierarchically organized models, such as linear models.
Traditional LIs use ensemble learning and hierarchical model structuring. Starting from the root node and progressing downward, the index model $I(k)$ predicts the subsequent layers to utilize for a query key $k$ based on $I(k) = M(k) \times N$, where $N$ represents the number of keys, and $M$ denotes the cumulative distribution function (CDF) that estimates probability $p(x \le k)$. Given the complexity of training and inference in sophisticated models, many LIs adopt piecewise linear models to approximate the CDF. Querying entails predicting the key position using $pos = a \times k + b$ with a maximum error $e$, where $a$ and $b$ are learned parameters, and $e$ is crucial for the final search to locate the target key. These indexes exhibit memory efficiency because of their lightweight parameters (an intercept along with a likely slope) compared to conventional indexes.

\vspace{3pt}
\noindent
\subsubsection{\textbf{LI on Persistent Storage}}

Recent studies~\cite{lidisk2024sigmod, wipe22024, apex2021, liDisk2024} have shown that current LIs do not outperform traditional index solutions like B+tree when KVs reside within persistent storage. These studies highlight that the design of LIs fails to leverage the characteristics of disk storage, necessitating multiple disk I/O operations during the last-mile search process as shown in Fig.~\ref{fig:li-storage}. A similar trend is observed in LI techniques for LSM-trees, which also entail substantial I/O operations during their final search phase. This underscores the importance of considering I/O as a critical element in the development of LI systems.

\vspace{3pt}
\noindent
\subsubsection{\textbf{LI on Strings}}
\label{sec:li-on-strings}
The exploration of LIs has predominantly revolved around fixed-sized integers or floating-point keys. However, their application to variable-length string keys has received relatively less attention, with only a few studies addressing this issue \cite{rss, wang2020sindex, lits2024vldb}. An advanced solution in this domain is the Radix String Spline (RSS) \cite{rss}, which employs a trie-based approach. In RSS, each trie node handles $8$-byte or $16$-byte keys and computes a Radix Spline (RS) model \cite{kipf2020radixspline} for these keys. The memory architecture and the programming language used typically impose limits on the size of integer types. In a 64-bit memory environment, the largest standard integer type that C++ natively supports is \texttt{\_\_uint64\_t}. However, some compilers provide non-standard support for 128-bit integers (e.g. \texttt{ \_\_uint128\_t}), which can be effectively utilized to convert a Trie tree node from a string format to an integer representation. The RS model, a piecewise linear model, provides monotonic CDF predictions within a specified error threshold. For keys that exceed the error limit (e.g. due to shared prefixes), RSS stores the key in a redirector map and creates a new child node for that key. RSS compares $8$-byte or $16$-byte segments of keys, incurring lower costs compared to full-key comparisons. Despite these relatively small comparisons, the last-mile search process remains costly. A recent study \cite{lits2024vldb} indicates that RSS spends more than $70\%$ of its time in the last-mile search phase, which points to significant optimization potential in the last-mile search for LIs on string keys.


\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{figs/li_persistant_storrage.pdf}}
  \vspace{-1.6em}
  \caption{\small{LI on Persistent Storage. The model last-mile search range may require loading multiple pages from the storage.}}
  \label{fig:li-storage}
  \vspace{-0.5em}
\end{figure}

\subsection{Key-Value Length in LSMs}
\label{sec:var-kv-lsm}
LSM-tree storage engines are commonly used in various applications and services, including social networks, financial systems, and AI workflows. Thus, they need to handle keys and values of all types and lengths. The study by Cao et al. \cite{modelrocksdb2020} examines the use of RocksDB in three different use cases at \textit{Meta}~(see Table~\ref{tab:kv-avg-sd}): \textbf{UDB} (storage engine of a SQL database), \textbf{ZippyDB} (storage engine of a distributed KV-store), and \textbf{UP2X} (persistent storage of an AI/ML service). The study presented the average (AVG) and standard deviation (SD) of the KV length in these applications. Both keys and values were found to have variable sizes, with a smaller standard deviation for keys and a larger for values. Previous LSM-tree LI solutions~\cite{Bourbon2020, TridentKV2022, leaderkv2024, wang2024learnedkv} are limited to fixed key sizes, rendering them unsuitable for these applications.

\begin{table}[]
\centering
\caption{\small{AVG and SD of key-values (in bytes) on RocksDB use cases on UDB, ZippyDB, and UP2X.}}
\vspace{-0.5em}
\label{tab:kv-avg-sd}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Application}} & \multicolumn{2}{c|}{\textbf{Key}}               & \multicolumn{2}{c}{\textbf{Value}}              \\ \cline{2-5} 
                                      & \multicolumn{1}{c|}{\textbf{AVG}} & \textbf{SD} & \multicolumn{1}{c|}{\textbf{AVG}} & \textbf{SD} \\ \hline
\textbf{UDB}     & \multicolumn{1}{c|}{27.1} & 2.6 & \multicolumn{1}{c|}{126.7} & 22.1 \\ \hline
\textbf{ZippyDB} & \multicolumn{1}{c|}{47.9} & 3.7 & \multicolumn{1}{c|}{42.9}  & 26.1 \\ \hline
\textbf{UP2X}    & \multicolumn{1}{c|}{10.4} & 1.4 & \multicolumn{1}{c|}{46.8}  & 11.6 \\ \hline
\end{tabular}

}
\vspace{-1em}
\end{table}

\subsection{Lexicographic Optimization}
\label{sec:lexi_opt}
Lexicographic optimization~\cite{abernethy2024lexicographic, chinchuluun2007survey} is a structured approach to solving multiobjective optimization problems by prioritizing objectives based on their importance. The highest priority objective is optimized initially, followed by the next objectives, within a feasible space that preserves the previously achieved optimizations. This method ensures that each objective is tackled sequentially according to its rank, ensuring that earlier optimizations remain unaffected by appropriately constraining the subsequent optimization problems. This method simplifies decision-making policy of exploration in feasible space with a clear hierarchy by addressing the objectives one at a time. It offers a deterministic way to handle multiple objectives, which is useful for non-negligible goals in resource allocation or scheduling. By prioritizing critical objectives, lexicographic optimization ensures that they are met without compromise, allowing secondary goals to be optimized within the feasible solution space.

