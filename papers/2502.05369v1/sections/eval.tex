\section{Evaluations}
\label{sec:eval}
This section presents the results of the \texttt{DobLIX} evaluation, emphasizing its advantages over current leading methods. We design the evaluations to answer the following questions: 1. What performance advantages does \texttt{DobLIX} offer? (\S~\ref{sec:eval:perf}) 2. How does the length of key-value pairs affect \texttt{DobLIX} and other baseline systems? (\S~\ref{sec:eval:kvlen}) 3. What advantages do \texttt{DobLIX} provide in the storage footprint? (\S~\ref{sec:eval:storage}) 4. How does the RL agent adjust its underlying parameters? (\S~\ref{sec:eval:param})


\subsection{Experimental Setup}
\label{sec:experimental_setup}

\noindent\subsubsection{\textbf{Environment}}
We implemented \texttt{DobLIX} in C++17 and integrated it with RocksDB version 8.1.1. The source files were compiled with GCC 9.4.0. We conducted our evaluation on a Linux machine running Ubuntu 20.04, powered by an AMD Ryzen ThreadRipper Pro 5995WX processor with 64 cores at 2.7GHz and equipped with 256GB of DDR4 RAM. The storage device used was a SAMSUNG 980 Pro 2TB M.2 NVMe SSD. This NVMe SSD offers fundamental read and write performance measures as follows: sequential read speeds of 7,000 MB/s, random read speeds of 1,237K IOPS, sequential write speeds of 5,000 MB/s, and random write speeds of 172.5K IOPS. A single experiment was performed on a Seagate BarraCuda 4TB SATA SSD. We allocate 4 background threads to RocksDB, and set the \say{max\_background\_compactions} and \say{max\_background\_flushes} options as 4.

\vspace{1pt}
\noindent\subsubsection{\textbf{Datasets}} 
We utilize four real-world datasets sourced from the Search on Sorted Data Benchmark (SOSD)\cite{kipf2019sosd} along with two synthetic datasets for evaluating \texttt{DobLIX}. These datasets have been used in previous studies\cite{TridentKV2022, Bourbon2020, ding2020alex}. Each contains $64$ million key-value pairs. The size of the key and the value are set according to the experiments used in our baselines (\cite{Bourbon2020,TridentKV2022}) as $8$ byte and $64$ byte. We also conduct experiments with other fixed key and value sizes, and also with varying key and value sizes to reflect real-world applications.
The following sections provide specific information about each dataset. \textit{WIKI~\cite{WikiTS}.} Edit timestamps for Wikipedia articles. \textit{AMZN}: Popularity of book sales collected from Amazon. \textit{FB~\cite{FB}}: An upsampled version of a Facebook user ID dataset.  \textit{OSM~\cite{OSM}}: Uniformly sampled locations as Google CellIds.  \textit{LOGN}: This synthetic dataset is generated from a lognormal distribution with parameters $\mu=0$ and $\sigma=2$, multiplied by $10^9$ and rounded down to the nearest integer. \textit{UNI}: This synthetic dataset is generated from a uniform distribution ranging from zero to $10^{16}$.

\vspace{1pt}
\noindent
\subsubsection{\textbf{Workloads}}
We evaluate \texttt{DobLIX} with four different workloads, each consisting of $10$ million operations. \textit{Read-Only:} Focuses solely on read operations. \textit{Read-Heavy:} Emphasizes reads ($90\%$) with a smaller proportion of inserts ($10\%$). \textit{Write-Heavy:} Involves an equal split between reads and inserts ($50\%$ each). \textit{Write-Only:} Concentrates entirely on write operations.
We also use YCSB~\cite{cooper2010benchmarking} real benchmarks to evaluate \texttt{DobLIX}. In all workloads, the search key is selected randomly from the existing set of keys in the index with a Zipfian distribution~\cite{cormode2008finding}, unless a different request distribution is specified.

\vspace{1pt}
\noindent
\subsubsection{\textbf{Baselines}} We select the following representative schemes as baselines for comparison. \textbf{\textit{{RocksDB~\cite{rocksdbpaper}}}}: An embedded high-performance KV-store used as the storage engine. We utilize the default indexing mechanism of RocksDB.
\textbf{\textit{{Bourbon~\cite{Bourbon2020}}}}: A KV store with an LI that accelerates lookups by understanding the distribution of keys. Based on WiscKey~\cite{lu2017wisckey}, a LevelDB~\cite{leveldb} variant, Bourbon retains fixed block sizes, but might need to load several blocks (See Fig.~\hyperref[fig:prev-designs]{\ref*{fig:prev-designs}A}). The Bourbon source code is publicly accessible~\cite{BourbonGithub}. We incorporate Bourbon indexing method into our RocksDB configuration. \textbf{\textit{{TridentKV~\cite{TridentKV2022}}}}: an LI variant of RocksDB that aims to retrieve KV pairs by loading only one data block. TridentKV modifies the size of the data blocks, potentially significantly increasing the size of the block, which can affect the lookup performance when loading a large block (see Fig.~\hyperref[fig:prev-designs]{\ref*{fig:prev-designs}C}). TridentKV code is available as open-source~\cite{TridentKVGithub} and is built on top of RocksDB. We adopt their implementation in our evaluations.

\begin{figure*}
  \centering
  \makebox[\textwidth][c]{\includegraphics[width=0.99\textwidth]{figs/eval_throughput.pdf}}
  \vspace{-2em}
  \caption{\small{Throughput Comparison.}}
  \label{fig:eval-throughput}
  \vspace{-0.5em}
\end{figure*}

\begin{figure*}
  \centering
  \makebox[\textwidth][c]{\includegraphics[width=0.99\textwidth]{figs/eval_latency.pdf}}
  \vspace{-2em}
  \caption{\small{Tail Latency Comparison.}}
  \label{fig:eval-latency}
  \vspace{-1em}
\end{figure*}

\noindent
\subsubsection{\textbf{Metrics}} We use the following metrics to evaluate \texttt{DobLIX} and all other baselines. \textbf{\textit{{Throughput:}}} Average rate of operations per second. \textbf{\textit{ {Latency:}}} Average latency in the $99^{th}$ percentile of all operations. \textbf{\textit{ {Tail Latency:}}}  The average latency of the $5\%$ slowest operations. \textbf{\textit{ {Index size:}}} LI and index block size. \textbf{\textit{{SSTable Compaction time:}}} The average duration taken to create SSTables in the compaction process.

\vspace{1pt}
\noindent
\subsubsection{\textbf{Parameters}}
\label{sec:parameters}
By default, all methods adhere to the default configuration settings of RocksDB. The default configurations of Bourbon and TridentKV are also employed.
For certain parameters such as the chosen learning method (either PLA or PRA), the maximum error bound, and the maximum block size, the tuning agent is responsible for making decisions. Initially, we use a $1\%$ sample from the datasets to train this agent, after which we incorporate the agent into the system. In the case of the PLA method, we use the same setup as RSS~\cite{rss} to configure a dynamic radix table with $18$ bits for the first level, $12$ bits for the second level and $8$ bits for the third level and beyond. 

Regarding the hyperparameters of the RL agent $\alpha$ and $\gamma$ (see \S~\ref{sec:design:agent}), we perform a sensitivity test with values $0.2$, $0.5$ and $0.8$. We observe that optimal results are achieved by assigning a low value of $0.2$ to $\alpha$ and a high value of $0.8$ to $\gamma$. We also set the initial value of $\epsilon$ at $0.99$, with its minimum value being $0.02$, allowing for a low exploration rate even after the training period (\S~\ref{sec:exp:rl-agent-tuning}). We set $\nu$ in the reward function as 1 to optimize solely on performance.


\subsection{Performance}
\label{sec:eval:perf}
In this section, we detail a set of experiments designed to evaluate \texttt{DobLIX} performance, alongside a comparison with baseline systems. Importantly, no cache was integrated into any system, ensuring that the comparisons remain fair.
The initial experiment (\S~\ref{sec:eval:throughput}) highlights that \texttt{DobLIX} improves throughput by up to $1.41\times$, $1.52\times$, and $2.21\times$ relative to TridentKV, Bourbon, and RocksDB, respectively.
In the second experiment (\S~\ref{sec:eval:tail}), \texttt{DobLIX} achieves up to $2.67\times$ better throughput in terms of tail latency.
The third experiment (\S~\ref{sec:eval:breakdown}) examines the optimization of \texttt{DobLIX} lookup latency components.
The fourth experiment (\S~\ref{sec:eval:ycsb}) showcases \texttt{DobLIX} enhanced performance on the realistic YCSB macrobenchmarks.
Finally, the last experiment (\S~\ref{sec:eval:req-distr}) demonstrates \texttt{DobLIX} performance across varying request distributions. 

\subsubsection{\textbf{Throughput}}
\label{sec:eval:throughput}
{\small\textbf{Read-Only Workload.}} Fig.~\hyperref[fig:eval-throughput]{\ref*{fig:eval-throughput}a} displays the throughput when subjected to a read-only workload on different datasets. In this scenario, \texttt{DobLIX} achieves the highest throughput, reaching a maximum of $105K~ops/sec$ with the LOGN dataset. Compared to TridentKV, Bourbon, and RocksDB, \texttt{DobLIX} increases the average throughput by $1.41\times$, $1.57\times$, and $1.92\times$, respectively. 

\vspace{1pt}
\noindent
{\small\textbf{Read-Heavy Workload.}} Fig.~\hyperref[fig:eval-throughput]{\ref*{fig:eval-throughput}b} displays the performance results under the read-heavy workload, where there is a minor write rate. \texttt{DobLIX} continues to show the highest throughput in all datasets, reaching up to $82K~ops/sec$.
Compared to TridentKV, Bourbon, and RocksDB, \texttt{DobLIX} increases the average throughput by $1.27\times$, $1.36\times$, and $1.22\times$, respectively. Despite the marginal reduction in throughput from write operations due to the slight training overhead for new SSTables, \texttt{DobLIX} and other LIs still maintain superior throughput compared to RocksDB indexing approach.

\vspace{1pt}
\noindent
{\small\textbf{Balanced Workload.}} In this workload with increased write operations, \texttt{DobLIX} continues to outperform other systems in terms of throughput. As depicted in Fig.~\hyperref[fig:eval-throughput]{\ref*{fig:eval-throughput}c}, \texttt{DobLIX} achieves an average improvement of $1.29\times$, $1.27\times$, and $1.31\times$ compared to TridentKV, Bourbon, and RocksDB, respectively. 

\vspace{1pt}
\noindent
{\small\textbf{Write-Heavy Workload.}} In Fig.~\hyperref[fig:eval-throughput]{\ref*{fig:eval-throughput}d}, we show data from a write-heavy workload scenario, representing the maximum write rate observed in our experiments. Once again, \texttt{DobLIX} shows the highest throughput, reaching $53K~ops/sec$.
Compared to TridentKV, Bourbon, and RocksDB, \texttt{DobLIX} increases the average throughput by $1.16\times$, $1.14\times$, and $1.04\times$, respectively. 


\vspace{1pt}
\noindent\textit{\small\textbf{Takeaway}:} \texttt{DobLIX} optimization enhances the throughput of the index and data access phase. By optimizing data access, \texttt{DobLIX} minimizes read amplification by ensuring that only one block is precisely read per query while maintaining optimal block sizes. In contrast, Bourbon loads multiple blocks per lookup, and TridentKV features excessively large block sizes. When handling write-heavy tasks, \texttt{DobLIX} outperforms Bourbon and TridentKV by minimizing write amplification through its smaller index size. Additionally, \texttt{DobLIX} enhancements in read queries make it slightly superior to RocksDB, even in scenarios with heavy write operations which the reason is explained in \S~\ref{sec:design:serialization}.


\subsubsection{\textbf{Tail Latency}} 
\label{sec:eval:tail}
Analyzing tail latencies of different methods is vital because it defines the performance expectations for users and applications regarding their back-end key-value storage engine under the most challenging scenarios.

\noindent
{\small\textbf{Read-Only Workload.}}
Fig.~\hyperref[fig:eval-latency]{\ref*{fig:eval-latency}a} shows the tail latency results for read-only workloads, indicating that \texttt{DobLIX} exhibits the lowest tail latency. Notably, \texttt{DobLIX} improvement in tail latency surpasses the improvements observed in throughput. Compared to TridentKV, Bourbon, and RocksDB, \texttt{DobLIX} achieves improvements of $1.85\times$, $1.89\times$, and $2.13\times$, respectively. This difference is due to the high read amplification and index lookup inefficiency of other methods, explained in \S~\ref{sec:eval:perf}.

\noindent
{\small\textbf{Workloads with Write.}} Fig.~\hyperref[fig:eval-latency]{\ref*{fig:eval-latency}\{b,c,d\}} presents the tail latency results for read-heavy, balanced, and write-heavy workloads. These figures illustrate that \texttt{DobLIX} achieves the lowest tail latency across all workload types. In terms of tail latency, RocksDB indexing outperforms both Bourbon and TridentKV, which employ LIing without optimizing for the data access phase. TridentKV exhibits the highest tail latency among these workloads, showing up to $2.67\times$ greater latency than \texttt{DobLIX} due to its large blocks, which negatively affect both read and write operations. Bourbon also experiences elevated tail latency in write-intensive workloads due to its dependence on a garbage collection mechanism\cite{Bourbon2020}, which degrades its performance during this period.


\vspace{1pt}
\noindent\textit{\small\textbf{Takeaway}:} The result shows that \texttt{DobLIX} has lower latency spikes while competing with its opponents in the ordinary time span. 

\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=0.9\columnwidth]{figs/eval_latency_breakdown.pdf}}
  \vspace{-2em}
  \caption{\small{Normalized Latency Breakdown of \texttt{DobLIX} (`D'), TridentKV(`T'), Bourbon(`B'), and RocksDB (`R'). `L' and `S', on the legend, stand for load and seek, respectively.}}
  \label{fig:eval-breakup}
  \vspace{-2em}
\end{figure}

\subsubsection{\textbf{Lookup Query Latency Breakdown}}
\label{sec:eval:breakdown}
In Fig.~\ref{fig:eval-breakup}, the breakdown of the latencies of systems in read-heavy workloads is presented using the AMZN and LOGN datasets. Latencies are normalized by RocksDB latency.
\texttt{DobLIX} achieves average latency improvements of up to $23\%$, $27\%$, and $56\%$ compared to TridentKV, Bourbon, and RocksDB. Certain components, such as the Seek file or $I_{SST}$ (S-file) and Load KV (L-KV), show minimal changes, as these steps are consistent across all methods. The load index block (L-index) sees an improvement of $10.35\%$ on average compared to RocksDB, attributed to \texttt{DobLIX} LI methods. \texttt{DobLIX} also improves the lookup of blocks or $I_{IndexBlock}$ (S-block) by up to $22\%$ compared to RocksDB. Significant enhancements compared to Bourbon and TridentKV are seen in block loading (L-block) and locating the target KV within the block or $I_{KV}$ (S-KV). \texttt{DobLIX} multi-objective optimization strategy targets block size and precise block retrieval, resulting in a decrease in average block loading time by as much as $12.2\%$ compared to other methods. In terms of the KV last-mile search (S-KV), \texttt{DobLIX} demonstrates an average reduction of 11.1\% due to two optimizations: the elimination of key prefixes and the narrowing of the error bound range (as discussed in \S~\ref{sec:design:lastmile}).

\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{figs/eval_distr.pdf}}
  \vspace{-2em}
  \caption{\small Throughput Comparison on YCSB macrobenchmarks and various distribution workloads.}
  \label{fig:ycsb-distr}
  \vspace{-1em}
\end{figure}

\subsubsection{\textbf{YCSB Macrobenchmarks}}
\label{sec:eval:ycsb}
Fig.~\hyperref[fig:ycsb-distr]{\ref*{fig:ycsb-distr}a} illustrates the throughput performance of \texttt{DobLIX} and the three baseline systems on various YCSB workloads. In particular, \texttt{DobLIX} consistently achieves the highest throughput on the six workloads.
In the read-heavy YCSB\{B,C,D\} workloads, \texttt{DobLIX} demonstrates superior throughput of $481K$, $607K$, and $916K~ops/sec$, respectively. On average, \texttt{DobLIX} increases throughput by $1.32\times$, $1.42\times$, and $2.02\times$ compared to TridentKV, Bourbon, and RocksDB, respectively.
For balanced workloads YCSB\{A,F\}, \texttt{DobLIX} reaches throughput levels of $320K$ and $264K$ ops/sec, respectively, outperforming other techniques with an average increase in throughput of $56\%$.
In the YCSB-E scan-focused benchmark, \texttt{DobLIX} achieves the highest throughput of $80K$ operations per second. However, its gains over TridentKV, Bourbon, and RocksDB are relatively modest at $9.2\%$, $16.7\%$, and $42.8\%$, respectively. This relative performance is because the LIs enhance point queries rather than scan operations. During the scan phase of the range queries, all techniques exhibit uniform performance.

\subsubsection{\textbf{Various Request Distributions}}
\label{sec:eval:req-distr}
Fig.~\hyperref[fig:ycsb-distr]{\ref*{fig:ycsb-distr}b} shows the throughput of \texttt{DobLIX} and the three baselines in read-only workloads in the OSM dataset on six different request distributions. The figure shows that \texttt{DobLIX} maintains its superior read performance in the six request distributions. On average, \texttt{DobLIX} achieves $39.4\%$, $52.1\%$, and $78.9\%$ higher throughput compared to TridentKV, Bourbon, and RocksDB.

\subsection{Impact of Key and Value Length}
\label{sec:eval:kvlen}

In the previous section, we evaluated \texttt{DobLIX} performance with fixed $8$-byte keys and $64$-byte values. Here, we first show \texttt{DobLIX} performance by changing the key and value sizes while maintaining their constancy during the experiments. Then, we perform more realistic experiments in which the key and value sizes vary within each experiment (see \S~\ref{sec:var-kv-lsm}).

\subsubsection{\textbf{Fixed-sized key-values}}
Fig.~\hyperref[fig:eval-kv-size]{\ref*{fig:eval-kv-size}a} shows the impact of different fixed key sizes on each method. We excluded Bourbon from this experiment because of its limitation to $8$-byte keys. The results of the OSM dataset with read-only workload show that \texttt{DobLIX} has the highest throughput, even as the key size increases due to its efficiency in removing the key prefixes and locating the block and target key value. However, TridentKV throughput drops drastically due to creating large block sizes by increasing the key size. The performance gap between \texttt{DobLIX} and RocksDB narrows with increasing size, as larger key-value (KV) loads impact both methods.
Fig.~\hyperref[fig:eval-kv-size]{\ref*{fig:eval-kv-size}b} shows the impact of various fixed value sizes on throughput. The results show that \texttt{DobLIX} has the highest throughput up to a value of $1KB$. For the value size $4KB$, Bourbon works better as it disaggregates keys and values, which is efficient for large value sizes.


\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{figs/eval_kv_size.pdf}}
  \vspace{-2em}
  \caption{\small{Impact of various key-value sizes.}}
  \label{fig:eval-kv-size}
  \vspace{-0.2em}
\end{figure}

\subsubsection{\textbf{Variable-sized key-values}}
In this section, we conduct practical experiments utilizing variable KV sizes. We used the mean and standard deviations of the KVs of three RocksDB applications (see \S~\ref{sec:var-kv-lsm}). As none of the previously established index methods can manage variable KVs, our comparison is exclusively with RocksDB. Fig.~\hyperref[fig:eval-kv-size]{\ref*{fig:eval-kv-size}c} shows the experimental results for these KV sizes. It illustrates that \texttt{DobLIX} functions effectively with variable key-value sizes and achieves $1.32\times$, $1.38\times$, and $1.62\times$ in key-value sizes such as UDB, ZippyDB, and UP2X.

\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{figs/eval_mem.pdf}}
  \vspace{-2em}
  \caption{\small{Average index size and SSTable creation time.}}
  \label{fig:eval-mem-construction}
  \vspace{-2em}
\end{figure}

\subsection{Storage Footprint}
\label{sec:eval:storage}

\label{sec:storage}
Fig.~\hyperref[fig:eval-mem-construction]{\ref*{fig:eval-mem-construction}a} shows the index size for the AMZN and LOGN datasets with $8$- and $64$-byte key sizes. We omit Bourbon as it only works with $8$-byte numerical keys. The index block for each method will be written to storage beside the actual key values, hence index size has a direct relation with space and write amplification. The figure shows that in $8$-byte keys, the index size of \texttt{DobLIX} and RocksDB has a negligible difference, while both are less than TridentKV. However, for $64$-byte keys, \texttt{DobLIX} improves indexing by $25.9\%$ on average. This is because \texttt{DobLIX} removes the longest common prefix from each cluster of keys on each node (see \S~\ref{sec:design:lastmile}), and each node only keeps $8$-byte numerical data. This shows that \texttt{DobLIX} has the lowest index size, and since it does not add any other data except the serialized LI model, it has the lowest write and space amplification compared to other methods.

\subsection{Compaction Time}
In \texttt{DobLIX} and other baselines, SSTables are constructed in the background flush and compaction processes~\cite{yu2024caas}. Fig.~\hyperref[fig:eval-mem-construction]{\ref*{fig:eval-mem-construction}b} demonstrates the average compaction time to build SSTables on the four different workloads and datasets mentioned in \S~\ref{sec:storage}.
The data indicate that the \texttt{DobLIX} LI model does not add overhead during the SSTable construction compared to RocksDB. Notably, with $64$-byte keys, \texttt{DobLIX} construction time is $5.9\%$ shorter than RocksDB, due to the smaller index size of \texttt{DobLIX}. Additionally, compared to TridentKV, \texttt{DobLIX} reduces construction time by $24.7\%$, attributed to TridentKV lack of prefix removal optimization in its string model. Specifically, \texttt{DobLIX} average SSTable construction time for $64$-byte keys is $6.2$ seconds, with model training taking $302$ ms ($4.8\%$ of the total time) and serialization and metadata writing taking $237$ ms ($3.7\%$ of the total time). In contrast, TridentKV model training time is $817$ ms, and serialization plus metadata writing time is $414$ ms. 


\subsection{Parameter Tuning}
\label{sec:eval:param}
\label{sec:exp:rl-agent-tuning}


In this section, we present the results of the RL agent for parameter tuning and analyze the impact of parameters on throughput and total index size. The evaluation uses the AMZN dataset under two read-heavy workloads: (1) point-query with a Zipfian distribution, and (2) range queries over $100$ consecutive KV pairs with a uniform distribution. We also vary the $\nu$ parameter in the RL reward function (\S\ref{sec:design:agent}) between $0$ and $1$ to evaluate trade-offs between index storage footprint and throughput optimization. The left side of Fig.~\hyperref[fig:eval-rl]{\ref*{fig:eval-rl}} illustrates throughput between two RL episodes (each episode is executed after $20$ SST creation) and the estimated total index size after each episode. The index size is estimated using a linear approximation based on current keys added to a total of $64$ million keys in the dataset. The right side of the figure displays the RL agent Q-table rewards, highlighting the best parameter configurations for each workload.

\noindent
\textbf{Throughput Optimization.} For point queries (Fig.~\hyperref[fig:eval-rl]{\ref*{fig:eval-rl}a}), the throughput increases from $45K$ to $93K$ ops/sec after $50$ episodes. This improvement is achieved by tuning parameters to prioritize throughput at the cost of increasing storage size. The heatmap shows that smaller block sizes yield higher rewards because they reduce read amplification, minimizing the size of the retrieved block per query. Among the index models, PLA with a maximum error of $128$ achieves the best performance. A smaller error increases the block count, while a larger error (i.e., $256$) raises the cost of intra-block last-mile searches.

For range queries (Fig.~\hyperref[fig:eval-rl]{\ref*{fig:eval-rl}c}), parameter rewards are less sensitive compared to point queries since range query performance is dominated by sequential KV iteration. However, a block size of $16KB$ paired with the PRA model achieves the highest reward. This size strikes a balance, being more efficient than $8KB$ since the data being read is more than $8KB$, so reading $16KB$ blocks lowers the amount of metadata reads. Additionally, the PRA model outperforms PLA with different error ranges at the $16KB$ block size, as this specific setting has a characteristic similar to Fig.~\hyperref[fig:compare-methods]{\ref*{fig:compare-methods}b} where $E' < E$.

\noindent
\textbf{Storage Optimization.} Fig.\hyperref[fig:eval-rl]{\ref*{fig:eval-rl}b} and Fig.\hyperref[fig:eval-rl]{\ref*{fig:eval-rl}d} show results when the RL agent prioritizes minimizing the index size. The heatmaps indicate that the trends for both workloads are similar, as the stored data index is identical. Larger block sizes consistently reduce the index size by decreasing the number of blocks and associated metadata. Among the index models, PLA with a maximum error of $256$ achieves the highest reward by reducing the number of splines, thus minimizing the index overhead. In both workloads, the chosen parameters achieve an estimated index size of less than $500MB$, which is half of the index size when the optimization is on the throughput.

\noindent
\textbf{DobLIX Limitation.} This experiment on the RL agent shows that DobLIX requires at least $50$ RL episodes to find near-optimal parameters that satisfy the throughput and storage goals. One possible future work is to reduce the cold start caused by the RL agent and speed up the exploration time.



% This section begins by examining the impact of the approximation error $E$ and the block size $B_{max}$ on \texttt{DobLIX} throughput and total index size with point and range query workloads. Following this, we present the outcomes of the tuning agent parameter optimization to maximize the rewards. All experiments discussed here use the AMZN dataset.

\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=0.9\columnwidth]{figs/eval_rl.pdf}}
  \vspace{-2em}
  \caption{\small{RL agent parameter tuning. Each row shows two plots for one specific workload and reward parameter, The right plot shows the improvement in the throughput and estimated index size during the workload. The left plot shows the reward heatmap at the end of the experiment. Darker colors have higher rewards.}}
  \label{fig:eval-rl}
  \vspace{-1.5em}
\end{figure}

% \subsubsection{\textbf{Error Bound Impact}}
% The error limit in an LI defines the maximum range for the last-mile search. (see \S~\ref{sec:li-storage}) Increasing the error bound initially extends the last-mile search within a block, which negatively affects throughput. However, it subsequently reduces the inference time by creating fewer spline points to search for blocks, positively impacting throughput. Optimal performance requires careful tuning of this parameter. Fig.~\hyperref[fig:eval-params]{\ref*{fig:eval-params}a} shows the impact of different error bounds on \texttt{DobLIX} throughput and total index storage footprint. It shows that the highest throughput for point queries is achieved with a maximum error of $64$, which is $96.9~ops/sec$, and is $14.1\%$ higher compared to when we set this parameter to $8$. For the range queries the difference in throughput is low with the best throughput for 128 error. The figure also shows that increasing the error bound always decreases the index size, this is because the LI model needs to store fewer data for the learned data distribution.

% \subsubsection{\textbf{Block Size Impact}}
% Max block size directly impacts the loading block in the data access part (see `L-block' in Fig.~\ref{fig:eval-breakup}). Fig.~\hyperref[fig:eval-params]{\ref*{fig:eval-params}b} shows that the impact of increasing the maximum block size for the point query is significant. The block with $8KB$ has $1.22\times$ less throughput than the $4KB$ blocks, and the reduction in throughput is $3.19\times$ for the $32KB$ blocks. For range queries, however, the difference is negligible with the best throughput in the 16KB block sizes.  In addition, increasing the block size reduces the index size, as \texttt{DobLIX} has fewer blocks to learn in its model. The index storage for 4KB is 969MB while for the 32KB is 496MB. 

% \subsubsection{\textbf{RL Agent Tuning}}
% \label{sec:exp:rl-agent-tuning}
% As we mentioned in \S~\ref{sec:parameters}, in all results, we measured the system on a pre-trained RL agent to make the results comparable. As a limitation of our system, we know the RL algorithm takes time to explore the system and find the optimum state. In this section, we study the behavior of the RL agent.  
% Fig.~\ref{fig:eval-reward} illustrates the average reward values of the RL agent for each state as observed in the experiment. The results are normalized relative to the highest reward value. The PRA method results are plotted on the same axis as the PLA method, but with varying errors, since these parameter sets can only be combined with different block sizes, not with each other. The data show that the PLA method produces the highest reward with a maximum error of $64$ and a maximum block size of $4KB$. Increasing the block size results in reduced rewards due to higher data access latency. The optimal maximum error size per block ranges between $64$ and $256$.

% \vspace{1pt}
% \noindent
% {\small\textbf{PLA vs. PRA.}}
% When comparing the PLA and PRA methods, $PLA64$ and $PLA128$ have higher rewards for blocks of $4KB$, $8KB$, and $16KB$, while PRA has higher rewards for a $32KB$ block. This is because the PRA method improves learning within each block, rather than seeking blocks, and increasing the block size reduces the number of blocks and increases the number of KVs within the block. This makes PRA lookup faster than PLA. However, in the optimal block size of $4KB$, PRA has a $12\%$ lower reward value due to the longer time it takes PRA to seek the target block.


% \begin{figure}[t]
%   \centering
%   \makebox[0.9\columnwidth][c]{\includegraphics[width=0.9\columnwidth]{figs/eval_reward.pdf}}
%   \vspace{-1em}
%   \caption{\textbf{Tuning agent normalized reward values.}}
%   \label{fig:eval-reward}
%   \vspace{-2.4em}
% \end{figure}


