\section{Introduction}
\label{sec:intro}
\noindent
\textbf{Context.}  key-value (KV) databases are crucial in various domains like cloud computing, e-commerce, big data analysis, and artificial intelligence. Among different KV store architectures, Log-Structured Merge Trees (LSM-trees) stand out for their exceptional write performance~\cite{CliffGuard}. They are widely used in industrial applications, such as RocksDB~\cite{rocksdbpaper}, Cassandra~\cite{lakshman2010cassandra}, LevelDB~\cite{leveldb}, and BigTable~\cite{chang2008bigtable}. 

The multi-level structure of LSM-trees results in significant drops in read performance due to high read amplification~\cite{TridentKV2022, heidari2150metahive}. The levels are divided into Sorted String Tables (SSTables) that contain KV pairs ordered by keys, with SSTables comprising fixed-size blocks (ranging from $4KB$ to $32KB$), and retrieving a specific KV pair involves an index lookup to locate the relevant level, file, block, and pair, followed by a data-access phase to load the block and fetch the desired pair. In current KV stores, with modern NVMe storage, both index lookup and data access contribute significantly to query latency (Fig.~\hyperref[fig:rocksdb-lookup]{\ref*{fig:rocksdb-lookup}a}).

A common strategy to improve index lookup involves using Machine Learning (ML) as a data structure, effectively revolutionizing data indexing by leveraging predictive capabilities and efficiently capturing records distribution patterns~\cite{heidari2024uplif,ding2020alex, ferragina2020pgm, galakatos2019fiting, heidari2024record,heidari2019holodetect}. This is known as a learned index (LI). These LI models usually reside inside memory and estimate the probable range containing a specific record by evaluating the target key through a monotonic function approximated by a cumulative distribution function (CDF)~\cite{heidari2020sampling, livshits2020approximate, case2023memory}. However, when KV pairs are stored in storage, the LI bottleneck emerges during the data retrieval process from storage to memory~\cite{lidisk2024sigmod, wipe22024, apex2021, liDisk2024}
($\S~\ref{sec:li-storage})$. Consequently, to overall enhance performance, it is essential to take into account these effective factors. 

\begin{figure}[t]
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{figs/rocksdb_indexing.pdf}}
  \vspace{-2em}
  \caption{\small{Lookup Latency Breakdown. Read performance on $10$ million $8$-byte KVs of the Wiki dataset using the native RocksDB index.}}
  \label{fig:rocksdb-lookup}
  \vspace{-2em}
\end{figure}
\vspace{3pt}
\noindent
\textbf{Current LSM Learned Index Research.}
The potential enhancement of LSM-tree lookup performance through LI integration has been a focal point in recent studies such as \textit{TridentKV}~\cite{TridentKV2022}, \textit{Bourbon}~\cite{Bourbon2020}, \textit{LeaderKV}~\cite{leaderkv2024}, and \textit{LearnedKV}~\cite{wang2024learnedkv}. These initiatives have introduced customized LI solutions for LSM-tree structures, showing performance gains in index~lookup efficiency. A significant oversight in these endeavors is the disregard for data~access overhead, especially the time-consuming retrieval of blocks from storage to memory, as shown in Fig.~\hyperref[fig:rocksdb-lookup]{\ref*{fig:rocksdb-lookup}b}, which leads to a dilemma depicted in Fig.~\ref{fig:prev-designs} when previous methodologies neglect this characteristic:
\textit{(1)} adhering to fixed block sizes optimized for read operations, which can result in inaccuracies in block lookup and require accessing multiple blocks during read processes. Solutions \textit{a} and \textit{b}, as depicted, face this challenge, a methodology also used by \textit{Bourbon} and \textit{LearnedKV};
\textit{(2)} Opting for variable block sizes, which could lead to significantly larger blocks, thus increasing block I/O time. Solution \textit{c} exhibits this issue, a methodology also adopted by \textit{TridentKV} and \textit{LeaderKV}. These dilemmas arise from the isolated optimization of LI models without considering the data access component. Consequently, while these techniques reduce average latency compared to LSM tree-based KV store index lookups, they show higher tail-latency in cases requiring multiple or large block retrievals. This paper proposes a new LI solution to optimize both the indexing and data access phases.

Furthermore, earlier studies have neglected the consideration of varying key lengths in their design (\S~\ref{sec:var-kv-lsm}), as well as the enhancement of the last mile search, which is the final phase to identify the target KV within the retrieved block. As shown in Fig.~\hyperref[fig:rocksdb-lookup]{\ref*{fig:rocksdb-lookup}c}, this step in the indexing process incurs nearly identical latency to block finding. Therefore, an LI methodology should incorporate optimizations for this critical step.



\begin{figure}
  \centering
  \makebox[\columnwidth][c]{\includegraphics[width=\columnwidth]{figs/stochastic_interpretation.pdf}}
  \caption{\small{Comparison of LI Solutions on SSTables. Considering Block Sizes $Par_{Block}$ and the LI $I_{IndexBlock}$ as stochastic variables. \textbf{a)} Small fixed-size blocks, \textbf{b)} Large fixed-size blocks with a guarantee on max block size. \textbf{c)} Variable block size with a model output ($M'(k)$) guarantee to load one block. \textbf{d)} Perfect solution with guarantees on model output ($M(k)$) for one block access with optimized block size.}}
  \label{fig:prev-designs}
  \vspace{-1.7em}
\end{figure}

\vspace{3pt}
\noindent\textbf{Our Approach.} We leverage multi-objective optimization techniques to incorporate data access overhead as an additional factor to design an LI framework~\cite{boyd2004convex}. To achieve a suitable configuration that optimizes index performance alongside this additional parameter, we employ an auto-tuning method based on reinforcement learning (RL). This enables us to develop configurations that generate index models capable of automatically adjusting when workloads change. Our approach specifically concentrates on constructing an index model for KVs at the LSM file level, allowing for \textbf{effective locating} of the desired \textbf{variable-size} KVs using merely \textbf{a single block} access with \textbf{ideal block size}, all while maintaining the LI model optimal performance.

We evaluate our framework using an array of real-world and synthetic datasets and workloads, showing a substantial reduction in indexing costs when measured against state-of-the-art indexing solutions. We show that our framework leads to better performance, with throughput improvements between $1.19\times$ and $2.21\times$ in various datasets and workloads.

\noindent\textbf{Technical Challenges.} Our approach needs to address multiple technical challenges:  

\begin{itemize}[noitemsep, leftmargin=*]
    \item \textit{\textbf{Index Model.}} Considering a secondary parameter makes the learning of index modeling challenging. The model should consider both parameters during the optimization process, while depending on the type of the second parameter, the cost feedback might come asynchronously.
    \item  \textit{\textbf{Restoration.}} During LSM tree query processing, data spans storage layers with varying performance. Missing metadata (e.g., the \say{index trajectory}) can disrupt queries and require costly recovery. To avoid this, index system must preserve critical metadata to efficiently handle gaps and ensure smooth query execution.
    \item  \textit{\textbf{Adaptation.}} In an index framework, the parameters of the system can change due to the behavior of the incoming data or workload shifts. The design must adapt to these changes to ensure efficient performance under varying conditions.
\end{itemize}

\noindent Implementing the framework within an LSM-tree-based KV store and conducting comprehensive benchmarking present challenges, primarily due to the incompatibility of many LI models with LSM trees, as well as the inadequacies of current benchmarks (e.g., YCSB) in capturing important edge cases.

\vspace{3pt}
\noindent
\textbf{Contributions.} We present the following contributions in this paper:
\begin{enumerate}[noitemsep, leftmargin=*]
    \item \textbf{\textit{Proposing \texttt{DobLIX}, a \underline{D}ual-\underline{ob}jective \underline{L}earned \underline{I}nde\underline{X} framework}}, for LSM tree-based KV stores ($\S~\ref{sec:design:overview}$). This framework optimizes the performance of the LI lookup while considering any additional secondary objective parameter using two innovative LI approximation models, PLA and PRA ($\S~\ref{sec:design:li}$). In this work, our design specifically optimizes indexing and data access as a secondary parameter.
    \item \textbf{\textit{Optimizing the last-mile search phase}} of the lookup process by incorporating the LI model traversal into the last mile search process ($\S~\ref{sec:design:lastmile}$).
    \item \textbf{\textit{Introducing an RL-based agent}} to dynamically adjust the indexing and data partitioning parameters in our system and to choose between the PLA and PRA methods ($\S~\ref{sec:design:agent}$).
    \item \textbf{\textit{Implementing our method on RocksDB}} and performing a comprehensive benchmark to demonstrate its superior performance compared to traditional and LI solutions ($\S~\ref{sec:eval}$).
\end{enumerate}

\noindent\textbf{Paper Organization.} The remainder of the paper proceeds as follows: In \S~\ref{sec:background}, we review the background concepts. \S~\ref{sec:doblix-design} provides a detailed overview of our LI framework design and RL-agent. In \S~\ref{sec:eval}, we evaluate our proposed solutions. We discuss related work in \S~\ref{sec:related-work} and summarize key points of the paper in \S~\ref{sec:conclusion}. 