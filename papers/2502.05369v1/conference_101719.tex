\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{graphicx} 
\usepackage{tikz}
\usepackage{pdfx}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{cite}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{balance}  %
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\usepackage{multirow}
\usepackage{color}
\usepackage{listings}
\usepackage{float}
\usepackage{outlines}
\usepackage[normalem]{ulem}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{cuted, lipsum}
\usepackage[listings,skins,breakable]{tcolorbox}
\usepackage{booktabs}
\PassOptionsToPackage{rgb,hyperref,table}{xcolor}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{marginnote}
\usepackage{lineno}
\usepackage{dirtytalk}

\begin{document}
\SetAlgoNlRelativeSize{-1}
\SetNlSty{textbf}{}{}
\LinesNumbered
\title{\texttt{DobLIX}: A Dual-Objective Learned Index for Log-Structured Merge Trees}


\author{\IEEEauthorblockN{1\textsuperscript{st} Alireza Heidari}
\IEEEauthorblockA{\textit{Huawei} \\
alireza.heidarikhazaei@huawei.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Amirhossein Ahmadi}
\IEEEauthorblockA{\textit{Huawei} \\
amirhossein.ahmadi@huawei.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Wei Zhang
}
\IEEEauthorblockA{\textit{Huawei} \\
wei.zhang6@huawei.com}
}

\maketitle

\begin{abstract}
In this paper, we introduce \texttt{DobLIX}, a dual-objective learned index specifically designed for Log-Structured Merge (LSM) tree-based key-value stores. Although traditional learned indexes focus exclusively on optimizing index lookups, they often overlook the impact of data access from storage, resulting in performance bottlenecks. DobLIX addresses this by incorporating a second objective, data access optimization, into the learned index training process. This dual-objective approach ensures that both index lookup efficiency and data access costs are minimized, leading to significant improvements in read performance while maintaining write efficiency in real-world LSM-tree systems. Additionally, \texttt{DobLIX} features a reinforcement learning agent that dynamically tunes the system parameters, allowing it to adapt to varying workloads in real-time. Experimental results using real-world datasets demonstrate that \texttt{DobLIX} reduces indexing overhead and improves throughput by $1.19\times$ to $2.21\times$ compared to state-of-the-art methods within RocksDB, a widely used LSM-tree-based storage engine.
\end{abstract}

\input{sections/introduction}
\input{sections/background}

\section{\texttt{DobLIX} Design}
\label{sec:doblix-design}

In this section, we describe how \texttt{DobLIX} is designed to speed up lookup queries. We first dive into the general architecture and core concepts of \texttt{DobLIX} (\S~\ref{sec:design:arch} and \S~\ref{sec:design:overview}). To align index modeling with dual objectives, \texttt{DobLIX} utilizes two LI approaches: it modifies the existing Piecewise Linear Approximation (PLA) method~\cite{kipf2020radixspline} by adjusting the spline operations to incorporate new objective functions, and introduces a novel indexing strategy called Piecewise Regression Approximation (PRA), which improves performance by effectively managing modeling errors (\S~\ref{sec:design:li}). \texttt{DobLIX} incorporates a string-compatible LI solution capable of handling variable-size KVs. To optimize the last-mile search process, it transfers model knowledge to narrow the search range and simplifies key comparisons by focusing solely on a limited part of the key bits decodable as an integer (\S~\ref{sec:design:lastmile}). Furthermore, it utilizes a reinforcement learning (RL) tuning agent to dynamically determine optimal parameters, such as the maximum approximation error of the allowed model and block size, and to select between the PLA and PRA algorithms (\S~\ref{sec:design:agent}).


\begin{figure}[t]
  \centering
  \makebox{\includegraphics[width=\columnwidth]{figs/overview.pdf}}
  \vspace{-1.7em}
  \caption{\small{\texttt{DobLIX} Architecture.}}
  \label{fig:DobLIX-arch}
  \vspace{-1.5em}
\end{figure}

\subsection{Overall Architecture}
\label{sec:design:arch}
Fig.~\ref{fig:DobLIX-arch} outlines the general architecture of \texttt{DobLIX}. This solution concentrates on learning the index at the SSTable level in detail. SSTables are preferred for LIs due to their unchanging nature, which removes the need for updates during their lifespan. Upon the formation of each SSTable, DobLIX trains an LI model based on its KVs. This model is crafted to accurately pinpoint the target block for all KVs within a specified error margin while ensuring that block sizes remain within the designated maximum limit. Consequently, \texttt{DobLIX} has an LI model and block partitioning, as illustrated in Fig.~\hyperref[fig:prev-designs]{\ref*{fig:prev-designs}d}. Subsequently, \texttt{DobLIX} serializes the LI model and deposits it in the index block within the SSTable metadata.


\vspace{1pt}
\noindent
{\small\textbf{Lookup Process.}}
In Fig.~\ref{fig:DobLIX-arch}, the stages involved in a \texttt{DobLIX} lookup query are outlined. \circled{1} Initially, it inspects the current MemTables; if the desired key is absent there, it looks through the unalterable MemTables. \circled{2} It then scrutinizes various levels of LSM-trees and \circled{3} loads the SSTable that may cover the target key within its range. \circled{4} \texttt{DobLIX} loads the LI model from the SSTable metadata into memory. \circled{5} It performs a search within its Trie tree to locate the node that houses the ultimate LI (\S~\ref{sec:li-on-strings}) and \circled{6} uses the trained CDF model within that node to \circled{7} determine the \textbf{exact block number} that contains the key and \textbf{narrows down the search scope} for the last-mile search in the block using the LI model. \circled{8} Following this, \texttt{DobLIX} loads the block from storage in memory and \circled{9} executes the final search within the specified range in $KVs~Adrr$ stored in the blocks' metadata to find the exact offset of the target KV pair in the $KVs~Data$ (\S~\ref{sec:design:lastmile}), and \circled{10} employs the retrieved address on the $KVs~Data$ to locate the actual KV. \circled{11} Finally, \texttt{DobLIX} measures the \textit{latency} of the current lookup query alongside the \textit{index size}, incorporating these measurements as feedback to refine the tuning agent (\S~\ref{sec:design:agent}).

\subsection{Concept Overview}
\label{sec:design:overview}
The management of LSM-tree data involves data partitioning and indexing phases (\S~\ref{sec:rocksdb}), and as we established earlier, any optimization strategy, especially those involving LIs, should improve overall performance. As depicted in Fig.~\ref{fig:prev-designs}, block partitioning is intertwined with block indexing ($Par_{Block} \not\perp I_{IndexBlock}$). Therefore, optimizing $I_{IndexBlock}$ requires the consideration of $Par_{Block}$. In contrast, previous designs illustrated in Fig.~\hyperref[fig:prev-designs]{\ref*{fig:prev-designs}\{a,b,c\}} from earlier research have significant data access expenses due to the independence between the LI model and the data partitioning component.

\noindent As described in \S~\ref{sec:li-storage}, within the traditional framework of LI modeling, $I(.)$ represents the result of approximating keys indexes drawn from an unknown distribution $\mathcal{D}_{keys}$ through practical optimization. Therefore, the classical design of the LI does not consider data access and the result coordinated by all data; however, in LSM-trees only a limited number of SSTable blocks reside in memory. In addition, the primary optimization objective is to minimize the error within the hypothesis spaces chosen, regardless of any secondary objectives. 

\texttt{DobLIX} aims to redefine LI models by integrating efficient block-based data access as a key objective. Enhances indexing performance by ensuring that the trained model accurately maps queries to the correct block, enabling the retrieval of only a single block while adhering to the optimal block size. A critical aspect of \texttt{DobLIX} is the relationship between the index approximation ($I_{IndexBlock}$) and block partitioning ($Par_{Block}$), where a one-to-one correspondence is established between the index approximation and the segments within $Par_{Block}$. This allows \texttt{DobLIX} to apply LI models that partition the key domain into segments, ensuring each segment corresponds to a specific block. The system performs a binary search on an array of offsets ($I_{IndexBlock}$) to find the start of each segment (i.e. block). Within each segment, it uses an index approximation ($I_{KV}$) to efficiently locate the keys. Since the trained index is based on the entire data in SSTable, the index model used for each block requires adjustment~(\S~\ref{sec:design:lastmile}). This approach optimizes both block access and key retrieval, providing efficient indexing.

To achieve this, we introduce a dual-objective optimization approach for two distinct LI methodologies. The first method is based on the piecewise linear approximation (PLA) modeling~\cite{kipf2020radixspline}, while the second method employs the piecewise regression approximation (PRA), based on the recursive model index~\cite{kraska2018case}. We delve into these methods in \S~\ref{sec:design:pla} and \S~\ref{sec:design:pra}.

\subsection{LI Approximation Methods}
\label{sec:design:li}
In this section, we present our LI algorithms that focus on dual-objective optimization to train the index model. Considering the importance of I/O performance in indexing, these algorithms are designed to partition the KV space based on their sizes and progressively systematically construct the index. Typically, we approximate the index for each segment using linear models. When it comes to searching for specific points during lookup processes, we employ a binary search on the points derived from the piecewise approximation to precisely pinpoint the required location, which is referred to as last-mile search. In the following, we elaborate on these algorithms in detail.


\begin{figure}[t]
  \centering
  \makebox{\includegraphics[width=0.9\columnwidth]{figs/methods.pdf}}
  % \vspace{-1em}
  \caption{\small{LI Models. $B_i$s represent the actual blocks added to SSTables.}}
  \label{fig:methods}
  \vspace{-1em}
\end{figure}

\subsubsection{\textbf{Dual-Objective Optimization}}
\label{sec:dual-objective-optimization}
To address both the data access and index approximation goals, we used lexicographic optimization (\S~\ref{sec:lexi_opt}). As demonstrated by the motivational experiment in Fig.~\hyperref[fig:rocksdb-lookup]{\ref*{fig:rocksdb-lookup}a}, data access significantly influences latency performance. Although various methods exist to tackle multi-objective optimization problems, we aim to prioritize the data access parameter more heavily than index lookup. Thus, we always finalize a block when its size exceeds the maximum block size $B_{max}$ by incorporating an additional pair of key values. This ensures that the block sizes remain below $B_{max}$, even if the approximation error $E$ has not yet been achieved. Note that both the configuration values $B_{max}$ and $E$ are given by the \textit{Tuning Agent} (\S~\ref{sec:design:agent}). 

\subsubsection{\textbf{Piecewise Linear Approximation (PLA)}}
\label{sec:design:pla}

In this method, the key space is divided into blocks $B_i$ using a linear approximation (spline), each block containing keys that share a common prefix. For each block $B_i$, a spline estimate of the positions is made, defined as $M_i(x) = a_i + b_i(x - x_i)$, where $a_i$ and $b_i$ are coefficients derived from control points and $x_i$ is the initial key in block $B_i$. The binary search is then used around $M_i(k)$ to identify the precise index $I(k)$. For each block $B_i$, the next key is added to a new block ($B_{i+1}$) of the SSTable if (1) the approximation error $M_i(x)$ reaches the maximum threshold $E$, or (2) the condition $|B_i|\ge B_{max}$ indicates that including the new pair of KV would exceed the maximum block size allowed. This approximation process is illustrated in Fig.~\hyperref[fig:methods]{\ref*{fig:methods}a}. As described in \S~\ref{sec:dual-objective-optimization} when the size of an added point exceeds $B_{max}$, a new block is created to maintain the optimal I/O (data access) performance of the previous block, as indicated by the cross in Fig.~\hyperref[fig:methods]{\ref*{fig:methods}a}.
This mechanism results in a new set of spline points, introducing new blocks when the secondary optimization criterion is met, in addition to the standard blocks formed by reaching the maximum approximation error. Algorithm~\ref{alg:pla} showcases this approach, where $APE(line,set)$ calculates the maximum distance the points in $set$ can have from the given line $line$. In this context, $a_i$ is defined as $\mathcal{R}[i][0][1]$, $x_i$ corresponds to $\mathcal{R}[i][0][0]$, and $b_i$ is calculated as $\frac{\mathcal{R}[i+1][0][1] - \mathcal{R}[i][0][1]}{\mathcal{R}[i+1][0][0] - \mathcal{R}[i][0][0]}$, while the term $\text{offset}_i$ refers to $\mathcal{R}[i][1]$.


\begin{algorithm}[t]
\SetKwInput{KwResult}{Output}
\SetKwInput{KwIn}{Input}
\DontPrintSemicolon
\LinesNumbered
\SetAlgoNlRelativeSize{-1}
\caption{Dual-objective PLA}\label{alg:pla}
\small
\KwIn{Set of KVs $D$}
\KwResult{Radix Points $\mathcal{R}$}
$\mathcal{R}\gets [~], \quad index\gets 0, \quad \text{offset}\gets 0$ \;

$E,B_{max} \gets TuningAgent()$ \;

$B_{curr} \gets [(k_0,v_0)]$\;

\While{$(k,v) \in D$}{
    \If{$\vert B_{curr}\vert > B_{max}$}{
      $\mathcal{R} \gets \mathcal{R} + [B_{curr}.last]$\;
      
      $B_{curr} \gets [(k,index)]$\;
    }
  \eIf{$\vert B_{curr}\vert>1~\wedge~
  APE(\text{Line}(B_{curr}.\text{first},(k,index)), B_{curr})\ge~E$}{
      $\mathcal{R} \gets \mathcal{R}+[(B_{curr}.\text{last},\text{offset})]$ \;
      
      $\text{offset} \gets \text{offset} + |B_{curr}|$\;
      
      $B_{curr} \gets [(k,index)]$\;
  }{
  $B_{curr} \gets B_{curr} + [(k,index)]$\;
  }
  $index \gets index + 1$\;
}
$\mathcal{R} \gets \mathcal{R} + [(B_{curr}.\text{last},\text{offset})]$\;
\end{algorithm}


\subsubsection{\textbf{Piecewise Regression Approximation (PRA)}}
\label{sec:design:pra}

This approach involves initially dividing the key domain into segments of size $B_{max}$ and then approximating each segment linearly. The maximum approximation error for each segment, $E'$, is noted and utilized during the last-mile search phase (refer to Fig.~\hyperref[fig:methods]{\ref*{fig:methods}b}). Start scanning the KVs from the beginning; if incorporating the new KV into the existing segment exceeds $B_{max}$, start a new segment. Algorithm~\ref{alg:par} performs this task in one pass, maintaining the integrity of each KV pair.

After partitioning the data using the optimal block size $B_{max}$, as outlined in Algorithm~\ref{alg:pra}, a new model can be constructed for each partition through a linear approximation. The boundary points are retained for search tasks to determine the appropriate model for lookup queries, and $E'$ is stored in $\mathcal{E}$, respectively.

\begin{algorithm}[t]
\SetKwInput{KwResult}{Output}
\SetKwInput{KwIn}{Input}
\DontPrintSemicolon
\LinesNumbered
\SetAlgoNlRelativeSize{-1}
\caption{Partition~$Par_B(.)$}\label{alg:par}
\small
\KwIn{Set of KVs $D$,  Maximum Block Size $B$}
\KwResult{Partition $\mathcal{P}$}

$t,\mathcal{P} \gets []~~\&~~ \text{offset}\gets 0$ \;

\While{$KV \in D$}{
    \If{$\vert t\vert + \vert KV\vert > B$}{
        $\mathcal{P} \gets \mathcal{P} + [(t,\text{offset})]$\;
        
        $\text{offset}\gets\text{offset} + |t| ~\&~ t \gets []$ \;
    }
    $t \gets t + [KV]$\;
}
$\mathcal{P} \gets \mathcal{P} + [(t,\text{offset})]$\;
\end{algorithm}

\begin{algorithm}
\SetKwInput{KwResult}{Output}
\SetKwInput{KwIn}{Input}
\DontPrintSemicolon
\LinesNumbered
\SetAlgoNlRelativeSize{-1}
\caption{Dual-objective PRA}\label{alg:pra}
\small
\KwIn{Set of KVs $D$}
\KwResult{Radix Points $\mathcal{R}$\newline Model Sets $\mathcal{M}$\newline Maximum Segment Errors $\mathcal{E}$}
$\mathcal{R},\mathcal{M}, \mathcal{E} \gets []$ \;

$B_{max} \gets TuningAgent()$ \;

$\mathcal{P} \gets Par_{B_{max}}(D)$ \Comment{from Algorithm~\ref{alg:par}}\;

\While{$Partition~par \in \mathcal{P}$}{
\If{$\vert par\vert>1$ }{
    $M\gets LinearRegression(par)$\;
    
    $\mathcal{M} \gets \mathcal{M} + [M]$\;

    $\mathcal{E} \gets \mathcal{E} + [APE(M, par)]$\;
}
    $\mathcal{R} \gets \mathcal{R} + [par.frist]$\;
}
$\mathcal{R} \gets \mathcal{R} + [par.last]$\;
\end{algorithm}

\begin{figure}[t]
  \centering
  \makebox{\includegraphics[width=0.9\columnwidth]{figs/compare-methods.pdf}}
  \vspace{-0.5em}
  \caption{\small{Comparison of PLA and PRA under different data distributions. (a) $E' < E$, indicating PRA performs better. (b) $E < E'$, indicating PLA performs better.}}
  \label{fig:compare-methods}
  \vspace{-1.6em}
\end{figure}

\noindent\subsubsection{\textbf{Comparing PRA and PLA}} In the context of building block approximations, both the Piecewise Linear Approximation (PLA) and the Piecewise Regression Approximation (PRA) rely on the scanning of data, resulting in a linear time complexity of $O(N)$, where $N$ is the number of data points. Each method utilizes closed-form formulas with a time complexity of $O(N)$ for computations within blocks: PLA determines maximum distances, $APE(.,.)$, to construct piecewise linear segments, while PRA computes two-dimensional regression, $LinearRegression(.)$, formulas within blocks.

With respect to space complexity, PLA requires one point per spline segment to be stored since the endpoint of one line serves as the beginning of the next. The distribution of data points influences PLA memory needs; for instance, with a uniform distribution, all data might fit within a single spline (provided its size is smaller than $B_{max}$), thus minimizing memory usage. In contrast, PRA must store both a point and a slope for each regression line, roughly doubling the memory requirement compared to PLA. The complexity of the number of regressions is $O(\frac{N}{B_{max}})$. Consequently, the memory comparison between PLA and PRA depends on the characteristics of the data: PLA may involve fewer blocks and needs just one point per block (linear segment), whereas PRA has to retain two parameters per block.

When comparing the behavior of PRA and PLA during lookups (\circled{7} in Fig.~\ref{fig:DobLIX-arch}), determining which method is superior is challenging, often leading to the interchangeable use of algorithms; in particular, we consider two scenarios that contrast PLA and PRA, as illustrated in Fig.~\ref{fig:compare-methods}, noting that both scenario~(a) and scenario~(b) can occur depending on the sizes of the KV pairs and the distribution of the keys. In the PRA model, the block is written in persistent storage once its size reaches the maximum block size $B_{max}$, whereas the PLA operates under two conditions for flushing: when the approximation error exceeds the error limit $E$, or when the block size reaches $B_{max}$ (the same maximum block size used in PRA). Consequently, depending on the arrangement and distribution of KV, the spline achievable with PLA can potentially lead to a maximum error $E$ that may be higher or lower than the maximum error $E'$ in PRA. As shown in Fig.~\hyperref[fig:compare-methods]{\ref*{fig:compare-methods}a}, PRA can result in $E > E'$, indicating a more accurate approximation than PLA. This directly affects the scope of the last-mile search and the overall efficiency of each algorithm.

This implies that the behavior of KVs beyond the block boundary determined by the maximum error $E$ plays a crucial role. If data points outside the block constrained by $E$ align with the regression trend of the points within the block, the approximation remains accurate. However, there might be situations where data points outside the block behave significantly differently from those within the block. In such cases, as clearly shown in Fig.~\hyperref[fig:compare-methods]{\ref*{fig:compare-methods}b}, this could lead to a less accurate linear regression in PRA, resulting in $E < E'$. This means that PRA has a lower performance than PLA in such scenarios. In some rare cases, all elements in $\mathcal{E}$~(Algorithm~\ref{alg:pra}), denoted as $E'$ for each segment, are equivalent to $E$. In such scenarios, both algorithms exhibit identical last-mile search performance.

\subsection{Serialize and Deserialize Models}
\label{sec:design:serialization}
After training, the LI models are serialized and stored in a metadata block called the Meta Index Block (see Fig.~\ref{fig:sst-format}). This process involves traversing the model tree structure via depth-first search (DFS), serializing each node byte-by-byte. During deserialization, the model reconstructs the tree by determining the type of each node (leaf or internal) and populating the corresponding data structure.

The learned indices generally outperform the binary search in terms of time complexity. Therefore, the size of the set of stored nodes for model approximation is smaller than $O(\log N)$ when reading, while writing the entire dataset takes $O(N)$. As a result, the serialized model size is asymptotically negligible (see \S~\ref{sec:eval:storage}). In practice, RocksDB writes occur in the background flush and compaction process, and since the model is deserialized only once, overall read performance is significantly improved (see \S\ref{sec:eval:throughput}).

\subsection{Last-mile Search Optimization}
\label{sec:design:lastmile}

\begin{figure}[t]
  \centering
\makebox{\includegraphics[width=\columnwidth]{figs/last-mile-optimization.pdf}}
\vspace{-1.6em}
  \caption{\small{Last-mile Search Optimization Flow}}
  \label{fig:last-mile-optimization}
  \vspace{-1.5em}
\end{figure}

The final phase of the search process involves the last-mile search of the recovered block from storage (step \circled{9} in Fig.~\ref{fig:DobLIX-arch}). As illustrated in Fig.~\hyperref[fig:rocksdb-lookup]{\ref*{fig:rocksdb-lookup}c}, this step accounts for more than $40\%$ of the indexing latency in RocksDB. Fig.~\ref{fig:last-mile-optimization} shows how \texttt{DobLIX} optimizes its performance by restoring data from the LI model computation trajectory (step \circled{5} in Fig.~\ref{fig:DobLIX-arch}) to improve the last-mile search process. Initially, the target key is searched within the string LI structure, as explained in \S~\ref{sec:li-storage}. The LI model in \texttt{DobLIX} subsequently provides: \textit{(1)} $Block~b_{P_L^j}$: The block containing the target KV pair.
\textit{(2)} $M(.)$: The LI estimation of the target KV pair index in $KVs Addr$ (Fig.~\ref{fig:DobLIX-arch}).
\textit{(3)} $Level~L$: The level at which the target key was found in the LI model.
\textit{(4)} $Error~E_{P_L^j}$: The maximum range required to search for the target KV pair.

\vspace{3pt}
\noindent
{\small\textbf{Optimizing Search Range.}}
As described in \S~\ref{sec:design:overview}, \texttt{DobLIX} necessitates a coordinate transformation to adapt the model output ($M(.)$) for indexing on the retrieved block ($b_{P_L^j}$). This is achieved by deducting the count of keys in the previous blocks (maintained as the parameter \say{offset} in the metadata of the block introduced in Algs.~\ref{alg:pla}\&~\ref{alg:par}):

\vspace{-0.5em}
{\small
\[
    I_{KV}(.)=M_{adj}(.) = M(.) - b_{P_L^j}.\text{offset}
    \vspace{-0.5em}
\]
}

\noindent
To better illustrate the adjustment, we consider $SST_j$ in Fig.~\ref{fig:prev-designs}, in which the model is trained on the whole SSTable indexes. However, using the above adjustment, the coordination of model output for our solution (i.e., Fig.~\hyperref[fig:prev-designs]{\ref*{fig:prev-designs}d}) is transformed to the retrieved block $B_2$.

\noindent Subsequently, \texttt{DobLIX} performs a binary search within the specified model error range $E_{P_L^j}$ on $M_{adj}(.)$. This error range can be less than the maximum error $E$ if a spline in the PLA method reaches the maximum block size $B_{max}$. In such cases, \texttt{DobLIX} calculates the spline error and incorporates it into the model, transmitting this information to the last-mile search process to streamline the number of key comparisons.

\vspace{1pt}
\noindent
{\small\textbf{Optimizing String Comparison.}}
\texttt{DobLIX} further optimizes the comparisons by avoiding full key comparisons. \texttt{DobLIX} provides the node level where the key is found in its string-LI structure. This level in the tree indicates the common prefix string, $P_L^j$, in the key (Fig.~\ref{fig:last-mile-optimization}). Then \texttt{DobLIX} can ignore this common prefix, as all keys within the retrieved block share the same prefix. Additionally, \texttt{DobLIX} only compares string keys up to their $K$ bytes, ensuring that the key can be identified by comparing only the $K$ byte following the prefix within the error range. Given that $K$ generally consists of $8$ or $16$ bytes as explained in \S~\ref{sec:li-on-strings} for shifts from demanding string comparisons to numerical comparisons.

\subsection{Tuning Agent}
\label{sec:design:agent}
\texttt{DobLIX} employs Q-learning~\cite{watkins1992q}, a lightweight reinforcement learning (RL) algorithm~\cite{mo2023learning}, to dynamically fine-tune the LI and data access parameters. \texttt{DobLIX} determines three key parameters in the creation of SSTables and the training model: \textit{(1)} The choice between using the PLA vs. the PRA. \textit{(2)} The maximum error of the LI in the PLA method ($E$), and \textit{(3)} The maximum size of a block ($B_{max}$).

The state space for the Q-learning agent comprises the index model (PLA with error values ranging from $32$ to $256$, doubling at each step, and PRA, resulting in $5$ distinct values), and the block size ($4KB$ to $32KB$, doubling at each step, yielding $4$ distinct values, as recommended by RocksDB). This creates a total of $20$ possible states. The action space consists of five actions: incrementing, decrementing, or maintaining the current value of the model error and block size.

The RL agent policy for exploring the feasible space is also guided by our optimization method (\S~\ref{sec:dual-objective-optimization}). It prioritizes block size over the approximation error. The policy evaluates the relationship between data block size and its load latency, identifying the first block size that alters this relationship (e.g. $O(log N)$ to $O(N)$) as the upper bound. This effectively reduces the feasible space for exploration by limiting larger block sizes.

The reward function is defined below to consider both system latency and index storage:
\noindent
\scalebox{0.8}{%
\centering{
$R(s, a) = -\nu.Norm(AVG(latency)) - (1 - \nu).Norm(AVG(index~size)) $
}
}

\noindent This reward function is calculated as the negative sum of the normalized average latency and the normalized average of SSTables index size. We use sigmoid normalization to ensure that both latency and index size contribute to the reward on a comparable scale. The weighting parameter $\nu$ controls the relative importance of latency and index size in determining the overall reward (evaluated in \S~\ref{sec:eval:param}). In our experiments, we set $\nu$ as 1 to achieve the lowest latency; however, in some scenarios, storage may be more critical than can be set by lower values of $\nu$.

\begin{algorithm}[t]
\small  % Set font size
\DontPrintSemicolon  % Disable automatic semicolon

\caption{System Tuning Agent}\label{alg:tune}

$Q \gets \text{initializeQTable()}$ \;
$\alpha, \gamma, \epsilon \gets \text{initializeVariables()}$ \;
\While{}{
    Fetch state $s_{t-1}$ \;
    $L_{t}, I_{t} \gets \text{fetchAverageLatencyAndSSTableIndexSize()}$ \;
    $R_{t} \gets \text{calculateReward}(L_{t}, I_{t})$ \;
    Observe state $s_{t}$ \;
    $a' \gets \arg\max_{a \in \text{Actions}} Q(s_{t},a)$ \;
    $Q(s_{t-1}, a_{t-1}) \gets (1-\alpha)Q(s_{t-1}, a_{t-1}) + \alpha\big(R_{t} + \gamma Q(s_{t},a')\big)$ \;
    $A_t \gets \text{getAvailableActions}(s_t)$ \;
    \eIf{$\text{generateRandNumber()} < \epsilon$}{
        $a_t \gets \text{getRandomAction}(A_t)$ \;
    }{
        $a_t \gets \arg\max_{a \in A_t} Q(s_t,a)$ \;
    }
    tuneSystem($a_t$) \;
    $s_t \gets s_{t+1}$ \;
    $\epsilon \gets \text{updateEpsilon}(\epsilon)$ \;
}

\end{algorithm}

Algorithm~\ref{alg:tune} outlines the agent tuning and execution process. The learning procedure begins by configuring the RL hyperparameters: $\alpha$ denotes the learning rate, while $\gamma$ signifies the discount factor that balances immediate and future rewards. An epsilon decay strategy is used by initially setting a high exploration rate ($\epsilon$) to investigate various actions, gradually lowering this value in successive iterations to exploit the optimal actions.
At every $20$ SSTables creation, the following steps occur: First, the agent obtains the average latency of the reads that have reached this level, as well as the index size of the previously created SSTable. Then, it measures the reward and updates it for the chosen action in the previous state. The agent then gets the available actions at that state. Then, depending on the value of $\epsilon$, the agent explores a new action or chooses the best action from the Q-table. In addition, we implement a reset mechanism for the RL agent that reverts $\epsilon$ to its initial value if the distribution undergoes a significant change. This ensures optimal exploration of the space under the new conditions.


\input{sections/eval}
\input{sections/related-work}


{\footnotesize \bibliographystyle{acm}
\bibliography{sample}}

\end{document}
