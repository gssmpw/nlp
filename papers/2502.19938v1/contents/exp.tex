\section{Experiments} \label{sec:exp}

This section presents the results of experiments that compare the performance of FBBMM with baseline clustering algorithms on different datasets. The compared methods include $k$-means, MeanShift, DBSCAN, Agglomerative Clustering, GMM, and MBMM. The experiments were carried out on synthetic and real-world datasets, including a structural dataset and an image dataset.

\subsection{Experimental Setup}

We preprocess the data such that the value of each feature is normalized: let $\vx_n = [x_{n,1}, \ldots, x_{n,m}]$, each $x_{n,j}$ is normalized below.

\begin{equation}
x_{n,j} = 0.01 + \frac{(x_{n,j} - \min(x_{*,j}))(0.99 - 0.01)}{\max(x_{*,j}) - \min(x_{*,j})},
\end{equation}
where $x_{*,j} = [x_{1,j}, x_{2,j}, \ldots, x_{N,j}]$, i.e., the $j$th feature of all instances.

\subsection{Experiments on the Synthetic Datasets}

The synthetic datasets were generated using scikit-learn to test the characteristics of different clustering algorithms. These datasets consist of five different shapes. Each dataset includes 500 two-dimensional data points.

\begin{figure}[tb]
\centering
\includegraphics[width=.91\textwidth]{fig/synthetic.pdf}
\caption{Clustering results on synthetic datasets}
\label{fig:synthetic_results}
\end{figure}

Figure~\ref{fig:synthetic_results} compares the clustering results of $k$-means, MeanShift, DBSCAN, Agglomerative Clustering, GMM, MBMM, and FBBMM on five synthetic datasets. 

The first dataset includes concentric circles. If a point from the outer circle is selected, the most distant data point is positioned on the opposite side of the same circle. This characteristic makes the synthetic dataset highly challenging for centroid-based and distribution-based methods to group the entire outer circle into a single cluster. As shown in the first row of Figure~\ref{fig:synthetic_results}, density-based algorithms (DBSCAN) and Hierarchical clustering method (Agglomerative Clustering) and two beta distribution-based models (MBMM and FBBMM) successfully separate the two circles. 

The second dataset contains two distant 2D Gaussian distributions with small variances in each dimension, and the third distribution has a large variance, located in the middle. Thus, several data points sampled from the third distribution are mixed with the first two distributions. Since the middle cluster has a wider spread, the centroid is far from some points within the same cluster, making certain clustering algorithms, $k$-means, MeanShift, and DBSCAN, misidentify some data points in the middle cluster as other clusters; details are in the second row of Figure~\ref{fig:synthetic_results}.

The third and fourth datasets each comprise three 2D Gaussian distributions with isolated means. However, the two covariates are highly correlated: the covariates are negatively correlated for the third dataset and positively correlated for the fourth. As a result, data points are sometimes closer to those generated from other distributions. Thus, $k$-means, MeanShift, DBSCAN, and Agglomerative Clustering make errors on some data points. MBMM only handles data points whose covariates are positively correlated~\cite{hsu2024multivariate}. FBBMM and GMM are the only models that handle the two datasets well, as presented in the third and fourth rows of Figure~\ref{fig:synthetic_results}.

Finally, the last dataset includes data points from three 2D Gaussian distributions with distant means and small variances in each dimension. Therefore, a data point is close to other data points within the same Gaussian distribution but far from others. All clustering algorithms perform well in this ideal case.

Overall, our proposed FBBMM performs well on all synthetic datasets.

% \subsection{The fitted PDF of FBBMM on Synthetic Datasets}

% Figure~\ref{fig:parameter_comparison} shows the final PDFs for the fitted FBBMM on the synthetic datasets. The first column displays FBBMM's clustering results, and the second column shows the PDF based on the learned parameters.

% \begin{figure}[tb]
% \centering
% \includegraphics[width=.6\columnwidth]{fig/final_param.pdf}
% \caption{The PDF of FBBMM based on the learned parameters on synthetic datasets}
% \label{fig:parameter_comparison}
% \end{figure}

\subsection{Experimented the Open Datasets}

The open datasets include the wine dataset~\cite{aeberhard1994comparative} and the MNIST dataset~\cite{lecun1998gradient}. The wine dataset contains chemical analysis results of wines grown in the same region of Italy but derived from three different cultivars. There are 178 instances with 13 features. The second dataset, the MNIST dataset, comprises 70,000 grayscale images of handwritten digits (0-9), with each image having 28x28 pixels. The two datasets represent structural data and image data, respectively.

\subsection{Evaluation Metrics for Open Datasets}

We evaluate clustering results using three metrics: Clustering Accuracy (CA), Adjusted Rand Index (ARI), and Adjusted Mutual Information (AMI).

Clustering Accuracy is calculated as the number of correctly clustered data points divided by the total number of data points. Since clustering results and actual labels may not directly correspond, a mapping is performed before computing accuracy. For example, assume that we have a dataset with four data points whose labels are $[a,a,b,b]$, and a clustering algorithm produces the output with cluster IDs $[b,b,a,a]$. Despite the mismatch between the cluster IDs and the actual labels, the clustering is perfect because all points with the actual label $a$ are grouped in cluster b and vise versa. We call a set of lists the \emph{identical lists} if one list can be transformed into another list by permuting the labels. Thus, clustering accuracy is defined as the maximum accuracy among all identical lists of predicted cluster IDs~\cite{chen2023toward}.

\begin{equation} \label{eq:clusteracc}
CA\left(\vy, \bm{\hat{y}}\right) := \max_{\forall {\bm{\check{y}}} \in P(\bm{\hat{y}})} \left\{\frac{1}{N}\sum_{i=1}^N I\left(\check{y}_i = y_i\right)\right\},
\end{equation}
where $\vy = [y_1, \ldots, y_N]$ is the list of ground-truth labels, $\bm{\hat{y}} = [\hat{y}_1, \ldots, \hat{y}_N]$ is a list of predicted cluster IDs, $P(\bm{\hat{y}})$ returns a set of all identical lists for $\bm{\hat{y}}$, $I()$ is an indicator function, and $\bm{\check{y}} = [\check{y}_1, \ldots, \check{y}_N]$ is an identical list of $\bm{\hat{y}}$.

We also use the Adjusted Rand Index (ARI) and  Adjusted Mutual Information (AMI) for evaluation. ARI and AMI are biased toward different types of
clustering results: ARI prefers balanced partitions (clusters with similar sizes),
and AMI prefers unbalanced partitions~\cite{romano2016adjusting,chen2023toward}.

% We also use the Adjusted Rand Index (ARI) to measure the similarity between two clustering results. It is defined as:

% \begin{equation}
% ARI(\vy,\bm{\hat{y}}) = \frac{RI(\vy,\bm{\hat{y}}) - \mathbb{E}[RI]}{1 - \mathbb{E}[RI(\vy,\bm{\hat{y}})]},
% \end{equation}
% where $RI(\vy,\bm{\hat{y}})$ is the Rand Index between $\vy$ and $\bm{\hat{y}}$ and $\mathbb{E}[RI]$ is the Rand Index for expected value for random clustering. ARI values are upper bounded by 1. 
% %Its lower bound can be negative, but the exact lower bound is complicated~\cite{chacon2023minimum}. 
% A higher ARI value indicates better clustering.

% We also employ the Adjusted Mutual Information (AMI) for evaluating the agreement between two clustering results. AMI is defined below.

% \begin{equation}
% AMI(\vy,\bm{\hat{y}}) = \frac{MI(\vy,\bm{\hat{y}}) - \mathbb{E}[MI]}{\frac{1}{2}(H(\vy) + H(\bm{\hat{y}})) - \mathbb{E}[MI]},
% \end{equation}
% where $MI(\vy,\bm{\hat{y}})$ is the Mutual Information between $\vy$ and $\bm{\hat{y}}$, $H$ is entropy, and $\mathbb{E}[MI]$ is the expected mutual information of random clusterings.

% We use CA, ARI, and AMI to evaluate clustering algorithms because these metrics capture different aspects of clustering quality. Clustering accuracy measures the proportion of correctly clustered data points by matching the predicted clusters to the true labels. ARI and AMI are biased toward different types of clustering results: ARI
% prefers balanced partitions (clusters with similar sizes), and AMI prefers unbalanced partitions~\cite{romano2016adjusting,chen2023toward}. %For a fair comparison, we report both metrics.

\subsection{Results on the Open Datasets}

%We compare the clustering methods on a structured dataset (wine dataset) and an image dataset (MNIST dataset).

Table~\ref{tab:wine_original} compares the clustering performance of FBBMM with six baseline clustering methods on the wine dataset. Since each of the six baseline methods can handle datasets with any number of features, we use the entire 13 features provided in the wine dataset. However, because FBBMM only handles datasets with bivariate variables, we use an autoencoder to reduce the original feature to two dimensions. As shown, FBBMM outperforms all baseline clustering methods.

\begin{table}[tb]
\centering
\caption{Clustering on Original Features (13 Dimensions) and FBBMM (2 Dimensions)}
\label{tab:wine_original}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{CA} & \textbf{ARI} & \textbf{AMI} \\ \midrule
k-means & 0.702 & 0.371 & 0.423 \\
MeanShift & 0.697 & 0.469 & 0.483 \\
DBSCAN & 0.506 & 0.297 & 0.380 \\
Agglomerative Clustering & 0.674 & 0.371 & 0.436 \\
GMM & 0.725 & 0.435 & 0.436 \\
MBMM & 0.719 & 0.391 & 0.389 \\
FBBMM (2D) & \textbf{0.983} & \textbf{0.947} & \textbf{0.927} \\ \bottomrule
\end{tabular}
\end{table}

We also project the dataset from the original 13-dimensional to 2-dimensional dataset using an autoencoder and apply baseline clustering algorithms on the 2-dimensional dataset. In doing so, we ensure a consistent evaluation environment that isolates the effects of the dimensionality reduction, enabling us to accurately assess the strengths and weaknesses of each method in this specific context.

Table~\ref{tab:wine_reduced} compares clustering performance on the wine dataset after dimension reduction. Probably because of autoencoder's ability in extracting key feature combinations, all baseline methods improved. FBBMM still performs the best in all three evaluation metrics, demonstrating its superiority in clustering.

\begin{table}[tb]
\centering
\caption{Clustering on Reduced Features (2 Dimensions)}
\label{tab:wine_reduced}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{CA} & \textbf{ARI} & \textbf{AMI} \\ \midrule
k-means (2D) & 0.961 & 0.882 & 0.860 \\
MeanShift (2D) & 0.961 & 0.882 & 0.860 \\
DBSCAN (2D) & 0.910 & 0.846 & 0.833 \\
Agglomerative Clustering (2D) & 0.978 & 0.930 & 0.900 \\
GMM (2D) & 0.949 & 0.847 & 0.833 \\
MBMM (2D) & 0.809 & 0.526 & 0.588 \\
FBBMM (2D) & \textbf{0.983} & \textbf{0.947} & \textbf{0.927} \\ \bottomrule
\end{tabular}
\end{table}

%\subsubsection{Breast Cancer Dataset}

% **4.4.3.1 Clustering on Original Features (30 Dimensions)**

% Table~\ref{tab:cancer_original} compares clustering performance on the original 30-dimensional features of the breast cancer dataset.

% \begin{table}[tb]
% \centering
% \caption{Clustering on Original Features (30 Dimensions) and FBBMM (2 Dimensions)}
% \label{tab:cancer_original}
% \begin{tabular}{@{}lccc@{}}
% \toprule
% \textbf{Method} & \textbf{Accuracy} & \textbf{ARI} & \textbf{AMI} \\ \midrule
% k-means & 0.854 & 0.491 & 0.464 \\
% MeanShift & 0.849 & 0.582 & 0.489 \\
% DBSCAN & 0.603 & \textbf{0.743} & 0.260 \\
% Agglomerative Clustering & \textbf{0.916} & 0.689 & \textbf{0.568} \\
% GMM & 0.902 & 0.640 & \textbf{0.568} \\
% MBMM & 0.909 & 0.664 & 0.558 \\
% FBBMM (2D) & 0.912 & 0.677 & 0.553 \\ \bottomrule
% \end{tabular}
% \end{table}

% Agglomerative Clustering performs best in accuracy and AMI, while DBSCAN excels in ARI. MBMM and FBBMM show comparable results, with MBMM slightly outperforming in some metrics.

% **4.4.3.2 Clustering on Reduced Features (2 Dimensions)**

% Table~\ref{tab:cancer_reduced} compares clustering performance on the breast cancer dataset after reducing features to 2 dimensions using an autoencoder.

% \begin{table}[tb]
% \centering
% \caption{Clustering on Reduced Features (2 Dimensions)}
% \label{tab:cancer_reduced}
% \begin{tabular}{@{}lccc@{}}
% \toprule
% \textbf{Method} & \textbf{Accuracy} & \textbf{ARI} & \textbf{AMI} \\ \midrule
% k-means (2D) & 0.893 & 0.615 & 0.489 \\
% MeanShift (2D) & 0.687 & 0.094 & 0.139 \\
% DBSCAN (2D) & 0.088 & 0.384 & 0.299 \\
% Agglomerative Clustering (2D) & 0.873 & 0.550 & 0.501 \\
% GMM (2D) & 0.910 & 0.672 & 0.547 \\
% MBMM (2D) & \textbf{0.914} & \textbf{0.684} & \textbf{0.575} \\
% FBBMM (2D) & 0.912 & 0.677 & 0.553 \\ \bottomrule
% \end{tabular}
% \end{table}

% MBMM shows the best performance across all three metrics, while FBBMM performs slightly lower, suggesting that MBMM might be better suited for positively correlated features.

%\subsubsection{MNIST Dataset}

%\figccwidth{convnet_fig}{The convolutional neural network used to extract features (i.e., converting into a $1\times 512$ vector) from MNIST images. }{fig:convnet}{\textwidth}

The MNIST dataset consists of 70,000 grayscale image. Due to the high dimensionality and a convolutional neural network (CNN)'s ability to handle images, we use a CNN as the feature extractor, followed by applying an autoencoder for dimension reduction. Since we are dealing with a clustering algorithm, we need to prevent CNN from learning information from the labels in the MNIST dataset. Thus, we use fashion-MNIST~\cite{lecun1998gradient} to train a CNN. After training, we remove the last fully connected layer. Then, we pass MNIST to this trained CNN to convert an image into a $1\times 512$ dimensional vector. Subsequently, we feed this vector into an autoencoder to reduce the features to 2 dimensional.

Table~\ref{tab:mnist_17} shows the clustering performance in clustering digits 1 and 7 after the feature reduction. FBBMM, again, achieves the best performance in all metrics, demonstrating its effectiveness in handling the digit recognition task.

\begin{table}[tb]
\centering
\caption{Clustering on MNIST Digits 1 and 7}
\label{tab:mnist_17}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{CA} & \textbf{ARI} & \textbf{AMI} \\ \midrule
k-means (2D) & 0.971 & 0.889 & 0.813 \\
MeanShift (2D) & 0.975 & 0.903 & 0.832 \\
DBSCAN (2D) & 0.944 & 0.857 & 0.761 \\
Agglomerative Clustering (2D) & 0.973 & 0.897 & 0.823 \\
GMM (2D) & 0.970 & 0.883 & 0.806 \\
MBMM (2D) & 0.930 & 0.738 & 0.633 \\
FBBMM (2D) & \textbf{0.976} & \textbf{0.907} & \textbf{0.841} \\ \bottomrule
\end{tabular}
\end{table}

% **4.4.4.2 Clustering on MNIST Digits 3 and 8**

% Table~\ref{tab:mnist_38} shows the clustering performance on digits 3 and 8 after feature reduction.

% \begin{table}[tb]
% \centering
% \caption{Clustering on MNIST Digits 3 and 8}
% \label{tab:mnist_38}
% \begin{tabular}{@{}lccc@{}}
% \toprule
% \textbf{Method} & \textbf{Accuracy} & \textbf{ARI} & \textbf{AMI} \\ \midrule
% k-means (2D) & 0.839 & 0.459 & 0.367 \\
% MeanShift (2D) & 0.823 & 0.484 & 0.374 \\
% DBSCAN (2D) & 0.347 & 0.440 & 0.387 \\
% Agglomerative Clustering (2D) & 0.835 & 0.450 & 0.355 \\
% GMM (2D) & 0.831 & 0.439 & 0.347 \\
% MBMM (2D) & \textbf{0.832} & \textbf{0.440} & \textbf{0.371} \\
% FBBMM (2D) & 0.829 & 0.433 & 0.349 \\ \bottomrule
% \end{tabular}
% \end{table}

