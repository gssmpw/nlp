\section{Flexible Bivariate Beta Mixture Model} \label{sec:method}

The Flexible Bivariate Beta Mixture Model (FBBMM) leverages the flexibility of the bivariate beta distribution to model clusters with a variety of shapes, addressing the limitations of traditional clustering methods, which often assume convex cluster shapes. In this section, we describe the FBBMM in detail, including the PDF of the flexible bivariate beta distribution, the FBBMM density function, and the parameter learning process.

\begin{table*}[tb]
\centering
\caption{Definition of Variables in FBBMM}
\begin{tabularx}{\columnwidth}{@{}cX@{}}
\toprule
\textbf{Variable} & \textbf{Definition} \\ \midrule
$N$ & Number of data points \\
$C$ & Number of clusters \\
$\vx_n$ & A data point (indexed by $n$), $\bm{x}_n = [x_{n,1}, x_{n,2}]$\\
$z_n$ & A latent variable indicating the cluster membership of $x_n$, $z_n \in \{1, 2, \ldots, C\}$\\
$\bm{\pi}$ & The probabilities of a data point belongs to the cluster $1, 2, \ldots, C$, $\bm{\pi} = [\pi_1, \ldots, \pi_C]$ \\
$\alpha_j^c$ & The $j$th parameter of the bivariate beta distribution for the cluster $c$, $j \in \{1,\ldots, 4\}, c\in\{1,\ldots,C\}$ \\
\bottomrule
\end{tabularx}
\label{tab:variables}
\end{table*}

\subsection{Bivariate Beta Distribution}

The definition of the beta distribution is unique. However, the beta distribution is only defined on a univariate variable
within the interval $[0, 1]$ or $(0, 1)$. When the number of variates is greater than one, the definition of the multivariate beta distribution is ambiguous~\cite{kotz2019continuous,hsu2024multivariate}. Eventually, we use the flexible bivariate beta distribution based on the definition provided by~\cite{olkin2015constructions} because this definition is one of the few that allows for a positive or negative correlation between covariates, making the cluster shapes more flexible.

Our bivariate beta distribution is defined based on Dirichlet distribution. Let $(U_1, U_2, U_3, U_4)$ be a set of random variables sampled from Dirichlet distributions with parameters $\bm{\alpha} = \{\alpha_1, \alpha_2, \alpha_3, \alpha_4\}$. The PDF is given by: 

\begin{equation}
f(u_1, u_2, u_3, u_4) = \frac{u_1^{\alpha_1 - 1} u_2^{\alpha_2 - 1} u_3^{\alpha_3 - 1} u_4^{\alpha_4 - 1}}{B(\bm{\alpha})},
\end{equation}
where $\alpha_{ij} \geq 0$ and $B(\bm{\alpha})$ is the normalization term, as defined below.

\begin{equation}
B(\bm{\alpha}) = \frac{\prod_{i} \Gamma(\alpha_i)}{\Gamma(\sum_i \alpha_i)}.
\end{equation}

The support of the Dirichlet distribution $u_j$s must follow the following two conditions: $0 \leq u_j \leq 1$ and $u_1 + u_2 + u_3 + u_4 = 1$. By replacing $u_4$ in the above formula with $1 - u_1 - u_2 - u_3$, we get a PDF involving three random variables:

\begin{equation}
%\begin{split}
f(u_1, u_2, u_3) = 
\frac{u_1^{\alpha_1 - 1} u_2^{\alpha_2 - 1} u_3^{\alpha_3 - 1} (1 - u_1 - u_2 - u_3)^{\alpha_4 - 1}}{B(\bm{\alpha})}.
%\end{split}
\end{equation}

Next, we define two random variables $X$ and $Y$:

\begin{equation}
X = U_1 + U_2, \quad Y = U_1 + U_3.
\end{equation}

The marginal distribution of the Dirichlet distribution is defined as a beta distribution. Thus, the PDF of the bivariate beta distribution of $X$ and $Y$ can be written as a function involving only $u_1$ as follows.

\begin{equation}
\begin{aligned}
BBe(x, y|\bm{\alpha}) &= \int_{\Omega} f(u_1, u_2, u_3) du_1 \\
& = \frac{1}{B(\alpha)} \int_{\Omega} u_1^{\alpha_1 - 1} (x - u_1)^{\alpha_2 - 1} \quad (y - u_1)^{\alpha_3 - 1} (1 - x - y + u_1)^{\alpha_4 - 1} du_1,
\end{aligned}
\end{equation}
where $\Omega = \{u_{11} : \max(0, x + y - 1) < u_{11} < \min(x, y)\}$.

\begin{figure}[tbh]
\centering
\includegraphics[width=.65\textwidth]{fig/flex_betapdf2.pdf}
\caption{The PDF plots of the bivariate beta distribution with different parameters. The top row: $\bm{\alpha}=(3,3,3,3)$; $\bm{\alpha}=(1,1,1,1)$; $\bm{\alpha}=(0.8,0.8,0.8,0.8)$. The middle row: $\bm{\alpha}=(2,4,2,2)$; $\bm{\alpha}=(4,2,2,2)$; $\bm{\alpha}=(4,2,4,0.5)$. The bottom row: $\bm{\alpha}=(2,2,2,0)$; $\bm{\alpha}=(1,1,1,0.5)$; $\bm{\alpha}=(0.5,1,1,1)$. The shapes could be nonconvex (e.g., upper right subfigure). The covariates could be positively correlated (e.g., the middle center subfigure) or negatively correlated (e.g., the lower left subfigure), or non-correlated (e.g., the upper middle subfigure).}
\label{fig:flex_beta_pdf2}
\end{figure}

Different parameters $\bm{\alpha}$ result in different bivariate beta distributions. The PDF mode changes according to the values of $\bm{\alpha}$. The mode is in the center if all the parameters $\alpha_j$ are equal. If the value of $\alpha_1$ becomes larger, the mode moves toward the upper right corner, i.e., the modes of the two varieties become larger. Figure~\ref{fig:flex_beta_pdf2} gives PDF examples using different $\bm{\alpha}$s.

\subsection{Generative Process and Probability Density Function of FBBMM}

\figwidth{fbbmm-plate-notation}{The plate notation of the flexible bivariate beta mixture model}{fig:plate-notation}{.35\columnwidth}

We introduce the FBBMM from the perspective of a generative process. Figure~\ref{fig:plate-notation} gives the plate notations of the observed and latent variables of the FBBMM, with the notations listed in Table~\ref{tab:variables}. An observed random variable $\vx_n$ is assumed to be sampled by the following process. First, we sample a latent variable $z_n$ from a multinomial distribution with parameters $\bm{\pi}=[\pi_1, \ldots, \pi_C]$. The latent variable $z_n$ represents the cluster ID of the data point $\vx_n$. Next, $\vx_n$ is sampled from the flexible bivariate beta distribution with parameters $\alpha_{1}^{z_n}, \alpha_{2}^{z_n}, \alpha_{3}^{z_n}, \alpha_{4}^{z_n}$, the four parameters defining the bivariate beta distribution for the cluster $z_n$.

Assume that all data points $\mX = [\vx_1, \ldots, \vx_n]$ are generated from an unknown parameterized FBBMM, the PDF of FBBMM is expressed as:

\begin{equation}
p(\mX | \bm{\theta}) = \prod_{n=1}^N p(\vx_n|\bm{\theta}) = \prod_{n=1}^N \sum_{c=1}^{C} \pi_c BBe(\vx_n|\bm{\theta}_c),
\end{equation}
where $\vx_n = [x_{n,1}, x_{n,2}]$ is a 2D data point, $n \in \{1, \ldots, N\}$, $\bm{\theta}_c = \{\alpha_1^c, \alpha_2^c, \alpha_3^c, \alpha_4^c$\} are the parameters of cluster $c$, $\bm{\theta}$ includes the parameters of all clusters, and $\pi_c$ is the probability that a data point belong to the cluster $c$, thus $\sum_{c=1}^{C} \pi_c = 1$.

\subsection{Parameter Learning for FBBMM}

In practice, we only observe $\vx_1, \ldots, \vx_N$, but the other variables $\alpha_1^{1:C}, \alpha_2^{1:C}, \alpha_3^{1:C}, \alpha_4^{1:C}$, and $\pi_1, \ldots, \pi_C$ are unknown. To learn the parameters of the FBBMM, we use the Expectation Maximization (EM) algorithm. Our objective is to find the parameters $\bm{\theta}$ that maximize the likelihood function:

\begin{equation}
L(\bm{\theta}) = p(\mX|\bm{\theta}) = \prod_{n=1}^{N} p(\vx_n|\bm{\theta}) = \prod_{n=1}^{N} \sum_{c=1}^{C} \pi_c BBe(\vx_n|\bm{\theta}_c).
\end{equation}

Due to the numerical instability of multiplications when $N$ is large, we take the logarithm of the likelihood function by convention to form the log-likelihood.

\begin{equation}
\log(L(\bm{\theta})) = \sum_{n=1}^{N} \log \left( \sum_{c=1}^{C} \pi_c BBe(\vx_n|\bm{\theta}_c) \right).
\end{equation}

Assuming that we know the latent variable $z_n$, which indicates the membership of the cluster of each $x_n$, the complete log-likelihood is:

\begin{equation} \label{eq:log-prob}
\log(L(\bm{\theta})) = \sum_{n=1}^{N} \sum_{c=1}^{C} I(z_n = c) (\log \pi_c + \log BBe(\vx_n|\bm{\theta}_c)),
\end{equation}
where $I()$ is the indicator function, i.e., its output is 1 if $z_n = c$ and 0 otherwise.

In practice, since $z_n$ is unobservable, we compute $\gamma_{n,c}$, the expected probability that $x_n$ belongs to cluster $c$.

\begin{equation}
\label{eq:gamma-nc}
\gamma_{n,c} = \frac{\pi_c BBe(\vx_n|\bm{\theta}_c)}{\sum_{k=1}^{C} \pi_k BBe(\vx_n|\bm{\theta}_k)}.
\end{equation}

In the E-step of EM, we assume that all the parameters $\bm{\theta}_c$s and $\pi_c$s are correct and use them to compute $\gamma_{n,c}$ (Equation~\ref{eq:gamma-nc}). In the M-step, we update the parameters using maximum likelihood estimation. The update for $\pi_c$ is:

\begin{equation} \label{eq:pi-c}
\pi_c = \frac{1}{N} \sum_{n=1}^{N} \gamma_{n,c}.
\end{equation}

For the parameters $\bm{\theta}_c = \{\alpha_1^c, \alpha_2^c, \alpha_3^c, \alpha_4^c\}$ of each cluster $c$, we use the Sequential Least Squares Programming optimizer (SLSQP) to maximize the expected value of Equation~\ref{eq:log-prob} since there seems to be a lack of closed-form solutions.

\begin{equation} \label{eq:log-prob-exp}
E_{z_{1:N}}[\log(L(\bm{\theta}))] = \sum_{n=1}^N \sum_{c=1}^C \gamma_{n,c} (\log \pi_c + \log BBe(\vx_n|\bm{\theta}_c)).
\end{equation}

The algorithm~\ref{alg:fbb-learn} provides the pseudocode for parameter learning in FBBMM.

\begin{algorithm}[tb]
\caption{FBBMM Parameter Learning}
\label{alg:fbb-learn}
\begin{algorithmic}[1]
\State \textbf{Input:} $\mX = \{\vx_1, \ldots, \vx_N\}$: $N$ input data points, each data point $\vx_n$ is 2-dimensional; $C$: the number of clusters
\State \textbf{Output:} Final parameters $\bm{\theta}_{1:C} = \{\alpha_1^{1:C}, \alpha_2^{1:C}, \alpha_3^{1:C}, \alpha_4^{1:C}\}; \bm{\pi} = \{\pi_1, \ldots, \pi_C\}$
\State Initialize parameters $\bm{\theta}_{1:C}$ and $\bm{\pi}$
\State $\text{Old\_prob} \gets -\infty$
\For{$i = 1$ to \text{Epochs}}
    \State // E-step
    \State Compute each $\gamma_{n,c}$ by Equation~\ref{eq:gamma-nc}
    \State Compute $\text{New\_prob}$ by Equation~\ref{eq:log-prob}
    \If {$|\text{New\_prob} - \text{Old\_prob}| < \epsilon$}
        \State break
    \EndIf

    \State $\text{Old\_prob} \gets \text{New\_prob}$
    
    \State // M-step
    \State Compute $\alpha_1^{1:C}, \alpha_2^{1:C}, \alpha_3^{1:C}, \alpha_4^{1:C}$ by maximizing Equation~\ref{eq:log-prob-exp} using SLSQP
    \State Compute each $\pi_c$ using Equation~\ref{eq:pi-c}
\EndFor
\end{algorithmic}
\end{algorithm}