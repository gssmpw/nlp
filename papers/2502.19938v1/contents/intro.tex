\section{Introduction}

Clustering is a fundamental task in data analysis and machine learning that aims to group data points into clusters such that the points in the same cluster are more similar than those in other clusters. This unsupervised learning method is widely used in various applications, including image analysis, information retrieval, text analysis, bioinformatics, and many more~\cite{wang2023detecting,lien2019visited,johnson1967hierarchical,yang2010image}. Clustering helps uncover the underlying structure of the data, facilitates data summarization, and sometimes serves as a preprocessing step for other algorithms~\cite{lien2019visited}.

Despite its widespread use, one of the primary challenges many traditional clustering algorithms face is that they often assume that the data points form clusters with convex shapes. For example, centroid-based algorithms like $k$-means and distribution-based models like Gaussian Mixture Models (GMM) typically produce clusters that are hyperspherical or ellipsoidal~\cite{hsu2024multivariate}. Although this assumption simplifies the clustering process, it restricts the flexibility of these models to handle complex data distributions that do not conform to convex shapes.

This convexity constraint can lead to suboptimal clustering results, especially when the data inherently possesses nonconvex structures. Examples include data points that form concentric circles, crescent shapes, or other intricate patterns. Traditional methods may fail to correctly group these points, leading to less optimal clustering results and loss of valuable structural information.

We propose the Flexible Bivariate Beta Mixture Model (FBBMM) to address these limitations. Unlike conventional models, FBBMM leverages the flexibility of the bivariate beta distribution, which can accommodate a wide range of shapes, including convex, concave, and other irregular forms. This adaptability is crucial for accurately capturing the proper structure of complex datasets.

The FBBMM offers several advantages over traditional clustering algorithms. First, versatile cluster shapes: FBBMM can model clusters with various shapes using the bivariate beta distribution, providing a better fit for nonconvex data structures. Second, soft clustering: like GMM, FBBMM assigns a probability to each data point to belong to different clusters, offering a better and more flexible representation of data point memberships. Third, generative capability: FBBMM, being a generative model, can generate new data points that resemble the original data, which is helpful in data augmentation and simulation tasks.

In this paper, we detail the formulation of FBBMM, describe its probability function and the parameter estimation process using the Expectation Maximization (EM) algorithm, and demonstrate its effectiveness through experiments on synthetic and real-world datasets. The results indicate that FBBMM outperforms traditional models in handling nonconvex clusters and provides a robust framework for flexible and accurate data clustering.

The rest of the paper is organized as follows. In Section~\ref{sec:rel-work}, we review famous clustering algorithms of various types. Section~\ref{sec:method} presents the bivariate beta distribution, the FBBMM model, and the parameter learning process. Section~\ref{sec:exp} compares FBBMM with famous clustering algorithms using both synthetic and open datasets. Finally, we conclude our work and discuss the limitations of FBBMM and future work in Section~\ref{sec:disc}.