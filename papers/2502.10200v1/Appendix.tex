\section{Detailed Task Description for Sequential Navigation Task}\label{App:Task}
\noindent
{\bf Initial agent location}\\
Once per four episodes: randomly chosen on the four sides of the $18\times 18$ square
whose center is on the origin to promote the learning in the peripheral area.\\
Other episodes: randomly chosen in the whole field.\\
{\bf Visual sensor}\\
Number of cells: $11 \times 11=121$. 
Cell size: $1\times1$.
The sensor is fixed with its center at the origin, and the cells are arranged in a square without overlap.
Appearance of agent: $1\times1$ square.\\
Cell output: the area occupied by the agent as a valued from 0.0 to 1.0.\\
{\bf Agent's one-step move}\\
Each actor output $\rightarrow$ multiplied by 1.25 $\rightarrow$ clipped between -1.0 and 1.0.
Furthermore, keeping the direction of the move vector, the size of the vector is normalized
so as that the maximum size for the direction is 1.0.
For example, if the actor outputs are (0.0, 0.9), the agent will move by (0.0, 1.0),
and if they are (0.3, 0.4), it will move by $(0.3, 0.4)$.
As a result, the one-step movable area becomes a circle with a radius of 1.0.\\
{\bf Failure criteria}\\
The average number of steps to the goal was calculated for every 500 episodes.
The simulation run was terminated when one of the following conditions was satisfied.
(1) ($episode\_No. \geq 5000\ \&\ average >190$) for 5 consecutive times
or (2) ($episode\_No. \geq 10000\ \&\ average > 150$) for 5 consecutive times
or\\.                               (3) ($episode\_No. \geq 10000\ \&\ average > 100$) for 10 consecutive times


\section{Initial Weights and Learning Rate}\label{App:Table}
The parameters used in this paper are detailed further in Tables \ref{Table:WeightsAndBiases}.
\begin{table}[h]
  \caption{Parameters for Critic and Actor networks.
  When Dynamic RL was used in the actor network, only the learning rate for SRL is written.
  The learning rate for SAL was always set as $\eta_{SAL} = \eta_{SRL}*0.1$.
  in, hid, hid1, hid2, out: input, hidden, lower hidden, upper hidden, output layer respectively.
  In the column of ``Initial value'', N: Normal-distributed random number (R: Spectral radius),
  Number only: Uniform random number (Its absolute value is less than the value).}
  \label{Table:WeightsAndBiases}
  \centering \small
  \vspace{5mm}
  Critic network\\
  \vspace{3mm}
   \begin{tabular}{cccccc}
  \hline
                       & layer $\rightarrow$ layer & \multicolumn{2}{c}{Initial value} & \multicolumn{2}{c}{Learning rate}\\
                       &  or layer                           & Switch & Crank & Switch & Crank\\                       
  \hline                     
  Weight & in $\rightarrow$ hid & 0.2 & 0.5 & 0.2 & 0.3\\
                       & hid $\rightarrow$ hid & \multicolumn{2}{c}{N R1.3} & \multicolumn{2}{c}{0.002}\\
                       & hid $\rightarrow$ out & \multicolumn{2}{c}{0.0} & \multicolumn{2}{c}{0.02}\\
  \hline
  Bias& hid, out & \multicolumn{2}{c}{0.0} & \multicolumn{2}{c}{0.02}\\
  \hline
  \end{tabular}
 
  \vspace{5mm}
  Actor network (Dynamic RL)\\
  \vspace{3mm}
  \centering \small
  \begin{tabular}{cccccc}
  \hline
                      &   layer $\rightarrow$ layer  & \multicolumn{2}{c}{Initial value} & \multicolumn{2}{c}{Learning rate $\eta_{SRL}$}\\
                       &  or layer                             & Switch & Crank & Switch & Crank\\                       
  \hline                     
  Weight & in $\rightarrow$ hid & 0.2 & 0.5 & 0.01 & 0.005\\
                       & hid1,2 $\rightarrow$ hid2,1 & \multicolumn{2}{c}{0.1} & 0.005 & 0.002\\
                       & hid2 $\rightarrow$ hid2 & \multicolumn{2}{c}{N R3.0} & 0.005 & 0.002\\
                       & hid1 $\rightarrow$ out & \multicolumn{2}{c}{0.1} & 0.01 & 0.005\\
  \hline
  Bias& hid1, hid2 & \multicolumn{2}{c}{0.0} & 0.002 & 0.0002\\
         & out  & \multicolumn{2}{c}{0.0} & 0.002 & 0.0005\\
  \hline
  \end{tabular}
  
  \vspace{5mm}
  Actor network (conventional RL)\\
  \vspace{3mm}
  \centering \small
  \begin{tabular}{cccccccc}
  \hline
                      &   layer $\rightarrow$ layer & \multicolumn{2}{c}{Initial value} & \multicolumn{4}{c}{Learning rate}\\
                       &  or layer                           & Switch & Crank & \multicolumn{2}{c}{Switch} & \multicolumn{2}{c}{Crank}\\                       
                       &                                         &         &        &  BPTT& SAL  & BPTT& SAL\\                       
  \hline                     
  Weight & in $\rightarrow$ hid & 0.2 & 0.5 & 0.1 & 0.01 & 0.2 & 0.01\\
                       & hid1,2 $\rightarrow$ hid2,1 & \multicolumn{2}{c}{0.1} & 0.01 & 0.01 & 0.01 & 0.005\\
                       & hid2 $\rightarrow$ hid2 & \multicolumn{2}{c}{N R1.3} & 0.002 & 0.01 & 0.002 & 0.005\\
                       & hid1 $\rightarrow$ out & \multicolumn{2}{c}{0.0} & 0.01 & - & 0.01 & -\\
  \hline
  Bias& hid1 & \multicolumn{2}{c}{0.0} & 0.002 & 0.01 & 0.002 & 0.002\\
         & hid2 & \multicolumn{2}{c}{0.0} & 0.001 & 0.01 & 0.001 & 0.002\\
         & out  & \multicolumn{2}{c}{0.0} & 0.001 & - & 0.001 & -\\
  \hline
  \end{tabular}
\end{table}
%\begin{table}[t]
%  \caption{Other parameters}
%  \label{Table:OtherParameters}
%  \centering \small
%  \begin{tabular}{ccc}
%  \hline
%  & Switch Task & Crank Task\\
%  \hline
%  Discount factor $\gamma$ for critic in Eqs.~(\ref{Eq:TDerr}),(\ref{Eq:C_train})& \multicolumn{2}{c}{0.98}\\
%  Rate of regulation $\eta_{reg}$ in Eq.~(\ref{Eq:Regularize}) & $1E^{-6}$ & $1E^{-7}$\\
%  Moving average for sensitivities $\alpha$ in Eq.~(\ref{Eq:Ave_sen}) &  \multicolumn{2}{c}{0.001}\\
%  Moving average for Raising critic $\beta$ in Eq.~(\ref{Eq:Ave_C}) &  \multicolumn{2}{c}{0.0001}\\
%  Threshold of Raising critic $C_{th}$ in Eq.~(\ref{Eq:Raise_Critic}) & \multicolumn{2}{c}{0.1}\\
%  Rate of raising critic $\eta_{raise}$ in Eq.~(\ref{Eq:Raise_Critic}) & 0.0005 & 0.0002\\
%  SAL Target $s_{th}$ in Fig.~\ref{fig:DynamicRL} & 1.3 & 1.6\\
%  %Learning rate for SAL & \multicolumn{2}{c}{0.1}\\
%  Truncated steps in BPTT & 20 & 10\\
%  \hline
%  \end{tabular}
%\end{table}


\section{Computation of Exploration Exponent}\label{App:Expl_factor}
Before each episode, an agent is replicated with the same actor and critic networks as the original.
The original agent was placed in one of the nine starting positions, which are the same as those shown in Fig.~\ref{fig:Task1_Loci}(b) or (d).
The replicated agent was placed randomly at a position $10^{-6}$ units away from the original one.
The two agents move according to their actor outputs without learning until one of them finishes its episode.
Subsequently, the Euclidian distance between their states is observed, and the equation below is calculated.
Here, the agent's state is represented by the direct product space (222 dimensions)
of the input signal space (122 dimensions) and the upper hidden layer state space (100 dimensions).
%the inputs of the first hidden layer with 222 dimensions are used as the agent's state.
%The state of the system is defined as a point in the input signal space of the lower hidden layer with 222 dimensions.
%That is the sum space of the sensor signal space (122 dimensions)
%and the space of the neuron outputs in the upper hidden layer (100 dimensions).
%The Euclidian distance $\delta$ between the corresponding states between the two episodes is observed.
This was repeated for the nine start positions, and the exploration exponent was averaged across the nine sets.

The exploration exponent is defined as 
\begin{equation}
\lambda = \frac{1}{T_{end}-T_{start}}\sum_{t=T_{start}+1}^{T_{end}} ln \frac{\delta_{t}}{\delta_{t-1}}
=\frac{1}{T_{end}-T_{start}}ln\frac{\delta_{T_{end}}}{\delta_{T_{start}}}
\end{equation}
where $\delta_t$:  the distance between the two states at the $t$-th step, $T_{start}$: the start step
after the preparation steps, $T_{end}$: the final step in the episode.
When the distance $\delta$ is greater than $10^2$ or less than $10^{-10}$,
$T_{end}$ was set to the previous step number.
When the agent went to a corner of the field, the locations often became exactly the same
between the two episodes,
but the computation continued as long as the distance remained in the range of $10^2$ to $10^{-10}$.

\section{Equation of Motion of Slider-Crank Mechanism}\label{App:Eq_Crank}
As the equation of motion of the slider-crank mechanism, as shown in Fig. \ref{fig:Crank},
the author used the following equation
\begin{equation}
J\ddot{\theta} = RF_{\theta} - D\dot{\theta}\\
\end{equation}
where
\begin{equation}
F_{\theta} = f \frac{sin(\phi+\theta)}{cos\phi}\\
\end{equation}
\begin{equation}
cos\phi = \frac{L^2+x^2-R^2}{2Lx}\\
\end{equation}
\begin{equation}
x = Rcos\theta + \sqrt{L^2 - R^2sin^2\theta}\\
\end{equation}

%\allowdisplaybreaks

\begin{align}
J&: \text{the moment of inertia of the rotor} (=10.0)\nonumber \\
\theta&: \text{the rotational angle of the rotor}\nonumber \\
R&: \text{the distance from the center of the rotor to the crank} (=1.0)\nonumber \\
D&: \text{the damping coefficient for the rotation of the rotor} (=0.1)\nonumber \\
F_{\theta}&: \text{the force applied to the rotor in the direction of its rotation}\nonumber \\
f &: \text{the force applied laterally to the end of the rod, determined by the actor's outputs}\nonumber \\
\phi&: \text{the angle of the rod}\nonumber \\
L&: \text{the length of the rod} (=3.0)\nonumber \\
x&: \text{the distance from the center of the rotor to the rod end}\nonumber
\end{align}


