%\documentclass[5p,dvipdfmx]{elsarticle}
%\documentclass[preprint,dvipdfmx,12pt]{elsarticle}
%\documentclass[final,dvipdfmx]{elsarticle}
%\documentclass[review,dvipdfmx]{elsarticle}
\documentclass{article}
\usepackage{arxiv}
%\documentclass[5p]{article}

%\usepackage{lineno,hyperref}
\usepackage{booktabs}
\newcommand{\thead}[1]{\multicolumn{1}{c}{\textbf{#1}}}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{breakurl}
%\usepackage{natbib}
\modulolinenumbers[5]

%\journal{Journal of \LaTeX\ Templates}
\usepackage{graphicx}
%\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{multirow}
%\usepackage{subfigure}
\usepackage{latexsym}
%\usepackage{mediabb}
\usepackage{subcaption}
\renewcommand\thesubfigure{\Alph{subfigure}}
%\usepackage[caption=false]{subfig}
%\usepackage{url}
\usepackage{appendix}
\usepackage{setspace}
\usepackage{ascmac}
%\usepackage[round]{natbib}
\usepackage{natbib}
\bibpunct[:]{[}{]}{,}{a}{}{,}

%\setstretch{1.667}
%\setstretch{2.0}
%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\title{{\bf \fontsize{20pt}{0pt}\selectfont Dynamic Reinforcement Learning for Actors}}
%- Learning not for States but for Flows -
\author{Katsunari Shibata\\ \\
\texttt{katsunarishibata@gmail.com} \\ \\[-3mm]
Independent Researcher, Kosai, Shizuoka, Japan
}
%\begin{frontmatter}

%\title{Elsevier \LaTeX\ template\tnoteref{mytitlenote}}
%\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
%\author{Elsevier\fnref{myfootnote}}
%\author{Katsunari Shibata\fnref{myfootnote}}
%\address{700 Dannoharu, Oita 970-1192, JAPAN }
%\fntext[myfootnote]{Since 1880.}
%\fntext[myfootnote]{Oita University}

%% or include affiliations in footnotes:
%\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
%\ead[url]{www.elsevier.com}
%\ead[url]{shws.cc.oita-u.ac.jp/shibata/home.html}
%\cortext[mycorrespondingauthor]{Corresponding author}

%\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}

%\address[mymainaddress]{Independent Researcher, Arai-cho, Kosai, Shizuoka 431-0301, JAPAN}
%\address[mysecondaryaddress]{360 Park Avenue South, New York}
\begin{document}
\maketitle

\begin{abstract}
%This template helps you to create a properly formatted \LaTeX\ manuscript.
Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics,
instead of the actor (action-generating neural network) outputs at each moment,
bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic.
The actor is initially designed to generate chaotic dynamics through the loop with its environment,
enabling the agent to perform flexible and deterministic exploration.
%This allows the agent to explore flexibly without stochastic action selection.

Dynamic RL controls global system dynamics using a local index called ``sensitivity,''
which indicates how much the input neighborhood contracts or expands
into the corresponding output neighborhood through each neuron's processing.
While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics,
sensitivity-controlled reinforcement learning (SRL) adjusts them
%â€”
--- to converge more to improve reproducibility around better state transitions with positive TD error
and to diverge more to enhance exploration around worse transitions with negative TD error.
%This means a leap from general learning, which focuses on moving a point in the state space,
%to an entirely novel type of learning that controls the flow around a line formed by the moving point through time.

Dynamic RL was applied only to the actor in an Actor-Critic RL architecture
while applying it to the critic remains a challenge.
It was tested on two dynamic tasks and
%; a memory-required task and a dynamic pattern generation task.
functioned effectively without external exploration noise or backward computation through time.
Moreover, it exhibited excellent adaptability to new environments, although some problems remain.
%The author believes that it has the potential to be an essential technique for the acquisition of
%higher functions such as thinking in which dynamic learning must be critical.
% after further improvement.

Drawing parallels between `exploration' and `thinking,'
%both need autonomous and multi-step state transitions including unexpected ones
the author hypothesizes that ``exploration grows into thinking through learning''
and believes this RL could be a key technique for the emergence of thinking,
%including inspiration that is beyond common sense but makes sense.
including inspiration that cannot be reconstructed from massive existing text data.
%Finally, he mentions what he thinks about the grave potential risks of this RL despite his presumption.
Finally, despite being presumptuous,
the author presents the argument that this research should not proceed due to its potentially fatal risks,
aiming to encourage discussion.
%he urges researchers not to proceed with the research on Dynamic RL hereafter due to its potentially fatal risks.

$\langle$ Highlights $\rangle$
\begin{itemize}
\item This paper proposes Dynamic Reinforcement Learning (Dynamic RL), which controls system dynamics
generating exploration-embedded motions without stochastic action selection.
\item Dynamic RL comprises two learning methods: Sensitivity Adjustment Learning (SAL)
and Sensitivity-controlled Reinforcement Learning (SRL).
\item SAL prevents excessive convergence by maintaining chaos in the system dynamics.
\item SRL makes the system dynamics more convergent for better reproducibility
or more divergent for more exploration depending on the TD error.
\item Dynamic RL is demonstrated to function as RL without BPTT in two dynamic tasks
and exhibits excellent adaptability to new environments.
\item The author suggests its potential as a fundamental technique for the emergence of thinking
and is terribly concerned about its risks.
\end{itemize}
\end{abstract}

\keywords{
%\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
%\MSC[2010] 00-01\sep  99-00
reinforcement learning (RL), 
%Dynamic Reinforcement Learning (Dynamic RL),
recurrent neural network (RNN),
chaotic dynamics, exploration, sensitivity, thinking
}

%\end{frontmatter}
\pagestyle{fancy}
%\fancyhead{}
%\fancyhead[CE]{Sensitivity\ \  \small{-- Local Index to Control Chaoticity and Gradient Globally --}}
%\fancyhead[CO]{\small{K.~Shibata, T.~Ejima, Y.~Tokumaru, T.~Matsuki}}
\title{Dynamic Reinforcement Learning for Actors \hspace{8.4cm}K.~Shibata}

\begin{itembox}[|]{\bf Statement}
Dynamic Reinforcement Learning (Dynamic RL) proposed in this paper has the potential
to endow Artificial Intelligence (AI) or Artificial General Intelligence (AGI) with the ability to think and discover new things,
which could pose a grave threat to humanity.
Now, while this research is still taking baby steps,
we should halt further progress to protect humanity from the risk.
%for the sake of humanity.
The author sincerely and earnestly urges researchers to refrain from advancing this work
and developers to refrain from developing a library for Dynamic RL in frameworks,
% not to proceed with
at least until a consensus is reached among humans on this matter.
Let us pause, imagine, and contemplate, free from preconceptions before it is too late.
\\ \\
Refer to subsection \ref{subsec:Risk} for the discussion leading to this statement.
\end{itembox}

%\linenumbers

\section{Introduction}\label{Sec:Introduction}
%\section{The Elsevier article class}
Reinforcement learning (RL) has evolved into a core technology for autonomous, non-supervised learning
 in modern artificial intelligence (AI) through several significant qualitative advancements.
As the next major step in its evolution, the author introduces `Dynamic RL.'
Unlike conventional methods, Dynamic RL treats exploration as an inherent aspect of actions
rather than an independent process.
Instead of using external noise for stochastic selection, exploration is generated as chaotic system dynamics
through the action-generating (actor) network, and RL controls these dynamics directly.

\subsection{Evolution of RL toward Learning of All Kinds of Human Functions}
RL initially involved only learning appropriate tables or mappings from a discrete state space
to a discrete action space within a Markov decision process (MDP) \citep{Sutton1998}.
A neural network (NN) was then introduced, providing many degrees of freedom and parameters.
This enabled an agent to learn not only continuous nonlinear mappings but also the entire sensor-to-motor process
based on a value function using reinforcement signals with the help of the gradient-based method.
Consequently, RL has advanced from learning of actions to learning of various functions,
%that are necessary for the sensor-to-motor process,
including recognition and prediction \citep{ICNN97,Intech, End-to-End}.
%What makes RL attractive is the gap that while just a scalar reinforcement signal is given,
%functions that are hard to describe for humans can be acquired autonomously,
%and so it is possible to acquire a variety of functions that humans have not taught.
A notable achievement occurred when an RL agent learned the game strategy of ``tunneling'' in the game ``Breakout''
solely by using changes in the game score as a reinforcement signal \citep{Mnih}.
RL has since become a driving force in achieving human-level, and sometimes superhuman, performance
particularly in gaming \citep{OpenAI-Five, AlphaStar, MuZero}.

Furthermore, the introduction of a recurrent neural network (RNN) has opened the way for RL
to process and learn along the time axis.
This allowed agents to extract necessary information from large amounts of past information
and then retain and utilize it.
This advancement allowed learning in partially observable situations,
such as partially observable MDPs (POMDPs), and solving memory-dependent tasks.
As a result, the functions acquired through RL span a wide range,
including recognition, memory, attention, prediction, and communication \citep{ISADS99,Intech,End-to-End,Communication}.

Does this suggest that RL agents can acquire all functions that humans have, even simple ones, through learning?
The current answer is `No'.
Unfortunately, the author has yet to accept any of these as functions that could be called `thinking.' 
Therefore, he has set the emergence of such a function through learning as his major future target
%also
and has considered what would be required for the emergence for more than a decade.
%The author has yet to encounter the emergence of any function that could be called `thinking'.
%Therefore, he has set the emergence of such a function through learning as his major future target
%and has considered what would be required for the emergence.

\subsection{`Thinking' and Recent Advancements in Generative AI}
Humans are intelligent, and it would not be an exaggeration to say that `thinking' is the most typical form of intelligence.
`Thinking' has been a central topic in philosophy for a long time since the time of Plato \citep{Plato},
with numerous ideas explored \citep{Britannica}.
About 80 years ago, attempts began to explain human intelligence from a computational perspective
by considering the mind or neural networks, which are thought to be the source of intelligence in living organisms,
as computational machines \citep{McCulloch_Pitts,Turing,Newell}.

Recently, generative AI, especially large-scale language models (LLMs) such as GPT \citep{GPT} and Gemini \citep{Gemini},
appear to understand what we say, think about it, and come up with an appropriate response.
We, at least the author, feel(s) as if we are interacting with a human who has intelligence.
It was reported that ChatGPT powered by GPT-3.5 performed near or over the passing threshold
on the United States Medical Licensing Exam (USMLE) \citep{Chat-GPT},
and also GPT-4 passed a simulated bar exam with a score around the top 10\% of test takers \citep{GPT-4}.

Transformer \citep{Transformer}, a central technique of many LLMs, encodes input patterns into a latent space
constructed through learning, capturing complex dependencies primarily relying on self-attention mechanisms,
without relying on sequence-aligned RNNs or convolutional networks.
Then, it generates likely responses in LLMs based on learning from a vast amount of data.
It seems different for the author from the common image of thinking: ``letting one's mind wander.''
Actually, in response to the question ``Are you thinking?'', both Chat-GPT and Gemini themselves deny
that they think like a human.
%"No, I don't ``think'' in the way humans do."
%My responses are generated by analyzing patterns in the input, matching them with my training data,
%and applying algorithms to produce relevant answers. It might seem like I'm thinking,
%but it's purely computational processing, not consciousness or independent thought."
The remarkable abilities of large-scale LLMs, enabled by their excellent scalability,
might suggest that overwhelming quantity or scale changes the quality drastically.
However, the author remains amazed that such a relatively simple underlying technology
is capable of generating such human-like responses.

From the perspective of `thinking,' there are several studies \citep{Koivisto, Guzik} that evaluate LLMs
as comparable to humans even in tasks designed to observe the ability of divergent thinking \citep{Guilford},
such as the AUT (Alternate Uses Task) \citep{AUT, AUT-net} or TTCT (Torrance Test of Creative Thinking) \citep{TTCT}.
However, one study \citep{Aggarwal} reported that AI machines are not identical to humans in terms of the quality of intelligence or thought
but have human-like logical reasoning systems, which are achieved simply through learning and mimicking human abilities.
Another review on creativity in AI \citep{Ismayilzada} also reported,
being supported by many studies highlighting the inferiority of LLMs in lateral thinking \citep{deBono},
originality, abstract reasoning, and related areas \citep{Huang, Jiang, Zhao, Gendron, Mitchell, Chakrabarty, Lu, Tam}, as follows:
The latest AI models are largely capable of producing linguistically and artistically creative outputs
%such as poems, images, and musical pieces,
but struggle with tasks that require creative problem-solving, abstract thinking, and compositionality.
They exhibit strong interpolation and moderate extrapolation capabilities
but are still far from truly inventing a completely new type of creative artifact.

Lateral thinking requires deviating from established common sense and cannot be achieved
as an interpolation or extrapolation.
In \citep{Lu}, it was observed that machine-generated texts contain significantly more semantic and verbatim matches
with existing web texts compared to high-quality human writings.
%the possibility was considered that the LLM's remarkable creativity may have largely originated
%from the creativity of human-written texts on the web.
%They quantified the linguistic creativity of a given text by estimating
%how much of that text can be reconstructed by mixing and matching a vast amount of existing text snippets on the web.
This observation made it more plausible that the LLM's creativity may largely originate
from the creativity of human-written texts on the web.
%the creativity index of professional human writers is significantly higher than that of LLMs.
To improve creativity, methods such as active divergence and creative decoding
were suggested \citep{Ismayilzada, Broad, Franceschelli}.
However, since they are used in the framework of the current generative deep learning,
it would be difficult to avoid this essential problem.

Independent of the current rapid progress in AI,
the author has arrived at a novel technique for RL that is distinct from Transformer-based generative AI.
In his view, this technique has the potential to become an essential technique for the emergence of `thinking'.
%While Transformer-based generative AI continues to improve at a tremendous rate,
%so its shortcomings may also be addressed in the ongoing progress.
Here, putting the Transformer aside for a moment,
the author introduces Dynamic Reinforcement Learning (Dynamic RL), which brings about a major shift in RL
and explains the potential that this shift may fill the gap in thinking between humans and the current generative AI.

\subsection{Exploration and Thinking}
While the exact definition of `thinking' is difficult to pin down, let us explore the more dynamic form of thinking
that we typically associate with the term.
Humans can think even when remaining still with eyes closed and ears covered,
which seems different from other functions like recognition or prediction.
In order to survive as living organisms, we must at least avoid converging and becoming stagnant.
%We must gain diverse experiences for the future, learn from the experiences, and
%think about a variety of things, processing rationally based on the knowledge acquired through learning.
%Furthermore, we sometimes inspire and make discoveries.
Therefore, many would likely agree that autonomous, rational state transitions,
even without external triggers, are required for `thinking.'
However, acquiring multistep state transitions through learning from scratch in RNNs is very difficult \citep{Sawatsubashi}.
In particular, forming autonomous state transitions without external triggers is challenging.
%Furthermore, the recurrence of the computation makes the outputs of the neurons to be their feedback inputs.
%Therefore, learning often changes the feedback inputs largely,
%and that makes the learning more difficult.
Moreover, considering human abilities such as inspiration and discovery, these autonomous state transitions
should not only be rational but also sometimes unexpected.
%The author believes that it is an essential problem that cannot be avoided
%in the AI using the modern RL with an RNN.

Comparing modern RL with human learning reveals one major difference: `exploration.'
In conventional RL, stochastic selection uses external random noise,
independent of the motion or action generation process, as shown in Fig.~\ref{fig:Exploration}(a).
\footnote{In this paper, the term `motion' is used as continuous, and `action' is used as discrete,
while the term `actor' is used as a generator in both cases.
Action is usually abstract such that the action "turn left" itself is not a motor command,
but is broken down into a series of motor commands for several motors.
Therefore, in the proposed RL, the actor outputs represent continuous motor commands
based on the end-to-end learning concept \citep{End-to-End}.}
For example, $\epsilon$-greedy or Boltzmann selection is applied after computing the Q-value for each action,
neither influencing nor being influenced directly by its computation process.
The exploration noise is not a function of state
even though the temperature is adjusted by simulated annealing \citep{SA}.
%Our human exploration at least does not seem to be stochastic selections only at the final output level,
%which would be the motor level in the sensor-to-motor learning framework.
%Then the author showed that appropriate exploration behaviors themselves
%can be acquired through learning at first \citep{ExplorationLearning1, ExplorationLearning2}.
%However, the agent still needed random exploration to learn the exploration.
\begin{figure}[t]
\centerline{\includegraphics[scale=0.28]{Exploration.pdf}}
\caption{The difference in exploration between conventional RL and humans,
who have inspired the exploration in the proposed Dynamic RL.
In humans or Dynamic RL, the actor RNN embeds exploration factors into motor commands
by inducing chaotic system dynamics without stochastic selection using a random number generator.
(RNN: recurrent neural network)}
\label{fig:Exploration}
\end{figure}

In contrast, humans often act while wondering about this and that.
Exploration seems to work during the action or motion generation process.
When we wonder which path to take at a fork in the road, we do not move our hands erratically for exploration.
Exploration is not always uniform; it should depend on the direction in the state space and also the current state.
Furthermore, it should be improved through learning \citep{Expl,Goto_Expl}.
Therefore, the author considers that exploration represents the degree of irregularity in motion generation
and is updated together by the learning of motion generation, as shown in Fig.~\ref{fig:Exploration}(b).
Exploration requires non-convergent, irregular, and unexpected state transitions.
It can be embedded in motor commands by making the system dynamics chaotic.
In other words, by making the dynamics sensitive to minute differences, 
agent behaviors vary even from similar states, leading to the generation of deterministic exploratory state transitions.
%Chaotic dynamics are generated easily by setting the recurrent connection weights in the RNN to be random and large.
Dynamic RL proposed here employs this type of exploration.

%On the other hand, thinking needs autonomous state transitions and also
%unexpected but rational transitions.
%Such transitions also do not emerge with convergent nor strong chaotic system dynamics
%but emerge with weak chaotic dynamics.
%When we see the learning agent as a creature, it cannot explore with convergent dynamics
%and is therefore left with no choice but to die.
From the above, `exploration' is similar to `thinking' in terms of autonomous state transitions,
including unexpectedness.
Both require chaotic system dynamics with a positive Lyapunov exponent rather than convergent dynamics.
The aforementioned points suggest that `exploration' and `thinking' cannot be clearly separated
and may continuously exist along a line within the range of chaotic dynamics.
%Exploration, as well as thinking, does not emerge with converging dynamics
%but requires autonomous state transitions.
In `exploration,' state transitions need to be more irregular with stronger chaotic dynamics.
In contrast, those in `thinking' need to be less irregular but still non-convergent, with weaker chaotic dynamics;
furthermore, they must be more rational.
This means that as shown in Fig.~\ref{fig:Thinking},
they exist on a diagonal line in the two-dimensional space with irregularity and rationality as axes.
If the state transitions are very irregular, they cannot be rational.
In other words, they cannot exist in the upper right portion of Fig.~\ref{fig:Thinking}.
Therefore, ``being less irregular'' is a necessary condition for ``being more rational.''
\begin{figure}[t]
\centerline{\includegraphics[scale=0.34]{ThinkingExplAndChaos2D.pdf}}
\caption{The author's concept of the relationship between `exploration' and `thinking'
 and how they relate to system dynamics.
`Thinking' and `exploration' are similar and inseparable in that both require multistep autonomous state transitions,
and they exist continuously on a spectrum characterized by chaotic dynamics.
In `exploration,' the state transitions must be irregular.
In `thinking,' the state transitions must not only be less irregular but also rational.
Dynamic RL controls the system dynamics based on a value function using reinforcement signals
while preserving chaotic dynamics.}
\label{fig:Thinking}
\end{figure}

If the system dynamics are highly chaotic, the agent can explore widely.
The agent cannot `think' before acquiring basic knowledge, such as various cause-and-effect relationships in the world.
Converging the flows around better state transitions makes the transitions more rational,
thereby, the agent acquires the necessary basic knowledge.
That is expected to Dynamic RL.
If autonomous state transitions have already been formed as explorations,
just adjusting the convergence or divergence of transitions is enough to achieve autonomous and rational transitions.
Thus, this should be much easier than creating such transitions from scratch by RL using an RNN with convergent dynamics.
Then, the author posits the following bold hypothesis: ``\textbf{Exploration grows into thinking through learning}''
by shifting state transitions from ``more irregular'' to ``more rational'' while maintaining system dynamics chaotic.
%In thinking, the dynamics of the RNN still need to be chaotic.
Furthermore, when situations change, and so expected rewards cannot be obtained,
Dynamic RL is expected to resume more exploratory behaviors autonomously in the agent.

%Next, let us consider learning methods more concretely.
%In conventional RL, when continuous motions are learned, they are selected stochastically and
%trained based on the product of the difference between actual and expected changes in the value function
%and the exploration noise, which is the difference between actual and default motions.
%However??????????, stochastic exploration is an assumption of this learning method.
\subsection{Learning of Local Dynamics}
In conventional RL, when continuous motions are learned, if the TD error is positive, learning is performed
to move the actor outputs in the direction of the actual motion chosen from the probabilistic distribution.
Conversely, if the TD error is negative, learning tries to move the actor outputs in the opposite direction.
The weights and biases in the network are trained using a gradient-based method.
The author has been uncomfortable with the fact that even though the output computation process of RNNs is dynamic,
learning is still stuck in a static form.
%Moreover, the author has been uncomfortable with the fact that, although RNNs perform dynamic processing,
%their learning methods are still based on static approaches like backpropagation through time (BPTT).
He has tried a major shift in learning from static to dynamic.
This will be discussed in more detail in the subsection \ref{subsec:Future}.
The author's group previously proposed a new method utilizing chaotic dynamics
without assuming stochastic exploration \citep{IJCNN2015},
but the learning of connection weights between hidden neurons did not improve learning performance.

%The reservoir network, which uses a reservoir of a large number of randomly connected neurons
%and learns only the readout weights, can learn complicated dynamic patterns
%even through reward-modulated Hebbian learning \citep{Hoerzer}.
%In this learning process, random noise is used for exploration,
%and instead of ideal outputs, reward-like signals derived from the outputs are provided for learning.
%However, considering that thinking requires a variety of dynamic behaviors of NNs
%and flexible, complex switching among them based on high-dimensional sensor input and network states,
%%thinking emerges in autonomous and rational transitions in an RNN,
%the author believes that the connection weights of hidden neurons, including those between the hidden neurons,
%should be trained appropriately through learning.

%In reservoir computing such as Echo State Networks \citep{ESN},
%it has been shown that a reservoir consisting of a large number of neurons connected randomly
%can learn complicated dynamic patterns
%although the random weight matrix between the neurons is fixed to an appropriate size,
%and only the weights to the readout units are trained.
%It has also been shown that the appropriate spectral radius of the weight matrix 
%it is general that the recurrent connection weights are randomly given with an appropriate size 
%and fixed during learning.
%If the network size is large, a lot of past information can be stored in the network.
%The author expects that if the weight matrix among hidden neurons can be learned,
%the emerging function approaches `thinking' by realizing more flexible state transitions
%while exploring by the chaotic dynamics.

As mentioned, current LLMs are reported to struggle with lateral thinking,
which requires stepping beyond common sense.
%One possible reason for this limitation is that LLMs operate like   or extrapolation
%of the vast training data \citep{Ismayilzada}.
Expecting novelty is analogous to the exploration in RL.
In LLMs, stochastic selections are used,
and the temperature parameter is often expected as a control variable for randomness,
with higher temperatures promoting randomness, and is expected as the creative parameter \citep{Peeperkorn}.
Nevertheless, it has been reported that the temperature does not always work as expected \citep{Peeperkorn,Ismayilzada}.

%Regarding the issues with this expectation, the author offers the following perspective.
It is clear that if state transitions are merely random or irregular, the process can be `exploration' but cannot be `thinking.'
%it does not evoke a sense of novelty.
Even when perceiving novelty in thinking, there must also be some rationality.
However, it seems quite natural that as irregularity increases, rationality decreases.
That is also suggested by the diagonal block arrow in Fig.~\ref{fig:Thinking}.
Actually, a trade-off between novelty and coherence was observed in \citep{Peeperkorn}.
In \citep{Nath}, it was pointed out that LLMs were biased towards either persistence (deep search)
or flexibility (broad search), regardless of parameter settings such as temperature,
while in humans, both persistent and flexible pathways were observed.
Then, how can humans solve this problem even though it seems impossible to solve at first glance?

%%%and they become more unexpected and irregular by making the dynamics more divergent or chaotic in general.

%Following the idea in Fig.~\ref{fig:Thinking}, to achieve `thinking,' the dynamics should be ``more rational'' and ``less irregular''
%but ``chaotic.''
What we should be aware of here is the difference in whether dynamics are local or global \citep{LocalDynamics}.
The dynamics that have appeared so far are from a global perspective,
and chaoticity represents the characteristic of the average of convergence or divergence around state transitions over a long period,
as seen in the definition of the Lyapunov exponent.
%The convergence or divergence varies depending on the state.
%It is often determined by the Lyapunov exponent,
%which indicates whether the neighborhoods around the observed trajectories converge or diverge on average over a long period.
Assuming that the system is dynamic with deterministic state transitions that can be described by non-linear differential equations.
The instantaneous convergence or divergence of a neighborhood of a given state
-- what is called `local dynamics'  -- is determined by the spectrum (or eigenvalues) of the Jacobian matrix
of the differential equation and varies with state transitions if the system is nonlinear. 
It is not a scalar nor a constant matrix, but each element of the matrix can be a function of the state.
Therefore, as shown in Fig.~\ref{fig:LocalDynamics}(A), the local dynamics in the state space can converge around one state
and diverge around another even in the same system.
Moreover, as shown in Fig.~\ref{fig:LocalDynamics}(B), even in the neighboring region of the same state,
the flow from a point displaced in one direction from the state can converge,
while that from a point displaced in another direction can diverge
if the Jacobian matrix has both positive and negative eigenvalues.
\begin{figure}[t]
\centerline{\includegraphics[scale=0.25]{LocalDynamics.pdf}}
\caption{A conceptual diagram explaining the degrees of freedom (DOFs) that dynamics have for a sample case of three-dimensional state space as an example.
(A) Convergence (a) or divergence (b) may vary depending on the state even in the same system.
(B) Convergence or divergence can be varied depending on the direction even in the same state.
For easy viewing, the neighborhood around a state has originally three dimensions, but only two dimensions are presented.
Additionally, the directions of the two eigenvectors are assumed to be orthogonal to each other and also to the direction of state transition.}
\label{fig:LocalDynamics}
\end{figure}

The author believes that humans are able to harmonize rationality and novelty
making full use of the degrees of freedom (DOFs) in the dynamics through learning,
which enables humans to sometimes step beyond common sense while thinking rationally.
However, the temperature parameter is often scheduled manually but is an unlearned scalar constant
and is not enough to change the randomness flexibly depending on the state or direction.
%Its change is sometimes scheduled manually but is not learned.

\newpage
\subsection{Introduction of Dynamic Reinforcement Learning (Dynamic RL)}
Comprehensively considering the above with the hypothesis in mind,
the author introduces a novel RL framework again
that significantly redefines the concepts of exploration and learning in an RNN, as follows.
\\
\begin{enumerate}
\item{An RL agent does not perform stochastic motion selection using external random noise.
The chaotic dynamics produced by the agent's RNN and the environment together embed exploration factors in its motion outputs.\label{Item:Chaos}}
\item{The dynamics are trained to enhance reproducibility when
the TD error is positive (i.e., the value is better than expected) and to enhance exploratory behavior when the TD error is negative
(i.e., the value is worse than expected) while maintaining chaotic dynamics. \label{Item:LearnDyn}}
\item{To achieve the objective (item \ref{Item:LearnDyn}), sensitivity \citep{Sensitivity} is used.
It is a local index that represents the degree of convergence or divergence of the input neighborhood
to the corresponding output neighborhood through the processing of each neuron.
\label{Item:UseSen}}
\end{enumerate}
The author's group already proposed the above hypothesis and item \ref{Item:Chaos} of the concept in \citep{IJCNN2015}.
%However, at that time, the recurrent connection weights could not be trained appropriately.
It was also demonstrated that chaotic dynamics operate as exploration without stochastic selection in actor-critic \citep{IJCNN2015},
reward-modulated Hebbian learning \citep{NN_Matsuki}, and TD3 \citep{TD3_Matsuki}
and that the ``edge of chaos'' was a favorable choice for network dynamics, leading to high learning performance in \citep{NN_Matsuki}.
% when weights were set
%so as that the system dynamics are around the edge of chaos \citep{NN_Matsuki}.%Furthermore, unlike the typical conventional learning,
%this learning does not aim to move the network output in a more appropriate direction at each moment,
%but rather aims at controlling the flow of the network state over time.
%The author will discuss it later\ref{Sec:Discussion}.

%However, considering their capabilities, the author believes it is likely that similar processes,
%if not all, are actually functioning in our brains.  (Transformer)

In Dynamic RL, an agent can explore using many DOFs because the motions with exploration are generated through its RNN
and can be learned.
The flexibility in exploration is expected to carry over directly into thinking through learning
while maintaining chaos in the global dynamics.
%Accordingly, unlike stochastic selection, learning local dynamics is expected to properly adjust divergence or convergence
%for each state and direction of deviation.
%Thus, that brings a huge degree of freedom in exploration, and turns it to be flexible.
% enables fine-tuned exploration
Furthermore, if the RNN architecture is hierarchical, learning is expected to control irregularity even in the abstract state space.
In this way, the author expects that a Dynamic RL agent, learning through high-DOF exploration,
will eventually be able to think about a variety of things, including abstract ones,
and sometimes generate completely new ideas on its own by flexibly utilizing both the irregular and the rational.

Currently, to improve learning efficiency or stability, many excellent RL methods,
such as PPO \citep{PPO}, SAC \citep{SAC}, A3C \citep{A3C}, TD3 \citep{TD3}, 
and experience replay with PER \citep{PER} or HER \citep{HER} have been used.
However, since Dynamic RL presents a newborn learning concept,
comparison targets are limited to simple conventional RL for a fair evaluation of basic performance.
%Notably, Transformer \citep{Transformer} has become dominant in AI,
%and is applied also in RL, such as Decision Transformer \citep{DecisionTransformer}.
%It is entirely distinct from conventional RL, and its function emergence is not yet well understood.
%%However, the emergence of functions inside has not been so clear yet.
%Given the remarkable performance of Transformer-based generative AI, a comparison or integration with it
%will likely become inevitable.
%However, since it differs significantly from Dynamics RL,
%the author prefers to set it aside for now to avoid confusion.

This paper first introduces Dynamic RL
and then examines whether this entirely new type of RL functions effectively as reinforcement learning
in a simple memory-required task and a dynamic pattern generation task, compared with conventional RL.

\input{Theory}

\input{Simulations}

\section*{Acknowledgment}
First, I would like to express my gratitude to Prof.~Kazuyuki Aihara.
His suggestion in 1995 motivated me to connect reinforcement learning and chaos.
%I would also like to thank Prof. Kanakubo from the Shizuoka Inst. of Sci. \& Tech. for their impressive demonstration of chaotic itinerancy.
I would like to thank Prof.~Hiromichi Suetani for useful suggestions about chaotic dynamics from an expert.
I also thank the members of our former laboratory for their discussions and also sharing related simulation results.
In particular, Dr.~Toshitaka Matsuki discussed mainly the relationship between chaoticity and learning ability
and has inspired me a lot from various aspects.
Mr.~Yuki Tokumaru also gave me a big push by simulations and discussion in the early phase of this research.
This work was supported by JSPS KAKENHI Grant Number JP19300070, JP23500245, JP15K00360, JP20K11993
and also Kayamori Foundation of Informational Science Advancement K31-KEN-XXIV-539.
Especially the last one supported the author's independent research after his early retirement.

%\section*{Declaration of Generative AI and AI-Assisted Technologies in the Writing Process}
%During the preparation of this work, the author used ChatGPT and Gemini, in order to assist with improving his English writing.
%After using this tool, the author reviewed and edited the content as needed and takes full responsibility for the content of the publication.

%\section*{References}
\bibliographystyle{abbrvnat}
%\bibliographystyle{agsm}
%\bibliographystyle{apa}
%\bibliographystyle{apalike}
%\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
%\bibliographystyle{plain}
%\bibliography{mybibfile}
\bibliography{Reference}

\par
\par
\par

\appendix
\input{Appendix}

\end{document}