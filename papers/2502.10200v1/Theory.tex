\section{Learning Method}
As a base architecture for RL, the actor-critic model is used,
in which the actor outputs do not represent probabilities for actions
but instead represent continuous motor commands.
Dynamic RL is applied solely to the actor, while the critic is trained by conventional RL using BPTT \citep{PDP}
although ideally, all learning should be dynamic.
For clarity, each actor and critic consists of a separate RNN with sensor signals as inputs.
%Q-learning is more widely used.
%However, it is the learning for discrete actions, and some more process is required
%before getting the final motor commands.
%From the view of building autonomous learning agents,
%there remains the problem how the process is acquired through RL.
%On the other hand, the outputs of the actor in actor-critic can be dealt with as continuous motion signals.

%Figure \ref{fig:neuron_forward} shows a general static-type neuron model with $m$ inputs.
In each dynamic neuron, its internal state $u$ at time $t$ is derived
as the first-order lag of the inner product of the connection weight vector ${\bf w}=(w_1, ... , w_m)^\mathrm{T}$
and input vector ${\bf x}_t=(x_{1t}, ... , x_{mt})^\mathrm{T}$ where $m$ is the number of inputs as
\begin{equation}
u_t = \left(1-\frac{\Delta t}{\tau}\right)u_{t-1}+\frac{\Delta t}{\tau}{\bf w}\cdot{\bf x}_t
\label{Eq:internal_state}
\end{equation}
where $\tau$ is a time constant and $\Delta t$ is the step width, which is 1.0 in this paper.
For static-type neurons, the internal state $u$ is just the inner product as
\begin{equation}
u_t = {\bf w}\cdot{\bf x}_t.
\label{Eq:internal_state_static}
\end{equation}
%by setting $\tau=\Delta t$.
The inputs ${\bf x}_t$ can be the external inputs or the pre-synaptic neuron outputs at time $t$,
%which may be outputs of neurons.
but for the feedback connections, where the inputs come from the same or an upper layer,
they are the outputs of the pre-synaptic neuron at time $t-1$. 
The output $o_t$ is derived from the internal state $u_t$ as
\begin{equation}
o_t = f(U_t)=f(u_t+\theta)
\label{Eq:output}
\end{equation}
where $U_t=u_t+\theta$, $\theta$ is the bias, and $f(\cdot)$ is an activation function,
which is a hyperbolic tangent in this paper.

Dynamic RL controls the dynamics of the system, including RNN, directly by adjusting the sensitivity \citep{Sensitivity} in each neuron.
Sensitivity is an index for each neuron that is the Euclidian norm of the output gradient
with respect to the input vector ${\bf x}$.
It is defined as
%how a neuron is sensitive to a small change in its inputs.
%It is defined as the Euclidean norm of the output gradient with respect to the input vector ${\bf x}$ as
\begin{equation}
s(U; {\bf w}) = \|\nabla_{\bf x} o\| = f'(U)\|{\bf w}\|.
\label{Eq:sensitivity}
\end{equation}
Here, $\| {\bf v} \| = \sqrt{\sum_i^mv_i^2}$ for a vector ${\bf v}=(v_1, ..., v_m)^\mathrm{T}$.
In the form of a vector elements, the sensitivity is represented as
\begin{equation}
s(U; {\bf w}) = \sqrt{\sum_i^m \left( \frac{\partial o}{\partial x_i} \right)^2} = f'(U)\sqrt{\sum_i^m w_i^2}\ .
\label{Eq:sensitivity_non_vector}
\end{equation}
Sensitivity refers to the maximum ratio of the absolute value of the output deviation $do$
to the magnitude of the infinitesimal variation $d{\bf x}$ in the input vector space.
It represents the degree of contraction or expansion from the neighborhood around the current inputs
to the corresponding neighborhood around the current output through the neuron's processing.
In the previous work \citep{Sensitivity}, it was defined only for static-type neurons
(Eq.~(\ref{Eq:internal_state_static})).
In this study, the same definition is also applied to dynamic neurons (Eq.~(\ref{Eq:internal_state})),
assuming that the infinitesimal variation $d{\bf x}$ of the input ${\bf x}$
changes slowly enough compared to the time constant $\tau$.
%it is assumed that the infinitesimal deviation $d{\bf x}$ of the input ${\bf x}$
%is a constant vector near the time $t$.
%By solving the linear asymptotic equation as in Eq.~(\ref{Eq:internal_state}),
%the deviation $du$ of the internal state can be represented as
%\begin{equation}
%du_t \approx \left\{1+\left(1-\alpha\right)+\left(1-\alpha\right)^2+...\right\}\alpha{\bf w}\cdot d{\bf x}_t 
%= \sum_{i=0}^\infty (1-\alpha)^i\alpha{\bf w}\cdot d{\bf x}_t = {\bf w}\cdot d{\bf x}_t
%\end{equation}
%where $0.0 < \alpha = \frac{\Delta t}{\tau} \leq 1.0$.
%Then the gradient of the internal state $u$ with respect to the input ${\bf x}$ becomes
%\begin{equation}
% \|\nabla_{{\bf x}_t} u_t\| = \|{\bf w}\|,
%\end{equation}
%and we can derive Eq.~(\ref{Eq:sensitivity}) as well also for the dynamic neurons.
%if the activation function $f$ is a monotonically increasing function.

In the previous research \citep{Sensitivity},  the author's group proposed sensitivity adjustment learning (SAL).
SAL was applied to ensure the sensitivity of each neuron in parallel with gradient-based supervised learning.
This approach is beneficial not only for maintaining sensitivity during forward computation in the neural network
but also for avoiding diverging or vanishing gradients during backward computation.
Because Dynamic RL incorporates SAL and sensitivity-controlled RL (SRL), which is an extension of SAL for RL,
SAL will be explained first.

In SAL, the moving average of sensitivity $\bar{s}$ is computed first as
\begin{equation}
 \bar{s}_t \leftarrow (1-\alpha) \bar{s}_{t-1}  + \alpha s_t
 \label{Eq:Ave_sen}
\end{equation}
where $\alpha$ is a small constant, and this computation is performed across episodes.
When the average sensitivity $\bar{s}$ is below a predetermined constant $s_{th}$,
the weights and bias in each neuron are updated locally to the gradient direction of the sensitivity as
\begin{align}
\Delta {\bf w}_t &= \eta_{SAL}\frac{\Delta t}{\tau} \nabla_{\bf w} s(U_t; {\bf w})
%                        = \eta_{SAL}\frac{\Delta t}{\tau} \nabla_{\bf w} \{f'(U_t)\|{\bf w}\|\}
                        = \eta_{SRL}\frac{\Delta t}{\tau} \left( f'(U_t)\frac{\bf w}{\| {\bf w} \|} + \| {\bf w} \| \nabla_{\bf w} f'(U_t) \right)
\label{Eq:SAL_ORG}\\
%\end{equation}
%and
%\begin{equation}
\Delta {\theta}_t &= \eta_{SAL} \frac{\Delta t}{\tau}\frac{\partial s(U_t; {\bf w})}{\partial \theta}
%                          = \eta_{SAL} \frac{\Delta t}{\tau}\frac{\partial \{f'(U_t)\|{\bf w}\|\}}{\partial \theta}
                          = \eta_{SRL}\frac{\Delta t}{\tau} \| {\bf w} \| \frac{\partial f'(U_t)}{\partial \theta}.
\label{Eq:SAL_Bias_ORG}
\end{align}
where $\eta_{SAL}$ is the learning rate for SAL.
$\Delta t / \tau$ is multiplied to adjust the update to the neuron's time scale.
By expanding the equation with the activation function being hyperbolic tangent,
\begin{align}
\Delta {\bf w}_t &= \eta_{SAL}\frac{\Delta t}{\tau} (1-o_t^2) \left( \frac{\bf w}{\| {\bf w}\|} - 2o_t\|{\bf w}\|{\bf x}_t \right)
\label{Eq:SAL} \\
%\end{equation}
%\begin{equation}
\Delta {\theta}_t &= -2 \eta_{SAL}\frac{\Delta t}{\tau} o_t(1-o_t^2) ||{\bf w}||
\label{Eq:SAL_Bias}
\end{align}
are derived.
%, where
%\begin{equation}
%\frac{do}{dU} = f'(U) = \frac{dtanh(U)}{dU} =\frac{1}{\cosh^2(U)} = 1- o^2.
%\end{equation}

\begin{figure}[t]
\centerline{\includegraphics[scale=0.35]{DynamicRL.pdf}}
%\centerline{\includegraphics[scale=0.5, pagebox=cropbox, clip]{Task1.pdf}} 
\caption{Dynamic RL applies either SAL or SRL depending on the condition in each neuron.}
\label{fig:DynamicRL}
\end{figure}
In Dynamic RL proposed here, as shown in Fig.\ref{fig:DynamicRL},
SAL is applied when the moving average of the sensitivity $\overline{s}$
is less than a constant $s_{th}$, otherwise sensitivity-controlled RL (SRL) is applied in each neuron.
%When not less
SAL always tries to increase the sensitivity in each neuron,
but whether SRL tries to increase or decrease the sensitivity depends on the temporal difference (TD) error ${\hat r}$ as
\begin{align}
\Delta {\bf w}_t &= -\eta_{SRL}\frac{\Delta t}{\tau} \hat{r}_t \nabla_{\bf w} s(U_t; {\bf w})\\
%\label{Eq:SRL} \\
%\end{equation}
%\begin{equation}
\Delta \theta_t &= -\eta_{SRL}\frac{\Delta t}{\tau} \hat{r}_t \frac{\partial s(U_t; {\bf w})}{\partial \theta}
%\label{Eq:SRL_Bias}
\end{align}
where $\eta_{SRL}$ is the learning rate for SRL.
TD error is computed as
\begin{equation}
\hat{r}_t = \gamma C_{t+1} + r_{t+1} - C_t = \gamma\left(C_{t+1}-\frac{C_t-r_{t+1}}{\gamma}\right)
\label{Eq:TDerr}
\end{equation}
where $\gamma\ (0.0<\gamma<1.0)$ is the discount factor, $C_t$ is the critic output (state value),
and $r_t$ is the reinforcement signal, which can be a reward or a penalty, at time $t$.
As the basic concept summarized in Fig.\ref{fig:BasicConcept},
when TD error is positive, in other words, the new critic (state value) $C_{t+1}$ is greater than the expected value
$\frac{C_t-r_{t+1}}{\gamma}$,
RL reduces the sensitivity to reinforce the reproducibility.
When it is negative, i.e., the new state value is less than expected,
RL makes the sensitivity greater to reinforce the exploratory nature.
This is expected to control the local convergence or divergence, depending on how good or bad the state is.

\begin{figure}[ht]
\centerline{\includegraphics[scale=0.28]{BasicConcept.pdf}}
%\centerline{\includegraphics[scale=0.5, pagebox=cropbox, clip]{Task1.pdf}} 
\caption{Basic concept of Dynamic RL (or more specifically, SRL) proposed in this paper.}
\label{fig:BasicConcept}
\end{figure}

Upon expansion, we obtain,
\begin{align}
\Delta {\bf w}_t &= -\eta_{SRL}\frac{\Delta t}{\tau} \hat{r}_t \left( f'(U_t)\frac{\bf w}{\| {\bf w} \|} + \| {\bf w} \| \nabla_{\bf w} f'(U_t) \right)\\
\Delta \theta_t &= -\eta_{SRL}\frac{\Delta t}{\tau} \hat{r}_t \| {\bf w} \| \frac{\partial f'(U_t)}{\partial \theta}.
\end{align}
By further expanding as the activation function $f(\cdot)$ being $\tanh$,
\begin{align}
\Delta {\bf w}_t &= - \eta_{SRL}\frac{\Delta t}{\tau} \hat{r}_t (1-o_t^2) \left( \frac{\bf w}{\| {\bf w}\|} - 2o_t\|{\bf w}\|{\bf x}_t \right)
\label{Eq:SRL}\\
\Delta \theta_t &= 2\eta_{SRL}\frac{\Delta t}{\tau} \hat{r}_t o_t (1-o_t^2) \|{\bf w}\|.
\label{Eq:SRL_Bias}
\end{align}
%The equation is rewritten in the element form as
%\begin{equation}
%\Delta w_i = \eta_{SAL} \frac{(1-o^2) \left\{ w_i -2ox_i \sum_k w_k^2 \right\}}{\sqrt{\sum_k w_k^2}}.
%\end{equation}
%The author calls the first term $-\eta \hat{r} (1-o^2){\bf w}/\left|{\bf w}\right|$ the linear term,
%which is originated from $|{\bf w}|$ in Eq.~(\ref{Eq:sensitivity}).
%The second term $2 \eta \hat{r} (1-o^2) o|{\bf w}|{\bf x}$ is called non-linear term,
%which is originated from $f'(x)$ in Eq.~(\ref{Eq:sensitivity}).
%Different from the case of weight, bias $\theta$ cannot increase the sensitivity directly, but
%can increase it indirectly by updating the bias so that the value $U$ becomes closer to 0.0.
Notably, this computation can be done locally in each neuron except for receiving the TD errors.
Furthermore, since the dynamics are generated not only by the loops inside the RNN
but also influenced by the loops that are formed with the outside world,
this learning can be applied to all the neurons, including those outside the loop in the RNN, including the output neurons.

In the following simulations, the proposed RL is compared to the conventional RL using BPTT.
%Then, the conventional RL used here is explained next.
Now many techniques have been proposed to improve the performance, but for a pure comparison of base methods,
simple learning using gradient-based BPTT is employed.
In Dynamic RL, the motor command vector ${\bf M}_t$
is a function ${\bf M}(\cdot)$ of the actor output vector ${\bf A}_t$ as ${\bf M}_t = {\bf M}({\bf A}_t)$,
%is identical to the actor output vector ${\bf A}_t$,
but in the conventional RL, since a random noise vector ${\bf \epsilon}_t$ is added to the actor output vector
as explorations, the actual motor command vector ${\bf M}_t$ is expressed as
\begin{equation}
%{\bf M}_t = {\bf A}_t + {\bf \epsilon}_t
{\bf M}_t = {\bf M}({\bf A}_t + {\bf \epsilon}_t)
\end{equation}
For conventional RL, training signals for the actor network are derived as
\begin{equation}
{\bf A}_{train,t} = {\bf A}_t + \hat{r}_t {\bf \epsilon}_t .
\label{Eq:ConvRL}
\end{equation}
Then, the actor network is trained based on the BPTT method by these training signals.
In this paper, it learned 10 or 20 steps backward in time, depending on the task.
While, in the Dynamic RL, since no calculation going back through time is necessary,
the computational cost is considerably smaller than in the case of conventional RL.

\begin{figure}[t]
\centerline{\includegraphics[scale=0.31]{ConvRL.pdf}}
%\centerline{\includegraphics[scale=0.5, pagebox=cropbox, clip]{Task1.pdf}} 
\caption{A conceptual diagram of conventional RL.
RL aims to control the actor output vector based on the TD error.
It does not utilize information about time changes in the RNN's state and is closed only at each step.}
\label{fig:ConvRL}
\vspace{5mm}
\centerline{\includegraphics[scale=0.31]{DYN_RL.pdf}}
%\centerline{\includegraphics[scale=0.5, pagebox=cropbox, clip]{Task1.pdf}} 
\caption{A conceptual diagram of Dynamic Reinforcement Learning (RL).
RL aims to control the convergence or divergence of the flow around state transitions according to the TD error
by controlling the sensitivity in each neuron.}
\label{fig:DYN_RL}
\end{figure}
%As described in the Introduction, 
Dynamic RL has a significant difference in the way of learning
from conventional RL.
For better understanding, the author attempts to illustrate their differences with diagrams at the expense of accuracy.
In the conventional RL, external noise ${\bf \epsilon}$  is added to the actor output vector ${\bf A}$.
As shown in Fig.~\ref{fig:ConvRL}, according to Eq.~(\ref{Eq:ConvRL}),
if the value function is better than expected, i.e., if the TD error $\hat{r}$ is positive,
the network is trained to move the output vector ${\bf A}$ to the direction of the noise ${\bf \epsilon}$.
By contrast, if the TD error $\hat{r}$ is negative, the network is trained to move the output vector ${\bf A}$ to the opposite direction.
This RL does not use the temporal change in the outputs or network states; rather, it considers only the outputs at that moment in time.
All the weights and biases are updated to move the output vector with the help of the gradient method
using error backpropagation even through time.

On the other hand, Dynamic RL does not aim to move the state or output directly,
but as shown in Fig.~\ref{fig:DYN_RL}, it aims to control the convergence or divergence of the neighborhood
around the state transition by changing each neuron's sensitivity depending on the TD error.
The concept of controlling dynamics can also be applied to other types of learning, such as supervised learning.
The author refers to it as Dynamic Learning from a broader perspective and will discuss it in the subsection
\ref{subsec:Future}.
%Therefore, the learning in the neurons that are not included in any loop in the RNN
%also influences the dynamics.

%In this paper, to improve the performance further,
%another gradient-based learning is applied to the output neurons referring to \citep{Hoerzer,Matsuki}.
%Here, the deviation from the moving average is computed.
%\begin{equation}
%\tilde{o}_t = {o}_t - \bar{o}_t
%\end{equation}
%where $\bar{o}_t = 0.8 \bar{o}_{t-1} + 0.2 {o}_t$, and the weight vector is updated
%by the product of it and TD error as
%\begin{equation}
%\Delta {\bf w}_t = \eta_{grad} \hat{r}_t \tilde{o}_t {\bf x}_t.
%\label{Eq:GradL}
%\end{equation}
%The biases are updated as
%\begin{equation}
%\Delta {\bf \theta}_t = \eta_{grad} \hat{r}_t \tilde{o}_t.
%\label{Eq:GradL_Bias}
%\end{equation}
This concept should also be introduced to the critic network,
but here, conventional learning is used for the critic, regardless of how the actor network is trained.
The training signal is derived as
\begin{equation}
C_{train,t} = \gamma C_{t+1} + r_{t+1} = C_t + \hat{r}_t,
\label{Eq:C_train}
\end{equation}
and the critic network is always trained with BPTT using this training signal.

In Dynamic RL, the network outputs were often saturated (close to $1.0$ or $-1.0$ in hyperbolic tangent),
and it is difficult to perform fine and smooth control.
To avoid saturation, the regularization was applied only to the output layer's connection weights in the actor network as
\begin{equation}
  \Delta {\bf W} = -\eta_{reg} {\bf W}.
  \label{Eq:Regularize}
\end{equation} 
This learning was applied in both Dynamic and conventional RL cases.
% for fair comparison.
 
Furthermore, one more technique used in this paper is ``critic raising''.
When an agent cannot reach its goal for a long time,
since the critic output becomes small, the gradient of the critic also becomes small.
Therefore, referring to the ``optimistic initial value'' \citep{Sutton1998},
when the moving average $\bar{C}$ of the critic output $C$ is less than a constant $C_{th}$,
the bias of the output layer in the critic network is increased to raise the critic value as
\begin{equation}
  \Delta \theta_t = \eta_{raise} (C_{th}-\bar{C}_t)
  \label{Eq:Raise_Critic}
\end{equation} 
where $\bar{C}$ is the moving average of the critic output $C$ as 
\begin{equation}
 \bar{C}_t \leftarrow (1-\beta) \bar{C}_{t-1}  + \beta C_t
 \label{Eq:Ave_C}
\end{equation}
where $\beta$ is a small constant, and this computation is performed across episodes.
%, but except for the preparation steps,
%in which only the RNN was computed without actually moving for preparation.
%This was also applied in both Dynamic and conventional RL cases.
