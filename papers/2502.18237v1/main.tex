\documentclass{article} %
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{graphicx}  
\usepackage{caption} 
\usepackage{bbm} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{enumitem,kantlipsum}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}


\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{patterns}
\usetikzlibrary{math}

\newcommand{\ktdt}[1]{\textcolor{black}{#1}}
\newcommand{\ozm}[1]{\textcolor{black}{#1}}
\newcommand{\teba}[1]{{\color{black} #1}}
\newcommand{\rebuttal}[1]{\textcolor{black}{#1}}


\newcommand{\layer}{Disjunctive Refinement Layer}
\newcommand{\lsymb}{\text{DRL}}
\newcommand{\sample}{\tilde{x}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\pp}{\stackrel{+}{+}}
\newcommand\msmall[1]{\mbox{\scriptsize\ensuremath{#1}}}
\newcommand{\spm}{{\msmall{\pm}}}
\newcommand{\leftbh}{l_i^{\Phi}(\sample)}
\newcommand{\rightbh}{r_i^{\Phi}(\sample)}
\newcommand{\satleftb}{\hat l_i(\sample)}
\newcommand{\satrightb}{\hat r_i(\sample)}
\newcommand{\ub}{\mathit{ub}}
\newcommand{\lb}{\mathit{lb}}
\newcommand{\plusplus}{{\rotatebox{90}{${\msmall{\text{+}\mkern-5mu\text{+}}}$}}}

\newcommand{\leftb}{l_i^\Psi}
\newcommand{\rightb}{r_i^\Psi}
\newcommand{\leftpi}[1]{l_i^{#1}}
\newcommand{\rightpi}[1]{r_i^{#1}}
\newcommand{\leftp}{\leftpi{\Pi}}
\newcommand{\rightp}{\rightpi{\Pi}}
\newcommand{\CPres}{\mathit{CPres}}
\newcommand{\Pisi}{\widetilde{\Pi}_{i}}


\newcommand{\phishing}{URL}
\newcommand{\botnet}{Botnet}
\newcommand{\faults}{FSP}
\newcommand{\wids}{WiDS}
\newcommand{\lcld}{LCLD}
\newcommand{\news}{News}
\newcommand{\heloc}{Heloc}
\newcommand{\cervical}{CCS}
\newcommand{\house}{House}

\newcommand{\fone}{\text{F1}}
\newcommand{\wfone}{\textsl{w}F1}
\newcommand{\auc}{\text{AUC}}
\newcommand{\mae}{\text{MAE}}
\newcommand{\rmse}{\text{RMSE}}
\newcommand{\cvr}{\text{CVR}}
\newcommand{\cvc}{\text{CVC}}
\newcommand{\scvc}{\text{sCVC}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}{Example}
\newtheorem{definition}[theorem]{Definition}


\newcommand{\wgan}{WGAN}
\newcommand{\tablegan}{TableGAN}
\newcommand{\ctgan}{CTGAN}
\newcommand{\tvae}{TVAE}
\newcommand{\goggle}{GOGGLE}
\newcommand{\gdgm}{P-DGM}

\newcommand{\pac}{PAC}
\newcommand{\sgd}{SGD}
\newcommand{\adam}{Adam}
\newcommand{\rmsprop}{RMSProp}


\title{Beyond the convexity assumption: \\ Realistic tabular data generation under quantifier-free real linear constraints}


\author{Mihaela C\u{a}t\u{a}lina Stoian\\
University of Oxford\\
\texttt{mihaela.stoian@cs.ox.ac.uk}\\
\And
Eleonora Giunchiglia\\
Imperial College London\\
\texttt{e.giunchiglia@imperial.ac.uk}
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
Synthetic tabular data generation has traditionally been a challenging problem due to the high complexity of the underlying distributions that characterise this type of data. Despite recent advances in deep generative models (DGMs), existing methods often fail to produce realistic datapoints that are well-aligned with available background knowledge.
In this paper, we address this limitation by introducing Disjunctive Refinement Layer (\lsymb), a novel layer designed
to enforce the alignment of generated data with the background knowledge specified in user-defined constraints.
\lsymb{} is the first method able to automatically make deep learning models inherently compliant with constraints as expressive as quantifier-free linear formulas, which can define non-convex and even disconnected spaces. 
Our experimental analysis shows that \lsymb{} not only guarantees constraint satisfaction but also improves efficacy in downstream tasks. Notably, when applied to DGMs that frequently violate constraints, \lsymb{} eliminates violations entirely. Further, it improves performance metrics by up to 21.4\% in F1-score and 20.9\% in Area Under the ROC Curve, thus demonstrating its practical impact on data generation. %
\end{abstract}


\section{Introduction}

The problem of tabular data generation is a critical area of research, driven by its numerous practical applications across various domains. High-quality synthetic data offers solutions to pressing challenges such as data scarcity~\citep{Choi2017_scarcity}, bias in unbalanced datasets~\citep{breugel2021_fairness}, and the general need for 
privacy protection %
\citep{lee2021invertible}. 
However, due to the varied nature of the data distributions in the tabular domain---which are often multi-modal, and present complex dependencies among features---it is difficult to create models able to 
generate realistic data.  
Indeed, no matter the Deep Generative Model (DGM) used, when synthetic datapoints are tested for alignment with the available background knowledge, they frequently fail such a test. 
Even when considering simple knowledge 
like {\sl``the feature representing the maximum recorded level of hemoglobin should be greater than or equal to the one representing its minimum"}, 
DGMs often generate datapoints violating it.  So far, this problem has only been solved by either rejecting the non-aligned samples, or by adding a layer to the DGM that restricts its output space to coincide with the one defined by linear inequalities~\citep{stoian2024}. However, while the first solution is not feasible in the presence of a high violation rate, the second is only available when the  knowledge can be captured by linear inequalities, which  have very limited~expressivity.


\begin{wrapfigure}{r}{0.29\textwidth}
\centering
\vspace{-1.05cm}
\begin{minipage}{0.1\textwidth}
\centering\captionsetup[subfigure]{justification=centering}
\vspace{0.6cm}\resizebox{0.95\textwidth}{!}{\input{tikz/convex.tikz}}
\vspace{-0.12cm}
\subcaption{}
\end{minipage}
\hfill
\begin{minipage}{0.18\textwidth}
\centering\captionsetup[subfigure]{justification=centering}
\resizebox{0.95\textwidth}{!}{\input{tikz/non_convex_overall.tikz}}
\vspace{-0.1cm}
\subcaption{}
\end{minipage}
\vspace{-0.3cm}
\caption{Example of spaces defined by (a) a set of linear inequalities and (b) a set of  QFLRA formulas.\vspace{-0.6cm}}\label{fig:spaces_examples}
\end{wrapfigure} 
In this paper, we propose a novel layer---called \layer{} ($\lsymb$)---able to constrain any DGM output space according to background knowledge expressed as Quantifier-Free Linear Real Arithmetic (QFLRA) formulas. QFLRA formulas 
can capture any relationship over the features that can be represented as a combination of conjunctions, disjunctions and negations of linear inequalities. Thanks to their expressivity, QFLRA formulas can define spaces that are not only non-convex but can also be disconnected.
On the contrary, linear inequalities can only capture convex output spaces.
See Figure~\ref{fig:spaces_examples} for an example of spaces defined by linear inequalities and QFLRA formulas.
While linear inequalities establish a single lower and upper bound (if existent) for each feature, QFLRA formulas define multiple intervals where the background knowledge holds, each with its own boundaries. This significantly increases the complexity of the problem, as compiling knowledge into \lsymb{} not only requires keeping track of these intervals but also deriving the intricate hidden interactions among variables. 
\begin{example}\label{ex:intro}
The knowledge: ``The value of $x_5$ should be always at least $x_1$, and if greater than  $x_2$ then it should also be at least equal to $x_3$. In any case,  $x_5$ should never be greater than $x_4$'', which cannot be expressed  by a set of linear inequalities, corresponds to the QFLRA formula:
\begin{equation}
    \label{ex:qfrla}
(x_5 \ge x_1) \land    ((x_5 > x_2) \to (x_5 \ge x_3)) \land  (x_5 \le x_4).
\end{equation}
 Moreover, this formula entails other hidden relations among  the variables such as, e.g., $\neg(x_1 > x_4)$.%
\end{example}
To derive such additional hidden relations, we developed a novel variable elimination method which generalises the analogous procedure for systems of linear inequalities based on the Fourier-Motzkin result (see, e.g., \citep{dechter1999}).
Once compiled, by definition, \lsymb{} (i) guarantees the satisfaction of the constraints, (ii) can be seamlessly added to the topology of any neural model, (iii) allows the backpropagation of the gradients at training time, (iv) performs all the computations in a single forward pass (i.e., no cycles), and (v) given a sample generated by a DGM, it returns a new one that is optimal with respect to the original (intuitively, which minimally differs from the original sample while taking into account the user preferences on which features should be changed first). 
Our experimental analysis also shows that adding \lsymb{} to DGMs  improves their machine learning efficacy~\citep{xu2019_CTGAN} on a range of different scenarios. This is the most widely used performance measure for evaluating the quality of synthetic data, as it assesses how useful the generated data is for downstream tasks. In particular, we considered five DGMs,  
added \lsymb{} into their topology and got improvements for all datasets of up to
21.4\%, 20.5\%, and 20.9\%
in terms of F1-score, weighted F1-score, and Area Under the ROC Curve, respectively.
Finally, our experiments demonstrate a strong need for a method like ours.
Indeed, DGMs generate synthetic datapoints violating the background knowledge more often than expected. In 13 out of 25 scenarios, the DGMs produced datasets with over 50\% datapoints violating the constraints, and in five cases this reached 100\%. %

\paragraph{Main contributions:} (i) We propose the first-ever layer that can be integrated into any DGM to enforce background knowledge expressed as QFLRA formulas.
This required generalising the Fourier-Motzkin variable elimination procedure in order to handle disjunctions of  linear inequalities.
(ii) We show experimentally how integrating our layer in DGMs improves their machine learning efficacy, even when the constraints define a possibly non-convex and disconnected space.










\section{Problem Definition and Notation}


{\sl Constrained generative modelling} is defined as the problem of learning the parameters $\theta$ of a generative model, given an unknown distribution $p_X$ over $X \in \mathbb{R}^D$, a training dataset $\mathcal{D}$ consisting of $N$ i.i.d. samples drawn from $p_X$, and formally expressed background knowledge about the problem---stating which samples are admissible and which are not---such that (i) the model distribution $p_{\theta}$ approximates $p_X$, and (ii) the sample space of $p_{\theta}$ is aligned with what is stated in the background knowledge.
As described in the introduction, so far this problem has only been solved by either rejecting the non-aligned samples or by including a layer into the DGM that restricts its output space to coincide with the one defined by the linear inequalities~\citep{stoian2024}.


In this paper, we allow for background knowledge expressed as a set of formulas, each being a disjunction of linear inequalities. 
This enables us to capture any relationship among features which can be represented as Quantifier-Free Linear Real Arithmetic (QFLRA) formulas. Indeed,  through syntactic manipulation using De Morgan's laws and the mathematical properties of linear inequalities, any knowledge formulated as a combination of conjunctions, disjunctions, and negations of linear inequalities can be rewritten as a set of disjunctions over linear inequalities. 
Formally, we consider a set $\Pi$ of constraints, where a {\sl constraint} is a  disjunction of $n_\Psi \in \mathbb{N}$ linear inequalities of the form:
\begin{equation}
\label{eq:constraint}
\Psi = \Phi_1 \vee \Phi_2 \vee \dots \vee \Phi_{n_\Psi},
\end{equation}
 where each $\Phi_i$ is a linear inequality over the set of variables $\mathcal{X} = \{x_k \mid k = 1, \dots, D\}$,  each variable uniquely corresponding to a feature in the dataset. We assume each linear inequality has~form:
\begin{equation}
\label{eq:inequality}
\sum_k w_k x_k + b  \ge 0,
\end{equation}
with $w_k \in \mathbb{R}$, $b \in \mathbb{R}$, and $x_k$ ranging over $\mathbb{R}$. When $w_i \neq 0$, we say that $x_i$ {\sl occurs} in (\ref{eq:inequality}), and that it occurs {\sl positively} if $w_i > 0$ and {\sl negatively} otherwise.


For an easy formulation of the problem and its solution, given two linear expressions $\varphi = \sum_k w_k x_k + b$ and $\varphi' = \sum_k w'_k x_k + b'$, we write, e.g., $(\varphi + \varphi')$ for the linear expression $\sum_k (w_k+w'_k) x_k + (b+b')$, and similarly for $(\varphi - \varphi')$ and $\varphi/w$ if $w \in \mathbb{R}\setminus\{0\}$. %
We will also express the linear inequality~(\ref{eq:inequality}) as
$w_ix_i + \varphi$, by this implicitly assuming $w_i \neq 0$ and that $x_i$ does not occur in $\varphi$, i.e., that $\varphi = \sum_{k \not = i} w_k x_k + b$.
Finally,  we also write $\varphi \ge \varphi'$ (resp. $\varphi \le \varphi'$) as abbreviations for $\varphi - \varphi' \ge 0$ (resp. $\varphi' - \varphi \ge 0$).

Given a DGM with distribution $p_{\theta}$, a {\sl sample} $\sample \sim p_{\theta}$ is an assignment to the variables in $\X$, and $\sample_k$ indicates the value assigned by $\sample$ to the variable $x_k$. 
We say that a sample $\sample$ {\sl satisfies} 
\begin{itemize}
\item the linear inequality (\ref{eq:inequality}) if $\sum_k w_k\sample_k + b \ge 0$, %
\item the constraint $\Psi$ with form (\ref{eq:constraint}) if 
$\Phi_i$ is satisfied by $\sample$ for some $i = 1, \ldots, n_{\Psi}$, and 
\item a set $\Pi$ of constraints if $\sample$ satisfies all the constraints in $\Pi$.
\end{itemize}
Further, we associate to each linear inequality $\Phi$, (resp. constraint $\Psi$, resp. set of constraints $\Pi$)  the set  $\Omega(\Phi)$ (resp. $\Omega(\Psi)$, resp. $\Omega(\Pi)$) of  the points in $\mathbb{R}^D$ that satisfy $\Phi$ (resp. $\Psi$, resp. $\Pi$). Clearly, 
$\Omega(\Phi)$,  $\Omega(\Psi)$ and $\Omega(\Pi)$ define a subspace of $\mathbb{R}^D$, and have the following properties:
\begin{enumerate}
    \item $\Omega(\Phi)$ is non-empty and convex,
    \item $\Omega(\Psi)$ is non-empty but may be non-convex and also disconnected, and
    \item $\Omega(\Pi)$  may be empty, non-convex and also disconnected,
    \end{enumerate}
all the above assuming some variable occurs in $\Phi$, $\Psi$ and $\Pi$.
A linear inequality $\Phi$ (resp. a constraint $\Psi$, resp. a set of constraints $\Pi$) is {\sl violated} by a sample $\sample$ if $\sample$ does not belong to the corresponding set $\Omega(\Phi)$ (resp. $\Omega(\Psi)$, resp. $\Omega(\Pi)$).
A linear inequality $\Phi$ (resp. a constraint $\Psi$, resp. a set of constraints $\Pi$) is {\sl satisfiable} if the corresponding set $\Omega(\Phi)$ (resp. $\Omega(\Psi)$, resp. $\Omega(\Pi)$) is not empty.
Notice that, for the sake of simplicity, we do not consider strict  inequalities (i.e., inequalities with $>$). From a theoretical perspective, the entire theory can be easily generalised to consider them. From a practical perspective, for any computing system of choice,  we can simply rewrite each strict inequality of the following form:
$\sum_k w_k x_k + b  > 0 \text{ as } \sum_k w_k x_k + b - \epsilon \ge 0,
$
where
$\epsilon > 0$ denotes the desired precision of the representation, taking into account the limitations of floating-point accuracy.  Finally, we represent each constraint $\Psi \in \Pi$ of form (\ref{eq:constraint}) also as the set $\{\Phi_1,\Phi_2,\ldots,\Phi_{n_\Psi}\}$. With this notation, $\Pi$ is a set of sets of linear inequalities. Hence, the linear inequalities in a set should be interpreted as disjunctively defining a constraint in $\Pi$, while the constraints are to be interpreted as conjunctively defining $\Pi$.

\section{\layer}
Given a finite set of constraints $\Pi$ and a DGM, we show how to build a layer with all the desired properties stated in the introduction. \ktdt{In Appendix~\ref{app:visualizations} we visualize how to add our \lsymb{} to each of the DGMs considered in the experimental analysis.} Before illustrating the general case, in the following subsection we assume $\Pi$ is a finite set of constraints in a single variable $x_i$.

\subsection{Single Variable Case} 

Each constraint $\Psi$ of form (\ref{eq:constraint}) 
defines a single {\sl left boundary} $\leftb$ and a single {\sl right boundary} $\rightb$ for the variable $x_i$:
\footnote{We use the ``left'' and ``right'' terminology because in a non-vacuous constraint we have $\leftb < \rightb$. We assume the function $\min(\mathcal{S})$ over a finite set $\mathcal{S}$ of values in $\mathbb{R}$ to be defined as $\min(\emptyset) = +\infty$, and $\min(\{v\}\cup \mathcal{S}') = v$ if $v \leq \min(\mathcal{S}')$ and $\min(\mathcal{S}')$ otherwise. Analogously for the function $\max(\mathcal{S})$.}
\begin{equation}\label{eq:left_right_bounds}  
    \leftb = \max_{(w_ix_i +{\varphi} \ge 0) \in \Psi: w_i < 0} \Big(-\frac{\varphi}{w_i}\Big), \qquad \qquad \rightb = \min_{(w_ix_i + {\varphi} \ge 0) \in \Psi : w_i > 0} \Big(-\frac{\varphi}{w_i}\Big).
\end{equation}
Assuming $\Psi$ contains a linear inequality in which $w_i \neq 0$, a sample $\sample$ satisfies $\Psi$ if and only if either $\sample_i \le \leftb$  or
$\sample_i \ge \rightb$, as represented in Figure~\ref{fig:single_constr}. As the Figure clearly shows, $\Omega(\Psi)$ is already  non-convex whenever
$\leftb \neq -\infty$, $\rightb \neq +\infty$ and $\leftb < \rightb$. 
When considering a set $\Pi$ with multiple
\begin{wrapfigure}{r}{0.28\textwidth}
    \vspace{-0.08cm}
   \includegraphics[trim={16cm 11cm 27cm 19.5cm},clip,width=\linewidth]{img/single_constr.pdf}
   \setlength{\abovecaptionskip}{-6pt}
   \caption{Visualisation of left and right boundaries defined by constraint $\Psi$. The green regions correspond to the values of $x_i \in \Omega(\Psi)$.}
   \vspace{-0.6cm}
   \label{fig:single_constr}
\end{wrapfigure}
  constraints in $x_i$, we may arrive at a set $\Omega(\Pi)$ that is the union of up to $|\Pi|+1$ disjoint intervals. %

In general, computing the intervals requires finding the satisfying boundaries and then ordering them.
Luckily, given a sample $\sample$ violating some constraint in $\Pi$,
we are only interested in setting $\lsymb(\sample)_i$ equal to the  bound that satisfies $\Pi$ and that is at minimal Euclidean distance from $\sample_i$. To this end, we first define the {\sl closest satisfying left and right boundary for $\sample_i$}  as: 
\begin{equation}
    \leftp(\sample) = \max_{\Psi \in \Pi}(\{\leftb :  \sample_i > \leftb, \leftb \in \Omega(\Pi)\}), \quad
\rightp(\sample) = \min_{\Psi \in \Pi}(\{\rightb :  \sample_i < \rightb, \rightb \in \Omega(\Pi)\}),
\end{equation}
respectively.
Then, for $k\neq i$, $\lsymb(\sample)_k = \sample_k$ and %
\begin{equation}
    \lsymb(\sample)_i = 
    \begin{cases}
        \sample_i &\qquad \text{if } \sample \in \Omega(\Pi), \\
        \leftp(\sample) &\qquad \text{if } \sample \not\in \Omega(\Pi) \text{ and } |\sample_i - \leftp(\sample)| < |\sample_i - \rightp(\sample)|, \\
        \rightp(\sample) &\qquad \text{otherwise}.
    \end{cases}
\end{equation}
By construction,
$\lsymb(\sample)$ satisfies the constraints in $\Pi$ and is {\sl optimal w.r.t. $\sample$}: there does not exist a sample satisfying $\Pi$ with smaller  Euclidean distance from $\sample$.


\begin{lemma}\label{lemma:single_var}
    Let $\Pi$ be a finite and satisfiable set of constraints in a single variable $x_i$. For every sample $\sample$, $\lsymb(\sample)$ satisfies $\Pi$ and is optimal w.r.t. $\sample$.
\end{lemma}
The proof of the Lemma can be found in Appendix~\ref{proof:lemma_single_var}.

\begin{minipage}{0.65\textwidth}
\begin{example}\label{ex:one_dim}
Let 
 $\Pi$ be the set of constraints $\{\Psi_1, \Psi_2, \Psi_3\}$ over the unique variable $x_5$, with
$\Psi_1,\Psi_2$ and $\Psi_3$  as shown 
in Figure~\ref{fig:example_bounds}. 
Then, $l_5^{\Psi_1} = -\infty, l_5^{\Psi_2} = b,  l_5^{\Psi_3} = d, 
r_5^{\Psi_1} = a,  r_5^{\Psi_2} = c,  r_5^{\Psi_3} = +\infty$. Depending on the value of $\sample_5$, we get correspondingly different values for
$\lsymb(\sample)_5$. In particular,
\end{example}
\vspace{-0.1cm}
\end{minipage}
\begin{minipage}{0.34\textwidth}
\vspace{-0.12cm}
    \includegraphics[trim={16cm 10.5cm 16cm 15cm},clip,width=\linewidth]{img/example_1_var_new.pdf}
    \setlength{\abovecaptionskip}{-14pt}
   \captionof{figure}{Constraints for Ex.~\ref{ex:one_dim}. %
   }
   \vspace{-0.1cm}
   \label{fig:example_bounds}
\end{minipage}
\begin{enumerate}[leftmargin=*]
\begin{multicols}{2}
    \item \textit{if $\sample_5 < a$ then $\lsymb(\sample)_5 = a$,}
    \item \textit{if $a \le \sample_5 \le b$ then $\lsymb(\sample)_5 = \sample_5$,}
    \item \textit{if $b < \sample_5 < (b+c)/2$ then $\lsymb(\sample)_5 = b$,}
    \item \textit{if $(b+c)/2 \le \sample_5 < c$ then $\lsymb(\sample)_5 = c$,}
    \item \textit{if $c \le \sample_5 \le d$ then $\lsymb(\sample)_5 = \sample_5$,}
    \item \textit{if $\sample_5 > d$ then $\lsymb(\sample)_5 = d$.}
\end{multicols}
\end{enumerate}
\vspace{-0.3cm}
\textit{Independently from the value of $\sample_5$, $\lsymb(\sample)_5$ satisfies the constraints and is optimal w.r.t. $\lsymb(\sample)_5$.}
\subsection{General Case}
Among the desiderata for our $\lsymb$, we have that all the necessary computations need to be done in a single forward pass. %
To this end, we 
consider a {\sl variable ordering} $x_1;x_2;\ldots;x_D$ corresponding to the order of computation of the features. 
The ordering can be arbitrarily selected or, more appropriately, may reflect the user preferences on which features should be changed first when the sample violates the constraints. Indeed, the value of each feature $x_i$ will be computed taking into account the values of the features $x_1,\ldots,x_{i-1}$, the latter considered immutable. 
To make this possible, when building the layer, we need to ensure that the chosen value for the variables  $x_j$, with $j < i$, guarantees the existence of a value for $x_i$ satisfying the constraints. 
Starting from $\Pi_D = \Pi$ and $i=D$, this amounts to deriving a finite set $\Pi_{i-1}$ of constraints in the variables 
$x_1, x_2, \ldots, x_{i-1}$ whose conjunction is logically equivalent to $\exists x_i \bigwedge_{\Psi \in \Pi_i} \Psi$. This entails that for every value of $x_1, x_2, \ldots, x_{i-1}$ satisfying $\Pi_{i-1}$ there must exist a value for $x_i$ satisfying $\Pi_i$, or alternatively, that each assignment to $x_1, x_2, \ldots, x_{i-1}$ and satisfying $\Pi_{i-1}$ can be extended to satisfy also $\Pi_i$.

In order to define such set $\Pi_{i-1}$,  given two constraints 
$
\Psi = (\bigvee_{k=1}^n(w_k x_i + \varphi_k \ge 0) \vee \Phi)$ and
$\Psi' = (\bigvee_{j=1}^m(w'_j x_i + \varphi'_j \ge 0) \vee \Phi')
$,
with $w'_1,\ldots,w'_m < 0 < w_1,\ldots,w_n$ and $m,n\ge 1$, we define the {\sl cutting planes (CP) resolution rule between $\Psi$ and $\Psi'$ on $x_i$} to be:
\begin{equation}\label{eq:res_rule}
    \frac{\bigvee_{j=1}^m(w'_j x_i + \varphi'_j \ge 0) \vee \Phi' 
    \qquad 
    \bigvee_{k=1}^n(w_k x_i + \varphi_k \ge 0) \vee \Phi}
{\bigvee_{j=1}^m\bigvee_{k=1}^n({\varphi_k}/{w_k} - {\varphi'_j}/{w'_j} \ge 0) \vee \Phi \vee \Phi'}.
\end{equation}
In the above rule, $\Psi$ and $\Psi'$ are the {\sl premises}, and the formula below the line is the {\sl conclusion} denoted with
$\CPres_i(\Psi,\Psi')$. 
This rule, which can be derived from the standard propositional and CP rules defined, e.g., in \citep{DBLP:journals/jsyml/Krajicek98}, is sound for any possible $\Phi$ and $\Phi'$. Despite this, 
we assume that $\Phi'$ (resp. $\Phi$) does not contain negative (resp. positive) occurrences of $x_i$. As we will see, it is possible to impose much stronger conditions (defined later) on the applicability of the rule, still enabling the derivation of a set of constraints $\Pi_{i-1}$ with the desired properties.

\begin{lemma}\label{lemma:soundness}
    The CP resolution rule is sound: the premises entail the conclusion of the rule.
\end{lemma}

\begin{example}[Example~\ref{ex:intro}, cont'd]\label{ex:valid_intervals}  
The QFLRA formula in the introduction translates into the set of constraints:
$
\Pi = \{\Psi_1,\Psi_2,\Psi_3\}
$
with $\Psi_1 = (x_5 \ge x_1), \Psi_2 =    ((x_5 \le x_2) \lor (x_5 \ge x_3))$ and $\Psi_3 =  (x_5 \le x_4)$.
By applying the CP resolution rule, we can obtain a new set of constraints entailed by $\Pi$ and logically equivalent to $\exists x_5 \bigwedge_{\Psi \in \Pi} \Psi$.
    $$
        \CPres_5(\Psi_1, \Psi_2) = x_1 \le x_2 \vee x_5 \ge x_3 \qquad 
        \CPres_5(\Psi_1, \Psi_3) = x_1 \le x_4
    $$
    $$
        \CPres_5(\Psi_3, \CPres_5(\Psi_1, \Psi_2)) = x_1 \le x_2 \vee x_3 \le x_4.
    $$
    As derived from the multiple application of the CP resolution rule, the above set of constraints admits a solution for $x_5$ if and only if $(x_1 \le x_4) \wedge (x_1 \le x_2 \vee x_3 \le x_4)$.
\end{example}
The proof of Lemma~\ref{lemma:soundness} is in Appendix~\ref{app:proof_soundness}. In the example, there is only one constraint with both positive and negative occurrences of $x_i$ and the CP resolution of any two distinct constraints always leads to a conclusion with either only positive or negative occurrences of $x_i$. However, in  general, the CP resolution of two constraints $\Psi$ and $\Psi'$ will lead to a new constraint $\CPres_i(\Psi,\Psi')$ which might contain both positive and negative occurrences of $x_i$. This new constraint can be the premise of other CP resolutions which can produce new constraints and the process can iterate. Nevertheless, our goal is to  derive the constraints in the variables $x_1,\ldots,x_{i-1}$ whose satisfying assignments can be extended to satisfy also the constraints with $x_i$. The standard solution to make all the possible CP resolutions on $x_i$ while considering also the CP resolvent of the already done resolution may turn out to be too computationally expensive.
Luckily, we can further restrict to CP resolutions between two constraints $\Psi$ and $\Psi'$ in which  $\Psi$ does not contain negative occurrences of $x_i$. To this end, let
 \begin{enumerate}
    \item $\Pi^+_i$ (resp. $\Pi^-_i$) to be the set of constraints in $\Pi_i$ with (resp. without) positive occurrences of $x_i$ and without (resp. with) negative occurrences of $x_i$ ;
    \item $\Pi^\pm_i$ to be the set of constraints in $\Pi_i$ with both positive and negative occurrences of $x_i$; 
    \item $\Pi^{\plusplus}_i$ to be the set of constraints obtained by the recursive application of the CP-resolution between one constraint without negative occurrences of $x_i$ and one constraint in $\Pi^\pm_i$:
    $$
    \Pi^{\plusplus}_i = \bigcup_{k=0}^{|\Pi_i^{\pm}|}\Pi^{k}_i \qquad \text{ with } \Pi^{k+1}_i= \{\CPres_i(\Psi,\Psi') \mid \Psi \in \Pi^{k}_i, \Psi' \in \Pi^\pm_i\}, 
    $$
    and  $\Pi_i^0 = \Pi^+_i$. Every constraint in $\Pi^{\plusplus}_i$ has only positive occurrences of $x_i$.
\end{enumerate}
 
Then, $\Pi_{i-1}$ is the set of constraints in $\Pi_i$ in which $x_i$ does not occur plus the set of constraints obtained by the CP resolution of the constraints in $\Pi^+_i \cup \Pi^-_i \cup \Pi^{\plusplus}_i$. More formally,
\begin{equation}\label{eq:pi_i}
    \Pi_{i-1} = \ktdt{(}\Pi_i \setminus (\Pi^+_i \cup \Pi^-_i \cup \Pi^\pm_i) \ktdt{)} \cup \{\CPres_i(\Psi,\Psi') \mid \Psi \in   \Pi^{\plusplus}_i, \Psi' \in \Pi^-_i \}.
\end{equation}
Clearly, each set $\Pi_{i-1}$ does not contain any occurrence of $x_i$ and can contain a non-polynomial number of constraints, the latter fact echoing similar results for variable elimination methods in propositional logic and sets of linear inequalities  \citep{dechter1999}. The above definition, generalises to disjunctions of linear inequalities the standard variable elimination procedure proposes for systems of linear inequalities based on the Fourier-Motzkin result.
\begin{example}[Example~\ref{ex:valid_intervals}, cont'd] \label{ex:pis_computation} 
Consider the variable ordering $x_1; x_2; x_3; x_4; x_5$. Then, $\Pi_5 = \Pi, \Pi_5^- = \{\Psi_3\}, \Pi_5^\pm=\{\Psi_2\}, \Pi_5^+=\Pi_5^0=\{\Psi_1\}, \Pi_5^1 = \{x_1 \le x_2 \lor x_5 \ge x_3\}$, and $\Pi_5^\plusplus = \Pi_5^0 \cup \Pi_5^1$. As a consequence, $
\Pi_4 = \{x_1 \le x_4, x_1 \le x_2 \vee x_3 \le x_4\}$, 
 and $\Pi_3 = \Pi_2 = \Pi_1 = \emptyset$.
\end{example}

For each set of constraints $\Pi_i$, the set $\Pi_{i-1}$ has the desired property, stated in the lemma~below.

\begin{lemma}\label{lemma:extendibility}
    Let $\Pi$ be a set of constraints in the variables $x_1, \ldots, x_i$. $\Pi_i = \Pi$ and $\Pi_{i-1}$ are equisatisfiable, and each assignment to the variables $x_1, \ldots, x_{i-1}$ satisfying $\Pi_{i-1}$ can be extended in order to satisfy $\Pi_i$. 
\end{lemma}
The proof of the Lemma is in  Appendix~\ref{proof:lemma_extendibility}. As a corollary of the above lemma we have that the CP resolution is {\sl refutationally complete}:  if $\Pi$ is unsatisfiable then it is  possible to derive a disjunction of linear inequalities \ozm{in $\Pi_0$} in which each inequality (\ref{eq:inequality}) has $w_i=0$ for $i = 1, \ldots, D$ and $b < 0$ (otherwise, it is possible to incrementally define assignments satisfying $\Pi_1, \Pi_2, \ldots, \Pi_D = \Pi$). \ozm{Thus, at the end of the layer construction, we are able to automatically detect $\Pi$ unsatisfiability, returning a corresponding value in such case.}

\begin{corollary}
     For  any finite set of constraints, the CP resolution rule is refutationally complete.
\end{corollary}
Starting from $i = 1$, the value of $\lsymb(\sample)_i$ is computed considering the constraints in $\Pisi$, where $\Pisi$ is the set of constraints in the variable $x_i$ obtained by substituting  the variables $x_1,x_2, \ldots, x_{i-1}$ with 
$\lsymb(\sample)_1,\lsymb(\sample)_2, \ldots, \lsymb(\sample)_{i-1}$ in $\Pi_i$. As in the single variable case,
assuming
 $\sample_i$ violates some constraint in $\Pisi$,
we define the {\sl closest satisfying left and right boundaries for $\sample_i$}  as: 
\begin{equation*}
        \leftpi{\Pisi}(\sample) = \max_{\Psi \in \Pisi}(\{\leftb :  \sample_i > \leftb, \leftb \in \Omega(\Pisi)\}), \quad
    \rightpi{\Pisi}(\sample) = \min_{\Psi \in \Pisi}(\{\rightb :  \sample_i < \rightb, \rightb \in \Omega(\Pisi)\}).
\end{equation*}
Then, for $j > i$, $\lsymb(\sample)_j = \sample_j$, for $j < i$, $\lsymb(\sample)_j = \lsymb(\sample)_i$  and
\begin{equation}\label{eq:DRL}
    \lsymb(\sample)_i = 
    \begin{cases}
        \sample_i &\qquad \text{if } \sample_i \in \Omega(\Pisi), \\
        \leftpi{\Pisi}(\sample) &\qquad \text{if } \sample_i \not\in \Omega(\Pisi) \text{ and } |\sample_i - \leftpi{\Pisi}(\sample)| < |\sample_i -\rightpi{\Pisi}(\sample)|, \\
        \rightpi{\Pisi}(\sample) &\qquad \text{otherwise}.
    \end{cases}
\end{equation}
\teba{A simple, non-optimised version of the algorithm is given in Algorithm~\ref{alg:overview}. The compilation step happens only once before training, while the application step is performed for each sample.}
\begin{example}[Examples~\ref{ex:one_dim},~\ref{ex:pis_computation}, cont'd] 
    Consider a sample $\sample$ where $\sample_1 = a, \sample_2 = b, \sample_3 = c$, and $\sample_4 = d$ (i.e., arranged as in Figure~\ref{fig:example_bounds}). Then, since $\Pi_3 = \Pi_2 = \Pi_1 = \emptyset$, \lsymb{} leaves the values unchanged for the features $x_1, x_2, x_3$ and $\lsymb(\sample)_1 = a,  \lsymb(\sample)_2 = b,  \lsymb(\sample)_3 = c$. Regarding $\sample_4$, we know that $\widetilde{\Pi}_4 = \{x_4 \ge a, a \le b \vee x_4 \ge c\}$ which reduces to $\{x_4 \ge a\}$, and, since it is satisfied, $\lsymb(\sample) = d$. Finally, $\widetilde{\Pi}_5 = \{x_5 \ge a, x_5 \le b \vee x_5 \ge c, x_5 \le d\}$ and the value of $\lsymb(\sample)_5$ can be computed on the ground of $\sample_5$, as detailed in Example~\ref{ex:one_dim}.
\end{example}

\begin{algorithm}[t]
\teba{
\caption{\teba{Compile \& Apply \lsymb{}} \label{alg:overview}}
\begin{minipage}{0.49\textwidth}
\begin{algorithmic}
\Function{DRL\_compile}{$\Pi, x_1;\ldots;x_D$}
\State $\Pi_D \gets \Pi$ 
\For{$i \gets D$ \textbf{downto} $1$}
\State compute $\Pi_i^+,\Pi_i^-,\Pi_i^{\pm}$, $\Pi_i^{\plusplus}$
\State $    \Pi_{i-1} \gets (\Pi_i \setminus (\Pi^+_i \cup \Pi^-_i \cup \Pi^\pm_i)) \cup$ 
\State \ \ \  $\{\CPres_i(\Psi,\Psi') \!\mid\! \Psi \!\in\!   \Pi^{\plusplus}_i, \Psi' \!\in\! \Pi^-_i \}$
\EndFor
\If{$\Pi_0$ is unsatisfiable}
\State \Return  {\sc UNSAT\_FLAG}
\Else\  \Return $\Pi_1; \dots; \Pi_D$
\EndIf
\EndFunction
\end{algorithmic}
\end{minipage}
\begin{minipage}{0.51\textwidth}
\begin{algorithmic}
\Function{DRL\_apply}{$\sample$, $\Pi_1, \ldots, \Pi_D$}
    \For{$i \gets 1$ \textbf{to} $D$}
        \State compute $\Pisi$, $\Omega(\Pisi)$, $\leftpi{\Pisi}(\sample)$, $\rightpi{\Pisi}(\sample)$
        \If{$\sample_i \in \Omega(\Pisi)$}\ $\lsymb(\sample)_i \gets \sample_i$ 
        \ElsIf{$|\sample_i - \leftpi{\Pisi}(\sample)| < |\sample_i - \rightpi{\Pisi}(\sample)|$} 
            \State $\lsymb(\sample)_i \gets \leftpi{\Pisi}(\sample)$
        \Else \quad $\lsymb(\sample)_i \gets \rightpi{\Pisi}(\sample)$
        \EndIf
    \EndFor
    \State \Return $\lsymb(\sample)_1; \ldots ;\lsymb(\sample)_D$
\EndFunction
\end{algorithmic}
\end{minipage}
}
\end{algorithm}

\begin{theorem}\label{th:guaranteed_sat}
    Let $\Pi$ be a finite and satisfiable set of constraints. For any sample $\sample$ and variable ordering, the corresponding sample $\lsymb(\sample)$ satisfies $\Pi$.
\end{theorem}
Further, considering the variable ordering $x_1;x_2;\ldots;x_D$,
$\lsymb(\sample)$ is {\sl optimal w.r.t. $\sample$ and $\Pi$
and the variable ordering}: for each $i = 1,\ldots,D$ there does not exist a sample $\sample' \in \Omega(\Pi)$ such that $|\sample_i - \sample'_i| < |\sample_i - \lsymb(\sample)_i|$, and for all $j < i$,
$\sample'_j = \lsymb(\sample)_j$.

\begin{theorem}\label{th:min_dist}
    Let $\Pi$ be a finite and satisfiable set of constraints. For any sample $\sample$ and variable ordering, the corresponding sample $\lsymb(\sample)$ is optimal w.r.t. $\sample$, $\Pi$ and the variable ordering.
\end{theorem}
The proofs of Theorems~\ref{th:guaranteed_sat} and~\ref{th:min_dist} are in Appendix~\ref{app:proof_guaranteed_sat} and~\ref{app:proof_min_dist}, respectively.


\section{Experimental Analysis}

To assess how \lsymb{}\footnote{The code is available at \url{https://github.com/mihaela-stoian/DRL_DGM}.} performs in practice, we conduct the following studies.
First, in Section~\ref{sec:efficiency_uncons_vs_DRL}, we investigate whether our layer improves the quality of the synthetic data generated by standard DGMs. In Section~\ref{sec:efficiency_Cmodels_vs_DRL}, we then compare our constrained models (which we refer to as DGMs+DRL) with the models obtained by considering only the linear constraints in each dataset and adding the layer proposed by~\cite{stoian2024}.
We refer to the linearly constrained DGMs as DGMs + Linear Layer (DGMs+LL). 
Then, in  Section~\ref{sec:sample_gen_time}, we conduct experiments to determine how the background knowledge injection affects the sample generation time.
Before delving into these studies, we describe the metrics we use to compute the sample quality, along with the models and datasets.





\textbf{Sample Quality Evaluation.} To judge the quality of our samples we measure (i) how well they align with the background knowledge and (ii) how well they can replace the real data in downstream tasks. To measure background knowledge alignment,
we consider the metrics proposed in~\citep{stoian2024}: i.e, 
 {\sl constraint violation rate} (\cvr), {\sl constraint violation coverage} (\cvc), and {\sl samplewise constraints violation coverage} (\scvc). To determine their usability in downstream tasks
we consider the metric {\sl machine learning efficacy} \citep{kim2023stasy}, also known as utility (e.g., in \citep{liu2022goggle}).
To compute it, we follow the ``Train on Synthetic, Test on Real'' protocol~\citep{Esteban2017tstr}.
Specifically, to compute the efficacy for classification (resp., regression) datasets, we train six classifiers (resp., four regressors) on synthetic data and test them on real data.
A detailed description of the evaluation protocol and the hyperparameter tuning description for the classifiers and regressors can be found in Appendix~\ref{appdx:stasy_evaluation_protocol}. 
For classification datasets, we report: F1-score (\fone), weighted F1-score (\wfone), and Area Under the ROC Curve (\auc), while for the regression dataset, we compute the mean absolute error (\mae) and the root mean square error (\rmse).
For reference, we report the same metrics when training on the real data in Table~\ref{tab:real-utility} of Appendix~\ref{appdx:real_data_efficiency}. 



\textbf{Models.}
We consider five DGMs: \wgan~\citep{Arjovsky2017_WGAN}, \tablegan~\citep{park2018_tableGAN}, \ctgan~\citep{xu2019_CTGAN}, \tvae~\citep{xu2019_CTGAN}, and \goggle~\citep{liu2022goggle}, and build our \lsymb{} on top of each to create DGM+\lsymb{} models.
A description of these models is in Appendix~\ref{appdx:models}. 


\textbf{Datasets.}
We consider five real-world datasets and associated constraints.
Four datasets (i.e., \phishing, \cervical, \lcld, and \heloc) are used for classification tasks, while one dataset (i.e., \house) is used for regression.
A detailed description of the datasets and their respective constraints are in Appendix~\ref{appdx:datasets}.








\begin{figure}[t]
    \centering
     \includegraphics[trim={0.9cm 1cm 0.5cm 0cm},clip,width=0.92\linewidth]{plots/border_with_Cmodels/model1_withCmodels_TVAE_0_3_2_kc_Zipcode-Price.pdf}
    \caption{Sample distributions for real and synthetic data from TVAE, TVAE+LL and TVAE+\lsymb{}. The regions where samples violate the constraints are in red.\vspace{-0.4cm}} 
    \label{fig:main_background_knowledge_alignment}
\end{figure}

\subsection{Synthetic Data Quality}
\label{sec:efficiency_uncons_vs_DRL}

\paragraph{Background knowledge alignment.}
\input{results/main/constraint_violations_uncons_vs_DRL}
To assess how often the samples violate the constraints, we calculate the \cvr{}, which
is defined as the percentage of samples that violate at least one constraint.
Table~\ref{tab:cons-sat_uncons_vs_DRL} shows the \cvr{} for each unconstrained model (first five rows) and our models equipped with the \lsymb{} (last row). More detailed findings are reported in  Appendix~\ref{appdx:uncons_vs_DRL_constr_violation} (Table~\ref{tab:cons-sat-breakdown_uncons_vs_DRL}), where the results for  \scvc{} and \cvc{}  can also be found (Tables~\ref{tab:scvc-breakdown_uncons_vs_DRL}, \ref{tab:cvc-breakdown_uncons_vs_DRL}).
As expected, the models with our \lsymb{} always satisfy the constraints, while the samples obtained with standard DGMs very often violate them.
Additionally, in many cases, the \cvr{} is extremely high: out of 25 cases, \rebuttal{13} cases have \cvr{} greater than 50\% and 5 cases have \cvr{} equal to 100\%, thus making the standard procedure of rejecting non-aligned samples unfeasible. 
Further, to visualise the impact of DRL, we consider the features {\sl Price} and {\sl Zipcode} from the House dataset and create the scatter plots of 
(i) the real data, and the synthetic data from
(ii) the unconstrained DGMs, 
 (iii) the DGMs+LL,
 and (iv) the DGMs+DRL.
We also highlight in red the regions that violate the constraints: (i) \textsl{if the Zipcode is 98004 or 98005 then the Price is greater than 400K USD} and (ii)
\textsl{if the Zipcode is between 98006 and 98008 then the Price exceeds 225K USD}. The scatter plots obtained for the real datapoints and TVAE (with and without LL and DRL) are shown in Figure~\ref{fig:main_background_knowledge_alignment}, while the ones obtained from the other models are given in Figure~\ref{fig:background_knowledge_alignment}, Appendix~\ref{appdx:qualitative}. The Figures clearly show that standard DGMs and DGMs+LL fail to comply with the constraints, and indeed, 
many of the samples fall in the
red-shaded regions. %
On the contrary, the samples obtained using DRL not only never violate the constraints, but also better match the real
data distribution. 



\paragraph{Machine Learning Efficacy.} 
\input{results/main/one_table_uncons_vs_DRL_all_metrics}

 Table~\ref{tab:main_efficiency_uncons_vs_DRL} shows that: \ktdt{(i) making the samples compliant with the constraints via rejection sampling (RS) often reduces their machine learning efficacy (indicated as DGMs+RS), and that (ii)} adding DRL improves the performance of the unconstrained models according to at least one metric in all cases but one (TableGAN over the CCS dataset). \ktdt{Regarding the performance obtained with rejection sampling we can see that it decreases with respect to the standard DGMs  in 17, 17 and 17 out of 20 cases for \fone, \wfone{} and \auc, respectively.} 
\ktdt{Regarding the performance of DGMs+\lsymb{}, the layer}  improves the performance w.r.t. the unconstrained models in 19, 18 and \rebuttal{17} out of 20 cases for \fone, \wfone{} and \auc, respectively.
Additionally, the improvements are often non-negligible.
For \fone, in more than half of the cases, the improvement is of at least 3.5\%, with the largest one recorded on \goggle{} for \cervical{} of 21.4\%.
For \wfone, in more than half of the cases, the improvement is of at least 1.0\%, with the largest improvement, of 20.5\%, again recorded on \goggle{} for \cervical.  
And, for \auc, the improvement is at least \rebuttal{1.2\% in half of the cases}, with the largest improvement, of 20.9\%, recorded on \goggle{} for \cervical{}. On the regression dataset, \house, we find a similar trend in terms of improvements brought by \lsymb{} (Appendix~\ref{appdx:efficiency_wrt_uncons},Table~\ref{tab:kc_efficiency_uncons_vs_DRL}), where
the DGM+\lsymb{} models improve the performance w.r.t. the unconstrained models in all cases.
We also verify the statistical significance of the results following the recommendation of
  \citep{demsar2006}. We perform the Wilcoxon signed-rank test on the efficacy results for the classification datasets and we obtain  p-value $<0.01$ w.r.t. the \fone{} and \wfone{} results and $<0.05$ w.r.t. \auc{}, thus confirming that DRL significantly improves the performances of DGMs. 












\subsection{Linear vs. QFLRA Constraints}
\label{sec:efficiency_Cmodels_vs_DRL}

\paragraph{Background knowledge alignment.}
Table \ref{tab:cons-sat_Cmodels_vs_DRL} shows the~\cvr{} for each DGM+LL model (first five rows) and for the DGM+\lsymb{} models (last row). 
As expected, DGMs+LL cannot guarantee the compliance with QFLRA constraints and
in  \rebuttal{5} out of 25 cases we see a \cvr{} greater than 50\%.
Moreover, we have one case %
where \cvr{} is 100\%, thus demonstrating the need for models that support more expressive constraints. In Appendix~\ref{appdx:Cmodels_vs_DRL}, Tables~\ref{tab:cons-sat-breakdown_Cmodels_vs_DRL}, \ref{tab:scvc-breakdown_Cmodels_vs_DRL}, \ref{tab:cvc-breakdown_Cmodels_vs_DRL}, we report the results for all metrics: \cvr, \scvc, and \cvc{}.





\paragraph{Machine Learning Efficacy.} 
\input{results/main/one_table_Cmodels_vs_DRL_all_metrics}
\input{results/main/constraint_violations_Cmodels_vs_DRL}
For the sake of completeness, in Table~\ref{tab:main_efficiency_Cmodels_vs_DRL} we include the comparison between DGMs+\lsymb{} and DGMs+LL on the classification datasets.
Since \cite{stoian2024}  
already reported improvements over their unconstrained counterpart by adding the linear layer, as expected in this scenario, we get more modest improvements than the ones w.r.t. the unconstrained models. As we can see from the Table, the DGM+\lsymb{} models improve the efficacy w.r.t. the DGMs+LL for at least one metric in 17 out of 20 cases. %
Similarly, the number of times DGM+DRL outperforms the respective DGM+LL is lower than the number of times it outperforms its unconstrained counterpart. 
Indeed, out of 20 comparisons, the models with \lsymb{} outperform their linearly constrained counterparts 13, 10 and \rebuttal{11} times for \fone, \wfone, and \auc, respectively.
Regarding the regression dataset, \house,  Table~\ref{tab:kc_efficiency_Cmodels_vs_DRL} in Appendix~\ref{appdx:efficiency_wrt_Cmodels} shows that the DGM+\lsymb{} models have a comparable performance to the DGM+LL models, with 6 out of 10 the cases showing an improvement in performance when using our layer. 
As in the previous experiment, we use the Wilcoxon signed-rank test to assess whether adding DRL significantly improves over the linear layer. 
In this case, we obtain p-value $<0.05$ for F1, while as expected, the test confirms that the performances of DGMs+DRL and DGMs+LL are not statistically different w.r.t. wF1 and  AUC.





\subsection{Sample Generation Time}
\label{sec:sample_gen_time}
\begin{wraptable}{r}{0.45\linewidth}
\vspace{-0.45cm}
\caption{Sample generation time in seconds. \hspace*{\fill}}
\vspace{-0.5cm}
\small
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
           & \phishing{}  & \cervical{} & \lcld{} & \heloc{} & \house{} \\ \midrule
DGM & 0.15	&0.08	&0.07	&0.06	&0.05\\
DGM+RS & 0.37	& 0.83	&1.54	&0.66	& - \\
DGM+\lsymb &0.22	&0.13	&0.14&	0.10&	0.13\\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:runtime_avg}
\end{wraptable}
To assess the impact of constraints on sample generation time, we compare the runtimes of unconstrained DGMs, DGMs+\lsymb{} \ktdt{and DGMs+RS}.
We generate 
 1,000 samples for each model and dataset using five different seeds and report the average runtime in Table~\ref{tab:runtime_avg} (for a detailed breakdown, see~Appendix~\ref{appdx:runtime}, Table~\ref{tab:runtime_appdx}). 
As expected, DGMs+DRL are slower on average than their unconstrained counterparts. 
\ktdt{However,  they are faster than DGMs+RS.
Indeed, excluding extreme cases with 100\% CVR  (where we were unable  to generate samples even in 24h),  in all other cases,  DGMs+RS take more than twice as long as the unconstrained DGMs.
}

























\section{Related Work}

Our work lies at the intersection of two fields: Neuro-symbolic AI and tabular data generation. Thus, our related work section will mirror this duality.

\paragraph{Neuro-symbolic AI.} Neuro-symbolic AI~\citep{raedt_survey,third_wave} refers to the broad area of AI
that combines the strengths of symbolic reasoning with neural networks. %
As our work falls into the more specific field of injection of background knowledge into neural models, (see, e.g.,~\citep{stewart2017,
hoernle2022multiplexnet,giunchiglia2023manifesto,daniele2023_refining,calanzone2024logicallyconsistentlanguagemodels}) we will focus the discussion on this topic.
Many methods for this task are based on the intuition that logical constraints can be transformed into differentiable loss function terms that penalise the networks for violating them (see, e.g.,~\citep{xu2018semantic,serafini2022ltn,diligenti2012SBR,fischer2019DL2}). As expected, since these methods operate at a loss level, they give no guarantee that the constraints will be satisfied. Other works manage to integrate neural networks and probabilistic reasoning through the mapping of predicates appearing in logical formulae to neural networks~\citep{manhaeve2018deepproblog,yang2020neurasp,sachan2018nuts_and_bolts,pryor2023NeuPSL,krieken2023anesi}. This allows these methods to both perform reasoning on the networks' predictions as well as constrain the output according to the background knowledge. The most similar line of work to ours is the one where the constraints in input are automatically compiled into neural layers~\citep{giunchiglia2021jair,ahmed2022spl,giunchiglia2024ccn+}. However, these methods can compile and incorporate constraints that are at best as expressive as propositional logic formulae. %
Focusing specifically on the incorporation of constraints on generic generative models, we can find the work by~\cite{stoian2024}, where the tabular generation process was simply constrained by linear inequalities. If we consider different application domains, we can find the work proposed by~\cite{misino2023tvael}, where ProbLog~\citep{Raed2007_problog} works in tandem with variational-autoencoders, and the one by~\cite{liello2020}, where the authors incorporate propositional logic constraints on GANs for structured objects generation. %

\paragraph{Tabular Data Generation.} In recent years, various DGMs have been proposed to tackle the problem of tabular data synthesis. Many of these approaches are based on Generative Adversarial Networks (GANs), like TableGAN~\citep{park2018_tableGAN}, CTGAN~\citep{xu2019_CTGAN}, IT-GAN~\citep{lee2021invertible}, OCT-GAN~\citep{kim2021oct}, and PacGAN~\citep{lin2018pacgan}.
Other methods try to reduce the problems that often characterise GANs, such as mode collapse and unstable training, by introducing Variational AutoEncoders (VAEs) based models, see, e.g.,~\citep{xu2018semantic,srivastava2017_veegan,wan2017vae_imbalanced}. An alternative solution to such problems is given by the usage of denoising diffusion probabilistic models as done in~\citep{kotelnikov2023tabddpm} or~\citep{kim2023stasy}, where the authors designed a self-paced learning method and a fine-tuning approach to adapt the standard score-based generative modeling to the challenges of tabular data generation. Finally, GOGGLE~\citep{liu2022goggle} uses graph learning to infer relational structure from the data and use it to their advantage especially in data-scarce setting. Since synthetic tabular data are often used to replace the original dataset to preserve privacy in sensitive settings, a parallel line of research revolves around the development of DGMs with privacy guarantees. Examples of models that have such privacy guarantees are PATEGAN~\citep{yoon2020_privacy}  and DP-CGAN~\citep{rehaneh2020dpcgan_privacy}. 
\section{Conclusions}

In this paper, we have proposed \layer{} (\lsymb) the first-ever Neuro-symbolic AI layer able to automatically compile constraints expressed as QFLRA formulas into a neural layer and thus guarantee their satisfaction. This sort of work is really needed in the tabular data synthesis field, as our experimental analysis shows that Deep Generative Models (DGMs) very frequently generate datapoints that are not aligned with the background knowledge, with some extreme cases where all the datapoints are violating the constraints. \lsymb{} presents many desirable properties: (i) it can be seamlessly integrated into the topology of any neural network, (ii) it allows the backpropagation of the gradients, (iii) it performs all the computations in a single forward pass (i.e., there are no cycles), (iv) it optimally refines the original predictions and, last but not least, (v) it improves the performance of all the tested DGMs in terms of machine learning efficacy. Indeed, in our experimental analysis we got improvements for all datasets of up to
21.4\%, 20.5\%, and 20.9\%
in terms of F1-score, weighted F1-score, and Area Under the ROC Curve, respectively.

\newpage

\section{Ethics Statement}

The development and application of synthetic data generation techniques, particularly in tabular data, have the potential to significantly impact a wide range of sectors, including healthcare, finance, and social sciences. While our method, Disjunctive Refinement Layer (DRL), improves the quality and fidelity of generated data by ensuring alignment with user-specified constraints, there are ethical implications of synthetic data use. Firstly, there is the potential for misuse. Synthetic data may be seen as a substitute for real-world data, but it should not be viewed as a perfect replacement. Secondly, the use of synthetic data in automated decision-making systems poses risks for fairness and bias. While DRL allows for the specification of constraints that align with real-world domain knowledge, it is important that the user-specified constraints do not encode existing biases or discrimination. %

\section{Reproducibility Statement}
To ensure the reproducibility of our results, we included all the necessary details in the Appendix of the paper. The proofs of the Lemmas and Theorems can be found in Appendices~\ref{proof:lemma_single_var}, \ref{app:proof_soundness}, \ref{proof:lemma_extendibility}, \ref{app:proof_guaranteed_sat}, and \ref{app:proof_min_dist}, the detailed description of the datasets, the baseline models used  (together with their links), evaluation protocol for  the machine learning efficacy metric, and the chosen hyperparameters can be found in~\ref{appdx:datasets}, \ref{appdx:models}, \ref{appdx:stasy_evaluation_protocol}, and \ref{appdx:hyperparameter_search}.


\subsubsection*{Acknowledgments}
Mihaela C\u{a}t\u{a}lina Stoian is supported by the EPSRC under the grant EP/T517811/1. She has also received support for this work through the G-Research Women in Quant Finance Grant and St Hilda's College Travel for Research and Study Grant. 
We also acknowledge the use of the Advanced Research Computing (ARC) facilities of University of Oxford.

\bibliography{main}
\bibliographystyle{iclr2025_conference}


\newpage

\appendix


\section{\ktdt{\layer{} Visualizations}\label{app:visualizations}}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[trim={20cm 16cm 20cm 8cm},clip,width=\linewidth]{schemas/WGAN+DRL.pdf}
        \caption{\ktdt{GAN-based models}}
        \label{fig:GAN+DRL}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[trim={20cm 16cm 20cm 11cm},clip,width=\linewidth]{schemas/VAE+DRL.pdf}
        \caption{\ktdt{VAE-based models}}
        \label{fig:VAE+DRL}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[trim={20cm 16cm 20cm 11cm},clip,width=\linewidth]{schemas/GOGGLE+DRL.pdf}
        \caption{\ktdt{GOGGLE-like models}}
        \label{fig:GOGGLE+DRL}
    \end{subfigure}
    \caption{\ktdt{Visualisation of the considered types of DGMs  and how to add DRL in their topology.}}
    \label{fig:schemas_visualizations}
\end{figure}

\ktdt{In Figure~\ref{fig:schemas_visualizations} we give an overview on how to add \lsymb{} in the topology of the three types of models we considered. In all figures we indicate with $z$ a noise vector, with $x$ a real datapoint  from the original dataset, with $\sample$ a sample generated with the DGM, and with $\lsymb(\sample)$ the final sample obtained from $\lsymb{}$. Considering each of the Figures, we can see that: 
\begin{itemize}
    \item Figure~\ref{fig:GAN+DRL} shows that \lsymb{} needs to be added on top of the generator module in GAN-based models, 
    \item Figure~\ref{fig:VAE+DRL} shows that \lsymb{} needs to be added after the decoder module in VAE-based models, and 
    \item Figure~\ref{fig:GOGGLE+DRL} shows that \lsymb{} needs to be added after the generator module in GOGGLE-like models.
\end{itemize}
In general, we can see that \lsymb{} can be added in many different DGMs, and it simply needs to be added right after the sample $\sample$ is generated.}

\section{Proof of Lemma~\ref{lemma:single_var}}\label{proof:lemma_single_var}
\input{proofs/proof_lemma_single_var}

\section{Proof of Lemma~\ref{lemma:soundness}}\label{app:proof_soundness}
\input{proofs/proof_sound}

\section{Proof of Lemma~\ref{lemma:extendibility}}\label{proof:lemma_extendibility}
\input{proofs/proof_lemma_extend}

\section{Proof of Theorem~\ref{th:guaranteed_sat}}\label{app:proof_guaranteed_sat}
\input{proofs/proof_sat_guarantee}

\section{Proof of Theorem~\ref{th:min_dist}}\label{app:proof_min_dist}
\input{proofs/proof_min_dist}


\section{Datasets}
\label{appdx:datasets}
Below we provide a brief description for each dataset and the links to the pages where they can be downloaded.
\begin{itemize}
    \item URL\footnote{Link to dataset: {https://data.mendeley.com/datasets/c2gw7fy2j4/2}} ~\citep{hannousse2021towards} is used to perform webpage phishing detection with features describing statistical properties of the URL itself as well as the content of the page. 
    \item \cervical \footnote{Link to dataset:{https://www.kaggle.com/datasets/ranzeet013/cervical-cancer-dataset/data}} is used to identify individuals at high risk of cervical cancer from features describing the patients' demographic and medical history, including age, sexual behavior, contraceptive use, and various medical test results.
    \item LCLD\footnote{Link to dataset: https://figshare.com/s/84ae808ce6999fafd192} is used to predict whether the debt lent is unlikely to be collected from features related to the loan as well as client history. In particular, we use the feature-engineered dataset from~\cite{simonetto2022}, inspired from the LendingClub loan data. 
    \item HELOC\footnote{Link to dataset: https://huggingface.co/datasets/mstz/heloc} is a dataset from FICO used to predict whether customers will repay their credit lines within 2 years from features related to the credit line and the client's history.
    \item \house \footnote{Link to dataset: {https://www.kaggle.com/datasets/harlfoxem/housesalesprediction/data}} was used to predict the prices of houses in King County (USA) and contains data collected from May 2014 to May 2015. The features describe various features of the sold houses, including the date of sale, house prices, the number of bedrooms and bathrooms, square footage, condition, grade, year built, and location, among others.
\end{itemize}


For each dataset above, Table~\ref{tab:app_dataset} shows the number of samples in the train, validation and test partitions, along with the number of features and the number of constraints. 
\ktdt{Regarding the constraints, they were already included in some of the original datasets. This is true for URL, Heloc and LCLD. Regarding CCS and House, we manually annotated the constraints using our background knowledge about the problem, then we checked whether the data were compliant with our constraints and finally we retained only those constraints that were satisfied by all the datapoints. Simple examples of these constraints (from CCS) state simple facts like ``if the feature capturing the number of cigarettes packs per day is greater than 0 then the feature capturing whether the patient smokes or not should be equal to 1'' or ``the feature representing the age should always be higher than the age at the first intercourse''.}
\input{results/appendix/dataset_stats_appdx}


\section{Models}
\label{appdx:models}

Below we give a brief description of each of the models used in our experimental analysis:
\begin{itemize}
    \item WGAN~\citep{Arjovsky2017_WGAN}: it is a GAN based model which has been trained by using the Wasserstein distance as a loss, which improves the stability of learning.
    \item TableGAN~\citep{park2018_tableGAN}: is a GAN-based model designed for generating realistic tabular data. It uses a convolutional neural network (CNN) as a discriminator to better the capture dependencies among features.
    \item CTGAN~\citep{xu2019_CTGAN}: is again a GAN-based model which uses a conditional generator to model feature distributions and applies a mode-specific normalisation technique to improve the generation of imbalanced categorical data.
    \item TVAE~\citep{xu2019_CTGAN}: uses a variational autoencoder architecture to capture both continuous and categorical feature distributions, learning a probabilistic latent space representation of the data. By optimising the evidence lower bound, it balances reconstruction accuracy and regularisation.
    \item GOGGLE~\citep{liu2022goggle}: uses a VAE framework combined with a graph neural network, which allows the model to learn complex feature relationships by representing the data as a graph, where each node corresponds to a feature, and edges capture dependencies between features.
\end{itemize}






\section{Efficacy Evaluation Protocol}
\label{appdx:stasy_evaluation_protocol}

In order to evaluate the efficacy of the models, we closely follow the protocol outlined in \citep{kim2023stasy}.
For clarity, we describe the protocol below.


First, we generate a synthetic dataset and split it into training, validation, and test sets, maintaining the same proportions as in the real dataset.
Next, we conduct a hyperparameter search using the synthetic training set to train various classifiers and regressors.
Specifically, for the classification datasets (i.e., \phishing{}, \cervical{}, \lcld{}, \heloc{}), we use the following classifiers: 
AdaBoost~\citep{adaboost},
Decision Tree~\citep{decision_tree}, 
Logistic Regression~\citep{logistic_regression} 
Multi-layer Perceptron (MLP)~\citep{MLP}, 
Random Forest~\citep{random_forest}, 
and XGBoost~\citep{xgboost}.
For the regression dataset (i.e., \house{}), we use: Linear Regression, MLP, Random Forest regressors, and XGBoost.
Across all classifiers and regressors, we use the same hyperparameter settings as those in Table 26 of~\citep{kim2023stasy}. Then, based on the F1-score obtained on the real validation set, we select the best hyperparameter configuration.
As a last step, we evaluate the selected models on the real test set and average the performance across all classifiers/regressors. 
The results for all models are reported using three metrics (i.e., F1-score, weighted F1-score, and the Area Under the ROC Curve) for the classification datasets, and two metrics (i.e., Mean Absolute Error and Root Mean Square Error) for the regression dataset. 

We run the entire process five times for each model, then compute the average results for each metric individually across the repetitions.




\input{results/appendix/hyperparameters}

\section{Hyperparameter Search}
\label{appdx:hyperparameter_search}

We carried out an extensive hyperparameter search 
to identify the  optimal configurations for each DGM.
We selected these configurations based on the efficacy performance: for the classification datasets (i.e., \phishing, \cervical, \lcld, and \heloc), we used the average of the F1-score, weighted F1-score, and Area Under the ROC Curve (AUC), whereas for the regression dataset (i.e., \house{}), we used the average of the Mean Absolute Error and Root Mean Square Error.


For clarity, we describe the hyperparameter search space below, for each of the considered models.
For the GOGGLE model, we adopted the same optimiser and learning rate settings as in \citep{liu2022goggle}. Specifically, we used the \adam{} optimizer~\citep{adam} with five learning rates: ${ \num{1e-3}, \num{5e-3}, \num{1e-2} }$.
Additionally, we experimented with a set of values for the threshold parameter: $\{ \num{1e-1}, \num{2e-1}\}$.
For the \tvae{} model, \adam{} was used again, but with a different set of five learning rates: ${ \num{5e-6}, \num{1e-5}, \num{1e-4}, \num{2e-4}, \num{1e-3} }$.
For the other three models (i.e., \wgan, \tablegan, and \ctgan), we tested three different optimisers: \adam{}, \rmsprop{}~\citep{rmsprop}, and \sgd{}~\citep{sgd}, each paired with its own set of learning rates, as follows:
\begin{itemize}
    \item WGAN: \adam{}: $\{ \num{1e-4}, \num{2e-4}, \num{1e-3}\}$, \rmsprop{}: $\{\num{5e-5}, \num{1e-4}, \num{1e-3}\}$, \sgd{}: $\{\num{1e-4}, \num{1e-3}\}$
    \item TableGAN: \adam{}: $\{\num{5e-5}, \num{1e-4}, \num{2e-4}, \num{1e-3}, \num{1e-2}\}$, \rmsprop{}: $\{\num{1e-4}, \num{2e-4}, \num{1e-3}\}$, \sgd{}: $\{\num{1e-4}, \num{1e-3}\}$
    \item CTGAN: \adam{}: $\{\num{5e-5}, \num{1e-4}, \num{2e-4}, \num{1e-3}\}$, \rmsprop{}: $\{\num{1e-4}, \num{2e-4}, \num{1e-3}\}$, \sgd{}: $\{\num{1e-4}, \num{1e-3}\}$
\end{itemize}

Additionally, we explored various batch sizes for each model:
\begin{itemize}
    \item WGAN: $\{64, 128, 256, 510\}$
    \item TableGAN: $\{128, 256, 510\}$
    \item CTGAN and TVAE: $\{70, 280, 500\}$
    \item GOGGLE: $\{32, 64, 128\}$
\end{itemize}


We did not separately tune our DGM+\lsymb{} models, nor the DGM+LL models.
However, for each of the  DGM+\lsymb{} and DGM+LL models, we ran three versions corresponding to three different ways of ordering the variables  that decided the order in which the layers (LL and DRL) change the values of the features.
More precisely, we tried: (i) a random (Rnd) ordering, (ii) the correlation (Corr)-based ordering. (iii) the Kernel Density Estimation (KDE)-based ordering.
The last two orderings are proposed by~\cite{stoian2024}, and are defined in the Appendix of their paper.
For completeness, we also define them here.
The Corr-based ordering is computed as follows: for each feature, the absolute difference is taken between the pairwise feature correlations (with respect to all other features) of samples generated by the unconstrained DGM and the real data. 
The features are then ranked in ascending order based on these scores. This ensures that features with the most similar correlations between the generated and real data are prioritised by DRL (and LL) first.
The KDE-based ordering is computed by first fitting a Kernel Density Estimator (KDE) on the real data and estimating the log-likelihood for each real and synthetic sample. In a discrete setting, regardless of the variable domains, two marginal probability mass functions are approximated for each variable using the real and synthetic data, respectively. The variables are then ranked by computing the Kullback-Leibler divergence between these two and sorting the results in ascending order.
We then selected the best ordering separately for each  DGM+\lsymb{} and DGM+LL model. In  Table~\ref{tab:best_hyperparam_configs}, we report the optimal hyperparameter configurations, which we use in all experiments presented in our paper that involve the DGM, DGM+LL and DGM+\lsymb{} models.


\section{Synthetic data quality.}
\label{appdx:uncons_vs_DRL}


\paragraph{Background knowledge alignment.} 
\label{appdx:uncons_vs_DRL_constr_violation}
\input{results/appendix/constraint_violations_uncons_vs_DRL}

In addition to the \cvr{} metric reported in the main paper, we also compute the samplewise constraints violation coverage (\scvc{}) and the constraint violation coverage (\cvc{}), where \scvc{} indicates the average percentage of constraints violated per sample, across all samples, and \cvc{} represents the percentage of constraints violated at least once by the samples.
In Tables~\ref{tab:cons-sat-breakdown_uncons_vs_DRL}, ~\ref{tab:scvc-breakdown_uncons_vs_DRL}, and ~\ref{tab:cvc-breakdown_uncons_vs_DRL}, we provide the \cvr, \scvc, and \cvc{}, respectively, for the unconstrained DGM models, according to the two possible types: linear and disjunctive constraints. 
Indeed, as constraints expressed as linear inequalities are a special case of the QFLRA constraints, we have also partitioned the constraints between those that can be captured as linear inequalities and those that cannot. Then, we checked how much each of the two partitions  
contributes to the high \cvr{} results, and 
 found that in 13 (resp. 6) out of 25 cases, the  \cvr{} was greater than 25\% (resp. 50\%) on the constraints presenting disjunctions alone.


\paragraph{Efficacy.}
\label{appdx:efficiency_wrt_uncons}

Tables~\ref{tab:utility_and_stddevs_f1_uncons_vs_DRL}, \ref{tab:utility_and_stddevs_weightedf1_uncons_vs_DRL}, \ref{tab:utility_and_stddevs_auc_uncons_vs_DRL} show the efficacy, along with the standard deviations from the mean, for each unconstrained DGM model and the corresponding DGM+\lsymb{} models on every classification dataset, using the \fone, \wfone, and \auc{} metrics, respectively.
Similarly, in Table~\ref{tab:kc_efficiency_uncons_vs_DRL}, we report \mae{} and \rmse{} for each unconstrained DGM model and the corresponding DGM+\lsymb{} models on the regression dataset (i.e., \house).\\
\input{results/appendix/uncons_vs_bestDRL}
\input{results/appendix/regression_efficiency_uncons_vs_DRL}











\section{Linear vs. QFLRA Constraints}
\label{appdx:Cmodels_vs_DRL}

\paragraph{Background knowledge alignment.}
\input{results/appendix/constraint_violations_Cmodels_vs_DRL}
In Tables~\ref{tab:cons-sat-breakdown_Cmodels_vs_DRL}, ~\ref{tab:scvc-breakdown_Cmodels_vs_DRL}, and ~\ref{tab:cvc-breakdown_Cmodels_vs_DRL}, we provide the \cvr, \scvc, and \cvc{}, respectively, for the DGM+LL models, according to the two possible types: linear and disjunctive constraints.



\paragraph{Efficacy.} 
\label{appdx:efficiency_wrt_Cmodels}
\input{results/appendix/bestCmodels_vs_bestDRL}
\input{results/appendix/regression_efficiency_Cmodels_vs_DRL}

Tables~\ref{tab:utility_and_stddevs_f1_Cmodels_vs_DRL}, \ref{tab:utility_and_stddevs_weightedf1_Cmodels_vs_DRL}, \ref{tab:utility_and_stddevs_auc_Cmodels_vs_DRL} show the efficacy, along with the standard deviations from the mean, for each DGM+LL model and the corresponding DGM+\lsymb{} models on every classification dataset, using the \fone, \wfone, and \auc{} metrics, respectively.
Similarly, in Table~\ref{tab:kc_efficiency_Cmodels_vs_DRL}, we report \mae{} and \rmse{} for each unconstrained DGM+LL model and the corresponding DGM+\lsymb{} models on the regression dataset (i.e., \house).







\section{Background knowledge alignment: A Qualitative Analysis}
\label{appdx:qualitative}

Figure~\ref{fig:background_knowledge_alignment} accompanies Figure~\ref{fig:main_background_knowledge_alignment} from the main body of our paper and shows the relevant sample space for the same two constraints from the \house{} dataset: 
\textsl{if the Zipcode is 98004 or 98005 then the Price is greater than 400K USD} and 
\textsl{if the Zipcode is between 98006 and 98008 then the Price exceeds 225K USD}.
As we can see, in all cases, the unconstrained DGMs and the DGMs+LL fail to comply with the constraints.
Unlike the synthetic data from the unconstrained DGMs, the samples generated using our \lsymb{} layer never cross into the areas that mark regions where datapoints violate the constraints and, in addition, their distribution resembles more closely the real data in all five cases.

In addition, we show a similar comparison but for a different dataset and different constraints.
Specifically, we consider the following two constraints from the \phishing{} dataset:
\textsl{if the Number of Subdomains is less than 2 then the Hostname length is less than 30} and 
\textsl{if the Number of Subdomains is less than 3 then the Hostname length is less than 55}.
The two constraints capture the relation between the two features (i.e., \textsl{Number of Subdomains} and \textsl{Hostname length}) and, differently from the two constraints from the \house{} dataset mentioned above, their respective violation space intersects, as shown in red in Figure~\ref{fig:url_background_knowledge_alignment}.
Nevertheless, our constrained models never violate any of the constraints, unlike the unconstrained and DGM+LL models.



\ktdt{Finally, in order to show the unintended consequences of using rejection sampling, we visualise using  t-SNE~\citep{tsne} the differences between the distributions of the samples generated by the standard DGMs and by DGMs+RS (i.e, with rejection sampling). These visualisations can be found in Figure~\ref{fig:t-sne}. As we can see in the Figure, there can be cases where rejection sampling actually creates some changes in the distribution, which can then affect the machine learning efficacy.}


\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{plots/border_with_Cmodels/withCmodels_WGAN_3_2_3_kc_Zipcode-Price.pdf}
    \includegraphics[width=.8\linewidth]{plots/border_with_Cmodels/withCmodels_TableGAN_2_1_2_kc_Zipcode-Price.pdf}
    \includegraphics[width=.8\linewidth]{plots/border_with_Cmodels/withCmodels_CTGAN_0_1_0_kc_Zipcode-Price.pdf}
    \includegraphics[width=.8\linewidth]{plots/border_with_Cmodels/withCmodels_TVAE_0_3_2_kc_Zipcode-Price.pdf}
        \includegraphics[width=.8\linewidth]{plots/border_with_Cmodels/withCmodels_GOGGLE_0_1_2_kc_Zipcode-Price.pdf}
    \caption{Comparison of sample distributions between real data and synthetic data generated by the unconstrained DGM models and their corresponding DGM+LL and DGM+\lsymb{} models, using the \textsl{Zipcode} and \textsl{Price} features from the \house{} dataset. In order (from the top to the bottom row), the DGM models used in the plots are: \wgan, \tablegan, \ctgan, \tvae, and \goggle. The areas in red  indicate regions where samples violate the constraints on the given features.}     \label{fig:background_knowledge_alignment}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.78\linewidth]{plots/URL_border_with_Cmodels/URL_withCmodels_WGAN_3_2_3_url_.pdf}
    \includegraphics[width=.78\linewidth]{plots/URL_border_with_Cmodels/URL_withCmodels_TableGAN_2_1_2_url_.pdf}
    \includegraphics[width=.78\linewidth]{plots/URL_border_with_Cmodels/URL_withCmodels_CTGAN_0_1_0_url_.pdf}
    \includegraphics[width=.78\linewidth]{plots/URL_border_with_Cmodels/URL_withCmodels_TVAE_3_4_2_url_.pdf}
        \includegraphics[width=.78\linewidth]{plots/URL_border_with_Cmodels/URL_withCmodels_GOGGLE_4_0_0_url_.pdf}
    \caption{Comparison of sample distributions between real data and synthetic data generated by the unconstrained DGM models and their corresponding DGM+LL and DGM+\lsymb{} models, using the \textsl{No. subdomains} and \textsl{Hostname length} features from the \phishing{} dataset. In order (from the top to the bottom row), the DGM models used in the plots are: \wgan, \tablegan, \ctgan, \tvae, and \goggle. The areas in red  indicate regions where samples violate the constraints on the given features.}     \label{fig:url_background_knowledge_alignment}
\end{figure}



    

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
        \includegraphics[trim={1cm 0 1cm 1.2cm},clip,width=\textwidth]{img/tsne/res_WGAN.png}

         \caption{}
         \label{fig:t-sne_ccs_wgan}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[trim={1cm 0 1cm 1.2cm},clip,width=\textwidth]{img/tsne/res_TableGAN.png}
         \caption{}
         \label{fig:t-sne_ccs_tablegan}
     \end{subfigure}\\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
        \includegraphics[trim={1cm 0 1cm 1.2cm},clip,width=\textwidth]{img/tsne/res_CTGAN.png}
         \caption{}
         \label{fig:t-sne_ccs_ctgan}
     \end{subfigure}
          \begin{subfigure}[b]{0.49\textwidth}
         \centering
      \includegraphics[trim={1cm 0 1cm 1.2cm},clip,width=\textwidth]{img/tsne/res_TVAE.png}
         \caption{}
         \label{fig:t-sne_ccs_tvae}
              \end{subfigure}
          \begin{subfigure}[b]{0.49\textwidth}
                 \centering
    \includegraphics[trim={1cm 0 1cm 1.2cm},clip,width=\textwidth]{img/tsne/res_GOGGLE.png}
         \caption{}
         \label{fig:t-sne_ccs_goggle}
     \end{subfigure}
        \caption{t-SNE visualisations of the distribution of the samples generated for the CCS dataset by (i) WGAN and WGAN+RS in Figure~\ref{fig:t-sne_ccs_wgan}, (ii) TableGAN and TableGAN+RS in Figure~\ref{fig:t-sne_ccs_tablegan}, (iii) CTGAN and CTGAN+RS in Figure~\ref{fig:t-sne_ccs_ctgan}, (iv) TVAE and TVAE+RS in Figure~\ref{fig:t-sne_ccs_tvae}, and (v) GOGGLE and GOGGLE+RS in Figure~\ref{fig:t-sne_ccs_goggle}. Note that changes in distribution are marked with red contours.}
        \label{fig:t-sne}
\end{figure}

\section{Sample generation time}
\label{appdx:runtime}
\ktdt{As we can see from Table~\ref{tab:runtime_appdx}, the DGM+\lsymb{} models bring additional time to the sample generation runtimes. However, this is often much smaller than the additional time required to use an unconstrained model and then doing rejection sampling. 
Indeed, the largest runtime difference registered between a unconstrained DGM and its constrained counterpart is of only 0.12 seconds (i.e., for \ctgan{} on the \phishing{} and \lcld{} datasets).
Notably, in 20 out of 25 cases, the overhead for the DGM+\lsymb{} models is less than 0.1 seconds. On the other hand, for the DGM+RS models, the sample generation procedure timed out after 24h for all the models tested on the House dataset (where we had 100\% CVR). The registered times are also not very promising for any of the other datasets when using DGM+RS, where the minimum absolute difference registered equals 0.07 seconds and the maximum equals 5.55 seconds (notice that no DGM nor DGM+DRL model has sampling time above 0.29 seconds).}%


\input{results/main/runtimes}

\section{Real Data Performance}
\label{appdx:real_data_efficiency}
In Table~\ref{tab:real-utility} we report the average F1-score, weighted F1-score and Area Under the ROC Curve obtained by training the same six classifiers (resp. four regressors) on the four real classification datasets (resp. real regression dataset). This allows us to compare the machine learning efficacy of the synthetic data with one of the real data. As we can see from the results, the synthetic data generated with all the DGMs (unconstrained, +LL and +DRL) manage to obtain very good results. In multiple occasions, the models trained on the synthetic data not only  get results comparable with the ones obtained with the real data, but actually perform better than them. 
In spite of this, the classification models trained on synthetic data never manage to get better performance than those trained on the real data for all metrics. On the other hand, the regressors trained on the synthetic version of the House dataset always manage to get lower MAE and RMSE, no matter the DGM used to generate the synthetic data (with the exception of TVAE+LL which got slightly higher MAE than the one obtained with the real data). 

\section{Comparison between DGMs+\lsymb{} and LLM-based Tabular Data Generation}
\begin{wraptable}{r}{0.62\linewidth}
 \vspace{-0.45cm}
 \centering
\caption{\cvr{} for each model and dataset. Cases with \cvr{}$\ge$50\% are \underline{underlined}. Best results are in bold. 
}
\footnotesize
 \setlength{\tabcolsep}{2.6pt}
 \vspace{-0.2cm}
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
 & \phishing{}            & \cervical{}          & \lcld{}        & \heloc{}           & \house{}                    \\ \midrule
 \ktdt{GreAT} & \ktdt{0.7\msmall{\pm0.2}}	 &\ktdt{\underline{98.0\msmall{\pm0.3}}}	& \ktdt{1.1\msmall{\pm0.1}}	 & \ktdt{9.60\msmall{\pm0.5}} &	\ktdt{15.7\msmall{\pm1.4}} \\
\midrule
All + \lsymb     & \textbf{0.0\msmall{\pm0.0}}   & \textbf{0.0\msmall{\pm0.0}}   & \textbf{0.0\msmall{\pm0.0}}    & \textbf{0.0\msmall{\pm0.0}}  & \textbf{0.0\msmall{\pm0.0}}      \\ \bottomrule
\end{tabular}
 \vspace{-0.1cm}
\label{tab:greatvsDRL}
\end{wraptable}
\ktdt{A recent trend in the tabular data generation field has been to use LLMs to perform the task. While 
these models are very promising, they are also not exempt from the 
problems that affect the other DGMs. In this section we thus compare the performance of the standard DGMs equipped with our \lsymb{} and GreAT~\citep{borisov2023great}, a state-of-the-art LLM-based tabular data generator. For all datasets considered, GreAT was trained and run on an A100 GPU with 40GB of RAM, using the pre-defined hyperparameters.}

\ktdt{Firstly, in Table~\ref{tab:greatvsDRL}, we report the CVR obtained with GreAT and with all the models+\lsymb{}. As we can see from the Table, even though in two out of five cases GreAT manages to get a very low CVR, for CCS its CVR shoots up to 98.0\%. As CCS is by far our smallest dataset (with only 1K datapoints in the training set), this hints to the fact that LLM-based models struggle to learn the distribution from datasets with few datapoints.}

\begin{wraptable}{r}{0.52\linewidth}
 \centering
 \vspace{-0.45cm}
\caption{Sample generation time in seconds.
}
\footnotesize
 \setlength{\tabcolsep}{2.9pt}
  \vspace{-0.2cm}
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
 & \phishing{}            & \cervical{}          & \lcld{}        & \heloc{}           & \house{}                    \\ \midrule
 \ktdt{GreAT} & 51.8 & 	28.4 &	22.3 &	26.3 & 	14.4 \\
\midrule
DGM+\lsymb &0.22	&0.13	&0.14&	0.10&	0.13\\
\bottomrule
\end{tabular}
\vspace{-0.15cm}
\label{tab:great_sampling_time}
\end{wraptable}
\ktdt{Secondly, we generate 1,000 samples with GreAT and we report the average runtime in Table~\ref{tab:great_sampling_time} together with the average sampling time obtained with the DGMs+\lsymb.}
\ktdt{As we can see from the Table, GreAT takes two orders of magnitude longer to perform the sampling than the average DGM+\lsymb{} model. This difference is particularly striking given that GreAT was the only model requiring an A100 to run.}

\ktdt{This analysis shows that not only LLM-based are still prone to the violation of the constraints, but also that they require more powerful hardware and much longer time to sample.}


\input{results/appendix/real_data_efficiency}



\end{document}
