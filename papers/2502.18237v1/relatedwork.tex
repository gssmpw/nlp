\section{Related Work}
Our work lies at the intersection of two fields: Neuro-symbolic AI and tabular data generation. Thus, our related work section will mirror this duality.

\paragraph{Neuro-symbolic AI.} Neuro-symbolic AI~\citep{raedt_survey,third_wave} refers to the broad area of AI
that combines the strengths of symbolic reasoning with neural networks. %
As our work falls into the more specific field of injection of background knowledge into neural models, (see, e.g.,~\citep{stewart2017,
hoernle2022multiplexnet,giunchiglia2023manifesto,daniele2023_refining,calanzone2024logicallyconsistentlanguagemodels}) we will focus the discussion on this topic.
Many methods for this task are based on the intuition that logical constraints can be transformed into differentiable loss function terms that penalise the networks for violating them (see, e.g.,~\citep{xu2018semantic,serafini2022ltn,diligenti2012SBR,fischer2019DL2}). As expected, since these methods operate at a loss level, they give no guarantee that the constraints will be satisfied. Other works manage to integrate neural networks and probabilistic reasoning through the mapping of predicates appearing in logical formulae to neural networks~\citep{manhaeve2018deepproblog,yang2020neurasp,sachan2018nuts_and_bolts,pryor2023NeuPSL,krieken2023anesi}. This allows these methods to both perform reasoning on the networks' predictions as well as constrain the output according to the background knowledge. The most similar line of work to ours is the one where the constraints in input are automatically compiled into neural layers~\citep{giunchiglia2021jair,ahmed2022spl,giunchiglia2024ccn+}. However, these methods can compile and incorporate constraints that are at best as expressive as propositional logic formulae. %
Focusing specifically on the incorporation of constraints on generic generative models, we can find the work by~\cite{stoian2024}, where the tabular generation process was simply constrained by linear inequalities. If we consider different application domains, we can find the work proposed by~\cite{misino2023tvael}, where ProbLog~\citep{Raed2007_problog} works in tandem with variational-autoencoders, and the one by~\cite{liello2020}, where the authors incorporate propositional logic constraints on GANs for structured objects generation. %

\paragraph{Tabular Data Generation.} In recent years, various DGMs have been proposed to tackle the problem of tabular data synthesis. Many of these approaches are based on Generative Adversarial Networks (GANs), like TableGAN~\citep{park2018_tableGAN}, CTGAN~\citep{xu2019_CTGAN}, IT-GAN~\citep{lee2021invertible}, OCT-GAN~\citep{kim2021oct}, and PacGAN~\citep{lin2018pacgan}.
Other methods try to reduce the problems that often characterise GANs, such as mode collapse and unstable training, by introducing Variational AutoEncoders (VAEs) based models, see, e.g.,~\citep{xu2018semantic,srivastava2017_veegan,wan2017vae_imbalanced}. An alternative solution to such problems is given by the usage of denoising diffusion probabilistic models as done in~\citep{kotelnikov2023tabddpm} or~\citep{kim2023stasy}, where the authors designed a self-paced learning method and a fine-tuning approach to adapt the standard score-based generative modeling to the challenges of tabular data generation. Finally, GOGGLE~\citep{liu2022goggle} uses graph learning to infer relational structure from the data and use it to their advantage especially in data-scarce setting. Since synthetic tabular data are often used to replace the original dataset to preserve privacy in sensitive settings, a parallel line of research revolves around the development of DGMs with privacy guarantees. Examples of models that have such privacy guarantees are PATEGAN~\citep{yoon2020_privacy}  and DP-CGAN~\citep{rehaneh2020dpcgan_privacy}.