\section{Related work}
\subsection{Self-Supervised Representation Learning}
Self-supervised representation learning leverages the inherent structure within data to generate supervisory signals, thereby mitigating the need for extensive labeled datasets. A prominent approach in this field is contrastive learning, which has demonstrated significant success in learning robust representations. Methods such as InstDis \citep{wu2018unsupervised}, SimCLR \citep{chen2020Simple}, and the MoCo series \citep{he2020Momentum, chen2021empirical} focus on contrasting positive pairs of similar instances against negative pairs of dissimilar instances to learn effective features. In contrast, BYOL \citep{grill2020bootstrap}, SimSiam \citep{chen2021Exploring}, and DINO \citep{caron2021emerging} improve performance by avoiding negative samples altogether and adopting a self-distillation approach. These methods have achieved notable results in various visual tasks, such as image classification and object detection, showcasing the effectiveness of self-supervised learning to perform exceptionally well with large-scale unlabeled data. However, despite these successes, existing self-supervised learning methods predominantly focus on static images without considering the spatiotemporal context inherent in certain datasets, such as urban environments captured over time and space. The lack of integration of spatiotemporal information limits the models' ability to capture dynamics over time and across spatial regions, especially in tasks requiring an understanding of both spatial and temporal dependencies. Therefore, there is a need for self-supervised learning approaches that effectively incorporate spatiotemporal information to enhance performance in such tasks.

\subsection{Spatiotemporal Contrastive Learning in Vision Tasks}
Spatiotemporal contrastive learning enhances traditional contrastive learning by integrating both spatial and temporal information, enabling models to capture underlying relationships in unlabeled data that vary over space and time.

Temporal contrastive learning excels in sequential data by differentiating between related and unrelated frames. For example, Contrastive Predictive Coding (CPC)~\citep{oord2019representation} applies temporal contrastive learning by using consecutive video frames as positive pairs and shuffled or temporally distant frames as negative pairs, helping models learn temporal coherence. 
% Similarly, TCGL~\citep{liu2022tcgl} captures multi-scale temporal dependencies in videos using graph-based contrastive learning to enhance temporal representation learning, while LSTCL~\citep{wang2022longshort} aligns long-term and short-term views of videos to improve clip-level representations for video understanding tasks.
SeCo~\citep{manas2021seasonal} and GeoSSL \citep{ayush2021geographyaware} use multi-season remote sensing images for self-supervised pre-training, enhancing model performance in remote sensing tasks.


Spatial contrastive learning improves a model's ability to represent spatial scenes from various angles, perspectives, and locations. Multi-view contrastive learning approach is typically applied within a single scene from multiple angles at one location~\citep{tian2020contrastive}.
Building on these concepts, geospatial contrastive learning contrasts data from different geographic locations or regions. By ensuring that data from similar spatial locations are closer in the feature space while data from different regions are more distant, models can more effectively capture spatial patterns and geographic features~\citep{deuser2023sample4geo,klemmer2024satclip,mai2023csp,guo2024spatialscene2vec}. This approach enhances the understanding of spatial relationships across wider geographic contexts.

\subsection{Street View Representation Learning for Downstream Tasks}
Street view imagery has been widely used in various urban downstream tasks, such as road defect detection~\citep{chacra2018municipal}, urban function recognition~\citep{huang2023comprehensive}, and socioeconomic prediction~\citep{fan2023urban}. However, existing research on street view representation often relies on supervised models trained on datasets like Places365~\citep{zhou2017places} or directly uses the pixel proportions of semantic segmentation results. These approaches fail to fully capture the rich semantic information embedded in street view imagery. Unlike natural images, street view imagery not only contains complex visual semantics but also encodes valuable spatiotemporal information in its metadata. Effectively representing this dual semantic nature -- both visual and spatiotemporal -- remains a significant challenge for improving its use in urban downstream tasks. Although a few studies have explored spatiotemporal self-supervised learning approaches to represent street view imagery~\citep{stalder2024selfsupervised}, these methods still have limitations. For instance, Urban2Vec~\citep{wang2020urban2vec} incorporates spatial information into self-supervised training by constructing positive sample pairs based on nearest neighbors, while KnowCL~\citep{liu2023knowledgeinfused} integrates knowledge graphs with contrastive learning to align locale and visual semantics, improving the accuracy of socioeconomic prediction using street view imagery. However, these approaches fail to explore the natural meanings of the spatiotemporal attributes of street view imagery and how to leverage these attributes to construct self-supervised methods suitable for various downstream tasks.