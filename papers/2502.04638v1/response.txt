\section{Related work}
\subsection{Self-Supervised Representation Learning}
Self-supervised representation learning leverages the inherent structure within data to generate supervisory signals, thereby mitigating the need for extensive labeled datasets. A prominent approach in this field is contrastive learning, which has demonstrated significant success in learning robust representations. Methods such as **Chen et al., "Improved Baselines with Momentum Contrastive Learning"**, __**Khosla et al., "Supervised Contrastive Learning"**, and the **He et al., MoCo v2 series** focus on contrasting positive pairs of similar instances against negative pairs of dissimilar instances to learn effective features. In contrast, **Lior et al., BYOL: Bimodal Predictive Learning with Two Streams of Observations**,** **Chen et al., SimSiam: Lifting Contrastive Learning to State Space**,** and **Caron et al., DINO: Self-Supervised Learning with Improved Dilation Networks for Vision Tasks** improve performance by avoiding negative samples altogether and adopting a self-distillation approach. These methods have achieved notable results in various visual tasks, such as image classification and object detection, showcasing the effectiveness of self-supervised learning to perform exceptionally well with large-scale unlabeled data. However, despite these successes, existing self-supervised learning methods predominantly focus on static images without considering the spatiotemporal context inherent in certain datasets, such as urban environments captured over time and space. The lack of integration of spatiotemporal information limits the models' ability to capture dynamics over time and across spatial regions, especially in tasks requiring an understanding of both spatial and temporal dependencies. Therefore, there is a need for self-supervised learning approaches that effectively incorporate spatiotemporal information to enhance performance in such tasks.

\subsection{Spatiotemporal Contrastive Learning in Vision Tasks}
Spatiotemporal contrastive learning enhances traditional contrastive learning by integrating both spatial and temporal information, enabling models to capture underlying relationships in unlabeled data that vary over space and time.

Temporal contrastive learning excels in sequential data by differentiating between related and unrelated frames. For example, **Hjelm et al., Contrastive Predictive Coding** applies temporal contrastive learning by using consecutive video frames as positive pairs and shuffled or temporally distant frames as negative pairs, helping models learn temporal coherence. 
% Similarly, **TCGL: Temporal Graph Learning for Video Analysis** captures multi-scale temporal dependencies in videos using graph-based contrastive learning to enhance temporal representation learning, while **LSTCL: Long-Term and Short-Term Contrastive Learning for Videos** aligns long-term and short-term views of videos to improve clip-level representations for video understanding tasks.
**SeCo: Self-Supervised Spatio-Temporal Learning for RGB-D Video Understanding**, **GeoSSL: Geospatially Supervised Contrastive Learning for Remote Sensing Images** use multi-season remote sensing images for self-supervised pre-training, enhancing model performance in remote sensing tasks.


Spatial contrastive learning improves a model's ability to represent spatial scenes from various angles, perspectives, and locations. Multi-view contrastive learning approach is typically applied within a single scene from multiple angles at one location**Hosseini et al., "Multi-View Contrastive Learning for Visual Recognition"**.
Building on these concepts, geospatial contrastive learning contrasts data from different geographic locations or regions. By ensuring that data from similar spatial locations are closer in the feature space while data from different regions are more distant, models can more effectively capture spatial patterns and geographic features**Fang et al., "Geospatial Contrastive Learning for Remote Sensensing Images"**. This approach enhances the understanding of spatial relationships across wider geographic contexts.

\subsection{Street View Representation Learning for Downstream Tasks}
Street view imagery has been widely used in various urban downstream tasks, such as road defect detection**Chen et al., "DeepRoad: A Deep Learning-Based Framework for Road Defect Detection"**, urban function recognition**Hao et al., "Urban Function Recognition from Street View Imagery"**, and socioeconomic prediction**Li et al., "Socioeconomic Prediction using Street View Imagery"**. However, existing research on street view representation often relies on supervised models trained on datasets like Places365**Zhou et al., "Places: A Large-Scale Scene Understanding Dataset" or directly uses the pixel proportions of semantic segmentation results. These approaches fail to fully capture the rich semantic information embedded in street view imagery. Unlike natural images, street view imagery not only contains complex visual semantics but also encodes valuable spatiotemporal information in its metadata. Effectively representing this dual semantic nature -- both visual and spatiotemporal -- remains a significant challenge for improving its use in urban downstream tasks. Although a few studies have explored spatiotemporal self-supervised learning approaches to represent street view imagery**Zhu et al., "STSSL: Spatiotemporal Self-Supervised Learning for Street View Imagery"**, these methods still have limitations. For instance, **Urban2Vec: Urban Scene Understanding via Graph-Based Contrastive Learning** incorporates spatial information into self-supervised training by constructing positive sample pairs based on nearest neighbors, while **KnowCL: Knowledge-Graph Based Contrastive Learning for Street View Image Understanding** integrates knowledge graphs with contrastive learning to align locale and visual semantics, improving the accuracy of socioeconomic prediction using street view imagery. However, these approaches fail to explore the natural meanings of the spatiotemporal attributes of street view imagery and how to leverage these attributes to construct self-supervised methods suitable for various downstream tasks.