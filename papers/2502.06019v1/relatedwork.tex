\section{Related Work}
\noindent\textbf{Zero-Shot VLM Generalization:}
Pre-trained on large image-text datasets in a self-supervised way, VLMs such as CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} have shown strong generalization capabilities. For example, CLIP's remarkable zero-shot transfer performance can be attributed to the diversity and scale of the data on which it was trained. Nonetheless, adapting them effectively to specific downstream tasks when data is scarce remains a challenge. One straightforward yet effective approach to enhance CLIP’s zero-shot performance on image classification is the use of \textit{soft prompts} \cite{zhou2022coop} which are \textit{learned} in a few-shot training setup. 

% VLMs still face challenges with distribution shifts \cite{taori2020measuring,hendrycks2021natural} when training and test domains differ, such as in image styles, new object types, or subtle language variations. These shifts can degrade performance, affecting model reliability in real-world scenarios.

%Various few-shot approaches have been introduced to improve zero-shot generalization in VLMs. For instance, CoOp  fine-tunes CLIP by learning a set of prompts within the text encoder, while CoCoOp \cite{zhou2022cocoop} addresses CoOp’s limitations in generalization by dynamically conditioning text prompt tokens on image embeddings. 
% MaPLe [26] advances this by jointly learning deep prompts in both vision and text encoders.
% However, despite these improvements, many methods still depend on pre-trained weights, which can be problematic in real-world applications where no target domain training data is available.

\vspace{0.2cm}
\noindent\textbf{Test-Time Optimization:}
% Test-time adaptation approaches have become increasingly important for handling unseen or shifted data distributions. Test-time training and its variants \cite{shu2022test, zhao2023test} incorporate feedback mechanisms at inference, allowing models to update weights dynamically. For instance, RLCF \cite{zhao2023test} adapts Vision-Language Models (VLMs) at test time by using a reinforcement learning-based reward signal from CLIP to guide model outputs, enhancing zero-shot generalization across diverse visual tasks. Test-Time Prompt Tuning (TPT) \cite{shu2022test} introduces the concept of learnable prompts that adapt VLMs to new domains without modifying the model's core parameters. TPT optimizes prompt embeddings in response to incoming test data, improving alignment with the pre-trained model's feature space and enabling the model to handle unseen data distributions effectively. Building on these approaches, we propose a novel direction by adding learnable noise to test-time samples. This lightweight, adaptable method directly augments test data, mitigating distribution shifts without adjusting model weights or embeddings.
% Test-time adaptation (TTA) plays a key role in enhancing the generalization of vision-language models (VLMs) like CLIP to unseen data distributions. 
Approaches such as TPT \cite{shu2022test} adjust prompts at test time to reduce entropy across augmented views of a single test sample, improving accuracy without additional training data. However, TPT does not address \textit{model calibration}, which is essential for uncertainty estimation.
To remedy this, C-TPT \cite{yooncctpt} improves calibration by optimizing prompt selection based on the dispersion of text features, eliminating the need for labeled data. Reinforcement learning with CLIP feedback (RLCF) \cite{zhaotestrlcf} further enhances generalization by offering continuous feedback during TTA, correcting predictions and preventing overconfidence associated with entropy minimization in TPT. Sample-wise Temperature Scaling (SaLS) \cite{murugesan2024robust} modifies temperature scaling during TTA on top of TPT to boost calibration, refining the model’s confidence. CALIP \cite{guo2023calip}, enhances CLIP’s zero-shot performance by incorporating an attention module that enables interaction between visual and textual features, all without requiring additional training or parameters.

\vspace{0.2cm}
\noindent\textbf{Noise-based Learning:} 
While learnable noise has not been explored for TTA, two recent works -- BadCLIP \cite{bai2024badclip} and BAPLE \cite{hanif2024baple} -- have used it to inject backdoor triggers into the image encoder's input within a few-shot training setup. These approaches introduce learnable noise during the prompt learning stage to compromise the VLM, demonstrating that simply adding noise can alter the model's behavior. This insight raises an intriguing question: \textit{could such noise be harnessed positively}?

% Learnable noise has been investigated in generative models, including Generative Adversarial Networks \cite{goodfellow2014generative}, Variational Autoencoders \cite{kingma2013auto}, and diffusion models \cite{ho2020denoising, rezende2014stochastic, radford2015unsupervised}. In these models, noise functions as a latent variable or random vector, manipulated during training to guide output generation. While noise is often seen as disruptive to machine learning models, recent studies indicate its .
% % Outside generative modeling, noise-based learning has been applied to implicitly improve augmentations in training contrastive models. For example, noise modulation in contrastive learning has been investigated to boost performance in zero-shot and few-shot scenarios \cite{chen2020simple, he2020momentum}.


% Current literature lacks extensive research on learnable noise for domain adaptation and zero-shot generalization. BAPLe \cite{hanif2024baple} has recently been introduced for adversarial robustness in Vision-Language Models (VLMs). BAPLe implements a trainable noise trigger within input images alongside learnable prompts in the text encoder, effectively embedding a backdoor that modulates the model's sensitivity to specific inputs. By adjusting the noise trigger, BAPLe directs VLM responses to compromised data without impacting clean data performance. This method, focused on security concerns in prompt learning and noise injection, provides a basis for exploring learnable noise in controlled test-time adaptation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/method_main2.pdf}
    \vspace{-0.50cm}
    \caption{\textbf{Test-Time Noise Tuning (TNT)} (1) generates augmented views of a test image, (2) applies adaptive \textit{learnable} noise, and (3) computes logits and feature vectors for each view. (4) Top-\(K\) views are selected by confidence, with (5) entropy loss [Eq. \ref{eq:entropy}] enforcing confident predictions and (6) inter-view consistency loss [Eq. \ref{eq:Lvc_loss}] aligning feature representations. (7) The combined loss is backpropagated to iteratively refine the noise, enabling adaptive test-time noise tuning.}
    \label{fig:method}
    \vspace{-0.20cm}
\end{figure*}