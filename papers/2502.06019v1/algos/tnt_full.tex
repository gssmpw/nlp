% \begin{minipage}[t]{0.52\textwidth}
\begin{algorithm}[t]
\caption{PyTorch style Pseudocode for TNT}
\label{alg:tnt}
\vspace{-1.ex}
\begin{lstlisting}[style=Pytorch,escapeinside={(@}{@)}]
# image = single test image 
# model = pre-trained VLM
# eps = perturbation budget for noise
# lr = learning rate to update noise
# top_k = number of top-K views
# t = number of steps
def TNT(model, image, top_k, t):
  noise = torch.randn(image.shape)
  noise = noise.clamp(-eps,eps)
  # Get N augmented views of test image
  images = augment(image) # (N,C,H,W)
  for step in range(t)
     # Add noise to augmented views of test image
     # and get logits and views' feature vectors
     logits, images_feats = model(images + noise)
     # Get indices of top-K views 
     k_indices = confidence_filter(logits, top_k)
     # Compute entropy loss on top-K views
     loss_e = entropy(logits[k_indices])
     # Compute inter-view consistency loss
     loss_vc = vc_loss(images_feats[k_indices]) 
     loss = loss_e + loss_vc
     loss.backward()
     # Update the learnable noise
     noise = noise - torch.sign(noise.grad)*lr
     noise = noise.clamp(-eps,eps)
  # Inference after noise adaptation
  logits, _ = model(images + noise)
  # Apply temperature scaling on top-K views
  probs = (logits[k_indices]/tau).softmax()
  predicted_label =  argmax(probs.mean(dim=0))
  return predicted_label
\end{lstlisting}
\vspace{-1.ex}
\end{algorithm}
% \end{minipage}
