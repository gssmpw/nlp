% \begin{minipage}[t]{0.52\textwidth}
\begin{algorithm}[t]
\caption{PyTorch style Pseudocode for TNT}
\label{alg:tnt}
\vspace{-1.ex}
\begin{lstlisting}[style=Pytorch,escapeinside={(@}{@)}]
# image = single test image 
# model = pre-trained VLM
# eps = perturbation budget for noise
# lr = learning rate to update noise
# top_k = number of top-K views
# t = number of steps
def TNT(model, image, top_k, t):
  noise = torch.randn(image.shape)
  noise = noise.clamp(-eps,eps)
  # Get N augmented views of test image
  images = augment(image) # (N,C,H,W)
  for step in range(t)
    # Add noise to augmented views of test image 
    images = images + noise
    # Get logits and feature vectors of all views 
    logits, images_feats = model(images)
    # Get indices of top-K views 
    k_indices = confidence_filter(logits, top_k)
    # Compute entropy loss on top-K views
    loss_e = entropy(logits[k_indices])
    # Compute inter-view consistency loss
    loss_vc = vc_loss(images_feats[k_indices]) 
    loss = loss_e + loss_vc
    loss.backward()
    # Update the learnable noise
    noise = noise - torch.sign(noise.grad)*lr
    noise = noise.clamp(-eps,eps)
return noise
\end{lstlisting}
\vspace{-1.ex}
\end{algorithm}
% \end{minipage}
