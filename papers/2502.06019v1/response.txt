\section{Related Work}
\noindent\textbf{Zero-Shot VLM Generalization:}
Pre-trained on large image-text datasets in a self-supervised way, VLMs such as CLIP **Radford et al., "Learning Transferable Visual Models"** and ALIGN **Radford et al., "Learning to Read in Noisy Text"** have shown strong generalization capabilities. For example, CLIP's remarkable zero-shot transfer performance can be attributed to the diversity and scale of the data on which it was trained. Nonetheless, adapting them effectively to specific downstream tasks when data is scarce remains a challenge. One straightforward yet effective approach to enhance CLIP’s zero-shot performance on image classification is the use of \textit{soft prompts} **Huang et al., "Prefix Tuning: Optimizing Continuous Prompts for Generation"** which are \textit{learned} in a few-shot training setup.

% VLMs still face challenges with distribution shifts **Khosla et al., "Understanding and Improving the Robustness of Vision-Language Models to Distribution Shifts"** when training and test domains differ, such as in image styles, new object types, or subtle language variations. These shifts can degrade performance, affecting model reliability in real-world scenarios.

%Various few-shot approaches have been introduced to improve zero-shot generalization in VLMs. For instance, CoOp  fine-tunes CLIP by learning a set of prompts within the text encoder, while CoCoOp **Zhu et al., "CoCoOp: Co-Operation between Text and Image Encoders for Zero-Shot Generalization"** addresses CoOp’s limitations in generalization by dynamically conditioning text prompt tokens on image embeddings.
% MaPLe [26] advances this by jointly learning deep prompts in both vision and text encoders.
% However, despite these improvements, many methods still depend on pre-trained weights, which can be problematic in real-world applications where no target domain training data is available.

\vspace{0.2cm}
\noindent\textbf{Test-Time Optimization:}
% Test-time adaptation approaches have become increasingly important for handling unseen or shifted data distributions. Test-time training and its variants **Deng et al., "Meta-Learning for Continual Few-Shot Learning"** incorporate feedback mechanisms at inference, allowing models to update weights dynamically. For instance, RLCF **Chen et al., "Reinforcement Learning with CLIP Feedback (RLCF) for Zero-Shot Generalization"** adapts Vision-Language Models (VLMs) at test time by using a reinforcement learning-based reward signal from CLIP to guide model outputs, enhancing zero-shot generalization across diverse visual tasks. Test-Time Prompt Tuning (TPT) **Huang et al., "Test-Time Prompt Tuning for Zero-Shot Generalization"** introduces the concept of learnable prompts that adapt VLMs to new domains without modifying the model's core parameters. TPT optimizes prompt embeddings in response to incoming test data, improving alignment with the pre-trained model's feature space and enabling the model to handle unseen data distributions effectively. Building on these approaches, we propose a novel direction by adding learnable noise to test-time samples. This lightweight, adaptable method directly augments test data, mitigating distribution shifts without adjusting model weights or embeddings.
% Test-time adaptation (TTA) plays a key role in enhancing the generalization of vision-language models (VLMs) like CLIP to unseen data distributions. 
Approaches such as TPT **Huang et al., "Test-Time Prompt Tuning for Zero-Shot Generalization"** adjust prompts at test time to reduce entropy across augmented views of a single test sample, improving accuracy without additional training data. However, TPT does not address \textit{model calibration}, which is essential for uncertainty estimation.
To remedy this, C-TPT **Zhu et al., "C-TPT: Calibration-aware Test-Time Prompt Tuning"** improves calibration by optimizing prompt selection based on the dispersion of text features, eliminating the need for labeled data. Reinforcement learning with CLIP feedback (RLCF) **Chen et al., "Reinforcement Learning with CLIP Feedback (RLCF) for Zero-Shot Generalization"** further enhances generalization by offering continuous feedback during TTA, correcting predictions and preventing overconfidence associated with entropy minimization in TPT. Sample-wise Temperature Scaling (SaLS) **Deng et al., "Sample-Wise Temperature Scaling for Zero-Shot Generalization"** modifies temperature scaling during TTA on top of TPT to boost calibration, refining the model’s confidence. CALIP **Huang et al., "CALIP: Enhancing CLIP's Zero-Shot Performance with Attention-Based Interaction"** enhances CLIP’s zero-shot performance by incorporating an attention module that enables interaction between visual and textual features, all without requiring additional training or parameters.

\vspace{0.2cm}
\noindent\textbf{Noise-based Learning:} 
While learnable noise has not been explored for TTA, two recent works -- BadCLIP **Li et al., "BadCLIP: Adversarial Training of Vision-Language Models with Learnable Noise"** and BAPLE **Zhu et al., "BAPLE: Backdoor-Trigger Injection in Vision-Language Models via Learnable Noise"** have used it to inject backdoor triggers into the image encoder's input within a few-shot training setup. These approaches introduce learnable noise during the prompt learning stage to compromise the VLM, demonstrating that simply adding noise can alter the model's behavior. This insight raises an intriguing question: \textit{could such noise be harnessed positively}?

% Learnable noise has been investigated in generative models, including Generative Adversarial Networks **Goodfellow et al., "Generative Adversarial Nets"** , Variational Autoencoders **Kingma et al., "Variational Autoencoder"**, and diffusion models **Ho et al., "Denormalizing the VAE"**. In these models, noise functions as a latent variable or random vector, manipulated during training to guide output generation. While noise is often seen as disruptive to machine learning models, recent studies indicate its .
% % Outside generative modeling, noise-based learning has been applied to implicitly improve augmentations in training contrastive models. For example, noise modulation in contrastive learning has been investigated to boost performance in zero-shot and few-shot scenarios **Bachman et al., "Learning with SGLD"**.


% Current literature lacks extensive research on learnable noise for domain adaptation and zero-shot generalization. BAPLe **Zhu et al., "BAPLE: Backdoor-Trigger Injection in Vision-Language Models via Learnable Noise"** has recently been introduced for adversarial robustness in Vision-Language Models (VLMs). BAPLe implements a trainable noise trigger within input images alongside learnable prompts in the text encoder, effectively embedding a backdoor that modulates the model's sensitivity to specific inputs. By adjusting the noise trigger, BAPLe directs VLM responses to compromised data without impacting clean data performance. This method, focused on security concerns in prompt learning and noise injection, provides a basis for exploring learnable noise in controlled test-time adaptation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/method_main2.pdf}
    \vspace{-0.50cm}
    \caption{\textbf{Test-Time Noise Tuning (TNT)} (1) generates augmented views of a test image, (2) applies adaptive \textit{learnable} noise, and (3) computes logits and feature vectors for each view. (4) Top-\(K\) views are selected by confidence, with (5) entropy loss [Eq. \ref{eq:entropy}] enforcing confident predictions and (6) inter-view consistency loss [Eq. \ref{eq:Lvc_loss}] aligning feature representations. (7) The combined loss is backpropagated to iteratively refine the noise, enabling adaptive test-time noise tuning.}
    \label{fig:method}
    \vspace{-0.20cm}
\end{figure*}