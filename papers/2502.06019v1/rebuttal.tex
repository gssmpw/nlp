\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}
\usepackage{setspace}
\setstretch{0.95}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
% \usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

\newcommand{\inc}[1]{\ensuremath{_{\text{\textcolor{green}{(+#1)}}}}}
\newcommand{\heading}[1]{\vspace{0.5mm}\noindent\underline{\textbf{#1}}\vspace{-0.2mm}}
\usepackage{bm}
\newcommand{\dec}[1]{\ensuremath{_{\text{\textcolor{magenta}{(#1)}}}}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[font=small]{caption}
\usepackage{multirow}
\usepackage{pythonhighlight}
% \definecolor{LightCyan}{RGB}{247, 223, 231}
\definecolor{Lightgreen}{RGB}{218, 246, 230 }
% \newcolumntype{a}{>{\columncolor{LightCyan}}c}
\definecolor{Gray}{gray}{0.90}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[algo2e]{algorithm2e}

\definecolor{Gray}{gray}{0.90}
\definecolor{white}{rgb}{1.0, 1.0, 1.0}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{section}{0}
\renewcommand{\thesection}{A\arabic{section}}



% \newcommand{\headingred}[1]{\vspace{1mm}\noindent\underline{\textcolor{red}{\textbf{#1}}}\vspace{-0.5mm}}
% \newcommand{\headingorange}[1]{\vspace{1mm}\noindent\underline{\textcolor{orange}{\textbf{#1}}}\vspace{-0.5mm}}
% \newcommand{\headingcyan}[1]{\vspace{1mm}\noindent\underline{\textcolor{cyan}{\textbf{#1}}}\vspace{-0.5mm}}
% \newcommand{\headinggreen}[1]{\vspace{1mm}\noindent\underline{\textcolor{myforestgreen}{\textbf{#1}}}\vspace{-0.5mm}}
\newcommand{\headingred}[1]{\vspace{1mm}\noindent{\textcolor{red}{\textbf{#1}}}\vspace{-0.5mm}}
\newcommand{\headingorange}[1]{\vspace{1mm}\noindent{\textcolor{orange}{\textbf{#1}}}\vspace{-0.5mm}}
\newcommand{\headingcyan}[1]{\vspace{1mm}\noindent{\textcolor{cyan}{\textbf{#1}}}\vspace{-0.5mm}}
\newcommand{\headinggreen}[1]{\vspace{1mm}\noindent{\textcolor{myforestgreen}{\textbf{#1}}}\vspace{-0.5mm}}
\newcommand{\question}[1]{\noindent{\textcolor{black}{\textbf{#1}}}}



\definecolor{eccvblue}{rgb}{0.12,0.49,0.85}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}

\newcommand{\stdvuno}[1]{\footnotesize{\color{black}(#1)} {\color{black}}}
\newcommand{\stdvun}[1]{\footnotesize\textcolor{black}{(#1)} \large\textcolor{myforestgreen}{$\uparrow$}}
\newcommand{\stdvuru}[1]{\footnotesize\textcolor{black}{({#1})} \large\textcolor{red}{$\downarrow$}}
\newcommand{\stdvueq}[1]{\footnotesize\textcolor{black}{({#1})} \textcolor{black}{$\approx$}}

\definecolor{citecolor}{RGB}{0, 113, 188}
\definecolor{myforestgreen}{RGB}{34, 200, 34}
\definecolor{firebrick}{rgb}{0.7, 0.13, 0.13}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{deepskyblue}{rgb}{0.0, 0.75, 1.0}
\definecolor{mypink2}{rgb}{.99,.96,.98}%浅
\definecolor{mypink1}{rgb}{.99,.93,.98}
\definecolor{mypink}{rgb}{.99,.90,.98}%深
% \definecolor{mypink}{rgb}{.99,.91,.95}
\definecolor{mygray}{rgb}{.95,.95,.95}
\definecolor{lv14}{rgb}{0.5,0.5,0.5}
\definecolor{tabvline}{HTML}{a8a495}
\definecolor{prompt_blue}{HTML}{1f78b4}
\definecolor{prompt_red}{HTML}{d45c43}
\definecolor{green_im}{rgb}{0.0, 0.5, 0.0}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16940} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\noindent We thank all the reviewers for their feedback for highlighting the innovative and novel approach (\textcolor{orange}{GsD5}, \textcolor{myforestgreen}{Tooi}) and well-written and justified work (\textcolor{orange}{GsD5}, \textcolor{red}{4Rgj}, \textcolor{myforestgreen}{Tooi}). We will incorporate suggestions in revised manuscript.\\
\headingorange{1. Reviewer GsD5}
\question{$\textcolor{orange}{{{Q_1}}}-$Comparison with BadCLIP} The learnable noise in BadCLIP and \textit{TNT} serves different purposes: the former uses it as a universal trigger for injecting backdoor attack in the model in a \underline{supervised} few-shot prompt learning setup, whereas \textit{TNT} adapts noise specific to each test sample under a TTA (\underline{un-supervised}) setup to improve zero-shot generalization.
\question{$\textcolor{orange}{{{Q_2}}}-$Prompting vs Noise Learning}
Our approach is indeed analogous to visual prompting. However, there are key differences: please refer to \headinggreen{3.${{Q_2}}$}.
\question{$\textcolor{orange}{{{Q_3}}}-$Comparison with TDA} \textbf{(a)} \textit{TDA} utilizes memory-based modules, fundamentally differing from our approach as they avoid prompt tuning. \textbf{(b)} \textit{TDA} requires a \underline{stream of test samples} (used to compute the statistics of the test dataset in memory modules), unlike \textit{TPT} and our \textit{TNT}, which adapts to a single test sample. Unlike TDA, previous test samples do not influence the prediction score of current sample in TNT. None of our 8 baselines assume access to a stream of test samples for inference; instead, they operate on a single test sample at a time, and without any external model or resources. \textbf{(c)} \textit{TDA} relies on \underline{dataset-specific} configurations (as evident from their codebase) necessitating extensive hyperparameter tuning for each downstream dataset. In contrast, our method employs a same configuration across all datasets, demonstrating its robustness and effectiveness. 
\question{$\textcolor{orange}{{{Q_4}}}-$Number of Augmentations} All of our baselines default to using 63 augmentations in their original implementations. To ensure consistency, we also used 63 augmentations in our method. However, in Figure 5(b), we present an \underline{ablation study} with varying numbers of augmentations, demonstrating that \textit{TNT} outperforms other baselines.
\question{$\textcolor{orange}{{{Q_5}}}-$Reproducibility} We assure the reviewer that our well-documented code will be made publicly available. \question{$\textcolor{orange}{{{Q_6}}}-$Noise} Same noise is used in all augmentations.
% Kindly note that we have used a single set of hyperparameters across all datasets and do not use dataset-specific configurations. 

\headingred{2. Reviewer 4Rgj}
\question{$\textcolor{red}{{{Q_1}}}-$Training-free Methods} For comparison with \textit{TDA}, please refer to 
\headingorange{1.${{Q_3}}$}. Kindly note that MTA is not a purely training-free method, as it requires \underline{optimizing the mode} of the Gaussian kernel for each test sample (See Eq. 1 in MTA), rather than relying on any form of prompt optimization. Although MTA is faster, it performs lower (65.68, Table 1 last row of MTA) than TNT (66.08) on ImageNet variants.
\question{$\textcolor{red}{{{Q_2}}}-$TDA vs TNT Scores} While we acknowledge that \textit{TDA} demonstrates higher performance, we have not made any direct comparisons with \textit{TDA} due to differences in experimental settings. Our proposed method, \textit{TNT}, differs from \textit{TDA} in three significant aspects, as outlined in \headingorange{1.${{Q_3}}$}.
\question{$\textcolor{red}{{{Q_3}}}-$TPT vs TNT Scores} We reproduced all baselines, including \textit{TPT}, using their original settings. The results we report are the \underline{mean of two seeds}, which may result in a slight variation from the scores originally reported in the respective papers. \question{$\textcolor{red}{{{Q_4}}}-$ResNet50 Backbone} As shown in the following Table, both \textit{TNT*} and \textit{TNT} outperform the baselines across the RN50 backbone, similar to ViT-B/16.
{\renewcommand{\arraystretch}{0.80}
\begin{table}[t]
% \setlength{\tabcolsep}{4pt}
\vspace{-0.2cm}
\centering
% \caption{}
% \label{tab:computation}
% \fontsize{12}{12}\selectfont
\resizebox{0.38\textwidth}{!}{%
\begin{tabular}{l|cc|cccc}
\hline
\textbf{RN50} & \textbf{CLIP} & \textbf{CoOp} & \textbf{TPT}  & \textbf{RCLF} & \textbf{TNT*} & \textbf{TNT} \\
\hline
ImageNet     & 58.23    & 63.35    & 60.93   & 61.10   & \underline{62.73}    & \textbf{65.12}   \\
ImageNet-A     & 21.47    & 23.42    & 26.74   & 25.98   & \underline{32.44}    & \textbf{34.26}   \\
\hline
\end{tabular}}
\vspace{-0.4cm}
\end{table}}
\question{$\textcolor{red}{{{Q_5}}}-$Sensitivity Analysis} We consistently set $\alpha = 0.1$ and $\beta = 0.1$ across all datasets, with performance remaining \textit{nearly unchanged} for other values (e.g., 0.2, ..., 0.5). From Eq. 7 in our paper, impact of $K$ on ImageNet-A is shown in following table:
{\renewcommand{\arraystretch}{0.80}
\begin{table}[h]
% \setlength{\tabcolsep}{4pt}a
\vspace{-0.2cm}
\centering
% \caption{}
% \label{tab:k-impact}
% \fontsize{12}{12}\selectfont
\resizebox{0.32\textwidth}{!}{%
\begin{tabular}{l|cccccc}
\hline
$K$       & $1$ & $6$ & $12$  & $24$ & $32$ & $64$ \\
\hline
\textit{TNT*} & 60.23    & \textbf{61.87}    & 61.54    & 60.24    & 59.02    & 58.93   \\
\textit{TNT} & 62.77    & \textbf{63.93}    & 62.49    & 60.93    &  59.48   & 59.12   \\
\hline
\end{tabular}}
\vspace{-0.4cm}
\end{table}}

\headinggreen{3. Reviewer Tooi}
\question{$\textcolor{myforestgreen}{{{Q_1}}}-$Difference with CoTTA} 
\textit{CoTTA} introduced the concept of enforcing cross-entropy consistency between the predictions of student and teacher models. The teacher model, an off-the-shelf source pre-trained model, generates pseudo-labels based on \underline{augmentation-averaged prediction}s. In contrast, our approach does not rely on any external model. Instead, we maintain consistency among top-K augmented views by minimizing the average distance between \underline{image embeddings} (not predictions) across augmented views of a single test sample. To highlight the core difference, refer to Eq. 3 in \textit{CoTTA} and Eq. 4 in our work.
\question{$\textcolor{myforestgreen}{{{Q_2}}}-$Difference with VPT} Noise can be considered analogous to a Visual Prompt; however, there are key distinctions: \textbf{(a)} While \textit{VPT} learns a generic visual prompt from few-shot training data, \textit{TNT} learns noise specific to each individual test sample, highlighting their application in very different settings. \textbf{(b)} \textit{VPT} operates in the latent space by appending learnable tokens to the input layer of transformer (Eq. 4 of \textit{VPT}), whereas \textit{TNT} uses learnable noise tensor (of shape $224\times224\times3$) directly in the pixel space (Eq. 2 of our work). \textit{VPT} requires access to inside of the model whereas we do not need such access, making our approach easier to implement. Furthermore, when applied in the \textit{TTA} setting, VPT requires a significantly higher number of learnable parameters compared to our approach (\textit{TNT}), as illustrated in Figure 3 (Visual Prompting) of our work.
\question{$\textcolor{myforestgreen}{{{Q_3}}}-$Impact of CoOp} \textit{TNT*} represents our core contribution. In cases where \textit{TNT*} does not outperform the baselines, the competing methods are either \textit{CoOp} or \textit{RLCF*}. However, this comparison is not entirely fair, as \textit{CoOp} relies on pre-trained few-shot prompts, while \textit{RLCF*} involves tuning a visual encoder with over 86 million parameters-significantly more than the 150k parameters used in our approach.
When evaluated under a fair comparison, our final method, \textit{TNT*+CoOp} (i.e., \textit{TNT}), outperforms all other methods, including both \textit{CoOp} and \textit{RLCF*}, as shown in Table 1 of our work.
\question{$\textcolor{myforestgreen}{{{Q_4}}}-$Comparison with TDA} Please refer to our response in \headingorange{1.${{Q_3}}$}.

\question{Results on CIFAR-10-C and ImageNet-C}
{\renewcommand{\arraystretch}{0.80}
\begin{table}[h]
% \setlength{\tabcolsep}{4pt}
\vspace{-0.2cm}
\centering
% \caption{}
% \label{tab:computation}
% \fontsize{12}{12}\selectfont
\resizebox{0.42\textwidth}{!}{%
\begin{tabular}{l|cc|cccc}
\hline
\textbf{Corruption Dataset}       & \textbf{CLIP} & \textbf{CoOp} & \textbf{TPT}  & \textbf{RCLF} & \textbf{TNT*} & \textbf{TNT} \\
\hline
CIFAR-10-C      & 82.91    & 83.22    & 82.60    & 82.66    & \underline{83.25}    & \textbf{83.54}   \\
ImageNet-C    & 55.74    & 60.09    & 56.24    & 55.86    & \underline{56.51}    & \textbf{60.29}   \\
\hline
\end{tabular}}
\vspace{-0.4cm}
\end{table}}

\question{Impact of Temperature ($\tau$) on TNT* and TNT} 
% We obtain $\mu(\pm\sigma){=}63.95(\pm0.78)$ upon evaluation of TNT with different values of $\tau=[9e^{-1},9e^{-2},9e^{-3},9e^{-4},9e^{-5}]$) on ImageNet-A. 
{\renewcommand{\arraystretch}{0.80}
\begin{table}[h]
% \setlength{\tabcolsep}{4pt}
\vspace{-0.2cm}
\centering
% \caption{}
% \label{tab:computation}
% \fontsize{12}{12}\selectfont
\resizebox{0.32\textwidth}{!}{%
\begin{tabular}{l|ccccc}
\hline
\textbf{Temperature}       & $9e^{-1}$ & $9e^{-2}$ & $9e^{-3}$  & $9e^{-4}$ & $9e^{-5}$ \\
\hline
TNT*      & 59.57    & 60.14    & 61.25    & \textbf{61.87}    & 61.88 \\
TNT    & 60.82    & 61.45    & 63.02    & \textbf{63.93}    & 63.89 \\
\hline
\end{tabular}}
\vspace{-0.4cm}
\end{table}}


%%%%%%%%% REFERENCES
% {
%     \footnotesize
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }


\end{document}



% Let the $TNT$ strategy be denoted as $\mathcal{C}-$. Table 1 shows the following:
% \([TNT (64.59) - \mathcal{C} = CoOp (59.79)]\) and  
% \([TNT^* (62.63) - \mathcal{C} = CLIP (57.21)]\).  
% This highlights that combining CoOp with $TNT^*$ to achieve $TNT$ significantly improves accuracy across all ImageNet variants. Notably, $TNT^*$ outperforms baseline methods on 2 out of 5 datasets. However, it is surpassed by $CoOp$ and $RLCF^*$ on the V and R ImageNet variants. This comparison, however, is not entirely fair, as $CoOp$ itself uses pre-trained few-shot prompts and $RLCF^*$ involves whole encoder tuning.
% Under a fair comparison, our final approach, $TNT^*$+CoOp, outperforms all other methods, including both $CoOp$ and $RLCF^*$.



% {\renewcommand{\arraystretch}{1.0}
% \begin{table}[h]
% % \setlength{\tabcolsep}{4pt}
% \vspace{-0.2cm}
% \centering
% % \caption{}
% % \label{tab:computation}
% % \fontsize{8}{8}\selectfont
% \resizebox{0.32\textwidth}{!}{%
% \begin{tabular}{l|cccccc}
% \hline
% Temperature       & x & y & z  & a & b & c \\
% \hline
% Top-1 Acc.     & -    & -    & -    & -    & -    & -   \\
% \hline
% \end{tabular}}
% \vspace{-0.4cm}
% \end{table}}