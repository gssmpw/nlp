% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16940} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Can a Noise be Learned in Zero-Shot Settings?}
% \title{Test-Time Noise Tuning (TNT)}
\title{Noise is an Efficient Learner for Zero-Shot Vision-Language Models}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Raza Imam\quad Asif Hanif\quad Jian Zhang\quad Khaled Waleed Dawoud\quad \\ Yova Kementchedjhieva \quad Mohammad Yaqub\\
Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE\\
% Institution1 address\\
{\tt\small \{firstname.lastname\}@mbzuai.ac.ae}
\\ \small {\url{https://github.com/Razaimam45/TNT}}
}


% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{caption}   % 
\usepackage{listings}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}

\newcommand{\x}{\bm{\mathrm{x}}}
\newcommand{\z}{\bm{\mathrm{z}}}
\newcommand{\txt}{\bm{\mathrm{t}}}
\newcommand{\pa}{\bm{\mathrm{p}}}
\newcommand{\n}{\bm{\mathrm{n}}}
\newcommand{\s}{\bm{\mathrm{s}}}


\definecolor{codegray}{rgb}{0.5,0.5,0.5}


\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{teal},
  stringstyle=\color{red},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  tabsize=4,
}

\lstdefinestyle{Pytorch}{
    language         = Python,
    backgroundcolor  = \color{white},
    basicstyle = \fontsize{8.0pt}{10pt}\selectfont\ttfamily\bfseries,
    columns          = fullflexible,
    breaklines       = true,
    captionpos       = b,
    commentstyle     = \fontsize{4pt}{4pt}\color{codeblue},
    keywordstyle     = \fontsize{4pt}{4pt}\color{codekw},
    morekeywords     = {noise, confidence\_filter, vc\_loss, entropy},
    frame=None,
}


% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
% \usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
%\usepackage{hyperref}
\usepackage{orcidlink}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{float}
% \usepackage{pgfplotstable}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{wrapfig}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\definecolor{citecolor}{RGB}{0, 113, 188}
\definecolor{myforestgreen}{RGB}{34, 200, 34}
\definecolor{firebrick}{rgb}{0.7, 0.13, 0.13}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{deepskyblue}{rgb}{0.0, 0.75, 1.0}
\definecolor{mypink2}{rgb}{.99,.96,.98}%浅
\definecolor{mypink1}{rgb}{.99,.93,.98}
\definecolor{mypink}{rgb}{.99,.90,.98}%深
% \definecolor{mypink}{rgb}{.99,.91,.95}
\definecolor{mygray}{rgb}{.95,.95,.95}
\definecolor{lv14}{rgb}{0.5,0.5,0.5}
\definecolor{tabvline}{HTML}{a8a495}
\definecolor{prompt_blue}{HTML}{1f78b4}
\definecolor{prompt_red}{HTML}{d45c43}
\definecolor{green_im}{rgb}{0.0, 0.5, 0.0}

\definecolor{customblue}{HTML}{6aaed6} % Replace with your desired hex code
\definecolor{customgreen}{HTML}{89bf91} % Another color if needed
\definecolor{customrocket}{HTML}{f58860} % And so on

\definecolor{codeblue}{rgb}{0.25, 0.5, 0.5}
\definecolor{codekw}{rgb}{0.35, 0.35, 0.75}

\newcommand{\stdvuno}[1]{\footnotesize{\color{black}(#1)} {\color{black}}}
\newcommand{\stdvun}[1]{\footnotesize\textcolor{black}{(#1)} \textcolor{myforestgreen}{$\blacktriangle$}}
% \newcommand{\stdvunnn}[1]{\footnotesize\textcolor{black}{(#1)} \textcolor{myforestgreen}{$\blacktriangle\blacktriangle$}}
% \newcommand{\stdvur}[1]{\footnotesize\textcolor{black}{(#1)} \textcolor{red}{$\blacktriangledown$}}
% \newcommand{\stdvunu}[1]{\footnotesize\textcolor{black}{(\underline{#1})} \textcolor{myforestgreen}{$\blacktriangle$}}
% \newcommand{\stdvunnnu}[1]{\footnotesize\textcolor{black}{(\underline{#1})} \textcolor{myforestgreen}{$\blacktriangle\blacktriangle$}}
\newcommand{\stdvuru}[1]{\footnotesize\textcolor{black}{({#1})} \textcolor{red}{$\blacktriangledown$}}
\newcommand{\stdvueq}[1]{\footnotesize\textcolor{black}{({#1})} \textcolor{black}{$\approx$}}


\begin{document}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
    % Recent test-time tuning methods often face challenges in stabilizing Vision-Language Models (VLMs) under distributional shifts while managing computational efficiency. To tackle these issues, we present \textbf{TNT}, a novel approach that focuses on optimizing noise adaptation to enhance VLM performance using only a single test sample.
    % At the core of \textbf{TNT} is the principle of adaptive noise injection, where noise is optimized through entropy as a loss function. This allows the model to dynamically adjust the noise introduced to the visual space, facilitating improved out-of-distribution (OOD) generalization. Additionally, we minimize the mean distance between embeddings of confident augmented views, fostering a more coherent and aligned representation across the different views. 
    % At inference, \textbf{TNT} leverages the diversity of confident views by selecting the top-augmented representations instead of the original input, enhancing prediction accuracy. This is further complemented by temperature scaling applied to the resulting logits, stabilizing the model’s outputs and leading to more confident predictions. By integrating these elements, \textbf{TNT} provides a more generalizable and computationally efficient solution for test-time adaptation in VLMs.

    Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning learnable prompts; however, this approach overlooks potential distribution shifts in the visual representations themselves. In this work, we address this limitation by introducing \textbf{Test-Time Noise Tuning (TNT)}, a novel method for handling unpredictable shifts in the visual space. TNT leverages, for the first time, a \textbf{noise adaptation} strategy that optimizes learnable noise directly in the visual input space, enabling adaptive feature learning from a single test sample. We further introduce a novel approach for \textit{inter-view representation alignment} by explicitly enforcing coherence in embedding distances, ensuring consistent feature representations across views. Combined with scaled logits and confident view selection at inference, TNT substantially enhances VLM generalization and calibration, achieving average gains of +7.38\% on natural distributions benchmark and +0.80\% on cross-dataset evaluations over zero-shot CLIP. These improvements lay a strong foundation for adaptive out-of-distribution handling. 
    % Our code implementation will be released upon publication.
    % Our code is available at at \href{https://github.com/Razaimam45/TNT}{https://github.com/Razaimam45/TNT}.
    

    % Vision-Language Models (VLMs) have shown remarkable capabilities in zero-shot generalization across various visual and textual tasks. However, these models encounter limitations when exposed to real-world data variations and domain shifts. In this work, we propose an innovative approach to enhance the robustness and adaptability of VLMs by incorporating learnable noise into the model’s feature space during inference. This learnable noise acts as a form of dynamic augmentation, enabling the model to capture subtle variations and improve generalization to OOD samples. We develop a method to optimize the noise parameters concurrently with the model weights, allowing for targeted noise adaptations that enhance resilience to unseen conditions. Additionally, we minimize the mean distance between embeddings of confident augmented views by selecting the top-augmented representations. This is further enhanced by applying temperature scaling to the resulting logits, which stabilizes the model's outputs and produces more confident predictions. Experimental results on OOD and fine-grained datasets demonstrate that our approach significantly improves performance on downstream tasks, particularly in scenarios with substantial visual and linguistic discrepancies. 
    % Our findings suggest that the integration of learnable noise into VLMs holds promise for advancing the robustness of these models in real-world applications.
    
    
    % Despite these gains, test-time noise adaptation can degrade model calibration, particularly under distributional drift, weakening the zero-shot baseline’s robustness. To counter this, we propose adaptive temperature scaling during inference, which dynamically adjusts the temperature based on model uncertainty, improving calibration without sacrificing generalization performance.
    
  \end{abstract}

%%%%%%%%% BODY TEXT

\section{Introduction}
% Needs polishing but the opening message should be about the problem itself -> we want to do zero-shot classification and that's hard in distribution shift settings
% Mention this paper RAZA (An Empirical Study Into What Matters for Calibrating Vision–Language Models)
Vision-language models (VLMs) have been shown to successfully perform various downstream tasks in a zero-shot fashion, which eliminates the need for creating task-specific training data and storing multiple models. The ability of VLMs to generalize on open-world problems, however, degrades as real-world data shifts away from the distribution that the models were trained on. Test-Time Adaptation (TTA) has thus emerged as a critical approach to enhance model robustness while maintaining the advantages of zero-shot learning  \cite{sun2020test, wang2020tent}. TTA operates at inference time, leveraging unlabeled test data to dynamically adjust existing or newly added model parameters to the desired distribution.

Various forms of parametrization for TTA have been explored, both on the side of the image encoder and the text encoder of VLMs \cite{xiao2024beyond}. These include learning a soft prompt for the text encoder \cite{shu2022test, yooncctpt, murugesan2024robust}, adapting the batch norm layers of the VLM \cite{li2016revisitingbatchnormalizationpractical, zhang2022memotesttimerobustness}, learning LoRA layers in model components \cite{imam2024testtimelowrankadaptation}, or even updating whole components of the model, such as the vision encoder \cite{zhao2023test}.
Amidst an abundance of TTA studies, one form of parametrization that remains unexplored is \textit{noise}.
%, thus circumventing the need for additional training data and pre-processing steps. This approach is especially advantageous for large-scale pre-trained models, as it allows real-time adaptability without modifying the original training process or accessing the training data \cite{shu2022test}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/concepts2.pdf}
    \vspace{-0.6cm}
    \caption{ As top-\( K \) augmented view embeddings grow more consistent with each optimization step \( t \), the attention mechanism focuses on relevant regions, leading to improved accuracy.  \textit{Attention Difference} illustrates the absolute difference between the clean attention map and the noise-tuned attention map. CLIP zero-shot incorrectly classifies the original image as {\color{red}{amaga}}, while TNT correctly classifies the optimized image as {\color{myforestgreen}{garter snake}}.
    }
    \label{fig:concept}
    \vspace{-0.4cm}
\end{figure}

While noise is often seen as disruptive to machine learning models, it has also proved valuable in many ways. In generative models such as generative adversarial networks \cite{goodfellow2014generative}, variational autoencoders \cite{kingma2013auto}, and, more recently, diffusion models \cite{ho2020denoising, rezende2014stochastic, radford2015unsupervised}, \textit{random} noise has been used to initialize and guide output generation with impressive results. 
Building on findings that noise can enhance representation learning and improve robustness under varying conditions \cite{bengio2013representation, song2019generative}, recent works \cite{hanif2024baple, bai2024badclip} have applied learnable noise to examine adversarial vulnerabilities in VLMs using prompt learning setup. Inspired by this approach, we propose a TTA framework based on noise adaptation.


 % These parameters are often learned by minimizing the marginal entropy across confident predictions from different augmentations of the input image \cite{zhang2022memo}. The motivation behind this objective is to adapt 

% approach is to learn a sample-specific task prompt that better aligns the label space with the input space. However, this approach does not modify the image representation, overlooking potential (and highly likely) distribution shifts on the input side.   
% Bringing the embeddings of augmented views of the same image closer together, often achieved through contrastive learning, plays a crucial role in improving model generalization. By ensuring that different augmentations—such as rotations, cropping, color changes, or noise—of a single image map to similar points in the embedding space, the model learns to focus on core, invariant features that define the image rather than superficial details. This invariance to augmentations makes the model more robust to variations in lighting, orientation, and other contextual changes that occur naturally in real-world settings. As a result, the model becomes better equipped to generalize across diverse and unseen datasets, as it has learned to emphasize fundamental patterns and relationships within the data that remain constant, even under transformation. This strengthened ability to generalize is especially valuable in applications requiring high adaptability to new domains or scenarios, where subtle visual discrepancies often exist.



% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.52\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/attn_snake.pdf}
%         \caption{Impact of Noise Tuning on Attention Maps Across Optimization Steps. \textit{Attention Difference} illustrates the absolute difference between the clean attention map and the noise-tuned attention map at each optimization step \( t \).}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.50\textwidth}
%         \centering
%         \includegraphics[width=0.50\textwidth]{Figures/ridge_cdist.pdf}
%         \includegraphics[width=0.36\textwidth]{Figures/ridge_barplot.pdf}
%         \caption{Effect of Inter-View Mean Embedding Distance \textit{w.r.t.} Optimization Steps $t$ for top-\( K \) augmented views on Top-1 Accuracy. Each density curve represents a distribution of mean distance values for ImageNet-A samples.}
%     \end{subfigure}
%     \caption{(a) Adaptive noise tuning enhances \textbf{\textit{attention}} on key features, with focus on sensitive regions evolving at each optimization step \( t \). (b) As top-\( K \) augmented view embeddings converge \textbf{\textit{closer}} with each step \( t \), predictions become more confident, leading to improved accuracy.}


%     \label{fig:concept}
% \end{figure}

% We propose a novel TTA method for improving the processing of the input image by learning a noise modulation over it. 
Our approach, dubbed Test-time Noise Tuning (TNT), relies on \textit{learnable noise}, applied over the augmented views of an input image to enhance regions relevant to its correct classification. The noise is sampled from a standard Gaussian distribution and optimized with a two-fold objective: (1) minimizing marginal entropy \cite{zhang2022memo}, an approach that proves effective when applied to input adaptation, as demonstrated previously in label adaptation \cite{shu2022test}; and (2) maximizing inter-view consistency, a novel objective introduced to promote consistency across representations of different augmented views of the input image. By ensuring that different augmentations of the image map to similar points in the embedding space, the model learns to focus on core, invariant features over superficial details (see an example in Figure~\ref{fig:concept}.) The learned noise is applied to both the input image and its augmentations in an enhanced inference procedure, with performance further boosted through temperature scaling \cite{murugesan2024robust,tu2024empirical}. 
% In the TTA scenario, the goal is to achieve higher classification accuracy while reducing calibration error. Although fine-tuning the model might seem like an obvious solution, this approach would violate TTA constraints. Therefore, in line with \cite{tu2024empirical, tu2024closerlookrobustnesscontrastive}, we incorporate temperature scaling alongside our proposed method in an enhanced inference procedure.

In a comparative evaluation against seven strong TTA baselines on two established out-of-distribution benchmarks, our approach proves both more accurate and better calibrated, which is crucial to real-world applications.
Our method achieves this performance without considerable latency compared to other methods, and proves highly effective even with a limited parametrization budget. 
In summary, our main contributions are as follows:
\begin{itemize}[leftmargin=0.40cm, itemsep=0.10em]
    \item We propose \textbf{TNT}, a novel \textit{noise} adaptation strategy that optimizes VLM's vision encoder's input space by incorporating learnable noise at test time for a single test sample, enhancing model robustness to distributional shifts and improving out-of-distribution generalization. 
    \item We introduce an inter-view consistency loss for the noise tuning strategy that minimizes the distance between confident augmented views, fostering more aligned representations and reducing prediction uncertainty. This approach harmonizes the benefits of consistency loss and entropy minimization on top of sole noise adaptation.
    \item We demonstrate that TNT significantly improves VLM generalization, achieving state-of-the-art performance on a range of natural shift and cross-dataset benchmarks, all with reduced computational overhead.
\end{itemize}
\noindent To the best of our knowledge, we are the first to explore noise optimization for representation learning within VLMs, offering a novel TTA approach. Our findings on TNT highlight the potential of noise-adaptive architectures, encouraging further research into enhancing model robustness and generalization.

\section{Related Work}
\noindent\textbf{Zero-Shot VLM Generalization:}
Pre-trained on large image-text datasets in a self-supervised way, VLMs such as CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} have shown strong generalization capabilities. For example, CLIP's remarkable zero-shot transfer performance can be attributed to the diversity and scale of the data on which it was trained. Nonetheless, adapting them effectively to specific downstream tasks when data is scarce remains a challenge. One straightforward yet effective approach to enhance CLIP’s zero-shot performance on image classification is the use of \textit{soft prompts} \cite{zhou2022coop} which are \textit{learned} in a few-shot training setup. 

% VLMs still face challenges with distribution shifts \cite{taori2020measuring,hendrycks2021natural} when training and test domains differ, such as in image styles, new object types, or subtle language variations. These shifts can degrade performance, affecting model reliability in real-world scenarios.

%Various few-shot approaches have been introduced to improve zero-shot generalization in VLMs. For instance, CoOp  fine-tunes CLIP by learning a set of prompts within the text encoder, while CoCoOp \cite{zhou2022cocoop} addresses CoOp’s limitations in generalization by dynamically conditioning text prompt tokens on image embeddings. 
% MaPLe [26] advances this by jointly learning deep prompts in both vision and text encoders.
% However, despite these improvements, many methods still depend on pre-trained weights, which can be problematic in real-world applications where no target domain training data is available.

\vspace{0.2cm}
\noindent\textbf{Test-Time Optimization:}
% Test-time adaptation approaches have become increasingly important for handling unseen or shifted data distributions. Test-time training and its variants \cite{shu2022test, zhao2023test} incorporate feedback mechanisms at inference, allowing models to update weights dynamically. For instance, RLCF \cite{zhao2023test} adapts Vision-Language Models (VLMs) at test time by using a reinforcement learning-based reward signal from CLIP to guide model outputs, enhancing zero-shot generalization across diverse visual tasks. Test-Time Prompt Tuning (TPT) \cite{shu2022test} introduces the concept of learnable prompts that adapt VLMs to new domains without modifying the model's core parameters. TPT optimizes prompt embeddings in response to incoming test data, improving alignment with the pre-trained model's feature space and enabling the model to handle unseen data distributions effectively. Building on these approaches, we propose a novel direction by adding learnable noise to test-time samples. This lightweight, adaptable method directly augments test data, mitigating distribution shifts without adjusting model weights or embeddings.
% Test-time adaptation (TTA) plays a key role in enhancing the generalization of vision-language models (VLMs) like CLIP to unseen data distributions. 
Approaches such as TPT \cite{shu2022test} adjust prompts at test time to reduce entropy across augmented views of a single test sample, improving accuracy without additional training data. However, TPT does not address \textit{model calibration}, which is essential for uncertainty estimation.
To remedy this, C-TPT \cite{yooncctpt} improves calibration by optimizing prompt selection based on the dispersion of text features, eliminating the need for labeled data. Reinforcement learning with CLIP feedback (RLCF) \cite{zhaotestrlcf} further enhances generalization by offering continuous feedback during TTA, correcting predictions and preventing overconfidence associated with entropy minimization in TPT. Sample-wise Temperature Scaling (SaLS) \cite{murugesan2024robust} modifies temperature scaling during TTA on top of TPT to boost calibration, refining the model’s confidence. CALIP \cite{guo2023calip}, enhances CLIP’s zero-shot performance by incorporating an attention module that enables interaction between visual and textual features, all without requiring additional training or parameters.

\vspace{0.2cm}
\noindent\textbf{Noise-based Learning:} 
While learnable noise has not been explored for TTA, two recent works -- BadCLIP \cite{bai2024badclip} and BAPLE \cite{hanif2024baple} -- have used it to inject backdoor triggers into the image encoder's input within a few-shot training setup. These approaches introduce learnable noise during the prompt learning stage to compromise the VLM, demonstrating that simply adding noise can alter the model's behavior. This insight raises an intriguing question: \textit{could such noise be harnessed positively}?

% Learnable noise has been investigated in generative models, including Generative Adversarial Networks \cite{goodfellow2014generative}, Variational Autoencoders \cite{kingma2013auto}, and diffusion models \cite{ho2020denoising, rezende2014stochastic, radford2015unsupervised}. In these models, noise functions as a latent variable or random vector, manipulated during training to guide output generation. While noise is often seen as disruptive to machine learning models, recent studies indicate its .
% % Outside generative modeling, noise-based learning has been applied to implicitly improve augmentations in training contrastive models. For example, noise modulation in contrastive learning has been investigated to boost performance in zero-shot and few-shot scenarios \cite{chen2020simple, he2020momentum}.


% Current literature lacks extensive research on learnable noise for domain adaptation and zero-shot generalization. BAPLe \cite{hanif2024baple} has recently been introduced for adversarial robustness in Vision-Language Models (VLMs). BAPLe implements a trainable noise trigger within input images alongside learnable prompts in the text encoder, effectively embedding a backdoor that modulates the model's sensitivity to specific inputs. By adjusting the noise trigger, BAPLe directs VLM responses to compromised data without impacting clean data performance. This method, focused on security concerns in prompt learning and noise injection, provides a basis for exploring learnable noise in controlled test-time adaptation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/method_main2.pdf}
    \vspace{-0.50cm}
    \caption{\textbf{Test-Time Noise Tuning (TNT)} (1) generates augmented views of a test image, (2) applies adaptive \textit{learnable} noise, and (3) computes logits and feature vectors for each view. (4) Top-\(K\) views are selected by confidence, with (5) entropy loss [Eq. \ref{eq:entropy}] enforcing confident predictions and (6) inter-view consistency loss [Eq. \ref{eq:Lvc_loss}] aligning feature representations. (7) The combined loss is backpropagated to iteratively refine the noise, enabling adaptive test-time noise tuning.}
    \label{fig:method}
    \vspace{-0.20cm}
\end{figure*}

\section{TNT: Test-Time Noise Tuning}

\subsection{Zero-shot Image Classification with VLMs}
Foundation VLMs, such as CLIP and ALIGN, have been shown to perform well on the task of image classification using a simple but effective zero-shot approach. Given an input image, \(\x\), and a set of class descriptions, \(\txt = \{t_1, t_2, \dots, t_c\}\) for a total of \(c\) classes, the prediction scores can be obtained as: 
% \paragraph{Contrastive Language-Image Pretraining (CLIP)} is a powerful foundation model that learns to represent images and text in a shared embedding space. It consists of two primary components: an image encoder \( f_I \) and a text encoder \( f_T \). The image encoder \( f_I \) takes an input image \((\x)\) and maps it to a feature vector \(f_I(\x) \in \mathbb{R}^d\) in the embedding space, while the text encoder \( f_T \) performs the same transformation for a text input \((t)\) i.e. \(f_T(t) \in \mathbb{R}^d\). % Given \(\txt = \{t_1, t_2, \dots, t_c\}\) as the set of class descriptions, 
\begin{equation}
\label{eq:logits}
f(\x,\txt) = \bigg\{\operatorname{sim}\big(f_{I}(\x), f_{T}(t_i)\big)\bigg\}_{i=1}^{c}
\end{equation}
where \( f_I \) denotes the image encoder of the VLM, \( f_T \), the text encoder and \(\operatorname{sim(\dot)}\) is cosine-similarity. For brevity, we hereafter drop \(\txt\) and denote the scores as \(f(\x) \in  \mathbb{R}^{c} \). In this work, we rely on this general framework for zero-shot image classification and experiment specifically with CLIP, without loss of generalization. Although current VLMs exhibit impressive generalization across visual domains and object classes, we aim to improve their performance further through \textit{noise optimization} in the context of TTA.

% \paragraph{Test-Time Adaptation (TTA)} 
% Consider an image-label pair \((\x_s, y_s)\) drawn from a \textit{source} distribution \(p_s\) and a model \(f_{\theta}\) trained on samples from this distribution, i.e., \(\forall~(\x_s, y_s) \sim p_s\). Similarly, let \((\x_t, y_t)\) denote the image-label pair drawn from a \textit{target} distribution \(p_t\). 
% % Both distributions exist within the joint space \((\mathcal{X}\times \mathcal{Y})\), where \(\mathcal{X}\) represents the input space and \(\mathcal{Y}\) represents the label space. 
% TTA aims to address distribution shifts in machine learning algorithms, where the core challenge is the discrepancy between the source and target distributions, i.e., \(p_s(\mathbf{x}_s, y_s) \neq p_t(\mathbf{x}_t, y_t)\). This discrepancy implies that a model \(f_\theta\), trained on the source distribution, may underperform on samples drawn from \(p_t\). Given a labeled \(p_s\) and an unlabeled \(p_t\), TTA aims to \textit{align} the model \(f_\theta\) with the target sample \(\mathbf{x}_t\) such that, rather than relying solely on pre-trained features, it can effectively \textit{adapt} and perform well on the target domain despite the distribution shift. This alignment can be achieved through various approaches, such as \textit{model} adaptation, \textit{inference} adaptation, \textit{normalization} adaptation, \textit{prompt} adaptation, and \textit{sample} adaptation \cite{xiao2024beyond}. Specifically, in sample-adaptive TTA, inference relies solely on an unlabeled sample \(\mathbf{x}_t\) from the target distribution \(p_t\) for adaptation.



\subsection{Noise Optimization}
% \paragraph{Why optimize input space of vision encoder?}
% In contrast to traditional fine-tuning, which often results in domain-specific adaptations that compromise generalization, prompt tuning preserves the model's pre-trained features by modifying only the input text context \cite{zhou2022coop}. This approach optimizes the input space of the VLM's text encoder, tailoring prompts to individual test samples and enabling VLM to retrieve relevant knowledge with precision. However, optimizing the text encoder's input alone may still leave gaps if the visual representations do not fully convey relevant information \cite{zhou2022cocoop,khattakMaPLe}. This could result in suboptimal alignment with the visual encoder, limiting the model's ability to capture nuanced visual features essential for new tasks or domains. Without adapting the visual input space, the model may struggle with real-world variations (e.g., lighting, angles, noise), impacting performance, particularly on challenging or out-of-distribution images. Relying solely on text input optimization can overlook crucial visual details needed for precise differentiation.In this work, we propose an efficient approach to optimize the vision encoder's input space for TTA by introducing \textit{learnable} noise. Optimizing the vision encoder's input space can enable the model to capture subtle, complex features in visual content, enhancing alignment with text prompts and improving accuracy in fine-grained classification. Additionally, this adjustment may increase the adaptability to variations, making the model more robust in low-quality or noisy data scenarios by preserving critical visual information. By strengthening the role of visual features in predictions, this approach can reduce the over-reliance on language cues, leading to better performance in tasks that require detailed visual understanding.
Unlike traditional fine-tuning, which risks domain-specific biases, prompt tuning refines the input text context to preserve the VLM's pre-trained features, enhancing its ability to retrieve relevant knowledge with precision. However, relying solely on text optimization may overlook essential visual details. We propose enhancing the image encoder's input with \textit{learnable} noise, allowing the model to capture subtle features, align with text prompts, and improve performance on varied or noisy images by preserving critical visual cues. Our approach keeps the model's weights unchanged, preserving its zero-shot capabilities. The workflow of our approach is outlined below.

\vspace{0.2cm}
\noindent\textbf{Noise in Augmented Views:} 
Consider an input image \( \x \in \mathbb{R}^{C \times H \times W} \) at test time, where \( C \), \( H \), and \( W \) denote channels, height, and width, respectively. To generate diverse views, \( N \) \textit{augmented} versions of \( \x \), represented by \( (\x_1, \x_2, \dots, \x_N) \), are created using random transformations. The key feature of our approach is a \textit{learnable} noise map \( \xi \in \mathbb{R}^{C \times H \times W}\), which is added to each augmented view and tuned through test-time adaptation.  Noise values, \( \xi \),  are constrained to \([-\epsilon, +\epsilon]\), where \(\epsilon\) is the perturbation budget. The perturbed \(i_\text{th}\) augmented view is obtained as follows:
\begin{equation}
\x^{\prime}_i = \operatorname{clamp}(\x_i + \xi,~ 0,~1)
\end{equation}
where \( \operatorname{clamp}(\cdot) \) constrains the input values within the valid range \([0, 1]\). At each optimization step, the noise values are iteratively updated to reduce model uncertainty following the objectives described below.  

\vspace{0.2cm}
\noindent\textbf{Entropy Loss:} The model, \( f(\cdot) \), generates logits (unnormalized prediction scores) for each augmented view, denoted as \( f(\mathbf{x}'_i) \) for \( i \in \{1, 2, \dots, N\} \). Following the approach in \cite{shu2022test}, we retain only the high-confidence views, selecting the top-\( K \) views with lowest self-entropy.\footnote{Notice that the dynamic percentile threshold implemented in \cite{shu2022test} effectively implements a top-K selection process as well.} Using these top-\( K \) views, we compute the marginal entropy loss, denoted by \(\mathcal{L}_{\text{entropy}}(\cdot)\), as follows:
\begin{equation}
\mathcal{L}_{\text{entropy}} =  \mathcal{H}\bigg(\frac{1}{K}\sum_{k \in \mathcal{K}}\operatorname{softmax}(f(\mathbf{x}'_k))\bigg)
\label{eq:entropy}
\end{equation}
where \(\mathcal{H}(\cdot)\) computes the entropy of the average probability distribution. This distribution is obtained by first applying \(\operatorname{softmax}(\cdot)\) to each logit \( f(\mathbf{x}'_k) \) and then averaging the resulting probabilities across the top-\( K \) high-confidence views. Here, \(\mathcal{K}\) represents the set of indices corresponding to these top-\( K \) views.

\vspace{0.2cm}
\noindent\textbf{Inter-view Consistency Loss:}  
To ensure consistency among multiple augmented views, we introduce \textit{inter-view consistency} loss minimization objective to penalize large embedding variations among high-confidence views. Let \( f_{I}(\x^{\prime}_i) \in \mathbb{R}^{d} \) represent the embedding of  perturbed \(i_\text{th}\) augmented view \( \x^{\prime}_i \). We calculate pairwise Euclidean \( (\ell_2) \) distance among embeddings of the top-\( K \) selected views and define the \textit{inter-view consistency} loss, denoted by \( \mathcal{L}_{\text{vc}} \), as follows:
\begin{equation}
\mathcal{L}_{\text{vc}} = \sum_{i \in \mathcal{K}} \sum_{j \in \mathcal{K}} \big\| f_{I}(\x^{\prime}_i) - f_{I}(\x^{\prime}_j) \big\|_2
\label{eq:Lvc_loss}
\end{equation}
where \(i\neq j\). \( \mathcal{L}_{\text{vc}} \) loss, combined with noise perturbation, penalizes high variance in representations, encouraging the model to maintain stable and consistent feature embeddings for the selected confident views. 

\vspace{0.2cm}
\noindent\textbf{Noise Tuning:}  The final loss \(\mathcal{L}\) combines the entropy and inter-view consistency losses i.e. \(\mathcal{L} = \alpha \mathcal{L}_{\text{entropy}} + \beta \mathcal{L}_{\text{vc}}\), where \(\alpha\) and \(\beta\) control the weights of the respective losses. The learnable noise is adapted by minimizing \(\mathcal{L}\) as follows:
\begin{equation}
\label{eq:tnt_objective}
\underset{ \xi }{\operatorname{minimize}}~~~~ \alpha \mathcal{L}_{\text{entropy}} + \beta \mathcal{L}_{\text{vc}}
\end{equation}
This objective encourages both high-confidence predictions (through entropy minimization) and consistent embeddings across views (via inter-view distance minimization), thereby enhancing model robustness and calibration under distribution shifts. To update the noise \( \xi \) in each iteration, we compute the gradient of the loss \( \mathcal{L} \) and update \( \xi \) as follows:
\begin{equation}
\xi \leftarrow \xi - \gamma \cdot \operatorname{sign}(\nabla_\xi \mathcal{L})
\end{equation}
where \( \gamma \) is the learning rate, and \( \operatorname{sign}(\cdot) \) denotes the element-wise sign function \cite{goodfellow2015explainingharnessingadversarialexamples}. After each update step, the noise \( \xi \) values are clamped to the interval \( [-\epsilon, +\epsilon] \). Algorithm \ref{alg:tnt} provides a high-level overview of the noise update process in TNT.

\vspace{0.2cm}
\noindent\textbf{Inference:} 
After the noise adaptation phase, during inference, we add the \textit{learned} noise \( \xi \) to each of the \(N\) augmented views of the image \(\mathbf{x}\). Based on self-entropy, we then select the top-\(K\) most confident views, denoted as \( \{\mathbf{x}_k + \xi\}_{k \in \mathcal{K}} \). Using Equation \ref{eq:logits}, we obtain the logits  for each of the perturbed top-\(K\) views and compute the final probability distribution \(\mathbf{p}\) as follows:
\begin{equation}
\mathbf{p} = \frac{1}{K} \sum_{k \in \mathcal{K}} \operatorname{softmax}_{\tau}\big(f(\mathbf{x}_k + \xi)\big)
\label{eq:inference}
\end{equation}
where \(\tau\) is the temperature parameter in the \(\operatorname{softmax}\) function which scales the obtained logits. The predicted label is then obtained by taking the \(\operatorname{argmax}\) of the probability distribution i.e. 
\begin{equation}
\hat{y} = \underset{ i\in \{1,2,\dots,c\} }{\operatorname{argmax}}~~p_i
\end{equation}
where \(p_i\) is the probability of \(i_{\text{th}}\) class in distribution \(\mathbf{p}\). 

\input{algos/tnt_full}

\input{tables/natural_shifts}

\section{Experiments and Results}
\subsection{Experimental Setup}

\noindent\textbf{Datasets:}
We conduct experiments on a diverse range of benchmark datasets to assess the performance and robustness of our method, specifically testing its out-of-domain generalization across different domains. The selected datasets include ImageNet-A \cite{hendrycks2021natural}, ImageNet-V2 \cite{recht2019imagenet}, ImageNet-R \cite{hendrycks2021many} and ImageNet-Sketch (denoted as ImageNet-K) \cite{wang2019learning}, which have been considered as out-of-distribution (OOD) data for ImageNet to assess model robustness under different conditions and distributions.

For cross-domain generalization, following \cite{shu2022test}, we include Flowers102 \cite{nilsback2008automated}, DTD \cite{cimpoi2014describing}, Pets \cite{parkhi2012cats}, UCF \cite{soomro2012ucf101}, and Caltech101 \cite{fei2004learning}. These datasets were chosen to analyze the model's ability to distinguish subtle differences between similar classes. Additionally, we include Aircraft \cite{maji2013fine}, EuroSAT \cite{helber2019eurosat}, Cars \cite{krause20133d}, Food \cite{bossard2014food}, and SUN397 \cite{xiao2010sun} to further test the model’s adaptability across distinct categories, including aerial images, satellite images, and object-centric as well as scene-centric datasets.

\vspace{0.1cm}
\noindent\textbf{Implementation Details:}
% We implemented one noise map for 64 augmented images and clip the value range to [-1/255,1/255].
% \begin{lstlisting}[]
% shape = images.shape[1:]  # [C,H,W]
% eps = 1/255.0 # hyperparameter
% noise = torch.randn(shape).clamp(-eps, eps)
% \end{lstlisting}
% We use CoOp \cite{zhou2022learning} to initialize the parameters of prompt learner (denoted as Ours$^{\dagger}$). Ours* means Ours without coop initialization.\\
We initialize the noise \(\xi \in \mathbb{R}^{3\times 224\times224}\) by sampling from a standard Gaussian distribution and set \(\epsilon=1/255\). This noise is applied to \(N=64\) images, consisting of the original image and 63 augmented views, which are generated through random resized crops and horizontal flips of the original image. Noise is updated with a learning rate of \(1\mathrm{e-}3\) across all datasets. For temperature scaling, we use a constant value of $\tau = 7\mathrm{e-}3$ across all settings.
We use fixed prompts in two configurations: first, a hand-crafted prompt ("a photo of a \{CLASS\}"), referred to as TNT*; and second, 4-shot context weights obtained using CoOp \cite{zhou2022coop} on ImageNet, referred to as TNT.  All experiments are conducted on a single NVIDIA A6000 48GB GPU.

\vspace{0.1cm}
\noindent\textbf{Baselines:}
We evaluate a total of seven zero-shot baselines to assess the effectiveness of our approach. In addition to CLIP zero-shot \cite{radford2021learning} and CLIP with the CoOp pretrained soft prompt \cite{zhou2022learning}, we \textit{reproduce} several recently published test-time adaptation methods: TPT \cite{shu2022test},  CALIP \cite{guo2023calip}, C-TPT \cite{yooncctpt}, SaLS \cite{murugesan2024robust} and RLCF \cite{zhao2023test}.
% TPT \cite{shu2022test}, a state-of-the-art test-time prompt tuning method, optimizes learnable prompts across multiple augmented views. CALIP \cite{guo2023calip} introduces parameter-free attention to enhance the exchange of informative features between images and text in CLIP. Built on top of TPT, C-TPT \cite{yooncctpt} employs higher text feature dispersion to improve prediction calibration. SaLS \cite{murugesan2024robust} uses unsupervised sample-wise temperature scaling on TPT to increase model confidence and reduce calibration error. RLCF \cite{zhao2023test} incorporates a feedback mechanism from a CLIP backbone, addressing the issue of incorrect predictions. 
All methods are reproduced on our system with a single update step and the same consistent backbones to ensure fair comparisons. Specifically, we use the same backbone for both the teacher and student models in RLCF.

\subsection{TNT Results}
\noindent\textbf{Natural Distribution Shifts:} 
% Table \ref{tab:natural_shift} compares our method with seven baselines, all using a ViT-B/16 backbone, including zero-shot CLIP. TNT*, our simpler variant, which uses a hand-crafted prompt, outperforms all baseline methods on average across the four OOD datasets, and also on the in-domain ImageNet. This shows that learning noise through marginal entropy and inter-view distance minimization is an effective way to adapt the input features for more accurate classification. We achieve this without having to modify the large vision encoder itself, as was done in \(\text{RLCF}^{*}\), yet we score substantially higher at a lower computational cost. Comparing TNT* to \(\text{TPT}\), we see a limited gain in performance in-domain (\textless 0.5 points), but a more notable gain OOD (close to 2 points on average), mainly due to  ImageNet-A benefiting \textit{considerably} from the noise adaptation. We further investigate the role of the number of trainable parameters and text vs. image adaptation in \S\ref{comp_analysis}. 
Table \ref{tab:natural_shift} compares our method with seven baselines using a ViT-B/16 backbone, including zero-shot CLIP. Our simpler variant, TNT*, which uses a hand-crafted prompt, achieves the highest average performance across four OOD datasets and in-domain on ImageNet. This demonstrates that adapting input features through noise learning with marginal entropy and inter-view distance minimization effectively enhances classification accuracy. We achieve this \textit{without having to modify the large vision encoder} itself, as was done in \(\text{RLCF}^{*}\), yet we score substantially higher at a lower computational cost. While TNT* shows a minor in-domain improvement over \(\text{TPT}\) (\(< 0.5\) points), it achieves a substantial OOD gain (around 2 points), with ImageNet-A benefiting notably from noise adaptation. Further analysis of trainable parameters and text vs. image adaptation is detailed in \S\ref{comp_analysis}.

% Our stronger variant, \(\text{TNT}\), uses a CoOp-initialized prompt in the text encoder. We see that this brings an additional ~2 points in average performance both in- and out-of-domain. Interestingly, the CoOp approach itself does not rank well among the baselines, and underperforms TNT by almost 5 points. We observe an interesting synergy here between an approach that in itself has limited capacity but proves very effective as an initialization technique in our setting. The prompt initialization informs our system of the general task of image classification, such that the noise can adapt the input features accordingly. In \S\ref{ablations} we show that further tuning this prompt helps calibrate the model predictions but has no positive impact on the accuracy.  
Our stronger variant, \(\text{TNT}\), uses a CoOp-initialized prompt in the text encoder, yielding an additional ~2 points in average performance both in- and out-of-domain. Interestingly, CoOp alone ranks poorly among the baselines, underperforming TNT by nearly 5 points. We observe an interesting synergy where the CoOp in itself has limited capacity but proves very effective as an initialization technique in TNT, enabling noise to adapt visual features and improving classification performance. In \S\ref{ablations}, we show that further tuning this prompt calibrates model predictions but does not improve accuracy.



% Specifically, TNT achieves OOD average accuracy of 62.63\% with hand-crafted prompts (TNT*) and 64.59\% with CoOp-initialized prompts (TNT), while also surpassing the baselines in In-domain accuracy, achieving 70.27\% and 72.06\%, respectively. In contrast, CoOp itself achieves consistently lower results than TNT across both OOD and In-domain datasets.
% % These gains underscore TNT’s strength in adapting effectively to various shifts in data distribution.
% Unlike adapting the whole image encoder, TNT optimizes noise parameters such that visual features of the input samples are adapted without modifying the (black-box) model itself, capturing intricate distribution shifts that are often missed by traditional prompt-tuning techniques like TPT \cite{shu2022test} and RLCF \cite{zhaotestrlcf}. 
% % Moreover, TNT’s versatility is evident in both initialization strategies, as demonstrated by the solid OOD average of 62.63\% with hand-crafted prompts (TNT*), further emphasizing its adaptive capabilities across diverse scenarios.
% Moreover, TNT’s noise adaptation outperforms RLCF* (see Section \ref{sec:train_param}), which optimizes the entire image encoder, despite utilizing fewer trainable parameters. This highlights that even a small portion of noise acts as a dynamic augmentation during test time, enabling the model to refine its attention maps by modulating feature representations at spatial scales. This enables to focus on relevant regions (see Figure \ref{fig:concept}), resulting in better adaptation to fine-grained variations in OOD samples.
\vspace{0.1cm}
\noindent\textbf{Cross-Dataset Generalization:} 
% Table \ref{tab:cross_data} reports the performance of TNT for cross-dataset evaluation across ten datasets, following \cite{shu2022test}. \(\text{TNT}\) surpasses all baselines on average, with the highest average of 64.48\%. Specifically, \(\text{TNT}\) achieves best generalization on seven out of ten datasets. TNT*, with an average of 64.07\%, also demonstrates strong generalization, outperforming CLIP, CoOp, and \(\text{TPT}\) on five, six, and four of the ten datasets, respectively. Notably,  TNT* outperforms TPT with a higher margin than other prompt-tuning-based baselines on OxfordPets, UCF, Aircraft, and StanfordCars. TNT's use of learnable noise refines the visual feature space, \textit{enabling the model to capture subtle distinctions in cross-datasets}. The minimization of inter-view mean distance ensures consistent embeddings, enhancing the model's ability to differentiate class-specific details that \(\text{TPT}\) \cite{shu2022test} and \(\text{RLCF}\) \cite{zhaotestrlcf} miss. This visual space adaptation enhances zero-shot generalization for cross-data tasks by handling distribution shifts without altering the black-box model.
Table \ref{tab:cross_data} shows TNT's cross-dataset evaluation across ten datasets, following \cite{shu2022test}. TNT outperforms all baselines with an average accuracy of 64.48\% and achieving top generalization on seven out of ten datasets. TNT* also performs well, averaging 64.07\% and surpassing CLIP, CoOp, and TPT on several datasets. Notably, TNT* excels over TPT on OxfordPets, UCF, Aircraft, and StanfordCars. TNT's use of learnable noise enhances the visual feature space, capturing subtle distinctions across datasets and ensuring consistent, class-specific embeddings for improved zero-shot generalization, all without modifying the pre-trained model. The minimization of inter-view mean distance ensures consistent embeddings, enhancing the model's ability to differentiate class-specific details that \(\text{TPT}\) \cite{shu2022test} and \(\text{RLCF}\) \cite{zhaotestrlcf} miss.



\input{tables/cross_data}


\section{Analysis and Ablations}

We conduct extensive analysis and ablations to assess how design choices impact performance, using the ImageNet-A benchmark with the ViT-B/16 backbone for consistency, as it represents a basic domain generalization variant.

% Table \ref{tab:cross_data} presents a comprehensive comparison of Top-1 accuracy across different models with two feature dimensions (32 and 64) on the ImageNet-A dataset. Our proposed model, TNT, achieves the highest Top-1 accuracy of 62.087\% with 64 features, surpassing other models such as CTPT, RLCF, TPT, and SALS. This demonstrates the effectiveness of our approach in capturing meaningful patterns from the dataset.
% When comparing the performance between feature dimensions, a slight decrease in accuracy is observed when reducing from 64 to 32 features. For instance, TNT achieves 62.087\% accuracy with 64 features, while it drops marginally to 62.01\% with 32 features. This pattern holds across most models, indicating a small but consistent impact on performance due to feature dimensionality reduction. However, this degradation remains minimal, often less than 0.1\%, which suggests that models are robust to feature reduction to an extent.

% \begin{table}[H]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Features} & \textbf{Acc\#1} \\
% \hline
% CTPT & \multirow{5}{*}{64} & 54.438 \\
% \cline{1-1} \cline{3-3}
% RLCF &  & 54.451 \\
% \cline{1-1} \cline{3-3}
% TPT &  & 54.358 \\
% \cline{1-1} \cline{3-3}
% SALS &  & 54.504 \\
% \cline{1-1} \cline{3-3}
% \textbf{TNT (OURS)} &  & 62.087 \\
% \hline
% CTPT & \multirow{5}{*}{32} & 54.118 \\
% \cline{1-1} \cline{3-3}
% RLCF &  & 54.08 \\
% \cline{1-1} \cline{3-3}
% TPT &  & 54.11 \\
% \cline{1-1} \cline{3-3}
% SALS &  & - \\
% \cline{1-1} \cline{3-3}
% \textbf{TNT (OURS)} &  & - \\
% \hline
% \end{tabular}
% \caption{\textbf{Top-1 Accuracy (\%)} comparison for different models with varying feature dimensions on the ImageNet-A dataset. The results indicate a slight performance decrease when reducing the number of features, though this degradation is minimal. We recommend using 64 features for optimal performance.}


% \label{tab:model_comparison}
% \end{table}

% \begin{table}[H]
% \centering
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Model} & \textbf{Features} & \textbf{Acc\#1} & \textbf{Features} & \textbf{Acc\#1} \\
% \hline
% CTPT & \multirow{5}{*}{64} & 54.438 & \multirow{5}{*}{32} & 54.118 \\
% \cline{1-1} \cline{3-3} \cline{5-5}
% RLCF &  & 54.451 &  & 62.01 \\
% \cline{1-1} \cline{3-3} \cline{5-5}
% TPT &  & 54.358 &  & 54.11 \\
% \cline{1-1} \cline{3-3} \cline{5-5}
% SALS &  & 54.504 &  & 53.95 \\
% \cline{1-1} \cline{3-3} \cline{5-5}
% TNT (OURS) &  & \textbf{62.087} &  & \textbf{61.4} \\
% \hline
% \end{tabular}
% \caption{\textbf{Top-1 Accuracy (\%)} comparison for different models with varying feature dimensions on the ImageNet-A dataset.}
% \label{tab:model_comparison}
% \end{table}
%%%

\begin{figure}[t]
    % \centering
    \includegraphics[width=0.48\textwidth]{Figures/trainable_params.pdf}
     % \caption{\textbf{Analysis of Trainable Parameters (TP)} for TNT, textual tuning. Circle size indicates the \#TP. Textual Tuning methods {\color{customrocket}TPT}, {\color{customrocket}C-TPT}, {\color{customrocket}SaLS}, and {\color{customrocket}RLCF} use the same TP count to optimize prompts. {\color{customblue}RLCF\textsuperscript{*}} refers to RLCF with all visual encoder parameters trainable, {\color{customblue}{Layer Norm}} limits trainable parameters to only Layer Norms, and {\color{customblue}{Visual Prompt}} applies learnable prompts to the visual side across 12 layers of the ViT encoder. {\color{customgreen}{TNT‡}} indicates TNT with only \(224\times9\) trainable noise parameters, compared to standard {\color{customgreen}{TNT}} with \(224\times224\times3\) TP. {\color{customgreen}{Noise}} denotes optimization based on only entropy using \(224\times9\) TP.  Test-Time Noise Tuning (TNT) achieves the best trade-off between Trainable Parameters (TP) and Accuracy compared to prompt tuning and visual encoder tuning.} (denoted with {\color{customrocket}{$\boldsymbol{\bullet}$}}) 
     \vspace{-0.6cm}
     \caption{\textbf{Analysis of Trainable Parameters (TP)} for TNT, textual tuning, and encoder tuning. Circle size indicates the \#TP. {\color{customrocket}{Textual Tuning methods}} use the same TP count of 2K to optimize prompts. {\color{customblue}RLCF\textsuperscript{*}} refers to RLCF with all visual encoder parameters trainable, {\color{customblue}{Layer Norm}} limits trainable parameters of visual encoder to only Layer Norms, and {\color{customblue}{Visual Prompt}} applies learnable prompts to the visual encoder across 12 layers of the ViT encoder. {\color{customgreen}{TNT$\ddag$}} indicates TNT with only \(224\times9\) trainable noise parameters, compared to standard {\color{customgreen}{TNT}} with \(224\times224\times3\) TP. {\color{customgreen}{Noise}} denotes optimization with \(224\times9\) TP in noise and with only \(\mathcal{L}_{\text{entropy}}\) loss.  
     % Test-Time Noise Tuning (TNT) achieves the best trade-off between Trainable Parameters (TP) and Accuracy compared to prompt tuning and visual encoder tuning.
     }
    \label{fig:params}
    \vspace{-2em}
\end{figure}

% {\renewcommand{\arraystretch}{1.1}
% \begin{table}[t]
% \caption{Analysis of Trainable Params}
% \label{tab:params}
% % \fontsize{12}{12}\selectfont
% \resizebox{0.475\textwidth}{!}{%
% \begin{tabular}{l|l|c|c}
% \hline
% \multicolumn{1}{l|}{\textbf{Group}} & \textbf{Method} & \multicolumn{1}{l|}{\textbf{Top-1 Acc}} & \multicolumn{1}{l}{\textbf{\#Params}} \\ \hline
% \multicolumn{1}{l|}{CLIP-ZS} & CLIP-ViT-B/16 & 47.85 & 0 \\ \hline
%  & TPT & 54.31 & 2048 \\
%  & C-TPT & 50.52 & 2048 \\
%  & SaLS & 56.65 & 2048 \\ \hline
% \multirow{-4}{*}{Textual Tuning} & RLCF & 54.77 & 2048 \\
%  & RLCF* & 59.54 & 86192640 \\
%  & Layer Norm & 57.22 & 49936 \\
% \multirow{-3}{*}{Visual Tuning} & Visual Prompts & 46.38 & 1180000 \\ \hline
% \rowcolor[HTML]{E2EFDA} 
% \cellcolor[HTML]{E2EFDA} & TNT & 63.93 & 150528 \\
% \rowcolor[HTML]{E2EFDA} 
% \cellcolor[HTML]{E2EFDA} & TNT+ & 61.97 & 2016 \\
% \rowcolor[HTML]{E2EFDA} 
% \multirow{-3}{*}{\cellcolor[HTML]{E2EFDA}Ours} & Noise & 54.01 & \textbf{2016} \\ \hline
% \end{tabular}}
% \end{table}}

\subsection{Computational Analysis}
\label{comp_analysis}
\noindent\textbf{Trainable Parameters (TP) vs. Accuracy:}
\label{sec:train_param}
Test-time tuning methods like TPT, C-TPT, SaLS, and RLCF (prompt-tuning variant) adapt textual prompts with only 2048 TP (4 tokens of d=512) but show limited generalization, with Top-1 ImageNet-A accuracy around 50\% to 55\%. In contrast, visual adaptation based methods \textit{e.g.}  encoder tuning, visual prompting, and layer norm optimization—require more TP.
As depicted in Figure \ref{fig:params}, these Visual Tuning approaches offer moderate to lower generalization with increased TP, and that too while accessing the encoder itself.
% Vision Encoder tuning requires ~86M TP with a Top-1 accuracy of 59.54, Visual Prompting uses 1.18M TP with 46.38 accuracy, and Layer Norm adaptation involves a lower 50k TP, reaching 57.22 accuracy. 
In contrast, TNT effectively balances this trade-off with 150k (\(224\times224\times3\)) TP, achieving the highest generalization at 63.93, \textit{while preserving the black-box assumption}. Remarkably in our ablation, when TNT is initialized with 2016 (\(224\times9\)) TP (similar to the parameter count of textual tuning methods), it still demonstrates stronger generalization than baselines, highlighting its adaptability in handling distribution shifts through noise tuning. Furthermore, when TNT’s noise component (with 2016 TP) is tuned using only entropy (without \(\mathcal{L}_{\text{vc}}\) or temperature-scaled inference), it achieves performance on par with that of textual tuning approaches.
% Test-time Textual tuning methods like TPT, C-TPT, SaLS, and RLCF use only 2048 trainable parameters (TP) but achieve modest generalization, with Top-1 accuracy around 56. In contrast, visual adaptations for Vision-Language Models (VLMs) in zero-shot setups, such as encoder tuning (\(\sim \)86M TP, 59.54 accuracy), visual prompting (\(\sim \)1.18M TP, 46.38 accuracy), and layer norm optimization (\(\sim \)50k TP, 57.22 accuracy), require more TP. TNT optimizes this trade-off with \(\sim \)150k TP (\(224\times224\times3\)), reaching 63.93 accuracy. Notably, when TNT is initialized with 2016 TP (\(224\times9\)), it still generalizes effectively, demonstrating adaptability to distribution shifts. Additionally, tuning TNT’s noise solely with entropy achieves performance similar to textual tuning.

\vspace{0.2cm}
\noindent\textbf{Trade-off between Accuracy, Time, and Memory:} Table \ref{tab:computation} shows that TNT achieves the highest performance while maintaining optimal inference time and memory efficiency if not the best. 
% TNT applies inter-view mean consistency minimization directly on \textit{image embeddings which account for higher memory usage}, as it requires storing multiple high-dimensional embeddings for each view, and computing pairwise relationships between them, significantly increasing the memory load. 
Although TNT demands a moderate memory increase (8.57 GB), it maintains practical inference speed and accuracy compared to alternatives like C-TPT, which incurs higher memory (up to 33.44 GB for ImageNet-V) and slower inference times. However, our approach demonstrates the best trade-off by achieving top accuracy with efficient memory use, especially in cases where feature complexity necessitates higher resource allocation for robust predictions. 

\input{tables/computation}


%%%
\subsection{Ablations}\label{ablations}
\noindent\textbf{Effect of Different Components:}
% Existing TTA approaches focus on textual prompt adaptation, yielding moderate accuracy improvements but with less robustness in handling OOD data. 
Although TNT's noise tuning shows enhanced results, it is intriguing to understand how each component contributes to improved generalization and calibration. As depicted in Figure \ref{fig:different_components}, initialized noise, when optimized with only entropy (\textbf{E}), shows comparable generalization and calibration to textual tuning baselines. Adding the inter-view consistency loss (\textbf{E+V}) improves the alignment of image embeddings, resulting in more consistent and confident predictions. This approach not only outperforms entropy minimization alone but also reduces the Expected Calibration (EC) error by 3\%. Intuitively, this process encourages the embeddings to be closer in feature space, leading to lower intra-class variance while preserving inter-class differences. 
Furthermore, during inference, applying temperature \(\tau\) to scale the output logits (Eq.~\ref{eq:inference}), as in \textbf{E+V+T$^\prime$}, increases accuracy by 3\%. Extending this approach to consider the top-\(K\) views, as in \textbf{E+V+T}, further improves performance by 2\%. This configuration corresponds to our TNT* variant. The top-\(K\) strategy focuses final predictions on the most confident views rather than relying solely on a single sample, resulting in notable accuracy gains.
% TNT*, which combines top-$K$ views-based inference with temperature scaling and noise adaptation, consistently surpasses baseline models. 
When TNT* is initialized with CoOp, resulting in \textbf{TNT}, it achieves the highest generalization (63.93 Top-1 accuracy) and reliable calibration error \cite{yooncctpt, murugesan2024robust}. TNT achieves an ECE of 11.46, second only to C-TPT, while outperforming all other baselines in accuracy. This result indicates strong uncertainty estimation with effective noise tuning.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/different_components_a.pdf}
        \vspace{-0.6cm}
        \caption{Top-1 Accuracy}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/different_components_a_ece.pdf}
        \vspace{-0.6cm}
        \caption{Expected Calibration Error \cite{yooncctpt}}
    \end{subfigure}
    \caption{
    \textbf{Effect of TNT Components on (a) Top-1 Accuracy (Higher$\uparrow$ is better) and (b) ECE (Lower$\downarrow$ is better)}.
    \textbf{E}: Noise optimization with Entropy minimization $\mathcal{L}_{\text{entropy}}$.
    \textbf{E+V}: Adds $\mathcal{L}_{\text{vc}}$ loss (Eq.~\ref{eq:Lvc_loss}) to $\mathcal{L}_{\text{entropy}}$.
    \textbf{E+V+T$^\prime$}: Adds temperature scaling during inference to E+V. 
    \textbf{E+V+T}: Makes use of top-$K$ views instead of one test image (Eq.~\ref{eq:inference}), i.e. TNT*. 
    \textbf{TNT}: TNT* with CoOp initialization, 
    \textbf{TNT+PT}(\textit{Prompt Tuning}): Optimizes textual prompts with TNT. 
    \textbf{TNT+ET}(\textit{Encoder Tuning}): Optimizes the visual encoder with TNT. Optimization Steps $t=1$ is used consistently. The same Legend is used for (a) and (b).
    }
    \label{fig:different_components}
    \vspace{-0.50cm}
\end{figure}

\vspace{0.2cm}
\noindent\textbf{Impact of Combinative Tuning:} Assumably, when combining the proposed TNT with prompt tuning (PT) or encoder tuning (ET), one might expect better generalization. Interestingly, we observe in Figure \ref{fig:different_components} that both \textbf{TNT+PT} and \textbf{TNT+ET} achieve comparable generalization to TNT while \textit{significantly improving calibration error}. Intuitively, this is because \textbf{TNT+PT} refines the alignment between textual and visual features, enhancing calibration. On the other hand, \textbf{TNT+ET} improves calibration by allowing the model to better align its learned visual representations with the specific distribution of the input data. This shows that calibration error can be further minimized by optimizing the interaction between the model's visual and textual components, on top of the TNT framework.



\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=0.49\textwidth]{Figures/steps_vs_acc.pdf}
        \includegraphics[width=0.49\textwidth]{Figures/steps_vs_ece.pdf}
        \caption{Effect of Number of Optimization Steps (with 64 augmentations)}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=0.49\textwidth]{Figures/augs_vs_acc.pdf}
        \includegraphics[width=0.49\textwidth]{Figures/ece_vs_augs.pdf}
        \caption{Effect of Number of Augmentations (with 1 optimization step)}
    \end{subfigure}
    \vspace{-0.6cm}
    \caption{Increasing the number of optimization steps and augmentations both result in higher \textbf{(a) Top-1 Accuracy}, and lower \textbf{(b) Expected Calibration (EC) Error}. TNT* and TNT denote hand-crafted and CoOp-based prompts, respectively. The legend is shared throughout.}
    \label{fig:ablations}
    \vspace{-0.55cm}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.235\textwidth}
%             \includegraphics[width=\textwidth]{Figures/augs_vs_acc.pdf}
%         \caption{Top-1 Accuracy vs \#Augs}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.235\textwidth}
%             \includegraphics[width=\textwidth]{Figures/ece_vs_augs.pdf}
%         \caption{ECE vs \#Augmentations}
%     \end{subfigure}
%     \caption{\textbf{Effect of Number of Augmentations on (a) Top-1 Accuracy (Higher$\uparrow$ is better) and (b) ECE (Lower$\downarrow$ is better)}, with Optimization Steps $t=1$. TNT* and TNT denote hand-crafted and CoOp-based prompt initializations, respectively. Same Legend is used for (a) and (b).}
%     \label{fig:augs_vs}
% \end{figure}


\vspace{0.2cm}
\noindent\textbf{Effect of Optimization Steps:}
All reported results thus far have been obtained with a single training step; however, we observe that TNT’s efficacy is influenced by the number of optimization steps, impacting both accuracy and ECE \cite{yooncctpt}. In Figure \ref{fig:ablations}(a)(left), we observe that increasing the number of steps leads to a 2\% performance gain for both TNT* and TNT configurations from step $t=1$ to $t=5$. While other baselines follow a similar trend across optimization steps, they exhibit lower accuracy compared to TNT, indicating TNT’s adaptability for more efficient applications. Interestingly, in Figure \ref{fig:ablations}(a)(right), we find that TNT variants achieve notably lower calibration error with more optimization steps, as TNT* reaches an ECE of 5.67 at step $t=5$, outperforming zero-shot CLIP (ECE 8.32) and other baselines by a considerable margin. The additional optimization steps in TNT encourage the noise to increasingly adapt to more representative features, as shown in Figure \ref{fig:concept}.


\vspace{0.2cm}
\noindent\textbf{Effect of Number of Augmentations:}
Similarly, accuracy improves with an increasing number of augmentations across TNT configurations and baselines, reaching a plateau at \(N=64\) for most baselines as shown in Figure \ref{fig:ablations}(b)(left). Notably, TNT achieves a Top-1 Accuracy of 65.82 with 128 augmentations, though with higher memory requirements. In contrast, TNT* sees a modest 0.20\% increase from \(N=64\) to \(N=128\) augmentations. Regarding ECE, TNT also reduces ECE with an increasing number of augmentation steps, where TNT* achieves the second-best ECE at 9.99, following C-TPT at 8.04, while TPT reaches 14.15 as shown in Figure \ref{fig:ablations}(b)(right). Varied augmentations provide more \textit{diverse} and class-specific crops, leading to better overall generalization and lower EC error. Given the linear increase in memory usage associated with additional augmentations, we adopt single-step optimization as the default setting for both TNT* and TNT, as they already improve the average OOD accuracy over zero-shot CLIP by 5.42\% and 7.38\% respectively. 

% \noindent\textbf{Effect of Learning Rates.} 
% We conducted experiments to assess the impact of noise learning rate on model performance across three datasets, as presented in Table \ref{tab:noise_lr}. Our findings indicate that the noise learning rate has a marginal effect on performance. Based on these results, we selected 0.001 as the optimal noise learning rate for subsequent experiments.
% \begin{table}[t]
% \centering
% \caption{Noise Learning Rate Comparison on Different Datasets (Top1 acc. $\blacktriangle$ / Top5 acc. $\blacktriangle$) with CoOp initialization}
% % \setlength{\tabcolsep}{7mm}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% {Noise LR} & ImageNet-A  & {Flower102} & {ImageNet-V2} \\
% \midrule
% 0.001   & 63.93 / 88.36 & 68.74 / 83.92 & 66.64 / 89.69 \\
% 0.0001  & 63.97 / 89.49 & 67.92 / 84.45 & 66.87 / 89.70 \\
% 0.00001 & 63.85 / 89.62 & 68.01 / 83.80 & 66.45 / 89.86 \\
% \bottomrule
% \end{tabular}
% }
% \label{tab:noise_lr}
% \end{table}

%%%



\subsection{Qualitative Analysis}
\noindent\textbf{Impact of Noise Optimization on Attention Maps:}
TNT optimizes noise in a way that it \textit{implicitly} influences the input’s attention maps, guiding the model to adaptively focus on the most relevant features within the test input. This process enables TNT to dynamically emphasize important regions, ensuring the model attends to critical information for each sample. As illustrated in Figure \ref{fig:concept}, there is a noticeable refinement in the attention between the original and adapted samples; with each optimization step, the adapted noise becomes increasingly \textit{aligned} with the salient features, effectively suppressing irrelevant details and amplifying relevant cues. This progressive adaptation results in more confident and accurate predictions, as the model’s focus narrows on features that are contextually meaningful and better aligned with the target task.

\vspace{0.2cm}
\noindent\textbf{Feature Shifts via Adaptation:}
Figure \ref{fig:tsne} examines the shift in the distribution of visual features after optimization, comparing the results from baselines and TNT.
As depicted, CLIP-ZS and TPT show scattered feature distributions, reflecting their struggle to distinguish class boundaries effectively in naturally shifted data. In contrast, TNT demonstrates tightly \textit{clustered} and \textit{well-separated} features, which suggests that TNT’s noise adaptation mechanism and consistency losses promote feature alignment and separation more effectively than entropy-based or prompt-tuning methods alone. 
% By guiding the model to emphasize class-relevant features while suppressing noise, TNT encourages the formation of distinct clusters, which improves both generalization and calibration. 
This focused feature adaptation helps establish more \textit{defined decision boundaries} and mitigates the impact of irrelevant features, thus leading to better overall performance on out-of-distribution samples.

% \begin{figure}[t]
%     \centering
%     % \includegraphics[width=0.48\textwidth]{fig/noise_vis.png} 
%     \includegraphics[width=0.49\textwidth]{Figures/attn_susan.pdf}
%     % \includegraphics[width=0.49\textwidth]{Figures/attn_snake.pdf}
%     \caption{\textbf{Impact of Noise Tuning on Attention Maps Across Optimization Steps.} \textit{Attention Difference} illustrates the absolute difference between the clean attention map and the noise-tuned attention map at each optimization step \( t \), highlighting regions of sensitivity that evolve with each adaptation step. 
%     % \textbf{GT} represents the Ground Truth prediction, while \textbf{ZS} and \textbf{TNT} denote the predictions from zero-shot CLIP-ViT-B/16 and TNT, respectively. Please zoom in to observe subtle distinctions more clearly.
%     }
%     \label{fig:attn}
% \end{figure}

\begin{figure}[t]
    \centering    
    \begin{subfigure}[b]{0.155\textwidth}
        \includegraphics[width=\textwidth]{Figures/clip_features_A.png.pdf}
        \caption{CLIP-ZS \cite{radford2021learning}}
    \end{subfigure}
    \begin{subfigure}[b]{0.155\textwidth}
        \includegraphics[width=\textwidth]{Figures/tpt_features_A.png.pdf}
        \caption{TPT \cite{shu2022test}}
    \end{subfigure}
    \begin{subfigure}[b]{0.155\textwidth}
        \includegraphics[width=\textwidth]{Figures/tnt_features_A_confident_augs.png.pdf}
        \caption{\textbf{TNT} (Ours)}
    \end{subfigure}
    \caption{\textbf{t-SNE visualizations} of the final class embedding from the test sets of ImageNet-A dataset, following Table \ref{tab:natural_shift}. TNT could produce \textit{more clustered and separable features} than other zero-shot generalization baselines.}
    \label{fig:tsne}
    \vspace{-0.5cm}
\end{figure}
% As shown in Figure \ref{fig:tsne}, with adaptive Gaussian noise guided by CDIST and entropy minimization, the 2D points of t-SNE \cite{van2008visualizing} for feature maps of 64 augmented images produced by our method exhibit greater compactness compared to those of zero-shot CLIP.

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}{0.15\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{fig/3_image_0.png}
%         \caption{Image}
%         \label{fig:tsne_image}
%     \end{subfigure}
% \hfill
%     \begin{subfigure}{0.15\textwidth}
%         \centering
%         \fbox{\includegraphics[width=\textwidth]{fig/tSNE_ZeroShot3.png}}
%         \caption{Zero Shot}
%         \label{fig:tsne_zeroshot}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.15\textwidth}
%         \centering
%         \fbox{\includegraphics[width=\textwidth]{fig/tSNE_Ours3.png}}
%         \caption{TNT}
%         \label{fig:tsne_ours}
%     \end{subfigure}
    
%     \caption{An example of t-SNE visualization of the image encoder's feature maps for 64 augmented images, comparing zero-shot CLIP with our proposed method.}
%     \label{fig:tsne}
% \end{figure}

% \section{Planned work for remaining weeks}

% \begin{itemize}
%     \item Completing the missing sections of the report, including the literature review, specific parts of the methodology, and additional experimental details such as inference time and computational complexity.
    
%     \item Providing more qualitative results and visualizations to achieve a comprehensive analysis.
% \end{itemize}

\section{Conclusion}
We introduce Test-Time Noise Tuning (TNT), a novel noise adaptation strategy for zero-shot settings that enhances out-of-distribution generalization by tuning a learnable noise for visual input of a VLM, improving robustness and calibration. TNT demonstrates the potential of noise tuning in challenging VLM benchmarks, setting a foundation for adaptive OOD handling. Our noise tuning demonstrates, for the first time, a positive impact on representation learning in VLMs, paving the way for further exploration across other modalities. Extending TNT’s noise tuning and inter-view consistency loss to other vision-language tasks, such as retrieval, as well as applications like medical imaging, would be valuable.
A promising direction for future research is to explore strategies for reducing the memory requirements of TNT, enhancing its scalability and applicability in resource-constrained environments.
 

{
\small
\bibliographystyle{ieeenat_fullname}
\bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
