\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
%\thispagestyle{empty}
%\rhead{ \textit{ }} 

% Update your Headers here
%\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}

%My
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{thmtools}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{authblk}
\usepackage{kotex}
\usepackage{multicol}
\usepackage{wrapfig,lipsum}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
%\usepackage[ruled,vlined]{algorithm2e}
%\usepackage{caption}
\usepackage{subcaption}
% theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{idea}[theorem]{Idea}

\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption,stackengine}
\usepackage{bm}


  
%% Title
\title{Tensor Product Neural Networks for Functional ANOVA Model}

\author{%
Seokhun Park$^{1}$ \quad Insung Kong$^{2}$ \quad Yongchan Choi$^{3}$ \quad Chanmoo Park$^{1}$ \ \quad Yongdai Kim$^{1}$ \\
$^1$Seoul National University \quad $^{2}$University of Twente \quad $^{3}$Toss bank \\
\texttt{\{shrdid,chanmoo13,ydkim903\}@snu.ac.kr} \\
\texttt{insung.kong@utwente.nl} \\
\texttt{pminer32@gmail.com}\\
}


\begin{document}
\maketitle


\begin{abstract}
Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. 
The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions (commonly referred to as {\it components}), is one of the most popular tools for interpretable AI, and recently, various neural networks have been developed for estimating each component in the functional ANOVA model. 
However, such neural networks are highly unstable when estimating each component since the components themselves are not uniquely defined. 
That is, there are multiple functional ANOVA decompositions for a given function. 
In this paper, we propose a novel neural network  
which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably and accurately. 
We call our proposed neural network {\it ANOVA Tensor Product Neural Network (ANOVA-TPNN)} since
it is motivated by the tensor product basis expansion.
Theoretically, we prove that ANOVA-TPNN can approximate any smooth function well.
Empirically, we show that ANOVA-TPNN provide much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural networks do.
\end{abstract}


\section{Introduction}
\vskip -0.2cm

Interpretability has become more important as artificial intelligence (AI) models have become more sophisticated and complicated in recent years. 
Various methods such as LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified} have been suggested to interpret complex black-box AI models. 
However, these methods explain a given black-box model through
a locally approximated interpretable models and thus
often fail to provide a faithful global view of the model \cite{AttackSHAP&2020&Slack}.

The functional ANOVA model, which approximates a given complex high-dimensional function by the sum of low-dimensional (e.g., one or two dimensional) functions, is a well known transparent-box AI model.
One of the most representative examples of the functional ANOVA model is the generalized additive model (GAM, \cite{gam}), which consists of the sum of one-dimensional functions, each corresponding to each input feature.
Low-dimensional functions are easier to understand, and thus the functional ANOVA model is popularly used for interpretable AI \cite{func_puri, neural_decom}.

%In this paper, we develop a novel neural networks for the functional ANOVA model.
Recently, several specially designed neural networks for the functional ANOVA model have been proposed, including NAM \cite{nam} and NBM \cite{nbm}.
These neural networks can be learned by standard stochastic gradient descent algorithms and thus can be applied to large sized data compared to traditional learning algorithms based on basis expansions \cite{wood2006low} and regularizations \cite{SSANOVA}.
However, existing neural networks are not good at estimating each component (each
low-dimensional function in the functional ANOVA model) mainly due to unidentifiability.  
Here, `unidentifiability' means that there exist multiple different functional ANOVA decompositions of a given function and we do not know which decomposition a gradient descent algorithm converges on. Note that poor estimation of the components would result in poor interpretation. 

In this paper, we develop new neural networks for the functional ANOVA model such that each component is identifiable but they are learnable by standard stochastic gradient descent algorithms. Identifiability makes our proposed neural networks be good at estimating the components and thus provide reliable interpretation.
In addition, it is robust to outliers and easy to reflect monotone constraints.

To develop the proposed neural networks, we begin with the tensor product basis expansion \cite{wood2006low} and replace
each basis function by specially designed neural networks so that 
each component becomes identifiable and robust to outliers.
We call our proposed neural networks {\it Tensor Product Neural Networks} (TPNN).
Finally, we propose ANOVA-TPNN, which estimates each component in the functional ANOVA model using TPNNs. Theoretically,
we prove that ANOVA-TPNN has the universal approximation property in the sense that
it approximates any Lipschitz function well.

Overall, our contributions are summarized as follows.
\vskip -0.2cm
\begin{itemize}
    \item We propose novel neural networks  (TPNN) for the functional ANOVA model with which 
    we can estimate each component stably and accurately  by use of a standard stochastic gradient descent algorithm.

    \item We prove the universal approximation property in the sense that TPNN can approximate any Lipschitz function up to an arbitrary precision.
    
    \item By analyzing multiple benchmark datasets, we demonstrate that TPNN provides more accurate and stable estimation and interpretation of each component compared to the baseline models, including NAM \cite{nam}, NBM \cite{nbm}, NODE-GAM \cite{nodegam} and XGB \cite{chen2016xgboost} without losing prediction accuracy. 
\end{itemize}

\section{Background}
\subsection{Notation}\label{sec:notation}
Let $\textbf{x}=(x_{1},...,x_{p})^\top \in \mathcal{X} = \mathcal{X}_{1}\times ... \times \mathcal{X}_{p}$
be a vector of input features, where we assume $\mathcal{X} \subseteq [-a,a]^p$ for some $a>0$.
We denote $[p] = \{1,\dots,p\}$, and denote its power set as $\mathcal{P}([p])$.
For $\textbf{x} \in \mathcal{X}$ and $S \subseteq [p]$, let $\textbf{x}_{S}=(x_j,j\in S)^\top.$ 
We denote $f_{S}$ as a function of $\textbf{x}_{S}$. 
For a real-valued function  $f : \mathcal{X} \to \mathbb{R}$, 
we denote $||f||_{\infty} = \operatorname{sup}_{\textbf{x} \in \mathcal{X}}|f(\textbf{x})|.$
\vskip -0.1cm
\vskip -0.4cm
\subsection{Functional ANOVA model}

The functional ANOVA model \cite{func_first} decomposes a high-dimensional function $f$ into the sum of low-dimensional functions 
{\footnotesize \begin{align*}
 f(\textbf{x}) &= \beta_0+ \sum_{j=1}^p f_j(x_j)+ \sum_{j<k} f_{jk}(x_j,x_k) + \cdots,
\end{align*}
}
\vskip -0.3cm
which is considered as one of the most important XAI tools \cite{nam, nbm, func_puri}.
Typically, $f$ is defined as $f(\textbf{x})=g(\mathbb{E}(Y|X=x))$ where $g$ is a link function and $Y$ is target variable.
In the functional ANOVA model, $f_j, j\in [p]$ are called the main effects, and $f_{j,k}, (j,k)\in [p]^2$ are called the second interaction terms and so on. 
In practice, only interactions of lower orders (e.g., the main and second order only) are included in the decomposition for easy interpretation.

The Generalized Additive Model (GAM, \cite{gam}) is a special case of the functional ANOVA model with only main effects included, that is
{\footnotesize
\begin{align*}
f(\textbf{x}) = \beta_0+ \sum_{j=1}^p f_j(x_j).  
\end{align*}
}
\vskip -0.3cm
Similarly, G$\text{A}^{2}$M is defined as the functional ANOVA model including
all of the main effects and second order interactions,
{\footnotesize
\begin{align*}
f(\textbf{x}) = \beta_{0} + \sum_{S \subseteq [p], |S|\leq 2} f_{S}(\textbf{x}_{S}),
\end{align*}
}
\vskip -0.3cm
or more generally we can consider G$\text{A}^{d}$M defined as
{\footnotesize
\begin{align*}
f(\textbf{x}) = \beta_{0} + \sum_{S \subseteq [p], |S|\leq d} f_{S}(\textbf{x}_{S}).    
\end{align*}
}
\vskip -0.2cm
Several learning algorithms for the functional ANOVA model have been proposed.
\cite{SSANOVA} applied the smoothing spline to learn the functional ANOVA model, \cite{cosso} 
developed a component-wise sparse penalty, \cite{anovaboost} proposed a boosting algorithm for the functional ANOVA model, and \cite{wood2006low, rugamer2024scalable} proposed methods for estimating the functional ANOVA model using basis expansion.

In addition, the functional ANOVA model has been applied to various problems such as sensitivity analysis \cite{GHoff}, survival analysis \cite{func_hazard}, diagnostics of high-dimensional functions \cite{hooker_diag} and machine learning models \cite{func_puri, neural_decom}. 

Recently, learning the functional ANOVA model using neural networks has received much attention since gradient descent based learning algorithms can be easily scaled up. 
Examples are Neural Additive Model (NAM, \cite{nam}), Neural Basis Model (NBM, \cite{nbm}) and NODE-GAM \cite{nodegam} to name just a few.
NAM models each component of GAM by DNNs, and NBM achieves a significant reduction in training time compared to NAM by using basis DNNs to learn all components simultaneously. 
NODE-GAM extends Neural Oblivious Decision Ensembles (NODE, \cite{node}) for GAM.


\subsection{Tensor Product Basis}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{picture/fig_id_explain.png}
%     \caption{\footnotesize\textbf{Plots of the estimated components for 5 different random datasets on \textsc{calhousing} dataset.} }
%     \label{fig_id_importance}
% \end{figure*}

Let $\mathcal{B}_j=\{B_{j,k}(\cdot), k=1,\ldots,M_j\}$ be a given set of basis functions 
on $\mathcal{X}_j$ (e.g. truncated polynomials or B-splines). Then, the main effect is approximated as follows:
{\footnotesize
\begin{align*}
f_j(x_j) &\approx \sum_{k=1}^{M_j} \alpha_{jk} B_{j,k}(x_j) \\
&=\textbf{B}_j(x_j)^\top {\bm \alpha}_j.
\end{align*}
}
\vskip -0.2cm
where {\footnotesize $\textbf{B}_{j}(x_{j}) = (B_{j,1}(x_{j}),...,B_{j,M_{j}}(x_{j}))^{\top}$} and {\footnotesize ${\bm \alpha}_{j} = (\alpha_{j,1},...,\alpha_{j,M_{j}} )^{\top}$}.

For the second order interactions, $f_{jk}(x_j,x_k)$ is approximated by 
{\footnotesize
\begin{eqnarray*}
f_{jk}(x_j,x_k)&\approx& (\textbf{B}_j(x_j) \otimes \textbf{B}_k(x_k)) {\bm \alpha}_{j,k}\\
&=&\sum_{l=1}^{M_j}\sum_{h=1}^{M_k} B_{j,l}(x_j)B_{k,h}(x_k) \alpha_{j,k,l,h}.
\end{eqnarray*}
}
\vskip -0.1cm
Here,
{\footnotesize $\textbf{B}_j(x_j) \otimes \textbf{B}_k(x_k)=(B_{j,l}(x_j)B_{kh}(x_k)_{l\in [M_l], k\in [M_k] })$} are called the tensor product basis functions and ${\bm \alpha}_{j,k} = (\alpha_{j,k,l,h}, l \in [M_{l}], k \in [M_{k}])^{\top} \in \mathbb{R}^{M_{l}M_{k}}$.

Similarly, for $S \subseteq [p]$, a general order interaction $f_S(\textbf{x}_S)$ is approximated by
{\footnotesize
$$ f_S(\textbf{x}_S) \approx (\otimes_{j\in S} \textbf{B}_j(x_j)) {\bm \alpha}_S,$$
}
\vskip -0.2cm
where {\footnotesize $\otimes_{j\in S} \textbf{B}_j(x_j)$} and {\footnotesize ${ \bm \alpha}_S$} are {\footnotesize $\prod_{j \in S}M_{j}$}-dimensional tensors.

A problem of using tensor product basis functions is that the number of learnable parameters (i.e. $\alpha_{Si_{1}\cdots i_{|S|}}, i_{j} \in [M_{j}]$) increases exponentially in $|S|$ and thus estimating higher order interactions would be computationally demanding.
Thus, they are typically used for low-dimensional datasets.


\section{Tensor Product Neural Networks} \label{sec:proposed_model}

The aim of this section is to propose a specially designed neural models so called Tensor Product Neural Networks (TPNN).
The most important advantage of TPNN is that they satisfy an identifiability condition of each component
in the functional ANOVA model and so  we can estimate each component accurately and stably by use of a gradient descent algorithm.
Additionally, unlike the conventional tensor product basis approach \cite{wood2006low, rugamer2024scalable}, our model does not lead to an exponential increase in the number of learnable parameters with respect to $|S|$, making it applicable to high-dimensional datasets.
Finally, TPNN is robust to input outliers and easy to accommodate the monotone constraint on the main effects. 

In this section, we first explain the sum-to-zero condition for the identifiability of each component in the functional ANOVA model and  then propose TPNNs that always satisfy the sum-to-zero condition, and develop a corresponding learning algorithm for the functional ANOVA model. Moreover, we show theoretically that TPNNs are flexible enough so that they can approximate any Lipschitz continuous function well. A modification of Neural Basis Models with TPNNs is also  discussed.

\subsection{Sum-to-zero condition for identifiable components} \label{sec:id_issu}
The functional ANOVA model itself is not identifiable.
That is, there are multiple functional ANOVA decompositions for a given function. 
For example, 
{\footnotesize
$$f(x_{1},x_{2})=f_{1}(x_{1}) + f_{2}(x_{2}) + f_{12}(x_{1},x_{2}),$$
} 
\vskip -0.1cm
where {\footnotesize
$f_{1}(x_{1})=x_{1},f_{2}(x_{2})=x_{2}, f_{1,2}(x_{1},x_{2})=x_{1}x_{2}$ }
can be expressed as  
{\footnotesize $$f(x_{1},x_{2})=f_{1}^{*}(x_{1}) + f_{2}^{*}(x_{2}) + f^{*}_{1,2}(x_{1},x_{2}),$$} 
\vskip -0.3cm
where {\footnotesize $f_{1}^{*}(x_{1})=-x_{1}, f_{2}^{*}(x_{2})=x_{2}, f_{12}^{*}(x_{1},x_{2})=x_{1}(x_{2}+2)$ } \cite{func_puri}.
Without the identifiability condition, each component cannot be estimated uniquely and thus interpretation 
through the estimated components becomes unstable and inaccurate.
See empirical evidences of instability of estimated components by NAM \cite{nam} and NBM \cite{nbm} are given in Appendix \ref{app:Uniqueness of component functions}.


%Figure \ref{fig_id_importance} presents the plots of the estimated main effects for 5 different training data randomly sampled from \textsc{calhousing} dataset. 
%Our model (ANOVA-TPNN), which satisfies the identifiability condition, stably estimates each component, whereas other models (NAM, \cite{nam}, NBM, \cite{nbm}, ANOVA-TPNN without identifiability condition) exhibit instability. 
%In the functional ANOVA model, since interpretation is provided based on the estimated components, the identifiability of the estimated components is crucial for enhancing the reliability of the interpretation.

A simple remedy to ensure the identifiability of each component is to put a constraint. 
One of the most popular constraints for identifiability of each component in the functional ANOVA model is so called the {\it sum-to-zero} condition \cite{hooker_diag, anovaboost}.
The {\it sum-to-zero} condition requires  that each component $f_S$ should satisfy
\footnotesize
\begin{align}
\begin{split}
&\forall  j \in S, \: \: \forall \textbf{z} \in \mathcal{X}_{S \backslash \{j\}}, \: \: \int_{\mathcal{X}_{j}}f_{S}(\textbf{x}_{S \backslash \{j\}} = \textbf{z}, x_j) \mu_{j}(dx_j)=0  \label{identfiable cond}
\end{split}
\end{align}
\normalsize
for some probability measure $\mu_j$ on $\mathcal{X}_j.$
For $\mu_j,$ the empirical distribution of the input feature $x_j$ or the uniform distribution on $\mathcal{X}_j$ can be used in practice.
With the sum-to-zero in (\ref{identfiable cond}), the functional ANOVA model becomes identifiable, as can be seen in proposition 3.1. 
Let $\mu=\prod_j \mu_j.$


\begin{proposition} \label{thm:id_unique} \cite{hooker_diag}
Consider two component sets $\{f_{S}^{1} , S \subseteq [p], |S|\le d \}$ and $\{f_{S}^{2} , S \subseteq [p], |S|\le d\}$ which satisfy (\ref{identfiable cond}). Then, $\sum_{S: |S|\le d }f_{S}^{1}(\cdot) 
\equiv \sum_{S: |S|\le d }f_{S}^{2}(\cdot)$ almost everywhere (with respect to $\mu$) if and only if $f_{S}^{1}(\cdot) 
\equiv f_{S}^{2}(\cdot)$ almost everywhere (with respect to $\mu$) for every $S\subseteq [p]$ with $|S|\le d.$  
\end{proposition}

\cite{herren2022statistical} demonstrated that
there is an interesting relation between the sum-to-zero condition and SHAP \cite{lundberg2017unified} which is a well known interpretable AI method.
That is, the SHAP value of a given input can be calculated easily from the prediction values of each component 
of the functional ANOVA model.
For a given function $f$ and input vector $\textbf{x}$, the SHAP value of the $j$th input variable is defined as
{\footnotesize
\begin{align*}
\phi_{j}(f,\textbf{x}) = \sum_{S \subseteq [p]\backslash \{j\}}{|S|!(p-|S|-1)!\over p!}(v_{f}(\textbf{x}_{S\cup \{j\}}) - v_{f}(\textbf{x}_{S})),
\end{align*}
}
\vskip -0.1cm
where $v_{f}(\textbf{x}_{S}) = \mathbb{E}[f(\textbf{X})|\textbf{X}_{S}=\textbf{x}_S],$ where $\textbf{X}\sim \mu.$ 

\begin{proposition} \cite{herren2022statistical}
For a given GA$^{d}$M $f$ satisfying the sum-to-zero condition, we have
{\footnotesize
\begin{equation}
\label{eq:ANOVA-SHAP}
\phi_j(f,\textbf{x})= \sum_{S\subseteq [p], |S|\le d, j\in S} f_S(\textbf{x}_S)/|S|.
\end{equation}
}
\end{proposition}
\vskip -0.2cm

Equation (\ref{eq:ANOVA-SHAP}) provides an interesting implication - the functional ANOVA model satisfying the sum-to-zero condition also decomposes the SHAP value.
That is, the contribution of the interaction between $x_j$ and $\textbf{x}_S$
to the SHAP value $\phi_j(f,\textbf{x})$ is $f(\textbf{x}_{S'})/|S'|,$ where $S'=S \cup \{j\}.$ 
This result could provide a new way of calculating the SHAP value approximately. Specifically, 
we first approximate a given function by a GA$^{d}$M satisfying sum-to-zero condition and calculate the SHAP value
of the approximated GA$^{d}$M by use of (\ref{eq:ANOVA-SHAP}). 
We refer this approximated SHAP value as {\it ANOVA-SHAP}. 
The results of numerical studies in Appendix \ref{app:ANOVA-SHAP-result} 
confirm that ANOVA-SHAP is similar to Kernel-SHAP calculated from the shap python package \cite{lundberg2017unified} 
One obvious advantage of ANOVA-SHAP is that it is significantly faster to compute compared to 
existing methods of calculating SHAP values such as Deep-SHAP and Kernel-SHAP proposed by \cite{lundberg2017unified}, as well as Tree-SHAP proposed by \cite{lundberg1802consistent}.
%Experiment results regadring ANOVA-SHAP are given in Appendix \ref{sec:ANOVA-SHAP}.


\begin{table*}[t] 
\centering
\tiny
\caption{\footnotesize \textbf{Stability scores on real datasets.} 
Lower stability score means more stable interpretation.
The textbf faces highlight the best results among GAM and G$\text{A}^{2}$M.}
\label{table:stable_result}
\begin{tabular}{c|ccc|ccc}
\hline
 & \multicolumn{3}{c|}{GA$^{1}$M} & \multicolumn{3}{c}{G$\text{A}^{2}$M} \\ \hline \hline
 \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dataset \end{tabular}} & \begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{1}$PNN\end{tabular} & NA$^{1}$M & NB$^{1}$M & \begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN\end{tabular} & N$\text{A}^{2}$M & N$\text{B}^{2}$M \\ \hline \hline
\textsc{Calhousing} \cite{calhousing} & $\textbf{0.012}$  & 0.045  & 0.039    & $\textbf{0.035}$  & 0.071  & 0.075    \\
\textsc{Wine}\cite{wine_quality}& $\textbf{0.011}$ & 0.058  & 0.043    & $\textbf{0.049}$  & 0.087  & 0.065    \\
\textsc{Online} \cite{online} &  $\textbf{0.031}$ & 0.076  & 0.054  & $\textbf{0.052}$ & 0.072  & 0.072  \\ 
\textsc{Abalone} \cite{abalone} &  $\textbf{0.008}$ & 0.013  & 0.026  &  $\textbf{0.028}$ & 0.047  & 0.038   \\ \hline \hline
\textsc{FICO} \cite{fico} & $\textbf{0.035}$  & 0.046 & 0.046   & $\textbf{0.048}$  & 0.089  & 0.075    \\ 
\textsc{Churn}\cite{churn} &  $\textbf{0.017}$  & 0.027 & 0.047 &  $\textbf{0.047}$  & 0.089 & 0.080     \\ 
\textsc{Credit} \cite{credit} &  $\textbf{0.021}$ & 0.069  & 0.025 & $\textbf{0.036}$  & 0.089  & 0.053   \\
\textsc{Letter} \cite{Letter} & 0.017  & 0.022  & $\textbf{0.014}$  &  $\textbf{0.026}$ & 0.075 & 0.081  \\
\textsc{Drybean}\cite{drybean} & $\textbf{0.028}$ & 0.074 & 0.070 & $\textbf{0.053}$ & 0.088 & 0.081 \\
\hline 
\end{tabular}%
\vskip -0.4cm
\end{table*}

\subsection{ANOVA-TPNN} \label{sec:ANOVA NODE}

We first propose basis neural networks for the main effects and extend these basis neural networks for higher order interactions through tensor product which we call tensor product neural networks (TPNN).
Finally, we propose ANOVA-TPNN which estimates each component by TPNN.

\paragraph{Main effects.}
We first consider the main effects.
Let $\phi_j(x|\theta)$ be a basis neural networks for the main effect $f_j$ parametrized by $\theta.$ 
That is, we approximate $f_j(x_j)$ by
{\footnotesize 
\begin{align}
f_j(x_j) \approx \sum_{k=1}^{K_j} \beta_{jk} \phi_j(x_j|\theta_{j,k})  
\label{eq:j_main}
\end{align}
}
\vskip -0.3cm
for $\beta_{jk}\in \mathbb{R}.$
To ensure the sum-to-zero condition, we choose $\phi_j$ such that
$\int_{x\in \mathcal{X}_j} \phi_j(x|\theta)\mu_j(dx)=0$ for all $\theta.$

Our choice of $\phi_j(x|\theta)$ is as follows:
{\footnotesize 
\begin{align*}
\phi_j(x|\theta)=\bigg \{ 1- \sigma\left(\frac{x-b}{\gamma}\right) \bigg \}
+ c_j(b,\gamma) \sigma\left(\frac{x-b}{\gamma}\right) 
\end{align*}
}
\vskip -0.1cm
for $b\in \mathbb{R}$ and $\gamma>0,$
where $\theta=(b,\gamma),$ $\sigma(\cdot)$ is an activation function and
$c_j(b,\gamma)= -(1- \eta_j(b,\gamma)) /\eta_j(b,\gamma)$
with $\eta_j(b,\gamma)=\int_{x\in \mathcal{X}_j} \sigma\left(\frac{x-b}{\gamma}\right) \mu_j(dx).$
We introduce the term $c_j(b,\gamma)$ to ensure $\phi_j(x|\theta)$ satisfies the sum-to-zero condition.
For $\sigma(\cdot)$ function, any differentiable function can be used but in this paper we 
use the sigmoid function for $\sigma(x)$ (i.e.
$\sigma(x)=1/(1+\exp(-x))$), since it is robust to input outliers and
provides a nice theoretical property:
an universal approximation theorem given in Section \ref{sec:uni_thm}.

The standard basis function approach can be understood as a method
of fixing the parameters $\theta_{jk}$s a priori and only learning $\beta_{jk}$s in (\ref{eq:j_main}).
In contrast, we learn $\theta_{jk}$ as well as $\beta_{jk}.$ That is, the terms $\phi_j(x|\theta_{j,k})$ can be considered as data-adaptive basis functions.
Since there is no constraint (except the nonnegative constraint on $\gamma$) on the parameters, a gradient descent algorithm can be used for learning.
The number of basis functions $K_j$ is fixed in advance as is done in the basis expansion approaches.

\paragraph{Higher order interactions.}

For $f_S,$ we consider the following tensor product neural network (TPNN)
\begin{align}
f_S(\textbf{x}_S) \approx \sum_{k=1}^{K_S} \beta_{S,k} \phi_S(\textbf{x}_{S}|\theta_{S,k})    
\label{eq:high_order_tpnm}
\end{align}
where $\phi_S(\textbf{x}_S|\theta_{S,k})=\prod_{j\in S} \phi_j(x_j|\theta_{j,k}^{S} )$.
Since $\phi_j(x_j|\theta_{j,k}^{S})$ satisfies the sum-to-zero condition, $\phi_S(\textbf{x}_S|\theta_{S,k})$ does and so does $f_S.$
As is done for the main effect, we learn $\beta_{S,k}$s and $\theta_{S,k}$s while we let $K_S$ a hyper-parameter.


% \noindent{\bf Remark.} Traditional basis expansion approaches \cite{wood2006low, rugamer2024scalable} consider
% \begin{align}
% f_S(\textbf{x}_S) \approx (\otimes_{j\in S} \textbf{B}_{j}(x_j) ) \bm{\alpha}_S,    
% \label{eq:traditional_basis}
% \end{align}
% where $\textbf{B}_j(x_j)=(B_{j1}(x_{j}),...,B_{jK_{j}}(x_{j}))^{\top}$ and $\bm{\alpha}_S$ is a $|S|$-dimensional tensor.
% This expansion shares the parameters between the components and thus the number of basis function to be estimated is smaller than the expansion in (\ref{eq:high_order_TPNN}).
% However, the computational complexity of calculating (\ref{eq:traditional_basis}) would increase exponentially with respect to $|S|$, and thus the total computation complexity would be much larger than that in the expansion (\ref{eq:high_order_tpnm}).
% This is an important advantage of the proposed TPNM compared to the traditional tensor product basis expansions.
\noindent{\bf Remark.} As the traditional tensor product basis expansion approaches \cite{wood2006low, rugamer2024scalable} do, we may consider
$$f_S(\textbf{x}_S)\approx (\otimes_{j\in S} \Phi_j(x_j)) \bm{\beta}_S,$$
where $\Phi_j(x_j)=(\phi_j(x_j|\theta_{j,k}), k\in [K_j]),$
$\bm{\beta}_S$ is a $\prod_{j \in S}K_{j}$-dimensional tensor, and 
$\theta_{j,k}$s are those used for the main effect.
This expansion shares the parameters between the components and thus the number of parameters $\theta_{jk}$s
is smaller than the expansion in (\ref{eq:high_order_tpnm}).
However, the number of parameters in $\bm{\beta}_S$
is exponentially proportional to $|S|$ and thus the number of the total learnable parameters would be much larger than that in the expansion (\ref{eq:high_order_tpnm}).
This is an important advantage of the proposed TPNNs compared to
the traditional tensor product basis expansions.
\medskip


\paragraph{ANOVA-T$^{d}$PNN.}
Finally, we propose ANOVA-T$^{d}$PNN that estimates each component of the functional ANOVA model using TPNN as follows.
$$f(\textbf{x}) = \beta_0+ \sum_{S:|S|\le d} \sum_{k=1}^{K_S} \beta_{S,k} \phi_S(\textbf{x}_S|\theta_{S,k}).$$
In ANOVA-T$^{d}$PNN, the learnable parameters are $\beta_0$ and $(\beta_{S,k},\theta_{S,k}), k \in [K_{S}], S\subseteq [p], |S|\le d.$
Unless there is any confusion, we use ANOVA-TPNN and ANOVA-T$^{d}$PNN for general $d$
interchangeably.

%{\it Not that unlike traditional basis expansion methods, in ANOVA-TPNN, the main effects and higher-order interactions do not share the parameters.}

\paragraph{Training.} 
For given training data $(\textbf{x}_1,y_1),\ldots, (\textbf{x}_n,y_n)$ and a given loss function $\ell,$
we learn the parameters by minimizing the empirical risk $\sum_{i=1}^n \ell (y_i, f(\textbf{x}_i))$
by a gradient descent algorithm. 
Overfitting can be avoided by selecting the number of epochs and learning rate carefully.
%Note that there is no constraint on the parameters except $\gamma_{S,k}>0$ and thus we can learn all parameters simultaneously using a gradient descent algorithm. 

\paragraph{Data preprocessing.}
The term $c_j(\alpha,\gamma)$ could be too large when $\eta(b,\gamma)$ is close to 0, which can happen when $\mu_j$ is the empirical distribution and there exist outliers.
To avoid this problem, we transform each input feature based on the marginal ranks to
make $\mu_j$ of the transformed data be similar to the uniform distribution. 
Since ANOVA-TPNN is nonparametric, this data transformation does not affect much to the final prediction model.


\subsection{Universal approximation thorem}
\label{sec:uni_thm}

An interesting theoretical property of ANOVA-TPNN is the universal approximation property as the standard neural network has \cite{hornik1989multilayer}.
That is, ANOVA-TPNN can approximate any arbitrary G$\text{A}^{d}$M function to a desired level of accuracy, as stated in the following theorems.


\begin{theorem}\label{thm_approx}
Suppose that $\mu_j, j\in[p]$ have lower and upper bounded densities with respect to the Lebesgue measure.
Then, for any $L$-Lipschitz continuous function\footnote{A given function $v$ defined on $\mathcal{Z}$ is $L$-Lipschitz continuous if $|v(z_1)-v(z_2)| \leq L \|z_1-z_2\|$ for all $z_1,z_2 \in \mathcal{Z}$, where $\|\cdot\|$ is a certain norm defined on $\mathcal{Z}.$} 
$g_{0,S}:\prod_{j\in S} \mathcal{X}_j \rightarrow \mathbb{R}$
satisfying the sum-to-zero condition,
there exists a TPNN with $K_S$ many basis neural networks such that
{\footnotesize
\begin{align*}
    \left \| g_{0,S}(\cdot) - \sum_{k=1}^{K_{S}}\beta_{S,k}\phi_{S}(\cdot|\theta_{S,k}) \right \|_{\infty} 
    < C_S  {|S| \over K_{S}^{1\over |S|} + 1}
\end{align*}
}
for some constant $C_S>0$ and $S \subseteq [p].$
\end{theorem}


Theorem \ref{thm_approx} shows that TPNN can approximate any Lipschitz continuous function
satisfying the sum-to-zero condition.
with an arbitrary precision by choosing $K_S$ sufficiently large, and
the required $K_S$ for a given precision should increase as $|S|$ increases.
% An obvious corollary of Theorem \ref{thm_approx} is that ANOVA-TPNN can approximate
% any componentwisely Lipschitz continous G$^d$AM model satisfying the sum-to-zero condition.
An obvious corollary of Theorem 3.3 is that ANOVA-TPNN can approximate any GA$^{d}$M model where each component is Lipschitz continuous and satisfies the sum-to-zero condition.

\begin{corollary}\label{cor_approx}
Let $g_0(\textbf{x}):= \sum_{S\subseteq [p], |S|\le d} g_{0,S} (\textbf{x}_S)$
be a given GA$^d$M function satisfying the sum-to-zero condition. 
If $\mu_j, j\in[p]$ be probability measures having bounded densities with respect to the Lebesgue measure. 
and each $g_{0,S}$ is $L$-Lipschitz continuous, then, there
exists $f_{\text{ANOVA-T$^d$PNN}}$ such that
{\footnotesize
\begin{align*}
    \left \| g_{0}(\cdot) - f_{\text{ANOVA-T$^d$PNN}}(\cdot) \right \|_{\infty} 
    < C \sum_{S \subseteq [p],|S|\leq d} {|S| \over K_{S}^{1\over |S|} + 1}
\end{align*}
}
for some constant $C>0,$ where  $K_{S}$ is the number of basis neural networks for component $S$.
\end{corollary}

%If each true component $g_{0,S}$ is a $\beta$-H\" older continuous function, from Theorem \ref{thm_approx}, we conjecture that the ratio of $K_{S}$, which ensures the minimax convergence rate of ANOVA-TPNN is optimal, is approximately $\mathcal{O}(n^{\beta |S| \over 2\beta + |S| })$.

\subsection{Extension to Neural Basis Models}
\label{app:NBM_exp}
Similarly to NBM \cite{nbm}, we extend ANOVA-TPNN to NBM-TPNN, which estimates each component as a linear combination of common basis neural networks. 

Let $\mathcal{X}_j, j\in [p]$ be all equal to $\mathcal{X}_0$ (e.g. $\mathcal{X}_0=[-1,1]$) and let
$\mu_j, j\in [p]$ be also all equal to $\mu_0$ (e.g. the uniform distribution on $[-1,1]$).
Let $\phi(x|\theta)$ be a basis neural network on $\mathcal{X}_0$ satisfying the sum-to-zero condition
with respect to $\mu_0.$
Then, NBM-TPNN approximates $f_S$ by
$$f_S(\textbf{x}_S)\approx \sum_{k=1}^{K} \beta_{S,k} \prod_{j\in S} \phi(x_j|\theta_k).$$
That is, NBM-TPNN shares basis neural networks for each component, which reduces
the number of learnable parameters much.
The experimental results for NBM-TPNN are provided in Section \ref{sec:nbm-tpnm}.
%The results of prediction performance and plots of the estimated components are in Appendix \ref{app:NBM_exp_result}.

\begin{table*}[t]
\tiny
\centering
\caption{\footnotesize \textbf{Performance of component selection.}  We report the averages (standard deviations) of AUROCs of the estimated importance scores of each component
on $f^{(1)}$, $f^{(2)}$, $f^{(3)}$ synthetic datasets.
The textbf faces highlight the best results.}
\label{table:component_select_results}
\begin{tabular}{c|ccc|ccc|ccc}
\hline
True model & \multicolumn{3}{c|}{$f^{(1)}$}  & \multicolumn{3}{c|}{$f^{(2)}$}  & \multicolumn{3}{c}{$f^{(3)}$}   \\ \hline
\hline
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Models \end{tabular}}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN\end{tabular}}   & \multicolumn{1}{c|}{NA$^{2}$M}  & NB$^{2}$M   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN \end{tabular}}   & \multicolumn{1}{c|}{NA$^{2}$M}  & NB$^{2}$M & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN\end{tabular}}   & \multicolumn{1}{c|}{NA$^{2}$M}   & NB$^{2}$M  \\ \hline
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}AUROC $\uparrow$ \end{tabular}} & 
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\textbf{1.000}$\\ (0.00)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.330\\ (0.08)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.522\\ (0.16)\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\textbf{0.943}$\\ (0.01)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.311\\ (0.08)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.481\\ (0.09)\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\textbf{0.956}$\\ (0.02)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}0.381\\ (0.13)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.477\\ (0.07)\end{tabular} \\ \hline
\end{tabular}
\end{table*}




\begin{table*}[t] 
\scriptsize
\centering
\caption{\footnotesize \textbf{Prediction performance.} We report the averages (standard deviations) of the prediction performance measure. In addition, we report the averages of ranks of prediction performance of each model on nine datasets.
The optimal (or suboptimal) results are highlighted in $\textbf{bold}$ (or underlined).}
\label{table:real_result_1}
\begin{tabular}{c|c|p{0.5cm}p{0.5cm}p{0.5cm}p{0.8cm}|p{0.6cm}p{0.5cm}p{0.5cm}p{0.8cm}|p{0.5cm}p{0.75cm}}
\hline
&  &\multicolumn{4}{c}{GA$^{1}$M} & \multicolumn{4}{|c}{G$\text{A}^{2}$M}  & \multicolumn{2}{|c}{Black box}   \\ \hline \hline
       Dataset & Measure &\begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{1}$PNN \end{tabular} & \:\:\begin{tabular}[c]{@{}c@{}}NODE\\ GA$^{1}$M\end{tabular} &\begin{tabular}{c} NA$^{1}$M \end{tabular} & \begin{tabular}{c} NB$^{1}$M \end{tabular} & \begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN \end{tabular} &  \begin{tabular}[c]{@{}c@{}} \: NODE\\ \: G$\text{A}^{2}$M\end{tabular} & \begin{tabular}{c} N$\text{A}^{2}$M \end{tabular} & \begin{tabular}{c} N$\text{B}^{2}$M \end{tabular} & \begin{tabular}{c} XGB \end{tabular} & \begin{tabular}{c} DNN \end{tabular}  \\ 
       \hline 
       \hline
\textsc{Calhousing} & RMSE $\downarrow$ &
\begin{tabular}{p{0.1cm}} 0.614 \\ (0.01)\end{tabular} &  \begin{tabular}{p{0.1cm}} 0.581 \\ (0.01)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.659 \\ (0.01)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.594 \\ (0.08)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.512 \\ (0.01)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.515 \\ (0.01)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.525 \\ (0.02)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\underline{\text{0.502}}$ \\ (0.03)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\textbf{0.452}$ \\ (0.01)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.518 \\ (0.01)\end{tabular}
\\

\textsc{Wine} & RMSE $\downarrow$ &
\begin{tabular}{p{0.1cm}} 0.725 \\(0.02)\end{tabular}  & \begin{tabular}{p{0.1cm}} 0.723 \\(0.02)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.733 \\(0.02)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.724 \\(0.02)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.704 \\(0.02)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.730 \\(0.02)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.720 \\(0.02)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.702 \\(0.03)\end{tabular}  &  \begin{tabular}{p{0.1cm}} $\textbf{0.635}$ \\(0.03)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\underline{\text{0.696}}$ \\ (0.01)\end{tabular}
\\

\textsc{Online} & RMSE $\downarrow$ &
\begin{tabular}{p{0.1cm}} $\textbf{1.111}$ \\(0.25)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\underline{\text{1.121}}$ \\(0.27)\end{tabular} &
\begin{tabular}{p{0.1cm}} 1.350 \\(0.57)\end{tabular} & 
\begin{tabular}{p{0.1cm}} 1.187 \\(0.25)\end{tabular} &  
                       
\begin{tabular}{p{0.1cm}} $\textbf{1.111}$ \\ (0.25) \end{tabular} &
\begin{tabular}{p{0.1cm}} 1.137 \\ (0.26) \end{tabular} &
\begin{tabular}{p{0.1cm}} 1.313 \\ (0.46) \end{tabular} & 
\begin{tabular}{p{0.1cm}} 1.179 \\ (0.21) \end{tabular} & 
\begin{tabular}{p{0.1cm}} 1.122 \\ (0.26) \end{tabular} &
\begin{tabular}{p{0.1cm}} 1.123 \\ (0.26) \end{tabular}
 \\

\textsc{Abalone} & RMSE $\downarrow$ &
\begin{tabular}{p{0.1cm}} 2.135 \\(0.09)\end{tabular} &
\begin{tabular}{p{0.1cm}} 2.141\\(0.09)\end{tabular} &
\begin{tabular}{p{0.1cm}} 2.171 \\(0.08)\end{tabular} & 
\begin{tabular}{p{0.1cm}} 2.167 \\(0.09)\end{tabular} &  
                       
\begin{tabular}{p{0.1cm}} $\underline{2.087}$ \\ (0.08) \end{tabular} &
\begin{tabular}{p{0.1cm}} 2.100 \\ (0.10) \end{tabular} &
\begin{tabular}{p{0.1cm}} 2.088 \\ (0.08) \end{tabular} & 
\begin{tabular}{p{0.1cm}} 2.088 \\ (0.08) \end{tabular} &
\begin{tabular}{p{0.1cm}} 2.164 \\ (0.09) \end{tabular} &
\begin{tabular}{p{0.1cm}} 
$\textbf{2.071}$ \\ (0.10) \end{tabular}
\\
\hline 
\hline
\textsc{FICO} &  AUROC $\uparrow$ &
\begin{tabular}{p{0.1cm}} $\underline{0.799}$ \\ (0.007)\end{tabular} &  
\begin{tabular}{p{0.1cm}} 0.795 \\ (0.009)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.788 \\ (0.006)\end{tabular} &
\begin{tabular}{p{0.7cm}} 0.797 \\ (0.006)\end{tabular} &

\begin{tabular}{p{0.1cm}} $\textbf{0.800}$ \\ (0.007)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.793 \\ (0.007)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\underline{0.799}$ \\ (0.007)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\underline{0.799}$ \\ (0.008)\end{tabular} & \begin{tabular}{p{0.1cm}} 0.796 \\ (0.008)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.793 \\ (0.008) \end{tabular}
 \\
\textsc{Churn} &  AUROC $\uparrow$ &
\begin{tabular}{p{0.1cm}} 0.839 \\ (0.012)\end{tabular} & 
\begin{tabular}{p{0.1cm}} 0.824 \\ (0.012)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\textbf{0.846}$ \\ (0.011)\end{tabular} &
\begin{tabular}{p{0.7cm}} $\underline{0.845}$ \\ (0.012)\end{tabular} &

\begin{tabular}{p{0.1cm}} 0.842 \\ (0.012)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.830 \\ (0.011)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.844 \\ (0.011)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.844 \\ (0.011)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\textbf{0.846}$ \\ (0.012)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.842 \\ (0.013) \end{tabular}
 \\
 
\textsc{Credit} & AUROC $\uparrow$ &
\begin{tabular}{p{0.1cm}} 0.983 \\(0.005)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.983 \\(0.005)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.976 \\(0.012)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.972 \\(0.011)\end{tabular}  &

\begin{tabular}{p{0.1cm}} $\underline{0.984}$ \\(0.006)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\textbf{0.985}$ \\(0.006)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.980 \\(0.007)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\textbf{0.985}$ \\(0.004)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.983 \\(0.004)\end{tabular} &
\begin{tabular}{p{0.1cm}} 0.980 \\ (0.006) \end{tabular}
 \\
\textsc{Letter} & AUROC $\uparrow$ &
\begin{tabular}{p{0.1cm}} 0.900 \\(0.003)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.910 \\(0.002)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.904 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.910 \\(0.001)\end{tabular}  &

\begin{tabular}{p{0.1cm}} 0.984 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.988 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.986 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.990 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\textbf{0.997}$ \\(0.001)\end{tabular} &
\begin{tabular}{p{0.1cm}} $\underline{\text{0.996}}$ \\ (0.001) \end{tabular}
 \\ 
\textsc{Drybean} & AUROC $\uparrow$ &
\begin{tabular}{p{0.1cm}} 0.995 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.996 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.996 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.994 \\(0.001)\end{tabular}  &

\begin{tabular}{p{0.1cm}} $\textbf{0.998}$ \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.996 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.995 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.995 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\underline{0.997}$ \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\underline{\text{0.997}}$ \\ (0.001) \end{tabular}
 \\  \hline \hline
 & Rank avg $\downarrow$ &
\begin{tabular}{p{0.8cm}} \:6.22 \end{tabular} &
\begin{tabular}{p{0.8cm}} \:5.44 \end{tabular} &
\begin{tabular}{p{0.8cm}} \:8.11\end{tabular} &
\begin{tabular}{p{0.8cm}} \:7.44 \end{tabular} &
\begin{tabular}{p{0.8cm}} \:$\textbf{3.11}$ \end{tabular} &
\begin{tabular}{p{0.8cm}} \:5.56\end{tabular}&
\begin{tabular}{p{0.8cm}} \:5.33 \end{tabular}&
\begin{tabular}{p{0.8cm}} \:3.56 \end{tabular}&
\begin{tabular}{p{0.8cm}} \:$\textbf{3.11}$ \end{tabular}&
\begin{tabular}{p{0.8cm}} \:$\underline{\text{4.33}}$ \end{tabular}
\end{tabular}
\end{table*}


\section{Experiments}
\label{sec:experiments}

This section presents the results of numerical experiments.
More results along with details about data, algorithms and selection of hyper-parameters are provided in Appendices \ref{sec:all_details} to \ref{app:post}.
\vskip -0.3cm
\subsection{Stability in component estimation} \label{sec:robust}

Similarly to NAM \cite{nam} and NBM \cite{nbm}, ANOVA-TPNN provides interpretation through the estimated components. 
Thus, if the components are not estimated stably, the interpretations based on the estimated components would not be reliable. In this subsection, we investigate the stability of the component estimation of ANOVA-TPNN compared
with the other baseline models including NAM and NBM.
For this purpose, we generate randomly sampled training data 
and estimate the components of the functional ANOVA model. 
We repeat this procedure 10 times to obtain 10 estimates of each component, and measure how similar these 10 estimates are.
For the similarity measure, we use 
{\footnotesize 
\begin{align*}
\mathcal{SC}(f_{S})= \frac{1}{n}  \sum_{i=1}^{n}  {\sum_{j=1}^{10}  (f_S^j(\textbf{x}_i) -\bar{f}_S(\textbf{x}_i))^{2} \over \sum_{j=1}^{10}  (f_{S}^{j}(\textbf{x}_{i}) )^2}
\end{align*}
}
for given pre-selected $n$ many input vectors $\textbf{x}_{i},\: i=1,\ldots,n,$
where $f_{S}^{j},\: j=1,\ldots,10$ are the $10$ estimates of $f_S$ and $\bar{f}_S$ is their average.
A smaller value of $\mathcal{SC}(f_{S})$ means a more stable estimation (and thus more reliable interpretation).

We compare the overall stability score $\mathcal{SC}(f)=\sum_{S}\mathcal{SC}(f_{S}) / |S|.$ 
For each of nine benchmark datasets, we calculate the overall stability scores of ANOVA-TPNN, NAM, and NBM, whose results are given in Table \ref{table:stable_result}.
The results again confirm that ANOVA-TPNN is superior in terms of the stability of component estimation.

Also, the plots of the functional relations of the main effects are provided in Appendix \ref{app:Uniqueness of component functions}, which amply demonstrate the instability of NAM and NBM in component estimation.

The stability of ANOVA-TPNN with respect to the choice of initial values are illustrated in Appendix \ref{sec:stability inital}.

\subsection{Performance in component selection} \label{sec:component_selection}
To investigate how well ANOVA-TPNN selects the true signal components, we conduct an experiment similar to the one in \cite{tsang2017detecting}.
We consider the $l_{1}$ norm of each estimated component (i.e, $\Vert f_{S}(\textbf{x}_{S})\Vert_{1}$) as the important score, and select the components whose important scores are large.
We generate synthetic datasets from $Y=f^{k}(\textbf{x})+\epsilon,$ where $f^{k}$ is the true prediction model defined in Appendix \ref{sec:exper_syn} for $k=1,2,3$.
Then, we apply ANOVA-T$^{2}$PNN, NA$^{2}$M and NB$^{2}$M 
to calculate the importance scores of the main effects and second order interactions and examine how well they predict whether a given component is signal.
Table \ref{table:component_select_results} compares the AUROCs of ANOVA-T$^{2}$PNN, NA$^{2}$M and NB$^{2}$M,
which clearly indicates that ANOVA-T$^{2}$PNN outperforms the baseline models in component selection.
More details regarding component selection with ANOVA-T$^{2}$PNN are given in Appendix \ref{app:comp_exp_detail}.

% \begin{table*}[t]
% \scriptsize
% \centering
% \caption{\textbf{Results of the prediction performance and stability scores of ANOVA-TPNN and Spline-GAM on the \textsc{Calhousing} dataset and \textsc{Abalone} dataset.} The optimal results are highlighted in $\textbf{textbf}$.}
% \label{table:splinegam}
% \begin{tabular}{c|c|cc|cc}
% \hline
%  &  & \multicolumn{2}{c|}{GA$^{1}$M} & \multicolumn{2}{c|}{GA$^{2}$M} \\ \hline \hline
% Dataset & Measure & \multicolumn{1}{c|}{ANOVA-T$^{1}$PNN} & Spline-GA$^{1}$M & \multicolumn{1}{c|}{ANOVA-T$^{2}$PNN} & Spline-GA$^{2}$M \\ \hline
% \multirow{2}{*}{Calhousing} & RMSE $\downarrow$ (std) & \multicolumn{1}{c|}{ $\textbf{ 0.614}$ (0.01) } &  0.636 (0.03) & \multicolumn{1}{c|}{ $\textbf{0.512}$ (0.01)} &   1.326 (2.058)  \\ \cline{2-6} 
%  & Stability score $\downarrow$ & \multicolumn{1}{c|}{\textbf{0.012}} & 0.033 & \multicolumn{1}{c|}{\textbf{0.035}} & 0.052 \\ \hline
% \end{tabular}%
%\end{table*}



\begin{table*}
\scriptsize
\centering
\vskip 0.2cm

\caption{\footnotesize \textbf{Prediction performance on high-dimensional datasets.} We report the averages (standard deviations) of the prediction performance for 10 randomly sampled training data from the high-dimensional datasets.
The textbf faces highlight the best results.}
\label{table:high_dim_result}
\begin{tabular}{c|c|ccc|ccc}
\hline
 & \multicolumn{1}{c|}{} &\multicolumn{3}{c|}{GA$^{1}$M} & \multicolumn{3}{c}{ G$\text{A}^{2}$M}  \\ \hline \hline
 \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dataset \end{tabular}} & 
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Measure \end{tabular}}&
  \:\:\begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{1}$PNN \end{tabular} & \:\:\:\:NA$^{1}$M & \:\:\: NB$^{1}$M & NID + \begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN \end{tabular} & NID + N$\text{A}^{2}$M & NID + N$\text{B}^{2}$M \\  \hline 
\textsc{Microsoft} \cite{micro} & RMSE $\downarrow$ & 
0.756 (0.001) & 0.774 (0.001) & 0.770  (0.001)  &  \textbf{0.754} (0.001)   & 0.761  (0.001)  &   0.755 (0.001)  \\
\textsc{Yahoo} \cite{yahoo} & RMSE $\downarrow$ & 
 0.787  (0.002)  & 0.797  (0.002) &  0.783  (0.002)  & \textbf{0.779}  (0.001)  &  0.793  (0.002)  & \textbf{0.779} (0.002)  \\
\textsc{Madelon} \cite{madelon_171}  &AUROC $\uparrow$ &
0.587  (0.02)  & 0.587  (0.02)  & 0.582  (0.03)  &   \textbf{0.605} (0.01)  & 0.568  (0.03)  &0.594  (0.02) 
\\ \hline 
\end{tabular}%

\vskip 0.2cm

\caption{\footnotesize \textbf{Stability scores on the high-dimensional datasets.} For each dataset, stability scores of of GA$^{1}$M (GA$^{2}$M) models are presented.
Lower stability scores imply more stable interpretation. 
The bold faces highlight the best results.}
\label{table:high_dim_result_stable}
\begin{tabular}{c|ccc|ccc}
\hline
  &\multicolumn{3}{c|}{GA$^{1}$M} & \multicolumn{3}{c}{G$\text{A}^{2}$M} \\ \hline \hline
 \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dataset \end{tabular}} & 
  \begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{1}$PNN\end{tabular} & NA$^{1}$M & NB$^{1}$M & NID + \begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN \end{tabular} & NID + N$\text{A}^{2}$M & NID + N$\text{B}^{2}$M \\ \hline 
\textsc{Microsoft} & $\textbf{0.030}$ & 0.089 & 0.118 & $\textbf{0.040}$ & 0.083  & 0.089  \\
\textsc{Yahoo} & $\textbf{0.049}$ & 0.088 & 0.126 & $\textbf{0.049}$ & 0.090 & 0.074  \\
\textsc{Madelon} & $\textbf{0.070}$ & 0.137 & 0.141 & $\textbf{0.076}$ & 0.090 & 0.086 \\
\hline 
\end{tabular}%

\end{table*}



\subsection{Prediction performance} \label{sec:perfor}
\vskip -0.2cm
We compare prediction performance of ANOVA-TPNN with baseline models. 
We randomly split the train, validation and test data into the ratio 70/10/20, where the validation data is used to select the optimal epoch 
and the test data is used to measure the prediction performance of the estimated models. 
We repeat this random split 10 times to obtain 10 performance measures for prediction.
For the  performance measure, we use the Root Mean Square Error (RMSE) for regression datasets and the Area Under the ROC curve (AUROC) for classification datasets.  

Table \ref{table:real_result_1} presents the results of prediction performance of ANOVA-TPNN, NODE-GAM, NAM, and NBM as well as two black box models including deep neural networks (DNN, \cite{rosenblatt1958perceptron}) and XGB \cite{chen2016xgboost}).
At the final line, the average ranks of each model over the nine datasets are presented, which shows that ANOVA-T$^{2}$PNN exhibits comparable or superior prediction performance compared to the baseline models.
Details about the experiments are given in Appendix \ref{app:details_real}.
\vskip -0.1cm
\vskip -0.1cm
\subsection{Application to high-dimensional data}
\label{sec:high-dimenional}
%The input dimensions of the 9 datasets in Table \ref{table:real_result_1} are no more than a few dozens.
To see whether ANOVA-TPNN is applicable to high-dimensional data, we analyze three additional datasets with input dimensions ranging from 136 to 699. 
See Table \ref{Table : Dataset} of Appendix for details of these three datasets.
For ANOVA-T$^{1}$PNN, we include all main effects into the model. For ANOVA-T$^{2}$PNN, however, the number of second order interactions is too large so that considering all the main effects and second order interactions would be difficult unless very large computing resources are available. 
A simple alternative is to screen out unnecessary second order interactions a priori and include only selected second order interactions (and all the main effects) into the model. 
In the experiment, we use Neural Interaction Detection (NID) proposed by \cite{tsang2017detecting} for the interaction screening.
The numbers of selected interactions are given in Appendix \ref{app:details_real}.

From Table \ref{table:high_dim_result} and Table \ref{table:high_dim_result_stable}, we observe that ANOVA-TPNN shows favorable prediction performance compared to NAM and NBM and estimates the components more stably on high-dimensional datasets. 
Note that the reported RMSE of NB$^2$M with all second order interactions on \textsc{microsoft} 
by \cite{nbm} is 0.750, which indicates that screening interactions using NID does not hamper prediction performance much. 

\begin{figure}[h]
\centering
    \includegraphics[width=0.8 \textwidth]{picture/celeba_monotone-.png}
    \vskip -0.2cm
    \caption{\footnotesize \textbf{Plots of the functional relations of `No Beard' and `Wearing Lipstick' 
    on \textsc{CelebA} dataset estimated by ANOVA-T$^{1}$PNN with and without the monotone constraint.}}
    \label{fig_celeba_monotone}
\end{figure}

\begin{table}[h]
\scriptsize
\centering
\caption{\footnotesize \textbf{Results of the prediction performance and stability scores of ANOVA-TPNN and Spline-GAM on the \textsc{Calhousing} dataset.} The optimal results are highlighted in $\textbf{bold}$.}
\label{table:splinegam}
\begin{tabular}{c|c|c}
\hline
 Measure & ANOVA-T$^{1}$PNN & Spline-GA$^{1}$M \\ \hline
 RMSE $\downarrow$ (std) & $\textbf{ 0.614}$ (0.01)  &  0.636 (0.03)  \\
 Stability score $\downarrow$ & \textbf{0.012} & 0.033  \\ \hline
\hline 
 Measure & ANOVA-T$^{2}$PNN & Spline-GA$^{2}$M \\ \hline
 RMSE $\downarrow$ (std) & $\textbf{0.512}$ (0.01) &   1.326 (2.058)  \\ 
  Stability score $\downarrow$ & \textbf{0.035} & 0.052  \\ \hline
\end{tabular}%
\end{table}

% \begin{table}[h]
% \tiny
% \centering
% \caption{\footnotesize \textbf{Results of the prediction performance and stability scores of ANOVA-TPNN and Spline-GAM on the \textsc{Calhousing} dataset.} The optimal results are highlighted in $\textbf{bold}$.}
% \label{table:splinegam}
% \begin{tabular}{c|c|c|c|c}
% \hline
%  Measure & ANOVA-T$^{1}$PNN & Spline-GA$^{1}$M & ANOVA-T$^{2}$PNN & Spline-GA$^{2}$M \\ \hline
%  RMSE $\downarrow$ (std) & $\textbf{ 0.614}$ (0.01)  &  0.636 (0.03) & \begin{tabular}[c]{@{}c@{}}$\textbf{0.512}$ \\ 0.01 \end{tabular} &   1.326 (2.058)   \\
%  Stability score $\downarrow$ & \textbf{0.012} & 0.033 & \textbf{0.035} & 0.052  \\ \hline
% \end{tabular}%
% \end{table}



\vskip -1.0cm
\subsection{Comparison between ANOVA-TPNN and the basis expansion approach} 

In this section, we conduct experiments to compare ANOVA-TPNN and Spline-GAM which estimates each component by using cubic B-spline basis functions.
We evaluate the prediction performance and stability of the component estimation in ANOVA-TPNN and Spline-GAM on \textsc{Calhousing} dataset.
We implement Spline-GAM using pygam python package \cite{serven2018pygam}.

Table \ref{table:splinegam} presents the results of the prediction performance and stability scores of ANOVA-TPNN and Spline-GAM on \textsc{Calhousing} dataset.
A surprising result is that ANOVA-TPNN is superior to Spline-GAM in both of prediction performance and stability
of component estimation. By investigating details of the empirical results,
we find that Spline-GAM is vulnerable to input outliers. That is,
when there is an outlier input (i.e. an input vector in the test data locating outside
the range of input vectors in the training data), the prediction   
at the outlier could be inaccurate since the B-splie basis uses the linear extrapolation outside
the domain of the training data. In contrast, the basis neural networks in TPNN
use the sigmoid activation function which is bounded outside the range of input vectors  and 
so robust to input outliers.
The details of experimental results for Spline-GAM are presented in Appendix \ref{app:splin_gam}.


\subsection{Experiment results for NBM-TPNN}
\label{sec:nbm-tpnm}

We conduct experiment to evaluate NBM-TPNN. 
Table \ref{table:nbm_tpnn} shows the prediction performance and stability score of NBM-T$^{1}$PNN on \textsc{calhousing} dataset and \textsc{abalone} dataset.
We observe that NBM-T$^{1}$PNN also exhibits
similar prediction performance and stability to ANOVA-T$^{1}$PNN.
The plots of estimated main effects are given in Appendix \ref{app:NBM_exp_result} and the results of computation time are given in Appendix \ref{app:computation time}. We can observe that relative computation times
of NBM-TPNN compared to ANOVA-TPNN decreases as the number of input features increases.

\begin{table}[h]
\centering
\scriptsize
\caption{\footnotesize \textbf{Results of prediction performance and stability scores of ANOVA-T$^{1}$PNN and NBM-T$^{1}$PNN.}}
\label{table:nbm_tpnn}
\begin{tabular}{c|c|c|c}
\hline
Dataset & Measure & ANOVA-T$^{1}$PNN & NBM-T$^{1}$PNN \\ \hline \hline
\multirow{2}{*}{\textsc{Calhousing}} & RMSE $\downarrow$ (std) & 0.614 (0.001) & 0.604 (0.001)  \\ \cline{2-4} 
 & Stability score $\downarrow$ & 0.012 & 0.009 \\ \hline
\multirow{2}{*}{\textsc{Wine}} & RMSE $\downarrow$ (std) & 0.725 (0.02) & 0.720 (0.02)  \\ \cline{2-4} 
 & Stability score $\downarrow$ & 0.011 & 0.017 \\ \hline
\end{tabular}%
\end{table}



\subsection{ANOVA-TPNN with monotone constraints} 
\label{sec:monotone}
\vskip -0.2cm
\paragraph{Monotone constraint.} 
In practice, prior knowledge that some main effects are monotone functions is available and it is needed to reflect this prior knowledge in the training phase. 
A notable example is the credit scoring model where certain input features should have monotone main effects \cite{chen2014credit, chen2022monotonic}.

An additional advantage of ANOVA-TPNN is to accommodate the monotone constraints in the model easily. 
Suppose that $f_j$ is monotonically increasing. 
Then, ANOVA-TPNN can estimate $f_j$ monotonically increasingly by 
letting the $\beta_{jk}$ in equation (\ref{eq:j_main}) be less than or equal to 0.
%See Appendix \ref{app:ANOVA-NODT_main} for details. 

\paragraph{Application to Image data.} 
Monotone constraint helps avoiding unreasonable interpretation.
To illustrate this advantage, we conduct an experiment with an image dataset. 
We use \textsc{CelebA} \cite{celeba} dataset which has 40 binary attributes for each image. 
To apply ANOVA-TPNN to \textsc{CelebA} dataset, we consider Concept Bottleneck Model (CBM, \cite{cbm&2020&koh}) similar to the one used in \cite{nbm}.
In CBM, rather than directly inputting the embedding vector derived from an image data through a CNN into a classifier, the CNN initially predicts each concept accompanied with each image.
Then, these predicted values of each concept  are subsequently used as the input of a DNN classifier.
For our experiment, we use a pretrained ResNet18, where the last layer consists of a linear transformation with a softmax activation function,
and we replace the final DNN classifier with ANOVA-TPNN. 

Among the attributes, we set `gender' as the target label and the remaining attributes are set as concepts for images. 
Since `male' is labeled as 1 and `female' as 0, a higher value of each component results in a higher chance of being classified as `male'.

Figure \ref{fig_celeba_monotone} presents the functional relations of the two main effects corresponding to the concepts `No Beard' and `Wearing Lipstick', estimated on a randomly sampled training dataset
with and without the monotone constraint. 
Note that the functional relations are quite different even though their prediction performances, which
are given in Table 
\ref{tab:pre-monotone} of Appendix \ref{app:celeba_result}, are similar.
It is a common sense that an image having the concept of `No Beard' and `Wearing Lipstick' has a higher chance of being a female and thus the functional relations are expected to be decrease.
Figure \ref{fig_celeba_monotone} illustrates that a completely opposite result to our common sense could be obtained in practice.
Implications of the opposite functional relations to interpretation of each image are discussed in Appendix \ref{app:celeba_result}.

\subsection{Additional Experiments}

In Appendix \ref{app:relu}, we explore ANOVA-TPNN with ReLU activation function.
In Appendix \ref{app:GAM-NODE}, we confirm that the component estimation of ANOVA-T$^{2}$PNN becomes highly unstable when the sum-to-zero condition is not imposed. 
Finally, in Appendix \ref{app:post}, we discuss a method for enforcing the sum-to-zero condition after training NAM or NBM.

\section{Conclusion} \label{sec:conclusion}

In this paper, we propose a novel XAI model called ANOVA-TPNN for estimating the functional ANOVA model stably by use of TPNNs. We theoretically demonstrate that ANOVA-TPNN can approximate a smooth function well.
We also empirically show that prediction performance of ANOVA-TPNN is comparable to its competitors. 
Additionally, we proposed NBM-TPNN that improves the scalability of ANOVA-TPNN using the idea from NBM \cite{nbm}.
One advantage of the basis function in NBM-TPNN is that the number of basis functions does not depend on the dimension of the input feature.

Even though it is computationally more efficient that standard basis expansion approaches,
ANOVA-TPNN with high order interactions is still computationally demanding to be applied to high-dimensional data. 
A remedy would be to develop a tool to select and estimate signal components simultaneously in the functional ANOVA model, which makes it possible to detect higher order interactions in high-dimensional data. 



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}


%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  

\newpage
\appendix
\onecolumn



\vskip 0.2in
\begin{center}
    \huge
    \textbf{Supplementary material}
\end{center}
%\vskip 0.2in
%\begin{center}
%    \Large
%    \textbf{ANOVA-NODE...}
%\end{center}
\vskip 0.2in
\begin{center}
    \Large
    \textbf{Appendix}
\end{center}

\section{Proof of Theorem \ref{thm_approx}}
\label{sec:all_proof}

\subsection{Case of $|S|=1$}
\label{app: |S|=1}
Without loss of generality, we assume that $S= \{ j \}$.
The proof is done by the following 2 steps.

\begin{itemize}
    \item Firstly, we find a function $f_{\mathcal{E},j}$ that approximates the true component $g_{0,j}$ well.

    \item Next, we decompose the approximation function $f_{\mathcal{E},j}$ into a sum of TPNNs. 
\end{itemize}

\paragraph{Step 1.} Finding a approximation function $f_{\mathcal{E},j}$

%Our goal is to show that for any $j \in [p]$, there exist $\beta_{j,k}$ and $\theta_{j,k}$
%for  $k=1,...,K_{j}$ such that 
%\begin{align*}
%    \left\|g_{0,j}(\cdot) -  \sum_{k=1}^{K_{j}}\beta_{j,k}\phi_{j}(\cdot|\theta_{j,k}) \right\|_{\infty} \leq {C_{j} \over K_{j}+1},
%\end{align*}
%where $C_{j} > 0 $ is a constant.

Let $0 < p_L < p_R < \infty$ be the lower and upper bounds of the density of $\mu_j$, respectively.
Let $\{ \Omega_{k}^{j} \}_{k=1}^{K} =\{ [\chi_{k-1}^{j},\chi_{k}^{j}) \}_{k=1}^{K}$ be an interval partition of $\mathcal{X}_j$ such that $\mu_j(\Omega_{k}^{j}) = {1\over K}$ for some positive integer $K$.
Then, we have $|\chi_k^{j} - \chi_{k-1}^{j} | \leq \frac{1}{p_L K}$ for $k \in [K]$.
For $\gamma_{j} = 1/K^3$, we define $\ell_{j,k}(\cdot)$ as 
\begin{align*}
\ell_{j,1}(x) &= 1 - \sigma\bigg({x-\chi_{j,1}^{j} \over \gamma_{j} 
 }\bigg), \\
\ell_{j,k}(x) &= \sigma\bigg({x-\chi_{k-1}^{j} \over \gamma_{j} 
 }\bigg) - \sigma\bigg({x-\chi_{k}^{j} \over \gamma_{j} }\bigg), && k \in \{2,\dots,K-1 \}\\ 
\ell_{j,K}(x) &=  \sigma\bigg({x - \chi_{K-1}^{j} \over \gamma_{j}}\bigg).
\end{align*}
Note that for every $x \in \mathcal{X}_j$, $\sum_{k=1}^{K} \ell_k^{j}(x) = 1$ and for every $k \in  [K]$, $0 \leq \ell_{j,k}(\cdot) \leq 1$ holds.
Also, $\{\ell_{j,k}\}_{k=1}^{K}$ satisfy  Lemma \ref{lemma_ec_upp} below, whose proof is provdied in Section \ref{subsec_auxproof}.
\begin{lemma} \label{lemma_ec_upp}
For any $k \in [K]$, we have
    $$\mathbb{E}_{\mathcal{X}_j} [\ell_{j,k} (X_j) \mathbb{I}(X_j \notin \Omega_k) ] \leq \frac{2 p_U }{{K}^2} $$
    and
    $$ \mathbb{E}_{\mathcal{X}_j} [\ell_{j,k} (X_j) \mathbb{I}(X_j \in \Omega_k) ] \geq \frac{C_{0}}{K}$$
    for some positive constant $C_{0}$.
\end{lemma}

Now, we consider the function defined as
\begin{align*}
f_{\mathcal{E},j}(x) = \sum_{k=1}^{K}\delta_{k}\ell_{j,k}(x),
\end{align*}
where $\delta_k$s are defined as 
\begin{align*}
\quad \quad \quad \quad \quad \delta_{k} = { \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)g_{0,j}(X_j) ] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j) ] }
\end{align*}
for $k \in [K]$.
Then, for any $x \in \mathcal{X}_j$, we have
\begin{align}
\bigg | g_{0,j}(x) - f_{\mathcal{E},j}(x) \bigg | &= \bigg | g_{0,j}(x) - \sum_{k=1}^{K}\delta_{k}\ell_{j,k}(x) \bigg| \nonumber \\
&= \bigg | \sum_{k=1}^{K} (g_{0,j}(x) - \delta_{k} ) \ell_{j,k}(x) \bigg | \nonumber \\
&\leq \sum_{k=1}^{K} | g_{0,j}(x) - \delta_{k} | \ell_{j,k}(x) \nonumber \\
&= \sum_{k=1}^{K} \bigg | g_{0,j}(x) - { \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)g_{0,j}(X_j)] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)]  } \bigg | \ell_{j,k}(x) \nonumber \\
&= \sum_{k=1}^{K} \bigg | { \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)g_{0,j}(x)] - \mathbb{E}_{\mathcal{X}_j}[g_{0,j}(X_j)\ell_{j,k}(X_j)] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)] } \bigg | \ell_{j,k}(x) \nonumber \\
&\leq L \sum_{k=1}^{K} \bigg | {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j )]  } \bigg | \ell_{j,k}(x),
\label{eq:holder_propty}
\end{align}
where $L > 0$ is the Lipschitz constant for $g_{0,j}(\cdot)$.

For a given $x \in \mathcal{X}_{j}$, let $r \in [K]$ be the index such that
$x \in [\chi_{r-1}^{j},\chi_{r}^{j}]$.
For $k \in \{r-1, r, r+1\}$, we have
\begin{align*}
     {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j )]  } 
    \leq &  {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j| \mathbb{I}(X_j \in \Omega_k^{j} )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j ) \mathbb{I}(X_j \in \Omega_k^{j} ) ]  }
    +
     {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j| \mathbb{I}(X_j \notin \Omega_k^{j} )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j ) \mathbb{I}(X_j \in \Omega_k^{j} ) ]  }  \\
    \leq &  {\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)(\frac{2}{p_L K }) \mathbb{I}(X_j \in \Omega_k^{j} )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j ) \mathbb{I}(X_j \in \Omega_k^{j} ) ]  } 
    + 
     {2 \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)  \mathbb{I}(X_j \notin \Omega_k^{j} )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j ) \mathbb{I}(X_j \in \Omega_k^{j} ) ]  }  \\
     \leq & \frac{2}{p_L K} + \frac{4 p_{U}}{C_{0}K} \\
     = & \frac{C'}{K}
\end{align*}
for $C'={2\over p_{L}} + {4p_{U}\over C_{0}}$.
Also, for $k \geq r+2$, we have $x \leq \chi_{r}^{j}$ and $\chi_{k-1}^{j} \geq \chi_{r+1}^{j}$, and hence
\begin{align*}
    |\ell_{j,k}(x)| \leq & \sigma\bigg({x-\chi_{k-1}^{j} \over \gamma_{j}}\bigg) \\
    \leq & \sigma\bigg({\chi_{r}^{j}-\chi_{r+1}^{j} \over \gamma_{j}}\bigg) \\
    \leq & \frac{1}{1+\exp(K)}.
\end{align*}

To sum up, we have
\begin{align*}
|g_{0,j}(x) - f_{\mathcal{E},j}(x)| &\leq L \sum_{k=1}^{K} \bigg | {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j )]  } \bigg | \ell_{j,k}(x) \\
&= L \bigg ( \sum_{k \in \{r-1,r,r+1\}} \bigg | {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j )]  } \bigg | \ell_{j,k}(x) + \sum_{k \notin \{r-1,r,r+1\}} \bigg | {\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j )]  } \bigg | \ell_{j,k}(x)  \bigg ) \\
&\leq L \bigg ( {3C' \over K} + {1\over 1+ \exp(K)} \bigg ).
\end{align*}

\newpage

\paragraph{Step 2.} Decomposing the approximation function $f_{\mathcal{E},j}$ into a sum of TPNNs.


Now, for $f_{\mathcal{E},j}(x) = \sum_{k=1}^{ K }\delta_{k}\ell_{j,k}(x)$, our goal is to show that there exists $\beta_{j,k}$ and $\phi_{j}(\cdot|\theta_{j,k})$,\:\: $k=1,...,K-1$ such that 
\begin{align*}
    f_{\mathcal{E},j}(x) = \sum_{k=1}^{K-1}\beta_{j,k}\phi_{j}( x |\phi_{j,k})
\end{align*}
for every $x \in \mathcal{X}_j$.
To derive this result, we use Lemma \ref{lemma_decomp} below whose proof is provided in Appendix \ref{subsec_auxproof}.
\begin{lemma} \label{lemma_decomp}
    For every $T \in \{2, \dots, K\}$, we define
\begin{align*}
\ell_{1,T}(x) &= 1 - \sigma\bigg({x-\chi_{1}^{j} \over \gamma_{j}}\bigg), \\
\ell_{t,T}(x) &= \sigma\bigg({x-\chi_{t-1}^{j} \over \gamma_{j}}\bigg) - \sigma\bigg({x-\chi_{t}^{j} \over \gamma_{j}}\bigg), && t \in \{2,\dots,T-1 \}\\ 
\ell_{T,T}(x) &=  \sigma\bigg({x - \chi_{T-1}^{j} \over \gamma_{j}}\bigg).
\end{align*}
Then, for any given $T \in \{3, \dots, K\}$ and for any $\rho_{1,T} , \dots, \rho_{T,T}$ satisfying 
\begin{align}
    \mathbb{E}_{\mathcal{X}_j}\left[ \sum_{t=1}^T \rho_{t,T} \ell_{t,T}(X_j) \right]=0, \label{decom_assum}
\end{align} 
there exist $\rho_{1,T-1},\dots,\rho_{T-1,T-1}$, $\eta$ and $\tau$ such that
\begin{align}
    &\sum_{t=1}^T \rho_{t,T} \ell_{t,T}(x) =  \sum_{t=1}^{T-1} \rho_{t,T-1} \ell_{t,T-1}(x) + \left[\eta \cdot \bigg(1-\sigma\bigg({x -\chi_{T-1}^{j} \over \gamma_{j}}\bigg)\bigg) + \tau \cdot \sigma \bigg({x-\chi_{T-1}^{j} \over \gamma_{j}} \bigg) \right], \label{decon_result_1}   \\
\end{align}
and
\begin{align} 
    &\mathbb{E}_{\mathcal{X}_j}\left[ \sum_{t=1}^{T-1} \rho_{t,T-1} \ell_{t,T-1}(X_j) \right]=0 , \quad 
    \mathbb{E}_{\mathcal{X}_j}\left[\eta \cdot \bigg(1-\sigma\bigg({X_{j} -\chi_{T-1}^{j} \over \gamma_{j}}\bigg)\bigg) + \tau \cdot \sigma \bigg({X_{j}-\chi_{T-1}^{j} \over \gamma_{j}} \bigg) \right] =0. \label{decon_result_2}
\end{align}
\end{lemma}

Since 
\begin{align*}
\mathbb{E}_{\mathcal{X}_j}[f_{\mathcal{E},j}(X_j)] &=
\mathbb{E}_{\mathcal{X}_j}\left[\sum_{k=1}^{ K }\delta_{k}\ell_{j,k}(X_j)\right] \\
&=
\sum_{k=1}^{ K }{\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)g_{0,j}(X_j)] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)]}\mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)] \\
&= \sum_{k=1}^{K} \mathbb{E}_{\mathcal{X}_j}[\ell_{j,k}(X_j)g_{0,j}(X_j)] \\
&= \mathbb{E}_{\mathcal{X}_j}\bigg[\sum_{k=1}^{ K }\ell_{j,k}(X_j)g_{0,j}(X_j)\bigg] \\
&= \mathbb{E}_{\mathcal{X}_j}[g_{0,j}(X_j)] \\
&= 0,
\end{align*}
we can decompose $f_{\mathcal{E},j}(\cdot)$ into the sum of functions using Lemma \ref{lemma_decomp} and mathematical induction, i.e.,
\begin{align*}
f_{\mathcal{E},j}(x) &= ( \rho_{1,2}\ell_{1,2}(x) + \rho_{2,2}\ell_{2,2}(x) ) + \sum_{k=2}^{K-1}\bigg(\eta_{k}\bigg(1 - \sigma\bigg( {X_{j} - \chi_{k}^{j} \over \gamma_{j}} \bigg) \bigg) + \tau_{k}\sigma\bigg( {X_{j} - \chi_{k}^{j} \over \gamma_{j}} \bigg) \bigg).
\end{align*}
Here, since
\begin{align*}
\mathbb{E}_{\mathcal{X}_{j}}[\rho_{1,2}\ell_{1,2}(X_{j}) + \rho_{2,2}\ell_{2,2}(X_{j})] &= 0,
\end{align*}
we can express $\rho_{1,2}\ell_{1,2}(x) + \rho_{2,2}\ell_{2,2}(x)$ using $\beta_{j,1}$ and $\theta_{j,1}$ such that
$$\rho_{1,2}\ell_{1,2}(x) + \rho_{2,2}\ell_{2,2}(x) = \beta_{j,1}\phi_{j}(x|\theta_{j,1}).$$
Similarly, since
\begin{align*}
\mathbb{E}_{\mathcal{X}_{j}}\bigg[\eta_{k}\bigg(1 - \sigma\bigg( {X_{j} - \chi_{k}^{j} \over \gamma_{j}} \bigg) \bigg) + \tau_{k}\sigma\bigg( {X_{j} - \chi_{k}^{j} \over \gamma_{j}} \bigg) \bigg] &= 0 
\end{align*}
we can express $\eta_{k}\bigg(1 - \sigma\bigg( {x - \chi_{k}^{j} \over \gamma_{j}} \bigg) \bigg) + \tau_{k}\sigma\bigg( {x - \chi_{k}^{j} \over \gamma_{j}}\bigg)$
using $\beta_{j,k}$ and $\theta_{j,k}$ such that $$ \eta_{k}\bigg(1 - \sigma\bigg( {x - \chi_{k}^{j} \over \gamma_{j}} \bigg) \bigg) + \tau_{k}\sigma\bigg( {x - \chi_{k}^{j} \over \gamma_{j}}\bigg) = \beta_{j,k}\phi_{j}(x|\theta_{j,k})$$ for $k=2,...,K-1$.

\paragraph{Completion of the proof.}
By summarizing the results obtained in Step 1 and Step 2 and letting $K_j=K-1$, we have
\begin{align*}
    \left\| g_{0,j}(\cdot) - \sum_{k=1}^{K_{j}}\beta_{j,k}\phi_{j}( \cdot |\theta_{j,k}) = f_{\mathcal{E}}(\cdot)\right\|_{\infty} &\leq L \left( \frac{3C'}{ K } + \frac{1}{1+\exp(K)} \right). \\
    &\leq {C_{j} \over K_{j}+1}
\end{align*}
where $C_{j}$ is a positive constant. 
\qed

\newpage

\subsection{Proof of Lemma \ref{lemma_ec_upp}} \label{subsec_auxproof}

For $x \leq \chi_{k-1}^{j} - \frac{1}{K^2}$, we have
    \begin{align*}
        |\ell_{j,k} (x)| \leq & \sigma\bigg({x-\chi_{k-1}^{j} \over \gamma_{j}}\bigg) \\
        \leq & \sigma \bigg( -\frac{1}{K^2 \gamma_{j} } \bigg) \\
        = & \frac{1}{1+\exp(K)}.
    \end{align*}
Also, for $x \geq \chi_{k}^{j} + \frac{1}{K^2}$, we have
    \begin{align*}
        |\ell_{j,k} (x)| \leq & 1 - \sigma\bigg({x-\chi_{k}^{j} \over \gamma_{j}}\bigg) \\
        \leq & 1 - \sigma \bigg( \frac{1}{K^2 \gamma_{j} } \bigg) \\
        = & \frac{1}{1+\exp(K)}.
    \end{align*}
Hence, we obtain
\begin{align*}
    \mathbb{E}_{\mathcal{X}_j} [\ell_{j,k} (X_j) \mathbb{I}(X_j \notin \Omega_k^{j}) ]
    \leq& \mathbb{P} \left(X_j \in \left[\chi_{k-1}^{j} - \frac{1}{K^2}, \chi_{k-1}^{j} \right] \bigcup \left[\chi_{k}^{j}, \chi_{k}^{j} + \frac{1}{K^2}\right] \right) + \frac{1}{1+\exp(K)} \\
    \leq & \frac{2 p_U }{K^2}.
\end{align*}
Also, for $x \in \left[ \chi_{k-1}^{j} + \frac{1}{K^2}, \chi_{k}^{j} - \frac{1}{K^2} \right]$, we have
    \begin{align*}
        \ell_{j,k} (x)  \geq&  \sigma\bigg({x-\chi_{k-1}^{j} \over \gamma_{j}}\bigg) - \sigma\bigg({x-\chi_{k}^{j} \over \gamma_{j}}\bigg) \\
        \geq&  \sigma( K ) - \sigma ( -K ) \\
        \geq & \frac{1}{2} 
    \end{align*}
for sufficiently large $K$.    
Hence, we obtain
\begin{align*}
    \mathbb{E}_{\mathcal{X}_j} [\ell_{j,k} (X_j) \mathbb{I}(X_j \in \Omega_k^{j}) ]
    \geq & \mathbb{P} \left(X_j \in \left[ \chi_{k-1}^{j} + \frac{1}{K^2}, \chi_{k}^{j} - \frac{1}{K^2} \right] \right) \cdot  \frac{1}{2} \\
    \geq & \frac{C_{0}}{K}
\end{align*}
for some positive constant $C_{0}$.
\qed

\newpage

\subsection{Proof of Lemma \ref{lemma_decomp}} 

We define
\begin{align*}
\eta  &:= -\mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)](\rho_{T}-\rho_{T-1}), \\ %= \alpha_{1}
\tau  &:= \sum_{t=1}^{T-1}\mathbb{E}_{\mathcal{X}_j}[\ell_{t,T}(X_j)](\rho_{T}-\rho_{T-1}),\\ %= \phi_{1} \\
\kappa_t &:= \rho_{t}-\eta
\end{align*}
for $t \in [T-1]$.
Then, we obtain (\ref{decon_result_1}) by
\begin{align*}
    &\sum_{t=1}^{T-1} \kappa_t \ell_{t,T-1}(x) + \left[\eta \cdot \bigg(1-\sigma\bigg( {x-\chi_{T-1}^{j} \over \gamma_{j}}\bigg)\bigg) + \tau \cdot \sigma\bigg({x-\chi_{T-1}^{j} \over \gamma_{j}} \bigg) \right] \\
    &= \sum_{t=1}^{T-2}(\rho_{t}-\eta)\ell_{t,T}(x) + (\rho_{T-1}-\eta) \cdot \sigma \bigg({x-\chi_{T-2}^{j} \over \gamma_{j} }\bigg)
    + (\tau - \eta) \cdot \sigma \bigg( {x-\chi_{T-1}^{j} \over \gamma_{j} } \bigg) + \eta\\
    &= \sum_{t=1}^{T-2}(\rho_{t}-\eta)\ell_{t,T}(x) 
    + (\rho_{T-1}-\eta) \cdot \left( \sigma \bigg({x-\chi_{T-2}^{j} \over \gamma_{j} }\bigg) - \sigma \bigg({x-\chi_{T-1}^{j} \over \gamma_{j}}\bigg) \right) \\
    & \quad + (\rho_{T-1}-\eta) \cdot \sigma \bigg({x-\chi_{T-1}^{j} \over \gamma_{j}}\bigg) + (\rho_{T} - \rho_{T-1}) \cdot \sigma \bigg( {x-\chi_{T-1}^{j} \over \gamma_{j} } \bigg) + \eta \\
    &= \sum_{t=1}^{T-1}(\rho_{t}-\eta)\ell_{t,T}(x) + (\rho_{T} - \eta) \cdot \sigma \bigg( {x-\chi_{T-1}^{j} \over \gamma_{j} } \bigg) + \eta \\
    &= \sum_{t=1}^{T}(\rho_{t}-\eta)\ell_{t,T}(x) + \eta \\
    &= \sum_{t=1}^T \rho_t \ell_{t,T}(x).
\end{align*}
\qed

\newpage

\subsection{Case of $|S| =d$}
Without loss of generality, we consider $S=\{1,...,d\}$.
Similarly to the case of $S=\{ j \}$, we define an interval partition of $\mathcal{X}_{j}$ as $\{\Omega_{k}^{j}\}_{k=1}^{K} = \{[\chi_{k-1}^{j},\chi_{k}^{j}]\}_{k=1}^{K}$ such that 
$\mu_{j}(\Omega_{k}^{j}) = {1\over K}$ and $|\chi_{k-1}^{j}-\chi_{k}^{j}| \leq {1\over p_{L}K}$ for $j=1,...,d$.
Additionally, let $\bm{\Omega}_{k_{1},....,k_{d}} =  \Omega_{k_{1}}^{1} \times \cdots \times \Omega_{k_{d}}^{d}$ for $k_{j} \in [K], \: j=1,...,d$.
For $\bm{\Omega}_{k_{1},....,k_{d}}$, we define $\ell_{k_{1},...,k_{d}}(\textbf{x}_{S})$ as 
\begin{align*}
\ell_{k_{1},...,k_{d}}(\textbf{x}_{S}) = \prod_{j=1}^{d}\ell_{j,k_{j}}(x_{j}),
\end{align*}
where $\ell_{j,k_{j}}(\cdot)$ is defined in the same way as in Step 1 for $|S|=1$. 

Consider the approximation function $f_{\mathcal{E},S}(\textbf{x}_{S})$ defined as 
\begin{align}
\begin{split}
f_{\mathcal{E},S}(\textbf{x}_{S}) &= \sum_{k_{1},...,k_{d}=1}^{K}\delta_{k_{1},...,k_{d}}\ell_{k_{1},...,k_{d}}(\textbf{x}_{S}),
\end{split}
\label{eq:mult_f}
\end{align}
where
\begin{align*}
\delta_{k_{1},...,k_{d}} = {\mathbb{E}_{\mathcal{X}_{S}}[\ell_{k_{1},..,k_{d}}(\textbf{X}_{S})g_{0,S}(\textbf{X}_{S})] \over \mathbb{E}_{\mathcal{X}_{S}}[\ell_{k_{1},...,k_{d}}(\textbf{X}_{S})] }.
\end{align*}

Using a similar approach as is done in Step 1 for $|S|=1$, we have
\begin{align*}
\bigg | g_{0,S}(\textbf{x}_{S}) - f_{\mathcal{E},S}(\textbf{x}_{S}) \bigg | \leq {C_{S}d \over K}
\end{align*}
for some positive constant $C_{S}$.

In turn, we decompose $f_{\mathcal{E},S}$ by Lemma \ref{lemma:multi_decompose}, i.e., there exist $\beta_{S,k}$ and $\theta_{S,k}$ for $k=1,...,(K-1)^{d}$ such that
\begin{align*}
f_{\mathcal{E},S}(\textbf{x}_{S}) &= \sum_{k_{1},...,k_{d}=1}^{K}\delta_{k_{1},...,k_{d}}\ell_{k_{1},...,k_{d}}(\textbf{x}_{S}) \\
&= \sum_{k=1}^{(K-1)^{d}}\beta_{S,k}\phi_{S}(\textbf{x}_{S}|\theta_{S,k}).
\end{align*}
Finally, we have
\begin{align*}
\bigg \Vert g_{0,S}(\cdot) - \sum_{k=1}^{K_{S}}\beta_{S,k}\phi_{S}(\cdot|\theta_{S,k})  \bigg \Vert_{\infty} \leq {C_{S}d \over K_{S}^{1\over d}+1}
\end{align*}
where $K_{S} = (K-1)^{d}$ and $C_{S}$ is a positive constant.
\qed

\newpage

\begin{lemma}
\label{lemma:multi_decompose}
For a given $f_{\mathcal{E},S}(\textbf{x}_{S}) = \sum_{k_{1},...,k_{|S|}=1}^{K}\delta_{k_{1},...,k_{|S|}}\ell_{k_{1},...,k_{|S|}}(\textbf{x}_{S})$  defined in (\ref{eq:mult_f}), there exist $\beta_{S,k}$ and $\theta_{S,k}$ for $k=1,...,(K-1)^{|S|}$ such that
\begin{align*}
f_{\mathcal{E},S}(\textbf{x}_{S}) =   \sum_{k=1}^{(K-1)^{|S|}}\beta_{S,k}\phi_{S}(\textbf{x}_{S}|\theta_{S,k}).
\end{align*}
\end{lemma}


Proof.)

Without loss of generality, we assume that $S=\{1,...,d\}$.
Let $\textbf{part}(f)_{j}$ be an interval partition for $x_{j}$ used when the function $f$ is defined.
For example, for $f_{\mathcal{E},S}(\textbf{x}_{S})$, we have
$$\textbf{part} ( f_{\mathcal{E},S} )_{j} = \{ [\chi_{k-1}^{j},\chi_{k}^{j} ] \}_{k=1}^{K} \quad  , \quad \bigg | \textbf{part}(f_{\mathcal{E},S})_{j} \bigg | = K$$
for $j=1,...,d$.
Since, $\mathbb{E}_{\mathcal{X}_{1}}[f_{\mathcal{E},S}(\textbf{X}_{S})]=0$, as is done in 
the proof of Step 1 for  $|S|=1$, by Lemma \ref{lemma_decomp} and mathematical induction, we decompose $f_{\mathcal{E},S}(\cdot)$ into a sum of $f_{1}^{(1)}(\cdot),...,f_{K-1}^{(1)}(\cdot)$ such that
\begin{align*}
f_{\mathcal{E},S}(\textbf{x}_{S}) & = \sum_{k_{1}=1}^{K-1}f_{k_{1}}^{(1)}(\textbf{x}_{S}),
\end{align*}
where $f_{k_{1}}^{(1)}(\cdot)$s satisfy the sum-to-zero condition,
 $\textbf{part}(f_{k_{1}}^{(1)})_{1} = 2$ and $\textbf{part}(f_{k_{1}}^{(1)})_{2} =\cdots = \textbf{part}(f_{k_{1}}^{(1)})_{d} = K$ for any $k_{1} \in [K-1]$.
Similarly, for any $k_{1} \in [K-1]$, we decompose $f_{k_{1}}^{(1)}(\cdot)$ into a sum of $f_{k_{1},1}^{(1,2)}(\cdot),...,f_{k_{1},K-1}^{(1,2)}(\cdot)$ such that $$f_{k_{1}}^{(1)}(\cdot) = \sum_{k_{2}=1}^{K-1}f_{k_{1},k_{2}}^{(1,2)}(\cdot),$$
where $f_{k_{1},k_{2}}^{(1,2)}(\cdot)$s satisfy the sum-to-zero condition, 
$\textbf{part}(f_{k_{1},k_{2}}^{(1,2)})_{1}=\textbf{part}(f_{k_{1},k_{2}}^{(1,2)})_{2} = 2$ and $\textbf{part}(f_{k_{1},k_{2}}^{(1,2)})_{j} = K$ for $j=3,...,d$ and $k_{2}=1,...,K-1$. 
We repeat this decomposition to have $\{ f_{k_{1},....,k_{d}}^{1,...,d} \}_{k_{1},....,k_{d}=1}^{K}$ such that $$f_{\mathcal{E},S}(\cdot) = \sum_{k_{1}=1}^{K-1}\cdots \sum_{k_{d}=1}^{K-1} f_{k_{1},...,k_{d}}^{(1,...,d)}(\cdot),$$ where $f_{k_{1},...,k_{d}}^{(1,...,d)}(\cdot)$s satisfy the sum-to-zero condition,
$\textbf{part}(f_{k_{1},...,k_{d}}^{(1,...,d)})_{j} = 2$ for $j \in[d]$ and $k_{i} \in [K-1]$ for all $i \in [d]$.

For given $i\in [d]$ and $k_{i} \in [K-1],$ we can express $f_{k_{1},...,k_{d}}^{(1,...,d)}(\cdot)$ using $\{\eta_{k_{1},...,k_{d}}^{j},\tau_{k_{1},...,k_{d}}^{j} \}_{j=1}^{d}, \gamma_{S},$ and $\{ \chi_{k_{j}}^{j} \}_{j=1}^{d} $ such that
\begin{align}
f_{k_{1},...,k_{d}}^{(1,...,d)}(\textbf{x}_{S}) = \prod_{j=1}^{d}\bigg \{ \eta_{k_{1},...,k_{d}}^{j}\bigg(1 - \sigma\bigg( {x_{j} - \chi^{j}_{k_{j}} \over \gamma_{S}} \bigg)\bigg)  + \tau_{k_{1},...,k_{d}}^{j}\sigma\bigg( {x_{j} - \chi^{j}_{k_{j}} \over \gamma_{S}} \bigg) \bigg \}.
\label{eq:mutl_f}
\end{align}
Since $\prod_{j=1}^{d}\bigg \{ \eta_{k_{1},...,k_{d}}^{j}\bigg(1 - \sigma\bigg( {x_{j} - \chi^{j}_{k_{j}} \over \gamma_{S}} \bigg)\bigg)  + \tau_{k_{1},...,k_{d}}^{j}\sigma\bigg( {x_{j} - \chi^{j}_{k_{j}} \over \gamma_{S}} \bigg) \bigg \}$ satisfies the sum-to-zero condition, similarly to Step 2 for $|S|=1$, it is expressed as
\begin{align*}
\prod_{j=1}^{d}\bigg \{ \eta_{k_{1},...,k_{d}}^{j}\bigg(1 - \sigma\bigg( {x_{j} - \chi^{j}_{k_{j}} \over \gamma_{S}} \bigg)\bigg)  + \tau_{k_{1},...,k_{d}}^{j}\sigma\bigg( {x_{j} - \chi^{j}_{k_{j}} \over \gamma_{S}} \bigg) \bigg \} = \beta_{S,k_{1},...,k_{d}}\phi_{S}(\textbf{x}_{S}|\theta_{S,k_{1},...,k_{d}})
\end{align*}
for some $\beta_{S,k_{1},...,k_{d}}$ and $\theta_{S,k_{1},...,k_{d}},$
which implies that  $f_{\mathcal{E},S}(\cdot)$ is decomposed into the sum of $(K-1)^{d}$ TPNNs, i.e., we have
$$f_{\mathcal{E},S}(\textbf{x}_{S}) = \sum_{k=1}^{(K-1)^{d}}\beta_{S,k}\phi_{S}(\textbf{x}_{S}|\theta_{S,k}).$$
\qed

% \newpage

% \begin{itemize}
%     \item Firstly, for $S \subseteq [p]$, we find a function $f_{\mathcal{E},S}$ that approximates the true component $g_{0,S}$ well.

%     \item Next, we decompose the approximation function $f_{\mathcal{E},S}$ into TPNNs. 

%     \item Finally, using these results, we have
% \begin{align*}
% \bigg \Vert g_{0}(\cdot) - f_{ANOVA-T^{d}PNN}(\cdot) \bigg \Vert_{\infty} & < \sum_{S \subseteq [p],|S|\leq d} \bigg \Vert g_{0,S}(\cdot) - \sum_{k=1}^{K_{S}}\beta_{S,k}\phi_{S}(\cdot|\theta_{S,k}) \bigg \Vert_{\infty} \\
% &< C \sum_{S\subseteq [p], |S| \leq d} {|S| \over K_{S}^{1\over |S|} + 1}
% \end{align*}
% where $C = \max_{S} C_{S}$ is a constant.
% \end{itemize}

% For $S \subseteq [p], |S| \leq d$, the proofs of 1 step and 1 step are provided below, and the most important and challenging part is decomposing the ensemble function $f_{\mathcal{E},S}$, which approximates the true function $g_{0,S}$ well, into \{ $\beta_{S,k}\phi_{S}(\bold{x}_{S}|\theta_{S,k})$,\:\: $k=1,...,K_{S}$ \}.


% \paragraph{Case of $S=\{j\}$.}
% Our goal is to show that for any $j \in [p]$, there exists a set of \{ $\beta_{j,k}\phi_{j}(x_{j}|\theta_{j,k})$,\:\: $k=1,...,K_{j}$ \} such that 
% \begin{align*}
%     \left\|g_{0,j}(\cdot) -  \sum_{k=1}^{K_{j}}\beta_{j,k}\phi_{j}(x_{j}|\theta_{j,k}) \right\|_{\infty} < {C_{j} \over K_{j}+1}
% \end{align*}
% where $\theta_{j,k} = (b_{j,k} , \gamma_{j,k})$ and $C_{j} > 0 $ is a constant.

% We denote $\sigma(x) := 1/(1+\exp(-x))$ as the sigmoid function.
% Also, we assume that $\mu_j$ for $j \in [p]$ admits a density with respect to Lebesgue measure which is bounded above and below. 
% %In cases where the density does not exist, such as  the empirical distribution, we can construct a $K_j$-equal-sized partition of $\mathcal{X}_j$ and then handle regions with zero measure and non-zero measure separately, similar to the proof described below. 
% Let $0 < p_L < p_R < \infty$ be the lower and upper bound of the density of $\mu_j$, respectively.
% Let $\{ \Omega_{k} \}_{k=1}^{K} =\{ [\chi_{k-1},\chi_{k}) \}_{k=1}^{K}$ be a interval partition of $\mathcal{X}_j$ such that $\mu_j(\Omega_{k}) = {1\over K}$. 
% We have $|\chi_k - \chi_{k-1} | \leq \frac{1}{p_L K}$ for $k \in [K]$.  
% For $\gamma_{j,k} = 1/K^3$, we define $\ell_k(\cdot)$ as 
% \begin{align*}
% \ell_{1}(x) &= 1 - \sigma\bigg({x-\chi_{1} \over \gamma_{j,k} 
%  }\bigg), \\
% \ell_{k}(x) &= \sigma\bigg({x-\chi_{k-1} \over \gamma_{j,k} 
%  }\bigg) - \sigma\bigg({x-\chi_{k} \over \gamma_{j,k} }\bigg), && k \in \{2,\dots,K-1 \}\\ 
% \ell_{K}(x) &=  \sigma\bigg({x - \chi_{K-1} \over \gamma_{j,k}}\bigg).
% \end{align*}
% Note that for every $x \in \mathcal{X}_j$, $\sum_{k=1}^{K} \ell_k(x) = 1$ and $0 \leq \ell_k(\cdot) \leq 1$ holds for every $k \in [K]$.
% Also, $\{\ell_k\}_{k=1}^{K}$ have the following properties.
% \begin{lemma} \label{lemma_ec_upp}
% For any $k \in [K]$, we have
%     $$\mathbb{E}_{\mathcal{X}_j} [\ell_k (X_j) \mathbb{I}(X_j \notin \Omega_k) ] < \frac{3 p_U }{{K}^2} $$
%     and
%     $$ \mathbb{E}_{\mathcal{X}_j} [\ell_k (X_j) \mathbb{I}(X_j \in \Omega_k) ] > \frac{1}{3K}.$$
% \end{lemma}

% The proof of Lemma \ref{lemma_ec_upp} is provided in Section \ref{subsec_auxproof}.
% Now, we consider the ensemble function
% \begin{align*}
% f_{\mathcal{E}}(x) = \sum_{k=1}^{K}\delta_{k}\ell_{k}(x),
% \end{align*}
% where $\delta_k$ is defined as 
% \begin{align*}
% \delta_{k} = { \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)g_{0}(X_j) ] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j) ] }.
% \end{align*}

% For any $x \in \mathcal{X}_j$, we have
% \begin{align}
% \bigg | g_{0}(x) - f_{\mathcal{E}}(x) \bigg | &= \bigg | g_{0}(x) - \sum_{k=1}^{K}\delta_{k}\ell_{k}(x) \bigg| \nonumber \\
% &= \bigg | \sum_{k=1}^{K} (g_{0}(x) - \delta_{k} ) \ell_{k}(x) \bigg | \nonumber \\
% &\leq \sum_{k=1}^{K} | g_{0}(x) - \delta_{k} | \ell_{k}(x) \nonumber \\
% &= \sum_{k=1}^{K} \bigg | g_{0}(x) - { \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)g_{0}(X_j)] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)]  } \bigg | \ell_{k}(x) \nonumber \\
% &= \sum_{k=1}^{K} \bigg | { \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)g_{0}(x)] - \mathbb{E}_{\mathcal{X}_j}[g_{0}(X_j)\ell_{k}(X_j)] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)] } \bigg | \ell_{k}(x) \nonumber \\
% &\leq L \sum_{k=1}^{K} \bigg | {\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j )]  } \bigg | \ell_{k}(x).  \label{eq:holder_propty}
% \end{align}

% Let $r\in [K]$ is the index of partition such that $x \in [\chi_{r-1}, \chi_r)$. 
% For $k \in \{r-1, r, r+1\}$,  
% we have
% \begin{align*}
%      {\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)|x - X_j|] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j )]  } 
%     \leq &  {\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)|x - X_j| \mathbb{I}(X_j \in \Omega_k )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j ) \mathbb{I}(X_j \in \Omega_k ) ]  }
%     +
%      {\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)|x - X_j| \mathbb{I}(X_j \notin \Omega_k )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j ) \mathbb{I}(X_j \in \Omega_k ) ]  }  \\
%     \leq &  {\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)(\frac{2}{p_L K }) \mathbb{I}(X_j \in \Omega_k )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j ) \mathbb{I}(X_j \in \Omega_k ) ]  } 
%     + 
%      {2a \cdot \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)  \mathbb{I}(X_j \notin \Omega_k )]  \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j ) \mathbb{I}(X_j \in \Omega_k ) ]  }  \\
%      \leq & \frac{2}{p_L K} + \frac{12 a C_{max}}{K} \\
%      \leq & \frac{C'}{K},
% \end{align*}
% for some constant $C'>0$, where the third inequality holds by Lemma \ref{lemma_ec_upp}. 
% For $k \leq r-2$, we have $x \geq \chi_{r-1}$ and $\chi_k \leq \chi_{r-2}$ and hence
%     \begin{align*}
%         |\ell_k (x)| \leq & 1 - \sigma\bigg({x-\chi_{k} \over \gamma_{j,k}}\bigg) \\
%         \leq & 1 - \sigma\bigg({\chi_{r-1}-\chi_{r-2} \over \gamma_{j,k}}\bigg) \\
%         < & \frac{1}{1+\exp(K)}.
%     \end{align*}
% Also, for $k \geq r+2$, we have $x \leq \chi_{r}$ and $\chi_{k-1} \geq \chi_{r+1}$ and hence
%     \begin{align*}
%         |\ell_k (x)| \leq & \sigma\bigg({x-\chi_{k-1} \over \gamma_{j,k}}\bigg) \\
%         \leq & \sigma\bigg({\chi_{r}-\chi_{r+1} \over \gamma_{j,k}}\bigg) \\
%         < & \frac{1}{1+\exp(K)}.
%     \end{align*}
% To sum up, we get
% \begin{align*}
%     \left | g_{0}(x) - f_{\mathcal{E}}(x) \right | \leq& (\ref{eq:holder_propty}) \\
%     \leq& 
%     L \left( \frac{C'}{ K } + \frac{K-1}{1+\exp(K)} \right)\\
% \end{align*}
% where $L_{j} >0$ is a constant.

% Now, for $f_{\mathcal{E}}(x) = \sum_{k=1}^{ K }\delta_{k}\ell_{k}(x)$, our goal is to show that there exists a set \{ $\beta_{j,k}\phi_{j}(\cdot|\theta_{j,k})$,\:\: $k=1,...,K-1$ \} such that 
% \begin{align*}
%     \sum_{k=1}^{K-1}\beta_{j,k}\phi_{j}( x |\phi_{j,k}) = f_{\mathcal{E}}(x)
% \end{align*}
% for every $x \in \mathcal{X}_j$.
% To derive the result, we use the following lemma.

% \begin{lemma} \label{lemma_decomp}
%     For every $T \in \{2, \dots, K\}$, we define
% \begin{align*}
% \ell_{1,T}(x) &= 1 - \sigma\bigg({x-\chi_{1} \over \gamma_{j,k}}\bigg), \\
% \ell_{t,T}(x) &= \sigma\bigg({x-\chi_{t-1} \over \gamma_{j,k}}\bigg) - \sigma\bigg({x-\chi_{t} \over \gamma_{j,k}}\bigg), && t \in \{2,\dots,T-1 \}\\ 
% \ell_{T,T}(x) &=  \sigma\bigg({x - \chi_{T-1} \over \gamma_{j,k}}\bigg).
% \end{align*}
% Then, for any given $T \in \{3, \dots, K\}$ and for any $\rho_1 , \dots, \rho_T$ satisfying 
% \begin{align}
%     \mathbb{E}_{\mathcal{X}_j}\left[ \sum_{t=1}^T \rho_t \ell_{t,T}(X_j) \right]=0, \label{decom_assum}
% \end{align} 
% there exist $\kappa_1,\dots, \kappa_{T-1}$, $\eta$ and $\tau$ such that
% \begin{align}
%     &\sum_{t=1}^T \rho_t \ell_{t,T}(x) =  \sum_{t=1}^{T-1} \kappa_t \ell_{t,T-1}(x) + \left[\eta \cdot \bigg(1-\sigma\bigg({x -\chi_{T-1} \over \gamma_{j,k}}\bigg)\bigg) + \tau \cdot \sigma \bigg({x-\chi_{T-1} \over \gamma_{j,k}} \bigg) \right], \label{decon_result_1}   \\
% \end{align}
% where
% \begin{align} 
%     &\mathbb{E}_{\mathcal{X}_j}\left[ \sum_{t=1}^{T-1} \kappa_t \ell_{t,T-1}(X_j) \right]=0 , \quad 
%     \mathbb{E}_{\mathcal{X}_j}\left[\eta \cdot \bigg(1-\sigma\bigg({X_{j} -\chi_{T-1} \over \gamma_{j,k}}\bigg)\bigg) + \tau \cdot \sigma \bigg({X_{j}-\chi_{T-1} \over \gamma_{j,k}} \bigg) \right] =0. \label{decon_result_2}
% \end{align}
% \end{lemma}

% The proof of Lemma \ref{lemma_decomp} is provided in Section \ref{subsec_auxproof}.
% Since 
% \begin{align*}
% \mathbb{E}_{\mathcal{X}_j}[f_{\mathcal{E}}(X_j)] &=
% \mathbb{E}_{\mathcal{X}_j}\left[\sum_{k=1}^{ K }\delta_{k}\ell_{k}(X_j)\right] \\
% &=
% \sum_{k=1}^{ K }{\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)g_{0}(X_j)] \over \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)]}\mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)] \\
% &= \sum_{k=1}^{ K } \mathbb{E}_{\mathcal{X}_j}[\ell_{k}(X_j)g_{0}(X_j)] \\
% &= \mathbb{E}_{\mathcal{X}_j}\bigg[\sum_{k=1}^{ K }\ell_{k}(X_j)g_{0}(X_j)\bigg] \\
% &= \mathbb{E}_{\mathcal{X}_j}[g_{0}(X_j)] \\
% &= 0,
% \end{align*}
% we can decompose $f_{\mathcal{E}}(x)$ using Lemma \ref{lemma_decomp} by numerical induction.
% Note that $\rho_1 \ell_{1,2}(\cdot) + \rho_2 \ell_{1,2}(\cdot)$ with
% $\mathbb{E}_{\mathcal{X}_j} [\rho_1 \ell_{2,2}(X_j) + \rho_2 \ell_{1,2}(X_j) ] = 0$ is an $\beta_{j,1}\phi_{j}(\cdot | \theta_{j,1})$ and for $k \in \{2, \dots, K_{j}\}$, $\eta \bigg( 1 -  \sigma\bigg({ x- \chi_{T-1} \over \gamma_{j,k} }\bigg) \bigg ) + \tau  \sigma \bigg ( {x - \chi_{T-1} \over \gamma_{j,k}}\bigg)$ with $\mathbb{E}_{\mathcal{X}_j} \bigg[ \eta \cdot \bigg ( 1 - \sigma \bigg({X_j -\chi_{T-1} \over \gamma_{j,k}} \bigg) \bigg ) + \tau \cdot \sigma \bigg({X_j-\chi_{T-1} \over \gamma_{j,k}} \bigg)\bigg]$ is also $\beta_{j,k}\phi_{j}(x | \theta_{j,k})$.
% Hence, we can find a set \{ $\beta_{j,k}\phi_{j}(\cdot|\theta_{j,k})$,\:\: $k=1,...,K-1$ \} such that 
% \begin{align*}
%     \left\| g_{0,j} - \sum_{k=1}^{K_{j}}\beta_{j,k}\phi_{j}( \cdot |\theta_{j,k}) = f_{\mathcal{E}}(\cdot)\right\|_{\infty} &< L \left( \frac{C'}{ K } + \frac{K-1}{1+\exp(K )} \right). \\
%     &< {C_{j} \over K_{j}+1}
% \end{align*}
% where $C_{j} >0 $ is a constant and $K_{j}=K-1$.

% \qed

% \subsection{Proofs for auxiliary lemmas}



% \begin{proof}[Lemma \ref{lemma_ec_upp}]
% For $x \leq \chi_{k-1} - \frac{1}{K^2}$, we have
%     \begin{align*}
%         |\ell_k (x)| \leq & \sigma\bigg({x-\chi_{k-1} \over \gamma_{j,k}}\bigg) \\
%         \leq & \sigma \bigg( -\frac{1}{K^2 \gamma_{j,k} } \bigg) \\
%         = & \frac{1}{1+\exp(K)}.
%     \end{align*}
% Also, for $x \geq \chi_{k} + \frac{1}{K^2}$, we have
%     \begin{align*}
%         |\ell_k (x)| \leq & 1 - \sigma\bigg({x-\chi_{k} \over \gamma_{j,k}}\bigg) \\
%         \leq & 1 - \sigma \bigg( \frac{1}{K^2 \gamma_{j,k} } \bigg) \\
%         = & \frac{1}{1+\exp(K)}.
%     \end{align*}
% Hence, we obtain
% \begin{align*}
%     \mathbb{E}_{\mathcal{X}_j} [\ell_k (X_j) \mathbb{I}(X_j \notin \Omega_k) ]
%     \leq& \mathbb{P} \left(X_j \in \left(\chi_{k-1} - \frac{1}{K^2}, \chi_{k-1}\right) \cup \left(\chi_{k}, \chi_{k} + \frac{1}{K^2}\right) \right) + \frac{1}{1+\exp(K)} \\
%     <& \frac{3 p_U }{K^2}.
% \end{align*}
% Also, for $x \in \left[ \chi_{k-1} + \frac{1}{K^2}, \chi_{k} - \frac{1}{K^2} \right]$, we have
%     \begin{align*}
%         \ell_k (x)  \geq&  \sigma\bigg({x-\chi_{k-1} \over \gamma_{j,k}}\bigg) - \sigma\bigg({x-\chi_{k} \over \gamma_{j,k}}\bigg) \\
%         \geq&  \sigma( K ) - \sigma ( -K ) \\
%         > & \frac{1}{2} 
%     \end{align*}
% for sufficiently large $K$.    
% Hence, we obtain
% \begin{align*}
%     \mathbb{E}_{\mathcal{X}_j} [\ell_k (X_j) \mathbb{I}(X_j \in \Omega_k) ]
%     > & \mathbb{P} \left(X_j \in \left[ \chi_{k-1} + \frac{1}{K^2}, \chi_{k} - \frac{1}{K^2} \right] \right) \cdot  \frac{1}{2} \\
%     > & \frac{1}{3K}. 
% \end{align*}
% \end{proof}

% \begin{proof}[Lemma \ref{lemma_decomp}]
% \label{subsec_auxproof}

% We define
% \begin{align*}
% \eta  &:= -\mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)](\rho_{T}-\rho_{T-1}), \\ %= \alpha_{1}
% \tau  &:= \sum_{t=1}^{T-1}\mathbb{E}_{\mathcal{X}_j}[\ell_{t,T}(X_j)](\rho_{T}-\rho_{T-1}),\\ %= \phi_{1} \\
% \kappa_t &:= \rho_{t}-\eta, && t \in [T-1].
% \end{align*}
% Then, we obtain (\ref{decon_result_1}) by
% \begin{align*}
%     &\sum_{t=1}^{T-1} \kappa_t \ell_{t,T-1}(x) + \left[\eta \cdot \bigg(1-\sigma\bigg( {x-\chi_{T-1} \over \gamma_{j,k}}\bigg)\bigg) + \tau \cdot \sigma\bigg({x-\chi_{T-1} \over \gamma_{j,k}} \bigg) \right] \\
%     &= \sum_{t=1}^{T-2}(\rho_{t}-\eta)\ell_{t,T}(x) + (\rho_{T-1}-\eta) \cdot \sigma \bigg({x-\chi_{T-2} \over \gamma_{j,k} }\bigg)
%     + (\tau - \eta) \cdot \sigma \bigg( {x-\chi_{T-1} \over \gamma_{j,k} } \bigg) + \eta\\
%     &= \sum_{t=1}^{T-2}(\rho_{t}-\eta)\ell_{t,T}(x) 
%     + (\rho_{T-1}-\eta) \cdot \left( \sigma \bigg({x-\chi_{T-2} \over \gamma_{j,k} }\bigg) - \sigma \bigg({x-\chi_{T-1} \over \gamma_{j,k}}\bigg) \right) \\
%     & \quad + (\rho_{T-1}-\eta) \cdot \sigma \bigg({x-\chi_{T-1} \over \gamma_{j,k}}\bigg) + (\rho_{T} - \rho_{T-1}) \cdot \sigma \bigg( {x-\chi_{T-1} \over \gamma_{j,k} } \bigg) + \eta \\
%     &= \sum_{t=1}^{T-1}(\rho_{t}-\eta)\ell_{t,T}(x) + (\rho_{T} - \eta) \cdot \sigma \bigg( {x-\chi_{T-1} \over \gamma_{j,k} } \bigg) + \eta \\
%     &= \sum_{t=1}^{T}(\rho_{t}-\eta)\ell_{t,T}(x) + \eta \\
%     &= \sum_{t=1}^T \rho_t \ell_{t,T}(x),
% \end{align*}
% where the second equality holds by 
% \begin{align*}
%     \tau - \eta = \sum_{t=1}^{T}\mathbb{E}_{\mathcal{X}_j}[c_{t}(X_j)](\rho_{T}-\rho_{T-1}) = \rho_{T}-\rho_{T-1}.
% \end{align*}

% Also, since we have  
% \begin{align*}
% &\mathbb{E}_{\mathcal{X}_j}\bigg[\eta \cdot \bigg ( 1 - \sigma \bigg ( {X_{j}-\chi_{T-1} \over \gamma_{j,k}} \bigg) \bigg ) + \tau \cdot \sigma \bigg( {X_{j}-\chi_{T-1} \over \gamma_{j,k}} \bigg) \bigg] \\
% &= \eta \cdot \left(1-\mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)]\right) + \tau \cdot \mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)] \\
% &= \eta + \mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)](\tau-\eta) \\
% &= -\mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)](\rho_{T}-\rho_{T-1}) + \mathbb{E}_{\mathcal{X}_j}[\ell_{T,T}(X_j)](\rho_{T}-\rho_{T-1}) \\
% &= 0,
% \end{align*}
% we obtain (\ref{decon_result_2}) by (\ref{decom_assum}) and (\ref{decon_result_1}).
% \end{proof}

% \newpage

% \paragraph{Case of general $S \subseteq [p]$.}
% Without loss of generality, we only consider $S=\{1,...,d\}$.
% Similarly to the case of $S=\{ j \}$, we consider the interval partition of $\mathcal{X}_{j}, \{\Omega_{k}^{j}\}_{k=1}^{K} = \{[\chi_{k-1}^{j},\chi_{k}^{j}]\}_{k=1}^{K}$ such that 
% $\mu_{j}(\Omega_{k}^{j}) = {1\over K}$ and $|\chi_{k-1}^{j}-\chi_{k}^{j}| \leq {1\over p_{L}K}$ for $j=1,...,d$.
% Additionally, we consider partition $\{ \bm{\Omega}_{k} \}_{k=1}^{K^{d}} = \{ [\chi_{k_{1}-1}^{1},\chi_{k_{1}}^{1}] \times \cdots \times [\chi_{k_{d}-1}^{d},\chi_{k_{d}}^{d}] \}_{k_{1},....,k_{d}=1}^{K}$.
% For $k=1,...,K^{d}$, $\ell_{k}(x)$ which is defined in the case of $d=1$ can be easily extended to $\ell_{k}(\bold{x}_{S})$. 

% Consider approximation function $f_{\mathcal{E},S}(\bold{x}_{S})$ which is defined as 
% \begin{align}
% \begin{split}
% f_{\mathcal{E},S}(\bold{x}_{S}) &= \sum_{k=1}^{K^{d}}\delta_{k}\ell_{k}(\bold{x}_{S}) \\
% &= \sum_{k=1}^{K^{d}}{\mathbb{E}_{\mathcal{X}_{S}}[\ell_{k}(\bold{X}_{S})g_{oS}(\bold{X}_{S})] \over \mathbb{E}_{\mathcal{X}_{S}}[\ell_{k}(\bold{X}_{S})] }\ell_{k}(\bold{x}_{S}) 
% \end{split}
% \label{eq:mult_f}
% \end{align}

% Then, we can easily prove the following using the same method as the proof of case $d=1$.

% \begin{align*}
% \bigg | g_{0,S}(\bold{x}_{S}) - f_{\mathcal{E},S}(\bold{x}_{S}) \bigg | \leq {C_{S}d \over K}
% \end{align*}
% where $C_{S} >0$ is a constant.

% Furthermore, we can decompose $f_{\mathcal{E},S}$ by using Lemmga \ref{lemma:multi_decompose} as follows.
% \begin{align*}
% f_{\mathcal{E},S}(\bold{x}_{S}) &= \sum_{k=1}^{K^{d}}\delta_{k}\ell_{k}(\bold{x}_{S})\\
% &= \sum_{k=1}^{K_{S}}\beta_{S,k}\phi_{S}(\bold{x}_{S}|\theta_{S,k})
% \end{align*}
% where $K_{S} = (K-1)^{d}$.
% Finally, we have
% \begin{align*}
% \bigg \Vert g_{0,S}(\cdot) - \sum_{k=1}^{K_{S}}\beta_{S,k}\phi_{S}(\cdot|\theta_{S,k})  \bigg \Vert_{\infty} < {C_{S}d \over K_{S}^{1\over d}+1}
% \end{align*}
% where $C_{S} >0$ is a constant and $K_{S}$ is the number of TPNNs for component $S$.
% \qed


% \newpage

% \begin{lemma}
% \label{lemma:multi_decompose}
% For a given $f_{\mathcal{E},S}(\bold{x}_{S}) = \sum_{k=1}^{K^{d}}\delta_{k}\ell_{k}(\bold{x}_{S})$ which is defined in (\ref{eq:mult_f}), there exist TPNNs $\{\beta_{S,k}\phi_{S}(\bold{x}_{S}|\theta_{S,k}), k=1,...,(K-1)^{d} \}$ such that 
% \begin{align*}
% \sum_{k=1}^{K^{d}}\delta_{k}\ell_{k}(\bold{x}_{S}) = \sum_{k=1}^{(K-1)^{d}}\beta_{S,k}\phi_{S}(\bold{x}_{S}|\theta_{S,k})    
% \end{align*}
% where $|S|=d$, and $\beta_{S,k}\phi_{S}(\bold{x}_{S}|\theta_{S,k})$ satisfies the sum-to-zero-condition for $k=1,...,(K-1)^{d}$.

% Proof.)

% Without loss of generality, we assume that $S=\{1,...,d\}$.
% Let $\textbf{part}(f)_{j}$ be the interval partition set for $x_{j}$ where function $f$ is defined.
% For example, for $j=1,...,d$, we have
% $$\textbf{part} ( f_{\mathcal{E},S} )_{j} = \{ [\chi_{k-1}^{j},\chi_{k}^{j} ] \}_{k=1}^{K} \quad  , \quad \bigg | \textbf{part}(f_{\mathcal{E},S})_{j} \bigg | = K$$
% where $f_{\mathcal{E},S} = \sum_{k=1}^{K^{d}}\delta_{k}\ell_{k}(\bold{x}_{S})$.

% We apply Lemma \ref{lemma_decomp} to decompose $f_{\mathcal{E},S}(\cdot) = \sum_{k_{1}=1}^{K-1}f_{k_{1}}^{(1)}( \cdot )$, where $\textbf{part}(f_{k_{1}}^{(1)})_{1} = 2$ and $\textbf{part}(f_{k_{1}}^{(1)})_{j} = K$ for $j=2,...,d$ and $k_{1}=1,...,K-1$.
% In turn, for any $k_{1} \in [1,...,K-1]$, we can decompose $f_{k_{1}}^{(1)}(\cdot)$ into $\{f_{k_{1},k_{2}}^{(1,2)} \}_{k_{2}=1}^{K-1}$ such that $f_{k_{1}}^{(1)}(\cdot) = \sum_{k_{2}=1}^{K-1}f_{k_{1},k_{2}}^{(1,2)}(\cdot)$ where $\textbf{part}(f_{k_{1},k_{2}}^{(1,2)})_{2} = 2$ and $\textbf{part}(f_{k_{1},k_{2}}^{(1,2)})_{j} = K$ for $j=3,...,d$ and $k_{2}=1,...,K-1$. 
% We repeat this decomposition to have $\{ f_{k_{1},....,k_{d}}^{1,...,d} \}_{k_{1},....,k_{d}=1}^{K}$ such that $f_{\mathcal{E},S}(\cdot) = \sum_{k_{1}=1}^{K-1}\cdots \sum_{k_{d}}^{K-1} f_{k_{1},...,k_{d}}^{(1,...,d)}(\cdot)$ where $\textbf{part}(f_{k_{1},...,k_{d}}^{(1,...,d)})_{j} = 2$ for $j=[d]$ and $f_{k_{1},...,k_{d}}^{(1,...,d)}(\cdot)$ is a TPNN satisfies the sum-to-zero condition.
% \qed
% \end{lemma}

% %\newpage
% % \section{Details for Tensor product neural model (TPNN)} \label{app:ANOVA-NODT}
% % In this section, we describes the specific form of TPNN $\beta_{S}\phi_{S}(\bold{x}_{S}|\theta_{S})$ for $S \subseteq [p].$

% % \subsection{Tensor product neural model (TPNN) for the main effect.}
% % \label{app:ANOVA-NODT_main}
% % Without loss of generality, we assumes that $S=\{ i \}$.  
% % We define TPNN as:
% % \begin{align}
% % \begin{split}
% % TPNN(x_{i}) &= \beta_{i}\phi_{i}(x_{i}|\theta_{i}^{i}) \\
% % &= \beta_{i}\bigg ( 1 - \sigma \bigg( {x_{i}-b_{i}^{i} \over \gamma_{i}^{i} } \bigg) \bigg ) + \beta_{i}c_{i}(b_{i}^{i},\gamma_{i}^{i})\sigma \bigg({x_{i}-b_{i}^{i} \over\gamma_{i}^{i}}\bigg), \label{eq:anova_nodt}
% % \end{split}
% % \end{align}
% % where 
% % \begin{align*}
% % c_{i}(b_{i}^{i},\gamma_{i}^{i}) = -{ 1 - \mathbb{E}\bigg[\sigma\bigg({X_{i} - b_{i}^{i} \over \gamma_{i}^{i}}\bigg)\bigg] \over  \mathbb{E}\bigg[\sigma\bigg({X_{i} - b_{i}^{i} \over \gamma_{i}^{i}}\bigg)\bigg] } 
% % \end{align*}
% % and $\beta_{i}, \: b_{i}^{i}$ and $\gamma_{i}^{i}$ are learnable parameters. 
% % We can easily check that equation (\ref{eq:anova_nodt}) satisfies the sum-to-zero condition in \ref{identfiable cond}.

% % \paragraph{Monotone constraint.}
% % $\sigma(x)$ is a increasing function with respect to $x$.
% % Therefore, if $\beta_{i}$ is greater than 0, then $TPNN(x_{i})$ becomes a decreasing function with respect to $x_{i}$ since $c_{i}(b_{i}^{i},\gamma_{i}^{i})$ is always less than 0.

% % \subsection{Tensor product neural model (TPNN) for general $d$ order component.} \label{app:second_formula}

% % Without loss of generality, we consider index set $S =\{I_{1},...,I_{d}\}$.
% % We define TPNN as:
% % \begin{align*}
% % TPNN(\bold{x}_{S}) &= \beta_{S}\prod_{i=1}^{d} \phi_{I_{i}}(x_{I_{i}}|\theta_{I_{i}}^{S})
% % \end{align*}
% % where $\phi_{I_{i}}(x_{I_{i}}|\theta_{I_{i}}^{S})$ is defined in the same way as in Appendix \ref{app:ANOVA-NODT_main}.
% % Since $TPNN(\bold{x}_{S})$ is a tensor product of functions that satisfy the sum-to-zero condition, 
% % it also satisfies the sum-to-zero condition.



\newpage

\section{Details of the experiments} \label{sec:all_details}
%All models except XGB were trained via Adam optimizer.
%Likewise in \cite{node}, we set $\nu = 1.5$ for $entmax_{\nu}$ in all the experiments.
All experiments are run with RTX 3090, RTX 4090, and 24GB memory.

\subsection{Details for Synthetic datasets} \label{sec:exper_syn}

\begin{table}[H]
\fontsize{8pt}{8pt}
\selectfont
\renewcommand{\arraystretch}{2}
\centering
\caption[9pt]{\footnotesize\textbf{Test suite of synthetic functions.}}
\label{table:synthetic_func}
\begin{tabular}{l|c}
\hline
$f^{(1)}$ &  $Y = 10X_{1}+10X_{2} + 20(X_{3}-0.3)(X_{3}-0.6) + 20X_{4} + 5X_{5} + 10\sin (\pi X_{1}X_{2}) + \epsilon$ \\ \hline
$f^{(2)}$ &  $Y = \pi^{X_1 X_2} \sqrt{2X_3} - \sin^{-1}(X_4) + \log(X_3+X_5) - \frac{X_9}{X_{10}} \sqrt{\frac{X_7}{X_{8}}} - X_2 X_7 + \epsilon$ \\ \hline 
$f^{(3)}$ &  $Y= \exp|X_1-X_2| + |X_2 X_3| - X_3^{2|X_4|} + \log(X_4^2+X_5^2+X_7^2+X_8^2) + X_9 + \frac{1}{1+X_{10}^2} + \epsilon$\\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\fontsize{8pt}{8pt}
\selectfont
\renewcommand{\arraystretch}{2}
\centering
\caption[9pt]{\footnotesize\textbf{Distributions of input features corresponding to each synthetic function.}}
\label{table:distribution}
\begin{tabular}{l|c}
\hline
$f^{(1)}$ & $X_{1},X_{2},X_{3},X_{4},X_{5} \sim^{iid} U(0,1)$ \\ \hline
$f^{(2)}$ &  $X_{1},X_{2},X_{3},X_{6},X_{7},X_{9} \sim^{iid} U(0,1)$ and $X_{4},X_{5},X_{8},X_{10} \sim^{iid} U(0.6,1)$.  \\ \hline
$f^{(3)}$ &  $X_{1},X_{2},X_{3},X_{4},X_{5},X_{6},X_{7},X_{8},X_{9},X_{10} \sim^{iid} U(-1,1)$ \\
\hline
\end{tabular}
\end{table}

The synthetic function $f^{(1)}$ is a slightly modified version of Friedmans synthetic function used in \cite{chipman2010bart}.
$f^{(2)}$ and $f^{(3)}$ are taken from the interaction detection
experiments in \cite{tsang2017detecting}.
We generate 15K data samples from the distribution in the Table \ref{table:distribution} and functions in the Table \ref{table:synthetic_func}. 
Also, we divide them into train, validation and test datasets with ratio 0.7, 0.1 and 0.2, respectively. 
For all of the synthetic functions, the number of basis neural networks for each component $S \subseteq [p]$, $K_{S},$ is set to be 30. 

\subsection{Details of the experiments with real datasets.} \label{app:details_real}

\begin{table}[H]
\begin{center}
\begin{small}
\caption{\footnotesize \textbf{Descriptions of real datasets.}}
\label{Table : Dataset}
%\begin{sc}
\begin{tabular}{cccccr}
\toprule
Dataset       & Size & Number of features & Problem & Number of Class\\
\midrule
\midrule
\textsc{Calhousing}    & 21k& 8  & Regression &  -\\
\textsc{Wine}    & 4k &  11  & Regression & -\\
\textsc{Online}  & 40k& 58  & Regression & -\\
\textsc{Abalone}   & 4k & 10  & Regression & -\\
\midrule
\textsc{FICO}   & 10k & 23 & Classification & 2 \\
\textsc{Churn}   & 7k  & 39 & Classification & 2\\
\textsc{Credit}  & 284k & 30 & Classification & 2\\
\textsc{Letter}   & 20k & 16 & Classification & 2 \\
\textsc{Drybean}  & 13k & 16 & Classification & 7 \\
\midrule
\textsc{Microsoft} & 960k & 136 & Regression & - \\
\textsc{Yahoo} & 700k & 699 & Regression & -\\
\textsc{Madelon} & 2.6k  & 500 & Classification & 2\\
\midrule
\textsc{ CelebA}  & 200K &  & Classification & 2 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\end{table}

%\subsubsection{Experiment details for tabular dataset.}

\paragraph{Implementation of baseline model.}
We conduct experiments for all baseline models (NAM, NBM, NODE-GAM) using the official source codes. 
For DNN, we utilize the pytorch python package \cite{NEURIPS2019_9015} and
for XGB, we utilize the xgboost package \cite{chen2016xgboost}.

\paragraph{Data descriptions.} Table \ref{Table : Dataset} summarizes
the descriptions of 9 real datasets analyzed in the numerical studies.


\paragraph{Data preprocessing.} 
Minimax scaling is applied to NAM and NBM, while the standardization is used for NODE-GAM, DNN, and XGB. 
For ANOVA-TPNN, we transform each input feature based on the empirical quantiles to
make the marginal empirical distribution of the transformed input features be close to the uniform distribution.
Additionally, all categorical input variables are encoded using the one-hot encoding.

\paragraph{Learning rate.} For all models except XGB, we set the learning rate of Adam optimizer 5e-3 and batch size  4096. We find the optimal learning rate of XGB via grid search. 

\paragraph{Model hyperparameters.}

We set the architectures of NAM and NBM as defined in \cite{nam} and \cite{nbm}.
In other words, in NA$^{1}$M, the dimensions of the hidden layers of each component consists of [64,32,16]
for \textsc{Microsoft}, \textsc{Yahoo} and \textsc{Madelon}, and [64,64,32] for other datasets.
In N$A^{2}$M, the hidden layers consist of [64,32,16] for the \textsc{Online}, \textsc{Credit} and \textsc{Drybean} datasets, [64,16,8] for \textsc{Microsoft}, \textsc{Yahoo} and \textsc{Madelon}, and [64,64,32] for the other datasets.

For XGB, DNN, and NODE-GAM, we randomly split the train, validation and test data into the ratio 70/10/20 and evaluated its performance on the validation dataset using the model trained on the train dataset.
We repeated this process 10 times with randomly split data, resulting in 10 prediction performance values for the validation datasets. 
Then, we selected the optimal hyper-parameters by the grid search based on the average of the prediction performance values for the validation datasets.

Finally, with the optimal hyper-parameters selected earlier, we fixed the model's hyper-parameters and used the 10 train-test dataset pairs obtained from the previous data splitting to train the model on the train datasets and evaluate its performance on the test datasets.

For XGB, the range of hyper-parameters for the grid search is as below.
\begin{itemize}
    \item The number of tree : \{50,100,200,300,400,500,600,700,800,900,1000\} 
    \item max depth : \{3 , 5 , 7\}
    \item learning rate : \{0.0001, 0.005, 0.01, 0.05 , 0.1\}
\end{itemize}

The hyper-parameters for NODE-GA$^{1}$M and NODE-GA$^{2}$M is determined through grid search, using similar settings to those employed in \cite{nodegam}.
That is, the range of hyper-parameters for the grid search is as below.
\begin{itemize}
    \item The number of layer : \{2, 4, 8\} 
    \item tree depth : \{6, 8\}
    \item The number of trees in each layer : \{256, 512\}
\end{itemize}

For DNN, we report the best prediction performance among the three architectures, as in \cite{nbm}.
\begin{itemize}
    \item 2 hidden layers with 2048 and 1024 units
    \item 3 hidden layers with 1024, 512, and 512 units
    \item 5 hidden layers with 128, 128, 64, 64, and 64 units
\end{itemize}

For Spline-GAM, we implement it using pygam python package \cite{serven2018pygam}. 
We set the number of knot as 20 for each components and set $\lambda_{S}$ to be the same for all components $S$, i.e., $\lambda_{S} = \lambda$ for all $S \subseteq [p]$.
Also, similar to the approach taken in \cite{nodegam}, we find the best $\lambda$ penalty by using grid search on the range as below.
$$\text{Range of $\lambda$} = [0.001,0.01,0.1,0.3,0.6,1,10,100,1000]$$

Due to the limitation of our computational environment,
in ANOVA-TPNN, for all real datasets except \textsc{online}, \textsc{credit}, \textsc{microsoft}, \textsc{yahoo}, and \textsc{madelon},
we select the optimal hyper-parameters by the grid search on the range $K_{S} : \{ 10, 30, 50, 100 \}$.
For other datasets, we use the range $K_{S} : \{10,30,50\}$ for grid search.

\paragraph{Selected components by NID for high-dimensional datasets.}
Table \ref{table:num_component} presents the number of components used in training ANOVA-T$^{2}$PNN and baseline models.
All main effects are used, and the second order interactions are selected using NID \cite{tsang2017detecting}. 
To find the optimal number of second order interactions, we conduct grid search over [100, 300, 500].
For \textsc{Microsoft}, 300 second order interactions are used; 500 for \textsc{Yahoo}, 500; and 300 for \textsc{Madelon}.


%\subsubsection{Experiment details for high-dimensional dataset.}
%\label{app:high_details}

\begin{table}[H]
\caption{\footnotesize \textbf{The number of components used in training ANOVA-T$^{2}$PNN, NA$^{2}$M, and NB$^{2}$M.}}
\label{table:num_component}
\centering
\small
\begin{tabular}{c|c|c|c} \hline
Dataset & \textsc{Microsoft}  & \textsc{Yahoo}  &  \textsc{Madelon} \\ \hline \hline
\# of selected components & 136(Main) + 300(2nd)  & 699(Main) + 500(2nd)  & 500(Main) + 300(2nd)  \\ \hline
\end{tabular}
\end{table}


\subsection{Experiment details for image dataset.} \label{app:details_image}
For \textsc{CelebA} image dataset, we use the Concept Bottleneck Model (CBM) in \cite{cbm&2020&koh}.
The main idea of CBM\cite{cbm&2020&koh} is not to directly input the embedding vector derived from image data through a CNN into a classifier for classification. 
Instead, CNN predicts given concepts (attributes) for the image, and the predicted values for these concepts are then used as an input of the final classifier. 
\cite{cbm&2020&koh} used DNN for the final classifier which is a black box model.
In this paper, we replace DNN with ANOVA-TPNN, NAM, NBM and NODE-GAM. 
For CNN that predicts concepts, we use linear heads for each concept on the top of the pretrained ResNet18.

All models are trained via the Adam optimizer with the learning rate 1e-3 and the batch size for training  256.
For ANOVA-T$^{1}$PNN, $K_{S}$ is determined through grid search on [10,30,50].
For NA$^{1}$M and NB$^{1}$M, the neural network consists of 3 hidden layer with sizes (64,64,32) and (256,128,128), respectively.
Due to the limitations of the computational environment,
for ANOVA-T$^{2}$PNN, we set $K_{S} = 10$ for the main effects and $K_S=3$ for the second order interactions.
For NA$^{2}$M, we use the neural network consisting of 3 hidden layers with sizes (16,16,8), and for NB$^{2}$M, the neural network consists of 3 hidden layers with sizes (128,64,64).
For NODE-GA$^{1}$M and NODE-GA$^{2}$M, the number of trees and the number of layers are determined through grid search on the range as :
\begin{itemize}
    \item The number of layers : \{2, 4\}
    \item Tree depth : \{4, 6\}
    \item The number of trees in each layer : \{50, 125, 256\}
\end{itemize}

\subsection{Experiment details for component selection}
\label{app:comp_exp_detail}


An important implication of stable estimation of the components is 
the ability of selecting signal components.
That is, ANOVA-TPNN can effectively identify signal components in the true function by measuring the variations of the estimated components.  
For example, we consider the $l_{1}$ norm of each estimated component (i.e, $\Vert f_{S}(\textbf{x}_{S})\Vert_{1}$) as the important score, and select the components whose important scores are large.
This simple component selection procedure would not perform well if component estimation is unstable.

To investigate how well ANOVA-TPNN selects the true signal components, we conduct an experiment similar to the one in \cite{tsang2017detecting}.
We generate synthetic datasets from $Y=f(\textbf{x})+\epsilon,$ where $f$ is the true prediction model and $\epsilon$ is a noise generated from a Gaussian distribution with mean 0 and variance $\sigma^2_\epsilon.$
Then, we apply ANOVA-T$^{2}$PNN, NA$^{2}$M and NB$^{2}$M 
to calculate the importance scores of the main effects and second order interactions and examine how well they predict whether a given component is signal.
For the performance measure of component selection, we use AUROC obtained from the pairs of $\Vert \hat{f}_S \Vert_{1}$ and $r_S$ for all $S\subset [p]$ with $|S|\le 2,$
where $\hat{f}_S$ are the estimates of $f_S$ in $f$
and $r_S=\mathbb{I}(\|f^{(k)}_{S}\|_1>0)$ are the indicators whether $f_S$ 
are signal or not. 

For the true prediction model, we consider the three functions $f^{(k)}, k=1,2,3$
whose details are given in Appendix \ref{sec:exper_syn}.
We set the data size to 15,000 and set $\sigma_\epsilon^2$ to make the signal-to-noise ratio become 5.


\newpage

\section{Ablation studies.}

\subsection{The choice of $K_{S}$ in ANOVA-TPNN.}
\label{app:tree_num}

Table \ref{table:pred_num_tree} presents the averages (the standard deviations) of prediction performance 
of ANOVA-TPNN on 10 randomly sampled datasets from \textsc{abalone} for various values of $K_S.$
For simplicity, we set all $K_S, S\subseteq [p]$ to be equal.
We observe that $K_S$ around 50 yields the best results for ANOVA-T$^{1}$PNN and $K_S$ around 10 for 
ANOVA-T$^{2}$PNN. The results suggest that the choice of optimal $K_S$ is important for  prediction performance
and a smaller $K_S$ is required for a model with higher order interactions.
%{\bf We report that similar results are obtained for other datasets.}

\begin{table}[H]
\centering
\scriptsize
\caption{\footnotesize \textbf{Results of prediction performance for various $K_{S}$ on \textsc{abalone} dataset.}}
\label{table:pred_num_tree}
\begin{tabular}{c|c|c|c|c|c}
\hline
$K_{S}$ & 1 & 5 & 10 & 50 & 100  \\ \hline \hline
ANOVA-T$^{1}$PNN & 2.176 (0.09) & 2.163 (0.08)  & 2.160 (0.09)  & 2.135 (0.09)  & 2.159 (0.08)  \\ \hline
ANOVA-T$^{2}$PNN &  2.103 (0.08) & 2.102 (0.08) & 2.087 (0.08) & 2.105 (0.08) & 2.122 (0.08) \\ \hline
\end{tabular}
\end{table}

\subsection{Impact of the initial parameter values to stability}
\label{sec:stability inital}
We investigate how the choice of initial parameter values affects the stability of the estimated components
of ANOVA-TPNN by analyzing synthetic datasets generated from $f^{(1)}$. 
We fit ANOVA-T$^{2}$PNN on 5 independently generated datasets, and
the 5 estimated main effects  are presented in Figure \ref{fig_synthetc_robust_init_anovanode}.
We observe ANOVA-T$^{2}$PNN is insensitive to the choice of initial parameter values.

\begin{figure}[H]
\vskip -0.3cm
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_synthetic_robust_anovanode.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in ANOVA-T$^{2}$PNN on synthetic datasets generated from $f^{(1)}$.}}
    \label{fig_synthetc_robust_init_anovanode}
\vskip -0.5cm
\end{figure}

%\section{Additional Experiments} \label{app:additional_experi}

\section{Illustration of interpretability of ANOVA-TPNN.}
\label{app:interpretability_anova_node}

%\subsection{Illustration of interpretability} 

We consider the two concepts of interpretation: Local and Global which are roughly defined as:
\paragraph{Local Interpretation:} Information about how each feature of a given datum affects
the prediction. SHAP \cite{lundberg2017unified} is a notable example of local interpretation. For the functional ANOVA model,
the predictive values of each component at a given datum would be considered as local interpretation.

\paragraph{Global Interpretation:} Information about how each feature is related to the
final prediction model. The importance scores of each feature (e.g. global SHAP \cite{molnar2020interpretable})
and the functional relations between each feature and the prediction model (e.g. the dependency plot of SHAP
\cite{molnar2020interpretable} are examples of global interpretation. For the functional ANOVA model,
the importance score, which can be defined as the $l_1$ norm of the corresponding component
as is done in Section \ref{sec:component_selection}, and the functional relation  identified by
the functional form of each component are two tools for global interpretation.

\subsection{Illustration of interpretability on \textsc{calhousing} dataset.}

\begin{table}[H]
\centering
\scriptsize
\caption{\footnotesize \textbf{Feature descriptions of \textsc{Calhousing} dataset.}}
\label{table:des of cal1}
\begin{tabular}{c|c|c|c}
\hline
Feature name & Index &Description & Feature type \\ \hline
\hline
MedInc & 1&Median income in block & Numerical \\ \hline
HouseAge & 2&Median house age in block & Numerical  \\ \hline
AveRooms & 3&Average number of rooms & Numerical \\ \hline
AveBedrms & 4 &Average number of bedrooms & Numerical  \\ \hline
Population & 5 &Population in block & Numerical \\ \hline
AveOccup & 6 &Average house occupancy & Numerical \\ \hline
Latitude & 7 &Latitude of house block & Numerical   \\ \hline
Longitude & 8 &Longitude of house block & Numerical   \\
\hline
\end{tabular}
\end{table}

\paragraph{Local Interpretation on \textsc{calhousing} dataset.}
We conduct an experiment on \textsc{Calhousing} \cite{calhousing} dataset to illustrate local interpretation of ANOVA-T$^{1}$PNN. Note that ANOVA-T$^{1}$PNN is given as
\begin{align*}
\hat{f}_{\text{ANOVA-T$^{1}$PNN}}(\textbf{x}) = \sum_{j=1}^{8}\hat{f}_{j}(x_{j}).
\end{align*}
Thus, it is reasonable to treat $\hat{f}_j(x_j)$ as the contribution of $x_j$
to $\hat{f}(\textbf{x}).$ In fact, we have seen in Section \ref{sec:id_issu} that
this contribution is equal to SHAP \cite{lundberg2017unified}.
As an illustration, for a given datum
$$\textbf{x}=(-0.2378, -0.4450, 0.0036, -0.1531, 0.3814, -0.067, 0.5541, -0.1111)^\top,$$
the contributions of each feature to $\hat{f}(\textbf{x})$ are 
$$(\hat{f}_{1},...,\hat{f}_{8}) = (-4.9900, 0.3278, -0.0456, 0.4432, -0.1730, 2.7521, -11.6190, 6.5184).$$
That is, the 7th variable contributes most to the prediction value of $\hat{f}(\textbf{x}),$
which can be interpreted as `the housing price is low because the latitude is not good'.

\paragraph{Global Interpretation on \textsc{calhousing} dataset.}
%We explore global interpretations for \textsc{Calhousing} dataset with ANOVA-NODE.
Figure \ref{fig_main_anova_node} and Table \ref{table:import_score} present 
the functional relations of each input feature to the prediction model learned by ANOVA-T$^1$PNN and their importance scores. 
From these results, we can see that the location is the most important features and 
the south-west area is the most expensive.

Table \ref{table:import_score_second} describes the 10 most important components 
with descending order of the importance scores
of ANOVA-T$^2$PNN normalized by the maximum importance score. The results are bit different from those of ANOVA-T$^1$PNN.
In particular, the interaction between `latitude' and `longitude' emerges as a new important feature while
the main effects of `latitude' and `longitude' become less important.

\begin{figure}[H]
\centering
\begin{flushleft}
    \includegraphics[width = 1.0 \textwidth]{picture/fig_calhusing_anovanode.png}
    \vskip -0.1cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in ANOVA-T$^{1}$PNN on \textsc{calhousing} dataset.}}
    \label{fig_main_anova_node}
\end{flushleft}
\end{figure}

% \begin{figure}[H]
% \centering
%     \includegraphics[width = 0.8 \textwidth]{picture/fig_contour.png}
%     \vskip -0.3cm
%     \caption{\footnotesize \textbf{Contour plots of the functional relations of the interactions in ANOVA-T$^{2}$PNN on \textsc{calhousing} dataset.}}
%     \label{fig_contour}
% \end{figure}

\begin{table}[H]
\vskip -0.1cm
\scriptsize
\centering
\caption{\footnotesize \textbf{Importance scores of ANOVA-T$^{1}$PNN on \textsc{calhousing} dataset.}}
\label{table:import_score}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Feature index & 7 & 8 & 1 & 6 & 3 & 2 & 4 & 5 \\ \hline
Importance score & 1.000 & 0.906 & 0.564 & 0.284 & 0.107  & 0.093  & 0.057  & 0.049   \\ \hline
\end{tabular}
\vskip 0.1cm
\caption{\footnotesize \textbf{Importance scores of ANOVA-T$^{2}$PNN on \textsc{calhousing} dataset.}}
\label{table:import_score_second}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
\hline
Feature index & 6 & (7,8) & (1,7) & (3,8) & 7 & (1,8) & (4,8) & (2,7) & (1,5) & 8\\ \hline
Importance score & 1.000 & 0.347 & 0.324 & 0.268 & 0.258 & 0.247 & 0.212 & 0.194 & 0.193 & 0.178 \\ \hline
\end{tabular}
\end{table}

\section{Prediction performance and Interpretability  on \textsc{CelebA} dataset.}
\label{app:celeba_result}

\paragraph{Comparison with baseline models in terms of prediction performance.}
We consider two types of CBMs: one is the joint concept bottleneck model (JCBM), where the CNN and the classifier are trained simultaneously, and the other is the independent concept bottleneck model (ICBM), where the CNN is kept fixed, and only the classifier is trained.
In Table \ref{table:pred_celeba}, we compare prediction performance of various models for the final classifier
in the JCBM, which shows the prediction performances of ANOVA-T$^{1}$PNN and ANOVA-T$^{2}$PNN are comparable or superior to their competitors.
In Table \ref{table:pred_celeba_ibm}, we compare the prediction performance of ANOVA-T$^{1}$PNN, NA$^{1}$M, NB$^{1}$M, DNN, and Linear model. 
The hidden layers of DNN are consists of five layers with size of (128,128,64,64,64).
In ICBM, the prediction performance is inferior to other nonlinear models when the classifier is linear.
However, ANOVA-T$^{1}$PNN is found to have comparable prediction performance compared to other baseline models including NA$^{1}$M, NB$^{1}$M, and DNN.


\paragraph{Prediction performance with and without the monotone constraint.}
Table \ref{tab:pre-monotone} presents the prediction performances
of ANOVA-TPNN with and without the monotone constraint.
For attributes `Bald', `Big Nose', `Goatee' and `Mustache', we apply the increasing monotone constraint, while for attributes  `Arched Eyebrows', `Attractive', `Heavy Makeup', `No Beard',  `Wearing Earrings', `Wearing Lipstick',  `Wearing Necklace', `Wearing Necktie', we use the decreasing monotone constraint.
Prediction performances are similar regardless
of the monotone constraint but interpretation of the estimated model can be quite different which
is discussed in the followings.

\paragraph{Global interpretation on \textsc{CelebA} dataset.} 
In ANOVA-T$^{1}$PNN without the monotone condition, we select the two features which have incorrect global interpretations among the 10 most important ones.
Table \ref{table:import_score_celeba} gives the importance scores (normalized by of the maximum important score) and its ranks of 2 
components obtained by ANOVA-T$^{1}$PNN on a randomly sampled data from \textsc{CelebA} dataset.


\paragraph{Local interpretation on \textsc{celeba} dataset.}
%We interpret the images misclassified by ANOVA-N$^{1}$ODE with and without the monotone constraint.
In Table \ref{table:miss_monotone}, we observe that Image 2-1 of Figure \ref{fig_missing_celeba} is correctly classified
when the monotone constraint is applied,  whereas it is misclassified without the monotone constraint.
Despite Image 2-1 of Figure \ref{fig_missing_celeba} having `No Beard', `Heavy Makeup', and `Wearing Lipstick', 
the scores of these features without the monotone constraint make the probability of being  male increase.
However, ANOVA-T$^{1}$PNN with the monotone constraint can avoid these unreasonable interpretations and classifies the image correctly.

In Image 2-2 of Figure \ref{fig_missing_celeba}, we observe that ANOVA-T$^{1}$PNN with the monotone constraint assigns a negative score to `No Beard' that increases the probability of being classified as female. 
However, ANOVA-T$^{1}$PNN without the monotone condition assigns a positive score to `No Beard' that increases the probability of being classified as male, even though `No Beard' is present.

Note that we can understand why ANOVA-T$^1$PNN with the monotone constraint classifies Image 2-2 of Figure \ref{fig_missing_celeba} incorrectly because there is no bear in the image. In contrast, it is not easy to understand why
ANOVA-T$^1$PNN without the monotone constraint classifies Image 2-1 of Figure \ref{fig_missing_celeba} incorrectly.  
That is, imposing the monotone constraint is helpful to learn more reasonably interpretable models.


\paragraph{Attributes to which monotone constraints are applied.} 
For attributes `Bald', `Big Nose', `Goatee' and `Mustache', we apply the increasing monotone constraint, while for attributes  `Arched Eyebrows', 
`Attractive', `Heavy Makeup', `No Beard',  `Wearing Earrings', `Wearing Lipstick',  `Wearing Necklace', `Wearing Necktie', we used the decreasing monotone constraint.

\begin{table}[H]
\caption{\footnotesize \textbf{Accuracies (standard deviations) on \textsc{CelebA} dataset in JCBM.}}
\label{table:pred_celeba}
  \begin{adjustbox}{center,max width=\linewidth}
  \setlength{\aboverulesep}{0.2pt}
  \setlength{\belowrulesep}{0.2pt}
  \scalebox{0.7}{
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
ANOVA-T$^{1}$PNN & NODE-GA$^{1}$M & NA$^{1}$M & NB$^{1}$M & ANOVA-T$^{2}$PNN & NODE-GA$^{2}$M  & NA$^{2}$M  & NB$^{2}$M  \\ \hline \hline
0.985 (0.001)  &  0.981 (0.006)  & 0.982 (0.002) &  0.980 (0.002)  & $\textbf{0.986}$ (0.001) & 0.981 (0.006)  & $\textbf{0.986}$ (0.001)  &  0.980 (0.002)  \\ \hline
\end{tabular}
}\end{adjustbox}
\end{table}

\begin{table}[H]
\caption{\footnotesize \textbf{Accuracies (standard deviations) on \textsc{CelebA} dataset in ICBM.}}
\label{table:pred_celeba_ibm}
  \begin{adjustbox}{center,max width=\linewidth}
  \setlength{\aboverulesep}{0.2pt}
  \setlength{\belowrulesep}{0.2pt}
  \scalebox{0.7}{
\begin{tabular}{c|c|c|c|c}
\hline
ANOVA-T$^{1}$PNN  & NA$^{1}$M & NB$^{1}$M & DNN & Linear    \\ \hline \hline
0.929 (0.001)    & 0.927 (0.001) &  0.930 (0.001)  & 0.936 (0.001) & 0.876 (0.006)   \\ \hline
\end{tabular}
}\end{adjustbox}
\end{table}

\begin{table}[h]
\centering
\scriptsize
\caption{\footnotesize \textbf{Results of prediction performance of ANOVA-TPNN with and without the monotone constraint.}}
\label{tab:pre-monotone}
\begin{tabular}{c|c|c|c}
\hline
 & Measure &ANOVA-T$^{1}$PNN & ANOVA-T$^{2}$PNN  \\ \hline \hline
Without Monotone constraint & Accuracy $\uparrow$ & 0.985 (0.001) & 0.986 (0.001) \\ \hline
With Monotone constraint & Accuracy $\uparrow$ & 0.984 (0.001) & 0.985 (0.001)  \\ \hline
\end{tabular}
\vskip -0.1cm
\end{table}


\begin{table}[H]
\centering
\scriptsize
\caption{\footnotesize \textbf{Importance scores and ranks for the 3 important components.}}
\label{table:celeba}
\label{table:import_score_celeba}
\begin{tabular}{c|c|c|c|c}
\hline
Components & Monotone &  No Beard & Wearing Lipstick \\ \hline \hline
Score (Rank) & X & 0.794 (3)  & 0.465 (4) \\ \hline 
Score (Rank) & O & 0.757 (6)  & 0.738 (7) \\ \hline 
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\scriptsize
\caption{\footnotesize \textbf{Results of local interpretation with and without the monotone constraint.}}
\label{table:miss_monotone}
\begin{tabular}{c|c|c|c|c|c|c}
\hline 
Image index & Monotone &  Heavy Makeup  & No beard & Wearing Lipstick  & classified label & True label \\ \hline \hline
2-1 & X & 0.030 &  0.035 & 0.093 & male & female \\ \hline
2-1 & O  & -0.080 & -0.161 & -0.106 & female & female \\ \hline
2-2 & X & 0.036   & 0.104 & 0.095 & male & male \\ \hline
2-2 & O  & -0.081 & -0.183  & -0.106 & female & male \\ \hline
\end{tabular}
\end{table}
 
\begin{figure}[H]
    \centering
\includegraphics[width=0.5 \textwidth]{picture/fig_miss2_monotone.png}
\vskip -0.3cm
    \caption{\footnotesize \textbf{Two misclassified images.}}
    \label{fig_missing_celeba}
\end{figure}



\newpage

\section{Additional experiments for the stability of ANOVA-TPNN.}

\subsection{Stability of the estimated components with respect to variations of training data} \label{app:Uniqueness of component functions}
We investigate the stability of components estimated by ANOVA-TPNN when training datasets vary.
We analyze \textsc{Calhousing} \cite{calhousing} and \textsc{Wine} \cite{wine_quality} datasets and 
compare ANOVA-TPNN with NAM and NBM in view of stability of component estimation.
For 5 randomly sampled training and test datasets, we train ANOVA-TPNN, NAM, and NBM on the training datasets and plot the functional relations of the main effects on the test datasets.

\paragraph{Experiment for \textsc{Calhousing} dataset.} 

Figures \ref{fig_anova_node_caltrain}, \ref{fig_nam_caltrain} and \ref{fig_nbm_caltrain} present the plots of the functional relations of the main effects estimated by ANOVA-T$^{1}$PNN, NA$^{1}$M, and NB$^{1}$M for 5 randomly sampled training datasets,
and Figures \ref{fig_anova_n2ode_caltrain}, \ref{fig_n2am_caltrain} and \ref{fig_n2bm_caltrain} present the plots of the functional relations of the main effects estimated by ANOVA-T$^{2}$PNN, NA$^{2}$M, and NB$^{2}$M for 5 randomly sampled datasets. We observe that the 5 estimates of each component estimated 
by ANOVA-TPNN are  much more stable compared to those by  NAM and NBM.
Note that in Figure \ref{fig_n2am_caltrain}, we observe that some components are estimated as a constant function
in NA$^{2}$M,  which would be partly because the main effects are absorbed into the second order interactions.

\paragraph{Experiment for \textsc{Wine} dataset.} Figures \ref{fig_anova_node_winetrain}, \ref{fig_nam_winetrain} and \ref{fig_nbm_winetrain} present the plots of the functional relations of the main effects estimated by ANOVA-T$^{1}$PNN, NA$^{1}$M, and NB$^{1}$M for 5 randomly sampled datasets, and
Figures \ref{fig_anova_n2ode_winetrain}, \ref{fig_n2am_winetrain} and \ref{fig_n2bm_winetrain} 
present the plots of the functional relations of the main effects estimated by ANOVA-T$^{2}$PNN, NA$^{2}$M, and NB$^{2}$M for 5 randomly sampled datasets. The results are similar to those of \textsc{Calhousing} dataset.

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_anova_node_cal_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{ Plots of the functional relations of the main effects in ANOVA-T$^{1}$PNN on \textsc{Calhousing} dataset.}}
    \label{fig_anova_node_caltrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_nam_cal_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in NA$^{1}$M on \textsc{Calhousing} dataset.}}
    \label{fig_nam_caltrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_nbm_cal_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{ Plots of the functional relations of the main effects in NB$^{1}$M on \textsc{Calhousing} dataset.}}
    \label{fig_nbm_caltrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_anova_n2ode_cal_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in ANOVA-T$^{2}$PNN on \textsc{Calhousing} dataset.}}
    \label{fig_anova_n2ode_caltrain}
\end{figure}


\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_n2am_cal_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in N$\text{A}^{2}$M on \textsc{Calhousing} dataset.}}
    \label{fig_n2am_caltrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_n2bm_cal_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in N$\text{B}^{2}$M on \textsc{Calhousing} dataset.}}
    \label{fig_n2bm_caltrain}
\end{figure}


\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_anova_node_wine_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in ANOVA-T$^{1}$PNN on \textsc{Wine} dataset.}}
    \label{fig_anova_node_winetrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_nam_wine_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in NA$^{1}$M on \textsc{Wine} dataset}.}
    \label{fig_nam_winetrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_nbm_wine_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in NB$^{1}$M on \textsc{Wine} dataset}.}
    \label{fig_nbm_winetrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_anova_n2ode_wine_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in ANOVA-T$^{2}$PNN on \textsc{Wine} dataset}.}
    \label{fig_anova_n2ode_winetrain}
\end{figure}


\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_n2am_wine_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in N$\text{A}^{2}$M on \textsc{Wine} dataset}.}
    \label{fig_n2am_winetrain}
\end{figure}

\begin{figure}[H]
\centering
    \includegraphics[width=0.9 \textwidth]{picture/fig_n2bm_wine_train.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects in N$\text{B}^{2}$M on \textsc{Wine} dataset}.}
    \label{fig_n2bm_winetrain}
\vskip -0.2cm
\vskip -0.2cm
\end{figure}


\newpage

\section{Stability of NBM-TPNN.}
\label{app:NBM_exp_result}

Figures \ref{fig_ANOVA-NBM-cal} and \ref{fig_ANOVA-NBM-wine} show the plots of the functional relations of the main effects estimated by  NBM-T$^{1}$PNN on 5 randomly sampled datasets from
\textsc{Wine} and \textsc{Calhousing} dataset, respectively, which
amply show that NBM-TPNN is also highly stable in estimation of components.

%\begin{figure}[H]
%    \centering
%\includegraphics[width=0.9 \textwidth]{picture/fig_num_parameter_ratio.png}
%    \caption{\small \textbf{Plots of ratio $|$ANOVA-N$^{d}$ODE$|$ $\backslash$ $|$NBM-N$^{d}$ODE$|$ for d=1,2,3,4}}
%    \label{fig_num_parameter_NBM}
%\end{figure}

\begin{figure}[H]
    \centering
\includegraphics[width=0.9 \textwidth]{picture/fig_anova_nbm_cal_train.png}
\vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effect estimated by NBM-T$^{1}$PNN on 5 randomly sampled training data from \textsc{Calhousing} dataset.}}
    \label{fig_ANOVA-NBM-cal}
\end{figure}

\begin{figure}[H]
    \centering
\includegraphics[width=0.9 \textwidth]{picture/fig_anova_nbm_wine_train.png}
\vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects estimated by NBM-T$^{1}$PNN on 5 randomly sampled training data from \textsc{Wine} dataset.}}
    \label{fig_ANOVA-NBM-wine}
\end{figure}


\newpage

\section{Additional experiments for component selection} \label{app:component selection}

Table \ref{table:syn_result} presents the averages (the standard deviations) of the prediction performance of the models used in the component selection experiment in Section \ref{sec:component_selection}, which indicates that
ANOVA-TPNN, NAM and NBM perform similarly.

\begin{table}[H] 
\scriptsize
\centering
\caption{\footnotesize \textbf{The results of prediction performance.} 
We report the averages and standard deviations of RMSEs 
of ANOVA-T$^2$PNN, NA$^2$M and NB$^2$M on 10 synthetic datasets generated from $f^{(1)}$, $f^{(4)}$ and $f^{(3)}$.}
\label{table:syn_result}
\begin{tabular}{c|c|p{0.6cm}p{0.6cm}p{1.0cm}}
\hline
&   & \multicolumn{3}{|c}{G$\text{A}^{2}$M}     \\ \hline \hline
       Synthetic function & Measure &\begin{tabular}[c]{@{}c@{}}ANOVA\\ T$^{2}$PNN \end{tabular} & \begin{tabular}{c} N$\text{A}^{2}$M \end{tabular} & \begin{tabular}{c} N$\text{B}^{2}$M \end{tabular}  \\ \hline 
$f^{(1)}$ & RMSE $\downarrow$ & 
\begin{tabular}{p{0.1cm}} 3.483 \\(0.03)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\textbf{3.474}$ \\(0.03)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 3.511 \\(0.03)\end{tabular}  \\
\hline
$f^{(2)}$ & RMSE $\downarrow$ & 
\begin{tabular}{p{0.1cm}} 0.076 \\(0.001)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.088 \\(0.005)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\textbf{0.075}$ \\(0.001)\end{tabular}  \\
\hline
$f^{(3)}$ & RMSE $\downarrow$ & 
\begin{tabular}{p{0.1cm}} 0.161 \\(0.003)\end{tabular}  &
\begin{tabular}{p{0.1cm}} 0.183 \\(0.016)\end{tabular}  &
\begin{tabular}{p{0.1cm}} $\textbf{0.137}$ \\(0.003)\end{tabular}  \\
\hline
\end{tabular}
\end{table}

\newpage

\section{Comparison of ANOVA-SHAP and Kernel-SHAP}
\label{app:ANOVA-SHAP-result}


In this section, we conduct an experiment to investigate the similarity between ANOVA-SHAP and Kernel-SHAP \cite{lundberg2017unified}. 
We calculate ANOVA-SHAP from ANOVA-T$^{1}$PNN approximating a given black-box model.
For the black-box model, we use  XGB with 100 trees, a maximum depth of 7, and a learning rate of 0.1
trained on \textsc{wine} dataset.  Below, we compare the SHPA values for three inputs.

For input $\textbf{x}_{1} = (6.9, 0.22, 0.32, 9.3, 0.04, 22, 110, 0.99, 3.34, 0.54, 10.7)^\top$, the SHAP values are given as:
\begin{align*}
\text{Kernel-SHAP} &:(0.057,  0.095,  0.065,  0.088, -0.009, -0.136,  0.034, -0.119, 0.149,  0.040, -0.082)  \\
\text{ANOVA-SHAP} &: (-0.017,  0.056,  0.104,  0.132,  0.006, -0.118,  0.055, -0.163, 0.093,  0.016, -0.081).
\end{align*}
For input $ \textbf{x}_{2} = (7.00, 0.17, 0.33, 4.00, 0.03, 17, 127, 0.99, 3.19, 0.39, 10.6)^\top$, the SHAP values are given as:
\begin{align*}
\text{Kernel-SHAP} &: (0.015,  0.327,  0.031, -0.080,  0.028, -0.146,  0.050,  0.043, -0.050, -0.01, -0.115)   \\  
\text{ANOVA-SHAP} &: (-0.015,  0.243,  0.093, -0.148,  0.024, -0.111,  0.075,  0.141, -0.061, -0.022, -0.101).
\end{align*}
For input $\textbf{x}_{3} = (6.90, 0.25, 0.35, 9.20, 0.03, 42,
       150, 0.99, 3.21, 0.36, 11.5 )^\top$, the SHAP values are given as:
\begin{align*}
\text{Kernel-SHAP} &: (-0.004,  0.018,  0.060,  0.092,  0.032,  0.047,  0.014, -0.024, -0.123, -0.027,  0.084) \\  
\text{ANOVA-SHAP} &: (-0.017,  0.003,  0.071,  0.128,  0.024,  0.088,  0.062, -0.004, -0.062, -0.046,  0.118).
\end{align*}

The results suggest that ANOVA-SHAP and Kernel SHAP are similar. At least, the signs are exactly the same.
An obvious advantage of ANOVA-SHAP is computation. Only one model fitting for finding a ANOVA-TPNN
approximating a given black-box model is required for ANOVA-SHAP. In contrast,
Kernel SHAP requires training a linear models for each data point.
For illustration, if we want to calculate the SHAP values for 1000 data points,
computation time of ANOVA-SHAP is approximately 6,500 times faster than that of Kernel SHAP.




\newpage

\section{Additional Experiments for prediction performance comparison with Decision Tree}


\begin{table}[H]
\centering
\footnotesize
\caption{\footnotesize\textbf{Results of the prediction performance in Decision Tree and ANOVA-TPNN.}}
\label{table:pred_per_dec}
\begin{tabular}{c|c|c|c|c}
\hline
Dataset & Measure & Decision Tree & ANOVA-T$^{1}$PNN & ANOVA-T$^{2}$PNN \\ 
 \hline \hline
\textsc{Calhousing} & RMSE $\downarrow$ &
 0.671 ( 0.02 ) & 0.614 ( 0.01 ) &   0.512  ( 0.01 )  \\
\textsc{Wine} & RMSE $\downarrow$ &
 0.811 ( 0.03 ) & 0.725 ( 0.02 ) &   0.704  ( 0.02 )  \\
\textsc{Online} & RMSE $\downarrow$ &
 1.119 ( 0.26 ) & 1.111 ( 0.25 ) & 1.111  ( 0.25 )  \\ 
\textsc{Abalone} & RMSE $\downarrow$ &
 2.396 ( 0.08 ) & 2.135 ( 0.09 ) & 2.087  ( 0.08 )  \\ 
\textsc{FICO} & AUROC $\uparrow$ &
 0.704 ( 0.02 ) & 0.799 ( 0.007 ) & 0.800  ( 0.007 )  \\
 \textsc{Churn} & AUROC $\uparrow$  &
 0.676 ( 0.03 ) & 0.839 ( 0.012 ) & 0.842  ( 0.012 )  \\
\textsc{Credit} & AUROC $\uparrow$ &
 0.890 ( 0.02 ) & 0.983 ( 0.005 ) & 0.984  ( 0.006 )  \\
 \textsc{Letter} & AUROC $\uparrow$ &
 0.745 ( 0.001 ) & 0.900 ( 0.003 ) & 0.984  ( 0.001 )  \\
 \textsc{Drybean} & AUROC $\uparrow$ &
 0.975 ( 0.0002 ) & 0.995 ( 0.001 ) & 0.997  ( 0.001 )  \\
 \hline
\end{tabular}
\end{table}

Table \ref{table:pred_per_dec} presents the averages and standard deviations of the prediction performance of Decision Tree \cite{breiman2017classification} for 10 trials.
We observe that the performance of ANOVA-TPNN is significantly better than that of Decision Tree.
We implement Decision Tree by using the scikit-learn python package \cite{pedregosa2011scikit} and turned by using the optuna python package based on below range of hyper-parameters.


\begin{itemize}
    \item Range of max depth = [2 ,12]

    \item Range of min\_samples\_leaf = [2,10]

    \item Range of min\_samples\_split = [2,10]

    \item Range of max\_leaf\_nodes = [2,10]
\end{itemize}

\newpage

\section{Additional Experiments for runtime on various datasets.}
\label{app:computation time}

\begin{table}[h]
\centering
\scriptsize
\caption{\footnotesize\textbf{Results of runtimes of NA$^{1}$M, NB$^{1}$M, ANOVA-T$^{1}$PNN, and NBM-T$^{1}$PNN.}}
\label{table:runtime}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
 Dataset & Size of dataset & \# of features & NA$^{1}$M & NB$^{1}$M & ANOVA-T$^{1}$PNN & NBM-
 T$^{1}$PNN  \\ 
 \hline \hline
\textsc{Abalone} & 4K & 10 & 6.6 sec & 3.0 sec & 1.6 sec & 1.5 sec\\
\textsc{Calhousing} & 21K & 8 & 14.1 sec & 4.1 sec & 3.8 sec & 3.5 sec\\
\textsc{Online} & 40K & 58 & 68 sec & 15.6 sec & 65 sec & 9.8 sec\\
\hline
\end{tabular}
\end{table}

We conduct experiments to assess the scalability of NBM-TPNN. 
We consider NA$^{1}$M, which has 3 hidden layers with 16, 16, and 8 nodes; 10 basis DNNs for NB$^{1}$M, which have 3 hidden layers with 32, 16, and 16 nodes; $K_{S} = 10$ for each component $S$ in ANOVA-T$^{1}$PNN; and 10 basis functions in NBM-T$^{1}$PNN.
Table \ref{table:runtime} presents the results of runtimes of NA$^{1}$M, NB$^{1}$M, ANOVA-T$^{1}$PNN, and NBM-T$^{1}$PNN on $\textsc{Abalone}$, $\textsc{Calhousing}$, and $\textsc{Online}$ datasets.
When the dimension of input features is small, there is little difference in runtime between ANOVA-T$^{1}$PNN and NBM-T$^{1}$PNN. However, as the input dimension increases, the runtime gap becomes more pronounced.
%Note that our computational environment consists of RTX 3090 and RTX 4090.


\newpage

\section{Additional Experiments for ReLU activation function}
\label{app:relu}


\begin{table}[h]
\centering
\scriptsize
\caption{\footnotesize \textbf{Results of prediction performance of ANOVA-TPNN with ReLU}. We report the averages of RMSE (standard deviation) and stability score for 10 trials.}
\label{table:ReLU}
\begin{tabular}{c|c|c|c}
\hline
 & Dataset &ANOVA-T$^{1}$PNN with ReLU & ANOVA-T$^{1}$PNN \\ \hline \hline
\multirow{2}{*}{ \begin{tabular}{c} RMSE $\downarrow$
\end{tabular}} &
\textsc{Abalone} & 2.148 (0.08) & 2.135 (0.09)\\ 
&\textsc{Wine} & 0.735 (0.02) & 0.725 (0.02) \\ \hline 
\multirow{2}{*}{ \begin{tabular}{c} Stability score $\downarrow$
\end{tabular}} &
\textsc{Abalone} & 0.016 & 0.008\\ 
&\textsc{Wine} & 0.018 & 0.011 \\ \hline 
\end{tabular}%
\end{table}

We conduct additional experiments to evaluate the performance of ANOVA-TPNN with the ReLU activation function i.e., $\sigma(x) = \max(0,x).$
The $K_{S}$s for ANOVA-T$^{1}$PNNs with ReLU and sigmoid are determined through grid search on the range [10,30,50]. Table \ref{table:ReLU} presents the results of stability scores and prediction performance of ANOVA-TPNNs with the ReLU and sigmoid activations  on \textsc{abalone} and \textsc{wine} dataset. 
We observe that ANOVA-TPNN with Relu is slightly inferior to ANOVA-TPNN with sigmoid
in view of both prediction performance and stability of component estimation.
This would be because ANOVA-TPNN with sigmoid is more robust to input outliers than
ANOVA-TPNN with ReLU.

\newpage

\section{ANOVA-TPNN without sum-to-zero condition} \label{app:GAM-NODE}

\begin{table}[h]
\centering
\scriptsize
\caption{\footnotesize \textbf{ 
Comparison of ANOVA-TPNN and GAM-TPNN}. We report the stability score (normalized by the that of ANOVA-T$^{1}$PNN or ANOVA-T$^{2}$PNN) for 10 trials.}
\label{table:GAM-NODE}
\begin{tabular}{c|c|c|c|c}
\hline
& ANOVA-T$^{1}$PNN & GAM-T$^{1}$PNN & ANOVA-T$^{2}$PNN &GAM-T$^{2}$PNN \\ \hline \hline
\textsc{Calhousing} & $\textbf{1.000}$  & 1.500 &  $\textbf{1.000}$ & 1.690 \\ \hline 
\textsc{Wine} & $\textbf{1.000}$ & 2.550& $\textbf{1.000}$  & 1.300  \\ \hline
\end{tabular}%
\end{table}

\begin{figure}[H]
    \centering
\includegraphics[width=0.9 \textwidth]{picture/fig_new_gam_node_cal.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects on 
    5 randomly sampled training data from  \textsc{Calhousing} datasets.}}    
    \label{fig_GAM_NODE_plot_cal}
\end{figure}

\begin{figure}[H]
    \centering
\includegraphics[width=0.9 \textwidth]{picture/fig_new_gam_node_wine.png}
    \vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of the main effects on 
    5 randomly sampled training data from  \textsc{Wine} datasets.}}    
    \label{fig_GAM_NODE_plot_wine}
\end{figure}

We investigate the stability in component estimation of ANOVA-TPNN without the sum-to-zero condition,
which we denote GAM-TPNN,
by analyzing \textsc{Calhousing} and \textsc{Wine} datasets. 
In GAM-T$^{1}$PNN, we approximate $f_{j}(x_{j})$ by
\begin{align*}
f_{j}(x_{j}) \approx \sum_{k=1}^{K_{j}} \bigg \{ \beta_{jk}^{1} \bigg ( 1- \sigma \bigg({x_{j}-b_{jk}\over \gamma_{jk}} \bigg) \bigg) + \beta_{jk}^{2}\sigma \bigg({x_{j}-b_{jk}\over \gamma_{jk}} \bigg) \bigg \}
\end{align*}
where $\beta_{jk}^{1},\beta_{jk}^{2}, b_{jk},\gamma_{jk}$ are learnable parameters.
GAM-T$^{1}$PNN can be easily extended to GAM-T$^{d}$PNN in a similar way to ANOVA-T$^{d}$PNN.

Table \ref{table:GAM-NODE} presents the stability score of ANOVA-TPNN and GAM-TPNN based on 10 randomly selected datasets. 
Without the sum-to-zero condition, we observe increasing in the stability score. 
In particular, when the second order interactions are in the model, the main effects are estimated very unstably. 

% In Table \ref{table:GAM-NODE}, we observe that the prediction performance of GAN-NODE 
% is (slightly) better than that of ANOVA-NODE.
% One reason could be that ANOVA-NODE is more vulnerable to the local minima problem. 
% Further studies would be worth pursuing.

Figure \ref{fig_GAM_NODE_plot_cal} and \ref{fig_GAM_NODE_plot_wine} present the plots of the functional relations of the main effects on \textsc{calhousing} and \textsc{wine} dataset in GAM-T$^{2}$PNN.
We observe that GAM-T$^{2}$PNN estimates the components more unstable compared to ANOVA-T$^{2}$PNN.


\newpage

\section{On the post-processing for the sum-to-zero condition}
\label{app:post}

\begin{table}[h]
\centering
\small
\caption{\footnotesize \textbf{Stability scores for `Latitude' and `Longitude' of
$\textsc{Calhousing}$ dataset after post-processing}}
\label{table:stab_lat_long}
\begin{tabular}{c|c|c|c}
\hline
Model & ANOVA-T$^{2}$PNN & N$\text{A}^{2}$M & N$\text{B}^{2}$M \\ \hline \hline
Latitude & 0.006 & 0.067 & 0.104 \\ \hline
Longitude & 0.015  & 0.094 & 0.103 \\ \hline
\end{tabular}
\end{table}
\vskip -0.3cm

\begin{figure}[h]
    \centering
\includegraphics[width=0.9 \textwidth]{picture/fig_post_comparison.png}
\vskip -0.3cm
    \caption{\footnotesize \textbf{Plots of the functional relations of `Latitude' and `Longitude'
    of $\textsc{Calhousing}$ dataset after post-processing.}}
\label{fig_post_compare}
\end{figure}

We have seen that N$\text{A}^{2}$M and N$\text{B}^{2}$M are competitive to ANOVA-TPNN in prediction performance even though they are poor in estimating the components. 
There is a way to transform any estimates of the components to those that satisfy the sum-to-zero condition \cite{func_puri}. 


We consider an arbitrary estimated GA$^{d}$M $\hat{f}(\textbf{x}) = \beta_{0} + \sum_{S \subseteq [p], |S|\le d} \hat{f}_{S}(\textbf{x}_{S}).$ 
We fix $\textbf{x}$ and write $f_{S}$ instead of $f_{S}(\textbf{x}_{S})$ for notational simplicity.

For given $S$ with $|S|=d,$ 
we first transform
$\hat{f}_{S}$ into 
\begin{equation}
\label{eq:transform}
\Tilde{f}_{S} = \hat{f}_{S} + \sum_{k=1}^{d}\sum_{V \subseteq S, |V| = k}(-1)^{d-k} \int_{\mathcal{X}_{V}}\hat{f}_{S}d\Pi_{j \in V}\mu_{j}.  
\end{equation}
Then, $\Tilde{f}_{S}$ satisfies the sum-to-zero condition \cite{hooker_diag, func_puri}.
In turn for any $S'\subset S$ 
we redefine $\hat{f}_{S'}$ into
\begin{equation}
\label{eq:transform2}
\hat{f}_{S'} = \hat{f}_{S'} - (-1)^{|S'|}\int_{\mathcal{X}_{S\backslash S'} } \hat{f}_{S}d\Pi_{j\in S\backslash S'}\mu_{j}.
\end{equation}

We apply the transformation (\ref{eq:transform}) to $\hat{f}_{S}$ for $|S|=d-1$ 
to have $\Tilde{f}_{S},$ and redefine
$\hat{f}_{S'}$ for $S'\subset S$ by (\ref{eq:transform2}) . 
We repeat this process sequentially until $|S|=1$
to have $\Tilde{F}_S$ for all $S\subset [p]$ with $|S|\le d$
which satisy the sum-to-zero condition.

Computational complexity of this post-processing for a fixed input 
is $\mathcal{O}(dn^{d-1})$ and thus computational complexity of calculating
$\Tilde{f}_S$ for all training data becomes $\mathcal{O}(dn^{d})$ which is demanding
when $n$ or $d$ is large. 
Furthermore, performing the post-processing requires storing thw whole dataset, 
which causes memory efficiency issues.

Table \ref{table:stab_lat_long} compares the stability scores 
of the main effects of `Latitude' and `Longitude' of $\textsc{Calhousing}$ dataset estimated by
ANOVA-T$^{2}$PNN and post-processed NA$^{2}$M and NB$^{2}$M, and
Figure \ref{fig_post_compare} draws the 5 functional relations
of the estimated main effects of `Latitude' and `Longitude' on 5 randomly sampled training data.
It is observed that NA$^{2}$M and NB$^{2}$M are still unstable even after the post-processing,
which suggests that instability in NAM and NBM is not only from unidentifiability but also
instability of DNN.


\newpage

\section{Details of Spline-GAM}
\label{app:splin_gam}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{picture/fig_spline_gam_scatter.png}
    \caption{\footnotesize \textbf{Scatter plot of ($y^{test}_{i}$, $\hat{f}_{\text{Spline-GA$^{1}$M}}(x^{test}_{i})$), and the plot of the main effect `AveOccup' estimated by Spline-GA$^{1}$M.}}
    \label{fig_splinegam_scatter}
\end{figure}

\begin{table}[h]
\centering
\scriptsize
\caption{\footnotesize \textbf{Contribution of the main effects in Spline-GA$^{1}$M.}}
\label{table:con_spline-gam}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Main effect & `MedInc' & `HouseAge' & `AveRooms' & `AveBedrms' & `Population'  & `AveOccup' & `Latitude' & `Longitude'\\ \hline
Contribution & 1.64 & 0.36 & -4.18 & 4.82 & 0.25 & -13.78 & -0.47 & 1.55 \\ \hline
\end{tabular}%
\end{table}


In this section, we describe Spline-GAM \cite{wood2017generalized} which estimates each component by tensor product spline basis functions.
For $j \in [p]$, let $t_{j,1},...,t_{j,M_{j}+5}$ be sorted knots into non-decreasing order on $\mathcal{X}_{j}$. 
For a given sequence of knots, the cubic spline basis functions for $f_{j}(\cdot)$ are defined as
\begin{align*}
B_{j,i,0}(x) &= \mathbb{I}(t_{i} \leq x < t_{i+1}) \\
B_{j,i,k}(x) &= {x-t_{i} \over t_{i+k} - t_{i}}B_{j,i,k-1}(x) +  {t_{i+k+1} - x \over t_{i+k+1} - t_{i+1}}B_{j,i+1,k-1}(x) 
\end{align*}
for $k=1,2,3$ and $i=1,...,M_{j}$.

For general $S \subseteq [p]$, Spline-GAM estimates $f_{S}(\textbf{x}_{S})$ by
\begin{align*}
f_{S}(\textbf{x}_{S}) &\approx ( \otimes_{j \in S} \textbf{B}_{j}(x_{j})) \bm{\alpha}_{S}  \\
&= g_{S}(\textbf{x}_{S})
\end{align*}
where $\textbf{B}_{j}(x_{j}) = ( B_{j,1,3}(x_{j}),..., B_{j,M_{j},3}(x_{j}) )^\top$ and 
the second derivatives of the basis functions at the boundaries are set to be zero. 
These basis functions are called the Natural Cubic Splines \cite{hastie2009elements}.

Furthermore, to estimate each component in the functional ANOVA model by the smooth function, Spline-GAM employs the penalty term.
For $S \subseteq [p]$, the penalty term for $f_{S}(\textbf{x}_{S})$ is defined as
$$J(g_{S}) =  \lambda_{S}\sum_{j \in S} \int \bigg( {\partial^{2} g_{S}(\textbf{x}_{S}) \over \partial^{2} x_{j} }
 \bigg)^{2}d\textbf{x}_{S}$$
The smoothness of the spline model can be adjusted by tuning $\lambda_{S} > 0$.

\paragraph{Inaccurate prediction beyond the boundary.}
Spline-GAM is a model that sets knots based on the training data and interpolates between the knots using cubic spline basis functions.
If the test data contains input outliers, the prediction performance of Spline-GAM may deteriorate for those outliers.

We conduct additional analysis on \textsc{calhousing} dataset to examine the effect of input outliers.
Figure \ref{fig_splinegam_scatter} shows the scatter plot of ($y^{test}_{i}$, $\hat{f}_{\text{Spline-GA$^{1}$M}}(x^{test}_{i})$), $i=1,....,n_{\text{test}}$, and the plot of the functional relation of the main effect `AveOccup' estimated by Spline-GA$^{1}$M.
In scatter plot, we observe that for a given data point $x^{*}=(10.2, 45, 3.17, 0.83, 7460, 1243, 38.32, -122)^{\top}$, the value of $y^{*}$ is approximately 1.3, while the corresponding prediction from Spline-GA$^{1}$M is approximately -10.

To investigate the reason behind such an inaccurate prediction for the data point $x^{*}$, we explore the contribution of each main effect in Spline-GA$^{1}$M as shown in Table \ref{table:con_spline-gam}.
We observe that the contribution of the main effect `AveOccup' is abnormally high.
Upon examining the plot of the main effect `AveOccup' estimated by the Spline-GA$^{1}$M, we conclude that the inaccurate prediction arises because the `AveOccup' feature value of $x^{*}$ is an outlier and linear extrapolation of the cubic spline basis functions outside the range of inputs is used. In contrast, the TPNN with the sigmoid activation is bounded outside the range of inputs and so
robust to input outliers.


\end{document}
