\section{Related Work}
\paragraph{Vision-Language Models for \RL.}

Our work is related to the literature on VLM rewards and preferences for embodied manipulation tasks____. These methods can be divided into three categories: (\romannumeral1) representation-based pre-training, (\romannumeral2) zero-shot inference, and (\romannumeral3) downstream fine-tuning.
For representation-based approaches, R3M____ is pre-trained on the Ego4D dataset____ to learn useful representations for downstream tasks. LIV____, which extends VIP____ to multi-modal representations, is pre-trained on EpicKitchen dataset____, and can also be fine-tuned on target domain.
For zero-shot inference methods, VLM-RM____ utilizes CLIP____ as zero-shot vision-language rewards.
\RC____ uses S3D____, which is pre-trained on HowTo100M dataset____, as video-language model to compute vision-language reward with a single demonstration (a video or a text).
RL-VLM-F____ leverages Gemini-Pro____ and GPT-4V____ for zero-shot preference feedback.
CriticGPT____ is the representative method of (\romannumeral3), which fine-tunes multimodal LLMs on a instruction-following dataset, and utilizes the tuned model to provide preference feedback for downstream policy learning.
\ourmethod differs from these approaches that we do not suffer from burdensome training of (\romannumeral1) and (\romannumeral3), showing great computing efficiency. And \ourmethod learns more embodied manipulation knowledge compared with VLMs pre-trained on natural image-text data.


\begin{table*}[t]
\centering
\caption{Success rate of RLHF methods with scripted labels and \ourmethod labels. The results are reported with mean and standard deviation across five random seeds. The result of \ourmethod is \colorbox{\mycolor}{shaded} and is \textbf{bolded} if it exceeds or is comparable with that of RLHF approaches with scripted labels. \ourmethod Acc. denotes the accuracy of preference labels inferred by \ourmethod compared with scripted labels.}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l|ll|ll|ll|l}
    \toprule
    {\bf Task} & {\bf P-IQL} & {\bf P-IQL+\ourmethod} & {\bf IPL} & {\bf IPL+\ourmethod} & {\bf CPL} & {\bf CPL+\ourmethod} & {\bf \ourmethod Acc.} \\
    \midrule
    Button Press
    & \mean{72.6} \std{7.1} % P-IQL
    & \highlight \bmean{90.1} \std{3.9} % P-IQL+ours
    & \mean{50.6} \std{7.9} % IPL
    & \highlight \bmean{56.0} \std{1.4} % IPL+ours
    & \mean{74.5} \std{8.2} % CPL
    & \highlight \bmean{83.9} \std{11.8} % CPL+ours
    & \mean{93.0} % ours
    \\
    Door Close
    & \mean{79.2} \std{6.3} % P-IQL
    & \highlight \bmean{79.2} \std{6.3} % P-IQL+ours
    & \mean{61.5} \std{9.4} % IPL
    & \highlight \bmean{61.5} \std{9.4} % IPL+ours
    & \mean{98.5} \std{1.0} % CPL
    & \highlight \bmean{98.5} \std{1.0} % CPL+ours
    & \mean{100.0} % ours
    \\
    Drawer Close
    & \mean{49.3} \std{4.2} % P-IQL
    & \highlight \bmean{64.9} \std{2.9} % P-IQL+ours
    & \mean{64.3} \std{9.6} % IPL
    & \highlight \mean{63.2} \std{4.7} % IPL+ours
    & \mean{45.6} \std{3.5} % CPL
    & \highlight \bmean{57.5} \std{14.3} % CPL+ours
    & \mean{96.0} % ours
    \\
    Faucet Close
    & \mean{51.1} \std{7.5} % P-IQL
    & \highlight \bmean{51.1} \std{7.5} % P-IQL+ours
    & \mean{45.4} \std{8.6} % IPL
    & \highlight \bmean{45.4} \std{8.6} % IPL+ours
    & \mean{80.0} \std{2.9} % CPL
    & \highlight \bmean{80.0} \std{2.9} % CPL+ours
    & \mean{100.0} % ours
    \\
    Window Open
    & \mean{62.4} \std{6.4} % P-IQL
    & \highlight \bmean{69.7} \std{6.8} % P-IQL+ours
    & \mean{54.1} \std{6.7} % IPL
    & \highlight \bmean{61.4} \std{8.6} % IPL+ours
    & \mean{91.6} \std{1.7} % CPL
    & \highlight \bmean{99.1} \std{1.1} % CPL+ours
    & \mean{98.0} % ours
    \\
    \midrule
    {\bf Average}
    & \mean{62.9} % P-IQL
    & \highlight \bmean{71.0} % P-IQL+ours
    & \mean{55.2} % IPL
    & \highlight \bmean{57.5} % IPL+ours
    & \mean{78.0} % CPL
    & \highlight \bmean{83.8} % CPL+ours
    & \mean{97.4} % ours
    \\
    \bottomrule
\end{tabular}
}
\label{tab:main_rlhf}
\end{table*}


\begin{table*}[!t]
\centering
\caption{Success rate of \ourmethod (i.e., P-IQL trained with \ourmethod labels) against IQL with VLM \textbf{rewards}. The results are reported with mean and standard deviation across five random seeds. The result of \ourmethod is \colorbox{\mycolor}{shaded} and the best score of all methods is \textbf{bolded}.}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|lllllll}
    \toprule
    {\bf Task} & {\bf R3M} & {\bf VIP} & {\bf LIV} & {\bf CLIP} & {\bf VLM-RM (0.0)} & {\bf VLM-RM (1.0)} & {\bf \ourmethod} \\
    \midrule
    Button Press
    & \mean{10.1} \std{2.3} % R3M
    & \mean{68.4} \std{6.4} % VIP
    & \mean{56.3} \std{1.9} % LIV
    & \mean{59.5} \std{6.1} % CLIP
    & \mean{60.3} \std{6.1} % VLM-RM (0.0)
    & \mean{64.3} \std{8.4} % VLM-RM (1.0)
    & \highlight \bmean{90.1} \std{3.9} % ours
    \\
    Door Close
    & \mean{70.9} \std{5.3} % R3M
    & \mean{74.8} \std{9.5} % VIP
    & \mean{43.3} \std{3.2} % LIV
    & \mean{43.6} \std{3.9} % CLIP
    & \mean{45.8} \std{8.5} % VLM-RM (0.0)
    & \mean{41.1} \std{3.4} % VLM-RM (1.0)
    & \highlight \bmean{79.2} \std{6.3} % ours
    \\
    Drawer Close
    & \mean{46.6} \std{2.6} % R3M
    & \mean{70.4} \std{4.5} % VIP
    & \mean{61.8} \std{5.7} % LIV
    & \mean{69.4} \std{4.1} % CLIP
    & \mean{69.4} \std{4.5} % VLM-RM (0.0)
    & \bmean{73.5} \std{5.4} % VLM-RM (1.0)
    & \highlight \mean{64.9} \std{2.9} % ours
    \\
    Faucet Close
    & \mean{25.7} \std{23.6} % R3M
    & \mean{40.9} \std{8.0} % VIP
    & \mean{42.2} \std{6.3} % LIV
    & \mean{59.6} \std{7.5} % CLIP
    & \bmean{60.1} \std{5.1} % VLM-RM (0.0)
    & \mean{33.7} \std{15.3} % VLM-RM (1.0)
    & \highlight \mean{51.1} \std{7.5} % ours
    \\
    Window Open
    & \mean{39.0} \std{6.6} % R3M
    & \mean{42.7} \std{11.3} % VIP
    & \mean{33.8} \std{6.4} % LIV
    & \mean{26.4} \std{2.0} % CLIP
    & \mean{23.9} \std{1.9} % VLM-RM (0.0)
    & \mean{23.7} \std{4.9} % VLM-RM (1.0)
    & \highlight \bmean{69.7} \std{6.8} % ours
    \\
    \midrule
    {\bf Average}
    & \mean{38.5} % R3M
    & \mean{59.4} % VIP
    & \mean{47.5} % LIV
    & \mean{51.7} % CLIP
    & \mean{51.9} % VLM-RM (0.0)
    & \mean{47.3} % VLM-RM (1.0)
    & \highlight \bmean{71.0} % ours
    \\
    \bottomrule
\end{tabular}
}
\label{tab:main_vlm}
\end{table*}


\paragraph{Preference-based Reinforcement Learning.}

\PbRL is a promising framework for aligning the agent with human values. However, feedback efficiency is a crucial challenge in \pbRL, with multiple recent studies striving to tackle. PEBBLE____ improves the efficiency by unsupervised pre-training. SURF____ proposes to obtain pseudo labels using reward confidence. RUNE____ employs reward uncertainty to guide exploration. Meta-Reward-Net____ takes advantage of the performance of the Q-function as an additional signal to refine the accuracy of the reward model. ____ leverages meta-learning to pre-train the reward model, enabling fast adaptation to new tasks with few preference labels. SEER____ enhances the efficiency of \pbRL by label smoothing and policy regularization. RAT____ proposes to use \pbRL to attack deep RL agents.
Recently, a growing number of studies focus on offline \pbRL with the population of offline RL____.
PT____ introduces a Transformer-based architecture for reward modeling. OPPO____ proposes to learn policies without a reward function. IPL____ learns the Q-function from preferences, also eliminating the need of reward learning. CPL____ further views \pbRL as a supervised learning problem, directly learning policies from preferences. FTB____ introduces a diffusion model for better trajectory generation. PEARL____ proposes cross-task preference alignment to transfer preference labels between tasks and learn reward models robustly via reward distributional modeling. CAMP____ learns a diffusion-based preference model for preference alignment in multi-task RL____.
\ourmethod addresses the labeling cost by learning a \vl preference model via \vl alignment, thereby providing generalized preferences to novel tasks.