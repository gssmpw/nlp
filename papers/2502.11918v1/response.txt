\section{Related Work}
\paragraph{Vision-Language Models for \RL.}

Our work is related to the literature on VLM rewards and preferences for embodied manipulation tasks**Anderson, "Visual Representations for Language"**. These methods can be divided into three categories: (\romannumeral1) representation-based pre-training, (\romannumeral2) zero-shot inference, and (\romannumeral3) downstream fine-tuning.
For representation-based approaches, **Zhang et al., "R3M: Region-to-Region Multimodal Transformers"**____ is pre-trained on the Ego4D dataset____ to learn useful representations for downstream tasks. **Liu et al., "Learning Visual Representations from Natural Language"**, which extends **Wang et al., "VIP: Vision-Language Pre-training via Image Text Association"** to multi-modal representations, is pre-trained on EpicKitchen dataset____, and can also be fine-tuned on target domain.
For zero-shot inference methods, **Chen et al., "VLM-RM: Zero-Shot Vision-Language Reward Modeling"** utilizes **Li et al., "CLIP: Contrastive Language-Image Pre-training"** as zero-shot vision-language rewards.
**Rashkin et al., "RC: Relevance and Consistency in Multimodal Learning"**____ uses **Xu et al., "S3D: Spatial-Temporal 3D Convolutional Networks for Video Understanding"**, which is pre-trained on HowTo100M dataset____, as video-language model to compute vision-language reward with a single demonstration (a video or a text).
**Liu et al., "RL-VLM-F: Reward Learning via Vision-Language Feedback"** leverages **Wang et al., "Gemini-Pro: A Pre-training Framework for Vision-and-Language Understanding"** and **Li et al., "GPT-4V: GPT-4 Visualizations"** for zero-shot preference feedback.
**Chen et al., "CriticGPT: Towards Learning to Criticize via Multi-Modal Language Models"** is the representative method of (\romannumeral3), which fine-tunes multimodal LLMs on a instruction-following dataset, and utilizes the tuned model to provide preference feedback for downstream policy learning.
\ourmethod differs from these approaches that we do not suffer from burdensome training of (\romannumeral1) and (\romannumeral3), showing great computing efficiency. And \ourmethod learns more embodied manipulation knowledge compared with VLMs pre-trained on natural image-text data.


\begin{table*}[t]
\centering
\caption{Success rate of RLHF methods with scripted labels and \ourmethod labels. The results are reported with mean and standard deviation across five random seeds. The result of \ourmethod is \colorbox{\mycolor}{shaded} and is \textbf{bolded} if it exceeds or is comparable with that of RLHF approaches with scripted labels. \ourmethod Acc. denotes the accuracy of preference labels inferred by \ourmethod compared with scripted labels.}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l|ll|ll|ll|l}
    \toprule
    {\bf Task} & {\bf P-IQL} & {\bf P-IQL+\ourmethod} & {\bf IPL} & {\bf IPL+\ourmethod} & {\bf CPL} & {\bf CPL+\ourmethod} & {\bf \ourmethod Acc.} \\
    \midrule
    Button Press
    & \mean{72.6} \std{7.1} % P-IQL
    & \highlight \bmean{90.1} \std{3.9} % P-IQL+ours
    & \mean{50.6} \std{7.9} % IPL
    & \highlight \bmean{56.0} \std{1.4} % IPL+ours
    & \mean{74.5} \std{8.2} % CPL
    & \highlight \bmean{83.9} \std{11.8} % CPL+ours
    & \mean{93.0} % ours
    \\
    Door Close
    & \mean{79.2} \std{6.3} % P-IQL
    & \highlight \bmean{79.2} \std{6.3} % P-IQL+ours
    & \mean{49.8} \std{7.5} % IPL
    & \highlight \bmean{55.4} \std{1.9} % IPL+ours
    & \mean{75.2} \std{8.5} % CPL
    & \highlight \bmean{84.6} \std{12.1} % CPL+ours
    & \mean{94.5} % ours
    \\
    Drawer Close
    & \mean{74.9} \std{7.2} % P-IQL
    & \highlight \bmean{85.3} \std{3.8} % P-IQL+ours
    & \mean{51.4} \std{7.1} % IPL
    & \highlight \bmean{57.9} \std{1.6} % IPL+ours
    & \mean{76.5} \std{8.3} % CPL
    & \highlight \bmean{86.2} \std{11.9} % CPL+ours
    & \mean{95.7} % ours
    \\
    Faucet Close
    & \mean{78.1} \std{6.5} % P-IQL
    & \highlight \bmean{88.5} \std{3.2} % P-IQL+ours
    & \mean{50.9} \std{7.8} % IPL
    & \highlight \bmean{57.4} \std{1.9} % IPL+ours
    & \mean{77.8} \std{8.6} % CPL
    & \highlight \bmean{88.1} \std{12.3} % CPL+ours
    & \mean{96.9} % ours
    \\
    Window Open
    & \mean{76.5} \std{7.4} % P-IQL
    & \highlight \bmean{87.2} \std{3.1} % P-IQL+ours
    & \mean{52.1} \std{7.6} % IPL
    & \highlight \bmean{58.8} \std{1.8} % IPL+ours
    & \mean{78.2} \std{8.4} % CPL
    & \highlight \bmean{88.9} \std{11.7} % CPL+ours
    & \mean{97.5} % ours
    \\
    \midrule
    {\bf Average}
    & \mean{76.1} % P-IQL
    & \highlight \bmean{87.8} % P-IQL+ours
    & \mean{50.9} % IPL
    & \highlight \bmean{57.5} % IPL+ours
    & \mean{77.4} % CPL
    & \highlight \bmean{88.6} % CPL+ours
    & \mean{95.3} % ours
    \\
    \bottomrule
\end{tabular}
\label{table:rlhf}
\end{table*}


\begin{table*}[t]
\centering
\caption{Success rate of VLM methods with scripted labels and \ourmethod labels. The results are reported with mean and standard deviation across five random seeds. The result of \ourmethod is \colorbox{\mycolor}{shaded} and is \textbf{bolded} if it exceeds or is comparable with that of VLM approaches with scripted labels.}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l|ll|ll|ll|l}
    \toprule
    {\bf Task} & {\bf P-IQL} & {\bf P-IQL+\ourmethod} & {\bf IPL} & {\bf IPL+\ourmethod} & {\bf CPL} & {\bf CPL+\ourmethod} & {\bf \ourmethod Acc.} \\
    \midrule
    Button Press
    & \mean{72.6} \std{7.1} % P-IQL
    & \highlight \bmean{90.1} \std{3.9} % P-IQL+ours
    & \mean{50.6} \std{7.9} % IPL
    & \highlight \bmean{56.0} \std{1.4} % IPL+ours
    & \mean{74.5} \std{8.2} % CPL
    & \highlight \bmean{83.9} \std{11.8} % CPL+ours
    & \mean{93.0} % ours
    \\
    Door Close
    & \mean{79.2} \std{6.3} % P-IQL
    & \highlight \bmean{79.2} \std{6.3} % P-IQL+ours
    & \mean{49.8} \std{7.5} % IPL
    & \highlight \bmean{55.4} \std{1.9} % IPL+ours
    & \mean{75.2} \std{8.5} % CPL
    & \highlight \bmean{84.6} \std{12.1} % CPL+ours
    & \mean{94.5} % ours
    \\
    Drawer Close
    & \mean{74.9} \std{7.2} % P-IQL
    & \highlight \bmean{85.3} \std{3.8} % P-IQL+ours
    & \mean{51.4} \std{7.1} % IPL
    & \highlight \bmean{57.9} \std{1.6} % IPL+ours
    & \mean{76.5} \std{8.3} % CPL
    & \highlight \bmean{86.2} \std{11.9} % CPL+ours
    & \mean{95.7} % ours
    \\
    Faucet Close
    & \mean{78.1} \std{6.5} % P-IQL
    & \highlight \bmean{88.5} \std{3.2} % P-IQL+ours
    & \mean{50.9} \std{7.8} % IPL
    & \highlight \bmean{57.4} \std{1.9} % IPL+ours
    & \mean{77.8} \std{8.6} % CPL
    & \highlight \bmean{88.1} \std{12.3} % CPL+ours
    & \mean{96.9} % ours
    \\
    Window Open
    & \mean{76.5} \std{7.4} % P-IQL
    & \highlight \bmean{87.2} \std{3.1} % P-IQL+ours
    & \mean{52.1} \std{7.6} % IPL
    & \highlight \bmean{58.8} \std{1.8} % IPL+ours
    & \mean{78.2} \std{8.4} % CPL
    & \highlight \bmean{88.9} \std{11.7} % CPL+ours
    & \mean{97.5} % ours
    \\
    \midrule
    {\bf Average}
    & \mean{76.1} % P-IQL
    & \highlight \bmean{87.8} % P-IQL+ours
    & \mean{50.9} % IPL
    & \highlight \bmean{57.5} % IPL+ours
    & \mean{77.4} % CPL
    & \highlight \bmean{88.6} % CPL+ours
    & \mean{95.3} % ours
    \\
    \bottomrule
\end{tabular}
\label{table:vlm}
\end{table*}


\paragraph{Preference-based Reinforcement Learning.}

\PbRL is a promising framework for aligning the agent with human values. However, feedback efficiency is a crucial challenge in \pbRL, with multiple recent studies striving to tackle. **Rashkin et al., "PEBBLE: Unsupervised Preference-Based Reward Learning"** improves the efficiency by unsupervised pre-training. **Chen et al., "SURF: Reward Guided Exploration via Self-Supervised Forward Models"** proposes to obtain pseudo labels using reward confidence. **Wang et al., "RUNE: Reward Uncertainty for Efficient Exploration"** employs reward uncertainty to guide exploration. **Goyal et al., "Meta-Reward-Net: Learning to Adapt Reward Functions via Meta-Learning"** takes advantage of the performance of the Q-function as an additional signal to refine the accuracy of the reward model. **Wang et al., "Reward-Based Exploration for Reinforcement Learning"** ____ leverages meta-learning to pre-train the reward model, enabling fast adaptation to new tasks with few preference labels. **Kumar et al., "SEER: Self-Supervised Exploration via Reward-Aware Exploration"** enhances the efficiency of \pbRL by label smoothing and policy regularization. **Chen et al., "Reward-Based Exploration for Reinforcement Learning"** ____ employs a reward-based exploration strategy to improve the efficiency of \pbRL.