[
  {
    "index": 0,
    "papers": [
      {
        "key": "CLIP",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "R3M",
        "author": "Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea and Gupta, Abhinav",
        "title": "{R3M}: A Universal Visual Representation for Robot Manipulation"
      },
      {
        "key": "LIV",
        "author": "Ma, Yecheng Jason and Kumar, Vikash and Zhang, Amy and Bastani, Osbert and Jayaraman, Dinesh",
        "title": "{LIV}: Language-Image Representations and Rewards for Robotic Control"
      },
      {
        "key": "VLM-RM",
        "author": "Juan Rocamonde and Victoriano Montesinos and Elvis Nava and Ethan Perez and David Lindner",
        "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning"
      },
      {
        "key": "RL-VLM-F",
        "author": "Wang, Yufei and Sun, Zhanyi and Zhang, Jesse and Xian, Zhou and Biyik, Erdem and Held, David and Erickson, Zackory",
        "title": "{RL-VLM-F}: Reinforcement Learning from Vision Language Foundation Model Feedback"
      },
      {
        "key": "CriticGPT",
        "author": "Liu, Jinyi and Yuan, Yifu and Hao, Jianye and Ni, Fei and Fu, Lingzhi and Chen, Yibin and Zheng, Yan",
        "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "R3M",
        "author": "Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea and Gupta, Abhinav",
        "title": "{R3M}: A Universal Visual Representation for Robot Manipulation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Ego4D",
        "author": "Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others",
        "title": "Ego4d: Around the world in 3,000 hours of egocentric video"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "VIP",
        "author": "Yecheng Jason Ma and Shagun Sodhani and Dinesh Jayaraman and Osbert Bastani and Vikash Kumar and Amy Zhang",
        "title": "{VIP}: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "VIP",
        "author": "Yecheng Jason Ma and Shagun Sodhani and Dinesh Jayaraman and Osbert Bastani and Vikash Kumar and Amy Zhang",
        "title": "{VIP}: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "EpicKitchen",
        "author": "Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others",
        "title": "Scaling egocentric vision: The epic-kitchens dataset"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "VLM-RM",
        "author": "Juan Rocamonde and Victoriano Montesinos and Elvis Nava and Ethan Perez and David Lindner",
        "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "CLIP",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "RoboCLIP",
        "author": "Sontakke, Sumedh and Zhang, Jesse and Arnold, S{\\'e}b and Pertsch, Karl and B{\\i}y{\\i}k, Erdem and Sadigh, Dorsa and Finn, Chelsea and Itti, Laurent",
        "title": "RoboCLIP: One demonstration is enough to learn robot policies"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "S3D",
        "author": "Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin",
        "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "HowTo100M",
        "author": "Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef",
        "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "RL-VLM-F",
        "author": "Wang, Yufei and Sun, Zhanyi and Zhang, Jesse and Xian, Zhou and Biyik, Erdem and Held, David and Erickson, Zackory",
        "title": "{RL-VLM-F}: Reinforcement Learning from Vision Language Foundation Model Feedback"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "GPT-4V",
        "author": "OpenAI",
        "title": "{GPT-4V}(ision) system card"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "CriticGPT",
        "author": "Liu, Jinyi and Yuan, Yifu and Hao, Jianye and Ni, Fei and Fu, Lingzhi and Chen, Yibin and Zheng, Yan",
        "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "PEBBLE",
        "author": "Lee, Kimin and Smith, Laura M and Abbeel, Pieter",
        "title": "{PEBBLE}: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "SURF",
        "author": "Jongjin Park and Younggyo Seo and Jinwoo Shin and Honglak Lee and Pieter Abbeel and Kimin Lee",
        "title": "{SURF}: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "RUNE",
        "author": "Xinran Liang and Katherine Shu and Kimin Lee and Pieter Abbeel",
        "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "Meta-Reward-Net",
        "author": "Liu, Runze and Bai, Fengshuo and Du, Yali and Yang, Yaodong",
        "title": "{Meta-Reward-Net}: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "hejna2023few",
        "author": "Hejna III, Donald Joseph and Sadigh, Dorsa",
        "title": "Few-shot preference learning for human-in-the-loop RL"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "SEER",
        "author": "Bai, Fengshuo and Zhao, Rui and Zhang, Hongming and Cui, Sijia and Wen, Ying and Yang, Yaodong and Xu, Bo and Han, Lei",
        "title": "Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "RAT",
        "author": "Fengshuo Bai and Runze Liu and Yali Du and Ying Wen and Yaodong Yang",
        "title": "{RAT}: Adversarial Attacks on Deep Reinforcement Agents for Targeted Behaviors"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "levine2020offline",
        "author": "Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin",
        "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems"
      },
      {
        "key": "IQL",
        "author": "Ilya Kostrikov and Ashvin Nair and Sergey Levine",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning"
      },
      {
        "key": "SEABO",
        "author": "Jiafei Lyu and Xiaoteng Ma and Le Wan and Runze Liu and Xiu Li and Zongqing Lu",
        "title": "{SEABO}: A Simple Search-Based Method for Offline Imitation Learning"
      },
      {
        "key": "OTDF",
        "author": "Jiafei Lyu and Mengbei Yan and Zhongjian Qiao and Runze Liu and Xiaoteng Ma and Deheng Ye and Jing-Wen Yang and Zongqing Lu and Xiu Li",
        "title": "Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "PT",
        "author": "Kim, Changyeon and Park, Jongjin and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin",
        "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "OPPO",
        "author": "Kang, Yachen and Shi, Diyuan and Liu, Jinxin and He, Li and Wang, Donglin",
        "title": "Beyond Reward: Offline Preference-guided Policy Optimization"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "IPL",
        "author": "Joey Hejna and Dorsa Sadigh",
        "title": "Inverse Preference Learning: Preference-based {RL} without a Reward Function"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "CPL",
        "author": "Joey Hejna and Rafael Rafailov and Harshit Sikchi and Chelsea Finn and Scott Niekum and W. Bradley Knox and Dorsa Sadigh",
        "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "FTB",
        "author": "Zhilong Zhang and Yihao Sun and Junyin Ye and Tian-Shuo Liu and Jiaji Zhang and Yang Yu",
        "title": "Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "PEARL",
        "author": "Runze Liu and Yali Du and Fengshuo Bai and Jiafei Lyu and Xiu Li",
        "title": "{PEARL}: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "CAMP",
        "author": "Xudong Yu and Chenjia Bai and Haoran He and Changhong Wang and Xuelong Li",
        "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "PiCor",
        "author": "Bai, Fengshuo and Zhang, Hongming and Tao, Tianyang and Wu, Zhiheng and Wang, Yanna and Xu, Bo",
        "title": "PiCor: Multi-Task Deep Reinforcement Learning with Policy Correction"
      }
    ]
  }
]