We compared $\dappl$'s MEU evaluation via BBIR to two existing approaches:

\begin{itemize}[leftmargin=*]
  \item \textit{Enumeration.}
  Every possible policy is enumerated, then evaluated according to the expected utility.
  % This strategy closely resembles an enumerative solution to solving the optimization problem
  % given in \ref{def:meu for dappl}, perhaps with additional room for
  % factorized representations of the program and dynamic programming to reduce the search space.
  We compare against \problog~2 as a representative of this strategy~\citep{de2007problog}.
  \item \textit{Order-constrained 2AMC approaches}.
  % The key hardness of MEU problems comes from the fact that maximization must happen
  % after summing; hence, most approaches to MEU work by sequentially summing and then
  % maximizing (as efficiently as possible).
  % \citet{kiesel2022efficient}
  % introduced a strategy for solving optimization-over-semiring problems
  % in which all the choice variables are sifted to the top of the variable ordering,
  % reducing the MEU problem into a model counting problem over two different semirings.
  \citet{derkinderen2020algebraic} introduced a state-of-the-art decision-theoretic \problog{}
  implementation that
  compiles programs into an order-constrained representation;
  we use this implementation as a representative strategy from this category.
\end{itemize}


Thus, we generated several benchmarks as both \dappl{} and \problog{} programs
to test the performance of the IRs.  As of yet there is no standard suite of
benchmarks for evaluating the MEU task, so we generated a new set of benchmarks
for validating performance.  Throughout our experiments we made a
best-effort attempt to write the most efficient programs in all languages.

\subsubsection{Bayesian Network Experiments}\footnote{Bayesian networks were selected
from \url{https://www.bnlearn.com/bnrepository/}.}
\label{sec:bn-eval}
  Bayesian networks are a well-established source of difficult, realistic,
  and useful probabilistic inference problems. It is straightforward to
  translate a Bayesian network into a \dappl{} or \problog{} program.
  However, Bayesian networks only represent probabilistic inference, and not decision making.
  We generated a standardized suite of challenging decision-theoretic problems on
  Bayesian networks by following the process in~\citet{derkinderen2020algebraic}.
  First, we transformed the root nodes of a Bayesian network into a decision.
  Then, if there were less than four decisions made through this process,
  each node of the Bayesian network was converted into a decision
  with probability 0.5. Utilities were added via one of two random methods:
  \begin{enumerate}[leftmargin=*]
    \item For each node in the Bayesian network, a utility of an integer
    between 0 and 100 was assigned with probability 0.8 for when the node yielded true,
    and assigned with probability 0.3 for when the node yielded false.
    \item We introduced five new ``reward nodes'' in the Bayesian network,
    on which rewards were assigned whether it was true or not.
    The reward nodes are true if and only if at least one of five randomly generated
    assignments to the existing nodes of the Bayesian network are true.
  \end{enumerate}
  We call the first utility assignment strategy ``Existing'',
  and the second strategy ``New nodes''.
  The Bayesian networks studied were Asia, Earthquake, and Survey,
  as they were the ones studied in previous work~\citep{derkinderen2020algebraic}.
  % \sh{say something about size; what's the biggest we can handle}
  % cant do this bc time constraint
Table \ref{table : bn} reports the performance of \dappl{} in comparison with
\dtproblog{}. We observe that \dappl~excels at computing the MEU over all three
Bayesian networks, across both methods to add utilities.
It is not surprising to
see an improvement over the enumerative strategy, but it is surprising to see that
the cost of
constraining the variable order to have choices-first is burdensome to the point
of timeout.
This is most likely because moving each choice to the
top of the order can lead to blowup, and this happens multiple times.

\subsubsection{Scaling Experiments}

In these experiments, we generate a family
of progressively larger examples to study how \dappl{} and \dtproblog{} scale
as the size of the example grows.
\begin{itemize}[leftmargin=*]
  \item \textit{Diminishing Returns (DR).}
  The scenario goes as this: we flip a coin with some bias.  If heads, we choose
  between 2-6 utilities, uniformly distributed between 0 and 100.  If tails, we
  flip another coin with another bias, but enter the same scenario.  This
  example scales in $n$ coin flips.
  This behavior is nicely modeled in \dappl{}: see the supplementary
  materials for example programs.
  The decision scenario has a simple solution to us: since each decision is independent
  of each other, it suffices to pick the choice maximizing the utility for each coin flip.
  \item \textit{One-shot faulty network ladder diagnosis (One-shot ladder).}
  We adapt a ladder network model
  as outlined in~\citet{holtzen2020scaling} into a decision--theoretic scenario.
  The network topology is visualized as follows: \begin{tikzpicture}[node distance=0.2cm, baseline=-1em]
  \node[] (init1) {$\dots$};
  \node[below = of init1] (init2) {$\dots$};
  \node[draw, circle, right=of init1] (S1) {};
  \node[draw, circle, right=of init2] (S2) {};
  \node[draw, circle, right=of S1] (S3) {};
  \node[draw, circle, right=of S2] (S4) {};
  \node[right=of S3] (final1) {$\dots$};
  \node[right=of S4] (final2) {$\dots$};

  \draw[->] (init1) -- (S1);
  \draw[->] (init2) -- (S2);

  \draw[->] (S1) -- (S3);
  \draw[->] (S1) -- (S4);

  \draw[->] (S2) -- (S3);
  \draw[->] (S2) -- (S4);

  \draw[->] (S3) -- (final1);
  \draw[->] (S4) -- (final2);
  % \draw[->] (S1) -- (S2);
  % \draw[->] (S1) -- (S3);
  % \draw[->] (S2) -- (S4);
  % \draw[->] (S3) -- (S4);
  % \draw[->] (S4) -- (final);
\end{tikzpicture}. Each circle represents a router, and each arrow
represents a link.
  We construct a ladder network with $2n$ routers, where $n$ is the scaling parameter. We observe that
  an incoming packet does not make it to the end of the network. Then,
  the task is to find a faulty router. If we choose a faulty router,
  then we obtain a reward uniformly distributed between 0 and 100;
  otherwise we receive a reward of 0.
  This benchmark is difficult as performing inference on the network is
  already quite difficult~\citep{holtzen2020scaling}
  but we additionally introduce a choice with $2n$ many alternatives.
  \item \textit{$k$-shot faulty network ladder diagnosis ($k$-shot ladder).}
  We keep the same ladder network as above,
  but if we fail to find a faulty router the first try,
  we can continue up until $k$ tries,
  where $k$ is less than the total number of nodes in the network ladder.
  This benchmark is the hardest, as the number of possible policies is factorial
  with respect to the number of nodes.
\end{itemize}
\begin{figure}
  \begin{subfigure}[t]{0.33\linewidth}
    \centering
     \begin{tikzpicture}
    \begin{axis}[
		height=3.5cm,
    ymin=0,
    ymax=1000000,
    % xmin=1,
		grid=major,
    width=4cm,
    % xmode=log,
    ymode=log,
  xtick={1,2,3,4,5,6,7,8},
    ytick={1,10,100,1000,10000,100000},
    xlabel={\# Columns in DR},
    ylabel={Time (ms)},
    legend columns=6,
    legend style={at={(13em,7em)},anchor=north}
    % legend style={at={(0.5,1)},anchor=south},
    ]
    \addplot[mark=triangle, thick] table [x index={0}, y index={1}] {\hmmdata};
    \addlegendentry{\dappl{}};
    \addplot[mark=x, thick, red] table [x index={0}, y index={2}] {\hmmdata};
    \addlegendentry{ProbLog 2};
    \addplot[mark=square, thick, blue] table [x index={0}, y index={3}] {\hmmdata};
    \addlegendentry{\citet{derkinderen2020algebraic}};
  \end{axis}
  \end{tikzpicture}

  % \includegraphics[width=\textwidth]{figs/dr.pdf}
  \caption{DR Benchmark.}
  \label{fig : dr}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.33\linewidth}
     \begin{tikzpicture}
    \begin{axis}[
		height=3.5cm,
    ymin=0,
    ymax=1000000,
    % xmin=1,
		grid=major,
    width=4cm,
    ymode=log,
    xtick={1,4,6,8,10},
    ytick={1,10,100,1000,10000,100000},
    xlabel={\# nodes in ladder},
    ylabel={Time (ms)},
    % legend style={at={(0.5,1)},anchor=south},
    ]
    \addplot[mark=triangle, thick] table [x index={0}, y index={1}] {\ladderlongdata};
    \addplot[mark=x, thick, red] table [x index={0}, y index={2}] {\ladderlongdata};
    \addplot[mark=square, thick, blue] table [x index={0}, y index={3}] {\ladderlongdata};
  \end{axis}
  \end{tikzpicture}

  % \includegraphics[width=\textwidth]{figs/long_ladder.pdf}
  \caption{One-shot ladder Benchmark.}
  \label{fig : 1shotladder}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.33\linewidth}
     \begin{tikzpicture}
    \begin{axis}[
		height=3.5cm,
    ymin=0,
    ymax=1000000,
		grid=major,
    width=4cm,
    ymode=log,
    xtick={1,2,3,4},
    ytick={1,10,100,1000,10000,100000},
    xlabel={Number of tries $k$},
    ylabel={Time (ms)},
    ]
    \addplot[mark=triangle, thick] table [x index={0}, y index={1}] {\ladderfour};
    \addplot[mark=x, thick, red] table [x index={0}, y index={2}] {\ladderfour};
    \addplot[mark=square, thick, blue] table [x index={0}, y index={3}] {\ladderfour};
  \end{axis}
  \end{tikzpicture}
  % \includegraphics[width=\textwidth]{figs/ladder_4.pdf}
  \caption{8-node $k$-shot ladder benchmark.}
  \label{fig : ladder4}
  \end{subfigure}
  \caption{Scaling results comparing \dappl{}, ProbLog 2, and \citet{derkinderen2020algebraic} on
  MEU tasks.
  The average number of choices in DR is $4 \times \text{\# of columns}.$
  The number of choices in one-shot ladder is twice the number of nodes.
  The number of choices in one-shot ladder is $\prod_{i \leq \text{\# of tries}} 8-(1-i)$.}
  \label{fig:scaling}
\end{figure}
{\footnotesize
\begin{table}
\caption{Comparison of different MEU tools on Bayesian network benchmarks.
  Time is in milliseconds (ms), with timeout 5 minutes = 300000ms.
  All reported times are the average over several runs;
  see the text for details.
  ``Avg. Times Pruned'' is the average number of times a partial policy (of any size)
  was not traversed in our randomly generated experiments.
  }
\begin{tabular}{lllll|l}
\toprule
\multicolumn{2}{c}{}  & $\dappl$ & \problog~2 & 2AMC &\\
Bayesian Network & Utility Method &  &  &  & Avg. Times Pruned\\
\midrule
\multirow[m]{2}{*}{Asia} & Existing & \textbf{1.4±0.3} & 28.6±11.4 & 86.5±40.8 & 7.6\\
 & New nodes & \textbf{6.0±0.3} & 53.4±5.5 & 119.2±20.7 & 4.7\\
\cline{1-6}
\multirow[m]{2}{*}{Earthquake} & Existing & \textbf{1.0±0.2} & 15.2±4.8 & 19.4±5.6 &3.0\\
 & New nodes & \textbf{2.4±0.2} & 33.2±2.3 & 24.6±4.5 & 3.7\\
\cline{1-6}
\multirow[m]{2}{*}{Survey} & Existing & \textbf{8.3±0.8} & 319.1±194.3 & 16532.8±1096.3 &3.4\\
 & New nodes & \textbf{103±0.8} & 182.3±43.2 & 19485.3±8173.8 & 2.7\\
\cline{1-6}
\bottomrule
\end{tabular}
\label{table : bn}
\end{table}
}
The results of these scaling experiments are reported in~\cref{fig:scaling}.
We observe that in the DR and one-shot ladder benchmarks,
\dappl{} feels the effects of its theoretical worst-case performance,
performing marginally better or worse than its competitors.
We believe that the primary reason \dappl{} scales poorly for these
examples is because
as the number of policies grow, there are many policies that are similar in expected reward yet incompatible,
decreasing opportunities for pruning.
% There are several reasons why \dappl~may
% suffer on this example.
% The first is that the expected utility of each policy requires inference
% on the ladder network, which is difficult to scale without the use of
% first-class functions and iterators \`a la Dice~\citep{holtzen2020scaling}.
% The second is that the number of alternatives is $2n$, and having an
% $\text{ExactlyOne}(2n)$ constraint in the IR greatly increases the treewidth,
% which is known to be a large hindrance in scaling inference problems~\citep{darwiche2002knowledge}.
On the contrary, we see that the order-constrained approach of 2AMC IR
is particularly performant on this task.
We believe this is because the structure of this problem is particularly
amenable to a constrained approach: the decision problem and the
ladder network can be defined almost entirely separately from each other,
resulting in an easier constraint on the order. Furthermore, bringing
all choice variables to the front of the order mitigates much of the treewidth
blowup faced in \dappl. In the future, we hope to synthesize the strengths
of order-constrainedness and our branch-and-bound approach to scale to
these examples that require exploiting this form of structure.

% \begin{wrapfigure}{R}{0.5\textwidth}
%   \includegraphics[width=0.5\textwidth]{figs/ladder_4.pdf}
%   \caption{$k$-shot ladder benchmark with eight nodes.}
%   \label{fig : ladder4}
% \end{wrapfigure}

% \sh{maybe we can transition a bit more delicately here; it feels like we just got a bit defensive
% and made the example harder. Let's motivate why this new problem is cool first.}
Next, we consider the sequential decision-making task of
diagnosing a faulty router, the $k$-shot ladder benchmark.  For a ladder with
eight nodes (four columns), we see that \dappl{} outscales 2AMC, although
neither were able to go past 3-shot ladder within the timeout. This example
was particularly challenging and performance was dependent
on our randomized
strategy for creating rewards
and heuristics for selecting where to branch first; due to this variability, \dappl{} timed out on
3 tries but successfully computed the MEU for 4.

\subsubsection{Gridworld: Scaling on Markov Decision Processes}
\label{sec:gridworld}
{\footnotesize
\begin{table}
\caption{Comparison of finite unrolling of Gridworld MDP benchmarks.
  The grid was an $n\times n$ grid of dimension $n$ with randomly generated start, finish, and obstacles.
  Time is in milliseconds (ms), with timeout 5 minutes = 300000ms.
  All reported times are the average over several runs;
  see \Cref{sec:gridworld} for details.}
\begin{tabular}{l|llll|llll|llll}
\toprule
Grid dim.  & \multicolumn{4}{c}{3} & \multicolumn{4}{c}{4} & \multicolumn{4}{c}{5} \\
\midrule
Horizon     & 1 & 2 & 3 & 4         & 1 & 2 & 3 & 4         & 1 & 2 & 3 & 4 \\
\midrule
$\dappl$    & \textbf{0.30} & \textbf{0.31} & \textbf{0.52} & \textbf{19.36} & \textbf{0.29} & \textbf{1.12} & \textbf{811.38} & \textbf{24511.99} & \textbf{0.30} & \textbf{0.81} & 21012.71 & \tiny{TO} \\
\problog~2  & 2.07 & 6.17 & 839.36 & 3904.34 & 2.95 & 41.51 & 7988.60 & \tiny{TO} & 2.92 & 176.95 & \tiny{TO} & \tiny{TO} \\
2AMC        & 0.49 & 5.20 & 61.96 & 183.10 & 0.88 & 50.36 & 12009.15 & 82836.61 & 1.40 & 41.03 & 24596.71 & \tiny{TO}\\
\bottomrule
\end{tabular}

\label{table : mdp}
\end{table}
}

Next we evaluate \dappl{}'s scalability on a the \emph{grid world} task, a
standard example commonly used to introduce Markov decision processes
(MDPs)~\citep{russell2016artificial}. The grid world task is defined as follows:
\emph{  A robot is in an $n \times n$ grid and starts at location $(0, 0)$. Some grid
  cells are \emph{traps}: if the robot enters these, it receives 0 utility and can
  no longer move, ending the simulation.
  Some grid cells are \emph{obstacles}: the robot cannot pass through these.
  One grid cell is a \emph{goal}: if the robot enters this cell, it receives a
  fixed positive utility. On each time step, the robot picks a direction (up,
  down, left, or right) to make a move. There is some probability that
  this move goes wrong: with probability $p$, the robot will accidentally move in a
  random wrong direction.}

\Cref{table : mdp} shows the results that compare \dappl{}, \problog{}~2, and
2AMC~\citep{derkinderen2020algebraic} on encodings of this example: \dappl{}
significantly outperforms these existing PPL-based approaches.

An alternative approach to solving the grid world example is to explicitly model
the problem as an MDP and solve for the optimal policy using a specialized MDP
solution method such as value iteration or policy
iteration~\citep{sutton2018reinforcement}. These MDP-specific approaches scale
much better than PPL-based approaches on this example: using value iteration,
the optimal policy can be solved on these small-scale MDPs in only a few
iterations, taking microseconds~\citep[Ch.  17]{russell2016artificial}.
However, like all inference strategies, MDP-specific solution methods have
tradeoffs that make them better for some problem instances and worse for others.
Value iteration and policy iteration excel at long-horizon low-dimensional
problems like the grid-world problem. For example, during value iteration, it is
only necessary to keep track of the expected utility of $n \times n$ states for
the grid world; this is quite feasibly represented as a matrix. However, MDPs struggle with
high-dimensional short-horizon decision-making
problems like those encoded by large Bayesian networks~\citep{holtzen2021model}:
in these problems, it is difficult for MDPs to efficiently reason about the
high-dimensional probability distribution on many random variables.
Additionally, the branch-and-bound approach is guaranteed to compute an
\emph{exact} optimal policy, while MDP solution strategies are not guaranteed to
produce the optimal policy unless they are run to a fixpoint, which can take an
unbounded number of iterations.

It is possible to use PPLs that do not support first-class decision-making as
part of an inner-loop for an MDP solving algorithm: this strategy is showcased
by WebPPL, where a probabilistic program computing the expected
utility of a fixed policy can be then used as an inner-loop for policy
evaluation during policy iteration~\citep{agentmodeling,dippl}. Such
specialized MDP-solutions, this approach scales quite well on the grid world
examples, completing in milliseconds.  However, WebPPL struggles to perform
inference on Bayesian networks (see \citet[Fig. 10]{holtzen2020scaling}), and so
this strategy cannot scale to the high-dimensional decision-making problems
considered in \Cref{sec:bn-eval}.
