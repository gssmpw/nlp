First, we will define formally the MEU and MMAP problems
as well as demonstrate the core ideas behind BBIR via two illustrative examples.
The first example in $\dappl$ (\cref{subsec:dappl-overview}) will show
the generalization of the \textit{reasoning-via-compilation} scheme
to lattice semirings, BBIR's theoretical foundation.
The second example in $\pineappl$ (\cref{subsec:pineappl-overview}) will illustrate
how we can model mid-program optimization through the BBIR via staging.

\subsection{The Maximum Expected Utility Problem}\label{subsec:dappl-overview}
In this section, we first introduce the maximum expected utility (MEU) problem
through example (\cref{subsubsec:meu-example}). Then we describe our
approach to solving MEU via compilation (\cref{subsubsec:eu-via-compilation}
and \cref{subsubsec:optimization-via-compilation}).

\subsubsection{Defining MEU}\label{subsubsec:meu-example}

\begin{wrapfigure}{r}{0.3\linewidth}
  \begin{dapplcodeblock}[basicstyle=\tiny\ttfamily]
rainy <- flip 0.1;
// observe rainy ;
choose [Umb, No_umb]
| Umb -> if rainy then
    reward 10 else reward -5
| No_umb -> if rainy then
    reward -100 else ()
\end{dapplcodeblock}
\caption{Example $\dappl$ program.}
\label{fig:motivation-dappl}
\end{wrapfigure}
Consider the following simple decision-making scenario that we model as a $\dappl$ program
in~\cref{fig:motivation-dappl}.

\begin{quote}
\textit{``Today there is a 10\% chance of rain.
  If it rains and you have your umbrella, you are dry and happy.
  If it rains and you do not have your
  umbrella, you are very unhappy. However, you prefer not to carry your umbrella,
  so you are mildly annoyed if it does not rain and you brought your umbrella.
  Should you bring your umbrella?''}
\end{quote}

\Cref{fig:motivation-dappl} shows how we encode this scenario in \dappl{}.
On Line 1 (indicated on the right of~\cref{fig:motivation-dappl}),
we model the fact that there is a $10\%$ chance of rain via the syntax
\dapplcode{flip 0.1}, which outputs $\tt$ with probability $0.1$ and $\ff$
otherwise; in \dappl{}, all random variables are finite and discrete. The syntax
\dapplcode{choose [Umb, No\_umb]} on Line 3 denotes
a non-deterministic \emph{choice} about whether
or not to bring an umbrella; similar to random variables, all choices must be
finite and discrete. On Lines 5 and 7, we assign rewards to specific outcomes with
the \dapplcode{reward} keyword, which is an effectful operation that
accumulates a reward when it is executed: in this case, the outcome of ``it is raining and I brought
my umbrella'' is assigned a reward of $10$ and the outcome of
``it is not raining and I brought my umbrella'' is assigned a reward of $-5$.

The goal of a \dappl{} program is to compute the assignment to all choices --
i.e., the \emph{policy} -- that maximizes the expected accumulated reward.
In \dappl{}, all policies are deterministic.
For
the program in~\cref{fig:motivation-dappl}, there are two possible policies: $\pi_1
= \texttt{Umb}$, where the umbrella is taken, and $\pi_2 =
\texttt{No\_umb}$ where it is not.  Given a policy $\pi$, we let
$e|{_\pi}$ denote the \dappl{} program that results from substituting all
choices in the program $e$ for their corresponding policies $\pi$.  Then, we can
define an evaluation function $\EU(e|_{\pi}) = v$ for a \dappl{} program $e$
under a fixed specific policy $\pi$ that yields the expected utility $v$ of that
policy; we make this precise in Section~\ref{subsec:util}.
With this in mind, we can compute the maximum expected utility of our example,
call it $ex$, by comparing the expected utility of the two policies:

{\footnotesize
\begin{equation}\label{eqn:example}
  \MEUfn {ex} =
    \max \bigg\{k_1, k_2 :\begin{gathered}
    \EU(ex|_{\pi_1})= k_1, \\
    \EU(ex|_{\pi_2})= k_2
  \end{gathered}\bigg\} =
    \max\bigg\{\begin{gathered}
    \overbrace{0.1 \times 10}^{\texttt{rainy}=\tt}  + \overbrace{0.9 \times (-5)}^{\texttt{rainy}=\ff} \\
    \underbrace{0.1 \times (-100)}_{\texttt{rainy}=\tt} + \underbrace{0.9 \times 0}_{\texttt{rainy}=\ff}
    \end{gathered}\bigg\} =
  \max \{-3.5,-10\} =-3.5.
\end{equation}
}

However, if we choose to uncomment Line 2 of~\cref{fig:motivation-dappl},
we add to our scenario that we \textit{observe} that it is raining today.
Thus, to compute MEU we must compute the \textit{conditional expected utility}
of each of our policies given that it is raining.
If we say $ex_{\texttt{obs}}$ as our motivating example with the \dapplcode{observe},
then we can now compute the MEU \textit{conditional on the fact that it is raining},
which yields a different answer than that of~\cref{eqn:example}:

{\footnotesize
\begin{equation}\label{eqn:example-with-observe}
  \MEUfn {ex_{\texttt{obs}}} =
    \max \bigg\{k_1, k_2 :\begin{gathered}
      \EU(ex_{\texttt{obs}}|_{\pi_1})= k_1, \\
      \EU(ex_{\texttt{obs}}|_{\pi_2})= k_2
  \end{gathered}\bigg\} =
    \max\bigg\{\begin{gathered}
    \overbrace{1 \times 10}^{\texttt{rainy}=\tt}  + \overbrace{0 \times (-5)}^{\texttt{rainy}=\ff} \\
    \underbrace{1 \times (-100)}_{\texttt{rainy}=\tt} + \underbrace{0 \times 0}_{\texttt{rainy}=\ff}
    \end{gathered}\bigg\} =
  \max \{10,-100\} =10.
\end{equation}
}

% \diff{We conclude this section with a brief discussion of some of the language design
% decisions we made in \dappl{}. There is a long history of
% approaches to solving such decision-making problems under uncertainty including
% Markov decision processes (MDPs)~\citep{sutton2018reinforcement} and influence
% diagrams~\citep{howard2005influence,sanner2010relational}; we discuss these in
% more detail in \Cref{sec:related-work}. \dappl{} focuses on a particular subset of
% decision-making problems: \emph{discrete finite-horizon decision-making
% problems with deterministic policies}. Such decision-making problems are quite common in diagnosis and
% planning, and have typically been represented using decision-theoretic Bayesian networks~\citep[Ch. 16]{russell2016artificial}. It is important to note what kinds
% of program features \dappl{} deliberately does not support: there is no support
% for continuous random variables or decisions, no support for dynamically bounded or
% almost-surely terminating loops, and it is not possible to make observations that
% depend on the results of decisions. These restrictions do limit \dappl{}'s
% expressivity -- it cannot represent continuous planning problems such as those
% in robotics, and it cannot solve optimal value-of-information problems where the goal
% is to decide what kinds of events to observe~\citep[\S 16.6]{russell2016artificial}.
% But, in exchange for this loss in expressivity, \dappl{} supports an extremely
% efficient optimal decision-making strategy. This exact tradeoff is quite useful
% since there are many problems that do not need these expressive features.}
% Next, we describe this new approach.
% The remainder of~\cref{subsec:dappl-overview}
% will consider~\cref{fig:motivation-dappl} without \dapplcode{observe};
% see~\cref{subsubsec:meu} for a detailed incorporation of conditioning in Boolean compilation.

\subsubsection{Expected Utility of Boolean Formulae}\label{subsubsec:eu-via-compilation}
Now we begin working towards our new approach to scaling MEU for \dappl{}
programs. The core of our approach is to compile a \dappl{} program into a data
structure for which computing upper-bounds on the expected utility of partial
policies is \emph{efficient in the size of the compiled representation}.  Our
approach is a generalization to the recent approaches to performing
probabilistic inference via knowledge compilation, which is currently the
state-of-the-art approach for performing exact discrete probabilistic
inference~\citep{holtzen2020scaling,fierens2015inference}.  The idea with
inference via knowledge compilation is to reduce the problem of inference to
performing a weighted model count of a Boolean formula, for which there exist
specialized scalable solutions. Formally, a \emph{weighted Boolean formula} is a
pair $(\varphi, w)$ where $\varphi$ is a logical formula and $w$ is a function
that maps literals (assignments to variables in $\varphi$) to real-valued
weights.  A \emph{model} $m$ is a total  assignment to variables in $\varphi$
that satisfies the formula.  The weight of a model $m$ is the product of the
weights of each literal.  Then, the \emph{weighted model count}
$\texttt{WMC}(\varphi, w)$ is defined to be the sum of weights of each model of
$\varphi$, i.e. \texttt{WMC}$(\varphi, w) \triangleq \sum_{m \models \varphi} w(m)$.

\citet{holtzen2020scaling} showed how to reduce probabilistic inference for a
small language similar to \dappl{} (but without decisions or rewards) to weighted model counting. However,
our problem is MEU, not probabilistic inference; to connect these ideas,
we leverage a well-known generalization of WMC that allows one to instead
perform weighted model counts where the weights come from an
arbitrary \emph{semiring}~\citep{kimmig2017algebraic,kimmig2011algebraic}:

\begin{definition}[Semiring]\label{def:semiring}
  A semiring is a tuple $\mathcal R = (R, \oplus, \otimes, \mathbf{1}, \mathbf{0})$
  where $R$ is a set, $\oplus$ is a commutative monoid on $R$ with
  unit $\mathbf{0}$, $\otimes$ is a monoid on $R$ with unit $\mathbf{1}$,
  $\mathbf{0}$ annihilates $R$ under $\otimes$, and $\otimes$ distributes over $\oplus$.
\end{definition}

This invites a natural definition of an algebraic model count where
literals are permitted to be weighted by elements of a semiring instead of the real numbers,
similar to \emph{weighted programming}~\citep{batz2022weighted}:
\begin{definition}[Algebraic model counting~\citep{kimmig2011algebraic,kimmig2017algebraic}]
\label{def:amc}
  Let $\varphi$ be a propositional formula, $\vars(\varphi)$ be the variables in $\varphi$,
  and $\lits(\varphi)$ denote the set of literals for variables in $\varphi$.
  Let $w : \lits(\varphi) \rightarrow \mathcal R$ be a weight function that
  maps literals to a weight in semiring $\mathcal R$.
  Then, the \emph{weight of a model of $\varphi$} is the product of the weights
  of the literals in that model: i.e., for some model $m$ of $\varphi$,
  we define $w(m) = \bigotimes_{\ell \in m} w(l)$.
  Then, the \emph{algebraic model count} is the weighted sum of models of $\varphi$:
  \begin{align}\label{eq:amc}
    \AMC(\varphi, w) \triangleq \bigoplus_{m \models \varphi} w(m).
  \end{align}
\end{definition}

Now we illustrate how we reduce computing the MEU of the \dappl{}
program in~\Cref{fig:motivation-dappl} to performing an algebraic
model count of a particular formula.
We
construct formulae with two kinds of Boolean variables: \emph{probabilistic variables}
and \emph{reward variables} that indicate
whether or not the agent receives a reward.
In~\cref{fig:motivation-dappl}, we have a single probabilistic variable $r$ that is true if and only if it is \texttt{rainy},
and three reward variables $R_v$ that are
true if and only if the agent receives a reward of $v$. Then, we can give a
Boolean formula $\varphi_{u}$ and $\varphi_{\overline{u}}$ for the two policies
of bringing and not bringing an umbrella respectively:\footnote{We write the
negation of a variable using an overline.}
\begin{align}
  \varphi_u &= (r \land R_{10} \land \overline{R_{-5}} \land \overline{R_{-100}}) \lor (\overline{r} \land \overline{R_{10}} \land {R_{-5}} \land \overline{R_{-100}}) \label{eq:formula-umbrella} \\
  \varphi_{\overline{u}} &= (r \land \overline{R_{10}} \land \overline{R_{-5}} \land {R_{-100}}) \lor (\overline{r} \land \overline{R_{10}} \land \overline{R_{-5}} \land \overline{R_{-100}})
  \label{eq:formula-no-umbrella}
\end{align}

% \textit{How can one efficiently compute the expected utility from a Boolean formula?}
% To answer this question,
% let us recall that in knowledge compilation~\citep{darwiche2002knowledge,holtzen2020scaling,de2007problog},
% probabilistic inference over a formula is done by assigning a probability weight to each literal in the formula,
% now called a \textit{weighted Boolean formula}, at which point inference specializes to the task of
% \textit{weighted model counting}.
% As now there are rewards involved, we need to weigh literals by more than just probabilities:
% we need to weight them by a suitable semiring.

Continuing with our reduction,
we can now encode expected utility computations as an algebraic model count
over a particular kind of semiring, the expectation semiring:

\begin{definition}[Expectation semiring~\citep{eisner2002parameter}]
\label{def:expectation semiring}
  The expectation semiring
  $\mathcal S$
  is a semiring on a base set $S = \R^{\geq 0} \times \R$, where the first component
  is a probability and the second represents expected utility.
  Addition is defined component-wise $(p,u) \oplus (q,v) \triangleq (p+q, u+v)$,
  multiplication defined as $(p,u) \otimes (q,v) \triangleq (pq, pv+qu)$,
  the multiplicative unit is $\mathbf{1} \triangleq (1,0)$,
  and the additive unit is $\mathbf{0} \triangleq (0,0)$.
\end{definition}

To continue the reduction, we want to design an algebraic model count for
$\varphi_u$ that computes the expected utility of the policy for bringing an
umbrella. To do this, we give weights to each literal:
\begin{align*}
  w(r) = (0.1, 0) \quad& w(R_{10}) = (1, 10) \quad& w(R_{100}) = (1, 100) \quad& w(R_{-5}) = (1, -5)\\
  w(\overline{r}) = (0.9, 0) \quad& w(\overline{R_{10}}) = (1, 0) \quad& w(\overline{R_{100}}) = (1, 0) \quad& w(\overline{R_{-5}}) = (1, 0)
\end{align*}
Intuitively, since $r$ represents the outcome of \texttt{flip 0.1} being true, it has a
probability component of $0.1$ and a reward component of 0.
These weights are carefully designed so that the algebraic model count computes
the expected utility of the policy:
\begin{align}
  \AMC(\varphi_u, w)
  &= \Big(\underbrace{(0.1,0)  \otimes (1,10) \otimes (1, 0) \otimes (1, 0)}_{r,\ R_{10},\ \overline{R_{-5}},\ \overline{R_{-100}}} \Big)
  \oplus \Big(\underbrace{ (0.9,0) \otimes  (1, 0) \otimes(1,-5) \otimes (1, 0)}_{\overline{r},\ \overline{R_{10}},\ R_{-5},\ \overline{R_{-100}}}\Big) \nonumber \\
  & = (0.1, 1) \oplus (0.9, -4.5) = (1, -3.5).
  \label{eq:ex-amc}
\end{align}
% We capture in~\cref{appendix:amc invariant}
% precisely how the algebraic model count aligns with the traditional
% expectation-based definition of
% expected utility shown in~\cref{subsubsec:meu-example}.
% Repeating the same process for the policy \texttt{not\_umbrella} and comparing the expected utility as
% given by the AMC would yield the MEU. We can make this precise:

\input{tikz/bdd_example.tex}
At this point in the reduction we are left with an arbitrary AMC, which in general is \#P-hard~\cite{kimmig2017algebraic}; it seems like we have not yet
made progress. This is where knowledge compilation comes into
play~\cite{darwiche2002knowledge,chavira2008probabilistic, sang2005performing}.
The key idea of knowledge compilation is to compile Boolean formulae into representations
that support particular queries: for instance, \dice{} compiles Boolean formulae into
binary decision diagrams (BDDs), which support linear-time weighted model counting,
in order to perform inference.
This compilation is expensive, but once performed, inference is efficient in the
size of the result; this amortization benefit will be crucial for our subsequent search strategy.
This process scales well because
BDDs naturally exploit repeated
sub-structure in the program such as conditional independence.
\citet{kimmig2017algebraic} showed that an analogous knowledge compilation strategy
can also be used to solve algebraic model counts.
This is visualized in Figure~\ref{fig:bdd umbrella}, which shows a
compiled representation of \cref{eq:formula-umbrella} (where we
have elided the negated reward variables for space).
Fig.~\ref{fig:policy umbrella} shows how to interpret the BDD in
Fig.~\ref{fig:bdd umbrella} as a circuit compactly representing $\AMC$.
The leaves of the circuit are elements of the
expectation semiring $\mathcal S$, and nodes are semiring operations $\oplus$
and $\otimes$, instead of the real-valued operations $+$ and $\times$.
% To construct this circuit from a BDD, we associate each internal variable
% node with a sum-over-products that multiplies the weight of the variable with
% the circuit representing its children. For instance, the left path from the root
% $\oplus$ node in Figure~\ref{fig:policy umbrella} corresponds to the BDD
% path where $r$ is true in Figure~\ref{fig:bdd umbrella}.
The algebraic model
computation is shown in gray, and only requires a linear-time
bottom-up pass of the graph, mirroring the weighted model count.
In Section~\ref{sec:eval}, we will show that we
can compile very large \dappl{} programs into surprisingly compact circuits
due to the opportunities for structure sharing.

% As intuition, for a WBF $(\varphi, w)$ over the
% expectation semiring, the components of the weight $(a,b) = w(\ell)$ for a literal $\ell$
% can be interpreted as
% To reflect this intuition, we write, for $s \in \mathcal S$, $s_{\Pr} = \mathrm{proj}_1 \ s$
% and $s_{\EU} = \mathrm{proj}_2 \ s$, where $\mathrm{proj}_1$ and $\mathrm{proj}_2$
% are the first and second projections, respectively.
% How does AMC over the expectation semiring accurately capture expected utility of a WBF?
% To answer this question precisely, we first must define what an expected utility of a WBF
% is in the first place.

% \begin{definition}\label{def:wbf-induced pr and eu}
%   Let $(\varphi,w)$ be a WBF.
%   Let $\Omega$ be the set of all assignments to variables in $\varphi$. The
%   distribution on $\Omega$
%   induced by the weight map is defined by, for each $m \in \Omega$:
%   \begin{align}\label{eq:induced probability}
%     \Pr(m) = \prod_{\ell \in m} w(\ell)_{\Pr} = \left[\bigotimes_{\ell \in m} w(\ell)\right]_{\Pr}.
%   \end{align}
%   Then, the expected utility of $(\varphi,w)$ is
%   \begin{align}\label{eq:induced eu}
%     \EU[(\varphi,w)] = \sum_{m \models \varphi }\Pr(m) \paren{\sum_{\ell \in m} \frac{w(\ell)_{\EU}}{w(\ell)_{\Pr}}}.
%   \end{align}
% \end{definition}

% To give intuition as to why Equation~\ref{eq:induced eu}
% is a suitable definition for expected utility,
% we remark that it closely resembles the expected value
% of the event that $m$ models $\varphi$, with each outcome $m$
% being assigned a utility of the sum of its literals, unnormalized
% (recall that for $(a,b) \in \mathcal S$, $b$ is scaled accordingly to $a$).


% \begin{theorem}\label{thm:amc invariant}
%   Let $(\varphi,w)$ be a WBF over $\mathcal S$. Then
%   \begin{equation}\label{eq:amc invariant}
%     \AMC_{\mathcal S}(\varphi, w) = \paren{\sum_{m \models \varphi} \Pr(m), \EU[\varphi]}.
%   \end{equation}
% \end{theorem}




% \sh{todo make this a lot nicer, it's critical we explain this well, connect back to running example}
% It is not at all obvious on first glance how the expectation semiring relates to
% the computation of expectations, so we will work through an example.  First,
% some intuition: for a pair $(p,u) \in \mathcal S$, the first component denotes
% an unnormalized probability, and the second an accumulated expectation.  For instance consider two semiring elements $A
% = (1, 10)$ and $B = (0.1, 0)$ of $\mathcal{S}$.
% The element $A$ denotes
% Then, $\otimes$ denotes
% sequential composition of $A$ and $B$: $A \otimes B = (1 \times 0.1, 1 \times 0
% + 0.1 \times 10) = (0.1, 1)$: the second component accumulates a new expected utility,
% which is now 1, which reflects the fact that ...

% The addition operation $oplus$ denotes the
% unnormalized probability and expectation of the disjunction of two events and
% $\otimes$ denotes the probability and expectation of the conjunction of two
% events.


\subsubsection{Optimization-via-Compilation}\label{subsubsec:optimization-via-compilation}
At this point, we know how to use algebraic model counting to compute the expected utility
of a particular policy, but we do not yet know how to efficiently \emph{search for an optimal policy}.
We now return to our task of finding the \emph{optimal policy}
for a $\dappl$ program, which is our key new novelty.
A na\"{i}ve approach can be to
associate a Boolean formula to every policy as in~\cref{subsubsec:eu-via-compilation},
compute the expected utilities via $\AMC$, then find the maximum over this collection.
However, this approach is clearly exponential in the number of decisions and wasteful:
it unnecessarily recompiles the same sub-program into a BDD
numerous times, even if it is shared across the different policies.
What we
desire is a \emph{single compilation pass} on which to do repeated
efficient evaluation of different policies for \dappl{} programs.

% \begin{definition}[MEU of a Boolean Formula]
% \label{def:meu boolean formula}
%   Let $\varphi$ be a Boolean formula that consists of reward variables
%   $\{R_i\}$, probabilistic variables $\{P_j\}$,
%   decision variables $\{D_k\}$, utility map $U$, and probability map $\Pr$.
%   Then, the maximum expected utility of $\varphi$ is:
%   \begin{align}
%     M\EU(\varphi) \triangleq \max_{\{d_k\}} \EU[\varphi \mid \{d_k\}]
%   \end{align}
%   where $\varphi \mid \{d_k\}$ denotes setting $D_k = d_k$ in $\varphi$.
%   Applying Lemma~\ref{thm:amc invariant}, we can give an alternative definition
%   \begin{align}
%     M\EU(\varphi) \triangleq \max_{\{d_k\}} \AMC(\varphi | \{d_k\},w)_{\EU}.
%   \end{align}
%   We call each $d_k$ a \emph{policy}, with the witness for $\MEU(\varphi)$ being the
%   \emph{optimal policy.}
%   \label{def:meu on wbf}
% \end{definition}

% In Section~\ref{subsubsec:meu}, we extend Definition~\ref{def:meu boolean formula}
% to handle evidence.
% This reduction enables us to write down a single Boolean formula that represents
% the entire state-space of our example program, including decisions and
% probabilistic outcomes. Figure~\ref{fig:motiv-a-bdd} shows a BDD representing
% this formula for the motivating example in Figure~\ref{fig:motiv-a}: this BDD contains
% a Boolean variable $u$ that is true if and only if an umbrella is brought.

\begin{figure}
  \begin{subfigure}{0.55\linewidth}
    \centering
    \scalebox{0.9}{
    \begin{tikzpicture}
        \def\lvl{20pt}
      \node (rainy) at (0, 0) [bddnode] {$r$};

      \node (u2) at ($(rainy) + (45bp, -\lvl)$) [bddnode] {$u$};
      \node (u) at ($(rainy) + (-45bp, -\lvl)$) [bddnode] {$u$};


      \node (r1) at ($(u) + (-28bp, -\lvl)$) [bddformula] {$R_{10} \land \overline{R_{-100}}\land \overline{R_{-5}}$};
      \node (r2) at ($(u) + (28bp, -\lvl-20)$) [bddformula] {$\overline{R_{10}} \land R_{-100}\land \overline{R_{-5}}$};

      \node (r21) at ($(u2) + (-28bp, -\lvl)$) [bddformula] {$\overline{R_{10}} \land \overline{R_{-100}}\land R_{-5}$};

      \node (r22) at ($(u2) + (28bp, -\lvl-20)$) [bddformula] {$\overline{R_{10}} \land \overline{R_{-100}} \land \overline{R_{-5}}$};
      % \node (r22) at ($(u2) + (28bp, -\lvl)$) [bddterminal] {$\false$};

      \begin{scope}[on background layer]
        \draw [highedge] (rainy) -- (u);
        \draw [lowedge] (rainy) -- (u2);
        \draw [highedge] (u) -- (r1);
        \draw [lowedge] (u) -- (r2);
        \draw [highedge] (u2) -- (r21);
        \draw [lowedge] (u2) -- (r22);
      \end{scope}
      \end{tikzpicture}
    }
    \caption{State-space of~\cref{fig:motivation-dappl} as a partially-rendered BDD.}
    \label{fig:motiv-a-bdd}
    \end{subfigure}
\hfill
\begin{subfigure}{0.4\linewidth}
  \centering
  \scalebox{0.9}{
  \begin{tikzpicture}[every node/.style={inner sep=0,outer sep=0}]
      \def\lvl{20pt}

    \node (root) at (0, 0) [bddnode] {$\bigoplus$};
    \node at ($(root) + (20bp, 0)$) {{\color{gray} (1, 1)}};

    \node (mul1) at ($(root) + (-40bp, -\lvl)$) [bddnode] {$\bigotimes$};
    \node at ($(mul1) + (-23bp, 0)$) {{\color{gray} (0.1, 1)}};
    \node (mul2) at ($(root) + (40bp, -\lvl)$) [bddnode] {$\bigotimes$};
    \node at ($(mul2) + (23bp, 0)$) {{\color{gray} (0.9, 0)}};

    \node[bddformula] (term5) at ($(mul1) + (20bp, -\lvl)$) {$(0.1, 0)$};
    \node[bddformula] (term6) at ($(mul2) + (-20bp, -\lvl)$) {$(0.9, 0)$};

    \node (cup) at ($(mul1) + (-10bp, -\lvl)$) [bddnode] {\tiny{$\max$}};
    \node at ($(cup) + (-20bp, 0)$) {{\color{gray} (1, 10)}};
    \node (cup2) at ($(mul2) + (10bp, -\lvl)$) [bddnode] {\tiny{$\max$}};
    \node at ($(cup2) + (20bp, 0)$) {{\color{gray} (1, 0)}};


    \node[bddformula] (term1) at ($(cup) + (-20bp, -\lvl)$) {$(1, 10)$};
    \node[bddformula] (term2) at ($(cup) + (20bp, -\lvl)$) {$(1, -100)$};

    \node[bddformula] (term3) at ($(cup2) + (-20bp, -\lvl)$) {$(1, -5)$};
    \node[bddformula] (term4) at ($(cup2) + (20bp, -\lvl)$) {$(1, 0)$};

    \begin{scope}[on background layer]
      \draw [highedge] (root) -- (mul1) ;
      \draw [highedge] (root) -- (mul2);
      \draw [highedge] (mul1) -- (term5);
      \draw [highedge] (mul2) -- (term6);
      \draw [highedge] (mul1) -- (cup);
      \draw [highedge] (mul2) -- (cup2);
      \draw [highedge] (cup) -- (term1);
      \draw [highedge] (cup) -- (term2);
      \draw [highedge] (cup2) -- (term3);
      \draw [highedge] (cup2) -- (term4);
    \end{scope}
    \end{tikzpicture}
  }
  \caption{Faulty branch and bound circuit. The correct version uses $\sqcup$ instead of $\max$.}
  \label{fig:bb circuit example}
  \end{subfigure}
\caption{Branch-and-bound intermediate representation for the example program in~\cref{fig:motivation-dappl}.}
\label{fig:bbir-for-motivation-dappl}
\end{figure}

This leads to one of our main contributions: a new intermediate representation we call
the branch-and-bound intermediate representaion (BBIR). The example in
Figure~\ref{fig:motiv-a-bdd} already solves the problem of unnecessarily
repeatedly recompiling sub-programs: we can perform policy search directly on
the BDD by exhaustively enumerating all possible assignments to decision
variables and computing an expected utility using the method outlined in~\cref{subsubsec:eu-via-compilation}.
However, this enumeration strategy still suffers from
search-space explosion, and is exponential in the number of decision variables.
To avert this and scale to \dappl{} programs with a large number of choices, we
leverage the compiled BDD in Figure~\ref{fig:motiv-a-bdd} to efficiently compute
upper-bounds on the expected utility of \emph{partial policies},
defined formally in~\ref{def:partial policy}.
This lets us
design a branch-and-bound algorithm in Section~\ref{sec:bbir} to prune
policies during search.

Let us illustrate why a branch-and-bound algorithm is necessary and
a single bottom-up pass, such as the one in~\cref{subsubsec:eu-via-compilation},
is not sufficient.
Consider the circuit description of Figure~\ref{fig:motiv-a-bdd} that efficiently encodes a
solution to our decision scenario. A
straightforward approach to find MEU may be to associate every decision node in
the BDD with a $\max$ operation,
where $\max$ selects the higher utility node.
This circuit is visualized in Figure~\ref{fig:bb circuit example}.

However, there is a problem with the circuit in Figure~\ref{fig:bb circuit example}!
Recall the computations in~\cref{eqn:example}. The
maximum is the \emph{very last} operation
performed in the computation of MEU, performed over all decision variables.
In the bottom-up computation of the circuit in Figure~\ref{fig:bb circuit example},
the maximum is the \emph{very first} operation.
Thus this circuit will compute the wrong answer, as it is
\emph{not} generally the case that $\max_x \sum_y f(x,y) = \sum_y \max_x f(x,y)$
for an ordered semiring-valued function $f$, even in the real setting.
To solve this problem, we can force all
decision variables occur first in the top-down variable order
of the BDD, forcing maximums the final operations taken.
This is the approach
taken by the \emph{two-level algebraic model counting (2AMC)} approach of
\citet{derkinderen2020algebraic}.
As we will show in Section~\ref{sec:eval},
this order constraint can be catastrophic for performance, as
the size of a BDD is very sensitive to the variable order, and hence compiling
to order-constrained BDDs scales very poorly compared to compiling to BDDs where
the variables can be optimally ordered.

Our main contribution, in Section~\ref{sec:bbir}, gives a circuit representation
for upper-bounding the utility of a partially assigned policy
without constraining the variable order during BDD
compilation.
Our approach relies on the following intuition: for a \emph{real-valued} function $f$,
while it not generally the case that $\max_x \sum_y f(x,y) = \sum_y \max_x f(x,y)$, it
\emph{is the case} that $\max_x \sum_y f(x,y) \le \sum_y \max_x f(x,y)$:
commuting sums and maxes yields upper bounds for the real semiring $\R$.
This powerful \emph{commuting bound} holds for the reals,
and more broadly semirings with
a join-semilattice structure:
we verify this intuition via a lemma in~\cref{appendix:commute join}.

% A similar observation is leveraged by \citet{huang2006solving} to
% design a highly effective branch-and-bound strategy for solving the marginal-map problem
% for Bayesian networks; see Section~\ref{subsubsec:mmap} for a discussion.
% Suppose we have two partial-policies $\pi_1$ and $\pi_2$

\begin{definition}[Lattice semiring]\label{def:latticed semiring}
  A \textbf{lattice semiring} is a semiring $\mathcal S = (S, \oplus, \otimes,
  \mathbf{0}, \mathbf{1})$ equipped with a partial order $\sqsubseteq$ on
  $S$ respecting $\oplus$ -- i.e., if $a \plt b$ and $c \plt d$, then
  $a \oplus c \plt b \oplus d$ -- that admits both meets (greatest lower bounds,
  denoted $\sqcap$) and joins (least upper bounds, denoted $\sqcup$).
\end{definition}

If $f$ is a lattice-semiring-valued function, $\bigsqcup_x \bigoplus_y
 f(x,y) \le  \bigoplus_y \bigsqcup_x f(x,y)$.  For the expectation
semiring, we define the partial order pointwise: $(a, b) \plt (c, d)$ if and
only if $a \le c$ and $b \le d$. This implies that $(a, b) \sqcup (c, d) =
(\max(a, c), \max(b, d))$, and similarly so for meets.

Returning to our goal of using BDDs to efficiently compute upper-bounds on
utilities, we can interpret decision variables as joins: this computation is visualized in
Figure~\ref{fig:bb circuit example}. The computed upper-bound is visualized in gray;
the final computed upper-bound $(1,1)$ is indeed an upper-bound
(with respect to $\sqsubseteq$)
on the expected
utility of the optimal policy, which we expect to be $(1, -3.5)$.
Ultimately, this insight allows us to give a branch-and-bound procedure
to solve both a general class of optimization problems over probabilistic
inference. Next, we will show another instantiation of this framework
for solving maximum marginal a-posteriori (MMAP) problems.

% Ultimately, in Algorithm~\ref{algorithm:bb} we give a branch-and-bound procedure
% for solving max-sum problems defined on branch-and-bound semirings: we will show
% that this broad class of problems includes maximum expected utility, marginal
% maximum a-posteriori, and maximum sensitivity (see Section~\ref{subsec:msp
% jsp}). This branch-and-bound uses the partial order given in
% Definition~\ref{def:latticed semiring} to prune the policy space, and the
% partial-order compatibility requirement ensures that the optimal policy cannot
% be pruned during this search. In Section~\ref{sec:eval}, we will show that this
% search policy works very well in practice, and strikes a new and useful
% scalability tradeoff for exactly solving MEU.

% Then, we aim
% to solve
% the following optimization problem:


% The intuition behind Definition~\ref{def:branch-and-bound semiring}
% is closely tied to the \textit{computation}
% of the algebraic model count. Recall that the algebraic model count
% over a semiring takes the form $\bigoplus \bigotimes w(\ell)$.
% Say we wanted to choose the more desired value between two AMCs.
% If we knew the values of all $w(\ell)$, then we can simply
% compute the algebraic model counts and pick the more desired value.
% Thus $\leq$ is a total order that allows selection between
% ``fully computed values''.
% If not, and only some values of $w(\ell)$ are not known,
% we can at best \textit{partially compute} the
% algebraic model count, and compare the resulting expression.
% Sometimes a desired value will still be able to be selected
% (e.g.~if the remaining
% $w(\ell)$ cancel out)
% but sometimes they cannot.
% Thus $\sqsubseteq$ represents a partial order that
% allows the selection between ``partially computed values''
% Finally, the compatibility condition can be phrased as
% ``comparable partially evaluated values
% are still comparable if we stop evaluation.''



% First,

% We have seen how circuit compilation \`a la Figure~\ref{fig:motiv-a circuit 1}
% can yield a fast way to compute the expected utility. These circuits not only
% \textit{factorize} Boolean formulae by its structure, but also yield a fast method
% to calculate expected utility given a policy.
% However, we still face the \textit{policy-space explosion} as mentioned in the
% beginning of this section. Indeed, the method outlined in Section~\ref{subsec:amc,wbf}
% forces us to compute AMC on a circuit for every unique policy. It would be ideal
% if we could transform Figure~\ref{fig:motiv-a-bdd} in a manner such that we can
% avoid the intermediate step of considering every policy.

% Towards this end, we present lattice semirings.

% \begin{definition}[Lattice semiring \TODO{citation}]\label{def:latticed semiring}
%   A \textbf{lattice semiring} is a semiring $\mathcal S = (S, + , \times, 0 ,1)$
%   equipped with a partial order $\sqsubseteq$ respecting $+$
%   that admits both meets (greatest lower bounds) and joins (least upper bounds).
% \end{definition}

% It is not hard to show that our expectation semiring $\mathcal S$ is a lattice semiring with
% $(p,u) \sqsubseteq (q,v)$ if and only if both $p \leq q$ and $u \leq v$, with join
% being coordinatewise maximum and meet being coordinatewise minimum.

% \begin{wrapfigure}{R}{0.45\linewidth}
%   \centering
% \end{wrapfigure}

% Now, since we have a lattice over our semiring; we have the notion of joins, which
% can select superior rewards in $\mathcal S$.
% This means we can now encode all types of behavior--probabilistic, utility, and
% decision-making--seen in Figure~\ref{fig:motiv-a-bdd} into a circuit,
% seen in Figure~\ref{fig:bb circuit example}. $u$, which represented a decision in our
% BDD, is now a node $\sqcup$ that represents the choosing of more favorable utilities.

% Has this given us what we need? In fact, it does not:

% \begin{equation}\label{eq:sample ub calc}
% \begin{split}
%   &((1,10) \sqcup (1,-100)) \otimes (0.1,0) \oplus
%   ((1,-5) \sqcup (1,0)) \otimes (0.9,0) \\
%   &\qquad  = (1,10) \otimes (0.1,0) \oplus (1,0) \otimes (0.9,0) = (1, 1).
% \end{split}
% \end{equation}

% This is entirely the wrong maximum expected utility! Where did we go wrong?
% The problem is that the circuit does not select \textit{policies}.
% Although we have endowed the circuit to select ``better'' expected utilities,
% there is no guarantee that the expected utility corresponds to a valid policy.
% Indeed, the resulting value in Equation~\ref{eq:sample ub calc} above corresponds
%  to
% the scenario in which we do not have an umbrella if it rains,
% and we do if it is not.
% This fails to even satisfy the definition of a policy: a deterministic assignment to all decisions.

% Fear not, not all hope is lost.
% Observe that the expected utility returned in Equation~\ref{eq:sample ub calc}
% returns an upper bound to the true maximum expected utility.
% In Section~\ref{sec:bbir}, we demonstrate that
% we can use this observation to
% safely search and prune policies while maintaining
% the compact circuit representation achieved in Figure~\ref{fig:bb circuit example}.
% Furthermore, in Section~\ref{sec:dappl} we make formal the informal
% compilation to weighted Boolean formulae that we performed from Figure~\ref{fig:motiv-a}
% to Figure~\ref{fig:motiv-a circuit 1}.

% 1. Motivating example, what is the MEU problem, expectation semiring, how AMC solves EU
% 2. BBIR, plus branch and bound algorithm
% 3. Intuition for compilation

% \subsection{What is \dappl?}\label{subsec:overview of dappl}


% \subsection{Algebraic model counting and expected utilities}\label{subsec:amc}

% \subsection{The maximum expected utility problem and its intractability}
% \label{subsec: meu overview}

\input{sections/mmap.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../oopsla-appendix"
%%% End:
