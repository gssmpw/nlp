\subsection{First-Class Marginal Maximum A Posteriori}\label{subsec:pineappl-overview}
% \cref{subsec:dappl-overview} demonstrated how the BBIR--and more generally, the extension
% of reasoning-via-compilation techniques to semirings--allows us to
% express and solve optimization problems over inference.
% However, $\dappl$ uses BBIR to solve MEU as the \textit{result} of a program.
% What if we want to use the optimal values in intermediate computation?
% How is this compatible with the BBIR?
% To answer this question, we introduce $\pineappl$
% and the marginal maximum a posteriori (MMAP) problem (\cref{subsubsec:mmap-pineappl})
% and how the BBIR is used to solve intermediate optimization queries
% (\cref{subsubsec:staging}), completing our motivation and overview for BBIR.

% \subsubsection{Using MMAP as a first-class reasoning primitive}
% \label{subsubsec:mmap-pineappl}

One of the key benefits of our algebraic approach to solving MEU in the previous
section is that it can be generalized to different semirings, and therefore applied
to a diverse set of reasoning problems.
Another powerful and common form of optimization-over-inference that is useful in
probabilistic reasoning is the \emph{marginal maximum a-posteriori problem} (MMAP),\footnote{The formal definition of MMAP is given
in~\cref{subsubsec:mmap}.}
which has historically had broad applications in diagnosis.
Consider the $\pineappl$ program
in \cref{fig:motiv-pineappl} presenting the following scenario:

\begin{quote}
\textit{``You are a doctor attempting to diagnose a patient. There is a 50\% chance that any given person has the disease.  If someone has the
disease, there is a 70\% percent chance that they have a headache. If
they do not have disease, there is still a 10\% chance that they have
a headache. You make the most likely diagnosis based on observing the
patient has a headache. There are consequences for misdiagnosis, either
diagnosing the patient when they do not have the disease or failing to
diagnose the patient when they do. What is the probability of complications
arising in a patient observing a headache?''}
\end{quote}

\begin{wrapfigure}{r}{0.44\linewidth}
\begin{pineapplcodeblock}[basicstyle=\tiny\ttfamily]
disease = flip 0.5;
if disease { headache=flip 0.7; }
  else { headache=flip 0.1; }
diagnosis = mmap(disease) with { headache }
if diagnosis && disease { complications=ff; }
  else if diagnosis && !disease { complications=flip 0.4; }
  else if !diagnosis && disease { complications=flip 0.9; }
  else { complications=ff; }
pr(complications)
\end{pineapplcodeblock}
\caption{Example $\pineappl$ program.}
\label{fig:motiv-pineappl}
\end{wrapfigure}

The key new element in this scenario is \emph{first-class optimization}: within
this example, a doctor wants to know the most likely symptom given a disease,
and then take some further action based on the outcome of that query.
\Cref{fig:motiv-pineappl} shows how this is encoded as a program.
On Line 1, we define our prior on whether a member of the population will have
the disease. Lines 2--3 model the conditional probability of a member of the
population having a headache based on whether they have the disease.
Then, on Line 4,
we bind \pineapplcode{diagnosis} to the \textit{most likely state
of} \pineapplcode{disease}, given the observation that \pineapplcode{headache}
is true. Lines 5--8 model the conditional probability of complications based on
the state of \pineapplcode{disease} and \pineapplcode{diagnosis}. Finally, on
Line 9 we calculate the probability of complications given the previous model.

The goal of a $\pineappl$ program is to perform probabilistic inference, much
like standard PPLs, but with the added complexity that random variables can
depend on the most likely state of previously defined variables.  For example,
the most likely state, or MMAP,  of \pineapplcode{disease} when
observing \pineapplcode{headache} is $\tt$. We can derive this by
computing the probability of \pineapplcode{disease} conditioned
on the observation of \pineapplcode{headache}:

{\footnotesize
  \begin{align}
    \Pr[\texttt{diagnosis} = \tt]
    &= \Pr[\texttt{disease} = \tt \mid \texttt{headache} = \tt] \nonumber
    = \frac{\Pr[\texttt{headache} = \tt \mid \texttt{disease} = \tt] \times \Pr[\texttt{disease} = \tt]}{\Pr[\texttt{headache} = \tt]} \nonumber \\
    &= \frac{0.7 \times 0.5}{(0.7 \times 0.5) + (0.3 \times 0.1)}
                                  = 0.92
                                    &\label{eq:mmap-motiv-pineappl}
  \end{align}
}
So, when computing \pineapplcode{Pr(complications)}, we need only consider
where \pineapplcode{diagnosis} is $\tt$:

{\footnotesize
\begin{equation}
  \Pr[\texttt{complications}] = \Pr [\texttt{disease} = \tt] \times 0 +
  \Pr[\texttt{disease} = \ff] \times 0.4 = 0 + 0.5 \times 0.4 = 0.2
\end{equation}}


% To give an answer to the program in \cref{fig:motiv-pineappl} we must first
% define the MMAP for some set of variables in a boolean formula and the
% probability of a satisfying a boolean formula, which we cast as a problem of
% weighted model counting (WMC), similar to Dice~\citep{holtzen2020scaling}.

% \begin{definition}[MMAP of a Boolean Formula]
%   Let $\varphi$ be a boolean formula consisting of variables $V$, $M \subseteq
%   V$, and $N = V \setminus M$. Assuming the existence of a map $\Pr: inst(V) \to
%   \mathbb{R}$, we define the $\mathrm{MMAP}$ of $\varphi$ with respect to $M$ as
%   follows:
%   \[
%     MMAP(\varphi, M) = \argmax_{m \in inst(M)} \left( \sum_{n \in N}
%     \Pr[{m \cup n \mid m \cup n \models \varphi}]\right)
%   \]
% \end{definition}

% \begin{definition}[Weighted Model Counting]
%   Let $\varphi$ be a boolean formula consisting of variables $V$ and $M$ be the
%   models of $\varphi$. Assuming the existence of a map $\Pr: inst(V) \to
%   \mathbb{R}$, we define the weighted model count a boolean formula as
%   follows:
%   \[
%     WMC(\varphi) = \sum_{m \in M} \Pr(m)
%   \]
% \end{definition}

% We give formal definitions for these queries with respect to both an operational
% semantics for $\pineappl$ in \cref{sec:pineappl-sem} and a boolean compilation
% strategy in \cref{sec:pineappl-compl}.

\subsubsection{Staging BBIR Compilation for Meta-Optimization}
\label{subsubsec:staging}

In this section, we demonstrate that the BBIR's unrestricted variable order,
as addressed in~\cref{subsubsec:optimization-via-compilation},
paves the way for a \textit{compositional, staged approach} to
efficiently compiling programs with meta-optimization such as MMAP.

Attempting to emulate the methodology in~\cref{subsubsec:eu-via-compilation}
quickly leads to blowup.
We would have to create two Boolean formulae and compare their $\AMC$ over $\R$:
one for when \texttt{diagnosis} is $\tt$ and one for when \texttt{diagnosis} is $\ff$.
Then, the two formulae will have a duplicated subformula--the subformula
that declares the variables \texttt{disease} and \texttt{headache}.
As the number of MMAP queries increase, this quickly becomes intractable --
the number of times needed to recompile subformulae grows exponentially
with respect to the number of input variables.

To combat this blowup, we apply the idea of \textit{staged compilation}.
In traditional staging, the idea is to accelerate expensive and/or
repeated computation
by precompiling it into an optimized representation (see \citet{taha1999multistage}).
Such computations must be identified in the code and compiled first,
reaping performance benefits by avoiding repeated compilation.

In the case of MMAP, the expensive computation is determined
entirely by the input variables to a \pineapplcode{mmap} query.
By compiling the subformula representing the input variables into BBIR
first, we can then use the
branch-and-bound over BBIR to find the most likely state
(in this case, \texttt{diagnosis} being $\tt$)
and continue compilation of the program with that assignment in mind.

Let us see this idea in action.
Drawing another analogy to staging~\citep{devito2013terra},
we can pre-compile the first three lines of our program:

\begin{equation}\label{eq:partial-pineappl}
  \underbrace{\texttt{disease} \leftrightarrow f_{0.5}}_{\text{Line 1}}
  \land
  \underbrace{\texttt{headache} \leftrightarrow (\texttt{disease} \land f_{0.7} \lor \neg\texttt{disease} \land f_{0.1})}_{\text{Lines 2-3}}.
\end{equation}

We insert auxiliary variables \texttt{disease} and \texttt{headache} to maintain
sequentiality of the program: if we were to simply say
$f_{0.5} \land \left(f_{0.5} \land f_{0.7} \lor \neg f_{0.5} \land f_{0.1}\right)$,
then the program will return $\bot$ once any of the sampled values returned $\ff$,
which is incorrect.

BBIR allows \cref{eq:partial-pineappl} to be compiled as a branch-and-bound
circuit that can be efficiently queried, in a manner
similar to $\dappl$. Thus, we can deduce that the most likely assignment to
\texttt{diagnosis} is $\tt$, and then extend~\cref{eq:partial-pineappl} as such:
\begin{equation}
  \texttt{disease} \leftrightarrow f_{0.5} \land
  \texttt{headache} \leftrightarrow (\texttt{disease} \land f_{0.7} \lor \neg\texttt{disease} \land f_{0.1}) \land
  \texttt{diagnosis} \leftrightarrow T
\end{equation}
\noindent
at which point the compilation of the program can resume
by reusing the precompiled BBIR for~\cref{eq:partial-pineappl} and
\textit{without having computed a separate formula for when \texttt{diagnosis} is $\ff$.}

This is achieved without significant blowup because the variable order within the BDD
has no restrictions.
If the variable order were to be restricted
as per the approach of~\citet{derkinderen2020algebraic},
then at every call to \pineapplcode{mmap}
we would need to sift the queried variables to the top,
which is known to be expensive and can blow up
the size of the BDD~\citep{holtzen2020scaling,holtzen2021model}.

To summarize, we have demonstrated the key insights that make BBIR an ideal
compilation target for PPLs performing optimization:
\begin{enumerate}
  \item BBIR generalizes knowledge compilation beyond the real numbers,
  allowing for more general
  optimization problems over inference such as MEU to be expressed.
  \item BBIR does not enforce any variable order, which allows us to
  express probabilistic programs with meta-optimization queries
  through staged compilation of the BBIR.
\end{enumerate}
The next section will delve into technical details of how we achieve both objectives.
%\input{sections/pineappl_bbir.tex}





%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../oopsla-appendix.tex"
%%% End:
