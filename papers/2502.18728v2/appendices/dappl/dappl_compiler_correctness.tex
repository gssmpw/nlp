\subsection{Proof of Theorem~\ref{thm:compiler correctness}}\label{appendix:dappl correctness}

The architecture of the proof is as follows. First, we prove that~\cref{thm:compiler correctness}
reduces to the following:

\begin{theorem}\label{thm:util correspondence}
  Let $\Gamma \proves e : \Giry \Bool$ a $\util$ expression. Let 
  $e \leadsto (\varphi, \gamma, w, R)$ via~\cref{fig:dappl full bc}.
  Let $\NoWt{\varphi}$ be the variables in $\varphi$ without a defined 
  weight; that is, variables whose literals are not in the domain of $w$, 
  and $\mathcal W (\varphi)$ be the set of maps $lits(\NoWt{\varphi}) \to \mathcal S$.

  Then, there exists a function $f : \denote{\Gamma} \to\mathcal W (\varphi)$
  making the following diagram commute:

  \[\begin{tikzcd}
	\llbracket\Gamma\rrbracket & {\mathcal W(\varphi)} \\
	& {\mathbb R}
	\arrow["f", from=1-1, to=1-2]
	\arrow["{\text{EU}_{(\varphi,\gamma, R)}}", from=1-2, to=2-2]
	\arrow["{\EU \circ \llbracket e \rrbracket}"'{pos=0.2}, from=1-1, to=2-2]
\end{tikzcd}\]
where, for $\overline w \in \mathcal W (\varphi)$,
\begin{equation}\label{eq:util correspondence normalized}
  \text{EU}_{(\varphi,\gamma, R)}(\overline w) 
    = \frac{\AMC(\varphi \land \gamma \land R, w \cup \overline w)_{\EU}}
      {\AMC(\gamma, w \cup \overline w)_{\Pr}}.
\end{equation}
In particular, if there are no \texttt{observe}s (conditioning) 
in the program,~\cref{eq:util correspondence normalized} reduces to
\begin{equation}
  \text{EU}_{(\varphi,T, R)}(\overline w) 
    = \AMC(\varphi\land R, w \cup \overline w)_{\EU}.
\end{equation}
\end{theorem}

Then, we prove~\cref{thm:util correspondence} to complete the proof.

\subsubsection{Reduction of Theorem~\ref{thm:compiler correctness}
to Theorem~\ref{thm:util correspondence}}

The key observation is the following Lemma:

\begin{lemma}[Policy space correspondence]\label{lemma:policy space}
  Let $\cdot \proves e : \tau$ be a \dappl{} program and 
  let $\mathcal A = C_1 \times \cdots C_k$ 
  be the policy space of $e$. 
  Let $e \leadsto \target$. 
  Let $X$ be the set of Boolean variables
  representing choices in $\varphi$. 
  Then:
  \begin{enumerate}
    \item There is an bijective correspondence $\cup_i C_i \to X$,
    \item which lifts into a canonical injective map $\iota :\mathcal A \to inst(X)$,
    \item such that for which for all $\pi\in \mathcal A$, $\iota(\pi)$ satisfies all
    ExactlyOne clauses.
  \end{enumerate} 
\end{lemma}

\begin{proof}
The bijective correspondence is the map assigning $\alpha \in \cup_i C_i$
  to the Boolean variable generated by 
  Boolean compilation of $C_i = [\alpha, \cdots, \alpha_n]$.
  This is injective, as for $\alpha,\beta \in \cup_i C_i$ such that $\alpha \neq \beta$,
  the Boolean compilation rules will always introduce fresh variable names for $\alpha$
  and $\beta$ that cannot coincide. It is surjective, as variables in $X$ are only 
  introduced in the \texttt{bc/[]} rule, which also introduces choices. We see in the 
  \texttt{bc/[]} rule that for $n$ many alternatives in a choice $C$, $n$ many variables
  are generated.

  Call such a bijection $b$. Then the canonical injective map $\iota$ simply maps $b$
  on each factor of a policy $\pi \in \mathcal A$. The fact that 
  $\iota(\pi)$ satisfies all ExactlyOne clauses is an induction on the Boolean
  compilation rules.
\end{proof}

We can generalize~\cref{lemma:policy space} to general judgements $\Gamma \proves e : \tau$,
in particular get a map from policies $\pi_{\Gamma}$ 
in the context policy space $\mathcal{A}_{\Gamma}$ (recall~\cref{def:context policy space})
to variables in the compiled Boolean formula corresponding to the free variables.

\begin{lemma}\label{lemma:context policy space}
  Let $\Gamma \proves e : \tau$ be a $\dappl$ expression.
  Let $\mathcal{A}_{\Gamma}$ be the context policy space. 
  Let $e \leadsto \target$.
  Let $\mathsf{Var}_{\mathsf{Choice}}(\varphi)$
  be the variables in $\varphi$ that correspond to names of type 
  $\Choice S$ for some $S$ in $\Gamma$; that is,
  $$
  \mathsf{Var}_{\mathsf{Choice}}(\varphi) = \prod_{\Choice S \in \Gamma} S.
  $$
  Then for each $\pi_{\Gamma} \in \mathcal{A}_{\Gamma}$ there is a bijective map $\rho_{\pi_{\Gamma}}:  \pi_{\Gamma} \to \mathsf{Var}_{\mathsf{Choice}}(\varphi)$ 
  on which the inverse is a valid substitution of $\varphi$.
\end{lemma}

\begin{proof}
  The map simply maps each component of $\pi_{\Gamma}$ to its corresponding variable.
  This is a valid substitution as we can always generated fresh Boolean variable names corresponding to the component, which has already been assumed WLOG in the \texttt{bc/choose} rule.
\end{proof}

We can now state the following Lemma:

\begin{lemma}\label{lemma:util square}
  Let $\Gamma \proves e : \tau$ a well-typed $\dappl$ program and let $\iota$ 
  be the canonical injective map from the policy space 
  as described in~\cref{lemma:policy space} and $\rho$ the canonical map 
  from the context policy space to $\mathsf{Var}_{\mathsf{Choice}}(\varphi)$
  as described in ~\cref{lemma:context policy space}. 
  Let $\mathcal{A}_{\Gamma}$ be the context policy space and $\pi_{\Gamma} \in \mathcal{A}_{\Gamma}$.
  Let $\pi$ be a valid policy of $e$ and let 
  $e \leadsto \target$ and 
  $e|_{\pi} \leadsto (\varphi_{\pi}, \gamma_{\pi}, w_\pi, R_\pi)$.

  Let $\mathsf{subst}$ denote the operation that takes in a formula $\varphi$,
  and substitutes
  variables in $\varphi$
  corresponding to variables $x$ of type $\Choice S$ for some $S$ 
  with either $\iota^{-1}(x)$ or $\rho^{-1}(x)$.

  Then the following square commutes up to equisatisfiability of Boolean formulae:
  \[\begin{tikzcd}
	{e} & {(\varphi, \gamma)} \\
	{e|_{\pi \cup \pi_{\Gamma}}} & {(\varphi_{\pi}, \gamma_{\pi})}
	\arrow["\text{Reduction, see~\cref{appendix:reduction soundness}}"', from=1-1, to=2-1]
	\arrow[squiggly, from=1-1, to=1-2]
	\arrow[squiggly, from=2-1, to=2-2]
	\arrow["{(\mathsf{subst}(-), \mathsf{subst}(-))}", from=1-2, to=2-2]
  \end{tikzcd}\]
  in which the $w,R$ are elided as $w \supseteq w_{\pi}$ and $R \supseteq R_\pi$. 
\end{lemma}

\begin{proof}
  The proof follows from an induction on the syntax of $e$. All cases are straightforward except for three cases:
  \begin{itemize}
    \item If $e = x$ and $x$ is of type $\Choice S$ for some $S$ in $\Gamma$, then 
    the reduction yields the empty program so the square is trivially satisfied.
    \item If $e = [\alpha_1, \cdots, \alpha_n]$, then $e|_{\pi \cup \pi_{\Gamma}}$ is 
    $\return \tt$, which compiles to $\top$.
    There are no free variables in $e$; so the substitution must come from the 
    policy space. By~\cref{lemma:policy space} we know this satisfies the ExactlyOne
    clause of $\varphi$; so it is $\top$ as well. $\gamma$ and $\gamma|_{\pi}$ are both
    $\top$. So we are done.
    \item If $e = \choose x {\alpha_i \implies e_i}$, then WLOG assume that $x$ is substituted for $\alpha_1$. Then $\varphi$ will simplify to $a_i \land e_i|_{\pi \cup \pi_{\Gamma}} \land \bigwedge_{j \neq 1} \conjneg{R_j}$. This is equisatisfiable to $e_i|_{\pi \cup \pi_{\Gamma}} $, at which point the IH kicks in and we are done.
  \end{itemize}
\end{proof}

To prove Theorem 4, consider a $\dappl$ program $e$ (that is, $\cdot \proves e : \tau$) and let $\pi$ be an arbitrary policy. We need not consider context policy spaces as the context is empty. 
Then by~\cref{lemma:util square} we can reduce to a valid $\util$ program. 
Onto this $\util$ program we can apply~\cref{thm:util correspondence}
to know that this is the correct expected utility. Then, by knowing that this is true in 
particular for the optimal policy, and knowing that $\text{bb}$ (Algorithm~\ref{algorithm:bb}) finds this optimal policy
via~\cref{thm:soundness of bb}, we are done.

\subsubsection{Proof of Theorem~\ref{thm:util correspondence}}

We state helpful lemmata, some of which are 
applications of Theorem~\ref{thm:amc invariant} to Propositions
proven in~\citet{holtzen2020scaling}.

\begin{lemma}[Independent conjunction of probabilities]
\label{lemma:ind conj prob}
  For $\varphi, \psi$ Boolean formulas that share no variables
  and any weight function $w : lits(\varphi) \cup lits(\psi) \to \mathcal S$,
  $\AMC(\varphi,w)_{\Pr}\times\AMC(\psi,w)_{\Pr} = \AMC(\varphi \land \psi,w)_{\Pr}$
\end{lemma}

\begin{lemma}[Inclusion-exclusion of probabilities]
\label{lemma:inc exc prob}
  For $\varphi, \psi$ Boolean formulas
  and any weight function $w : lits(\varphi) \cup lits(\psi) \to \mathcal S$,
  $\AMC(\varphi,w)_{\Pr}+ \AMC(\psi,w)_{\Pr} -
  \AMC(\varphi \land \psi, w)_{\Pr} = \AMC(\varphi \lor \psi,w)_{\Pr}$.
\end{lemma}

The following additional Lemma extends Lemma~\ref{lemma:ind conj prob}
to expected utilities.

\begin{lemma}[Independent conjunction of expected utilities]
\label{lemma:ind conj eu}
  For $\varphi, \psi$ Boolean formulas that share no variables
  and any weight function $w : lits(\varphi) \cup lits(\psi) \to \mathcal S$,
  \begin{align*}
      \AMC(\varphi,w)\times\AMC(\psi,w) &= \AMC(\varphi \land \psi,w)\\
    &=\AMC(\varphi,w)\cdot\AMC(\psi,w)_{\Pr}+\AMC(\psi,w)\cdot\AMC(\varphi,w)_{\Pr}  
  \end{align*}
  where $\times$ is multiplication in the expectation semiring $\mathcal S$ and $\cdot$
  is scalar multiplication distributing over $\mathcal S$. In particular, if 
  $\AMC(\varphi, w)_{\EU} = 0$,
  \begin{align*}
    [\AMC(\varphi,w)\times\AMC(\psi,w)]_{\EU} &= [\AMC(\varphi \land \psi,w)]_{\EU}\\
    &=\AMC(\varphi,w)_{\EU}\cdot\AMC(\psi,w)_{\Pr}.
  \end{align*}
\end{lemma}

\begin{proof}
  We observe that if $\varphi, \psi$ are disjoint, then 
  the models $m \models \varphi \land \psi$ are exactly the set
  $\{m_{\varphi} \cup m_{\psi}: 
  m_{\varphi} \models \varphi \texttt{ and } m_\psi \models \psi\}$;
  the proof follows.
\end{proof}

This Lemma extends Lemma~\ref{lemma:inc exc prob} for
expected utilities, but specifically for compiled formulas.

\begin{lemma}[Additive expected utility.]\label{lemma:inc exc eu}
  Let $\varphi, \psi$ be two programs such that
  the variables of each formula can be partitioned into 
  disjoint sets of probabilistic and reward variables
  $vars(\varphi) = P_X \cup R_X$ and $vars(\psi) = P_Y \cup R_Y$.

  Let $w$ be a weight function such that for literals
  $p \in lits(P_X) \cup lits(P_Y)$, $w(p)_{\EU} = 0$, 
  and for $r \in lits(R_X) \cup lits(R_Y)$,
  $w(r)_{\Pr} = 1$, identifying that probabilistic
  variables carry no utility and reward variables carry
  probability 1. 

  If $R_X, R_Y$
  are disjoint, then
  \begin{align*}
    [\AMC((\varphi \land \conjneg{R_Y} )
    \lor (\psi \land \conjneg{R_X}), w)]_{\EU}
    &= [\AMC(\varphi,w)]_{\EU} + [\AMC(\psi, w)]_{\EU}.
  \end{align*}
\end{lemma}

\begin{proof}
  Consider the models $m$ such that
    $m \models (\varphi \land \conjneg{R_Y} )
    \lor (\psi \land \conjneg{R_X}).$
  The models will will either:
  \begin{enumerate}
    \item model $\varphi \land \conjneg{R_Y}$
    but not $\psi \land \conjneg{R_X}$,
    \item model $\psi \land \conjneg{R_X}$
    but not $\varphi \land \conjneg{R_Y}$, or
    \item model both $\varphi \land \conjneg{R_Y}$
    and $\psi \land \conjneg{R_X}$.
  \end{enumerate}
  In Cases (1) and (2), $\psi \land \conjneg{R_X}$
  and $\varphi \land \conjneg{R_Y}$ respecitvely will not
  contribute any expected utility as they are not modeled.
  In Case (3), as any model will make all reward variables in
  $R_X$ and $R_Y$ false, it will contribute no expected utility.
  Thus in summary

  \begin{align*}
    [\AMC((\varphi \land \conjneg{R_Y})
    \lor (\psi \land \conjneg{R_X}), w)]_{\EU}
    &= \left[\sum_{m \models \varphi \land \conjneg{R_Y},
        m \not\models \psi \land \conjneg{R_X}} w(m)\right]_{\EU} \\
    &\quad + \left[\sum_{m \not\models \varphi \land \conjneg{R_Y},
        m \models \psi \land \conjneg{R_X}} w(m)\right]_{\EU} \\
    &\quad + \left[\sum_{m \models \varphi \land \conjneg{R_Y},
        m \models \psi \land \conjneg{R_X}} w(m)\right]_{\EU} \\
    &= \left[\sum_{m \models \varphi \land \conjneg{R_Y}} w(m)\right]_{\EU}
      + \left[\sum_{
        m \models \psi \land \conjneg{R_X}} w(m)\right]_{\EU} &(\star)\\
    &= \left[\sum_{m \models \varphi} w(m)\right]_{\EU}
      + \left[\sum_{
        m \models \psi} w(m)\right]_{\EU} &(\dagger)\\
    &= [\AMC(\varphi,w)]_{\EU} + [\AMC(\psi, w)]_{\EU}, 
  \end{align*}
  where $w(m)$ denotes the weight of a model defined as the product of its literals. $(\star)$ is the usage of the fact that if 
  $m \models \varphi \land \conjneg{R_Y}$,
  then either $m \models \psi \land \conjneg{R_X}$ or it does not.
  If it does, then $w(m)$ is zero as all reward variables are negated.
  If not, then $m \not\models \psi \land \conjneg{R_X}$ so the formula 
  $\psi \land \conjneg{R_X}$ contributes nothing. The analogous is true 
  for when $m \models \psi \land \conjneg{R_X}$.
  $(\dagger)$ uses the fact that the weight of a negated
  reward literal is $(1,0)$, the multiplicative unit, so
  it can be factored out when calculating $w(m)$.
\end{proof}

It is worthwhile to note that $[\AMC(\varphi,w)]_{\EU} = \EU[\varphi]$ as mentioned in Lemma~\ref{thm:amc invariant}. 
Since expected utility is indeed an expectation, we can use 
techniques such as taking conditional expectations $\EU[\varphi | \gamma]$. We reap the benefits of this observation in the proof of
Theorem~\ref{thm:compiler correctness}.


Now we prove intermediate results about the distribution of a $\util$ program.

\begin{definition}
  Let $\Gamma \proves e : \Giry \Bool$ a $\util$ program. 
  Then we can define a probability distribution $\Pr : \{\tt, \ff, \bot\} \to [0,1]$
  by:
  \begin{align*}
    \Pr(\tt) = \sum_{r \in \R} \denote{e} \denote{\Gamma}((\tt,r))
    &&
    \Pr(\ff) = \sum_{r \in \R} \denote{e} \denote{\Gamma}((\ff,r)),
  \end{align*}
  identically we can write
  \begin{align*}
    \Pr(\tt) = \sum_{v = (\tt,r) \in \R} \denote{e} \denote{\Gamma}(v)
    &&
    \Pr(\ff) = \sum_{v = (\ff,r) \in \R} \denote{e} \denote{\Gamma}(v).
  \end{align*}
  With an abuse of notation we write $\Pr[\denote{e} \denote{\Gamma}]$ for this.
\end{definition}

\begin{theorem}\label{thm:util pr correspondence}
  Let $\Gamma \proves e : \Giry \Bool$ a $\util$ expression. 
  Let 
  $e \leadsto (\varphi, \gamma, w, R)$ via~\cref{fig:dappl full bc}.
  Let $\NoWt{\varphi}$ be the variables in $\varphi$ (hence, in $\gamma$ as well) 
  without a defined 
  weight; that is, variables whose literals are not in the domain of $w$, 
  and $\mathcal W (\varphi)$ be the set of maps $lits(\NoWt{\varphi}) \to \mathcal S$.

  Then, there exists a function $f : \denote{\Gamma} \to\mathcal W (\varphi)$
  making the following diagrams commute:

  \[\begin{tikzcd}
	\llbracket\Gamma\rrbracket & {\mathcal W(\varphi)} \\
	& {\mathbb R}
	\arrow["f", from=1-1, to=1-2]
	\arrow["{\mathsf{Prob_{\varphi,\gamma, R}}}", from=1-2, to=2-2]
	\arrow["{\Pr \circ \llbracket e \rrbracket (-)(\tt)}"'{pos=0.2}, from=1-1, to=2-2]
  \end{tikzcd}
  \qquad 
  \begin{tikzcd}
    \llbracket\Gamma\rrbracket & {\mathcal W(\varphi)} \\
    & {\mathbb R}
    \arrow["f", from=1-1, to=1-2]
    \arrow["{\mathsf{Prob_{\overline{\varphi},\gamma, R}}}", from=1-2, to=2-2]
    \arrow["{\Pr \circ \llbracket e \rrbracket (-)(\ff)}"'{pos=0.2}, from=1-1, to=2-2]
  \end{tikzcd}\]
  where, for $\overline w \in \mathcal W(\varphi)$,
  \begin{equation}
    \mathsf{Prob}_{\varphi, \gamma, R} (\overline w)
       = \AMC(\varphi \land \gamma \land R, w \cup \overline w)_{\Pr}.
  \end{equation}
  and
  \begin{equation}
    \mathsf{Prob}_{\overline\varphi, R} (\overline w)
       = \AMC(\overline \varphi \land \gamma \land R, w \cup \overline w)_{\Pr}.
  \end{equation}
  That is, computes the \emph{unnormalized probabilities of $e$ returning $\tt$ or $\ff$.}
\end{theorem}

\begin{proof}
  Recall that $\Gamma$ must only hold variables of 
  type $\Bool$ by~\cref{lemma:util context}. \
  So $\denote{\Gamma}$ will hold maps of variables to distributions $\mathbf{TT}$ or $\mathbf{FF}$. 
  Also note that 
  variables in $\NoWt{\varphi}$ are precisely the free variables of $e$; 
  these variables must be defined in $\Gamma$. 
  Thus we can define $f$ to be the map mapping $\{x \mapsto \mathbf{TT}\}$ to $\{x \mapsto (1,0), \overline x \mapsto (0,0)\}$ and $\{x \mapsto \mathbf{FF}\}$ to $\{x \mapsto (0,0), \overline x \mapsto (1,0)\}$.

  We prove that this is exactly what we need. This is done by simultaneous induction on syntax. 
  \begin{itemize}[leftmargin=*]
    \item If $e = \tt, \ff, \flip \theta$, then we are done after a simple evaluation.
    
    \item If $e = x$, then $x \leadsto (x, \top, \eset, \eset)$. 
    If $x \mapsto \mathbf{TT} \in \denote{\Gamma}$ then $\Pr \circ \denote{x} (\Gamma) (\tt,0) = 1$. Identically 
    \begin{equation}
      \AMC(x \land \top, w \cup (x \mapsto (1,0), \overline x \mapsto (0,0)))_{\Pr} 
      \AMC(x, w \cup (x \mapsto (1,0), \overline x \mapsto (0,0)))_{\Pr} 
      = 1.
    \end{equation}
    If $x \mapsto \mathbf{FF} \in \denote{\Gamma}$ then $\Pr \circ \denote{x} (\Gamma) (\ff,0) = 1$. Identically 
    \begin{equation}
      \AMC(\overline x, w \cup (x \mapsto (0,0), \overline x \mapsto (1,0)))_{\Pr} = 1.
    \end{equation}

    \item For $e = \return P$, for $g \in \denote{\Gamma}$,
    $\denote{e}g = \denote{P}g$. 
    Identically $e$ and $P$ compiles to the same Boolean formula. By the IH we are done.

    \item For $e = \reward k {e'}$, for $g \in \denote{\Gamma}$, we observe that
      $$\Pr \circ (\denote{e} g) (\tt) 
        = \sum_{r \in R} (\denote{e}g)(r)
        = \sum_{r \in R} (\denote{e'}g)(r) = \Pr \circ (\denote{e'} g)(\tt).$$
    Identically, we observe that
    $$
      \AMC(\varphi \land \gamma \land R \land r_k, w)_{\Pr}
      = \AMC(\varphi \land \gamma \land R, w)_{\Pr}
    $$
    as a straightforward application of~\cref{lemma:ind conj prob}. By the IH we are done. The case is identical for $\Pr \circ \denote e \denote \gamma  (\ff)$.


    \item For $e = \ite x {e'} {e''}$, for $g \in \denote{\Gamma}$, we case on $g(x)$. Assume it is $\mathbf{TT}$; the other case is symmetrical. Then 
    $\denote{e} g = \denote{e'} g$. 

    On the other hand this implies that $f(x \mapsto \mathbf{TT}) = \{x \mapsto (1,0), \overline x \mapsto (0,0)\}$. Consider that, using notation from \texttt{bc/ite}
    and writing 
    $$\varphi = (x \land \varphi_t \land R_t \land \conjneg{R_e}) 
            \lor (\overline{x} \land \varphi_e \land R_e \land \conjneg{R_t})
            \land (x \land \gamma_t) \lor (\overline x \land \gamma_e),$$
    we get, after simplification,
    \begin{align}
      &\AMC(\varphi, w_t \cup w_e \cup f(x \mapsto \mathbf{TT}))_{\Pr} \\
      &=\AMC(x \land \varphi_t \land \gamma_t \land R_t \land \conjneg{R_e})_{\Pr} \\
      &=\AMC(\varphi_t \land R_t \land \gamma_t \land \conjneg{R_e} ) \\
      &=\AMC(\varphi_t \land R_t \land \gamma_t)
    \end{align}
    where (22) is due to~\cref{lemma:inc exc prob} and (23), (24) is due to~\cref{lemma:ind conj prob}. At this point the IH works and we are done. The case is identical for $\Pr \circ (\denote e g)  (\ff)$.

  \item For $e = \observe x {e'}$, for $g \in \denote{\Gamma}$, we case on $g(x)$.
 
  If $g(x) = \mathbf{TT}$, then $\denote{e} g = \denote{e'} g$. Also, by following \texttt{bc/obs}, we get that 
      $$\AMC(\varphi \land \gamma \land x, w_t \cup w_e \cup f(x \mapsto \mathbf{TT}))_{\Pr}
      =\AMC(\varphi \land \gamma, w_t \cup w_e)_{\Pr}$$
      by~\cref{lemma:ind conj prob} and we are done by IH.

  If $g(x) = \mathbf{FF}$, then $\denote{e} g = \pmb{\bot}$. Identically we get
    $$\AMC(\varphi \land \gamma \land x, w_t \cup w_e \cup f(x \mapsto \mathbf{FF}))_{\Pr}
      =0$$
  concluding the case.
  \item For $e = \bind x {b} {e'}$, for $g \in \denote{\Gamma}$, we define some custom notation:
  \begin{itemize}
    \item $\mathcal D = \denote{e} g$,
    \item $\mathcal B = \denote{b} g$,
    \item $\mathcal{E}_{\mathbf{TT}} = (\denote{e'} g) \cup (x \mapsto \mathbf{TT})$,
    \item $\mathcal{E}_{\mathbf{FF}} =( \denote{e'} g )\cup (x \mapsto \mathbf{FF})$,
    \item $b \leadsto (\varphi_b, T, R_b)$, and
    \item $e' \leadsto (\varphi_{e'}, T, R_{e'})$.
  \end{itemize}
  Then, we can identify, via our denotational semantics (refer to~\cref{appendix:util semantics}), 
  expand 
  $\Pr \circ \mathcal D$:
  \begin{align*}
    \Pr \circ \mathcal D (\tt)
      &= \sum_{r \in \R} \mathcal D (\tt,r) 
      &\text{[Definition]}\\
      &=\sum_{r \in \R} 
        \left[
          \sum_{s \in \R} \mathcal B [(\tt,s)] \mathcal{E}_{\mathbf{TT}} (\tt, s-r)
          + \sum_{s \in \R} \mathcal B [(\ff,s)] \mathcal{E}_{\mathbf{FF}} (\tt, s-r)
        \right]
      &\text{[Unfolding]}\\
      &=\sum_{r \in \R} 
        \left[
          \sum_{s \in \R} \mathcal B [(\tt,s)] \mathcal{E}_{\mathbf{TT}} (\tt, r)
          + \sum_{s \in \R} \mathcal B [(\ff,s)] \mathcal{E}_{\mathbf{FF}} (\tt, r)
        \right]
      &\text{[Invariance]}\\
      &= 
          \sum_{s \in \R} \mathcal B [(\tt,s)] \sum_{r \in \R}\mathcal{E}_ {\mathbf{TT}} (\tt, r)
          + \sum_{s \in \R} \mathcal B [(\ff,s)] \sum_{r \in \R}\mathcal{E}_{\mathbf{FF}} (\tt, r)
      &\text{[Distributivity]} \\
      &= 
        \Pr \circ \mathcal B (\tt)
        \Pr \circ \mathcal{E}_ {\mathbf{TT}} (\tt)
        + \Pr \circ \mathcal B (\ff)
        \Pr \circ \mathcal{E}_ {\mathbf{FF}} (\tt)
      &\text{[Definition]}\\
      &= 
        \AMC(\varphi_b \land \gamma_b)_{\Pr}
        \AMC(\varphi_{e'}[x / T] \land \gamma_{e'}[x/T])_{\Pr}
      &\\
      &\quad
        + \AMC(\overline{\varphi_b} \land \gamma_b)_{\Pr}
        \AMC(\varphi_{e'}[x / F] \land \gamma_{e'}[x/F])_{\Pr}
      &\text{[IH]}\\
      &= 
        \AMC((\varphi_b \land \gamma_b \land \varphi_{e'}[x / T] \land \gamma_{e'}[x/T])
      &\\
      &\quad 
          \lor 
          (\overline{\varphi_b}\land \varphi_{e'}[x / F]\land \varphi_{e'}[x / F] \land \gamma_{e'}[x/F]))_{\Pr}
      &\text{[Lemmas]}\\
      &= 
        \AMC(\varphi_{e'}[x/\varphi_b] \land \gamma_b \land \gamma_e'[x/\varphi_b])_{\Pr}
      &\text{[Equisatisfiability]}
  \end{align*}
    which concludes the proof.
  \end{itemize}
\end{proof}

We can also additionally prove a similar Lemma.

\begin{lemma}\label{lemma:util pr bot}
  Let $\Gamma \proves e : \Giry \Bool$ a $\util$ expression. Let 
  $e \leadsto (\varphi, \gamma, w, R)$ via~\cref{fig:dappl full bc}.
  Let $f$ be the function defined in~\cref{thm:util pr correspondence}. 
  Then for $g \in \denote{\Gamma}$,
  \begin{equation}
    \Pr \circ (\denote{e}g)[\tt \texttt{ or } \ff] = \AMC(\gamma, w \cup \overline w)_{\Pr}.
  \end{equation}
\end{lemma}

\begin{proof}
  We observe that $\Pr \circ (\denote{e}g)[\tt]$ 
  and $\Pr \circ (\denote{e}g)[\ff]$
  are disjoint events. Then by~\cref{thm:util pr correspondence}
  we get that 
  \begin{align*}
    &\Pr \circ (\denote{e}g)[\tt] +  \Pr \circ (\denote{e}g)[\tt]&\\
    &= \AMC(\varphi \land \gamma \land R)_{\Pr} + \AMC(\overline{\varphi} \land \gamma \land R)_{\Pr}&\\
    &\AMC(\varphi \land \gamma \lor \overline{\varphi} \land \gamma)_{\Pr}
    &\text{[Lemmas]}\\
    &\AMC(\gamma)_{\Pr}
    &[\Pr[\varphi \lor \overline{\varphi}] = 1]
  \end{align*}
  which concludes the proof.
\end{proof}

With this we prove, automatically, a corollary:

\begin{corollary}\label{cor:util pr correspondence}
  Let $\Gamma \proves e : \Giry \Bool$ a $\util$ expression. Let 
  $e \leadsto (\varphi, \gamma, w, R)$ via~\cref{fig:dappl full bc}.
  Let $f$ be the function defined in~\cref{thm:util pr correspondence}. 
  Let $g \in \denote{\Gamma}$.
  Then
  \begin{equation}
    \Pr \circ (\denote{e}g)[\tt \mid \texttt{not }\bot]
    = \frac
      {\Pr \circ (\denote{e}g)[\tt]}
      { \Pr \circ (\denote{e}g)[\tt \texttt{ or } \ff]}
     = \frac{\AMC(\varphi \land \gamma, w \cup \overline w)_{\Pr}}{\AMC(\gamma, w \cup \overline w)_{\Pr}}
  \end{equation}
\end{corollary}

This Corollary is essentially a denotational version of the main theorem proven in~\citet{holtzen2020scaling}.
Now onto the good part.

\begin{proof}[Proof of~\cref{thm:util correspondence}]

  We claim that the same $f$ used for~\cref{thm:util pr correspondence} suffices. 
  Let $g \in \denote{\Gamma}$.
  By an application of~\cref{lemma:util pr bot}, it suffices to prove the unnormalized
  case. That is, let $\EU_{\mathsf{unn}}$ be the unnormalized expected utility, where,
  in contrast to~\cref{def:eu util},
  \begin{equation}
    \EU_{\mathsf{unn}}(\denote{e} g) (b)
      = \sum_{r \in \R} r \times (\denote{e}g)(b,r).
  \end{equation}
  It suffices to prove 
  \begin{equation}
    \EU_{\mathsf{unn}}(\denote{e} g)(\tt)
      = \AMC(\varphi \land \gamma \land R)_{\EU},
    \quad
    \EU_{\mathsf{unn}}(\denote{e} g)(\ff)
      = \AMC(\overline \varphi \land \gamma \land R)_{\EU}.    
  \end{equation}
  We induct on syntax once more.

  \begin{itemize}[leftmargin=*]
    \item If $e = \tt, \ff, \flip \theta, x,$ and $\return P$, 
    the expected utility is always zero,
    which proves the theorem.

    \item For $e = \reward k {e'}$, we observe that
    $$\denote{e}g = \lambda v. \begin{cases}
      (\denote{e'}g)(b,s-k) & v = (b,s)\\
      (\denote{e'}g)(v)  & \text{else}
    \end{cases} $$
    so in particular, writing $\mathcal D = \denote{e'}g$,
    \begin{align*}
      \EU_{\mathsf{unn}} \circ( \denote{e} g) (\tt)
        &= \sum_{r \in \R} r \times \mathcal D [(\tt, r - k)] &\\
        &= \sum_{r \in \R} (r + k) \times \mathcal D [(\tt, r)] &\text{[Rewriting]} \\
        &= \sum_{r \in R} r \times \mathcal D [(\tt, r)] 
          + k \sum_{r \in R}\mathcal D [(\tt, r)]. &\text{[Arithmetic]}
    \end{align*}
    Let $e \leadsto (\varphi, \gamma, w, R \cup r_k)$ as per \texttt{bc/reward}. Let $\overline w = f \denote{\Gamma}$. We see that
    \begin{align*}
      EU_{(\varphi, \gamma, R \cup \{r_k\})} (\overline w) 
        &=\AMC(\varphi \land \gamma \land R \land r_k, w \cup \overline w)_{\EU} &\\ 
        &=\AMC(\varphi \land \gamma \land R)_{\EU} \times \AMC(r_k)_{\Pr}
        + \AMC(\varphi \land \gamma \land R)_{\Pr} \times \AMC(r_k)_{\EU}
          &\text{[\cref{lemma:ind conj eu}]}\\
        &=\AMC(\varphi \land \gamma \land R)_{\EU}
          + \AMC(\varphi \land \gamma \land R)_{\Pr} \times k  
          &\text{[Evaluation]}\\
        &=\sum_{r \in R} r \times \mathcal D [(\tt, r)] + \AMC(\varphi \land \gamma \land R)_{\Pr} \times k
          &\text{[IH]}\\
        &=\sum_{r \in R} r \times \mathcal D [(\tt, r)] + k\sum_{r \in R} \mathcal D [(\tt, r)]  &\text{[\cref{thm:util pr correspondence}]}
    \end{align*}
    and the case is identical for $\ff$. 
    
    \item For $e = \ite x {e'} {e''}$, we case on $g(x)$. 
    WLOG assume $g(x) = \mathbf{TT}$ as the other case is symmetric. 
    Then 
    $$
    \denote{e} g = \denote{e'}g.
    $$
    Furthermore 
    $$e \leadsto ((x \land \varphi_t \land R_t \land \conjneg{R_e}) 
    \lor (\overline{x} \land \varphi_e \land R_e \land \conjneg{R_t}),
    (x \land \gamma_t) \lor (\overline x \land \gamma_e),
    w_t \cup w_e,
          \eset);$$
    writing $\mathcal D = \denote{e'} g$, $\overline w = f g$, and $\varphi, \gamma$ for the unnormalized and normalizing Boolean formulae we see that
    \begin{align*}
      EU_{(\varphi, \gamma, \eset)}(\overline w) 
        &=\AMC(\varphi \land \gamma)_{\EU}&\\
        &=\AMC((x \land \varphi_t \land \gamma_t \land R_t \land \conjneg{R_e}))
          &\text{[Evaluation, Lemmas]}\\
        &=\AMC((\varphi_t \land R_t \land \gamma_t))_{\EU}
        &\text{[\cref{lemma:ind conj eu},\cref{lemma:ind conj prob}]}\\
        &=\EU_{\mathsf{unn}}(\denote{e'} g) (\tt)
    \end{align*}
    which concludes the case via~\cref{appendix:util semantics}. 
    The case for $\ff$ is identical.

  \item For $e = \observe x {e'}$, we again case on $g(x).$ For the remainder of this case let $\mathcal D = \denote{e'} g$, $\overline w = f g$.
  
  \begin{itemize}
    \item If $g(x) = \mathbf{TT}$, then $\denote{e} g = \denote{e'} g$. Also, by following \texttt{bc/obs}, we get that 
    $$EU_{(\varphi, \gamma, R)}(\overline w) 
      = \AMC(\varphi \land R \land \gamma \land x)_{\EU} = \AMC(\varphi \land \gamma\land R)_{\EU}
      $$
    by~\cref{lemma:ind conj prob} and we are done by IH.
    \item If $g(x) = \mathbf{FF}$, then $g \denote{\Gamma} = \pmb{\bot}$. So $\EU \circ (\denote{e} g) (\tt) = 0$. Also, by following \texttt{bc/obs}, we get that 
    $$\AMC(\varphi \land x \land R \land \gamma )_{\Pr} 
    = 0$$
    by~\cref{lemma:ind conj prob} and we are done. 
  \end{itemize}
  the proof for $\ff$ is identical.
  \item For $e = \bind x {b} {e'}$, assume $\mathcal D, \mathcal B, \mathcal{E}_{\mathbf{TT}}, \mathcal{E}_{\mathbf{FF}}$ from the proof of~\cref{thm:util pr correspondence}.
  Then we can derive 
  \begin{align*}
    \EU_{\mathsf{unn}} \circ \mathcal D (\tt)
      &= \sum_{r \in \R} r \times \mathcal D (\tt,r) 
      &\text{[Definition]}\\
      &=\sum_{r \in \R} r \times
        \left[
          \sum_{s \in \R} \mathcal B [(\tt,s)] \mathcal{E}_{\mathbf{TT}} (\tt, s-r)
          + \sum_{s \in \R} \mathcal B [(\ff,s)] \mathcal{E}_{\mathbf{FF}} (\tt, s-r)
        \right]
      &\text{[Unfolding]}\\
      &=\sum_{r \in \R} \sum_{s \in \R} r \times
         \mathcal B [(\tt,s)] \mathcal{E}_{\mathbf{TT}} (\tt, s-r)
      &\\
      &\quad + 
        \sum_{r \in \R }\sum_{s \in \R} r \times \mathcal B [(\ff,s)] \mathcal{E}_{\mathbf{FF}} (\tt, s-r)
      &\text{[Rewriting]}\\
      &=\sum_{r \in \R} \sum_{s \in \R} (r + k) \times
         \mathcal B [(\tt,r)] \mathcal{E}_{\mathbf{TT}} (\tt, k)
      &\\
      &\quad + 
        \sum_{r \in \R }\sum_{s \in \R} (r + k) \times \mathcal B [(\ff,r)] \mathcal{E}_{\mathbf{FF}} (\tt, k)
      &\text{[Rewriting]}\\
      &=\sum_{r \in \R} \sum_{s \in \R} r \times 
         \mathcal B [(\tt,r)] \mathcal{E}_{\mathbf{TT}} (\tt, k)
      + \sum_{r \in \R} \sum_{s \in \R} k \times 
         \mathcal B [(\tt,r)] \mathcal{E}_{\mathbf{TT}} (\tt, k)
      &\\
      &\quad + 
        \sum_{r \in \R} \sum_{s \in \R} r \times 
          \mathcal B [(\ff,r)] \mathcal{E}_{\mathbf{FF}} (\tt, k)
        + \sum_{r \in \R} \sum_{s \in \R} k \times 
          \mathcal B [(\ff,r)] \mathcal{E}_{\mathbf{FF}} (\tt, k)
      &\text{[Rewriting]}\\
      &=\AMC(\varphi_b \land R_b \land \gamma_b)_{\EU} 
        \times \AMC(\varphi_{e'}[x/T] \land \gamma_{e'}[x/T] \land R_{e'})_{\Pr}
      &\\
      & \quad + \AMC(\varphi_b \land R_b \land \gamma_b)_{\Pr} 
        \times \AMC(\varphi_{e'}[x/T] \land \gamma_{e'}[x/T] \land R_{e'})_{\EU}
      &\\
      &\quad + \AMC(\overline{\varphi_b} \land R_b \land \gamma_b)_{\EU} 
        \times \AMC(\varphi_{e'}[x/F] \land \gamma_{e'}[x/F] \land R_{e'})_{\Pr}
      &\\
      & \quad + \AMC(\overline{\varphi_b} \land R_b \land \gamma_b)_{\Pr} 
        \times \AMC(\varphi_{e'}[x/F] \land \gamma_{e'}[x/F] \land R_{e'})_{\EU}
      &\text{[IH]}
  \end{align*}
  at which point routine applications of~\cref{lemma:inc exc eu,lemma:ind conj eu}
  complete the proof. The case for $\ff$ is identical.
  \end{itemize}

\end{proof}
