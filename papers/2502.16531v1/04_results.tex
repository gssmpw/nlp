% !TEX root = template.tex

\section{Main Results}\label{sec:results}
% In this section, we present the algorithm's structure and our theoretical results. 
Our approach consists of an offline initial planning and an online adaptation phase. The latter involves agents exchanging request, reply, and confirmation messages to identify the best candidates for assisting with collaborative actions.
\subsection{Offline Initial Planning}\label{subsec:res-planning}
Given the FTS of an agent $\mathcal{T}_{\mathcal{G}}^{a_{i}}$ and the locally assigned task $\varphi^{a_{i}}_r$, the offline planner (see Sec. \ref{subsec:planning}) outputs the optimal initial plan for the agent, $\tau^{a_i}_{init}$, namely a sequence of states that satisfy $\varphi^{a_i}_r$, and the associated sequence of actions $\rho^{a_i}_{init}$. The resulting plan guarantees that $\mathit{trace}\left(\tau^{a_{i}}_{init}\right) \models \varphi^{a_{i}}_r$ where the satisfaction relation is defined in Sec. \ref{subsec:LTL}.

\subsection{Collaboration Request}\label{subsec:res-request}
 Given a collaborative action $\sigma^{a_i}_c\in\Sigma^{a_i}_c$, the request message sent by agent $a_i$ to all the other agents is
    \begin{equation*}
    \mathbf{Req}^{a_i}=\{(\sigma_d, \pi_{d}, T^{a_i}_c) \forall \sigma_d\in\mathrm{Depd}^{a_i}(\sigma^{a_i}_c)\}.
    \end{equation*}
Here, $\sigma_d$ are the assistive actions required to complete $\sigma^{a_i}_c$, $\pi_{d}$ are the regions where $\sigma_d$ takes place, $T^{a_i}_c$ is the time required before $\sigma^{a_i}_c$ starts according to $a_i$'s plan.
%\vspace{-0.3cm}
\begin{algorithm2e}[t]
\DontPrintSemicolon
\SetKwFunction{ChooseROI}{Choose\_ROI}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\ResetInOut{Output}

\Input{$l$, $\rho^{a_i}$, $H^{a_i}$, $T_{rem}$}
\Output{$\mathbf{Req}^{a_i}$}
\BlankLine
$s=1$, $T^{a_i}_c=T_{rem}$\;
\While{$T^{a_i}_c<H^{a_i}$}
{   
    \If{$\rho^{a_i}[l+s]\in\Sigma^{a_i}_c$}
    {
        \ForAll{$\sigma_d\in\mathrm{Depd}^{a_i}(\rho^{a_i}[l+s])$}
        {
            $\pi_d=$\ChooseROI{}\;
            add $(\sigma_d,\ \pi_d, T^{a_i}_c)$ to $\mathbf{Req}^{a_i}$\;            
        } 
        \Return{$\mathbf{Req}^{a_i} $}
    }
    $T^{a_i}_c = T^{a_i}_c + T^{a_i}_{\mathcal{G}}(\rho^{a_i}[l+s])$, $s=s+1$\;
}
\Return{$\emptyset$}
\caption{Check in horizon and Request}\label{alg:request}
\end{algorithm2e}
%\vspace{-0.3cm}
Algorithm \ref{alg:request} generates $\mathbf{Req}^{a_i}$ by assuming a generic time instant where agent $a_i$ is in state $\pi^{a_i}_{\mathcal{G}, l}$, the $l$-th element of its plan $\tau^{a_i}$ (possibly different from $\tau^{a_i}_{init}$ due to modification from the application of our approach). The agent executes action $\sigma^{a_i}_l$, corresponding to $\rho^{a_i}[l]$. The algorithm inputs the current state and action index ($l$), the action sequence $\rho^{a_i}$, the time remaining for the current action ($T_{rem}$), and the horizon length ($H^{a_i}$), considered to be of a similar order of magnitude of the actions in the experimental scenario. The algorithm checks future actions within the horizon for any collaborative tasks. If found, it creates a request, otherwise, it returns an empty set, sending no message.  
The \ChooseROI function selects an ROI based on the specific experimental setup. As a result, we cannot provide a general implementation; however, the specific one used in our experiment is detailed in Sec. \ref{subsec:exp-system}. Lastly, at the end of each iteration, we update $T^{a_i}_c$ by adding the duration of the last action.
\subsection{Reply}\label{subsec:res-reply}
Given a request for collaboration, $\mathbf{Req}^{a_i}$, the reply message sent by agent $a_j$ to $a_i$ is
\begin{equation*}
        \mathbf{Reply}^{a_j}=\{(\sigma_d, \pi_d, b^{a_j}_{d}, t^{a_j}_{d}) \forall (\sigma_d, \pi_d, T^{a_i}_c) \in \mathbf{Req}^{a_i}\}.        
\end{equation*}
Here, $b^{a_j}_{d}$ is a Boolean variable indicating that agent $a_j$ can execute $\sigma_d$ at region $ \pi_d$ and at time $t^{a_j}_{d}$.

%\vspace{-0.3cm}
\begin{algorithm2e}[t]
\DontPrintSemicolon
%\SetKwFunction{GetDetour}{GetDetour}
\SetKwFunction{Algorithmt}{Alogithm3}
\SetKwFunction{Dijkstra}{Dijkstra}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\ResetInOut{Output}

\Input{$\mathbf{Req}^{a_i}$, $m$, $\tau^{a_j}$, $\mathcal{T}_{\mathcal{G}}^{a_{j}}$, $\overline{T}^{a_j}$, $T_{rem}$}
\Output{$\mathbf{Reply}^{a_j}$, $D^{a_j}$}
\BlankLine
\ForAll{$(\sigma_d, \pi_d, T^{a_i}_c)\in \mathbf{Req}^{a_i}$}
{
    \eIf{$\overline{T}^{a_j}$ and $\langle \pi_d , \sigma_d\rangle \in \Pi^{a_j}_{\mathcal{G}}$}{
        $\pi^{a_j}_{\mathcal{G}, init}=\tau^{a_j}[m+1]$, \quad $\pi^{a_j}_{\mathcal{G}, fin}=\tau^{a_j}[m+2]$\;
        $\pi^{a_j}_{\mathcal{G}, targ}=\langle\sigma_d, \pi_d\rangle$\; 

        $(D_1, C_1)$=\Dijkstra{$\mathcal{T}_{\mathcal{G}}^{a_{j}}$, $\pi^{a_j}_{\mathcal{G}, init}$, $\pi^{a_j}_{\mathcal{G}, targ}$ }\;
        $(D_2, C_2)$=\Dijkstra{$\mathcal{T}_{\mathcal{G}}^{a_{j}}$, $\pi^{a_j}_{\mathcal{G}, targ}$, $\pi^{a_j}_{\mathcal{G}, fin}$ }\;
        $D^{a_j}(\sigma_d)=D_1+D_2[1:end]$\;
        $t^{a_j}_{d}=\sum_{i=0}^{len(C_1)-2} C_1[i]$\;
        add $(\sigma_d, \pi_d, \top ,  T_{rem}+t^{a_j}_{d})$ to $\mathbf{Reply}^{a_j}$\;
    }{
      add $(\sigma_d, \pi_d, \bot, K)$ to $\mathbf{Reply}^{a_j}$\;
    }
}
\Return{$D^{a_j},\ \mathbf{Reply}^{a_j}$}
\caption{Reply of  $a_j$ to a request from  $a_i$}\label{alg:reply}
\end{algorithm2e}
\begin{comment}
    

\begin{algorithm2e}
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\ResetInOut{Output}
\Input{$\pi_{\mathcal{G}, init}$, $\pi_{\mathcal{G}, fin}$, $\pi_{\mathcal{G}, targ}$, $\mathcal{T}_{\mathcal{G}}$ }
\Output{$T_D$, $D$}
\BlankLine
$(D_1, C_1)$=\Dijkstra{$\mathcal{T}_{\mathcal{G}}$, $\pi_{\mathcal{G}, init}$, $\pi_{\mathcal{G}, targ}$ }\;
$(D_2, C_2)$=\Dijkstra{$\mathcal{T}_{\mathcal{G}}$, $\pi_{\mathcal{G}, targ}$, $\pi_{\mathcal{G}, fin}$ }\;
$D$=$D_1$+$D_2[1:end]$

$T_D$=$\sum\limits_{i=0}^{len(C_1)-2} C_1[i]$\

\Return{$D,\ T_D$}
\caption{Build a detour from the given states}\label{alg:shortest_path}
\end{algorithm2e}
\end{comment}
%\vspace{-0.3cm}
Algorithm \ref{alg:reply} generates $\mathbf{Reply}^{a_j}$ by assuming a request arrives when agent $a_j$ is in state $\pi^{a_j}_{\mathcal{G}, m}$, the $m$-th element of its plan $\tau^{a_j}$, currently executing action $\sigma^{a_j}_m$. Alg. \ref{alg:reply} takes as inputs the request from $a_i$, the current state and action index ($m$), the plan $\tau^{a_j}$, the FTS $\mathcal{T}_{\mathcal{G}}^{a_j}$, the availability indicator $\overline{T}^{a_j}$ ($\top$ if available, $\bot$ if collaborating), and the time remaining for the current action $T_{rem} = \max(T^{a_j}_{\mathcal{G}}(\sigma^{a_j}_m) - \Delta t, 0)$ with $\Delta t$ the elapsed time from the start of $\sigma^{a_j}_m$. It outputs $\mathbf{Reply}^{a_j}$ and a detour dictionary $D^{a_j}$, which includes possible path detours, e.g., $D^{a_j}(\sigma_d)$ represents a detour that includes the state $\langle\pi_d, \sigma_d\rangle$. The algorithm checks if $a_j$ can assist with any requested actions, if so, from lines 3 to 7, it builds the detour between initial ($\pi^{a_j}_{\mathcal{G}, init}$) and final ($\pi^{a_j}_{\mathcal{G}, fin}$) states, passing through a target ($\pi^{a_j}_{\mathcal{G}, targ}$) state. It uses a modified \Dijkstra algorithm that returns the shortest path $D$ and an array $C$ with the associated action costs. The first call yields $D_1$ and $C_1$ i.e., the path and cost from the initial to the target state, similarly the second call produces $D_2$ and $C_2$ from the target to the final state. Then it merges the two paths into the detour saving it in the dictionary $D^{a_j}$. Lastly, it calculates the time $t^{a_j}_{d}$ for $a_j$ to start the assistive action, excluding the current action's completion time. Note that the summation avoids adding the cost of $\sigma_d$, stopping at $len(C_1)-2$. If a state is not feasible or $a_j$ is occupied, it sets $t^{a_j}_d = K$, where $K \gg T^{a_i}_c$; in practice, we set $K=10 T^{a_i}_c$, but any finite value would work. 
\begin{remark} \label{rem:delays}
    Note that $t^{a_j}_d$, $T^{a_i}_c$ are nominal times, but an agent might be affected by finite delays due to actions taking longer than expected in experiments.
\end{remark}
The definition of $T_{rem} \geq 0$ ensures nonnegative times from action delays. Note $a_i$ replies negatively to its request.
\subsection{Confirmation}\label{subsec:res-confirm}
 Given $\mathbf{Req}^{a_i}$, the confirmation message sent by  $a_i$ to $a_j$, based on $\mathbf{Reply}^{a_j} \quad \forall a_j \in \mathcal{N}$, has the following structure:
\begin{equation*}
          \mathbf{Conf}^{a_i}_{a_j}=\{(\sigma_d, \pi_d, c^{a_j}_{d},  T^{a_j}_d)\ \forall (\sigma_d, \pi_d, T^{a_i}_c)\in \mathbf{Req}^{a_i}\}. 
\end{equation*}
Here, $c^{a_j}_{d}$ is a Boolean variable indicating whether $a_j$ is confirmed to perform $\sigma_d$ at ROI $\pi_d$, and $T^{a_j}_d$ is the estimated start time for the collaboration. If $a_j$ is not selected, $T^{a_j}_d < 0$.
To build the confirmation messages, the Boolean variables $\{c^{a_j}_{d},\ a_j \in \mathcal{N}\}$ must satisfy two constraints: 1) Each agent in $\mathcal{N}$ can be confirmed for at most one $(\sigma_d, \pi_d) \in \mathbf{Req}^{a_i}$; 2) Exactly one agent must be confirmed for each action $(\sigma_d, \pi_d)$. Moreover, agents are selected based on their ability to perform the assisting action $\sigma_d$ as close as possible to the target time $T^{a_i}_c$.
Given $ |\mathbf{Req}^{a_i}| = M $, denote the assisting actions as $\{\sigma_d \mid d=1,\ldots, M\}$. The problem of finding $\{c^{a_j}_d\}$ can be formulated as a MIP solved for $\mathcal{R}=\mathcal{N}$
\begin{subequations}\label{eq:MIP}
    \begin{align}
        \min\limits_{\{c^{a_j}_d , a_j\in\mathcal{R}\}_{d=1}^M} &\quad \sum^{M}_{d=1}\sum^{|\mathcal{R}|}_{j=1}c^{a_j}_{d} | t^{a_j}_d-T^{a_i}_c| \label{eq:mip-cost}\\
        \text{s.t.}\ &\quad \sum^{M}_{d=1} b^{a_j}_{d}  c^{a_j}_{d}\leq1\ \ \forall a_j\in  \mathcal{R}\label{eq:mip-constr1}\\
         &\quad \sum^{ |\mathcal{R}|}_{j=1} b^{a_j}_{d} c^{a_j}_{d}=1\ \ \forall d\in \{1,...,M\}.  \label{eq:mip-constr2} 
    \end{align}
\end{subequations}
Once solved, if $c^{a_j}_d = \top$, $T^{a_j}_d = t^{a_j}_d$ otherwise, $T^{a_j}_d = -1$.
\begin{comment}
\begin{remark}\label{rmk:necessary_condition_MIP}
    A necessary condition for feasibility is $N > M$, since each action requires a distinct agent. Additionally, agent $a_i$ cannot be assigned to any actions, as its reply will have $b^{a_i}_d = \bot \ \forall \sigma_d$.
\end{remark}
\begin{remark}\label{rmk:feasibility_MIP}
    To ensure feasibility, there must exist a subset $\mathcal{N}_H \subseteq \mathcal{N}$ such that $ |\mathcal{N}_H| = M $; each agent can assist with at least one action and, there exists a combination of these agents such that we can assign to each of them exactly one action, with no overlap.
\end{remark}
\end{comment}
To handle the complexity of \eqref{eq:MIP}, which grows exponentially with the number of binary variables involved, we propose a filtering procedure to reduce the number of agents involved while maintaining optimality.
\begin{procedure2}[\textit{Filtering Procedure}]\label{proc:filtering}
    1) Define $\mathcal{N}_U\subseteq\mathcal{N}$ as the set of agents who can assist in at least one action i.e., $a_j \in \mathcal{N}_U$ if $b^{a_j}_d = \top$ for any $(\sigma_d, \pi_d, b^{a_j}_d, t^{a_j}_d) \in \mathbf{Reply}^{a_j}_{a_i}$.\\
    2) For each $\sigma_d \in \mathbf{Req}^{a_i}$, sort agents in ascending order of $\Delta^{a_j}_{d} = |t^{a_j}_d - T_c^{a_i}|$ and store the first $M$ for each $\sigma_d$ in $\mathcal{N}_F$.
\end{procedure2}
%Once filtering is complete, the MIP is solved for $\mathcal{R}=\mathcal{N}_F$.
\begin{theorem}\label{thm:MIP}
    Consider the set of agents $\mathcal{N}$ and the $M$ actions in $\mathbf{Req}^{a_i}$. Let $\mathcal{N}_F$ be the set of filtered agents given by Proc. \ref{proc:filtering}. Assume that the MIP in \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}$ is feasible. Then, the MIP in \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}$ and for $\mathcal{R}=\mathcal{N}_F$ have identical optimal solutions.
\end{theorem}
\begin{comment}
        \textbf{Feasibility}: We will show that $\mathcal{N}_F$ includes a feasible solution if $\mathcal{N}$ contains one. Let $\mathcal{N}_H \subseteq \mathcal{N}_F$ with $|\mathcal{N}_H|\geq M$. Note that by Proc. \ref{proc:filtering}, $\mathcal{N}_U$ contains agents who can assist in at least one action, ensuring each agent in $\mathcal{N}_F$ can help with at least one action. To construct $\mathcal{N}_H$, consider sets $\mathcal{N}_d$ ($d \in \{1,\ldots,M\}$), where $a_j \in \mathcal{N}_d$ if $b^{a_j}_{d}=\top$ and denote their cardinality by $|\mathcal{N}_d|=M_d$, w.l.o.g assume $M_1 \leq M_2 \leq \ldots \leq M_M$. Order agents in ascending order of $\Delta^{a_j}_d$ for all $\mathcal{N}_d$. Starting with $\mathcal{N}_{1}$, select the first agent $a_p$, assign it to $\sigma_1$, and add it to $\mathcal{N}_H$. For $\mathcal{N}_{2}$, find the first agent different from $a_p$, say $a_q$, assign it to $\sigma_2$, and add it to $\mathcal{N}_H$. Since $M_2 \geq 2$, $a_q \neq a_p$. Continue until $\mathcal{N}_M$, where $a_r$ is selected and assigned to $\sigma_M$, completing $\mathcal{N}_H$. This construction ensures that $\mathcal{N}_H \subseteq \mathcal{N}_F$ meets the properties in Rmk \ref{rmk:feasibility_MIP}, maintaining feasibility.\\ \textbf{Optimality}: 
\end{comment}
\begin{proof}
    Let $d\in\{1,...,M\}$ be the set of actions as denoted in Proc. \ref{proc:filtering} and rewrite the objective function in \eqref{eq:mip-cost} for $\mathcal{R}=\mathcal{N}$ as $\sum^{|\mathcal{N}|}_{j=1}c^{a_j}_{1}\cdot \Delta^{a_j}_1 +\ldots + \sum^{|\mathcal{N}|}_{j=1}c^{a_j}_{M}\cdot \Delta^{a_j}_M$. For each term (action), define $\mathcal{N}_d$ as the set of $M$ agents with the smallest $\Delta^{a_j}_d$ and then define $\mathcal{N}_F = \bigcup_{d=1}^{M} \mathcal{N}_d$. Rewrite the filtered objective function as $\sum_{a_j\in\mathcal{N}_1}c^{a_j}_{1}\cdot \Delta^{a_j}_1 +\ldots + \sum_{a_j\in\mathcal{N}_M}c^{a_j}_{M}\cdot \Delta^{a_j}_M$. Let $\{c^{a_j}_d\}^{*}_{\mathcal{N}}$ and $\{c^{a_j}_d\}^{*}_{\mathcal{N}_F}$ be the optimal solutions of \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}$, and $\mathcal{R}=\mathcal{N}_F$ respectively. Assuming the solutions differ, there must exist an agent $a_j \in \mathcal{N} \setminus \mathcal{N}_F$ so that $c^{a_j}_d = 1$ in $\{c^{a_j}_d\}^{*}_{\mathcal{N}}$ would result in a smaller value for the objective function compared to any agent $a_i \in \mathcal{N}_F$ where $c^{a_i}_d = 1$ in $\{c^{a_j}_d\}^{*}_{\mathcal{N}_F}$, implying $\sum^{|\mathcal{N}|}_{j=1}c^{a_j}_{d}\cdot \Delta^{a_j}_d<\sum_{a_j\in\mathcal{N}_d}c^{a_j}_{d}\cdot \Delta^{a_j}_d$.  Simplifying the inequality under the condition that $c^{a_j}_{d} = 1$ for only one $a_j$ leads to $\Delta^{a_j}_d < \Delta^{a_i}_d$. Since $a_j$ would then appear in $\mathcal{N}_d$, it must belong to $\mathcal{N}_F$ by Proc. \ref{proc:filtering} which contradicts the assumption. Hence, the optimal solutions are identical.
\end{proof}
Based on the solution of \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}_F$, $\mathbf{Conf}^{a_i}_{a_j} \forall\ a_j \in \mathcal{N}$ is built as follows: if $a_j\in\mathcal{N}_F$ and \eqref{eq:MIP} is feasible, add $(\sigma_d, \pi_d, c^{a_j}_d, T^{a_j}_d)$ to $\mathbf{Conf}^{a_i}_{a_j}$, otherwise add $(\sigma_d, \pi_d, \bot, -1)$.
Lastly, $a_i$ sends $\mathbf{Conf}^{a_i}_{a_j}$ to all $a_j\in\mathcal{N}$.
\par As in \cite{meng_paper}, the focus is on loosely coupled MAS where collaborations are sporadic compared to the total actions. This permits us to make the following assumption.
\begin{assumption}\label{ass:loosely_coupled} \textit{(Loosely Coupled System)} There exists a finite time $\mathbf{T}>0$ such that for each agent $a_i\in\mathcal{N}$ and any collaborative action $\sigma^{a_i}_c$ requested at time $t_c>0$, \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}_F$ will attain a solution within $t_c + \mathbf{T}$ for $\sigma^{a_i}_c$.
\end{assumption}
Assumption \ref{ass:loosely_coupled} allows for collaboration delays if no immediate solution is found. A plan adaptation process repeats until a solution is achieved, ensuring eventual success, as discussed in Sec. \ref{sub:res-plan-adaptation}. This assumption is reasonable for the tasks defined by \eqref{eq:recurringLTL}. We next show that based on the availability of agents and the scarcity of collaborative actions, both task progress and assistance can be accomplished. %Asm. \ref{ass:loosely_coupled} also excludes tightly coupled systems.

\subsection{Plan Adaptation}\label{sub:res-plan-adaptation}

After agent $a_i$ sends the confirmation messages to all agents $a_j \in \mathcal{N}$, two scenarios arise.
1) If \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}_F$ has no solution, $\sigma^{a_i}_c$ cannot be fulfilled based on the current replies. Agent $a_i$ will delay $\sigma^{a_i}_c$ by adding a detour of duration $T_{delay}$ (design parameter), using the self-loop $\sigma_0 = \mathit{None}$, and reattempt the request-reply-confirmation (RRC) cycle after this delay, while all the other agents proceed as planned and discard the detours. By Asm. \ref{ass:loosely_coupled}, a solution will eventually be found even if the RRC cycle might be repeated multiple times.
2) If \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}_F$ has a solution, each assisting agent $a_j$ checks if it has been selected (i.e., $c^{a_j}_d = \top$ for any $\sigma_d$). If selected, the agent updates $\tau^{a_j}$ to include $D^{a_j}(\sigma_d)$ and sets $\overline{T}^{a_j} = \bot$. If not selected, the agent continues with its original plan. The requesting agent $a_i$ sets $\overline{T}^{a_i} = \bot$.
Note that for assisting agents, we will set $\overline{T}^{a_j} = \top$ at the end of the detour to ensure they continue their plans. For $a_i$, we will set $\overline{T}^{a_i} =\top$ after $\sigma^{a_i}_c$ is completed.
\subsection{Time Synchronization}\label{subsec:res-synchro}
Suppose \eqref{eq:MIP} for $\mathcal{R}=\mathcal{N}_F$ has a solution, establishing a collaboration between the requesting agent $a_i$ and some assisting agents $a_j\in\mathcal{N}$. To successfully complete it, according to Def. \ref{def:succesful-collab} it is necessary to synchronize the execution of the actions. Let $\mathcal{S} \subseteq \mathcal{N}$ denote the set of agents involved in this collaboration. Once $a_j \in \mathcal{S} \setminus a_i$ is ready to execute $\sigma_d$, it sends a $\mathbf{Ready}^{a_j}$ message to $a_i$. When $a_i$ is ready to execute $\sigma^{a_i}_c$ and has received all $\mathbf{Ready}^{a_j}$ messages, it sends a $\mathbf{Start}$ message to all $a_j$, ensuring all agents start simultaneously. The following Lemma underpins the synchronization process.
\begin{lemma}\label{lemma:collaboration}
    There exists a finite time $T_s \geq 0$ such that, if $t$ is the time when $\mathbf{Conf}^{a_i}_{a_j}$ is sent, then by $t + T_s$ $\mathbf{Start}$ will be sent, and the agents will begin their collaboration.
\end{lemma}
\begin{proof}
    Consider an agent $a \in \mathcal{S} \setminus a_i$ in state  $\pi^{a}_{\mathcal{G}, init}$ at time $t$. After completing the assistive action $\sigma_d$ , it transitions to  $\pi^{a}_{\mathcal{G}, fin} = \langle \pi_d, \sigma_d \rangle$  via a finite sequence $\rho_d \subset \rho^a$, where  $\rho^a$ is the action sequence associated with the plan $\tau^a$. Each action $ \sigma_i \in \rho_d $ has an effective duration $ T_{\sigma_i} + d_{\sigma_i} $, where $ T_{\sigma_i} = T^{a}_{\mathcal{G}}(\sigma_i) < \infty $ and $ 0\leq d_{\sigma_i}<\infty $ is the delay (Rmk. \ref{rem:delays}).
    The time before starting $ \sigma_d $ is $ T^{a}_S = \sum_{i=0}^{|\rho_d| - 2} (T_{\sigma_i} + d_{\sigma_i}) < \infty $. After $ T^{a}_S $, $ a $ starts $ \sigma_d $ and sends $ \mathbf{Ready}^{a} $. The same holds for $ a_i $.
    Let $ T_S = \max_a \{T^{a}_S\} < \infty $ for $ a \in \mathcal{S} $. At $ t + T_S $, all agents send $ \mathbf{Ready}^{a} $, and $ a_i $ sends $\mathbf{Start}$, synchronizing the agents.
\end{proof}

\subsection{Approach Summary}
%With the following theorem, we will summarize our approach and prove its correctness.
\begin{theorem}
    Given the set of agents $\mathcal{N}$, let $a_i\in\mathcal{N}$ build the FTS $\mathcal{T}^{a_i}_{\mathcal{G}}$ \eqref{eq:agent-model} and get assigned a recurring LTL task $\varphi^{a_i}_r$ \eqref{eq:recurringLTL}. After constructing the initial plan as in Sec. \ref{subsec:res-planning} implement the  RRC cycle for any collaborative action as detailed in Alg. \ref{alg:request}, Alg. \ref{alg:reply}, and  Sec. \ref{subsec:res-confirm}.
    Then, apply the plan adaptation from Sec. \ref{sub:res-plan-adaptation}, followed by the synchronization method in Sec. \ref{subsec:res-synchro}. Under Asm. \ref{ass:loosely_coupled} this approach solves Problem \ref{problem:task}.
\end{theorem}
\begin{proof}
Consider an agent $a_i \in \mathcal{N}$. The initial plan satisfies the task specification $\varphi^{a_i}_r$, as ensured by the planner (Sec. \ref{subsec:res-planning}). 
As plan adaptation is permitted by the definition in \eqref{eq:recurringLTL}, task satisfaction is guaranteed. Furthermore, under Asm. \ref{ass:loosely_coupled} all collaborations are established within a finite time.By Lemma \ref{lemma:collaboration}, every collaboration will be successfully completed (Def. \ref{def:succesful-collab}), as there exists a finite time $T_S \geq 0$ that ensures all actions in the collaboration start simultaneously at $t + T_S$. Therefore, $\varphi^{a_i}_r$ is satisfied. Since $a_i$ can be any agent, this holds for all $a_i \in \mathcal{N}$, ensuring that the proposed approach solves Problem \ref{problem:task}.
\end{proof}
\subsection{Complexity Reduction}\label{subsec:res-complexity}
Compared to \cite{meng_paper}, by using the ROI representation instead of grid partitioning, we reduce the size of $\mathcal{T}^{a_i}_{\mathcal{M}}$, which also reduces the size of $\mathcal{T}^{a_i}_{\mathcal{G}}$. Additionally, in Alg. \ref{alg:reply}, we replace the more complex PBA $\mathcal{A_P}$ with the simpler FTS $\mathcal{T}^{a_i}_{\mathcal{G}}$ to ensure task satisfaction, leveraging the properties given by \eqref{eq:recurringLTL} to maintain LTL compliance. This reduces our approach's overall complexity, particularly since the most expensive operations in Alg. \ref{alg:reply} are the two calls to \Dijkstra, which have a quadratic cost on the number of states. Lastly, the filtering procedure before solving the MIP ensures that only $M \leq |\mathcal{N}_F| \leq M^2$ agents are involved, offering substantial complexity reduction when $N \gg M$. Moreover, by Thm. \ref{thm:MIP}, if $M=1$ Proc. \ref{proc:filtering} returns the optimal agent, solving  the MIP \eqref{eq:MIP} directly under negligible computational cost.

