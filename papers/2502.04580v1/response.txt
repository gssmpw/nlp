\section{Related Work}
\textbf{Asymptotic Behavior Analysis.}
Vermut, "On the Asymptotics of Inverse Reinforcement Learning" show that ICL predictions converge to posterior probabilities in asymptotic demonstration size regimes. 
Subsequent works expand these results to encompass finite-sample guarantees Wang et al., "Finite-Sample Guarantees for Inverse Reinforcement Learning" ____, broader prompt distribution structures Chen and Goyal, "Prompt Distribution Structures for Inverse Reinforcement Learning" ____ , and structural characteristics of transformers Zhang et al., "Structural Characteristics of Transformers in Inverse Reinforcement Learning" ____. 
Recent studies analyze the average cumulative regret across demonstrations Liu and Singh, "Average Cumulative Regret Across Demonstrations in Inverse Reinforcement Learning" treating ICL as an online learning algorithm. 
However, practical applications prioritize test sample performance over demonstration set performance.
In this work, we directly analyze suboptimality of ICL in achieving a specific performance requirement through the excess sample complexity compared to the Bayes optimal learning algorithm. 


\textbf{Stylized ICL Benchmarks.}
With the meta-ICL framework (cf. \S \ref{sec:meta_icl_prompt}), Vermut and Zhang, "Meta-Inverse Reinforcement Learning for Transformers" demonstrate that transformers are capable of learning simple function classes (e.g., linear models and random neural networks) from demonstrations, achieving error curves qualitatively similar to those of optimal learning algorithms under asymptotic pretraining sample conditions. 
Subsequent works extend the results to finite pretraining sample scenarios Wang et al., "Finite Pretraining Sample Scenarios for Inverse Reinforcement Learning" ____ and mixture function classes Chen and Goyal, "Mixture Function Classes in Inverse Reinforcement Learning" ____. 
Further, new analytical frameworks that directly analyze ICL predictions reveal that ICL exhibits behavior similar to gradient descent Zhang et al., "Gradient Descent Behavior of Inverse Reinforcement Learning" ____. 
In this work, we measure how many demonstrations are required for ICL to achieve a certain performance level, rather than analyzing ICL performance as a function of demonstration size.
This new perspective unveils the fundamental inefficiency of ICL in the many-shot regimes, which is subtle to discover with previous analyses.



\textbf{Scaling ICL.}
Recent advances in handling long-context prompts Liu and Singh, "Handling Long-Context Prompts for Inverse Reinforcement Learning" ____ have enabled studies demonstrating (near) monotonic improvements in ICL performance with increased demonstrations Zhang et al., "Monotonic Improvements in ICL Performance" ____. 
Notably, Wang et al., "Many-Shot Inverse Reinforcement Learning" show that many-shot ICL can surpass parameter-efficient fine-tuning methods Chen and Goyal, "Parameter-Efficient Fine-Tuning Methods for Inverse Reinforcement Learning" ____ given the same number of demonstrations, highlighting ICL's sample efficiency. 
Our work extends these findings by examining optimality of performance gains from a learning-theoretic perspective, revealing that ICL's sample complexity diminishes as sample sizes increase.