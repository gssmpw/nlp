@inproceedings{agarwal2024many,
  title={Many-shot in-context learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie and Zhang, Biao and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and others},
  booktitle={Neural Information Processing Systems},
  year={2024}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@inproceedings{anil2024many,
  title={Many-shot jailbreaking},
  author={Anil, Cem and Durmus, Esin and Rimsky, Nina and Sharma, Mrinank and Benton, Joe and Kundu, Sandipan and Batson, Joshua and Tong, Meg and Mu, Jesse and Ford, Daniel J and others},
  booktitle={Neural Information Processing Systems},
  year={2024}
}

@inproceedings{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{bertsch2024context,
  title={In-context learning with long-context models: An in-depth exploration},
  author={Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham},
  journal={arXiv preprint arXiv:2405.00200},
  year={2024}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@inproceedings{garg2022can,
  title={What can transformers learn in-context? A case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{jeon2024information,
  title={An information-theoretic analysis of in-context learning},
  author={Jeon, Hong Jun and Lee, Jason D and Lei, Qi and Van Roy, Benjamin},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{li2023_manydems,
  title={In-context learning with many demonstration examples},
  author={Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2302.04931},
  year={2023}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@inproceedings{li2023transformers_topic,
  title={How do transformers learn topic structure: Towards a mechanistic understanding},
  author={Li, Yuchen and Li, Yuanzhi and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@inproceedings{panwar2023context,
  title={In-context learning through the {B}ayesian prism},
  author={Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
    booktitle={International Conference on Learning Representations},
    year={2024},
}

@inproceedings{pathak2023transformers,
  title={Transformers can optimally learn regression mixture models},
  author={Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@inproceedings{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-{B}ayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@inproceedings{xie2021explanation,
  title={An explanation of in-context learning as implicit {B}ayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
    booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{zhang2023and,
  title={What and how does in-context learning learn? {B}ayesian model averaging, parameterization, and generalization},
  author={Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.19420},
  year={2023}
}

