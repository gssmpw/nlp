%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% \usepackage{caption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage{scalerel}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

\input{math_commands}

% If accepted, instead use the following line for the camera-ready submission:

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
% \usepackage[dvipsnames]{xcolor}

% \newcommand{\todo}[1]{{\color{red}[TOOD:#1]}}
% \newcommand{\change}[1]{{\color{blue}#1}}
% \newcommand{\ccm}[1]{{\color{red}#1}}
% \newcommand{\note}[1]{{\color{ForestGreen}[Comment: #1]}}
% \newcommand{\confusing}[1]{{\color{ForestGreen}#1}}
% \newcommand{\emergency}[1]{{\color{YellowOrange}#1}}
% \newcommand{\later}[1]{{\color{CadetBlue}[#1]}}

\usepackage[noorphans,vskip=0.1ex]{quoting}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context}

\begin{document}

\twocolumn[
\icmltitle{Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context}




% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Taejong Joo}{nu}
\icmlauthor{Diego Klabjan}{nu}

\end{icmlauthorlist}

\icmlaffiliation{nu}{Department of Industrial Engineering \& Management Sciences, Northwestern University, Evanston, IL, USA}

\icmlcorrespondingauthor{Taejong Joo}{taejong.joo@northwestern.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{In-Context Learning, Transformer, Large-Language Model, Long Context, Many-Shot ICL, Scaling, Meta Learning, Benchmark}

\vskip 0.3in
]

\printAffiliationsAndNotice{} % otherwise use the standard text.


\begin{abstract}
Transformers have demonstrated remarkable in-context learning (ICL) capabilities, adapting to new tasks by simply conditioning on demonstrations without parameter updates. Compelling empirical and theoretical evidence suggests that ICL, as a general-purpose learner, could outperform task-specific models. However, it remains unclear to what extent the transformers optimally learn in-context compared to principled learning algorithms. To bridge this gap, we introduce a new framework for quantifying optimality of ICL as a learning algorithm in stylized settings. Our findings reveal a striking dichotomy: while ICL initially matches the efficiency of a Bayes optimal estimator, its efficiency significantly deteriorates in long context. Through an information-theoretic analysis, we show that the diminishing efficiency is inherent to ICL. These results clarify the trade-offs in adopting ICL as a universal problem solver, motivating a new generation of on-the-fly adaptive methods without the diminishing efficiency. 
\end{abstract}


\section{Introduction}
Transformers, particularly large language models (LLMs), are able to perform \emph{in-context learning} (ICL) \citep{mann2020language}; they can adapt to new tasks simply by conditioning on demonstrations in their input prompt \citep{xie2021explanation}.  
Not only conveniently operated without any explicit parameter updates, but ICL even with just a few demonstrations (a.k.a. \textit{few-shot} ICL) surprisingly outperforms task-specific state-of-the-art models in diverse tasks, from question answering to common sense reasoning \citep{chowdhery2023palm, touvron2023llama,   mann2020language}.



This raises a fundamental question about how we shape artificial intelligence systems: Could ICL serve as a universal learner, obviating the need for task-specific models? 
To answer this, we must first address a more precise question:
\begin{quoting}
    \emph{How optimal is ICL as a learning algorithm, compared to principled learning algorithms?}
\end{quoting}

At first glance, the impressive performance of few-shot ICL and more recently many-shot ICL \citep{bertsch2024context,agarwal2024many} might seem to be an affirmative answer. 
However, this conclusion would be premature. 
Even when ICL outperforms state-of-the-art task-specific models or matches (super) human-level performances, it may still not be an optimal learning algorithm. 
This is evidenced by the results that carefully fine-tuned LLMs often outperform ICL when provided with the same amount of demonstrations  \citep{min2021metaicl,zhao2024context}. 


The question in principle can be accurately answered by comparing ICL with principled learning algorithms across LLMs with different data and model scales \citep{wei2023larger, raventos2024pretraining} on diverse types of tasks \citep{srivastava2022beyond,wei2022emergent}. 
However, the computational demands for training modern LLMs pose significant challenges for evaluating optimality of ICL as a learning algorithm.
The goal of this work is to answer the question without such prohibitive computational demands.





\textbf{Previous attempts.}
To answer the question, theoretical studies have analyzed \textit{asymptotic behavior of ICL} using rich tools from statistics and learning theory, such as regret and generalization bounds \citep{jeon2024information, zhang2023and, bai2024transformers,li2023transformers}. 
However, these asymptotic results fall short of fully characterizing real-world LLM behavior.
For instance, the regret upper bound for LLMs become nearly vacuous in few-shot regimes \citep{langford2001not,dziugaite2017computing}, which cannot explain the striking few-show ICL performances. 
Moreover, because other principled learning algorithms have the similar asymptotic behavior, it remains unclear whether ICL is a \emph{better} learning algorithm than such learning algorithms.


\emph{Physics-style} or synthetic benchmarking approaches have provided valuable insights that \emph{transformers might optimally learn in-context}, isolating core aspects of LLM training in controlled environments \citep{allen2023physics, garg2022can, ahn2023learning}. 
These approaches by nature can enable an efficient, comprehensive comparison between ICL and principled learning algorithms with arbitrarily high levels of statistical significances, while providing insights that often generalize to real-world LLMs despite inherent simplifications (cf. \S \ref{subsec:on_stlyzed_setting}). 
Concretely, by examining ICL performances across different demonstration sizes in a stylized benchmark, \citet{garg2022can} and follow-up works \citep{akyurek2022learning,von2023transformers} show that ICL seemingly learns new tasks with an efficiency comparable to provably optimal algorithms. 
However, these works have not yet provided an explicit relationship between relevant quantities (e.g., sample complexity and the optimality gap). 
Thus, the question of \textit{to what extent} transformers can learn optimally in-context remains unanswered.



\textbf{New benchmarking framework.} 
We revisit the performance profiles \citep{dolan2002benchmarking}—classic benchmarking framework for optimization software—for \textit{benchmarking} optimality of ICL as a learning algorithm in the stylized ICL setting \citep{garg2022can}.
Our framework can quantify \emph{how many more demonstrations} are required for ICL to achieve a certain performance compared to principled learning algorithms.
Thus, our analysis can accurately describe optimality of ICL with a more intuitive measure, making fundamental progress in physics-style approaches for ICL. 


\textbf{Unveiling diminishing efficiency in long context.} As a result, we uncover a new insight on optimality of ICL in \S \ref{sec:benchmark_icl}:
\begin{quoting}
    \emph{While ICL initially matches the efficiency of the Bayes optimal estimator, its efficiency significantly deteriorates in long context.}
\end{quoting}



More precisely, for low performance requirements, ICL achieves near optimal sample complexity comparable to the Bayes optimal estimator, aligning with its strong few-shot performance observed in practice. 
However, ICL's sample complexity sharply deteriorates beyond a certain threshold, often requiring 1.5 times more demonstrations to achieve high performance requirements than the Bayes optimal estimator.  
Further, we provide evidence that ICL may lack fundamental statistical properties (e.g., consistency and asymptotic efficiency) unlike the principled learning algorithms, which allow learning algorithms to benefit from large sample sizes.
Crucially, this novel insight would be difficult to uncover through many-shot ICL experiments on real-world LLMs due to intractability \citep{agarwal2024many} or analysis tools in the stylized setting \citep{garg2022can}, as ICL errors generally decrease with more demonstrations.


\textbf{Intrinsic suboptimality of ICL.} 
We prove that ICL without diminishing efficiency has stringent necessary conditions (e.g., negligible excess risk) using information-theoretic tools.
Crucially, the result is independent to particular instantiation of models and environments, suggesting the diminishing efficiency is intrinsic to the ICL mechanism itself.



This discovery unveils a hidden \emph{technical debt} in the ICL mechanism: the price we pay for its training-\emph{free} adaptability is a fundamental inefficiency in sample complexity that compounds as we push toward higher performance targets with the current ICL mechanism \textit{as is}. 


\textbf{Impact and outlook.}
Taken together, our findings suggest a more nuanced view of ICL than the prevailing excitement for replacing task-specific fine-tuned models with ICL as a universal problem solver. 
While ICL's ability to adapt without training remains attractive, our work reveals the fundamental technical debt that must be considered in AI system designs.
Crucially, this debt appears intrinsic to the ICL mechanism and thus unlikely to be serviced by simply scaling data and model sizes.
We hope these insights clarify the trade-offs in adopting ICL as a universal problem solver and motivate a new generation of ``on-the-fly'' adaptive methods without the diminishing efficiency.





\section{Setup} \label{sec:setup}
In \S \ref{sec:meta_icl_prompt}, we describe the meta ICL environment for evaluating ICL as a learning algorithm, followed by designs of a transformer for solving the meta ICL task (\S \ref{subsec:transformer}). We then devise principled predictors (\S \ref{subsec:principled_baselines}) and compare them with transformers using performance measures defined in \S \ref{subsec:measures}.


\subsection{Meta ICL Environment} \label{sec:meta_icl_prompt} 
In the meta ICL \citep{garg2022can}, each prompt characterizes an instance of a learning problem. 
Specifically, a prompt $H_T$ consists of demonstrations with a test input, i.e., $H_T \triangleq (X_1, Y_1,\cdots, X_T, Y_T, X_{T+1})$, and each output is generated by some function $f^*$, i.e., $Y_t = f^*(X_t)$ for $t \in [T+1] \triangleq \{ 1,2, \cdots, T+1 \}$. 
Here, the goal of a transformer is formalized as accurately predicting $Y_{T+1}$ with $H_t$, which requires to (implicitly) infer the underlying function $f^*$ from the demonstrations.
We denote the set of demonstrations as $D_T \triangleq (X_1, Y_1, \cdots, X_T, Y_T)$.


For the data generating distribution of a prompt $H_T$, we follow the approach of sampling target functions \(f^*\) from a  \emph{hierarchical distribution} \citep{panwar2023context} to capture a more interesting aspect of a learning algorithm---model selection. 
Under the hierarchical $f^*$, the prompt $H_T$ is realized by the following sampling process, which is denoted as $H_T \sim \PR(\cdot; \gE)$ with parameters $\gE \triangleq ([M], \sigma_w^2, \sigma_\epsilon^2)$. 


\textbf{1)} Sample the implicit dimension $m \sim \gU([M])$ from a uniform distribution over set $[M]$ and construct the (unobservable) feature space $\Phi_m(\gX)$:
\begin{equation*}
    \Phi_m (x) \triangleq [1, \cos(\frac{\pi x}{\gT}), \sin(\frac{\pi x}{\gT}), \cdots, \cos(\frac{m \pi x}{\gT}), \sin(\frac{m \pi x}{\gT})]
\end{equation*}
where $\gT > 0$ controls the frequency of the trigonometric functions.

\textbf{2)} Sample weight $w_m \sim \gN(0, \sigma_w^2 \mathbf{I}_{2m + 1})$, where \(\mathbf{I}_{2m+1}\) is the identity matrix with rank $2m-1$. 
The weight $w_m$ defines the target function $f^*$: 
\begin{equation*}
    f^*(x) \triangleq w^{\top}_m \Phi_m(x) / \sqrt{2m+1},
\end{equation*}
where the constant $\sqrt{2m + 1}$ makes the variance of $f^*$ remains constant across different \(m\). We let $\gF_m \triangleq \{ w^T \Phi_m(\cdot) | w \in \R^{2m+1}, \Phi_m: \gX \rightarrow \R^{2m+1} \}$ denote the set of all target functions with implicit dimension $m$.
    
\textbf{3)} Construct a prompt $H_T$ with a test output $y_{T+1}$ by $x_t \sim \mathcal{U}([x_{\min}, x_{\max}]), y_t = f^*(x_t) + \epsilon_t$ for $t \in [T+1]$, where $\epsilon_t \sim \gN(0, \sigma_\epsilon^2)$ is a random observation noise.

This hierarchical sampling ensures a diverse range of target functions with varying complexities, where a different realization of $(m, w_m)$ instantiates a new learning problem. 

In this work, we benchmark ICL with respect to different configurations of $\gE$, called \textit{scenario}, to enable comprehensive evaluations that could be encountered in practical scenarios (e.g., low signal-to-noise ratio (SNR),  defined as $Var(f^*) / \sigma_\epsilon^2$, for emulating a highly noisy environment). 
Following \citet{panwar2023context}, we set $\gT = x_{\max} = - x_{\min} = 5$ (our findings are indifferent to these values).
We denote $\gS$ as a set of scenarios and $\gE_s$ as parameters of a scenario $s \in \gS$. 
We also have $H_T^s \triangleq (X_1^s, Y_1^s, \cdots, X_{T+1}^s)$ generated from $\PR(\cdot ; \gE_s)$ for each scenario $s$, where we omit superscripts when there is no ambiguity.


\subsection{Transformers} \label{subsec:transformer}
For a transformer $\text{TF}_\theta$, we adopt the setup from \citet{garg2022can} and follow-up works \citep{panwar2023context,von2023transformers,akyurek2022learning,raventos2024pretraining} that use the GPT-2 \citep{radford2019language} architecture (cf. details in Appendix \ref{appx:add_details}).
For optimizing $\theta$ in the pretraining stage, we use the following minimization objective
\begin{equation} \label{eq:icl-pretrain}
    \gL(\theta) 
    \triangleq \E_{H_{T_\text{train}}} \left[ \tfrac{1}{T_\text{train}} {\textstyle \sum_{t=0}^{T_\text{train} - 1}} l(\text{TF}_\theta(H_{t}), Y_{t+1}) \right]
\end{equation}
where $H_{T_\text{train}}$ is generated by the prompt distribution described in \S \ref{sec:meta_icl_prompt}. We use the squared loss function for $l$, following previous works in the regression setting. 
Also, we set $T_\text{train} = 50$ for all scenarios, which is roughly $2 \cdot (2M+1)$ as in the previous works \citep{garg2022can, panwar2023context}. We train $\text{TF}_\theta$ separately for each scenario.



\subsection{Principled Baselines} \label{subsec:principled_baselines}
To benchmark ICL, we derive principled baselines that learn from demonstrations $D_t$ and produce a prediction function $f_{b}(\cdot; D_t)$, where $b$ is the identifier of a particular baseline. 
We denote $f^{t}_{b}(x) \triangleq f_{b}(x; D_t)$ and $f^{t}_{\textsf{ICL}}(X_{t+1}) \triangleq \text{TF}_\theta(H_t)$ when ever there is no ambiguity. 


The optimal baseline is Bayesian model averaging (BMA), which makes prediction by aggregating models from different hypothesis classes:
\begin{equation}\label{eq:optimal_predictor}
    f^t_{\textsf{BMA}}(x) = {\textstyle \sum_{m \in [M]}} p(\mathcal{F}_m \mid D_t) \, \hat{w}_m^{\top}(D_t) \Phi_m(x),
\end{equation}
where \( p(\mathcal{F}_m \mid D_t) \) is the posterior probability of model class \( \mathcal{F}_m \) and \( \hat{w}_m \) is the ridge regression estimator for \( \mathcal{F}_m \), defined as $\hat{w}_m(D_t) = ( \Phi_{m,t}^\top \Phi_{m,t} + \tfrac{\sigma_\epsilon^2}{\sigma_w^2} \mathbf{I}_{2m+1} )^{-1} \Phi_{m,t}^\top \mY_t$ with \( \Phi_{m,t} \in \mathbb{R}^{t \times (2m+1)} \) whose \( k \)-th row is \( \Phi^{\top}_m(X_k) \) and \( \mY_t = (Y_1, \cdots, Y_t) \in \R^t \). 
It is a standard result that 
\begin{equation} \label{eq:bma_optimality}
    f^t_{\textsf{BMA}} \in \argmin_{f \in \gF} \E_{Y_{t+1}} \left[ l(f(X_{t+1}; D_t), Y_{t+1}) \mid H_t \right]
\end{equation}
holds almost everywhere for all $t \in \mathbb{N}$, where $\gF$ is the set of all functions from $H_t$ to $\R$ \citep{ahuja2023closer, bishop2007}.


In addition, we consider a family of principled baselines that embodies different model selection strategies while having the same model fitting capacity as the optimal predictor. Such baselines make predictions by 
\begin{equation} \label{eq:single_model_pred}
    f^t_{b}(x; D_t) = \hat{w}^\top_{m^\dagger_b}(D_t) \Phi_{m^\dagger_b}(x),
\end{equation}
where $m^\dagger_b \in \argmax_{m \in [M]} \{ \text{Score}_b(m) \}$ with $\text{Score}_b(\cdot)$ being some model selection criterion of $b$. 


\subsection{Measures for Benchmarking Optimality of ICL} \label{subsec:measures}
Inspired by seminal work \citep{dolan2002benchmarking} that benchmarks (deterministic) optimization software, we first define the base metric measuring the optimality of a learning algorithm in $s \in \gS$. Then, we present the performance measures summarizing the base metric across $\gS$. In the following, we let $\gB$ contain all baseline learning algorithms and ICL. 
We set the test prompt length as $T = 2 T_{\text{train}} = 100 $, which is within the length generalization regime \citep{zhou2024transformers}.

\textbf{Base metric.}
Our base metric is the \textit{performance ratio}, which normalizes the sample complexity of a learning algorithm by that of the best algorithm among all baselines.

\begin{definition}
\label{def:performance_ratio}
    For $b \in \gB$ at $s \in \gS$, the \textit{performance ratio} of a requirement $r$ against $\tilde{\gB} \subseteq \gB$ is defined as 
    $\etR_{b}^{s}(r; \tilde{\gB}) = 
        \etN_{b}^{s}(r) / \min_{\tilde{b} \in \tilde{\gB}} \{ \etN_{\tilde{b}}^{s}(r) \}$, 
    where $\etN_{b}^{s}(r) \triangleq 
    \min \left\lbrace t \mid 
        \E [l(f^{t}_{b}(X_{t+1}^s), Y_{t+1}^s)]
    \leq r \right\rbrace$ is the sample complexity of achieving the performance $r$.
\end{definition}


The performance ratio quantifies the relative efficiency of a learning algorithm, addressing data-dependent nature of the sample complexity. 
Specifically, $\etR_{b}^{s}(r; \tilde{\gB})$ indicates that the learning algorithm $b$ requires $\etR_{b}^{s}(r; \tilde{\gB})$ times more demonstrations to achieve performance $r$ at scenario $s$ compared to the best learner among $\tilde{\gB}$. When $\textsf{BMA} \in \tilde{\gB}$, algorithms with $\etR_{b}^{s}(r; \tilde{\gB}) = 1$ have optimal efficiency at $s$ due to \eqref{eq:bma_optimality}.

\textbf{Performance measures.}
Based on the performance ratio across different scenarios, our goal is to report a ``single'' score that summarizes how optimal ICL is across $\gS$. 
However, naively summarizing the performance ratio for a requirement $r$ is inappropriate because the difficulty of achieving $r$ varies across learning problems, making comparisons inconsistent.
Therefore, we define the \textit{reference performance quantile} $\psi_{\gB^{\text{ref}}}^{\gQ}(s)$ as the $\gQ$-th quantile of reference performances at $s$ for $\gQ \in (0,1)$. 
Here, we measure the performance quantile in a reverse order, for making higher performance quantile analogous to higher performance.
The reference performances at $s$ is defined as a set of performances achieved by reference models $\gB^{\text{ref}} \subseteq \gB$; that is, $\{\E [l(f_b^t(X_{t+1}^s), Y_{t+1}^s)] | b \in \gB^{\text{ref}}, t \in [T] \}$. 



With this idea, the performance ratios across $\gS$ is summarized by the \textit{mean performance ratio} and the \textit{performance profile}, which are defined as follows.

\begin{definition}
    For the performance quantile $\psi_{\gB^{\text{ref}}}^{\gQ}$, the \textit{mean performance ratio} of $b \in \gB$ against $\tilde{\gB} \subseteq \gB$ is defined as $\texttt{MPR}(b; \psi_{\gB^{\text{ref}}}^{\gQ}, \tilde{\gB}) \triangleq \frac{1}{|\gS|} \sum_{s \in \gS} \etR_{b}^s(\psi_{\gB^{\text{ref}}}^{\gQ}(s); \tilde{\gB})$.
\end{definition}


\begin{definition}
\label{def:performance_profile}
    For the performance quantile $\psi_{\gB^{\text{ref}}}^{\gQ}$, the \textit{performance profile} of $b \in \gB$ against $\tilde{\gB} \subseteq \gB$ at a ratio $\tau \geq 1$ is defined as 
    \begin{equation*}
        \rho_b(\tau; \psi_{\gB^{\text{ref}}}^{\gQ}, \tilde{\gB}) = \tfrac{1}{|\gS|} | \{ s \in \gS : \etR_{b}^{s}(\psi_{\gB^{\text{ref}}}^{\gQ}(s); \tilde{\gB}) \leq \tau \} |. 
    \end{equation*}
\end{definition}


The two measures capture complementary aspects of optimality of ICL.
Specifically, the mean performance quantile quantifies the average inefficiency of a model $b$ in attaining a certain performance, which is assumed to be achievable by $b$. 
In contrast, the performance profile measures the frequency with which a model $b$ can achieve the performance quantile given a tolerance for inefficiency. 
These intuitive measures provide novel insights into optimality of ICL that are not apparent in previous error rates-based comparisons and asymptotic analyses.

\subsection{On Usage of Stylized Setting}
The stylized setting discussed in this section provides a rigorous benchmark for studying optimality of ICL with, in theory, arbitrarily high levels of statistical significance, including performance comparisons against BMA as a fundamental limit. While it simplifies certain real-world elements (e.g., autoregressive loss), empirical evidence suggests that insights from this setting generalize remarkably well to real-world tasks \citep{ahn2023linear, li2023transformers_topic}. Though the setting necessarily simplifies real-world LLMs, its ability to deepen our understanding of ICL demonstrates its value. Refer to Appendix \ref{subsec:on_stlyzed_setting} for more details.


\section{Benchmarking ICL Efficiency} \label{sec:benchmark_icl}
We measure to what extents transformers efficiently learn a new task through ICL compared to the optimal learning algorithm (\S\ref{subsec:compare_opt}) and principled baselines (\S\ref{subsec:compare_base}). 


\subsection{Can Transformer Optimally Learn In Context?} \label{subsec:compare_opt}
We first examine the efficiency of ICL compared to the Bayes optimal predictor, which learns new concepts with optimal efficiency (cf. \eqref{eq:bma_optimality}). 
For comprehensive evaluation, we design the test scenarios with various levels of SNRs: $\gS = \{([M], \sigma_y^2, \sigma_w^2) \mid M=10, \sigma_y^2 \in \{ 0.003, 0.03, 0.3\}, \sigma_w^2 \in \{0.1, 1, 10\} \}$ (cf. \S \ref{sec:meta_icl_prompt}). 
Also, to minimize the impacts of stochasticity of the sampling process of $H_t$, we evaluate performances for each scenario 512 times.
Then, we analyze the mean performance ratio of ICL against BMA for all quantiles of performances achieved by ICL; that is, we measure $\texttt{MPR}(\textsf{ICL}; \psi_{\gB^{\text{ref}}_1}^{\gQ}, \tilde{\gB}_1)$ with $\gB^{\text{ref}}_1 \triangleq \{\textsf{ICL}\}$, $\gQ \in \{0.01, 0.1, \cdots, 0.9, 0.99 \}$, and $\tilde{\gB}_1 \triangleq \{\textsf{ICL}, \textsf{BMA}\}$. 
In this way, we measure the efficiency of ICL in achieving each performance level under various difficulties in extracting information from prompts. 
In the following, we regard prompts with more than 40 demonstrations as the many-shot regime where the average performance quantile is approximately 0.5 (cf. Figure \ref{results:perf_quant_samples} in Appendix).




\begin{figure}
    \vskip -0.1in
    \centering
    \includegraphics[width=0.8\linewidth]{figs/mpr.pdf}
    \vskip -0.25in
    \caption{Mean performance ratio of ICL against BMA across different performance requirements. The shaded areas represent the standard deviation of the corresponding performance ratio.}
    \label{benchmark:performance_ratio}
    \vskip -0.25in
\end{figure}




Figure \ref{benchmark:performance_ratio} reveals a striking dichotomy in optimality of ICL.

\textbf{Near optimal few-show efficiency.} For low performance quantiles ($\gQ \leq 0.3$), ICL demonstrates its remarkable near optimal efficiency.
Specifically, the mean performance ratio is at most 1.1, which means that it requires only 10\% more demonstrations on average than the optimal learning algorithm to achieve the performance lower than $\psi_{\gB^{\text{ref}}_1}^{0.3}(s)$ for $s \in \gS$. Considering the average sample complexity for the performance quantile of 0.3 is 19, this explains ICL's impressive few-shot performance observed in practice (e.g., demonstration sizes of 5 and 15 in \citet{mann2020language}).


\textbf{Suboptimal many-shot efficiency.} 
Starting from $\gQ = 0.3$ or more apparently from $\gQ=0.7$ onward, the performance ratio grows almost monotonically with $\gQ$, increasing from around 1.1 at $\gQ=0.3$ to around 1.2 at $\gQ = 0.7$ and to around 1.45 at $\gQ = 0.99$. 
That is, ICL becomes increasingly suboptimal compared to the optimal learning algorithm when pursuing high performance requirements.
Considering higher performance quantiles requires larger demonstration sizes, this indicates that efficiency of ICL decreases with the demonstration size.




Importantly, these findings do \textit{not} contradict established benefits of many-shot ICL \citep{bertsch2024context,agarwal2024many}; as we analyze later in Figure \ref{results:prim_anal}, ICL still achieves monotonic improvement in MSE with more demonstrations.
Rather, our novel evaluation framework reveals that this improvement comes at an increasingly inefficient sample complexity, indicating significant diminishing returns in extracting information from demonstrations.


\subsection{Benchmarking ICL Against Principled Baselines} \label{subsec:compare_base}
% \subsection{Efficiency of ICL Compared to Baselines with Restricted Capacity} 
We have shown that ICL is significantly inefficient compared to BMA in high performance regimes.
While BMA is learnable by minimizing \eqref{eq:icl-pretrain}, it might seem unrealistic for ICL to compete with BMA that performs the expensive model averaging operation.
Thus, we compare ICL with more practical baselines with a computational constraint that select a single model using principled criteria (cf. \eqref{eq:single_model_pred}):
Akaike Information Criterion (AIC) \citep{akaike1974new} as a minimax-rate optimal model selection mechanism, Bayesian Information Criterion (BIC) \citep{schwarz1978estimating} as a consistent model selection mechanism, and Bayesian Model Comparison (BMC) as an efficient BMA alternative selecting maximum a posteriori model class.
These baselines represent the spectrum of principled model selection methods, which often asymptotically converge to either AIC or BIC \citep{ding2018model}.




To quantitatively assess relative efficiency, we measure performance profiles $\rho_b(\tau; \psi_{\gB^{\text{ref}}_2}^{\gQ}, \tilde{\gB}_2)$ with $\gB^{\text{ref}}_2 =  \{\textsf{ICL}, \textsf{AIC}\}$ and $\tilde{\gB}_2 = \{\textsf{ICL}, \textsf{AIC}, \textsf{BIC}, \textsf{BMC}\}$. This allows us to measure the probability that each method achieves a reference performance level within given sample complexity budgets, which evaluates both efficiency and effectiveness (i.e., maximum achievable performances) of learning algorithms.




\begin{figure}
    \vskip -0.1in
    \centering
    \includegraphics[width=\linewidth]{figs/pp_threecol.pdf}
    \vskip -0.2in
    \caption{Performance profiles $\rho_b$ across different performance ratios $\tau$ under different target performance quantiles $\gQ$. Each curve represents the probability that a method achieves the desired performance within a factor $\tau$ of the best method's sample complexity (x-axes). Figure \ref{results:full_perf_prof} in Appendix illustrates results for all $\gQ$.}
    \label{results:prim_perf_prof}
    \vskip -0.23in
\end{figure}


\textbf{Superiority of ICL in few-shot regimes.}
Perhaps not surprisingly (given the results from comparison with BMA), ICL dominates the baselines with restricted capacity under low performance requirements. Specifically, it achieves the perfect performance profile at $\tau = 1$ for $\gQ \leq 0.3$. This means that it optimally attains the performance requirement in \textit{all} scenarios when $\gQ \leq 0.3$. 
Given that each baseline has its own strength in certain scenarios, this guarantee is quite strong and not observed in other baselines.
Further, for $\gQ = 0.4$, ICL reaches a perfect performance profile within $\tau \leq 1.2$. This means that ICL attains the required performance of $\gQ = 0.4$ in all scenarios by using at most 20\% more demonstrations on average compared to the best method in each scenario. 
Conversely, all baselines selecting a single model struggle in the low-performance regime due to high uncertainty under a small number of demonstrations preventing them from selecting the proper model class \citep{hoeting1999BMA, wasserman2000bayesian}. 



\begin{figure}
    \vskip -0.1in
     \centering
     \includegraphics[width=\linewidth]{figs/mse_twocol.png}
     \vskip -0.15in
    \caption{Mean squared errors for different demonstration sizes. Figure \ref{results:full_mse} in Appendix illustrates results for all $s \in \gS$ and $b \in \gB$.}
    \label{results:prim_anal}
    \vskip -0.23in
\end{figure}


\textbf{Inferiority of ICL in many-shot regimes.}
Figure \ref{results:prim_perf_prof} illustrates diminishing efficiency of ICL in long context regimes.
Specifically, as the performance requirement increases, the initial performance profile at $\tau=1$ is reduced, indicating the reduced probability that ICL learns the most efficiently among $\tilde{\gB}_2$.
Beside, the computational budget $\tau$ required to reach perfect performance profile increases as the performance requirement increases. 
Eventually for $\gQ \geq 0.8$, even at $\tau=3$, ICL achieves the performance profile around 0.8, which means that ICL cannot reach the performance requirements for 20\% of cases by using even 3 times more demonstrations than other models. 


Crucially, this increasingly suboptimal behavior is opposite to the behaviors of principled baselines.
In Figure \ref{results:prim_perf_prof}, as opposed to ICL, the principled learning algorithms significantly reduce the time to reach the (near) perfect performance profiles as $\gQ$ increases.
Eventually, despite their significant deficiencies in few-shot regimes, all such baselines become more effective (achieving higher performance profiles at $\tau = 3$) and more efficient (sharply improving the performance profiles with respect to $\tau$) than ICL in many-shot regimes. 
Therefore, some characteristics enabling learning algorithms to leverage large number of demonstrations might be missing in the ICL mechanism.


To gain further insights, we qualitatively analyze MSEs across different numbers of demonstrations for each scenario. 
As a trivial baseline, we also consider an ensemble that aggregates the ridge estimators $\{\hat{w}_m \}_{m \in [M]}$ using equal weights.
Figure~\ref{results:prim_anal} shows that while all methods show decreasing MSEs with more demonstrations, ICL exhibits persistent discrepancies from the principled learning algorithms in many-shot regimes.
Further, in Figure \ref{results:spd_anal}, we analyze the squared prediction difference between each model and the Bayes optimal predictor for each scenario. 
Critically, it reveals that while consistent estimators (BMC, BIC) seem to converge in $L^2$ (albeit at different rates), ICL's $L^2$ distance to $f^t_{\textsf{BMA}}$ plateaus after receiving few demonstrations.
This behavior mirrors the trivial ensemble, which does not update its hypothesis about the model class with demonstrations.
This suggests another fundamental limitation: ICL may lack asymptotic efficiency and consistency (cf. \citet{ding2018model} for formal definitions). These findings challenge the prevailing optimism about ICL's scalability.






\begin{figure}
    \vskip -0.1in
     \centering
     \includegraphics[width=\linewidth]{figs/spd_twocol.png}
     \vskip -0.15in
    \caption{Squared prediction differences between BMA and other methods for different demonstration sizes. Figure \ref{results:full_spd} in Appendix illustrates results for all $s \in \gS$.}
    \label{results:spd_anal}
    \vskip -0.23in
\end{figure}



\subsection{On Sources of the Diminishing Efficiency}
We observe a significant suboptimality of ICL under high performance requirements, which typically requires longer context sizes than the pretraining prompt (cf. Figure \ref{results:perf_quant_samples} in Appendix). 
Given universally observed deficiencies of machine learning models in the out-of-distribution regimes \citep{hendrycks2019benchmarking,koh2021wilds}, it is tempting to attribute the diminishing efficiency to the deficiencies in out-of-distribution regimes. 


We take a closer look at this in Figure \ref{results:prim_anal}, which corresponds to the achievable error due to the bias-variance decomposition. 
Recalling that $T_{\text{train}}=50$ was used for pretraining, Figure \ref{results:prim_anal} and Figure \ref{results:full_spd} in Appendix show no apparent differences in the achievable error between in-distribution and out-of-distribution regimes, except in low SNR scenarios $(\sigma_w^2, \sigma_\epsilon^2) = (0.1, 0.03)$ and $(\sigma_w^2, \sigma_\epsilon^2) = (1, 0.3)$. This finding aligns with the length generalization literature, which suggests that transformers often generalize to contexts up to 2.5 times longer than those seen during pretraining \citep{zhou2024transformers}.
Further, given that the average performance quantile at $T_{\text{train}}$ is 0.6, Figure \ref{benchmark:performance_ratio} reveals that fundamental inefficiency already emerges in the in-distribution regime.
 




Therefore, the diminishing efficiency observed in \S\ref{subsec:compare_opt} and \S\ref{subsec:compare_base} cannot be fully attributed to the transformers' out-of-distribution generalization capability. 
Rather, as we analyze next in \S \ref{sec:info_analysis_subopt}, it is intrinsic to the ICL mechanism itself. 




\section{Analyzing Suboptimality of ICL} \label{sec:info_analysis_subopt}
Using information-theoretic tools, we explain why ICL's efficiency as a learning algorithm diminishes in long context. 

\subsection{ICL Error Decomposition} \label{subsec:theory_setup}
Adopting a Bayesian viewpoint \citep{jeon2024information}, we denote the oracle distribution with $e$ drawn from an environment $\gE$ by $\bar{P}^{t}_{e}(\cdot) \triangleq \PR(Y_{t+1} \in \cdot | H_t, e) = \PR(Y_{t+1} \in \cdot | X_{t+1}, e)$ (e.g., $\gE$ characterizes the sampling process in \S \ref{sec:meta_icl_prompt} with $e = (m, w_m)$). 
Similarly, we let $\text{TF}_\theta$ models the conditional distribution of outputs, i.e., $\text{TF}_\theta(H_t) \triangleq P_\theta(Y_{t+1} \in \cdot \mid H_t) \triangleq P_{\theta}^t(\cdot)$. 
All subsequent discussions in this section assumes no distribution shift; that is, $\gE$ is the environment under which $\text{TF}_{\theta}$ was pretrained.
We assume that $Y_{t+1}$ is either discrete or continuous.


With this notation, the ICL performance with $t$ demonstrations from $\gE$ is defined as $\E \left[ -\log P^t_\theta(Y_{t+1}) \right] = \E \left[ -\log \bar{P}^{t}_{e}(Y_{t+1}) \right] + \E \left[ 
    \KL( \bar{P}^{t}_{e} \parallel P^{t}_{\theta})
\right] $ \citep{jeon2024information}. Here, the first term is the (irreducible) aleatoric uncertainty and constant with respect to $t$ in our setting.
The second term can be further decomposed as
\begin{multline} \label{eq:icl_perf_decomp}
    \E \left[ 
        \KL( \bar{P}^{t}_{e} \parallel P^{t}_{\theta})
    \right] 
        = \E \left[ 
            \int \log \frac{d\bar{P}^{t}_{e}}{dP^{t}_{\theta}}(y)
            \bar{P}^{t}_{e}(dy)
        \right] \\ 
        = \underbrace{\E \left[ 
            \KL(\bar{P}^{t}_{e} \parallel \hat{P}^{t}_{\gE})
        \right]}_{\triangleq \epsilon^t_{\text{Bayes}} (\text{Bayes risk})} 
        + \underbrace{\E \left[ 
            \log \frac{\hat{P}^{t}_{\gE}(Y_{t+1})}{P^{t}_{\theta}(Y_{t+1})} 
        \right]}_{\triangleq \epsilon^t_{\text{XS}} (\text{Excess risk})},
\end{multline}
where the second equality comes from the law of total expectation and $\hat{P}^{t}_{\gE}(Y_{t+1}) \triangleq \PR(Y_{t+1} \in \cdot | H_t, \gE)$ is the posterior over $Y_{t+1}$ given $H_t$. 



In \eqref{eq:icl_perf_decomp}, the Bayes risk $\epsilon_{\text{Bayes}}^t$ measures how well the Bayes-optimal predictor performs under uncertainty on $e$.
It is non-negative and \textit{decreases monotonically} with more demonstrations; that is, $\epsilon^{t + 1}_{\text{Bayes}} \leq \epsilon^t_{\text{Bayes}}$ for all $t \in \mathbb{N}$ \citep{jeon2022information}. 
Demonstration size $t$ required to bring this risk below a threshold $q$ is captured by $\etN_{\textsf{BMA}}(q) \triangleq \min_{t \in \mathbb{N}}  \{ \epsilon_{\text{Bayes}}^t \leq q \}$.
Here, $q$ represents the absolute value of the performance requirement (e.g., MSE), whereas $\gQ$ in \S \ref{sec:benchmark_icl} denotes the performance quantile.

The excess risk $\epsilon^t_{\text{XS}}$ measures the performance of the transformer relative to the Bayes optimal predictor.
Due to the non-negativity of excess risk and independence between $\text{TF}_\theta$ and $\epsilon_{\text{Bayes}}^t$, this term determines when ICL emerges and how well it can perform.  
For instance, if $\text{TF}_\theta$ achieves an excess risk curve such that $\epsilon^t_{\text{XS}} - \epsilon^0_{\text{XS}} \leq \epsilon_{\text{Bayes}}^0 - \epsilon_{\text{Bayes}}^t$, non-trivial ICL performance emerges, improving upon the zero-shot performance with demonstrations. 
Further, if $\epsilon^t_{\text{XS}} \to 0$ as $t \to \infty$, then ICL is Bayes-risk consistent and asymptotically matches BMA.
In \S \ref{subsec:on_excess_risk}, we dissect $\epsilon_{\text{XS}}^t$ based on our empirical results (\S\ref{sec:benchmark_icl}). 




\subsection{On Excess Risk} \label{subsec:on_excess_risk}
Interpreting the transformer's prediction in the meta-ICL setup as the Gaussian distribution (e.g., by adding a small random Gaussian noise to the prediction), the squared prediction difference in Figure \ref{results:spd_anal} is directly proportional to the excess risk, up to a constant scale and shift. 
The same applies to each baseline’s squared prediction difference, interpreted as its own excess risks.


In this regard, Figure \ref{results:spd_anal} illustrates that the transformer's excess risk remains roughly bounded within a modest interval in a certain length generalization regime (e.g., $t \leq 2 T_{\text{train}}$), suggesting that it would perform ICL non-trivially due to the monotonicity of $\epsilon_{\text{Bayes}}^t$.
However, once the context length becomes much longer than the one seen during pretraining (e.g., $t > 2 T_{\text{train}}$ in Figure \ref{fig:discussion_fig}), the excess risk deteriorates sharply. 
This explains why ICL is not a consistent learner, being dominated by the principled learning algorithms in large sample regimes, as we observed in \S\ref{subsec:compare_base}. 




We formally encode the above empirical observations about the non-vanishing excess risk curve into Assumption~\ref{assumption:bounded_appx}. 


\begin{assumption} \label{assumption:bounded_appx}
    There exist constants $(\bar{t}, \triangle_{\text{XS}} ) \in (\mathbb{N}, \R_{+})$ such that $0 \leq \triangle_{\text{XS}} \leq \epsilon_{\text{XS}}^{t^\prime}$ for all $t^\prime \geq \bar{t}$.
\end{assumption}

The assumption states that, after some reference point $\bar{t}$, the excess risks of $\text{TF}_\theta$ can be lower bounded. In other words, it assumes that $\text{TF}_\theta$ does not magically reduce its excess risk in the out-of-distribution context length regimes.  



Crucially, as we show in \S \ref{subsec:analysis}, $\triangle_{\text{XS}}$ controls a lower bound of ICL's suboptimal efficiency in learning from demonstrations. 
For a transformer with a strong length generalization ability, $\epsilon_{\text{XS}}^{t^\prime}$ in the assumption can also be upper bounded, making the subsequent suboptimality analysis nearly tight. 
In this regard, our analysis encompasses plausible (near) future advances in length generalization capability.
Therefore, our analysis under Assumption \ref{assumption:bounded_appx} is a general result highlighting the ICL mechanism's intrinsic flaws, isolating them from the transformer's length generalization capability. 



\subsection{Analyzing Suboptimality of ICL} \label{subsec:analysis}
Next, we explain the critical suboptimality of ICL observed in \S \ref{sec:benchmark_icl}, where ICL initially matches the efficiency of the optimal learning algorithm but starts to significantly deteriorate in many-shot regimes. 
To this end, we define suboptimality of ICL at performance requirement $q$ as the additional number of demonstrations required for ICL to achieve requirement $q$ compared to the Bayes optimal estimator, denoted as $\texttt{SubOpt}(q) \triangleq \min_{t} \{ t - \etN_{\textsf{BMA}}(q) \mid \epsilon_{\text{Bayes}}^{t} + \epsilon_{\text{XS}}^{t} \leq q  \}$. 
Here, we define suboptimality at $q$ with respect to the reducible part of the ICL performance (i.e., $\E \left[ \KL( \bar{P}^{t}_{e} \parallel P^{t}_{\theta})
\right]$), which is equivalent to defining it with respect to the ICL performance up to constant scaling in $q$.

The following theorem constructs a lower bound of $\texttt{SubOpt}(q)$ under Assumption \ref{assumption:bounded_appx} where $\I$ denotes the mutual information. 


\begin{theorem} \label{lemma:suboptimality_bound}
    Let us assume a tuple $(\bar{t}, \triangle_{\text{XS}})$ satisfies Assumption \ref{assumption:bounded_appx}. 
    For a sufficiently small $q$ such that  $\etN_{\textsf{BMA}}(q) \geq \bar{t}$, it holds that
    \begin{multline} \label{ineq:lemma_subopt}
        \texttt{SubOpt}(q) \geq LB(q) \triangleq \\  
        \min_{t \in \mathbb{N}} \left\lbrace t \mid 
            \I(Y_{\etN_{\textsf{BMA}}(q)};\tilde{D}_{t+1} \mid H_{\etN_{\textsf{BMA}}(q) - 1} )
             > \triangle_{\text{XS}} \right\rbrace
    \end{multline}
    where $\tilde{D}_{t+1}$ is a sample from the same distribution as $D_{t+1}$.
\end{theorem}



Theorem \ref{lemma:suboptimality_bound} intuitively characterizes suboptimality (cf. Figure \ref{fig:suboptimality_illustration} in Appendix for an illustration of the concept). 
Specifically, suppose the Bayes optimal learner requires $\etN_{\textsf{BMA}}(q)$ demonstrations to achieve the performance $q$.
Then, \(\texttt{SubOpt}(q)\) represents the additional demonstrations required for ICL to compensate for the excess risk \(\epsilon_{\text{XS}}^t\).
Here, the compensation represents how much the new demonstrations $\tilde{D}_{t+1}$ reduce the uncertainty about $Y_{\etN_{\textsf{BMA}}(q)}$ given a prompt $H_{\etN_{\textsf{BMA}}(q) - 1}$, which corresponds to the conditional mutual information in \eqref{ineq:lemma_subopt}.
The theorem is proven in \S \ref{appx:proof_suboptim_bounds}. 




Characterizing suboptimality with $\I(Y_{\etN_{\textsf{BMA}}(q)}; \tilde{D}_{t+1} \mid H_{\etN_{\textsf{BMA}}(q) - 1} )$ provides clear insights into ICL's suboptimality.
Specifically, transformers with small excess risks in the non-vanishing regime are less subject to suboptimality.
Besides, since a higher performance requirement (i.e., a smaller $q$) increases $\etN_{\textsf{BMA}}(q)$, suboptimality naturally increases due to reduced conditional mutual information. 
The following theorem, which is proven in \S \ref{appx:proof_necessary}, makes this intuition precise by establishing necessary conditions for $\texttt{SubOpt}(q)$ being constant with respect to $q$.




\begin{theorem} \label{thm:necessary_conditions}
    Let us assume a tuple $(\bar{t}, \triangle_{\text{XS}})$ satisfies Assumption \ref{assumption:bounded_appx} and let $q$ be such that $\etN_{\textsf{BMA}}(q) \geq \bar{t}$.
    If $LB(q^\prime) = LB(q)$ for all $\triangle_{\text{XS}} < q^\prime < q$, then either of the following condition holds:
    \begin{enumerate}
        \item \textbf{Negligible excess risk:}
        $\triangle_{\text{XS}} \leq \I(Y_{t}; \tilde{D}_1 | H_{t - 1})$ for all $t \geq \etN_{\textsf{BMA}}(q),$ and $LB(q) = 0$,
        \item \textbf{Negligible diminishing returns:} 
        $\I(Y_{\tilde{t}}; \tilde{D}_1 | H_{\tilde{t} - 1})  
                < \left(1 + \tfrac{1}{LB(q)} \right) \I(Y_{t}; \tilde{D}_1 | H_{t - 1})$ 
        for all $t \geq \etN_{\textsf{BMA}}(q)$, where $\tilde{t} \triangleq \etN_{\textsf{BMA}}(q) + LB(q)$ and $LB(q) > 0$.
    \end{enumerate}
\end{theorem}



Non-deteriorating suboptimality has stringent necessary conditions that rarely hold in practice.
Specifically, the \textit{negligible excess risk} condition requires that the information gain from a single demonstration, regardless of demonstration size, dominates the excess risk. While this may hold for few-shot regimes (explaining the significant efficiency of few-shot ICL), ensuring this assumption across all prompt lengths is quite strong given the diminishing nature of $\I(Y_t; \tilde{D}_1 \mid H_{t-1})$ with $t$ in most learning scenarios \citep{rissanen1984universal, clarke1990information}. 
For a similar reason, the \textit{negligible diminishing returns} condition, which requires a constant lower bound of $\I(Y_t; \tilde{D}_1 \mid H_{t-1})$ for all demonstration sizes $t$, is quite strong. 
Therefore, \(\texttt{SubOpt}(q)\) inevitably grows as \( q \) decreases, leading to increasing suboptimality of ICL under a high performance requirement as observed in \S \ref{sec:benchmark_icl}. 



As a concrete intuition on suboptimality, we consider the following crude approximations: (A1) $\epsilon_{\text{Bayes}}^{t} \approx C_1 / \sqrt{t}$ for some constant factor $C_1$ and
(A2) $\epsilon_{\text{XS}} \lesssim \epsilon_{\text{XS}}^{t}$ for all $t \in \mathbb{N}_{+}$. 
Here, (A1) corresponds to sublinear convergence of the Bayes posterior estimator, which holds in many cases \citep{rissanen1984universal, clarke1990information}, and (A2) corresponds to Assumption \ref{assumption:bounded_appx} with $(\bar{t}, \triangle_{\text{XS}}) = (0,\epsilon_{\text{XS}})$.
Replacing (A1) with other common bounds, such as $\epsilon_{\text{Bayes}}^{t} \approx C_1 / t$ or $\epsilon_{\text{Bayes}}^{t} \approx C_1 \exp(-t)$, yields similar results. 


Under (A1) and (A2), for performance achievable by the transformer (i.e., $q > \epsilon_{\text{XS}}$), a simple calculation gives $\texttt{SubOpt}(q) \gtrsim \frac{C_1^2}{(q-\epsilon_{\text{XS}})^2} - \frac{C_1^2}{q^2} \geq \frac{C_1^2 \epsilon_{\text{XS}}}{q^2 (q - \epsilon_{\text{XS}})}$.
Here, the rapid growth of $\texttt{SubOpt}(q)$ as $q$ decreases highlights the significant inefficiency of ICL in achieving high performance requirement.
Moreover, another way of improving suboptimality by reducing \(\epsilon_{\text{XS}}\), from the perspective of the rough power law estimations from the scaling laws \citep{kaplan2020scaling}, would require an exponential increase in pretraining data size or computational resources.
Thus, in either way, a transformer exhibits significant suboptimality in achieving high performance through ICL compared to principled learning algorithms.


\subsection{Impacts of Scaling Computations}
We find that simply scaling model size or pretraining prompt length does not fundamentally eliminate inefficiency in long context even though these modifications can reduce overall excess risk.  As shown in Figure \ref{fig:discussion_fig} in Appendix, larger capacities and longer pretraining contexts lower the magnitude of the excess risk but do not change its ``shape'' in many-shot regimes, leading to persistent suboptimality.  Thus, enhancing transformers' ability to handle longer contexts alone does not resolve this inefficiency. See Appendix \ref{appx:scaling_comps} for experimental settings and results.


 

\section{Related Work}
\textbf{Asymptotic Behavior Analysis.}
\citet{xie2021explanation} show that ICL predictions converge to posterior probabilities in asymptotic demonstration size regimes. 
Subsequent works expand these results to encompass finite-sample guarantees \citep{li2023transformers,zhang2023and,bai2024transformers}, broader prompt distribution structures \citep{li2023transformers, li2023transformers_topic, zhang2023and}, and structural characteristics of transformers \citep{zhang2023and}. 
Recent studies analyze the average cumulative regret across demonstrations \citep{zhang2023and,jeon2024information}, treating ICL as an online learning algorithm. 
However, practical applications prioritize test sample performance over demonstration set performance.
In this work, we directly analyze suboptimality of ICL in achieving a specific performance requirement through the excess sample complexity compared to the Bayes optimal learning algorithm. 


\textbf{Stylized ICL Benchmarks.}
With the meta-ICL framework (cf. \S \ref{sec:meta_icl_prompt}), \citet{garg2022can} demonstrate that transformers are capable of learning simple function classes (e.g., linear models and random neural networks) from demonstrations, achieving error curves qualitatively similar to those of optimal learning algorithms under asymptotic pretraining sample conditions. 
Subsequent works extend the results to finite pretraining sample scenarios \citep{raventos2024pretraining} and mixture function classes \citep{pathak2023transformers, panwar2023context}. 
Further, new analytical frameworks that directly analyze ICL predictions reveal that ICL exhibits behavior similar to gradient descent \citep{von2023transformers,akyurek2022learning}. 
In this work, we measure how many demonstrations are required for ICL to achieve a certain performance level, rather than analyzing ICL performance as a function of demonstration size.
This new perspective unveils the fundamental inefficiency of ICL in the many-shot regimes, which is subtle to discover with previous analyses.



\textbf{Scaling ICL.}
Recent advances in handling long-context prompts \citep{chen2023extending,su2024roformer,press2021train} have enabled studies demonstrating (near) monotonic improvements in ICL performance with increased demonstrations \citep{li2023_manydems, agarwal2024many, anil2024many}. 
Notably, \citet{bertsch2024context} show that many-shot ICL can surpass parameter-efficient fine-tuning methods \citep{hu2021lora} given the same number of demonstrations, highlighting ICL's sample efficiency. 
Our work extends these findings by examining optimality of performance gains from a learning-theoretic perspective, revealing that ICL's sample complexity diminishes as sample sizes increase.




\section{Conclusion}
The surprisingly strong ICL performance of LLMs suggest its potential to eliminate the need for task-specific models. 
To rigorously examine this potential, we developed a novel framework for benchmarking optimality of ICL as a learning algorithm against principled learning algorithms. 
We found that while few-shot ICL's efficiency is comparable to the Bayes optimal learning algorithm, its efficiency quickly diminishes with more demonstrations. 
Our information-theoretic analysis showed that the non-vanishing excess curve in long context causes fundamental inefficiency in many-shot regimes. 
This highlights the need for a new adaptation method that can reduce excess risk with more demonstrations, enabling sample-efficient learning of novel tasks while preserving the update-free nature of ICL.

\newpage


\section*{Impact Statement}
In this work, we study optimality of in-context learning as a learning algorithm against principled learning algorithms. Because our study focuses on theoretical aspects rather than practical applications, we do not foresee direct ethical concerns or societal impacts arising from our findings.


\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}    


\section{Additional Details} \label{appx:add_theoretical_details}




\subsection{On Usage of Stylized Setting}  \label{subsec:on_stlyzed_setting}


\textbf{Comprehensive analyses with statistical significance.}
The benchmark in the stylized settings in principle enables comprehensive comparisons across different environments (e.g., $\gS$) and architectures (e.g., different $\text{TF}_\theta$), achieving arbitrarily high levels of statistical significance. In empirical studies, these factors are constrained to the configurations of the datasets or the computational budgets.

\textbf{Comparison with the optimal method.}
The stylized setting enables comparison with principled learning algorithms. Specifically, BMA considered in \eqref{eq:optimal_predictor} provides the minimum achievable performance of any learning algorithms \textit{at all prompt lengths}. This strong guarantee is typically not possible in empirical studies, as even human performances could not be an oracle or simply not possible to attain with only the data provided to the transformer. 
Also, the theoretical studies themselves do not allow for precise performance comparison, except analyzing the general asymptotic behavior that is shared among reasonable learning algorithms. 


\textbf{From stylized settings to practical LLMs.}
Although we study stylized settings in a rigorous manner, it does not capture all aspects of LLMs. 
For example, the ICL objective in \eqref{eq:icl-pretrain} is not an autoregressive loss used for pretraining LLMs, omitting the losses of predictions at each $Y_t$.
Therefore, one potential concern is the generalization of results obtained in stylized settings. 
While it cannot be shown precisely, the findings from such stylized settings have been surprisingly well generalized to the real-world tasks \citep{ahn2023linear, li2023transformers_topic}. 
For instance, \citet{ahn2023linear} perform synthetic experiments even with simplified transformers to study optimization methods for LLMs that surprisingly well reproduce the results from the real-world natural language data. 

Given the significance of actionable insights from the stylized settings such as foretelling impacts of scaling ICL to the asymptotic region of the demonstration size, which is extremely challenging with real-world LLMs, we hold positive views on the role of stylized settings in LLM research whose significant advantages outweigh the potential concerns on its generalization to the LLMs in practice.




\subsection{Detailed Configurations} \label{appx:add_details}


\textbf{Model. }
For the model, we use the GPT-2 \citep{radford2019language} architecture for $\text{TF}_\theta$, which is a standard architecture in the meta ICL and other stylized experimental settings; that is, we define $\text{TF}_\theta$ as a decoder-only transformers \citep{vaswani2017attention} with 12 layers, 8 attention heads, and 256-dimensional embedding space. 
For readers unfamiliar with transformers, we refer to the excellent tutorials \citep{turner2023introduction, bishop2023transformers}. 
We remark that viewing $\text{TF}_\theta$ as a function from a sequence of vectors with an arbitrary length to a vector with the same dimension does not significantly impact the understanding of core findings in this paper. 


\textbf{Optimization. }
For minimizing the ICL objective $l(\theta)$, we compute the stochastic gradient with 64 prompts and update $\theta$ by using the Adam optimizer \citep{kingma2014adam} with fixed learning rate of $10^{-4}$ for one million training iterations. 
Also, in order to boost the convergence speed, we use curriculum learning \citep{bengio2009curriculum} as recommended in \citep{garg2022can, panwar2023context} by increasing the length of the prompt by 2 every 2,000 training iterations until it reaches $(2M+1)$ (and the order of Fourier series by 1 until it reaches $M$).



\subsection{Impacts of Scaling Computations} \label{appx:scaling_comps}
We show that the non-vanishing excess risk curve of the transformer in long context causes the efficiency of learning to diminish with more demonstrations. 
Therefore, a natural question is whether enhancing transformers' capacity of handling longer context can make the excess risk decrease with more demonstrations and thus resolve the fundamental inefficiency. 
We analyze the impacts of scaling the pretraining context lengths (by setting $T_{\text{train}}$ to 100 and 200) and the model sizes (by scaling the number of layers, the number of heads, the embedding dimension by factors of 0.5, 2, and 3) on the excess risk. 
Note that we did not explore different positional encoding methods since we already use no positional encoding scheme that is effective at length generalization \citep{anil2022exploring, kazemnejad2024impact}, which is from an inductive bias for the sample order-stable learning algorithms. 



\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figs/excess_length.pdf}
    \includegraphics[width=0.4\linewidth]{figs/excess_model_size.pdf}
    \vskip -0.15in
    \caption{
    Impacts of the pretraining prompt length (left) and the model size (right) on the excess risk curve in $\gE = ([M], \sigma_y^2, \sigma_w^2) = ([10], 0.03, 10)$. 
    For (left), the pretraining losses are 1.06, 0.58, and 0.34 for models trained with $T_{\text{train}} = 50$, $T_{\text{train}} = 100$, and $T_{\text{train}} = 200$, respectively.
    For (right), the pretraining losses are 1.36, 1.19, 0.99, and 0.90 for half-capacity, standard, double-capacity, and triple-capacity models respectively. 
    }
    \vskip -0.15in
    \label{fig:discussion_fig}
\end{figure}


Figure \ref{fig:discussion_fig} (left) shows that increasing $T_{\text{train}}$ significantly reduces the excess risk values, especially for long-context regimes as desired. 
However, overall shape of the excess risk curve remains non-vanishing in the long-context regime.   
We observe from Figure \ref{fig:discussion_fig} (right) similar effects of increasing the model sizes.
Interestingly, larger models do not increase the length generalization regime, which is consistent with previous results \citep{zhou2024transformers}.


The results suggest that simply increasing computations with a larger model and a longer pretraining prompt length does not fundamentally change the \textit{shape} of the excess risk, even though their overall scales improve.
Therefore, while the degree of suboptimality can be relaxed with reduced excess risk, the inefficiency in many-shot regimes persist. 

\section{Proof of Claims}



\subsection{Proof of Theorem \ref{lemma:suboptimality_bound}} \label{appx:proof_suboptim_bounds}
\begin{proof}
    We first characterize suboptimality by the Bayes risk as follows:
    \begin{align}
        \texttt{SubOpt}(q) 
            & = \min_{t \in \mathbb{Z}_+} \left\lbrace t - \etN_{\textsf{BMA}}(q) \mid \epsilon_{\text{Bayes}}^{t} + \epsilon_{\text{XS}}^{t} \leq q  \right\rbrace \\ 
            & = \min_{t \in \mathbb{Z}_+} \left\lbrace t \mid \epsilon_{\text{Bayes}}^{t} \leq q - \epsilon_{\text{XS}}^{t} \right\rbrace - \etN_{\textsf{BMA}}(q). \label{init_pf_subopt}
    \end{align}
    Since $q < \epsilon_{\text{Bayes}}^{\etN_{\textsf{BMA}} (q) - 1} $, the monotonicity of $\epsilon_{\text{Bayes}}^{t}$ and the non-negativity of $\epsilon_{\text{XS}}^{t}$ give
    \begin{equation}
        \min_{t \in \mathbb{Z}_+} \left\lbrace t \mid \epsilon_{\text{Bayes}}^{t} \leq q - \epsilon_{\text{XS}}^{t} \right\rbrace 
        = \min_{t \geq \etN_{\textsf{BMA}}(q)} \left\lbrace t \mid \epsilon_{\text{Bayes}}^{t} \leq q - \epsilon_{\text{XS}}^{t} \right\rbrace 
            \geq \min_{t \geq \etN_{\textsf{BMA}}(q)} \left\lbrace t \mid \epsilon_{\text{Bayes}}^{t} < \epsilon_{\text{Bayes}}^{\etN_{\textsf{BMA}} (q) - 1} - \epsilon_{\text{XS}}^{t} \right\rbrace.
    \end{equation}

    To prove the theorem, we note the following.

    \textbf{(N1). Bayes error reduction as the conditional mutual information}:
    The Bayes error can be expressed as the reduction of (differential) entropy as follows.
    \begin{align}
        \epsilon_{\text{Bayes}}^t = 
        \E \left[ 
            \KL(\bar{P}^{t}_{e} \parallel \hat{P}^{t}_{\gE})
        \right] 
        & = h(Y_{t+1}|H_t ) - h(Y_{t+1}|H_t, e ), \: \text{ for continuous } Y_{t+1}  \\ 
        \epsilon_{\text{Bayes}}^t = 
        \E \left[ 
            \KL(\bar{P}^{t}_{e} \parallel \hat{P}^{t}_{\gE})
        \right] 
        & = \mathbb{H}(Y_{t+1}|H_t ) - \mathbb{H}(Y_{t+1}|H_t, e ), \: \text{ for discrete } Y_{t+1} 
    \end{align}
    where $h$ is the differential entropy and $\mathbb{H}$ is the Shannon entropy. 

    Therefore, for any $u \leq v$ and continuous $Y_{t+1}$, we have
    \begin{align}
        \epsilon_{\text{Bayes}}^u - \epsilon_{\text{Bayes}}^v 
        & = h(Y_{u+1}|H_u ) - h(Y_{u+1}|H_u, e ) 
         - (h(Y_{v+1}|H_v ) - h(Y_{v+1}|H_v, e )) \\ 
        & = h(Y_{u+1}|X_{u+1}, D_u ) - h(Y_{v+1}|X_{v+1}, D_v ) 
        \\
        & = \I(Y_{u+1}; \tilde{D}_{v - u} | X_{u+1}, D_u )
    \end{align}
    where $\tilde{D}_{v-u} \triangleq (\tilde{X}_1, \tilde{Y}_1, \cdots, \tilde{X}_{v-u}, \tilde{Y}_{v-u})$ is independently sampled from the same distribution as $D_{v-u}$, the second equality comes from the conditional independence $Y_{n+1} \perp D_n | X_{n+1}, e $ for any $n \in \mathbb{N}_{+}$, and the last equality comes from the chain rule. For the discrete $Y$'s, the same process can be applied by replacing $h$ with $\mathbb{H}$.

    \textbf{(N2). Lower bound of the excess risk}: Let $q$ be such that $\etN_{\textsf{BMA}}(q) \geq \bar{t}$. Therefore, by Assumption \ref{assumption:bounded_appx}, we have
    \begin{equation}
        \left\lbrace t \in \mathbb{N} \mid 
        t \geq \etN_{\textsf{BMA}}(q), \epsilon_{\text{Bayes}}^{t} < \epsilon_{\text{Bayes}}^{\etN_{\textsf{BMA}} (q) - 1} - \epsilon_{\text{XS}}^{t} \right\rbrace 
            \subseteq \left\lbrace t  \in \mathbb{N} \mid 
            t \geq \etN_{\textsf{BMA}}(q), \epsilon_{\text{Bayes}}^{\etN_{\textsf{BMA}} (q) - 1}  - \epsilon_{\text{Bayes}}^{t}  > \triangle_{\text{XS}} \right\rbrace.
    \end{equation}

    By applying \textbf{(N1)} and \textbf{(N2)} to \eqref{init_pf_subopt}, we get the desired result as 
    \begin{multline}
        \texttt{SubOpt}(q) 
        = \min_{t \geq \etN_{\textsf{BMA}}(q)} \left\lbrace t \mid \epsilon_{\text{Bayes}}^{t} \leq q - \epsilon_{\text{XS}}^{t} \right\rbrace - \etN_{\textsf{BMA}}(q) 
        \\
            \geq \min_{t \geq \etN_{\textsf{BMA}}(q)} \left\lbrace t \mid 
            \epsilon_{\text{Bayes}}^{\etN_{\textsf{BMA}} (q) - 1}  - \epsilon_{\text{Bayes}}^{t}  > \triangle_{\text{XS}} \right\rbrace - \etN_{\textsf{BMA}}(q) 
            = \min_{t \in \mathbb{N}} \left\lbrace t \mid 
            \epsilon_{\text{Bayes}}^{\etN_{\textsf{BMA}} (q) - 1}  - \epsilon_{\text{Bayes}}^{t + \etN_{\textsf{BMA}}(q)}  > \triangle_{\text{XS}} \right\rbrace  \\ 
            = \min_{t \in \mathbb{N}} \left\lbrace t \mid 
            \I(Y_{\etN_{\textsf{BMA}}(q)};\tilde{D}_{t+1} \mid H_{\etN_{\textsf{BMA}}(q) - 1} )
             > \triangle_{\text{XS}} \right\rbrace.
    \end{multline}
\end{proof}

\subsection{Proof of Theorem \ref{thm:necessary_conditions}} \label{appx:proof_necessary}

\begin{proof} 
    Consider $q_1, q_2 \in (\triangle_{\text{XS}}, q)$ such that $q_1 < q_2 < q$ and $\etN_{\textsf{BMA}}(q_1) > \etN_{\textsf{BMA}}(q_2)$. 
    The goal is to show necessary conditions for $LB(q_1) \leq LB(q_2) $. 

    Note that
    $LB(q_1) < LB(q_2)$ is impossible because $\I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_1) - 1}) \leq \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_2) - 1})$ for any $t \in \mathbb{N}$.
    Specifically, we have    
    \begin{equation}
        \I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_1) - 1}) \leq \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_2) - 1}), \quad \forall t \in \mathbb{N}
    \end{equation}
    , which implies 
    \begin{equation}
        \left\lbrace t \in \mathbb{N} \mid
            \I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_1) - 1}) >  \triangle_{\text{XS}}
        \right\rbrace \subseteq \left\lbrace t \in \mathbb{N} \mid
            \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_2) - 1}) >  \triangle_{\text{XS}}
        \right\rbrace
    \end{equation}
    , and in turn $LB(q_1) \geq LB(q_2)$.

    Therefore, we next show the necessary condition for $LB(q_1) = LB(q_2)$. 

    \textbf{(NC 1). Negligible excess risk}: 
    Let us suppose $\triangle_{\text{XS}} \leq \I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_1) - 1}) \leq \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_2) - 1})$. 
    In this case, $LB(q_1) = LB(q_2) = 0$ as desired. 
    Since $q_1$ and $q_2$ are chosen arbitrary, the first necessary condition is given by 
    \begin{equation}
        \triangle_{\text{XS}} \leq \I(Y_{t}; \tilde{D}_1 | H_{t - 1}), \quad t \geq \bar{t}. 
    \end{equation}
    
    \textbf{(NC 2). No diminishing returns}: 
    If \textbf{(NC 1)} does not hold, we have $\triangle_{\text{XS}} > \I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_1) - 1}) $. 
    In this case, we rule out the possibility 
    $\I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_1) - 1}) < \triangle_{\text{XS}} \leq \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_2) - 1})$ because this gives $LB(q_2) = 0 \text{ and } LB(q_1) > 0 $, which contradicts $LB(q_1) = LB(q_2)$.  

    Thus, we consider the case $\I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_1) - 1}) \leq \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q_2) - 1}) < \triangle_{\text{XS}}$. In this case, $LB(q_1) = LB(q_2)$ requires the following condition
    \begin{equation}
        \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_{LB(q_2)} | H_{\etN_{\textsf{BMA}}(q_2) - 1}) 
            <  \I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_{LB(q_2) + 1} | H_{\etN_{\textsf{BMA}}(q_1) - 1}),
    \end{equation}
    where the condition comes from $\I(Y_{\etN_{\textsf{BMA}}(q_1)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_1) - 1}) \leq \I(Y_{\etN_{\textsf{BMA}}(q_2)}; \tilde{D}_{t+1} | H_{\etN_{\textsf{BMA}}(q_2) - 1})$ for any $t \in \mathbb{N}$.
    
    
    By the construction of $q_1$ and $q_2$, we get 
    \begin{equation}
        \I(Y_{\etN_{\textsf{BMA}}(q)}; \tilde{D}_{LB(q)} | H_{\etN_{\textsf{BMA}}(q) - 1}) 
            <  \I(Y_{\etN_{\textsf{BMA}}(q) + k}; \tilde{D}_{LB(q) + 1} | H_{\etN_{\textsf{BMA}}(q) - 1 + k}), \quad \forall k \in \mathbb{N}_{+}.
    \end{equation}
    Due to the chain rule of the mutual information, for any $\tilde{k} \in \mathbb{N}_{+}$, it holds that 
    \begin{equation}
        \I(Y_{\etN_{\textsf{BMA}}(q)}; \tilde{D}_{\tilde{k}} | H_{\etN_{\textsf{BMA}}(q) - 1}) 
            = \sum_{i=0}^{\tilde{k}-1} \I(Y_{\etN_{\textsf{BMA}}(q) + i}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q)  - 1 + i})
            \geq \tilde{k} \I(Y_{\etN_{\textsf{BMA}}(q) + \tilde{k} - 1}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q) + \tilde{k} - 2}).
    \end{equation}
    Similarly, 
    \begin{multline}
        \I(Y_{\etN_{\textsf{BMA}}(q) + k}; \tilde{D}_{\tilde{k} + 1} | H_{\etN_{\textsf{BMA}}(q) - 1 + k}) 
            = \sum_{i=0}^{\tilde{k}} \I(Y_{\etN_{\textsf{BMA}}(q) + k + i}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q) - 1 + k + i}) \\
            \leq (1 + \tilde{k}) \I(Y_{\etN_{\textsf{BMA}}(q) + k}; \tilde{D}_1 | H_{\etN_{\textsf{BMA}}(q) - 1 + k}). 
    \end{multline}

    Therefore, we get the second necessary condition as 
    \begin{equation}
        \I(Y_{t}; \tilde{D}_1 | H_{t - 1}) 
            \leq \I(Y_{\bar{t} + \tilde{k} - 1}, \tilde{D}_1 | H_{\bar{t} + \tilde{k} - 2})  
                < \left(1 + \frac{1}{\tilde{k}} \right) \I(Y_{t}; \tilde{D}_1 | H_{t - 1}), \quad \forall t \geq \bar{t}, 
    \end{equation}
    where $\tilde{k} = LB(q) > 1$ for $q$ such that  $\etN_{\textsf{BMA}}(q) \geq \bar{t}$. 
    
    
\end{proof}




\section{Additional Figures}



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/illustrate_suboptim_lemma.png}
    \vskip -0.15in
    \caption{Graphical illustration of Theorem \ref{lemma:suboptimality_bound} when $q=0.08 - \sigma^2$, where $\sigma^2 = \E \left[ -\log \bar{P}^{t}_{e}(Y_{t+1}) \right]$ is the irreducible aleatoric uncertainty. 
    The solid orange and blue lines represent MSEs of BMA and ICL, respectively. Here, the dashed orange line corresponds to the $\sigma^2 + \epsilon^t_{\text{Bayes}} + \triangle_{\text{XS}}$, which serves as a lower bound on MSEs of ICL. 
    The shift by $\triangle_{\text{XS}}$ induces suboptimality that requires at least $LB(q)$ additional number of demonstrations for ICL to achieve the requirement $q$, compared to BMA. 
    }
    \vskip -0.1in
    \label{fig:suboptimality_illustration}
\end{figure}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.4\linewidth]{figs/quantile_sample.pdf}
    \caption{The number of demonstrations (y-axis) required to achieve each performance quantile (x-axis). The shaded area represents the standard error. We note that performance quantile $\gQ = 0.6$ is achieved by $T_{\text{Train}}$ number of demonstrations on average.}
    \label{results:perf_quant_samples}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/pp_full.pdf}
    \caption{Performance profiles $\rho_b$ across different performance ratios $\tau$ under different target performance quantiles $\gQ$. Each curve represents the probability that a method achieves the desired performance within a factor $\tau$ of the best method's sample complexity (x-axes).}
    \label{results:full_perf_prof}
\end{figure*}



\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/mse_full.pdf}
    \caption{Mean squared errors for different demonstration sizes.}
    \label{results:full_mse}
\end{figure*}



\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/spd_full.pdf}
    \caption{Squared prediction differences between BMA and other methods for different demonstration sizes.}
    \label{results:full_spd}
\end{figure*}


% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
