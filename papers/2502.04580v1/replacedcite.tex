\section{Related Work}
\textbf{Asymptotic Behavior Analysis.}
____ show that ICL predictions converge to posterior probabilities in asymptotic demonstration size regimes. 
Subsequent works expand these results to encompass finite-sample guarantees ____, broader prompt distribution structures ____, and structural characteristics of transformers ____. 
Recent studies analyze the average cumulative regret across demonstrations ____, treating ICL as an online learning algorithm. 
However, practical applications prioritize test sample performance over demonstration set performance.
In this work, we directly analyze suboptimality of ICL in achieving a specific performance requirement through the excess sample complexity compared to the Bayes optimal learning algorithm. 


\textbf{Stylized ICL Benchmarks.}
With the meta-ICL framework (cf. \S \ref{sec:meta_icl_prompt}), ____ demonstrate that transformers are capable of learning simple function classes (e.g., linear models and random neural networks) from demonstrations, achieving error curves qualitatively similar to those of optimal learning algorithms under asymptotic pretraining sample conditions. 
Subsequent works extend the results to finite pretraining sample scenarios ____ and mixture function classes ____. 
Further, new analytical frameworks that directly analyze ICL predictions reveal that ICL exhibits behavior similar to gradient descent ____. 
In this work, we measure how many demonstrations are required for ICL to achieve a certain performance level, rather than analyzing ICL performance as a function of demonstration size.
This new perspective unveils the fundamental inefficiency of ICL in the many-shot regimes, which is subtle to discover with previous analyses.



\textbf{Scaling ICL.}
Recent advances in handling long-context prompts ____ have enabled studies demonstrating (near) monotonic improvements in ICL performance with increased demonstrations ____. 
Notably, ____ show that many-shot ICL can surpass parameter-efficient fine-tuning methods ____ given the same number of demonstrations, highlighting ICL's sample efficiency. 
Our work extends these findings by examining optimality of performance gains from a learning-theoretic perspective, revealing that ICL's sample complexity diminishes as sample sizes increase.