\section{Related Work}
\textbf{Asymptotic Behavior Analysis.}
\citet{xie2021explanation} show that ICL predictions converge to posterior probabilities in asymptotic demonstration size regimes. 
Subsequent works expand these results to encompass finite-sample guarantees \citep{li2023transformers,zhang2023and,bai2024transformers}, broader prompt distribution structures \citep{li2023transformers, li2023transformers_topic, zhang2023and}, and structural characteristics of transformers \citep{zhang2023and}. 
Recent studies analyze the average cumulative regret across demonstrations \citep{zhang2023and,jeon2024information}, treating ICL as an online learning algorithm. 
However, practical applications prioritize test sample performance over demonstration set performance.
In this work, we directly analyze suboptimality of ICL in achieving a specific performance requirement through the excess sample complexity compared to the Bayes optimal learning algorithm. 


\textbf{Stylized ICL Benchmarks.}
With the meta-ICL framework (cf. \S \ref{sec:meta_icl_prompt}), \citet{garg2022can} demonstrate that transformers are capable of learning simple function classes (e.g., linear models and random neural networks) from demonstrations, achieving error curves qualitatively similar to those of optimal learning algorithms under asymptotic pretraining sample conditions. 
Subsequent works extend the results to finite pretraining sample scenarios \citep{raventos2024pretraining} and mixture function classes \citep{pathak2023transformers, panwar2023context}. 
Further, new analytical frameworks that directly analyze ICL predictions reveal that ICL exhibits behavior similar to gradient descent \citep{von2023transformers,akyurek2022learning}. 
In this work, we measure how many demonstrations are required for ICL to achieve a certain performance level, rather than analyzing ICL performance as a function of demonstration size.
This new perspective unveils the fundamental inefficiency of ICL in the many-shot regimes, which is subtle to discover with previous analyses.



\textbf{Scaling ICL.}
Recent advances in handling long-context prompts \citep{chen2023extending,su2024roformer,press2021train} have enabled studies demonstrating (near) monotonic improvements in ICL performance with increased demonstrations \citep{li2023_manydems, agarwal2024many, anil2024many}. 
Notably, \citet{bertsch2024context} show that many-shot ICL can surpass parameter-efficient fine-tuning methods \citep{hu2021lora} given the same number of demonstrations, highlighting ICL's sample efficiency. 
Our work extends these findings by examining optimality of performance gains from a learning-theoretic perspective, revealing that ICL's sample complexity diminishes as sample sizes increase.