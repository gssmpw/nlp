@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}


@inproceedings{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{ahuja2024provable,
  title={On provable length and compositional generalization},
  author={Ahuja, Kartik and Mansouri, Amin},
  journal={arXiv preprint arXiv:2402.04875},
  year={2024}
}

@article{wang2024length,
  title={Length Generalization of Causal Transformers without Position Encoding},
  author={Wang, Jie and Ji, Tao and Wu, Yuanbin and Yan, Hang and Gui, Tao and Zhang, Qi and Huang, Xuanjing and Wang, Xiaoling},
  journal={arXiv preprint arXiv:2404.12224},
  year={2024}
}

@article{jelassi2023length,
  title={Length generalization in arithmetic transformers},
  author={Jelassi, Samy and d'Ascoli, St{\'e}phane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2306.15400},
  year={2023}
}

@article{bhattamishra2024separations,
  title={Separations in the Representational Capabilities of Transformers and Recurrent Architectures},
  author={Bhattamishra, Satwik and Hahn, Michael and Blunsom, Phil and Kanade, Varun},
  journal={arXiv preprint arXiv:2406.09347},
  year={2024}
}


@article{allen2023physics,
  title={Physics of language models: Part 1, learning hierarchical language structures},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprints, abs/2305.13673},
  year={2023}
}

@article{ahn2023learning,
  title={Learning threshold neurons via edge of stability},
  author={Ahn, Kwangjun and Bubeck, S{\'e}bastien and Chewi, Sinho and Lee, Yin Tat and Suarez, Felipe and Zhang, Yi},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}
@inproceedings{li2023transformers_topic,
  title={How do transformers learn topic structure: Towards a mechanistic understanding},
  author={Li, Yuchen and Li, Yuanzhi and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@article{kim2022self,
  title={Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator},
  author={Kim, Hyuhng Joon and Cho, Hyunsoo and Kim, Junyeob and Kim, Taeuk and Yoo, Kang Min and Lee, Sang-goo},
  journal={arXiv preprint arXiv:2206.08082},
  year={2022}
}

@article{shmueli2010explain,
  title={To Explain or to Predict?},
  author={Shmueli, Galit},
  journal={Statistical Science},
  volume={25},
  number={3},
  pages={289--310},
  year={2010}
}

@article{liu2024let,
  title={Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning},
  author={Liu, Yinpeng and Liu, Jiawei and Shi, Xiang and Cheng, Qikai and Huang, Yong and Lu, Wei},
  journal={arXiv preprint arXiv:2402.10738},
  year={2024}
}

@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{rubin2021learning,
  title={Learning to retrieve prompts for in-context learning},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  journal={arXiv preprint arXiv:2112.08633},
  year={2021}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{ahuja2023closer,
  title={A closer look at in-context learning under distribution shifts},
  author={Ahuja, Kartik and Lopez-Paz, David},
  journal={arXiv preprint arXiv:2305.16704},
  year={2023}
}

@article{sorensen2022information,
  title={An information-theoretic approach to prompt engineering without ground truth labels},
  author={Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher Michael and Shaw, Alexander Glenn and Rogers, Kyle Jeffrey and Delorey, Alexia Pauline and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
  journal={arXiv preprint arXiv:2203.11364},
  year={2022}
}
@article{liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Ye, Haotian and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={19},
  pages={1--53},
  year={2017}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}

@article{ding2018model,
  title={Model selection techniques: An overview},
  author={Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={6},
  pages={16--34},
  year={2018},
  publisher={IEEE}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author = {Kalman, Rudolph Emil},
  journal={Journal of Basic Engineering},
  volume={82},
  number={1},
  pages={35--45},
  year={1960}
}

@inproceedings{leeattention,
  title={Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability},
  author={Lee, Ivan and Jiang, Nan and Berg-Kirkpatrick, Taylor},
    year={2024},
  booktitle={International Conference on Learning Representations}
}


@inproceedings{ahn2023linear,
  title={Linear attention is (maybe) all you need (to understand transformer optimization)},
  author={Ahn, Kwangjun and Cheng, Xiang and Song, Minhak and Yun, Chulhee and Jadbabaie, Ali and Sra, Suvrit},
  booktitle={International Conference on Learning Representations},
  year={2024}
}


@inproceedings{zhou2023algorithms,
  title={What algorithms can transformers learn? A study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  booktitle={International Conference on Learning Representations},
  year={2024}
}


@inproceedings{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{pathak2023transformers,
  title={Transformers can optimally learn regression mixture models},
  author={Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
      author={Diederik P. Kingma and Jimmy Ba},
  booktitle={International Conference on Learning Representations},
  year={2015}
}


@article{hochreiter1997long,
  title={Long short-term memory.},
  author={Hochreiter, S and Schmidhuber, J},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@article{zhao2024context,
  title={Is In-Context Learning Sufficient for Instruction Following in LLMs?},
  author={Zhao, Hao and Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2405.19874},
  year={2024}
}

@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}


@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021}
}

@article{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}


@InProceedings{siyu2024dynamics,
  title = 	 {Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
  author =       {Siyu, Chen and Heejune, Sheen and Tianhao, Wang and Zhuoran, Yang},
  booktitle = 	 {Conference on Learning Theory},
  pages = 	 {4573--4573},
  year = 	 {2024},
  volume = 	 {247}
}

@article{mcallester1999some,
  title={Some {PAC}-{B}ayesian Theorems},
  author={McAllester, David A},
  journal={Machine Learning},
  volume={3},
  number={37},
  pages={355--363},
  year={1999}
}


@article{dai2022can,
  title={Why can gpt learn in-context? Language models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{fu2023transformers,
  title={Transformers learn higher-order optimization methods for in-context learning: A study with linear models},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2310.17086},
  year={2023}
}

@article{jiang2023latent,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv preprint arXiv:2304.09960},
  year={2023}
}

@incollection{thrun1998learning,
  title={Learning to learn: Introduction and overview},
  author={Thrun, Sebastian and Pratt, Lorien},
  booktitle={Learning to learn},
  pages={3--17},
  year={1998},
  publisher={Springer}
}

@inproceedings{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={11398--11442},
  year={2023},
  organization={PMLR}
}
@inproceedings{mcallester1999pac,
  title={{PAC}-{B}ayesian model averaging},
  author={McAllester, David A},
  booktitle={Annual Conference on Computational Learning Theory},
  pages={164--170},
  year={1999}
}

@article{vapnik1998statistical,
  title={Statistical learning theory},
  author={Vapnik, Vladimir},
  journal={John Wiley},
  year={1998}
}

@inproceedings{ahn2024transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{dolan2002benchmarking,
  title={Benchmarking optimization software with performance profiles},
  author={Dolan, Elizabeth D and Mor{\'e}, Jorge J},
  journal={Mathematical Programming},
  volume={91},
  pages={201--213},
  year={2002},
  publisher={Springer}
}



@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}
@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}


@inproceedings{garg2022can,
  title={What can transformers learn in-context? A case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  year={2023}
}


@inproceedings{jeon2024information,
  title={An information-theoretic analysis of in-context learning},
  author={Jeon, Hong Jun and Lee, Jason D and Lei, Qi and Van Roy, Benjamin},
  booktitle={International Conference on Machine Learning},
  year={2024}
}


@inproceedings{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@phdthesis{schmidhuber1987evolutionary,
  title={Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author={Schmidhuber, J{\"u}rgen},
  year={1987},
  school={Technische Universit{\"a}t M{\"u}nchen}
}


@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@article{dahl2023benchmarking,
  title={Benchmarking neural network training algorithms},
  author={Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others},
  journal={arXiv preprint arXiv:2306.07179},
  year={2023}
}

@article{kunstner2024heavy,
  title={Heavy-tailed class imbalance and why adam outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@inproceedings{francazi2023theoretical,
  title={A theoretical analysis of the learning dynamics under class imbalance},
  author={Francazi, Emanuele and Baity-Jesi, Marco and Lucchi, Aurelien},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@misc{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  howpublished={\url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}},
  note = "[Online; accessed 24-November-2024]"
}

@misc{sutton2019Bitter,
  title={The bitter lesson},
  author={Sutton, Richard},
  year={2019},
  howpublished={\url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}},
  note = "[Online; accessed 23-December-2024]"
}


@article{niyogi1996relationship,
  title={On the relationship between generalization error, hypothesis complexity, and sample complexity for radial basis functions},
  author={Niyogi, Partha and Girosi, Federico},
  journal={Neural Computation},
  volume={8},
  number={4},
  pages={819--842},
  year={1996},
  publisher={MIT Press}
}

@article{barron1994approximation,
  title={Approximation and estimation bounds for artificial neural networks},
  author={Barron, Andrew R},
  journal={Machine Learning},
  volume={14},
  pages={115--133},
  year={1994},
  publisher={Springer}
}

@inproceedings{mann2020language,
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}


@article{goldblum2023no,
  title={The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning},
  author={Goldblum, Micah and Finzi, Marc and Rowan, Keefer and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2304.05366},
  year={2023}
}

@inproceedings{wang2023large,
  title={Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning},
  author={Wang, Xinyi and Zhu, Wanrong and Saxon, Michael and Steyvers, Mark and Wang, William Yang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{
guo2024how,
    title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
    author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
    booktitle={International Conference on Learning Representations},
    year={2024},
}


@inproceedings{xie2021explanation,
  title={An explanation of in-context learning as implicit {B}ayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
    booktitle={International Conference on Learning Representations},
  year={2022}
}



@inproceedings{panwar2023context,
  title={In-context learning through the {B}ayesian prism},
  author={Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
    booktitle={International Conference on Learning Representations},
    year={2024},
}

@inproceedings{pan2023context,
  title={What In-Context Learning" Learns" In-Context: Disentangling Task Recognition and Task Learning},
  author={Pan, Jane and Gao, Tianyu and Chen, Howard and Chen, Danqi},
  booktitle={Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}

@inproceedings{
wies2023the,
title={The Learnability of In-Context Learning},
author={Noam Wies and Yoav Levine and Amnon Shashua},
booktitle={Neural Information Processing Systems},
year={2023}}


@article{zhang2023and,
  title={What and how does in-context learning learn? {B}ayesian model averaging, parameterization, and generalization},
  author={Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.19420},
  year={2023}
}

@inproceedings{kossen2024context,
  title={In-context learning learns label relationships but is not conventional learning},
  author={Kossen, Jannik and Gal, Yarin and Rainforth, Tom},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@inproceedings{
    lin2024dual,
    title={Dual Operating Modes of In-Context Learning},
    author={Ziqian Lin and Kangwook Lee},
    booktitle={International Conference on Machine Learning},
    year={2024},
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@inproceedings{bahri2024explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  booktitle={Proceedings of the National Academy of Sciences},
  year={2024}
}

@article{rosenfeld2019constructive,
  title={A constructive prediction of the generalization error across scales},
  author={Rosenfeld, Jonathan S and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
  journal={arXiv preprint arXiv:1909.12673},
  year={2019}
}

@article{bertsch2024context,
  title={In-context learning with long-context models: An in-depth exploration},
  author={Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham},
  journal={arXiv preprint arXiv:2405.00200},
  year={2024}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{
srivastava2023beyond,
title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=uyTL5Bvosj},
note={Featured Certification}
}


@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{schwarz1978estimating,
  title={Estimating the dimension of a model},
  author={Schwarz, Gideon},
  journal={The Annals of Statistics},
  pages={461--464},
  year={1978},
  publisher={JSTOR}
}


@article{kass1995bayes,
  title={Bayes factors},
  author={Kass, Robert E and Raftery, Adrian E},
  journal={Journal of the American Statistical Association},
  volume={90},
  number={430},
  pages={773--795},
  year={1995},
  publisher={Taylor \& Francis}
}

@article{golub1979generalized,
  title={Generalized cross-validation as a method for choosing a good ridge parameter},
  author={Golub, Gene H and Heath, Michael and Wahba, Grace},
  journal={Technometrics},
  volume={21},
  number={2},
  pages={215--223},
  year={1979},
  publisher={Taylor \& Francis}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={International Conference on Machine Learning},
  year={2009}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{turner2023introduction,
  title={An introduction to transformers},
  author={Turner, Richard E},
  journal={arXiv preprint arXiv:2304.10557},
  year={2023}
}

@incollection{bishop2023transformers,
  title={Transformers},
  author={Bishop, Christopher M and Bishop, Hugh},
  booktitle={Deep Learning: Foundations and Concepts},
  pages={357--406},
  year={2023},
  publisher={Springer}
}

@article{cheng2024exploring,
  title={Exploring the robustness of in-context learning with noisy labels},
  author={Cheng, Chen and Yu, Xinzhi and Wen, Haodong and Sun, Jinsong and Yue, Guanzhang and Zhang, Yihao and Wei, Zeming},
  journal={arXiv preprint arXiv:2404.18191},
  year={2024}
}

@article{cooley1965algorithm,
  title={An algorithm for the machine calculation of complex Fourier series},
  author={Cooley, James W and Tukey, John W},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  publisher={JSTOR}
}

@book{bishop2007,
  author = {Bishop, Christopher M.},
  publisher = {Springer},
  title = {Pattern Recognition and Machine Learning},
  year = 2007
}

@article{shen2023pretrained,
  title={Do pretrained Transformers Really Learn In-context by Gradient Descent?},
  author={Shen, Lingfeng and Mishra, Aayush and Khashabi, Daniel},
  journal={arXiv preprint arXiv:2310.08540},
  year={2023}
}


@article{nogueira2021investigating,
  title={Investigating the limitations of transformers with simple arithmetic tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}

@inproceedings{bengio2019system,
  title={From system 1 deep learning to system 2 deep learning},
  author={Bengio, Yoshua and others},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and Brain Sciences},
  volume={40},
  pages={e253},
  year={2017},
  publisher={Cambridge University Press}
}

@inproceedings{munkhdalai2019metalearned,
  title={Metalearned neural memory},
  author={Munkhdalai, Tsendsuren and Sordoni, Alessandro and Wang, Tong and Trischler, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{langford2001not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  year={2001}
}



@article{mosbach2023few,
  title={Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation},
  author={Mosbach, Marius and Pimentel, Tiago and Ravfogel, Shauli and Klakow, Dietrich and Elazar, Yanai},
  journal={arXiv preprint arXiv:2305.16938},
  year={2023}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{ramsauer2020hopfield,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2020}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{ahmad2021unified,
  title={Unified pre-training for program understanding and generation},
  author={Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2103.06333},
  year={2021}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{akaike1974new,
  title={A new look at the statistical model identification},
  author={Akaike, Hirotugu},
  journal={IEEE Transactions on Automatic Control},
  volume={19},
  number={6},
  pages={716--723},
  year={1974},
  publisher={Ieee}
}

@article{
Belkin2019,
author = {Mikhail Belkin  and Daniel Hsu  and Siyuan Ma  and Soumik Mandal },
title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
journal = {Proceedings of the National Academy of Sciences},
volume = {116},
number = {32},
pages = {15849-15854},
year = {2019},
doi = {10.1073/pnas.1903070116},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1903070116},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1903070116},}

@book{burnham2002model,
  added-at = {2012-04-11T20:12:39.000+0200},
  author = {Burnham, K.P. and Anderson, D.R.},
  biburl = {https://www.bibsonomy.org/bibtex/211eec3e24b6cb0750295344ee2cd494d/jabreftest},
  groups = {public},
  interhash = {f78194a59013e3439091a94fcb746660},
  intrahash = {11eec3e24b6cb0750295344ee2cd494d},
  keywords = {},
  publisher = {Springer Verlag},
  timestamp = {2012-04-11T20:12:39.000+0200},
  title = {Model selection and multimodel inference: a practical information-theoretic approach},
  username = {jabreftest},
  year = 2002
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@article{bartlett2006convexity,
  title={Convexity, classification, and risk bounds},
  author={Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  pages={138--156},
  year={2006},
  publisher={Taylor \& Francis}
}

@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{rao1962efficient,
  title={Efficient estimates and optimum inference procedures in large samples},
  author={Rao, C Radhakrishna},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={24},
  number={1},
  pages={46--63},
  year={1962},
  publisher={Wiley Online Library}
}

@inproceedings{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-{B}ayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}


@book{nemenyi1963distribution,
  title={Distribution-Free Multiple Comparisons.},
  author={Nemenyi, Peter Bjorn},
  year={1963},
  publisher={Princeton University}
}

@article{friedman1937use,
  title={The use of ranks to avoid the assumption of normality implicit in the analysis of variance},
  author={Friedman, Milton},
  journal={Journal of the American Statistical Association},
  volume={32},
  number={200},
  pages={675--701},
  year={1937},
  publisher={Taylor \& Francis}
}

@article{mackay1992bayesian,
  title={Bayesian interpolation},
  author={MacKay, David JC},
  journal={Neural Computation},
  volume={4},
  number={3},
  pages={415--447},
  year={1992}
}


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}


@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}


@article{clauset2009power,
  title={Power-law distributions in empirical data},
  author={Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, Mark EJ},
  journal={SIAM Review},
  volume={51},
  number={4},
  pages={661--703},
  year={2009},
  publisher={SIAM}
}

@article{jeon2022information,
  title={An information-theoretic framework for supervised learning},
  author={Jeon, Hong Jun and Zhu, Yifan and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2203.00246},
  year={2022}
}



@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}


@article{clark2018think,
  title={Think you have solved question answering? Try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}


@article{kwiatkowski2019natural,
  title={Natural questions: A benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}


@article{fu2024data,
  title={Data engineering for scaling language models to 128k context},
  author={Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao},
  journal={arXiv preprint arXiv:2402.10171},
  year={2024}
}

@inproceedings{anil2024many,
  title={Many-shot jailbreaking},
  author={Anil, Cem and Durmus, Esin and Rimsky, Nina and Sharma, Mrinank and Benton, Joe and Kundu, Sandipan and Batson, Joshua and Tong, Meg and Mu, Jesse and Ford, Daniel J and others},
  booktitle={Neural Information Processing Systems},
  year={2024}
}


@inproceedings{agarwal2024many,
  title={Many-shot in-context learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie and Zhang, Biao and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and others},
  booktitle={Neural Information Processing Systems},
  year={2024}
}

@article{li2023_manydems,
  title={In-context learning with many demonstration examples},
  author={Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2302.04931},
  year={2023}
}


@article{rissanen1984universal,
  title={Universal coding, information, prediction, and estimation},
  author={Rissanen, Jorma},
  journal={IEEE Transactions on Information Theory},
  volume={30},
  number={4},
  pages={629--636},
  year={1984},
  publisher={IEEE}
}

@article{clarke1990information,
  title={Information-theoretic asymptotics of {B}ayes methods},
  author={Clarke, Bertrand S and Barron, Andrew R},
  journal={IEEE Transactions on Information Theory},
  volume={36},
  number={3},
  pages={453--471},
  year={1990},
  publisher={IEEE}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}


@article{li2024long,
  title={Long-context llms struggle with long in-context learning},
  author={Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
  journal={arXiv preprint arXiv:2404.02060},
  year={2024}
}


@article{hoeting1999BMA,
 author = {Jennifer A. Hoeting and David Madigan and Adrian E. Raftery and Chris T. Volinsky},
 journal = {Statistical Science},
 number = {4},
 pages = {382--401},
 title = {Bayesian Model Averaging: A Tutorial},
 volume = {14},
 year = {1999}
}


@article{zhou2024transformers,
  title={Transformers can achieve length generalization but not robustly},
  author={Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09371},
  year={2024}
}

@article{hellstrom2023generalization,
  title={Generalization bounds: Perspectives from information theory and PAC-{B}ayes},
  author={Hellstr{\"o}m, Fredrik and Durisi, Giuseppe and Guedj, Benjamin and Raginsky, Maxim},
  journal={arXiv preprint arXiv:2309.04381},
  year={2023}
}

@article{wasserman2000bayesian,
  title={Bayesian model selection and model averaging},
  author={Wasserman, Larry},
  journal={Journal of Mathematical Psychology},
  volume={44},
  number={1},
  pages={92--107},
  year={2000},
  publisher={Elsevier}
}