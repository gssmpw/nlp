\newpage
\appendix

\onecolumn

\section{X-IL Details}
\subsection{X-Block}
\label{subsec:x-block}
An X-Block is similar to Diffusion Transformer (DiT) block, but generalized. The key difference is the X-Layer which is capable to plug-in different backbones, including Transformer, Mamba and xLSTM. The Adaptive LayerNorm (AdaLN) is used to make the input tokens conditionally activated on the context. A MLP maps the given context to factors $\alpha$, $\gamma$ and $\beta$. These factors are then applied to scaling and shifting operations in order to manipuate the latent embeddings.

\subsection{Representation Encoders}
\label{subsec:repr-encoders}
\subsubsection{RGB}

\begin{itemize}
    \item \textbf{ResNet} ResNet-18 with a latent dimension of 512 is used in this paper. 
    \item \textbf{FiLM-ResNet} FilM introduces a general conditioning layer for visual reasoning tasks. ResNet-18 with a latent dimension of 512 is used for FilM-ResNet. The FilM has a condition dimension of 512.
    
\end{itemize}

\subsubsection{Point Cloud}
\input{figures/pc_encoders/pc_encoders}
    \begin{itemize}
    \item \textbf{Attention} We use a 4-layer self-attention Transformer to encode the point cloud tokens. A CLS token is used to capture the whole geometric representation. See Figure \ref{fig:pc_encoders_attention}. 
    \item \textbf{MLP with MaxPooling} We use the same point cloud encoder in 3D Diffusion Policy \cite{Ze2024DP3}, with a 3-layer MLP and Max Pooling to get the compact 3d representations. See Figure \ref{fig:pc_encoders_mlp}. 
    \end{itemize}

\subsubsection{Language}
We used pre-trained CLIP to encode language. CLIP is a vision-language model that aligns the latent embeddings of visual and language inputs. Pre-trained model ViT-B/32 is used as language encoder. We freeze the weights of both models during the training.

\subsection{Backbone Details}
Hyperparameters of three different backbones, i.e., Transformer, Mamba, and xLSTM, used in different policies are shown in the Table \ref{table:appendix_backbone_transformer}, \ref{table:appendix_backbone_mamba} and \ref{table:appendix_backbone_xlstm}. These hyperparameters are not tuned for a specific task but are used for all tasks reported in this paper.

\input{results/tables/appendix_backbone_transformer_col}
\input{results/tables/appendix_backbone_mamba_col}
\input{results/tables/appendix_backbone_xlstm_col}

\newpage

% \subsection{Architectures}

\subsection{Policies}
\label{appendix:policy}
\textbf{Behavior Cloning} Behavior cloning (BC) assumes a Gaussian distribution as policy representation and has therefore limited model capacity. Maximizing the likelihood of the policy results in a mean squared error (MSE) minimization between ground truth and predicted actions. Due to its simplicity, BC is often used as a default naive baseline for comparing imitation learning policies.  

\textbf{Diffusion-Based Policies}  
Denoising diffusion probabilistic models (DDPM) \cite{ho2020denoising} are a popular choice for policy representation due to their simplicity and minimal design choices compared to advanced models like BESO \cite{reuss2023goal}. BESO, based on a continuous-time diffusion framework, allows for varying diffusion steps during training and inference, as well as diverse sampling techniques such as DDIM. Despite these differences, both DDPM and BESO rely on regression losses, either learning a score function or a denoising model. Our framework supports both DDPM-style policies and continuous-time BESO-style policies.  

\textbf{Flow-Based Policies}  
Continuous-time normalizing flows trained via flow matching \cite{lipman2022flow} have recently gained a lot of attention and are also suitable as policy representation. These methods, often referred to as rectified flows (RF) \cite{liu2022flow} or stochastic interpolants \cite{albergo2022building}, are fully supported in our framework.  


\section{Additional Task Details}

\subsection{LIBERO Benchmark}

The LIBERO Benchmark comprises five distinct task suites: \textit{LIBERO-Spatial}, \textit{LIBERO-Object}, \textit{LIBERO-Goal}, \textit{LIBERO-Long}, and \textit{LIBERO-90}. Each suite is designed to evaluate distinct dimensions of robotic learning and manipulation capabilities:

\begin{itemize}
    \item \textit{LIBERO-Spatial} evaluates spatial reasoning precision through tasks requiring differentiation of identical objects (e.g., bowls) based solely on their relational positioning (e.g., placement relative to plates or other objects).
    \item \textit{LIBERO-Object} tests object-centric manipulation by requiring precise recognition and interaction with unique objects (e.g., pick-and-place tasks) in each trial, emphasizing perceptual discrimination.
    \item \textit{LIBERO-Goal} assesses goal-conditioned adaptability by defining distinct objectives (e.g., placing objects in varying sequences or configurations) within fixed object-layout environments, necessitating dynamic behavioral adjustments.
    \item \textit{LIBERO-Long} challenges long-term planning and endurance with multi-stage tasks demanding sustained execution, error mitigation, and coordination across extended timelines.
    \item \textit{LIBERO-90} serves as a generalization benchmark, aggregating 90 short-horizon tasks across heterogeneous settings to evaluate robustness to variability in objects, layouts, and objectives.
\end{itemize}

\input{figures/libero_tasks}

With the exception of \textit{LIBERO-90}, which includes a diverse collection of 90 tasks, the remaining suites each consist of 10 tasks accompanied by 50 demonstrations per task. For the purposes of training and evaluation efficiency, our study focuses on the first four suites (LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long).

\subsection{Robocasa Benchmark}

RoboCasa is a large-scale simulation framework developed to train generalist robots in realistic and diverse home environments, with a particular focus on kitchen scenarios. The benchmark comprises 100 tasks, including 25 atomic tasks with 50 human demonstrations and 75 composite tasks with auto-generated demonstrations. These tasks are centered around eight fundamental robotic skills relevant to real-world home environments: (1) pick-and-place, (2) opening and closing doors, (3) opening and closing drawers, (4) twisting knobs, (5) turning levers, (6) pressing buttons, (7) insertion, and (8) navigation.

% ADD Scene variation

To comprehensively evaluate our method, we selected five tasks from the atomic tasks, each representing a distinct skill:

\begin{itemize}
    \item \textit{Close Single Door} :  Opening and closing doors
    \item \textit{Open Drawer} :  Opening and closing drawers
    \item \textit{Turn on Stove} :  Twisting knobs
    \item \textit{Coffee Press Button} :  Pressing buttons
    \item \textit{Coffee Serve Mug} :  Insertion
\end{itemize}

One of the key advantages of RoboCasa is its provision of both image-based (RGB) and 3D observations, enabling evaluations across different sensing modalities. Accordingly, we assessed our method using both RGB and 3D observations.

% \section{Model Details}

% \subsection{Parameter Comparison}
% We evaluate the inference time on a local PC

% \input{results/tables/parameters}


% \input{figures/blocks_ablation/blocks_ablation}

\section{Additional Baselines Details}
\subsection{Baselines}
% What it is - what is the advantage
\label{appendix:baselines}

\textbf{Diffusion Policy} \cite{chi2023diffusion} is a visuomotor policy that optimizes the action distribution iteratively using a conditional denoising diffusion process on a learned gradient field. It demonstrates the capability to capture the multi-modal action distributions. Incorporating techniques such as receding horizon control and visual conditioning, the learned visuomotor policy can be deployed to real-world embodiments.
% generates robot action sequence by iteratively optimizing the action distribution on a learned gradient field using stochastic Langevin dynamics steps. The gradient field is learned to mimic a conditional denosiing diffusion process.


\textbf{Octo} \cite{octo_2023} is an open-source vision-language-action (VLA) model. It uses a transformer-based diffusion policy that supports both language and goal image as task input. The policy is trained on a large-scale dataset and can be deployed to various embodiments.


\textbf{OpenVLA} \cite{kim2024openvlaopensourcevisionlanguageactionmodel}is another open-source vision-language-action model. Different from Octo, OpenVLA is based on a much larger model Llama 2 7B. It uses 256 reserved tokens for action values, providing higher resolution for robot control signals.

\textbf{MDT} \cite{reuss2024multimodaldiffusiontransformerlearning} is a diffusion-based framework that is able to learn versatile behavior from multimodal goal specification including images and languages. It introduces latent goal representations by aligning the latent embeddings of the goal image and the language instructions of the same tasks. This alignment is especially beneficial when there are few language annotations in the dataset. 

\textbf{MaIL} \cite{jia2024mailimprovingimitationlearning} uses MAMBA to replace transformer-based backbones in the imitation learning. It demonstrates superior performance compared to transformer-based architectures, especially in the case of small datasets. 

\textbf{ATM} \cite{wen2024anypointtrajectorymodelingpolicy} Any-point Trajectory Modeling (ATM) is a framework learning from video demonstrations. ATM predicts the trajectories of arbitrary points in a video frame using images and language instructions as input. The robot action is derived from trajectory predictions and the current video frame using MLP. 

\textbf{EnerVerse} \cite{huang2025enerverseenvisioningembodiedfuture} is a framework designed for future space generation regarding robotic manipulation tasks. This framework uses Free Anchor View (FAV) and 4D Gaussian Splatting (4DGS) to generate the next frames of the videos in manipulation scenarios. A policy head is added to the video generator in order to predict the corresponding action simultaneously. EnverVerse shows improved performance in long-horizon manipulation tasks.
