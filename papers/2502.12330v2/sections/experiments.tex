\section{Experiments}
\label{sec:experiments}

To explore the design space of Imitation Learning, we conduct extensive experiments on two robot learning benchmarks: LIBERO and RoboCasa. Our study systematically examines various backbones, architectures, and policy designs for both visual and point cloud-based imitation learning.

% Our experiments mainly focus on the following questions:
% \textbf{Q1)} How do the models in X-IL compare to the state-of-the-art models \\
% \textbf{Q2)} Compared to Transformer-based imitation learning, would Mamba and xLSTM bring further advantages? \\
% \textbf{Q3)} What makes a strong representation of Imitation Learning, from the perspective of vision and point cloud? \\
% \textbf{Q4)} How do different policy representations 

\subsection{Simulation Benchmark}

\textbf{LIBERO} \cite{liu2023liberobenchmarkingknowledgetransfer}
% include both 10 trajectories and 50 trajectories, compare different structures.
We evaluate our modular framework with various model architectures and policy heads using RGB inputs on the LIBERO benchmark, which comprises four distinct task suites: \textit{LIBERO-Spatial}, \textit{LIBERO-Object}, \textit{LIBERO-Goal}, and \textit{LIBERO-Long}. These task suites are specifically designed to evaluate different aspects of robotic learning and manipulation capabilities.

To thoroughly compare the performance of each architecture, we conduct evaluations using both 10 trajectories (20\% of the available demonstrations) and 50 trajectories (the full dataset). All models were trained for 100 epochs in LIBERO task suites, and we used the last checkpoint for evaluation. Following the official LIBERO benchmark settings, we simulated 50 rollouts for each sub-task, totaling 500 simulations per task suite. We report the average success rate for each task suite over 3 seeds. 

\input{figures/libero_robocasa_diff/lebero_robocas_diff}


\textbf{RoboCasa} \cite{nasiriany2024robocasalargescalesimulationeveryday} is a large-scale simulation framework, which provides various tasks in everyday scenarios. Besides the large amounts of tasks, there are extensive intra-task variations in RoboCasa. The variations include scenes, objects, and initial positions of the robot and the objects, while LIBERO does not provide this kind of diversity. As shown in Figure \ref{fig:libero_robocasa_diff}, in the CoffeeServeMug task, the coffee mugs, the coffee machine, and their surroundings are different and indicate different constraints, i.e. the robot can not grasp the mug from left in the left third variation due to the toast machine. This high level of diversity demands strong generalization from the model, which makes this benchmark very challenging.

% Due to this diversity, it is very challenging to train a policy to achieve a reasonable success rate using the same amount of demonstrations as in LIBERO, i.e. only 50 trajectories.

We evaluate 5 tasks in RoboCasa with 50 human demonstrations for each task. The 5 tasks contain different behaviors, CloseSingleDoor, OpenDrawer, TurnOnStove, CoffeePressButton, and CoffeeServeMug. For training, we train each model for 200 epochs and rollout the models for 50 episodes for each task. We report the success rate over 3 seeds.
% \begin{figure}
%     \centering
%     \includesvg{}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}

\input{results/tables/main_table_2d}

\subsection{Experimental Setup in X-IL}
To ensure a fair comparison, we match the model sizes of Transformer, Mamba, and xLSTM. For both the diffusion policy and flow matching policy, we set the number of sampling steps to 4 in the main experiments. In the LIBERO Benchmark, all models use ResNet-18 for image processing, whereas in the RoboCasa Benchmark, we employ FiLM-ResNet18 for image encoding and an attention-based encoder for point cloud inputs.

\subsection{Baselines}
% What it is - what is the advantage
We additionally report the performance of the following baselines:

\textbf{BC-Transfromer} BC-Transformer is used in RoboCasa \cite{nasiriany2024robocasalargescalesimulationeveryday}. It uses a CLIP model and a ResNet-18 with FilM layers to encode goal instructions and the the image-based observations, respectively.

\textbf{Diffusion Policy} \cite{chi2023diffusion} is a visuomotor policy that optimizes the action distribution iteratively using a conditional denoising diffusion process on a learned gradient field. 
% It demonstrates the capability to capture the multi-modal action distributions.


\textbf{Octo} \cite{octo_2023} is an open-source vision-language-action (VLA) model trained on a large-scale dataset. It uses a transformer-based diffusion policy that supports both language and goal image as task input. 

\textbf{OpenVLA} \cite{kim2024openvlaopensourcevisionlanguageactionmodel} is an vision-language-action model based on a much larger model Llama 2 7B. 

\textbf{MDT} \cite{reuss2024multimodaldiffusiontransformerlearning} is a diffusion-based framework that is able to learn versatile behavior from multimodal goal specification including images and languages. 
% It introduces latent goal representations by aligning the latent embeddings of the goal image and the language instructions.

\textbf{MaIL} \cite{jia2024mailimprovingimitationlearning} uses MAMBA to replace transformer-based backbones in the imitation learning. It demonstrates superior performance compared to transformer-based architectures, especially in the case of small datasets. 

\textbf{ATM} \cite{wen2024anypointtrajectorymodelingpolicy} Any-point Trajectory Modeling (ATM) is a framework learning from video demonstrations. ATM predicts the trajectories of arbitrary points in a video frame using images and language instructions as input. 

\textbf{EnerVerse} \cite{huang2025enerverseenvisioningembodiedfuture} is a framework designed for future space generation regarding robotic manipulation tasks. A policy head is added to the video generator in order to predict the corresponding action simultaneously.

\textbf{3D Diffusion Policy (DP3)} \cite{ze20243d}. DP3
extracts point-wise features from single-view points clouds. Robot actions are generated conditioned on these features and the current robot states.

% \textbf{3D Diffuser Actor (3DA)} \cite{ke20243d}. 3DA is a diffusion-based policy conditioned on 3D scene features from single or multi-view depth images and language instructions.
% The policy use rotation and translation of the robot's end-effector as action.

More detailed description of the policy representations see Appendix \ref{appendix:baselines}.

\input{results/tables/main_table_3dpc}

\subsection{Evaluation on Visual Inputs}
\textbf{LIBERO.} We report the main results in Table \ref{table:main_results_2d}. To evaluate our framework on LIBERO, we tested BC, BESO, and RF policies using Decoder-only architectures across Transformer, Mamba, and xLSTM backbones. Our results demonstrate that X-IL achieves state-of-the-art performance, surpassing publicly available models. Specifically, xLSTM shows great potential in both 20\% and 100\% data settings, where it achieves 74.5\% average success and 92.3\% average success respectively. 

\textbf{RoboCasa.} We report the main results in Table \ref{table:main_results_robocasa}. Compared to LIBERO, RoboCasa presents a more challenging benchmark due to its dynamically changing background scenes and object variations across demonstrations and evaluations. We tested X-BESO on five tasks within RoboCasa and observed that our approach outperforms the results reported in the original paper. Specifically, using xLSTM-based models, we achieved a higher average success rate of 53.6\%, compared to 40.0\% of BC-Transformer, demonstrating the effectiveness of our method in handling complex and dynamic environments. Additionally, we observe that Mamba and xLSTM outperform Transformer-based backbones, which is consistent with our findings from LIBERO. This result further highlights the potential of leveraging new sequential models in imitation learning, suggesting that alternative architectures beyond Transformers can offer improved efficiency and performance in complex robotic tasks.

\input{figures/architecture_ablation/architecture_ablation}

\subsection{Evaluation on Point Cloud Inputs}
We report the main results in Table \ref{table:main_results_robocasa}.
We evaluate X-BESO using point cloud inputs on RoboCasa and achieve superior results compared to 3D Diffusion Policy. An interesting observation from our results is that point cloud-based inputs do not necessarily outperform RGB-based inputs.

Our analysis suggests that this is due to the complexity of RoboCasa's scenarios, where point clouds are captured from diverse sources, leading to significant information loss during sampling—especially in tasks involving small objects. In such cases, only a sparse set of points remains, limiting the effectiveness of point cloud representations. This highlights the potential benefits of object-centric approaches that focus on preserving critical task-relevant details.

Additionally, we evaluate the performance of combining Point Cloud and RGB inputs. A compact representation is first extracted from the point cloud and then concatenated with the RGB features. Experimental results demonstrate that incorporating both modalities significantly enhances performance, particularly for the xLSTM-based model, which achieves a 60.9\% success rate—compared to 53.6\% with RGB alone and 32.8\% with Point Cloud alone. This highlights the importance of exploring more effective multi-modal fusion strategies to fully leverage the strengths of each modality.

\input{figures/diffusion_steps/diffusion_steps}

\subsection{Comparison on different architectures}
We conduct experiments on four tasks—Spatial (20\%) and Long (20\%) from LIBERO, as well as TurnOnStove and CoffeeServeMug from RoboCasa—to compare the performance of Decoder-only and Encoder-Decoder architectures. The results, presented in Figure \ref{fig:architecture_ablation}, show that the AdaLN-conditioned Encoder-Decoder architecture achieves superior performance on most tasks, highlighting its effectiveness. Furthermore, by processing observations and actions separately, this design offers more flexibility in choosing different layers for the encoder and decoder, making it more scalable to larger models.

% \input{results/tables/architecture_ablations}


% \input{results/tables/inference_time_diffusion}
\subsection{Comparison of Diffusion Models Across Varying Inference Steps}
We evaluate Decoder-only xLSTM with DDPM, BESO, and RF on the challenging TurnOnStove task in RoboCasa, comparing performance and inference speed across 1, 4, 8, 12, and 16 inference steps (Figure \ref{fig:diffusion_steps}). DDPM struggles with a single step, while BESO and RF perform well and improve with more steps. Their inference times are similar, and the speed advantage of flow matching is less noticeable due to the lower action dimension.

\subsection{Comparison on different encoders}
We evaluate different image encoders on the RoboCasa dataset using Dec-xLSTM BESO, comparing FiLM-ResNet18, FiLM-ResNet34, and CLIP (frozen) to assess their impact on performance. We also compare the Max-Pooling and Attention-based Point Cloud encoders. The results are presented in Figure \ref{fig:image_encoders}.

Our findings reveal that despite RoboCasa's requirement for generalization to new scenes and objects, frozen CLIP encoders perform poorly on most tasks. In contrast, fine-tuned ResNet18 and ResNet34 demonstrate strong performance, suggesting that domain adaptation plays a crucial role in achieving effective visual representations for imitation learning. A potential reason for CLIP’s underperformance is 
the significant domain gap between its pretraining data and robot manipulation datasets. This indicates that pretraining on robotics-specific datasets could further improve visual representations for imitation learning.

Compared to the Max-Pooling encoder, the Attention-based encoder shows better performance on most tasks, which indicates that attention can better capture the geometric structures of Point Cloud.

% \input{results/tables/inference_time_diffusion}
% inference speed
% agent inference time.
% time agent, 
% 10 layers transformer decoder-only backbone
% average on 5000 testing data
% RTX 4070Ti Super

% \subsection{Inference Speed comparison}
% TODO
% image  and point cloud

% choose decoder-only transformer, mamba 1/2, xlstm, test inference speed

% agent inference speed, 4 diffusion steps

% future: encoder / decoder;
% \input{results/tables/inference_time_architecture}
