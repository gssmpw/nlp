\section{Related Work} \label{sec: related work}

\textbf{Multi-modal Imitation Learning.} 
Early imitation learning methods relied on either state \cite{schaal1996learning, ho2016generative, torabi2018behavioral} or images \cite{pomerleau1988alvinn, lynch2020learning, young2021visual} to describe the environment and define the goal. However, obtaining accurate state information in real-world setting is not straightforward, and state-based representation struggles to capture the complexity of unstructured environments. Conversely, images provide a rich representation for behavior learning \cite{robomimic2021} and can be directly acquired from raw sensory input. Despite these advantages, using images as goal conditions in imitation learning is limited by their ambiguity in goal representation and difficulty in goal specification, making them less flexible for real-world deployment.
To address this, natural language has been explored as an alternative goal representation, offering a more intuitive and accessible way to specify tasks. Recent studies \cite{shridhar2022cliport,reuss2024multimodal, bharadhwaj2024roboagent} have explored the integration of language goals with image observation, enabling more flexible policy learning. Another line of research fine-tunes Vision-Language Models (VLMs) models to obtain Vision-Language Action Models (VLAs) \cite{kim2024openvlaopensourcevisionlanguageactionmodel, li2023vision, li2023generalist}. However, purely image-based representation lack crucial 3D structural information, which is essential for many tasks. Therefore, there is a recent trend on incorporating richer 3D scene presentations, such as point clouds, to enhance policy performance \cite{ke20243d, Ze2024DP3}. 

\textbf{Imitation Learning with Sequence Models.}
In recent years, sequence models have been increasingly applied to learning human behaviors, as human decision-making is inherently non-Markov and requires incorporating historical observations \cite{robomimic2021}. Early works utilized RNN-based structures\cite{robomimic2021}. However, these models suffers from vanish gradient for handling long observation sequence and low training efficient due to the sequential processing nature. To address this limitation, Transformer-based architectures have been widely adopted \cite{shafiullah2022behavior,reuss2023goal,bharadhwaj2024roboagent}, offering superior scalability and sequence modeling capabilities. Most recently, State-Space Models(SSM) \cite{gu2024mambalineartimesequencemodeling,jia2024mailimprovingimitationlearning} have emerged as a promising alternative to Transformers, demonstrating remarkable efficiency on small datasets and the ability to learn consistent representation.  Additionally, improved RNN-based architectures, such as xLSTM \cite{beck2024xlstmextendedlongshortterm}, have shown potential to rival both Transformer and SSMs in natural language processing. However, their application in imitation learning remains largely unexplored. X-IL aims to bridge this gap by incorporating Transformers, Mamba, and xLSTM as modular components and empirically evaluating their performance under different tasks and representations.

% LSTM, Transformer, State Space Model, xLSTM
\textbf{Modular Imitation Learning Libraries.}
While numerous open-source libraries provide algorithm-specific implementation of imitation learning methods \cite{chi2023diffusion, lee2024behavior, jia2024mailimprovingimitationlearning}, only a few offer modular design that spans multiple algorithms and architectures. Robomimic \cite{robomimic2021} implements Behavior Cloning (BC) with MLP, RNN, and Transformer-based policies,
while Imitation \cite{gleave2022imitation} provides modular implementations of several imitation learning and inverse reinforcement learning methods. However, these libraries do not include recent diffusion-based imitation learning approaches.
To address this gap, a recent work, CleanDiffuser \cite{cleandiffuser}, introduces a modular implementation for diffusion models in decision-making, supporting policy architectures such as MLP, UNet, ResNet, and Transformer. However, its evaluation is limited to tasks with low dimensional state input and 2D image input. In contrast, X-IL expands modularity by supporting multi-modal inputs, including 2D images, point clouds, and language-conditioned goals. Additionally, X-IL integrates state-of-the-art sequence models, such as Mamba and xLSTM, broadening its applicability to more complex environments and diverse IL architectures.