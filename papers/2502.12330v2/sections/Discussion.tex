\section{Discussion} \label{sec: discussion}
Here, we list several general observations \textbf{O1)-O4)} that are based on the experiments from Section \ref{sec:experiments}.

%%%%%%%%%%%%%%%%
\textbf{O1)} Given similar model sizes, Mamba and xLSTM-based policies outperform Transformer-based policies in both RGB inputs and Point Cloud inputs across LIBERO and RoboCasa, demonstrating their potential as viable alternatives to Transformers in imitation learning. 

\textbf{O2)} AdaLN conditioning is a strong method to build Encoder-Decoder architectures in imitation learning and is suitable for all kinds of sequential models. Injecting observation representations to the action decoder through adaLN conditioning could further improve the model's performance compared to the Decoder-only structure.
% However, their inference efficiency varies depending on sequence length.
% While Mamba and xLSTM excel in tasks with long sequences, imitation learning typically involves shorter sequences, where Transformers benefit from years of optimization, including techniques like Flash Attention. As a result, Mamba and xLSTM exhibit slower inference speeds in this domain.
% Regarding training efficiency, Mamba incurs a significantly higher computational cost, whereas xLSTM has training times comparable to Transformers. Given this trade-off, we recommend xLSTM as a practical choice for researchers exploring new architectures in imitation learning, as it balances efficiency, performance, and training speed effectively.

%%%%%%%%%%%%%%%%
\textbf{O3)} Point cloud representations do not necessarily outperform RGB-based representations in imitation learning. While previous works have demonstrated strong performance using point cloud inputs, their evaluations were typically conducted on simpler tasks, where the sampled points were highly task-relevant.\input{figures/image_encoders/image_encoders}
For more complex tasks, however, the Furthest Point Sampling (FPS) method distributes points evenly across the scene, often leading to significant information loss—especially for tasks requiring fine-grained object interactions. This suggests that standard point cloud sampling techniques may not always be optimal, and developing task-aware sampling or object-centric approaches could further improve the effectiveness of point cloud representations in imitation learning.


%%%%%%%%%%%%%%%%
\textbf{O4)} Better strategies for combining point cloud and image representations need further exploration. Our experiments show that simply concatenating point cloud and RGB inputs to the policy improves performance, but the gains are not significant. This suggests that a more structured fusion mechanism is required to fully leverage the complementary nature of these modalities. Finding the right balance between point cloud and image features remains an open challenge.


%%%%%%%%%%%%%%%%
\textbf{O5)} Our experiments highlight the necessity of robot-specific trained encoders. Comparing fine-tuned FiLM-ResNet with frozen CLIP, we observe that CLIP performs poorly on most tasks. We attribute this to the fact that manipulation tasks require task-specific features, which are not well captured by CLIP’s broad, vision-language pretraining.