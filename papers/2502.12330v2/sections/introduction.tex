\section{Introduction}
Imitation learning (IL) \cite{osa2018algorithmic} has emerged as a powerful paradigm for teaching agents complex behaviors by mimicking demonstrations, eliminating the need for explicit reward engineering \cite{argall2009survey}. However, the rapid development of novel machine-learning techniques across various domains makes it challenging to assess their potential impact on imitation learning.
To address this, we introduce X-IL, a novel framework designed to integrate and explore recently developed techniques within an imitation learning pipeline. Our framework decomposes the imitation learning process into four key modules: (1) observation representations, (2) backbones, (3) architectures, and (4) policy representations. Each module is interchangeable, enabling systematic exploration of the design space for imitation learning policies. This modularity facilitates rapid prototyping, benchmarking, and deployment.
Figure \ref{fig:arch_moil} provides an overview of our framework.

More specifically, we offer various observation representations, including 2D RGB inputs and 3D point cloud representations, to accommodate diverse perception tasks. Our framework incorporates versatile encoders, such as MLPs, ResNet \cite{he2015deepresiduallearningimage}, ViT \cite{dosovitskiy2021imageworth16x16words}, and pre-trained models, which can be tailored to different input types. Furthermore, we offer both Decoder-only and Encoder-Decoder architectures. Decoder-only is simpler with fewer parameters, while Encoder-Decoder supports additional representation learning for better generalization and scaling.
% \todo[inline]{@David: Briefly explain why you offer these, i.e., why are they important?}.
We define the backbone as the core computational unit of the policy architecture, offering support for Transformer \cite{vaswani2017attention}, Mamba \cite{gu2024mambalineartimesequencemodeling}, and xLSTM \cite{beck2024xlstmextendedlongshortterm}.
% \todo[]{use your definition @David.}
Lastly, we offer several state-of-the-art policy representations including popular diffusion- and flow-based models \cite{ho2020denoising, chi2023diffusion, reuss2023goal,lipman2023flowmatchinggenerativemodeling, du2022toflowefficientcontinuousnormalizing}.

% Our framework provides seamless integration of multimodal observations, including 2D RGB inputs and 3D point cloud representations, to accommodate diverse perception tasks. It incorporates versatile encoders, such as MLPs, ResNet \cite{he2015deepresiduallearningimage}, ViT \cite{dosovitskiy2021imageworth16x16words}, and pretrained models, which can be tailored to different input types. Architecturally, the framework supports state-of-the-art models, including Transformers \cite{vaswani2017attention}, Mamba \cite{gu2024mambalineartimesequencemodeling, jia2024mailimprovingimitationlearning}, and xLSTM \cite{beck2024xlstmextendedlongshortterm}, enabling it to scale to complex temporal dependencies and multimodal fusion. Policy heads such as Behavior Cloning (BC) \cite{pomerleau1988alvinn}, Denoising Diffusion Probabilistic Models (DDPM) \cite{ho2020denoising, chi2023diffusion}, BESO \cite{reuss2023goal} and Flow Matching \cite{lipman2023flowmatchinggenerativemodeling, du2022toflowefficientcontinuousnormalizing} are also supported, ensuring compatibility with cutting-edge imitation learning approaches.
% In addition to flexibility in input and architecture, our framework is designed to handle diverse environments, including robotic manipulation tasks like LIBERO \cite{liu2023liberobenchmarkingknowledgetransfer} and RoboCasa \cite{nasiriany2024robocasalargescalesimulationeveryday}. 
% By modularizing the imitation learning pipeline, we facilitate rapid prototyping, benchmarking, and deployment of modern imitation learning policies.

\input{figures/architectures}
Our contributions can be summarized as follows:

\textbf{1)} We introduce X-IL, a user-friendly and highly modular framework for imitation learning that supports multi-modal inputs, flexible encoders, diverse architectures and policy representations.

\textbf{2)} We leverage our framework to systematically explore the design space of IL policies. In doing so, we obtain novel policy designs that achieve state-of-the-art results on the LIBERO \cite{liu2023liberobenchmarkingknowledgetransfer} and RoboCasa \cite{nasiriany2024robocasalargescalesimulationeveryday} benchmarks. 
% \todo[]{Cite}
 
\textbf{3)} Our extensive experiments yield valuable insights, such as the potential of recent sequence models as strong alternatives to Transformers, or, that fusing different input modalities can lead to significant performance improvements.

% \textbf{4)} We find that Point Cloud-based models do not always outperform RGB-based models. However, combining both modalities leads to significant performance improvements, highlighting the benefits of multi-modal fusion.
    

% \begin{itemize}
%     \item We introduce X-IL, a highly modular framework for imitation learning that supports multi-modal inputs, flexible encoders, diverse architectures, and various policy representations.
%     \item We conduct extensive experiments on the recent LIBERO and RoboCasa benchmarks, demonstrating that policies from X-IL achieve state-of-the-art performance compared to existing public methods.
%     \item We find that the new sequential models, Mamba and xLSTM, consistently outperform Transformers, highlighting their potential as strong alternatives for imitation learning.
%     \item We demonstrate that Point Cloud-based models do not always outperform RGB-based models. However, combining both modalities leads to significant performance improvements, highlighting the benefits of multi-modal fusion.
    
%     % \item We demonstrate the frameworkâ€™s efficacy across a range of tasks, showcasing its versatility and ease of use.
% \end{itemize}

Our work is organized as follows. In \Cref{sec: related work}, we review related work. \Cref{sec: framework} describes the proposed framework, detailing design choices and its modular components. In \Cref{sec:experiments}, we provide experimental evaluations across multiple benchmarks, followed by a discussion of results and observations in Section \ref{sec: discussion}. 

