\section{The X-IL Framework} \label{sec: framework}
In this section, we introduce X-IL, a modular and open-source framework for imitation learning, based on the following design principles:

{\textbf{Modularity.}} X-IL systematically decomposes the imitation learning pipeline into different modules with different components that are easily interchangeable. This modular design enables flexible integration and evaluation of different approaches, facilitating systematic exploration of the design space of imitation learning policies.

{\textbf{Ease-of-use principle.}} Our framework is easy to use, supporting popular tools such as Hydra \cite{Yadan2019Hydra} for configuration management and Weights \& Biases (Wandb) \cite{wandb} for logging and visualization, streamlining the experimentation process.

{\textbf{Incorporation of Novel Techniques.}} X-IL integrates recent advancements such as Mamba \cite{gu2024mambalineartimesequencemodeling} and xLSTM \cite{beck2024xlstmextendedlongshortterm} for sequence modeling and Diffusion and Flow Matching for policy learning, improving the efficiency and generalization of imitation learning policies.

To enable flexible experimentation, we decompose the imitation learning pipeline into four key modules: 1) observation representations, 2) backbones, 3) architectures, and 4) policy representations. An overview of our framework is shown in Figure \ref{fig:arch_moil}. Below, we provide a detailed description of each module and its components.

\subsection{Observation Representations} \label{sec: observation}
% The choice of representation of observations plays a crucial role in determining the generalization capabilities of learned policies \todo[]{CITE}. 
Our framework considers three primary types of representations: RGB inputs, Point Cloud, and Language.
Below, we introduce these representations and how we encode them.

% \textbf{Robot Proprioception}
% provides internal state information, which can include joint positions, velocities, and end-effector states, which are essential for precise motion control. These signals are typically low-dimensional but highly informative for robotic manipulation. In X-IL, we standardize proprioceptive data through normalization and use MLP to encode it.

\textbf{RGB Inputs.} 
Visual imitation learning has received significant attention in recent research \cite{chi2023diffusion}. RGB images, captured from multiple camera viewpoints, provide essential texture and semantic information for object recognition and scene understanding. Prior works have demonstrated that ResNet is a strong encoder for manipulation tasks, making it a widely adopted choice \cite{shafiullah2022behaviortransformerscloningk, wan2024lotuscontinualimitationlearning, zhu2023violaimitationlearningvisionbased}. To effectively leverage RGB data, X-IL supports various feature extractors, including ResNet, FiLM-ResNet \cite{turkoglu2022filmensembleprobabilisticdeeplearning}, and CLIP \cite{radford2021learningtransferablevisualmodels}, with a modular codebase that allows easy integration of additional image encoders. Each image is encoded as a single token and passed into the backbone (see \Cref{sec: backbones}) for further processing, enabling flexible and efficient representation learning.

% \input{figures/pc_encoders/pc_encoders}

\textbf{Point Cloud.}
Point clouds provide 3D spatial structures obtained from RGB-D cameras or LiDAR sensors, offering geometric information for manipulation tasks. Unlike RGB images, point clouds inherently encode object positions and shapes, making them ideal for tasks requiring fine-grained spatial reasoning. Prior works have emphasized the importance of preserving geometric features for effective representation learning \cite{wan2024lotuscontinualimitationlearning, ze20243ddiffusionpolicygeneralizable, gyenes2024pointpatchrlmaskedreconstruction}. In X-IL,
we use Furthest Point Sampling (FPS) \cite{qi2017pointnet} to downsample points, which helps preserve the geometric structures of the 3d space. We adopt a token-based representation to capture scene geometry efficiently. Our framework supports two encoders: a lightweight MLP with Max Pooling \cite{ze20243ddiffusionpolicygeneralizable} for computational efficiency and an attention-based encoder with class token for enhanced feature extraction. The implementation details can be found in Appendix \ref{subsec:repr-encoders}.

\textbf{Language.}
Language-guided imitation learning \cite{stepputtis2020languageconditionedimitationlearningrobot, lynch2021languageconditionedimitationlearning, mees2022matterslanguageconditionedrobotic, yu2023usingdemonstrationslanguageinstructions, reuss2024multimodaldiffusiontransformerlearning} has gained increasing attention as it provides a high-level, abstract way to describe tasks, object attributes, and robot actions, making it a valuable modality in imitation learning. Unlike visual and geometric inputs, language offers context that enhances generalization and adaptability across diverse tasks. To process language, X-IL integrates the pre-trained language model CLIP \cite{radford2021learningtransferablevisualmodels} to convert textual information into dense embeddings. These embeddings are then fused with visual and point cloud features, enabling a richer multimodal representation for policy learning.

\input{figures/x_block}

\subsection{Backbones: X-Block} \label{sec: backbones}
The choice of backbone architectures is critical for learning effective policies in imitation learning, as it determines how different input modalities are processed and how sequential dependencies are captured. We define backbones as the core components responsible for modeling sequential information. Previous works \cite{chi2023diffusion, jang2022bczzeroshottaskgeneralization, chen2021decisiontransformerreinforcementlearning, rouxel2024flowmatchingimitationlearning, shaier2022datadrivenapproachespredictingspread, bertasius2021spacetimeattentionneedvideo} have predominantly utilized convolutional 1D architectures, such as U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical}, or Transformer-based models for sequence modeling in imitation learning. However, recent advancements in sequence modeling, such as Mamba and xLSTM, remain largely unexplored in this domain. We provide a comparative analysis of these architectures:

\textbf{Transformer} \cite{vaswani2017attention}. A widely used attention-based model that has demonstrated strong performance in imitation learning due to its ability to handle non-Markovian behavior in human demonstrations. Most imitation learning models including Vision-Language Action Models (VLAs) use transformers as the backbone.

\textbf{Mamba} \cite{gu2024mambalineartimesequencemodeling}. A structured state-space model (SSM) that significantly improves the efficiency of SSMs while rivaling Transformers in performance. Unlike Transformers, Mamba maintains linear computational complexity. Mamba Imitation Learning (MaIL) \cite{jia2024mailimprovingimitationlearning} has shown that Mamba-based policies outperform Transformer-based policies with small datasets.

\textbf{xLSTM} \cite{beck2024xlstmextendedlongshortterm}. A variant of LSTM that is designed to enhance long-term dependency modeling while maintaining computational efficiency. Unlike standard LSTMs, which struggle with long-range dependencies, xLSTM incorporates architectural improvements to mitigate vanishing gradient issues. While recurrent models generally lack the expressiveness of self-attention, xLSTM offers a balance between efficiency and performance, making it a potential alternative for imitation learning tasks where computational constraints are a concern.  

% \textcolor{red}{Need to be refined!!!}
% \begin{itemize} 
%     \item \textbf{Transformer.} A widely used attention-based model that has demonstrated strong performance in imitation learning due to its ability to handle non-Markovian behavior in human demonstrations. However, their quadratic complexity in sequence length results in high computational costs, making training and inference less efficient, particularly in resource-constrained scenarios.
%     \item \textbf{Mamba.} A structured state-space model (SSM) that significantly improves the efficiency of SSMs while rivaling Transformers in performance. Unlike Transformers, Mamba maintains linear computational complexity, making it more scalable for long-horizon tasks. Mamba Imitation Learning (MaIL) \cite{jia2024mailimprovingimitationlearning} has shown that Mamba-based policies not only match but often outperform Transformer-based policies across various LIBERO IL tasks. Furthermore, Mamba exhibits superior robustness to input noise and remains effective even with smaller datasets, demonstrating its efficiency in imitation learning.  
%     \item \textbf{xLSTM.} A variant of LSTM designed to enhance long-term dependency modeling while maintaining computational efficiency. Unlike standard LSTMs, which struggle with long-range dependencies, xLSTM incorporates architectural improvements to mitigate vanishing gradient issues. While recurrent models generally lack the expressiveness of self-attention, xLSTM offers a balance between efficiency and performance, making it a potential alternative for imitation learning tasks where computational constraints are a concern.  
% \end{itemize}

In X-IL, we aim to investigate the potential of these architectures for policy learning. Inspired by the DiT-Block \cite{peebles2023scalable} structure, our framework introduces X-Block, as illustrated in Figure \ref{fig:x_block}. The core component of X-Block is the X-Layer, which is responsible for processing temporal information. We provide three backbone options: Transformer, Mamba, and xLSTM, allowing flexibility in sequential modeling. Additionally, AdaLN conditioning \cite{peebles2023scalable} is incorporatedâ€”not only for conditioning time embeddings in diffusion models but also for integrating representation features. Our findings indicate that using representations as conditioning signals enhances performance, further improving the effectiveness of policy learning.

\subsection{Architectures} \label{sec: architectures}
The architecture of an imitation learning model defines how input representations are processed and how action outputs are generated. X-IL supports two architectures: Decoder-Only and Encoder-Decoder.
% \todo[inline]{What are they and how are they different? Explain a bit more here.}
Prior works such as ACT \cite{zhao2023learning} and MDT \cite{reuss2024multimodaldiffusiontransformerlearning} adopt an encoder-decoder design, whereas PearceTransformer \cite{pearce2023imitating} and MoDE \cite{reuss2024efficient} follow a decoder-only approach.
Below, we introduce these architectures and explain their integration within our framework. The illustrations of them are given in Figure \ref{fig:arch_moil}. 
% \textcolor{red}{pros and cons?}

\textbf{Decoder-only Models.}
In X-IL, the Decoder-Only architecture is implemented by stacking multiple X-Blocks, where both observations and actions are jointly processed within the decoder. The model outputs only the action tokens, which are then used to train the policy representations.

% The Decoder-Only architecture follows the design of autoregressive models, where the policy directly predicts actions based on input observations. It processes multimodal inputs sequentially and generates outputs in a step-by-step manner, making it particularly effective for causal modeling and decision-making.

% In X-IL, the Decoder-Only architecture is implemented using Transformer, Mamba, or xLSTM backbones, where observations from proprioception, RGB, point clouds, and language are fed into the decoder, and the model iteratively predicts actions without requiring an explicit encoding phase. This design is computationally efficient and well-suited for autoregressive policy learning.

\textbf{Encoder-Decoder Models.}
The Encoder-Decoder architecture in X-IL follows a two-stage approach: the Encoder first encodes multi-modal inputs into a latent representation, and the decoder then generates actions based on this structured embedding. 
Prior works primarily utilize cross-attention to connect the encoderâ€™s output with the decoderâ€™s input. However, Mamba and xLSTM lack a built-in mechanism to handle variable-length sequences in this manner. Instead, we find that AdaLN conditioning provides an efficient and flexible alternative for constructing the Encoder-Decoder architecture, enabling effective integration of encoded representations into the decoding process.

% Both the encoder and decoder consist of multiple X-Blocks, with the decoder specifically incorporating AdaLN layers that condition the encoderâ€™s output to generate action predictions.

% Notice that prior works mainly use cross-attention to connect the Encoder's output and the Decoder's input. However, there is no such mechanism for Mamba and xLSTM to process different lengths of sequences. In contrast, we find that using adaLN conditioning is an efficient way to build the Encoder-Decoder architecture.

% This separation allows for richer feature extraction and improved generalization, especially in complex, long-horizon tasks.

% In X-IL, the Encoder-Decoder architecture processes inputs through a Transformer, Mamba, or xLSTM encoder, which extracts meaningful representations before passing them to a decoder. This setup is particularly useful for tasks requiring global context understanding, such as hierarchical imitation learning or language-conditioned policies.

% Both architectures provide flexibility in policy design, allowing X-IL to explore different modeling strategies based on task complexity, computational efficiency, and learning objectives.

\subsection{Policy Representations} \label{sec: policy representations}
Besides naive behavior cloning approaches, our framework offers a variety of state-of-the-art policy representations, which can be broadly categorized as diffusion-based and flow-based models.  


\textbf{Behavior Cloning} Behavior cloning (BC) assumes a Gaussian distribution as policy representation and maximizes the likelihood of predicted actions in the given ground truth distributions.

%Maximizing the likelihood of the policy results in a mean squared error (MSE) minimization between ground truth and predicted actions.

\textbf{Diffusion-Based Policies}
Denoising diffusion probabilistic models (DDPM) \cite{ho2020denoising} captures the score function field and iteratively optimizes the action. BESO \cite{reuss2023goal} is based on a continuous-time diffusion framework. BESO allows for varying diffusion steps, as well as diverse sampling techniques. Our framework supports both DDPM-style and continuous-time BESO-style policies.    

\textbf{Flow-Based Policies}  
Continuous-time normalizing flows trained via flow matching \cite{lipman2022flow} have recently gained a lot of attention and are also suitable as policy representations. 
These methods, often referred to as rectified flows (RF) \cite{liu2022flow} are fully supported in our framework.
% These methods are fully supported in our framework.

More detailed description of the policy representations see Appendix \ref{appendix:policy}.



