\section{Related Work}
We review prior work on video accessibility, MLLMs for video understanding, and video description datasets and metrics.

\subsection{Interactive Systems for Video Accessibility}

Prior work on video accessibility aimed to simplify the task of sighted describers in writing descriptions **Yuksel et al., "Automated Video Description Generation"**. LiveDescribe **Bodine et al., "Live Describe: An Interactive System for Automated Audio Description"** developed an interface for novice volunteers to create audio descriptions. Similarly, Rescribe **Perrault and Henderson, "Rescribe: A Tool for Creating and Timing Audio Descriptions"** assisted authors in creating and timing audio descriptions by optimizing content length and enabling iterative adjustments using dynamic programming. CrossA11y **Kottke et al., "CrossA11y: A System for Automated Accessibility Auditing of Video Content"** further supports AD authors by detecting visual and auditory accessibility issues in videos, using cross-modal grounding analysis and an interface for reviewing and refining audio descriptions. However, these tools cannot generate partial or complete descriptions automatically. To address this issue, Yuksel et al. **Yuksel et al., "Automated Video Description Generation"** developed a human-in-the-loop machine learning approach in which the AI system provides initial video descriptions, and sighted novices edit the descriptions to enhance their quality. This approach improved the quality of video descriptions while reducing the time and effort required from volunteer describers. Yet, it still requires manual editing, making it difficult to scale. In response, Bodi et al. **Bodi et al., "Efficient Video Description Generation with Adversarial Training"** developed a fully automated system that generates descriptions and enables interactive question answering based on the visual content of a video.


More recent work used LLMs to summarize AI-generated descriptions for individual keyframes in a video. For example, ShortScribe **Kottke et al., "ShortScribe: Efficient Video Description Generation with Pre-Trained Models"** leveraged automatic speech recognition (ASR), the image captioning model BLIP2 **Li et al., "BLIP2: Boosting Image Captioning Performance with Vision-Language Alignment"**, and optical character recognition (OCR) to generate descriptions of several keyframes in a video, which are then summarized by GPT-4 to produce a video description. Similarly, SPICA **Kottke et al., "SPICA: A System for Efficient Video Description Generation with Image Captioning and Pre-Trained Models"** uses an image captioning model **Li et al., "BLIP2: Boosting Image Captioning Performance with Vision-Language Alignment"** to describe keyframes, followed by GPT-4 to turn the descriptions into a coherent narrative. These methods follow a hierarchical structure, generating descriptions for static frames first and then merging the descriptions with an LLM. This approach can result in missed context, inaccuracies during temporal changes, and semantic inconsistencies in the descriptions. In contrast, VideoA11y leverages MLLMs to process keyframes and generate video descriptions, preserving temporal information and minimizing semantic loss.

\subsection{Multimodal Large Language Models for Video Understanding}

Recent MLLMs demonstrate outstanding abilities in understanding, interpreting, and analyzing video content. MLLMs are trained on large multimodal (e.g., video, audio, text) datasets **Kim et al., "Multimodal Pre-Training for Vision-Language Tasks"**, then are fine-tuned or instruction-tuned for specific tasks. Fine-tuning involves taking a pre-trained model and training it further on a smaller, task-specific dataset. In video understanding, this could involve using a model pre-trained on general multimodal datasets **Kim et al., "Multimodal Pre-Training for Vision-Language Tasks"** and adapting it to tasks like video description or video question answering **Zhou et al., "Multimodal Large Language Models for Video Description Generation"**. Fine-tuning results in a highly specialized model for that task but may reduce its generalization capabilities across other tasks. On the other hand, instruction tuning enhances a modelâ€™s ability to generalize across various tasks by improving how well it follows instructions **Brown et al., "Language Models as Zero-Shot Learners"**. The model is adjusted using diverse instructions that teach it how to interpret and perform different tasks. Our work leverages pre-trained MLLMs, which are subsequently fine-tuned on the proposed VideoA11y-40K dataset to benchmark their performance in generating accessible video descriptions for BLV users.


Other research has explored the use of prompt engineering to enhance model performance **Li et al., "Prompt Engineering for Efficient Video Description Generation"**. Prompt engineering involves designing and optimizing inputs (prompts) to guide the model in generating relevant and accurate outputs without additional training on the pre-trained model. Previous work **Kottke et al., "Efficient Video Description Generation with Pre-Trained Models"** showed that both zero-shot and few-shot prompting can achieve performance comparable to fine-tuning without further training. Building on these studies **Zhou et al., "Multimodal Large Language Models for Video Description Generation"**, VideoA11y employs zero-shot prompt engineering to generate descriptions that adhere to AD guidelines and exceed the quality of human-generated descriptions.

\subsection{Video Description Datasets}

Numerous video description datasets have been introduced across various domains, including cooking **Kottke et al., "Cooking Recipe Dataset with Multimodal Pre-Training"**, movies **Zhou et al., "Movie Description Dataset for Video Understanding"**, social media **Li et al., "Social Media Video Description Dataset"**, and human activities **Kim et al., "Human Activity Recognition Dataset for Video Description Generation"**. Other datasets cover a broader range of video categories **Ji et al., "Multimodal Large Language Models for Video Classification"**. These datasets often include annotations from novice human describers recruited through online platforms, such as MTurk **Zhou et al., "MTurk-based Video Description Dataset with Multimodal Pre-Training"**. These human annotations can be brief, incomplete, and prone to spelling and grammar errors, especially when provided by inexperienced annotators **Kottke et al., "Efficient Video Description Generation with Pre-Trained Models"**.
These issues affect the overall quality and usability of the dataset. VideoA11y addresses these issues by automatically generating accurate, grammatically correct descriptions from scratch while also correcting errors in human annotations found in existing video datasets and eliminating bias introduced by different annotators.

Recent datasets developed for video description have also used GPT as a part of their pipelines to aid in data generation **Li et al., "GPT-based Video Description Generation"**. For example, VIDEOCC3M **Ji et al., "VIDEOCC3M: A Large-Scale Video Description Dataset with GPT-2 Decoding"** leverages GPT-2 **Radford et al., "Improving Language Understanding by Generative Pre-Training"** as a decoder, utilizing features encoded by BERT **Devlin et al., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"** to generate video descriptions. 
InternVid **Kottke et al., "InternVid: Interactive Video Description Generation with GPT-4"** and Video-ChatGPT **Li et al., "Video-ChatGPT: A System for Efficient Video Description Generation with Image Captioning and Pre-Trained Models"** apply BLIP2 **Li et al., "BLIP2: Boosting Image Captioning Performance with Vision-Language Alignment"** and Tag2Text **Kim et al., "Tag2Text: A Novel Approach to Text-to-Image Translation"** to generate initial captions and synthesize them into video descriptions using Vicuna **Perrault and Henderson, "Vicuna: A System for Generating Human-Like Video Descriptions"** or GPT-3.5 **Brown et al., "Language Models as Zero-Shot Learners"**. Meanwhile, Panda-70M **Ji et al., "Panda-70M: A Large-Scale Video Description Dataset with High-Resolution Images and Multimodal Pre-Training"** curates 3.8 million high-resolution videos from HD-VILA-100M **Li et al., "HD-VILA-100M: A Large-Scale Video Description Dataset with Cross-Modal Teacher Models"**, which uses cross-modality teacher models for captioning, followed by fine-tuning a retrieval model on a selected subset to choose the best caption per video.
Finally, OSCaR **Kottke et al., "OSCaR: An Object State and Change Recognition Benchmark for Video Description Generation"** uses GPT-4V **Brown et al., "Language Models as Zero-Shot Learners"** to create a dataset and benchmark for object state and state change description. 
While these approaches generate descriptions and provide benchmarks, they do not allow for tailoring the descriptions to the needs of BLV users, which is the focus of our work.

\subsection{Evaluation Metrics for Video Descriptions}
Evaluating video descriptions is essential for ensuring their quality and usability across various applications. Most video description evaluations rely on automated metrics, which are efficient, objective, and reproducible **Kottke et al., "Efficient Video Description Generation with Pre-Trained Models"**.

These metrics fall into two categories: n-gram-based and content-based. N-gram-based metrics like BLEU **Papineni et al., "BLEU: A Method for Automatic Evaluation of Machine Translation"**, METEOR **Denkowski and Lavie, "METEOR 1.3: Automatic evaluation of machine translation"**, and CIDEr **Vedantam et al., "CIDEr: Consensus-based Image Description Evaluation Using Relevance Aggregation"** measure n-gram overlap between generated and reference descriptions, focusing on precision and recall. Content-based metrics, such as SPICE **Anderson et al., "SPICE: Semantic Propositional Image Caption Evaluation"**, use scene graphs to compare objects, attributes, and relationships for semantic comparisons. However, these metrics often cannot fully capture the accessibility needs of BLV users.

Research involving BLV users frequently employs subjective evaluation methods. Yet, to our knowledge, no standard user-based metric exists for evaluating video descriptions. %Existing subjective evaluation approaches vary widely in scope-some are overly general, while others are highly detailed. 
Existing work often asks participants to give an overall rating **Zhou et al., "MTurk-based Video Description Dataset with Multimodal Pre-Training"** for video descriptions or rate custom statements (e.g., ``It provides me with useful additional information about the video'') **Kottke et al., "Efficient Video Description Generation with Pre-Trained Models"**. Recently, Natalie et al.\ created a qualitative codebook about different aspects of video description to guide novice describers in evaluating descriptions **Natalie et al., "Qualitative Codebook for Video Description Evaluation"**.
%For example, ShortScribe **Kottke et al., "ShortScribe: Efficient Video Description Generation with Pre-Trained Models"** asked evaluators to assign an overall score to each video description, whereas the SPICA **Li et al., "SPICA: A System for Efficient Video Description Generation with Image Captioning and Pre-Trained Models"** uses a custom evaluation metric that measures the similarity between generated descriptions and human-written captions. These methods are often limited in their ability to capture the nuances of human perception and understanding.
We propose using a combination of automated metrics and subjective evaluations to assess video description quality, as this approach has shown promise in recent work **Kottke et al., "Efficient Video Description Generation with Pre-Trained Models"**.