\section{Related Work}
We review prior work on video accessibility, MLLMs for video understanding, and video description datasets and metrics.

\subsection{Interactive Systems for Video Accessibility}

Prior work on video accessibility aimed to simplify the task of sighted describers in writing descriptions~\citep{lei-etal-2020-mart, kobayashi2010synthesized, video_scene_natalie_2022}. LiveDescribe~\cite{livedescribe} developed an interface for novice volunteers to create audio descriptions. Similarly, Rescribe~\cite{rescribe} assisted authors in creating and timing audio descriptions by optimizing content length and enabling iterative adjustments using dynamic programming. CrossA11y~\cite{crossa11y} further supports AD authors by detecting visual and auditory accessibility issues in videos, using cross-modal grounding analysis and an interface for reviewing and refining audio descriptions. However, these tools cannot generate partial or complete descriptions automatically. To address this issue, Yuksel et al.~\cite{hitl_blv_2020,yuksel2020increasing} developed a human-in-the-loop machine learning approach in which the AI system provides initial video descriptions, and sighted novices edit the descriptions to enhance their quality. This approach improved the quality of video descriptions while reducing the time and effort required from volunteer describers. Yet, it still requires manual editing, making it difficult to scale. In response, Bodi et al.~\cite{bodi_2021,ihorn2021narrationbot} developed a fully automated system that generates descriptions and enables interactive question answering based on the visual content of a video.


More recent work used LLMs to summarize AI-generated descriptions for individual keyframes in a video. For example, ShortScribe~\cite{shortscribe} leveraged automatic speech recognition (ASR), the image captioning model BLIP2~\cite{blip2}, and optical character recognition (OCR) to generate descriptions of several keyframes in a video, which are then summarized by GPT-4 to produce a video description. Similarly, SPICA~\cite{spica} uses an image captioning model~\cite{ofa} to describe keyframes, followed by GPT-4 to turn the descriptions into a coherent narrative. These methods follow a hierarchical structure, generating descriptions for static frames first and then merging the descriptions with an LLM. This approach can result in missed context, inaccuracies during temporal changes, and semantic inconsistencies in the descriptions. In contrast, VideoA11y leverages MLLMs to process keyframes and generate video descriptions, preserving temporal information and minimizing semantic loss.

\subsection{Multimodal Large Language Models for Video Understanding}

Recent MLLMs demonstrate outstanding abilities in understanding, interpreting, and analyzing video content. MLLMs are trained on large multimodal (e.g., video, audio, text) datasets~\citep{vast, howto100m, yt-temporal}, then are fine-tuned or instruction-tuned for specific tasks. Fine-tuning involves taking a pre-trained model and training it further on a smaller, task-specific dataset. In video understanding, this could involve using a model pre-trained on general multimodal datasets and adapting it to tasks like video description or video question answering~\citep{zhao2023lavila, vast, vid2seq, merlin}. Fine-tuning results in a highly specialized model for that task but may reduce its generalization capabilities across other tasks. On the other hand, instruction tuning enhances a modelâ€™s ability to generalize across various tasks by improving how well it follows instructions~\citep{videollama, videollava, vtimellm, gpt4video, VideoChatGPT}. The model is adjusted using diverse instructions that teach it how to interpret and perform different tasks. Our work leverages pre-trained MLLMs, which are subsequently fine-tuned on the proposed VideoA11y-40K dataset to benchmark their performance in generating accessible video descriptions for BLV users.


Other research has explored the use of prompt engineering to enhance model performance~\citep{gpt2, fewshot, cot_prompt, optimization_prompt}. Prompt engineering involves designing and optimizing inputs (prompts) to guide the model in generating relevant and accurate outputs without additional training on the pre-trained model. Previous work~\cite{gpt2,fewshot} showed that both zero-shot and few-shot prompting can achieve performance comparable to fine-tuning without further training. Building on these studies~\cite{gpt2}, VideoA11y employs zero-shot prompt engineering to generate descriptions that adhere to AD guidelines and exceed the quality of human-generated descriptions.

\subsection{Video Description Datasets}

Numerous video description datasets have been introduced across various domains, including cooking~\cite{youcook2}, movies~\citep{mad, movienet, cmd}, social media~\cite{tvc}, and human activities~\citep{activity, msvd, vatex, charades, charadesego, videoxum}. Other datasets cover a broader range of video categories~\citep{chen2023valor, msr-vtt, webvid, internvid, panda70m, youkumplug, vitt}. These datasets often include annotations from novice human describers recruited through online platforms, such as MTurk~\citep{tvc, activity, msvd, vatex, msr-vtt}. These human annotations can be brief, incomplete, and prone to spelling and grammar errors, especially when provided by inexperienced annotators~\cite{klie2023annotation,chen2022msrvideo}. %Common errors include duplicate sentences, spelling mistakes, and syntax errors~\cite{chen2022msrvideo}. 
These issues affect the overall quality and usability of the dataset. VideoA11y addresses these issues by automatically generating accurate, grammatically correct descriptions from scratch while also correcting errors in human annotations found in existing video datasets and eliminating bias introduced by different annotators.

Recent datasets developed for video description have also used GPT as a part of their pipelines to aid in data generation~\cite{videocc,internvid,VideoChatGPT,panda70m}. For example, VIDEOCC3M~\cite{videocc} leverages GPT-2~\cite{gpt2} as a decoder, utilizing features encoded by BERT~\cite{bert} to generate video descriptions. 
InternVid~\cite{internvid} and Video-ChatGPT~\cite{VideoChatGPT} apply BLIP2~\cite{blip2} and Tag2Text~\cite{tag2text} to generate initial captions and synthesize them into video descriptions using Vicuna~\cite{vicuna} or GPT-3.5. Meanwhile, Panda-70M~\cite{panda70m} curates 3.8 million high-resolution videos from HD-VILA-100M~\cite{hdvila} uses cross-modality teacher models for captioning, followed by fine-tuning a retrieval model on a selected subset to choose the best caption per video.
Finally, OSCaR~\cite{oscar} uses GPT-4V to create a dataset and benchmark for object state and state change description. 
While these approaches generate descriptions and provide benchmarks, they do not allow for tailoring the descriptions to the needs of BLV users, which is the focus of our work.

\subsection{Evaluation Metrics for Video Descriptions}
Evaluating video descriptions is essential for ensuring their quality and usability across various applications. Most video description evaluations rely on automated metrics, which are efficient, objective, and reproducible~\cite{aafaq2019video}. 

These metrics fall into two categories: n-gram-based and content-based. N-gram-based metrics like BLEU~\cite{bleu}, METEOR~\cite{meteor}, and CIDEr~\cite{cider} measure n-gram overlap between generated and reference descriptions, focusing on precision and recall. Content-based metrics, such as SPICE~\cite{spice}, use scene graphs to compare objects, attributes, and relationships for semantic comparisons. However, these metrics often cannot fully capture the accessibility needs of BLV users.

Research involving BLV users frequently employs subjective evaluation methods. Yet, to our knowledge, no standard user-based metric exists for evaluating video descriptions. %Existing subjective evaluation approaches vary widely in scope-some are overly general, while others are highly detailed. 
Existing work often asks participants to give an overall rating~\cite{shortscribe} for video descriptions or rate custom statements (e.g., ``It provides me with useful additional information about the video'')~\cite{spica}. Recently, Natalie et al.\ created a qualitative codebook about different aspects of video description to guide novice describers in evaluating descriptions~\cite{metrics}. 
%For example, ShortScribe~\cite{shortscribe} asked evaluators to assign an overall score to each video description, whereas the SPICA~\cite{spica} had evaluators rate descriptions based on several specific questions. To improve the efficiency and standardization of subjective evaluations, 
We build on this codebook by proposing four custom metrics tailored to BLV users' needs: descriptive, objective, accurate, and clear. These metrics provide a structured framework to guide human evaluators and ensure consistency in assessing video descriptions.