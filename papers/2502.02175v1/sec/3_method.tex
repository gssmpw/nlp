\section{Preliminary}
\subsection{Vision-Language-Action Models}
Vision-Language-Action (VLA) models extend large-scale VLMs to generate robotic actions based on multimodal inputs, including visual observations and task instructions. These models are typically trained on large-scale datasets, such as Open X Embodiment~\cite{o2024open}, which contains over a million real-world robot trajectories. The training pipeline involves a pre-trained vision encoder \cite{liu2021swin,radford2021learning} extracting image features, which are projected into the input space of a Large Language Model (LLM). The LLM, fine-tuned alongside a projector module, encodes these inputs to predict actions. 

During inference, VLA models process sequential frames to iteratively guide the robot, but the full Transformer-based LLM inference at each step results in substantial computational overhead. The quadratic complexity of self-attention, combined with large key-value (KV) caches~\cite{ge2023model}, leads to high memory consumption and slow inference speeds, posing challenges for real-time deployment. While model compression techniques such as quantization~\cite{lin2024awq} and pruning~\cite{park2024quantization, DeeR-VLA} have been explored to reduce computational cost, they typically require re-training and may degrade performance, limiting their practicality in real-time applications. To mitigate these inefficiencies, our approach leverages token caching mechanisms to accelerate inference in a training-free manner, reducing computational cost while preserving inference accuracy.

\subsection{Key-Value Caching in Transformer Models}
Key-Value (KV) caching is a well-established optimization in autoregressive Transformers~\cite{vaswani2017attention,floridi2020gpt}, allowing previously computed key ($\mathbf{K}$) and value ($\mathbf{V}$) representations to be stored and reused in subsequent timesteps. Given input tokens $\mathbf{X}$, self-attention first projects them into queries, keys, and values:
\begin{equation}
\mathbf{Q} = \mathbf{X} W_Q, \quad
\mathbf{K} = \mathbf{X} W_K, \quad
\mathbf{V} = \mathbf{X} W_V.
\end{equation}
The attention output is computed as:
\begin{equation}
\mathrm{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
\mathrm{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{D}}\right) \mathbf{V}.
\end{equation}

In standard inference, new keys and values are appended to the cache:
\begin{equation}
\mathbf{K}_t = \mathrm{Concat}(\mathbf{K}_{t-1}, \mathbf{K}_{\text{new}}), \quad
\mathbf{V}_t = \mathrm{Concat}(\mathbf{V}_{t-1}, \mathbf{V}_{\text{new}}).
\end{equation}
While this improves efficiency by avoiding redundant recomputation, it does not reduce memory or computational costs, as all tokens are still processed.

VLA-Cache extends this approach by introducing \textbf{dynamic token reuse}. Instead of appending all new tokens, it identifies static tokens across frames and skips their recomputation:
\begin{equation}
\mathbf{K}_t(i) =
\begin{cases}
\mathbf{K}_{t-1}(i), & i \in \mathcal{P}_{\text{reuse}}, \\
W_K \mathbf{H}_t(i), & \text{otherwise}.
\end{cases}
\end{equation}
\begin{equation}
\mathbf{V}_t(i) =
\begin{cases}
\mathbf{V}_{t-1}(i), & i \in \mathcal{P}_{\text{reuse}}, \\
W_V \mathbf{H}_t(i), & \text{otherwise}.
\end{cases}
\end{equation}
where $\mathcal{P}_{\text{reuse}}$ represents the subset of tokens that remain visually and semantically unchanged between frames. By selectively updating only task-relevant tokens, VLA-Cache significantly reduces redundant computation while preserving high-fidelity action predictions.

This caching mechanism integrates seamlessly into the Transformer decoding process, requiring no architectural modifications or additional training. The following sections detail the implementation of VLA-Cache and its effectiveness in improving inference efficiency for VLA models.


\begin{figure*}[t]
\centering
{\includegraphics[width=1.98\columnwidth]{fig/main.pdf}}
\caption{In this paper, we propose VLA-Cache, which includes two procedures. (a) Dynamic Token Selection: First, a static token set is identified, followed by filtering out task-relevant tokens to ensure critical information undergoes full computation. Tokens that remain in the set and overlap with those from the previous step will be retrieved from cache.
(b) Adaptive Token Caching: The fraction of reused tokens is dynamically adjusted at each layer based on its attention distribution.
}
\label{fig:method}
\vspace{-0.2 cm}
\end{figure*}


\section{Methodology}

In robotic action prediction, consecutive image frames often show only minor changes in the manipulator or target objects, while the background remains mostly unchanged. This motivates us to reuse some of unchanged tokens in the last step to accelerate the inference at current step. However, the choice of reusing tokens should be made very carefully, as even small changes to the manipulator or target objects can be pivotal for accurate action prediction. Treating such tokens as static could severely degrade performance. Consequently, the core challenge in accelerating VLA inference via token caching lies in accurately distinguishing genuinely static regions from those that are dynamic or closely tied to the task. By automatically detecting static tokens across consecutive frames and discarding those related to the manipulator or the immediate task, one can realize substantial computational savings while still preserving high-quality action predictions. 


\subsection{Dynamic Token Selection}

\label{sec:dynamic_selection}
\textbf{Static Token Selection.}
To enhance inference efficiency, we identify visually static tokens across consecutive frames and reuse their associated Key-Value (KV) representations. This process involves three steps: partitioning the input image into patches, computing patch-wise similarity, and selecting the most static patches for reuse.

Given an image \(I\) of size \(H \times W\), we divide it into \(N \times N\) non-overlapping patches of size \(p \times p\), yielding \(N^2\) patches \(\mathcal{P} = \{\mathbf{P}_{i,j}\}\). Each patch \(\mathbf{P}_{i,j}\) is directly represented by its raw pixel values, without additional feature encoding.

For two consecutive frames \(I_t\) and \(I_{t-1}\), we calculate the cosine similarity between their corresponding patches:
\begin{equation}
\label{eq:sim}
\text{Sim}\big(\mathbf{P}_t^{i,j}, \mathbf{P}_{t-1}^{i,j}\big) 
= \frac{\mathbf{P}_t^{i,j} \cdot \mathbf{P}_{t-1}^{i,j}}{\|\mathbf{P}_t^{i,j}\|_2 \,\|\mathbf{P}_{t-1}^{i,j}\|_2}.
\end{equation}
A patch is considered static if its similarity score exceeds a predefined threshold \(\tau\). To further refine the selection, we apply a Top-\(k\) filter:
\begin{equation}
\label{eq:topk}
\mathcal{P}_{\mathrm{top\text{-}k}}
= \mathrm{Top}\text{-}k \Big(\{\mathbf{P}_t^{i,j} \mid \mathrm{Sim}(\mathbf{P}_t^{i,j}, \mathbf{P}_{t-1}^{i,j}) \ge \tau\}\Big).
\end{equation}
Thus, patches with minimal visual variation are identified for reuse, while those displaying notable differences are recomputed. This approach accurately selects truly static tokens across consecutive frames, significantly reducing redundant computations and accelerating inference without compromising overall performance.

\textbf{Evicting Task-Relevant Tokens.}
While many tokens remain visually static across consecutive frames, certain tokens that are highly relevant to the task (e.g., the gripper or the target object) cannot be directly reused. In our experiments, we observed that as the robotic arm approaches the target object, small but critical changes in the target region often occur. These changes are primarily caused by factors such as lighting variations or camera noise. Recomputing tokens around the target region before the gripper reaches it significantly improves task accuracy. To address this issue, we leverage the attention mechanism of the VLA model to identify task-relevant tokens in the next frame that require recalculation.

Specifically, we calculate the text-to-vision attention scores from the cross-attention module of the VLA model to determine the task relevance of each vision token. For each layer \( l \), the attention weights are represented as 
\(\mathbf{A}^l \in \mathbb{R}^{N_{\text{heads}} \times N_{\text{tokens}} \times N_{\text{tokens}}}\). 
From these weights, we extract the attention mapping from text tokens to vision tokens:
\begin{equation}
\label{eq:avistext}
\mathbf{A}^l_{\text{vis-text}} 
= \mathbf{A}^l[:,\, v_{\text{start}} : v_{\text{end}},\, t_{\text{start}} : t_{\text{end}}],
\end{equation}
where \( v_{\text{start}}, v_{\text{end}} \) and \( t_{\text{start}}, t_{\text{end}} \) are the indices of the vision and text tokens, respectively.

To aggregate the attention scores across multiple heads, we compute the mean attention for each vision token as $\mathbf{A}^l_{\text{avg}} 
= \mathrm{Mean}_{\text{heads}}\!\bigl(\mathbf{A}^l_{\text{vis-text}}\bigr)$. For task relevance across multiple layers \( \mathcal{L} \), the final task relevance scores are obtained by averaging the scores across the selected layers as $\mathbf{S}_{\text{task-relevance}} 
= \mathrm{Mean}_{l \in \mathcal{L}}\!\bigl(\mathbf{A}^l_{\text{avg}}\bigr)$.

Using these scores, we rank the vision tokens based on their task relevance and apply a threshold \( \tau_{\text{task}} \) to select the most task-relevant tokens:
\begin{equation}
\label{eq:ptask}
\mathcal{P}_{\text{task-relevant}} 
= \{ \mathbf{P}_t^{i,j} \mid \mathbf{S}_{\text{task-relevance}}[i,j] \geq \tau_{\text{task}} \}.
\end{equation}

Finally, we combine the set of static tokens \( \mathcal{P}_{\mathrm{static}} \) selected in the first step with the task-relevant tokens. Tokens that are both static and highly task-relevant are removed from the reusable token set to ensure they are recomputed in the current step:
\begin{equation}
\label{eq:pfinal}
\mathcal{P}_{\mathrm{final}} 
= \mathcal{P}_{\mathrm{static}} \;\setminus\; \mathcal{P}_{\text{task-relevant}}.
\end{equation}

By filtering out high-attention tokens, this approach ensures that the VLA model accurately updates the critical objects in the scene, thereby preventing failures caused by reusing outdated static tokens. This method effectively balances computational efficiency with task precision, enabling robust performance in robotic control tasks.

\subsection{Layer Adaptive Token Reusing}
While static token selection and task-relevance filtering eliminate a large portion of redundant computation, we observe that attention distributions within the VLA decoder vary significantly across different layers. This finding is consistent with observations reported by prior work \textit{FastV}~\cite{chen2025image}, indicating that both VLA and VLM decoders exhibit similar patterns of attention flow: early layers display dispersed attention, followed by fluctuations in intermediate layers, and eventually a partial rebound near the final layers.

To account for these differences, we propose a layer-adaptive strategy that adjusts the fraction of reused tokens based on each layer’s attention concentration. Specifically, we quantify the attention distribution at layer \(l\) via an \emph{entropy} measure, following the same mean-attention computation described in equation ~\ref{eq:avistext}. Let \(H^l\) denote the resulting entropy. We then define an \emph{entropy ratio} \(R^l\), which captures how much more concentrated the attention is in layer \(l\) compared to layer \(l-1\):
\begin{equation}
\label{eq:entropy_ratio}
R^l \;=\; \frac{H^{l-1} - H^l}{H^{l-1}}.
\end{equation}
A positive $R^l$ indicates that the attention distribution at layer $l$ is more focused than that of layer $l-1$. 

We accumulate these ratios across layers to obtain a cumulative score \(R_{\text{cum}}^l=\sum_{j=1}^{l} R^j\), which in turn determines the proportion \(\alpha^l\) of static tokens (from \(\mathcal{P}_{\mathrm{final}}\)) that are reused at layer \(l\). Formally,
\begin{equation}
\label{eq:reuse_ratio}
\alpha^l \;=\; \min\Bigl(k \sum_{j=1}^l R^j,\; 1\Bigr),
\end{equation}
where \(k\) is a hyperparameter that governs the impact of attention concentration. Layers with larger cumulative entropy reduction are allowed to reuse a higher fraction of tokens, reflecting the insight that as attention becomes more focused, fewer tokens are likely to require recomputation.

In practice, this layer-adaptive mechanism dynamically adjusts token reuse based on the evolving attention patterns in the VLA decoder, effectively balancing computational efficiency with task accuracy. By selectively retaining only the most relevant tokens at each layer, our method significantly reduces redundant computations while maintaining reliable action prediction. 


\begin{algorithm}[t]
\caption{Adaptive Token Caching}
\label{alg:vla_cache}
\begin{algorithmic}[1] % Added [1] to enable line numbers
\STATE \textbf{Input:} Frames $\{I_{t-1}, I_t\}$, previous KV cache $\{\mathbf{K}_{t-1}^l, \mathbf{V}_{t-1}^l\}$, thresholds $\tau, \tau_{\text{task}}$, hyperparameter $k$
\STATE \textbf{Output:} Updated KV cache $\{\mathbf{K}_t^l, \mathbf{V}_t^l\}$

\STATE \textbf{Static Token Selection:}
\STATE Patchify $I_{t-1}, I_t$ and compute $\text{Sim}(\mathbf{P}_t, \mathbf{P}_{t-1})$
\STATE Select $\mathcal{P}_{\text{static}}$ where similarity $\geq \tau$
\STATE Apply top-$k$ filtering to refine static token selection

\STATE \textbf{Evict Task-Relevant Tokens:}
\STATE Compute text-to-vision attention scores $\mathbf{S}_{\text{task-relevance}}$
\STATE Select $\mathcal{P}_{\text{task-relevant}}$ where attention $\geq \tau_{\text{task}}$
\STATE Compute reusable tokens: $\mathcal{P}_{\mathrm{final}} = \mathcal{P}_{\text{static}} \setminus \mathcal{P}_{\text{task-relevant}}$

\STATE \textbf{Layer-Adaptive Token Reuse:}
\STATE Compute entropy reduction $R^l$, $R_{\text{cum}}^l$; Determine reuse ratio $\alpha^l$; Compute reusable subset $\mathcal{P}_{\mathrm{reuse}} \subseteq \mathcal{P}_{\mathrm{final}}$

\FOR{each layer $l$ and token $i$} 
    \IF{$i \in \mathcal{P}_{\mathrm{reuse}}$}
        \STATE Reuse cached values:  \\ \quad\quad\quad $\mathbf{K}_t^l(i) = \mathbf{K}_{t-1}^l(I), 
         \mathbf{V}_t^l(i) = \mathbf{V}_{t-1}^l(i)$
    \ELSE
        \STATE Recompute: \\ \quad\quad\quad $\mathbf{K}_t^l(i) = W_K^l \mathbf{H}_t^l(i), \mathbf{V}_t^l(i) = W_V^l \mathbf{H}_t^l(i)$
    \ENDIF
    \STATE Compute self-attention: 
    \STATE $\mathrm{Attn}(\mathbf{Q}_t^l, \mathbf{K}_t^l, \mathbf{V}_t^l) = \mathrm{Softmax}\!\Bigl(\frac{\mathbf{Q}_t^l (\mathbf{K}_t^l)^\top}{\sqrt{D}}\Bigr) \mathbf{V}_t^l$
\ENDFOR

\STATE \textbf{return} Updated KV cache $\{\mathbf{K}_t^l, \mathbf{V}_t^l\}$

\end{algorithmic}
\end{algorithm}

\textbf{Inference with Cached Representation.}
During inference, VLA-Cache selectively caches and reuses previously computed representations to reduce redundant computations while maintaining accurate robotic action prediction. Instead of recomputing all tokens at each timestep, the model distinguishes between static and dynamic tokens. Static tokens, which exhibit minimal visual change across consecutive frames, retain their cached key-value (KV) representations from the previous timestep, bypassing unnecessary computation. Dynamic tokens, which undergo significant changes or are task-relevant, are recomputed to ensure precise motion execution. This selective token reuse is performed at each layer, guided by entropy-based attention analysis to dynamically adjust reuse ratios. The full inference procedure, including static token selection, task-relevance filtering, and layer-adaptive token reuse, is outlined in Algorithm~\ref{alg:vla_cache}, with further details provided in Appendix \ref{app:inference_detail}.


\subsection{Theoretical Analysis of Computational Complexity}

\noindent
\textbf{Computational Cost Reduction.} 
In standard VLA inference, each Transformer layer processes $L$ tokens, with a total FLOP cost per layer:
\begin{equation}
\label{eq:baseline_layer_flops}
\text{FLOPs} \approx 4 L D^2 + 2 L^2 D + 2 L D M.
\end{equation}
Our method reduces the effective token count per layer to $L_r = \alpha \times \mathcal{P}_{\mathrm{final}}$, leading to theoretical savings:
\begin{equation}
\label{eq:reuse_saving}
\Delta \text{FLOPs}_{\mathrm{layer}} \approx 4 L_r D^2 + 2 L_r^2 D + 2 L_r D M.
\end{equation}
These savings scale across multiple layers, significantly lowering computation while maintaining accuracy.

\noindent
\textbf{Overhead of Token Selection.} 
The cost of static token identification is approximately $\mathcal{O}(H^2)$ due to patch similarity checks, while task-relevance filtering introduces a cross-modal attention aggregation cost of $\mathcal{O}(L_t L_v D)$. The entropy-based layer-adaptive strategy incurs an additional $\mathcal{O}(L^2 D)$ complexity, which remains significantly lower than the baseline per-layer cost.

\noindent
\textbf{Total Complexity Reduction.} 
Bringing all components together, the theoretical overall FLOP reduction per layer is:
\begin{equation}
\label{eq:final_theoretical_gain}
\begin{aligned}
\Delta \text{FLOPs}_{\text{total}} 
\approx & \Bigl( 4 L_r D^2 + 2 L_r^2 D + 2 L_r D M \Bigr) \\
& - \Bigl( H^2 + L_t L_v D + L^2 D \Bigr).
\end{aligned}
\end{equation}


\smallskip
\noindent
\textbf{Further Derivations and Complexity Details.} 
See Appendix~\ref{appendix:complexity} for detailed derivations, including static token selection costs, attention filtering complexity, and layer-adaptive entropy calculations.

