\section{Appendix}


\subsection{Complexity Analysis Details}
\label{appendix:complexity}

\noindent
\textbf{Static Token Selection.} 
We compute patch-wise similarity for visual tokens:
\begin{equation}
\label{eq:patch_sim_flops}
\text{FLOPs}_{\text{static-sim}} = N_{\mathrm{patch}}^2 D_{\mathrm{patch}} \approx H^2.
\end{equation}
Since $N_{\mathrm{patch}} = H/p$, this cost remains small relative to Transformer computations.

\noindent
\textbf{Task-Relevance Filtering.} 
The attention-based filtering step computes cross-modal importance scores:
\begin{equation}
\label{eq:task_relevance_flops}
\text{FLOPs}_{\text{task-filter}} \approx L_t L_v D.
\end{equation}
A sorting operation of $\mathcal{O}(L \log L)$ follows for threshold selection.

\noindent
\textbf{Layer-Adaptive Entropy Computation.} 
The entropy-based reuse strategy involves:
\begin{equation}
\label{eq:entropy_cost}
\text{FLOPs}_{\text{entropy}} \approx L^2 D.
\end{equation}
The per-layer reuse ratio is then computed as:
\begin{equation}
\label{eq:reuse_ratio_append}
\alpha^l = \min\Bigl( k \sum_{j=1}^l R^j, 1 \Bigr).
\end{equation}
These overheads remain modest compared to the full forward pass cost, enabling efficient token reuse.

\noindent
\textbf{Final FLOP Reduction.} 
The total FLOP savings across all layers follow:
\begin{equation}
\label{eq:total_flop_savings}
\Delta \text{FLOPs}_{\text{total}} = \sum_{l=1}^{\Omega} \Delta \text{FLOPs}_{\text{layer}}.
\end{equation}
This confirms that dynamic token reuse significantly reduces computation without sacrificing model performance.

\subsection{VLA Cache Inference}
\label{app:inference_detail}
Having identified both static tokens and task-relevant tokens as described in Section~\ref{sec:dynamic_selection}, we now detail how these tokens are reused in the VLA decoder via KV caching. Below, we first present a high-level overview of the procedure, then introduce several formulas to clarify the partial update of key-value (KV) representations under our selective token reuse strategy.

\paragraph{Overall Procedure.}
At time step \(t-1\), while predicting the action for frame \(I_{t-1}\), the model produces the attention weights (used to evaluate task relevance) and the KV cache, \(\{\mathbf{K}_{t-1}^l,\mathbf{V}_{t-1}^l\}\) for each layer \(l\). When a new frame \(I_t\) arrives, we compare it with \(I_{t-1}\)~\eqref{eq:sim} to obtain a set of static tokens sorted by similarity scores, and further filter out any high task-relevance tokens~\eqref{eq:ptask}. Simultaneously, we determine a layer-specific reuse ratio \(\alpha^l\) ~\eqref{eq:reuse_ratio} based on the entropy of attention distributions at time \(t-1\). This yields:
\begin{equation}
\label{eq:final_token_set}
\mathcal{P}_{\mathrm{final}} \;\text{and}\; \{\alpha^l\}_{l=1}^{\Omega},
\end{equation}
where \(\mathcal{P}_{\mathrm{final}}\) denotes the final subset of tokens eligible for reuse, and \(\Omega\) is the total number of decoder layers. 

\paragraph{Selective Token Reuse.}
During decoding at time \(t\), each decoder layer \(l\) reuses only the top-\(k\) tokens (determined by \(\alpha^l\) and similarity ranking) from \(\mathcal{P}_{\mathrm{final}}\). Let \(\mathcal{P}_{\mathrm{reuse}}\subseteq \mathcal{P}_{\mathrm{final}}\) be this chosen set of reusable tokens, with positions (IDs) \(\{i\mid i\in \mathcal{P}_{\mathrm{reuse}}\}\). For tokens \emph{not} in \(\mathcal{P}_{\mathrm{reuse}}\), we recompute their key-value (KV) representations from scratch. Concretely, let \(\mathbf{H}_t^l\in\mathbb{R}^{L\times D}\) be the hidden states at layer \(l\) for the new frame \(I_t\). The per-layer self-attention requires three components: queries \(\mathbf{Q}_t^l\), keys \(\mathbf{K}_t^l\), and values \(\mathbf{V}_t^l\). We define:
\begin{equation}
\label{eq:q_formula}
\mathbf{Q}_t^l(i) \;=\; W_Q^l\,\mathbf{H}_t^l(i), 
\end{equation}
for all token indices \(i \in \{1,\ldots,L\}\), where \(W_Q^l\in\mathbb{R}^{D\times D}\) is the learnable projection of queries.

\noindent
For \(\mathbf{K}_t^l\) and \(\mathbf{V}_t^l\), we partially reuse cached values from time \(t-1\):
\begin{equation}
\label{eq:k_formula}
\mathbf{K}_t^l(i) \;=\; 
\begin{cases}
\mathbf{K}_{t-1}^l(i), & \text{if } i\in \mathcal{P}_{\mathrm{reuse}},\\
W_K^l\,\mathbf{H}_t^l(i), & \text{otherwise},
\end{cases}
\end{equation}
\begin{equation}
\label{eq:v_formula}
\mathbf{V}_t^l(i) \;=\; 
\begin{cases}
\mathbf{V}_{t-1}^l(i), & \text{if } i\in \mathcal{P}_{\mathrm{reuse}},\\
W_V^l\,\mathbf{H}_t^l(i), & \text{otherwise},
\end{cases}
\end{equation}
where \(W_K^l, W_V^l\in\mathbb{R}^{D\times D}\) are the key and value projection matrices, respectively, and \(\mathbf{K}_{t-1}^l(i), \mathbf{V}_{t-1}^l(i)\) are the cached key-value vectors from the previous time step. This partial-update scheme allows us to skip computation for those tokens deemed both static and non-task-critical.

Finally, the self-attention output at layer \(l\) is computed as:
\begin{equation}
\label{eq:attn_formula}
\mathrm{Attn}(\mathbf{Q}_t^l,\mathbf{K}_t^l,\mathbf{V}_t^l) \;=\; 
\mathrm{Softmax}\!\Bigl(\tfrac{\mathbf{Q}_t^l(\mathbf{K}_t^l)^\top}{\sqrt{D}}\Bigr)\;\mathbf{V}_t^l.
\end{equation}
Only the newly computed tokens update \(\mathbf{K}_t^l,\mathbf{V}_t^l\), while reused tokens retain their entries from \(\mathbf{K}_{t-1}^l,\mathbf{V}_{t-1}^l\). 

\paragraph{Implementation Details.}
Listing~\ref{lst:cache_inference} outlines our partial token reuse within the VLA decoder:
\begin{enumerate}
\label{lst:cache_inference}
    \item \textbf{Position IDs and Causal Masks.} We maintain a \emph{cache\_position} array reflecting the positions of tokens that require computation. Tokens not recomputed do not receive new position IDs, enabling a pruned causal mask aligned with the updated token set.
    \item \textbf{Computing Rotary Embeddings.} Once the reusable tokens are omitted from re-encoding, we apply a rotary embedding function to the reduced set of token embeddings to generate position-dependent features for the next layer.
    \item \textbf{KV Cache Updates.} Only newly computed tokens update their corresponding key-value entries in \(\{\mathbf{K}_t^l,\mathbf{V}_t^l\}\). Reused tokens keep their entries from \(\{\mathbf{K}_{t-1}^l,\mathbf{V}_{t-1}^l\}\). This partial-update mechanism leverages the permutation-invariance property of Transformers, ensuring consistent attention outputs even if certain tokens are omitted from recomputation.
\end{enumerate}

Since Transformers naturally support skipping previously processed tokens in a self-attention cache, our layer-adaptive token reuse is readily integrated with standard caching strategies. Notably, the most significant reduction in computation occurs when generating the first action token at time \(t\). After that, the autoregressive procedure continues as usual, incurring no additional overhead for subsequent tokens.

\paragraph{Efficiency and Effect on Performance.}
By leveraging selective token reuse, VLA-Cache decreases the total number of tokens that must be re-encoded and re-attended at each time step \(t\). This leads to a substantial reduction in decoding time, especially given the short sequences in robotic action prediction. Meanwhile, the complexity of predicting subsequent tokens in the same time step follows the usual KV-caching paradigm, adding minimal overhead. In practice, this method preserves task accuracy by \emph{forcing recomputation} of highly dynamic or task-critical tokens while reusing the remainder. We present a detailed evaluation in Section~\ref{sec:experiments}, demonstrating that VLA-Cache achieves notable speedups with negligible performance degradation.


\subsection{Finetuning Data for Real Robot Experiments}

\paragraph{Robot Setup.}
The setup of the Franka Robot is shown in Figure ~\ref{fig:real_robot}. In this example, a Kinova Jaco robot arm with 6 degrees of freedom is rigidly fixed to the frame. We use a Sony AX53 camera, which is placed opposite the robot arm. The camera is facing the operating table and transmits the video in real time.

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.6\linewidth]{fig/real_robot.png} 
    \caption{Kinova Jaco Robot Setup}
    \label{fig:real_robot}
\end{figure}

\paragraph{Data Collection.}
Our data collection work is based on the \textit{CLVR\_Jaco\_Play} dataset. We employed PyBullet as an inverse kinematics (IK) controller. The system receives incremental Cartesian displacement inputs ($\Delta x, \Delta y, \Delta z$) from an Xbox controller, which are processed to generate joint velocity commands. These commands are transmitted to the robotic arm\textquotesingle s velocity controller at a 10Hz control frequency for real-time execution. We record four types of observations: Third-person camera observations (\textbf{front\_cam\_ob}), End-effector Cartesian pose (\textbf{ee\_cartesian\_pos\_ob}), End-effector Cartesian velocity (\textbf{ee\_cartesian\_vel\_ob}), Jaco arm joint positions (\textbf{joint\_pos\_ob}).


\paragraph{Data Preprocessing. }
The preprocessing procedure was conducted as follows: Initially, the recorded frames underwent center cropping, reducing the resolution from 1280×720 to 912×720, followed by resizing to 224×224. Subsequently, episodes were manually selected based on a visual inspection of the video sequences generated from the recorded frames. To minimize potential biases associated with excessively long episodes, those exceeding 250 steps were excluded from the dataset. Furthermore, steps in which all recorded action values were zero were removed to ensure data relevance and integrity.

\subsection{Simulated Evaluation Detail.}

\paragraph{LIBERO Task Definitions.}
Similarly, we also utilize all task suites provided in LIBERO for our evaluations. The Robosuite-based robot setup includes the following tasks: 1) ``place bowl on plate with {spatial variation}'' (e.g., drawer positions), 2) ``pick {object}'' (e.g., ketchup, bowl, apple), 3) ``(open / close) {target} drawer; {action} {object}'' (e.g., ``open top drawer; place apple into drawer''), and 4) ``achieve {goal} using shared objects'' (e.g., rearranging spatial relationships or altering object states).

\paragraph{SIMPLER Task Definitions.}
We utilize all task variants provided in SIMPLER for our evaluations, which include the Google robot setup with the following tasks: 1) ``pick Coke can'', 2) ``move {obj1} near {obj2}'', 3) ``(open / close) (top / middle / bottom) drawer'', and 4) ``open top drawer; place apple into top drawer''. Evaluations for the Google robot setup are provided for both Visual Matching (VM) and Variant Aggregations (VA).

\paragraph{Implementation Details.} Simulated evaluations for CogACT and SIMPLER are conducted on a single NVIDIA RTX 4090 GPU in BF16 precision. During inference, we use DDIM sampling with 10 steps and a classifier-free guidance (CFG) coefficient of 1.5. Similarly, for OpenVLA and LIBERO, inference is performed on a single NVIDIA RTX 4090 GPU in BF16 precision.
\subsection{Additional Simulation Results}


\paragraph{Result of Subtask on LIBERO Spatial Task Suit.}
\input{tab/libero_spatial}
Table~\ref{tab:libero_spatial} presents detailed results on each subtask in the LIBERO-Spatial suite. We observe that VLA-Cache, along with methods like SparseVLM and FastV, occasionally surpasses the baseline’s success rate on individual subtasks. This suggests that certain redundant tokens may distract the baseline model, and pruning or reusing tokens can in fact enhance its robustness.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/simulate_test_traj.png}
    \caption{VLA-Cache test results and attention heat map in a simulated environment}
    \label{fig:simulate_test_traj}
\end{figure}

\paragraph{Visualization Results.}
Figure \ref{fig:simulate_test_traj} present evaluation examples of each tasks executed by OpenVLA with VLA-Cache in LIBERO.

\subsection{Real-World Evaluation Detail.}

\paragraph{Task Definitions.}

\begin{enumerate}
    \item \textbf{Pick Up Orange Pot} (single-instruction): The robot\textquotesingle s goal is to grasp the orange pot and lift it completely off the table. We collected 218 valid demonstrations for the training dataset, with slight random adjustments to the initial positions of the pot and the robotic arm in each episode. During evaluation, each trial is recorded as a success (1) or failure (0); there is no partial credit.
    \item \textbf{Place Blue Cube in Box} (single-instruction): The robot\textquotesingle s goal is to place the held blue cube into the target container. We collected 212 valid demonstrations for the training dataset, with randomized container positions and robotic arm initial configurations in each episode. During evaluation, each trial is recorded as a success (1) or failure (0); there is no partial credit.
    \item \textbf{Put Sausage in Blue Pan} (single-instruction): The robot\textquotesingle s goal is to stably place the held sausage into the blue pan. We collected 219 valid demonstrations for the training dataset, with randomized pan coordinates and robotic arm joint angles across trials. During evaluation, each trial is recorded as a success (1) or failure (0); there is no partial credit.
    \item \textbf{Wipe Table} (single-instruction): The robotic arm\textquotesingle s goal is to sweep scattered items into a fixed-position dustpan using a broom. For the training dataset, we collected 187 valid demonstrations featuring randomized placements of simulated items (e.g., fries/cheese) on the table and varied initial joint configurations of the robotic arm, while the dustpan location remained fixed. This task explicitly incorporates dynamic environmental variations to rigorously test generalization capabilities. During evaluation, each trial is recorded as a success (1) or failure (0); there is no partial credit.
\end{enumerate}

\paragraph{Visualization Results. }

Figure \ref{fig:real_test_traj} present evaluation examples of each tasks executed by OpenVLA with VLA-Cache
on the Kinova Jaco Robot Arm.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/real_test_traj.png}
    \caption{Comparison between baseline(OpenVLA) and VLA-Cache in real environment}
    \label{fig:real_test_traj}
\end{figure}