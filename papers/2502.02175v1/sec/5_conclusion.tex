\section{Conclusion} 
\label{sec:conclusion}
In this paper, we introduce \textbf{VLA-Cache}, a training-free method for VLA that selectively reuses static tokens while filtering out task-relevant ones, reducing redundant computation without sacrificing accuracy. Additionally, our layer-adaptive token reuse strategy improves model success rates by adjusting token reuse based on attention concentration. Extensive experiments on two VLA models, OpenVLA and CogAct, across two simulation environments, LIBERO and SIMPLER, demonstrate that VLA-Cache achieves a \textbf{1.7$\times$ speedup} while maintaining performance. 
