\section{Introduction}

% \blue{
Learning a robust and generalizable policy for robotic manipulation through policy learning has long been a challenging problem~\cite{kim2020domain}, with traditional reinforcement learning approaches~\cite{choi2024domain,bica2021invariant} often suffering from poor robustness and limited generalization. Recently, the rapid advancement of foundational Vision-Language Models (VLMs)~\cite{awadalla2023openflamingo,liu2024visual} has demonstrated remarkable capabilities in multimodal understanding and generalization. Leveraging large-scale real-world robotic datasets~\cite{o2024open,fang2024rh20t}, pioneering works~\cite{zitkovich2023rt,octo_2023,niu2024llarva,kim24openvla} have introduced Vision-Language-Action (VLA) models, which integrate vision and language modalities to directly generate robotic actions in an end-to-end manner. This emerging paradigm holds great promise for enhancing the adaptability and generalization of robotic control systems, but leaves a large computational demand.

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig1-2.pdf}
    \vskip -0.07in
    \caption{During the inference of the VLA model, static tokens of the input image remain largely consistent across steps. This consistency allows for caching the computations of these tokens from the previous step and improving the efficiency of inference.}
    \vskip -0.15in
    \label{fig:motivation}
\end{figure}

To mitigate the extensive cost of VLA models, existing works often adopt generic acceleration techniques, such as model lightweighting~\cite{wen2024tinyvla}, quantization~\cite{park2024quantization}, and early exit~\cite{DeeR-VLA}. However, these approaches fail to account for the unique characteristics of VLA models, leading to suboptimal efficiency gains and limited performance improvements.


In this paper, we argue that robotic manipulation, characterized by sequential action-scene interactions, inherently prioritizes visual inputs related to the robotic arm and the target object, while the background remains largely static and less critical. As illustrated in Figure \ref{fig:motivation}, the static tokens of the input image remain largely consistent across steps. In this context, the background tokens, which contribute significantly to computational overhead and repeat across sequential steps in the VLA process, are often redundant.

As a result, we propose \textbf{VLA-Cache}, a novel approach that identifies and reuses static tokens across sequential VLA steps. Specifically, static tokens in each step refer to visual tokens that exhibit minor changes compared to the previous step. Instead of recomputing these tokens, VLA-Cache directly substitutes them with their cached representations from the preceding step, significantly reducing redundant computations and improving inference efficiency. Nevertheless, we observe that model precision is highly sensitive to task-relevant tokens (\textit{i.e.}, those related to the robotic arm and the target object) in the ablation study, where even a slight positional shift can lead to a significant performance drop. To address this, we introduce a fine-grained selection scheme that filters out task-relevant tokens from the static token set, ensuring that these critical tokens continue to undergo full computational interactions, thereby preserving model accuracy while maintaining efficiency.


Additionally, based on the observation that attention distributions within the VLA decoder vary significantly across different layers, we propose a layer-adaptive strategy that adjusts the fraction of reused tokens
based on each layer’s attention concentration. We evaluate VLA-Cache on a diverse set of robotic manipulation tasks in the LIBERO benchmark, where it demonstrates over 1.7× acceleration compared to the baseline, with only slight drop on the success rate. Furthermore, VLA-Cache is deployed on the Kinova Jaco robot arm, showing that it can achieve practical acceleration in real-world scenarios.
