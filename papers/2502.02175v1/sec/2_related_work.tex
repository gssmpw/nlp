\section{Related Work}
\textbf{Vision-Language-Action Models.}
Large-scale vision-language models (VLMs) have significantly advanced multimodal learning by integrating image understanding and language reasoning~\cite{liu2024visual, bai2023qwen}. Extending these capabilities, Vision-Language-Action (VLA) models~\cite{zitkovich2023rt,li2023vision} incorporate an action modality, enabling end-to-end visuomotor control. These models typically adopt large VLM backbones~\cite{touvron2023llama} and fine-tune them on robot data~\cite{o2024open}, with approaches varying from discretizing actions as language-like tokens~\cite{kim24openvla, chi2023diffusion} to incorporating specialized diffusion policy heads~\cite{black2024pi_0}. Despite their effectiveness in tasks like object retrieval and assembly~\cite{huang2024rekep, zhao2023learning}, VLA models demand substantial computation, making real-time deployment challenging, particularly in resource-constrained environments.

\textbf{Acceleration for Vision-Language Models.}
Acceleration techniques for VLMs, such as quantization~\cite{10.1145/3664647.3680838} and pruning~\cite{lin2024mope}, have shown success in vision-language tasks but often overlook the real-time demands of action generation. Token-level methods like FastV~\cite{chen2025image} and SparseVLM~\cite{zhang2024sparsevlm} optimize inference by pruning or merging redundant tokens, but their effectiveness in VLA tasks, which require short and highly specialized action sequences, remains unclear. Some works address efficiency by modifying VLA architectures, such as DeeR-VLA~\cite{DeeR-VLA}, which dynamically adjusts inference depth, and QAIL~\cite{park2024quantization}, which integrates quantization-aware training. Others, like RoboMamba~\cite{liu2024robomamba} and TinyVLA~\cite{wen2024tinyvla}, focus on replacing traditional attention mechanisms or training compact models from scratch, often requiring re-training and additional data collection. In contrast, VLA-Cache offers a training-free acceleration approach by selectively caching static tokens and recomputing only dynamic or task-relevant ones, preserving crucial temporal cues while reducing inference latency and improving real-time robotic control efficiency.

