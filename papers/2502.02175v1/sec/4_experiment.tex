
\section{Experiment}
\label{sec:experiments}

\subsection{Experimental Settings}
To validate the generalization of VLA-Cache, we implement our method in both simulation and real-world settings. In simulation, we evaluate VLA-Cache on two open-source VLA models: OpenVLA~\cite{kim24openvla} and CogAct~\cite{li2024cogact}, using the LIBERO benchmark~\cite{liu2024libero} and SIMPLER environment~\cite{li2024evaluating}, respectively. For fair comparison, we adopt two token-pruning acceleration methods, SparseVLM~\cite{zhang2024sparsevlm} and FastV~\cite{chen2025image}, as baselines in the LIBERO environment. OpenVLA processes 256 visual tokens per image, and all comparative experiments use an identical token pruning and reuse count, setting Top-K to 100 tokens.

For VLA-Cache, the default parameters are set as follows: similarity score threshold $\tau=0.996$, top-$k = 100$, and task relevance threshold $\tau_{task}=0.5$. Additionally, we use OpenVLA’s default caching mechanism and other default parameters for inference acceleration. In the SIMPLER environment, we apply VLA-Cache with the same parameter settings as OpenVLA while using CogAct’s default hyperparameters.

For real-world evaluation, we fine-tune OpenVLA for deployment on a Kinova Jaco2 6-DoF manipulator. The model is trained for 50,000 steps with default parameters. During testing, we modify only the similarity score threshold to $\tau=0.85$, keeping all other parameters unchanged. All experiments are conducted on an NVIDIA RTX 4090 GPU.

\subsubsection{LIBERO Benchmark}
The LIBERO Benchmark~\cite{liu2024libero} is designed to evaluate lifelong robotic manipulation across four diverse task suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. Each suite introduces distinct variations in spatial layouts, object selection, task objectives, or long-horizon planning, challenging models to generalize across different manipulation scenarios.

Following the experimental setup of OpenVLA, we conduct evaluations on all four LIBERO suites, each containing ten subtasks. To ensure consistency, we use the same machine and official fine-tuned weights. For each subtask, multiple evaluation episodes are run, measuring key performance metrics such as success rate and inference latency. We evaluate VLA-Cache’s acceleration capabilities within the OpenVLA framework and conduct an ablation study on the top-$k$ parameter in the LIBERO environment.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/simulate_campare.png}
    \caption{Simulation Tasks on LIBERO Benchmark and the SIMPLER Environment.}
    \label{fig:simulate_campare}
\vskip -0.1in
\end{figure}
\begin{figure}[!t] 
    \centering
    \includegraphics[width=0.5\textwidth]{fig/real_tasks.png} 
    \caption{Robot Manipulation Tasks on Jaco2 Real-world Platform.}
    \label{fig:example}
\vskip -0.2in
\end{figure}

\begin{figure*}[!t]
\centering
{\includegraphics[width=2\columnwidth]{fig/simulate_demos.png}}
\vskip -0.1in
\caption{Heatmap Visualization of Robot Actions in a Simulated Environment with Prompt Descriptions and Outputs}
\end{figure*}
\input{tab/libero_main}
\input{tab/simpler_eval}


\subsubsection{SIMPLER Environments}
The SIMPLER simulation environment~\cite{li2024evaluating} is designed to closely mirror real-world robotic setups, facilitating more realistic evaluations.

SIMPLER supports two evaluation settings: \emph{Visual Matching} (aiming to minimize visual discrepancies between simulation and reality) and \emph{Variant Aggregation} (building on Visual Matching by introducing variations in lighting, background, table textures, etc.). We adopt the same SIMPLER configuration used by CogAct and evaluate two settings, \emph{Visual Matching} and \emph{Variant Aggregation}, on a Google robot arm. Each setting contains four tasks: 1) \emph{Pick coke can(PickCan)}, 2) \emph{Move near(MoveNear)}, 3) \emph{Open/close drawer(Drawer)}, 4) \emph{Open top drawer and place apple(DrawerApple)}.

To validate the general applicability of our method, we use CogAct as the baseline model within the SIMPLER environment. CogAct extends OpenVLA by incorporating a Diffusion Policy head to predict actions in continuous space, a common strategy among recent VLA models. Similar to OpenVLA, CogAct provides publicly available weights and code for reproducible simulation-based evaluations. Through these experiments, we try to verify its potential as a general VLA acceleration technique.


\subsubsection{Real-world Robot Setting}
We further evaluate our method on a Kinova Jaco2 6-DoF manipulator, equipped with a Sony AX53 camera positioned in front of the arm to capture the manipulation scene.

\textbf{Tasks.}
We design four real-world robotic manipulation tasks:1) \emph{Pick up the orange pot (PickPot)},2) \emph{Place the blue cube in the box (PlaceCube)}, 3) \emph{Put the sausage in the blue pan (PutSausage)},4) \emph{Wipe the table (WipeTable)}.
For the wiping task, we place objects of various shapes and colors on the table to challenge the model's robustness to unseen objects.

\textbf{Data Collection.}
Using an Xbox Gamepad, we teleoperate the Jaco2 arm and collect demonstrations at a frequency of 10\,Hz, recording RGB images and robot states throughout each trajectory. We gather 150--200 demonstrations per task to form the training dataset.

\textbf{Baseline.}
We fine-tune OpenVLA via LoRA (Low-Rank Adaptation~\cite{hu2021lora}) on each task and then deploy it on the Jaco2 to assess real-world performance. The fine-tuning process follows the same hyperparameter setup as in LIBERO.

\subsection{Results on Simulation Environment}

\input{tab/real_robot}
\input{tab/method_ablation}
\input{tab/libero_ablation}

\paragraph{Main Results on LIBERO.}
Table~\ref{tab:libero_main} summarizes the success rate, FLOPs, and CUDA inference time across the four LIBERO task suites. Compared to the baseline, VLA-Cache achieves a \textbf{27.31\%} FLOPs reduction and a \textbf{1.63$\times$} speedup, with only a minimal 0.3\% drop in the overall success rate, confirming its efficiency without compromising accuracy. While VLA-Cache slightly underperforms on spatial, object, and long-horizon tasks, it notably surpasses the baseline on the goal suite, highlighting its robust adaptability.

Notably, both FastV and SparseVLM fail to improve inference time, often exceeding the baseline. FastV merely masks tokens in attention computation without reducing GPU workload, introducing additional masking overhead, while SparseVLM incurs extra cost from token pruning, merging, and recycling. These methods were originally designed for long-sequence VLM tasks, where efficiency gains are more pronounced, but struggle in VLA settings, where action sequences are typically short. In contrast, VLA-Cache leverages inter-frame token reuse with negligible overhead, making it effective for robotic action generation.


\textbf{Ablation on Reusing/Pruning Rate.}  
 Table~\ref{tab:libero_ablation} presents a comparative analysis of different methods through an ablation study on the top-$k$ parameter, while also varying the number of pruned or reused tokens. For all methods, increasing the number of reused/pruned tokens leads to a decline in success rates, highlighting the importance of retaining sufficient information for accurate action predictions. 
Our approach maintains a relatively stable success rate at a reuse rate of 40\%, but a more noticeable drop occurs when reusing 200 tokens. In contrast, FastV removes a significant portion of tokens entirely, and its performance drop primarily results from the loss of critical visual details rather than potential errors in token reuse. Additionally, SparseVLM and FastV incur longer inference times due to extra attention operations during GPU execution, whereas VLA-Cache directly updates KV cache entries, making inference more efficient.

\textbf{Token Selection Strategies} As presented in Table~\ref{tab:selection_ablation}, we evaluate different token selection strategies for VLA-Cache. The direct reuse of static tokens results in a significant decline in success rate. In contrast, filtering task-relevant tokens maintains a success rate of 82.6\%. Notably, our layer-adaptive approach further enhances the success rate to 83.8\%, albeit with a marginal increase in computational overhead from 1.304 FLOPs to 1.382 FLOPs. This overhead arises because fewer tokens are reused in the early layers, which ensures the retention of critical updates in deeper layers, thereby improving overall performance.


\paragraph{Main Results on SIMPLER}
Table~\ref{tab:simpler_eval} indicates that the VLA-Cache exhibits comparable success rates than the baseline CogACT in the SIMPLER environment while substantially reducing computational overhead. Specifically, VLA-Cache achieves an average success rate of 74.4\% in the standard Visual Matching setting compared to 74.8\% for CogACT and 62.3\% in the Variant Aggregation setting versus 61.3\% for CogACT. 

The efficiency gains are evident in the FLOPs and inference time measurements. VLA-Cache achieves roughly 20\% fewer FLOPs than the baseline, coupled with a 1.37× reduction in inference latency. Notably, these results highlight the portability of VLA-Cache across different action heads, establishing it as a general acceleration strategy for VLA. 

\subsection{Results on Real Robot}
Table~\ref{tab:real_robot} illustrates the performance of VLA-Cache in real-world robotic tasks. Among the four tasks, \emph{PickPot} shows a slightly lower success rate than the baseline, whereas VLA-Cache exceeds the baseline on the other three tasks. The method also achieves considerable reductions in FLOPs and inference time. Overall, VLA-Cache improves the average success rate by 2.4\%, an outcome that, although unexpected, is potentially explained by a more robust baseline model, which we trained extensively using additional data and more training steps than the LIBERO fine-tuning. With improved robustness as a foundation, VLA-Cache’s ability to prune or reuse redundant tokens may further enhance the model’s resilience, thus yielding higher success rates.


