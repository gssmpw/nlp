





\section{Pre-training: Learning Agile Humanoid Skills}
\label{sec:deepmimic}

\subsection{Data Generation: Retargeting Human Video Data}
To track expressive and agile motions, we collect a video dataset of human movements and retarget it to robot motions, creating imitation goals for motion-tracking policies, as shown in \Cref{fig:data_processing} and \Cref{fig:ASAP} (a).

\paragraph{Transforming Human Video to SMPL Motions}

We begin by recording videos (see \Cref{fig:data_processing} (a) and \Cref{fig:action_noise}) of humans performing expressive and agile motions. Using TRAM~\cite{wang2025tram}, we reconstruct 3D motions from videos. TRAM estimates the global trajectory of the human motions in SMPL parameter format~\cite{loper2023smpl}, which includes global root translation, orientation, body poses, and shape parameters, as shown in \Cref{fig:data_processing} (b). The resulting motions are denoted as ${\mathcal{D}}_{\text{SMPL}}$. 





\paragraph{Simulation-based Data Cleaning}

Since the reconstruction process can introduce noise and errors~\cite{he2024learning}, some estimated motions may not be physically feasible, making them unsuitable for motion tracking in the real world. To address this, we employ a ``sim-to-data'' cleaning procedure. Specifically, we use MaskedMimic~\cite{tessler2024maskedmimic}, a physics-based motion tracker, to imitate the SMPL motions from TRAM in IsaacGym simulator~\cite{makoviychuk2021isaac}. The motions (\Cref{fig:data_processing} (c)) that pass this simulation-based validation are saved as the cleaned dataset ${\mathcal{D}}_{\text{SMPL}}^{\text{Cleaned}}$.

\paragraph{Retargeting SMPL Motions to Robot Motions}

With the cleaned dataset ${\mathcal{D}}_{\text{SMPL}}^{\text{Cleaned}}$ in SMPL format, we retarget the motions into robot motions following the shape-and-motion two-stage retargeting process~\cite{he2024learning}. Since the SMPL parameters estimated by TRAM represent various human body shapes, we first optimize the shape parameter $\boldsymbol{\beta}^{\prime}$ to approximate a humanoid shape. By selecting 12 body links with correspondences between humans and humanoids, we perform gradient descent on $\boldsymbol{\beta}^{\prime}$ to minimize joint distances in the rest pose. Using the optimized shape $\boldsymbol{\beta}^{\prime}$ along with the original translation $\boldsymbol{p}$ and pose $\boldsymbol{\theta}$, we apply gradient descent to further minimize the distances of the body links. This process ensures accurate motion retargeting and produces the cleaned robot trajectory dataset ${\mathcal{D}}_{\text{Robot}}^{\text{Cleaned}}$, as shown in \Cref{fig:data_processing} (d). 
% \guanya{no dynamics or RL to get ? Do we need a figure to show a-b-c?}

\subsection{Phase-based Motion Tracking Policy Training}

We formulate the motion-tracking problem as a goal-conditioned reinforcement learning (RL) task, where the policy $\pi$ is trained to track the retargeted robot movement trajectories in the dataset ${\mathcal{D}}_{\text{Robot}}^{\text{Cleaned}}$. Inspired by ~\cite{peng2018deepmimic}, the state $s_t$ includes the robot’s proprioception $s_t^{\mathrm{p}}$ and a time phase variable $\phi \in [0,1]$, where $\phi=0$ represents the start of a motion and $\phi=1$ represents the end. This time phase variable alone $\phi$ is proven to be sufficient to serve as the goal state $\boldsymbol{s}_t^{\mathrm{g}}$ for single-motion tracking~\cite{peng2018deepmimic}. 
The proprioception $s_t^{\mathrm{p}}$ is defined as $s_t^{\mathrm{p}} \triangleq \left[\dofposhist, \dofvelhist,  \rootangvelhist, \gravityhist, \actionhist \right]$, with 5-step history of joint position $\dofpos\in\mathbb{R}^{23}$, joint velocity $\dofvel\in\mathbb{R}^{23}$, root angular velocity $\rootangvel\in\mathbb{R}^3$, root projected gravity $\gravity\in\mathbb{R}^3$, and last action $\actionprev\in\mathbb{R}^{23}$.
Using the agent’s proprioception $s_t^{\mathrm{p}}$ and the goal state $\boldsymbol{s}_t^{\mathrm{g}}$, we define the reward as  $r_t=\mathcal{R}\left(s_t^{\mathrm{p}}, s_t^{\mathrm{g}}\right)$, which is used for policy optimization.  The specific reward terms can be found in~\Cref{tab:deepmimic_reward}. The action $\boldsymbol{a}_t \in \mathbb{R}^{23}$ corresponds to the target joint positions and is passed to a PD controller that actuates the robot’s degrees of freedom. To optimize the policy, we use the proximal policy optimization (PPO)~\cite{schulman2017proximal}, aiming to maximize the cumulative discounted reward $\mathbb{E}\left[\sum_{t=1}^T \gamma^{t-1} r_t\right]$. We identify several design choices that are crucial for achieving stable policy training:

\paragraph{Asymmetric Actor-Critic Training}

Real-world humanoid control is inherently a partially observable Markov decision process (POMDP), where certain task-relevant properties that are readily available in simulation become unobservable in real-world scenarios. However, these missing properties can significantly facilitate policy training in simulation. To bridge this gap, we employ an asymmetric actor-critic framework, where the critic network has access to privileged information such as the global positions of the reference motion and the root linear velocity, while the actor network relies solely on proprioceptive inputs and a time-phase variable. This design not only enhances phase-based motion tracking during training but also enables a simple, phase-driven motion goal for sim-to-real transfer. Crucially, because the actor does not depend on position-based motion targets, our approach eliminates the need for odometry during real-world deployment—overcoming a well-documented challenge in prior work on humanoid robots~\cite{he2024learning,he2024omnih2o}.

% \TODO{\tairan{Check what else is known for critic}}
\paragraph{Termination Curriculum of Tracking Tolerance}
Training a policy to track agile motions in simulation is challenging, as certain motions can be too difficult for the policy to learn effectively. For instance, when imitating a jumping motion, the policy often fails early in training and learns to remain on the ground to avoid landing penalties. To mitigate this issue, we introduce a termination curriculum that progressively refines the motion error tolerance throughout training, guiding the policy toward improved tracking performance. Initially, we set a generous termination threshold of 1.5m, meaning the episode terminates if the robot deviates from the reference motion by this margin. As training progresses, we gradually tighten this threshold to 0.3m, incrementally increasing the tracking demand on the policy. This curriculum allows the policy to first develop basic balancing skills before progressively enforcing stricter motion tracking, ultimately enabling successful execution of high-dynamic behaviors.

% \guanya{a bit more details? termination condition curriculum}
% Specifically, we: \TODO{\tairan{Describe the details of the termination curriculum}}

\paragraph{Reference State Initialization}
Task initialization plays a crucial role in RL training. We find that naively initializing episodes at the start of the reference motion leads to policy failure. For example, in Cristiano Ronaldo's jumping training, starting the episode from the beginning forces the policy to learn sequentially. However, a successful backflip requires mastering the landing first—if the policy cannot land correctly, it will struggle to complete the full motion from takeoff. To address this, we adopt the Reference State Initialization (RSI) framework~\cite{peng2018deepmimic}. Specifically, we randomly sample time-phase variables between 0 and 1, which effectively randomizes the starting point of the reference motion for the policy to track. We then initialize the robot’s state based on the corresponding reference motion at that phase, including root position and orientation, root linear and angular velocities and joint positions and velocities. This initialization strategy significantly improves motion tracking training, particularly for agile whole-body motions, by allowing the policy to learn different motion phases in parallel rather than being constrained to a strictly sequential learning process.

\paragraph{Reward Terms}
We define the reward function $r_t$ with the sum of three terms: 1) penalty, 2) regularization, and 3) task rewards. A detailed summary of these components is provided in \Cref{tab:deepmimic_reward}.
\input{tables/deepmicic_reward}
\paragraph{Domain Randomizations}
To improve the robustness of the pre-trained policy in \Cref{fig:ASAP} (a), we utilized basic domain randomization techniques listed in \Cref{tab:deepmimic_DR}.
% \input{tables/deepmimic_DR}