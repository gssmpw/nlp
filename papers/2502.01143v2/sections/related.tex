\section{Related Works}
\label{sec:relatedwork}

\subsection{Learning-based Methods for Humanoid Control}

In recent years, learning-based methods have made significant progress in whole-body control for humanoid robots. Primarily leveraging reinforcement learning algorithms~\cite{schulman2017proximal} within physics simulators~\cite{makoviychuk2021isaac, mittal2023orbit, todorov2012mujoco}, humanoid robots have learned a wide range of skills, including robust locomotion~\cite{li2019using, xie2020learning, li2021reinforcement, liao2024berkeley, li2024reinforcement, radosavovic2024real, radosavovic2402humanoid, gu2024advancing, zhang2024whole}, jumping~\cite{li2023robust}, and parkour~\cite{long2024learning, zhuang2024humanoid}. More advanced capabilities, such as dancing~\cite{zhang2024wococo, ji2024exbody2, cheng2024expressive}, loco-manipulation~\cite{he2024learning, lu2024mobile, fu2024humanplus, he2024omnih2o}, and even backflipping~\cite{Unitree2024H1Backflip}, have also been demonstrated. Meanwhile, the humanoid character animation community has achieved highly expressive and agile whole-body motions in physics-based simulations~\cite{peng2022ase, tessler2024maskedmimic, luo2023perpetual}, including cartwheels~\cite{peng2018deepmimic}, backflips~\cite{peng2018sfv}, sports movements~\cite{yuan2023learning, wang2024strategy, luo2024smplolympics, wang2023physhoi, wang2024skillmimic}, and smooth object interactions~\cite{tessler2024maskedmimic, gao2024coohoi, hassan2023synthesizing}. However, transferring these highly dynamic and agile skills to real-world humanoid robots remains challenging due to the dynamics mismatch between simulation and real-world physics. 
To address this challenge, our work focuses on learning and compensating for this dynamics mismatch, enabling humanoid robots to perform expressive and agile whole-body skills in the real world. 

\subsection{Offline and Online System Identification for Robotics}

The dynamics mismatch between simulators and real-world physics can be attributed to two primary factors: inaccuracies in the robot model descriptions and the presence of complex real-world dynamics that are difficult for physics-based simulators to capture. Traditional approaches address these issues using system identification (SysID) methods~\cite{kozin1986system, aastrom1971system}, which calibrate the robot model or simulator based on real-world performance. These methods can be broadly categorized into \textit{offline SysID} and \textit{online SysID}, depending on whether system identification occurs at test time. \textit{Offline SysID} methods typically collect real-world data and adjust simulation parameters to train policies in more accurate dynamics. The calibration process may focus on modeling actuator dynamics~\cite{tan2018sim, hwangbo2019learning, yang2024agile}, refining robot dynamics models~\cite{khosla1985parameter, an1985estimation, gautier2011dynamic, han2020iterative, janot2013generic}, explicitly identifying critical simulation parameters~\cite{yu2019sim, chebotar2019closing, du2021auto,wu2024loopsr}, learning a distribution over simulation parameters~\cite{ramos2019bayessim, heiden2022probabilistic, antonova2022bayesian}, or optimizing system parameters to maximize policy performance~\cite{muratore2021data, ren2023adaptsim}. \textit{Online SysID} methods, in contrast, aim to learn a representation of the robotâ€™s state or environment properties, enabling real-time adaptation to different conditions. These representations can be learned using optimization-based approaches~\cite{yu2018policy, yu2020learning, lee2022pi, peng2020learning}, regression-based methods~\cite{yu2017preparing, kumar2021rma, wang2024cts, gu2024advancing, ji2022concurrent, margolis2023learning, fu2023deep, qi2023hand, margolis2024rapid, kumar2022adapting, miki2022learning, lee2020learning}, next-state reconstruction techniques~\cite{nahrendra2023dreamwaq, long2024hybrid, luo2024pie, wang2024toward, shirwatkar2024pip}, direct reward maximization~\cite{li2024reinforcement}, or by leveraging tracking and prediction errors for online adaptation~\cite{o2022neural, lyu2024rl2ac, huang2023datt, gao2024neural}.
Our framework takes a different approach from traditional SysID methods by learning a residual action model that directly compensates for dynamics mismatch through corrective actions, rather than explicitly estimating system parameters.

\subsection{Residual Learning for Robotics}

Learning a residual component alongside learned or pre-defined base models has been widely used in robotics. Prior work has explored residual policy models that refine the actions of an initial controller~\cite{silver2018residual, johannink2019residual, carvalho2022residual, alakuijala2021residual, davchev2022residual, haldar2023teach, ankile2024imitation, jiang2024transic, lee2020learning}. Other approaches leverage residual components to correct inaccuracies in dynamics models~\cite{o2022neural,karnan2020reinforced,kloss2022combining,shi2021neural,he2024self} or to model residual trajectories resulting from residual actions~\cite{chi2024iterative} for achieving precise and agile motions. RGAT~\cite{karnan2020reinforced} uses a residual action model with a learned forward dynamics to refine the simulator. Our framework builds on this idea by using RL-based residual actions to align the dynamics mismatch between simulation and real-world physics, enabling agile whole-body humanoid skills.


