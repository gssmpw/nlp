\section{Post-training: Training Delta Action Model and Fine-tuning Motion Tracking Policy}
% \zi{The subtitle is a little bit strange. Should be Post-training \textbf{AND} Fine-tuning? Or just: ASAP: Training Delta Dynamics and Fine-tuning Motion Tracking Policy}
The policy trained in the first stage can track the reference motion in the real-world but does not achieve high motion quality. Thus, during the second stage, as shown in ~\Cref{fig:ASAP}~(b) and (c), we leverage real-world data rolled out by the pre-trained policy to train a delta action model, followed by policy refinement through dynamics compensation using this learned delta action model.

\subsection{Data Collection}
We deploy the pretrained policy in the real world to perform whole-body motion tracking tasks (as depicted in~\Cref{fig:data-collect}) and record the resulting trajectories, denoted as $\mathcal{D}^\text{r} = \{s^\text{r}_0, a^\text{r}_0, \dots, s^\text{r}_T, a^\text{r}_T\}$, as illustrated in~\Cref{fig:ASAP}~(a). At each timestep $t$, we use a motion capture device and onboard sensors to record the state: 
$
s_t = [p^\text{base}_t, v_t^\text{base}, \alpha^\text{base}_t, \omega^\text{base}_t, q_t, \dot{q}_t],
$
where $p^\text{base}_t \in \mathbb{R}^3$ represents the robot base 3D position, $v_t^\text{base} \in \mathbb{R}^3$ is base linear velocity, $\alpha^\text{base}_t \in \mathbb{R}^4$ is the robot base orientation represented as a quaternion, $\omega^\text{base}_t \in \mathbb{R}^3$ is the base angular velocity, $q_t \in \mathbb{R}^{23}$ is the vector of joint positions, and $\dot{q}_t \in \mathbb{R}^{23}$ represents joint velocities.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/baselines-crop.pdf}
    \vspace{-1mm}
    \caption{Baselines of \method. (a) Model-free RL training. (b) System ID from real to sim using real-world data. (c) Learning delta dynamics model using real-world data. (d) Our proposed method, learning delta action model using real-world data. }
    \label{fig:baselines}
    \vspace{-4mm}
\end{figure*}

\subsection{Training Delta Action Model}
\label{sec:train-delta-action-model}

Due to the sim-to-real gap, when we replay the real-world trajectories in simulation, the resulting simulated trajectory will likely deviate significantly from real-world recorded trajectories. This discrepancy is a valuable learning signal for learning the mismatch between simulation and real-world physics. We leverage an RL-based delta/residual action model to compensate for the sim-to-real physics gap.

As illustrated in~\Cref{fig:ASAP} (b), the delta action model is defined as $\Delta a_t = \pi^\Delta_\theta(s_t, a_t)$, where the policy $\pi^\Delta_\theta$ learns to output corrective actions based on the current state $s_t$ and the action $a_t$. These corrective actions ($\Delta a_t$) are added to the real-world recorded actions ($a^r_t$) to account for discrepancies between simulation and real-world dynamics.

The RL environment incorporates this delta action model by modifying the simulator dynamics as follows: $ s_{t+1} = f^\text{sim}(s_t, a^r_t + \Delta a_t)$ where $f^\text{sim}$ represents the simulator's dynamics, $a^r_t$ is the reference action recorded from real-world rollouts, and $\Delta a_t$ introduces corrections learned by the delta action model. 




\input{tables/deltaA_reward}
During each RL step: 
\begin{enumerate}
    \item The robot is initialized at the real-world state $s^r_t$.
    \item  A reward signal is computed to minimize the discrepancy between the simulated state $s_{t+1}$ and the recorded real-world state $s^r_{t+1}$, with an additional action magnitude regularization term $\exp(-\lVert a_t \rVert) - 1)$, as specified in~\Cref{tab:deltaA_FT_reward}. The workflow is illustrated in \Cref{fig:ASAP}~(b).
    \item PPO is used to train the delta action policy $\pi^\Delta_\theta$, learning corrected $\Delta a_t$ to match simulation and the real world.
\end{enumerate}



By learning the delta action model, the simulator can accurately reproduce real-world failures. For example, consider a scenario where the simulated robot can jump because its motor strength is overestimated, but the real-world robot cannot jump due to weaker motors. The delta action model $\pi^\Delta_\theta$ will learn to reduce the intensity of lower-body actions, simulating the motor limitations of the real-world robot. This allows the simulator to replicate the real-world dynamics and enables the policy to be fine-tuned to handle these limitations effectively.

\subsection{Fine-tuning Motion Tracking Policy under New Dynamics}
With the learned delta action model $\pi^\Delta (s_t, a_t)$, we can reconstruct the simulation environment with 
$$
s_{t+1} = f^{\text{\method}}(s_t, a_t) = f^\text{sim}(s_t, a_t + \pi^\Delta(s_t, a_t)),
$$
As shown in~\Cref{fig:ASAP} (c), we keep the $\pi^\Delta$ model parameters frozen, and fine-tune the pretrained policy with the same reward summarized in~\Cref{tab:deepmimic_reward}. 






\subsection{Policy Deployment}
Finally, we deploy the fine-tuned policy without delta action model in the real world as shown in \Cref{fig:ASAP}~(d). The fine-tuned policy shows enhanced real-world motion tracking performance compared to the pre-trained policy. Quantitative improvements will be discussed in \Cref{sec:EXP1}.