% \subsection{Reward Terms in Pre-Training}
% \label{sec:reward-pre-train}
% We define the reward function $r_t$ with the sum of three terms: 1) penalty, 2) regularization, and 3) task rewards. A detailed summary of these components is provided in Table~\ref{tab:deepmimic_reward}.
% \input{tables/deepmicic_reward}

\subsection{Domain Randomization in Pre-Training}
\label{sec:dr-pre-train}
To improve the robustness and generalization of the pre-trained policy in \Cref{fig:ASAP} (a), we utilized the domain randomization technics listed in \Cref{tab:deepmimic_DR}.
\input{tables/deepmimic_DR}

% \subsection{Reward Terms in DeltaA Training}
% For the delta action model training, it has similar reward terms to the pre-training stage, as shown in~\Cref{tab:deltaA_FT_reward}. The key difference is the regularization terms discussed in~\Cref{sec:train-delta-action-model}.
% \input{tables/deltaA_reward}

\subsection{SysID Parameters}
We identify the following representative robot parameters in our simulated model that best align the ones in the real world: base center of mass (CoM) shift $(c_x, c_y, c_z)$, base link mass offset ratio $k_m$ and low-level PD gain ratios $(k^i_p, k^i_d)$ where $i=1,2,...,23$, as shown in~\Cref{tab:sysid_params}.
\input{tables/sysid_params}.


\subsection{Implementation of Delta Dynamics Learning}
\label{sec:appendix-dynamics}
Using the collected real-world trajectory, we replay the action sequence $\{a^\text{real}_0, \dots, a^\text{real}_T\}$ in simulation and record the resulting trajectory $\{s^\text{sim}_0, \dots, s^\text{sim}_T\}$. The neural dynamics model $f^\Delta_\theta$ is trained to predict the difference: 
\[
s^\text{real}_{t+1} - s^\text{sim}_{t+1} = f^\Delta_\theta(s^\text{real}_t, a^\text{real}_t), \quad \forall t.
\]

In practice, we compute the mean squared error (MSE) loss in an autoregressive setting, where the model predicts forward for $K$ steps and uses gradient descent to minimize the loss. To balance learning efficiency and stability over long horizons, we implement a schedule that gradually increases $K$ during training. Formally, the optimization objective is:
\[
\mathcal{L}= \bigg\lVert s^\text{real}_{t+K} - \underbrace{f^\text{sim} \big( \dots f^\text{sim}}_{K} (s_t, a_t) + f^\Delta_\theta(s_t, a_t), \dots, a_{t+K} \big) \bigg\rVert.
\]

After training, we freeze the residual dynamics model $f^\Delta_\theta$ and integrate it into the simulator. During each simulation step, the robot's state is updated by incorporating the delta predicted by the dynamics model. In this augmented simulation environment, we finetune the previously pretrained policy to adapt to the corrected dynamics, ensuring improved alignment with real-world behavior.

\subsection{Derivation of Training-free Methods of Using Delta Action}
\label{sec:appendix_more_deltaA_usage}
To formalize the problem, we start by assuming one-step consistency between real and simulated dynamics:
\[
f^\text{real}(s, \pi(s)) = f^\text{sim}(s, \pi(s) + \pi^\Delta(s, \pi(s))).
\]
Under this assumption, one-step matching leads to the condition:
\begin{align}
    \pi(s) &+ \pi^\Delta(s, \pi(s)) = \hat{\pi}(s), \\
    \Rightarrow \pi(s) &= \hat{\pi}(s) - \pi^\Delta(s, \pi(s)).
    \label{eq:solve_pi}
\end{align}

To solve \Cref{eq:solve_pi}, we consider:
\begin{enumerate}
    \item \textbf{Fixed-Point Iteration}: We initialize \(y_0 = \hat{\pi}(s)\) and iteratively update:
   \begin{equation}
       y_{k+1} = \hat{\pi}(s) - \pi^\Delta(s, y_k),
       \label{eq:fix_point}
   \end{equation}
   where \(y_k\) converges to a solution after \(K\) iterations.

    \item \textbf{Gradient-Based Optimization}: Define the loss function:
    \begin{equation}
       l(y) = \| y + \pi^\Delta(s, y) - \hat{\pi}(s) \|^2.
    \end{equation}
    A gradient descent method minimizes this loss to solve for \(y\).
\end{enumerate}

These methods approximate \(\pi(s)\), but suffer from OOD issues when trained on limited trajectories. RL fine-tuning, in contrast, directly optimizes \(\pi(s)\) for real-world deployment, resulting in superior performance.

\textbf{Problem of One-Step Matching.} Note that \Cref{eq:solve_pi} is derived from the one-step matching assumption (i.e., $\pi(s) + \pi^\Delta(s, \pi(s)) = \hat{\pi}(s)$). For multi-step matching, one has to differentiate through $f^\text{sim}$, which is, in general, intractable. Therefore, both fixed-point iteration and gradient-based optimization assume one-step matching. This also explains the advantages of RL-based fine-tuning: it effectively performs a gradient-free multi-step matching procedure.
