\section{EXTENSIVE STUDIES AND ANALYSES}

In this section, we aim to thoroughly analyze \method by addressing three central research questions:
\begin{itemize}
    \item \textbf{Q4}: How to best train the delta action model of \method?
    \item \textbf{Q5}: How to best use the delta action model of \method?
    \item \textbf{Q6}: Why and how does \method work?
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/exp_ablate_deltaA.pdf}
     \vspace{-4mm}
    \caption{Analysis of dataset size, training horizon, and action norm on the performance of $\pi^\Delta$. (a) \textbf{Dataset Size}: Mean Per Joint Position Error (MPJPE) is evaluated for both in-distribution (green) and out-of-distribution (blue) scenarios. Increasing dataset size leads to enhanced generalization, evidenced by decreasing errors in out-of-distribution evaluations. Closed-loop MPJPE (red bars) also shows improvement with larger datasets. (b) \textbf{Training Horizon}: Open-loop MPJPE (heatmap) improves across evaluation points as training horizons increase, achieving the lowest error at 1.5s. However, closed-loop MPJPE (red bars) shows a sweet spot at a training horizon of 1.0s, beyond which no further improvements are observed. The red dashed line represents the pretrained baseline without $\pi^\Delta$ fine-tuning. (c) \textbf{Action Norm}: The action norm weight significantly influences performance. Both open-loop and closed-loop MPJPE decrease as the weight increases up to 0.1, achieving the lowest error. However, further increases in the action norm weight result in degradation of open-loop performance, highlighting the trade-off between action smoothness and policy flexibility.}
    \label{fig:deltaA_ablation}
\end{figure*}

\subsection{Key Factors in Training Delta Action Models}
\label{sec:VA}
To Answer \textbf{Q4} (\textit{How to best train the delta action model of \method}). 
we conduct a systematic study on key factors influencing the performance of the delta action model. 
Specifically, we investigate the impact of dataset size, training horizon, and action norm weight, evaluating their effects on both open-loop and closed-loop performance. Our analysis uncovers the essential principles for effectively training a high-performing delta action model.

\paragraph{Dataset Size} We analyze the impact of dataset size on the training and generalization of $\pi^\Delta$. Simulation data is collected in Isaac Sim, and $\pi^\Delta$ is trained in Isaac Gym. Open-loop performance is assessed on both in-distribution (training) and out-of-distribution (unseen) trajectories, while closed-loop performance is evaluated using the fine-tuned policy in Isaac Sim. As shown in~\Cref{fig:deltaA_ablation}~(a), increasing the dataset size improves $\pi^\Delta$â€™s generalization, evidenced by reduced errors in out-of-distribution evaluations. However, the improvement in closed-loop performance saturates, with a marginal decrease of only $0.65\%$ when scaling from $4300$ to $43000$ samples, suggesting limited additional benefit from larger datasets.

\paragraph{Training Horizon} The rollout horizon plays a crucial role in learning $\pi^\Delta$. As shown in~\Cref{fig:deltaA_ablation}~(b), longer training horizons generally improve open-loop performance, with a horizon of 1.5s achieving the lowest errors across evaluation points at 0.25s, 0.5s, and 1.0s. However, this trend does not consistently extend to closed-loop performance. The best closed-loop results are observed at a training horizon of 1.0s, indicating that excessively long horizons do not provide additional benefits for fine-tuned policy.

\paragraph{Action Norm Weight} Training $\pi^\Delta$ incorporates an action norm reward to balance dynamics alignment and minimal correction. As illustrated in~\Cref{fig:deltaA_ablation}~(c), both open-loop and closed-loop errors decrease as the action norm weight increases, reaching the lowest error at a weight of $0.1$. However, further increasing the action norm weight causes open-loop errors to rise, likely due to the minimal action norm reward dominates in the delta action RL training. This highlights the importance of carefully tuning the action norm weight to achieve optimal performance.



\subsection{Different Usage of Delta Action Model}
To answer \textbf{Q5} \textit(How to best use the delta action model of \method?), we compare multiple strategies: fixed-point iteration, gradient-based optimization, and reinforcement learning (RL). Given a learned delta policy \(\pi^\Delta\) such that:
\[
f^\text{sim}(s, a + \pi^\Delta(s, a)) \approx f^\text{real}(s, a),
\]
and a nominal policy \(\hat{\pi}(s)\) that performs well in simulation, the goal is to fine-tune \(\hat{\pi}(s)\) for real-world deployment.

A simple approach is one-step dynamics matching, which leads to the relationship:
\[
\pi(s) = \hat{\pi}(s) - \pi^\Delta(s, \pi(s)).
\]
We consider two RL-free methods: fixed-point iteration and gradient-based optimization. Fixed-point iteration refines \(\hat\pi(s)\) iteratively, while gradient-based optimization minimizes a loss function to achieve a better estimate. These methods are compared against RL fine-tuning, which adapts \(\hat\pi(s)\) using reinforcement learning in simulation. The detailed derivation of these two baselines is summarized in \Cref{sec:appendix_more_deltaA_usage}.

Our experiments in \Cref{fig: use_deltaA} show that RL fine-tuning achieves the lowest tracking error during deployment, outperforming training-free methods. 
Both RL-free approaches are myopic and suffer from out-of-distribution issues, limiting their real-world applicability (more discussions in \Cref{sec:appendix_more_deltaA_usage}). 




\begin{figure}[htp]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/exp_use_deltaA.pdf}
    \vspace{-2mm}
    \caption{MPJPE comparison over timesteps for fine-tuning methods using delta actionmodel. RL Fine-Tuning achieves the lowest error, while Fixed-Point Iteration and Gradient Search perform worse than the baseline (Before DeltaA) showing the highest error.}
    \label{fig: use_deltaA}
    \vspace{-4mm}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/exp_ablate_noise.pdf}
    \vspace{-2mm}
    \caption{MPJPE vs. Noise Level for policies fine-tuned with random action noise. Policies with noise levels $\beta \in [0.025, 0.2]$ show improved performance compared to no fine-tuning. Delta action achieves better tracking precision (126 MPJPE) compared to the best action noise (173 MPJPE).}
    \label{fig:action_noise}
    \vspace{-4mm}
\end{figure}




\subsection{Does \method Fine-Tuning Outperform Random Action Noise Fine-Tuning?}
To answer \textbf{Q6} (\textit{How does \method work?}), we validate \method finetuning is better than injecting random-action-noise-based finetuning. And we visualize the average magnitude of the delta action model for each joint.

Random torque noise~\cite{rfi} is a widely used domain randomization technique for legged robots. To determine whether delta action facilitates fine-tuning of pre-trained policies toward real-world dynamics rather than merely enhancing robustness through random action noise, we analyze its impact. Specifically, we assess the effect of applying random action noise during policy fine-tuning in Isaac Gym by modifying the environment dynamics as $s_{t+1} = f^\text{sim}(s_t, a_t + \beta \delta_a)$, where $\delta_a \sim \mathcal{U}[0, 1]$, and deploy it in Genesis. We conduct an ablation study to examine the influence of the noise magnitude, $\beta$, varying from $0.025$ to $0.4$. As shown in~\Cref{fig:action_noise}, within the constrained range of $\beta \in [0.025, 0.2]$, policies fine-tuned with action noise outperform those without fine-tuning in terms of global tracking error (MPJPE). However, the performance of the action noise approach (MPJPE of $150$) does not match the precision achieved by \method (MPJPE of $126$). Furthermore, we visualize the average output of $\pi^\Delta$ learned from IsaacSim data in~\Cref{fig:vis_deltaA_magnitude}, which reveals non-uniform discrepancies across joints. For example, in the G1 humanoid robot under our experimental setup, lower-body motors exhibit a larger dynamics gap compared to upper-body joints. Within the lower-body, the ankle and knee joints show the most pronounced discrepancies. Additionally, asymmetries between the left and right body motors further highlight the complexity. Such structured discrepancies cannot be effectively captured by merely adding uniform action noise.
These findings, along with the results in~\Cref{fig:ASAP_openloop_curves}, demonstrate that delta action not only enhances policy robustness but also enables effective adaptation to real-world dynamics, outperforming naive randomization strategies.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/vis_magnitude.pdf}
    \vspace{-2mm}
    \caption{Visualization of IsaacGym-to-IsaacSim $\pi^\Delta$ output magnitude. We compute the average absolute value of each joint over the 4300-episode dataset. Larger red dots indicate higher values. The results suggest that lower-body motors exhibit a larger discrepancy compared to upper-body joints, with the most significant gap observed in the ankle pitch joint of the G1 humanoid.}
    \label{fig:vis_deltaA_magnitude}
    \vspace{-4mm}
\end{figure}
