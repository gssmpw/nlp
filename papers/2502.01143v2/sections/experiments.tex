
\section{Performance Evaluation of \method}
\label{sec:EXP1}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/ASAP-OpenLoop-Curves-crop.pdf}
    \vspace{-6mm}
    \caption{Replaying IsaacSim State-Action trajecories in IsaacGym. The upper four panels visualize the Unitree G1 humanoid executing a soccer-shooting motion under four distinct open-loop actions. Corresponding metric curves (bottom) quantify tracking performance. Importantly, our delta action model (ASAP) is trained across multiple motions and is not overfitted to this specific example.}
    \label{fig:ASAP_openloop_curves}
    \vspace{-3mm}
\end{figure*}

In this section, we present extensive experimental results on three policy transfers: IsaacGym~\cite{makoviychuk2021isaac} to IsaacSim~\cite{mittal2023orbit}, IsaacGym to Genesis~\cite{Genesis}, and IsaacGym to real-world Unitree G1 humanoid robot. Our experiments aim to address the following key questions:
\begin{itemize}
    \item \textbf{Q1}: Can \method outperform other baseline methods to compensate for the dynamics mismatch? 
    \item \textbf{Q2}: Can \method finetune policy to outperform SysID and Delta Dynamics methods? 
    \item \textbf{Q3}: Does \method work for sim-to-real transfer?
\end{itemize}

\textbf{Experiments Setup.}
To address these questions, we evaluate \method on motion tracking tasks in both simulation (\Cref{sec:sim-open-loop} and \Cref{sec:sim-close-loop}) and real-world settings (\Cref{sec:real-exp}).

In the simulation, we use the retargeted motion dataset from the videos we shoot, denoted as ${\mathcal{D}}_{\text{Robot}}^{\text{Cleaned}}$, which contains diverse human motion sequences. We select 43 motions categorized into three difficulty levels: easy, medium, and hard (as partially visualized in~\Cref{fig:demo_task_difficulty}), based on motion complexity and the required agility. \method is evaluated through simulation-to-simulation transfer by training policies in IsaacGym and using two other simulators, IsaacSim and Genesis, as a proxy of ``real-world'' environments. This setup allows for a systematic evaluation of \method's generalization and transferability. The success of the transfer is assessed by metrics described in subsequent sections.

For real-world evaluation, we deploy \method on Unitree G1 robot with fixed wrists to track motion sequences that has obvious sim-to-real gap. These sequences are chosen to capture a broad range of motor capabilities and demonstrate the sim-to-real capability for agile whole-body control.




\textbf{Baselines.} We have the following baselines:



\texttt{Oracle}: This baseline is trained and evaluated entirely within IsaacGym. It assumes perfect alignment between the training and testing environments, serving as an upper bound for performance in simulation.

\texttt{Vanilla} (\Cref{fig:baselines} a): The RL policy is trained in IsaacGym and evaluated in IsaacSim, Genesis, or the real world.

\texttt{SysID} (\Cref{fig:baselines} b): 
We identify the following representative parameters in our simulated model that best align the ones in the real world: base center of mass (CoM) shift $(c_x, c_y, c_z)$, base link mass offset ratio $k_m$ and low-level PD gain ratios $(k^i_p, k^i_d)$ where $i=1,2,...,23$. Specifically, we search the best parameters among certain discrete ranges by replaying the recorded trajectories in real with different simulation parameters summarized in \Cref{tab:sysid_params}. We then finetune the pre-trained policy in IsaacGym with the best SysID parameters.



\texttt{DeltaDynamics} (\Cref{fig:baselines} c): We train a residual dynamics model $f^\Delta_{\theta}(s_t, a_t)$ to capture the discrepancy between simulated and real-world physics. The detailed implementation is introduced in~\Cref{sec:appendix-dynamics}




\textbf{Metrics.}
We report success rate, deeming imitation unsuccessful when, at any point during imitation, the average difference in body distance is on average further than 0.5m.
% We calculate tracking error regarding kinematic pose. 
We evaluate policy’s ability to imitate the reference motion by comparing the tracking error of the global body position $E_\text{g-mpjpe}$ (mm), the root-relative mean per-joint (MPJPE) $E_{\text{mpjpe}}$ (mm), acceleration error $E_\text{acc}$ ($\text{mm/frame}^2$), and root velocity $E_\text{vel}$ (mm/frame). The mean values of the metrics are computed across all motion sequences used.



\input{tables/openloop_metrics}
\input{tables/main_result}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/demon_task_difficulty.pdf}
    \vspace{-6mm}
    \caption{Visual comparisons of motion imitation results across different difficulty levels (Easy, Medium, Hard) for various tasks including Jump Forward, Side Jump, Single Foot Balance, Squat, Step Backward, Step Forward, and Walk.}
    \vspace{-2mm}
    % \guanya{this is a placeholder?}}
    \label{fig:demo_task_difficulty}
\end{figure*}


\subsection{Comparison of Dynamics Matching Capability}
\label{sec:sim-open-loop}

To address \textbf{Q1} (\textit{Can \method outperform other baseline methods to compensate for the dynamics mismatch?}), we establish sim-to-sim transfer benchmarks to assess the effectiveness of different methods in bridging the dynamics gap. IsaacGym serves as the \textit{training environment}, while IsaacSim and Genesis function as \textit{testing environments}. The primary objective is to evaluate the generalization capability of each approach when exposed to new dynamics conditions.
\textit{Open-loop} evaluation measures how accurately a method can reproduce testing-environment trajectories in the training environment. This is achieved by rolling out the same trajectory executed in the testing environment and assessing tracking discrepancies using key metrics such as MPJPE. An ideal method should minimize the discrepancies between training and testing trajectories when replaying testing-environment actions, thereby demonstrating an improved capacity for compensating dynamics mismatch. Quantitative results in~\Cref{tab:open-loop} demonstrate that \method consistently outperforms the OpenLoop baseline across all replayed motion lengths, achieving lower $E_\text{g-mpjpe}$ and $E_\text{mpjpe}$ values, which indicate improved alignment with testing-env trajectories. 
While SysID helps address short-horizon dynamics gaps, it struggles with long-horizon scenarios due to cumulative error buildup. DeltaDynamics improves upon both SysID and OpenLoop for long horizons but suffers from overfitting, as evidenced by cascading errors magnified over time, as shown in~\Cref{fig:ASAP_openloop_curves}. 
\method, however, demonstrates superior generalization by learning residual policies that effectively bridge the dynamics gap. Comparable trends are observed in the Genesis simulator, where \method achieves notable improvements across all metrics relative to the baseline. These results emphasize the efficacy of learning delta action model to reduce the physics gap and improve open-loop replay performance.







\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/ASAP-Finetune-crop.pdf}    \caption{Visualization of G1 motion tracking before and after \method fine-tuning in IsaacGym, IsaacSim and Genesis. Top: LeBron James’ “Silencer” motion tracking policy fine-tuning for IsaacGym to IsaacSim. Bottom: \textit{single foot balance} motion tracking policy fine-tuning for IsaacGym to Genesis.}
    \vspace{-2mm}
    \label{fig:ASAP_sim_close_loop}
\end{figure*}




\subsection{Comparison of Policy Fine-Tuning Performance}
\label{sec:sim-close-loop}
To address \textbf{Q2} (\textit{Can \method finetune policy to outperform SysID and Delta Dynamics methods?}), we evaluate the effectiveness of different methods in fine-tuning RL policies for improved testing-environment performance. 
We fine-tune RL policies in modified training environments and subsequently deploy them in the testing environments, quantifying motion-tracking errors in testing environments.
As shown in~\Cref{tab:closed-loop}, \method consistently outperforms baselines such as Vanilla, SysID, and DeltaDynamics across all difficulty levels (Easy, Medium, and Hard) in both simulators (IsaacSim and Genesis). For the Easy level, our method achieves the lowest $E_\text{g-mpjpe}$ and $E_\text{mpjpe}$ in IsaacSim ($E_\text{g-mpjpe} = 106$ and $E_\text{mpjpe} = 44.3$) and Genesis ($E_\text{g-mpjpe} = 125$ and $E_\text{mpjpe} = 73.5$), with minimal acceleration ($\text{E}_{\text{acc}}$) and velocity ($\text{E}_{\text{vel}}$) errors. In more challenging tasks, such as the Hard level, our method continues to excel, significantly reducing motion-tracking errors. For instance, in Genesis, it achieves $E_\text{g-mpjpe} = 129$ and $E_\text{mpjpe} = 77.0$, outperforming SysID and DeltaDynamics by substantial margins. Additionally, our method consistently maintains a 100\% success rate across both simulators, unlike DeltaDynamics, which experiences lower success rates in harder environments. 
To further illustrate the advantages of \method, we provide per-step visualizations in \Cref{fig:ASAP_sim_close_loop}, comparing \method with RL policies deployed without fine-tuning. These visualizations demonstrate that \method successfully adapts to new dynamics and maintains stable tracking performance, whereas baseline methods accumulate errors over time, leading to degraded tracking capability.
These results highlight the robustness and adaptability of our approach in addressing the sim-to-real gap while preventing overfitting and exploitation. The findings validate that \method is an effective paradigm for improving closed-loop performance and ensuring reliable deployment in complex real-world scenarios.




\subsection{Real-World Evaluations}
\label{sec:real-exp}
To answer \textbf{Q3} (\textit{Does \method work for sim-to-real transfer?}). We validate \method on real-world Unitree G1 robot.

\textbf{Real-World Data.}
\label{subsec:real-world-data}In the real-world experiments, we prioritize both motion safety and representativeness by selecting five motion-tracking tasks, including (i)~\textit{kick}, (ii)~\textit{jump forward}, (iii)~\textit{step forward and back}, (iv)~\textit{single foot balance} and (v)~\textit{single foot jump}. 
However, collecting over 400 real-world motion clips— the minimum required to train the full 23-DoF delta action model in simulation, as discussed in\Cref{sec:train-delta-action-model}—poses significant challenges. Our experiments involve highly dynamic motions that cause rapid overheating of joint motors, leading to hardware failures (two Unitree G1 robots broke during data collection). 
Given these constraints, we adopt a more sample-efficient approach by focusing exclusively on learning a 4-DoF ankle delta action model rather than the full-body 23-DoF model. This decision is motivated by two key factors: (1) the limited availability of real-world data makes training the full 23-DoF delta action model infeasible, and (2) the Unitree G1 robot~\cite{Unitree2024G1} features a mechanical linkage design in the ankle, which introduces a significant sim-to-real gap that is difficult to bridge with conventional modeling techniques~\cite{KimJLALS20}.
Under this setting, the original 23 DoF delta action model reduces to 4 DoF delta action model, which needs much less data to be trainable. In practice, we collect 100 motion clips, which prove sufficient to train an effective 4-DoF delta action model for real-world scenarios.

We execute the tracking policy 30 times for each task. In addition to these motion-tracking tasks, we also collect 10 minutes of locomotion data. The locomotion policy will be addressed in the next section, which is also utilized to bridge different tracking policies.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{fig/ASAP-RW-DeltaA-crop.pdf}
    \caption{Visualization of LeBron James’ ``Silencer'' motion on the G1 robot before (upper figure enclosed in \textcolor{blue}{blue}) and after (bottom figure enclosed in \textcolor{red}{red}) \method policy finetuning. The left half shows the policy finetuning for the in-distribution motions while the right half shows the out-of-distribution ones. After \method finetuning, the robot behaves more smoothly and reduces jerky lower-body motions.}
    \label{fig:ASAP_real_compare}

\end{figure*}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/ASAP-MoreReal-crop.pdf}
     \vspace{-3mm}
    \caption{We deploy the pretrained policy of a forward jump motion tracking task, challenging the 1.35m-tall Unitree G1 robot for a forward leap over 1m.}
    \label{fig:data-collect}
    \vspace{-4mm}
\end{figure}

\textbf{Policy Transition.}
In the real world, we cannot easily reset the robot as in simulators, and therefore we train a robust locomotion policy for the policy transition between different motion-tracking tasks. Our locomotion command contains $(v, \omega, \Pi)$, where $v$ and $\omega$ indicate the linear and angular velocities while $\Pi$ indicates the command to walk or stand still. After each motion-tracking task is done, our locomotion policy will take over to keep the robot balance until the next motion-tracking task begins. In this way, the robot is able to execute multiple tasks without manually resetting. 


\textbf{Real-World Results.}
% Compared to prior gaps that existed across different simulators, the \simtoreal gap is relatively more apparent due to noisy sensor input, inaccurate robot modeling, and actuator differences. 
The \simtoreal gap is more pronounced than simulator-to-simulator discrepancies due to factors such as noisy sensor input, inaccuracies in robot modeling, and actuator differences.
To evaluate the effectiveness of \method in addressing these gaps, we compare the closed-loop performance of \method with the \textit{Vanilla} baseline on two representative motion tracking tasks  (kicking and ``Silencer'') in which observe obvious \simtoreal gaps. 
To show the generalizability of the learned delta action model for out-of-distribution motions, we also fine-tune the policy for LeBron James’ ``Silencer'' motion as shown in~\Cref{fig:firstpage} and~\Cref{fig:ASAP_real_compare}. The experiment data is summarized in \Cref{tab:real-exp-close-loop}. It shows that \method outperforms the baseline on both in-distribution and out-of-distribution humanoid motion tracking tasks, achieving a considerable reduction of the tracking errors across all key metrics ($E_\text{g-mpjpe}, E_\text{mpjpe}, E_{acc}$ and $E_{vel}$). These findings highlight the effectiveness of \method in improving \simtoreal transfer for agile humanoid motion tracking.
% \guanya{should we discuss closed-loop results? closed-looop only. also need to at least mention figure 1...}

\input{tables/real_world_result}