\section{Related Work}
% \subsection{Jailbreak attacks against LLMs}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/main3.pdf}
    \caption{Overview of our jailbreak attack method.
    \Rmnum{1}: Use a seed prompt to guide the LLM in generating an instruction-following prefix $\mathbf{x}$. \Rmnum{2}: Embed $\mathbf{x}$ into the MLM's hidden state using the embedding layer $f_e$ and the hidden layer $f_h$. \Rmnum{3}: Calculate the logistic distribution \textbf{$\Theta$} of the hidden state after adding the perturbation $\delta$ through the decoding head $\mathcal{H}$. Optimize $\delta$ using the decode loss $\mathcal{L}_d$ and the jailbreak loss $\mathcal{L}_j$ to balance the attack performance and readability of the jailbreak prefix.}
    \label{fig:main}
\end{figure*}

Jailbreak attacks can be categorized into black-box ____ and white-box attacks ____ based on the degree of access to the internal parameters of the target LLM. Inspired by social engineering principles, black-box attacks primarily create complex malicious queries by manually crafting high-quality jailbreak prompt templates. Additionally, some studies  ____ employ persona customization and persuasion techniques to generate jailbreak prompts, leading LLMs to produce harmful responses. 

Jailbroken____ points out that existing jailbreak methods, such as prompt injection and refusal suppression, arise from the conflict between LLMs' instruction-following abilities and safety goals. 
Building on this, some white-box attacks optimize jailbreak prompts at the token level, forcing the LLM to affirmatively respond to harmful queries and produce unsafe outputs. GCG ____ uses greedy and gradient-based search techniques to optimize randomly initialized jailbreak suffixes, prompting the LLM to affirm harmful requests. However, the jailbreak prompts generated by this method lack readability. In contrast, AutoDAN  ____ employs a genetic algorithm to refine manually crafted jailbreak prompts. Although this method generates prompts with lower PPL, it depends on human-written templates and external models (e.g., GPT-4) for mutation, limiting scalability. BEAST  ____ and COLD-Attack  ____ utilize beam search and gradient-based methods, respectively, to optimize the top $k$ tokens generated by the victim LLM. Since the initial jailbreak suffix is merely a simple concatenation of the LLMâ€™s output tokens, the generated jailbreak prompts, while theoretically having low PPL, still lack sufficient semantic readability. 

To better assess the safety vulnerabilities of open-source models in worst-case scenarios, we propose an efficient token-level jailbreak attack algorithm under the white-box setting. Unlike black-box attack algorithms, our method does not require manually crafted prompt templates or external large models for auxiliary computation. Compared to white-box attack methods like GCG, our approach achieves a higher success rate in jailbreak attacks while maintaining strong semantic readability.