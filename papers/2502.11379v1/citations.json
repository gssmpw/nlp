[
  {
    "index": 0,
    "papers": [
      {
        "key": "shen2023anything",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models"
      },
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      },
      {
        "key": "mehrotra2023tree",
        "author": "Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin",
        "title": "Tree of attacks: Jailbreaking black-box llms automatically"
      },
      {
        "key": "greshake2023not",
        "author": "Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario",
        "title": "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      },
      {
        "key": "zhu2023autodan",
        "author": "Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong",
        "title": "Autodan: Automatic and interpretable adversarial attacks on large language models"
      },
      {
        "key": "sadasivan2024fast",
        "author": "Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil",
        "title": "Fast Adversarial Attacks on Language Models In One GPU Minute"
      },
      {
        "key": "guo2024cold",
        "author": "Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin",
        "title": "Cold-attack: Jailbreaking llms with stealthiness and controllability"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "shah2023scalable",
        "author": "Shah, Rusheb and Pour, Soroush and Tagade, Arush and Casper, Stephen and Rando, Javier and others",
        "title": "Scalable and transferable black-box jailbreaks for language models via persona modulation"
      },
      {
        "key": "zeng2024johnny",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan",
        "title": "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhu2023autodan",
        "author": "Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong",
        "title": "Autodan: Automatic and interpretable adversarial attacks on large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sadasivan2024fast",
        "author": "Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil",
        "title": "Fast Adversarial Attacks on Language Models In One GPU Minute"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "guo2024cold",
        "author": "Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin",
        "title": "Cold-attack: Jailbreaking llms with stealthiness and controllability"
      }
    ]
  }
]