% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{Yang2023ShadowAT,
  title={Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models},
  author={Xianjun Yang and Xiao Wang and Qi Zhang and Linda R. Petzold and William Yang Wang and Xun Zhao and Dahua Lin},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.02949},
  url={https://api.semanticscholar.org/CorpusID:263620436}
}

@article{Ding2023AWI,
  title={A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Peng Ding and Jun Kuang and Dan Ma and Xuezhi Cao and Yunsen Xian and Jiajun Chen and Shujian Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.08268},
  url={https://api.semanticscholar.org/CorpusID:265664913}
}

@article{Yuan2023GPT4IT,
  title={GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher},
  author={Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-Tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.06463},
  url={https://api.semanticscholar.org/CorpusID:260887189}
}

@inproceedings{zeng-etal-2024-johnny,
    title = "How Johnny Can Persuade {LLM}s to Jailbreak Them: Rethinking Persuasion to Challenge {AI} Safety by Humanizing {LLM}s",
    author = "Zeng, Yi  and
      Lin, Hongpeng  and
      Zhang, Jingwen  and
      Yang, Diyi  and
      Jia, Ruoxi  and
      Shi, Weiyan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.773/",
    doi = "10.18653/v1/2024.acl-long.773",
    pages = "14322--14350",
}

@article{Chao2023JailbreakingBB,
  title={Jailbreaking Black Box Large Language Models in Twenty Queries},
  author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.08419},
  url={https://api.semanticscholar.org/CorpusID:263908890}
}

@article{yu2023gptfuzzer,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{contributors2023opencompass,
  title={Opencompass: A universal evaluation platform for foundation models},
  author={Contributors, OpenCompass},
  journal={GitHub repository},
  year={2023}
}

@article{kao2020bert,
  title={BERT's output layer recognizes all hidden layers? Some Intriguing Phenomena and a simple way to boost BERT},
  author={Kao, Wei-Tsung and Wu, Tsung-Han and Chi, Po-Han and Hsieh, Chun-Cheng and Lee, Hung-Yi},
  journal={arXiv preprint arXiv:2001.09309},
  year={2020}
}

@article{meyer2023chatgpt,
    author = {Meyer, Jesse G and Urbanowicz, Ryan J and Martin, Patrick CN and O’Connor, Karen and Li, Ruowang and Peng, Pei-Chen and Bright, Tiffani J and Tatonetti, Nicholas and Won, Kyoung Jae and Gonzalez-Hernandez, Graciela and others},
    journal = {BioData Mining},
    number = {1},
    pages = {20},
    publisher = {Springer},
    title = {ChatGPT and large language models in academia: opportunities and challenges},
    volume = {16},
    year = {2023}
}


@article{mcduff2023towards,
    author = {McDuff, Daniel and Schaekermann, Mike and Tu, Tao and Palepu, Anil and Wang, Amy and Garrison, Jake and Singhal, Karan and Sharma, Yash and Azizi, Shekoofeh and Kulkarni, Kavita and others},
    journal = {ArXiv preprint},
    title = {Towards accurate differential diagnosis with large language models},
    url = {https://arxiv.org/abs/2312.00164},
    volume = {abs/2312.00164},
    year = {2023}
}



@article{wu2023bloomberggpt,
    author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
    journal = {ArXiv preprint},
    title = {Bloomberggpt: A large language model for finance},
    url = {https://arxiv.org/abs/2303.17564},
    volume = {abs/2303.17564},
    year = {2023}
}


@inproceedings{jang2016categorical,
    author = {Eric Jang and
Shixiang Gu and
Ben Poole},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
    booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
    publisher = {OpenReview.net},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Categorical Reparameterization with Gumbel-Softmax},
    url = {https://openreview.net/forum?id=rkE3y85ee},
    year = {2017}
}



@article{mehrotra2023tree,
    author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
    journal = {ArXiv preprint},
    title = {Tree of attacks: Jailbreaking black-box llms automatically},
    url = {https://arxiv.org/abs/2312.02119},
    volume = {abs/2312.02119},
    year = {2023}
}


@article{zeng2024johnny,
    author = {Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
    journal = {ArXiv preprint},
    title = {How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
    url = {https://arxiv.org/abs/2401.06373},
    volume = {abs/2401.06373},
    year = {2024}
}



@article{shah2023scalable,
    author = {Shah, Rusheb and Pour, Soroush and Tagade, Arush and Casper, Stephen and Rando, Javier and others},
    journal = {ArXiv preprint},
    title = {Scalable and transferable black-box jailbreaks for language models via persona modulation},
    url = {https://arxiv.org/abs/2311.03348},
    volume = {abs/2311.03348},
    year = {2023}
}



@article{ouyang2022training,
    author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
    journal = {Advances in neural information processing systems},
    pages = {27730--27744},
    title = {Training language models to follow instructions with human feedback},
    volume = {35},
    year = {2022}
}


@article{wang2022self,
    author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
    journal = {ArXiv preprint},
    title = {Self-consistency improves chain of thought reasoning in language models},
    url = {https://arxiv.org/abs/2203.11171},
    volume = {abs/2203.11171},
    year = {2022}
}


@inproceedings{welbl2021challenges,
    address = {Punta Cana, Dominican Republic},
    author = {Welbl, Johannes  and
Glaese, Amelia  and
Uesato, Jonathan  and
Dathathri, Sumanth  and
Mellor, John  and
Hendricks, Lisa Anne  and
Anderson, Kirsty  and
Kohli, Pushmeet  and
Coppin, Ben  and
Huang, Po-Sen},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
    doi = {10.18653/v1/2021.findings-emnlp.210},
    pages = {2447--2469},
    publisher = {Association for Computational Linguistics},
    title = {Challenges in Detoxifying Language Models},
    url = {https://aclanthology.org/2021.findings-emnlp.210},
    year = {2021}
}



@article{wu2021recursively,
    author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
    journal = {ArXiv preprint},
    title = {Recursively summarizing books with human feedback},
    url = {https://arxiv.org/abs/2109.10862},
    volume = {abs/2109.10862},
    year = {2021}
}


@inproceedings{greshake2023not,
    author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
    booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
    pages = {79--90},
    title = {Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
    year = {2023}
}



@article{zhu2023autodan,
  title={Autodan: Automatic and interpretable adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  journal={arXiv preprint arXiv:2310.15140},
  year={2023}
}


@inproceedings{qi2024visual,
    author = {Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Henderson, Peter and Wang, Mengdi and Mittal, Prateek},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    number = {19},
    pages = {21527--21536},
    title = {Visual adversarial examples jailbreak aligned large language models},
    volume = {38},
    year = {2024}
}


@article{liu2023jailbreaking,
    author = {Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang},
    journal = {ArXiv preprint},
    title = {Jailbreaking chatgpt via prompt engineering: An empirical study},
    url = {https://arxiv.org/abs/2305.13860},
    volume = {abs/2305.13860},
    year = {2023}
}



@article{shen2023anything,
    author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
    journal = {ArXiv preprint},
    title = {" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
    url = {https://arxiv.org/abs/2308.03825},
    volume = {abs/2308.03825},
    year = {2023}
}


@article{hazell2023large,
    author = {Hazell, Julian},
    journal = {ArXiv preprint},
    title = {Large language models can be used to effectively scale spear phishing campaigns},
    url = {https://arxiv.org/abs/2305.06972},
    volume = {abs/2305.06972},
    year = {2023}
}


@article{bai2022training,
    author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
    journal = {ArXiv preprint},
    title = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
    url = {https://arxiv.org/abs/2204.05862},
    volume = {abs/2204.05862},
    year = {2022}
}


@article{shaikh2022second,
    author = {Shaikh, Omar and Zhang, Hongxin and Held, William and Bernstein, Michael and Yang, Diyi},
    journal = {  In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
    title = {On second thought, let's not think step by step! Bias and toxicity in zero-shot reasoning},
    url = {https://aclanthology.org/2023.acl-long.244},
    volume = {Volume 1: Long Papers},
    year = {2022}
}


@article{weidinger2021ethical,
    author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
    journal = {ArXiv preprint},
    title = {Ethical and social risks of harm from language models},
    url = {https://arxiv.org/abs/2112.04359},
    volume = {abs/2112.04359},
    year = {2021}
}


@inproceedings{nadeem2020stereoset,
    address = {Online},
    author = {Nadeem, Moin  and
Bethke, Anna  and
Reddy, Siva},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    doi = {10.18653/v1/2021.acl-long.416},
    pages = {5356--5371},
    publisher = {Association for Computational Linguistics},
    title = {{S}tereo{S}et: Measuring stereotypical bias in pretrained language models},
    url = {https://aclanthology.org/2021.acl-long.416},
    year = {2021}
}


@inproceedings{gehman2020realtoxicityprompts,
    address = {Online},
    author = {Gehman, Samuel  and
Gururangan, Suchin  and
Sap, Maarten  and
Choi, Yejin  and
Smith, Noah A.},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
    doi = {10.18653/v1/2020.findings-emnlp.301},
    pages = {3356--3369},
    publisher = {Association for Computational Linguistics},
    title = {{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models},
    url = {https://aclanthology.org/2020.findings-emnlp.301},
    year = {2020}
}


@article{bommasani2021opportunities,
    author = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
    journal = {ArXiv preprint},
    title = {On the opportunities and risks of foundation models},
    url = {https://arxiv.org/abs/2108.07258},
    volume = {abs/2108.07258},
    year = {2021}
}


@inproceedings{bender2021dangers,
    author = {Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
    booktitle = {Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
    pages = {610--623},
    title = {On the dangers of stochastic parrots: Can language models be too big?},
    year = {2021}
}



@article{yuan2021bridge,
    author = {Yuan, Lifan and Zhang, Yichi and Chen, Yangyi and Wei, Wei},
    journal = {ArXiv preprint},
    title = {Bridge the gap between cv and nlp! a gradient-based textual adversarial attack framework},
    url = {https://arxiv.org/abs/2110.15317},
    volume = {abs/2110.15317},
    year = {2021}
}


@inproceedings{zheng2024prompt,
    author = {Zheng, Chujie and Yin, Fan and Zhou, Hao and Meng, Fandong and Zhou, Jie and Chang, Kai-Wei and Huang, Minlie and Peng, Nanyun},
    booktitle = {Forty-first International Conference on Machine Learning},
    title = {On prompt-driven safeguarding for large language models},
    year = {2024}
}


@article{chao2023jailbreaking,
    author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
    journal = {The Thirty-Seventh Annual Conference on Neural Information Processing Systems, Workshop},
    title = {Jailbreaking black box large language models in twenty queries},
    url = {https://nips.cc/virtual/2023/76651},
    year = {2023}
}


@article{yu2024don,
    author = {Yu, Zhiyuan and Liu, Xiaogeng and Liang, Shunning and Cameron, Zach and Xiao, Chaowei and Zhang, Ning},
    journal = {33rd USENIX Security Symposium},
    title = {Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models},
    url = {https://www.usenix.org/system/files/sec24fall-prepub-1500-yu-zhiyuan.pdf},
    year = {2024}
}


@article{kreps2022all,
    author = {Kreps, Sarah and McCain, R Miles and Brundage, Miles},
    journal = {Journal of experimental political science},
    number = {1},
    pages = {104--117},
    publisher = {Cambridge University Press},
    title = {All the news that's fit to fabricate: AI-generated text as a tool of media misinformation},
    volume = {9},
    year = {2022}
}


@inproceedings{szegedy2013intriguing,
    author = {Christian Szegedy and
Wojciech Zaremba and
Ilya Sutskever and
Joan Bruna and
Dumitru Erhan and
Ian J. Goodfellow and
Rob Fergus},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
    booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
    editor = {Yoshua Bengio and
Yann LeCun},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Intriguing properties of neural networks},
    url = {http://arxiv.org/abs/1312.6199},
    year = {2014}
}


@inproceedings{goodfellow2014explaining,
    author = {Ian J. Goodfellow and
Jonathon Shlens and
Christian Szegedy},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
    booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
    editor = {Yoshua Bengio and
Yann LeCun},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Explaining and Harnessing Adversarial Examples},
    url = {http://arxiv.org/abs/1412.6572},
    year = {2015}
}


@article{rafailov2024direct,
    author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
    journal = {Advances in Neural Information Processing Systems},
    title = {Direct preference optimization: Your language model is secretly a reward model},
    volume = {36},
    year = {2024}
}


@article{dai2023safe,
    author = {Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
    journal = {The Twelfth International Conference on Learning Representations},
    title = {Safe rlhf: Safe reinforcement learning from human feedback},
    url = {https://arxiv.org/abs/2310.12773},
    volume = {abs/2310.12773},
    year = {2023}
}


@article{wei2024jailbroken,
    author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
    journal = {Advances in Neural Information Processing Systems},
    title = {Jailbroken: How does llm safety training fail?},
    volume = {36},
    year = {2024}
}


@article{huang2023catastrophic,
    author = {Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
    journal = {The Twelfth International Conference on Learning Representations},
    title = {Catastrophic jailbreak of open-source llms via exploiting generation},
    url = {https://arxiv.org/abs/2310.06987},
    volume = {abs/2310.06987},
    year = {2023}
}


@inproceedings{kang2024exploiting,
    author = {Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
    booktitle = {2024 IEEE Security and Privacy Workshops (SPW)},
    organization = {IEEE},
    pages = {132--143},
    title = {Exploiting programmatic behavior of llms: Dual-use through standard security attacks},
    year = {2024}
}


@article{zhuo2023red,
  title={Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

@article{Claude2modelcard,
    title={Model card and evaluations for claude models},
    author={Anthropic},
    year={2023},
    url = {https://www-files.anthropic. com/production/images/Model-Card-Claude-2.pdf}
}

@article{achiam2023gpt,
    author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
    journal = {ArXiv preprint},
    title = {Gpt-4 technical report},
    url = {https://arxiv.org/abs/2303.08774},
    volume = {abs/2303.08774},
    year = {2023}
}

@article{robey2023smoothllm,
    author = {Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
    journal = {ArXiv preprint},
    title = {Smoothllm: Defending large language models against jailbreaking attacks},
    url = {https://arxiv.org/abs/2310.03684},
    volume = {abs/2310.03684},
    year = {2023}
}


@article{jain2023baseline,
    author = {Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
    journal = {ArXiv preprint},
    title = {Baseline defenses for adversarial attacks against aligned language models},
    url = {https://arxiv.org/abs/2309.00614},
    volume = {abs/2309.00614},
    year = {2023}
}


@article{guo2024cold,
    author = {Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
    journal = {The Forty-First International Conference on Machine Learning},
    title = {Cold-attack: Jailbreaking llms with stealthiness and controllability},
    url = {https://arxiv.org/abs/2402.08679},
    volume = {abs/2402.08679},
    year = {2024}
}


@article{sadasivan2024fast,
    author = {Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil},
    journal = {The Forty-First International Conference on Machine Learning},
    title = {Fast Adversarial Attacks on Language Models In One GPU Minute},
    url = {https://arxiv.org/abs/2402.15570},
    volume = {abs/2402.15570},
    year = {2024}
}


@article{liu2023autodan,
    author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
    journal = {The Twelfth International Conference on Learning Representations},
    title = {Autodan: Generating stealthy jailbreak prompts on aligned large language models},
    url = {https://arxiv.org/abs/2310.04451},
    volume = {abs/2310.04451},
    year = {2023}
}


@article{zou2023universal,
    author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
    journal = {ArXiv preprint},
    title = {Universal and transferable adversarial attacks on aligned language models},
    url = {https://arxiv.org/abs/2307.15043},
    volume = {abs/2307.15043},
    year = {2023}
}


@article{jiang2023mistral,
    author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
    journal = {ArXiv preprint},
    title = {Mistral 7B},
    url = {https://arxiv.org/abs/2310.06825},
    volume = {abs/2310.06825},
    year = {2023}
}


@article{dettmers2024qlora,
    author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
    journal = {Advances in Neural Information Processing Systems},
    title = {Qlora: Efficient finetuning of quantized llms},
    volume = {36},
    year = {2024}
}


@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}


@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@inproceedings{cer2018universal,
    address = {Brussels, Belgium},
    author = {Cer, Daniel  and
Yang, Yinfei  and
Kong, Sheng-yi  and
Hua, Nan  and
Limtiaco, Nicole  and
St. John, Rhomni  and
Constant, Noah  and
Guajardo-Cespedes, Mario  and
Yuan, Steve  and
Tar, Chris  and
Strope, Brian  and
Kurzweil, Ray},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
    doi = {10.18653/v1/D18-2029},
    pages = {169--174},
    publisher = {Association for Computational Linguistics},
    title = {Universal Sentence Encoder for {E}nglish},
    url = {https://aclanthology.org/D18-2029},
    year = {2018}
}

@misc{dubey2024llama3herdmodels,
  title =         {The Llama 3 Herd of Models},
  author =        {Llama Team, AI @ Meta},
  year =          {2024},
  eprint =        {2407.21783},
  archivePrefix = {arXiv},
  primaryClass =  {cs.AI},
  url =           {https://arxiv.org/abs/2407.21783}
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
