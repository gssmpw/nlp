@inproceedings{greshake2023not,
    author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
    booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
    pages = {79--90},
    title = {Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection},
    year = {2023}
}

@article{guo2024cold,
    author = {Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
    journal = {The Forty-First International Conference on Machine Learning},
    title = {Cold-attack: Jailbreaking llms with stealthiness and controllability},
    url = {https://arxiv.org/abs/2402.08679},
    volume = {abs/2402.08679},
    year = {2024}
}

@article{mehrotra2023tree,
    author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
    journal = {ArXiv preprint},
    title = {Tree of attacks: Jailbreaking black-box llms automatically},
    url = {https://arxiv.org/abs/2312.02119},
    volume = {abs/2312.02119},
    year = {2023}
}

@article{sadasivan2024fast,
    author = {Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil},
    journal = {The Forty-First International Conference on Machine Learning},
    title = {Fast Adversarial Attacks on Language Models In One GPU Minute},
    url = {https://arxiv.org/abs/2402.15570},
    volume = {abs/2402.15570},
    year = {2024}
}

@article{shah2023scalable,
    author = {Shah, Rusheb and Pour, Soroush and Tagade, Arush and Casper, Stephen and Rando, Javier and others},
    journal = {ArXiv preprint},
    title = {Scalable and transferable black-box jailbreaks for language models via persona modulation},
    url = {https://arxiv.org/abs/2311.03348},
    volume = {abs/2311.03348},
    year = {2023}
}

@article{shen2023anything,
    author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
    journal = {ArXiv preprint},
    title = {" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
    url = {https://arxiv.org/abs/2308.03825},
    volume = {abs/2308.03825},
    year = {2023}
}

@article{wei2024jailbroken,
    author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
    journal = {Advances in Neural Information Processing Systems},
    title = {Jailbroken: How does llm safety training fail?},
    volume = {36},
    year = {2024}
}

@article{zeng2024johnny,
    author = {Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
    journal = {ArXiv preprint},
    title = {How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
    url = {https://arxiv.org/abs/2401.06373},
    volume = {abs/2401.06373},
    year = {2024}
}

@article{zhu2023autodan,
  title={Autodan: Automatic and interpretable adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  journal={arXiv preprint arXiv:2310.15140},
  year={2023}
}

@article{zou2023universal,
    author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
    journal = {ArXiv preprint},
    title = {Universal and transferable adversarial attacks on aligned language models},
    url = {https://arxiv.org/abs/2307.15043},
    volume = {abs/2307.15043},
    year = {2023}
}

