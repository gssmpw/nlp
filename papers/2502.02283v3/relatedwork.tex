\section{Related Work}
\subsection{Gaussian Processes}
\label{gps}
Gaussian Processes (GPs) are a collection of random variables, any finite number of which subjects to a joint Gaussian distribution~\cite{rasmussen2003gaussian}. GPs are particularly effective in handling sparse data, making them well-suited for scenarios with limited observations~\cite {wang2021computationally}. Their flexibility and built-in uncertainty quantification enable robust predictions, which has led to their widespread adoption in various computer vision tasks~\cite{lu2023robust,zhou2023interactive,zhu2021convolutional,yasarla2020syn2real}. 

Mathematically, a GP is fully specified by its mean function $m(\mathbf{x})$ and covariance function $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$, a.k.a. kernel function, where $\mathbf{x}$ and $\mathbf{x}^{\prime}$ are inputs. The covariance function typically depends on a set of hyperparameters  $\varphi$, such as length scale \( l \), variance \( \sigma^2 \), or other parameters depending on the kernel type (e.g., Mat√©rn). A GP is usually used as a prior over a latent function defined as:
\begin{equation}
    f(\mathbf{x}) \sim \mathcal{GP}\Big(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')\Big). 
\end{equation}
Training a GP model involves optimizing the hyperparameters \( \theta \) of the kernel function to maximize the likelihood of the observed data. This is achieved by maximizing the log marginal likelihood, given by:
\begin{equation}
\log p(\mathbf{y} \mid \mathbf{X}, \theta)=-\frac{1}{2} \mathbf{y}^T \mathbf{K}^{-1} \mathbf{y}-\frac{1}{2} \log |\mathbf{K}|-\frac{n}{2} \log 2 \pi,
\end{equation}
\noindent
where $\mathbf{y}$ are the observed outputs, $\mathbf{X}$ are the observed inputs, \( \mathbf{K} \in \mathbb{R}^{n \times n} \) is the Gram matrix with entries computed using the kernel function \( k(\mathbf{x}_i, \mathbf{x}_j) \) across all training inputs, and \( n \) is the number of training points. After training, the predictive distribution at a new test point \( \mathbf{x}_* \) follows a Gaussian distribution: $f\left(\mathbf{x}_*\right) \mid \mathbf{X}, \mathbf{y}, \mathbf{x}_* \sim \mathcal{N}\left(\mu_*, \sigma_*^2\right)$, where the predictive mean and variance are computed as:
\begin{equation}
    \mu_* = \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{y}, \quad
    \sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*,
\end{equation}

where \( \mathbf{k}_* = [k(\mathbf{x}_*, \mathbf{x}_i)] \in \mathbb{R}^{n \times 1} \) represents the covariance between the test point \( \mathbf{x}_* \) and all training points, while \( k(\mathbf{x}_*, \mathbf{x}_*) \) denotes the self-variance of the test point.







%\theta$ is the hyperparameter of the kernel, $K$ is the covariance matrix computed using the kernel function across all pairs of training inputs, $n$ is the number of training points. After the model is trained, predictions at new test points are made based on the joint Gaussian nature of the prior and the observed data. The predictive distribution at a new test point $\mathbf{x}_*$ is also gaussian $f\left(\mathbf{x}_*\right) \mid \mathbf{X}, \mathbf{y}, \mathbf{x}_* \sim \mathcal{N}\left(\mu_*, \sigma_*^2\right)$, with mean $\mu_*=\mathbf{k}_*^T K^{-1} \mathbf{y}$ and variance 
%$\sigma_*^2=k\left(\mathbf{x}_*, \mathbf{x}_*\right)-\mathbf{k}_*^T K^{-1} \mathbf{k}_*$, where $\mathbf{k}_*$ is the vector of covariances between the test point $\mathbf{x}_*$ and all training points, $k\left(\mathbf{x}_*, \mathbf{x}_*\right)$ is the variance at the test point according to the kernel. 

\subsection{3D Gaussian Splatting}
\label{Neural Radiance Fields}
%NeRF~\cite{mildenhall2021nerf} utilize implicit representation for the first time to achieve photo-realistic perspective synthesis, subsequently inferring the 3D structure of the scene. It utilizes a limited number of input views to train a neural network to represent a continuous volumetric scene, enabling the generation of new perspectives of the scene once the neural network is trained. To be specific, given the 3D location $\mathbf{p}=(x,y,z)$ of a spatial sampling point (to be rendered) and 2D view direction $\mathbf{d}=(\theta,\phi)$ of the camera, NeRF predicts the color $\mathbf{c}=(r,g,b)$ of the sampling point and the volume density $\sigma$ through a multi-layer perception (MLP) neural network~\cite{popescu2009multilayer} which can be represented as $F_{\Theta}:(\mathbf{p}, \mathbf{d}) \rightarrow(\mathbf{c}, \sigma)$, where $F_{\Theta}$ is the MLP parameterised by $\Theta$.
%The color is estimated by volumetric rendering via quadrature, which can be formulated as
%\begin{equation}
   % C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma\big(\mathbf{r}(t)\big) \mathbf{c}\big(\mathbf{r}(t), \mathbf{d}\big) d t, 
%\end{equation}
%where $T(t)=\exp \left(-\int_{t_n}^t \sigma\big(\mathbf{r}(s)\big) d s\right)$, $C(\mathbf{r})$ is the sampling pixel value and is calculated by integrating the radiance value $\mathbf{c}$ along the ray $\mathbf{r}(t)=\mathbf{o}+t \mathbf{d}$, in which $\mathbf{o}$ is the camera position, $\mathbf{d}$ is the direction from the camera to the sampled pixel, within near and far bounds $t_n$ and $t_f$, and the function $T(t)$ denotes the accumulated transmittance along each ray from $t_n$ to $t$. In cases where insufficient input images are used for training, the rendering quality of NeRF can be fairly poor. To improve NeRF's rendering quality, various enhancements have been proposed. For instance, some works incorporates depth supervision to enforce geometric constraints~\cite{deng2022depth,guo2024depth,wei2021nerfingmvs,neff2021donerf}, and others have addressed challenges in low-light conditions~\cite{wang2023lighting,chen2023bidirectional} to enhance the scene representation and synthesize normal-light novel views. However, the inherent inverse rendering approach in NeRF-based methods that necessitates dense sampling along each ray and represents the scene using MLP is computationally intensive and presents challenges in optimization. Consequently, this hinders NeRF's applications in scenarios requiring rapid rendering or real-time performance.

%In contrast to NeRF's inverse rendering methodology, 
3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} employs a forward rendering approach. This technique represents scenes using 3D Gaussians and achieves efficient rendering by directly projecting these Gaussians onto the 2D image plane. By circumventing the need for extensive ray sampling and complex volumetric integration, 3DGS facilitates real-time, high-fidelity scene reconstruction. Specifically, it computes pixel colours by depth sorting and $\alpha$-blending of projected 2D Gaussians. This method avoids the complex calculation of ray marching and volume integration and can achieve real-time high-quality rendering and NVS. Several works have enhanced 3DGS, such as mitigating artefacts arising from camera pose sensitivity~\cite{yu2024mip}. Others manage points to improve rendering quality~\cite{yang2024gaussian,zhang2024pixel,bulo2024revising}. To the best of our knowledge, only a few studies have explored the densification of SfM point clouds for 3DGS performance improvement~\cite{cheng2024gaussianpro,chan2024point}. While there is evidence showing that densification helps to improve the rendering quality of 3DGS, they often overlook the mathematical relationship between pixels and point clouds derived from the initial sparse SfM.

To bridge this gap, our method introduces a MOGP model, to learn the mapping relationship between pixels and point clouds, which facilitates the densification of 3D Gaussians, thereby enhancing the rendering quality of 3DGS.
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/ourgpgsoverview.pdf}
        %\vspace{-8mm}
    \caption{\textbf{The overview of GP-GS.} \textbf{(a)} Multi Views and Depth Priors: Multi-view images are first processed with depth estimating models like Depth Anything~\cite{yang2024depth} to extract per-view depth maps. \textbf{(b)} Point Cloud Densification: Sparse point cloud is initially reconstructed using SfM. Next, MOGP is trained to take pixel coordinates and depth as inputs $\mathbf{X}=(u,v,d)$  and predicts dense point clouds $\mathbf{Y}=(x,y,z,r,g,b)$ with uncertainty (variance). The loss function ensures an optimal mapping between input pixels and point clouds. \textbf{(c)} Uncertainty-Based Filtering: The predicted dense point clouds undergo filtering based on uncertainty. This improves the quality of the dense point clouds that are used to initialize dense 3D Gaussians, which are then optimized to refine geometric details. The final rendered novel views demonstrate the effectiveness of GP-GS in reconstructing fine details while maintaining structural coherence.}
    %\vspace{-4mm}
    \label{fig:overview}
\end{figure*}