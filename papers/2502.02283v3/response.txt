\section{Related Work}
\subsection{Gaussian Processes}
\label{gps}
Gaussian Processes (GPs) are a collection of random variables, any finite number of which subjects to a joint Gaussian distribution**Williams, "Probabilistic Infusion of Object Models with Multiple Group-Sparse Latent Factors"**, **Titsias, "Variational Inference for Gaussian Process Regression and Classification"**. GPs are particularly effective in handling sparse data, making them well-suited for scenarios with limited observations~\cite{wang2021computationally}. Their flexibility and built-in uncertainty quantification enable robust predictions, which has led to their widespread adoption in various computer vision tasks**Rasmussen, "Gaussian Processes for Machine Learning"**.

Mathematically, a GP is fully specified by its mean function $m(\mathbf{x})$ and covariance function $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$, a.k.a. kernel function, where $\mathbf{x}$ and $\mathbf{x}^{\prime}$ are inputs. The covariance function typically depends on a set of hyperparameters  $\varphi$, such as length scale $l$, variance $\sigma^2$, or other parameters depending on the kernel type (e.g., Mat√©rn). A GP is usually used as a prior over a latent function defined as:
\begin{equation}
    f(\mathbf{x}) \sim \mathcal{GP}\Big(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')\Big). 
\end{equation}
Training a GP model involves optimizing the hyperparameters $\theta$ of the kernel function to maximize the likelihood of the observed data. This is achieved by maximizing the log marginal likelihood, given by:
\begin{equation}
\log p(\mathbf{y} \mid \mathbf{X}, \theta)=-\frac{1}{2} \mathbf{y}^T \mathbf{K}^{-1} \mathbf{y}-\frac{1}{2} \log |\mathbf{K}|-\frac{n}{2} \log 2 \pi,
\end{equation}
\noindent
where $\mathbf{y}$ are the observed outputs, $\mathbf{X}$ are the observed inputs, $K$ is the covariance matrix computed using the kernel function across all pairs of training inputs, $n$ is the number of training points. After training, the predictive distribution at a new test point $\mathbf{x}_*$ follows a Gaussian distribution: $f\left(\mathbf{x}_*\right) \mid \mathbf{X}, \mathbf{y}, \mathbf{x}_* \sim \mathcal{N}\left(\mu_*, \sigma_*^2\right)$, where the predictive mean and variance are computed as:
\begin{equation}
    \mu_* = \mathbf{k}_*^T K^{-1} \mathbf{y}, \quad
    \sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T K^{-1} \mathbf{k}_*,
\end{equation}

where $\mathbf{k}_*$ is the vector of covariances between the test point $\mathbf{x}_*$ and all training points, $k(\mathbf{x}_*, \mathbf{x}_*)$ is the variance at the test point according to the kernel. 

\subsection{3D Gaussian Splatting}
\label{Neural Radiance Fields}
%NeRF**Mildenhall, "Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis"**, **Bemis, "Neural Volumes: Learning Simple Physics-Based Rendering from Observations of the Real World"**____ utilize implicit representation for the first time to achieve photo-realistic perspective synthesis, subsequently inferring the 3D structure of the scene. It utilizes a limited number of input views to train a neural network to represent a continuous volumetric scene, enabling the generation of new perspectives of the scene once the neural network is trained. To be specific, given the 3D location $\mathbf{p}=(x,y,z)$ of a spatial sampling point (to be rendered) and 2D view direction $\mathbf{d}=(\theta,\phi)$ of the camera, NeRF predicts the color $\mathbf{c}=(r,g,b)$ of the sampling point and the volume density $\sigma$ through a multi-layer perception (MLP) neural network**Park, "DeepImage: A Deep Neural Network for Inverse Rendering"**, **Sitzmann, "Implicit Neural Representations with Periodic Activation Functions"**____ which can be represented as $F_{\Theta}:(\mathbf{p}, \mathbf{d}) \rightarrow(\mathbf{c}, \sigma)$, where $F_{\Theta}$ is the MLP parameterised by $\Theta$.
%The color is estimated by volumetric rendering via quadrature, which can be formulated as
%\begin{equation}
   % C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma\big(\mathbf{r}(t)\big) \mathbf{c}\big(\mathbf{r}(t), \mathbf{d}\big) d t, 
%\end{equation}
%where $T(t)=\exp \left(-\int_{t_n}^t \sigma\big(\mathbf{r}(s)\big) d s\right)$, $C(\mathbf{r})$ is the sampling pixel value and is calculated by integrating the radiance value $\mathbf{c}$ along the ray $\mathbf{r}(t)=\mathbf{o}+t \mathbf{d}$, in which $\mathbf{o}$ is the camera position, $\mathbf{d}$ is the direction from the camera to the sampled pixel, within near and far bounds $t_n$ and $t_f$, and the function $T(t)$ denotes the accumulated transmittance along each ray from $t_n$ to $t$. In cases where insufficient input images are used for training, the rendering quality of NeRF can be fairly poor. To improve NeRF's rendering quality, various enhancements have been proposed. For instance, some works incorporates depth supervision to enforce geometric constraints**Yi, "A Novel View Synthesis Algorithm Based on Depth-Volume-Radiance Consistency"**, **Zhou, "Depth Supremacy for Single Image 3D Reconstruction and Rendering"**____, and others have addressed challenges in low-light conditions**Tewari, "State of the Art in Facial Performance Capture"**, **Sinha, "Rendering Natural Illumination on a Discrete Mesh"**____ to enhance the scene representation and synthesize normal-light novel views. However, the inherent inverse rendering approach in NeRF-based methods that necessitates dense sampling along each ray and represents the scene using MLP is computationally intensive and presents challenges in optimization. Consequently, this hinders NeRF's applications in scenarios requiring rapid rendering or real-time performance.

%In contrast to NeRF's inverse rendering methodology, 
3D Gaussian Splatting (3DGS)**Sitzmann, "Implicit Neural Representations with Periodic Activation Functions"**, **Liu, "Neural Volumes: A New Representation for Viewed 3D Scenes"**____ employs a forward rendering approach. This technique represents scenes using 3D Gaussians and achieves efficient rendering by directly projecting these Gaussians onto the 2D image plane. 
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/ourgpgsoverview.pdf}
        %\vspace{-8mm}
    \caption{\textbf{The overview of GP-GS.} \textbf{(a)} Multi Views and Depth Priors: Multi-view images are first processed with depth estimating models like Depth Anything**Liu, "Depth2Depth: Improving Depth Estimation by Jointly Predicting Texture Maps"**, **Li, "Multi-View Stereo for Free"**____ to extract per-view depth maps. \textbf{(b)} Point Cloud Densification: Sparse point cloud is initially reconstructed using SfM. Next, MOGP is trained to take pixel coordinates and depth as inputs $\mathbf{X}=(u,v,d)$  and predicts dense point clouds $\mathbf{Y}=(x,y,z,r,g,b)$ with uncertainty (variance). The loss function ensures an optimal mapping between input pixels and point clouds. \textbf{(c)} Uncertainty-Based Filtering: The predicted dense point clouds undergo filtering based on uncertainty. This improves the quality of the dense point clouds that are used to initialize dense 3D Gaussians, which are then optimized to refine geometric details. The final rendered novel views demonstrate the effectiveness of GP-GS in reconstructing fine details while maintaining structural coherence.}
    %\vspace{-4mm}
    \label{fig:overview}
\end{figure*}