
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{booktabs}
\usepackage[table]{xcolor}
\definecolor{lightred}{rgb}{1, 0.76, 0.76}
\definecolor{lightgreen}{rgb}{0.88, 1, 0.88}
\definecolor{lightorange}{rgb}{1, 0.93, 0.76}

\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT

\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{hyperref}

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

% added packages
\usepackage{amsmath,amsfonts,amssymb,bm}
\usepackage{bibentry}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}




\setcounter{secnumdepth}{2} 
\nocopyright 
\title{GP-GS: Gaussian Processes for Enhanced Gaussian Splatting}
\author{
    Zhihao Guo$^{1,*}$ \quad
    Jingxuan Su$^{2,*}$ \quad
    Shenglin Wang$^{3}$ \quad
    Jinlong Fan$^{4}$ \quad
    Jing Zhang$^{5}$ \quad
    Liangxiu Han$^{1}$ \quad
    Peng Wang$^{1,\dag}$
}

\affiliations{
    $^{1}$ Manchester Metropolitan University \\
    $^{2}$ SECE, Peking University\\
    $^{3}$ Pengcheng Laboratory\\
    $^{4}$ Hangzhou Dianzi University \\
    $^{5}$ Wuhan University
}




\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Co-first author.}
\footnotetext[2]{Corresponding author.}
\renewcommand{\thefootnote}{\arabic{footnote}}  % Reset footnote numbering

\begin{figure*}[t]
    \centering
	\includegraphics[width=\textwidth]{figures/res_overview.pdf}
	\caption{Illustration of the point cloud densification and improved rendering results. (a) Conventional SfM only yields sparse point clouds, which poorly capture rich scene details. Our Multi-Output Gaussian Process model addresses this issue by densifying the sparse point clouds across various datasets, achieving 71\% (indicated by $R^2$ score~\cite{edwards2008r2}) position and colour accuracy within the predicted confidence interval. (b) Rendered images comparison with 3DGS* (official improvements). GP-GS substantially enhances rendering quality, particularly in intricate regions like dense foliage or under challenging lighting conditions.} 

    \label{fig:res_overview}
\end{figure*}
%%% abstract
\begin{abstract}
3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds consistently compromises the scene reconstruction quality. To address these limitations, this paper proposes a novel 3D reconstruction framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output Gaussian Process model is developed to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. The densified point clouds provide high-quality initial 3D Gaussians to enhance reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework. \href{https://github.com/zhihaohaoran/GPGS}{Code} is available.
\end{abstract}


\section{Introduction}
\label{Introduction}
Novel View Synthesis (NVS) is a fundamental yet challenging problem in computer vision and computer graphics, aiming to generate novel viewpoints of a given scene from multi-view observations. It plays a critical role in applications such as Digital Twinning~\cite{wang2024deep}, virtual reality~\cite{fei20243d}, and robotics~\cite{xiong2024event3dgs,wang2024robot}. Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf} have revolutionized NVS by implicitly modelling volumetric scene representations, achieving high-fidelity rendering without explicit 3D geometry reconstruction. However, NeRF-based methods suffer from computational inefficiency and the requirement of dense sampling along rays, leading to slow inference speeds despite recent acceleration efforts~\cite{guo2024depth,mildenhall2021nerf,roessle2022dense,zhang2020nerf++},.

To overcome these limitations, 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} has emerged as a promising alternative for real-time rendering. Unlike NeRF, which relies on ray-marching and volumetric integration, 3DGS represents scenes explicitly using a set of 3D Gaussians with learnable attributes like positions and colours. The rasterization-based splatting strategy can be considered a forward rendering technique that avoids costly ray sampling and enables efficient parallel rendering, making it a compelling choice for NVS applications. However, existing 3DGS pipelines rely heavily on Structure-from-Motion (SfM) to extract 3D points by detecting and matching texture features like SIFT points, leading to sparse or incomplete point clouds, which is particularly the case in texture-less or clustered regions, leading to poor initialization of 3D Gaussians. Consequently, artefacts arise in the rendering process, causing a loss of fine details. To mitigate the limitations of sparse SfM-based reconstructions, the Adaptive Density Control (ADC) strategy has been introduced to 3DGS pipelines. ADC aims to improve scene coverage by dynamically duplicating Gaussians in underrepresented regions and removing redundant ones in over-reconstructed areas. However, ADC operates without explicit geometric priors, often producing noisy distributions that fail to adhere to the underlying scene structure. As a result, this leads to blurry reconstructions and poor occlusion handling, limiting the overall fidelity of 3DGS-based novel view synthesis. These challenges significantly limit the fidelity and robustness of 3DGS-based NVS methods, motivating the community for a more structured and adaptive approach to point cloud refinement and rendering.

In this paper, we propose GP-GS (Gaussian Processes for Enhanced 3D Gaussian Splatting), a novel framework designed to enhance the initialization of 3D Gaussians and improve rendered quality, especially in complex regions with densely packed objects (e.g., foliage) or under challenging lighting conditions. To be specific, we introduce a Multi-Output Gaussian Process (MOGP) model to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we formulate point cloud densification as a continuous regression problem, where the MOGP learns a mapping from 2D image pixels and their depth priors to a denser point cloud with position and colour information. To ensure a structured and robust densification process, we propose an adaptive neighbourhood-based sampling strategy, where pixels are selected as candidate MOGP inputs for densification. For each sampled pixel, we predict its corresponding 3D point cloud attributes (position, colour, variance) using the MOGP model. To further refine the densified point cloud, we apply a variance-based filtering method, which removes high-uncertainty predictions, thereby reducing noise accumulation and preserving high-confidence reconstructions. The densified point clouds provide high-quality 3D Gaussian blobs (or Gaussians for brevity) to enhance the reconstruction performance.


\textbf{Our contributions} can be summarized as: 1) Gaussian Processes for Point Cloud Densification: We propose a MOGP model to densify sparse SfM point clouds by learning mappings from 2D image pixels and depth information to 3D positions and colours, with uncertainty awareness. 2) Adaptive Sampling and Uncertainty-Guided Filtering: We introduce an adaptive neighbourhood-based sampling strategy that generates candidate inputs for MOGP for 3D points prediction, followed by variance-based filtering to remove high uncertainty predictions, ensuring geometric consistency and enhancing rendering quality. 3) Our GP-GS framework can be seamlessly integrated into existing SfM-based pipelines, making it a flexible plug-and-play module that improves the rendering quality of other NVS models.



\section{Related Work}


\subsection{Gaussian Processes}
\label{gps}
Gaussian Processes (GPs) are a collection of random variables, any finite number of which subjects to a joint Gaussian distribution~\cite{rasmussen2003gaussian}. GPs are particularly effective in handling sparse data, making them well-suited for scenarios with limited observations~\cite {wang2021computationally}. Their flexibility and built-in uncertainty quantification enable robust predictions, which has led to their widespread adoption in various computer vision tasks~\cite{lu2023robust,zhou2023interactive,zhu2021convolutional,yasarla2020syn2real}. 

Mathematically, a GP is fully specified by its mean function $m(\mathbf{x})$ and covariance function $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$, a.k.a. kernel function, where $\mathbf{x}$ and $\mathbf{x}^{\prime}$ are inputs. The covariance function typically depends on a set of hyperparameters  $\varphi$, such as length scale \( l \), variance \( \sigma^2 \), or other parameters depending on the kernel type (e.g., Matérn). A GP is usually used as a prior over a latent function defined as:
\begin{equation}
    f(\mathbf{x}) \sim \mathcal{GP}\Big(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')\Big). 
\end{equation}
Training a GP model involves optimizing the hyperparameters \( \theta \) of the kernel function to maximize the likelihood of the observed data. This is achieved by maximizing the log marginal likelihood, given by:
\begin{equation}
\log p(\mathbf{y} \mid \mathbf{X}, \theta)=-\frac{1}{2} \mathbf{y}^T \mathbf{K}^{-1} \mathbf{y}-\frac{1}{2} \log |\mathbf{K}|-\frac{n}{2} \log 2 \pi,
\end{equation}
\noindent
where $\mathbf{y}$ are the observed outputs, $\mathbf{X}$ are the observed inputs, \( \mathbf{K} \in \mathbb{R}^{n \times n} \) is the Gram matrix with entries computed using the kernel function \( k(\mathbf{x}_i, \mathbf{x}_j) \) across all training inputs, and \( n \) is the number of training points. After training, the predictive distribution at a new test point \( \mathbf{x}_* \) follows a Gaussian distribution: $f\left(\mathbf{x}_*\right) \mid \mathbf{X}, \mathbf{y}, \mathbf{x}_* \sim \mathcal{N}\left(\mu_*, \sigma_*^2\right)$, where the predictive mean and variance are computed as:
\begin{equation}
    \mu_* = \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{y}, \quad
    \sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*,
\end{equation}

where \( \mathbf{k}_* = [k(\mathbf{x}_*, \mathbf{x}_i)] \in \mathbb{R}^{n \times 1} \) represents the covariance between the test point \( \mathbf{x}_* \) and all training points, while \( k(\mathbf{x}_*, \mathbf{x}_*) \) denotes the self-variance of the test point.







%\theta$ is the hyperparameter of the kernel, $K$ is the covariance matrix computed using the kernel function across all pairs of training inputs, $n$ is the number of training points. After the model is trained, predictions at new test points are made based on the joint Gaussian nature of the prior and the observed data. The predictive distribution at a new test point $\mathbf{x}_*$ is also gaussian $f\left(\mathbf{x}_*\right) \mid \mathbf{X}, \mathbf{y}, \mathbf{x}_* \sim \mathcal{N}\left(\mu_*, \sigma_*^2\right)$, with mean $\mu_*=\mathbf{k}_*^T K^{-1} \mathbf{y}$ and variance 
%$\sigma_*^2=k\left(\mathbf{x}_*, \mathbf{x}_*\right)-\mathbf{k}_*^T K^{-1} \mathbf{k}_*$, where $\mathbf{k}_*$ is the vector of covariances between the test point $\mathbf{x}_*$ and all training points, $k\left(\mathbf{x}_*, \mathbf{x}_*\right)$ is the variance at the test point according to the kernel. 

\subsection{3D Gaussian Splatting}
\label{Neural Radiance Fields}
%NeRF~\cite{mildenhall2021nerf} utilize implicit representation for the first time to achieve photo-realistic perspective synthesis, subsequently inferring the 3D structure of the scene. It utilizes a limited number of input views to train a neural network to represent a continuous volumetric scene, enabling the generation of new perspectives of the scene once the neural network is trained. To be specific, given the 3D location $\mathbf{p}=(x,y,z)$ of a spatial sampling point (to be rendered) and 2D view direction $\mathbf{d}=(\theta,\phi)$ of the camera, NeRF predicts the color $\mathbf{c}=(r,g,b)$ of the sampling point and the volume density $\sigma$ through a multi-layer perception (MLP) neural network~\cite{popescu2009multilayer} which can be represented as $F_{\Theta}:(\mathbf{p}, \mathbf{d}) \rightarrow(\mathbf{c}, \sigma)$, where $F_{\Theta}$ is the MLP parameterised by $\Theta$.
%The color is estimated by volumetric rendering via quadrature, which can be formulated as
%\begin{equation}
   % C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma\big(\mathbf{r}(t)\big) \mathbf{c}\big(\mathbf{r}(t), \mathbf{d}\big) d t, 
%\end{equation}
%where $T(t)=\exp \left(-\int_{t_n}^t \sigma\big(\mathbf{r}(s)\big) d s\right)$, $C(\mathbf{r})$ is the sampling pixel value and is calculated by integrating the radiance value $\mathbf{c}$ along the ray $\mathbf{r}(t)=\mathbf{o}+t \mathbf{d}$, in which $\mathbf{o}$ is the camera position, $\mathbf{d}$ is the direction from the camera to the sampled pixel, within near and far bounds $t_n$ and $t_f$, and the function $T(t)$ denotes the accumulated transmittance along each ray from $t_n$ to $t$. In cases where insufficient input images are used for training, the rendering quality of NeRF can be fairly poor. To improve NeRF's rendering quality, various enhancements have been proposed. For instance, some works incorporates depth supervision to enforce geometric constraints~\cite{deng2022depth,guo2024depth,wei2021nerfingmvs,neff2021donerf}, and others have addressed challenges in low-light conditions~\cite{wang2023lighting,chen2023bidirectional} to enhance the scene representation and synthesize normal-light novel views. However, the inherent inverse rendering approach in NeRF-based methods that necessitates dense sampling along each ray and represents the scene using MLP is computationally intensive and presents challenges in optimization. Consequently, this hinders NeRF's applications in scenarios requiring rapid rendering or real-time performance.

%In contrast to NeRF's inverse rendering methodology, 
3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} employs a forward rendering approach. This technique represents scenes using 3D Gaussians and achieves efficient rendering by directly projecting these Gaussians onto the 2D image plane. By circumventing the need for extensive ray sampling and complex volumetric integration, 3DGS facilitates real-time, high-fidelity scene reconstruction. Specifically, it computes pixel colours by depth sorting and $\alpha$-blending of projected 2D Gaussians. This method avoids the complex calculation of ray marching and volume integration and can achieve real-time high-quality rendering and NVS. Several works have enhanced 3DGS, such as mitigating artefacts arising from camera pose sensitivity~\cite{yu2024mip}. Others manage points to improve rendering quality~\cite{yang2024gaussian,zhang2024pixel,bulo2024revising}. To the best of our knowledge, only a few studies have explored the densification of SfM point clouds for 3DGS performance improvement~\cite{cheng2024gaussianpro,chan2024point}. While there is evidence showing that densification helps to improve the rendering quality of 3DGS, they often overlook the mathematical relationship between pixels and point clouds derived from the initial sparse SfM.

To bridge this gap, our method introduces a MOGP model, to learn the mapping relationship between pixels and point clouds, which facilitates the densification of 3D Gaussians, thereby enhancing the rendering quality of 3DGS.
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/ourgpgsoverview.pdf}
        %\vspace{-8mm}
    \caption{\textbf{The overview of GP-GS.} \textbf{(a)} Multi Views and Depth Priors: Multi-view images are first processed with depth estimating models like Depth Anything~\cite{yang2024depth} to extract per-view depth maps. \textbf{(b)} Point Cloud Densification: Sparse point cloud is initially reconstructed using SfM. Next, MOGP is trained to take pixel coordinates and depth as inputs $\mathbf{X}=(u,v,d)$  and predicts dense point clouds $\mathbf{Y}=(x,y,z,r,g,b)$ with uncertainty (variance). The loss function ensures an optimal mapping between input pixels and point clouds. \textbf{(c)} Uncertainty-Based Filtering: The predicted dense point clouds undergo filtering based on uncertainty. This improves the quality of the dense point clouds that are used to initialize dense 3D Gaussians, which are then optimized to refine geometric details. The final rendered novel views demonstrate the effectiveness of GP-GS in reconstructing fine details while maintaining structural coherence.}
    %\vspace{-4mm}
    \label{fig:overview}
\end{figure*}


\section{Preliminaries}
\label{Preliminaries: 3D Gaussian Splatting}
3DGS~\cite{kerbl20233d} initialize SfM sparse point clouds as 3D Gaussians (ellipsoid shaped), each defined by specific parameters such as position (mean), rotation, scales, covariance, opacity $\alpha$, and colour represented as spherical harmonics. For the sake of brevity, this paper abuses $\mathbf{x}$, representing the position of Gaussians, to define 3D Gaussian $G$ as follows:
\begin{equation}
G(\mathbf{x})=e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})},
\end{equation}
where $\boldsymbol{\mu} \in \mathbb{R}^{3 \times 1}$ is the mean vector, $\boldsymbol{\Sigma} \in \mathbb{R}^{3 \times 3}$ is the covariance matrix. 
% This Gaussian is multiplied by opacity $\alpha$. 
The 3D covariance $\mathbf{\Sigma}$ is a positive semi-definite matrix, which can be denoted as: $\mathbf{\Sigma} = \mathbf{R} \mathbf{S} \mathbf{S}^T \mathbf{R}^T$, where $\mathbf{R} \in \mathbb{R}^{3 \times 3}$ is an orthogonal rotation matrix, and $\mathbf{S} \in \mathbb{R}^{3 \times 3}$ is a diagonal scale matrix.

3D Gaussians will be projected to 2D image space using the splatting-based rasterization technique~\cite{zwicker2001surface}. Specifically, the transformation is approximated with a first-order Taylor expansion at a projected point in the camera coordinate frame. This ensures efficient and accurate rendering from a given viewpoint. The 2D covariance matrix $\boldsymbol{\Sigma}^{\prime}$, which describes the elliptical shape of each Gaussian in the image space, is then computed as: 
\begin{equation}
    \boldsymbol{\Sigma}^{\prime}=\mathbf{J W } \boldsymbol{\Sigma} \mathbf{W}^T \mathbf{J}^T, 
\end{equation}
where $\mathbf{J}$ is the Jacobian of the affine approximation of the projective
transformation, and $\mathbf{W}$ denotes the view transformation matrix~\cite{zwicker2001ewa}. The colour of each pixel is calculated by blending sorted Gaussians based on $\alpha$, as follows:
\begin{equation}
c=\sum_{i=1}^n c_i \alpha_i \prod_{j=1}^{i-1}\left(1-\alpha_j\right),
\end{equation}
where $n$ is the number of points, $c_i$ is the color of the $i-th$ point, $\alpha_i$ can be obtained by evaluating a projected 2D Gaussian with covariance $\boldsymbol{\Sigma}^{\prime}$ multiplied with a learned opacity for each point.

\section{Methodology}


\subsection{Overview}
\label{methodoverview}
This paper proposes a novel framework GP-GS that enhances 3D Gaussians initialization, thereby improving 3DGS rendering quality. The overview of GP-GS is presented in Figure~\ref{fig:overview}. First, addressing the limitations of sparse SfM point clouds, we propose the MOGP model to formulate point cloud densification as a regression problem. 
(Sec.~\ref{MultiOutputGaussianProces}). The training strategy of MOGP is introduced in Appendix~\ref{MOGP training process}. Next, we adaptively densify
sparse SfM point clouds and propose an uncertainty-based filtering strategy to remove high-uncertainty predictions. (Sec.~\ref{Point Cloud Densification}).


%our method consider point cloud densification as a regression problem using MOGP, 



\subsection{Multi Output Gaussian Process} \label{MultiOutputGaussianProces}
This section introduces MOGP to learn the relationship between pixels with depth priors and sparse SfM point clouds with position and colour information. 

\textbf{Problem Definition.} We consider a MOGP regression problem, where the goal is to predict dense point clouds given image pixels and their corresponding depth priors. Given a set of RGB images $\mathcal{I}=\{I_i\}_{i=1}^n$, where $n$ is the total number of images, we extract the corresponding sparse point clouds \( \mathcal{P}=\{\mathbf{P}_j\}_{j=1}^N \), where \( N \) is the number of 3D points, each point \( \mathbf{P}_j \) in the point clouds contains both 3D spatial position and colour information: $\mathbf{P}_j = (x_j, y_j, z_j, r_j, g_j, b_j)$, where \( (x_j, y_j, z_j) \) are 3D coordinates and \( (r_j, g_j, b_j) \) are RGB grey values. However, one point in the 3D point clouds may correspond to multiple image pixels from various views. To deal with this, we build a correspondence between 2D pixels and 3D point clouds  \( f : \mathbf{V}_i \rightarrow \mathbf{P}_j \), where $\mathbf{V}_i=\left(u_i, v_i\right)$, with \( (u_i, v_i) \) the pixel coordinates in the image space. All pixel coordinates are denoted by \( \mathcal{V} = \{\mathbf{V}_i\}_{i=1}^{\tilde{n}} \). When multiple pixels \( \mathbf{V}_{i_1}, \mathbf{V}_{i_2}, ..., \mathbf{V}_{i_m} \) contribute to the estimation of a single point \( \mathbf{P}_j \), we denote the correspondence as  
% To establish the correspondence between pixels and point clouds, we define a mapping function \( f : \mathbf{V}_i \rightarrow \mathbf{P}_j \), which determines the pixel-to-point correspondence:
\begin{equation}
\mathbf{P}_j = f(\mathbf{V}_{i_1}, \mathbf{V}_{i_2}, \dots, \mathbf{V}_{i_m}).
\end{equation}
% where multiple pixels \( \mathbf{V}_{i_1}, \mathbf{V}_{i_2}, ..., \mathbf{V}_{i_m} \) may contribute to the estimation of a single point \( \mathbf{P}_j \). 

% As the depth of pixels contributes to the densification performance, we exploit depth priors $\mathcal{D}=\{D_i\}_{i=1}^n$ from monocular depth estimation methods like~\cite{yang2024depth}, which provides accurate estimated depth value \( D_i(x_i, y_i) \) for each pixel coordinate \( (x_i, y_i) \), and maintains high efficiency. This will help to accelerate the whole GP-GS performance.

With the observation that the depth of pixels contributes to the densification performance, we leverage depth priors $\mathcal{D}=\{D_i\}_{i=1}^n$ obtained from monocular depth estimation methods, such as~\cite{yang2024depth}, to provide accurate estimated depth values $D_i(u_i, v_i)$ for each pixel with coordinate $(u_i, v_i)$. For brevity, we will use $D_i$ instead of $D_i(u_i, v_i)$ below. The rationale behind choosing the monocular depth estimation method is that it strikes a trade-off between accuracy and efficiency, which will accelerate the overall performance of the GP-GS framework.


\textbf{MOGP Formulation.} The input features $\mathbf{X}$ of the MOGP consist of pixel coordinates and depth information in each image defined as $\mathbf{x}_{i,j} = \big(u_i, v_i, D_i\big)$, where \( (u_i, v_i) \) are pixel coordinates, and \( D_i \) is the corresponding depth value. The output targets $\mathbf{Y}$ include the corresponding position coordinates and RGB values $\mathbf{y}_j = (x_j, y_j, z_j, r_j, g_j, b_j)$.
\noindent Notably, $\mathbf{x}_{i,j}$ is the entry of $\mathbf{X}$ indexed by the subscripts $i$ and $j$, and $\mathbf{y}_j$ is the $j$-th entry of $\mathbf{Y}$.

We then define the MOGP model as:
\begin{equation}
    \mathbf{Y} \sim \mathcal{MOGP}\big(\mathbf{m}(\mathbf{x}), \mathbf{K}(\mathbf{x}, \mathbf{x}')\big),
\end{equation}
where \( \mathbf{m}(\mathbf{x}) \) is the mean function and \( \mathbf{K}(\mathbf{x}, \mathbf{x}') \) is the covariance function:
\begin{equation}
    \mathbf{m}(\mathbf{x}) = \begin{bmatrix} 
    m_1(\mathbf{x}) & m_2(\mathbf{x}) & \dots & m_6(\mathbf{x}) 
    \end{bmatrix}^T,
\end{equation}
\begin{equation}
    \mathbf{K}(\mathbf{x}, \mathbf{x}') = \begin{bmatrix}
    k_{11}(\mathbf{x}, \mathbf{x}') & \cdots & k_{16}(\mathbf{x}, \mathbf{x}') \\
    \vdots & \ddots & \vdots \\
    k_{61}(\mathbf{x}, \mathbf{x}') & \cdots & k_{66}(\mathbf{x}, \mathbf{x}')
    \end{bmatrix}.
\end{equation}

\textbf{Loss Function and Optimization.} For MOGP training, we employ a combined loss function that integrates negative log marginal likelihood and L2 regularization as shown in equation (\ref{eq:total-loss}).
% , where the L2 regularization is implemented as the weight decay in the Adam optimizer. 
Such an optimisation approach balances data likelihood maximization and regularization, leading to a more robust and generalizable MOGP model. 
\begin{equation}\label{eq:total-loss}
\mathcal{L}_{\text{total}} = -\log p(\mathbf{y} | \mathbf{X}) + \lambda \sum_{i=1}^{N} \| \theta_i \|_2^2,
\end{equation}
\noindent
where \(p(\mathbf{y} | \mathbf{X})\) represents the marginal likelihood of the observed output given the inputs, $\lambda \sum_{i=1}^{N} \| \theta_i \|_2^2$ is the L2 regularization term, with $\lambda = 10^{-6}$ representing the weight decay. The L2 regularization helps prevent overfitting by penalizing large weights in the model parameters $\theta$.

To address the challenges posed by the sparse point clouds obtained through SfM,
% using multi-view feature extraction and matching, which can lead to non-uniform density, noisy observations, and localized discontinuities. 
we employ a generalized Radial Basis Function (RBF) kernel, specifically the Matérn kernel for training our MOGP model. In contrast to the standard RBF kernel, the Matérn kernel introduces a smoothness parameter \(\nu\), which governs the differentiability and local variations of the latent function, and ends up with a better trade-off between smoothness and computational efficiency. Formally, the Matérn kernel between two points \(\mathbf{x}\) and \(\mathbf{x}'\) is defined as:
\begin{equation}\label{eq:matern-kernel}
    k_{\nu}(\mathbf{x}, \mathbf{x}') \;=\; 
    \sigma^{2}
    \frac{2^{\,1-\nu}}{\Gamma(\nu)} 
    \bigl(\sqrt{2\nu}\,\|\mathbf{x} - \mathbf{x}'\|\bigr)^{\nu}\,
    K_{\nu}\!\Bigl(\sqrt{2\nu}\,\|\mathbf{x} - \mathbf{x}'\|\Bigr),
\end{equation}
where \(\Gamma(\cdot)\) is the Gamma function, \(\sigma^2\) is the variance, and \(K_{\nu}(\cdot)\) is a modified Bessel function of the second kind. The parameter \(\nu\) modulates how smooth the function can be: smaller \(\nu\) values permit abrupt transitions, while larger \(\nu\) values enforce smoother spatial changes. Details of tuning \(\nu\) on various datasets can be found in Table~\ref{tab:PerformanceMetrics}. Our MOGP training procedure can be found in Appendix~\ref{MOGP training process} Algorithm~\ref{alg:mogp}.



\subsection{Point Cloud Densification}
\label{Point Cloud Densification}
The process of point cloud densification involves an adaptive neighbourhood-based sampling strategy, which selectively identifies pixels as candidate inputs for the MOGP model (in section~\ref{MultiOutputGaussianProces}). The MOGP model subsequently predicts additional point clouds and applies uncertainty filtering to enhance the density.

\textbf{Adaptive Sampling Strategy.} To adaptively generate sampling pixels within the image domain, we introduce an adaptive neighbourhood-based sampling mechanism that places samples in circular neighbourhoods surrounding each available training point \((u_i, v_i)\). Specifically, we define a set of $N$ sampled pixels \(\mathcal{\tilde{P}}\) as follows:
\begin{align}
\mathcal{\tilde{P}}
=\;& \bigcup_{i=1}^N \bigcup_{j=1}^M 
\Bigl\{
\Bigl(\frac{u_i + r \cos \theta_j}{W},\, \frac{v_i + r \sin \theta_j}{H}\Bigr)\nonumber\\
&\quad \Bigm|\,
(u_i,\, v_i) \in \mathcal{V}_{i}
\Bigr\},
\label{eq:dynamic_sampling}
\end{align}
where 
% \(\mathcal{P}_{i}\) denotes the set of valid sampling points 
% associated with the \(i\)-th training point, and
$\theta_j \in [0, 2\pi)$ are uniformly distributed angles that control the sampling directions, $r = \beta \cdot \min(H, W)$ is the adaptive movement radius,  $\beta \in (0, 1)$ controls the sampling scale, \(W\) and \(H\) denote the image width and height, respectively. 
$W$ and $H$ are used to normalize all samples to the range \([0,1]\) in equation (\ref{eq:dynamic_sampling}). The parameter $M$ determines the angular resolution of the sampling process. For each sampled pixel $(u_i,v_i)$, the corresponding depth value $D_i$ is retrieved from the depth image $\mathcal{D}$. 

This adaptive procedure inherently controls spatial coherence by balancing the need for densification while avoiding redundant sampling. It selects regions near the training data to ensure that the test samples remain within the learned distribution of MOGP, mitigating extrapolation errors. Simultaneously, it prevents over-sampling by maintaining a distance between samples so that the generated points are not in the immediate proximity of existing Gaussians, thus avoiding their mergers within the adaptive density control framework of 3DGS.

\textbf{Uncertainty-Based Filtering.} The MOGP model then takes the sampled pixels $\mathcal{\tilde{P}}$ as inputs and provides inferred point clouds $\widehat{\mathcal{P}}$ with uncertainty (variances). To identify and filter out low-quality predictions, we employ an empirical distribution of the average variances in the RGB channels, where we choose \(R^2\) defined in equation (\ref{r2}) as the quantile level, the details of which can be found in Table~\ref{tab:PerformanceMetrics}. 
Let \(\widehat{N}\) be the total number of inferred points in \(\widehat{\mathcal{P}}\), and \(\mathbf{\Sigma} \in \mathbb{R}^{\widehat{N} \times 6}\) be the predicted variance, we compute the average variance over the RGB channels as they change dramatically as shown in Figure~\ref{fig:pcd}, indicating high uncertainty. To refine the densified point clouds, we rank the average variances \(\{\bar{\sigma}_i^2\}_{i=1}^{\widehat{N}}\) in ascending order and determine the filtering threshold \(\tau\) as $\tau = \bar{\sigma}^2_{\lceil R^2 \cdot \widehat{N} \rceil}$, where \( R^2 \) is the variance quantile threshold and \( \lceil \cdot \rceil \) denotes the ceiling function. Any point with an average variance exceeding \( \tau \) is removed to maintain geometric consistency. Finally, the filtered densified point clouds \(\mathcal{\tilde{P}}^*\) are formed by uniting the original training points with all inferred points: 
$\tilde{\mathcal{P}}^*=\tilde{\mathcal{P}} \cup \widehat{\mathcal{P}}.$


An overview of the uncertainty-based filtering procedure can be found in Appendix~\ref{MOGP training process} Algorithm~\ref{alg:dynamic_sampling_filtering}.
\section{Experiments}
\subsection{Dataset and Implementation Details}
\label{Dataset and Implementation Details}
\textbf{Dataset.} Previous approaches to SfM often overlook the explicit correspondence between 2D image pixels and sparse 3D point clouds, we address this gap by introducing the Pixel-to-Point-Cloud dataset. This dataset provides a detailed mapping between image pixels and their corresponding 3D point clouds, thus facilitating more accurate modelling and enabling the training of our MOGP model. We further conduct a comprehensive evaluation of our method on dataset spanning NeRF Synthetic~\cite{mildenhall2021nerf}, Mip-NeRF 360~\cite{barron2022mip}, and Tanks and Temples~\cite{knapitsch2017tanks}. These benchmarks are popular in 3D reconstruction and novel-view synthesis due to their diverse characteristics, including high-fidelity 3D models, large-scale outdoor scenes with intricate scenes like clustered leaves, and lighting condition variations. By leveraging our new dataset and evaluation framework, we demonstrate the effectiveness of our approach in advancing point cloud densification and improving reconstruction fidelity across varied and challenging scenarios. 

\textbf{Implementation Details} The MOGP-based densification process in GP-GS can be regarded as a plug-and-play module and can be integrated into any 3D reconstruction framework based on SfM sparse point clouds. We have integrated the process into 3DGS*, one of the state-of-the-art 3D reconstruction models, and compared their performance over the datasets mentioned earlier following 3DGS*’s training hyperparameters.
% as baseline for comparison, and following 3DGS’s training hyperparameters. 
Our MOGP models are trained for 1,000 iterations across all scenes, we set L2 regularization weight \(\lambda = 10^{-6}\), learning rate $l = 0.01$, dynamic sampling resolution $M = 8$, adaptive sampling radius $r = 0.25$. All experiments are conducted on an RTX 4080 GPU.

\textbf{Metrics.} The evaluation of the GP-GS is twofold, i.e., the evaluation of the SfM sparse point densification, and the evaluation of the novel view rendering performance. For densification, in line with prior studies on point cloud reconstruction and completion~\cite{yu2021pointr,yuan2018pcn}, we adopt the mean Chamfer Distance (CD) to measure the discrepancy between the predicted point clouds and the ground truth. Specifically, for a predicted point set $\mathcal{P}$ and its corresponding ground truth set $\mathcal{G}$, the CD between them is calculated as:
\begin{align}
d_{CD}(\mathcal{P}, \mathcal{G}) &= \frac{1}{|\mathcal{P}|} 
\sum_{\mathbf{p_i} \in \mathcal{P}} 
\min _{\mathbf{g_i} \in \mathcal{G}} \|\mathbf{p_i}-\mathbf{g_i}\| \nonumber \\
&\quad + \frac{1}{|\mathcal{G}|} 
\sum_{\mathbf{g_i} \in \mathcal{G}} 
\min _{\mathbf{p_i} \in \mathcal{P}} \|\mathbf{g_i}-\mathbf{p_i}\|.
\end{align}

% {\color{red}Are the following two for GP only?}
We also compute the Root Mean Squared Error (RMSE) and $R^2$~\cite{edwards2008r2} to show the robustness of GP-GS under various metrics. The RMSE is calculated to quantify the average squared deviation between the predicted and true values, using $\mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\mathbf{g_i} - \mathbf{p_i})^2}$, a lower RMSE value indicates a more accurate prediction. The \(R^2\) score captures the proportionate reduction in the residual variance:
\begin{equation}
\label{r2}
    R^2 = 1 - \frac{\sum_{i=1}^{n} (\mathbf{g_i} - \mathbf{p_i})^2}{\sum_{i=1}^{n} (\mathbf{g_i} - \bar{\mathbf{g_i}})^2},
\end{equation}

where \(\mathbf{g_i}\) is the true value,  \(\mathbf{p_i}\) is the predicted value, and $\bar{\mathbf{g_i}}$ is the mean of the true values. A higher \(R^2\) (closer to 1) indicates better Gaussian process model performance. 

For novel view rendering performance evaluation, we compare our method against state-of-the-art NVS approaches based on commonly used image-based metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM)~\cite{wang2004image}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable} on the rendered images in the test views.


\subsection{Results of Gaussian Processes}
\label{results on gp}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/mogp_3dgaussians.png}
    \caption{Qualitative comparison of 3D Gaussians during 3DGS training process. The left column shows the ground truth images. The middle column presents 3D Gaussians from 3DGS*, while the right column shows 3D Gaussians generated using our MOGP.}
    \label{fig: comparison_3dgaussians}
\end{figure*}
\textbf{Quantitative Results} To ensure the best performance of MOGP for densification, we evaluated MOGP model with different hyperparameters on our Pixel-to-Point-Cloud dataset, e.g., \(\nu\) with values such as  (\(\nu \in \{0.5, 1.5, 2.5\}\)). The data were split into an 80\% training subset and a 20\% testing subset, and the model was subsequently assessed using \(R^2\), RMSE, and CD as we mentioned in section~\ref{Dataset and Implementation Details}. The average outcomes of this analysis are summarized in Table~\ref{tab:matern-nu-study}, which shows that \(\nu = 0.5\) consistently yields the most favourable results. Consequently, we adopt \(\nu = 0.5\) as our default configuration for subsequent experiments. Detailed metrics across multiple datasets for this kernel selection procedure are presented in Appendix~\ref{results on gp} Table~\ref{tab:PerformanceMetrics}.
\begin{table}[t]
\centering
\footnotesize
\caption{Comparison of different Matern kernel smoothness parameters \(\nu\). The best results are highlighted.}
\label{tab:matern-nu-study}
\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{\textbf{\(\nu\)} Value} & \(R^2 \uparrow\) & \(\mathrm{RMSE} \downarrow\) & \(\mathrm{CD} \downarrow\) \\
\midrule
0.5  & \cellcolor{lightred}0.71 & \cellcolor{lightorange}0.13 & \cellcolor{lightgreen}0.21 \\
1.5  & 0.67 & 0.15 & 0.24 \\
2.5  & 0.61 & 0.14 & 0.26 \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}

To validate the effectiveness of our variance-based filtering algorithm, we analyze the variance distributions across multiple Pixel-to-Point datasets, including NeRF Synthetic, Tanks \& Temples, and Mip-NeRF 360, due to their diverse characteristics. 
% NeRF Synthetic provides high-fidelity 3D models with well-defined geometry, Tanks \& Temples features large-scale outdoor scenes with intricate structures such as clustered leaves, and Mip-NeRF 360 introduces challenges related to lighting condition variations. 
The reduction in variance in all datasets suggests that our filtering method generalizes well to different types of 3D scenes, improving overall point cloud quality and enabling more accurate scene representation, as shown in Figure~\ref{fig:filtered_variance}, the left column shows the spatial and colour variances before filtering on Tanks \& Temples (Truck), the right column presents the variances after applying our filter. The variances in spatial components $(x, y, z)$ and colour channels $(r, g, b)$ are significantly reduced, demonstrating that our filtering process effectively removes high-variance noisy points while preserving scene consistency. More results on various datasets can be found in Appendix~\ref{More MOGP Results}.

%for different datasets: NeRF Synthetic (Lego), Tanks \& Temples (Truck), and Mip-NeRF 360 (Flowers). The right column presents the variances after applying our filter. The variance in spatial components $(x, y, z)$ and color channels $(r, g, b)$ is significantly reduced, demonstrating that our filter effectively removes high-variance noisy points while preserving scene consistency.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/pointclouddense.png}
    \caption{Qualitative results of our point cloud densification approach.}
    \label{fig:pcd}
\end{figure}

\begin{figure}
    \centering
    %\vspace{1.5mm}
    \includegraphics[width=1\linewidth]{figures/variance_truck.png}
    \caption{Variance reduction comparison of our variance-based filtering algorithm on Tanks \& Temples (Truck).}
    \label{fig:filtered_variance}
\end{figure}

\textbf{Qualitative Results} Figure~\ref{fig:pcd} presents the comparison of the original SfM sparse point clouds (middle) with our densified point clouds (right) for NeRF Synthetic (lego) (middle) and Mip-NeRF 360 (counter and garden) (top and bottom). The densified point clouds produced by our method significantly improve spatial completeness, effectively reducing gaps and enhancing fine detail preservation. Notably, the improvements are most pronounced in regions where the original SfM point clouds exhibit severe sparsity. For example, in the NeRF Synthetic - lego scene, our approach reconstructs the intricate structural components of the excavator more faithfully. Similarly, in Mip-NeRF 360 - counter and garden, our method enhances surface continuity on the table while maintaining the overall geometric consistency of the scene. These qualitative results demonstrate the effectiveness of our approach in mitigating the limitations of sparse SfM reconstructions. 

Further validating our densified point cloud, we present Figure~\ref{fig: comparison_3dgaussians}, which shows 3D Gaussians during training. Our approach leads to more structured and coherent reconstructions, especially in complex regions with fine textures and occlusions. In the flower scene, our 3D Gaussian representation successfully reconstructs branches, trunks, and leaves on the left and right sides, while the original 3D Gaussian sphere remains blurry and lacks these details. Additionally, our method accurately preserves the flower shape in the middle, improving overall reconstruction fidelity. In the room scene, our method provides a clearer representation of the curtain folds on the left, whereas the original method produces irregularly shaped Gaussian spheres and visible artefacts. The highlighted insets further demonstrate that our method reduces visual artefacts and better preserves geometric consistency during training.

\subsection{Quantitative and Qualitative Results on NVS}
\label{Quantitative and Qualitative Results}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/render_qulity.pdf}
    \caption{Qualitative comparison of ours GP-GS and 3DGS*}
    \label{fig: comparison_quality}
\end{figure*}
To evaluate our method’s effectiveness in novel view synthesis, we conduct comprehensive experiments on datasets like Tanks \& Temples, Mip-NeRF 360, and NeRF Synthetic. We have compared the proposed GP-GS with the state-of-the-art 3DGS* using both quantitative metrics and qualitative visualizations.

\textbf{Quantitative Results}
Table~\ref{tab:results_tank}, ~\ref{tab:results_NeRF_Synthetic}, ~\ref{tab:results_mipnerf360} report PSNR, SSIM, and LPIPS scores across all datasets, respectively. Our method consistently achieves higher PSNR and SSIM values while maintaining lower LPIPS scores, indicating superior reconstruction quality and perceptual realism. The improvements are particularly evident in regions with high structural complexity and varying textures, such as dense foliage and intricate surfaces, where existing methods often suffer from over-smoothing and aliasing artefacts. Our approach effectively preserves fine details and enhances geometric consistency, even under challenging lighting conditions. Figure~\ref{fig: comparison_quality} provides a detailed visual comparison.
\begin{table}
\centering
\scriptsize
\caption{Comparison of Various Methods on Tanks and Temples}
\label{tab:results_tank}
\begin{tabular}{lccccccccc@{}}
\toprule
Dataset & \multicolumn{3}{c}{3DGS*} & \multicolumn{3}{c}{Ours} \\
\cmidrule(r){2-4} \cmidrule(l){5-7}
        & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  \\
\midrule
Truck  &    25.10   &    0.876   &   0.153    &     \cellcolor{lightred}25.18  &  \cellcolor{lightorange}0.877     &  0.153     \\
Train   &   22.13    &  0.819     &     0.202  &   \cellcolor{lightred} 22.22   &    \cellcolor{lightorange}0.823   & \cellcolor{lightgreen}0.199      \\
Average &    23.62   &    0.847   & 0.178      &  \cellcolor{lightred} 23.7    &  \cellcolor{lightorange}0.85     &    \cellcolor{lightgreen}0.176   \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}


\begin{table}
\centering
\scriptsize
\caption{Comparison of Various Methods on NeRF Synthetic}
\label{tab:results_NeRF_Synthetic}
\begin{tabular}{lccccccccc@{}}
\toprule
Dataset & \multicolumn{3}{c}{3DGS*} & \multicolumn{3}{c}{Ours} \\
\cmidrule(r){2-4} \cmidrule(l){5-7}
        & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  \\
\midrule
chair  &   30.99    &    0.978   &    0.024   &    30.99   &    0.978   &   0.024    \\
drums   &   25.52    & 0.940      &     0.065  &    25.52  &   0.940    &    0.065   \\
ficus  &    25.03   &     0.923  &    0.058   &     \cellcolor{lightred}25.20  &   0.923    &    0.058   \\
hotdog   &    34.53   &    0.979   &    0.037   &    \cellcolor{lightred}34.55   &   0.979    &    0.037   \\
lego  &    28.32   &   0.929    &  0.074     & \cellcolor{lightred}29.08      & \cellcolor{lightorange}0.936      & \cellcolor{lightgreen}0.067      \\
materials     &   \cellcolor{lightred}16.82    &   0.734    &     \cellcolor{lightgreen}0.239  &    16.68   &  \cellcolor{lightorange}0.736     &   0.241    \\
mic    &   18.90    &  0.855     &     0.144  &    \cellcolor{lightred}19.02   &  \cellcolor{lightorange}0.863     &      \cellcolor{lightgreen}0.123 \\
ship    & \cellcolor{lightred}27.08     &  0.848     &    0.157   &     27.05  &   \cellcolor{lightorange}0.849    &  \cellcolor{lightgreen}0.154     \\
Average    & 25.90    &  0.898     &    0.100   &     \cellcolor{lightred}26.01  &   \cellcolor{lightorange}0.901    &  \cellcolor{lightgreen}0.096    \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}



\begin{table}[t]
\centering
\scriptsize
\caption{Comparison of Various Methods on Mip-NeRF 360}
\label{tab:results_mipnerf360}
\begin{tabular}{lccccccccc@{}}
\toprule
Dataset & \multicolumn{3}{c}{3DGS*} & \multicolumn{3}{c}{Ours} \\
\cmidrule(r){2-4} \cmidrule(l){5-7}
        & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  \\
\midrule
bicycle  &  19.169     & 0.632      &  0.248     &   \cellcolor{lightred}19.219    &   \cellcolor{lightorange}0.634    &  0.248     \\
bonsai   &    18.66   &  0.579     &    0.458   &   \cellcolor{lightred}18.69    & \cellcolor{lightorange}0.581     &     \cellcolor{lightgreen}0.455  \\
counter  &   29.15    &   0.911    &    0.122   &     \cellcolor{lightred}29.17  &    \cellcolor{lightorange}0.915   &  \cellcolor{lightgreen}0.121     \\
garden   &     29.10  &   0.915    &   0.063    &    \cellcolor{lightred}29.38   &    \cellcolor{lightorange}0.920   &   \cellcolor{lightgreen}0.061    \\
kitchen  &  31.74     &  0.943     &  0.069     &  \cellcolor{lightred}32.28     &  \cellcolor{lightorange}0.948     &     \cellcolor{lightgreen}0.068  \\
room     &   31.78    &   0.934    &   0.126    &    \cellcolor{lightred}31.89   &  \cellcolor{lightorange}0.939     &     \cellcolor{lightgreen}0.121  \\
flowers    &  23.04     &   0.707    &     0.251  &    \cellcolor{lightred}23.16   & \cellcolor{lightorange}0.711      &   \cellcolor{lightgreen}0.246    \\
Average    &  26.09     &  0.803     &  0.191     &    \cellcolor{lightred}26.26   & \cellcolor{lightorange}0.807      &   \cellcolor{lightgreen} 0.189    \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table}

\textbf{Qualitative Results}
As shown in Figure~\ref{fig: comparison_quality}, we compare our method with 3DGS* on datasets Tanks \& Temples (Truck), Mip-NeRF 360 (Flowers and Bicycles), and NeRF Synthetic (Lego), we provide additional results in Appendix~\ref{morenvs} for each scene to further substantiate our conclusions. Our method demonstrates superior performance in NVS by effectively reconstructing fine-grained details, reducing artifacts, and enhancing occlusion handling. The visual comparisons highlight key challenges in 3DGS* rendering, including occlusions, high-frequency textures (e.g., object edges and surface details), and lighting variations. Notably, the zoom-in regions (marked with red and blue boxes) illustrate our model’s ability to recover fine structures more accurately than existing approaches. These results further validate the robustness of our method in handling complex scenes, offering improved perceptual quality and structural consistency across diverse datasets.

%\section{Ablation Study}
%\label{Ablation}
\section{Conclusion}
\label{conclusion}
%We Propose GP-GS, a novel framework that enhances 3DGS and improves rendering quality by addressing the limitations of sparse SfM reconstructions. Our approach formulates point cloud densification as a continuous regression problem using MOGP, which adaptively and uncertainty-guidedly densifies sparse SfM point clouds by learning mappings from 2D image pixels and depth priors to 3D positions and colors. Furthermore, we introduce an adaptive neighborhood-based sampling strategy to improve point cloud completeness while ensuring structured densification. To further refine the densified point clouds, we incorporate a variance-based filtering method, effectively removing high-uncertainty predictions and preserving geometric consistency. Extensive experiments on synthetic and real-world datasets show that GP-GS significantly improves 3D reconstruction quality, and can provide compact 3D Gaussian representation, especially in complex regions with densely packed objects or under challenging lighting conditions. Future work includes integrating temporal information for dynamic scenes and scaling to larger environments.
We propose GP-GS, a novel framework that enhances 3DGS initialization and improves rendering quality by addressing the limitations of sparse SfM reconstructions. Our method formulates point cloud densification as a continuous regression problem using MOGP, which adaptively densifies sparse SfM point clouds by learning mappings from 2D image pixels and depth priors to 3D positions and colours. We introduce an adaptive neighbourhood-based sampling strategy for structured densification and apply variance-based filtering to remove high-uncertainty predictions. Experiments on synthetic and real-world datasets show that GP-GS significantly improves 3D reconstruction quality, especially in densely packed or challenging lighting conditions. Future work includes integrating temporal information for dynamic scenes and scaling to larger environments.


\bibliography{aaai25}

\newpage
\appendix
\clearpage

\noindent


\onecolumn

\section{Appendix}
The Appendix provides additional details on our MOGP model training and testing. It also includes additional experimental results that are omitted from the main paper due to space constraints.
\subsection{MOGP Training and Dynamic Filtering Procedure}
\label{MOGP training process}
This section provides details on the training, dynamic sampling and uncertainty-based filtering process of our MOGP model for point cloud densification. The full MOGP training procedure is summarized in Algorithm~\ref{alg:mogp}, and the dynamic sampling and filtering process is detailed in Algorithm~\ref{alg:dynamic_sampling_filtering}.
\begin{algorithm}[th]
\caption{MOGP with Matérn Kernel}
\label{alg:mogp}

\textbf{Input:}  
RGB images \(\mathcal{I}\), sparse 3D points \(\mathcal{P}\), depth priors \(\mathcal{D}\), pixel correspondences \(\mathcal{V}\).  
Feature inputs \(\mathbf{X} \in \mathbb{R}^{n \times 3}\) , target outputs \(\mathbf{Y} \in \mathbb{R}^{n \times 6}\).

\textbf{Step 1: MOGP Definition}  
\begin{equation}
\mathbf{Y} \sim \mathcal{MOGP}(\mathbf{m}(\mathbf{x}), \mathbf{K}(\mathbf{x}, \mathbf{x}'))
\end{equation}
where \(\mathbf{m}(\mathbf{x})\) is the mean function and \(\mathbf{K}(\mathbf{x}, \mathbf{x}')\) is the covariance function.

\textbf{Step 2: Matérn Kernel}
\begin{equation}
k_{\nu}(\mathbf{x}, \mathbf{x}') = \sigma^{2} \frac{2^{1-\nu}}{\Gamma(\nu)}
(\sqrt{2\nu} \|\mathbf{x} - \mathbf{x}'\|)^{\nu} K_{\nu}(\sqrt{2\nu} \|\mathbf{x} - \mathbf{x}'\|)
\end{equation}
where \(\nu\) controls smoothness and \(\sigma^2\) defines variance.

\textbf{Step 3: Optimization}  
\textbf{Loss Function:}  
\begin{equation}
\mathcal{L}_{\text{total}} = -\log p(\mathbf{Y} | \mathbf{X}) + \lambda \sum_{i=1}^{N} \|\theta_i\|_2^2
\end{equation}
where \(\lambda = 10^{-6}\) is the L2 regularization weight.

\textbf{Gradient-Based Parameter Update:}  
\begin{itemize}
    \item Initialize: \(\theta = \{\sigma, \nu, \dots\}\).  
    \item \textbf{for} \(t = 1 \dots T\):
    \begin{itemize}
        \item Compute gradient \(\nabla_{\theta} \mathcal{L}_{\text{total}}\).
        \item Update:
        \begin{equation}
        \theta \leftarrow \theta - \eta \,\nabla_{\theta}\,\mathcal{L}_{\text{total}}
        \end{equation}
    \end{itemize}
\end{itemize}

\textbf{Output:}  
Optimized GP hyperparameters \(\theta\).  
Predicted outputs \(\widehat{\mathbf{Y}}\) for new inputs \(\widehat{\mathbf{X}}\) via the MOGP posterior.

\end{algorithm}
\begin{algorithm}[ptbh]
\caption{Dynamic Sampling and Filtering}
\label{alg:dynamic_sampling_filtering}

\textbf{Input:}  
Image size \((W, H)\), depth map \(D\), training pixels \(\{(u_i, v_i)\}_{i=1}^N\),  
angular resolution \(M\), scale factor \(\beta\), variance quantile threshold \(R^2\).  

\textbf{Step 1: Dynamic Sampling}  
\begin{enumerate}
  \item Compute adaptive sampling radius \(r = \beta \cdot \min(H, W)\).
  \item \textbf{for} each training pixel \((u_i, v_i)\):
    \begin{itemize}
      \item Generate \(M\) new samples in a circular neighborhood.
      \item Normalize \((u, v)\) and retrieve depth \(d\) from $\mathcal{D}$.
      \item Store \(\bigl(u, v, d\bigr)\) in sample set \(\mathcal{\tilde{P}}\).
    \end{itemize}
\end{enumerate}

\textbf{Step 2: GP Inference}  
Provide \(\mathcal{\tilde{P}}\) to MOGP to infer a densified point cloud \(\widehat{\mathcal{P}}\) with variance matrix \(\mathbf{\Sigma}\).

\textbf{Step 3: Variance-Based Filtering}  
\begin{enumerate}
  \item Compute mean RGB variance \(\bar{\sigma}^2\) for each inferred point.
  \item Set threshold \(\tau\) as the \(R^2\)-quantile of the variance distribution.
  \item Remove points with \(\bar{\sigma}^2 > \tau\).
\end{enumerate}

\textbf{Step 4: Final Point Cloud}  
\(\mathcal{\tilde{P}}^* \gets \mathcal{\tilde{P}} \cup \widehat{\mathcal{P}}\).  

\textbf{Output:} Densified and filtered point clouds \(\mathcal{\tilde{P}}^*\).

\end{algorithm}

\subsection{MOGP Training Loss}
\label{losscurves}
\begin{figure*}
    \centering    \includegraphics[width=0.8\linewidth]{figures/training_loss_curves.pdf}
    \caption{MOGP training loss curves for different datasets over 1,000 iterations, the loss converges as training progresses.}
    \label{fig:mogploss}
\end{figure*}
In this section, we present the training loss curves of our MOGP model across different datasets. The loss curves illustrate the convergence behaviour of the model during training, providing insight into its stability and optimization progress. Figure~\ref{fig:mogploss} shows the loss curves for various datasets over 1,000 iterations, including bicycle, garden, room, lego, kitchen, truck, flower, and drums. Our MOGP model demonstrates a smooth and consistent decline in loss, indicating effective learning and convergence.

\subsection{More MOGP Results}
\label{More MOGP Results}
\begin{table*}[tbph]
  \centering
  \footnotesize
  \caption{Performance Metrics for MOGP Model Using Different Matern Kernel Parameters. The best results are highlighted. One can see that $\nu = 0.5$ helps to produce the best densification results across various datasets.}
  \newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1.2pt\hskip\tabcolsep}}
   \label{tab:PerformanceMetrics}
    \begin{tabular}{llccccccccc}
        \toprule
        & & \multicolumn{3}{c}{$\nu = 0.5$} & \multicolumn{3}{c}{$\nu = 1.5$} & \multicolumn{3}{c}{$\nu = 2.5$} \\
        \cmidrule(r){3-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
        Main Dataset & Sub Dataset & $R^2$ $\uparrow$ & RMSE $\downarrow$ & CD $\downarrow$ & $R^2$ $\uparrow$ & RMSE $\downarrow$ & CD $\downarrow$ & $R^2$ $\uparrow$ & RMSE $\downarrow$ & CD $\downarrow$ \\
        \midrule
        \multirow{8}{*}{\textbf{NeRF Synthetic}} 
        & Lego               & \cellcolor{lightred}0.78 & \cellcolor{lightorange}0.12 & \cellcolor{lightgreen}0.18 & 0.75 & 0.13 & 0.20 & 0.69 & 0.15 & 0.21 \\
        & Chair              &\cellcolor{lightred} 0.63 & \cellcolor{lightorange}0.14 & \cellcolor{lightgreen}0.19 & 0.57 & 0.15 & 0.28 & 0.48 & 0.15 & 0.35 \\
        & Drums              & 0.52 & 0.22 & \cellcolor{lightgreen}0.61 & 0.52 & 0.22 & 0.64 & 0.52 & 0.22 & 0.64 \\
        & Hotdog             & \cellcolor{lightred}0.65 &\cellcolor{lightorange} 0.09 & \cellcolor{lightgreen}0.16 & 0.52 & 0.51 & 0.43 & 0.38 & 0.12 & 0.22 \\
        & Ficus             & \cellcolor{lightred}0.52 & \cellcolor{lightorange}0.22 & \cellcolor{lightgreen}0.68 & 0.46 & 0.23 & 0.77 & 0.45 & 0.23 & 0.77 
        \\
        & Materials             & \cellcolor{lightred}0.53 & \cellcolor{lightorange}0.19 & \cellcolor{lightgreen}0.24 & 0.49 & 0.20 & 0.24 & 0.48 & 0.20 & 0.39 
        \\
        & Ship             & \cellcolor{lightred}0.55 & \cellcolor{lightorange}0.11 & \cellcolor{lightgreen}0.22 & 0.51 & 0.12 & 0.23 & 0.25 & 0.14 & 0.23 
        \\
        & Mic             & \cellcolor{lightred}0.45 & 0.23 & \cellcolor{lightgreen}0.69 & 0.42 & 0.23 & 0.70 & 0.43 & 0.23 & 0.71 
        \\
        & Average             & \cellcolor{lightred}0.58 & \cellcolor{lightorange}0.17 & \cellcolor{lightgreen}0.13 & 0.53 & 0.22 & 0.44 & 0.46 & 0.18 & 0.44
        \\
        \midrule
        \multirow{2}{*}{\textbf{Tanks \& Temples}} 
        & Truck               & \cellcolor{lightred}0.78 & \cellcolor{lightorange}0.12 & \cellcolor{lightgreen}0.18 & 0.75 & 0.13 & 0.20 & 0.69 & 0.15 & 0.21 \\
        & Train              & \cellcolor{lightred}0.78 & \cellcolor{lightorange}0.10 & \cellcolor{lightgreen}0.05 & 0.74 & \cellcolor{lightorange}0.10 & 0.07 & 0.73 & 0.11 & 0.07 
        \\
        & Average             & \cellcolor{lightred}0.78 & \cellcolor{lightorange}0.11 & \cellcolor{lightgreen}0.11 & 0.75 & 0.12 & 0.14 & 0.71 & 0.13 & 0.14
        \\
        \midrule
        \multirow{8}{*}{\textbf{Mip-NeRF 360}} 
        & Bicycle               & \cellcolor{lightred}0.74 & \cellcolor{lightorange}0.10 & \cellcolor{lightgreen}0.12 & 0.70 & 0.11 & 0.12 & 0.47 & 0.16 & 0.41 \\
        & Bonsai              & \cellcolor{lightred}0.77 & \cellcolor{lightorange}0.12 & \cellcolor{lightgreen}0.19 & 0.72 & 0.13 & 0.23 & 0.70 & 0.13 & 0.25 \\
        & Counter              & \cellcolor{lightred}0.85 & \cellcolor{lightorange}0.10 & \cellcolor{lightgreen}0.13 & 0.82 & 0.11 & \cellcolor{lightgreen}0.13 & 0.82 & 0.11 & 0.14 \\
        & Garden             & \cellcolor{lightred}0.66 & \cellcolor{lightorange}0.12 & \cellcolor{lightgreen}0.13 & 0.60 & 0.13 & \cellcolor{lightgreen}0.13 & 0.57 & 0.13 & 0.14 \\
        & Kitchen             & \cellcolor{lightred}0.82 & \cellcolor{lightorange}0.08 & \cellcolor{lightgreen}0.05 & 0.79 & \cellcolor{lightorange}0.08 & 0.06 & 0.79 & 0.09 & 0.08 
        \\
        & Room             & \cellcolor{lightred}0.74 & \cellcolor{lightorange}0.097 & \cellcolor{lightgreen}0.136 & 0.71 & 0.104 & 0.143 & 0.68 & 0.109 & 0.156 
        \\
        & Flowers             & \cellcolor{lightred}0.75 &\cellcolor{lightorange} 0.115 & \cellcolor{lightgreen}0.15 & 0.72 & 0.122 & 0.214 & 0.69 & 0.125 & 0.152
        \\
        & Average             & \cellcolor{lightred}0.77 & \cellcolor{lightorange}0.10 & \cellcolor{lightgreen}0.13 & 0.72 & 0.11 & 0.15 & 0.67 & 0.12 & 0.19
        \\
        \bottomrule
    \end{tabular}
\end{table*}


In this section, we present additional experimental results to further evaluate the effectiveness of our MOGP model. Specifically, we analyze the impact of different Matérn kernel parameters on performance across various datasets and assess the effectiveness of our variance-based filtering approach. To validate our uncertainty-based filtering algorithm, we examine the variance distributions before and after applying the filter. 

\textbf{Evaluation of $\nu$ values on Pixel-to-Point-Cloud dataset.} 
The Matérn kernel introduces a smoothness parameter \(\nu\), enabling adjustable local variations depending on the scene. Through iterative gradient updates, we refine the kernel hyperparameters to enhance both accuracy and generalization. We show the details on various datasets in Table~\ref{tab:PerformanceMetrics} to choose the best performance $\nu$ value for the kernel.

\textbf{Effectiveness of Variance-Based Filtering.} Figure~\ref{fig:morefiltered_variance} illustrates that our filter effectively removes high-variance noisy points while maintaining scene consistency. Additional results on various datasets, including NeRF Synthetic (Lego), Tanks \& Temples (Truck), and Mip-NeRF 360 (Flowers), are presented. The right column displays the variances after applying our filter. The variance in spatial components $(x, y, z)$ and colour channels $(r, g, b)$ is significantly reduced, demonstrating the filter's efficacy in removing high-variance noisy points while preserving scene consistency.

\begin{figure*}
    \centering    \includegraphics[width=\linewidth]{figures/variance.png}
    \caption{Variance reduction comparison of our variance-based filtering Algorithm. The left and right columns display the variances before and after applying our filter. One can see that the variance in spatial components $(x, y, z)$ and RGB channels $(r, g, b)$ is significantly reduced, demonstrating the filter’s effectiveness in eliminating high-variance noise while preserving scene consistency and structural integrity.}
    \label{fig:morefiltered_variance}
\end{figure*}

These results combined with the results we presented in this paper further demonstrate the robustness and generalizability of our proposed MOGP-based point cloud densification method. By leveraging uncertainty estimation, our approach ensures a more structured and reliable reconstruction of complex 3D scenes.

\subsection{More Novel View Synthesis Results}
\label{morenvs}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/more_gs_res.pdf}
    \caption{Further Qualitative comparison examples of ours GP-GS and 3DGS*. One can see that GP-GS reconstructs intricate structures like branches, trunks, and leaves more accurately, while 3DGS* produces blurry results. Similar results are observed in the truck and kitchen scenes, i.e., GP-GS better preserves fine details, reducing artefacts and improving spatial consistency. }
    \label{fig: comparison_quality_more}
\end{figure*}
In this section, we present more qualitative comparison results of GP-GS and 3DGS*.

As show in Figure~\ref{fig: comparison_quality_more}. Our GP-GS (right) outperforms 3DGS* (middle) in complex regions with densely packed objects (e.g., foliage) and challenging lighting conditions. Notably, in the flower scene, GP-GS reconstructs intricate structures like branches, trunks, and leaves more accurately, while 3DGS* produces blurry results. Similarly, in the truck and kitchen scenes, our method better preserves fine details, reducing artefacts and improving spatial consistency. These results further validate the effectiveness of GP-GS in enhancing 3D Gaussian splitting for reconstruction.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}



