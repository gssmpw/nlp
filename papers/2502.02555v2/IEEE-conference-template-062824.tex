\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{array, multirow}
% \usepackage[font=small, skip=5pt]{caption}
% \captionsetup{belowskip=2pt} % Set the space below captions to 2pt
\setlength{\tabcolsep}{9pt}
\renewcommand{\arraystretch}{1.3}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late Dynamic Contrast Enhanced Prostate MRI Synthesis.\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Divya Bharti$^1{*}\quad$ Sriprabha Ramanarayanan$^{1,2}\quad$ Sadhana S$^1\quad$ Kishore Kumar M$^1\quad$ Keerthi Ram$^2\quad$ \\
Harsh Agarwal$^3\quad$ Ramesh Venkatesan$^3\quad$ Mohanasankar Sivaprakasam$^{1,2}\quad$}
\and 
\hspace{50mm}\textit{$^1$Indian Institute of Technology Madras (IITM), India}\\
\hspace{50mm}\textit{$^2$Healthcare Technology Innovation Centre (HTIC), India}\\
\hspace{50mm}\textit{$^3$GE HealthCare (GE), India}\\

\hspace{50mm}{\tt\small${*}$b13.divya@gmail.com}}


% \author{\IEEEauthorblockN{Author1$^1{*}\quad$ Author2 $^{1}\quad$ Author3$^1\quad$ Author4$^1\quad$ Author5$^2\quad$ \\
% Author6$^3\quad$ Author7$^3\quad$ Author8$^{1,2}\quad$}
% \and 
% \hspace{50mm}\textit{$^1$dept}\\
% \hspace{50mm}\textit{$^2$dept}\\
% \hspace{50mm}\textit{$^3$dept}\\}

% \hspace{50mm}{\tt\small${*}$b13.divya@gmail.com}}



% [12pt]
% $^1$Indian Institute of Technology Madras (IITM),
% India\\[4pt]
% $^2$Healthcare Technology Innovation Center (HTIC),
% India\\[4pt]
% $^3$GE HealthCare (GE), India \\ [4pt]

% \IEEEauthorblockA{\textit{Indian Institute of Technology, Madras} \\
% \textit{Indian Institute of Technology, Madras}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{Sriprabha Ramanarayanan$^{1}$}
% % \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% % \textit{name of organization (of Aff.)}\\
% % City, Country \\
% % email address or ORCID}
% \and
% \IEEEauthorblockN{Sadhana S$^1$}
% % \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% % \textit{name of organization (of Aff.)}\\
% % City, Country \\
% % email address or ORCID}
% \and
% \IEEEauthorblockN{Kishore Kumar M$^{1}$}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{Keerthi Ram$^2$}
% % \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% % \textit{name of organization (of Aff.)}\\
% % City, Country \\
% % email address or ORCID}
% \and
% \IEEEauthorblockN{Harsh Agarwal$^{3}$}
% % \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% % \textit{name of organization (of Aff.)}\\
% % City, Country \\
% % email address or ORCID}
% \and 
% \IEEEauthorblockN{Ramesh Venkatesan$^{3}$}
% \and
% \IEEEauthorblockN{Mohanasankar Sivaprakasam$^{1,2}$}



\maketitle

\begin{abstract}
Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical imaging technique that plays a crucial role in the detailed visualization and identification of tissue perfusion in abnormal lesions and radiological suggestions for biopsy. However, DCE-MRI involves the administration of a Gadolinium-based (Gad) contrast agent, which is associated with a risk of toxicity in the body. Previous deep learning approaches that synthesize DCE-MR images employ unimodal non-contrast or low-dose contrast MRI images lacking focus on the local perfusion information within the anatomy of interest. We propose AAD-DCE, a generative adversarial network (GAN) with an aggregated attention discriminator module consisting of global and local discriminators. The discriminators provide a spatial embedded attention map to drive the generator to synthesize early and late response DCE-MRI images. Our method employs multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient (ADC), and T1 pre-contrast for image synthesis. Extensive comparative and ablation studies on the ProstateX dataset show that our model (i) is agnostic to various generator benchmarks and (ii) outperforms other DCE-MRI synthesis approaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE for early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late response, and (ii) emphasize the importance of attention ensembling. Our code is available at https://github.com/bhartidivya/AAD-DCE.
\end{abstract}

\begin{IEEEkeywords}
DCE-MRI, Aggregated Attention, Multimodal, Prostate Cancer
\end{IEEEkeywords}
\vspace{-3mm}
\section{Introduction}
Dynamic contrast-enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical image scanning technique that quantifies tumor vasculature and perfusion characteristics \cite{b1}. The angiogenesis is captured by injecting a Gadolinium (Gad)-based contrast agent. This contrast agent exhibits a notable characteristic of rapid wash-in and wash-out kinetics occurring within a few seconds of Gad injection in the suspicious tissues, distinguishing it from normal healthy tissues. DCE-MRI plays a crucial role in prostate imaging, where the radiologists use a PI-RADS\footnote{https://www.acr.org/-/media/ACR/Files/RADS/Pi-RADS/PIRADS-v2-1.pdf} score (in the scale 1 to 5) to assess the cancer severity by visualizing non-contrast images (T2 or diffusion-weighted images (DWI)). A score of 3 suggests DCE-MRI acquisition, which helps to reduce unnecessary biopsies by 25\% \cite{b10}, avoiding over-diagnosis of insignificant cancers. However, Gad-based contrast agents cause patient discomfort and other contraindications like Nephrogenic Systemic Fibrosis \cite{b2}. Therefore, there is a need to decrease the dosage or refrain from using a Gad-based contrast agent in DCE-MRI.
% Furthermore, in prostate MRI, radiologists use PI-RADS\footnote{https://www.acr.org/-/media/ACR/Files/RADS/Pi-RADS/PIRADS-v2-1.pdf} scores (in the scale of 1-5) by visualizing non-contrast images (T2 or diffusion weighted images) to assess prostate cancer severity.
% \setlength{\belowcaptionskip}{-5mm}
% \begin{figure}[t]
% \vspace{-2mm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{new_block_fig.pdf}
%     \caption{(a) Concept diagram of DCE-GAN. The multimodal non-contrast inputs, including Apparent Diffusion Coefficient (ADC), with embedded attention map, synthesize early and late response DCE-MR images. (b) Previous DCE-MRI synthesis approaches \cite{b6},\cite{b7} are based on adversarial training objectives lacking focus on important regions on the anatomy of interest. The proposed approach involves a discriminator-based global and local aggregated attention mechanism, further driving the generator.}
%     \label{fig:block diagram}
% \end{figure}

% \setlength{\belowcaptionskip}{-5mm}
% \begin{figure}[t]
% \vspace{-2mm}
%     \centering
%     \includegraphics[width=1\linewidth]{new_block_fig (1).pdf}
%     \caption{(a) Concept diagram of AAD-DCE. Multimodal non-contrast inputs, with aggregated attention maps, synthesize early and late response DCE-MRI. (b) Previous DCE-MRI synthesis methods use adversarial training without focusing on key anatomical regions. The proposed approach integrates a discriminator-based global and local attention mechanism to enhance the generator's performance.}
%     \label{fig:block diagram}
% \end{figure}

\setlength{\belowcaptionskip}{-5mm}
\begin{figure}[t]
\vspace{-2mm}
    \centering
    \includegraphics[width=0.9\linewidth]{new_block_fig_final.pdf}
    \caption{(a) Concept diagram of AAD-DCE. Multimodal non-contrast inputs, with aggregated attention maps, synthesize early and late response DCE-MRI. (b) Previous DCE-MRI synthesis methods use adversarial training without focusing on key anatomical regions. The proposed approach integrates a discriminator-based global and local attention mechanism to enhance the generator's performance.}
    \label{fig:block diagram}
\end{figure}


% Concept diagram of DCE-GAN. The multimodal non-contrast inputs, including Apparent Diffusion Coefficient (ADC), with embedded attention map, synthesize early and late response DCE-MR images. (b) Previous DCE-MRI synthesis approaches \cite{b6},\cite{b7} are based on adversarial training objectives lacking focus on important regions on the anatomy of interest. The proposed approach involves a discriminator-based global and local aggregated attention mechanism, further driving the generator.



% DCE-MRI plays a crucial role in prostate imaging, where the radiologists use a PI-RADS\footnote{https://www.acr.org/-/media/ACR/Files/RADS/Pi-RADS/PIRADS-v2-1.pdf} score (in the scale 1 to 5) to assess the cancer severity by visualizing non-contrast images (T2 or diffusion-weighted images (DWI)). A score of 3 suggests DCE-MRI acquisition, which helps to reduce unnecessary biopsies by 25\% \cite{b10}, avoiding over-diagnosis of insignificant cancers. 
% DCE-MRI is crucial when the score is 3, reducing unnecessary biopsies by 25\% \cite{b10}, avoiding over-diagnosis of insignificant cancers. 
% However, Gad-based contrast agents cause patient discomfort and other contraindications like Nephrogenic Systemic Fibrosis \cite{b2}. Therefore, there is a need to decrease the dosage or refrain from using a Gad-based contrast agent in DCE-MRI.

% DCE-MRI plays a crucial role in prostate imaging, where the radiologists use a PI-RADS\footnote{https://www.acr.org/-/media/ACR/Files/RADS/Pi-RADS/PIRADS-v2-1.pdf}





% For instance, in prostate DCE-MRI, a careful analysis of ProstateX dataset shows that of the patient data is assessed with an intermediate  score of 3.0 by the radiologist and DCE-MRI is utilized to upgrade the score from 3 to 4 or 5, affirming that DCE-MRI is an essential modality for diagnostic decision making.
% However, Gad-based contrast agents are expensive and cause patient discomfort and other contraindications like Nephrogenic Systemic Fibrosis \cite{b2}. Therefore, there is a need to decrease the dosage or refrain from using a Gadolinium-based contrast agent in DCE-MRI.
% \vspace{-5mm}

% \begin{figure}
% \vspace{-2mm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{new_block_fig.pdf}
%     \caption{(a) Concept diagram of DCE-GAN. The multimodal non-contrast inputs, including Apparent Diffusion Constant (ADC), with aggregated attention map, synthesize early and later response DCE-MR images. (b) Previous DCE-MRI synthesis approaches \cite{b6},\cite{b7} are based on adversarial training objectives lacking focus on important regions on the anatomy of interest. The proposed approach involves discriminator-based global and local aggregated attention mechanism, further driving the generator.}
%     \label{fig:block diagram}
% \end{figure}

% \begin{figure}[t]
% \vspace{-2mm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{new_block_fig.pdf}
%     \caption{(a) Concept diagram of DCE-GAN. The multimodal non-contrast inputs, including Apparent Diffusion Coefficient (ADC), with embedded attention map, synthesize early and late response DCE-MR images. (b) Previous DCE-MRI synthesis approaches \cite{b6},\cite{b7} are based on adversarial training objectives lacking focus on important regions on the anatomy of interest. The proposed approach involves discriminator-based global and local aggregated attention mechanism, further driving the generator.}
%     \label{fig:block diagram}
% \end{figure}


Deep learning methods have been explored for MRI reconstruction tasks (\cite{20},\cite{21}) and to reduce or eliminate the use of Gad-based DCE-MRI. The Generative Adversarial Network (GAN) \cite{b3} using a 3D U-Net-like generator produces 3D isotropic contrast-enhanced images from a 2D T2-flair image stack by adopting spatial pyramid pooling, enhanced residual blocks, and deep supervision. Retina U-Net \cite{b4} extracts the semantic features from non-contrast brain MR images and uses a synthesis module for contrast-enhanced images. The convolutional neural network (CNN) based network \cite{b5} generates full-dose late-response images from pre-contrast and low-contrast images for brain MRI. A U-Net-based conditional-GAN \cite{b6} with residual loss function synthesizes late-response images from early-response breast MRI images. DCE-Diff \cite{b16} is a diffusion-based generative model that maps non-contrast to contrast-enhanced prostate images.


% \begin{figure*}
%     \centering
%      {\includegraphics[width=1\linewidth]{concept_diagram_icassp_new (1).pdf}}
%     \caption{(a) Overall pipeline of the proposed AAD-DCE comprises a Generator and Aggregated Attention Discrimiator (AAD). (b) AAD  with local and global attention discriminators, $D_{LA}$ and $D_{GA}$, respectively, utilizing (c) Attention Discriminator (AD) architecture. (d) Embedded attention map $M_{x}$ with local attention map $M_{L}$ embedded on global attention map $M_{G}$.}
%     \label{fig:network architecture}
% \end{figure*}


% \begin{figure*}
%     \centering
%      {\includegraphics[width=1\linewidth]{concept_diagram_icassp_new_final.pdf}}
%     \caption{(a) Overall pipeline of the proposed AAD-DCE comprises a Generator and Aggregated Attention Discrimiator (AAD). (b) AAD  with local and global attention discriminators, $D_{LA}$ and $D_{GA}$, respectively, utilizes (c) Attention Discriminator (AD) architecture. (d) Embedded attention map $M_{x}$ with local attention map $M_{L}$ embedded on global attention map $M_{G}$.}
%     \label{fig:network architecture}
% \end{figure*}


% \begin{figure*}
%     \centering
%      {\includegraphics[width=1\linewidth]{concept_diagram_icassp_new_final (1).pdf}}
%     \caption{(a) Overall pipeline of the proposed AAD-DCE comprises a Generator and Aggregated Attention Discrimiator (AAD). (b) AAD  with local and global attention discriminators, $D_{LA}$ and $D_{GA}$, respectively, utilizes (c) Attention Discriminator (AD) architecture. (d) Embedded attention map $M_{x}$ with local attention map $M_{L}$ embedded on global attention map $M_{G}$.}
%     \label{fig:network architecture}
% \end{figure*}


% \begin{figure*}
%     \centering
%     {\includegraphics[width=0.95\linewidth]{concept_diagram_icassp_new_final (2).pdf}}
%     \caption{(a) AAD-DCE architecture with a generator and a Aggregated Attention Discrimiator (AAD) module. (b) AAD  with local and global attention discriminators, $D_{LA}$ and $D_{GA}$, respectively, utilizes (c) Attention Discriminator (AD) architecture. (d) Embedded attention map $M_{x}$.}
%     \label{fig:network architecture}
% \end{figure*}


\begin{figure*}
    \centering
     {\includegraphics[width=0.92\linewidth]{concept_diagram.pdf}}
    \caption{(a) AAD-DCE architecture with a generator and an Aggregated Attention Discriminator (AAD) module. (b) AAD  with local and global attention discriminators, $D_{LA}$ and $D_{GA}$, respectively, utilizes (c) Attention Discriminator (AD) architecture. (d) Embedded attention map $M_{x}$.}
    \label{fig:network architecture}
\end{figure*}





However, the former methods (\cite{b3},\cite{b4},\cite{b5},\cite{b6}) either depend on low-dose images or do not completely utilize the perfusion information from the non-contrast DCE-MRI data. In DCE-MRI, the Apparent Diffusion Coefficient (ADC) map computed using DWI carries useful information needed for generating the perfusion information. Secondly, the GAN-based methods adopt a global discriminator that is not specialized to consider the importance of local distribution related to the perfusion information within the anatomy of interest. DCE-Diff focuses on structural correlation without local context and suffers from higher computation costs for training and inference.

The goal of this work is to synthesize DCE-MRI early and late response images leveraging the complementary information from multimodal non-contrast MRI inputs and provide nuanced and detailed feature representation for perfusion (Fig. \ref{fig:block diagram}). Our method employs a GAN framework where the generator obtains additional guidance via an attention map computed and fed back from its discriminator. The attention map helps to focus more on the most discriminative areas between abnormal and healthy regions in the anatomy under study. Our method uses two discriminators to learn (i) the global structural correlation between the pre-contrast MRI and non-contrast T2 Weighted (T2W) MR images and (ii) local perfusion information using the ADC maps. In contrast, previous methods (TSGAN\cite{b7}, ReconGLGAN\cite{b8}) use two discriminators for optimization without attention guidance.
% with only an adversarial training perspective.
% The focus of this work is to synthesize DCE-MRI  early and late response images leveraging the complementary information from multimodal non-contrast MRI input and using two discriminators to learn (i) the global structural correlation between the pre-contrast MRI and non-contrast T2 MR images, and (ii) local perfusion information in ADC maps. Methods like TS-GAN \cite{b7} and Recon-GLGAN \cite{b8} use twin discriminators - local and global to capture the local and global context in DCE-MRI image generation and MRI reconstruction, respectively. However, they utilize the discriminators only from an adversarial training perspective, lacking local contextual attention.
% on suspicious regions.

% We go one step further to drive the generator using the two discriminators, which provide an embedded spatial attention map highlighting perfusion in the anatomy of interest 
% We propose a Trainable Attention Module (TAM) \cite{spatial_attn}  within the local and global discriminators, which provides an embedded spatial attention map.
% We propose a pair of discriminators consisting of an Aggregated Attention Module (AAM), for global and local processing of the region of interest. The discriminators provide an embedded spatial attention map highlighting perfusion in the anatomy of interest.
% This map, rich in local details, guides the generator for improved synthesis of post-contrast images. Note that our AAM differs from TAM\cite{b9}, wherein we ensemble the local and global attention maps to obtain the composed map that drives the generator.

% We propose a pair of discriminators consisting of an Aggregated Attention Module (AAM) for local and global processing of the region of interest. The discriminators provide spatial attention maps highlighting perfusion in the anatomy of interest. This map, rich in local detail, guides the generator for improved synthesis of post-contrast images. Unlike traditional approaches such as TAM\cite{b9}, our AAM uniquely integrates the local and global attention maps to create a composed map that effectively guides the generator. 
% Additionally, we use focal frequency loss \cite{ffl} and SSIM loss to capture structural details in the spectral and image domains, respectively.

We propose an Aggregated Attention-based discriminator (AAD) module consisting of two discriminators for global and local processing of the region of interest (ROI). The discriminators provide embedded spatial attention maps, each highlighting perfusion at fine and abstract levels. The two maps are aggregated and the ensembled map, rich in local details, guides the generator for improved synthesis of post-contrast images. Different from the previous attention-based discriminator \cite{b9}, the proposed Aggregated Attention-based discriminator introduces a composition of attention maps to drive the generator.
We summarize our contributions as follows:
1) An end-to-end trainable GAN for DCE-MR image synthesis from non-contrast multimodal MRI inputs, namely T2W, ADC, and T1 pre-contrast, with aggregated attention guidance to the generator to learn discriminative features that highlight hyper-intense abnormal regions in the prostate anatomy.
2) A dual-discriminator framework with aggregated attention modules that (i) enables adversarial learning to capture both global and local perfusion characteristics of the prostate anatomy and (ii) learns an aggregated attention map that highlights essential details in the region of interest to guide the generator. Our discriminator is agnostic to varying generator architectures.
3) Extensive experiments on the ProstateX dataset demonstrate that the proposed model predicts early and late response contrast-enhanced images with improvement margins (i)+0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE for early response, and (ii) +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late response, surpassing the second best performing model, DCE-Diff. Our experiments highlight the importance of aggregated attention and ADC for accurate DCE-MRI synthesis.

% 3) We conducted extensive experiments on the ProstateX dataset, which demonstrates that the proposed model predicts early and late response DCE-MR images with improvement margins (i)+1.38 dB PSNR, +0.091 SSIM, -0.038 MAE, -13.92 FID for early response, and (ii) +0.95 dB PSNR, +0.069 SSIM, -0.040 MAE, -10.23 FID for late response, surpassing the second best performing model, ResViT. Our experiments highlight the importance of aggregated attention and ADC for accurate DCE-MRI synthesis.



% 1) An end-to-end trainable GAN for DCE-MRI image synthesis from non-contrast multimodal MRI inputs, namely T2, ADC, and T1 pre-contrast, with aggregated attention guidance from the discriminator to enhance the  of the generator.
% 2) A dual-discriminator framework that (i) enables adversarial learning to capture both global and local characteristics of the prostate anatomy, (ii) learns respective global and local attention maps based on their hidden representations, and (iii) composes the attention maps to obtain an aggregated attention map that highlights important regions in the anatomy of interest.
% 3) Extensive experiments on ProstateX dataset shows that the proposed model predicts the early and late response DCE-MR images with improvement margins of (i) +1.38 dB PSNR, +0.091 SSIM, -0.038 MAE, -13.92 FID for early response, and (ii) +0.95 dB PSNR, 0.069 SSIM, -0.040 MAE, -10.23 FID for late response, over the second best-performing model, ResViT.
% 3) We conducted extensive experiments on the ProstateX dataset, which demonstrates that the proposed model predicts early and late response DCE-MR images with improvement margins (i)+1.38 dB PSNR, +0.091 SSIM, -0.038 MAE, -13.92 FID for early response, and (ii) +0.95 dB PSNR, +0.069 SSIM, -0.040 MAE, -10.23 FID for late response, surpassing the second best performing model, ResViT. Additionally, an ablation study highlights the critical role of ADC in the model's performance.
% 1) We propose DCE-GLGAN, a generative adversarial model consisting of global and local discriminators and residual encoder-decoder-based CNN as the generator that synthesizes early and late DCE-MRI images from multimodal inputs - T2W, ADC, and T1 pre-contrast MRI.
% 2) The proposed GAN model has a Context Attention Discriminator (CAD) consisting of an aggregated attention module providing an embedded spatial attention map. The local attention map is embedded in the global attention map and guides the generator for enhanced image synthesis.
% 3) Extensive experimental results on the ProstateX dataset demonstrate that the proposed model performs better than the existing GAN, diffusion models, and image-to-image translation benchmarks, further highlighting the critical significance of ADC in improving diagnostic accuracy.
% 1) We propose DCE-GLGAN, an attention-guided generator with a Context Attention Discriminator (CAD) that synthesizes early and late DCE-MRI images from multimodal inputs.
% 2) We incorporate a combination of focal frequency loss and structural similarity loss to train our network.
% 3) Our extensive experimentation using two realistic prostate MRI datasets (ProstateX and Prostate-MRI) shows that the proposed discriminator-based attention performs better than other GAN models and image-to-image mapping benchmarks and also emphasizes the importance of ADC.

\section{Proposed Method}
Our method is based on GAN, where a generator (G) and a discriminator (D) compete with each other to synthesize ground truth-like images from multi-modal input images. The core concept of AAD-DCE is to employ embedded spatial attention maps generated by the Aggregated Attention Discriminator (AAD) to guide the generator. This approach drives the generator to focus more on ROI, thus enhancing the post-contrast images. The input $\boldsymbol{x}$ consists of T2W, ADC, and T1 pre-contrast images concatenated along the channel axis while $\boldsymbol{y}$ represents the early and late response DCE-MR images as seen in Fig. \ref{fig:network architecture}(a).
% predicted images.
% The adversarial approach is given by,
% \begin{equation}
%     G(\tilde{x})^*=\arg \min _G \max _D\left(G(x), D\left(y\right)\right) L_{G A N}
% \end{equation}

% \begin{table*}[t]
% \centering
% \caption{Quantitative comparison of the generated early and late response DCE-MRI images between DCE-GLGAN and other models for ProstateX dataset.}
% \vspace{-6mm}
% \label{tab:quantitative results}
% \begin{tabular}{cccclccccl}
%                                 &                           &                            & \multicolumn{1}{l}{}            &            &                           &                           &                            &                          &                           \\ \hline
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Early Response}}                                                           &                           & \multicolumn{3}{c}{\textbf{Late Response}}                                        &                           \\ \cline{2-10} 
%                                 & \textbf{PSNR$\uparrow$}   & \textbf{SSIM$\uparrow$}    & \multicolumn{2}{c}{\textbf{MAE$\downarrow$}} & \textbf{FID $\downarrow$} & \textbf{PSNR$\uparrow$}   & \textbf{SSIM$\uparrow$}    & \textbf{MAE$\downarrow$} & \textbf{FID $\downarrow$} \\ \hline
% ConvLSTM                        & 14.92 $\pm$ 1.50          & 0.2397 $\pm$ 0.04          & \multicolumn{2}{c}{0.1393}                   & 118.706                   & 15.27 $\pm$ 2.56          & 0.2392 $\pm$ 0.06          & 0.135                    & 115.480                   \\
% CycleGAN                        & 18.61 $\pm$ 1.90          & 0.5134 $\pm$0.06           & \multicolumn{2}{c}{0.1281}                   &                           & 17.20 $\pm$ 2.40          & 0.4982 $\pm$ 0.06          & 0.129                    &                           \\
% Pix2Pix                         & 19.53 $\pm$ 1.97          & 0.5719 $\pm$ 0.05          & \multicolumn{2}{c}{0.0667}                   &                           & 19.35 $\pm$ 1.94          & 0.5546 $\pm$ 0.06          & 0.128                    &                           \\
% RegGAN                          & 20.56 $\pm$ 0.02          & 0.5966 $\pm$ 0.02          & \multicolumn{2}{c}{0.0571}                   & 23.7963                   & 19.89 $\pm$ 0.02          & 0.5803 $\pm$ 0.02          & 0.069                    & 22.6124                   \\
% TSGAN                           & 21.16 $\pm$ 3.50          & 0.6253 $\pm$ 0.10          & \multicolumn{2}{c}{0.0693}                   & 23.7533                   & 20.08 $\pm$ 2.64          & 0.5926 $\pm$ 0.09          & 0.074                    & 24.6665                   \\
% ResViT                          & 21.46 $\pm$ 1.61          & 0.6308 $\pm$ 0.05          & \multicolumn{2}{c}{0.0638}                   &                           & 20.88 $\pm$ 1.69          & 0.6231 $\pm$ 0.05          & 0.070                    &                           \\
% \textbf{DCE-GLGAN}                   & \textbf{22.74 $\pm$ 1.94} & \textbf{0.7218 $\pm$ 0.05} & \multicolumn{2}{c}{\textbf{0.0256}}          & \textbf{18.5352}          & \textbf{21.83 $\pm$ 2.15} & \textbf{0.6924 $\pm$ 0.06} & \textbf{0.0296}          & 19.8306                   \\ \hline
% \end{tabular}
% \end{table*}






% \begin{table*}[t]
% \centering
% \caption{Quantitative comparison of the generated early and late response DCE-MRI images between DCE-GLGAN and other models for ProstateX dataset.}
% \vspace{-6mm}
% \label{tab:quantitative results}
% \begin{tabular}{cccclccccc}
%                                 &                                      &                                     & \multicolumn{1}{l}{}            &            &                           &                                      &                                     &                          &                           \\ \hline
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Early Response}}                                                                               &                           & \multicolumn{3}{c}{\textbf{Late Response}}                                                            &                           \\ \cline{2-10} 
%                                 & \textbf{PSNR$\uparrow$}              & \textbf{SSIM$\uparrow$}             & \multicolumn{2}{c}{\textbf{MAE$\downarrow$}} & \textbf{FID $\downarrow$} & \textbf{PSNR$\uparrow$}              & \textbf{SSIM$\uparrow$}             & \textbf{MAE$\downarrow$} & \textbf{FID $\downarrow$} \\ \hline
% ConvLSTM                        & 14.92 $\pm$ 1.50                     & 0.2397 $\pm$ 0.04                   & \multicolumn{2}{c}{0.1393}                   & 118.706                   & 15.27 $\pm$ 2.56                     & 0.2392 $\pm$ 0.06                   & 0.135                    & 115.480                   \\
% CycleGAN                        & 18.61 $\pm$ 1.90                     & 0.5134 $\pm$0.06                    & \multicolumn{2}{c}{0.1281}                   &                           & 17.20 $\pm$ 2.40                     & 0.4982 $\pm$ 0.06                   & 0.129                    &                           \\
% Pix2Pix                         & 19.53 $\pm$ 1.97                     & 0.5719 $\pm$ 0.05                   & \multicolumn{2}{c}{0.0667}                   &                           & 19.35 $\pm$ 1.94                     & 0.5546 $\pm$ 0.06                   & 0.128                    &                           \\
% RegGAN                          & 20.56 $\pm$ 0.02                     & 0.5966 $\pm$ 0.02                   & \multicolumn{2}{c}{0.0571}                   & 23.7963                   & 19.89 $\pm$ 0.02                     & 0.5803 $\pm$ 0.02                   & 0.069                    & 22.6124                   \\
% TSGAN                           & 21.16 $\pm$ 3.50                     & 0.6253 $\pm$ 0.10                   & \multicolumn{2}{c}{0.0693}                   & 23.7533                   & 20.08 $\pm$ 2.64                     & 0.5926 $\pm$ 0.09                   & 0.074                    & 24.6665                   \\
% ResViT                          & 20.12 $\pm$ 1.61                     & 0.6308 $\pm$ 0.05                   & \multicolumn{2}{c}{0.0638}                   &                           & 19.60 $\pm$ 1.69                     & 0.6153 $\pm$ 0.05                   & 0.070                    &                           \\
% \multicolumn{1}{l}{DEC-Diff}    & \multicolumn{1}{l}{22.10 $\pm$ 1.79} & \multicolumn{1}{l}{0.6722 $\pm$ 0.05} & \multicolumn{2}{c}{0.0415}                   & \textbf{10.59}            & \multicolumn{1}{l}{21.73 $\pm$ 1,95} & \multicolumn{1}{l}{0.6582 $\pm$ 0.06} & 0.05                     & \textbf{7.26}             \\
% \textbf{Ours}                   & \textbf{22.74 $\pm$ 1.94}            & \textbf{0.7218 $\pm$ 0.05}          & \multicolumn{2}{c}{\textbf{0.0256}}          & 18.5352                   & \textbf{21.83 $\pm$ 2.15}            & \textbf{0.6924 $\pm$ 0.06}          & \textbf{0.0296}          & 19.8306                   \\ \hline
% \end{tabular}
% \end{table*}



\begin{table*}[t]
\centering
\caption{Quantitative comparison of the generated early and late response DCE-MRI images between AAD-DCE and other models.}
\vspace{-6mm}
\label{tab:quantitative results}
\begin{tabular}{cccclccccc}
                                &                                      &                                     & \multicolumn{1}{l}{}            &            &                           &                                      &                                     &                          &                           \\ \hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Early Response}}                                                                               &                           & \multicolumn{3}{c}{\textbf{Late Response}}                                                            &                           \\ \cline{2-10} 
                                & \textbf{PSNR$\uparrow$}              & \textbf{SSIM$\uparrow$}             & \multicolumn{2}{c}{\textbf{MAE$\downarrow$}} & \textbf{FID $\downarrow$} & \textbf{PSNR$\uparrow$}              & \textbf{SSIM$\uparrow$}             & \textbf{MAE$\downarrow$} & \textbf{FID $\downarrow$} \\ \hline
ConvLSTM                        & 14.92 $\pm$ 1.50                     & 0.2397 $\pm$ 0.04                   & \multicolumn{2}{c}{0.139}                   & 118.706                   & 15.27 $\pm$ 2.56                     & 0.2392 $\pm$ 0.06                   & 0.135                    & 115.480                   \\
CycleGAN                        & 18.61 $\pm$ 1.90                     & 0.5134 $\pm$0.06                    & \multicolumn{2}{c}{0.128}                   & 40.4371                   & 17.20 $\pm$ 2.40                     & 0.4982 $\pm$ 0.06                   & 0.129                    & 41.6501                   \\
Pix2Pix                         & 19.53 $\pm$ 1.97                     & 0.5719 $\pm$ 0.05                   & \multicolumn{2}{c}{0.066}                   & 25.9182                   & 19.35 $\pm$ 1.94                     & 0.5546 $\pm$ 0.06                   & 0.128                    & 27.0816                   \\
RegGAN                          & 20.56 $\pm$ 0.02                     & 0.5966 $\pm$ 0.02                   & \multicolumn{2}{c}{0.057}                   & 23.7963                   & 19.89 $\pm$ 0.02                     & 0.5803 $\pm$ 0.02                   & 0.069                    & 22.6124                   \\
TSGAN                           & 21.16 $\pm$ 3.50                     & 0.6253 $\pm$ 0.10                   & \multicolumn{2}{c}{0.069}                   & 23.7533                   & 20.08 $\pm$ 2.64                     & 0.5926 $\pm$ 0.09                   & 0.074                    & 24.6665                   \\
ResViT                          & 20.12 $\pm$ 1.61                     & 0.6308 $\pm$ 0.05                   & \multicolumn{2}{c}{0.063}                   & 32.4624                   & 19.60 $\pm$ 1.69                     & 0.6153 $\pm$ 0.05                   & 0.070                    & 30.0611                   \\
\multicolumn{1}{l}{DCE-Diff}    & \multicolumn{1}{l}{22.10 $\pm$ 1.79} & \multicolumn{1}{l}{0.6700 $\pm$ 0.05} & \multicolumn{2}{c}{0.040}                   & \textbf{10.5900}            & \multicolumn{1}{l}{21.73 $\pm$ 1,95} & \multicolumn{1}{l}{0.6500 $\pm$ 0.06} & 0.050                     & \textbf{7.2600}             \\
\textbf{AAD-DCE}                   & \textbf{22.74 $\pm$ 1.94}            & \textbf{0.7218 $\pm$ 0.05}          & \multicolumn{2}{c}{\textbf{0.025}}          & 18.5352                   & \textbf{21.83 $\pm$ 2.15}            & \textbf{0.6924 $\pm$ 0.06}          & \textbf{0.029}          & 19.8306                   \\ \hline
\end{tabular}
\end{table*}




\subsection{Generator}\label{AA}
% The Generator (G) is an encoder-decoder based architecture with residual blocks.The encoder layer consists of series of convolutional layers to capture the hierarchy of localized features of source image with Residual CNN at the bottleneck layer. Followed by decoder layer consiting of CNN layers.

% The Generator (G) utilizes an encoder-decoder architecture with residual blocks. The encoder and decoder consists of convolution layer designed to capture hierarchial and localized features from the input image. At the bottleneck layer, residual convolution networks are employed to enhance the feature representation. The generator can be a any image-to-image mapping CNN or transformer network. Our discriminator is adaptle to any

% The Generator (G) can be any image-to-image mapping CNN or transformer-based network. Our discriminator is flexible, making it suitable for various network architectures. We utilized an encoder-decoder architecture with residual blocks. The encoder and decoder consists of convolution layer designed to capture hierarchial and localized features from the input image. At the bottleneck layer, residual convolution networks are employed to enhance the feature representation.

The Generator (G) can be any image-to-image mapping CNN or transformer-based network. Our discriminator is adaptable to various generator architectures (Section III B). The generator architecture in Fig. \ref{fig:network architecture}(a), is an encoder-decoder architecture with residual blocks.

% The encoder and decoder use convolutional layers to capture hierarchical and localized features from the input image, while residual convolution networks at the bottleneck enhance feature representation.



\subsection{Aggregated Attention Discriminator (AAD)}
The architecture of the AAD block is shown in Fig. \ref{fig:network architecture}(b). It consists of a Global Attention discriminator $D_{GA}$, a Local Attention discriminator $D_{LA}$, and a Classifier ($\Psi_C$). $D_{GA}$ and $D_{LA}$ are global and local feature extractors that employ a spatial guidance module named Attention Discriminator (AD). The input to $D_{GA}$ is the whole image ($H \times W$) whereas $D_{LA}$ operates only the ROI ($H^{\prime} \times W^{\prime}$).
The AD in Fig. \ref{fig:network architecture}(c) comprises two key components: the Attention branch and the Trunk branch inspired by RAM \cite{b19}. The Trunk branch made up of convolutional layers, extracts low-level features from the input $\boldsymbol{y}$ to produce the output T($\boldsymbol{y}$). Note that our trunk branch is much simpler than in RAM \cite{b19} while maintaining feature extraction capability. The Attention branch, using the bottom-up top-down structure \cite{b18}, learns an attention map A($\boldsymbol{y}$), which modulates the trunk branch's output by applying weights. The output of $D_{GA}$ and $D_{LA}$ that utilizes the AD module are shown in (\ref{equ:2}) and (\ref{equ:3}),



% AD, as seen in Figure \ref{fig:network architecture}(c), consists of an Attention branch and a Trunk branch. These are convolutional layers, where the initial layers extract the low-level features from the inputs and pass them through the subsequent branches. With the trunk branch generating output T($\boldsymbol{y}$) from input $\boldsymbol{y}$, the attention branch learns an attention map  A($\boldsymbol{y}$) that weights the output of the trunk. The output of $D_{GA}$ and $D_{LA}$ that utilizes the AD module are shown in (\ref{equ:2}) and (\ref{equ:3}),

% aatn branch modulates the featus
\vspace{-2mm}
\begin{equation}
\vspace{-2mm}
E_{G}= \left(A_{G}(y)+1\right) \times T_{G}(y)  
\label{equ:2}
\end{equation}
\begin{equation}
E_{L}= \left(A_{L}(y)+1\right) \times T_{L}(y)  
\label{equ:3}
\end{equation} 
% \vspace{-2mm}
where $A_{G}(y)$, $A_{L}(y)$ are the attention maps from attention branch and $T_{G}(y)$, $T_{L}(y)$ are trunk branch outputs for $D_{GA}$ and $D_{LA}$ respectively.
% and C is the set of channels.
The $N_{C}$ dimensional feature vectors $E_{G}$ and $E_{L}$ are concatenated into a $2N_{C}$ dimensional vector and passed to the classifier ($\Psi_C$) as shown in (\ref{equ:4}). This vector is then fed into a fully connected layer, followed by a sigmoid activation function to classify real or fake.
\vspace{-2mm}
\begin{equation}
D({y})=\Psi_C\left(E_{G}\|\ E_{L}\right)
\label{equ:4}
\end{equation}
% \vspace{-2mm}
% The attention map $M_{L}$, obtained from $D_{LA}$ is now embedded on attention map $M_{G}$ from $D_{GA}$, resulting in  $M_{x}$.
The mean value of the attention maps $M_{G}$ and $M_{L}$ obtained from the two discriminators, $D_{GA}$ and $D_{LA}$  are aggregated by embedding $M_{L}$ into $M_{G}$ to give $M_{x}$. $M_{x}$ is infused into the multimodal inputs ($x$) using Residual Hadamard Product (RHP) as seen in (\ref{equ:5}) and fed to the  Generator.
\vspace{-2mm}
\begin{equation}
    % \tilde{x}=x \oplus M_{x}=\left(g\left(M_{T_x} ; \theta\right)+1\right) \times x 
    % x^{\prime}=x \oplus M_{x}=\left(M_{x} +1\right) \times x 
    x^{\prime}=\left(M_{x} +1\right) \times x 
\label{equ:5}
\end{equation}
As the infusion of attention map is based on RHP, it is initialized to one at the start of the training. 
The adversarial GAN loss, generator loss, and the final objective function are: 
% (\ref{equ:6} \ref{equ:7},
\vspace{-1mm}
\begin{equation}
    \begin{split}
        L_{G A N}(G, D)= & \mathbb{E}_{y \sim \mathcal{Y}}[\log D(y)]+ \\
 & \mathbb{E}_{x \sim \mathcal{X}}\left[\log \left(1-D\left(G\left(x^{\prime}\right)\right)\right)\right] 
    \end{split}
\label{equ:6}
\end{equation}
\vspace{-2mm}
\begin{equation}
    L_{L 1}(G)=\mathbb{E}_{x, y}\left[\left\|y-G\left(x^{\prime}\right)\right\|_1\right]
\label{equ:7}
\end{equation}
\vspace{-2mm}
\begin{equation}
    \arg \min _G \max _D L_{G A N}(G, D)+\lambda L_{L 1}(G)
\label{equ:8}
\end{equation}
% where $x^{\prime}$ is computed from (\ref{equ:5}). 
% The generator loss is given as
% \vspace{-2mm}
% \begin{equation}
%     L_{L 1}(G)=\mathbb{E}_{x, y}\left[\left\|y-G\left(x^{\prime}\right)\right\|_1\right]
% \label{equ:7}
% \end{equation}
% \vspace{-2mm}
% The final objective function is:
% \vspace{-2mm}
% \begin{equation}
%     \arg \min _G \max _D L_{G A N}(G, D)+\lambda L_{L 1}(G)
% \label{equ:8}
% \end{equation}

% \vspace{-2mm}
% Here $g(;\theta)$ is the attention transfer block T, a small 3-layer convolution network that transfers the attention map to the corresponding pixel weight map.

% \begin{figure*}[!t]
%     \centering
%      {\includegraphics[width=0.85\linewidth]{final_793_prox_icassp.pdf}}
%     \caption{Enter Caption}
%     \label{fig:images_prosx}
% \end{figure*}


% \begin{figure*}[!t]
% \vspace*{-\baselineskip}
%     \centering
%      {\includegraphics[width=1\linewidth]{Copy of new_793_prox_icassp.pdf}}
%      \vspace{-2mm}
%     \caption{Visual results of early \& late response for ProstateX dataset with error maps (from left to right):Groundtruth, ConvLSTM, CycleGAN, Pix2Pix, RegGAN,
%  TSGAN, ResViT,and DCE-GAN(ours). The yellow bounding box represents the region of interest.}
%     \label{fig:image_posxl}
% \end{figure*}

% \begin{figure*}[!t]
% \vspace*{-\baselineskip}
%     \centering
%      {\includegraphics[width=1\linewidth]{Copy of new_793_prox_icassp.pdf}}
%      \vspace{-2mm}
%     \caption{Visual results of early \& late response for ProstateX dataset with error maps (from left to right):Groundtruth, ConvLSTM, CycleGAN, Pix2Pix, RegGAN,
%  TSGAN, ResViT,and DCE-GAN(ours). The yellow bounding box represents the region of interest.}
%     \label{fig:image_posxl}
% \end{figure*}


% \begin{figure*}[!t]
% \vspace*{-\baselineskip}
%     \centering
%      {\includegraphics[width=1\linewidth]{all_qualitative_result.pdf}}
%      \vspace{-4mm}
%     \caption{(a) Visual results of early \& late response for ProstateX dataset with error maps (from left to right).The yellow bounding box represents the region of interest. (b) The attention maps obtained by different ways of ensembling, illustrating that $D_{LA}$ embedded on $D_{GA}$ allows the model to focus more effectively on the suspicious regions pointed by the arrow and the bounding box in the ground truth DCE-MR image. (c) Incorporating ADC in the inputs yields a synthesis closer to the ground truth. The dark region highlighted by the arrow in the ADC image signifies an area where contrast enhancement is likely to occur following the contrast injection.}
%     \label{fig:image_posxl}
% \end{figure*}



% \begin{figure*}[!t]
%     \centering
%      {\includegraphics[width=1\linewidth]{all_qualitative_result (1).pdf}}
%     \caption{(a) Visual results of early and late response for the ProstateX dataset with error maps (left to right). The yellow bounding box marks the region of interest. (b) Attention maps from different ensembling methods. Attention embedding enables better focus on suspicious regions. (c) Ablative study with and without ADC maps.(d) Ablative study on various generator architectures.}
%     \label{fig:image_posxl}
% \end{figure*}




% \begin{figure*}[!t]
%     \centering
%      {\includegraphics[width=1\linewidth]{all_qualitative_result (2).pdf}}
%     \caption{(a) Visual results of early and late response for the ProstateX dataset with error maps (left to right). The yellow bounding box marks the region of interest. (b) Attention maps from different ensembling methods. Attention embedding enables better focus on suspicious regions. (c) Ablative study with and without ADC maps. (d) Ablative study on various generator architectures.}
%     \label{fig:image_posxl}
% \end{figure*}


% \begin{figure*}[!t]
%     \centering
%      {\includegraphics[width=0.96\linewidth]{Copy of all_qualitative_result.pdf}}
%     \caption{(a) Visual results of early and late response for the ProstateX dataset with error maps. The yellow bounding box marks the ROI. (b) Ablative study with and without ADC maps. (c) Attention maps from different ensembling methods. Attention embedding enables better focus on suspicious regions. }
%     \label{fig:image_posxl}
% \end{figure*}


\begin{figure*}[!t]
    \centering
     {\includegraphics[width=0.96\linewidth]{qualitative_result.pdf}}
    \caption{(a) Visual results of early and late response for the ProstateX dataset with error maps. The yellow bounding box marks the ROI. (b) Ablative study with and without ADC maps. (c) Attention maps from different ensembling methods. Attention embedding enables better focus on suspicious regions.}
    \label{fig:image_posxl}
\end{figure*}


% \vspace{-2mm}
% Including ADC in the inputs produces a synthesis closer to the ground truth, with the dark region in the ADC image (arrow) highlighting a potential site for contrast enhancement after injection


% Visual results of early and late response for the ProstateX dataset with error maps (left to right). The yellow bounding box marks the region of interest. (b) Attention maps from different ensembling methods show that embedding $D_{LA}$ into $D_{GA}$ enables the model to better focus on suspicious regions indicated by the arrow and bounding box in the ground truth DCE-MR image. (c) Including ADC in the inputs produces a synthesis closer to the ground truth, with the dark region in the ADC image (arrow) highlighting a potential site for contrast enhancement after injection.




% Visual results of early \& late response for ProstateX dataset with error maps (from left to right).The yellow bounding box represents the region of interest. (b) The attention maps obtained by different ways of ensembling, illustrating that $D_{LA}$ embedded on $D_{GA}$ allows the model to focus more effectively on the suspicious regions pointed by the arrow and the bounding box in the ground truth DCE-MR image. (c) Incorporating ADC in the inputs yields a synthesis closer to the ground truth. The dark region highlighted by the arrow in the ADC image signifies an area where contrast enhancement is likely to occur following the contrast injection.
% is able to pay more attention to the suspicious region pointed by the arrow in the ground truth DCE-MR image.







% \begin{figure*}[!t]
% \vspace*{-\baselineskip}
%     \centering
%      {\includegraphics[width=0.85\linewidth]{new_793_prox_icassp.pdf}}
%      \vspace{-2mm}
%     \caption{Visual results of early \& late response for ProstateX dataset with error maps (from left to right):Groundtruth, ConvLSTM, CycleGAN, Pix2Pix, RegGAN,
%  TSGAN, ResViT,and DCE-GAN(ours). The yellow bounding box represents the region of interest.}
%     \label{fig:image_posxl}
% \end{figure*}




\section{Experiments and Results}
\subsection{Dataset and Implementation Details}
We trained our model on the open-source ProstateX dataset \cite{b17} consisting of T2-Weighted, ADC, T1 pre-contrast, and DCE images. The dataset consists of 346 patient studies with 5520 images (4410 for training and 1104 for validation). In the DCE data, the early enhancement usually occurs within 10 seconds of the appearance of the injected contrast agent, and therefore, the early and late response time points are selected accordingly. The data is registered using SimpleITK rigid registration and is resampled to $H \times W \times 16$. Here $H \times W$ is $160\times160$ and $H^\prime \times W^\prime$ is $60\times60$. We set $N_{C}$ as 64.
% $160\times160\times16$.
% \vspace{-2mm}
\begin{table}[h]
\centering
\caption{Importance of ADC}
\vspace{-2mm}
\label{tab:adc_ablative}
\begin{tabular}{ccccc}
\hline
\textbf{DCE-MRI} & \multicolumn{2}{c}{\textbf{Early Response}} & \multicolumn{2}{c}{\textbf{Late Response}} \\ \hline
\textbf{ADC}     & w/o ADC          & w/ ADC                   & w/o ADC          & w/ ADC                  \\ \hline
\textbf{PSNR}    & 21.50            & \textbf{22.74}           & 20.43            & \textbf{20.64}          \\ \hline
\textbf{SSIM}    & 0.6841           & \textbf{0.7218}          & 0.6404           & \textbf{0.6924}         \\ \hline
\end{tabular}
\end{table}
% \vspace{-9mm}

The models are trained using Pytorch 2.0 on a 24GB RTX 3090 GPU. The Adam optimizer ($\beta_{1}=0.9$, $\beta_{2}=0.999$, learning rate 1e-3) is used for 200 epochs, batch size of 4, and $\lambda$ is 10. The evaluation metrics are Peak Signal-to-Nosie Ratio (PSNR), Structural Similarity Index (SSIM), Mean Absolute Error (MAE), and Frechet Inception Distance (FID).



% \vspace{}
\subsection{Results and Discussion}
\textbf{1. Comparative study with previous methods:} We compare AAD-DCE with baseline models, Pix2Pix \cite{b11}, CycleGAN \cite{b12}, ConvLSTM \cite{b13}, Reg-GAN \cite{b14}, ResViT \cite{b15}, TSGAN \cite{b7}, and DCE-Diff \cite{b16}. 
The quantitative results in Table \ref{tab:quantitative results} show that our model outperforms other GAN-based, transformer-based, and convLSTM methods in most cases. Our model outperforms DCE-Diff, by +0.64dB, +0.1 dB in PSNR, +0.0518, +0.0424 in SSIM, and -0.015, -0.021 in MAE for early and late response respectively, except in terms of FID due to diffusion models' ability to learn image distribution through probabilistic framework. Fig. \ref{fig:image_posxl}(a) illustrates the synthesis results highlighting the abnormal regions in the transition and peripheral zones (TZ and PZ) of the prostate. We observe that AAD-DCE is most comparable to the ground truth with fewer errors in the error map and better preserves the global and local details of the post-contrast images.
We believe that: (i) While $D_{GA}$ focuses on the overall anatomical features, $D_{LA}$ targets the suspicious regions within the prostate via the attention information. (ii) ADC provides perfusion information, while T2W and T1 pre-contrast MR images establish the structural correlations. Integrating these modalities enables learning the complementary information, resulting in more accurate and detailed structural representations and perfusion information in the predicted images.
% We believe the reason for this are: (i) $D_{LA}$ focuses on the ROI, particularly the prostate, and the embedded attention map captures and assess the critical contrast. (ii) ADC provides perfusion information, while T2W and T1 Pre-contrast MR images establish the structural correlations. Integrating these modalities enables the model to learn complementary information, resulting in more accurate and detailed structural representations in the output images.

% Table \ref{tab:quantitative results} shows that our method outperforms other methods in PSNR, SSIM and MAE, except DCE-Diff in terms of FID. Te 
% +1.58 dB in PSNR, +0.091 in SSIM and -0.0314 in MAE for early response and +1.75 dB in PSNR, +0.077 in SSIM and -0.0389 in MAE for late response.

% Fig.\ref{fig:image_posxl} illustrates the synthesis results on the ProstateX dataset. We observe that AAM-DCE is most comparable to the ground truth with fewer errors in the error map and better preserves the global and local details of the post-contrast images. We believe the reason for this are: (i) $D_{LA}$ focuses on the ROI, particularly the prostate, and the embedded attention map captures and assess the critical contrast. (ii) ADC provides perfusion information, while T2W and T1 Pre-contrast MR images establish the structural correlations. Integrating these modalities enables the model to learn complementary information, resulting in more accurate and detailed structural representations in the output images.

% \begin{figure*}[!t]
%     \centering
%      {\includegraphics[width=0.70\linewidth]{final_793_prox_icassp.pdf}}
%     \caption{Enter Caption}
%     \label{fig:images_prosx}
% \end{figure*}




% \begin{center}
% \begin{table}[]
% \centering
% \caption{Ablative study on G-TAM and L-TAM}
% \vspace{-2mm}
% \label{tab:discriminator_abalative}
% \begin{tabular}{llcc}
% \hline
% \textbf{Components}     & \textbf{PSNR} & \textbf{SSIM} \\ \hline
%  G-TAM                   & 22.0608       & 0.7013        \\ \hline 
%  L-TAM * G-TAM           & 21.6625       & 0.7014        \\ \hline 
%  L-TAM + G-TAM           & 22.3872       & 0.7038        \\ \hline 
%  L-TAM embedded on G-TAM & 22.7394       & 0.7218        \\ \hline 
% \end{tabular}
% \end{table}
% \end{center}

% \vspace{-6mm}
% \begin{table}[h]
% \centering
% \caption{Importance of ADC}
% \vspace{-2mm}
% \label{tab:adc_ablative}
% \begin{tabular}{llll}
% \hline
% \multicolumn{1}{c}{\textbf{DCE-MRI}} & \textbf{ADC}    & \textbf{PSNR}  & \textbf{SSIM}   \\ \hline
% Early Response                       & w/o ADC         & 21.50          & 0.6841          \\
%                                      & \textbf{w/ ADC} & \textbf{22.74} & \textbf{0.7218} \\ \hline
% Late Response                        & w/o ADC         & 20.43          & 0.6404          \\
%                                      & \textbf{w/ ADC} & \textbf{20.64} & \textbf{0.6924} \\ \hline
% \end{tabular}
% \end{table}


% \vspace{-4mm}
% \begin{table}[h]
% \centering
% \caption{Importance of ADC}
% \vspace{-2mm}
% \label{tab:adc_ablative}
% \begin{tabular}{ccccc}
% \hline
% \textbf{DCE-MRI} & \multicolumn{2}{c}{\textbf{Early Response}} & \multicolumn{2}{c}{\textbf{Late Response}} \\ \hline
% \textbf{ADC}     & w/o ADC          & w/ ADC                   & w/o ADC          & w/ ADC                  \\ \hline
% \textbf{PSNR}    & 21.50            & \textbf{22.74}           & 20.43            & \textbf{20.64}          \\ \hline
% \textbf{SSIM}    & 0.6841           & \textbf{0.7218}          & 0.6404           & \textbf{0.6924}         \\ \hline
% \end{tabular}
% \end{table}
% \vspace{-5mm}





% \begin{center}
% \begin{table}[]
% \centering
% \caption{Ablative study on $D_{LA}$ and $D_{GA}$}
% \vspace{-2mm}
% \label{tab:discriminator_abalative}
% \begin{tabular}{llc}
% \hline
% \textbf{Components}           & \textbf{PSNR} & \textbf{SSIM} \\ \hline
% $D_{GA}$                      & 22.0608       & 0.7013        \\ \hline
% $D_{LA}$ * $D_{GA}$           & 21.6625       & 0.7014        \\ \hline
% $D_{LA}$ + $D_{GA}$           & 22.3872       & 0.7038        \\ \hline
% $D_{LA}$ embedded on $D_{GA}$ & \textbf{22.7394}       & \textbf{0.7218}       \\ \hline
% \end{tabular}
% \end{table}
% \end{center}
\setlength{\tabcolsep}{4pt}
\begin{center}
\begin{table}[]
\centering
\caption{Ablative study on attention ensembling}
\vspace{-2mm}
\label{tab:discriminator_abalative}
\begin{tabular}{lcccc}
\hline
\textbf{Attention Maps} & $D_{GA}$ & $D_{LA}$*$D_{GA}$ & $D_{LA}+D_{GA}$ & \begin{tabular}[c]{@{}c@{}}$D_{LA}$ embed\\  on $D_{GA}$\end{tabular} \\ \hline
\textbf{PSNR}       & 22.06  & 21.66           & 22.38         & \textbf{22.73}                                                      \\ \hline
\textbf{SSIM}       & 0.7012   & 0.7014            & 0.7038          & \textbf{0.7218}                                                       \\ \hline
\end{tabular}
\end{table}
\end{center}
\vspace{-5mm}
% \vspace{-7mm}




% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{ablative_gtam_ltam_739.pdf}
%     \caption{Enter Caption}
%     \label{fig:gtam_ablative}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{dga_dla_ablative_study.pdf}
%     \caption{Enter Caption}
%     \label{fig:gl_ablative}
% \end{figure}


% \begin{figure}
% \vspace*{-\baselineskip}
%     \centering
%     \includegraphics[width=1\linewidth]{adc_attn_map_ablative_study.pdf}
%     \caption{(a) Qualitative result for the importance of ADC. (b) The attention maps obtained for different components and its seen that $D_{LA}$ embedded on $D_{GA}$ is able to pay more attention to the contrast region present in the ground truth DCE-MR image. }
%     \label{fig:ablative_study}
% \end{figure}




% \vspace{-6mm}
% \begin{table}[h]
% \centering
% \caption{Importance of ADC}
% \vspace{-2mm}
% \label{tab:adc_ablative}
% \begin{tabular}{llll}
% \hline
% \multicolumn{1}{c}{\textbf{DCE-MRI}} & \textbf{ADC}    & \textbf{PSNR}  & \textbf{SSIM}   \\ \hline
% Early Response                       & w/o ADC         & 21.50          & 0.6841          \\
%                                      & \textbf{w/ ADC} & \textbf{22.74} & \textbf{0.7218} \\ \hline
% Late Response                        & w/o ADC         & 20.43          & 0.6404          \\
%                                      & \textbf{w/ ADC} & \textbf{20.64} & \textbf{0.6924} \\ \hline
% \end{tabular}
% \end{table}


%only adc ablative study
% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{adc_ablative_226_icassp.pdf}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}


\vspace{-3.5mm}
\textbf{2. Importance of ADC:}\label{AAA}
% \subsection{Ablative Studies}\label{AAA}
Table \ref{tab:adc_ablative} shows the importance of using ADC as inputs. The incorporation of ADC images containing perfusion information improves the post-contrast image synthesis. Fig. \ref{fig:image_posxl}(b) shows the abnormality in PZ that correlates with the tumor region findings in the ground truth (highlighted in yellow arrows).

\textbf{3. Ablative study on attention ensembling:} We study the effect of only the global attention and different ways of ensembling the global and local attention maps, namely additive, multiplicative, and embedding operations (Table \ref{tab:discriminator_abalative}). In Fig. \ref{fig:image_posxl}(c), the embedded attention map focuses more precisely on the regions with contrast uptake
with greater clarity. 

\textbf{4. Ablative study on various generator architectures:} We have evaluated the proposed AAD on various generator benchmarks - encoder-decoder CNN (U-Net) \cite{b11}, vision transformer \cite{b15}. A U-Net with AAD improves PSNR by +1.49 dB and SSIM by +0.0387, while for the transformer-based generator, AAD provides improvements of +1.77 dB in PSNR and +0.0432 in SSIM (Fig. \ref{fig:gen_ablative}).

\textbf{5. Model Parameters}: Comparing with the recent state-of-the-art, DCE-Diff, shows that our model has 19.93 million parameters with superior performance over DCE-Diff with 125.08 million parameters. 
% A UNet with a AAD gives an improvement of +1.49 dB PSNR and +0.0387 SSIM and a tranformer based gennerator with AAD gives +1.77 dB in PSNR and +0.0432 SSIM improvement.
% The contribution of each module of our model is studied individually as seen in Table \ref{tab:discriminator_abalative}. 
% Furthermore, we study the effect of only the global attention and different ways of ensembling the global and local attention maps, namely additive, multiplicative and embedding operations as seen in Table \ref{tab:discriminator_abalative}. In Fig. \ref{fig:image_posxl}(b), the embedded attention map focuses more precisely on the contrast region, capturing it with greater clarity.

% We try to understand the effect of local and global attention maps by adding, multiplying and embedding the local with the global attention maps. Fig.\ref{fig:ablative_study}(b). 
% Table \ref{tab:adc_ablative} shows the ablative study on using ADC as inputs. The incorporation of ADC images containing perfusion information significantly improves the results.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{Untitled drawing.pdf}
%     \vspace{-2mm}
%     \caption{Ablative study on various generator architectures.}
%     \label{fig:gen_ablative}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{generator_ablative.pdf}
    \vspace{-2mm}
    \caption{Generator architecture with AAD enhances focus on the ROI.}
    \label{fig:gen_ablative}
\end{figure}



\section{Conclusion}
This work presents a GAN framework with aggregated attention feedback guidance to the generator from the discriminator to synthesize DCE-MR images from multimodal non-contrast MRI inputs. Extensive experimentation with comparative models and ablation studies with ADC, attention ensembling methods, and various generator architectures show that the proposed method can synthesize higher-quality DCE-MR images. We are currently aiming at clinical validation studies for practical use.


% \begin{figure*}[]
%     \centering
%      {\includegraphics[width=0.95\linewidth]{final_793_prox_icassp.pdf}}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure*}






% \subsection{Identify the Headings}\label{ITH}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}\label{FAT}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}

\bibitem{b1} Mazaheri, Yousef, Oguz Akin, and Hedvig Hricak. "Dynamic Contrast-enhanced Magnetic Resonance Imaging of Prostate Cancer: A Review of Current Methods and Applications." World Journal of Radiology 9, no. 12, pp. 416-425, 2017.
\bibitem{b2} Moshe Rogosnitzky and Stacy Branch, “Gadolinium-based contrast agent toxicity: a review of known and proposed mechanisms,” Biometals, vol. 29, no. 3, pp.365–376, 2016.
\bibitem{b3} Wang, Y., Wu, W., Yang, Y., Hu, H., Yu, S., Dong, X., Chen, F., \& Liu, Q, "Deep learning-based 3D MRI contrast-enhanced synthesis from a 2D non-contrast T2Flair sequence", Medical Physics, 49(7), pp. 4478–4493, 2022.
\bibitem{b4} Xie, H., Lei, Y., Wang, T., Roper, J., Axente, M., Bradley, J. D., Liu, T., \& Yang, X, "Magnetic resonance imaging contrast enhancement synthesis using cascade networks with local supervision", Medical Physics, 49(5), pp. 3278–3287, 2022.
\bibitem{b5} Enhao Gong, M. John, Max Wintermark Pauly, and Greg Zaharchuk, “Deep learning enables reduced gadolinium dose for contrast-enhanced brain MRI,” in Journal of Magnetic Resonance Imaging, vol. 48, pp. 330–340, 2018
\bibitem{b6} J. C. Caicedo R. D. Fonnegra, M. Liliana Hernandez and G. M. Díaz, “Early-to-late prediction of DCE-MRI contrast-enhanced images in using generative adversarial networks,” in 2023 IEEE 20th International Symposium on Biomedical Imaging, pp. 1–5, 2023.
\bibitem{b7} E. Kim, H. -H. Cho, J. Kwon, Y. -T. Oh, E. S. Ko and H. Park, "Tumor-Attentive Segmentation-Guided GAN for Synthesizing Breast Contrast-Enhanced MRI Without Contrast Agents," in IEEE Journal of Translational Engineering in Health and Medicine, vol. 11, pp. 32-43, 2023
\bibitem{b8} Murugesan, B., S, V.R., Sarveswaran, K., Ram, K., Sivaprakasam, M, "Recon-GLGAN: A Global-Local context based Generative Adversarial Network for MRI Reconstruction," Machine Learning for Medical Image Reconstruction Workshop, MLMIR 2019, held in conjuntion with MICCAI, pp. 3-15, 2019.
\bibitem{b9} Lin, Y., Wang, Y., Li, Y., Gao, Y., Wang, Z. and Khan, L., 2021. Attention-based spatial guidance for image-to-image translation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 816-825, 2021.
\bibitem{b10} S. G. Armato et al., “PROSTATEx Challenges for computerized classification of prostate lesions from multiparametric magnetic resonance images,” Journal of Medical Imaging, vol. 5, no. 04, p. 1, Nov. 2018, doi: 10.1117/1.jmi.5.4.044501.

% \bibitem{b11} Lin, Y., Wang, Y., Li, Y., Gao, Y., Wang, Z. and Khan, L., Attention-based spatial guidance for image-to-image translation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 816-825, .

\bibitem{b11} P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, "Image-to-Image translation with conditional adversarial networks," 
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125-1134, 2017.
\bibitem{b12} J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", IEEE/ICCV,  pp.2223-2232, 2017.
\bibitem{b13}  Xingjian GulrajSHI, Zhourong Chen, Hao Wang, DitYan Yeung, Wai-kin Wong, and Wang-chun WOO, “Convolutional lstm network: A machine learning approach for precipitation nowcasting,” in Advances in Neural Information Processing Systems, vol. 28, 2015.
\bibitem{b14} L. Kong, C. Lian, D. Huang, Y. Hu, and Q. Zhou,
“Breaking the dilemma of medical image-to-image
translation,” in Advances in Neural Information Processing Systems, vol. 34, pp. 1964–1978, 2021.
\bibitem{b15} O. Dalmaz, M. Yurt, and T. Cukur, “RESVIT: Residual Vision Transformers for multimodal medical Image synthesis,” IEEE Transactions on Medical Imaging, vol. 41, no. 10, pp. 2598–2614, Oct. 2022.
\bibitem{b16} S. Ramanarayanan, A. Sarkar, MN. Gayathri, K. RAM, M. Sivaprakasam, "DCE-Diff: Diffusion Model for Synthesis of Early and Late Dynamic Contrast-Enhanced MR Images from Non-Contrast Multimodal Inputs," In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5174-5183, 2024.
\bibitem{b17} Oscar Debats Geert Litjens, Nico Karssemeijer
Jelle Barentsz, and Henkjan Huisman, “Prostatex
Challenge data,” in The Cancer Imaging Archive, 2017.
\bibitem{b18} A. Newell, K. Yang, and J. Deng. Stacked hourglass
networks for human pose estimation. arXiv preprint
arXiv:1603.06937, 2016.
\bibitem{b19} Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
"Residual attention network for image classification", In
CVPR, pp. 3156–3164, 2017.
\bibitem{20} S. Ramanarayanan, B. Murugesan, K. Ram, and M. Sivaprakasam, “DC-WCNN: A deep cascade of Wavelet based convolutional neural networks for MR image Reconstruction,” IEEE 19th International Symposium on Biomedical Imaging (ISBI), pp. 1069-1073, 2022.
\bibitem{21} Y. Beauferris et al., “Multi-Coil MRI Reconstruction Challenge—Assessing brain MRI reconstruction models and their generalizability to varying coil configurations,” Frontiers in Neuroscience, vol. 16, Jul. 2022.

% \bibitem{b20} J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
% networks for semantic segmentation. In CVPR, 2015.



% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \bibitem{b8} D. P. Kingma and M. Welling, ``Auto-encoding variational Bayes,'' 2013, arXiv:1312.6114. [Online]. Available: https://arxiv.org/abs/1312.6114
% \bibitem{b9} S. Liu, ``Wi-Fi Energy Detection Testbed (12MTC),'' 2023, gitHub repository. [Online]. Available: https://github.com/liustone99/Wi-Fi-Energy-Detection-Testbed-12MTC
% \bibitem{b10} ``Treatment episode data set: discharges (TEDS-D): concatenated, 2006 to 2009.'' U.S. Department of Health and Human Services, Substance Abuse and Mental Health Services Administration, Office of Applied Studies, August, 2013, DOI:10.3886/ICPSR30122.v2
% \bibitem{b11} K. Eves and J. Valasek, ``Adaptive control for singularly perturbed systems examples,'' Code Ocean, Aug. 2023. [Online]. Available: https://codeocean.com/capsule/4989235/tree
\end{thebibliography}

% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
