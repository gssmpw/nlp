\vspace{-0.5em}
\section{Uni-Retrieval model}
\vspace{-0.5em}

\begin{figure*}[!t]
  \centering
   \includegraphics[width=0.9\linewidth]{imgs/model.pdf}
   \vspace{-3mm}
   \caption{\textbf{The Uni-Retreival model's architechture}.}
   \label{fig:model}
   \vspace{-5mm}
\end{figure*}

%In the query-based retrieval task, given an image $I_i$ or a text prompt $P_t$ from the style-specific query set $Q_s$, the retrieval model needs to compute the score between input and target queries and rank the corresponding answers $i \in I$ as high as possible.
%For our STEM education retrieval task settings, the goal is similar, ranking all answers correctly with input queries for various style-specific query sets $\{Q_s\}^n_{s=1}$. 
%If the dataset does not contain the corresponding different style queries, the model should list the same category queries as the suggestions. 
% What's more, if the input's style is not contained in the prompt bank, the model can also link to another databases to do extra search.

Our model consists of three main submodules: a \textbf{prototype learning module} for generating the prototype feature for each content style~(Sec. {\color{red}\ref{subsec:prototype}}), a \textbf{prompt bank} for saving features in the embedding space and selecting the prompt tokens which can represent the input's style~(Sec. {\color{red}\ref{subsec:prompt_bank}}), and a \textbf{feature extractor} based on different vision-language models for retrieving~(Sec.  {\color{red}\ref{subsec:foundation_model}}). Additionally, we further present the training and inference process in Sec. {\color{red}\ref{subsec:training_inference}}.

\subsection{Prototype Learning Module} \label{subsec:prototype}
For Uni-Retrieval, given an input query (freely based on audio, image, or text) \( x \subseteq \mathbb{R}^{L*C} \) and a feature encoder \( f \), map \( x \) into a \( d \)-dimensional shared latent space (Prototype Module) using \( f \), each style has $m$ images. 
For style extracting, researchers usually use the the pretrained models which contains rich semantic information as the feature encoders. For example, if the input queries focus on style images, we leverage the style encoder \cite{styler} to extract visual features. 
If the query emphasizes the need for more textual information from the context, the text encoder and tokenizer, which are pretrained on large text datasets such as the Pile \cite{pile}, can be utilized.
The input query is embedded as follows:
{\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\begin{eqnarray}
E_{0}^{i} = f(x_{0}^{i}), \quad E_{0}^{i}\subseteq \mathbb R^{d},i=1,2,\dots,m
\end{eqnarray}}

\noindent where $E_{0}^{i}$ denotes the 0-th style's i-th embedding. And then, using the average pooling operation to sum the different each style's embeddings to get the j-th prompt $P_j$:
{\setlength{\abovedisplayskip}{8pt}
\setlength{\belowdisplayskip}{5pt}
\begin{eqnarray}
    P_{j} = \mathrm{AvgPool}(\begin{matrix}\sum^{m}_{i=0}{E_{j}^{i}}\end{matrix})
\end{eqnarray}}

\subsection{Prompt Bank} \label{subsec:prompt_bank}
The Prompt Bank builds a hidden state like TTT \cite{TTT} and mamba \cite{gu2023mamba}, which are designed to store semantic and contextual information at a high level.
Unlike the previous method, which leverages clusters to represent different styles, the Prompt Bank uses hash-like sets to store universal information.
Prompt Bank contains $N$ prompts, and each prompt $k_i$ is associated as a value to a learnable key $P_i$:
{\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{equation}\setlength{\itemsep}{0.5pt}
Prompt\_Bank \!=\! \{(k_1, P_1),\dots,(k_N, P_N)\}
\end{equation}}

We denote the key set as $K = \{k_i\}^{N}_{i=1}$ and define $\gamma$ to score the match between $k_i$ and $P_i$. Given an input $x$, the $\gamma$ looks up the top-$n$ keys to expand the feature embedding of $x$. The aim to use the hash-liked structure is promoting the matching speed between the input and the Prompt Bank's tokens. For Uni-Retrieval model, we calculate the cosine similarity between the matching prompt $P_{j_i}$ and the key $K_i$:
{\setlength{\abovedisplayskip}{7pt}
\setlength{\belowdisplayskip}{7pt}
\begin{eqnarray}
K_x={\underset{\{j_i\}^{n}_{i=1}\subseteq [1,N]}{\arg\min}}\quad \begin{matrix}\sum^{n}_{i=1}\gamma(P_{j_i}, K_i)\end{matrix}
\end{eqnarray}}

The Prompt Bank is free to combine different prompt tokens and expand feature embedding space, allowing different tokens associated with specific styles to jointly represent an input query. Due to the generalization properties on out-of-distribution of our Prompt Bank, even unseen styles also can combine similar styles' tokens to enhance retrieval performance by expand the semantic/context information provided by Prompt Bank. The expanding method is suitable for both different styles of images and different expression of text. 
Usually the special token is put at the beginning of the sequence tokens:
{\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{eqnarray}
x_p = [CLS;P_{j_1};P_{j_2};\dots;P_{j_n};x_e]
% , \quad 1\leq n\leq N
\end{eqnarray}}

\noindent where $x_p$ denotes the image's input feature after expanding prompt tokens; $x_e$ represents  the original sequence tokens patched from the input; $CLS$ is the special token used for performing downstream tasks with different heads.

% \sz{The updatable properties of the Prompt Bank are not expressed.}

\subsection{Feature Extractor}
\label{subsec:foundation_model}
% Foundation models are used to extract features from the original input. 
In Uni-Retrieval, we apply the ViT structure as the visual feature extractor, leverage a tokenizer to embed the input text query $x \subseteq \mathbb{R}^{L*C}$, and then utilize the transformer \cite{vaswani2017attention} as the text encoder to extract features. 
The vision encoder and the text encoder are initialized with OpenCLIP, and gpt-neo \cite{gpt-neo} trained on the Pile dataset as the tokenizer, respectively.
What's more, we uses GPT-4o \cite{hurst2024gpt} as the optional choice of the external large language model to convert the audio clips to the text sequence.

The whole sequence tokens are feed into the feature extractor for training and inference layer by layer. Obtained from the Prompt Bank, visual prompt tokens represent various style information specific to different STEM subjects, while text prompt tokens convey distinct context information about different STEM subjects or different expression about the same subjects. 
The parameters are sharing between visual prompts and text prompts each layer to align vision and text modality. 
In the model training phase, the vision encoder, the tokeniser and the text encoder are fully frozen to retain the original semantic space.

\subsection{Training and Inference}
\label{subsec:training_inference}



For the training procedure, in every training step, the style/context features are extracted from the corresponding encoder $f$ to get the prompt $P_j$. Then, matching $P_j$ and the key $K_j$ to get the matching prompts $P_j$. Besides, the tokenizer and the patch layer map the inputs into sequence $x_t$: 
{\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{eqnarray}
x_t = \mathrm{Tokenizer/Patch}(x)
\end{eqnarray}}

\noindent where $x_t$ denotes the temp state of features. 
% The tokenizer layer embeddings the $x_t$ and then adds positional embeedings to $x_e$: 
% \begin{eqnarray}
% x_e = \mathrm{PosEmb+Embedding}(x_t)
% \end{eqnarray}
% where PosEmb denotes the positional embeddings operation. 
After selecting $n$ prompts following the aforementioned query strategy, the expanded feature embedding $x_p$ is fed into the foundation model $\delta$ and getting the final result $x_f$. We use the $CLS$ token to represent the whole sequence $x_p$ following the settings of LLaMA \cite{touvron2023llama}:
{\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{eqnarray}
x_f = \delta(x_p)[:, 0, :]
\end{eqnarray}}

\noindent The triplet loss function $\mathcal{L}$ utilizes the features $x_f$, $x_r$, and $x_h$ of an image or text, a retrieval target query, and a negative sample from a different category. 
%Minimizing $\mathcal L$ represents making the correct sample pair closer and distancing the negative sample pair. 
Minimizing $\mathcal{L}$ brings the correct sample pairs closer together while distancing the negative sample pairs.
With $\mu$ as margin and distance function $d(a,b)=(1-a*b)/(||a||-||b||)$, $\mathcal L$ is given as: 
{\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{multline}
\mathcal L = max\{0, \\
\mu + d(\delta(x_f), \delta(x_r))-d(\delta(x_f), \delta(x_h))\}
\end{multline}}
% \vspace{-3mm}

\noindent where $x_r, x_h$ denotes the embedding of the retrieval object and the negative sample respectively. Moreover, the key in Prompt Bank will be updated with a scale parameter $\lambda$ to weight the second loss function term:
{\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{eqnarray}
\min_{k,p,L} \mathcal L(x_f, x_r, x_h) + \lambda \begin{matrix}\sum_{K_x}\gamma(q(x), k_{si})\end{matrix}
\end{eqnarray}}

During the training procedure, Uni-Retrieval will jointly train both the Prompt Bank's keys $K$ and the prompt tokens $P$, which endow the Prompt Bank with the capability for continuous updates. The updatable parameters of the Uni-Retrieval model are limited compared to full-parameter fine-tuning, effectively saving computational resources and enhancing training speed. 
% \noindent

For the inference procedure, we first extract the prototype feature for the unknown-style query input $x$. We use the prototype feature as the query to retrieve the fittest prompt tokens from the prompt bank. If the style is unknown type, the Prompt Bank will use several different clusters to represent it jointly. Then we prepend the prompt tokens $x_p$ to the feature extractor $\delta$ for retrieval. The query embeddings are extracted in advanced and stored in the databases to accelerate the retrieval process.

