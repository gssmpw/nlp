

\section{Related Works}
\label{sec:related_work}

\subsection{Dataset Adaptation in Education}
Within the realm of education, image retrieval in education has distinct characteristics, as images often reflect the teaching intentions of educators. This facilitates the rapid and accurate alignment of visual content with teaching materials, thereby reducing educators' preparation workload and enhancing the precision of learning data. While existing researches has focused on classifying educational data \cite{choi2020ednet}, they often encounter constraints. Due to the complexity of incorporating an expansive range of teaching scenarios in STEM education and the scarcity of data, numerous studies often narrow the scope to a limited set of subject applications \cite{hendrycks2021measuring,pal2022medmcqa} or to a limited set of teaching strategy retrievals \cite{kwon2024biped,welbl2017crowdsourcing}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{imgs/data.pdf}
    \vspace{-7mm}
    \caption{\textbf{The SER Dataset} contains 24,000+ text captions and their corresponding queries with various styles, including Natural, Sketch, Art, Low-Resolution~(Low-Res) images and audio clips from different STEM subjects.}
    \vspace{-5mm}
    \label{fig:data_sample}
\end{figure*}


There is a considerable variation across existing STEM education datasets regarding its specific composition. Many datasets are cluttered with irrelevant or invalid data, lack comprehensive coverage of specialised content, and suffer from quality assurance issues \cite{patrinos2018global}. 
Although STEM education datasets are assembled from interactions between learners and large language models \cite{hou2024eval, wang2024large}, they are generally not well-suited for use by educators and learners across multiple domains. Furthermore, creating a precise and professional data retrieval repository for the educational domain requires efficient retrieval algorithms as support \cite{alzoubi2024enhancing}. To ensure efficient retrieval and usability in STEM education scenarios, we construct the SER dataset, which includes multiple query styles to enhance retrieval diversity.
%There are datasets assembled from interactions between learners and large models\cite{}\cite{}. 
%However, creating a precise and professional data retrieval repository for the educational domain requires grounding it in authentic teaching scenarios.
%Additionally, most datasets and retrieval methods are not well-suited for use by educators and learners across multiple domains. 
%To ensure efficient retrieval and usability, image-text alignment and voice input play pivotal roles.

\subsection{Multi-task Learning}

In STEM education, the multi-style retrieval model needs to leverage multi-task learning to align features and learning across different modal samples. Multi-task learning refers to the simultaneous training and optimization of multiple related tasks within a single model \cite{9392366}. By sharing parameters and representations across functions, it improves overall performance. Compared to other transfer learning methods, including domain adaptation \cite{farahani2021brief} and domain generalization \cite{zhou2022domain}, multi-task learning annotates data and achieves CLIP-level model fine-tuning and convergence, the data of each task in multi-task learning is well-labeled. 

Overall, multi-task learning introduces a new tool for STEM education practitioners that may help meet requirements, especially if speed and efficiency are preferred over performance. While many recent multi-task learning employ two clusters of contemporary techniques, hard parameter sharing and soft parameter sharing \cite{ruder2017overview}. In hard parameter sharing, most or all of the parameters in the network are shared among all tasks \cite{kokkinos2017ubernet}. In soft parameter sharing,the models are tied together either by information sharing or by requiring parameters to be similar \cite{yang2016trace}. Consequently, our Uni-Retrieval adopts a blended multi-task learning paradigm, adopt both hard and soft  parameter in different styles of tasks. Building upon successul multi-task learning method for CLIP, such as CoCoOP \cite{zhou2022conditional}, MaPLe \cite{khattak2023maple}, and FreestyleRet \cite{li2025freestyleret}, our study leverages these techniques to strengthen domain adaptation and multi-task learning. 

\begin{figure*}[!t]
  \centering
   \includegraphics[width=\linewidth]{imgs/categories.pdf}
   \vspace{-6mm}
   \caption{\textbf{Concept distribution of our SER dataset}. Our dataset exhibits a diverse distribution on different concept domains.}
   \label{fig:distribution}
   \vspace{-3mm}
\end{figure*}

\subsection{Query-based Retrieval}
Existing work in Query-based Image Retrieval (QBIR) primarily includes content-based image retrieval \cite{chen2022deep}, text-based image retrieval \cite{li2011text}, and multi-modal retrieval \cite{neculai2022probabilistic}. In content-based image retrieval, the visual features of images are directly utilized for retrieval. However, its reliance on fixed content and location makes it relatively inflexible in capturing diverse user intents \cite{lee-etal-2024-interactive}. Alternative methods like sketching \cite{chowdhury2022fs, chowdhury2023scenetrilogy} and scene graph construction \cite{johnson2015image} enable the retrieval of abstract images that are hard to describe verbally, though they lack the intuitive ease of natural language-based retrieval. In text-based image retrieval, enhancements to text queries often involve indicating content structure. However, these approaches are either restricted by closed vocabularies \cite{mai2017spatial, kilickaya2021structured} or face substantial challenges \cite{li2017generating} in deriving structures from natural language descriptions. Recent multi-modal approaches, such as cross-modal scene graph-based image-text retrieval \cite{wang2020cross} and joint visual-scene graph embedding for image retrieval \cite{belilovsky2017joint}, still depend on word embeddings and image features.

Despite advancements in QBIR, challenges including the semantic gap that can lead to inaccurate retrieval results, high computational complexity and resource costs for large-scale image databases, and the high cost of obtaining quality data annotations \cite{li-etal-2024-generative}. The application of QBIR to educational resource retrieval is promising but has been hindered by the complexity of educational discourse, the limitations of educational databases, and the associated costs \cite{zhou-etal-2024-vista}.  Our query model effectively combines multi-modal retrieval methods, integrating audio and natural language with multi-style image inputs. The former enables natural and rapid expression of content, while the latter facilitates accurate and intuitive image localization, enhancing educational data retrieval.

\subsection{Prompt Tuning}

Prompt tuning \cite{brown2020language} was first proposed in natural language processing~(NLP) and has been an efficient approach that bridges the gap between pre-trained language models and downstream tasks \cite{li2023weakly}. Prompt tuning leverages natural language prompts to optimize the language modelâ€™s ability to understand tasks, which demonstrates exceptional performance in few-shot and zero-shot learning. 
Recent studies have focused on optimizing various components of prompt tuning, such as  prompt generation, continuous prompt optimization \cite{prompt3}, and adapting to large-scale models through methods like in-context learning \cite{dong2024survey}, instruction-tuning \cite{wang2024pandalm}, and chain-of-thought \cite{prompt4}. For example, \citet{lester2021power} leverage soft prompts to condition frozen language models to enhance the performance of specific downstream tasks. \citet{long2024prompt} propose an adversarial in-context learning algorithm, which leverages adversarial learning to optimize task-related prompts.

Furthermore, prompt tuning has gradually become a pivotal technique in computer vision \cite{shen2024multitask}, enabling efficient adaptation of pre-trained models to diverse tasks. Notable methods include visual prompt tuning for classification \cite{jia2022visual}, learning to prompt for continual learning \cite{wang2022learning}, context optimization and conditional prompt learning for multi-modal models \cite{zhou2022conditional}, and prompt-based domain adaptation strategies \cite{ge2023domain}.
For example, \citet{nie2023pro} introduce the pro-tuning algorithm for learning task-specific vision prompts, applied to downstream task input images with the pre-trained model remaining frozen.
\citet{shen2024multitask} leverage cross-task knowledge to optimize prompts, thereby enhancing the performance of vision-language models and avoiding the need to independently learn prompt vectors for each task from scratch.
\citet{cho2023distribution} introduce distribution-aware prompt tuning for vision-language models, optimizing prompts by balancing inter-class dispersion and intra-class similarity.
MaPLe \cite{khattak2023maple} further transfers text features to the visual encoder during prompt tuning to avoid overfitting. These approaches leverage learnable prompts to enhance model performance across various applications. Despite significant advancements in previous research, challenges remain in extracting semantic features from style-diversified images and optimizing templates within cont.inuous prompt tuning. 
In this study, we employ both NLP and visual prompt tuning to optimize STEM educational content retrieval, enhancing retrieval accuracy and efficiency by adjusting prompt tokens.
%VPT vision prompt tuning

\begin{figure}[!tbp]
  \centering
   \includegraphics[width=\linewidth]{imgs/pipeline.pdf}
   \vspace{-5mm}
   \caption{\textbf{The pipeline of Uni-Retrieval.} The image-text/audio pairs are input into their respective modality encoders. During the training procedure, contrastive learning is applied between the modality features of the positive samples~(image-text/audio pairs) and the negative samples. During the inference procedure, the model calculates the similarity between the modality features of the query and the embeddings stored in the database. The retrieved results are ranked, and the performance is evaluated using R@1/R@5 as metrics.}
   \label{fig:pipeline}
   \vspace{-5mm}
\end{figure}

\section{Motivation and Scenarios}
In practical teaching scenarios, teachers often encounter the need for precise image retrieval, such as searching for hand-drawn sketches, student-created artistic images, blurry blackboard drawings captured from a distance, classroom photographs of physical objects, or images from textbooks. However, current retrieval models predominantly focus on text-natural image queries, overlooking the diverse query styles common in educational contexts. This limitation makes it challenging for teachers to efficiently identify and retrieve educational images or texts tailored to diverse teaching scenarios, such as accurately setting learning contexts, articulating key teaching points, presenting instructional materials, and quickly locating supplementary resources.

Our proposed method enables teachers to query various styles' answers with a range of retrieval approaches, including text, image, audio, or combinations of these modalities. This approach ensures fast and convenient retrieval, significantly reducing preparation time for teaching. Once teachers input their queries, our Uni-Retrieval system employs contrastive learning to compare images and text, calculating similarities based on attributes like objects, shapes, quantities, and orientations. The system ranks all database entries by similarity, outputting the top-1 or top-5 results to identify the most relevant and accurate teaching resource images, as illustrated in Fig.~\ref{fig:pipeline}. This approach empowers teachers to manage complex and dynamic teaching scenarios effortlessly, enhancing the clarity and effectiveness of STEM education.



\section{Experiments}
\label{supsubsec:Experiment Settings}

% \subsection{Dataset Construction Details}
% \label{supsec:dataset construction details}

In the database, the texts, their corresponding four images and audio structures for each dataset can share a single index, significantly reducing query time. All images and text are preprocessed using pretrained models to extract features, which are stored as embeddings. This approach eliminates the need for repeated feature extraction during use, saving time and reducing computational overhead, improving the efficiency of the retrieval system.

For the dataset selection, we choose four another datasets except our SER dataset, including the DSR dataset \cite{li2025freestyleret}, the ImageNet-X dataset, the SketchCOCO dataset \cite{gao2020sketchycoco} and the DomainNet dataset \cite{peng2019moment}. We use internVL-1.5 \cite{Chen_2024_CVPR} to annotate the paint/sketch caption for the SketchCOCO and the DomainNet dataset. For the model in the prototype learning module, we choose the VGG \cite{vgg} as the feature extractor. For the baseline selection, we apply two cross-modality pre-trained models (CLIP \cite{clip}, BLIP \cite{blip}), two multi-modality pre-trained models~(LanguageBind \cite{languagebind}, Unified-IO2 \cite{uio2}), two style retrieval models (SceneTrilogy \cite{scenetrilogy}, FashionNTM \cite{fashionntm}), four most recent cross-modality prompt learning models (VPT \cite{jia2022visual}, CoCoOP \cite{zhou2022conditional}, MaPLe \cite{khattak2023maple}, FreestyleRet \cite{li2025freestyleret}), and two database-driven retrieval models (GASKN \cite{GASKN}, MKG \cite{mkg}) for the fair comparison. Specifically, we fine-tune the cross-modality models (CLIP, BLIP) on SER for convergence. We also train the prompt learning models on SER dataset based on VPT's settings to adapt STEM style-diversified inputs. As for the multi-modality models, we evaluate the zero-shot performance on the style-diversified STEM education retrieval task due to multi-modality models' comprehensionability on multi-style image inputs.

For the experiments on the SER dataset, Uni-Retrieval is initialized with OpenCLIP's weights and trained on 8 A100 GPUs with batch size 24 per GPU and 20 training epochs. We use AdamW as the optimizer, set the learning rate to 1e-5 with a linearly warmed up operation in the first epochs and then decayed by the cosine learning rate schedule. The seed is set as 42. What's more, all input images are resized into $224\times224$ resolution and then augmented by normalized operation. All text are padding zero to the max length of 20.

For the fine-tuning CLIP and BLIP models, all experiment settings are the same as Uni-Retrieval except the learning rate is set as 1e-6. For prompt tuning models, we both use 4 prompt tokens to expand the token sequence. For all transformer-based models, we use the ViT-Large and 24-layers text transformer as the foundation models to keep balance between performance and efficiency.




