\section{Experiments}

\subsection{Experiments Settings}

\newcommand{\pub}[1]{\color{red}{\tiny{#1}}}
\newcommand{\Frst}[1]{{\textbf{#1}}}
\newcommand{\Scnd}[1]{{\underline{#1}}}
\begin{table*}[htp]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.05}  % 控制行高
\setlength{\tabcolsep}{2.75mm}        % 控制列间距
{
{
\begin{tabular}{c|c|cc|cc|cc|cc}
    \toprule[1.5pt]
    \multirow{2}{*}{\textbf{\#}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Text} \textbf{$\rightarrow$} \textbf{Image}} & \multicolumn{2}{c|}{\textbf{Sketch} \textbf{$\rightarrow$} \textbf{Image}} & \multicolumn{2}{c|}{\textbf{Art} \textbf{$\rightarrow$} \textbf{Image}} & \multicolumn{2}{c}{\textbf{Low-Res} \textbf{$\rightarrow$} \textbf{Image}} \\ 
    
    \cmidrule(rl){3-4}\cmidrule(rl){5-6}\cmidrule(rl){7-8}\cmidrule(rl){9-10}
    & & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} \\

    \noalign{\hrule height 1.5pt}
    \rowcolor{gray!20}\multicolumn{10}{c}{\it{\textbf{Pretrained Cross-Modality Models}}} \\
    \hline
    1& CLIP          & 54.6 & 78.4 & 47.3 & 68.9 & 46.8 & 71.3 & 53.7 & 72.9\\
    2& BLIP          & 55.8 & 79.2 & 48.2 & 69.2 & 47.5 & 74.4 & 51.5 & 74.2\\
    3& CLIP-Finetune                & 71.4 & 91.4 & 71.0 & 87.0 & 52.2 & 81.6 & 71.2 & 88.1\\
    4& BLIP-Finetune                & 70.2 & 92.0 & 71.6 & 89.2 & 54.3 & 82.3 & 69.7 & 86.8\\
    \hline
    \rowcolor{gray!20}\multicolumn{10}{c}{\it{\textbf{Large Multi-Modality Models}}} \\
    \hline
    5& LanguageBind     & 60.2 & 86.9 & 52.8 & 78.4 & 49.0 & 78.4 & 59.1 & 80.2\\
    6& Unified-IO2   & 67.5 & 89.2 & 59.6 & 84.1 & 55.9 & 82.9 & 64.3 & 84.0\\
    \hline
    \rowcolor{gray!20}\multicolumn{10}{c}{\it{\textbf{Style Retrieval Models}}} \\
    \hline
    7& SceneTrilogy  & 69.7 & 84.5 & 75.6 & \textbf{96.5} & 71.5 & 92.9 & 68.6 & 85.5\\
    8& FashionNTM    & 50.4 & 81.3 & 68.9 & 88.6 & 67.1 & 88.9 & 45.6 & 77.5\\
    \hline
    \rowcolor{gray!20}\multicolumn{10}{c}{\it{\textbf{Cross-Modality Prompt Learning Models}}} \\
    \hline
    9& VPT          & 69.9 & 84.1 & 53.3 & 72.3 & 62.7 & 83.2 & 67.4 & 79.1\\
    10& CoCoOP       & 72.2 & 86.7 & 53.8 & 74.8 & 66.4 & 87.4 & 70.8 & 81.6\\
    11& MaPLe       & 73.8 & 87.8 & 62.7 & 78.9 & 67.8 & 89.4 & 71.9 & 86.3\\
    12& FreestyleRet & 80.1 & 92.5 & 75.3 & 91.5 & 73.0 & \textbf{98.3} & 78.0 & 90.7\\
    \hline
    \rowcolor{gray!20}\multicolumn{10}{c}{\it{\textbf{Database-Driven Retrieval Models}}} \\
    \hline
    13& GASKN       & 55.7 & 80.8 & 47.6 & 68.7 & 48.5 & 75.9 & 53.6 & 70.5\\
    14& SKG         & 57.8 & 82.1 & 45.4 & 65.3 & 49.2 & 76.1 & 56.8 & 75.4\\
    \noalign{\hrule height 1pt}
    \rowcolor{aliceblue!60} 15& \textbf{Uni-Retrieval}  & \textbf{83.2} & \textbf{98.7} & \textbf{84.5} & 95.6 & \textbf{76.9} & 97.5 & \textbf{87.4} & \textbf{98.1}\\ 
 \bottomrule[1.5pt]
\end{tabular}
}
}
\vspace{-2mm}
\caption{Retrieval performance for STEM Education Retrieval task.}
\label{tab:main_results}
\vspace{-3mm}
\end{table*}

We use our SER as the main experiment analysis and another three retrieval datasets to comprehensively evaluate the Uni-Retrieval's performance. For evalution metric, We evaluate the R@1 and R@5 metrics and the inference speed~(ms) on all retrieval datasets. For R@1 and R@5, ``$\uparrow$'' denotes that higher is better. For ms, ``$\downarrow$'' denotes that quicker is better.
Implement Details are detailed in Appendix ~\ref{supsubsec:Experiment Settings}.

\begin{table}[htp]
    \centering
    \resizebox{1.0\columnwidth}!{
    \begin{tabular}{c|cccc}
        \toprule[1.5pt]
        \textbf{Method} & \textbf{Params(M)} & \textbf{Q2I(ms)$\downarrow$} & \textbf{Q2T(ms)$\downarrow$} & \textbf{T$\rightarrow$I(Acc)}$\uparrow$ \\
        % \noalign{\hrule height 1.5pt}
        \hline
        CLIP & 427M & 68ms & 63ms & 54.6\\
        BLIP & 891M & 62ms & 58ms & 55.8\\
        VPT & 428M & 73ms & 69ms & 69.9\\
        LanguageBind & 1200M & 372ms & 367ms & 60.2\\
        GASKN & 33M & 12ms & 10ms & 55.7\\
        \hline
        \rowcolor{aliceblue!60} \textbf{Uni-Retrieval} & 453M{$_{\textcolor{red}{(+26)}}$} & 77ms{$_{\textcolor{red}{(+9)}}$} & 70ms{$_{\textcolor{red}{(+7)}}$} & 83.2{$_{\textcolor{red}{(+28.6)}}$}\\ 
        \bottomrule[1.5pt]
        \hline
    \end{tabular}}
    \vspace{-2mm}
    \caption{The models inference speed comparison.}
    \label{tab:speed}
    \vspace{-5mm}
\end{table}

\begin{table}[!h]
    \centering
    \resizebox{1.0\columnwidth}!{
    \begin{tabular}{c|cccc}
        \toprule[1.5pt]
        % \hline
        Method & T$\rightarrow$I & T+S$\rightarrow$I & I$\rightarrow$T & I+S$\rightarrow$T \\
        % \noalign{\hrule height 1.5pt}
        \hline
        CLIP-Finetune & 54.6 & 55.3{$_{\textcolor{red}{(+0.7)}}$} & 47.4 & 46.6{$_{\textcolor{green}{(-0.8)}}$}\\
        VPT & 69.9 & 72.0{$_{\textcolor{red}{(+2.1)}}$} & 73.9 & 74.1{$_{\textcolor{red}{(+0.2)}}$}\\
        \hline
        \rowcolor{aliceblue!60} \textbf{Uni-Retrieval} & 83.2 & 87.4{$_{\textcolor{red}{(+4.2)}}$} & 81.7 & 83.3{$_{\textcolor{red}{(+1.6)}}$}\\ 
        \bottomrule[1.5pt]
        % \hline
    \end{tabular}}
    \vspace{-2mm}
    \caption{Retrieval performance with multi-style queries simultaneously on SER dataset.}
    \label{tab:multi-query}
    \vspace{-5mm}
\end{table}

\begin{table*}[!tbp]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.05}  % 控制行高
\setlength{\tabcolsep}{2.25mm}        % 控制列间距
{
\begin{tabular}{c|c|cccc|ccc|cc|c}
    \toprule[1.5pt]
    % \multirow{2}{*}{\textbf{\#}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Text} \textbf{$\rightarrow$} \textbf{Image}} & \multicolumn{2}{c|}{\textbf{Sketch} \textbf{$\rightarrow$} \textbf{Image}} & \multicolumn{2}{c|}{\textbf{Art} \textbf{$\rightarrow$} \textbf{Image}} & \multicolumn{2}{c}{\textbf{Low-Res} \textbf{$\rightarrow$} \textbf{Image}} \\ 
    
    \textbf{\#} & \textbf{Method} & \textbf{I$\rightarrow$T} & \textbf{S$\rightarrow$T} & \textbf{A$\rightarrow$T} &  \textbf{L$\rightarrow$T} & \textbf{T$\rightarrow$S} & \textbf{T$\rightarrow$A} & \textbf{T$\rightarrow$L} & \textbf{S$\rightarrow$A} & \textbf{S$\rightarrow$L} & \textbf{A$\rightarrow$L} \\
    
    % \cmidrule(rl){3-4}\cmidrule(rl){5-6}\cmidrule(rl){7-8}\cmidrule(rl){9-10}
    % & & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} \\

    \noalign{\hrule height 1.5pt}
    \rowcolor{gray!20}\multicolumn{12}{c}{\it{\textbf{Metric: R@1$\uparrow$ on SER Dataset}}} \\
    \hline
    1& CLIP          & 47.4 & 38.4 & 37.9 & 38.6 & 38.8 & 37.4 & 35.7 & 36.9 & 34.8 & 31.5 \\
    2& BLIP          & 48.9 & 39.2 & 38.4 & 39.5 & 39.7 & 37.1 & 36.5 & 35.0 & 34.9 & 32.6 \\
    3& CLIP-Finetune                & 75.7 & 72.4 & 71.3 & 69.8 & 70.2 & 68.5 & 67.7 & 65.4 & 66.8 & 66.3\\
    4& BLIP-Finetune                & 77.4 & 73.0 & 72.6 & 70.5 & 71.3 & 69.4 & 68.1 & 66.2 & 67.2 & 67.0\\
    \hline
    5& LanguageBind     & 55.4 & 54.9 & 53.1 & 53.4 & 49.7 & 48.7 & 49.1 & 46.2 & 46.8 & 45.9\\
    6& Unified-IO2   & 57.3 & 57.2 & 56.3 & 54.5 & 51.1 & 49.9 & 48.6 & 48.0 & 47.2 & 46.8\\
    \hline
    7& SceneTrilogy  & 72.4 & \textbf{76.5} & 70.6 & 71.5 & 69.3 & 69.9 & 68.7 & 65.2 & 66.2 & 64.4 \\
    8& FashionNTM   & 70.6 & 73.3 & 68.9 & 69.6 & 67.1 & 68.0 & 66.5 & 67.5 & 64.8 & 62.4 \\
    \hline
    9& VPT          & 73.9 & 71.8 & 70.4 & 68.7 & 69.0 & 68.2 & 67.4 & 66.6 & 64.5 & 63.8\\
    10& CoCoOP       & 76.5 & 74.7 & 73.4 & 74.0 & 71.4 & 72.3 & 70.8 & 68.9 & 67.2 & 67.3\\
    11& MaPLe      & 78.3 & 75.8 & 75.7 & 74.9 & 72.4 & 69.6 & 69.2 & 68.3 & 67.4 & 65.6 \\
    12& FreestyleRet & 80.8 & 73.5 & \textbf{75.5} & 71.4 & 73.0 & 68.3 & 68.0 & 69.4 & 70.6 & 68.9 \\
    \hline
    13& GASKN        & 53.8 & 52.9 & 52.6 & 50.7 & 49.4 & 47.9 & 46.0 & 47.1 & 47.3 & 45.9\\
    14& SKG          & 54.3 & 51.7 & 50.4 & 51.3 & 48.5 & 46.1 & 45.4 & 46.9 & 47.0 & 45.9\\
    \noalign{\hrule height 1pt}
    \rowcolor{aliceblue!60} 15& \textbf{Uni-Retrieval}  & \textbf{81.7} & 76.3 & \textbf{74.9} & \textbf{77.6} & \textbf{73.5} & \textbf{74.2} & \textbf{78.0} & \textbf{71.4} & \textbf{72.3} & \textbf{70.8}\\ 
 \bottomrule[1.5pt]
\end{tabular}
}
\vspace{-2mm}
\caption{Retrieval performance for STEM Education Retrieval task.}
\label{tab:other_results}
\vspace{-5mm}
\end{table*}



\subsection{Comparison Experiment}

On the SER dataset, Uni-Retrieval demonstrates superior performance across multiple scenarios with different query styles compared to other baselines, including multi-modality models, cross-modality pre-trained models, and prompt learning models. As shown in Tab.\ref{tab:main_results} and Tab.\ref{tab:other_results}, the $T+S\!\rightarrow\!I$ means inputting the text and the style images as the multi-queries and outputting the corresponding images as the target queries. The experiment results yield three key observations:

\noindent \textbf{The Uni-Retrieval achieves the best retrieval performance on the multi-style STEM Education Retrieval task:} This highlights the effectiveness of Uni-Retrieval in handling complex multi-modal queries. Due to the Prompt Bank's structure, Uni-Retrieval is a plug-and-play framework that can be highly flexible applied to various multi-modal models and enhance their retrieval capabilities. Line 15 in Tab.\ref{tab:main_results} provides a substantial performance boost compared to both CLIP and CLIP-Finetune, further validating the effectiveness of our framework.

\noindent \textbf{The Prototype module and Prompt Bank significantly outperform full-parameter fine-tuning:} As shown in lines 3-4 and line 15 of Tab.\ref{tab:main_results}, Uni-Retrieval surpasses its fine-tuned CLIP counterpart by a large margin. Leveraging the prior knowledge bias introduced by the Prototype module and the efficient memory space of the Prompt Bank, Uni-Retrieval achieves superior results while tuning less than 5\% of the model’s total parameters. This demonstrates the effectiveness of Uni-Retrieval’s design in achieving high performance with minimal parameter adjustments.

\noindent \textbf{Uni-Retrieval can simultaneously perform and mutually enhance traditional text-image retrieval performance:} As shown in Tab.\ref{tab:multi-query}, when handling text-image retrieval tasks, Uni-Retrieval allows multi-query inputs as additional references, significantly improving retrieval capability. This multi-query input design is not exclusive to Uni-Retrieval and can also benefit other retrieval models, offering a generalizable approach to enhancing retrieval tasks.

In addition to accuracy, inference speed is a crucial metric for evaluating retrieval models. As shown in Tab.\ref{tab:speed}, Uni-Retrieval adds just 9ms per search iteration. 
Compared to GASKN, Uni-Retrieval demonstrates significantly stronger retrieval performance than traditional database-driven methods. 
Additionally, when compared to other cross-modality methods, Uni-Retrieval excels in both tuning efficiency and retrieval accuracy, further validating its effectiveness and scalability.

\begin{table}[h]
    \centering
    
    % \vspace{-1.0em}
    \resizebox{1.0\columnwidth}!{
    \begin{tabular}{c|c|c|cccc}
        \toprule[1.5pt]
        \textbf{\#} & \textbf{Type} & \textbf{Token-Num} & \textbf{T\textbf{$\rightarrow$}I} & \textbf{S\textbf{$\rightarrow$}I} & \textbf{A\textbf{$\rightarrow$}I} & \textbf{L\textbf{$\rightarrow$}I} \\ 
        \hline
        % \midrule
        1 & Deep & 1 & 72.0 & 78.3 & 73.2 & 80.6\\
        2 & Deep & 2 & 77.1 & 81.2 & 75.5 & 85.8\\
        3 & Deep & 8 & \textbf{83.24} & 82.7 & 76.5 & 87.0\\
        4 & Shallow & 4 & 68.2 & 75.6 & 70.4 & 77.3\\
        \hline
        \rowcolor{aliceblue!60} 
        5 & Deep & 4 & 83.2 & \textbf{84.5} & \textbf{76.9} & \textbf{87.4}\\
        \bottomrule[1.5pt]
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{Ablation study for prompt settings.}
    \label{ablation:prompt}
    \vspace{-5mm}
\end{table}


\subsection{Ablation Study}

To quantitatively evaluate the role of prompts in the model, we conducted ablation studies on the insertion type and token number of prompt tokens within Uni-Retrieval for four style metrics. These studies aimed to assess their impact on the style-diversified STEM education retrieval task, providing insights into how prompts influence performance and model adaptability. The shallow type involves inserting prompt tokens only at the first layer, while the deep type inserts prompt tokens across all layers. The token number refers to the number of repeated prompts. %For instance, if the token number is two, the prompt token from the Prompt Bank is repeated twice and inserted into the token sequence.

As shown in lines 4-5 in Tab.\ref{ablation:prompt}, the results indicate that the deep type consistently outperforms the shallow type. Additionally, lines 1-3 and line 5 in Tab.\ref{ablation:prompt} demonstrate the change on number of prompt tokens. We observed that repeating the prompt tokens more than four times does not significantly improve model performance. Instead, it rapidly increases the number of tuning parameters, which slows down both the tuning process and inference speed. This indicates that four repetitions provide a balanced trade-off between performance and computational efficiency. Therefore, we ultimately selected four prompt tokens to be inserted at each layer as the standard configuration for Uni-Retrieval. This choice effectively balances model performance, tuning efficiency, and inference speed, which can serve as a valuable reference for other tuning models.

\begin{table}[h]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.05}  % 控制行高
\setlength{\tabcolsep}{0.5mm}        % 控制列间距
{
{
\begin{tabular}{c|cc|cc|cc}
    \toprule[1.5pt]
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Text} \textbf{$\rightarrow$} \textbf{Im.}} & \multicolumn{2}{c|}{\textbf{Sketch} \textbf{$\rightarrow$} \textbf{Im.}} & \multicolumn{2}{c}{\textbf{Art} \textbf{$\rightarrow$} \textbf{Im.}}  \\ 
    
    \cmidrule(rl){2-3}\cmidrule(rl){4-5}\cmidrule(rl){6-7}
    & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} & {R@1$\uparrow$} & {R@5$\uparrow$} \\

    \noalign{\hrule height 1.5pt}
    \rowcolor{gray!20}\multicolumn{7}{c}{\it{\textbf{Diverse-Style Retrieval Dataset}}} \\
    \hline
    LanguageBind     & 71.0 & 95.5 & 50.8 & 79.4 & 58.2 & 86.3 \\
    CoCoOP~       & 71.4 & 94.6 & 77.5 & 97.2 & 69.3 & 97.1 \\
    MaPLe         & 73.1 & 95.9 & 80.3 & 97.9 & 70.6 & 97.2 \\
    FreestyleRet  & 71.4 & 97.2 & 81.6 & 98.0 & 72.3 & \textbf{98.1} \\
    \hline
    \rowcolor{aliceblue!60}
    Uniretrieval          & \textbf{82.3} & \textbf{97.4} & \textbf{82.7} & \textbf{98.9} &                                             \textbf{75.1} & 98.0 \\
    \hline
    \rowcolor{gray!20}\multicolumn{7}{c}{\it{\textbf{DomainNet Dataset}}} \\
    \hline
    VPT          & 59.7 & 86.1 & 53.5 & 77.3 & 54.6 & 81.8 \\
    BLIP-Finetune               & 65.3 & 94.2 & 71.4 & 89.7 & 54.3 & 82.3 \\
    FreestyleRet & 70.2 & 95.2 & 75.2 & 93.2 & 73.1 & 92.6 \\
    \hline
    \rowcolor{aliceblue!60}
    Uniretrieval         & \textbf{70.7} & \textbf{96.0} & \textbf{77.6} & \textbf{94.1} &                                             \textbf{73.4} & \textbf{92.9} \\
    \hline
    \rowcolor{gray!20}\multicolumn{7}{c}{\it{\textbf{SketchCOCO Dataset}}} \\
    \hline
    MaPLe        & 26.4 & 53.9 & 18.0 & 48.3 & - & - \\
    SceneTrilogy & 30.6 & 65.8 & 22.5 & 51.6 & - & - \\
    FreestyleRet & 31.5 & 67.3 & 29.6 & 56.1 & - & - \\
    \hline
    \rowcolor{aliceblue!60}
    Uniretrieval         & \textbf{34.7} & \textbf{71.6} & \textbf{30.2} & \textbf{60.4} & - & - \\
 \bottomrule[1.5pt]
\end{tabular}
}
}
\vspace{-2mm}
\caption{The zero-shot retrieval performance comparison on retrieval datasets.}
\vspace{-5mm}
\label{tab:multi_results}
% \vspace{-1pt}
\end{table}

\begin{figure*}[h]
  \centering
   \includegraphics[width=0.95\linewidth]{imgs/case_study.pdf}
   \vspace{-2mm}
   \caption{The case study for our Uni-Retrieval and the FreestyleRet baseline.}
   \vspace{-5mm}
   \label{fig:visualization}
\end{figure*}

We also evaluated Uni-Retrieval’s zero-shot retrieval performance on several other multi-style datasets. As shown in Tab.\ref{tab:multi_results}, we compared Uni-Retrieval against various baseline models across three datasets: the DSR, DomainNet, and SketchCOCO dataset, each representing distinct domains of style-based queries. 
%The DSR dataset share the same style types as our SER dataset, making them suitable benchmarks for style-consistent retrieval evaluation. 
%The DomainNet dataset includes art images and sketch images, while the SketchCOCO dataset exclusively contains sketch images. Consequently, we evaluated the corresponding style retrieval performance on these datasets to assess Uni-Retrieval’s capability across varying domains and styles. 
As shown in Tab.\ref{tab:multi_results}, Uni-Retrieval demonstrates exceptional zero-shot performance across these diverse datasets, highlighting its capability to perform effective information retrieval in various previously unknown databases.
This performance underscores Uni-Retrieval’s scalability and robustness, significantly enhancing the adaptability and effectiveness of existing retrieval models in handling diverse and unstructured data domains.

\subsection{Visualization Result}


In Fig.\ref{fig:visualization}, we visualize the style-diversified query inputs and their corresponding retrieval answers from our Uni-Retrieval and the FreestyleRet baseline model. We summarize three common retrieval errors in the case analysis, where subject errors, semantic errors, and color errors represent the false retrieval result with false subjects, semantic information, and colors. We propose the subject error cases in Fig.\ref{fig:visualization}(a1)-(c1). The subject information is contained widely in different style queries. Thus, pose error cases occur in sketch, art, and low-resolution queries. 
Subject information is conveyed through the primary objects in images and their corresponding textual descriptions. Subject errors occur when there is incorrect recognition or classification of these objects, leading to mismatched associations between the image and text.

Semantic errors, on the other hand, arise from inaccuracies in describing object details. These errors frequently occur when irrelevant text is associated with specific parts of an object, particularly in the context of art descriptions. Such mismatches result in the model generating incorrect attention maps, thereby failing to accurately connect the visual and textual elements. Thus, in Fig.\ref{fig:visualization}(a2)-(c2), most of the semantic errors occur in the art-style retrieval task. 

For the low-resolution query retrieval task, color is vital retrieval information. We show the color errors from the low-resolution retrieval task.  Compared to the FreestyleRet baseline model, our Uni-Retrieval achieves fine-grained retrieval based on subject, semantic, and color information from style-diversified query inputs. It demonstrates a superior understanding of semantic information and fine-grained alignment between modalities, particularly in the precise description and representation of key object parts. This highlights the significant advantages and capabilities of our Uni-Retrieval framework.