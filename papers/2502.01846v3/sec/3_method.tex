\vspace{-0.15cm}
\section{Methodology}

\noindent\textbf{Preliminaries}:
% \dilin{just a minor suggestion, not sure if you would like to use $\mu$ to represent the center, $\sigma$ is often used as the standard derivation.} \ar{I was planning to use $\mu$ for VAE, but we didn't end up describing it. I'll change if we have time at the end.}
3DGS represents an object or a scene with a collection of Gaussians primitives to model the geometry and view-dependent appearance. 
% \dilin{we don't have view-dependent effect?} \ar{Nope}. 
For a 3DGS set, \(G =\{g_i\}_{i=1}^N\), representing an object with $N$ individual Gaussians, the geometry of the $i^{th}$ Gaussian is explicitly parameterized via 3D covariance matrix $\Sigma_i$ and it's center $\sigma_i \in \mathbb{R}^3$ as:
% \ns{put gaussian equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
$
    g_i(x) = e^{(-\frac{1}{2} (x - \sigma_i)^T \Sigma^{-1} (x - \sigma_i))}
$
where, the covariance matrix $\Sigma_i = r_i s_i s_i^T r_i^T$ is factorized into a rotation matrix $r_i \in \mathbb{R}^4$ and a scale matrix $s_i \in \mathbb{R}^3$. 
The appearance of the $i-th$ Gaussian is represented by a color value $c_i \in \mathbb{R}^3$ and an opacity value $o_i \in R$. 
In practice, the color is represented by a series of Spherical Harmonics (SH) coefficients, but for simplicity, we represent the view-independent color by just RGB values. 
% \ns{In practice I'm not sure if we can say that RGB values are used since several papers use SH directly}
% \ar{I see in most of the generative and reconstruction papers, people tend to ignore SH.}
Thus, a single Gaussian can be represented by a set of five attributes as $ g_i = \{ \sigma_i, ~r_i, ~s_i, ~o_i, ~c_i \} \in \mathbb{R}^{14}$, and the entire 3DGS can be represented by a set of $N$ such Gaussians as:
$G = \{ \{ \sigma_i, ~r_i, ~s_i, ~o_i, ~c_i \} \}_{i=1}^N$.

\subsection{Spherical Mapping}
3DGS is represented as a permutation invariant set with no structural correspondence among different Gaussians $g_i$, making it challenging to extract meaningful features from this set containing a few hundred thousands of them using neural networks. 
% Thus, it is fairly complicated and non-trivial to use the existing large number of neural network frameworks such as, generative models, autoencoders, or anything in general for feature extraction from
% \ns{same with above: SUCH AS ???} 
% \ar{Such as, generative models, autoencoders, anything in general for feature extraction.} 
% this representation. \dilin{no need to repeat (first sentence)?}
% \ar{Basically, what I want to emphasize that due to the unstructured and permutation invariant nature of 3DGS, we can not use the existing models directly to process 3DGS and extract local and global features. The same motivation is mentioned in the recently released DiffGS and other similar papers. Shall I cite them here??}
% \ns{maybe let's clarify a little what we mean by this sentence and provide a stronger motivation for the next sentence}
To address this, we introduce a novel representation that gives structure to this unstructured set of points and solves the permutation invariance issue for faster and better feature extraction. We propose to accomplish this by employing spherical mapping to map the 3DGS primitives to an image-like representation that is both invariant to random shuffling of 3DGS points and well structured. % Nikos: Minor comment but we haven't explained the novel aspect of it just yet. I'll eventually rephrase this
% \srinath{why spherical mapping? Why not cylindrical or cubic?}
% We prefer spherical mapping as others generally fails to capture the top and bottom parts of the object in the same UV map, and can introduce distortions for objects that extend far in the Z-direction.



We begin the mapping 
% that is described in detail in the supplementary 
% \dilin{cannot be in supplementary :) ?}, 
by inscribing the 3DGS object into a sphere with the same center as the object in the canonical space.
% \ns{inscribing??? can we maybe explain this with a few words}
Inscribing a 3DGS object into a sphere involves enclosing the object within a sphere. This begins by determining the geometric center of the object. The next step is to calculate the radius of the sphere, which is achieved by measuring the Euclidean distance from the center to the farthest point on the object. The radius of the sphere is defined such that the sphere fully encloses the object. The sphere acts as a bounding volume for the entire object.

We consider each Gaussian $g_i$ in 3D to be centered at the mean position represented by $\sigma_i$ with Cartesian coordinates $(x_i,y_i,z_i)$. 
The aim is to get the spherical coordinates $(\rho_i,\theta_i,\phi_i)$ for each Gaussian $g_i$. To do so, we calculate the azimuthal $\theta_i$ and polar $\phi_i$ angles for each $g_i$ along with the distance from the origin to the point, $\rho_i$. The spherical radius is defined as \(\rho_i = \sqrt{x_i^2 + y_i^2 + z_i^2}\), the azimuthal angle as \( \theta_i = \tan^{-1}(y_i, x_i)\), while the  polar angle as \( \phi_i = \cos^{-1}(z_i, \rho_i) \). 
The azimuthal and polar angles are then normalized, such that we can map them on a 2D UV map of $M \times N$ dimensionality with 14-channels. 
% \ns{Please Merge the following to one equation for theta and phi as converting to angles is not really interesting to take that much space :) }
\(\theta_i\) and \(\phi_i\) are converted to degrees and mapped to UV image coordinates: \(\theta_{i~\text{scaled}} = \left\lfloor \frac{\pi + \theta_{i}}{2\pi} \times \text{M} \right\rfloor, \; \; \phi_{i~\text{scaled}} = \left\lfloor \frac{\phi_i}{\pi} \times \text{N} \right\rfloor\)
   % \[
   % \theta_{i~\text{scaled}} = \left\lfloor \frac{\pi + \theta_{i}}{2\pi} \times \text{M} \right\rfloor
   % \]
   % \[
   % \phi_{i~\text{scaled}} = \left\lfloor \frac{\phi_i}{\pi} \times \text{N} \right\rfloor
   % \]
Each channel in the UV map stores 3DGS attributes, including $\{ \sigma_i, ~r_i, ~s_i, ~o_i, ~c_i \} \in \mathbb{R}^{14}$. We refer this 14-channel UV map as \emph{UVGS}, $U \in \mathbb{R}^{M\times N \times14}$ defined as:
\begin{equation}
    \text{U}[\phi_{i~\text{scaled}}, \theta_{i~\text{scaled}}, :] = [ \sigma_i, ~r_i, ~s_i, ~o_i, ~c_i ]. %\nonumber
\end{equation}

This transformed UVGS representation provides spatial coherence and solves the permutation invariance problem as any random arrangement of points will now map to the same UVGS representation $U$.
It should be noted that this kind of transformation will also preserve the spatial correlation between the Gaussian points in 3D and transform them to 2D UV maps by mapping them to neighboring pixels. 
This provides both the local level correspondence among the neighboring Gaussians and the overall global correspondence for the object. 
Thus, solving the unstructured and discreteness problems.
This enables standard neural network architectures (\eg CNNs) to effectively capture correlations among neighboring Gaussians for efficient feature extraction. 
% \dilin{even though it might be obvious already, but i feel like it would benefit to further discuss how nearby gaussians would be mapped to nearby pixels in the UVGS representation.}
% \ar{answered above}
% \st{In our case, we fix the size of the UV maps to $512\times512$. Through our experiments, we realized that UV maps of size $512 \times 512$ are sufficient to represent objects in our dataset and capable of storing upto $262K$ Gaussians.}\ar{transferred to experiments}

% \st{However, the use of this UVGS representation with the existing image based models still remains a challenge as most of these models are structurally defined to operate on a 3-channel image.} 
% \ar{To further unify the extracted position ($\sigma$), transformation ($r, s$), and color ($c, o$) maps in a same feature space and use the existing image foundation models, we map...}
% \dilin{This seems like a very strange argument to me. I guess, the point to be able to directly reuse all pretrained foundation models? since that you want to map the channel to 3? But there's no constraints on the network design? Also wouldn't it make more sense to say that because we now have scales and colors in the channel dimension, which have very different meanings, we would like to further unify them in the same feature space?} 
% \ar{Yes, I think this makes more sense and also provides a sufficient explanation to why we had to use branching.}
%
% \ns{In the previous sentece explain more clearly what do you mean by "application". I know it'll be clear later but currently it's not. }
To further unify the extracted position ($\sigma$), transformation ($r, s$), and color ($c, o$) maps in a same feature space and use the existing image foundation models, we map the obtained UVGS $U \in \mathbb{R}^{M\times N \times14}$ further to a 3-channel image $S \in \mathbb{R}^{M\times N \times 3}$ (termed as Super UVGS), using a Convolutional Neural Network (CNN).
% We might get a Q why we do that this way vs alternatives. Let's discuss a little 
A multi-branch forward mapping network is employed to map $U \in \mathbb{R}^{M\times N \times14}$ to the 3-channel Super UVGS $S \in \mathbb{R}^{M\times N \times 3}$. We provide all technical details in Sec.~\ref{ssec:mapping} and the supplementary material. 
% on that in Section~\ref{ssec:mapping}
% \ns{Can we draw a parallelism here with neural textures that embed in HxWxC dims a feature map that maps to textures while keep the first 3 channels for rgb.}
% \ar{"kind of" for UVGS. unlike neural textures, we also store position and transformation information in the maps.}

The Super UVGS representation effectively retains all the details of 3DGS attributes and can be directly utilized with existing widely available image-based models. 
We demonstrate this by showing perfect reconstruction of 3DGS object from Super UVGS image in the experiments section.
% \ns{where do we demonstrate this? Let's either get into details here or tell them that we do it in the experimental section} \ar{We are demonstrating this by showing almost perfect reconstruction of 3DGS object from Super UVGS image.}
This semantically structured representation $S$ offers both local and global correspondence 
in representing Gaussian attributes and needs relatively less storage. 
% \dilin{local and global correspondence was mentioned a couple of times but without an explicit explaintion, as well as why it's important} 
% \ar{answered above in RED}
% To reconstruct the object back from a Super UVGS represent, we train an inverse mapping network that maps a 3-channel Super UVGS image back to a 14-channel UV map representing all five 3DGS attributes. 
% Through inverse spherical projection, we can easily reconstruct the 3DGS object with minimal computation. 

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{imgs/dynamic_selection.jpg} 
\vspace{-0.2cm}
\caption{
\textbf{Dynamic Selection}. In spherical mapping of 3DGS points to UV maps, multiple points may map to the same pixel, creating a many-to-one issue. Our Dynamic Selection approach addresses this by retaining the attributes of the point with the highest opacity per pixel on the same ray.
% \dilin{should add an illustration of the multi-layer here, otherwise, the figure alone only delivers a negative information.}
}
\label{fig:dynamic_sel}
\vspace{-0.4cm}
\end{figure}
\vspace{-0.2cm}

% \vspace{-0.1cm}
\subsection{Dynamic GS Selection and Multiple Layers}
% \dilin{i would suggest to move this section right after sec 3.1} \ar{Moved here from 3.4}
When projecting 3DGS points to UV maps using spherical mapping, multiple points may map to the same pixel in UV space as shown in Fig.~\ref{fig:dynamic_sel}. 
Two 3DGS points $( g_1 )$ and $( g_2 )$ map to the same pixel on UV map $( P_a )$ causing many-to-one mapping issue. 
% \ns{Re-iterate here why we don't want that which will then motivate the next sentence}
To address this, we propose a Dynamic Selection approach where each UV pixel retains the 3DGS attribute with the highest opacity intersecting the same ray.
Using the same example in Fig.~\ref{fig:dynamic_sel}, if opacity $o_1$ of Gaussian $g_1$ is less than opacity $o_2$ of $g_2$. Then only $g_2$ will be stored in the UV map at pixel $P_a$. 
We observed that this method helps maintain the geometry and appearance of the 3DGS object while resolving many-to-one mapping issues with minimal quality loss.
% \st{Recent literature has also shown that a single Gaussian point can represent both simple and somewhat complex geometries with sufficient quality.} \dilin{this is kind of confusing, because we're discussing a multi-layer solution right afterwards}. % CITE Splatter-Image and LGM papers and whatever else
%
For more complex objects or real-world scene representation, we stack multiple such layers of UV maps, where each UVGS pixel now holds attributes of the top-K opacity values of 3DGS primitives. 
This can be accomplished by inscribing the 3DGS object inside multiple spheres where each sphere maps the 3DGS attribute corresponding to the top-$K^{th}$ opacity value along the same ray. More details on this are presented in the supplementary.
To show the effectiveness of proposed UVGS maps in capturing the intricacies of a complex objects, we used a pretrained image based autoencoder to reconstruct objects using a 4 layer UVGS as shown in Fig.~\ref{fig:complex_recons}. 
% \ar{Do we need to put a reference to Supplementary for scenes or something?}
% \ns{understand should be accomplished maybe???}\ar{fair point!}
% \ar{Moved the following to experiments section:}
% \st{Table XX compares 1-8 layer UV maps. 
% Fig.~[XX] in supplementary shows that this multi-layer UVGS mapping can be used to even map real-world complex scenes to structured UV maps and can be reconstructed back with high-fidelity. 
% Interestingly, for most of the objects in our dataset with convex geometry, a single-layer UVGS map (K=1) suffices without significant quality loss. Therefore, we assume a single-layer UV map for our experiments, unless stated otherwise. }
% \ns{lets not use concerned anywhere in the paper :) you can write general purpose objects, objects in our dataset or something more descriptive}
% \dilin{i feel the following is not necessary..it's very clear before already.} \st{Note that a multi-layer UVGS map should not be confused with a 14-channel UVGS $U \in \mathbb{R}^{M\times N \times14}$ representing the 5 attributes. A 2-layer UVGS map will have \(14 \times 2 = 28\) channels, with each attribute represented by 2 identical attribute specific layers. 
% Similarly, a 4-layer UVGS map will have \(14 \times 4 = 56\) channels.}




\vspace{-0.1cm}
\subsection{Mapping Networks}\label{ssec:mapping}
% FORWARD MAPPING
Our goal is to bring the extracted UVGS maps to a common feature space to better represent the object collectively and to make the 14-channel UVGS representations $U \in \mathbb{R}^{M\times N \times14}$ work with the widely available image based foundation models. To accomplish this, we map it to a 3-channel image which can be easily processed by the existing architectures while also maintaining the spatial correspondence. 
%
% \ar{To bring the extracted 3DGS attribute maps to a common feature space to better represent the object collectively and to make...}
% to make the 14-channel UVGS representations $U \in \mathbb{R}^{M\times N \times14}$ work with the widely available image based foundation models 
% \ar{, we map it to a 3-channel image which can be easily processed by the existing architectures while also maintaining the spatial correspondence. }
% \dilin{without the need of any type of training or finetuning?} \ar{Yes, in case of AE and VAE.}
% \ns{Is it really imperative? What other options exist and why we didn't go down these paths}\ar{todo}
%
We design a simple yet effective multi-branch CNN to extract features from different UVGS attributes and map them to a 3-channel feature-rich image, termed Super UVGS. The structured UVGS maps provides local and global features that can be learned by a CNN.
% \dilin{need to explain the intuition a little bit; otherwise, the setup doesn't look correctly. Essentially, there is redundancy between gaussians, and a CNN can encode some of this information implicitly. That's why we can compress the data.}\ar{answered above} \ap{this is still not answered. The intuition is redundancy reduction which is implicitly represented by CNN}

\noindent\textbf{Forward Mapping}
The first layer is a set of three mapping branches for position, transform, and appearance ($\phi^f_{P}, ~\phi^f_{T}, ~\phi^f_{A}$) respectively. We refer to them as position, transformation, and appearance branch. 
The position branch takes the mean position ($\sigma$) as an input and processes it to give a position feature map $M_{P}$.
Similarly, the transformation branch takes the rotation ($r$) and scale ($s$) together to generate another feature map $M_{T}$.
The last, appearance branch takes the color ($c$) and opacity ($o$) together to produce another feature map $M_{A}$. 
All the three features maps from position, transformation, and appearance branch are concatenated to get a final feature map, before passing them to the next module, called the Central Branch. The central branch ($\phi^f_{C}$) is composed of multiple hidden Convolution layers, where each layer is followed by BatchNorm and ReLu activation.
The last layer of the central branch is activated using $tanh$ to ensure the Super UVGS does not take any ambiguous value resulting in gradient explosion or undesired artifacts. 
The obtained Super UVGS $S$ representation squeezes all the 3DGS attributes to a 3 dimensional image while also maintaining local and global structural correspondence among them. 
% The intuiting behind branching is explained in the end of this section. \dilin{why the end?} \ar{To not discontinue the methodology flow. I can put it here if that makes more sense.}
% \ns{Please discuss here in detail the intuition behind this architectural design  mentioned above and what benefits we get from the separate branches etc.} \ar{I've put this in the end of this section. Would to suggest moving it here?}
%

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{imgs/complex_recons.jpg} 
\vspace{-0.3cm}
\caption{
Complex object reconstructions (K=4) using pretrained image-based autoencoder.
}
\vspace{-0.6cm}
\label{fig:complex_recons}
\end{figure}


% REVERSE MAPPING
\noindent\textbf{Inverse Mapping}
We design an inverse mapping network that aims to map the obtained 3-channel Super UVGS image $S \in \mathbb{R}^{M\times N \times 3}$ back to the UVGS maps to obtain each of the five different 3DGS attributes $\{ \sigma, ~r, ~s, ~o, ~c \}$. 
% \ns{Why do we need that. Atm is not motivated... you do it further down but you need to start with it}
The inverse mapping network simply follows the forward mapping network architecture in the reverse order, where at first, we put the Central Branch ($\phi_{iC}$) followed by attribute specific position, transformation, and appearance branches ($\phi_{iP}, ~\phi_{iT}, ~\phi_{iA}$). We provide more details on mapping network in the supplementary.


% RATIONALE BEHING BRANCHING
\noindent\textbf{Branched mapping layers}: 
% \dilin{this paragraph seems quite long..} \ar{made it short}
The rationale behind using branched mapping layers in both forward and reverse mapping networks is to prevent the incompatibility issues arising due the the different value distribution of 3DGS attributes. 
Note that the disparate distributions of values within each set of attributes in 3D Gaussian Splatting, (\ie, mean position, transformation, and color), pose a challenge to the model when processed collectively. 
For instance, neighboring Gaussians in UVGS maps show smooth changes in position and color values but typically have large variations in rotation, scale, and opacity values. This results in gradient anomalies and slow convergence.
To address this, we propose a multi-branch network architecture, where attribute-specific branches implicitly learn to process these distinct attribute specific properties, focusing on their unique features before passing them to the central branch. The central branch receives a concatenated stack of processed attributes and exploits the correlation between them by extracting local feature correspondences. This information is then mapped to a 3-channel Super UVGS image, effectively capturing the complex relationships between the various attributes.
 This approach enables our network to manage diverse attribute distributions, resulting in faster convergence, improved accuracy, and specialized processing for each attribute set.


\noindent\textbf{Reconstruction Losses}:
Since the obtained UVGS maps have both local and global features, we opted for image-based losses to train the overall architecture. 
We use a set of Mean Squared Error (MSE) and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018perceptual}  between the obtained UVGS from spherical mapping $U$ and the predicted UVGS $\hat{U}$ from inverse mapping network. 
% \dilin{clarifying LPIPS is only applied to colors?}. \ar{No, for 4 attributes} 
We calculate the LPIPS loss over four attributes of 3DGS including mean position ($\sigma$), view independent color ($c$), scale ($s$), and rotation ($r$). 
The overall LPIPS loss for UV maps can be written as a linear sum of individual attribute loss terms as: 
% \dilin{i thought lpips is only pretrained on image space?} \ar{Yes, that is true. But similar to other models, even LPIPS can be effectively applied on individual imge-like UV maps to perform feature matching.}
\begin{equation}
    \mathcal{L}_{UV-lpips} =\mathcal{L}_{\sigma} + \mathcal{L}_{s} + \mathcal{L}_{r} + \mathcal{L}_{c}
\end{equation}
The overall loss function for the training can be written as: \(\mathcal{L}_{uvgs} = \mathcal{L}_{mse} + \lambda . \mathcal{L}_{UV-lpips}\) where $\lambda$ is a scalar and varied from $0$ to $10$ during the course of training.
% \ns{you have 2 lpips losses in equations being equal to different things. While I know what you mean this is not clear enough so I'd suggest we rephrase or add different indices in the losses}



% \subsection{UVGS AutoEncoder \& Latent Diffusion Model}
% \dilin{maybe treat 3.4 as a new section, like Applications?} \ar{Trying to move this section to Experiments entirely - WIP.}

% % \ns{This needs to be motivated in a more clear manner and make a strong case. Let's provide details why are we doing this}



