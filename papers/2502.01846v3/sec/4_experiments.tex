\vspace{-0.2cm}
\section{Experiments}
\vspace{-0.1cm}

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\linewidth]{imgs/unconditional_trimmed.jpg} 
\vspace{-0.2cm}
\caption{
Figure shows a wide variety of high-quality unconditional generation result from our method using Latent Diffusion Model. We train an LDM to randomly sample Super UVGS images from random noise. The Super UVGS can be converted to 3DGS object using inverse mapping network and inverse spherical projection. The unconditional generation model was trained on Objaverse dataset.
}
\label{fig:unconditional}
\vspace{-0.4cm}
\end{figure*}

\noindent\textbf{3DGS Dataset and UV maps}
To train the mapping networks and learn a latent space for unconditional and conditional sampling, we need large amount of 3DGS assets. 
However, there's a lack of such a large-scale dataset for high quality 3DGS assets. To this end, we create a custom large scale dataset by converting the Objaverse~\cite{objaverse2023} meshes into 3DGS representation \footnote{Sketchfab data was filtered out of training data due to its license}.
% \ns{\textbf{TO DO}: Dilin/Aayush let's find out the best way to discuss this to not raise eyebrows}
We start by designing a scene of 88 cameras in a canonical space and use it to capture Objaverse objects from various angles covering all of the object views.
The 88 rendered views from different angles are then used to train a 3DGS for 10K iterations using \cite{3dgs2023}. 
This way, we create a high-quality and large-scale 3DGS dataset of $\mytilde 400K$ objects and scenes from Objaverse.
We only use static scenes or objects from Objaverse.
% \dilin{not sure why do we want to mention colmap here.}
% \st{Interestingly, we did not use COLMAP to get the extrinsic camera parameters, but instead we directly converted the Blender scene cameras into COLMAP format to use them for 3DGS fitting, thus avoiding loose feature matching arising due to COLMAP on simpler objects or scenes.}
After fitting all the object to 3DGS representation, we convert the objects to the corresponding UV maps (\ie, UVGS) through Spherical Mapping as illustrated in Fig.~\ref{fig:architecture}. 
For the course of our experiments, we only map the objects to a single layer UV maps as it was sufficient to represent the general purpose Objaverse objects with minimal quality loss. 
Through mapping, we gathered a UVGS dataset of  $\mytilde 400K$ maps.
We fix the size of the UVGS maps to $512\times512$. Through our experiments, we found that UV maps of size $512 \times 512$ are sufficient to represent objects in our dataset and capable of storing upto $262K$ unique Gaussians. 
Table~\ref{table:psnr} compares 1-4 layer UV maps. 
We also did experiments on ShapeNet~\cite{shapenet2015} cars dataset for evaluation purposes.
% \ar{\textbf{Do we need the following line in paper?} Fig.~[XX] in supplementary shows that this multi-layer UVGS mapping can be used to even map real-world complex scenes to structured UV maps and can be reconstructed back with high-fidelity.}

\vspace{-0.1cm}
\noindent\textbf{Baselines \& Metrics}: 
% We use the standard metrics to examine the quality of our reconstructions, unconditional and conditional generation experiments. 
To evaluate the quality of reconstructed 3DGS objects from both Super UVGS image and Autoencoder latent space to 3D, we use PSNR and LPIPS~\cite{lpips2018}. 
The aim is to convert the given 3DGS object to UVGS, and then to Super UVGS, and further to Autoencoder's latent space and calculate the metrics from the reconstructions at every step to prove the proposed method doesn't significantly affect the quality of reconstructions, while also providing a structurally meaningful representation that is much compact and easier to use with existing image based models. 
%
We compare the generational capabilities of our method against various conditional and unconditional SOTA 3D object generation method including the ones using multiview rendering for optimization DiffTF~\cite{difftf2023}, Get3D~\cite{get3d2022}, methods trying to give structural representation to Gaussians, GaussianCube~\cite{gaussiancube2024}, and general purpose SOTA large 3D content generation models like 
 DreamGaussian~\cite{dreamgaussian2023}, LGM~\cite{lgm2025}, and EG3D~\cite{eg3d2022}. 
We also compare the quality of our generation results using FID and KID.


\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{imgs/compare_cars.jpg} 
\vspace{-0.4cm}
\caption{
Comparison of unconditional 3D asset generation on the cars category with SOTA methods. Figure shows that DiffTF~\cite{difftf2023} produces low-quality, low-resolution cars lacking detail. While Get3D~\cite{get3d2022} achieve higher resolution, it suffers from 3D inconsistency, numerous artifacts, and lacks 3D detail. Similar issues are found in GaussianCube~\cite{gaussiancube2024} along with symmetric inconsistency in the results. In contrast, our method generates high-quality, high-resolution objects that are 3D consistent with sharp and well-defined edges.
}
\vspace{-0.4cm}
\label{fig:compare_cars}
\end{figure*}

% \vspace{0.2cm}

\noindent\textbf{Mapping Network Training Details}
We train the forward and inverse mapping networks to project the obtained UVGS maps $U \in \mathbb{R}^{M\times N \times14}$ to Super UVGS image $S \in \mathbb{R}^{M\times N \times 3}$, and back to the reconstructed UVGS maps $\hat{U} \in \mathbb{R}^{M\times N \times14}$. We provide an in-depth discussion of all implementation details in the supplementary material. 
% \ar{Moved here from Method section: 
% We used a pretrained image VAE [cite] $\Phi_{ae}$ for some epochs to let the mapping networks learn the VAE compatible Super UVGS images (needs to be rewritten in better words). 

\begin{table}[t]
\centering
\setlength{\tabcolsep}{0.75mm}
\renewcommand{\arraystretch}{1.2}
\caption{PSNR and LPIPS comparison for various reconstruction methods using UVGS and Super UVGS representations on Objaverse Cars and Full datasets. 
AE, VAE, VQVAE are pretrained image based models. $K$ is the number of UVGS layers used. 
We also report the compression \% (CP) compared to the fitted 3DGS.}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
  \toprule
 \textbf{Method} & \textbf{PSNR($C / F$)} & \textbf{LPIPS($C / F$)} & \textbf{CP($\%$)} \\
 \midrule
 3DGS &  $34.6 ~/~ 34.2$  & $0.02 ~/~ 0.02 $ & $0$ \\
 UVGS (@K=1) & $31.3 ~/~ 31.1$ &$0.06 ~/~ 0.06 $ & $53.0$ \\
 UVGS (@K=2) & $32.8 ~/~ 31.9$ & $0.04 ~/~ 0.05 $ & $45.6$ \\
 UVGS (@K=4) & $34.2 ~/~ 33.2$ & $0.02 ~/~ 0.03 $ & $33.3$ \\
 Super UVGS (@K=1) & $31.2 ~/~ 31.1 $ & $0.07 ~/~ 0.08 $ & $89.7$ \\
 AE (@K=1) & $30.9 ~/~ 30.8 $ & $0.07 ~/~ 0.09 $ & $99.5$ \\
 VAE (@K=1) & $30.6 ~/~ 30.9 $ & $0.07 ~/~ 0.09 $ & $99.5$ \\
 VQVAE (@K=1) & $30.3 ~/~ 30.1 $ & $0.08 ~/~ 0.10 $ & $99.7$ \\
\bottomrule 
\end{tabular}}
\label{table:psnr}
\vspace{-0.5cm}
\end{table}

\subsection{UVGS AutoEncoder and 3DGS Compression}\label{exp:uvgs_ae}
The obtained Super UVGS image is a structurally meaningful representation that can have various applications in the generation and reconstruction of new 3D assets as it contains features that can be learned by the existing image based models.
Through our experiments, we show that a 3-channel Super UVGS image can be directly reconstructed using a pretrained image based Autoencoders or VAEs without any fine-tuning. 
We tested on three different models including image AE, KL-VAE~\cite{KLvae2013}, VQVAE~\cite{vqvae2017} and each performed quite well without any significant quality loss. The reconstruction PSNR and LPIPS values are presented in Table~\ref{table:psnr}.
This means we can now leverage the powerful compression capabilities of image based Autoencoders to compress the storage requirements of 3DGS by more than $99\%$. We have shows the storage comparison results in Table~\ref{table:psnr}.
It is interesting to note that the Super UVGS representation itself can be used to compress the memory requirement for storing 3DGS object by up to $89.7\%$. 


% \dilin{we probably don't have enough time, but i guess reviewers would ask: 1/ what if you further finetune the AE/VAE; 2/ what if you train with UVGS from scratch (without even compressing on the channel dimension first}
% \ar{I'm training a VAE from scratch. Hope we get results in the next two days. Otherwise supplementarty or rebuttal.}

% 3DGS Compression

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{imgs/compare_text.jpg} 
\vspace{-0.4cm}
\caption{
We compare the performance of our model against various SOTA methods for text-conditional object synthesis. Our method not only generates high-quality assets for simpler objects, but also for complicated objects with intricate geometry. 
% \srinath{add circle around part of objects that people should focus on.}
}
\label{fig:compare_text}
\vspace{-0.6cm}
\end{figure}

\vspace{-0.1cm}
\subsection{Unconditional \& Conditional Generation}
We aim to show the effectiveness of Super UVGS representation for directly generating 3DGS objects from a learned latent space. 
We consider Super UVGS images as a compact and structured proxy for representing 3DGS objects as it maintains the 3D object information while also providing learnable features. 
The existing methods fail to directly generate a large number of Gaussians (\eg 100K+) to represent objects with sufficient quality either due to the lack of 3D generative model architectures that support such large number of unstructured points, or due to the lack of a large 3DGS dataset \cite{splatterimage2024, lgm2025, triplanemeetsgs2024, gaussiancube2024, gsd2024}. 
We leverage the Super UVGS representation and use the existing 2D image generative models like Diffusion Models~\cite{ddim} for this task. 
Specifically, to train a generative model capable of randomly sampling new high-quality 3DGS assets for various downstream tasks, we use an unconditional Latent Diffusion Model (LDM)~\cite{stable_diffusion} on the obtained Super UVGS images. 
As illustrated in Section~\ref{exp:uvgs_ae}, we can use a pretrained image VAE to map the Super UVGS image to a latent space and reconstruct back. Hence, we only train a LDM on the latent space. More implementation details are provided in the supplementary.

% \vspace{0.2cm}

\noindent\textbf{Unconditional LDM:}
To design a generative model capable of randomly sampling new high-quality 3DGS assets for various downstream tasks, we train an unconditional LDM~\cite{stable_diffusion} on the learned Super UVGS images. 
% We can consider Super UVGS images as a compact and structured proxy for representing 3DGS objects as it maintains the 3D information of the object, while also providing local and global features for learning. 
% As illustrated in the previous section, we are able to use pretrained image VAE for mapping the Super UVGS image to a latent space and back to the reconstructed Super UVGS. To this end, we only need to efficiently train a LDM on the latent space. 
Following~\cite{stable_diffusion, controlnet, egosonics}, we use DDIM~\cite{ddim} for faster and consistent sampling with up to 1000 time steps used in the forward diffusion process, and 20 during denoising. 
% Training was done using AdamW optimizer with a learning rate of $1e-4$ for 50 epochs on $8 \times A100 ~(80GB)$ GPUs.
Once trained, the model is used to randomly sample Super UVGS images, resulting in high quality 3DGS assets through inverse mapping. Results are presented in Fig~\ref{fig:unconditional}. We demonstrate that our method inherently learns to generate multiview consistent images due to the powerful Super UVGS representation unlike most prior works using rendering-based losses. 
% Baseline comparison for unconditional generation is presented in Table~\ref{table:unconditional}.

% \vspace{0.2cm}
% Conditional LDM
\noindent\textbf{Conditional LDM}:
Similar to unconditional generation, we also trained a text-conditioned LDM following the SD's~\cite{stable_diffusion, controlnet, egosonics} pipeline and using the predicted text for our dataset.
%
The trained model can be used to generate high-quality text-conditioned 3DGS assets that are multiview consistent. The results are demonstrated in Fig~\ref{fig:compare_cars}. 
% Iplementation details are provided in the supplementary.





% Closing Statement
The above experiments proves the effectiveness of our proposed Super UVGS representation in 3D object synthesis using widely available 2D image models. It also highlights that this compact 3-channel Super UVGS representation stores not just the spatial correspondence among different pixels, but also the rich 3D information of the objects. This way, we can easily convert a 3D asset generation problem into a 2D image generation problem without the use of any complex 3D architecture to handle large amount of unstructured and permutation invariant 3DGS primitives, and neither relying upon computationally expensive multiview rendering or SDS loss. Baseline comparison for unconditional and conditional generation is presented in Table~\ref{table:unconditional}.









\subsection{3DGS Inpainting}
Leveraging the powerful Super UVGS representation, we present in Fig.~\ref{fig:inpaint} one of the first experiments on inpainting 3DGS directly without using any multiview rendering or distilling information from diffusion models. 
We try to recover the missing Gaussians by leveraging the denoising capabilities of LDM and trying to predict the missing corresponding parts of the Super UVGS image. We believe, this can have potential applications in sparse view reconstruction.
More details are given in the supplementary.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{imgs/inpainting_circle.jpg} 
\vspace{-0.5cm}
\caption{\textbf{3DGS Inpainting}: We present one of the first inpainting results on 3DGS directly leveraging the Super UVGS images and the denoising capabilities of diffusion models.
% \srinath{add circle around part of objects that people should focus on.}
}
\label{fig:inpaint}
\vspace{-0.4cm}
\end{figure}


% \subsection{Baselines and Evaluation Metrics}
% We use the standard benchmarking metrics to examine the quality of our reconstructions, unconditional and conditional generation experiments. 
% To evaluate the quality of reconstructed 3DGS objects from both Super UVGS image to 3D and Autoencoder latent space to 3D, we use PSNR and LPIPS~\cite{lpips2018}. 
% The aim is to convert the given 3DGS object to UVGS, and then to Super UVGS, and further to Autoencoder's latent space and calculate the metrics from the reconstructions at every step to prove the proposed method doesn't significantly impact the quality of reconstructions, while also providing a structurally meaningful representation that is much compact and easier to use with existing image based models. The PSNR and LPIPS reconstruction results are presented in Table~\ref{table:psnr}. From the table it can be seen that the proposed UVGS representation with 4 layers (K=4) is able to match the direct reconstructions from 3DGS fitting. We also show that pretrained image based Autoencoder, VAE, and VQVAE can be used to project

% We compare the generational capabilities of our method against various conditional and unconditional SOTA 3D object generation method including the ones using multiview rendering for optimization including generative methods DiffTF~\cite{difftf2023}, Get3D~\cite{get3d2022}, methods trying to give structural representation to Gaussians, GaussianCube~\cite{gaussiancube2024}, TriplaneGaussian~\cite{triplanemeetsgs2024}, and general purpose SOTA large 3D content generation models like 
%  DreamGaussian~\cite{dreamgaussian2023}, LGM~\cite{lgm2025}, and EG3D~\cite{eg3d2022}. 
% We also compare the quality of our unconditional and conditional generation results using FID and KID.



% \begin{table}[t]
% \centering
% \caption{We compare the FID and KID of unconditional generation using the current SOTA methods on 20K randomly generated samples from each method and ours.}
% \scalebox{0.8}{\begin{tabular}{c|cc }
%  \textbf{Method} & \textbf{FID} $\downarrow$ & \textbf{KID} $\downarrow$ \\
%  \hline
%  \hline
%  Get3D &  $68.95$  & $4.38$ \\
%  DiffTF & $89.14$  &$8.73$ \\
%  EG3D & $74.51$ &  $6.62$  \\
%  GaussianCube & $48.44$ & $4.89$  \\
%  UVGS (Ours) & $\textbf{26.20}$ & $\textbf{3.24}$ \\

% \label{table:unconditional} 
% \end{tabular}}
% \end{table}




% \begin{table}[t]
% \centering
% \caption{Text Conditioned Generation}
% \scalebox{0.8}{\begin{tabular}{c|c}
%  \textbf{Method} & \textbf{CLIP Score} $\uparrow$ \\
%  \hline
%  \hline
%  DreamGaussian~\cite{dreamgaussian2023} &  $28.51$  \\
%  Shap. E~\cite{shapeditor2024} & $30.76$  \\
%  LGM~\cite{lgm2025} & $31.24$ \\
%  GaussianCube~\cite{gaussiancube2024} & $29.83$ \\
%  UVGS (Ours) & $\textbf{32.62}$ \\

% \label{table:text2_3d} 
% \end{tabular}}
% \end{table}






\subsection{Ablation Studies \& Discussion}

We conduct exhaustive ablation studies to justify some of our framework's design choices including the effect of branching in mapping networks, the use of single layer UVGS maps, and the resolution of UVGS maps. We performed our experiments on our custom Objaverse~\cite{objaverse2023} 3DGS dataset and evaluate the performance of our model in terms of PSNR, SSIM, and LPIPS. The results are presented in Table~\ref{table:ablation}. From the table, it can be seen that by using four layers of UVGS maps (K=4), we can almost match the reconstruction quality of fitted 3DGS results, while we realized that simply with a single layer UVGS, we are able to maintain the overall geometry and appearance of the object for our dataset with a PSNR of more than 30. 
We also compared the reconstruction performance of our method with and without using branching in the mapping network. It can be clearly seen that using branching network significantly increases the reconstruction quality from Super UVGS space. The main reasoning behind this is the specialization of attributes that the branching provides to individually process each attribute first.

\vspace{0.2cm}

\noindent\textbf{Limitations \& Future Work}: 
% Although, our method is able to synthesize and reconstruction a large number of diverse 3D assets using UV mapping, there are some limitations to it. 
While single layer UVGS images can recover the geometry of the object, they sometimes suffer in terms of appearance and the generated objects might look washed out. 
We believe this can be solved by using a multi-layer UVGS maps. 
Similarly, the single layer UVGS map is limited to representing simpler everyday objects, and may not be sufficient to represent highly-detailed and complex objects or scenes. 
In the future, we want to extend this framework to learn features for real-world scenes and complex objects like a human head with multi-layer UV mapping. 
We also want to make this representation more efficient by better utilizing the empty pixels of UVGS maps and Super UVGS images while also maintaining the underlying features and 3D information.
\begin{table}[t]
\centering
\setlength{\tabcolsep}{0.75mm}
\renewcommand{\arraystretch}{1.2}
\vspace{-0.2cm}
\caption{We compare the FID and KID of unconditional generation using the current SOTA methods on 20K randomly generated samples from each method and ours. We also compare our method against SOTA text-conditioned generation frameworks on CLIP Score for 10K generated objects from each method. 
}
\vspace{-0.2cm}
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{l|cc|||l |c }
 \toprule
 % 
 \multicolumn{3}{c}{\textbf{Unconditional Generation}} &  \multicolumn{2}{c}{\textbf{Text-Conditioned Generation}}\\
 \cmidrule(l){1-3} \cmidrule(l){4-5} 
 \textbf{Method} & \textbf{FID} $\downarrow$ & \textbf{KID} $\downarrow$ & \textbf{Method} & \textbf{CLIP Score} $\uparrow$ \\
 \midrule
 Get3D~\cite{get3d2022} &  $53.17$  & $4.19$  & DreamGaussian~\cite{dreamgaussian2023} &  $28.51$  \\
 DiffTF~\cite{difftf2023} & $84.57$  &$8.73$ & Shap. E~\cite{shapeditor2024} & $30.53$  \\
 EG3D~\cite{eg3d2022} & $74.51$ &  $6.62$  &  LGM~\cite{lgm2025} & $30.74$ \\
 GaussianCube & $34.67$ & $3.72$  & GaussianCube~\cite{gaussiancube2024} & $30.34$ \\
\textbf{ UVGS (Ours)} & $\textbf{26.20}$ & $\textbf{3.24}$ &  \textbf{UVGS (Ours)} & $\textbf{32.62}$ \\
 \bottomrule
\end{tabular}}
\label{table:unconditional} 
\vspace{-0.2cm}
\end{table}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{0.75mm}
\renewcommand{\arraystretch}{1.2}
\caption{We present quantitative ablation study for number of UVGS layers (K), UVGS map resolution, and the effect of branching in mapping network on the Objaverse 3DGS dataset.}
\vspace{-0.2cm}
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{l|cc||l|cc}
 \toprule
 \textbf{Method} & \textbf{PSNR} & \textbf{LPIPS} & \textbf{UVGS Size} & \textbf{PSNR} & \textbf{LPIPS} \\
 \midrule
  UVGS $@K=1$ & $31.1$  & $0.06$ & $512\times512~(@K=1)$ &  $31.1$  & $0.08$ \\
 UVGS $@K=2$ & $31.9$ & $0.05$ & $256\times256~(@K=1)$ &  $28.2$  & $0.23$ \\
 UVGS $@K=4$ & $33.2$ & $0.03$ & Without Branching &  $27.8$  & $0.31$ \\
 \bottomrule
\end{tabular}}
\label{table:ablation} 
\vspace{-0.4cm}
\end{table}


% \begin{table}[t]
% \centering
% \caption{Ablation Study}
% \scalebox{0.8}{\begin{tabular}{c|c|c}
%  \textbf{Method} & \textbf{PSNR} & \textbf{LPIPS} \\
%  \hline
%  \hline
%  UVGS $@K=1$ & $31.1$  & $0.06$ \\
%  UVGS $@K=2$ & $31.9$ & $0.05$ \\
%  UVGS $@K=4$ & $33.2$ & $0.03$ \\
%  \hline

%  UVGS Size &  $-$  & $-$ \\
%  $512\times512~(@K=1)$ &  $31.1$  & $0.08$ \\
%  $256\times256~(@K=1)$ &  $28.2$  & $0.23$ \\
%  Without Branching &  $25.4$  & $-$ \\

% \label{table:ablation} 
% \end{tabular}}
% \end{table}