\vspace{-0.8cm}
\section{Introduction}\label{sec:intro}
\vspace{-0.2cm}
%
% Contributions:
%
% 1. Dataset: from Objaverse mesh to 3DGS.\\
% - render 88 views\\
% - no Colmap
% 2. 3DGS to UVGS\\
% - spherical unwrapping\\
% - dynamic mapping using opacity \\
% - Compression using super UV gaussians. \\
% - address global correspondence
% 3. Mapping Networks\\
% - Multi-head networks\\
% - Pretrained Image Autoencoder
% 4. Caption Generation
% Why UV??
%
% - 3DGS is permutation invariant.\\
% - Although 3DGS has been widely studied in scene reconstruction tasks, its spatially unstructured nature presents a significant challenge when applied to mainstream generative modeling frameworks. \\
% - 3DGS is unstructured, thus poses a signifacnt challenge when applied to mainstream computer vision models, including AE, generative models. \\
% - A structured representation would eliminate the need of complex, specialized network designs that are often necessary with unstructured representations. \\
% - If we use a structured representation for 3DGS (spatial coherence), we can essentially use standard CNNs to capture the correlation among the neighboring gasussians, allowing efficient feature extraction. \\
% - Also address global correspondence.
%
% (First para: Highlight the need of 3D Object Reconstruction/Generation and the use of 3DGS.)
%

The creation of high-quality 3D content is essential in applications like virtual reality, game design, robotics, and  movie production, where realistic 3D representations play a critical role. Typical 3D representations like Neural Radiance Fields (NeRF)~\cite{nerf2021} are promising but require substantial computational resources, limiting their scalability for real-time applications. Moreover, NeRF is an implicit representation, which makes editing and manipulation challenging.
% \srinath{NeRF is also implicit making it hard to edit, etc.}
Recently, 3D Gaussian Splatting (3DGS) \cite{3dgs2023} emerged as a compelling alternative, enabling efficient and high-fidelity 3D rendering through a large set of Gaussian primitives that model spatial and visual properties.
As an explicit representation, 3DGS offers several advantages over NeRF.
% of easier manipulation and direct access to individual primitives. 
% \srinath{Explicit-ness of 3DGS is a big advantage.}
However, while 3DGS offers benefits in terms of speed and visual quality, its unstructured, permutation-invariant nature presents significant challenges for generative tasks. Much like point clouds, it lacks a coherent spatial structure, impeding its integration with conventional image-based generative models. This lack of structure and coherence among primitives hinders the application of image-based generative models~\cite{shapesplat2024, diffgs2024, gaussiancube2024}, which rely on structured data representations.
%and are unable to process unordered points effectively. 

Previous methods have tackled these challenges by transforming 3DGS into structured formats, such as voxel grids~\cite{gaussiancube2024, sdfusion2023, gvgen2025} or image-based representations like Splatter Image~\cite{splatterimage2024} or triplanes~\cite{zou2024triplane}. Other approaches employ diffusion models to directly predict 3DGS attributes~\cite{gsd2024}. These methods, while achieving impressive visual results, often require substantial computational resources, memory-intensive multi-view rendering, complex architectures limiting their scalability and flexibility for high-fidelity generation.
% Further, direct generation 
Generating and processing 3DGS directly by efficiently utilizing modern generative models like Variational Autoencoders (VAEs) and diffusion models is limited as the neural networks are not permutation invariant. % \srinath{incomplete sentence}

%Thus, an effective transformation of 3DGS into a structured representation is essential, enabling compatibility with image-based architectures and allowing for efficient local and global feature extraction without multi-view dependencies.

% \mj{motivating the problem here. May be need to add support with references.}
% To effectively leverage generative models for 3D Gaussian splatting, it is crucial to develop a structured representation for 3DGS that provides coherence without relying on multi-view rendering or other memory-intensive approaches. Such a transformation would allow compatibility with 2D image-based architectures, making it possible to efficiently extract local and global features from the 3D data.


% To this end, we propose the \emph{UV Gaussian Splatting (UVGS)}, a structured representation that leverages spherical mapping to organize the 3DGS attributes into a coherent 2D UV map. %By inscribing Gaussian splats within a spherical surface, UVGS transforms 3D Gaussian attributes—such as position, rotation, scale, opacity, and color—into an image-like 14-channel map, resolving the issue of permutation invariance.
% \mj{
To address these shortcomings, we introduce \textbf{UV Gaussian Splatting (UVGS)}, which provides a structured transformation of 3D Gaussian primitives into a 2D representation while preserving essential 3D information. 
We use spherical mapping~\cite{sphericalmapping2006} that inscribes Gaussian splats in a spherical surface, and projects attributes like position, rotation, scale, opacity, and color into an organized 14-channel image-like UV map.
This mapping introduces spatial structure, resolving issues of permutation invariance by introducing local correspondences between neighboring Gaussians and global coherence across the entire 3D object. 
%The result is a representation that is ``almost`` 3D \srinath{re-phrase as ``3D representation''}, but structured as a 2D map, enabling compatibility with powerful image-based neural network architectures. 
The result is a representation that functions as a ``3D representation" structured in a 2D map format, enabling compatibility with powerful image-based neural network architectures.

% }
% Our UVGS approach transforms 3DGS attributes, including position, rotation, scale, opacity, and color, into an organized 14-channel image-like representation that preserves both local and global correspondences. 
% This structured UVGS representation preserves both local correspondences among neighboring Gaussians and global coherence across the entire 3D object. 
% \dilin{i would suggest to explain local and global correspondence a little bit more as this is the main contribution of the paper..} 
% Thus, enabling the application of powerful image-based neural network architectures to unstructured 3D data, effectively bridging the gap between unstructured 3DGS and 2D image models. 
% \mj{Please add details and references as needed.} \ar{Need to add multi-layer unwrapping}. Additionally, we introduce multi-layer unwrapping in UVGS to enhance mapping fidelity across different scales.

% \dilin{highlight the magic of our method, it's "almost" a 3d representation, but it works as a 2d representation. zero-shot generalize to pretrained 2d foundation mdoels. while previous work, like triplanes, neural fields, occpuancy, voxels, point clouds, there first need a 3d backbones, it's not scalable and also requires 3d training data cannot benefit from the prior we learned from 2d easily.}


While UVGS introduces structure into 3D Gaussian Splatting, its full 14-channel attribute-specific representation presents challenges for direct integration with pretrained 2D generative models, as these models typically expect a simpler, image-compatible data. 
Each of its heterogeneous attributes—position, color, and transformation—has its own distinct distribution and resides in a separate feature space, making it challenging to represent the 3D object in a unified shared space.
%complicating efforts to collectively represent the 3D object.
 To address this, we introduce \textbf{Super UVGS}, a compact 3-channel representation that unifies these diverse attributes into a cohesive format. Using a carefully designed multi-branch mapping network, Super UVGS consolidates the distinct attribute spaces into a shared feature space, enabling a more collective representation of the object. This unified transformation not only facilitates zero-shot compatibility with pretrained 2D models but also optimizes memory usage and computational efficiency, making Super UVGS highly practical for large-scale 3D tasks.
 Unlike previous approaches that use Triplanes, voxels, occupancy grid, neural fields etc. and require specialized 3D architectures to train on 3D data, UVGS effortlessly leverages widely available pretrained 2D foundational models.
This zero-shot generalization capability allows UVGS to fully benefit from priors learned in 2D domains from large amount of data, improving both flexibility and scalability.
% Without adaptation, the 14-channel representation would require additional redesign or retraining of the 2D models.
%, limiting its zero-shot applicability. 
% To address this, we introduce \textbf{Super UVGS}, a compact 3-channel representation that transforms UVGS into a streamlined, image-compatible format. Using a multi-branch mapping network, Super UVGS efficiently consolidates the diverse Gaussian attributes into just three channels, allowing seamless integration with existing 2D models without compromising essential spatial information. This compression not only facilitates zero-shot use with 2D architectures but also reduces memory usage and computational demands, making Super UVGS highly practical for large-scale 3D tasks that require efficient processing.
% \dilin{missing the motivation on why we do want to introduce super uvgs, maybe move the second sentence to the first?} 
%Building on UVGS, we introduce \emph{Super UVGS}, a compact 3-channel representation achieved through a multi-branch mapping network. This network efficiently encodes distinct distributions of Gaussian attributes, producing a streamlined, image-compatible format without quality loss. This compact representation allows us to directly apply image-based generative models, such as auto-encoders and diffusion models, \mj{without additional training or architectural modifications}, facilitating conditional generation, in-painting, and  high-quality 3D asset synthesis. \dilin{highlight we don't even need training?}
%
To sum up our main contributions are:
%
\begin{itemize}
    \item \emph{Efficient Structured Representation of 3DGS}: We present UVGS, an image-like representation that solves permutation invariance and unstructured nature of discrete 3DGS through spherical mapping, making direct feature extraction possible by organizing unordered points into a coherent 2D representation compatible with 2D models.
    % \srinath{What is ``efficient'' about this representation?}
    \item \emph{Compact and Scalable Super UVGS Representation}: To address scalability while dealing with large scale 3DGS points and enabling the direct integration of pre-trained 2D foundation models, we introduce Super UVGS - a low-dimensional version of UVGS maps that retains high fidelity features while reducing memory overhead. 
    % Using a multi-branch mapping network, Super UVGS enables large-scale 3D tasks with minimal computational overhead. 
    %This compact format maintains high fidelity while reducing memory usage (\mj{refer to an experiment}), enhancing scalability for complex 3D tasks.
    % \srinath{briefly say why we need super uvgs}
    % \item \emph{Direct Application of Image-Based Models for 3D Generation}: By transforming 3DGS into a structured image-like representation, we enable the use of pre-trained image-based models without specialized 3D architectures, achieving multiview consistency and improved generation quality.
    \item \emph{Diverse 3D Applications}: Our approach unlocks seamless integration of 3DGS with pre-trained 2D foundation models for various tasks, including unconditional and conditional generation of 3DGS. 
    % Direct integration with 2D diffusion models marks one of the first instances of applying image-based generative models to 3D Gaussian data.
    %We validate our approach with various applications, including conditional generation, unconditional generation, and 3D inpainting using diffusion models. This marks one of the first experiments on applying image-based generative models directly to 3D Gaussian data.
%
\end{itemize}

% \srinath{IMO, we don't need to pad list of contributions. I would make applications the third contrib and stop at that.}

% In summary, UVGS and Super UVGS provide a robust solution for structuring 3D Gaussian Splatting in a compact, image-compatible format. This transformation not only addresses the challenges of unstructured 3D data but also enables seamless integration with image-based generative models, setting a new standard for efficient, high-quality 3D content generation.
% \srinath{This last para can be deleted/integrated compactly elsewhere.}



% #################################### OLD INTRO BEGINS HERE ###############################################

% \mj{Comments and previous text used for the above draft follows here:}

% \ns{Intro structure and idea of what to write is solid. I find it very robotic-y in how it reads though, and would like to make it an easier/less rigid read. For now let's leave it as is and we can do a pass once experiments are fully-in.}
% %
% Radiance fields have emerged as an important representation for modeling objects and scenes~\cite{}.
% They are now widely used in problems such as novel view synthesis~\cite{}, 3D reconstruction~\cite{}, and even robotics~\cite{}.
% The most commonly used method for obtained a radiance field is 3D Gaussian Splatting (3DGS)~\cite{3dgs2023} that represents the scenes using hundreds of thousands of 3D Gaussian primitives, each parametrized by position, orientation and color.
% The 3D Gaussians are stored in an unordered list that makes the representation \emph{permutation invariant}, \ie~changing the order of the list has no impact on the underlying representation.
% Morever, this list lacks information on spatial information about  3D Gaussians.
% This makes the representation unsuitable for generalization using autoencoders or generative models~\cite{shapesplat2024, diffgs2024, gaussiancube2024} since neural networks are not permutation invariant.
% % Additionally, these points are unstructured in the spatial domain, i.e., they lack spatial coherence.
% \srinath{don't call 3D Gaussians points -- they are not.}
% \srinath{large number of points}

% % does not impact the resultant shape of the object. 
% % This may result in almost infinite possibilities of arrangements with large number of 3DGS points. 
% % Consequently, it can be treated as a set rather than a vector for representation. 
% \dilin{this is the same problem with point cloud as well; it reads a little strange without discussing it. } \ar{Maybe something like: Consequently, just like point cloud it's a set of unstructured points, but in hundreds of thousands in number.}
% \dilin{this is probably still not true, see, e.g., \url{https://arxiv.org/pdf/2301.11445}, or more from the related work}
% % Additionally, these points are unstructured in the spatial domain, i.e., they lack spatial coherence. 
% % Both of these characteristics present substantial challenges when integrating them with conventional computer vision models \cite{shapesplat2024, diffgs2024, gaussiancube2024}, such as autoencoders and generative models as it becomes extremely difficult for neural networks for extract features from them.
% \dilin{Missing a very important discussion on why we want to work with Gaussian splats.} \ar{Generic Reply: 3DGS has demonstrated its potential to serve as the next generation 3D representation by enabling both real-time rendering and high-fidelity appearance modeling.}

% \dilin{I feel like we could use a quick summary here. There are a few discussions in this paragraph, but I don't quite see how they are related to our work. As I understand it, our main contribution is proposing a novel method to learn compact latent representations for 3DGS and demonstrating the generalization of our representation. However, most of the work does not seem quite relevant.} \ar{I believe now the main contribution is giving structure to unstructured and discrete gaussian points. The same is reflected in the title. Once we have a representation that provides local and global correspondence, we could use it for learning VAE, LDM, etc.}
% However, research in addressing these concerns \dilin{??? doesn't seem obvious to me} \ar{What I meant here is research in giving structure to 3DGS is sparse. There are only handful of papers targeting this.} in 3DGS remains sparse \cite{shapesplat2024, gsd2024}. Most existing reconstruction or generative models treat 3DGS as a black box, employing multi-view rendering to generate various views and optimize the Gaussians using rendering loss. 
% This approach is both time-consuming and memory-inefficient, and also necessitating complex architectures to generate 3DGS assets. 
% Some efforts attempt to address these challenges by directly predicting 3DGS attributes using diffusion models, such as GSD~\cite{gsd2024}, or by employing image-based representations like Splatter Image~\cite{splatterimage2024}. 
% The GSD approach is constrained to generating a limited number of 3DGS points (approximately 1,000) due to the unstructured and permutation-invariant nature of 3DGS, making it difficult to learn a bigger set of points. 
% Conversely, methods like Splatter Image~\cite{splatterimage2024} project Gaussian objects into image-based representations through direct projection. 
% These methods struggle with maintaining multiview consistency, as the model only infers seen poses correctly, while hallucinating for unseen poses. 
% The scarcity of extensive 3D data hampers the generalizable ability and to produce high-quality results for such methods. 
% Approaches like \cite{gaussiancube2024, sdfusion2023, gvgen2025} attempt to impose structure on Gaussian splats through voxelization, but they are similarly restricted, accommodating very less number of Gaussians constrained by voxel size. 
% Thus, most of the previous object reconstruction or generation approaches are not scalable to large number of 3DGS, resulting in low quality. 
% In practice, representing an object adequately may require up to a few hundred thousand Gaussians~\cite{diffgs2024}.

% \dilin{does not sound very exciting. "Permutation invariance nature and the large number of unstructured" is a common problem for many other data modalities. I would like to suggest motivating again why learning a compact latent representation for Gaussian splats is exciting work here.} \ar{answered in comments above}
% To address these limitations (permutation invariance nature and the large number of unstructured 3DGS points), we propose a new representation for 3DGS. 
% We draw inspiration from 3D graphics primitive and employ spherical mapping to transform the 3D Gaussian object into an image-based representation that is both invariant to random shuffling of 3DGS points and structured. 
% This approach enables the storage of the entire 3D object as a compact and structured image. \dilin{highlight again why do we want to map 3DGS to an image representation? Missing related work using similar ideas if any}
% We begin by inscribing the Gaussian object within a sphere and projecting its attributes onto the sphere. 
% The sphere is then unwrapped to create an image-like UV map, representing the position, rotation, scale, opacity, and color of the 3DGS with 14 channels. 
% \dilin{looks you're assuming SH=0, maybe moving the exact number of channels (14) to the end of the sentence, that reads a little bit more causal}
% \dilin{sorry, but the following feels a bit too detailed and doesn't sound exciting. Currently, it reads like we did ABC, and then we got super UVGS. It might be more natural to first explain why we need super UVGS on top of what we introdced before, and then describe how we did ABC and found it to be very effective?} \ar{Mihir now explained in the intro} This transformed representation for 3DGS provides spatial coherence, allowing standard neural network architectures to effectively capture correlations among neighboring Gaussians for efficient local and global feature extraction. 
% The 14-channel UV map (referred to as UVGS), is further compressed into a 3-channel RGB image (termed Super UVGS), using CNNs. 
% We propose a multi-branch mapping network to deal with different value distribution of 3DGS attributes to map the 14-channel UVGS to the 3-channel Super UVGS. 
% We demonstrate that the Super UVGS representation effectively retains all details of a 3DGS object and can be directly utilized with existing image-based models. 
% This semantically structured representation offers both local and global correspondence in representing Gaussian attributes. 
% To reconstruct the object back from a Super UVGS represent, we train an inverse multi-branch mapping network that maps a 3-channel Super UVGS image back to a 14-channel UV map representing all five 3DGS attributes. 
% Through inverse spherical projection, we can easily reconstruct the 3DGS object with minimal computation. 


% Since, directly generating hunderds of thousands of unstructured 3DGS points from neural network models is non-trivial and difficult~\cite{gaussiancube2024, diffgs2024, shapesplat2024, gsd2024}, we propose an effective way to deal with the 3D object reconstruction and generation problem with 2D models because of their wide availability and less complexity with large amount of training data. \dilin{previous sentence feels like a distraction.}
% \dilin{let's make the following few sentences more exciting :) - e.g., the simplicity,  how it seamlessly work with other foundation models, unlocking new applications..} \ar{Addressed in new intro}
% Notably, a pretrained image-based autoencoder can perfectly reconstruct the Super UVGS image without any fine-tuning, and thus can be used to represent an entire 3DGS object with a much compact and useful representation. 
% We also show the efficacy of this representation in working with image diffusion models for both unconditional and conditional synthesis of 3DGS objects.
% This compact representation makes the computation scalable to the resolution of Super UVGS image, rather than the number of unstructured points, thereby, increasing the quality of 3DGS reconstruction and generation. 
% It is interesting to note that using a multilayer Super UVGS \dilin{need more explanation, worth a separate paragraph even} \ar{Todo} representation, we can also represent complex objects and even real-world scenes with high quality as it enables us to store multiple layers of 3DGS points with varying opacities.

% \dilin{too slow...}
% Through extensive experiments, we have demonstrated the effectiveness of this representation across various existing model architectures, eliminating the need for complex, specialized network designs often required with unstructured representations like 3DGS. 
% We have also leveraged this representation to construct a semantically meaningful and interpretable latent space for 3DGS by training a VAE on Super UVGS images. 
% \dilin{here it's too fast.. how the latent space is related to 3D assets generation.. also discussions with other work should better move forward, here it's a little bit too late}
% Furthermore, we have shown that the proposed representation can efficiently generate 3D assets using simple feed-forward networks. 
% Unlike previous methods, we do not rely on rendering and SDS loss for generating multiview consistent objects. 
% Consequently, rendering is not a bottleneck, allowing us to directly apply losses (MSE and LPIPS) on UVGS images due to their local and global structure correspondences. 
% This approach significantly enhances efficiency in terms of both memory and time.

% In summary, our main contributions are (need to squeeze them):

% \begin{itemize}
%     \item We propose an efficient and effective way to deal with permutation invariant and unstructured nature of 3D Gaussian Splatting (3DGS) for reconstruction and generation of objects using existing image based neural network architectures.
    
%     \item We propose Super UVGS, a compact representation using spherical mapping from 3D graphics primitives to preserve the large number of 3DGS points in object reconstruction and generation tasks.

%     \item We propose a multi-branch mapping network to deal with different value distribution of 3DGS attributes to map the 14-channel UVGS to the 3-channel Super UVGS image and vice-versa.

%     \item We show that the proposed Super UVGS representation can be effectively used with the existing image based models and show that a pretrained image autoencoder can be used to represent a high quality 3D object with just a 3 channel latent representation saving upto 20x memory.

%     \item Using the proposed Super UVGS representation, we propose an unconditional and conditional diffusion models of 3D asset synthesis.

%     % \item (not sure) We created a custom large dataset of high-quality 3DGS assets by converting the Objaverse data into 3DGS representation. 
    
% \end{itemize}



% \ar{Interesting point to add somewhere in intro: There has been no exploration of direct learning on trained 3DGS attribute parameters in existing research. -- ShapeSplat dataset (August 2024)}
