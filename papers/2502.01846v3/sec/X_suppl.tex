% \clearpage
% \setcounter{page}{1}
\maketitlesupplementary
\setcounter{section}{0}
\setcounter{figure}{0}
\noindent Our supplementary material contains a wide range of information that cover implementation details for our networks and training procedures, as well as a large variety of qualitative results. 

\noindent\textbf{Supplementary Video}: We refer the interested reader to the supplementary video where we provide an overview of how our proposed approach works as well as a plethora of qualitative results across different tasks. 

\section{Spherical Mapping}


\noindent\textbf{Spherical Mapping}: 
Spherical mapping~\cite{sphericalmapping2006} is a fundamental technique in computer graphics that is used to project 3D meshes onto a 2D map generally for texture mapping, where a 2D image is wrapped around a 3D object, such as a cylinder or a sphere. 
However, cylindrical mapping fails to capture the top and bottom parts of the object in the same UV map, and can introduce distortions for objects that extend far in the Z-direction. Hence we opted for spherical mapping the process of which involves converting 3D Cartesian coordinates ($x,y,z$) into spherical coordinates ($\rho, \theta, \phi$) and then mapping these onto a 2D plane. Algorithm~[1] explains spherical unwrapping in detail for a single layer(K=1). The same process can be repeated for multiple layers, by keeping a track of opacity values.

\paragraph{Thresholding Opacity} 3DGS use multiple points with varying opacity values to represent an object from any specific viewpoint. 
However, it is oftentimes noticed that many of these points have very low opacity values and do not contribute to the object's overall representation or appearance.
We filter these points using a threshold opacity value with no impact on the object's overall geometry and representation to reduce the number of tractable primitives. 


\begin{algorithm}
\label{algo:spherical}
\caption{Spherical Unwrapping for UVGS map (K=1).}
\begin{algorithmic}[1]
\Require $3DGS \in \mathbb{R}^{n \times 14}$,~ $(M,N) \in \mathbb{Z},~ K=1$
\Ensure $position(\sigma), color(c), scale(s) \in \mathbb{R}^{n \times 3}$
\Ensure $rotation(r) \in \mathbb{R}^{n \times 4},~ opacity(o) \in \mathbb{R}^{n \times 1}$
\State Extract $xyz(\sigma),~opac(o)$ from $3DGS$
% \State $x \gets z1$, $y \gets y1$, $z \gets x1$
\State $ \text{Spherical radius, }r \gets \sqrt{x^2 + y^2 + z^2}$
\State $ \text{Azimuthal Angle, } \theta \gets tan^{-1}(y, x)$
\State $ \text{Polar Angle, } \phi \gets cos^{-1}(z,r)$
\State $(\theta,~ \phi) \gets (\text{deg}(\theta) + 180,~ \text{deg}(\phi))$
% \State $\phi \gets \text{degrees}(\phi)$
\State $\theta_{UV} \gets \text{round}((\theta / 360) \times M)$
\State $\phi_{UV} \gets \text{round}((\phi / 180) \times N)$
\State Initialize $UV_{map} \gets \text{zeros}(M, N, 14)$
\State Initialize $UV_{opac} \gets \text{zeros}(height, width)$
\ForAll{$(t, P, xyz, o)$ in $(\theta_{UV}, \phi_{UV}, 3DGS, opac)$}
    \If{$0 \leq P < height$ and $0 \leq t < width$}
        \If{$UV_{map}[P, t] \text{ is } 0$}
            \State $UV_{map}[P, t] \gets 3DGS[ind]$
            \State $UV_{opac}[P, t] \gets o$
        \Else
            \If{$o > UV_{opac}[P, t]$}
                \State $UV_{map}[P, t] \gets 3DGS[ind]$
            \EndIf
        \EndIf
    \EndIf
\EndFor \\
\Return $UV_{map}$
\end{algorithmic}
\end{algorithm}


\begin{figure*}[t]
\centering
\includegraphics[width=6.4in]{imgs/supp_recons.jpg} 
\caption{
In this figure, we show the qualitative results of reconstructing 3DGS object using pretrained Image Autoencoder (A) via Super UVGS. We obtain UVGS maps (U) through spherical projection of 3DGS objects, followed by using forward mapping network to get Super UVGS (S). A pretrained AE is used to reconstruct Super UVGS (S'), which can be converted to UVGS maps (U') through inverse mapping network. At last, through inverse spherical mapping, we can get predicted 3DGS object which has the same appearance and geometry as the input object with minimal loss.
}
\label{fig:supp_recons}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=6.8in]{imgs/complex_recons.jpg} 
\vspace{-0.3cm}
\caption{
Complex object reconstructions (K=4) using pretrained image-based autoencoder.
}
\vspace{-0.6cm}
\label{fig:supp_complex_recons}
\end{figure*}


\paragraph{Dynamic GS Selection and Multiple Layers}
When projecting 3DGS points to UV maps using spherical mapping, multiple points may map to the same pixel in UV space as shown in Fig.~\ref{fig:dynamic_sel}. 
The two 3DGS points $( g_1 )$ and $( g_2 )$ map to the same pixel on UV map $( P_a )$ causing many-to-one mapping. 
However, the UV map can only hold a single 3DGS primitive at any given pixel.
To address this, we propose a Dynamic Selection approach where each UV pixel retains the 3DGS attributes with the highest opacity intersecting the same ray from the centroid to the farthest 3DGS primitive along the ray.
Using the same example in Fig.~\ref{fig:dynamic_sel}, if opacity $o_1$ of Gaussian $g_1$ is less than opacity $o_2$ of $g_2$. then only $g_2$ attributes will be stored in the UV map at pixel $P_a$. 
Through multiple testing, we observed that this method helps retain the overall geometry and appearance of the 3DGS object while resolving many-to-one mapping issues with minimal quality loss.

\begin{figure*}[t]
\centering
\includegraphics[width=6.4in]{imgs/recons_img_K.jpg} 
\caption{
Reconstruction of a real-world scene for different K values. Smaller K results in many-to-one issue, hence lacking details.
}
\label{fig:supp_recons_K}
\end{figure*}




This method with single layer is applicable to most of the objects in our dataset. However, this might fail in the case of more complex objects or real-world scene representation. There could be multiple layers of Gaussians holding higher opacity and contributing to the overall scene's appearance or geometry, and even partial or full occlusions.
To better represent such objects and scenes and to prove the effectiveness of UVGS, we stack multiple layers of UV maps, where each UVGS layer holds the 3DGS primitives of the top-$K^{th}$ opacity value intersecting the same ray. 
This can be accomplished by inscribing the 3DGS object inside multiple spheres where each sphere maps the 3DGS attribute corresponding to the top-$K^{th}$ opacity value along the same ray. 
To show the effectiveness of proposed UVGS maps in capturing the intricacies of a complex real-world scene, we use a 12 layer UVGS map to reconstruct the real-world 3D scenes. The results are presented in Fig.~\ref{fig:supp_scene}. We also compare the effect of increasing the number of UVGS layers in representing a real-world 3D scene in Fig.~\ref{fig:supp_recons_K}
% Fig.~\ref{}\ar{FIG. REFERENCE} in shows that this multi-layer UVGS mapping can be used to even map real-world complex scenes to structured UV maps and can be reconstructed back with high-fidelity. 
In future work, we want to extend this ability for potentially many applications in 3D dynamic scene reconstructions using video diffusion models, and the segmentation or tracking of objects in 3DGS scenes as the features in the UVGS maps can be easily processed with the neural networks and tracked over time. 

% \ns{lets not use concerned anywhere in the paper :) you can write general purpose objects, objects in our dataset or something more descriptive}
% \dilin{i feel the following is not necessary..it's very clear before already.} \st{Note that a multi-layer UVGS map should not be confused with a 14-channel UVGS $U \in \mathbb{R}^{M\times N \times14}$ representing the 5 attributes. A 2-layer UVGS map will have \(14 \times 2 = 28\) channels, with each attribute represented by 2 identical attribute specific layers. 
% Similarly, a 4-layer UVGS map will have \(14 \times 4 = 56\) channels.}



\section{Mapping Networks}

\noindent\textbf{Forward Mapping Details}:
This process is defined as:
\begin{equation}
   f^f_{map} = [ ~[\phi^f_{P}(\sigma)] ~[\phi^f_{T}([r,s])] ~[\phi^f_{A}[o,c]] ~] 
\end{equation}
   
The central branch ($\phi^f_{C}$) is composed of $2L$ hidden Convolution layers.
The first $L$ hidden convolution layers increase the feature dimension at each step, while the last $L$ layers does the inverse and squeezes the high-dimensional feature maps to 3 channels to output Super UVGS image $S \in \mathbb{R}^{M\times N \times 3}$. 
% \dilin{no need to highlight all the dimensions and details here? or write it in a tone that suggests one particular implementation, as there are many other design approaches.}
% \ns{Same thing with before. Explain the intuition behind this choices and give me insights to understand why this makes sense.} \ar{Justufucation at the end of this section}
\begin{equation}
    S = tanh(~\phi^f_{C}[f^f_{map}]~) ~\in~\mathbb{R}^{(H,W,3)}
\end{equation}
Each CNN layer is followed by a batch normalization layer and ReLU activation both in multi-branch and central branch modules. 
The last layer of central branch is activated using $tanh$ to ensure the Super UVGS doesn't take any ambiguous value resulting in gradient explosion or undesired artifacts. 
The obtained Super UVGS $S$ representation squeezes all the 3DGS attributes to a 3 dimensional image while also maintaining local and global structural correspondence among them. 




\begin{figure*}[t]
\centering
\includegraphics[width=6.4in]{imgs/supp_mapping.jpg} 
\caption{
Forward Mapping Network for UVGS to Super UVGS mapping. The inverse mapping network follows just the inverse of this architecture with each attribute-specific branch now followed by $tanh()$ at the end.
}
\label{fig:mapping_network}
\end{figure*}



% \begin{figure*}[h!]
% \centering
% \begin{tikzpicture}[node distance=0.6cm]

% % Input
% \node (input) [startstop] {UVGS $(U)~\in~\mathbb{R}^{14 \times M \times N}$};

% % Branches
% \node (head_c) [head, below left=of input] {Position $(\sigma)\in~\mathbb{R}^{(3,M,N)}$};
% \node (branch2) [head, below=of input] {Transformation $(r, s)\in~\mathbb{R}^{(10,M,N)}$};
% \node (branch3) [head, below right=of input] {Color $(c, o)\in~\mathbb{R}^{(4,M,N)}$};

% % Branch Outputs
% \node (branch1out) [process, below=of head_c] {Conv ($3 \rightarrow 32 \rightarrow 64$)};
% \node (branch2out) [process, below=of branch2] {Conv ($10 \rightarrow 32 \rightarrow 64$)};
% \node (branch3out) [process, below=of branch3] {Conv ($4 \rightarrow 32 \rightarrow 64$)};

% % Merge
% \node (merge) [process, below right=0.5cm and -2cm of branch2out] {Concatenate (192)};

% % Combined Processing
% \node (conv96) [process, below=of merge] {Conv $(196 \rightarrow 512 \rightarrow 1024)$};
% \node (conv256) [process, below=of conv96] {Conv $(1024 \rightarrow 512)$};
% \node (conv512) [process, below=of conv256] {Conv $(512 \rightarrow 256 \rightarrow 128)$};
% \node (conv128) [process, below=of conv512] {Conv $(128 \rightarrow 3)$};
% \node (conv2) [process, below=of conv128] {Tanh};
% \node (output) [startstop, below=of conv2] {Super UVGS $(S)~\in~\mathbb{R}^{3 \times M \times N}$};

% % Connections
% \draw [arrow] (input) -- (head_c);
% \draw [arrow] (input) -- (branch2);
% \draw [arrow] (input) -- (branch3);
% \draw [arrow] (head_c) -- (branch1out);
% \draw [arrow] (branch2) -- (branch2out);
% \draw [arrow] (branch3) -- (branch3out);
% \draw [arrow] (branch1out) -- (merge);
% \draw [arrow] (branch2out) -- (merge);
% \draw [arrow] (branch3out) -- (merge);
% \draw [arrow] (merge) -- (conv96);
% \draw [arrow] (conv96) -- (conv256);
% \draw [arrow] (conv256) -- (conv512);
% \draw [arrow] (conv512) -- (conv128);
% \draw [arrow] (conv128) -- (conv2);
% \draw [arrow] (conv2) -- (output);

% \end{tikzpicture}
% \caption{Forward Mapping Network for UVGS to Super UVGS mapping. The inverse mapping network follows just the inverse of this architecture with each branch now followed by $tanh()$ at the end.}
% \label{fig:mapping_network}
% \end{figure*}



\noindent\textbf{Inverse Mapping}:
The first $L$ layers in the Central branch increases the feature dimension and the last $L$ layers reduces them to obtain a combined feature map. 

    \[
   f^i_{map} = \phi^i_{C}(S) 
   \]

The final layer is a set of 3 branches projecting the features to position, translation, and appearance attributes, respectively. 
% \dilin{similar issue, no need to use exact numbers, espeically in equations} 
% \ar{ToDo: Remove numbers from all equations.}

    \[
   f^i_{\sigma} = [\phi^i_{P}(f^i_{map})] 
   \]
    \[
   f^i_{r,s} = [\phi^i_{T}(f^i_{map})] 
   \]
    \[
   f^i_{o,c} = [\phi^i_{A}(f^i_{map})] 
   \]


Similar to the forward mapping network, each layer in the central branch and attribute specific branches is followed by batch normalization and $Relu(.)$ activation. 
The last set of branch layers are activated using $tanh(.)$ to prevent ambiguous values resulting in gradient explosion or reconstruction artifacts.

   \[
   \hat{U} = tanh(~ [ [f^i_{\sigma}]~[f^i_{r,s}]~[f^i_{o,c}] ] ~)
   \]

\noindent \textbf{Losses Details}: We used MSE to focus on pixel-wise difference during the training. We solely used MSE for a few iterations to make the mapping networks learn the overall structural representation of the UVGS map using:
\begin{equation}
\mathcal{L}_{mse} = \frac{1}{n} \sum_{i=1}^n (U_i - \hat{U}_i)^2.
\end{equation}
After training the model for few iterations using MSE, we introduce the LPIPS loss giving same weight to both MSE and LPIPS over a few iterations. We observed that increasing the weight value of LPIPS over the iterations resulted in better and faster convergence results.
\begin{equation}
    \mathcal{L}_{lpips} = \sum_{l} w_l \left\| \phi_l(x) - \phi_l(y) \right\|^2,
\end{equation}
where $\phi_l(x)$ and $\phi_l(y)$ are feature maps extracted from pretrained layers of AlexNet\cite{lpips2018}.


\noindent \textbf{Mapping Training Details}: 
% We fix the number of hidden layers in Central branch of forward and inverse mapping networks to six. The first three layers increase the feature dimension at each step reaching a maximum of 1024 features. While the last 3 layers does the inverse and squeezes the 1024 dimensional feature maps to 3 channels to output Super UVGS image $S \in \mathbb{R}^{M\times N \times 3}$. We used two layers in each of the attribute specific branch in both forward and reverse mapping networks.
% \ns{We already mentioned all the below stuff above no? I don't think the below text fits well in "experiments" and I'd suggest moving it to Suppl eventually}
% However, it should be noted that each set of attributes in 3DGS representing the mean position, transformation, and color has a completely different distribution of values and poses a significant burden on the CNN if processed together leading to uncanny gradient values and slower convergence. Thus, we propose to use a multi-branch network where the attribute specific branches implicitly learns to process these different attributes focusing on their individual features before passing them to the central branch. The central branch obtains a concatenated stack of processed attributes and exploits the correlation between different attributes by extracting the local feature correspondence between them and maps them to a 3 channel Super UVGS image. 
Before training the models, we normalized the different attributes in UVGS to [$-1,1$] using the same normalization functions as used in 3DGS paper\cite{3dgs2023}. 
The normalized UVGS maps are used to train the multi-branch forward and reverse mapping networks using MSE and LPIPS loss.
We trained the mapping networks on $8 \times A100 ~(80GB)$ GPUs with a Batch Size of 96 for 120 hours using Adam optimizer with a learning rate of $6e-5$ and set $\beta_1 = 0.5$ and $\beta_2 = 0.9$ with weight decay of $0.01$. 
We set the $\lambda$ for LPIPS loss to be $0$ for the first 24 hours of training and gradually increased it from 1 to 10 for the remaining training in a step of 1. 



\subsection{Interpolation with UVGS}

We show that the proposed SuperUVGS representation can be used to perform local editing and interpolation directly in the UV domain. We can perform edits like swapping the parts of one object from the other, cropping the 3D object, or merging two objects together simply with the SuperUVGS images without any learning based method. The results are demonstrated in Fig~\ref{fig:interpolation}.


\section{LDM - Unconditional and Conditional Generation}

\noindent\textbf{Caption Generation} To generate the relevant text captions for the objects in our dataset for conditional generation, we leverage CLIP~\cite{clip}, BLIP2~\cite{blip22023}, and GPT4~\cite{gpt42023} very similar to \cite{cap3d2024}. Specifically, we use BLIP2 to generate $N$ different captions for randomly selected 20 views from the 88 rendered views for each object in the dataset. CLIP encoders are used to encode and calculate the cosine similarity between the $N$ generated caption per view and the corresponding 20 views. The caption with max similarity is assigned to that particular view, resulting in 20 different captions for the same object. We now use GPT4 to extract a single caption distilling all the given 20 descriptions. We found that the resulting captions were very appropriate to the input objects, and thus we directly used them for conditional generation.


\begin{figure}[!h]
\centering
\includegraphics[width=3.2in]{imgs/interpolation.jpg} 
\caption{
Linear interpolation between two 3DGS objects using SuperUVGS representation.
}
\label{fig:interpolation}
\end{figure}


% \dilin{maybe move to experiments or even appendix?} 
LDMs~\cite{stable_diffusion, ddim} use pretrained VAEs~\cite{transformer_vae} to convert the original image $x \in R^{H\times W\times 3}$ into a compact latent representation $z \in R^{h\times w\times c}$, where the forward and reverse diffusion processes are applied~\cite{stable_diffusion}. 
The VAE decoder then converts the compact latent representation back to pixels. 
The objective function in latent diffusion model can be written as:

\begin{equation}
    \mathbb{L}_{LDM} := \mathbb{E}_{\epsilon(x),\epsilon \sim \mathbb{N}(0,1), t } [ || \epsilon - \epsilon_\theta ( z_t, t ) ||_2^2 ]
\end{equation}

where, $\mathbb{N}(0,1)$ is the Normal distribution, and $t$ is the number of time steps and $z_t$ is the noisy sample after $t$ time steps.

Training was done using AdamW optimizer with a learning rate of $1e-4$ for 75 epochs on $8 \times A100 ~(80GB)$ GPUs. 

Once trained, we can randomly sample new high-quality 3DGS assets from the learned generative model.

% Note that we do not use any conditioning vector for unconditional generation and instead replace the cross-attention layer with the self-attention layer to train the denoising process without any extra conditioning information.
% \ns{For practical reasons ;) I don't want to write Stable Diffusion as a word}

To allow generation of objects from text, we also trained a conditional LDM, where we use Stable Diffusion (SD)~\cite{stable_diffusion} pipeline as it can use text prompt conditioning to guide the image generation through cross-attention. Similar to unconditional LDM, we use pretrained SD's VAE for mapping the Super UVGS image to a latent space and back to the reconstructed Super UVGS.
The text prompts are given to a pretrained CLIP~\cite{clip} text encoder to generate a text embedding $c_t \in \mathbb{R}^{77 \times 768} $, which is then passed to the UNet encoder of SD for cross-attention. We used a set of CLIP encoder and BLIP2~\cite{blip22023}, and GPT4~\cite{gpt42023} to generate captions for our dataset. 
The overall objective function for conditional LDM now becomes:
%
\begin{equation}
    \mathbb{L}^C_{LDM} := \mathbb{E}_{\epsilon(x),\epsilon \sim \mathbb{N}(0,1), t, c_t } [ || \epsilon - \epsilon_\theta ( z_t, t, c_t ) ||_2^2 ]
\end{equation}

where, $\epsilon_\theta (\cdot, t)$ is a time-conditional U-Net~\cite{unet} model, $\mathbb{N}(0,1)$ is the Normal distribution, $z_t$ is the latent code, and $c_t$ is the text embedding.
Training was done using AdamW optimizer with a learning rate of $1e-4$ for 50 epochs on $8 \times A100 ~(80GB)$ GPUs.
Once trained, this conditional LDM can use used to generate text-conditioned Super UVGS images, which can later be mapped to high-quality 3DGS objects. 





% \section{ShapeNet Experiments}

% To show the effectiveness of our method againt varying datasets, we also conducted experiments on ShapeNet~\cite{shapenet2015} dataset. We fine-tuned the trained Mapping Networks and Unconditional and Conditional Diffusion Models on "cars" category of ShapeNet dataset. The results are presented in Fig.~\ref{fig:supp_uncond}. 



\begin{table}[t]
\centering
\setlength{\tabcolsep}{0.75mm}
\renewcommand{\arraystretch}{1.2}
\caption{We compare the FID and KID of unconditional generation using the current SOTA methods on 20K randomly generated samples from each method and ours. We also compare our method against SOTA text-conditioned generation frameworks on CLIP Score for 10K generated objects from each method. 
}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|||l |c }
 \toprule
 % 
 \multicolumn{3}{c}{\textbf{Unconditional Generation}} &  \multicolumn{2}{c}{\textbf{Text-Conditioned Generation}}\\
 \cmidrule(l){1-3} \cmidrule(l){4-5} 
 \textbf{Method} & \textbf{FID} $\downarrow$ & \textbf{KID} $\downarrow$ & \textbf{Method} & \textbf{CLIP Score} $\uparrow$ \\
 \midrule
 Get3D~\cite{get3d2022} &  $53.17$  & $4.19$  & DreamGaussian~\cite{dreamgaussian2023} &  $28.51$  \\
 DiffTF~\cite{difftf2023} & $84.57$  &$8.73$ & Shap. E~\cite{shapeditor2024} & $30.53$  \\
 EG3D~\cite{eg3d2022} & $74.51$ &  $6.62$  &  LGM~\cite{lgm2025} & $30.74$ \\
 GaussianCube & $34.67$ & $3.72$  & GaussianCube~\cite{gaussiancube2024} & $30.34$ \\
\textbf{ UVGS (Ours)} & $\textbf{26.20}$ & $\textbf{3.24}$ &  \textbf{UVGS (Ours)} & $\textbf{32.62}$ \\
 \bottomrule
\end{tabular}}
\label{table:supp_unconditional} 
\vspace{-0.1cm}
\end{table}



\section{Comparison with Baselines}

We compare the generational capabilities of our method against various conditional and unconditional SOTA 3D object generation method on ShapeNet-cars dataset. Specifically, we used the methods using multiview rendering for optimization, like DiffTF~\cite{difftf2023} and Get3D~\cite{get3d2022}. We also compared our approach again the current SOTA methods trying to give structural representation to Gaussians, including GaussianCube~\cite{gaussiancube2024} and TriplaneGaussian~\cite{triplanemeetsgs2024}. We also compared against general purpose SOTA large 3D content generation models like DreamGaussian~\cite{dreamgaussian2023}, LGM~\cite{lgm2025}, and EG3D~\cite{eg3d2022}. 

To compare the quality of our generation results, as a standard practice, we use FID and KID for unconditional generation, and Clip Score for text-conditioned generation. Table~\ref{table:unconditional} quantitatively compares the unconditional and conditional generation results of our method again various SOTA methods. 
% We also compare the results of our fine-tuned ShapeNet cars generation framework and the results are presented in Table~\ref{table:supp_unconditional} as \textbf{UVGS-S}. 
From this table, it can be seen that our method performs a good job in unconditional generation of good quality 3D assets. The main reason behind this is the learned Super UVGS representation which not only maintains the appearance of the 3DGS object, but also serves as a proxy for geometrical shape by encoding all the 3DGS attributes into the same coherent feature space. Table~\ref{table:supp_unconditional} compares the CLIP Score of our text-conditioned generation results and the current SOTA methods. The unconditional and conditional qualitative comparison results are presented in Fig.~\ref{fig:supp_cond} and Fig.~\ref{fig:supp_uncond}, respectively.



\begin{figure*}[!ht]
\centering
\includegraphics[width=6.8in]{imgs/supp_compare.jpg} 
\caption{
Here we show more comparison of unconditional 3D asset generation on the cars category with SOTA methods. Figure shows that DiffTF~\cite{difftf2023} produces low-quality, low-resolution cars lacking detail. 
While Get3D~\cite{get3d2022} achieve higher resolution, it suffers from 3D inconsistency, numerous artifacts, and lack richness in 3D detail. Similar issues are found in GaussianCube~\cite{gaussiancube2024} along with symmetric inconsistency in the results. 
In contrast, our method generates high-quality, high-resolution objects that are 3D consistent with sharp, well-defined edges.
The top three rows show the unconditional generation results of our method using ShapeNet dataset, while the bottom 3 show from Objaverse dataset.
}
\label{fig:supp_uncond}
\end{figure*}




\begin{figure*}[!h]
\centering
\includegraphics[width=6.4in]{imgs/supp_conditional.jpg} 
\caption{
Text-conditioned generation results on various baselines and the proposed method. Our method not only generates high-quality assets for simpler objects, but also for complicated objects with intricate geometries like \textit{the wheel} or \textit{the airplane}.
}
\label{fig:supp_cond}
\end{figure*}



\begin{figure*}[t]
\centering
\includegraphics[width=5.3in]{imgs/UVGS_scene.jpg} 
\caption{
To show the effectiveness of proposed UVGS maps in capturing the intricacies of a complex real-world scene, we used a 12 layer UV map to reconstruct the 3D scenes.
}
\label{fig:supp_scene}
\end{figure*}