\vspace{-0.1cm}
\section{Related Work}\label{sec:literature}
% \ar{in progress... please add anything that is relevant}
% \dilin{UV mapping..} \ap{} \ar{Do we really need that?}

\begin{figure*}[t]
\centering
\includegraphics[width=0.92\linewidth]{imgs/uvgs_arch.jpg} 
\vspace{-0.2cm}
\caption{The input 3DGS object is first converted to UVGS maps through spherical mapping. 
We use a multibranch forward mapping network to convert the obtained 14-channel UVGS to a compact 3-channel Super UVGS image. This represents the 3DGS object in a structured manner and can be used with image foundation models for reconstruction or generation. 
The Super UVGS is mapped back to UVGS through branched inverse mapping, which in turn can be reconstructed back to the 3DGS object through inverse spherical mapping. 
% The forward and inverse mapping networks are trained together using reconstruction losses. 
}
\label{fig:architecture}
\vspace{-0.5cm}
\end{figure*}

\noindent\textbf{3D Generative/Reconstruction Models for Objects}:
% \ar{Discuss about various 3D representations and 3D asset generation/reconstruction. Introduce 3DGS and recent advancements in object representation.}
%
Generation or reconstruction of 3D assets has been a long standing task \cite{dreamfusion2022, magic3d2023, prolificdreamer2024, dreambooth3d2023, latentnerf2023, gaussiandreamer2023, dreamcraft3d2023, makeit3d2023, eg3d2022, albedogan2024, 3dfacecam2023, tsdf2024, genheld2024}. Previous reconstruction approaches like NeRF~\cite{nerf2021, mipnerf2021} are often slow and do not provide a defining geometry \cite{latentnerf2023, dreambooth3d2023, neaf2023, udiff2024, tensorf2022, dnerf2021, plenoxels2022, neus22023, neusurf2024, geodream2023, zeroshot2024}. Advancements in the field led to the emergence of explicit voxel grid based representations that encode colors and opacities directly \cite{sdfusion2023, diffrf2023, volumediffusion2023}. These approaches achieve significant speed ups compared to the NeRF based approaches, but they can't produce high fidelity assets due to the low resolution of voxel grids. On the other hand, triplane representation \cite{3dgen2023, triplanemeetsgs2024, triplanediff2023, rodin2023} provides a trade-off between the quality and memory utilization. 
Another line of work \cite{geometry_img_diffusion2024, omages2024} splits the input mesh into different patches and simplifies the object generation problem to an image generation problem. 
However such methods rely on either cutting through the mesh to create a geometry image~\cite{geometry_img_diffusion2024} or rely on an existing UV representation of the geometry and utilize subset of existing UV islands~\cite{omages2024} resulting in loss of details. 
% However, the fact they assumes meshes to have patch decomposition makes them constrained and also results in visible cracks~\cite{geometry_img_diffusion2024} and artifacts~\cite{omages2024} in synthesized objects.
Recently, there has been a notable advancement in 3D Gaussian Splatting (3DGS) for the representation of objects and scenes leding to the emergence of 3DGS showcasing impressive real-time results in reconstruction and generation tasks \cite{3dgs2023, 4dgs_realtimescene2024, dngaussian2024, gaussiandreamer2023, dreamgaussian2023, viewconsistentediting3dgs2025, gsedit2024, grm2024, triplanemeetsgs2024, gslrm2025, gvgen2025}. 
%
Recent advances in the 3D generative models for asset synthesis using the existing geometries like NeRF, voxel grids, or triplane geometries \cite{dreamgaussian2023, grm2024, triplanemeetsgs2024, gslrm2025, get3d2022, difftf2023} leverage generative models \cite{stable_diffusion, ddim} and the existing 3D datasets~\cite{shapenet2015, objaverse2023}. However, most of the works employing 3DGS or other representations use multiview rendering and Score Distillation Sampling (SDS) to achieve convincing generation and reconstruction capabilities \cite{textto3Dusinggs2024, dreamfusion2022}. These approaches demand high memory and compute resources and are often quite slow in optimizing due to per scene optimization. 



%Direct Learning on Trained 3DGS / 
\noindent\textbf{Giving Structures to Discrete Gaussians}:
% \dilin{there are some repeating sentences here, eg. SDS has been mentioned before. Also we do we want to discuss SDS, SDS is an optimization technique, while we focus mostly presentation? these two are orthogonal? this could be even one of our followups, building on UVGS and using SDS to optimize very high quality 3DGS for data curation.}
Although, 3DGS has led to breakthrough in the reconstruction field by demonstrating superior performance in multiple domains, the generation of 3DGS directly remains challenging due to its discreteness and unstructured nature~\cite{diffgs2024, gaussiancube2024}. 
% The 3DGS framework is discrete and permutation invariant, meaning that randomly shuffling the order of points does not impact the resultant shape of the object. Consequently, it can be treated as a set of Gaussian points with no correspondence between them. Additionally, these points are unstructured in the spatial domain. 
These characteristics present substantial challenges when integrating them with conventional computer vision models, like Autoencoders and generative models \cite{diffgs2024}.
%
The research in direct learning of trained 3DGS primitives is largely unexplored~\cite{shapesplat2024}. 
% Most existing generation and reconstruction methods that utilize 3DGS employ multi-view rendering and SDS loss \cite{dreamgaussian2023, gaussiandreamer2023, dngaussian2024, gvgen2025, gslrm2025} to generate various views and optimize the Gaussians. 
% This approach is both time-consuming and memory-inefficient, necessitating complex architectures to generate 3DGS assets. 
%
Some efforts attempt to address this by directly predicting 3DGS attributes using diffusion models~\cite{gsd2024} while others
% These are constrained to generating a limited number of 3DGS Gaussian points due to the unstructured and permutation-invariant nature of 3DGS.
like Splatter Image~\cite{splatterimage2024} project Gaussian objects into image-based representations through direct 3D-unaware projection. 
These methods struggle with maintaining multiview consistency, as the model only infers seen poses correctly, while hallucinating for unseen poses. 
% The scarcity of extensive 3D data hampers the ability to produce high-quality results for such methods. 
%



Concurrent works~\cite{gaussiancube2024, gvgen2025} follow the voxel-based representations to transport Gaussians into structural voxel grids with volume generation models for generating Gaussians. However, these methods are computationally expensive for high-resolution voxels, face difficulties in preserving high-quality Gaussian reconstructions due to information loss during voxelization. 
% , and a restricted number of generated Gaussians constrained by voxel resolutions. 
% For example, GaussianCube~\cite{gaussiancube2024} can accommodating only up to 32,000 Gaussians. However, in practice, representing an object adequately with 3DGS may require up to a few hundred thousand Gaussians~\cite{3dgs2023, diffgs2024}.
% A very recent concurrent work, 
DiffGS~\cite{diffgs2024} tries to solve the above issues by proposing three continuous functions to represent 3DGS. However, it is limited to only category-level generation and learning generic probability functions for all the categories poses significant compute and design challenges.
%
In contrary, we introduce an efficient way to give structures to discrete Gaussians by taking inspiration from the developments in 3D graphics. 
Our method does not require any learning to map an unstructured set of Gaussians to this efficient and structured representation (termed UVGS). 
The proposed representation provides local and global correspondence among different Gaussian points making the widely available existing computer vision frameworks learn and extract underlying features from them.