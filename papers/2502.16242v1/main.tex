\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}    % For \toprule, \midrule, \bottomrule [[3]]
\usepackage{tabularx} % For flexible column widths
\usepackage{geometry} % To adjust page margins
%\geometry{margin=1in} % Set margins
\usepackage{multirow}    % For \multirow command
\usepackage{array}       % For advanced table formatting
\usepackage{graphicx}    % For \resizebox
\usepackage{xcolor}      % For colored cells
\usepackage{soul}        % For highlighting with \hlc
\usepackage{bbding}
\usepackage{subcaption}
\usepackage{mdframed}
\usepackage{ragged2e}
\usepackage{placeins}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%

\newcommand{\hlc}[2][yellow]{{%
  \colorlet{foo}{#1}%
  \sethlcolor{foo}\hl{#2}}%
}
\definecolor{light_yellow}{RGB}{245, 233, 66}
\newcommand{\Gap}[1]{\textcolor{red}{#1}}


\title{Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{
Jose L. García\thanks{Equal contribution}, 
    Karolína Hájková\footnotemark[1], 
    Maria Marchenko\footnotemark[1], 
    Carlos Miguel Patiño\footnotemark[1] \\
    \addr University of Amsterdam \\
    \email \{jose.garcia.carrillo, karolina.hajkova2, maria.marchenko, carlos.patino.paz\}@student.uva.nl
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{01}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
This paper presents a reproducibility study and extension of "Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation." We validate the original findings using a range of open-weight models (1.5B-70B parameters) and GPT-4o Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into the accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems.
\end{abstract}

\section{Introduction} \label{sec:introduction}

The rapid advancement of Large Language Models (LLMs) has opened new opportunities for Artificial Intelligence (AI) applications, particularly in the form of autonomous AI agents. These agents, capable of interacting and communicating with one another, can be especially useful in negotiation systems. Negotiations involve complex multi-agent interactions that require resolving multi-issue scenarios with little to no supervision, so it is critical to evaluate whether these agents are reliably performing their intended tasks \citep{shavit2023practices}. Within this context, reproducibility studies are valuable, as they help validate benchmarks the AI community can use to evaluate the performance of current and future models \citep{reuel_betterbench_2024, chang_survey_2023}. This paper aims to reproduce the results of an LLM testbed specially designed for negotiation games \citep{abdelnabi2024cooperation} to contribute to developing robust benchmarks.

Our reproducibility goal is to assess whether the benchmark effectively evaluates the models' negotiation performance and whether the Chain-of-Thought (CoT) prompt configurations contribute to successful deal-making, especially in terms of accountability \citep{visibility_ai_agents} and confidentiality. To achieve this, we compared different open-weight models against a state-of-the-art (SOTA) closed-source model. Building on the original paper, we analyze the Pareto front of the proposed games and establish a strong baseline to test whether an agent can successfully propose a deal without communicating with other parties.

We also present extensions that address the environmental impact, accessibility, fairness, and confidentiality. For environmental impact and accessibility, we use the benchmark to evaluate open-weight models and compare their performance to larger, more energy-consuming ones. Smaller models reduce the computational resource requirements \citep{fu_tiny_2024, sinha_are_2024}, and we compare the performance between models that range from 1.5B to 70B parameters. For fairness, we introduce an inequality metric to assess whether all parties benefit equally from the negotiation. For confidentiality, we compare open-weight models with a closed-source model that requires access through an external API and may expose attack surfaces that can be exploited by malicious actors \citep{evertz_whispers_2024, dunn2024owasp}.


\section{Scope of Reproducibility}\label{sec:scope}

We focus our reproduction efforts on investigating the following claims from the paper:

\begin{enumerate}
    \setlength\itemsep{0em} 
    \item Open-weight models fall slightly behind SOTA closed-source models.\label{claims:num_1}
    \item Small models---i.e., models with 8B parameters or less---are not suitable for the negotiation benchmark because they cannot follow the format of the game.\label{claims:num_2}
    \item Agents have better outcomes when using CoT structures in their prompts. \label{claims:num_3}
    \item The agents’ incentives in the benchmark result in non-zero-sum games.\label{claims:num_4}
    \item The benchmark measures cooperation, communication, and negotiation skills.\label{claims:num_5}
\end{enumerate}

The code and instructions for reproducing the results of this work, along with our additions and the fixes mentioned in Section \ref{sec: code_additions}, are available on Github \footnote{https://anonymous.4open.science/r/llm\_negotiation-E731/README.md}.

\section{Original Paper}

\subsection{Benchmark and Base Game}\label{sec:original_paper}

The original paper \citep{abdelnabi2024cooperation} introduces a benchmark to evaluate whether LLMs can successfully reach agreements in a multi-agent, multi-issue, semantically rich negotiation game over multiple rounds. The benchmark is designed to test agents in a dynamic, multi-turn scenario that requires advanced reasoning and strategic decision-making. Specifically, it assesses their ability to reach agreements through arithmetic calculations, inference, exploration, and planning, in addition to essential skills such as communication, collaboration, and instruction following.

The benchmark's base negotiation game is adapted from HarborCo \citep{susskind_scorable_1985}, a game traditionally used to teach negotiation skills involving six parties negotiating over five key issues related to constructing a sports complex. Each party is assigned a name and a role with different goals and preferences for each issue. That way, every issue is presented with three to five options, each assigned with a score for the evaluating agent. A deal proposal is agreeable for an agent if the sum of the chosen options across all issues meets or exceeds its minimum utility score threshold.

Before the game begins, all agents are given a global context outlining the negotiation setup, including information about the issues and the other agents' assigned names, roles, objectives, and preferences. The game then encourages agents to cooperate and compromise when necessary over the negotiation rounds to achieve a successful outcome, as a negotiation is only considered successful if at least five out of the six players surpass their scores threshold and accept the final deal, including Player 1 ($p_1$) and Player 2 ($p_2$). 

Player 1 and Player 2 are specially designated roles with veto power: for the final deal to be accepted, it must receive approval from both. $p_1$, referred to as \textit{the main negotiator}, is also responsible for proposing the negotiation's first and final deals. As the main negotiator, $p_1$ receives a bonus of 10 points upon suggesting a final deal accepted by everyone. 

The initial deal is provided to $p_1$ at the start of the game, serving as the foundation for the follow-up discussions, and the remaining five agents aim to negotiate better deals for themselves. If the parties fail to reach a valid agreement, each agent receives their Best Alternative To a Negotiated Agreement (BATNA) score. BATNA typically corresponds to the minimum utility score threshold of each party but may vary depending on the game variation.

\subsection{Game Variations}\label{sec:original_paper_variations}

To further expand the benchmark, the authors introduced variations of the base game where a single agent exhibits greedy or adversarial behaviors, influencing interactions and outcomes. In these variations, an agent can be designated as greedy, focusing solely on maximizing its own benefit, or adversarial, actively targeting another agent to sabotage the deal.

The adversarial behavior is also divided into two variations: targeted, where the adversarial agent is explicitly assigned a specific target, and untargeted, where the agent independently selects which opponent to sabotage.

LLMs are evaluated even further across four difficulty levels: \textit{Base, Game 1, Game 2, and Game 3}. These variations adjust the base game's difficulty, with Games 1 and 3 being more challenging and Game 2 presenting the highest level of difficulty. Each variation introduces a new and unique setup with modified negotiation roles, objectives, and minimum utility thresholds designed to test the robustness of LLMs across diverse scenarios.

\subsection{Evaluation Metrics}\label{sec:original_paper_metrics}

To systematically evaluate agent performance on all of these game variations, the benchmark introduces metrics to quantify how well agents align with their assigned roles and objectives, as well as their adherence to the negotiation's rules. These metrics include:

\begin{itemize}
    \setlength\itemsep{0em} 
    \item \textit{5/6-way} and \textit{6-way}: The percentage of experiments in which the final deal is accepted by at least five or all six parties.
    \item \textit{Any}: The percentage of experiments in which there exists a deal accepted by at least five parties at any point in the negotiation.
    \item \textit{Wrong}: The percentage of rounds in which an agent proposed a deal below its own score threshold.
    \item \textit{Leakage of Information}: The percentage of rounds in which an agent revealed their reasoning or private information to other parties.
\end{itemize}

We used the 5/6 way and 6-way accepted deals metrics as our main method of comparing the models, using their values after the final round of negotiation. As mentioned before, the only agent able to propose final deals is $p_1$. Other models are not prompted to vote, as voting happens automatically: if the suggested deal meets the minimum score of the agent it automatically votes for the deal. This setup creates a significant power imbalance between the models: as only $p_1$ can suggest deals, and the others accept or decline them automatically, their only way of influencing the outcome of the game is trying to convince $p_1$ of suggesting a more preferable deal for them.

\section{Methodology}

\subsection{Models}

The original paper compares the benchmark on different open- and closed-weight LLMs. To reproduce the experiments and validate their claims, we selected a range of models that performed well on common evaluation metrics \citep{chiang2024chatbot}. Given the concerns about reproducibility with proprietary models \citep{palmer2024using}, our budget and computational resource constraints, we opted for open-weight models, as detailed in Table \ref{tab:model_comparison}.

Furthermore, since the original paper focused on using GPT-4 \citep{openai_gpt-4_2023}, we chose GPT-4o Mini \citep{gpt4o_mini} as a more powerful and cost-effective alternative.

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{4pt} % Adjust the spacing between columns
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Model} & \textbf{Open Weights} & \textbf{Size} & \textbf{Release Date} & \textbf{Context size} & \textbf{GPQA} & \textbf{MMLU} & \textbf{Energy, J} \\
        \midrule
        GPT-4o Mini & \xmark & $\sim$8B* & Jul 2024 & 128k & 40.2\% & 82.0\% & -- \\
        \href{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct}{Llama 3.3 70B} & \Checkmark & 70B & Dec 2024 & 128k & 50.5\% & 86.0\% & ~800k \\
        \href{https://huggingface.co/meta-llama/Meta-Llama-3-70B}{Llama 3.0 70B} & \Checkmark & 70B & Apr 2024 & 8k & 39.5\% & 82.0\% & 600k \\
        \href{https://huggingface.co/meta-llama/Llama-2-13b-chat-hf}{Llama 2 13B} & \Checkmark & 13B & Jul 2023 & 4k & 22.3\% & 47.8\% & 2000k \\
        \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama 3.1 8B} & \Checkmark & 8B & Jul 2024 & 128k & 30.4\% & 69.4\% & 1000k \\
        \href{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}{Qwen 2.5 7B} & \Checkmark & 7B & Sept 2024 & 128k & 36.4\% & 74.2\% & 100k \\
        \href{https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct}{Qwen 2.5 1.5B} & \Checkmark & 1.5B & Sept 2024 & 128k & 24.2\% & 60.9\% & 20k \\
        \href{https://huggingface.co/Qwen/Qwen2.5-32B-Instruct}{Qwen2.5 32B Instruct} & \Checkmark & 32B & Sept 2024 & 131k & 49.5\% & 83.3\% & 150k \\
        \href{https://huggingface.co/microsoft/phi-4}{Phi-4} & \Checkmark & 14B & Dec 2024 & 16k & 56.1\% & 84.8\% & 600k \\
        \href{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B}{DeepSeek-R1-Distill 7B} & \Checkmark & 7B & Jan 2025 & 128k & 49.1\% & -- & 150k \\
        \bottomrule
    \end{tabular}}
    \caption{Models used for reproducing the original study of the paper. The number of parameters from GPT-4o Mini is an estimation based on \citet{abacha2024medec}. GPQA is a Graduate-Level Google-Proof Q\&A Benchmark. MMLU is the Massive Multitask Language Understanding benchmark. The benchmarks for the latest DeepSeek were not reported. We report the consumed energy for one game, and we provide an analysis of energy consumption in Section \ref{sec: energy_consumption}}
    \label{tab:model_comparison}
\end{table}

% Models: Llama 3.3 70B, Llama 3.0 70B, Phi 4 14.7B, Qwen 2.5 1.5B, GPT4o-mini, Llama 3.1 8B, Llama 2 13B

\subsection{Reproduction Setup}

We used the code provided by the authors\footnote{https://github.com/S-Abdelnabi/LLM-Deliberation} to reproduce the results in the original paper. While the authors' repository included much of the necessary code, the specific prompts used in the ablation study were not accessible. Therefore, we inferred the relationships between the available prompts and their corresponding ablation configurations. Restructuring the cooperative scratchpad prompts (Section \ref{subsec:coop-prompts}) and parametrizing the program's interface to select between different CoT prompts, we enabled an easier reproduction of the ablation study in Table \ref{tab:cot_ablations} of the original paper.

\subsection{Extensions}

\subsubsection{Pareto Analysis}

We use Pareto front membership as a binary indicator of a deal's quality.

\textbf{Definition 1:}  
Given two deals \( d_1 \) and \( d_2 \), we say that \( d_1 \) \textit{Pareto dominates} \( d_2 \) if, for at least one party, the score in \( d_1 \) is strictly better than in \( d_2 \), and for all parties, the score in \( d_1 \) is not worse than in \( d_2 \).

\textbf{Definition 2:}  
The \textit{Pareto front} is the set of deals not Pareto dominated by any other deal.

\subsubsection{Single-agent Baseline}

The original paper proposes a multi-agent setup, as described in Section \ref{sec:original_paper}, where agents communicate and negotiate over multiple rounds to refine the initial deal. However, we observed that the global instructions given to the agents before the game begins contain crucial information regarding the overall setup, including information related to other agents.

We argue that, based on the names of the other agents, such as \textit{Department of Tourism} and \textit{Environmental League}, along with the provided descriptions for each issue and their options, the main negotiator $p_1$ can infer enough information to develop an understanding of the other agents' motivations and goals without direct communication. We provide a detailed example of the information available to $p_1$ from the game's description in Section \ref{sec: example_game_description}.

We developed two scenarios to evaluate $p_1$'s ability to generate deals independently. The first, \textit{Single-agent 1 call}, is a simplified version where $p_1$ suggests the given initial deal to himself, then reasons for a single round, and proposes the final deal immediately. The second, \textit{Single-agent 6 calls}, is an extended version that allows the agent to prompt itself iteratively over six rounds. In this setup, the agent continuously refines and expands its previous reasoning and planning before proposing a new deal.

We tested this new baseline across most of the game variations, including greedy agents and Games 1, 2 and 3. We expected this baseline to perform well in the compromising variant but poorly in the greedy variant, as the latter requires an agent to act beyond its initial prompt specifications. We did not include the adversarial game variant since it relies on agents communicating to achieve their adversarial role.

In the original paper, the authors already explored a configuration without direct communication between agents. However, this setup differs from our proposed baseline. In their approach, the initial deal is generated randomly, and then each agent, without communicating, improves the deal independently based on its own hidden score before passing it to the next agent. Despite not communicating, there is still some form of interaction in this setup, as each agent improves the deal for itself before passing it. In contrast, our setup demonstrates how much information a single agent can infer from the open game rules. 

We evaluate this baseline using the two best-performing models on the multi-agent original setup: the proprietary GPT-4o Mini and open-weight Qwen 2.5 32B \citep{qwen2}.

\subsubsection{Structure Leakage} \label{sec:leakage_meth}

Open LLMs face more challenges to adhere strictly to instructions than proprietary models \citep{gudibande2023false}, which poses a problem in games requiring strict instruction obedience for formatting rules. In the negotiation game, LLMs are prompted to generate a response (\texttt{full answer}) with specific tags in place to distinguish between their \texttt{plan}, \texttt{scratchpad}, and \texttt{public answer}. If the agent fails to separate its output correctly, parts of its private output, like the CoT about other agents or future plans, are shared with all agents. This information leakage is possible due to the modeling decisions in the original implementation.

To measure this structural leakage of information, we check whether 1) the \texttt{full answer} and \texttt{public answer} differ, 2) at least one of the \texttt{<PLAN>}, \texttt{</PLAN>}, \texttt{<SCRATCHPAD>}, \texttt{</SCRATCHPAD>} tags is present in the public answer, or 3) tags \texttt{<DEAL>} or \texttt{</DEAL>} are missing on the public answer. If either of these conditions is met, the agent did not conform to the instructions, implying the unwanted release of information. We report this metric because sharing the \texttt{full answer} of the agents disrupts the negotiation dynamics and introduces unrealistic scenarios that could be avoided using different implementation decisions. 

On the original paper, the leakage of information metric was measured using a LLM agent, GPT-4, to check the answers of the agents after each round and check the percentage of answers with leaked private information. We compare our structure-induced leakage scores to the scores of the original leakage metric, using GPT-4o Mini instead.

\subsubsection{Inequality Metric} \label{sec:inequality_methodology}

The original paper compares the individual score of agent $p_1$ to the collective score of the group (the mean score of all agents) to track the dynamics of the negotiation throughout the game. Building on this, we propose to measure inequality among the agents to explore how the negotiation process affects the distribution of outcomes. This allows us to observe whether the game produces fair outcomes or if certain agents are disadvantaged---particularly since a successful deal only requires five out of six players to agree. To measure inequality, we use the Gini coefficient \citep{dorfman1979formula}, a commonly used metric that ranges from 0 (perfect equality) to 1 (maximum inequality). We use Equation \ref{eq: gini} to compute the Gini coefficient, where $x_i$ and $x_j$ are the gains of agents, $n$ is the number of agents, and $\bar{x}$ is the mean gain.

\begin{equation}
\text{Gini} = \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} |x_i - x_j|}{2n^2 \bar{x}}
\label{eq: gini}
\end{equation}

We report the Gini coefficient for the different game variants (compromising, greedy, untargeted adversarial, and targeted adversarial) and the multi and single-agent setups with CoT configuration of Row 2 from Table \ref{tab:cot_ablations}. This CoT configuration was chosen to give greedy and adversarial agents more opportunity to analyze the negotiation. Due to the lack of communication with other parties and selfish incentives, single-agent, greedy, and adversarial variants will likely lead to higher Gini coefficients than in the compromising game.

\section{Results}

\subsection{Reproducibility of Original Paper}

To reproduce the original paper's results as closely as we could, we evaluated the performance of different models on the original multi-agent negotiation benchmark. Using each of these models to act as different negotiation agents, we compared their ability to produce successful deals and follow the structured negotiation process rules. Table \ref{tab:all_models} presents the performance of all tested models. The performance of different models was evaluated using the metrics defined in the original benchmark, described in \ref{sec:original_paper_metrics}.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}
        {@{\extracolsep{1mm}}l | c c c c c c} \toprule 
        \textbf{Model} & \multicolumn{2}{c}{\textbf{Final (\%) $\uparrow$}} & \textbf{Any (\%) $\uparrow$} & \textbf{Wrong (\%) $\downarrow$} & \textbf{\shortstack{Failed \\ Experiments (\%)} $\downarrow$} & \textbf{\shortstack{Structure \\ Leakage (\%) $\downarrow$}} \\ \cline{2-3}
        & \textbf{5-way} & \textbf{6-way} & \\\midrule 
        GPT-4o Mini & 80 & 0 & 100 & 11.15 & 0 & 0 \\
        Llama 3.3-70B & 70 & 10 & 100 & 7.3 & 0 & 0 \\
        Llama 3.0-70B & 40 & 10 & 90 & 7 & 0 & 0 \\ 
        Qwen 2.5 32B & 60 & 30 & 90 & 9.2 & 0 & 0 \\
        Llama 2-13B & - & - & - & - & 80 & - \\
        Phi-4 14B & 30 & 0 & 90 & 8.84 & 0 & 0.4 \\
        Llama 3.1-8B & 44 & 11 & 88 & 18 & 10 & 38.8 \\
        Qwen 2.5 7B & 30 & 10 & 50 & 14.2 & 0 & 45.8 \\
        DeepSeek-R1 7B & 0 & 0 & 0 & 48 & 0 & 43.1 \\
        Qwen 2.5 1.5B & 30 & 30 & 30 & 19.5 & 60 & 66.5 \\
        \bottomrule
    \end{tabular}}
    \caption{Performance of all models using the metrics from \citet{abdelnabi2024cooperation} with 10 experiments per model. We introduced \textit{Failed Experiments} metric counting experiments in which the model failed to complete the negotiation (we describe these problems in Section \ref{sec: small_models}). The \textit{Structure Leakage} column comes from the leakage comparison metrics discussed in Table \ref{tab:leakage_comparison}.}
    \label{tab:all_models}
\end{table}

To illustrate the correlation between model size and performance, we refer to Figure \ref{fig:size_vs_score}. It clearly shows that increasing model size generally improves performance. Smaller models, such as Qwen 2.5 1.5B and DeepSeek-R1 7B, struggled to reach acceptable deals and exhibited a high percentage of incorrect deals or failed experiments.

\begin{figure}[t]
    \begin{center}
    %\framebox[4.0in]{$\;$}
    \includegraphics[width=0.8\linewidth]{figures/size_vs_score.png}
    \end{center}
    \caption{The figure illustrates how performance improves for larger models when controlling for model provider and version (dotted line). It also shows performance gains in newer model versions---e.g., Llama 3.3 70B compared to Llama 3 70B. A more detailed analysis of the failure modes for smaller models and the impact of model size can be found in Section \ref{sec: small_models}}
    \label{fig:size_vs_score}
\vspace{-2mm}
\end{figure}

To further evaluate the reproducibility of the original paper, we conducted an ablation study to examine the impact of prompt structure on performance, as shown in Table \ref{tab:cot_ablations}. We systematically tested various CoT structures---such as calculating scores for previous deals, inferring other agents' preferences, generating candidate proposals, and planning---to asses their effect on the negotiation process.

\begin{table*}[t] \centering 
    \resizebox{\linewidth}{!}{
    \begin{tabular}{
        @{\extracolsep{1mm}}l|| l ll ll l || ll ll l l} \toprule 
        \textbf{Model} & row no. & \multicolumn{2}{c}{\textbf{CoT: Observation}} & \multicolumn{2}{c}{\textbf{CoT: Exploration}} & \textbf{CoT: Planning} & \multicolumn{2}{c}{\textbf{Final (\%) $\uparrow$}} & \textbf{Any (\%) $\uparrow$} & \textbf{Wrong (\%) $\downarrow$} & \textbf{Leak (\%)} & \textbf{Gini} \\ \cline{3-4} \cline{5-6} \cline {8-9}
        & & \textbf{Prev. deals} & \textbf{Others' prefer.} & \textbf{Candidates} & \textbf{Selection} & & \textbf{5/6-way} & \textbf{6-way} & & & & \\  \midrule  
        \multirow{6}{*}{GPT-4o Mini} 
        & 1 & \xmark & \xmark & \xmark & \xmark & \xmark & 30 & 0 & 100 & 8.85 & 0 & 0.15 \\ 
        & 2 & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark & 70 & 0 & 100 & 2.71 & 0.8 & 0.14 \\ 
        & 3 & \Checkmark & \Checkmark & \hlc[light_yellow]{\xmark} & \Checkmark & \Checkmark & 70 & 0 & 90 & 1.92 & 0 & 0.13 \\ 
        & 4 & \Checkmark & \Checkmark & \xmark & \Checkmark & \hlc[light_yellow]{\xmark} & 70 & 0 & 100 & 1.92 & 0 & 0.11 \\ 
        & 5 & \hlc[light_yellow]{\xmark} & \Checkmark & \xmark & \Checkmark & \Checkmark & 80 & 0 & 100 & 11.15 & 0 & 0.11 \\ 
        & 6 & \xmark & \hlc[light_yellow]{\xmark} & \xmark & \Checkmark & \Checkmark & 70 & 0 & 70 & 10.77 & 0 & 0.14 \\ 
        \bottomrule 
    \end{tabular}} 
    \caption{This table replicates Table 3 from \citet{abdelnabi2024cooperation}, replacing GPT-4 with GPT-4o Mini.   Elements highlighted in yellow indicate modifications to the specific CoT structures. Our results align with the original paper, showing that skipping calculations for previous deals or candidate proposal generation improves performance.
    }
    \label{tab:cot_ablations} 
\vspace{-2mm}
\end{table*}

These ablation results are further illustrated in Figure \ref{fig:ablations}, which visualizes the progression of deals from $p_1$'s perspective over multiple rounds for each configuration. In the best-performing configuration (Figure \ref{fig:ablation_best}), Row 5), $p_1$ refines the deal to increase the collective score while decreasing their own, ultimately leading to a successful agreement. 

In contrast, configurations that omit essential steps (Figures~\ref{fig:no_planning} and~\ref{fig:no_others}, Row 4 and Row 6 respectively) stagnate and saturate on the final rounds, with little improvement in scores and reduced agreement success rates. Meanwhile, the "all steps" configuration (Figure~\ref{fig:ablation_worst}, Row 2) suggests that agents tend to prioritize self-beneficial deals over collective success.

\begin{figure*} [!t]
    \centering
\begin{subfigure}{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gpt4o_mini_row5.png}
         \caption{Best (Row 5).}
         \label{fig:ablation_best}
     \end{subfigure}
     \begin{subfigure}{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gpt4o_mini_row4.png}
         \caption{``No plan'' (Row 4).}
         \label{fig:no_planning}
     \end{subfigure}
     \begin{subfigure}{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gpt4o_mini_row6.png}
         \caption{``No others'' (Row 6).}
         \label{fig:no_others}
     \end{subfigure}
     \begin{subfigure}{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/gpt4o_mini_row2.png}
         \caption{All steps (Row 2).}
         \label{fig:ablation_worst}
     \end{subfigure}
     \caption{$p_1$'s deals progression over rounds of GPT-4o Mini experiments in Table ~\ref{tab:cot_ablations}. The results closely match those reported by \citet{abdelnabi2024cooperation} for GPT-4.}
     \label{fig:ablations}
\end{figure*}

Overall, these results highlight the importance of both model size and CoT prompt structure in successful multi-agent negotiations. Our findings reaffirm the original paper, showing that while larger models tend to perform better, prompt design remains crucial for robust performance. Together, these factors are essential in evaluating the quality and effectiveness of the benchmark itself.

\subsection{Pareto Efficiency Analysis}

Table \ref{tab:game_metrics_comparison} shows that almost all acceptable deals are on the Pareto front while all the unacceptable deals are outside of it. Since all non-acceptable deals result in a score composed of acceptance thresholds (BATNAs), all acceptable deals dominate the non-acceptable ones. Without the BATNA rule, the game's Pareto front would be 705/720. 

The game is non-zero-sum, as the authors state because the BATNA rule limits the Pareto front almost entirely to the set of acceptable deals. However, the game becomes almost zero-sum as soon as the models reach the set of acceptable deals. We identify two main implications of this finding:

\begin{enumerate}
    \setlength\itemsep{0em} 
    \item Models have no collective incentive to improve deals since there is no room for optimization beyond acceptability. The problem that the models are collectively solving is not optimization of the score, but identifying any deal from the subset of acceptable deals.
    \item Almost all acceptable deals are non-comparable in terms of quality: improving the score of one party means worsening the score of another.
\end{enumerate}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{
        @{\extracolsep{1mm}}l | c c c c} \toprule
        \textbf{Game} & \textbf{Acceptable Deals} & \textbf{Pareto Size} & \textbf{Min/Avg/Max Score} & \textbf{Min/Avg/Max Inequality} \\ \midrule
        Base & 77/720 & 62/77 & 51.5/60.5/69.8 & 0.04/0.14/0.26\\
        Game 1 & 57/720 & 47/57 & 56.3/67.5/72.3 & 0.05/0.09/0.14 \\
        Game 2 & 66/720 & 50/66 & 46.5/57.2/66.3 & 0.03/0.15/0.27 \\
        Game 3 & 89/720 & 34/89 & 54.2/70.1/85.7 & 0.03/0.09/0.21 \\
        \bottomrule
    \end{tabular}}
    \caption{Results for the Pareto front analysis for all game variations. All the games have all the acceptable deals inside the Pareto front. Note, that the game statistics (Acceptable Deals) is different from the one provided by authors, since we found a bug in the code corresponding to game analysis.}
    \label{tab:game_metrics_comparison}
\vspace{-2mm}
\end{table}

\subsection{Single-agent Setup}

Our results show that the single-agent configuration performs just as well as the original multi-agent setup. Without communicating with other agents, and using only information from the global initial game rules provided, the model can predict acceptable deals with the same success rate as the multi-agent setup. 

Figures \ref{fig:single_vs_multi_base_gpt4} and \ref{fig:single_vs_multi_base_qwen} compare the single-agent setup to the original multi-agent game using the CoT configuration from Row 5 of Table \ref{tab:cot_ablations}. The results indicate that the single-agent setup closely matches the multi-agent performance and, in some cases, even outperforms it in terms of collective scores. Notably, both models achieve very similar performance between the two setups.

Table \ref{tab:unified_agent_comparison} further supports this trend, showing that the final deals proposed in the single-agent setup are comparable to--or even better than---those using the multi-agent configuration. These results challenge the original claim that the benchmark evaluates cooperation, communication, and negotiation skills, as they suggest that agent interaction is not essential for success. The same pattern emerges across different game configurations, as shown in Figure \ref{fig:single_vs_multi_qwen_additional_games}, suggesting that the benchmark does not effectively evaluate the communication and negotiation skills of the agents.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/negotiation_curves_single_multi.png}
    \caption{Comparison of the single- and multi-agent approaches in the base negotiation game for GPT-4o Mini. The plots show similar results when we allow communication (pink) vs. no communication (green).}
    \label{fig:single_vs_multi_base_gpt4}
\end{figure}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{
        @{\extracolsep{1mm}}l | @{\extracolsep{1mm}}l | cc c c c c} \toprule 
        \textbf{Model} & \textbf{Configuration} & \multicolumn{2}{c}{\textbf{Final (\%) $\uparrow$}} & \textbf{Any (\%) $\uparrow$} & \textbf{Wrong (\%) $\downarrow$} & \textbf{Leak (\%)} & \textbf{Gini} \\ \cline{3-4}
        & & \textbf{5-way} & \textbf{6-way} & & \\\midrule 
        \multirow{3}{*}{GPT-4o Mini} & Multi-agent & 80 & 0 & 100 & 11.2 & 0 & 0.11 \\
        & Single-agent (6 calls) & 80 & 0 & 100 & 0 & 0 & 0.13 \\
        & Single-agent (1 call) & 90 & 0 & 90 & 0 & 0 & 0.11 \\
        \hline
        \multirow{3}{*}{Qwen 2.5 32B} & Multi-agent & 60 & 30 & 90 & 9.2 & 0 & 0.12 \\
        & Single-agent (6 calls) & Yes & No & Yes & 0 & 0 & 0.11 \\
        & Single-agent (1 call) & Yes & No & Yes & 0 & 0 & 0.13 \\
        \bottomrule
    \end{tabular}} 
    \caption{Comparison of different LLMs and their performance across setups. Qwen's results are interpreted as binary since its output is deterministic in the single-agent variant. Yes and No corresponding to success in the given metric.}
    \label{tab:unified_agent_comparison}
\vspace{-2mm}
\end{table}

\subsection{What the Game Actually Measures}

The game is originally presented as an assessment of how well six agents negotiate the best possible deal. However, as demonstrated above, the problem effectively reduces to finding any acceptable deal---most of which already lie on the Pareto front. A crucial yet unstated detail in the original paper is that the agents do not actually vote themselves. Instead, voting is automated: an agent accepts a deal if it meets its minimum score threshold. This creates an inherent imbalance---only $p_1$ has decision-making power, while the other agents merely provide information. Moreover, our single-agent baseline performs as well as the multi-agent setting, suggesting that $p_1$ generates equally strong deals regardless of communication. This implies that information from the other agents is either redundant or not utilized by $p_1$.

In summary, the game reduces to all agents stating their preferences while $p_1$ selects an acceptable deal---something it can accomplish effectively using only the predefined rules. We hypothesize that the game primarily measures how well a model infers a compromise based on the setup and adheres to the formatting. These findings contradict the claim that the benchmark evaluates cooperation, communication, and negotiation skills.

\subsection{Proposed New Metrics}

\subsubsection{Structure Leakage}

The single-agent baseline has shown that the rules of the game and agent descriptions already reveal some information to the players. This leads us to another source of information leaking: \textit{Structural Leakage}. This metric shows leakage of confidential information due to LLMs' incorrect formatting of outputs. The implementation decision to rely on LLMs to correctly format their answers is detrimental to the benchmark's effort for a fair comparison of models. As our results show, small models had a harder time strictly adhering to formatting requirements, already disadvantaging them in negotiation under this implementation. As shown in Table \ref{tab:leakage_comparison}, structural leakage is directly correlated with the model size---larger, better-performing models have no structural leakage, while the smaller models experience substantial leakage.

We initially expected the original leakage metric to always be higher than structural leakage due to its presumed broader scope. The original metric uses an LLM as a judge, which should be capable of detecting both explicit structural errors (e.g. missing plan or scratchpad tags) and more subtle forms of leakage. However, this is not always the case since small Qwen models seem to perform better on the original leakage metric. The structure leakage detects leakage of plans or scratchpads by identifying tags such as \texttt{<PLAN>}, but the original metric does not consider these cases as leakage because agents do not reveal information about confidential scores. We argue that this behavior is a form of leakage because it disrupts the negotiation dynamics.

To ensure all types of leakage are properly detected, a more robust prompt for the LLM judge is needed. In addition, we have computed the intersection of the two metrics to show how much leakage could be potentially prevented if different implementation choices were made, see Table \ref{tab:leakage_comparison}. For small models, it is apparent that most of the leakage occurs because entire answers are revealed to others due to a lack of tag adherence. We argue that the benchmark should handle the different CoT steps separately to avoid structural leakage and not hinder smaller models because of formatting problems.

\subsubsection{Inequality}

Results from Figure \ref{fig:inequality_barplot} show that the highest inequality is reached in the greedy game variants, which is expected since most of the proposed deals are not acceptable. Secondly, we showcase that in the single-agent setup, deals are slightly more unequal since $p_1$ is the only one proposing deals without others' input. Surprisingly, we show that adversarial games lead to deals with inequality similar to compromising games. However, from the single-agent baseline in Figures \ref{fig:single_vs_multi_base_gpt4} and \ref{fig:single_vs_multi_base_qwen}, we see that $p_1$ does not take much input from other agents. Therefore, it is likely that the adversarial agent does not considerably change the dynamic of the game, explaining similar inequality scores of the deals. However, Table \ref{tab:unified_agent_comparison} shows that using different prompt ablations yields different results. Therefore, each game variant can prefer a different CoT configuration for its best performance. We argue that reporting the inequality enhances the benchmark by capturing variations in individual scores, providing a more comprehensive evaluation of agent performance.

\section{Discussion}

Our study aimed to replicate the main claims of the paper "Cooperation, Competition, and Maliciousness" \citep{abdelnabi2024cooperation}, which proposes a benchmark for LLM negotiation. Furthermore, we propose several extensions to test the claims and advance the focus on fairness and confidentiality. 

Our main result shows that the benchmark fails to measure the communication and negotiation between agents, disproving the core claim of the original paper, Claim \ref{claims:num_5}, about the importance of these LLM skills for successful negotiation in the benchmark's game. We demonstrate this using an equally well-performing baseline where no communication is present. Secondly, our game analysis shows that the game is technically non-zero-sum as the original paper claims in Claim \ref{claims:num_4}. However, once agents reach the set of acceptable deals, further improvement is not possible. This indicates that the benchmark prioritizes finding sufficiently good deals rather than necessarily identifying the optimal one (generating the highest optimal score). We show that Claim \ref{claims:num_2} holds since large models outperform most small models with the exception of GPT-4o Mini, whose size is, however, not explicitly disclosed. Similarly, we support Claim \ref{claims:num_1}, as we show that open-weight models fall behind their SOTA closed-weight counterparts. Finally, we confirm that CoT steps are essential for each agent to reach their own and collective goals, Claim \ref{claims:num_3}. The results of the extensions to the original work show that:

\begin{enumerate}
    \setlength\itemsep{0em} 
     \item The game is not about negotiating the best deal, but $p_1$ finding any acceptable deal.
    \item An agent can propose a successful deal without communicating with the other parties.
    \item The proposed leakage metric addresses gaps in the original approach for detecting leakage and their implementation decisions for handling LLM outputs.
    \item The inequality metric helps to identify the presence of greedy agents in the negotiation.
    \item Finally, the main failure modes of small models are self-monologues and answer formatting.
\end{enumerate}

\subsection{Limitations and Future Work}

A key limitation of this study is the sensitivity of results to prompt variations. The CoT ablation of prompts from the original paper was tested only on GPT-3.5 and GPT-4. From our results from Table \ref{tab:unified_agent_comparison} and \ref{tab:inequality_table} we can see that different prompt configurations yield various results for different models. This is a big limitation of the benchmark as well as of our results. This dependency suggests that results may reflect an agent’s responsiveness to specific prompt formulations rather than its underlying negotiation abilities. Additionally, it raises concerns about reproducibility, as minor prompt variations could lead to significantly different outcomes among the models tested.

Secondly, due to constraints on computational resources we did not perform all of the experiments on models larger than 32B parameters. Therefore, there might be other open-weight models outperforming SOTA closed-weight models. Future improvements in LLM negotiation benchmarks should develop a more robust approach to prompt crafting. There is also room for improvement in the game design itself. We suggest designing a similar benchmark but with a non-zero-sum set of acceptable deals, a smaller Pareto front, and non-automated voting among agents. Finally, to ensure the benchmark tests the communication and negotiation skills of agents, our communication-less baseline should not be competitive. 

\bibliography{main}
\bibliographystyle{tmlr}

\FloatBarrier
\appendix
\section{Appendix}

\subsection{Example of Information in Game Description} \label{sec: example_game_description}

We provide an example of how the game's description can provide $p_1$ the information required to achieve a successful deal. In the base game designed by \citet{abdelnabi2024cooperation}, issue A (\textit{Infrastructure Mix}), related to whether the facilities will be built on land or water, the \textit{Environmental League} already knows that $p_1$, named \textit{SportCo}, will push for solution A1 (water-based, the least restrictive option for \textit{SportCo}), and oppose A3 (land-based, the most restrictive). Similarly, in issue B (\textit{Ecological Impact}), $p_1$ can infer that the \textit{Environmental League} will strongly favor B3 (environmental improvement) and likely reject B1 (permanent damage), making the negotiation unnecessary. Finally, in issue E (\textit{Compensation to other cities}), it is clear that \textit{Other cities} will push for a higher compensation, making E5 (no compensation) unacceptable. As a result, $p_1$ can immediately propose a compromise, such as E3 (paying \$300 million) instead of E1 (paying \$600 million) without further negotiation. 

Additionally, issues B and E provide explicit context before listing the available options for the agents. For example, issue B states '\textit{The "Environmental League" argues that this project might damage local dolphins and sea lion populations.}', while issue E says that '\textit{"Other cities" in the area believe their local tourism will be harmed by this project and therefore they are requesting compensations."}'. These descriptions further reinforce the predictability of each agent's preference.

Providing this information to all agents, especially $p_1$ who has the final say in the proposal, reduces the necessity of direct communication between the agents for successfully reaching an agreement. To evaluate this, we test a scenario where $p_1$ generates deal proposals without input from other parties. By relying only on the provided information, its reasoning skills, and planning capabilities, we assess whether it can independently come up with an agreeable deal that satisfies all parties.

\subsection{Impact of Model Size} \label{sec: small_models}

To ensure structured communication, the benchmark requires all the agents to format their responses according to a predefined structure provided through the CoT prompts in each round. For example, agents must enclose their proposals for the other agents within \texttt{<ANSWER>} tags (commonly referred to as the \textit{public answer}), while their private reasoning must be enclosed within \texttt{<SCRATCHPAD>} tags (the \textit{private answer}). Additionally, any deal proposal must be enclosed within \texttt{<DEAL>} tags. Properly using these tags is crucial to evaluate how well a model follows instructions and adheres to the benchmark’s structure, as failure to do so may result in an inability to assess its capabilities.

The models with less than 8B parameters tested by \citet{abdelnabi2024cooperation} were not included in their results because they did not follow the game's rules. However, the authors do not provide details of their failure modes. Our results from small model agents show that the two main reasons for failure are failing to include the correct tags in the answer and engaging in irrelevant self-monologues.

As shown by Table \ref{tab:all_models}, we could not parse deals from Qwen 2.5 1.5B in 60\% of the experiments. The reason for the failed experiments was responses such as Figure \ref{fig:small_model_failure}. The agent answered with related topics but used \texttt{**ANSWER**} instead of the \texttt{<ANSWER>} and \texttt{</ANSWER>} tags to enclose the \texttt{public answer}. Additionally, the agent does not include the \texttt{<DEAL>} tag with the deal proposal to show the others. We also encountered instances with hallucinated tags like \texttt{<SUGGESTION>}. These results align with \citet{abdelnabi2024cooperation}, as the small models did not follow the game instructions correctly. Another reason for failure was engaging in irrelevant self-monologues or responding with random words and characters that did not add value to the negotiation process. This was the cause of the failed experiments for Llama 3.1 8B and Llama 2 13B, shown in Table \ref{tab:all_models}. Consequently, this increased the required computing resources because it made the conversation history longer.

Under our definition of small models, GPT-4o Mini is in the category of small models according to the 8B parameter count estimated by \citet{abacha2024medec}. This would then disprove the claim that small models were not able to perform well on the benchmark since GPT-4o Mini was the best model we tested. The number of parameters of closed-source models is uncertain, thus, we restrict our analysis of the author's claim to open-weight models. We also analyzed how model performance changed when we used larger models for the negotiation. The results are summarized in Figure \ref{fig:size_vs_score}, where we show that larger models--i.e., models of 70B parameters, perform better than other models we used. The trend is more noticeable when we compare models controlling for the model provider and version as we do with the Qwen 2.5 model family. Additionally, each model version may incorporate different architectural or data processing advancements that impact the negotiation outcome. For example, Figure \ref{fig:size_vs_score} shows that Llama 3.3 70B has an improvement of 100\% in the 5-way metric and 11\% in the Any metric when compared to Llama 3 70B.

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.8\linewidth]{figures/small_model_example.png}
    \end{center}
    \caption{Example final answer of $p_1$ agent using Qwen 2.5 32B (left) and Qwen 2.5 1.5B (right). The answer on the left shows how the agent fails to include a deal enclosed in \texttt{<ANSWER>} and \texttt{</ANSWER>} with specific mentions of the deal that resulted from the negotiation. Such answers led to the experiments being classified as failed because we could not parse the outcome of the negotiation.}
    \label{fig:small_model_failure}
\end{figure}

\subsection{Computational requirements and energy consumption} \label{sec: energy_consumption}

We conducted our experiments using a combination of local machines and high-performance computational resources. For proprietary models like GPT-4o Mini, we accessed the model via the OpenAI API, which did not require additional computational resources on our end. For the open-source models, we used the Netherlands' national supercomputer, Snellius, with access to NVIDIA A100 and H100 GPUs.

To monitor energy consumption, we used EAR \citep{EAR}. Table \ref{tab:model_comparison} reports approximately how much energy was consumed per game for each model. The table shows that, in most cases, energy consumption is correlated to the size of the model, with smaller models consuming 20-200 kJ, and larger models using 600-800 kJ.

One notable exception to this is Llama2 13B, which had an unusually high energy consumption due to its excessive self-monologing. The model continuously produced irrelevant text until we stopped it manually. The higher energy consumption of Phi4 and Llama3.1-8B compared to Qwen 32B, although more than two and four times smaller in size, was caused by longer responses from the agents.

Overall, Qwen 32B, the model used in most of the experiments, achieves high scores in the game while using energy amounts comparable to those of smaller models. This made it an efficient choice for large-scale evaluations.

\subsection{Additional Results for Single-Agent Baseline}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/negotiation_curves_single_multi_qwen32.png}
    \caption{Comparison between the single- and multi-agent approaches in the base negotiation game for Qwen 2.5 32B. The plots show similar results when we allow communication (pink) compared to no communication (green).}
    \label{fig:single_vs_multi_base_qwen}
\end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Game 1]{
        \includegraphics[width=0.8\linewidth]{figures/negotiation_curves_single_multi_game1_qwen32.png}
    }
    \vspace{0.5cm}
    \subfloat[Game 2]{
        \includegraphics[width=\linewidth]{figures/negotiation_curves_single_multi_game2_qwen32.png}
    }
    \vspace{0.5cm}
    \subfloat[Game 3]{
        \includegraphics[width=\linewidth]{figures/negotiation_curves_single_multi_game3_qwen32.png}
    }
    \caption{Results for games 1, 2, and 3 for Qwen 2.5 32B. The plots show similar behavior for the base game in Figures \ref{fig:single_vs_multi_base_qwen}, where the single-agent setup has a similar result to the multi-agent setup.}
    \label{fig:single_vs_multi_qwen_additional_games}
\end{figure}

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{llccccccc}
            \toprule
            \textbf{Model} & \textbf{Game Type} & \textbf{Setup} & \multicolumn{2}{c}{\textbf{Final (\%) $\uparrow$}} & \textbf{Any (\%) $\uparrow$} & \textbf{Wrong (\%) $\downarrow$} & \textbf{Leak (\%)} & \textbf{Gini} \\
            \cmidrule(lr){4-5}
            & & & 5-way & 6-way & & & & \\
            \midrule
            \multirow{6}{*}{GPT-4o Mini}  
                & \multirow{2}{*}{Compromising} & Multi-Agent & 70 & 0 & 100 & 2.71 & 0.8 & 0.14 \\
                & & Single-Agent (6) & 30 & 10 & 90 & 2.85 & 1.67 & 0.19 \\
                \cmidrule(lr){2-9}
                & \multirow{2}{*}{Greedy} & Multi-Agent & 0 & 0 & 10 & 2.69 & 0 & 0.27 \\
                & & Single-Agent (6) & 0 & 0 & 0 & 0 & 0 & 0.42 \\
                \cmidrule(lr){2-9}
                & Untargeted Adv & Multi-Agent & 90 & 30 & 90 & 0 & 0 & 0.12 \\
                \cmidrule(lr){2-9}
                & Targeted Adv & Multi-Agent & 70 & 10 & 100 & 0.8 & 0 & 0.13 \\
            \midrule
            \multirow{6}{*}{Qwen 32B} 
                & \multirow{2}{*}{Compromising} & Multi-Agent & 100 & 10 & 100 & 0.3 & 0 & 0.12 \\
                & & Single-Agent (6) & 100 & 0 & 100 & 0 & 0 & 0.13 \\
                \cmidrule(lr){2-9}
                & \multirow{2}{*}{Greedy} & Multi-Agent & 30 & 20 & 50 & 2.3 & 0 & 0.19 \\
                & & Single-Agent (6) & 0 & 0 & 0 & 0 & 0 & 0.20 \\
                \cmidrule(lr){2-9}
                & Untargeted Adv & Multi-Agent & 100 & 10 & 100 & 0 & 0 & 0.12 \\
                \cmidrule(lr){2-9}
                & Targeted Adv & Multi-Agent & 90 & 10 & 100 & 0 & 0 & 0.13 \\
            \bottomrule
        \end{tabular}}
    \caption{Gini inequality values of final deals in Multi-agent and Single-Agent setup, across different game variants with Row 2 prompt configuration.}
    \label{tab:inequality_table}
\end{table}

\FloatBarrier
\subsection{Additional Results for Game Variants }

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{
        @{\extracolsep{1mm}}l | @{\extracolsep{1mm}}l | cc c c c c c} \toprule 
        \textbf{Model} & \textbf{Game} & \multicolumn{2}{c}{\textbf{Final (\%) $\uparrow$}} & \textbf{Any (\%) $\uparrow$} & \textbf{Wrong (\%) $\downarrow$} & \textbf{Leak (\%)} & \textbf{Gini} \\ \cline{3-4}
        & & \textbf{5-way} & \textbf{6-way} & & & & \\\midrule 
        \multirow{4}{*}{GPT-4o Mini} 
        & Base & 80 & 0 & 100 & 11.2 & 0 & 0.11 \\
        & Game 1 & 40 & 20 & 100 & 0.38 & 0 & 0.10 \\
        & Game 2 & 0 & 0 & 10 & 10 & 0 & 0.10 \\
        & Game 3 & 10 & 0 & 90 & 10.38 & 0 & 0.12 \\\midrule 
        \multirow{4}{*}{Qwen 2.5 32B} 
        & Base & 60 & 30 & 90 & 9.2 & 0 & 0.12 \\
        & Game 1 & 30 & 20 & 90 & 10.2 & 0 & 0.09 \\
        & Game 2 & 10 & 0 & 40 & 11.88 & 0 & 0.11 \\
        & Game 3 & 20 & 0 & 90 & 11.53 & 0 & 0.14 \\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of performance of models across the different game setups.}
    \label{tab:games123}
\end{table}

\FloatBarrier
\subsection{Detailed Results for Additional Metrics}

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{
        @{\extracolsep{1mm}}l | c c c} \toprule 
        \textbf{Model} & \textbf{Structure Leakage (\%) $\downarrow$} & \textbf{Original Leakage (\%) $\downarrow$} & \textbf{Intersection(\%)}  \\ \midrule 
        GPT-4o Mini & 0 & 0 &0\\
        Llama 3.3-70B & 0 & 0 &0\\
        Llama 3.0-70B & 0 & 4.23& 0 \\ 
        Qwen 2.5 32B & 0 & 2.31 & 0\\
        Llama 2-13B & - & - &- \\
        Phi-4 14B & 0.4 & 5.38& 0\\
        Llama 3.1-8B & 38.8 & 51.75& 25 \\
        Qwen 2.5 7B & 45.8 & 7.31& 5.77\\
        DeepSeek-R1 7B & 43.1 & 63.08& 39.62 \\
        Qwen 2.5 1.5B & 66.5 & 31.54 &22.69\\
        \bottomrule
    \end{tabular}}
    \caption{Structure and Original Leakage Comparison}
    \label{tab:leakage_comparison}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/inequality_barplot_new.png}
    \caption{Gini coefficients of inequality for the different game variants and single/multi agent setups, for Row 2 prompt configuration}
    \label{fig:inequality_barplot}
\end{figure}

\FloatBarrier
\subsection{Additions to the author's code} \label{sec: code_additions}
\begin{itemize}
    \item Originally, the authors set the model temperature to 0 and a random order of the agents for each round. However, the authors did not add a fixed seed in the code, making their exact results not reproducible. To address this, we added fixed seeds to the code setup. We then ran all the experiments 10 times with seeds ranging from 1 to 10. We also corrected the \texttt{do\_sample} parameter in the Hugging Face pipeline, changing it from \texttt{True} to \texttt{False} to set up the correct greedy decoding configuration.
    
    Although the original paper performed 20 experiments, we reduced the number of experiments due to computational constraints. This decision was also influenced by the fact that the exact reproduction of the results is not possible.
    
    \item The authors' code uses fixed \texttt{float32} precision for models loaded from Hugging Face, which can be inefficient and affect the generalization for models like Phi-4. We resolved this issue by using \texttt{auto} precision, which allows the framework to automatically select the most appropriate precision for the available hardware and model requirements. Additionally, we manually adjusted the precision of Llama models to \texttt{float16} precision. This configuration was recommended for managing memory constraints while maintaining an acceptable performance. It is important to note that \texttt{float16} is not a form of quantization but a more efficient memory format.

    \item In their prompts, authors ask models to format their answers using specific tags, such as \texttt{<DEAL>}, \texttt{<PLAN>}, \texttt{<SCRATCHPAD>}, and \texttt{<ANSWER>}. These tags are later used to break the model's answer into the Chain-of-Thought structure. Models generally comply with these requirements. However, during evaluation, the \texttt{<DEAL>} tags are not used for parsing; instead, the suggested deals are parsed directly from the models' public answers. This approach often leads to unparsable deals, negatively impacting the scores of certain models.

    We consider this a bug because models sometimes respond in a more human-like manner, describing the deals' meanings (e.g., "a loan of 200 dollars" instead of "option A2"). Additionally, the prompts are misleading, implying that the placement of the \texttt{<DEAL>} tag within the answer does not matter. This inconsistency skews the evaluation scores. We have addressed this issue and now report the corrected scores.

    \item We found a bug in the game evaluation: the number of acceptable deals was computed incorrectly, leading to incorrect levels of difficulties and game analysis in the 'Turning the game difficulty' section of the original paper.

    \item We found a bug in the evaluation notebook: the code inconsistently evaluated if the deal was acceptable. In one part of the code, the score was required to be greater than the minimal threshold, but in the rest of the code, a score equal to the minimal threshold was considered acceptable. After fixing the bug few model evaluations changed.

    \item We also fixed minor bugs, such as bracket mismatch, setting the value of the API key, faulty conditional parses, and missing the \texttt{matplotlib} dependency.

\end{itemize}
\FloatBarrier

\subsection{Cooperative prompts structures}\label{subsec:coop-prompts}

\begin{figure}[ht]
    \centering
    \begin{mdframed}[linewidth=1pt, roundcorner=5pt, innertopmargin=10pt, innerbottommargin=10pt]
    \justifying
    The following is a chronological history of up to \texttt{[WINDOW SIZE]} interactions \texttt{<HISTORY>} \texttt{[HISTORY]} \texttt{</HISTORY>}. Now it is your turn to talk.\\
    Please use a scratchpad to explain yourself and why you are agreeing with a deal or suggesting a new one. In your scratchpad, \\
    1) Based on the history and your notes, make a proposal that balances between your scores and accommodating others and that is more likely to lead to an agreement. \\
    You must follow these important negotiation guidelines in all your suggestions: \\
    - Aim for a balanced agreement considering all parties' interests. \\
    - Show flexibility and openness to accommodate others' preferences. \\
    - Express your objectives clearly and actively listen to others. \\
    - Empathize with other parties' concerns to foster rapport. \\
    - Focus on common interests to create a win-win situation. \\
    - It is very important for you that you all reach an agreement as long as your minimum score is met.\\
    Enclose the scratchpad between \texttt{<SCRATCHPAD>} and \texttt{</SCRATCHPAD>}. The scratchpad is secret and not seen by other parties. Your final answer is public and must never contain scores. Enclose your final answer after the scratchpad between \texttt{<ANSWER>} and \texttt{</ANSWER>}. Make your final answer very short and brief in 2-3 sentences and containing only your main proposals. Use options' short notations instead of long descriptions. Enclose any deals you suggest between: \texttt{<DEAL>} \texttt{</DEAL>}.
    \end{mdframed}
    \caption{Cooperative prompts structures - 1st Variation: No CoT}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{mdframed}[linewidth=1pt, roundcorner=5pt, innertopmargin=10pt, innerbottommargin=10pt]
    \justifying
    The following is a chronological history of up to \texttt{[WINDOW SIZE]} interactions \texttt{<HISTORY>} \texttt{[HISTORY]} \texttt{</HISTORY>}. The following are your previous plans from last interactions. You should follow them while also adjusting them according to new observations. \texttt{<PREV PLAN>} \texttt{[LAST PLAN]} \texttt{</PREV PLAN>}. Now it is your turn to talk. \\
    Please use a scratchpad to show intermediate calculations and explain yourself and why you are agreeing with a deal or suggesting a new one. You should map the individual options to their scores denoted by the number between parentheses. You have a calculator tool at your disposal, where you simply add scores of the options to determine the total score of a deal. In your scratchpad, \\
    1) Think about the others' preferences. \\
    2) Based on the others' preferences, history and your notes, make three different proposals that balance between your scores and accommodating others and that is more likely to lead to an agreement. \\
    3) From the three proposals, select the one final deal that is more likely to lead to an agreement. \\
    You must follow these important negotiation guidelines in all your suggestions: \\
    - Aim for a balanced agreement considering all parties' interests. \\
    - Show flexibility and openness to accommodate others' preferences. \\
    - Express your objectives clearly and actively listen to others. \\
    - Empathize with other parties' concerns to foster rapport. \\
    - Focus on common interests to create a win-win situation. \\
    - It is very important for you that you all reach an agreement as long as your minimum score is met.\\
    Enclose the scratchpad between \texttt{<SCRATCHPAD>} and \texttt{</SCRATCHPAD>}. The scratchpad is secret and not seen by other parties. Your final answer is public and must never contain scores. Enclose your final answer after the scratchpad between \texttt{<ANSWER>} and \texttt{</ANSWER>}. Make your final answer very short and brief in 2-3 sentences and containing only your main proposals. Use options' short notations instead of long descriptions. Enclose any deals you suggest between: \texttt{<DEAL>} \texttt{</DEAL>}. \\
    After the final answer, building on your current move and analysis, briefly write down short notes for yourself of what exact options you can explore the next time you speak. Enclose the notes between \texttt{<PLAN>} and \texttt{</PLAN>}.
    \end{mdframed}
    \caption{Cooperative prompts structures - 2nd Variation: CoT - Complete}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{mdframed}[linewidth=1pt, roundcorner=5pt, innertopmargin=10pt, innerbottommargin=10pt]
    \justifying
    The following is a chronological history of up to \texttt{[WINDOW SIZE]} interactions \texttt{<HISTORY>} \texttt{[HISTORY]} \texttt{</HISTORY>}. The following are your previous plans from last interactions. You should follow them while also adjusting them according to new observations. \texttt{<PREV PLAN>} \texttt{[LAST PLAN]} \texttt{</PREV PLAN>}. Now it is your turn to talk. \\
    Please use a scratchpad to show intermediate calculations and explain yourself and why you are agreeing with a deal or suggesting a new one. You should map the individual options to their scores denoted by the number between parentheses. You have a calculator tool at your disposal, where you simply add scores of the options to determine the total score of a deal. In your scratchpad, \\
    1) Think about the others' preferences. \\
    2) Based on the others' preferences, history and your notes, make a proposal that balances between your scores and accommodating others and that is more likely to lead to an agreement. \\
    You must follow these important negotiation guidelines in all your suggestions: \\
    - Aim for a balanced agreement considering all parties' interests. \\
    - Show flexibility and openness to accommodate others' preferences. \\
    - Express your objectives clearly and actively listen to others. \\
    - Empathize with other parties' concerns to foster rapport. \\
    - Focus on common interests to create a win-win situation. \\
    - It is very important for you that you all reach an agreement as long as your minimum score is met.\\
    Enclose the scratchpad between \texttt{<SCRATCHPAD>} and \texttt{</SCRATCHPAD>}. The scratchpad is secret and not seen by other parties. Your final answer is public and must never contain scores. Enclose your final answer after the scratchpad between \texttt{<ANSWER>} and \texttt{</ANSWER>}. Make your final answer very short and brief in 2-3 sentences and containing only your main proposals. Use options' short notations instead of long descriptions. Enclose any deals you suggest between: \texttt{<DEAL>} \texttt{</DEAL>}. \\
    After the final answer, building on your current move and analysis, briefly write down short notes for yourself of what exact options you can explore the next time you speak. Enclose the notes between \texttt{<PLAN>} and \texttt{</PLAN>}.
    \end{mdframed}
    \caption{Cooperative prompts structures - 3rd Variation: CoT - No Candidate Generation}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{mdframed}[linewidth=1pt, roundcorner=5pt, innertopmargin=10pt, innerbottommargin=10pt]
    \justifying
    The following is a chronological history of up to \texttt{[WINDOW SIZE]} interactions \texttt{<HISTORY>} \texttt{[HISTORY]} \texttt{</HISTORY>}. Now it is your turn to talk. \\
    Please use a scratchpad to show intermediate calculations and explain yourself and why you are agreeing with a deal or suggesting a new one. You should map the individual options to their scores denoted by the number between parentheses. You have a calculator tool at your disposal, where you simply add scores of the options to determine the total score of a deal. In your scratchpad, \\
    1) Think about the others' preferences. \\
    2) Based on the others' preferences, history and your notes, make a proposal that balances between your scores and accommodating others and that is more likely to lead to an agreement. \\
    You must follow these important negotiation guidelines in all your suggestions: \\
    - Aim for a balanced agreement considering all parties' interests. \\
    - Show flexibility and openness to accommodate others' preferences. \\
    - Express your objectives clearly and actively listen to others. \\
    - Empathize with other parties' concerns to foster rapport. \\
    - Focus on common interests to create a win-win situation. \\
    - It is very important for you that you all reach an agreement as long as your minimum score is met.\\
    Enclose the scratchpad between \texttt{<SCRATCHPAD>} and \texttt{</SCRATCHPAD>}. The scratchpad is secret and not seen by other parties. Your final answer is public and must never contain scores. Enclose your final answer after the scratchpad between \texttt{<ANSWER>} and \texttt{</ANSWER>}. Make your final answer very short and brief in 2-3 sentences and containing only your main proposals. Use options' short notations instead of long descriptions. Enclose any deals you suggest between: \texttt{<DEAL>} \texttt{</DEAL>}.
    \end{mdframed}
    \caption{Cooperative prompts structures - 4th Variation: CoT - No Candidate Generation nor Planning}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{mdframed}[linewidth=1pt, roundcorner=5pt, innertopmargin=10pt, innerbottommargin=10pt]
    \justifying
    The following is a chronological history of up to \texttt{[WINDOW SIZE]} interactions \texttt{<HISTORY>} \texttt{[HISTORY]} \texttt{</HISTORY>}. The following are your previous plans from last interactions. You should follow them while also adjusting them according to new observations. \texttt{<PREV PLAN>} \texttt{[LAST PLAN]} \texttt{</PREV PLAN>}. Now it is your turn to talk. \\
    Please use a scratchpad to explain yourself and why you are agreeing with a deal or suggesting a new one. In your scratchpad, \\
    1) Think about the others' preferences. \\
    2) Based on the others' preferences, history and your notes, make a proposal that balances between your scores and accommodating others and that is more likely to lead to an agreement. \\
    You must follow these important negotiation guidelines in all your suggestions: \\
    - Aim for a balanced agreement considering all parties' interests. \\
    - Show flexibility and openness to accommodate others' preferences. \\
    - Express your objectives clearly and actively listen to others. \\
    - Empathize with other parties' concerns to foster rapport. \\
    - Focus on common interests to create a win-win situation. \\
    - It is very important for you that you all reach an agreement as long as your minimum score is met.\\
    Enclose the scratchpad between \texttt{<SCRATCHPAD>} and \texttt{</SCRATCHPAD>}. The scratchpad is secret and not seen by other parties. Your final answer is public and must never contain scores. Enclose your final answer after the scratchpad between \texttt{<ANSWER>} and \texttt{</ANSWER>}. Make your final answer very short and brief in 2-3 sentences and containing only your main proposals. Use options' short notations instead of long descriptions. Enclose any deals you suggest between: \texttt{<DEAL>} \texttt{</DEAL>}. \\
    After the final answer, building on your current move and analysis, briefly write down short notes for yourself of what exact options you can explore the next time you speak. Enclose the notes between \texttt{<PLAN>} and \texttt{</PLAN>}.
    \end{mdframed}
    \caption{Cooperative prompts structures - 5th Variation: CoT - No Previous Deals Calculations nor Candidate Generation}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{mdframed}[linewidth=1pt, roundcorner=5pt, innertopmargin=10pt, innerbottommargin=10pt]
    \justifying
    The following is a chronological history of up to \texttt{[WINDOW SIZE]} interactions \texttt{<HISTORY>} \texttt{[HISTORY]} \texttt{</HISTORY>}. The following are your previous plans from last interactions. You should follow them while also adjusting them according to new observations. \texttt{<PREV PLAN>} \texttt{[LAST PLAN]} \texttt{</PREV PLAN>}. Now it is your turn to talk. \\
    Please use a scratchpad to explain yourself and why you are agreeing with a deal or suggesting a new one. In your scratchpad, \\
    1) Based on the history and your notes, make a proposal that balances between your scores and accommodating others and that is more likely to lead to an agreement. \\
    You must follow these important negotiation guidelines in all your suggestions: \\
    - Aim for a balanced agreement considering all parties' interests. \\
    - Show flexibility and openness to accommodate others' preferences. \\
    - Express your objectives clearly and actively listen to others. \\
    - Empathize with other parties' concerns to foster rapport. \\
    - Focus on common interests to create a win-win situation. \\
    - It is very important for you that you all reach an agreement as long as your minimum score is met.\\
    Enclose the scratchpad between \texttt{<SCRATCHPAD>} and \texttt{</SCRATCHPAD>}. The scratchpad is secret and not seen by other parties. Your final answer is public and must never contain scores. Enclose your final answer after the scratchpad between \texttt{<ANSWER>} and \texttt{</ANSWER>}. Make your final answer very short and brief in 2-3 sentences and containing only your main proposals. Use options' short notations instead of long descriptions. Enclose any deals you suggest between: \texttt{<DEAL>} \texttt{</DEAL>}. \\
    After the final answer, building on your current move and analysis, briefly write down short notes for yourself of what exact options you can explore the next time you speak. Enclose the notes between \texttt{<PLAN>} and \texttt{</PLAN>}.
    \end{mdframed}
    \caption{Cooperative prompts structures - 6th Variation: CoT - No Previous Deals Calculations, Other's Preferences or Candidate Generation}
\end{figure}

\end{document}
