\section{Introduction}
Lossless image compression aims to reduce image size as much as possible without introducing any distortion, making it essential for high-quality data storage and transmission. Furthermore, the techniques used in lossless compression often play a key role in lossy compression methods. 
Over the past few decades, numerous effective lossless image codecs have been developed. Among these, traditional codecs such as PNG~\cite{boutell1997png}, WebP~\cite{webp_tech_report}, FLIF~\cite{sneyers2016flif}, and JPEG-XL~\cite{alakuijala2019jpeg} have achieved strong compression performance through hand-crafted coding algorithms.
For example, JPEG-XL employs invertible transforms and a sophisticated context model, including tree structure and pre-context predictor selection, to compress images effectively. In recent years, learning-based lossless image codecs ~\cite{mentzer2019practical,mentzer2020learning,zhang2021ivpf,zhang2021iflow,bai2024deep} become increasingly popular. L3C~\cite{mentzer2019practical}, for instance, utilizes a hierarchical probability prediction framework and introduces auxiliary latent representations to model the probability distribution of image data. These state-of-the-art (SOTA) methods typically rely on empirical knowledge in image compression and employ meticulously designed models to achieve better compression performance.

Recently, Large Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing tasks, and their applications have extended to vision tasks, driving substantial progress in areas such as image generation~\cite{ge2024seed, Pang_2024_ICLR_frozen} and image restoration~\cite{zheng2024lm4lv}. The primary objective of LLMs is to predict the probability distribution of the next token in a sequence. Consequently, more advanced LLM results in more precise modeling of data distribution. Similarly, entropy coding in lossless compression seeks to accurately model data distribution to minimize the coding bitrate. This parallel suggests that LLMs could potentially serve as powerful tools for entropy coding. 

Recent work by Del{\'e}tang et al.~\cite{deletang2023language} supports this perspective, demonstrating that LLMs not only achieve impressive results in text compression but also demonstrate strong potential for lossless image compression. This highlights the advantages of leveraging LLMs in the compression domain.
However, pretrained LLMs primarily encapsulate textual prior knowledge, whereas image compression relies more on visual information for optimal performance. Therefore, it is crucial to bridge the gap between the textual nature of LLMs and  visual data compression tasks.
Unfortunately, the existing approach~\cite{deletang2023language} directly treats the pixel values of input images as indexes for LLMs, overlooking the inherent spatial relationships within the images.
Consequently, the compression efficiency of this method is suboptimal. For instance, the model proposed by Del{\'e}tang et al.~\cite{deletang2023language} with 7B parameters performs only slightly better than PNG~\cite{boutell1997png}. Thus, how to effectively unlock the prior knowledge of LLMs and activate their potential for lossless image compression remains a critical issue that deserves in-depth exploration.

In this work, we propose a novel framework for lossless image compression that leverages the LLM with visual prompts.
Specifically, the image is initially compressed using a lossy codec, and this lossy reconstruction is then employed as visual prompts for the LLM. Subsequently, the LLM is used to predict the probability distribution of the residual between the lossy reconstruction and the original image. 
Finally, the probability distributions of the residual pixels are modeled using the Gaussian Mixture Model (GMM), where the parameters are predicted from the output features generated by the LLM. Furthermore, by finetuning the pretrained LLM with Low-Rank Adaption (LoRA)~\cite{hu2021lora}, we further enhance our compression performance. Our approach has been evaluated on several benchmark datasets, including Kodak, CLIC, and DIV2K. The results demonstrate that our method achieves SOTA performance, comparable to other well-designed codecs. Our research provides novel insights into lossless image compression and highlights the potential of LLMs for this task.

Our main contributions can be summarized as follows:

\begin{itemize}
\item By employing the lossy reconstruction as visual prompts for the LLM, we guide the LLM for more efficient lossless data compression.
\item The extensive experimental results demonstrate the SOTA performance of our approach on benchmark datasets. Moreover, our approach can be readily applied to images from other domains, such as screen content images and medical images.
\end{itemize}