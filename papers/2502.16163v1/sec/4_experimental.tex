\begin{table*}[t]
\tabcolsep=0.4cm
\begin{center}
\begin{tabular}{llcccc}
\toprule[2pt]
Category &Codec  &DIV2K &CLIC.pro &CLIC.mobile &Kodak\\
\midrule[1pt]
\multirow{8}{*}{Traditional} &PNG~\cite{boutell1997png}  &4.23 &3.93 &3.93 &4.35 \\
&JPEG-LS~\cite{weinberger2000loco}  &2.99 &2.82 &2.53 &3.16 \\
&CALIC~\cite{wu1997context}  &3.07 &2.87 &2.59 &3.18 \\
&JPEG2000~\cite{skodras2001jpeg}  &3.12 &2.93 &2.71 &3.19 \\
&WebP~\cite{webp_tech_report}  &3.11 &2.90 &2.73 &3.18 \\
&BPG~\cite{bpg}  &3.28 &3.08 &2.84 &3.38 \\
&FLIF~\cite{sneyers2016flif}  &2.91 &2.72 &2.48 &2.90 \\
&JPEG-XL~\cite{alakuijala2019jpeg}  &2.79 &2.63 &2.36 &2.87 \\
\midrule[1pt]
\multirow{5}{*}{Learning-based} &L3C~\cite{mentzer2019practical}  &3.09 &2.94 &2.64 &3.26 \\
&RC~\cite{mentzer2020learning}  &3.08 &2.93 &2.54 &- \\
&iVPF~\cite{zhang2021ivpf}  &2.68 &2.54 &2.39 &- \\
&iFlow~\cite{zhang2021iflow}  &2.57 &2.44 &2.26 &- \\
&DLPR~\cite{bai2024deep}  &2.55 &2.38 &2.16 &2.86 \\
\midrule[1pt]
\multirow{2}{*}{LLM-based} &Del{\'e}tang et al.~\cite{deletang2023language}  &4.25 &3.99 &4.12 &4.84 \\
&\bf Ours  &\bf2.29 &\bf2.25 &\bf2.07 &\bf2.83 \\
\bottomrule[2pt]
\end{tabular}
\caption{Lossless image compression performance (bpsp) of our proposed method compared to other lossless image codecs on DIV2K, CLIC.pro, CLIC.mobile and Kodak datasets.}
\label{tab:main-results}
\end{center}
\end{table*}

\section{Experimental Results}
\subsection{Experimental Settings}

\textbf{Training Details.}
We train the entire framework in two stages. In the first stage, we freeze the LLM and optimize all other modules. This stage is trained on the ImageNet2012 dataset~\cite{ILSVRC15} using the AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $1 \times 10^{-4}$. In the second stage, we apply the LoRA~\cite{hu2021lora} to finetune the LLM. For this, we utilize the DIV2K training dataset~\cite{Ignatov_2018_ECCV_Workshops} to finetune the entire framework.

In this paper, we use LLaMA3-8B~\cite{dubey2024llama} as the default LLM, unless otherwise specified. The original images are lossy compressed using BPG~\cite{bpg} with the compression parameter of $Q = 28$. The lossy reconstructions and the original images are then randomly cropped into patch pairs of size $16 \times 16$, which serve as inputs to the model. To model the distribution, we employ a Gaussian Mixture Model with $K=5$.

Our method is implemented using the PyTorch framework~\cite{paszke2017automatic} and requires 3 days to train the entire model on 4 NVIDIA A100 GPUs. 
Additionally, the arithmetic coding is implemented using the yaecl tool library~\cite{xu2022bit}. 

\textbf{Datasets.}
To evaluate the performance of the model, we select four different datasets. 1) \textit{DIV2K}~\cite{Ignatov_2018_ECCV_Workshops}: This dataset contains 100 high-resolution color images.
2) \textit{CLIC.mobile}~\cite{CLIC2020}:  The CLIC mobile validation dataset consists of 61 color images taken with mobile phones, with most images in 2K resolution.
3) \textit{CLIC.pro}~\cite{CLIC2020}: The CLIC professional validation dataset includes 41 color images captured by professional photographers, with the majority of images in 2K resolution.
4) \textit{Kodak}~\cite{kodak1993kodak}: This dataset contains 24 uncompressed 768$\times$512 color images and is widely used as a benchmark for lossy image compression.


\textbf{Baseline Codecs.}
To validate the effectiveness of our method, we compare it against eight traditional lossless image encoders: PNG~\cite{boutell1997png}, JPEG-LS~\cite{weinberger2000loco}, CALIC~\cite{wu1997context}, JPEG2000~\cite{skodras2001jpeg}, WebP~\cite{webp_tech_report}, BPG~\cite{bpg}, FLIF~\cite{sneyers2016flif}, and JPEG-XL~\cite{alakuijala2019jpeg}. In addition, we include five representative learning-based lossless image compression methods for comparison: L3C~\cite{mentzer2019practical}, RC~\cite{mentzer2020learning}, iVPF~\cite{zhang2021ivpf}, iFlow~\cite{zhang2021iflow}, and DLPR~\cite{bai2024deep}. We also reproduce the LLM-based lossless image codec~\cite{deletang2023language} in our experiments. Since the LLM 
 used in their approach is not open-source, we substitute it with LLaMA3-8B as the default model while following their other settings. 

\textbf{Metric.} We use bits per subpixel (bpsp) as the metric to evaluate the compression ratios. The bpsp is calculated by dividing the total bits in the compressed file by the number of subpixels, where each RGB pixel consists of three subpixels.

\subsection{Main Results}

As shown in \cref{tab:main-results}, our proposed method achieves state-of-the-art lossless compression performance across all test datasets. On the high-resolution DIV2K and CLIC datasets, our approach further reduces file size by 12.3\%-17.9\% compared to the best traditional lossless compression scheme JPEG-XL. When compared to SOTA learning-based methods such as DLPR~\cite{bai2024deep} and iFlow~\cite{zhang2021iflow}, our approach also demonstrates superior results. For example, the bpsp of DLPR is 2.55, while our method achieves 2.29, reflecting a 10.2\% improvement.
Additionally, in comparison with a LLM-based codec~\cite{deletang2023language}, our method reduces the bpsp from 4.84 to 2.83 on the Kodak dataset. 
These results clearly demonstrate that LLMs can be effectively applied to lossless image compression, surpassing even the latest SOTA compression methods. Moreover, these results underscore how our architecture, enhanced with visual prompts, significantly improves the performance of LLM-based codecs in the lossless image compression task.

\subsection{Ablation Studies}\label{subsec:ablation}
To further analyze our architecture, we conduct ablation studies as shown in \cref{tab:deepmind-lora-results,tab:model_size,tab:lossy_codecs}.

\textbf{Visual Prompts.} 
We begin by establishing a simple baseline where the LLM is fixed, without the use of visual prompts. 
Experimental results show that introducing visual prompts, i.e. the information from lossy reconstruction, reduces the bpsp from 4.84 to 3.19, underscoring the effectiveness of visual prompts in enhancing the LLM-based compression framework.

\begin{table*}[t]
\tabcolsep=0.4cm
\begin{center}
\begin{tabular}{lllll}
\toprule[2pt]
Codec  &DIV2K &CLIC.pro &CLIC.mobile &Kodak\\
\midrule[1pt]
Del{\'e}tang et al.  &4.25 &3.99 &4.12 &4.84\\
Ours  &2.81(-33.9\%) &2.71(-32.1\%) &2.50(-39.3\%) &3.19(-34.1\%) \\
\midrule[1pt]
Del{\'e}tang et al. (after LoRA)  &2.54 &2.50 &2.34 &3.00\\
Ours (after LoRA)  &\bf2.29(-9.8\%) &\bf2.25(-10.0\%) &\bf2.07(-11.5\%) &\bf2.83(-5.7\%) \\
\bottomrule[2pt]
\end{tabular}
\caption{Performance comparision for Del{\'e}tang et al. (i.e., without visual prompts) and Ours (i.e., with visual prompts).}
\label{tab:deepmind-lora-results}
\end{center}
\end{table*}

To explore the role of visual prompts in conjunction with LoRA, we conduct the finetuning experiments based on the method of Del{\'e}tang et al.~\cite{deletang2023language}, and the experimental results are presented in \cref{tab:deepmind-lora-results}. It is evident that, after applying the LoRA finetuning, our visual prompts continue to achieve a performance gain of 9.8\% to 12.7\% on high-resolution DIV2K and CLIC datasets.

\textbf{Patch Size.} 
In our main experiments, we use a patch size of $16\times16$ and then extend our evaluation to $24\times24$. Increasing the patch size results in a slight performance improvement, with the bpsp decreasing from 3.19 to 3.16. 
This enhancement can be attributed to the larger patch sizes, which allow for longer contexts that provide more information for the model to process. This additional information enhances the model's ability to capture intricate details and relationships within the image data, ultimately facilitating better compression.

\textbf{LLM Size.}
We conduct experiments utilizing three LLaMA models with varying parameters and test them on the Kodak dataset to evaluate the impact of LLM size on compression performance. As shown in \cref{tab:model_size}, the results indicate that compression performance decreases as the model size decreases; however, the degradation in performance is not significant, as smaller models can still achieve acceptable performance. 

\begin{table}[t]
\tabcolsep=0.3cm
    \centering
    \begin{tabular}{lcc}
    \toprule[2pt]
    Method & bpsp & Loss\\ 
    \midrule[1pt]
    Ours (1B)    &3.24 &1.6\%\\
    Ours (3B)    &3.21 &0.6\%\\
    Ours (8B)    &3.19 &-\\
    \bottomrule[2pt]
    \end{tabular}
    \caption{Results comparison by LLM size on Kodak dataset.}
    \label{tab:model_size}
\end{table}

\textbf{Lossy Image Codec.}
In this experiment, we evaluate the impact of the quantization parameter (QP) in the BPG codec on the performance of our proposed framework. We train our framework using different QP values, with the corresponding results presented in \cref{tab:lossy_codecs}. While a lower QP increases the bpsp for lossy compression, it decreases the bpsp for lossless residual compression. Experiments show the final bpsp results are similar in range between [22, 34] and the QP value has a limited influence. Based on these findings, we select BPG with a QP value of 28 as the default lossy codec in our experiment. 

Additionally, our framework accommodates other lossy codecs, such as JPEG, which also demonstrates similar performance trends as presented in \cref{tab:lossy_codecs}. When quality settings are appropriate, both BPG and JPEG contribute positively to our architecture. However, extreme QP settings can lead to performance degradation; setting the QP too low increases the bitrate for lossy coding, while setting it too high results in excessive residuals needing lossless compression.

\textbf{Orders of GMM.}
GMM is commonly used in image compression\cite{Cheng_2020_CVPR_DGML, bai2024deep}. Residual image samples often exhibit complex distributions due to their high-frequency nature, making them challenging to model. Compared to the Gaussian Single Model (GSM), where $K=1$, GMM incorporates a minimal increase in parameters while providing significantly improved modeling capabilities. Our ablation study on the orders K in GMM indicates that $K=5$ significantly outperforms $K=1$, leading to a reduction in bpsp from 3.29 to 3.19. This highlights its superior ability to capture complex distributions. 

\begin{table}[t]
    \centering
    \begin{tabular}{lccc}
        \toprule[2pt]
        Lossy Codec &Lossy &Residual &Total\\
        \midrule[1pt]
        BPG (QP=14)  &0.95 &2.43 &3.38\\
        BPG (QP=22)  &0.48 &2.72 &3.20\\
        BPG (QP=28)  &0.27 &2.92 &3.19\\
        BPG (QP=34)  &0.13 &3.13 &3.26\\
        BPG (QP=42)  &0.04 &3.38 &3.42\\
        \midrule[1pt]
        JPEG (quality=30)         & 0.20           & 3.30              & 3.50           \\ 
JPEG (quality=50)         & 0.29           & 2.99              & 3.28           \\ 
JPEG (quality=70)         & 0.40           & 2.96              & 3.36           \\ 
        \bottomrule[2pt]
    \end{tabular}
    \caption{Ablation experiments for lossy image codecs, test results on the Kodak dataset, using bpsp as a metric.}
    \label{tab:lossy_codecs}
\end{table}

\subsection{Computational Complexity}\label{sec:complexity}

Although our LLM-based codec demonstrates superior performance, surpassing classical and other learned-based codecs through its advanced intelligence, its decoding time, as shown in \cref{tab:codec_comparison}, is considerably slower than other baselines. This is primarily due to the inherent limitations of autoregressive models and the large number of parameters in LLMs.

\begin{table*}[t]
    \centering
    \tabcolsep=0.5cm
    \begin{tabular}{lccc}
        \toprule[2pt]
        Codec & Params & Enc/Dec kMACs/pixel & Enc/Dec Times (second/image)\\ 
        \midrule[1pt]
        L3C~\cite{mentzer2019practical} & 5M & 252.59/431.31 &8.17/7.89\\
        DLPR~\cite{bai2024deep} & 37M & $1.8\times10^4/1.3\times10^4$ & 1.26/1.80\\
        \midrule[1pt]
        Del{\'e}tang et al.~\cite{deletang2023language} & 8B & $2.1\times10^7$ & 10.44/288.0 \\ 
        Ours (1B) & 1B+2M & $5.9\times10^6$ & 3.84/141.6 \\ 
        Ours (3B) & 3B+3M & $1.7\times10^7$ & 10.08/338.4 \\ 
        Ours (8B) & 8B+4M & $4.2\times10^7$ & 21.12/495.6 \\ 
        \bottomrule[2pt]
    \end{tabular}
    \caption{Comparison of runtimes and kMACs on Kodak dataset.}
    \label{tab:codec_comparison}
\end{table*}

\subsection{Lossless Compression for Images Across Diverse Domains}
In this section, we apply our proposed pipeline to images from various domains, including screen content images (SCIs) and medical images. Traditional codecs often require specialized tools, such as the intra block copy technique for SCIs, to improve compression performance, which introduces additional design complexity~\cite{7265040}. In contrast, learning-based codecs can adapt to these diverse image types through training on sufficiently large datasets. Our proposed pipeline further advances by leveraging the extensive prior information embedded in the LLM, resulting in enhanced compression performance across these diverse image types.

\textbf{Screen Content Image Compression.}
Screen Content Images (SCIs) typically contain text and graphics, with computer-generated elements constituting over 90\% of SCIs. Compared to natural images, SCIs are characterized by sharp edges, a limited color palette, high contrast, and markedly different regional complexity, often exhibiting little to no noise~\cite{nguyen2021overview}.

In this experiment, we utilize HM-SCC~\cite{7265040} as the default lossy codec (QP=28). We evaluate performance on the SCID dataset~\cite{8266580}, with the results presented in \cref{scc-results}. The results indicate that our method, finetuned on the natural image dataset (i.e., DIV2K), demonstrates competitive generalization ability and can be effectively applied to the SCI domain, achieving a 5.1\% improvement over DLPR. Furthermore, finetuning on the SCI dataset DSCIC~\cite{10577165} significantly enhances the model's performance within the SCI domain, reaching a SOTA level with a bpsp of 1.11, representing a substantial improvement of 10.5\% compared to JPEG-XL.

\begin{table}[t]
    \centering
    \tabcolsep=0.3cm
        \begin{tabular}{lcc}
            \toprule[2pt]
            Codec &bpsp &Gain\ \\
            \midrule[1pt]
            PNG~\cite{boutell1997png} &1.79 &+14.0\%\\
            BPG~\cite{bpg}  &1.57 &-\\
            WebP~\cite{webp_tech_report} &1.28 &-18.5\%\\
            JPEG-XL~\cite{alakuijala2019jpeg} &1.24 &-21.0\%\\
            HM-SCC~\cite{7265040} &1.18 &-24.8\%\\
            L3C~\cite{mentzer2019practical} &2.67 &+70.1\%\\
            DLPR~\cite{bai2024deep} &1.58 &+0.6\%\\
            \midrule[1pt]
            Ours(DIV2K) &1.50 &-4.5\%\\
            Ours(SCI) &\bf1.11 &\bf-29.3\%\\
            \bottomrule[2pt]
        \end{tabular}
        \caption{Applying our model to screen content image compression, test results on the SCID dataset, using bpsp as a metric.}
        \label{scc-results}
\end{table}


\textbf{Medical Image Compression.}
Most medical images are 3D, producing large data volumes that challenge storage and transmission. Effective compression is crucial. Although lossy compression offers higher ratios, it risks distorting images and compromising diagnostic accuracy, potentially leading to medical errors. Thus, lossless compression is preferred for maintaining data integrity and meeting strict standards.

Traditional lossless image compression methods, such as PNG~\cite{boutell1997png} and JPEG-XL~\cite{alakuijala2019jpeg}, individually encode each slice of 3D medical images. In addition, video coding techniques like HEVC~\cite{sullivan2012overview} and VVC~\cite{Bross_2021_TCSVT_VVC}, along with traditional medical image compression method JP3D~\cite{bruylants2009jp3d}, treat 3D medical images as video sequences or volumetric data. The latest learned lossless compression methods, including L3C~\cite{mentzer2019practical}, ICEC~\cite{chen2022exploiting}, and aiWave~\cite{xue2022aiwave}, are also used as baselines.

Given that medical images are three-dimensional, we split the input medical images into 3-channel slices for processing. In this experiment, we use JPEG-XL as our lossy codec, empirically setting the corresponding quality to 68. Following prior work~\cite{chen2022exploiting}, our framework is finetuned on the MRNet training dataset~\cite{bien2018deep} and tested on the MRNet validation dataset. The test results are presented in \cref{tab:medical-results}. 

Our model demonstrates superior compression performance for lossless medical image compression. For the Axial subset, the average bpsp of the proposed method is 4.46, compared to 4.72 for JPEG-XL. 
Moreover, when compared to the learning-based lossless codec L3C~\cite{mentzer2019practical}, which is also finetuned on medical images in this experiment, our approach shows significantly better compression performance. On the Coronal subset, our method further saves 6.1\% bit consumption compared with aiWave~\cite{xue2022aiwave}.
This improvement can be attributed to our method's utilization of the extensive prior information embedded in LLMs, enhancing overall performance.



\begin{table}[t]
    \centering
    \tabcolsep=0.3cm
        \begin{tabular}{lccc}
            \toprule[2pt]
            Codec &Axial &Coronal &Sagittal\\
            \midrule[1pt]
            PNG~\cite{boutell1997png}  &5.36 &4.58 &5.58\\
            JP3D~\cite{bruylants2009jp3d} &4.98 &4.15 &5.28\\
            JPEG-XL~\cite{alakuijala2019jpeg} &4.72 &3.89 &5.09\\
            HEVC~\cite{sullivan2012overview} &5.19 &4.47 &5.58\\
            VVC~\cite{Bross_2021_TCSVT_VVC} &4.96 &4.10 &5.32\\
            L3C~\cite{mentzer2019practical} &5.16 &4.45 &5.52\\
            ICEC~\cite{chen2022exploiting} &4.64 &3.84 &4.97\\
            aiWave~\cite{xue2022aiwave} &4.55 &3.80 &4.83\\
            \midrule[1pt]
            Ours &\bf4.46 &\bf3.57 &\bf4.83 \\
            \bottomrule[2pt]
        \end{tabular}
        \caption{Applying our model to medical image compression, test results on the MRNet dataset, using bpsp as a metric.}
        \label{tab:medical-results}
\end{table}