\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/model_219_1.pdf}
    \caption{Our distribution estimation framework based on LLM. Visual embeddings, including the global embeddings $\mathbf{z}_{g}$ and local embeddings $\mathbf{z}_{l}^n$, enhance the inference. The output feature of LLM $f^n$ are projected onto a Gaussian Mixture Model (GMM) to estimate the residual's probability distribution.}
    \label{fig:model}
\end{figure*}
\section{Methodology}
The overall framework of our proposed lossless image compression pipeline is illustrated in \cref{fig:pipeline}. The original image $\mathbf x$ is first compressed using a lossy codec, producing a lossy reconstructed image $\mathbf{x}_l$. 
Then we divide $\mathbf{x}_l$ and the residual image $\mathbf{r}$ into non-overlapping patches of size $p\times p$, denoted as $\{\mathbf{x}_l^1, \ldots, \mathbf{x}_l^{N}\}$ and $\{\mathbf{r}^1, \ldots, \mathbf{r}^{N}\}$, where $N$ represents the total number of patches. 
During the encoding process, each patch is processed independently.
We predict the probability distribution of each pixel within a residual patch in an autoregressive manner and encode these pixels using arithmetic coding.
For instance, when encoding patch $\mathbf{r}^n$ (where $n=1,2,\cdots,N$), the entire lossy reconstruction $\mathbf{x}_l$ and its corresponding lossy reconstructed patch $\mathbf{x}_l^n$ are used as visual prompts to extract visual embeddings for the LLM. The pixels in residual patch $\mathbf{r}^n$ are then autoregressively fed into the LLM to estimate the probability distribution.
Given the estimated distributions, we losslessly encode $\mathbf{r}^n$ into a bitstream using arithmetic encoding. The final bitstream comprises the lossy reconstruction $\mathbf{x}_l$ and its corresponding residual image $\mathbf{r}$. 

During the decoding procedure, the lossy reconstructed image $\mathbf{x}_l$ is first decoded. Both $\mathbf{x}_l$ and its patch $\mathbf{x}_l^n$ are then utilized as visual prompts to autoregressively obtain the distribution for each pixel in the residual patch $\mathbf{r}^n$. 
Finally, the full residual image is decoded, and the original image is reconstructed by combining the lossy reconstruction $\mathbf{x}_l$ with the residual image $\mathbf{r}$. 
It is important to note that for the lossy codecs, we can either choose a traditional compression method or employ an end-to-end learned compression method. Here we use BPG~\cite{bpg} as the default lossy codec.

\subsection{Input Embeddings} 


In existing LLMs, the tokenizer converts text into corresponding indexes, which are then used to obtain embeddings through an embedding layer. For image compression task, Del{\'e}tang et al.~\cite{deletang2023language} proposes using pixel values directly as indexes and reusing the embeddings originally trained for text dataset. However, this approach may not fully capture the relationships within the image domain, and the mismatch between textual embeddings and image pixel values may lead to poor performance. 
Moreover, the prompt technique, which is crucial for large language models, has been overlooked in~\cite{deletang2023language}.

To address the aforementioned challenges, we introduce visual prompts and visual embeddings as illustrated in \cref{fig:model}. For compressing a residual patch, the visual prompts consist of two components: global lossy image and local lossy patch. To extract global embeddings $\mathbf{z}_{g}\in\mathbb{R}^{k_{g}\times d}$, we design a simple Global Embedding Module that utilizes several convolutional layers to capture pixel relationships from $\mathbf{x}_l$. For the local embeddings $\mathbf{z}_l^n \in \mathbb{R}^{p^2 \times d}$ of patch $n$, we directly use pixel values as indexes, with the embedding layer jointly optimized with the entire framework. These global and local embeddings together form visual embeddings, supplying the LLM with both global and local visual information about the image. For compressing the residual patch $\mathbf{r}^n$, the learnable Residual Embedding Layer extracts residual embeddings $\mathbf{z}^n_r\in\mathbb{R}^{p^2\times d}$. These elements allow us to integrate image information with the LLM's prior knowledge, bridging the gap between image and text tasks, ultimately enhancing compression efficiency.

\subsection{Distribution Estimation Using LLM}

In our proposed framework, we utilize the LLM as a conditional probability estimator, leveraging the lossy reconstruction as visual prompts to predict the probability distribution of the residual image. The estimated distribution is then applied to losslessly encode the residual patch via arithmetic coding. 

As illustrated in \cref{fig:model}, the visual embeddings for the LLM consist of global embeddings $\mathbf{z}_{g}$ and local embeddings $\mathbf{z}_l^n$. The residual is compressed in a pixel-by-pixel manner. For each pixel in the residual patch $\mathbf{r}^n$, we employ an autoregressive approach to estimate its probability distribution. Specifically, to predict the probability distribution of residual pixel $r_{j}^n$ at position $j$ (where $j=1,2,\cdots,p^2$), the visual embeddings, along with previously obtained residual embeddings $\{z_{r,1}^n,\ldots, z_{r,j-1}^n\}$, are concatenated into a sequence and fed into the LLM. The LLM then outputs the corresponding prediction, calculated as follows:
\begin{equation}
    f_{j}^n=F(\mathbf{z}_g,\mathbf{z}_{l}^n,z_{r,1}^n,\ldots, z_{r,j-1}^n),
    % \nonumber 
\end{equation}
where $ f_{j}^n \in \mathbb{R}^d$ is the output feature of the LLM for the pixel at position $j$.

To estimate the distribution more accurately, we go beyond directly outputting probabilities and instead predict the parameters of the probability distribution. Specifically, we introduce a Gaussian Mixture Model (GMM)~\cite{Cheng_2020_CVPR_DGML} for effective distribution modeling. The parameters of the GMM are derived by linearly projecting the LLM output feature $f^n_j$. These parameters include the weights $\boldsymbol{w}^n_j$, means $\boldsymbol{\mu}^n_j$, and standard deviations $\boldsymbol{\sigma}^n_j$. Consequently, the probability distribution of the residual values can be expressed as follows: 
\begin{equation}
\begin{aligned}
p(r_j^n|\mathbf{x}_l,\mathbf{x}_l^n,r_{<j}^n) &= p(r_j^n|f_{j}^n) \\
&\sim \sum_{k=1}^K \boldsymbol w^{n,(k)}_j \mathcal{N}(\boldsymbol \mu^{n,(k)}_j, \boldsymbol \sigma^{2\ n,(k)}_j),
% \nonumber
\end{aligned}
\end{equation}
where $k$ denotes the index of mixtures, $K$ denotes the total number of mixtures, and $\mathcal{N}(\mu,\sigma^2)$ denotes a Gaussian distribution with mean $\mu$ and standard deviation $\sigma$.

\subsection{Loss Function}

In our proposed method, the primary objective is to minimize the discrepancy between the estimated distribution $p(r)$ and the real distribution $q(r)$. We quantify this discrepancy using cross-entropy: the lower the cross-entropy, the closer $p(r)$ approximates $q(r)$, resulting in fewer bits required by the entropy coder to encode $r$.
Specifically, we train our model by optimizing the following loss function:
\begin{equation}
\begin{aligned}
\mathcal{L} &= H(q, p) = \mathbb{E}_{r \sim q}[-\log p(r)] \\
&= -\sum_r q(r) \log p(r) \\
&= -\sum_{n=1}^{N} \sum_{j=1}^{p^2} \log \left\{ \sum_{k=1}^{K} \boldsymbol{w}^{n,(k)}_j \left[ c^{(k)}(r_j^n + \frac{1}{2}) \right. \right. \\
&\quad \left. \left. - c^{(k)}(r_j^n - \frac{1}{2}) \right] \right\},
\end{aligned}
\end{equation}
where $c^{(k)}(\cdot)$ is the cumulative distribution function of a Gaussian distribution defined by the mean $\boldsymbol \mu^{n,(k)}_j$ and the standard deviation $\boldsymbol \sigma^{n,(k)}_j$. 
