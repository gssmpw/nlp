\section{Related Work}

\subsection{Lossy Image Compression}
Lossy image compression methods aim to minimize coding distortion at a given bitrate. 
Traditional lossy image coding standards, such as JPEG~\cite{Wallace_1991_CACM_JPEG} and BPG~\cite{bpg}, employ manually designed modules to improve the compression performance. 
For instance, the widely-used JPEG codec leverages the discrete cosine transform (DCT) to reduce spatial redundancy and employs Huffman coding to further reduce bitrates losslessly. Most lossy codecs adhere to the rate-distortion principle, selecting optimal coding modes to achieve better compression performance.

Recent advancements in learning-based lossy image compression~\cite{Liu_2023_CVPR_LIC_TCM,Jiang_2023_ICMLW_MLIC++,Li_2024_ICLR_FLIC} have surpassed the SOTA traditional codecs like VVC~\cite{Bross_2021_TCSVT_VVC}. 
The hyperprior model by Ball{\'{e}} et al.~\cite{Balle_2018_ICLR_hyperprior} has been studied as a powerful paradigm, applying lossy transforms, quantization, and efficient lossless encoding of latent representations. Some works~\cite{Cheng_2020_CVPR_DGML,Zhu_2022_ICLR_SwinTCharm,Zou_2022_CVPR_winatten} employ advanced architectures, such as attention mechanism~\cite{Vaswani_2017_NIPS_attention} and Swin-Transformer~\cite{Liu_2021_ICCV_SwinT}, to improve information retention during lossy transforms. Additionally, studies like~\cite{Minnen_2018_NeurIPS_Joint} have optimized the lossless latent coding, incorporating autoregressive components with the hyperprior to capture causal context. Refinements of the context model have led to further improvements in compression~\cite{Minnen_2020_ICIP_Charm,He_2021_CVPR_Checkerboard,He_2022_CVPR_ELIC}.

Many advancements in hyperprior-based methods focus on enhancing the lossless compression of latent representations by achieving more accurate distribution estimation. Consequently, lossy and lossless image compression are closely related, with lossless compression techniques often contributing to greater efficiency in lossy compression.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/pipeline_221_3.pdf}
        \caption{Overview of the encoding and decoding process. A lossy reconstruction $\mathbf{x}_l$ and its patch $\mathbf{x}_l^n$ serve as visual prompts for the LLM to predict the residual's probability distribution, with the decoding process mirroring encoding by generating residual tokens autoregressively. The red dashed line represents the autoregressive process, where the decoded residuals serve as input to the LLM to predict the probability distribution of the next residual. This process continues until all residuals are decoded. (AE: Arithmetic Encoder. AD: Arithmetic Decoder. LLM: Large Language Model.)}
    \label{fig:pipeline}
\end{figure*}

\subsection{Lossless Image Compression} 

Traditional lossless image codecs, such as PNG~\cite{boutell1997png}, WebP~\cite{webp_tech_report}, FLIF~\cite{sneyers2016flif}, and JPEG-XL~\cite{alakuijala2019jpeg}, typically utilize hand-crafted techniques to reduce intra-image redundancy. These methods typically follow a process of filtering, transforming, quantizing, and applying entropy coding to generate the final bitstream. 
Recently, learning-based lossless image compression has gained significant attention, typically consisting of two stages:
1) constructing a statistical model to capture the probability distribution of image data. 
2) utilizing this statistical model to encode the image into a bitstream using entropy tools such as arithmetic coding (AC) or asymmetric numerical systems (ANS)~\cite{duda2013asymmetric}. We employ AC as the lossless data compression technique, due to its widespread use in coding systems and its ability to generate nearly optimal-length codes based on a given probability distribution and input sequence. It encodes an entire message as a single number within the interval [0, 1) (represented in binary), using a probabilistic model to subdivide the interval into subintervals proportional according to each symbol's probability. 

To enhance statistical models for lossless image compression, deep generative models have been introduced and can be broadly categorized into three types: 
1) \textit{Autoregressive models}, such as PixelRNN~\cite{van2016pixel},  PixelCNN~\cite{van2016conditional} and PixelCNN++~\cite{Salimans2017PixeCNN}, which predict pixel distributions based on conditional dependencies with previously obtained pixels via masked convolutions. 
2) \textit{Flow models}, such as iVPF~\cite{zhang2021ivpf} and iFlow~\cite{zhang2021iflow}, which leverage invertible transforms to simplify latent distributions for efficient entropy coding. 
3) \textit{Variational Auto-Encoder (VAE) models}, like L3C~\cite{mentzer2019practical}, which employ VAE architectures to model image distributions. It is noteworthy that some studies have managed to achieve lossless compression by first compressing the image using a lossy encoder, and then compressing the residuals. For example, RC~\cite{mentzer2020learning} integrates BPG for image compression and a CNN for residual compression, whereas DLPR~\cite{bai2024deep} combines VAE with autoregressive models to enhance performance.

However, these methods typically rely on complex network designs and are constrained by limited training datasets, especially in the fields like medical images where data is scarce. This highlights the need for a simple pipeline that leverages the extensive prior knowledge embedded in pretrained models from other datasets to enhance compression efficiency.

\subsection{Large Language Models}
Large language models (LLMs) have gained significant attention in natural language processing (NLP) and artificial general intelligence (AGI) for their impressive abilities in language generation, in-context learning, world knowledge, and reasoning~\cite{Wang_2023_NIPS_VisionLLM}.
LLMs can quickly adapt to specific tasks using techniques like Adapters~\cite{houlsby2019parameter} and Low-Rank Adaptation (LoRA)~\cite{hu2021lora}. Recent research has extended the potential of LLMs to computer vision tasks, such as image classification and segmentation~\cite{Gou_2024_arxiv_llmcimageclassification,yang2023improved}. However, these studies primarily focus on aligning textual and visual semantics while overlooking low-level visual features. Addressing this gap, LM4LV~\cite{zheng2024lm4lv} employs LLMs for image restoration, emphasizing their understanding of low-level visual features. Additionally, Del{\'e}tang et al.\cite{deletang2023language} demonstrates that LLMs, when viewed as compressors, can outperform traditional codecs like PNG in lossless compression for grayscale images, highlighting their potential in this field.