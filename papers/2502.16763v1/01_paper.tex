\section{Introduction}
Neural networks have long been known as universal function approximators: a two-layer feed-forward neural network with sufficient width can approximate any continuous function on a compact domain \citep{cybenkoUniversal, HORNIK1989359}. However, these classical theorems only guarantee the \emph{existence} of a suitable parameter configuration; they do not explain whether a standard training procedure (e.g., gradient-based training) can efficiently discover such parameters. While universal approximation highlights the expressive power of neural networks, it does not directly address how trained networks generalize beyond the specific data they observe. 

In contrast, the literature on domain generalization provides general learning guarantees from a Probably Approximately Correct (PAC) learning perspective. Given training and testing distributions, one can bound the expected test loss of \emph{any} hypothesis that minimizes the empirical training risk. This bound typically depends on the discrepancy between the training and testing distributions, the training sample size, and the complexity of the hypothesis class \citep{NIPS2006_b1b0432c, mohriadapt}. However, such bounds do not account for training dynamics, making them quite pessimistic, especially when a single hypothesis that fails to perform well on the testing data is a minimizer of the training loss, even if a standard training procedure would never find it. 

Our work complements these approaches in a more specialized setting. We focus on exact learning using the \emph{training dynamics} of a simple two-layer network in the \emph{infinite-width} Neural Tangent Kernel (NTK) regime. We show that an ensemble of such models can learn with arbitrarily high probability a global transformation, in our case, a \emph{permutation} function, from a logarithmically small training set.

%

%

\paragraph{Contributions.} We use the NTK framework to analyze the problem of learning permutations, a task many models need to perform as a subroutine, yet it is unknown if it can be performed exactly with high probability. We list our contributions below:
\begin{itemize}
    \item We show that when the inputs are nonzero, normalized, and binary\footnote{Our results can be extended to different vocabularies by considering, for example, one-hot encodings of the alphabet symbols. The key assumption is that all symbols are individually ``seen'' by the model during training.}, a two-layer fully connected feed-forward network in the infinite width limit trained on the \textit{standard basis} using gradient descent (or gradient flow) converges to a Gaussian Process whose mean captures the ground truth permutation via sign-based features. More precisely, each entry in the predicted mean is non-positive exactly when (and only when) the corresponding ground-truth value is zero.
    \item We show that this can be achieved with logarithmic training size by picking the training set to consist of the $k$ standard basis vectors (out of the $2^k-1$ vectors in the domain), where $k$ is the input length.
    \item We can achieve zero error on any input unseen during training, with high probability, by averaging the output of $\mathcal{O}(k\log k)$ independently trained models and applying a post-processing rounding step. To guarantee zero error \emph{simultaneously} for all inputs, the same procedure requires averaging the output of $\mathcal{O}(k^2)$ independently trained models.
\end{itemize}

\section{Literature Review}

The NTK framework \citep{NEURIPS2018_5a4be1fa} is a popular framework designed to provide insights into the continuous-time gradient descent (gradient flow) dynamics of fully connected feed-forward neural networks with widths tending to infinity. The initial results have been extended to discrete gradient descent \citep{NEURIPS2019_0d1a9651} as well as generalized to more architecture families like RNNs and Transformers \citep{yang2020tensorprogramsiineural, yang2021tensorprogramsiibarchitectural}. While the general NTK theory has been extensively studied, very few works use the framework to study the performance of neural architectures on specific tasks. Notably, the framework has been used to attempt to explain the ``double descent'' phenomenon \citep{Nakkiran2020Deep} observed in deep architectures \citep{pmlr-v119-adlam20a, singh2022phenomenology}. It has also been used to study the behavior of Residual Networks \citep{CSIAM-AM-3-4} and Physics-Informed Neural Networks \citep{WANG2022110768}. To the best of our knowledge, the closest work to ours is \cite{boix-adsera2024when}, where the authors use results from the general NTK theory to prove that Transformers can provably generalize out-of-distribution on template matching tasks. Furthermore, they show that feed-forward neural networks fail to generalize to unseen symbols for any template task, including copying, a special case of a permutation learning setting. In our paper, we show that feed-forward neural networks can indeed learn to copy without error with high probability. At first, this might seem to contradict \cite{boix-adsera2024when}. However, the key difference between our work and the work in \cite{boix-adsera2024when} lies in the problem setting. The setting in \cite{boix-adsera2024when} poses no restrictions to the input, whereas we require binary encoded input. Therefore, the model has access to all symbols during training. In particular, we specifically choose the training set, i.e. the standard basis, so that each symbol appears in every position. 
The restrictions above are what enable exact permutation learning.      

Lastly, various studies highlight the expressive power of neural networks through simulation results. For instance, \citet{siegelman95comp} demonstrates the Turing completeness of recurrent neural networks (RNNs), while \citet{hertrich2023provably} introduces specific RNN constructions capable of solving the shortest paths problem and approximating solutions to the knapsack problem. Moreover, other simulation studies on Transformers have established their Turing completeness \citep{perez2021attention, wei2022statistically} and showcased constructive solutions for linear algebra, graph-related problems \citep{giannou23a, pmlr-v235-back-de-luca24a, yang2023looped}, and parallel computation \citep{deluca2025positionalattentionexpressivitylearnability}. However, the results in these works are existential, and none considers the dynamics of the training procedure.

\section{Notation and Preliminaries}
\label{sec:prelim}
Throughout the text, we use boldface to denote vectors. We reserve the notation $\vect{e_1}, \vect{e_2}, \dots$ for the standard basis vectors. We use $\vect{1}$ to denote the vector of all ones.  For a vector $\vect{x}\in \RR^n$ we denote by $\|\vect{x}\| := \sqrt{\sum_{i=1}^n \vect{x}_i^2}$ the Euclidean norm of $\vect{x}$. We denote the $n\times n$ identity matrix by $I_n$. For $n \in \mathbb{N} := \{1,2,\dots\}$ let 
$$H_n=\{\vect{x} \in \RR^n : \vect{x}_i =0 \text{ or } \vect{x}_i = 1 \;\forall\, i=1,2,\dots,n\}$$
denote the $n$-dimensional Hamming cube (the set of all $n$-dimensional binary vectors). We use the notation $[n]$ to refer to the set $\{1,2,\dots,n\}$. For two matrices $A_1 \in \RR^{n_1 \times m_1}$, $A_2 \in \RR^{n_2 \times m_2}$ we denote by $A_1 \otimes A_2 \in \RR^{n_1n_2 \times m_1m_2}$ their Kronecker product. It is relatively easy to show that when $A$ and $B$ are square matrices (i.e. $n_1=m_1$ and $n_2=m_2$), the eigenvalues of $A_1 \otimes A_2$ are given exactly by the products of the eigenvalues of $A_1$ and $A_2$. In particular, $A_1 \otimes A_2$ is positive definite if $A_1$ and $A_2$ are positive definite.
\subsection{Permutation matrices}
\label{subsub:perm}
A permutation matrix is a square binary matrix $P \in \{0,1\}^{n\times n}$ with exactly one entry of $1$ in each row and column. Let $\text{Sym}([n])$ denote the set of all permutations of the set $[n]$. Every permutation matrix can be mapped uniquely to a permutation $\pi \in \text{Sym}([n])$ formed by taking $$\pi(i) = \text{the unique index } j \text{ such that } P_{i,j}=1$$
Multiplying a vector $\vect{v}\in \RR^n$ with $P$ from the left corresponds to permuting the entries of $v$ according to $\pi$, that is $(P\vect{v})_i = \vect{v}_{\pi^{-1}(i)}$. We say that a neural network has \textit{learned a permutation $P$} if on input $\mathbf{v}$ it outputs $P\vect{v}$. When $P$ is the identity matrix the neural network simply copies the input.

\subsection{NTK Results}
In what follows, we provide a brief overview of the necessary theory used to derive our results.  We refer the reader to \cite{golikov2022neuraltangentkernelsurvey} for a comprehensive treatment of the NTK theory.
\subsubsection{General Architecture Setup}
\label{subsub:arch}
We follow the notation of \citealt{NEURIPS2019_0d1a9651}, which is, for the most part, consistent with the rest of the NTK literature.\footnote{\citet{NEURIPS2019_0d1a9651} uses row-vector notation whereas in this work we use column-vector notation. Slight definition and computation discrepancies arise from this notational difference.} Let $\mathcal{D}\in \RR^{n_0} \times \RR^{k}$ denote the training set, $\mathcal{X} = \{x: (x, y) \in \mathcal{D}\}$ and $\mathcal{Y} = \{y: (x, y) \in \mathcal{D}\}$. The architecture is a fully connected feed-forward network with $L$
hidden layers with widths $n_l$, for $l=1,2,\dots, L$ and a readout layer with $n_{L+1}=k$. In our case, we consider networks with $L=1$ hidden layers. For each $\vect{x} \in \RR^{n_0}$,
we use $h^l(\vect{x}), x^l(\vect{x}) \in \RR^{n_l}$ to represent the pre- and post-activation functions at layer $l$ with input $\vect{x}$. The recurrence relation for a feed-forward network is given for $l = 1,2,\dots,L+1$ by:
$$
\left\{\begin{array} { l } 
{ h ^ { l} = W ^ { l } \vect{x} ^ { l-1 }  + b ^ { l } } \\
{ \vect{x} ^ { l } = \phi ( h ^ { l } ) }
\end{array} \quad \text{ and } \quad \left\{\begin{array}{ll}
W_{i, j}^l & =\frac{\sigma_\omega}{\sqrt{n_l}} \omega_{i j}^l \\
b_j^l & =\sigma_b \beta_j^l
\end{array}\right.\right.
$$
where $\vect{x}^0 := \vect{x}$, $\phi$ is the (pointwise) activation function (in our case, we take $\phi$ to be the ReLU activation) $W^{l}\in \RR^{n_{l} \times n_{l-1}}$ and $b^{l} \in \RR^{n_{l}}$ are the weights and biases, $\omega^{l}_{ij}$ and $\beta^{l}_j$ are trainable parameters initialized i.i.d from a standard Gaussian $\mathcal{N}(0,1)$, and $\sigma_\omega^2$ and $\sigma_b^2$ are the weight and bias variances. This parametrization is referred to as the ``NTK Parametrization''. The output of the network at training time $t$ on input $\vect{x} \in \RR^{n_0}$ is thus given by $f_t(\vect{x}):=h^{L+1}(\vect{x}) \in \RR^{k}$.

\subsubsection{The NTK and NNGP Kernels}
\label{subsub:kernels}
\paragraph{The Empirical NTK} The empirical Neural Tangent Kernel (NTK) is a kernel function that arises naturally when analyzing the gradient-descent dynamics of neural networks. For two inputs $\vect{x}, \vect{x'} \in \RR^{n_0}$ the empirical NTK kernel at time $t$ and layer $l$ is given by
\begin{equation*}
  \hat{\Theta}^l_t(\vect{x}, \vect{x'}) = \nabla_{\theta^l_t} h^l_t(\vect{x}) \nabla_{\theta^l_t} h^l_t(\vect{x'})^\top \in \RR^{n_l \times n_l},
\end{equation*}
where $\theta^l_t$ denotes the vector of all parameters of the network up to layer $l$. We can similarly define the empirical NTK for the whole training set as $\hat{\Theta}^l_t(\mathcal{X}, \mathcal{X})\in \RR^{n_l|\mathcal{D}| \times n_l|\mathcal{D}|}$ and the NTK for a testing point $\hat{\vect{x}}$ as  $\hat{\Theta}^l_t(\hat{\vect{x}}, \mathcal{\mathcal{X}})\in \RR^{n_l \times n_l|\mathcal{D}|}$ by defining $h^l_t(\mathcal{X})$ as the vectorization of all the $h^l_t(\vect{x})$ for $\vect{x} \in \mathcal{X}$. When the superscript $l$ is omitted, we implicitly refer to $l=L+1$. In that case, we will write 
\begin{equation*}
\hat{\Theta}_t(\vect{x}, \vect{x'}) = \nabla_{\theta_t} f_t(\vect{x}) \nabla_{\theta_t}f_t(\vect{x'})^\top \in \RR^{k\times k}.
\end{equation*}

\paragraph{The NNGP and limit NTK kernels} It can be shown that in the infinite width limit, we have the following two properties: the output of the $l$-th layer of the network at
initialization behaves as a zero-mean Gaussian process. The covariance kernel of the resulting Gaussian process is referred to as the \textit{NNGP kernel} and denoted by $\mathcal{K}^{l}$. Furthermore, the empirical NTK at initialization $\hat{\Theta}_0^{l}$ converges to a deterministic kernel $\Theta^{l}$, called the \textit{limit NTK}.
As before, we write $\mathcal{K}$ and $\Theta$ to refer to the NNGP and limit NTK kernels for $l = L+1$, respectively. In \cref{app:recursion}, we provide the recursive relations used to derive the two kernels. Lastly, the following theorem relates the limit NTK to the output of the network when trained with
gradient descent (or gradient flow) in the infinite width limit:

\begin{theorem}[Theorem 2.2 from \citealt{NEURIPS2019_0d1a9651}]
\label{thm:output}
    Let $n_1=n_2=\dots=n_L =n$ and assume that $\Theta:=\Theta(\mathcal{X},\mathcal{X})$ is positive definite. Suppose the network is trained with gradient descent (with small-enough step-size) or gradient flow to minimize the empirical MSE loss\footnote{The empirical MSE loss is defined as $$\mathcal{L}(\mathcal{D})=\frac{1}{2|\mathcal{D}|} \sum_{(\vect{x},\vect{y})\in \mathcal{D}} \|f_t(\vect{x})-\vect{y}\|^2$$}. Then, for every $\hat{\vect{x}} \in \RR^{n_0}$ with $\|\hat{\vect{x}}\| \leq 1$, as $n\to \infty$, $f_t(\hat{\vect{x}})$ converges in distribution to a Gaussian with mean and variance given by\footnote{Here ``$h.c.$'' is an abbreviation for the Hermitian conjugate}
    \begin{align}
        \mu(\hat{\vect{x}}) &= \Theta(\hat{\vect{x}}, \mathcal{X})\Theta^{-1}\mathcal{Y} \label{eq:mean_out} \\
        \Sigma(\hat{\vect{x}}) &= \mathcal{K}(\hat{\vect{x}},\hat{\vect{x}})+ \Theta(\hat{\vect{x}},\mathcal{X})\Theta^{-1}\mathcal{K}(\mathcal{X}, \mathcal{X}) \Theta^{-1} \Theta(\mathcal{X}, \hat{\vect{x}}) - (\Theta(\hat{\vect{x}}, \mathcal{X})\Theta^{-1}\mathcal{K}(\mathcal{X}, \hat{\vect{x}}) + h.c) \label{eq:var_out}
    \end{align}
    where $\mathcal{Y}$ denote the vectorization of all vectors $\vect{y} \in \mathcal{Y}$.
\end{theorem}

Our goal is to apply \Cref{thm:output} in the specific task of learning permutations and show that 2-layer feedforward neural networks can learn exactly, with high probability, any fixed permutation using just the standard basis as the training set.

\section{Summary of results}
\label{sec:desc}
In this section, we first formalize the task and then proceed with a summary of our key results.

\subsection{Task description and architecture}
Suppose $k \in \mathbb{N}$ and let $\mathbb{K} = \left\{\vect{x}/\|\vect{x}\| : \vect{x} \in H_{k}\setminus \{\vect{0}\} \right\}$\footnote{We chose to exclude the zero vector because it unnecessarily complicates the calculations.}. Let $P^* \in \{0,1\}^{k \times k}$ be any fixed permutation matrix. Consider the architecture of \Cref{subsub:arch} with $L = 1$ (one hidden layer), $n_0 = k$ and $\phi(x) = \relu{x}$ (where $\relu{x} = \max\{0, x\}$). Concretely, the architecture is defined as the function $F: \RR^{k} \to \RR^{k}$ with  
$$F(\vect{x}) = W^2\relu{W^1 \vect{x} + b^1} + b^2$$
The training set is 
$$\mathcal{D} = \{(\vect{e_i}, P^*\vect{e_i}):i=1,2,\dots, k\}$$ 
and so $\mathcal{X} = \{\vect{e_1}, \dots, \vect{e_k}\}$ and $\mathcal{Y} = \{P^*\vect{e_1}, \dots, P^*\vect{e_k}\}$. 

\subsection{Main results}
Here we present and discuss our two main results: \Cref{thm:mean_useful} and \Cref{thm:ensemble}. The former is proved in \Cref{sec:mean_var} and the latter in \Cref{sec:behavior}.
\begin{theorem}[The mean, at the limit, carries useful information]
\label{thm:mean_useful}
    When taking $\sigma_\omega = 1$ and $\sigma_b = 0$\footnote{We chose to take $\sigma_\omega = 1$ and $\sigma_b = 0$ as the absence of the bias term greatly simplifies the calculations.} (no bias terms) and letting the width of the hidden layer $n_1$ tend to infinity, the output of the network when trained with full-batch gradient descent with a small enough learning rate (or gradient flow) on the standard basis and evaluated at a test point $\hat{\vect{x}} \in \mathbb{K}$ converges in distribution as the training time $t\to \infty$ to a multivariate normal distribution $\mathcal{N}(\mu\left(\hat{\vect{x}}), \Sigma(\hat{\vect{x}})\right)$ with the following property: for all $i=1,2,\dots, k$ 
\begin{itemize}
    \item $\mu(\hat{\vect{x}})_i$ is equal to some \textit{non-positive} constant $\mu_0(n_{\hat{\vect{x}}})$ if $(P^* \hat{\vect{x}})_i = 0$.
    \item $\mu(\hat{\vect{x}})_i$ is equal to some \textit{strictly positive} constant $\mu_1(n_{\hat{\vect{x}}})$ if $(P^* \hat{\vect{x}})_i > 0$ (in which case $(P^* \hat{\vect{x}})_i = 1/\sqrt{n_{\hat{\vect{x}}}}$).
\end{itemize}
where $n_{\hat{\vect{x}}}$ denotes the number of nonzero entries in $\hat{\vect{x}}$. Therefore, in both cases, the corresponding entries of the mean depend only on the sparsity of $\hat{\vect{x}}$.
\end{theorem}

\Cref{thm:mean_useful} establishes that the NTK regime generates a distribution of models whose mean encapsulates the information required to solve the permutation problem. This property has important practical implications. The Strong Law of Large Numbers guarantees that the result of averaging the network's output over enough independent runs on the same input $\hat{\vect{x}}$ converges to the actual mean $\mu(\hat{\vect{x}})$. This, in turn, gives rise to a natural inference strategy: repeat the training procedure enough times, average the outputs of the network on input $\hat{\vect{x}}$ and then round each coordinate to $0$ or $1/\sqrt{n_{\hat{\vect{x}}}}$ depending on whether the result of the averaging is positive or not.
%
%
This averaging procedure identifies the target permutation $P^*$ using a logarithmic training sample size (only $k$ out of the $2^k - 1$ total possible inputs are needed) provided enough independent trials are averaged.
In \Cref{sec:behavior}, we quantify the number of independent trials needed to obtain any desired level of post-rounding accuracy, which we refer to as  \textit{ensemble complexity}, by proving the following theorem:

\begin{theorem}[Ensemble complexity of permutation learning]
\label{thm:ensemble}
Let $\delta \in (0,1)$. Under the assumptions of \Cref{thm:mean_useful}, when averaging over $\mathcal{O}(k \log \left(k/\delta\right))$ independent trials and rounding the average using the aforementioned rounding strategy, any test input $\hat{\vect{x}} \in \mathbb{K}$ is correctly permuted (post-rounding) according to $P^*$ with probability $1-\delta$. 
\end{theorem}

\Cref{thm:ensemble} gives an upper on the ensemble complexity for a \emph{single} input. By taking the union bound we can get an ensemble complexity bound for all inputs \emph{simultaneously}. This is presented in the following remark:

\begin{remark}
    \label{rem:simult}
    The conclusion of \Cref{thm:ensemble} holds simultaneously for all test inputs when averaging over $\mathcal{O}(k^2)$ models. 
\end{remark}

Combining \Cref{thm:mean_useful} with  \Cref{thm:ensemble} and \Cref{rem:simult} gives us the following conclusion: two-layer feed-forward neural networks can learn any fixed permutation on binary inputs of size $k$ with logarithmic training size and quadratic ensemble complexity. In what follows, we present the proofs of the theorems and experiments to verify them. 


\section{Derivation and interpretation of the mean and variance of the output distribution}
\label{sec:mean_var}
We provide the proof of \Cref{thm:mean_useful}. To do so, we analytically calculate the mean and variance as given by \Cref{thm:output} in the context of our task when $\sigma_\omega = 1$ and $\sigma_b = 0$. Let $\hat{\vect{x}} \in \mathbb{K}$ be a test point. According to \Cref{thm:output}, the output of the network $f_t(\hat{\vect{x}})$ in the infinite width limit converges to a multivariate Gaussian with mean and variance given by \Cref{eq:mean_out} and \Cref{eq:var_out}, respectively, 
provided that the matrix $\Theta(\mathcal{X}, \mathcal{X})$ is positive definite.
To calculate $\mu(\hat{\vect{x}})$ we first need to derive $\Theta(\hat{\vect{x}},\mathcal{X})$ and $\Theta(\mathcal{X}, \mathcal{X})$ which we refer to as the test and train NTK kernels, respectively. Some details regarding the derivations are presented in \Cref{app:derivations}. We divide the computation into sections and also provide an illustrative numerical example.

\subsection{Train NTK}
In this section, we derive the train NTK kernel $\Theta:=\Theta(\mathcal{X}, \mathcal{X})$. Recall that the train inputs are simply the standard basis vectors $\vect{e_1},\dots, \vect{e_n}$. Using the recursive formulas of \Cref{app:recursion} we obtain
\begin{equation}
\Theta := \Theta(\mathcal{X}, \mathcal{X}) = \left((d-c)I_{k} + c\vect{1}\vect{1}^\top\right) \otimes I_{k} \in \RR^{k^2 \times k^2}
\label{eq:limit_NTK}
\end{equation}
where 
\begin{align}
\label{eq:c_and_d}
    d = \frac{1}{k} \quad \textrm{ and } \quad
    c = \frac{1}{2\pi k}
\end{align}
We can observe that since $d>c>0$, $\Theta$ is positive definite. Indeed, since $\vect{1}\vect{1}^\top$ (the matrix of all ones) is positive semidefinite, $(d-c)I_{k} + c\vect{1}\vect{1}^\top$ is (strictly) positive definite and so the Kronecker product with $I_k$ is also positive definite (see \Cref{sec:prelim}). In that case, a direct application of \Cref{thm:sherman_mor} shows that 
\begin{equation}
\label{eq:invntk}
    \Theta^{-1} = \left(\frac{1}{d-c} I_k - \frac{c}{(d-c)(d-c+ck)}\vect{1}\vect{1}^\top\right) \otimes I_k
\end{equation}

Notice that the fact that the training set is an orthonormal set of vectors (the standard basis) the train NTK has a convenient structure, namely each block $\Theta(\vect{e_i}, \vect{e_j})$ is diagonal. This fact is captured in the Kronecker product structure of \Cref{eq:limit_NTK}.

\subsection{Test NTK}
In this section, we calculate the test NTK $\Theta(\hat{\vect{x}}, \mathcal{X})$ for an arbitrary test input $\hat{\vect{x}} \in \mathbb{K}$.
Again, using the recursive formulas of \Cref{app:recursion}, we find that 
\begin{equation}
\label{eq:testntkkron}
    \Theta(\hat{\vect{x}}, \mathcal{X}) = \mathbf{f}^\top \otimes I_{k} \in \RR^{k\times k^2}
\end{equation}
where for each $i=1,2,\dots, k$:
\begin{align}
\label{eq:vect_f}
   \mathbf{f}_i &=  \frac{\cos \hat{\theta}_i(\pi - \hat{\theta}_i) + \sin \hat{\theta}_i}{2k\pi}+ \hat{\vect{x}}_i\frac{\pi - \hat{\theta}_i}{2k\pi}
\end{align}
and 
\begin{equation}
\label{eq:theta_hat}
    \hat{\theta}_i = \arccos\left(\hat{\vect{x}}_i\right)
\end{equation}
Notice that since each $\hat{\vect{x}}_i$ is equal to either $0$ or $1/\sqrt{n_{\hat{\vect{x}}}}$ where $n_{\hat{\vect{x}}}$ is the number of nonzero entries of $\hat{\vect{x}}$, the resulting $\hat{\theta}_i$ takes two values $\hat{\theta}^0$ and $\hat{\theta}^1$ (corresponding to $\hat{\vect{x}}_i = 0$ and $\hat{\vect{x}}_i=1/\sqrt{n_{\hat{\vect{x}}}}$) and subsequently each $\mathbf{f}_i$ also takes two values $\text{f}^0$ and $\text{f}^1$. Substituting we get:  
\begin{equation}
\label{eq:binary_theta}
    \hat{\theta}^0 = \frac{\pi}{2} \quad \textrm{ and }\quad\hat{\theta}^1 = \arccos\left(\frac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)
\end{equation}
The corresponding $\mathrm{f}^0$ and $\mathrm{f}^1$ are derived by substituting $\hat{\theta}^0$ and $\hat{\theta}^1$ from \Cref{eq:binary_theta} to \Cref{eq:vect_f}.

\paragraph{Numerical example.}
Let us consider a concrete numerical example. Take $k = 5$ and $P^* = I$ the identity permutation. Consider the test inputs vector  $\hat{\vect{x}} = (1/\sqrt{2},1/\sqrt{2},  0,  0 , 0)^\top$. Calculating the values of $\mathrm{f^0}$ and $\mathrm{f^1}$ for $\hat{\vect{x}}$ we find 
\begin{equation}
    \mathrm{f^0} \approx 0.0318 \quad \textrm{ and } \quad\mathrm{f^1} \approx 0.1285
\end{equation}
Then 
\begin{equation*}
\Theta(\hat{\vect{x}}, \vect{e_i}) \approx \begin{cases}
    0.1285 & \text{if } i=1,2 \\ 
    0.0318 & \text{if } i=3,4,5
\end{cases} 
\end{equation*}
These numbers act as a measure of similarity between the test input and each element of the training set. In our example, the test input has nonzero entries in its first two coordinates, and hence the values of $\Theta(\hat{\vect{x}}, \vect{e_1})$ and $\Theta(\hat{\vect{x}}, \vect{e_2})$ are higher, reflecting a stronger ``match'' of the test input $\hat{\vect{x}}$ with the training vectors $\vect{e_1}$ and $\vect{e_2}$. 

\subsection{Computing  $\mu(\hat{\vect{x}})$}
Having calculated the required NTK kernels we are now ready to calculate the mean of the output distribution $\mu(\hat{\vect{x}})$ for a test input $\hat{\vect{x}} \in \mathbb{K}$.
Since $\Theta$ is positive definite we can now calculate the mean of the limiting Gaussian. Recall that we have
\begin{equation*}
    \mu(\hat{\vect{x}}) = \Theta(\hat{\vect{x}}, \mathcal{X})\Theta^{-1}\mathcal{Y}
\end{equation*}
where 
\begin{equation*}
\mathcal{Y} = \begin{bmatrix}
        P^*\vect{e_1} \\
        P^*\vect{e_2} \\
        \vdots \\
        P^*\vect{e_k}
    \end{bmatrix}
\end{equation*}

From \Cref{eq:testntkkron} and \Cref{eq:invntk} we get 
\begin{equation}
    \label{eq:firstprod}
    \Theta(\hat{\vect{x}}, \mathcal{X})\Theta^{-1} = \mathbf{f}^\top \left(\frac{1}{d-c} I_k - \frac{c}{(d-c)(d-c+ck)}\vect{1}\vect{1}^\top\right) \otimes I_k
\end{equation}
and from \Cref{lemm:kron} we get 
\begin{equation}
    \label{eq:mean_final}
    \mu(\hat{\vect{x}}) = P^*\left(\frac{1}{d-c} I_k - \frac{c}{(d-c)(d-c+ck)}\vect{1}\vect{1}^\top\right)\mathbf{f}
\end{equation}

There is an interesting pattern arising from \Cref{eq:mean_final}: since $\mathbf{f}_i$ only takes two values depending on whether $\hat{\vect{x}}_i$ is nonzero, $\mu(\hat{\vect{x}})$ also takes only two values. Remarkably, the multiplication by $P^*$ in \Cref{eq:mean_final} corresponds exactly to the ground truth permutation we want our model to learn. 

\paragraph{Numerical example.} 
Coming back to our numerical example of the previous section, we can compute the mean exactly by calculating $\Theta^{-1}\mathbf{f}$, which gives
\begin{equation*}
\mu(\hat{\vect{x}}) \approx (0.5606, 0.5606, -0.0146, -0.0146, -0.0146)^\top
\end{equation*}
Intuitively, multiplying $\mathbf{f}$ with the inverse of the train NTK matrix reweights the similarity scores to correct for correlations between the training set\footnote{This is similar to how multiplication by $(X^\top X)^{-1}$ in linear regression accounts for the correlation between observations.}. Finally, multiplying by $P^*$ (in this case the identity) permutes the reweighted similarities according to the ground truth permutation. Notice that the mean is negative for entries where the ground truth is zero and positive otherwise. Below, we show that this is not a coincidence.

\paragraph{Interpreting the mean.}
Substituting everything into \Cref{eq:mean_final} we find that 

\begin{equation}
    \label{eq:simplified_mean0}
    \mu(\hat{\vect{x}})_i = \frac{y^2-\sqrt{y^2-1} y-2 \pi  (y-1)+2 y \sec ^{-1}(y)-1}{(2 \pi -1) (k+2 \pi -1)}
\end{equation}
if $(P^* \hat{\vect{x}})_i = 0$ and 
\begin{align}
    \label{eq:simplified_mean1}
    \mu(\hat{\vect{x}})_i &= 
    -\frac{\left(\sqrt{y^2-1}-y\right) \left(y^2-k\right)}{(2 \pi -1) (k+2 \pi -1) y} +  \frac{\pi \left(k-y^2+2 \sqrt{y^2-1}-1\right)}{(2 \pi -1) (k+2 \pi -1) y} \nonumber\\&\quad\;\; + \frac{2\left(k-y^2+2 \pi -1\right) \csc ^{-1}(y)-\sqrt{y^2-1}+2 \pi ^2}{(2 \pi -1) (k+2 \pi -1) y}
\end{align}
if $(P^*\hat{\vect{x}})_i > 0$, where $y = \sqrt{n_{\hat{\vect{x}}}}$. Using a symbolic calculation system (Mathematica \citet{Mathematica}) we can establish that for all possible values of $y$ (i.e $y=\sqrt{m}$ for $m=1,2,\dots, k)$, $\mu(\hat{\vect{x}})_i \leq 0$ \textit{if and only if} $(P^* \hat{\vect{x}})_i = 0$.
This concludes the proof of \Cref{thm:mean_useful}. Furthermore, the expressions for $\mu_0(\hat{\vect{x}})$ and $\mu_1(\hat{\vect{x}})$ are given by \Cref{eq:simplified_mean0} and \Cref{eq:simplified_mean1}, respectively.

\subsection{Computing $\Sigma(\hat{\vect{x}})$}
\label{sub:variance}
We calculate the variance of the output distribution $\Sigma(\hat{\vect{x}})$ for a test input $\hat{\vect{x}} \in \mathbb{K}$. With a direct substitution on the formula for the variance $\Sigma(\hat{\vect{x}})$ of  \Cref{eq:var_out} we find that the variance for a test input $\hat{\vect{x}}$ is given by 
\begin{equation}
    \Sigma(\hat{\vect{x}}) = \left(\frac{d}{2}+\mathbf{f}^\top AMA \mathbf{f} - 2\mathbf{f}^\top A \mathbf{g}\right)I_k
\end{equation}
where \\
\begin{minipage}{0.49\textwidth}
\begin{align}
    M &= \left(\frac{d}{2}-c\right)I_k + c \vect{1}\vect{1}^\top \label{eq:M_matrix} 
\end{align}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\begin{align}
    A &= \frac{1}{d-c} I_k - \frac{c}{(d-c)(d-c+ck)}\vect{1}\vect{1}^\top \label{eq:A_matrix}
\end{align}
\end{minipage}
\\ \\
and for each $i=1,2,\dots,k$:
\begin{equation}
\mathbf{g}_i = \frac{\cos \hat{\theta}_i(\pi - \hat{\theta}_i) + \sin \hat{\theta}_i}{2k\pi}
\label{eq:vect_g}
\end{equation}
The scalars $c$ and $d$, the vector $\mathbf{f}$, and the angles $\hat{\theta}_i$ are as defined in \Cref{eq:c_and_d}, \Cref{eq:vect_f} and \Cref{eq:theta_hat}, respectively. This shows that the output coordinates are independent Gaussian random variables with the same covariance. Notice that whenever the test input is part of the training set, the variance is zero. This is expected and consistent with the NTK theory, which guarantees exact learning on the training set. In what follows we will use the notation $\sigma^2(\hat{\vect{x}}) := (d/2+\mathbf{f}^\top AMA \mathbf{f} - 2\mathbf{f}^\top A \mathbf{g}) \in \RR_{\geq 0}$.

\section{Deriving the ensemble complexity}
\label{sec:behavior}
The conclusion of \Cref{thm:mean_useful} suggests a simple rounding algorithm in practice: for each coordinate, if the output of the network is greater than zero round to $1$, otherwise round to $0$. As discussed in \Cref{sec:desc}, this motivates a practical implementation where the network is trained enough times, its outputs are averaged over all runs, and subsequently rounded accordingly. In the following sections, we provide the proof of \Cref{thm:ensemble}. First, we use the concentration inequality of  \Cref{lemm:concentration} to derive an estimate of the number of independent runs needed to achieve any desired level of post-rounding accuracy in terms of $k$, $\mu(\hat{\vect{x}})$ and $\sigma(\hat{\vect{x}})$. Subsequently, we compute the asymptotic orders of $\mu(\hat{\vect{x}})$ and $\sigma(\hat{\vect{x}})$ to arrive at the conclusion of \Cref{thm:ensemble}.

\subsection{Rounding procedure}
In this section, we analyze the practical rounding strategy in more detail. Specifically, we try to quantify how many runs are enough. Certainly, as the number of runs tends to infinity we can guarantee that an average over all of them will yield perfect classification. However, this statement is not very useful from a practical standpoint as it doesn't show the dependence on the size of the input $k$.
\paragraph{Ensuring perfect accuracy} Let $\hat{\vect{x}}$ be a test input and $\vect{\hat{y}} = P^*\hat{\vect{x}}$ be the corresponding ground truth output. Suppose we take an average of $N$ models. Let $F^j(\hat{\vect{x}})$ be the output of the $j$-th model. The $F^j(\hat{\vect{x}})$'s are independent samples from a multivariate normal distribution with mean $\mu(\hat{\vect{x}})$ and variance $\Sigma(\hat{\vect{x}})$, as given by \Cref{thm:output}. The $i$-th coordinates of the network's output, $F^{j}(\hat{\vect{x}})_i$, are therefore independent samples from a normal distribution with mean $\mu_i := \mu(\hat{\vect{x}})_i$ and variance $\sigma_i^2 = \vect{e_i}^\top \Sigma(\hat{\vect{x}})\vect{e} = \Sigma(\hat{\vect{x}})_{ii}$. Let $G(\hat{\vect{x}}) = \frac{1}{N} \sum_{j=1}^N F^j(\hat{\vect{x}})$ be the average over all $N$ model outputs. Given that $\mu_i \leq 0$ whenever $\vect{\hat{y}}_i = 0$ and $\mu_i > 0$ whenever $\vect{\hat{y}}_i > 0$ we can see that the rounding procedure will yield a correct result for the $i$-th coordinate if $|G(\hat{\vect{x}})_i - \mu_i| <|\mu_i|/2$. An application of the union bound and \Cref{lemm:concentration} shows that 
\begin{equation}
\label{eq:ensemble_bound}
\mathbb{P}\left\{|G(\hat{\vect{x}})_i - \mu_i| \geq |\mu_i|/2 \; \text{ for all }i\in \{1,2,\dots,k\} \right\} \leq 2k \max_{i=1,2,\dots,k}\exp\left(-\frac{N\mu_i^2}{8\sigma_i^2}\right)
\end{equation}
Setting the right-hand side of \Cref{eq:ensemble_bound} to be at most $\delta \in (0,1)$ and noting that $\sigma_i = \sigma^2(\hat{\vect{x}})$ for all $i \in [k]$ and each $\mu_i$ can take the two values $\mu_0(\hat{\vect{x}})$ and $\mu_1(\hat{\vect{x}})$ we get that our rounding procedure produces the correct output for a test input $\hat{\vect{x}} \in \mathbb{K}$ with probability at least $1-\delta$ whenever the number of models $N$ satisfies
\begin{equation}
\label{eq:ensemble_final}
N \geq 8 \sigma^2(\hat{\vect{x}}) \cdot \max\left\{ \frac{1}{\mu_0(\hat{\vect{x}})^2}, \frac{1}{\mu_1(\hat{\vect{x}})^2}\right\} \ln\left(\frac{2k}{\delta}\right)
\end{equation}
Similarly to how sample complexity is defined, we use the term \textit{ensemble complexity} to refer to the number of independent models we need to average to produce a correct output post-rounding. \Cref{eq:ensemble_final} gives an ensemble complexity bound for any specific test input $\hat{\vect{x}}$. Since the domain is finite we can turn this into a uniform bound by taking 
\begin{equation}
    \label{eq:ensemble_uniform}
    N \geq 8\ \max_{\hat{\vect{x}} \in \mathbb{K}} \left\{ \sigma^2(\hat{\vect{x}}) \cdot \max\left\{ \frac{1}{\mu_0(\hat{\vect{x}})^2}, \frac{1}{\mu_1(\hat{\vect{x}})^2}\right\}\right\} \ln\left(\frac{2k}{\delta}\right)
\end{equation}

\subsection{Asymptotic behavior of $\mu(\hat{\vect{x}})$ and $\sigma^2(\hat{\vect{x}})$}

To conclude the proof of \Cref{thm:ensemble} and derive an asymptotic bound on the ensemble complexity of the permutation learning problem we need to analyze the asymptotic behavior of $\mu(\hat{\vect{x}})$ and $\sigma^2(\hat{\vect{x}})$. The asymptotic orders are given in the following technical lemma:

\begin{restatable}{lemma}{orders}
\label{lemm:orders}
    Let $\hat{\vect{x}} \in \mathbb{K}$ be a test input with $n_{\hat{\vect{x}}}$ nonzero entries and let $\mu_0(\hat{\vect{x}})$, $\mu_1(\hat{\vect{x}})$ and $\sigma^2(\hat{\vect{x}})$ be as in \Cref{thm:mean_useful} and \Cref{sub:variance}, respectively. Then, as $k \to \infty$
    \begin{itemize}
        \item Depending on the relationship between  $n_{\hat{\vect{x}}}$ and $k$ we have
        $$
         \mu_0(\hat{\vect{x}}) \in 
        \begin{cases}
            \Theta\left(1/k\right) & \text{if}\; n_{\hat{\vect{x}}} \text{ is constant} \\
            \Theta\left(\sqrt{n_{\hat{\vect{x}}}}/k\right) &\text{if}\; n_{\hat{\vect{x}}} \text{ is non-constant}\\ &\text{and sublinear in } k \\
            \Theta\left(1/\sqrt{k}\right) & \text{if}\; n_{\hat{\vect{x}}} = ck \text{ for } c \in (0, 1]
        \end{cases} \qquad
         \mu_1(\hat{\vect{x}}) \in \begin{cases}
            \Theta(1) & \text{if}\; n_{\hat{\vect{x}}} \text{ is constant} \\
            \Theta\left(1/\sqrt{n_{\hat{\vect{x}}}}\right) & \text{if}\; n_{\hat{\vect{x}}} \text{ is non-constant}\\ &\text{and sublinear in } k\\
            \Theta\left(1/\sqrt{k}\right) & \text{if}\; n_{\hat{\vect{x}}} = ck \text{ for } c\in(0,1) \\ 
            \Theta\left(1/k\right) & \text{if}\; n_{\hat{\vect{x}}}=k
        \end{cases}
        $$
        \item $\sigma^2(\hat{\vect{x}}) \in \mathcal{O}\left(1/k\right)$
    \end{itemize}
\end{restatable}

The proof of \Cref{lemm:orders} can be found in \Cref{app:proof_orders}. In light of these asymptotic results, the uniform bound of \Cref{eq:ensemble_uniform} behaves like $\mathcal{O}(k\log k)$. Indeed, this follows directly from the fact that the maximum of the reciprocals of the means occurs for $\mu_0(\hat{\vect{x}})$ when $n_{\hat{\vect{x}}}$ is constant which yields a bound of order $\mathcal{O}(k^2)$. This simple observation is enough to conclude the proof of \Cref{thm:ensemble}.

\section{Experiments}
We present two experiments that empirically demonstrate our theoretical results. We train a two-layer fully connected feed-forward network with a hidden layer of width 50,000 only using standard basis vectors\footnote{The source code for these experiments is available at \href{https://github.com/opallab/permlearn}{https://github.com/opallab/permlearn}.}. Our goal is to learn some random permutation on normalized binary inputs of length $k$. For testing, we evaluate the performance on 1000 random vectors with $n_{\hat{\vect{x}}}=2$ nonzero entries. To match the assumptions of our theoretical analysis, we initialize the weights exactly as given in \Cref{sec:prelim} with $\sigma_\omega=1$ and $\sigma_b = 0$. 

\textbf{1. Number of models to achieve $90\%$ accuracy.}
Our first experiment examines how many independently trained models need to be averaged to achieve a testing accuracy of 90\% as a function of the input length $k$.

\textbf{2. Accuracy vs ensemble size.}
Our second experiment fixes $k\in\{5, 10, 15, 20, 25, 30\}$ and examines how the post-rounding accuracy varies as the ensemble size increases.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{plots/experiments.pdf}
    \caption{Experiment 1 (left) presents the required number of models (ensemble size) to achieve 90\% accuracy as a function of input size $k$, compared to the theoretical bound from \Cref{eq:ensemble_uniform} with the appropriate $\delta$. Experiment 2 (right) presents accuracy as a function of ensemble size for different input sizes $k$. }
    \label{fig:experiments}
\end{figure}

In \Cref{fig:experiments}, the left plot presents the results of Experiment 1 compared to the bound established in \Cref{eq:ensemble_uniform} for the appropriate choice of $\delta$ to guarantee $90\%$ accuracy for all test inputs simultaneously. We observe that the empirical ensemble size remains below the theoretical bound, and both curves follow a similar pattern as a function of $k$. For Experiment 2, the results on the right plot of \Cref{fig:experiments} illustrate the convergence to perfect accuracy as a function of the ensemble size for different input sizes $k$. As shown, larger input sizes require more models to achieve perfect accuracy, but with a sufficient number of models, all instances eventually reach perfect accuracy.

\section{Conclusion and Future Work}
We have demonstrated that, within the NTK framework, an ensemble of simple two-layer fully connected feed-forward neural networks can learn any fixed permutation on binary inputs of size $k$ with arbitrarily high probability from a training set of logarithmic size, namely, the standard basis vectors. Our analysis shows that the network’s output in the infinite width limit converges to a Gaussian process whose mean vector contains precise, sign-based information about the ground truth output. Furthermore, we quantified the number of independent runs required to achieve any desired level of post-rounding accuracy, showing a linearithmic dependence on the input size $k$ for test input, and a quadratic dependence for all input types. In terms of future work we identify a few exciting research directions:
\begin{itemize}
    \item \textbf{Generalization Beyond Permutations.} Although we focused on learning permutations, it would be interesting to extend our approach to a broader class of functions.
    \item \textbf{Wider Range of Architectures.} Our analysis centered on a two-layer fully connected feed-forward network. An important direction is exploring whether more sophisticated architectures like multi-layer networks, CNNs, RNNs, or Transformers can exhibit similar exact learning results.
    \item \textbf{Exact PAC-Like Framework for NTK Regime.} Finally, a crucial next step is to develop a more general PAC-like framework for exact learning guarantees specifically under the NTK regime. This would utilize the infinite-width limit and standard gradient-based training dynamics to give conditions under which neural architectures can learn global transformations with provable exactness. 
\end{itemize}

\section*{Acknowledgements}

K.~Fountoulakis would like to acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Cette recherche a \'et\'e financ\'ee par le Conseil de recherches en sciences naturelles et en g\'enie du Canada (CRSNG), [RGPIN-2019-04067, DGECR-2019-00147].

G.~Giapitzakis would like to acknowledge the support of the Onassis Foundation - Scholarship ID: F ZU 020-1/2024-2025.
