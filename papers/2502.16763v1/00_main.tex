\documentclass{article}
\usepackage[utf8]{inputenc}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\setlength{\parindent}{0em}
\setlength{\parskip}{0.65em}

%
\input{math_commands.tex}

\usepackage{minitoc}
\usepackage[bottom]{footmisc}


\usepackage{cancel}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{microtype}

\usepackage{subcaption}
\usepackage{dirtytalk}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epigraph}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage[nice]{nicefrac}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{natbib}

\usepackage{xfrac}
\usepackage{framed}
\usepackage{bbm}
\usepackage{comment}
\usepackage{thmtools, thm-restate}

\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\renewcommand \thepart{}
\renewcommand \partname{}

\makeatletter
\setlength{\@fptop}{0pt}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother

%
%
%
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=blue}

\usepackage[capitalize,noabbrev,nameinlink]{cleveref}

%

\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\definecolor{green}{HTML}{549D54}

\makeatletter
\setlength{\@fptop}{0pt}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother

\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
                                    {-2.5ex \@plus -1ex \@minus -.2ex} %
                                    {1.5ex \@plus.2ex} %
                                    {\normalfont\Large\bfseries}}
\makeatother

\makeatletter
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
                                    {-1.5ex \@plus -1ex \@minus -.2ex} %
                                    {1ex \@plus .2ex} %
                                    {\normalfont\large\bfseries}}
\makeatother

\title{Exact Learning of Permutations for Nonzero Binary Inputs with Logarithmic Training Size and Quadratic Ensemble Complexity}

\author{
    George Giapitzakis\qquad
    Artur Back de Luca\\
    Kimon Fountoulakis\\
    \vspace{-1mm}\\
    \normalsize{University of Waterloo}\\
    \vspace{-3mm}\\
    \normalsize{\texttt{\{\href{mailto:ggiapitz@uwaterloo.ca}{ggiapitz}, \href{mailto:abackdel@uwaterloo.ca}{abackdel}, \href{mailto:kimon.fountoulakis@uwaterloo.ca}{kimon.fountoulakis}\}@uwaterloo.ca}}\\
}

\date{}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%
\theoremstyle{plain}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]
\theoremstyle{plain}
\newtheorem{remark}{Remark}[section]

\newcommand{\FF}{\ensuremath{\mathbb{F}}}
\newcommand{\ZZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\RR}{\ensuremath{\mathbb{R}}}
\newcommand{\CC}{\ensuremath{\mathbb{C}}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\mlp}{\mathchoice{\mbox{MLP}}{\mbox{MLP}}{\mbox{{\tiny MLP}}}{\mbox{{\tiny MLP}}}}
%
\newcommand{\hardmax}{\mathrm{hardmax}}

\newcommand\hA{\hat{A}}
\newcommand\hX{\hat{X}}
\newcommand\hY{\hat{Y}}
\newcommand\hG{\hat{G}}
\newcommand\hw{\hat{w}}
\newcommand\mnorm[1]{\left\Vert#1\right\Vert_{2,\infty}}
\newcommand\norm[1]{\left\Vert#1\right\Vert_{2}}
\newcommand\normOne[1]{\left\Vert#1\right\Vert_{1}}
\newcommand\norminf[1]{\left\Vert#1\right\Vert_{\infty}}
\newcommand{\BigPhi}{\scalebox{1.25}[1]{$\Phi$}}

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\relu}[1]{\mathrm{ReLU}(#1)}

\usepackage[textsize=tiny]{todonotes}


%
\begin{document}
\maketitle

\def\thefootnote{*}
%
\def\thefootnote{\arabic{footnote}}

\vspace{-5mm}

\begin{abstract}
The ability of an architecture to realize permutations is quite fundamental. For example, Large Language Models need to be able to correctly copy (and perhaps rearrange) parts of the input prompt into the output. Classical universal approximation theorems guarantee the existence of parameter configurations that solve this task but offer no insights into whether gradient-based algorithms can find them. In this paper, we address this gap by focusing on two-layer fully connected feed-forward neural networks and the task of learning permutations on nonzero binary inputs. We show that in the infinite width Neural Tangent Kernel (NTK) regime, an ensemble of such networks independently trained with gradient descent on only the $k$ standard basis vectors out of $2^k - 1$ possible inputs successfully learns \emph{any} fixed permutation of length $k$ with arbitrarily high probability. By analyzing the exact training dynamics, we prove that the networkâ€™s output converges to a Gaussian process whose mean captures the ground truth permutation via sign-based features. We then demonstrate how averaging these runs (an ``ensemble'' method) and applying a simple rounding step yields an arbitrarily accurate prediction on any possible input unseen during training. Notably, the number of models needed to achieve exact learning with high probability (which we refer to as \emph{ensemble complexity}) exhibits a linearithmic dependence on the input size $k$ for a single test input and a quadratic dependence when considering all test inputs simultaneously.

\end{abstract}

%

\doparttoc %
\faketableofcontents %

%
\parttoc %

%

\input{01_paper}

\bibliography{references}
\bibliographystyle{plainnat}

\newpage
\appendix

\part{Appendix} %
\parttoc %
\input{99_appendix}


\end{document}
