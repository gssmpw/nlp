\section{Literature Review}
The NTK framework ____ is a popular framework designed to provide insights into the continuous-time gradient descent (gradient flow) dynamics of fully connected feed-forward neural networks with widths tending to infinity. The initial results have been extended to discrete gradient descent ____ as well as generalized to more architecture families like RNNs and Transformers ____. While the general NTK theory has been extensively studied, very few works use the framework to study the performance of neural architectures on specific tasks. Notably, the framework has been used to attempt to explain the ``double descent'' phenomenon ____ observed in deep architectures ____. It has also been used to study the behavior of Residual Networks ____ and Physics-Informed Neural Networks ____. To the best of our knowledge, the closest work to ours is ____, where the authors use results from the general NTK theory to prove that Transformers can provably generalize out-of-distribution on template matching tasks. Furthermore, they show that feed-forward neural networks fail to generalize to unseen symbols for any template task, including copying, a special case of a permutation learning setting. In our paper, we show that feed-forward neural networks can indeed learn to copy without error with high probability. At first, this might seem to contradict ____. However, the key difference between our work and the work in ____ lies in the problem setting. The setting in ____ poses no restrictions to the input, whereas we require binary encoded input. Therefore, the model has access to all symbols during training. In particular, we specifically choose the training set, i.e. the standard basis, so that each symbol appears in every position. 
The restrictions above are what enable exact permutation learning.      

Lastly, various studies highlight the expressive power of neural networks through simulation results. For instance, ____ demonstrates the Turing completeness of recurrent neural networks (RNNs), while ____ introduces specific RNN constructions capable of solving the shortest paths problem and approximating solutions to the knapsack problem. Moreover, other simulation studies on Transformers have established their Turing completeness ____ and showcased constructive solutions for linear algebra, graph-related problems ____, and parallel computation ____. However, the results in these works are existential, and none considers the dynamics of the training procedure.