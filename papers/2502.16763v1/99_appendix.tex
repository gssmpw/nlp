\section{Useful Results}
\label{app:lemm}
In this section, we state and prove some results from linear algebra and probability theory that were used to derive our main results. The following three results were used to derive the required NTK kernels:
\begin{theorem}[\citealt{sherman}]
\label{thm:sherman_mor}
    Suppose $A\in \RR^{n\times n}$ is an invertible matrix and $\vect{u},\vect{v} \in \RR^n$. Then $A + \vect{u}\vect{v}^\top$ is invertible if and only if $1+\vect{v}^\top A^{-1} \vect{u} \neq 0$. In this case, 
    $$(A + \vect{u}\vect{v}^\top)^{-1}=A^{-1}-\frac{A^{-1}\vect{u}\vect{v}^\top A^{-1}}{1+\vect{v}^\top A^{-1} \vect{u}}$$
\end{theorem}

\begin{lemma}
\label{lemm:kron}
    Suppose $A \in \RR^{n \times n}$, $\vect{a} = [a_1,\dots,a_n]^\top \in \RR^{n}$ and $\vect{y} \in \RR^{n^2}$ is given by 
    $$\vect{y} = \begin{bmatrix}
        A\vect{e_1} \\
        A\vect{e_2} \\
        \vdots \\
        A\vect{e_n}
    \end{bmatrix}$$
    Then $(\vect{a}^\top \otimes I_n)\vect{y} = A\vect{a}$.
\end{lemma}

\begin{proof}
    Both the right-hand side and left-hand side are vectors of size $n$. We shall show that for each $i=1,2,\dots, n$, the $i$-th entry of the left-hand side is equal to the $i$-th entry of the right-hand side. The $i$-th entry of $A\vect{a}$ is simply given by 
    $$(A\vect{a})_i = \sum_{j=1}^n A_{ij} a_j$$
    Now let $C = \vect{a}^\top \otimes I_n \in \RR^{n \times n^2}$. We can easily observe that $C$ has the following block structure: 
    $$C = \begin{bmatrix}
        a_1 I_n & a_2 I_n & \dots & a_n I_n
    \end{bmatrix}$$
    The $i$-th row of $C$ is therefore given by 
    $$C_i = \begin{bmatrix}
        a_1 \vect{e_i}^\top & a_2 \vect{e_i}^\top & \dots & a_n \vect{e_i}^\top 
    \end{bmatrix} \in \RR^{1\times n^2}$$
    Thus, the $i$-th entry of the left-hand side is given by 
    $$C_i \vect{y} = \sum_{j=1}^n a_j \vect{e_i}^\top A\vect{e_j} = \sum_{j=1}^n A_{ij}a_j = (A\vect{a})_i$$
    since $\vect{e_i}^\top A \vect{e_j} = A_{ij}$.
    This concludes the proof.
    
\end{proof}

\begin{lemma}
    \label{lemm:relu_exp}
    For variables $(u, v) \sim \mathcal{N}(\vect{0}, \Sigma)$ where 
    $$\Sigma = \begin{bmatrix}
        \sigma_u^2 & \sigma_{uv}^2 \\
        \sigma_{vu}^2 & \sigma_v^2
    \end{bmatrix}$$
    the following two identities hold:
    
    $$\mathbb{E}\left[\relu{u}\cdot \relu{v}\right] = \frac{\sigma_u \sigma_v\left(\cos(\theta)\cdot(\pi - \theta) + \sin(\theta)\right)}{2\pi}$$
    and
    $$\mathbb{E}\left[1_{[u > 0]} \cdot 1_{[v>0]}\right] = \frac{\pi - \theta}{2\pi}$$
    where 
    $$\theta = \arccos\left(\frac{\sigma_{uv}^2}{\sigma_u \cdot \sigma_v}\right)$$ and $1_{X}$ denotes the indicator function of the set $X$.
\end{lemma}

 For a proof of  \Cref{lemm:relu_exp} refer to Section 4.3 of \citet{golikov2022neuraltangentkernelsurvey}.
 
The last result is a concentration bound for sums of Gaussian random variables which we used to derive our ensemble complexity bounds:

\begin{lemma}
\label{lemm:concentration}
    Let $X_1,X_2,\dots,X_n$ be independent Gaussian random variables with mean $\mu$ and variance $\sigma^2$ and let $\overline{X}_n=\sum_{i=1}^n X_i$. Then for all $t>0$, we have:
    $$\mathbb{P}\left\{|\overline{X}_n - \mu| \geq t \right\} \leq 2 \exp\left(- \frac{nt^2}{2\sigma^2}\right)$$
\end{lemma}

\begin{proof}
We will use the Chernoff technique. Let $\lambda > 0$. We have $\overline{X}_n - \mu  \sim \mathcal{N}\left(0, \frac{\sigma^2}{n}\right)$ and so by symmetry and Markov's inequality we get: 
\begin{align}
\label{eq:lambda}
\mathbb{P}\left\{|\overline{X}_n-\mu| \geq t\right\} &= 2\mathbb{P}\left\{\overline{X}_n-\mu \geq t\right\} = 2\mathbb{P}\left\{e^{\lambda(\overline{X}_n-\mu) }\geq e^{\lambda t}\right\} \leq e^{-\lambda t}\cdot \mathbb{E}\left[e^{\lambda(\overline{X}_n-\mu)}\right]
\end{align}
The expectation on the right-hand side is equal to the moment-generating function of a normal distribution with mean $0$ and variance $\sigma^2/n$ and so it is equal to $\exp\left(\frac{\sigma^2 \lambda^2}{2n}\right)$. Now let 
$$\phi(\lambda) = \exp\left(\frac{\sigma^2 \lambda^2}{2n} - \lambda t\right)$$
be the right-hand side of \Cref{eq:lambda}. Minimizing $\phi(\lambda)$ with respect to $\lambda$ we find that the minimum occurs at $\lambda^* = \frac{nt}{\sigma^2}$ and plugging this back into \Cref{eq:lambda} gives the required bound.
\end{proof}

\section{Recursive formulas for the NTK and NNGP Kernels}
\label{app:recursion}

In this section, we provide the recursive formulas used to derive the NNGP and limit NTK kernels defined in \Cref{sec:prelim}. In the following, the notation is consistent with that of \Cref{sec:prelim}.

\paragraph{The NNGP Kernel} As the width of the network tends to infinity, it has been shown that the network at initialization behaves as a Gaussian process. That is, the stacked pre-activation outputs at each layer $l$ at initialization, $f_0^l(\mathcal{X})$ converges to a multivariate Gaussian distribution $\mathcal{N}(\vect{0}, \mathcal{K}^l(\mathcal{X}, \mathcal{X}))$ where the covariance matrix $\mathcal{K}$ can be computed using the formula 
$$\mathcal{K}^l(\vect{x}, \vect{x'}) = \Tilde{\mathcal{K}}^l(\vect{x}, \vect{x'}) I_{n_l}$$
where 
\begin{equation}
\tilde{\mathcal{K}}^l\left(\vect{x}, \vect{x}^{\prime}\right)=\sigma_\omega^2 \mathcal{T}\left(\left[\begin{array}{ll}
\tilde{\mathcal{K}}^{l-1}(\vect{x}, \vect{x}) & \tilde{\mathcal{K}}^{l-1}\left(\vect{x}, \vect{x}^{\prime}\right) \\
\tilde{\mathcal{K}}^{l-1}\left(\vect{x}, \vect{x}^{\prime}\right) & \tilde{\mathcal{K}}^{l-1}\left(\vect{x}^{\prime}, \vect{x}^{\prime}\right)
\end{array}\right]\right)+\sigma_b^2
\label{eq:recur_K}
\end{equation}
with base case $\tilde{\mathcal{K}}^1(\vect{x}, \vect{x'}) = \sigma_\omega^2 \cdot \frac{1}{n_0} \vect{x}^\top \vect{x'} + \sigma_b^2$.
The function $\mathcal{T}$ maps a $2\times 2$ positive semi-definite matrix $\Sigma$ to $\mathbb{E}[\phi(u)\phi(v)]$ for $(u, v)\sim \mathcal{N}(\vect{0}, \Sigma)$.
For details see \cite{lee2018deep} and Appendix E. of \cite{NEURIPS2019_0d1a9651}.

\paragraph{Limit NTK} It can be shown that in the infinite width limit, the empirical NTK at initialization $\hat{\Theta}^l_0$ converges to a deterministic kernel $\Theta^l$ given by the formula
$$\Theta^l(\vect{x}, \vect{x'}) = \tilde{\Theta}^l(\vect{x}, \vect{x'}) I_{n_l}$$
where 
\begin{align}
    \tilde{\Theta}^l&\left(\vect{x}, \vect{x}^{\prime}\right) =\tilde{\mathcal{K}}^l\left(\vect{x}, \vect{x}^{\prime}\right)+ \sigma_\omega^2 \tilde{\Theta}^{l-1}\left(\vect{x}, \vect{x}^{\prime}\right) \dot{\mathcal{T}}\left(\left[\begin{array}{cc}
\tilde{\mathcal{K}}^{l-1}(\vect{x}, \vect{x}) & \tilde{\mathcal{K}}^{l-1}\left(\vect{x}, \vect{x}^{\prime}\right) \\
\tilde{\mathcal{K}}^{l-1}\left(\vect{x}, \vect{x}^{\prime}\right) & \tilde{\mathcal{K}}^{l-1}\left(\vect{x}^{\prime}, \vect{x}^{\prime}\right)
\end{array}\right]\right)
\label{eq:recur_Theta}
\end{align}
with base case $\tilde{\Theta}^1(\vect{x},\vect{x'})=\tilde{\mathcal{K}}^1(\vect{x}, \vect{x'})$, and $\tilde{\mathcal{K}}^l$ is as defined in the previous paragraph. The function $\dot{\mathcal{T}}$ maps a $2\times 2$ positive semi-definite matrix $\Sigma$ to $\mathbb{E}[\phi'(u)\phi'(v)]$ for $(u, v) \sim \mathcal{N}(\vect{0}, \Sigma)$. For more details refer to \cite{NEURIPS2018_5a4be1fa} and Appendix E. of \cite{NEURIPS2019_0d1a9651}

\begin{remark}
In the case of ReLU activation, \Cref{lemm:relu_exp} gives closed-form expressions for both $\mathcal{T}$ and $\mathcal{\dot{T}}$.
\end{remark}

\section{Derivations of Train and Test NTK Matrices}
\label{app:derivations}
This section serves as a supplement to \Cref{sec:mean_var} of the main paper. Here, we provide an outline of how the formulas for the train and test NTK matrices on the main paper can be derived using the results of \Cref{app:recursion}. For clarity, we separate the derivation of train and test NTK matrices into two subsections.

\subsection{Train NTK}
Since the training set consists of the standard basis vectors, to get the train NTK kernel we need to compute $\mathcal{K}^{l}(\vect{e_i}, \vect{e_j})$ and $\Theta^{l}(\vect{e_i}, \vect{e_j})$ for $l = 1, 2$ and $i, j \in [k]$. We will use the results of \Cref{app:recursion} to compute them. We can easily see that the base case gives 
\begin{equation}
    \tilde{\mathcal{K}}^1(\vect{e_i}, \vect{e_j})=\begin{cases}
        \frac{\sigma_\omega^2}{k} + \sigma_b^2
 & \text{if } i=j \\
 \sigma_b^2 & \text{if } i \neq j\end{cases}
\end{equation}
Using the recursive formula of \Cref{eq:recur_K} we get for $i \neq j$
\begin{align}
    \tilde{\mathcal{K}}^2\left(\vect{e_i}, \vect{e_j}\right)&=\sigma_\omega^2 \mathcal{T}\left(\begin{bmatrix}
\frac{\sigma_\omega^2}{k}+\sigma_b^2 & \sigma_b^2 \\
\sigma_b^2 & \frac{\sigma_\omega^2}{k}+\sigma_b^2 
\end{bmatrix}\right)+\sigma_b^2 \nonumber \\
&= \sigma_\omega^2 \left(\frac{\sigma_\omega^2}{k} + \sigma_b^2 \right)\cdot \frac{\cos \theta \cdot (\pi - \theta) + \sin \theta}{2\pi} + \sigma_b^2
\end{align}
where 
\begin{equation}
    \theta = \arccos\left(\frac{\sigma_b^2}{\frac{\sigma_\omega^2}{k} + \sigma_b^2}\right)
\end{equation} Similarly, for $i = j$ we have 
\begin{align}
    \tilde{\mathcal{K}}^2\left(\vect{e_i}, \vect{e_j}\right)&=\sigma_\omega^2 \mathcal{T}\left(\begin{bmatrix}
\frac{\sigma_\omega^2}{k}+\sigma_b^2 & \frac{\sigma_\omega^2}{k}+\sigma_b^2 \\
\frac{\sigma_\omega^2}{k}+\sigma_b^2 & \frac{\sigma_\omega^2}{k}+\sigma_b^2 
\end{bmatrix}\right)+\sigma_b^2 \nonumber \\
&= \frac{\sigma_\omega^2}{2} \left(\frac{\sigma_\omega^2}{k} + \sigma_b^2 \right) + \sigma_b^2
\end{align}
Hence, substituting $\sigma_\omega=1$ and $\sigma_b=0$ above we get 
\begin{equation}
    \mathcal{K}(\mathcal{X}, \mathcal{X}) = \left(\left(\frac{d}{2} - c\right)I_k + c\vect{1}\vect{1}^\top\right) \otimes I_k \in \RR^{k^2 \times k^2} 
\end{equation}
where $d$ and $c$ are as in \Cref{eq:c_and_d}. In both cases, we used \Cref{lemm:relu_exp} to compute the function $\mathcal{T}$. In a similar fashion, following the recursive formula of \Cref{eq:recur_Theta} and using \Cref{lemm:relu_exp} we arrive at the limit NTK kernel of \Cref{eq:limit_NTK}.
\subsection{Test NTK}
To obtain an expression of the test NTK we again need to calculate $\mathcal{K}^{l}(\hat{\vect{x}}, \vect{e_i})$ and $\Theta^{l}(\hat{\vect{x}}, \vect{e_i})$ for any $\hat{\vect{x}} \in \mathbb{K}$,  $l = 1,2$ and $i \in [k]$. Consider an arbitrary test input  $\hat{\vect{x}} \in \mathbb{K}$ and a basis vector $\vect{e_i}$. From the base case of the recursion we have 
\begin{equation}
    \tilde{\mathcal{K}}^1(\vect{\hat{x}}, \vect{e_i}) =\begin{cases}
        \frac{\sigma_\omega^2 \hat{\vect{x}}_i}{k} + \sigma_b^2 & \text{if } \hat{\vect{x}}_i > 0 \\
        \sigma_b^2 & \text{if } \hat{\vect{x}}_i=0
    \end{cases}
\end{equation}
and (since $\hat{\vect{x}}$ is normalized)
\begin{equation}
    \tilde{\mathcal{K}}^1(\hat{\vect{x}}, \hat{\vect{x}}) = \frac{\sigma_\omega^2}{k} + \sigma_b^2
\end{equation}
where $n_{\hat{\vect{x}}}$ denotes the number of nonzero entries of $\hat{\vect{x}}$.
Using \Cref{eq:recur_K} we get
\begin{align}
    \tilde{\mathcal{K}}^2(\hat{\vect{x}},\vect{e_i}) &= \sigma_\omega^2 \mathcal{T}\left(\begin{bmatrix}
\frac{\sigma_\omega^2}{k}+\sigma_b^2 & \frac{\sigma_\omega^2\hat{\vect{x}}_i}{k}+\sigma_b^2 \\
\frac{\sigma_\omega^2 \hat{\vect{x}}_i}{k}+\sigma_b^2 & \frac{\sigma_\omega^2}{k}+\sigma_b^2 
\end{bmatrix}\right)+\sigma_b^2 \nonumber \\ &=
\sigma_\omega^2 \left(\frac{\sigma_\omega^2}{k} + \sigma_b^2\right) \cdot \frac{\cos \hat{\theta}_i (\pi - \hat{\theta}_i) + \sin \hat{\theta}_i}{2\pi} + \sigma_b^2
\end{align}
where 
\begin{equation}
 \hat{\theta_i} = \arccos\left(\frac{\sigma_\omega^2\frac{\hat{\vect{x}}_i}{k}+\sigma_b^2}{\frac{\sigma_\omega^2}{k}+\sigma_b^2}\right)
\end{equation}
The second equality follows from \Cref{lemm:relu_exp}. Substituting $\sigma_\omega = 1$ and $\sigma_b=0$ we therefore have 
\begin{equation}
    \mathcal{K}(\hat{\vect{x}}, \mathcal{X}) = \mathbf{g}^\top \otimes I_k \in \RR^{k \times k^2}
\end{equation}
with $\mathbf{g} \in \RR^k$ as in \Cref{eq:vect_g}. Similarly, we obtain the expression for the test NTK of \Cref{eq:testntkkron}.

%
\section{Proof of \Cref{lemm:orders}}
\label{app:proof_orders}
In this section, we analyze the mean and variance of the predictions under the limit NTK, expressing them as functions of the test vector $\hat{\vect{x}}$. Specifically, we characterize their dependence on the vector length $k$ and the number of nonzero elements $n_{\hat{\vect{x}}}$.
We establish that the variance of the predictions scales as $\mathcal{O}\left(1/k\right)$ while the mean coordinates exhibit two different behaviors: they scale with $\Theta(\nicefrac{\sqrt{n_{\hat{\vect{x}}}}}{k})$ when $P^*\hat{\vect{x}}=0$ and $\Theta(\nicefrac{1}{k})$ when $P^*\hat{\vect{x}}>0$.
\paragraph{Preliminary quantities}
We begin by rewriting several useful expressions from \Cref{sec:mean_var} and adding other relevant expressions to complement them. For $\sigma_w = 1$ and $\sigma_b=0$ as defined in \Cref{sec:desc}, recall:
\begin{equation*}
    d=\frac{1}{k}\quad \textrm{ and } \quad c=\frac{1}{2\pi k}
\end{equation*}
We further introduce 
\begin{equation*}
    d^\prime = \frac{d}{2} = \frac{1}{2k} \quad \textrm{ and }\quad c^\prime = c= \frac{1}{2\pi k}
\end{equation*}
Additionally, we rewrite \Cref{eq:binary_theta} in an alternative form:
\begin{equation*}
        \hat{\theta}_i = \begin{cases}
        \arccos\left(\frac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right) &\textrm{if } \hat{\vect{x}}_i \neq 0\\
        \frac{\pi}{2} &\textrm{otherwise}
    \end{cases}
\end{equation*}

From this, we directly obtain the cosine and sine of $\hat{\theta}_i$:
\begin{equation*}
    \cos\hat{\theta}_i = \begin{cases}
        \frac{1}{\sqrt{n_{\hat{\vect{x}}}}} &\textrm{if } \hat{\vect{x}}_i \neq 0\\
        0 &\textrm{otherwise}
    \end{cases}
    \qquad 
    \sin\hat{\theta}_i = \begin{cases}
        \sqrt{1-\frac{1}{n_{\hat{\vect{x}}}}} &\textrm{if } \hat{\vect{x}}_i \neq 0\\
        1 &\textrm{otherwise}
    \end{cases}
\end{equation*}

Using these quantities, we rewrite \Cref{eq:vect_g} as:
\begin{equation*}
    \mathbf{g}_i = \begin{cases}
        \frac{1}{2\pi k\sqrt{n_{\hat{\vect{x}}}}}\left(\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right) + \sqrt{n_{\hat{\vect{x}}}-1}\right) &\textrm{if } \hat{\vect{x}}_i \neq 0\\
        \frac{1}{2\pi k} &\textrm{otherwise}
    \end{cases}
\end{equation*}
and \Cref{eq:vect_f} as
\begin{equation*}
    \mathbf{f}_i = \begin{cases}
        \mathbf{g}_i + \frac{\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)}{2\pi k \sqrt{n_{\hat{\vect{x}}}}} &\textrm{if } \hat{\vect{x}}_i \neq 0,\\
        \mathbf{g}_i &\textrm{otherwise}
    \end{cases}
\end{equation*}

\subsection{Computing the mean $\mu(\hat{\vect{x}})$}
We first recall the expression for the mean:
\begin{equation*}
    \mu(\hat{\vect{x}}) = P^*\left(\frac{1}{d-c} I_k - \frac{c}{(d-c)(d-c+ck)}\vect{1}\vect{1}^\top\right)\mathbf{f}
\end{equation*}
which we decompose into two terms:
\begin{equation}
    \label{eq:mu_decomposed}
    \mu(\hat{\vect{x}}) = \frac{P^*\mathbf{f}}{d-c}  - \frac{c\vect{1}\vect{1}^\top\mathbf{f}}{(d-c)(d-c+ck)}
\end{equation}
The second term simplifies since $\vect{1}$ is the vector of all ones, effectively absorbing the permutation $P^*$.
For the first term in \Cref{eq:mu_decomposed}, we obtain:
\begin{equation*}
        \frac{(P_j^*)^\top\mathbf{f}}{d-c} = \begin{cases}
        \frac{1}{2\pi-1}\left(
        \frac{\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)}{\sqrt{n_{\hat{\vect{x}}}}}
        +\sqrt{1-\frac{1}{\sqrt{n_{\hat{\vect{x}}}}}}\right)
         &\textrm{if } \hat{\vect{x}}_j \neq 0\\
        \frac{1}{2\pi-1} &\textrm{otherwise}
    \end{cases}
\end{equation*}
The second term is a constant vector with coefficient:
\begin{equation*}
    \frac{c\vect{1}\vect{1}^\top\mathbf{f}}{(d-c)(d-c+ck)} = \frac{1}{(2\pi-1)(2\pi-1+k)}\left(\frac{n_{\hat{\vect{x}}}\left(\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)}{2 \sqrt{n_{\hat{\vect{x}}}}}+n_{\hat{\vect{x}}}\sqrt{1-\frac{1}{n_{\hat{\vect{x}}}}}+k-n_{\hat{\vect{x}}}\right)
\end{equation*}


We now evaluate each case of the mean separately.

\textbf{Case 1:} For $\mu_0(\hat{\vect{x}})$, that is, when $P^*\hat{\vect{x}}=0$, we have:
\begin{align}
\label{eq:mu_0}
    \mu_0(\hat{\vect{x}}) = &\frac{1}{2\pi-1}\left(
    1 - \frac{k-n_{\hat{\vect{x}}}}{2\pi + k -1} - \frac{2\pi kn_{\hat{x}}}{\left(2\pi+k-1\right)\left(\pi k\sqrt{n_{\hat{\vect{x}}}}\right)}\left(\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)\right)\nonumber\\
    -&\frac{1}{2\pi-1}\left(\frac{2\pi kn_{\hat{x}}}{\left(2\pi + k -1\right)\left(2\pi k\right)}\sqrt{1-\frac{1}{\sqrt{n_{\hat{\vect{x}}}}}}\right)
\end{align}

This expression behaves like $\Theta\left(\sqrt{n_{\hat{\vect{x}}}}/k\right)$, and setting $n_{\hat{\vect{x}}}$ to different regimes yields the bounds established in \Cref{lemm:orders}, namely:
\begin{equation*}
     \mu_0(\hat{\vect{x}}) \in 
        \begin{cases}
            \Theta\left(1/k\right) & \text{if}\; n_{\hat{\vect{x}}} \text{ is constant} \\
            \Theta\left(\sqrt{n_{\hat{\vect{x}}}}/k\right) &\text{if}\; n_{\hat{\vect{x}}} \text{ is non-constant}\\ &\text{and sublinear in } k \\
            \Theta\left(1/\sqrt{k}\right) & \text{if}\; n_{\hat{\vect{x}}} = ck \text{ for } c \in (0, 1]
        \end{cases}
\end{equation*}

\textbf{Case 2:} For $\mu_1(\hat{\vect{x}})$, that is, when $P^*\hat{\vect{x}}>0$, we have:
\begin{align}
\label{eq:mu_1}
    \mu_1(\hat{\vect{x}}) = &\frac{2\pi-1}{2\pi+k-1}\left(\frac{2}{\left(2\pi-1\right)\sqrt{n_{\hat{\vect{x}}}}}\left(\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)+\frac{1}{\left(2\pi-1\right)}\sqrt{1-\frac{1}{\sqrt{n_{\hat{\vect{x}}}}}}\right)\nonumber\\ &+ \frac{k-n_{\hat{\vect{x}}}}{2\pi+k-1}\left(\frac{2}{\left(2\pi-1\right)\sqrt{n_{\hat{\vect{x}}}}}\left(\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)+\frac{1}{\left(2\pi-1\right)}\sqrt{1-\frac{1}{\sqrt{n_{\hat{\vect{x}}}}}}-\frac{1}{\left(2\pi-1\right)}\right)
\end{align}
When setting $n_{\hat{\vect{x}}}=k$, we note that the second term becomes zero and the first term becomes $\Theta\left(1/k\right)$.
Alternatively, the first term in \Cref{eq:mu_1} is $\Theta\left(1/k\right)$ and the second term is $\Theta\left(1/\sqrt{n_{\hat{\vect{x}}}}\right)$, implying $\mu(\hat{\vect{x}})_1 \in \Theta\left(1/\sqrt{n_{\hat{\vect{x}}}}\right)$. 
Setting $n_{\hat{\vect{x}}}$ to different regimes yields the bounds established in \Cref{lemm:orders}, namely:
\begin{equation*}
\mu_1(\hat{\vect{x}}) \in \begin{cases}
            \Theta\left(1/\sqrt{n_{\hat{\vect{x}}}}\right) & \text{if}\; n_{\hat{\vect{x}}} \text{ is non-constant}\\ &\text{and sublinear in } k,\\
            \Theta\left(1/\sqrt{k}\right) & \text{if}\; n_{\hat{\vect{x}}} = ck \text{ for } c\in(0,1) \\ 
            \Theta\left(1/k\right) & \text{if}\; n_{\hat{\vect{x}}}=k
        \end{cases}
\end{equation*}

\subsection{Computing the variance $\Sigma(\hat{\vect{x}})$}
We begin by recalling the expression for the variance (substituting $d^\prime$ and $c^\prime$)\footnote{Even though in this case $d^\prime = d/2$ and $c^\prime = c$ we deliberately introduce the new variables to facilitate more general calculations for different values of $\sigma_\omega$ and $\sigma_b$.}:
\begin{equation*}
    \Sigma(\hat{\vect{x}}) = \left(d^\prime+\mathbf{f}^\top AMA \mathbf{f} - 2\mathbf{f}^\top A \mathbf{g}\right)I_k
\end{equation*}
where
\begin{equation*}
    M = \left(d^\prime-c^\prime \right)I_k + c^\prime
    \vect{1}\vect{1}^\top \quad \textrm{ and } \quad A = \frac{1}{d-c} I_k - \frac{c}{(d-c)(d-c+ck)}\vect{1}\vect{1}^\top
\end{equation*}
We aim to show that the variance is bounded by $\mathcal{O}\left(1/k\right)$. To facilitate this, we rewrite $\Sigma(\hat{\vect{x}})$ by expanding the matrix multiplication:
\begin{align*}
    \Sigma(\hat{\vect{x}}) &= d^\prime I_k+\mathbf{f}^\top \left(AMA \mathbf{f} - 2A (\mathbf{f}-\mathbf{z})\right)I_k\\
    &= d^\prime I_k+\mathbf{f}^\top \left(AMA \mathbf{f} - 2A\mathbf{f}\right)I_k + 2\mathbf{f}^\top A\mathbf{z}I_k
\end{align*}
where we define $\mathbf{z} = \mathbf{f} - \mathbf{g}$ for notational simplicity. It is straightforward to observe that the first term, $d^\prime I_k$, is bounded by $\mathcal{O}\left(1/k\right)$. Thus, we focus our attention on the remaining two terms. For the second term, we begin by showing that $AMA-2A$ has a maximum eigenvalue of $\mathcal{O}(1)$, and therefore:
$$\mathbf{f}^\top(AMA-2A)\mathbf{f}\in\mathcal{O}\left(1/k\right)$$
We begin by expressing $AMA$ in a more manageable form. A direct computation reveals:
$$AMA = uI_k - v\vect{1}\vect{1}^\top$$
where 
\begin{equation*}
 u = \frac{d^\prime - c^\prime}{\left(d-c\right)^2} \quad\textrm{ and }\quad v = \frac{1}{k}\left(\frac{d^\prime -c^\prime + kc^\prime}{\left(d -c + kc\right)^2}- \frac{d^\prime-c^\prime}{\left(d-c\right)^2}\right)
\end{equation*}
Subtracting $2A$ from this expression gives:
$$AMA-2A = \left(u-2a\right)I_k + \left(v+2b\right)\vect{1}\vect{1}^\top$$
where
\begin{equation*}
 a = \frac{d - c}{\left(d-c\right)^2} \quad\textrm{ and }\quad b = \frac{c}{\left(d-c+ck\right)\left(d-c\right)}
\end{equation*}

This matrix has two unique eigenvalues: $u-2a$ (with multiplicity $k-1$) and $u-2a + (v+2b)k$. We will show that the largest eigenvalue is $\mathcal{O}(1)$. To this end, we evaluate these two quantities, starting with $u - 2a$:
\begin{align*}
    u-2a &= \frac{d^\prime - c^\prime -2(d-c)}{(d-c)^2}\\
    &=-\frac{2\pi(3\pi-1)k}{(2\pi-1)^2}
\end{align*}
which is negative. For the second eigenvalue, we find:
\begin{align*}
    u-2a + (v+2b)k &= -\frac{2\pi(3\pi-1)k}{(2\pi-1)^2} + \frac{2\pi k\left(\pi+k-1\right)}{\left(2\pi+ k-1\right)^2}-\frac{2\pi k(\pi-1)}{\left(2\pi-1\right)^2} + \frac{4\pi k}{\left(2\pi-1\right)\left(2\pi+k-1\right)}\\
    &= \left(\frac{4\pi}{2\pi+k-1} + \frac{\pi+k-1}{(2\pi+k-1)^2}\right)k
\end{align*}
which is positive and clearly $\mathcal{O}(1)$. With this result, we can bound the multiplication by the norms of its components. Since $\lambda_{\max}(AMA - 2A) \in \mathcal{O}(1)$ and $\|\mathbf{f}\|^2 \in \mathcal{O}\left(1/k\right)$ (since each $\mathbf{f}_i \in \mathcal{O}\left(1/k\right))$, we conclude:
$$
\mathbf{f}^\top (AMA - 2A) \mathbf{f} \in \mathcal{O}\left( 1/k \right)
$$
We now turn to the third term, $2\mathbf{f}^\top A \mathbf{z}$. Expressing $\mathbf{z}$ component-wise we have:
\begin{equation*}
    \mathbf{z}_i = \begin{cases}
        \frac{\pi-\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)}{2\pi k \sqrt{n_{\hat{\vect{x}}}}} &\textrm{if } \hat{\vect{x}}_i \neq 0\\
        0 &\textrm{otherwise}
    \end{cases}
\end{equation*}

We then decompose the product $\mathbf{f}^\top A\mathbf{z}$ as:
\begin{equation}
    \label{eq:fTAz}
    \mathbf{f}^\top A\mathbf{z} = a\mathbf{f}^\top\mathbf{z} - b(\mathbf{f}^\top\mathbf{1})(\mathbf{1}^\top\mathbf{z})
\end{equation}
where
\begin{equation*}
    a = \frac{1}{d-c} \quad \textrm{ and } \quad b = \frac{2\pi k}{(2\pi-1)(2\pi+k-1)}
\end{equation*}
The first term $a\mathbf{f}^\top\mathbf{z}$ can be expressed as:
\begin{equation*}
    a\mathbf{f}^\top\mathbf{z} = \frac{1}{(2\pi -1)(2\pi k)^2}\left(2\pi - 2\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)- \sqrt{n_{\hat{\vect{x}}}-1}\right)\left(\pi - 2\arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)
\end{equation*}
which is positive and $\mathcal{O}\left(1/k\right)$. For the second term $b(\mathbf{f}^\top\vect{1})(\vect{1}^\top\mathbf{z})$, we start by expressing the  individual quantities $\mathbf{f}^\top\mathbf{1}$ and $\mathbf{1}^\top\mathbf{z}$:
\begin{equation*}
    \mathbf{f}^\top\mathbf{1} = \frac{\sqrt{n_{\hat{\vect{x}}}}}{\pi k}\left(\pi - \arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right) + \frac{\sqrt{n_{\hat{\vect{x}}}}\sqrt{n_{\hat{\vect{x}}}-1}}{2\pi k} + \frac{k-n_{\hat{\vect{x}}}}{2\pi k}
\end{equation*}
and 
\begin{equation*}
    \mathbf{1}^\top\mathbf{z} = \frac{\sqrt{n_{\hat{\vect{x}}}}}{2\pi k}\left(\pi - \arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)
\end{equation*}
Therefore, the entire term can be expressed as:
\begin{align*}
    b(\mathbf{f}^\top\vect{1})(\vect{1}^\top\mathbf{z}) = \frac{1}{2\pi k(2\pi-1)(2\pi-1+k)}\Big(& 2n\left(\pi - \arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)^2\\&+n\sqrt{n-1}\left(\pi - \arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)\\
    &+ (k-n)\sqrt{n_{\hat{\vect{x}}}}\left(\pi - \arccos\left(\nicefrac{1}{\sqrt{n_{\hat{\vect{x}}}}}\right)\right)\Big)
\end{align*}
which is positive and $\mathcal{O}\left(1/k\right)$, therefore, the subtraction of \Cref{eq:fTAz} (which is equal to $\mathbf{f}^\top A\mathbf{z}$) is $\mathcal{O}\left(1/k\right)$. Combining the bounds on all three terms, we conclude that:
$$
\Sigma(\hat{\vect{x}}) \in \mathcal{O}\left( 1/k \right)I_k
$$
completing the proof of the second part of \Cref{lemm:orders}.



