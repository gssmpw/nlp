@misc{atari2023humans,
  title={Which humans?},
  author={Atari, Mohammad and Xue, Mona J and Park, Peter S and Blasi, Dami{\'a}n and Henrich, Joseph},
  year={2023},
  publisher={PsyArXiv}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{birhane2022power,
  title={Power to the people? Opportunities and challenges for participatory AI},
  author={Birhane, Abeba and Isaac, William and Prabhakaran, Vinodkumar and Diaz, Mark and Elish, Madeleine Clare and Gabriel, Iason and Mohamed, Shakir},
  booktitle={Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
  pages={1--8},
  year={2022}
}

@article{dang2024aya,
  title={Aya expanse: Combining research breakthroughs for a new multilingual frontier},
  author={Dang, John and Singh, Shivalika and D'souza, Daniel and Ahmadian, Arash and Salamanca, Alejandro and Smith, Madeline and Peppin, Aidan and Hong, Sungjin and Govindassamy, Manoj and Zhao, Terrence and others},
  journal={arXiv preprint arXiv:2412.04261},
  year={2024}
}

@misc{durmus_towards_2023,
	title = {Towards {Measuring} the {Representation} of {Subjective} {Global} {Opinions} in {Language} {Models}},
	url = {http://arxiv.org/abs/2306.16388},
	abstract = {Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country’s perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model’s responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on.2 We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Durmus, Esin and Nyugen, Karina and Liao, Thomas I. and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and Lovitt, Liane and McCandlish, Sam and Sikder, Orowa and Tamkin, Alex and Thamkul, Janel and Kaplan, Jared and Clark, Jack and Ganguli, Deep},
	month = {June},
	year = {2023},
	note = {arXiv:2306.16388 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Durmus et al. - 2023 - Towards Measuring the Representation of Subjective.pdf:/Users/yosephineyosephine/Zotero/storage/DIV6M4H2/Durmus et al. - 2023 - Towards Measuring the Representation of Subjective.pdf:application/pdf},
}

@misc{hendy_how_2023,
	title = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}? {A} {Comprehensive} {Evaluation}},
	shorttitle = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}?},
	url = {http://arxiv.org/abs/2302.09210},
	abstract = {Generative Pre-trained Transformer (GPT) models have shown remarkable capabilities for natural language generation, but their performance for machine translation has not been thoroughly investigated. In this paper, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with stateof-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation. We experiment with eighteen different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci002. Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality. We perform comprehensive analysis and human evaluation to further understand the characteristics of GPT translations. We hope that our paper provides valuable insights for researchers and practitioners in the ﬁeld and helps to better understand the potential and limitations of GPT models for translation.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
	month = {February},
	year = {2023},
	note = {arXiv:2302.09210 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Hendy et al. - 2023 - How Good Are GPT Models at Machine Translation A .pdf:/Users/yosephineyosephine/Zotero/storage/P48D5SJB/Hendy et al. - 2023 - How Good Are GPT Models at Machine Translation A .pdf:application/pdf},
}

@misc{hu_expectations_2023,
	title = {Expectations over {Unspoken} {Alternatives} {Predict} {Pragmatic} {Inferences}},
	url = {http://arxiv.org/abs/2304.04758},
	abstract = {Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable – both within instances of a single scale, and across different scales – there have been few proposals that quantitatively explain both crossand within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we ﬁnd that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view of alternatives. Our results suggest that pragmatic inferences arise from context-driven expectations over alternatives, and these expectations operate at the level of concepts.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Hu, Jennifer and Levy, Roger and Degen, Judith and Schuster, Sebastian},
	month = {April},
	year = {2023},
	note = {arXiv:2304.04758 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear in TACL (pre-MIT Press publication version)},
	file = {Hu et al. - 2023 - Expectations over Unspoken Alternatives Predict Pr.pdf:/Users/yosephineyosephine/Zotero/storage/DQVHPLZE/Hu et al. - 2023 - Expectations over Unspoken Alternatives Predict Pr.pdf:application/pdf},
}

@inproceedings{jeretic_are_2020,
	address = {Online},
	title = {Are {Natural} {Language} {Inference} {Models} {IMPPRESsive}? {Learning} {IMPlicature} and {PRESupposition}},
	shorttitle = {Are {Natural} {Language} {Inference} {Models} {IMPPRESsive}?},
	url = {https://www.aclweb.org/anthology/2020.acl-main.768},
	doi = {10.18653/v1/2020.acl-main.768},
	abstract = {Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of {\textgreater}25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we ﬁnd that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by “some” as entailments. For some presupposition triggers like only, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.},
	language = {en},
	urldate = {2023-09-10},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jereti\v{c}, Paloma and Warstadt, Alex and Bhooshan, Suvrat and Williams, Adina},
	year = {2020},
	pages = {8690--8705},
	file = {Jeretic et al. - 2020 - Are Natural Language Inference Models IMPPRESsive.pdf:/Users/yosephineyosephine/Zotero/storage/AFLBGK5B/Jeretic et al. - 2020 - Are Natural Language Inference Models IMPPRESsive.pdf:application/pdf},
}

@article{koto2024indoculture,
  title={IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces},
  author={Koto, Fajri and Mahendra, Rahmad and Aisyah, Nurul and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2404.01854},
  year={2024}
}

@misc{liang_holistic_2022,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Project page: https://crfm.stanford.edu/helm/v1.0},
	file = {Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf:/Users/yosephineyosephine/Zotero/storage/L79KNNR8/Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@article{liu_adjective_2023,
	title = {Adjective {Scale} {Probe}: {Can} {Language} {Models} {Encode} {Formal} {Semantics} {Information}?},
	volume = {37},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Adjective {Scale} {Probe}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26559},
	doi = {10.1609/aaai.v37i11.26559},
	abstract = {It is an open question what semantic representations transformer-based language models can encode and whether they have access to more abstract aspects of semantic meaning. Here, we propose a diagnostic dataset to investigate how well language models understand the degree semantics of adjectives. In the dataset, referred as the Adjective Scale Probe (ASP), we semi-automatically generate 8 tests of Natural Language Inference (NLI) questions to test 8 key capabilities of adjective interpretation. We apply the ASP dataset to evaluate the performance of 3 language models, i.e., BERT, DeBERTa, and T0. It is found that language models perform below the majority baseline for most tests of the ASP, even when the models have been fine-tuned to achieve high performance on the large-scale MNLI dataset. But after we fine-tune the pre-trained models on a subset of the ASP, DeBERTa can achieve high performance on the untrained adjectives and untrained tests, suggesting that DeBERTa may have captured degree semantic information of adjectives through pre-training but it needs specific training data to learn how to apply such information to the current tasks. In sum, the ASP provides an easy-to-use method to test fine-grained formal semantic properties of adjectives, and reveals language models’ abilities to access formal semantic information.},
	language = {en},
	number = {11},
	urldate = {2023-09-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Wei and Xiang, Ming and Ding, Nai},
	month = jun,
	year = {2023},
	pages = {13282--13290},
	file = {Liu et al. - 2023 - Adjective Scale Probe Can Language Models Encode .pdf:/Users/yosephineyosephine/Zotero/storage/TJBNN9RI/Liu et al. - 2023 - Adjective Scale Probe Can Language Models Encode .pdf:application/pdf},
}

@misc{pandia_pragmatic_2021,
	title = {Pragmatic {Competence} of {Pre-trained} {Language} {Models} through the {Lens} of {Discourse} {Connectives}},
	url = {http://arxiv.org/abs/2109.12951},
	abstract = {As pre-trained language models (LMs) continue to dominate NLP, it is increasingly important that we understand the depth of language capabilities in these models. In this paper, we target pre-trained LMs’ competence in pragmatics, with a focus on pragmatics relating to discourse connectives. We formulate cloze-style tests using a combination of naturally-occurring data and controlled inputs drawn from psycholinguistics. We focus on testing models’ ability to use pragmatic cues to predict discourse connectives, models’ ability to understand implicatures relating to connectives, and the extent to which models show humanlike preferences regarding temporal dynamics of connectives. We ﬁnd that although models predict connectives reasonably well in the context of naturally-occurring data, when we control contexts to isolate high-level pragmatic cues, model sensitivity is much lower. Models also do not show substantial humanlike temporal preferences. Overall, the ﬁndings suggest that at present, dominant pretraining paradigms do not result in substantial pragmatic competence in our models.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Pandia, Lalchand and Cong, Yan and Ettinger, Allyson},
	month = sep,
	year = {2021},
	note = {arXiv:2109.12951 [cs]},
	keywords = {68T50, Computer Science - Computation and Language, I.2.7},
	annote = {Comment: Accepted at CoNLL 2021},
	file = {Pandia et al. - 2021 - Pragmatic competence of pre-trained language model.pdf:/Users/yosephineyosephine/Zotero/storage/HKRTFS2U/Pandia et al. - 2021 - Pragmatic competence of pre-trained language model.pdf:application/pdf},
}

@misc{parrish_nope_2021,
	title = {{NOPE}: {A} {Corpus} of {Naturally}-{Occurring} {Presuppositions} in {English}},
	shorttitle = {{NOPE}},
	url = {http://arxiv.org/abs/2109.06987},
	abstract = {Understanding language requires grasping not only the overtly stated content, but also making inferences about things that were left unsaid. These inferences include presuppositions, a phenomenon by which a listener learns about new information through reasoning about what a speaker takes as given. Presuppositions require complex understanding of the lexical and syntactic properties that trigger them as well as the broader conversational context. In this work, we introduce the Naturally-Occurring Presuppositions in English (NOPE) Corpus to investigate the context-sensitivity of 10 different types of presupposition triggers and to evaluate machine learning models’ ability to predict human inferences. We ﬁnd that most of the triggers we investigate exhibit moderate variability. We further ﬁnd that transformer-based models draw correct inferences in simple cases involving presuppositions, but they fail to capture the minority of exceptional cases in which human judgments reveal complex interactions between context and triggers.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Parrish, Alicia and Schuster, Sebastian and Warstadt, Alex and Agha, Omar and Lee, Soo-Hwan and Zhao, Zhuoye and Bowman, Samuel R. and Linzen, Tal},
	month = sep,
	year = {2021},
	note = {arXiv:2109.06987 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: CoNLL 2021. Data and code available at https://github.com/nyu-mll/nope},
	file = {Parrish et al. - 2021 - NOPE A Corpus of Naturally-Occurring Presuppositi.pdf:/Users/yosephineyosephine/Zotero/storage/23V7VBT5/Parrish et al. - 2021 - NOPE A Corpus of Naturally-Occurring Presuppositi.pdf:application/pdf},
}

@misc{qadri2025risksculturalerasurelarge,
      title={Risks of Cultural Erasure in Large Language Models}, 
      author={Rida Qadri and Aida M. Davani and Kevin Robinson and Vinodkumar Prabhakaran},
      year={2025},
      eprint={2501.01056},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.01056}, 
}

@article{romero2024cvqa,
  title={Cvqa: Culturally-diverse multilingual visual question answering benchmark},
  author={Romero, David and Lyu, Chenyang and Wibowo, Haryo Akbarianto and Lynn, Teresa and Hamed, Injy and Kishore, Aditya Nanda and Mandal, Aishik and Dragonetti, Alina and Abzaliev, Artem and Tonja, Atnafu Lambebo and others},
  journal={arXiv preprint arXiv:2406.05967},
  year={2024},
}

@misc{sailor2report,
  title={Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLM},
  author={{Sailor2 Team}},
  year={2024}
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {Hugging} {Face}},
	shorttitle = {HuggingGPT},
	url = {http://arxiv.org/abs/2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward advanced artiﬁcial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Speciﬁcally, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artiﬁcial intelligence 2.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = may,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf:/Users/yosephineyosephine/Zotero/storage/ESQNQ948/Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf:application/pdf},
}

@article{singh2024global,
  title={Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation},
  author={Singh, Shivalika and Romanou, Angelika and Fourrier, Cl{\'e}mentine and Adelani, David I and Ngui, Jian Gang and Vila-Suero, Daniel and Limkonchotiwat, Peerat and Marchisio, Kelly and Leong, Wei Qi and Susanto, Yosephine and others},
  journal={arXiv preprint arXiv:2412.03304},
  year={2024}
}

@article{smart2024socially,
  title={Socially Responsible Data for Large Multilingual Language Models},
  author={Smart, Andrew and Hutchinson, Ben and Amugongo, Lameck Mbangula and Dikker, Suzanne and Zito, Alex and Ebinama, Amber and Wudiri, Zara and Wang, Ding and van Liemt, Erin and Sedoc, Jo{\~a}o and others},
  journal={arXiv preprint arXiv:2409.05247},
  year={2024}
}

@inproceedings{someya-oseki-2023-jblimp,
    title = {{JBLiMP}: {Japanese} {Benchmark} of {Linguistic} {Minimal} {Pairs}},
    author = {Someya, Taiga  and
      Oseki, Yohei},
    booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
    month = {May},
    year = {2023},
    address = {Dubrovnik, Croatia},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2023.findings-eacl.117},
    pages = {1581--1594},
    abstract = {In this paper, we introduce JBLiMP (Japanese Benchmark of Linguistic Minimal Pairs), a novel dataset for targeted syntactic evaluations of language models in Japanese. JBLiMP consists of 331 minimal pairs, which are created based on acceptability judgments extracted from journal articles in theoretical linguistics. These minimal pairs are grouped into 11 categories, each covering a different linguistic phenomenon. JBLiMP is unique in that it successfully combines two important features independently observed in existing datasets: (i) coverage of complex linguistic phenomena (cf. CoLA) and (ii) presentation of sentences as minimal pairs (cf. BLiMP). In addition, JBLiMP is the first dataset for targeted syntactic evaluations of language models in Japanese, thus allowing the comparison of syntactic knowledge of language models across different languages. We then evaluate the syntactic knowledge of several language models on JBLiMP: GPT-2, LSTM, and n-gram language models. The results demonstrated that all the architectures achieved comparable overall accuracies around 75{\%}. Error analyses by linguistic phenomenon further revealed that these language models successfully captured local dependencies like nominal structures, but not long-distance dependencies such as verbal agreement and binding.},
}

@misc{srivastava_beyond_2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and {Extrapolating} the {Capabilities} of {Language} {Models}},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = {June},
	year = {2023},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench},
	file = {Srivastava et al. - 2023 - Beyond the Imitation Game Quantifying and extrapo.pdf:/Users/yosephineyosephine/Zotero/storage/CHUA4BVL/Srivastava et al. - 2023 - Beyond the Imitation Game Quantifying and extrapo.pdf:application/pdf},
}

@article{warstadt_blimp_2020,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {{BLiMP}},
	url = {https://direct.mit.edu/tacl/article/96452},
	doi = {10.1162/tacl_a_00321},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
	language = {en},
	urldate = {2023-09-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	month = {December},
	year = {2020},
	pages = {377--392},
	file = {Warstadt et al. - 2020 - BLiMP The Benchmark of Linguistic Minimal Pairs f.pdf:/Users/yosephineyosephine/Zotero/storage/ND8JZZNR/Warstadt et al. - 2020 - BLiMP The Benchmark of Linguistic Minimal Pairs f.pdf:application/pdf},
}

@inproceedings{xiang_climp_2021,
	address = {Online},
	title = {{CLiMP}: {A} {Benchmark} for {Chinese} {Language} {Model} {Evaluation}},
	shorttitle = {{CLiMP}},
	url = {https://aclanthology.org/2021.eacl-main.242},
	doi = {10.18653/v1/2021.eacl-main.242},
	abstract = {Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of these models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP), which can be used to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1,000 minimal pairs (MPs) for 16 syntactic contrasts in Mandarin, covering 9 major Mandarin linguistic phenomena. The MPs are semiautomatically generated, and human agreement with the labels in CLiMP is 95.8\%. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We ﬁnd that classiﬁer–noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the baˇ construction, binding, and ﬁller-gap dependencies. Overall, Chinese BERT achieves an 81.8\% average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.},
	language = {en},
	urldate = {2023-09-10},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Xiang, Beilei and Yang, Changbing and Li, Yu and Warstadt, Alex and Kann, Katharina},
	year = {2021},
	pages = {2784--2790},
	file = {Xiang et al. - 2021 - CLiMP A Benchmark for Chinese Language Model Eval.pdf:/Users/yosephineyosephine/Zotero/storage/58KIGQ32/Xiang et al. - 2021 - CLiMP A Benchmark for Chinese Language Model Eval.pdf:application/pdf},
}

@article{zhang2024seallms,
  title={Seallms 3: Open foundation and chat multilingual large language models for southeast asian languages},
  author={Zhang, Wenxuan and Chan, Hou Pong and Zhao, Yiran and Aljunied, Mahani and Wang, Jianyu and Liu, Chaoqun and Deng, Yue and Hu, Zhiqiang and Xu, Weiwen and Chia, Yew Ken and others},
  journal={arXiv preprint arXiv:2407.19672},
  year={2024}
}

@misc{zhang_benchmarking_2023,
	title = {Benchmarking {Large} {Language} {Models} for {News} {Summarization}},
	url = {http://arxiv.org/abs/2301.13848},
	abstract = {Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we ﬁnd instruction tuning, and not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and ﬁnetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we ﬁnd that LMM summaries are judged to be on par with human written summaries.},
	language = {en},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang, Percy and McKeown, Kathleen and Hashimoto, Tatsunori B.},
	month = {Jan},
	year = {2023},
	note = {arXiv:2301.13848 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Zhang et al. - 2023 - Benchmarking Large Language Models for News Summar.pdf:/Users/yosephineyosephine/Zotero/storage/G68CRQN3/Zhang et al. - 2023 - Benchmarking Large Language Models for News Summar.pdf:application/pdf},
}

