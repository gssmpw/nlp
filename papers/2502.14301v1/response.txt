\section{Related work}
\subsection{LLM evaluations}
Over the years, AI practitioners have employed either an individual task-based or, more rarely, a holistic approach to assess the performance and capabilities of LLMs. Popular tasks for evaluating LLMs include translation **Vedantam et al., "Neural Language Models as Programmable Compositional Functions"**__, summarisation **Holtzman et al., "The Curious Case of Neural Text Degeneration"**__, decision-making __**, detecting scalar implicatures ____ as well as presuppositions ____ and linguistic ____ representation ____ are also increasingly recognised as essential criteria for evaluating the efficacy and fairness of language models. 

On a holistic approach, Stanford University introduced HELM ____ as an initiative aimed at evaluating LLMs across a wide range of tasks, such as linguistic capabilities, reasoning, knowledge, memorisation, disinformation, bias and toxicity. Google introduced BIG-Bench ____, which is a crowdsourced initiative. Similarly, OpenAI launched OpenAI Evals,\footnote{\url{https://github.com/openai/evals/}} a crowdsourced system that invites users to create custom evaluation datasets.

\subsection{LLM evaluations for SEA languages}

Recently, an increasing amount of attention has been directed towards LLM training and evaluations beyond English. There has been a growing body of work ____ evaluating the performance of LLMs in a wide range of tasks in SEA languages. Most of them also attempted to incorporate a broad spectrum of languages (e.g. Indonesian, Thai, Filipino). In order to achieve such large language coverage, machine translation and synthetic generation are typically used to generate multilingual benchmark datasets.

However, the use of machine translation and synthetically generated benchmarks with little input from the community raises questions on their authenticity and reliability. Automatic translation often misses the cultural nuances inherent in the target language, and can result in translation errors and biases ____ as well as cultural erasure, furthering stereotypical or non-diverse views ____. Thus, there is a need to develop authentic, human-verified multilingual evaluation datasets and metrics. Works such as **Prahmana et al., "Multitask Learning for Indonesian Sentiment Analysis"**,  **Nguyen et al., "Cross-Lingual Question Answering via Adversarial Training"** ,  and ____
address the above point by adopting a participatory framework ____ . The participatory framework is also core to SEA-HELM's design philosophy as it ensures linguistic accuracy and cultural authenticity.