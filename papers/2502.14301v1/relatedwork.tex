\section{Related work}
\subsection{LLM evaluations}
Over the years, AI practitioners have employed either an individual task-based or, more rarely, a holistic approach to assess the performance and capabilities of LLMs. Popular tasks for evaluating LLMs include translation \cite{hendy_how_2023}, summarisation \cite{zhang_benchmarking_2023}, decision-making \citep{shen_hugginggpt_2023}, detecting scalar implicatures \citep{jeretic_are_2020,pandia_pragmatic_2021,hu_expectations_2023, liu_adjective_2023} as well as presuppositions \citep{jeretic_are_2020,parrish_nope_2021}. Additionally, linguistic \citep{warstadt_blimp_2020, xiang_climp_2021,someya-oseki-2023-jblimp} and cultural representation \citep{durmus_towards_2023, atari2023humans} are also increasingly recognised as essential criteria for evaluating the efficacy and fairness of language models. 

On a holistic approach, Stanford University introduced HELM \citep{liang_holistic_2022} as an initiative aimed at evaluating LLMs across a wide range of tasks, such as linguistic capabilities, reasoning, knowledge, memorisation, disinformation, bias and toxicity. Google introduced BIG-Bench \citep{srivastava_beyond_2023}, which is a crowdsourced initiative. Similarly, OpenAI launched OpenAI Evals,\footnote{\url{https://github.com/openai/evals/}} a crowdsourced system that invites users to create custom evaluation datasets.

\subsection{LLM evaluations for SEA languages}

Recently, an increasing amount of attention has been directed towards LLM training and evaluations beyond English. There has been a growing body of work \citep{sailor2report,zhang2024seallms,bai2023qwen,dang2024aya} evaluating the performance of LLMs in a wide range of tasks in SEA languages. Most of them also attempted to incorporate a broad spectrum of languages (e.g. Indonesian, Thai, Filipino). In order to achieve such large language coverage, machine translation and synthetic generation are typically used to generate multilingual benchmark datasets.

However, the use of machine translation and synthetically generated benchmarks with little input from the community raises questions on their authenticity and reliability. Automatic translation often misses the cultural nuances inherent in the target language, and can result in translation errors and biases \citep{singh2024global}. This can result in cultural erasure, furthering stereotypical or non-diverse views \citep{qadri2025risksculturalerasurelarge}. Thus, there is a need to develop authentic, human-verified multilingual evaluation datasets and metrics. Works such as \citet{singh2024global}, \citet{romero2024cvqa}, and \citet{koto2024indoculture}
address the above point by adopting a participatory framework 
\citep{birhane2022power, smart2024socially}. The participatory framework is also core to SEA-HELM's design philosophy as it ensures linguistic accuracy and cultural authenticity.