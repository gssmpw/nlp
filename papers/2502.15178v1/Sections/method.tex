
\begin{figure*}[t]
\centering
  \centerline{\includegraphics[width=0.9\textwidth]{./Figures_alone/st-model-arch-v2.pdf}}  
  \caption{The architecture of the proposed PaM method. The output feature of the prompt guides the routing of the MoE adapter where we use a fixed shared expert and a single routed expert. For each expert, the last hidden states of all encoders are concatenated with a set of fused hidden states derived from a fusion weight matrix. Subsequently, an FFN is applied to align with the dimensions of the LLM.}
  \label{fig:moe-model-arch}
  \vspace{-0.5em}
\end{figure*}


\section{Method}

In this section, we begin with an overview of the proposed PaM method (Figure~\ref{fig:moe-model-arch}). We then elaborate on the details of the encoder fusion process, executed by a single expert, and describe the prompt-aware routing method.

\subsection{Overall Architecture}

The architecture of the proposed PaM method is depicted in Figure~\ref{fig:moe-model-arch}. As described in Equation~\ref{eq:1}, the LLM accepts the text prompt $\mathbf{X}_\text{prompt}$, which includes task-related information, along with the speech features $\mathbf{H}_\text{audio}$ as input, and subsequently generates the response $\mathbf{Y}$.
 \begin{align}
 \label{eq:1}
 \mathbf{Y} = \text{LLM}(\mathbf{X}_\text{prompt}, \mathbf{H}_\text{audio})
\end{align}

\noindent To obtain $\mathbf{H}_\text{audio}$, we employ three encoders: the Whisper encoder, WavLM, and Wav2Vec2. For each encoder, the hidden states are initially processed by a feed-forward network (FFN) as described in Equation~\ref{eq:2}, resulting in the feature of each encoder, denoted as $\mathbf{H}_{i}$.
 \begin{align}
 \label{eq:2}
 \mathbf{H}_i = \text{FFN}_i(\text{Encoder}_i(\mathbf{X}_\text{audio}))
\end{align}

\noindent Next, as shown in Equation~\ref{eq:3}, we combine these features using an MoE fusion method, which includes a shared expert and \textit{N} routed experts where the number of routed experts corresponds to the number of predefined tasks. During inference, only one routed expert is selected, based on the task indicated by the prompt. 
 \begin{align}
 \label{eq:3}
  &\mathbf{H}_\text{audio} = \text{Expert}_\text{share}(\mathbf{H}_{\{1,2,3\}}) \nonumber \\&+ \sum_{j=1}^{N}G_j(\mathbf{X}_\text{prompt})\times\text{Expert}_j(\mathbf{H}_{\{1,2,3\}})
\end{align}

\noindent Overall, each expert processes the features from all encoders ($\mathbf{H}_{\{1,2,3\}}$) for feature fusion. The shared expert extracts common features for all tasks, while the routed expert performs task-specific feature fusion. The routing is determined by the user input, which is the prompt.

\subsection{Multi-layer Fusion}

Different encoders exhibit distinct strengths. For example, WavLM is excellent at extracting speaker information, while Wav2Vec2 excels in capturing semantic content. The Whisper encoder, trained on a vast amount of data, provides superior features for AC and ASR in noisy environments. Additionally, features from different layers contain varying levels of information. Deeper layers hold high-level semantic information, whereas lower layers may contain fine-grained acoustic details. Thus, for feature fusion, we consider features from all layers of all three encoders. Specifically, for the feature $\mathbf{H}_i$ from a single encoder, it includes hidden states from all $L_i$ Transformer~\cite{NIPS2017_3f5ee243} layers as well as $\mathbf{h}_i^{0}$, the hidden states following the convolutional layers (Equation~\ref{eq:4}).
\begin{align}
 \mathbf{H}_{i} = \{\mathbf{h}_i^{0}, \mathbf{h}_i^{1}, ... \mathbf{h}_i^{L_i}\}
  \label{eq:4}
\end{align}

\noindent As illustrated in Equation~\ref{eq:5}, we consider the hidden states $\mathbf{h}_i^{\{0\text{-}(L_i\text{-}1)\}}$ to derive the fused hidden states $\mathbf{h}_{k}^\text{fused}$. For the hidden states of each layer, a single scalar weight is assigned to control its importance. We utilize a set of scalar weight include $k$ elements $\{w_{i,j}^1, \ldots, w_{i,j}^k\}$ for each hidden state $\mathbf{h}_{i, j}$ from all three encoders to generate three fused hidden states $\mathbf{h}_{\{1,\ldots,k\}}^\text{fused}$. Thus, the dimension of the whole matrices is $\mathbf{W} \in \mathbb{R}^{3\times(L_1 + L_2 + L_3)}$.
\begin{align}
 \label{eq:5}
 \mathbf{h}_{k}^\text{fused} = \sum_{i=1}^3 \sum_{j=1}^{L_i} w_{i,j}^k \cdot \mathbf{h}_{i, j}
\end{align}

\noindent Finally, we concatenate the last hidden states of the three encoders $\mathbf{h}_{\{1,2,3\}}^{L_i}$ with the three fused hidden states $\mathbf{h}_{\{1,\ldots,k\}}^\text{fused}$ along the feature dimension. Afterward, we apply a FFN to compress the feature dimension to match the dimension of the LLM embedding (Equation~\ref{eq:6}).
\begin{align}
 \label{eq:6}
 \mathbf{h}^{\text{final}} &= \text{Concat}(\mathbf{h}_{\{1,2,3\}}^{L_i}, \mathbf{h}_{\{1,\ldots,k\}}^\text{fused}) \nonumber \\ 
 \text{Expert}(\cdot) &= \text{FFN}(\mathbf{h}^{\text{final}})
\end{align}

\noindent The parameters in our fusion method are independent among the routed experts. The use of fusion weight matrices highlights multi-level feature fusion, while the final concatenation followed by the FFN provides more fine-grained feature fusion.

\subsection{Prompt Aware Routing}

A prompt refers to a segment of text that provides context or objectives for the generation of the Speech LLM. Although the format of prompts can vary widely, they can typically be categorized into several distinct types according to the task the user wishes to perform. For instance, speech-related tasks, sound-related tasks and speech chat tasks~\cite{yang2024air}. In this paper, we investigate three tasks: ASR, speaker number verification, and AC. We utilize distinct experts for each task. For effective routing, the router must identify the task type based on the prompt. We employ a simple classification approach (Equation~\ref{eq:7} and~\ref{eq:8}) wherein we use the last hidden states $\mathbf{H}_\text{prompt}$ of the prompt from the LLM, followed by a FFN and Softmax activation, to obtain the task posteriors $\text{P}(\text{Task}|\mathbf{H}_\text{prompt})$.
\begin{align}
 \label{eq:7}
 \mathbf{H}_\text{prompt} &= \text{LLM}(\mathbf{X}_\text{prompt})\\ 
 \label{eq:8}
 \text{P}(\text{Task}|\mathbf{H}_\text{prompt}) &= \text{Softmax}(\text{FFN} (\mathbf{H}_\text{prompt}))
\end{align}

\noindent As shown in Equation~\ref{eq:9}, we select the routed expert with the Top-1 probability by the indicator function.
\begin{align}
 \label{eq:9}
 \text{G}_j = 
 \begin{cases}
 1 & \text{if } j\in\text{Top-1}(\text{P}(\text{Task}|\mathbf{H}_\text{prompt}))\\
 0 & \text{otherwise}
 \end{cases}
\end{align}

To train the FFN, we create diverse prompts for each task using the LLM. Specifically, we manually write several prompts for each task and instruct ChatGPT to rewrite these prompts. Examples of these prompts are shown in Table~\ref{table:example}. It is important to note that the audio features follow the prompt because we use the prompt to guide feature extraction and fusion. This approach differs from other works, where the audio features <|AUDIO|> can be positioned before the prompt.

\begin{table}[t]
    \centering
    \small
    \resizebox{\linewidth}{!}{\begin{tabular}{p{1.5cm}p{4.8cm}}
        \toprule
        \multirow{2}{*}{ASR}
        & 1. Hear the audio clip and transform it into text format. <|AUDIO|>\\
        & 2. Listen to the following audio and create a corresponding text transcript. <|AUDIO|>\\
        \midrule
        Speaker Number & 1. How many speakers' contributions are in this recording? <|AUDIO|>\\
        Verification & 2. What is the number of speakers in this spoken content? <|AUDIO|>\\
        \midrule
        \multirow{2}{*}{AC}
        & 1. Listen to this audio and provide a detailed description. <|AUDIO|>\\
        & 2. Analyze the recording and summarize its contents. <|AUDIO|>\\
        \bottomrule
    \end{tabular}}
    \caption{Examples of prompts for different tasks.}
    \label{table:example}
\end{table}

\subsection{Training Objective}

The training loss function, as illustrated in Equation~\ref{eq:10}, is the sum of the cross-entropy loss $\mathcal{L}_\text{G}$ for prompt-aware gating and the cross-entropy loss $\mathcal{L}_\text{llm}$ between the LLM's output $\mathbf{Y}$ and the ground truth $\hat{\mathbf{Y}}$.
 \begin{align}
 \label{eq:10}
 \mathcal{L} &= \mathcal{L}_\text{G}(\text{P}(\text{Task}|\mathbf{H}_\text{prompt}), \text{Task}) \nonumber \\ 
 &+ \mathcal{L}_\text{llm}(\mathbf{Y}, \hat{\mathbf{Y}}) 
\end{align}

