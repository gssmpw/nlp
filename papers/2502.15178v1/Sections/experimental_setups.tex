\section{Experimental Setups}

In this section, we present a detailed description of our experimental setups, covering datasets, evaluation metrics, hyperparameters for the model architecture, and training. The links to the pretrained models and datasets used can be found in Appendix~\ref{sec:appendix-datas-and-model}, while the implementation details of baseline methods are available in Appendix~\ref{sec:appendix-baseline-implement}.

\subsection{Datasets and Evaluation Metrics}

We assess the efficacy of our method across three audio-to-text tasks: automatic speech recognition (ASR), speaker number verification (SNV), and audio captioning (AC). The training set includes the following: 200 hours of ASR data from LibriSpeech-100~\cite{panayotov2015librispeech} and AMI~\cite{kraaij2005ami}, 150 hours of SNV data synthesized from Common Voice V4~\cite{ardila2019common}, and 100 hours of AC data from AudioCaps~\cite{audiocaps}. In total, the training data contains 450 hours of audio signals. For SNV, we randomly concatenate individual utterances to form new speech signals with the number of speakers ranging from one to four. The test dataset includes LibriSpeech-test-clean, LibriSpeech-test-other, AMI, the SNV test set from AIR-Bench~\cite{yang2024air}, and the test set of AudioCaps along with its corresponding QA version from AudioBench~\cite{wang2024audiobench}, which contains diverse questions. ASR tasks focus on semantics. The LibriSpeech test set originates from audiobooks, demonstrating ASR performance in a clean scenario. AMI, a real meeting corpus containing spontaneous talk, reflects ASR performance in a more challenging, real-world scenario. SNV and AC test sets can indicate the Speech LLM's ability to understand speaker and acoustic information. We evaluate the performance using word error rate (WER) for ASR tasks, accuracy for SNV, and METEOR~\cite{banerjee2005meteor} for AC. 


\begin{table*}[htp]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{l l l l l l l c}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{LibriSpeech} & AMI & SNV & AudioCaps & AudioCaps QA & \multirow{2}{*}{AVG Rank$\downarrow$} \\ 
\cmidrule(r){2-7}
 & WER(clean)$\downarrow$ & WER(other)$\downarrow$ & WER$\downarrow$ & Acc$\uparrow$ & METEOR$\uparrow$ & METEOR$\uparrow$ & \\ 
\midrule
\rowcolor{black!6} Single-encoder Baselines & & & & & & & \\
\quad- Whisper~\cite{radford2022robust} & 9.61 & 16.73 & \textbf{16.27} & 18.80\% & \textbf{32.96} & \textbf{15.04} & 5.67 \\
\quad- WavLM~\cite{chen2021wavlm} & 5.59 & 10.57 & 18.97 & \textbf{41.40\%} & 27.14 & 12.77 & 5.50 \\
\quad- Wav2Vec2~\cite{baevski2020wav2vec} & \textbf{4.30} & \textbf{09.46} & 26.69 & 39.00\% & 23.81 & 11.20 & 5.33 \\
\midrule
\rowcolor{black!6} Multi-encoder Baselines & & & & & & & \\
\quad- WavLLM~\cite{hu2024wavllmrobustadaptivespeech} & 4.95 (\textcolor{red!70!black}{-\;0.65}) & 09.19 (\textcolor{green!70!black}{+0.27}) & 15.29 (\textcolor{green!70!black}{+0.98}) & 39.20\% (\textcolor{red!70!black}{-\;2.20}) & 34.93 (\textcolor{green!70!black}{+1.97}) & 16.35 (\textcolor{green!70!black}{+1.31}) & 2.67 \\
\quad- SALMONN~\cite{tang2024salmonn} & 5.04 (\textcolor{red!70!black}{-\;0.74}) & 09.70 (\textcolor{red!70!black}{-\;0.24}) & 19.04 (\textcolor{red!70!black}{-\;2.77}) & 49.40\% (\textcolor{green!70!black}{+8.00}) & 34.86 (\textcolor{green!70!black}{+1.90}) & 15.97 (\textcolor{green!70!black}{+0.93}) & 3.50 \\
\quad- Average & 4.76 (\textcolor{red!70!black}{-\;0.46}) & 10.43 (\textcolor{red!70!black}{-\;0.97}) & 17.20 (\textcolor{red!70!black}{-\;0.93}) & 45.50\% (\textcolor{green!70!black}{+4.10}) & 33.22 (\textcolor{green!70!black}{+0.26}) & 15.53 (\textcolor{green!70!black}{+0.49}) & 3.67 \\
\midrule
\rowcolor{black!8} PaM (Ours) & 3.65 (\textcolor{green!70!black}{+0.65}) & 07.07 (\textcolor{green!70!black}{+2.39}) & 12.79 (\textcolor{green!70!black}{+3.48}) & 42.80\% (\textcolor{green!70!black}{+1.40}) & 35.47 (\textcolor{green!70!black}{+2.51}) & 15.70 (\textcolor{green!70!black}{+0.66}) & \textbf{1.67} \\
\bottomrule
\end{tabular}}
\caption{Comparison of the proposed PaM method with single and multi-encoder baselines. For multi-encoder baselines: WavLLM and SALMONN use concatenation followed by a linear layer and Q-former, respectively, while `Average' averages the three features. Values in the brackets indicate performance improvement (green) or degradation (red) compared to the best single encoder result. The avg rank column shows the average rank on each task. Smaller ranks indicate better performance.}
\label{tab:main}
\end{table*}


\subsection{Model Architecture and Training}

We train our model based on Huggingface Transformers Library\footnote{\url{https://github.com/huggingface/transformers}}. Our model consists of three audio encoders, a pre-fusion adapter for each encoder, a PaM fusion module, and an LLM. In our main experiments, the encoders are Whisper-Small~\cite{radford2022robust}, WavLM-Base-Plus~\cite{chen2021wavlm}, and Wav2Vec2-Base-960h~\cite{baevski2020wav2vec}, each with approximately 100 million parameters. We downsample the features from the Whisper encoder by a factor of two, resulting in a frame length of 40ms, consistent with the frame length of Wav2Vec2 and WavLM. The pre-fusion adapter is an FFN that transforms the encoder's hidden dimension $D_{\text{E}}$ to the LLM's hidden dimension $D_{\text{LLM}}$. Each expert in the PaM fusion module includes a fusion weight matrix ($\mathbb{R}^{3L\times3}$) and a linear layer ($\mathbb{R}^{6D_{\text{LLM}}\times D_{\text{LLM}}}$) to fuse features from all encoders. Here, $L$ represents the number of layers in the encoder, which is 12 for all encoders in our experiments. For the fused features, we set $k=3$, corresponding to the number of last hidden states. We utilize four routed experts, each corresponding to a specific task category: ASR-clean, ASR-noisy, SNV, and AC. For each category, we generate 50 prompts using ChatGPT~\cite{gpt2023gpt}. For the LLM model, we select the Qwen2.5-3B~\cite{qwen2.5}. In section~\ref{sec:results}, we also experiment with other encoders, including Hubert-Base-LS960, Whisper-Large-v3, and WavLM-Large.

We train our model for five epochs with a learning rate of 5e-5, 2000 warmup steps, and bf16 precision. We freeze all encoders and the LLM, training only the added parameters of the adapters and the fusion modules. For the LLM, we apply LoRA~\cite{hu2022lora} with a rank of 32 and an alpha of 64, adding LoRA only on the q\_proj and k\_proj. Each task has the same probability during training. During the inference stage, we select the last checkpoint on the validation set and perform greedy search.

