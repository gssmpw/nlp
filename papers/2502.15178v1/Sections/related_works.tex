\section{Related Works}

\paragraph{Audio Encoders:} Audio encoders transform raw waveforms or Fbank features into a high-level feature space that is informative for downstream tasks. These encoders can be classified into two categories: supervised models and self-supervised models. Supervised models typically employ ASR tasks to train an end-to-end model that includes an audio encoder and a text decoder, for instance, an attention-based decoder or a Connectionist Temporal Classification (CTC) decoder. By omitting the decoder, the encoder can serve as a feature extractor~\cite{radford2022robust, baevski2020wav2vec}. Self-supervised models can be trained on unlabeled speech signals. For instance, Wav2Vec2~\cite{baevski2020wav2vec} and HuBERT~\cite{hsu2021hubert} were trained to predict the pseudo-discrete target at masked time steps. WavLM~\cite{chen2021wavlm} is a variant of HuBERT, designed to facilitate speaker identity extraction by using multi-speaker signals. Different model architectures, training methods, and data can result in encoders with distinct properties and advantages, making the mixture of audio encoders effective for Speech LLMs.

\paragraph{Speech LLM:} The native LLM handles only text modality. For the LLM to directly process audio modality, it must comprehend audio features, which contain significantly more information than transcriptions and are substantially longer than text embeddings. Since text is represented as discrete tokens, it is natural to extract discrete tokens from continuous speech signals and then expand the vocabulary of text LLMs to understand these speech tokens~\cite{rubenstein2023audiopalm, veluri2024beyond, ma2024language}. An alternative is to use an adapter layer, to directly convert the continuous speech features to the continuous embedding space of the LLM. For example, QwenAudio~\cite{Qwen2-Audio} employs average pooling to downsample speech features, followed by two linear layers for projection. SALMONN~\cite{tang2024salmonn} utilizes the Q-former~\cite{yu2023connecting}, a cross-attention-based adapter, to achieve a higher compression ratio. Compared to previous works, our adapter handles more encoders and generates different features based on the prompt, rather than a single feature for all prompts.

\paragraph{Mixture of experts:} With the introduction of the Transformer architecture~\cite{NIPS2017_3f5ee243}, increasing model size has been shown to improve performance significantly~\cite{he2016deep,kaplan2020scaling}. This has led to growing interest in the MoE approach, which replaces the FFN sub-layer in Transformer models with multiple experts, thereby enabling substantial parameter growth while maintaining constant inference costs~\cite{shazeer2017outrageously}. These MoE methods typically employ massive experts and extremely sparse activation routing, without explicitly considering the specialization of individual experts~\cite{fedus2022switch, lepikhin2020gshard}. However, the vast scale of these models presents significant deployment challenges. In contrast, the earliest MoE research introduced a data-dependent, trainable combining method~\cite{jacobs1991adaptive, masoudnia2014mixture}, which aims to decompose complex tasks into simpler sub-tasks, each managed by a dedicated expert. Such works have inspired recent advances in developing modular models with emphasis on handling each task by a task-specific expert called expert specialization~\cite{ma2018modeling, gupta2022sparsely}, providing solutions for deploying large-scale MoE models~\cite{lu2024not} and enabling individual experts to learn and decompose diverse knowledge~\cite{dai2024deepseekmoe}. Inspired by these insights, we proposed a specialized MoE fusion method integrating multiple audio features to enhance Speech LLMs.

