\section{Introduction}

Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks, including question answering, machine translation, and summarization~\cite{gpt2023gpt}. In recent work, there has been a growing focus on merging speech encoders with LLMs, so that the LLM can understand the spoken content without the need for explicit transcription, promoting tasks such as direct speech translation~\cite{chen2023salm} and named entity recognition from speech~\cite{li2024usinglargelanguagemodel}. Much of this work leverages adapter layers to downsample and map speech features into the LLMâ€™s embedding space. These adapter layers include various forms, such as attention layers~\cite{yu2023connecting}, adaptive CTC downsamplers~\cite{ling2023adapting}, and convolutional layers~\cite{fathullah2023prompting}. Beyond semantic understanding tasks, Speech LLMs have been extended to encompass a broader range of applications, including audio event detection and audio captioning~\cite{Qwen2-Audio}. 

\begin{figure}[tp]
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{./Figures_alone/layer_importance.pdf}}  
  \caption{ASR and Audio Caption tasks favor different encoders and layers of features. The dotted lines indicate the average (AVG) importance of different encoders. The bar chart represents fine-grained layer importance.}
\label{fig:layer_importance}
\end{figure}

Multitasking requires that the input audio features contain as much relevant information as possible, representing the input speech, which may include speech content, noise, and speaker-specific characteristics. When fine-tuning self-supervised speech encoders, researchers assign learnable weights to each layer and observe that different downstream tasks prioritize different levels of features~\cite{chen2021wavlm}. In our Speech LLM framework, a similar trend is evident, where different tasks prioritize different encoders and feature levels (Figure~\ref{fig:layer_importance}). These biases arise from the inherent differences in the tasks themselves. For instance, the automatic speech recognition (ASR) task focuses solely on the speech content, disregarding other factors such as speaker characteristics and background noise. In contrast, tasks like audio captioning (AC) may rely on these additional factors that ASR intentionally excludes.

Consequently, researchers have proposed using multiple encoders to extract more robust features. For instance, WavLLM~\cite{hu2024wavllmrobustadaptivespeech} employs both the WavLM~\cite{chen2021wavlm} and the Whisper~\cite{radford2022robust} encoder, while SALMONN~\cite{tang2024salmonn} integrates the Whisper encoder and the BEATs~\cite{chen2023beats}. However, these approaches consider all encoders equally important, and merge the features from different encoders based on simple concatenation method across all tasks. Moreover, MoWE~\cite{zhang2024moweaudiomultitaskaudiollmsmixture} employs a strong encoder and multiple weaker encoders via the Mixture of experts (MoE) approach. However, MoWE only utilizes the input audio to control the gating mechanism, without incorporating task-specific information in prompts.
 
In this paper, we introduce Prompt-aware Mixture (PaM), a novel MoE method for merging multiple encoders to enhance Speech LLMs, our approach integrates a prompt-aware gating mechanism, emphasizes feature fusion, and considers the relative importance of each encoder for different tasks, aiming to improve all downstream performance. PaM employs three distinct audio encoders: the Whisper encoder~\cite{radford2022robust}, WavLM~\cite{chen2021wavlm}, and Wav2Vec2~\cite{baevski2020wav2vec}. We train a set of experts for prompt-aware feature fusion, comprising one shared expert and four task-specific experts. On each task, an expert learns the optimal weights for each encoder and its respective layers, and subsequently maps the resulting features to the embedding space of the Qwen2.5 model~\cite{qwen2.5}. The embedding of the prompt is utilized to determine the appropriate routing. Notably, in PaM, the routing guides the selection of the fusion parameters rather than the choice of the encoder. Experiments are conducted across three tasks: ASR, speaker number verification, and AC. On all datasets, including LibriSpeech~\cite{panayotov2015librispeech}, AMI~\cite{kraaij2005ami}, AIR-Bench (speaker number verification)~\cite{yang2024air}, and AudioCaps~\cite{audiocaps}, PaM consistently enhances performance in comparison to the best-performing single-encoder Speech LLM. The relative improvements are 15\%, 25\%, 3.4\%, and 7.6\%, respectively, based on the corresponding evaluation metrics. Additionally, we observed that without PaM, simple concatenation and averaging are generally effective for AC tasks; however, they result in performance declines for ASR. Our contributions can be summarized as follows:
\begin{itemize}

\item We propose a novel multi-encoder Speech LLM, which effectively leverages features from every layer of each encoder.
\item We introduce PaM, a specialized MoE method that incorporates a prompt-aware gating mechanism to assign distinct weights to each encoder and their layers based on the task.
\item We conducted comprehensive experiments demonstrating that PaM significantly enhances the overall performance of all downstream tasks. Additionally, we present detailed feature importance analyses and explore various combinations of speech encoders and LLMs. 

\end{itemize}

