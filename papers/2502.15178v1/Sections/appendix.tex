\newpage

\section{Appendix}
\label{sec:appendix}

\subsection{Details of Models and Datasets}\label{sec:appendix-datas-and-model}

In this paper, we leverage multiple audio encoders and LLM to construct the end-to-end speech LLM. Our training dataset is sourced from commonly used open-source datasets, totalling approximately 450 hours of audio data, corresponding to 313,208 samples, as outlined in Table~\ref{tab:whole-training-dataset}. 

\begin{table}[htp]
\small
% \scriptsize
\centering
\caption{The whole training datasets.}
\label{tab:whole-training-dataset}
\begin{tabular}{l c r c}
\toprule
Data Source & Task & Hours & Sample \\ 
\midrule
Librispeech-clean-100 & ASR & 100h & 28539 \\ 
AMI & ASR & 100h & 108502 \\ 
Common Voice V4 (Part) & SNV & $\sim$150h & 137041 \\ 
Audio Caption & AC & 100h & 39126 \\ 
\bottomrule
\end{tabular}
\end{table}

In our paper, we adopt multiple pre-trained audio encoders and LLMs, we list the architecture setting for all model we used in our experiments in Table~\ref{tab:model-setting-detail}. Notably, for the Whisper model, we only used its encoder part as an audio feature extractor.

\begin{table*}[htp]
\small
\centering
\begin{tabular}{l r c c c c c l}
\toprule
Audio Encoder Models & Enc Param & Layers & $d_{\text{model}}$ & $d_{\text{ffn}}$ & $d_k$ & $H$ & Norm \\ 
\midrule
\href{https://huggingface.co/openai/whisper-small}{openai/whisper-small} & 88M & 12 & 768 & 3072 & 64 & 12 & Pre \\
\href{https://huggingface.co/microsoft/wavlm-base-plus}{microsoft/wavlm-base-plus} & 94M & 12 & 768 & 3072 & 64 & 12 & Post \\
\href{https://huggingface.co/facebook/wav2vec2-base-960h}{facebook/wav2vec2-base-960h} & 94M & 12 & 768 & 3072 & 64 & 12 & Post \\
\href{https://huggingface.co/openai/whisper-medium}{openai/whisper-medium} & 307M & 24 & 1024 & 4096 & 64 & 16 & Pre \\
\href{https://huggingface.co/microsoft/wavlm-large}{microsoft/wavlm-large} & 315M & 24 & 1024 & 4096 & 64 & 16 & Post \\
\href{https://huggingface.co/facebook/wav2vec2-large-960h}{facebook/wav2vec2-large-960h} & 315M & 24 & 1024 & 4096 & 64 & 16 & Post \\
\midrule
Large Language Models & Lora Param & Layers & $d_{\text{model}}$ & $d_{\text{ffn}}$ & $d_k$ & $H$ & Norm \\
\midrule
\href{https://huggingface.co/Qwen/Qwen2.5-3B}{Qwen/Qwen2.5-3B} & 7M & 36 & 2048 & 11008 & 128 & 16 & Pre \\
\href{https://huggingface.co/Qwen/Qwen2.5-7B}{Qwen/Qwen2.5-7B} & 10M & 28 & 3584 & 18944 & 128 & 28 & Pre \\
\href{https://huggingface.co/meta-llama/Llama-3.2-3B}{meta-llama/Llama-3.2-3B} & 9M & 28 & 3072 & 128256 & 128 & 24 & Pre \\
\href{https://huggingface.co/meta-llama/Llama-3.1-8B}{meta-llama/Llama-3.1-8B} & 13M & 32 & 4096 & 14336 & 128 & 32 & Pre \\
\bottomrule
\end{tabular}
\caption{The settings of pre-trained model we used in our experiments. For the audio encoder models, we utilize only the encoder component and freeze all parameters. For the LLMs, we freeze the base model parameters and apply LoRA adapters to fine-tune the model.}


\label{tab:model-setting-detail}
\end{table*}

\begin{table*}[htp]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{c c c c c c c c c c c c c c}
\toprule
\multirow{2}{*}{Encoders} & \multirow{2}{*}{Whisper} & \multirow{2}{*}{WavLM} & \multirow{2}{*}{Wav2Vec2} & \multirow{2}{*}{HuBERT} & \multicolumn{2}{c}{LibriSpeech} & AMI & SNV & AudioCaps & AudioCaps QA & \multicolumn{2}{c}{Avg} & \multirow{2}{*}{AVG Rank} \\ 
\cmidrule(r){6-13}
& & & & & WER(clean)$\downarrow$ & WER(other)$\downarrow$ & WER$\downarrow$ & Acc$\uparrow$ & METEOR$\uparrow$ & METEOR$\uparrow$ & $\downarrow$ & $\uparrow$ & \\ 
\midrule
1 & $\surd$ & - & - & - & 9.61 & 16.73 & 16.27 & 18.8\% & 32.96 & 15.04 & 14.20 & 22.27 & 11.67 \\
1 & - & $\surd$ & - & - & 5.59 & 10.57 & 18.97 & 41.4\% & 27.14 & 12.77 & 11.71 & 27.10 & 11.50 \\
1 & - & - & $\surd$ & - & 4.30 & 09.46 & 26.69 & 39.0\% & 23.81 & 11.20 & 13.48 & 24.67 & 11.67 \\
1 & - & - & - & $\surd$ & 7.47 & 13.85 & N & 31.1\% & 23.94 & 11.28 & N & 22.10 & 14.00 \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{Best-1} & 4.30 & 9.46 & 16.27 & 41.4\% & 32.96 & 15.04 & 13.13 & 24.04 & \\
\midrule
2 & $\surd$ & $\surd$ & - & - & 5.07 & 09.50 & 13.59 & 49.5\% & \textbf{35.47} & 16.16 & 9.38 & 33.71 & 06.00 \\
2 & $\surd$ & - & $\surd$ & - & 3.82 & 07.55 & 13.51 & 38.4\% & 35.43 & 15.55 & 8.29 & 29.79 & 05.83  \\
2 & $\surd$ & - & - & $\surd$ & 4.37 & 09.70 & 14.79 & 43.2\% & 35.33 & 16.62 & 9.62 & 31.72 & 06.50 \\
2 & - & $\surd$ & $\surd$ & - & 3.17 & 06.76 & 19.60 & \textbf{61.2\%} & 31.70 & 14.89 & 9.84 & 35.93 & 05.83 \\
2 & - & $\surd$ & - & $\surd$ & 3.96 & 07.64 & 22.24 & 31.7\% & 30.28 & 13.99 & 11.28 & 25.32 & 10.33 \\
2 & - & - & $\surd$ & $\surd$ & 3.27 & 07.29 & 21.43 & 35.8\% & 29.85 & 14.62 & 10.66 & 26.76 & 09.00 \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{Best-2} & 3.17 & 06.76 & 13.51 & 61.2\% & 35.47 & 16.62 & 9.85 & 36.65 & \\
\midrule
3 & $\surd$ & $\surd$ & $\surd$ & - & 3.65 & 07.07 & \textbf{12.79} & 42.8\% & \textbf{35.47} & 15.70 & \textbf{7.83} & 31.32 & 04.00 \\
3 & $\surd$ & $\surd$ & - & $\surd$ & 4.42 & 10.26 & 13.47 & 47.4\% & 35.32 & 16.62 & 9.38 & 33.11 & 06.50 \\
3 & $\surd$ & - & $\surd$ & $\surd$ & 4.58 & 06.99 & 15.11 & 59.1\% & 35.33 & 16.62 & 8.89 & \textbf{37.02} & 05.50 \\
3 & - & $\surd$ & $\surd$ & $\surd$ & \textbf{3.09} & \textbf{06.64} & 22.80 & 26.0\% & 31.90 & 15.14 & 10.84 & 24.35 & 07.67 \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{Best-3} & 3.09 & 06.64 & 12.79 & 59.1\% & 35.47 & 16.62 & 9.24 & 31.45 & \\
\midrule
4 & $\surd$ & $\surd$ & $\surd$ & $\surd$ & 3.94 & 07.28 & 14.06 & 57.3\% & 35.37 & \textbf{16.79} & 8.43 & 36.49 & 04.00 \\
\bottomrule
\end{tabular}}
\caption{Detailed results of incorporating different combinations of audio encoders.}
\label{tab:results-multiple-encoder}
\end{table*}

\subsection{Details of Baseline Implement}\label{sec:appendix-baseline-implement}

In our work, we compare our method against two types of baselines. The first baseline consists of models using a single encoder, while the second baseline involves fusing multiple audio encoders either based on previous work~\cite{hu2024wavllmrobustadaptivespeech, tang2024salmonn} or through an averaging operation. 

For the single encoder baseline, we train the model using the same settings as in our method. For the second baseline, we train the model using the open-source codebases from \href{https://github.com/microsoft/SpeechT5/tree/main/WavLLM}{WavLLM} and \href{https://github.com/bytedance/SALMONN}{SALMONN}, we integrate the adapter components from these repositories into our code and train the baseline model using our training data, employing the same pre-trained audio encoders and LLMs as in our method. During training, we applied the same hyperparameters as our method. Since we use different encoders and LLMs compared to the baselines, we adjust the dimensions of the adapter to match the specific audio encoder and LLM we used while maintaining other dimensions independent of the audio encoders and LLM unchanged. Notably, we trained the SALMONN with query length=32 (as training with the original setting query length=1 failed) to ensure comparable performance with the other baseline methods.

\subsection{All result}

The detailed results of our experiments with multiple encoders are summarized in Table~\ref{tab:results-multiple-encoder}. We observe that, in most cases, the audio encoder that performs well on a single task also enhances the performance of the fusion model on that task. In cases where performance degradation occurs on a specific task when using the corresponding encoder, the fusion model consistently includes the HuBERT audio encoder, suggesting that incorporating the HuBERT model may have a detrimental effect. This could be attributed to the fact that the HuBERT model is trained on a smaller pre-trained dataset compared to other audio encoders. Notably, even in this case, fusing four audio encoders yields comparable results to fusing three encoders on the AVG Rank, indicating that incorporating more encoders can still lead to performance improvements.
