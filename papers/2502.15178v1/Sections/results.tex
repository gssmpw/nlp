\begin{table*}[t]
\scriptsize
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{l c c c c c c c}
\toprule
\multirow{2}{*}{Encoders} & \multicolumn{2}{c}{LibriSpeech} & AMI & SNV & AudioCaps & AudioCaps QA & \multirow{2}{*}{AVG Rank$\downarrow$} \\ 
\cmidrule(r){2-7}
& WER(clean)$\downarrow$ & WER(other)$\downarrow$ & WER$\downarrow$ & Acc$\uparrow$ & METEOR$\uparrow$ & METEOR$\uparrow$ & \\ 
\midrule
 Whisper+X & 4.42 & 8.92 & 13.96 & 43.70\% & \textbf{35.41} & 16.11 & 4.5 \\
WavLM+X & 4.07 & 7.97 & 18.47 & 47.47\% & 32.48 & 15.01 & 5.5 \\
Wav2Vec2+X & \textbf{3.42} & 7.20 & 18.18 & 45.13\% & 32.33 & 15.02 & 4.3 \\
HuBERT+X & 3.87 & 8.21 & 19.49 & 36.90\% & 31.82 & 15.08 & 6.8 \\
\midrule
Whisper+X+Y & 4.22 & 8.11 & \textbf{13.79} & \textbf{49.77\%} & 35.37 & \textbf{16.31} & \textbf{3.0} \\
WavLM+X+Y & 3.72 & 7.99 & 16.35 & 38.73\% & 34.23 & 15.82 & 4.0 \\
Wav2Vec2+X+Y & 3.77 & \textbf{6.90} & 16.90 & 42.63\% & 34.23 & 15.82 & 3.8 \\
HuBERT+X+Y & 4.03 & 7.96 & 17.13 & 44.17\% & 34.18 & 16.13 & 4.0 \\
\bottomrule
\end{tabular}}
\caption{Results with different combinations of encoders. The first and last four rows represent combinations of two and three encoders respectively. Each row's results are the average performance of a fixed encoder paired with all possible combinations of one or two other encoders, highlighting the unique strengths of each encoder.}
\label{tab:multiencoder}
\end{table*}

\begin{figure}[t]
\centering
  \centerline{\includegraphics[width=1.0\linewidth]{./Figures_alone/encoders_layer_fusion_weight.pdf}}  
  \vspace{-1.0em}
  \caption{The weights for each encoder and its layers.}
  \label{fig:hidden-layer-importance}
\vspace{-1.0em}
\end{figure}

\section{Results}\label{sec:results}

\subsection{Main Results}

As demonstrated in Table~\ref{tab:main}, we compare the proposed PaM method with single and multi-encoder baselines. Each encoder exhibits distinct advantages. When utilizing a single encoder, the Speech LLM with the Whisper encoder performs best on the AMI dataset and AC tasks. The primary reason is that the Whisper model is trained on a vast amount of speech data, exposing it to diverse acoustic conditions. Consequently, it excels in challenging ASR and AC tasks in real-world environments and noisy conditions. The WavLM encoder, trained on multi-speaker speech signals, provides the best features for the SNV task. The Wav2Vec2 encoder performs best on the LibriSpeech dataset mainly because it was pretrained on this dataset. However, since the LibriSpeech dataset consists of clean audiobooks, the Speech LLM with the Wav2Vec2 encoder shows poor performance on the AMI and AudioCap datasets.

We reimplemented the feature fusion methods of WavLLM~\cite{hu2024wavllmrobustadaptivespeech} and SALMONN~\cite{tang2024salmonn}, training the Speech LLM with the three audio encoders in our setups. Both methods use concatenation but are followed by different projection layers: WavLLM with a linear layer and SALMONN with a Q-former layer. Additionally, we implemented a simple averaging method that directly computes the average of the features from the three encoders. Compared to the best performance of single encoder baselines, all three fusion methods achieve better METEOR scores on AC tasks. However, performance may degrade on other tasks. For example, we observed performance degradation for all three methods on the LibriSpeech test-clean subset. This is expected since the same features are used for all tasks. Features containing more useful acoustic information for AC tasks may lack useful semantic information for ASR tasks.

PaM consistently outperforms all single encoder baselines, delivering performance improvements across all tasks. This consistent improvement can be attributed to the MoE adapter, which provides unique features tailored for each task. Compared to other fusion methods (i.e., concatenation and averaging), PaM achieves significantly lower WERs on the LibriSpeech and AMI datasets and similar performance on SNV and AC tasks.

\subsection{Feature Importance}

In Figure~\ref{fig:hidden-layer-importance}, we visualize the fusion weights for each expert, excluding the shared expert, which can be interpreted as the fusion weight for each task. To enhance clarity, we summed the weights for every four layers, resulting in the total weight for shallow, middle, and deep layers. Generally, different tasks require different features, so each expert has distinct fusion weights. Specifically, when the expert for ASR-clean is activated, it mainly focuses on features from WavLM and Wav2Vec2, especially the deep layers. When the expert for ASR-noise is activated, it primarily focuses on features from the Whisper encoder and WavLM. For SNV and AC tasks, all three encoders have similar fusion weights. For the SNV task, features from the middle layers are more important, while for the AC task, features from the shallow layers are more important.

\subsection{Combinations of Encoders}

In Table~\ref{tab:multiencoder}, we extend our investigation to encompass more combinations of encoders, including two and three encoders, and incorporate the HuBERT encoder. To highlight the strengths of each encoder, we calculate the performance by keeping one encoder fixed and varying the other encoders, then computing the average. The average (AVG) rank reflects the overall multitask performance. It is evident that using three encoders significantly outperforms using two encoders in all combinations, thereby demonstrating the effectiveness of employing more encoders for Speech LLMs.
 
We can observe that different encoders offer varying benefits, which proves that the tasks-specific encoders could significantly improve the performance on corresponding tasks. For example, when Whisper is used, regardless of whether two or three encoders are employed, the Speech LLM achieves the lowest WER on AMI and the highest METEOR scores on AudioCaps. On the other hand, Wav2Vec2 provides an advantage for recognizing speech signals in LibriSpeech. This indicates that when selecting encoders for Speech LLM, it is essential to consider the domain, downstream tasks, and the capabilities of each encoder. It is suggested to use a robust general domain model like Whisper in combination with domain-specific encoders such as Wav2Vec2.

\begin{table*}[t]
\tiny
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{l c c c c c c}
\toprule
\multirow{2}{*}{\makecell{Models}} & \multicolumn{2}{c}{LibriSpeech} & AMI & SNV & AudioCaps & AudioCaps QA \\ 
\cmidrule(r){2-7}
& WER(clean)$\downarrow$ & WER(other)$\downarrow$ & WER$\downarrow$ & Acc$\uparrow$ & METEOR$\uparrow$ & METEOR$\uparrow$ \\ 
\midrule
Base PaM & 3.65 & 07.07 & 12.79 & 42.8\% & 35.47 & 15.70 \\
\midrule
\rowcolor{black!6} PaM with Larger Encoders & & & & & & \\
\quad- \ding{172} Whisper-Medium & 3.93 & 07.93 & 12.43 & 47.5\% & 35.61 & 15.58 \\
\quad- \ding{173} WavLM-Large & 3.75 & 06.43 & 12.10 & 45.6\% & 35.95 & 16.71 \\
\quad- \ding{174} Wav2Vec2-Large & 3.74 & 06.49 & 15.06 & 47.6\% & 35.50 & \textbf{16.74} \\
\quad- \ding{172} + \ding{173} + \ding{174} & \textbf{3.58} & \textbf{05.93} & \textbf{11.51} & \textbf{56.9\%} & \textbf{36.94} & 16.50 \\
\midrule
\rowcolor{black!6} PaM with different LLMs & & & & & & \\
\quad- Qwen2.5-7B & 3.68 & 08.36 & 15.26 & 43.7\% & 35.46 & 15.71 \\
\quad- LLaMA3.2-3B & 4.98 & 11.57 & 15.57 & 50.5\% & 35.83 & 16.34 \\
\quad- LLaMA3.1-8B & 4.85 & 08.87 & 15.01 & 50.8\% & 35.81 & 15.82 \\
\bottomrule
\end{tabular}}
\caption{Results with larger encoders and various LLMs. To enhance performance, we replaced the Base version's encoders and experimented with different LLMs.}
\label{tab:llm}
\end{table*}

\subsection{Larger Encoders and LLMs}

We try to further enhance performance by replacing the encoders in the proposed method with their larger versions (Table~\ref{tab:llm}). Specifically, we replace the Whisper-Small encoder with the Whisper-Medium encoder, Wav2Vec2-Base-960h with Wav2Vec2-Large-960h, and WavLM-Base-Plus with WavLM-Large. Our observations indicate that on the LibriSpeech-clean dataset, performance does not significantly improve and may even slightly degrade. However, for the SNV and AC tasks, performance consistently improves, suggesting that more challenging sound-related tasks benefit more from better encoders. Additionally, we observe that when all encoders are replaced with their larger versions, we achieve the best performance across almost all tasks, albeit at the cost of increased computation.

In our investigation of other LLMs, including Qwen2.5-7B, LLaMA3.2-3B, and LLaMA3.1-8B, we observed some improvements in certain tasks. However, the overall performance was not superior to that of Qwen2.5-3B. The potential reason for this is that we used short audios, and both the prompts and answers were brief, thereby not fully utilizing the strong semantic understanding capabilities of the larger LLMs. Consequently, we opted to use Qwen2.5-3B in this paper. It is important to emphasize that for Speech LLMs, the extracted features may be more critical than the LLM itself for many downstream tasks.

\subsection{Parameters of the Adapter}

\begin{figure}[tp]
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{./Figures_alone/param_eff_bar_figure_small.pdf}}  
  \caption{Performance comparison of a smaller PaM (29M parameters) with Concatenation (37M parameters) and Average (24M parameters).}
\label{fig:small}
\end{figure}

In PaM, we employ multiple experts, merge and concatenate various features. Consequently, the number of parameters is slightly higher than that of the baseline Concatenation and Average methods. To ensure a fair comparison, we reduce the dimensionality within PaM, resulting in a parameter count of only 29M, which is similar to the baselines. As observed in Table~\ref{fig:small}, PaM outperforms or performs comparably to the baselines across all tasks.

