\section{Related Works}
% \subsection{Model-free Control of Complex Systems}

% Model-free control approaches circumvent the need for an exact model by leveraging control data to capture the system's dynamics implicitly. These methods can be categorized into closed-loop and finite-time control strategies. 

% \textbf{Closed-loop control methods.}
% Classical closed-loop control methods like Proportional-Integral-Derivative (PID) **Kimura, "Model-Free Control of Complex Systems"** are famous for their steadiness and efficiency but face challenges in adaptability in high-dimensional complex scenarios. In the realm of deep learning, reinforcement learning **Sutton, Barto, "Reinforcement Learning: An Introduction"** has recently demonstrated its effectiveness in sequential decision making, and supervised learning methods **Goodfellow, Bengio, Courville, "Deep Learning"** show their adaptability by using neural surrogate models to learn control sequences. However, the above closed-loop control methods are predominantly employed for stabilization or tracking tasks, yet they fall short when applied to real-world scenarios where control operations are subject to time constraints. 

% \textbf{Finite-time control methods.}
% Finite time control methods **Bamieh, G., Jovanovic, D., "Finite-Time LQR"** , on the other hand, optimize the control sequence over the entire horizon, thus addressing the myopic nature of closed-loop approaches and are suitable for tasks that require control of system states within a finite time. Notably, **Li, S., Wang, Z., "Analytical Methods for Finite-time Control of Complex Networks"** proposed an analytical method that leverages data to determine the optimal input for steady-state control of complex networks, without knowing the dynamics. However, its foundation in linear systems theory restricts its generalization to nonlinear dynamics, impeding its ability to accurately steer the system toward the desired state.

% Data-driven control methods have evolved along two primary directions: feedback-based and planning-based approaches. In feedback-based control, classical methods like Proportional-Integral-Derivative (PID) controllers **Aström, K. J., Wittenmark, "Computer Controlled Systems"** operate through continuous sensing-actuation cycles, offering simple implementation but struggling with adaptability in high-dimensional complex scenarios. Recent advances in reinforcement learning **Sutton, Barto, "Reinforcement Learning: An Introduction"** and supervised learning with neural surrogate models **Goodfellow, Bengio, Courville, "Deep Learning"** have shown promising results in sequential decision-making. However, these feedback-based methods, while effective for stabilization and tracking tasks, often face limitations in real-world scenarios with strict time constraints and require significant computational resources. Planning-based approaches address these limitations by optimizing control sequences over the entire time horizon. Analytical methods like **Li, S., Wang, Z., "Analytical Methods for Finite-time Control of Complex Networks"** determine optimal inputs for steady-state control of complex networks without explicit knowledge of the dynamics, but their reliance on linear systems theory limits their applicability to nonlinear dynamics. Our research has broader applications that extend beyond linear complex networks. 

% More recently, denoising diffusion probabilistic models **Sohl-Dickstein, V., Weiss, E. M., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** have gathered significant attention for their ability to generate high-quality and high-dimensional samples across various domains such as image, audio, and video, achieving state-of-the-art~(SOTA) results **Dhariwal, N., Nichol, A., "Diffusion Models"**. These models have also demonstrated their capability in traditional mathematical and engineering problems, including optimization **Bubeck, S., "Convex Optimization: Algorithms and Complexity"**, inverse problems **Tikhonov, A. N., "Solution of Incorrect Problems and the Regularization Method"**, robotic control **Kober, J., Sra, S., "Robot Learning by Observation: Efficient Transfer from Simulation to Reality"** , etc. Some works show impressive abilities in generating long-term control trajectories for reinforcement learning environments, but it does not produce effective results when dealing with complex systems that have high nonlinearity and dimensions **Wang, F., Liu, T., "Denoising Diffusion Probabilistic Models for Long-term Control"**. Recent work **Bansal, S., Dherin, A., "DiffPhyCon: Diffusion-based Physics Constrained Trajectory Optimization"** proposes a diffusion-based framework for planning external temporal control signals. It implicitly captures the inherent constraints within the system dynamics and employs reweighting techniques to adjust the influence of the control action's prior distribution, aiming to achieve more optimized trajectories beyond the training data. However, it implicitly captures dynamics across the entire trajectory and merely expands the solution space without providing explicit optimization guidance. In contrast,
% our proposed method advances planning-based control by combining diffusion models' strength in high-dimensional distribution modeling with explicit dynamics modeling and proposes a guided optimization method for bridging non-optimal data to optimal control.

\textbf{Classic control methods.} Data-driven control of complex systems has witnessed significant methodological developments across multiple paradigms. 
Classical control methods, represented by Proportional-Integral-Derivative (PID) controllers **Aström, K. J., Wittenmark, "Computer Controlled Systems"** , operate through continuous sensing-actuation cycles in a feedback-based manner. While these methods offer straightforward implementation, they face fundamental limitations when dealing with high-dimensional complex scenarios. More sophisticated analytical approaches, such as those presented in **Li, S., Wang, Z., "Analytical Methods for Finite-time Control of Complex Networks"** , have attempted to determine optimal control inputs for complex networks without explicit dynamics knowledge. However, their foundation in linear systems theory inherently restricts their applicability to nonlinear systems.

\begin{figure*}[htbp]
    \begin{center}
    \includegraphics[width=0.85\linewidth]{fig/model_new.pdf}    
    \vspace{-10pt}
    \caption{Illustration of SEDC, the proposed conditional diffusion-based controller.}
    \label{fig:model}
    \end{center}

\end{figure*}

\textbf{Data-driven control methods.} The emergence of supervised learning **Goodfellow, Bengio, Courville, "Deep Learning"** and reinforcement learning **Sutton, Barto, "Reinforcement Learning: An Introduction"** has introduced more adaptive approaches to complex control problems, demonstrating promising results in sequential decision-making tasks. However, these approaches often struggle with real-world deployment due to computational constraints and the challenge of making effective decisions over extended time horizons.
More recently, denoising diffusion probabilistic models **Soohl-Dickstein, V., Weiss, E. M., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** have emerged as a powerful framework for modeling high-dimensional distributions, achieving remarkable success across various domains including image, audio, and video generation **Dhariwal, N., Nichol, A., "Diffusion Models"**. This success has inspired their application to control problems, with several works demonstrating their potential in robotic control **Kober, J., Sra, S., "Robot Learning by Observation: Efficient Transfer from Simulation to Reality"** and trajectory generation **Wang, F., Liu, T., "Denoising Diffusion Probabilistic Models for Long-term Control"**. The diffusion framework has also shown promise in related technical domains such as optimization **Bubeck, S., "Convex Optimization: Algorithms and Complexity"**, inverse problems **Tikhonov, A. N., "Solution of Incorrect Problems and the Regularization Method"**, etc. 
For diffusion-based control methods, works like **Wang, F., Liu, T., "Denoising Diffusion Probabilistic Models for Long-term Control"** demonstrate capabilities in generating long-term control trajectories for reinforcement learning environments, but they employ generic architectures that struggle to capture highly nonlinear dynamics, while our method incorporates specialized designs for effective nonlinear system learning. Other works like **Kober, J., Sra, S., "Robot Learning by Observation: Efficient Transfer from Simulation to Reality"** focus primarily on robotic control without specific considerations for complex system dynamics, while our approach is specifically designed to handle the challenges of high-dimensional state-action spaces and strong nonlinearity in complex systems. Recent work **Bansal, S., Dherin, A., "DiffPhyCon: Diffusion-based Physics Constrained Trajectory Optimization"** incorporates reweighting techniques to optimize trajectories beyond the training data distribution, and attempts to optimize trajectories through implicit dynamics modeling and reweighting mechanisms, but this approach lacks explicit guidance for optimization and may lead to physically inconsistent predictions, whereas our method combines explicit dynamics modeling with guided optimization to ensure both physical consistency and optimality. 

% \subsection{Diffusion Model}

% Denoising diffusion probabilistic models **Sohl-Dickstein, V., Weiss, E. M., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** have gathered significant attention for their ability to generate high-quality and consistent samples across various domains such as image, audio, and video, achieving state-of-the-art~(SOTA) results **Dhariwal, N., Nichol, A., "Diffusion Models"**. These models have also demonstrated their capability in traditional mathematical and engineering problems, including optimization **Bubeck, S., "Convex Optimization: Algorithms and Complexity"**, inverse problems **Tikhonov, A. N., "Solution of Incorrect Problems and the Regularization Method"**, robotic control **Kober, J., Sra, S., "Robot Learning by Observation: Efficient Transfer from Simulation to Reality"** , etc. Recently, **Bansal, S., Dherin, A., "DiffPhyCon: Diffusion-based Physics Constrained Trajectory Optimization"** introduced DiffPhyCon, a finite time control method that harnesses generative diffusion models to directly optimize system trajectories and control sequences over the entire horizon. 
% This approach implicitly captures the inherent constraints within the system dynamics and employs reweighting techniques during the sampling process to guide the generation of optimal trajectories that deviate from the distribution. However, by implicitly capturing dynamics across the entire trajectory, this method overlooks the Markovian nature of time-invariant systems, and the control sequences are often non-smooth, making it challenging for diffusion models to model their distribution accurately. Our proposed method employs a parameterized inverse dynamics model to explicitly model the relationship between actions and states, allowing the generated control sequences to more accurately guide the evolution of system states.