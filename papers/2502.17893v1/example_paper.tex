%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage[ruled,vlined]{algorithm2e} % 引入算法包
% \usepackage{amsmath} % 数学公式支持
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sample-efficient diffusion-based control of complex nonlinear systems}

\begin{document}

\twocolumn[
\icmltitle{Sample-efficient diffusion-based control of complex nonlinear systems}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hongyi Chen}{yyy,sch}
\icmlauthor{Jingtao Ding}{comp}
\icmlauthor{Jianhai Shu}{comp}
\icmlauthor{Xinchun Yu}{xxx}
\icmlauthor{Xiaojun Liang}{sch}
\icmlauthor{Yong Li}{comp}
\icmlauthor{Xiao-Ping Zhang}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Shenzhen International Graduate School, Tsinghua University, China}
\icmlaffiliation{comp}{Department of Electronic Engineering, Tsinghua University, China}
\icmlaffiliation{sch}{Peng Cheng Laboratory, Shenzhen, China}
\icmlaffiliation{xxx}{School of Computer Science and Technology, Zhejiang Gongshang University, China}

\icmlcorrespondingauthor{Jingtao Ding}{dingjt15@tsinghua.org.cn}
\icmlcorrespondingauthor{Xiao-Ping Zhang}{xpzhang@ieee.org}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Complex nonlinear system control faces challenges in achieving sample-efficient, reliable performance. While diffusion-based methods have demonstrated advantages over classical and reinforcement learning approaches in long-term control performance, they are limited by sample efficiency. This paper presents SEDC (Sample-Efficient Diffusion-based Control), a novel diffusion-based control framework addressing three core challenges: high-dimensional state-action spaces, nonlinear system dynamics, and the gap between non-optimal training data and near-optimal control solutions. Through three innovations - Decoupled State Diffusion, Dual-Mode Decomposition, and Guided Self-finetuning - SEDC achieves 39.5\%-49.4\% better control accuracy than baselines while using only 10\% of the training samples, as validated across three complex nonlinear dynamic systems. Our approach represents a significant advancement in sample-efficient control of complex nonlinear systems. The implementation of the code can be found at https://anonymous.4open.science/r/DIFOCON-C019.
\end{abstract}

\section{Introduction}
\label{Introduction}

% Complex systems are composed of interacting components that exhibit emergent behaviors and nonlinear dynamics~\citep{ladyman2013complex}. Control problems are fundamental in both natural and engineering systems, where the goal is to direct system behavior toward desired outcomes by determining appropriate inputs. Traditional control methods often rely on precise mathematical models of a system~\citep{yan2012controlling,lindmark2018minimum,liu2011controllability,gao2014target}, but these models may not be feasible to obtain or may not capture all system dynamics accurately, especially in complex or real-world scenarios~\citep{citehere}. Data-driven approaches, on the other hand, use real-time collected data to infer system behavior and generate control strategies, making them highly adaptable and effective for systems with uncertain dynamics~\citep{citehere}. 
% The control of complex systems presents a fundamental challenge across engineering and natural sciences, where data-driven approaches have emerged as a promising solution~\citep{ladyman2013complex}. These control problems aim to achieve desired system behaviors through appropriate input signals, but traditional model-based methods often fall short when faced with real-world complexity~\citep{yan2012controlling,liu2011controllability}. Data-driven control strategies overcome these limitations by directly leveraging measured data to infer system dynamics and generate control policies, offering practical solutions for a wide range of applications, such as robotics~\citep{zhang2022pde}, fluid control~\citep{holl2020learning}, swing systems~\citep{bottcher2022ai}, among others.

The control of complex systems plays a critical role across diverse domains, from industrial automation~\citep{baggio2021data} and biological networks~\citep{gu2015controllability} to robotics~\citep{zhang2022pde}. Given the challenges in deriving governing equations for empirical systems, data-driven control methods—which design control modules directly based on experimental data collected from the system, bypassing the need for explicit mathematical modeling—have gained prominence for their robust real-world applicability~\cite{baggio2021data,janner2022planning,ajay2022conditional,zhou2024adaptive,liang2023adaptdiffuser,wei2024generative,ding2024artificialintelligencecomplexnetwork}.

% Various data-driven control methodologies have been proposed, yet they fall short of exploring long-term control under high-dimensional, highly nonlinear systems. Feedback-based control methods, such as PID and reinforcement learning~\citep{li2006pid,hwang2022solving,pomerleau1988alvinn,haarnoja2018soft}, rely on continuous cycles of sensing, processing, and decision-making, which makes them computationally intensive and often unsuitable for high-dimensional, finite-time control problems. 
% \citet{baggio2021data} developed an optimal control solution for linear systems by implicitly estimating system equations and parameters from empirical data, but their approach struggles when applied to nonlinear systems. Recently, diffusion models have shown promising capability of providing efficient long-term simulation~\citep{citehere} and control trajectory generation\citep{janner2022planning,ajay2022conditional,wei2024generative}, outperforming both classical control and reinforcement learning methods in complex systems control. What's more, the learned diffusion-based controller demonstrates the useful capability of planning for new rewards without retraining, as well as obtaining long-term control vision for optimal solutions~\citep{janner2022planning,wei2024generative}.

% Various data-driven control methodologies have emerged for complex systems, yet achieving long-term control in high-dimensional, highly nonlinear settings remains challenging. 
% Feedback-based methods, including PID and reinforcement learning, operate through continuous sensing-actuation cycles. 
% 按照ICLR25的分类方法，和实验部分一致
% While PID controllers offer simple implementation, they struggle with high-dimensional nonlinear systems~\citep{li2006pid}, and reinforcement learning, despite its broader applicability, faces challenges of high computational costs and sample inefficiency~\citep{hwang2022solving,haarnoja2018soft}. Planning-based approaches, which generate complete control trajectories before execution, present an alternative direction. Analytical methods like~\citep{baggio2021data} work well for linear systems but struggle with nonlinearity, while recent diffusion models have shown promising results in generating long-term control trajectories and adapting to new reward functions without retraining~\citep{janner2022planning,ajay2022conditional,wei2024generative}.
% 结论：diffusion-based方法的优势
% Before the prominence of data-driven control methods, in the field of complex system control, traditional approaches like Proportional-Integral-Derivative (PID)~\cite{li2006pid} controllers have long been the dominant methodology. PID regulates systems through continuous error correction using proportional, integral, and derivative terms. However, these classical methods face fundamental limitations when dealing with complex nonlinear systems characterized by delayed responses and multi-variable couplings, primarily due to their linear control law nature. The emergence of data-driven machine learning approaches has offered promising solutions to overcome these limitations by learning nonlinear control policies directly from system interaction data. These methods can be broadly categorized into three categories: supervised learning-based methods, reinforcement learning (RL)-based methods, and diffusion-based methods. Supervised learning approaches like Behavior Cloning (BC)~\cite{pomerleau1988alvinn} learn a direct mapping from states to control actions by minimizing the deviation between predicted actions and expert demonstrations, while RL methods like Batch Proximal Policy Optimization (BPPO)~\cite{zhuang2023behavior}, leverage historical data to learn control policies through value function approximation, demonstrating superior adaptability to high-dimensional state spaces compared to classical methods. Nevertheless, these RL approaches frequently succumb to myopic decision-making in long-horizon control tasks, primarily due to their iterative view of long-term control dynamics, resulting in the lack of long-term vision and optimization. In contrast, diffusion-based methods~\cite{janner2022planning,ajay2022conditional,zhou2024adaptive,liang2023adaptdiffuser,wei2024generative} have emerged as a promising alternative by changing the control problem into sequence generation, taking a long-term optimization perspective over the system trajectory across the whole horizon, thereby enabling comprehensive optimization of control sequences. These advantages enable diffusion-based methods to overcome the limitations of both classical and reinforcement learning approaches, achieving superior long-term control performance in complex systems.

Before data-driven control methods, traditional Proportional-Integral-Derivative (PID)~\cite{li2006pid} controllers dominated complex system control through continuous error correction. However, these classical methods show limitations with complex nonlinear systems due to their linear control nature. Data-driven machine learning approaches have emerged to address these limitations by learning nonlinear control policies from interaction data, falling into three categories: supervised learning, reinforcement learning (RL), and diffusion-based methods. Supervised learning approaches like Behavior Cloning (BC)~\cite{pomerleau1988alvinn} learn direct state-to-action mappings from expert demonstrations, while RL methods like Batch Proximal Policy Optimization (BPPO)~\cite{zhuang2023behavior} learn control policies through value function approximation, showing better adaptability to high-dimensional states than classical methods. However, both approaches often exhibit myopic decision-making in long-horizon tasks due to their iterative view of control dynamics. In contrast, diffusion-based methods~\cite{janner2022planning,ajay2022conditional,zhou2024adaptive,liang2023adaptdiffuser,wei2024generative} reformulate control as sequence generation, enabling comprehensive optimization over entire system trajectories. This long-term perspective allows diffusion-based methods to overcome limitations of both classical and RL approaches, achieving superior long-term control performance.


% To tackle data-driven control problems of complex systems, various methodologies have been proposed, which can be broadly categorized into three categories: classical control methods, reinforcement learning (RL)-based methods, and emerging diffusion-based methods. The Proportional-Integral-Derivative (PID)~\cite{li2006pid} controller, as a representative classical method, regulates systems through continuous error correction using proportional, integral, and derivative terms. However, its linear control law fundamentally limits performance in complex nonlinear systems with delayed responses and multi-variable couplings. Regarding RL-based methods, the advent of deep learning has propelled offline RL algorithms like Behavior Cloning (BC)~\cite{pomerleau1988alvinn} and Batch Proximal Policy Optimization (BPPO)~\cite{zhuang2023behavior}, which leverage historical data to learn control policies through value function approximation, demonstrating superior adaptability to high-dimensional state spaces compared to classical methods. Nevertheless, these RL approaches frequently succumb to myopic decision-making in long-horizon control tasks, primarily due to their iterative view of long-term control dynamics, resulting in the lack of long-term vision and optimization. In contrast, diffusion-based methods have emerged as a promising alternative by taking a long-term optimization perspective over the system trajectory across the whole horizon, thereby enabling comprehensive optimization of control sequences. These advantages enable diffusion-based methods to overcome the limitations of both classical and reinforcement learning approaches, achieving superior long-term control performance in complex systems.

% However, despite recent advances in diffusion-based control methodologies, significant challenges persist in achieving optimal control of complex nonlinear systems under real-world constraints. 
% First, achieving optimal long-term performance under high-dimensional, nonlinear physical dynamics constraints remains elusive. While diffusion-based Offline Reinforcement Learning~\citep{janner2022planning,ajay2022conditional} approaches have demonstrated promise, they exhibit substantial limitations when confronted with the non-optimality of the collected training data, complexity of system dynamics and diversity of task requirements. The recently proposed DiffPhyCon framework~\citep{wei2024generative} introduces an innovative approach through concurrent generation of control inputs and state trajectories. However, this concurrent generation methodology relies on implicitly learning the transition dynamics through end-to-end training, where the relationship between states and actions is captured indirectly within the neural network's parameters rather than through explicit modeling of the transition dynamics. Under high-dimensional, nonlinear dynamics, this implicit learning paradigm is inadequate in maintaining dynamics consistency. These dynamics discrepancies directly impact the computation of objective constraints, which evaluate the optimality of generated trajectories. When the predicted next states deviate from physically realizable transitions, the calculated objective values become unreliable measures of true system performance. Consequently, during the iterative denoising process, the model generates trajectories based on these biased objective evaluations, leading to a compounding effect where dynamics inconsistencies and suboptimal control decisions mutually reinforce each other, progressively steering the generation process away from feasible optimal solutions. 
% The second challenge emerges from the inherent limitations in offline training data collection, which typically fails to encompass the complete distribution of achievable system states and often substantially deviates from optimal solutions for specific control objectives. While DiffPhyCon attempts to mitigate this through reweighting mechanisms designed to reconcile training and optimal target distributions, this approach remains fundamentally passive in nature. The methodology modulating the influence of control action~(signal) prior distribution merely expands the solution space without providing explicit optimization guidance, ultimately failing to address the core challenge of achieving optimal control trajectories with non-optimal data.

% However, despite recent advances in diffusion-based control methodologies, three significant challenges persist in achieving optimal control of complex nonlinear systems.
% diffusion方法依赖于收集的数据。
% While data-driven control approaches have increasingly emerged, data efficiency remains a critical challenge due to the high costs, safety risks, and operational constraints in real-world data collection.  This efficiency challenge becomes particularly critical as modern control systems grow increasingly complex and high-dimensional. Understanding how to achieve data-efficient control requires examining the key technical barriers that currently limit performance:



The success of diffusion models in data-driven control stems from their exceptional ability to learn complex trajectory distributions from empirical data. In practice, these trajectories are typically collected from systems operated under empirical rules or random policies. Moreover, due to operational costs, the available data volume is often limited. Diffusion-based methods must therefore learn effective control policies from such non-optimal and sparse trajectory data—a challenge that manifests in three key aspects.
% 挑战更多的结合sample efficiency
\textbf{First, limited data volume impedes sample-efficient learning in high-dimensional systems.} Existing diffusion-based controllers (e.g., DiffPhyCon~\citep{wei2024generative}) attempt to directly generate long-term~($T$ steps) state-action trajectories by learning a $T\times(P+M)$-dimensional distribution of system states $y^P$ and control inputs $u^M$.  This joint distribution implicitly encodes system dynamics of state transitions under external control inputs, which often leads to physically inconsistent trajectories when training samples are insufficient.
\textbf{Second, learning control policies for nonlinear systems remains an open challenge both theoretically and practically.} 
Traditional analytical methods~\citep{baggio2021data} designed for linear systems fail to perform robustly when applied to nonlinear systems.
While diffusion-based approaches~\citep{janner2022planning,ajay2022conditional,zhou2024adaptive} employ deep neural networks (e.g., U-Net architectures) as denoising modules to capture nonlinearity, learning effective control policies from limited data remains particularly challenging for complex systems with strong nonlinearity, such as fluid dynamics and power grids.
\textbf{Third, extracting improved control policies from non-optimal training data poses fundamental difficulties.} Diffusion-based methods~\cite{janner2022planning} struggle when training data significantly deviates from optimal solutions. Although recent work~\citep{wei2024generative} introduces reweighting mechanism to expand the solution space during generation, discovering truly near-optimal control policies remains elusive without explicit optimization guidance.

% Collectively, these challenges underscore sample efficiency as the central bottleneck in deploying diffusion-based controllers to real-world controlled systems. To solve the problem, targeted designs that specifically tackle these challenges are required.



% To address these fundamental challenges, we propose DIFOCON(To-be-determined), a novel diffusion-based framework for complex nonlinear system control. At its core, DIFOCON employs a diffusion model that progressively refines noisy inputs into optimal control sequences, guided by objective constraints $J$ that evaluate trajectory optimality. To address the first challenge of dynamics inconsistency under high-dimensional, nonlinear systems, we introduce two key architectural innovations. First, we reformulate the denoising network through a dual-UNet architecture with residual connections. The primary UNet reconstructs target state trajectories and establishes linear relationships between target and initial states, while the secondary UNet learns coefficients of higher-order expansions,  refining the first-order approximations through residual connections. Second, to overcome the limitations of implicit dynamics learning in high-dimensional spaces, we propose a novel modeling paradigm that applies diffusion exclusively in the system's state space, coupled with an inverse dynamics model for control input generation. This approach ensures explicit preservation of system dynamics constraints, maintaining consistency between generated trajectories and physical reality.
% To overcome the second challenge of non-optimal offline training data, we introduce Guided Iterative Finetuning (GIF), a novel methodology for progressive optimization. GIF synthesizes guided control trajectories that serve as training data for subsequent fine-tuning iterations. This approach enables systematic exploration of the solution space beyond the limitations of the initial training data distribution, facilitating convergence toward optimal control strategies. 
% 方法和挑战的对应：名字；解释（通过xxx方法实现了xxx）；每个挑战/方法：1~2句话
% 开始->建模成生成问题->3个挑战
To address these challenges, we propose SEDC (\textbf{S}ample-\textbf{E}fficient \textbf{D}iffusion-based \textbf{C}ontrol), a novel diffusion-based framework for learning control policies of complex nonlinear systems with limited, non-optimal data. At its core, SEDC reformulates the control problem as a denoising diffusion process that samples control sequences optimized for reaching desired states while minimizing energy consumption.
To address the curse of dimensionality, we introduce Decoupled State Diffusion (DSD), which confines diffusion process within state space and leverages inverse dynamics to generate control inputs, i.e., actions. This approach reduces learning complexity in high-dimensional systems while ensuring physics-aware control synthesis.
To tackle strong nonlinearity, we propose Dual-Mode Decomposition (DMD) by designing a dual-UNet denoising module with residual connections. This architecture decomposes system dynamics into hierarchical linear and nonlinear components, enabling structured modeling of complex systems.
To bridge the gap between non-optimal offline training data and optimal control policies, we introduce Guided Self-finetuning (GSF). This method progressively synthesizes guided control trajectories for iterative fine-tuning, facilitating exploration beyond initial training data and convergence toward near-optimal control strategies.

We demonstrate SEDC's superiority over traditional, reinforcement learning, and diffusion-based methods through experiments on three typical complex nonlinear systems. Our model demonstrates 39.5\%-49.4\% improvement in control accuracy compared to state-of-the-art baselines while maintaining better balance between accuracy and energy consumption. In sample efficiency experiments, SEDC matches state-of-the-art performance using only
10\% of the training samples. Additional ablation studies validate the effectiveness of SEDC's key design components.

% The primary contributions of this work can be summarized as follows:
% (1) We propose SEDC, a novel diffusion-based framework that achieves optimal control of complex nonlinear systems under high-dimensional physical constraints, significantly advancing the state-of-the-art in trajectory generation and control synthesis. (2) We introduce comprehensive methodological innovations, including a dual-UNet architecture with state-space diffusion and inverse dynamics modeling, coupled with a Guided Iterative Refinement mechanism, that collectively resolve the fundamental challenges of dynamics inconsistency and non-optimal training data, demonstrating superior performance across diverse control scenarios. (3) We evaluate...



\section{Related Works}


% \subsection{Model-free Control of Complex Systems}

% Model-free control approaches circumvent the need for an exact model by leveraging control data to capture the system's dynamics implicitly. These methods can be categorized into closed-loop and finite-time control strategies. 

% \textbf{Closed-loop control methods.}
% Classical closed-loop control methods like Proportional-Integral-Derivative (PID)~\citep{li2006pid} are famous for their steadiness and efficiency but face challenges in adaptability in high-dimensional complex scenarios. In the realm of deep learning, reinforcement learning~\citep{pomerleau1988alvinn,haarnoja2018soft} has recently demonstrated its effectiveness in sequential decision making, and supervised learning methods~\citep{hwang2022solving} show their adaptability by using neural surrogate models to learn control sequences. However, the above closed-loop control methods are predominantly employed for stabilization or tracking tasks, yet they fall short when applied to real-world scenarios where control operations are subject to time constraints. 

% \textbf{Finite-time control methods.}
% Finite time control methods~\citep{baggio2021data,wei2024generative}, on the other hand, optimize the control sequence over the entire horizon, thus addressing the myopic nature of closed-loop approaches and are suitable for tasks that require control of system states within a finite time. Notably, \citet{baggio2021data} proposed an analytical method that leverages data to determine the optimal input for steady-state control of complex networks, without knowing the dynamics. However, its foundation in linear systems theory restricts its generalization to nonlinear dynamics, impeding its ability to accurately steer the system toward the desired state.

% Data-driven control methods have evolved along two primary directions: feedback-based and planning-based approaches. In feedback-based control, classical methods like Proportional-Integral-Derivative (PID) controllers \citep{li2006pid} operate through continuous sensing-actuation cycles, offering simple implementation but struggling with adaptability in high-dimensional complex scenarios. Recent advances in reinforcement learning \citep{haarnoja2018soft} and supervised learning with neural surrogate models \citep{hwang2022solving} have shown promising results in sequential decision-making. However, these feedback-based methods, while effective for stabilization and tracking tasks, often face limitations in real-world scenarios with strict time constraints and require significant computational resources. Planning-based approaches address these limitations by optimizing control sequences over the entire time horizon. Analytical methods like \citep{baggio2021data} determine optimal inputs for steady-state control of complex networks without explicit knowledge of the dynamics, but their reliance on linear systems theory limits their applicability to nonlinear dynamics. Our research has broader applications that extend beyond linear complex networks. 

% More recently, denoising diffusion probabilistic models~\citep{ho2020denoising} have gathered significant attention for their ability to generate high-quality and high-dimensional samples across various domains such as image, audio, and video, achieving state-of-the-art~(SOTA) results~\citep{dhariwal2021diffusion,kong2020diffwave,ho2022video}. These models have also demonstrated their capability in traditional mathematical and engineering problems, including optimization~\citep{krishnamoorthy2023diffusion,sun2023difusco}, inverse problems~\citep{chung2022diffusion}, robotic control~\citep{janner2022planning,ajay2022conditional}, etc. Some works show impressive abilities in generating long-term control trajectories for reinforcement learning environments, but it does not produce effective results when dealing with complex systems that have high nonlinearity and dimensions\citep{janner2022planning,ajay2022conditional,liang2023adaptdiffuser,zhou2024adaptive}. Recent work \citep{wei2024generative} proposes a diffusion-based framework for planning external temporal control signals. It implicitly captures the inherent constraints within the system dynamics and employs reweighting techniques to adjust the influence of the control action's prior distribution, aiming to achieve more optimized trajectories beyond the training data. However, it implicitly captures dynamics across the entire trajectory and merely expands the solution space without providing explicit optimization guidance. In contrast,
% our proposed method advances planning-based control by combining diffusion models' strength in high-dimensional distribution modeling with explicit dynamics modeling and proposes a guided optimization method for bridging non-optimal data to optimal control.

\textbf{Classic control methods.} Data-driven control of complex systems has witnessed significant methodological developments across multiple paradigms. 
Classical control methods, represented by Proportional-Integral-Derivative (PID) controllers \citep{li2006pid}, operate through continuous sensing-actuation cycles in a feedback-based manner. While these methods offer straightforward implementation, they face fundamental limitations when dealing with high-dimensional complex scenarios. More sophisticated analytical approaches, such as those presented in \citep{baggio2021data}, have attempted to determine optimal control inputs for complex networks without explicit dynamics knowledge. However, their foundation in linear systems theory inherently restricts their applicability to nonlinear systems.

\begin{figure*}[htbp]
    \begin{center}
    \includegraphics[width=0.85\linewidth]{fig/model_new.pdf}    
    \vspace{-10pt}
    \caption{Illustration of SEDC, the proposed conditional diffusion-based controller.}
    \label{fig:model}
    \end{center}

\end{figure*}

\textbf{Data-driven control methods.} The emergence of supervised learning~\cite{pomerleau1988alvinn} and reinforcement learning~\cite{haarnoja2018soft,zhuang2023behavior} has introduced more adaptive approaches to complex control problems, demonstrating promising results in sequential decision-making tasks. However, these approaches often struggle with real-world deployment due to computational constraints and the challenge of making effective decisions over extended time horizons.
More recently, denoising diffusion probabilistic models \citep{ho2020denoising} have emerged as a powerful framework for modeling high-dimensional distributions, achieving remarkable success across various domains including image, audio, and video generation \citep{dhariwal2021diffusion,kong2020diffwave,ho2022video}. This success has inspired their application to control problems, with several works demonstrating their potential in robotic control \citep{janner2022planning,ajay2022conditional} and trajectory generation \citep{liang2023adaptdiffuser,zhou2024adaptive}. The diffusion framework has also shown promise in related technical domains such as optimization \citep{krishnamoorthy2023diffusion,sun2023difusco} and inverse problems \citep{chung2022diffusion}. 
For diffusion-based control methods, works like \citep{janner2022planning,ajay2022conditional} demonstrate capabilities in generating long-term control trajectories for reinforcement learning environments, but they employ generic architectures that struggle to capture highly nonlinear dynamics, while our method incorporates specialized designs for effective nonlinear system learning. Other works like \citep{liang2023adaptdiffuser,zhou2024adaptive} focus primarily on robotic control without specific considerations for complex system dynamics, while our approach is specifically designed to handle the challenges of high-dimensional state-action spaces and strong nonlinearity in complex systems. Recent work DiffPhyCon \citep{wei2024generative} incorporates reweighting techniques to optimize trajectories beyond the training data distribution, and attempts to optimize trajectories through implicit dynamics modeling and reweighting mechanisms, but this approach lacks explicit guidance for optimization and may lead to physically inconsistent predictions, whereas our method combines explicit dynamics modeling with guided optimization to ensure both physical consistency and optimality. 

% \subsection{Diffusion Model}

% Denoising diffusion probabilistic models~\citep{ho2020denoising} have gathered significant attention for their ability to generate high-quality and consistent samples across various domains such as image, audio, and video, achieving state-of-the-art~(SOTA) results~\citep{dhariwal2021diffusion,kong2020diffwave,ho2022video}. These models have also demonstrated their capability in traditional mathematical and engineering problems, including optimization~\citep{krishnamoorthy2023diffusion,sun2023difusco}, inverse problems~\citep{chung2022diffusion}, robotic control~\citep{janner2022planning,ajay2022conditional}, etc. Recently, \citet{wei2024generative} introduced DiffPhyCon, a finite time control method that harnesses generative diffusion models to directly optimize system trajectories and control sequences over the entire horizon. 
% This approach implicitly captures the inherent constraints within the system dynamics and employs reweighting techniques during the sampling process to guide the generation of optimal trajectories that deviate from the distribution. However, by implicitly capturing dynamics across the entire trajectory, this method overlooks the Markovian nature of time-invariant systems, and the control sequences are often non-smooth, making it challenging for diffusion models to model their distribution accurately. Our proposed method employs a parameterized inverse dynamics model to explicitly model the relationship between actions and states, allowing the generated control sequences to more accurately guide the evolution of system states.



\section{Backgrounds}


\subsection{Problem Setting}

The dynamics of a controlled complex system can be represented by the differential equation $\dot{\mathbf{y}}_t = \Phi(\mathbf{y}_t,\mathbf{u}_t)$, where $\mathbf{y}_t \in \mathbb{R}^N$ represents the observed system state and $\mathbf{u}_t \in \mathbb{R}^M$ denotes the control input.
We assume the system satisfies the controllability condition without loss of generality: for any initial state $\mathbf{y}_0^*$ and target state $\mathbf{y}_f$, there exists a finite time $T$ and a corresponding control input $\mathbf{u}$ that can drive the system from $\mathbf{y}_0^*$ to $\mathbf{y}_f$. This assumption ensures the technical feasibility of our control objectives.
In practical applications, beyond achieving state transitions, we need to optimize the energy consumption during the control process. The energy cost can be quantified using the L2-norm integral of the control input: $J(\mathbf{y},\mathbf{u}) = \int_0^T |\mathbf{u}(t')|^2 dt'$.
For data-driven optimal control problems, we can only understand the system dynamics through observational data. Consider a dataset $D = \{\mathbf{u}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^P$ containing $P$ non-optimal control trajectories, where each trajectory consists of:(1) complete state trajectories $\mathbf{y}^{(i)}$ sampled at fixed time intervals; (2) corresponding control input sequences $\mathbf{u}^{(i)}$.
Based on this dataset, our objective is to find the optimal control input trajectory $\mathbf{u}^*\in \mathbb{R}^{T\times M} $ that satisfies:
\begin{equation}
\begin{aligned}
\mathbf{u}^* &= \arg\min_\mathbf{u} J(\mathbf{y},\mathbf{u}) \\
\text{s.t.} \quad & \Psi(\mathbf{u},\mathbf{y}) = 0,\quad
 \mathbf{y}_0 = \mathbf{y}_0^* ,\quad
 \mathbf{y}_T = \mathbf{y}_f,
\end{aligned}
\end{equation}
where $\mathbf{y}\in \mathbb{R}^{T\times N}$ is the corresponding complete state trajectory given $\mathbf{y}_0$ and $\Psi(\mathbf{u},\mathbf{y}) = 0$.
Here, $\Psi(\mathbf{u},\mathbf{y}) = 0$ represents the system dynamics constraint implicitly defined by dataset $D$. This constraint effectively serves as a data-driven representation of the unknown dynamics equation $\dot{\mathbf{y}}_t = \Phi(\mathbf{y}_t,\mathbf{u}_t)$.

Our key idea is to train a diffusion-based model to directly produce near-optimal control trajectories $\mathbf{u}_{[0:T-1]}$, providing a starting state $\mathbf{y}_0^*$, the target $\mathbf{y}_f$ and optimized by the cost $J$. Next, we summarize the details of the diffusion-based framework.

\subsection{Diffusion Model}

Diffusion models have become leading generative models, showing exceptional results across image synthesis, audio generation and other applications \citep{ho2020denoising,dhariwal2021diffusion,song2019generative}. These models, when applied to trajectory generation, operate by progressively adding noise to sequential data in the forward process and then learning to reverse this noise corruption through a denoising process. We denote that $\mathbf{x}^k$ represents the sequential data at diffusion timestep $k$.
In the forward process, a clean trajectory $\mathbf{x}^0$ is progressively corrupted through $K$ timesteps, resulting in a sequence of increasingly noisy versions $\mathbf{x}^1, \mathbf{x}^2, ..., \mathbf{x}^K$. Each step applies a small amount of Gaussian noise: $$q(\mathbf{x}^k | \mathbf{x}^{k-1}) = \mathcal{N}(\mathbf{x}^k; \sqrt{1-\beta^k}\mathbf{x}^{k-1}, \beta^k\mathbf{I}),$$
where $\beta^k$ is a variance schedule that controls the noise level. With a large enough $K$ we can get $q(\mathbf{x}^K)\approx \mathcal{N}(\mathbf{x}^K; \mathbf{0},\mathbf{I})$. In the reverse process, the diffusion model learns to gradually denoise the data, starting from pure noise $\mathbf{x}^K$ and working backward to reconstruct the original plausible trajectory $\mathbf{x}^0$. Each denoising step is conditioned on the start and target state:$$p_\theta(\mathbf{x}^{k-1} | \mathbf{x}^k, \mathbf{y}_0^*, \mathbf{y}_f) = \mathcal{N}(\mathbf{x}^{k-1}; \mathbf{\mu}_\theta(\mathbf{x}^k, k, \mathbf{y}_0^*, \mathbf{y}_f), \mathbf{\Sigma}^k),$$
where $\theta$ represents the learnable parameters of the model and $\Sigma^k$ is from a fixed schedule.

\textbf{Training of diffusion model.}
In order to facilitate the design of the denoising network, the network with $\theta$ for the denoising process does not directly predict $\mathbf{\mu}$. Instead, it is trained to learn to predict clean trajectory $\mathbf{x}^0$, outputting $\hat{\mathbf{x}}^0$.

The training objective for diffusion models typically involves minimizing the variational lower bound (VLB) on the negative log-likelihood \citep{sohl2015deep}. In practice, this often reduces to a form of denoising score matching \citep{song2019generative}:
$$\mathbb{E}_{\mathbf{x},k,\mathbf{y}_0^*,\mathbf{y}_f,\mathbf{\epsilon}}[||\boldsymbol{\mathbf{x}}-\boldsymbol{\mathbf{x}}_\theta(\boldsymbol{\mathbf{x}}^k, k,\mathbf{y}_0^*,\mathbf{y}_f)||^2],$$
where $\mathbf{x},k,\mathbf{y}_0^*,\mathbf{y}_f$ are sampled from the dataset, $k\sim \mathcal{U}\{1,2,...,K\}$ is the step index and $\mathbf{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$ is the noise used to corrupt $\mathbf{x}$.
% \subsection{Classifier-Free Guidance}

% Classifier-free guidance is a technique introduced by \citet{ho2022classifier} to enhance the sample quality and controllability of diffusion models without requiring a separate classifier. During training, randomly set the conditioning $y$ to a null token (e.g., empty string or zero vector) with probability $p$. This allows the model to learn both conditional and unconditional generation. The model is trained using a weighted combination of conditional and unconditional losses:
    
%     \begin{equation}
%         \mathcal{L} = \mathbb{E}_{x^0, \epsilon, k, y} \left[ (1-p) \| \epsilon - \epsilon_\theta(x^k, t, y) \|^2 + p \| \epsilon - \epsilon_\theta(x^k, k, \emptyset) \|^2 \right]
%     \end{equation}

%     where $\epsilon$ is the noise added to the clean data $x^0$ to obtain $x^k$, and $\| \cdot \|$ denotes the L2 norm.
% At inference time, it introduces a guidance scale $w$ and sample using:
% \begin{equation}
%     \tilde{\epsilon}_\theta(x^k, k, y) = (1+w)\epsilon_\theta(x^k, k, y) - w\epsilon_\theta(x^k, k, \emptyset)
% \end{equation}
% where $\tilde{\epsilon}_\theta$ is the guided noise prediction. This formulation allows for controlled generation without needing a separate classifier, offering a more streamlined and efficient method for guided synthesis. The guidance scale $w$ controls the trade-off between sample quality and adherence to the conditioning information. Higher values of $w$ typically result in samples that more closely match the conditioning but may sacrifice some diversity or realism.

% \section{Method}

% \begin{figure}[t]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \includegraphics[width=0.9\textwidth]{fig/overall.pdf} 
% \end{center}
% \caption{Illustration of the proposed conditional diffusion model-based controller.}
% \label{fig:overall}
% \end{figure}
\section{SEDC: the Proposed Method}
In this section, we introduce our three key innovative designs of SEDC: Decoupled State Diffusion, Denoising Network Design of Dual Mode Decomposition, and Guided Self-finetuning. 

\subsection{Decoupled State Diffusion~(DSD)}
\textbf{Decoupling Control Estimation using Inverse Dynamics.}
There are deep connections behind states, controls, and constraints, considering both the relationship between control and state and learning the dynamics behind state evolution. Such relation becomes more complex as the dimension of the system states goes up. However, some diffusion-based methods~(e.g., DiffPhyCon~\cite{wei2024generative}), which jointly diffuse over state and input, learn the relationship implicitly and may generate physically inconsistent state-control pairs that violate the underlying system dynamics. Additionally, control actions are less smooth than states, making their distribution more challenging to model~\cite{ajay2022conditional}. Therefore, rather than jointly sampling both control signals and intermediate states using the denoising network, we choose to decouple them and diffuse only states $\mathbf{y}$, i.e. $$\mathbf{x}:=\mathbf{y}_{[0:T]}.$$ Then we update the prediction of control $\mathbf{u}$ sequence by inputting the generated state trajectory to an inverse dynamic model $f_\phi$:
$$\mathbf{u}_{t,\text{update}}^0=f_\phi(\mathbf{y}_t^0,\mathbf{y}_{t+1}^0),$$
where $0$ denotes the final output of the denoising timestep from the diffusion model. We parameterize it using an Autoregressive MLP and optimize it simultaneously with the denoiser via training data. Our final optimization loss function is:
\begin{equation}
\begin{aligned}
L(\theta, \phi) &:= \mathbb{E}_{\mathbf{x},k,\mathbf{y}_0^*,\mathbf{y}_f,\mathbf{\epsilon}}[||\boldsymbol{\mathbf{x}}-\boldsymbol{\mathbf{x}}_\theta(\boldsymbol{\mathbf{x}}^k, k,\mathbf{y}_0^*,\mathbf{y}_f)||^2]\\&+\mathbb{E}_{\mathbf{y}_t,\mathbf{u}_t,\mathbf{y}_{t+1}}[||\mathbf{u}_t-f_\phi(\mathbf{y}_t, \mathbf{y}_{t+1})||^2],
\end{aligned}
\end{equation}
where $\mathbf{y}_t,\mathbf{u}_t,\mathbf{y}_{t+1}$ are sampled from the dataset. Note that the data used to train the diffusion model can also be utilized to train $f_\phi$.

\textbf{Cost Optimization via Gradient Guidance.}
% 在扩散模型和逆动力学方法得到充分的训练后，通过inference-time guidance对cost优化。对于cost函数J，guidance输入去噪中间步推导的clean trajectory 实时计算去噪过程中相对于中间步的梯度perturb去噪过程。此时采样过程为:$$\mathbf{\mu}_\theta = [图片]-\lambda\sigma^k(J(x^0)相对于x^t的梯度).考虑到x中不包含控制输入w，我们可以利用训练好的inverse model f_\phi从采样过程的states恢复相应的input。
After training both models, we optimize the cost function $J$ through inference-time gradient guidance. During the denoising process, we modify the sampling procedure by incorporating cost gradients:
\begin{equation}
\begin{aligned}
    \mathbf{\mu}_\theta(\mathbf{x}^k, k, \mathbf{y}_0^*, \mathbf{y}_f) =& \frac{\sqrt{\bar{\alpha}^{k-1}}\beta^k}{1-\bar{\alpha}^k}\hat{\mathbf{x}}^0 + \frac{\sqrt{\alpha^k}(1-\bar{\alpha}^{k-1})}{{1-\bar{\alpha}^k}}\mathbf{x}^k \\& - \lambda\Sigma^k\nabla_{\mathbf{x}^k}J(\hat{\mathbf{x}}^0(\mathbf{x}^k)),
\end{aligned}
\end{equation}
where $\lambda$ controls guidance strength, $\Sigma^k$ is the noise scale at step $k$, $\alpha^k := 1 - \beta^k \text{ and } \bar{\alpha}^k := \prod_{s=1}^k \alpha^s$. Since our diffusion model operates on states only, we recover control inputs using the inverse dynamics model $f_\phi$ at each step. This approach enables optimization of arbitrary cost functions without model retraining, while maintaining trajectory feasibility through the learned diffusion process.

\textbf{Target-conditioning as Inpainting.} Modeling whether the generated trajectory accurately satisfies the initial state of $\mathbf{y}_0^*$ and the desired target state of $\mathbf{y}_f$ can also be regarded as a constraint satisfaction of equations, that is, the generated trajectory should contain the start and target. We adopt a more direct method to solve this: we not only input it as an additional condition to the diffusion denoising network but also treat it as an inpainting problem similar to image generation. In brief, we substitute the corresponding location in the sampled trajectories $\mathbf{x}^{k-1}\sim p_\theta(\mathbf{x}^{k-1} | \mathbf{x}^k, \mathbf{y}_0^*, \mathbf{y}_f)$ with the given start and target $\mathbf{y}_0^*, \mathbf{y}_f$ after all diffusion timesteps, analogously to observed pixels in image generation~\cite{lugmayr2022repaint}.

\subsection{Dual Mode Decomposition~(DMD) for Denoising Module}

In this section, we propose a design for the denoising network that decomposes the modeling of linear and nonlinear modes in the sampled trajectory by a dual-Unet architecture, as shown in Figure~\ref{fig:model}.

Our design draws inspiration from control theory. For linear systems, \citet{yan2012controlling} demonstrated that optimal control signals have a linear relationship with a specific linear combination $\mathbf{y}_c$ of initial and target states. Building upon this insight, we develop a framework where a bias-free linear layer first learns this crucial linear combination $\mathbf{y}_c$ from the initial state $\mathbf{y}_0$ and target state $\mathbf{y}_f$. The first UNet then learns coefficients that map $\mathbf{y}_c$ to control signals, establishing first-order terms, while the second UNet learns coefficients for quadratic terms. These quadratic terms, introduced through residual connections, refine the first-order approximation and enhance the network's capacity to model complex dynamics. Note that the nonlinear terms essentially come from the nonlinearity of the dynamics.

The implementation includes several key components. The network accepts as input: (1) noisy trajectory $\mathbf{x}^k$ with dimension ($N$) corresponding to the network's channel dimension; (2) initial state $\mathbf{y}_0$ and target state $\mathbf{y}_f$, which generate $\mathbf{y}_c$ through a bias-less linear layer; and (3) diffusion timesteps $k$, encoded via sinusoidal embedding~\cite{ho2020denoising} as $\mathbf{k}_{\text{emb}}$. The first UNet generates first-order coefficients to compute an initial prediction using $\mathbf{y}_c$. Subsequently, the second UNet combines these features to generate quadratic coefficients, producing correction terms through quadratic operations with $\mathbf{y}_c$. The final output $\hat{\boldsymbol{x}}^0$ combines these components to predict the denoised trajectory.

The architecture decomposes system dynamics into linear and nonlinear components, effectively handling complex features in nonlinear control systems while maintaining numerical stability during training. The first-order and quadratic terms based on $\mathbf{y}_c$ incorporate fundamental control principles into the network structure, providing effective constraints for the learning process. This structured inductive bias significantly improves the model's data efficiency, enabling reliable control strategy learning from limited training samples.

% We parameterize the denoiser using a dual-Unet framework consisting of two consecutive 1-D U-nets. 1-D Unet is a combination of multiple 1-D convolutional layers. The dimension of $\boldsymbol{\tau}$ ($M+N$) corresponds to the channel dimension of the CNN. The U-Net effectively captures the relationship between control signals and system observation states along physical time dimension, thus better modeling the complete trajectory distribution~\citep{janner2022planning}. Condition labels $r$ and diffusion time steps $k$ are encoded through MLPs as $\mathbf{r}_{\text{emb}}$ and $\mathbf{k}_{\text{emb}}$, respectively, and concatenated as input to the U-Net. For initial and target states $\mathbf{y}_0$ and $\mathbf{y}_f$ condition, we found they significantly decide the optimal trajectory/control signal distribution. Therefore, we use them to explicitly adapt the network's output at positions close to the output.

% \citet{yan2012controlling} suggests that for linear systems, the optimal control signal has a linear relationship with $\mathbf{y}_c=\mathbf{y}_f-A\mathbf{y}_0$~(a linear combination of initial and target states, A is related to the network's parameter). We consider this linear operation in our model as the first-order expansion of optimal control with respect to $\mathbf{y}_c$ for nonlinear systems, with coefficients learned through the first U-Net. Correspondingly, we attempt to add another Unet to learn the coefficients of higher-order (second-order) expansions at the back end, fine-tuning the first-order results through residual connections. We first learn $\mathbf{y}_c$ by inputting $\mathbf{y}_0$ and $\mathbf{y}_f$ through a single linear layer without bias. The network formula is expressed as follows:

The network performs sequential transformations on the input signals. Let $B$ denote batch size, $T$ sequence length, $C_1$ and $C_2$ feature dimensions, and $N$ the dimension of $\mathbf{y}_c$. The input noisy trajectory $\mathbf{x}^k \in \mathbb{R}^{B\times T\times N}$ and $\mathbf{y}_{c} \in \mathbb{R}^{B\times C_1}$ are processed through two UNets to generate first-order and quadratic predictions:
% \begin{equation}
% \begin{aligned}
% \mathbf{C}_1 &= \text{UNet}_1(\mathbf{x}^k,\mathbf{k}_{\text{emb}})\\
% \mathbf{O}_1 &= \text{reshape}(\mathbf{C}_1)\cdot \mathbf{y}_c  \\
% \mathbf{C}_2 &= \text{UNet}_2([\mathbf{x}^k, \mathbf{C}_1],\mathbf{k}_{\text{emb}})  \\
% \mathbf{O}_2 &= \mathbf{y}_c^T\cdot\text{reshape}(\mathbf{C}_2)\cdot \mathbf{y}_c  \\
% \hat{\mathbf{x}}^0 &= \mathbf{O}_1+\mathbf{O}_2,
% \end{aligned}
% \label{eq:dmd}
% \end{equation}
\begin{equation}
\mathbf{C}_1 = \text{UNet}1(\mathbf{x}^k,\mathbf{k}{\text{emb}}),
\end{equation}
\begin{equation}
\mathbf{O}_1 = \text{reshape}(\mathbf{C}_1)\cdot \mathbf{y}_c,
\end{equation}
\begin{equation}
\mathbf{C}_2 = \text{UNet}_2([\mathbf{x}^k, \mathbf{C}1],\mathbf{k}{\text{emb}}),
\end{equation}
\begin{equation}
\mathbf{O}_2 = \mathbf{y}_c^T\cdot\text{reshape}(\mathbf{C}_2)\cdot \mathbf{y}_c,
\end{equation}
\begin{equation}
\hat{\mathbf{x}}^0 = \mathbf{O}_1+\mathbf{O}_2,
\end{equation}
where $\mathbf{C}_1$ produces first-order coefficients $\in \mathbb{R}^{B\times T\times (N\times C_1)}$, $\mathbf{O}_1$ computes linear predictions $\in \mathbb{R}^{B\times T\times N}$, $\mathbf{C}_2$ generates quadratic coefficients $\in \mathbb{R}^{B\times T\times (C_1\times N \times C_1)}$, and both $\mathbf{O}_2$ and the final output $\hat{\mathbf{x}}^0$ are $\in \mathbb{R}^{B\times T\times N}$. We illustrate the structural framework of the denoising network in Figure~\ref{fig:model}.


% \begin{alignat}{2}
% \mathbf{C}_1 &= \text{Unet}_1(\boldsymbol{\tau}^k,[\mathbf{k}_{\text{emb}},\mathbf{r}_{\text{emb}}]), \\&
%   \mathbb{R}^{B\times T\times C_1},\mathbb{R}^{B\times C_2}\to\mathbb{R}^{B\times T\times (C_1\times N)}, \\
% \mathbf{O}_1 &= \text{reshape}(\mathbf{C}_1)\cdot \mathbf{y}_c, \\&
%   \mathbb{R}^{B\times T\times (C_1\times N)},\mathbb{R}^{B\times N}\to\mathbb{R}^{B\times T\times C_1}, \\
% \mathbf{C}_2 &= \text{Unet}_2([\boldsymbol{\tau}^k, \mathbf{C}_1],[\mathbf{k}_{\text{emb}},\mathbf{r}_{\text{emb}}]), \\&
%   \mathbb{R}^{B\times T\times 2C_1},\mathbb{R}^{B\times C_2}\to\mathbb{R}^{B\times T\times (N\times C_1\times N)}, \\
% \mathbf{O}_2 &= \mathbf{y}_c^T\cdot\text{reshape}(\mathbf{C}_2)\cdot \mathbf{y}_c, \\&
%   \mathbb{R}^{B\times N},\mathbb{R}^{B\times T\times (N\times C_1\times N)},\mathbb{R}^{B\times N}\to\mathbb{R}^{B\times T\times C_1}, \\
% \hat{\boldsymbol{\tau}}^0 &= \mathbf{O}_1+\mathbf{O}_2,
% \end{alignat}

% where $B$ is the batch size, $T$ is the state sequence length, $C_1$ and $C_2$ are feature dimensions, and $N$ is the learned $\mathbf{y}_c$'s dimension. We illustrate the structural framework of the denoising network in Figure~\ref{fig:overall} and discuss its role in the experiments.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.95\linewidth]{fig/overall.pdf}    
    \vspace{-10pt}
    \caption{Comparison of target loss and energy cost $J$ across different datasets. The closer the data point is to the bottom left, the better the performance.}
    \label{fig:overall}
    \end{center}
\vspace{-5pt}
\end{figure*}

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.95\linewidth]{fig/improved_loss_comparison.pdf}
    \vspace{-10pt}
    \caption{Sample-efficiency comparison on Burgers, Kuramoto and Inverse Pendulum dynamics.}
    \label{fig:efficiency}
    \end{center}
\end{figure*}

\subsection{Guided Self-finetuning~(GSF)}

Randomly generated training data cannot guarantee coverage of optimal scenarios. To generate near-optimal controls that may deviate significantly from the training distribution. To address this limitation, we propose leveraging the model's initially generated data (under the guidance of cost function), which naturally deviates from the training distribution toward optimality, for iterative retraining to systematically expand the exploration space. 
This approach maintains physical consistency by ensuring generated samples adhere to the underlying system dynamics.
% Since gradient guidance can cause the distribution of the generated sequence to differ from the original distribution, we can use the data initially generated by the model, which has already deviated from the training distribution and tends towards the optimal distribution, to retrain the model and expand its exploration space. This approach is predicated on ensuring the quality of generated data, at least guaranteeing that the samples generated by the model conform to the underlying dynamics.

Our methodology involves extracting control sequences from the generated samples (i.e., the output of inverse dynamics $\mathbf{u}_{\text{update}}^0$) and reintroduces it into the system to interact and generate corresponding state sequences $\mathbf{y}_{\text{update}}^0$. Together we add the renewed $[\mathbf{u}_{\text{update}}^0,\mathbf{y}_{\text{update}}^0]$ to the retrain data pool used for a new round of fine-tuning, notably without requiring explicit system parameter identification. We iterate this process over multiple rounds specified by a hyperparameter, systematically expanding the model's exploration space to progressively approach optimal control policy. Denote the sampling process under cost $J$'s guidance and the following interacting process as $[\mathbf{u}_{\text{update}}^0,\mathbf{y}_{\text{update}}^0]=\mathcal{S}(\mathbf{x}^K,\mathbf{y}_0^*,\mathbf{y}_f,J,\Phi)$. The process can be formulated as:
% \begin{equation}
% \begin{aligned}
% &\hat\theta,\hat\phi \\=&  \mathrm{argmin}_{\theta,\phi}\{\mathbb{E}_{k,(\mathbf{x},\mathbf{y}_0^*,\mathbf{y}_f)\sim D,\mathbf{\epsilon}}[||\boldsymbol{\mathbf{x}}-\boldsymbol{\mathbf{x}}_\theta(\boldsymbol{\mathbf{x}}^k, k,\mathbf{y}_0^*,\mathbf{y}_f)||^2]\\&+\mathbb{E}_{(\mathbf{y}_t,\mathbf{u}_t,\mathbf{y}_{t+1})~\sim D}[||\mathbf{u}_t-f_\phi(\mathbf{y}_t, \mathbf{y}_{t+1})||^2]\}
% \end{aligned}
% \end{equation}
\begin{equation}
[\mathbf{u}_{\text{update}}^0,\mathbf{y}_{\text{update}}^0]=\mathcal{S}_{(\mathbf{x}^K,\mathbf{y}_0^*,\mathbf{y}_f)\sim D}(\mathbf{x}^K,\mathbf{y}_0^*,\mathbf{y}_f,J,\Phi),
\end{equation}
\begin{equation}
D = [D, [\mathbf{u}_{\text{update}}^0,\mathbf{y}_{\text{update}}^0]],
\end{equation}

%Using the re-sampled observation states from the system and the previous control sequences as new samples, we can enable the model (diffusion model, inverse dynamics) to better explore the trajectory space tending towards the optimal control distribution and learn from it. 
where $D$ is the training set.

We provide the algorithm form of SEDC in Appendix~\ref{sup:alg}.

\section{Experiments}

% \begin{table*}[t]
% \centering
% \caption{\textbf{Performance comparison of different models across three datasets.} Lower values indicate better performance for both metrics.}
% \label{tab:comparison}
% \begin{tabular}{l|cc|cc|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Burgers}} & \multicolumn{2}{c|}{\textbf{Kuramoto}} & \multicolumn{2}{c|}{\textbf{IP}} & \multicolumn{2}{c}{\textbf{Power}} \\
% \cmidrule{2-9}
% & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) \\
% \midrule
% PID & 1.30e-1 & 6.56 & 7.99e-1 & 30.35 & 1.50 & 8.38e-2 & 1.50 & 6.79e-2 \\
% DCCN & 2.59e-3 & \textbf{2.30e-2} & 1.70e-3 & \textbf{1.56} & 1.18e-3 & \textbf{3.84e-5} & 7.81e-2 & 6.79e-2 \\
% BPPO & 5.90e-4 & 9.72 & 1.56e-4 & 26.64 & 3.63e-3 & 4.16e-3 & 2.94e-2 & \underline{8.80e-4} \\
% BC & 4.78e-4 & 10.73 & 1.52e-4 & 27.59 & 3.63e-3 & 4.20e-3 & \underline{2.82e-2} & \textbf{8.72e-4} \\
% DecisionDiffuser & 2.46e-4 & 5.18 & \underline{3.88e-5} & 27.48 & 3.85e-4 & 9.00e-4 & 2.94e-2 & 3.07e-1 \\
% RDM & 2.70e-4 & 7.01 & 4.60e-4 & 29.03 & 3.65e-4 & \underline{7.38e-4} & 6.09e-2 & 1.20e-1 \\
% DiffPhyCon & \underline{1.62e-4} & 5.15 & 4.80e-4 & 18.72 & \underline{2.63e-4} & 9.00e-4 & 4.80e-2 & 1.11e-1 \\
% Ours & \textbf{9.80e-5} & \underline{5.01} & \textbf{8.90e-6} & \underline{14.90} & \textbf{6.49e-5} & 8.90e-4 & \textbf{2.29e-2} & 5.15e-1 \\
% \bottomrule
% \end{tabular}
% \end{table*}

% Our experimental design aims to address three key research questions: (1) Can DiffCON demonstrate superiority over traditional, analytical, reinforcement learning, and diffusion-based methods for complex systems control? (2) How does DIFOCON's performance compare to SOTA diffusion-based methods in sample-efficiency? (3) Do the proposed designs help DiffCon achieve better performance by solving the challenges? To answer these questions, we conduct experiments on control tasks in three nonlinear systems: the 1-D Burgers dynamics, the Kuramoto dynamics, the Swing dynamics, and the inverted pendulum dynamics.
\textbf{Experiment settings.} 
We conducted experiments on three nonlinear systems, following the instructions in the previous works for data synthesis. These systems include: the 1-D Burgers dynamics~\cite{hwang2022solving,wei2024generative}, which serves as a fundamental model for studying nonlinear wave propagation and turbulent fluid flow; the Kuramoto dynamics~\cite{acebron2005kuramoto,baggio2021data,gupta2022learning}, which is essential for understanding synchronization phenomena in complex networks and coupled oscillator systems; and the inverted pendulum dynamics~\cite{boubaker2013inverted}, which represents a classical benchmark problem in nonlinear control theory and robotic systems. For each system, we generated control/state trajectory data using the finite difference method and selected 50 trajectories as the test set. Detailed descriptions of the system dynamics equations and data synthesis procedures are provided in the appendix.

We evaluate two metrics which is crucial in complex system control: \textbf{Target Loss}, the mean-squared-error~(MSE) of $\mathbf{y}_T$ and desired target $\mathbf{y}_f$, i.e. $\frac{1}{N}\|\mathbf{y}_T - \mathbf{y}_f\|^2$. (Note that $\mathbf{y}_T$ is obtained by simulating the real system using the control inputs generated by each method, along with the given initial state conditions, rather than extracted from the sample trajectories of the diffusion-based methods); \textbf{Energy} $J = \int_0^T |\mathbf{u}(t')|^2 dt'$, which measures the cumulative control effort required to achieve the target state. Lower values of both metrics indicate better performance.

\textbf{Baselines.} We select the following state-of-the-art(SOTA) baseline methods for comparison. For traditional control approaches, we employ the classical PID (Proportional-Integral-Derivative) controller~\cite{li2006pid}, which remains widely used in industrial applications. For supervised learning ,we employ Behavioral Cloning (BC)~\cite{pomerleau1988alvinn}, an established imitation learning approach. In terms of reinforcement learning methods, we incorporate BPPO~\cite{zhuang2023behavior}, a state-of-the-art algorithm. For diffusion-based methods, we include several recent prominent approaches: DecisionDiffuser (DecisionDiff)~\cite{ajay2022conditional}, which is a SOTA classifier-free diffusion-based planner; AdaptDiffuser~\cite{liang2023adaptdiffuser}, which enhances DecisionDiffuser with a self-evolving mechanism; RDM~\cite{zhou2024adaptive}, which adaptively determines the timing of complete control sequence sampling; and DiffPhyCon~\cite{wei2024generative}, which is specifically designed for controlling complex physical systems. Detailed descriptions of the baselines are included in Appendix~\ref{sup:baselines}. 

\subsection{Overall Control Performance}

\textbf{Results.} In Figure~\ref{fig:overall}, we compare different methods' performance across three dynamical systems using two-dimensional coordinate plots, where proximity to the lower-left corner indicates better trade-offs between control accuracy and energy efficiency. Since unstable control can lead to system failure regardless of energy efficiency, we prioritize control accuracy and report metrics at each method's minimum Target Loss.
Our method achieves the closest position to the lower-left corner in three datasets, demonstrating best balance between accuracy and efficiency despite varying system characteristics. We achieve the best Target Loss across all systems, outperforming the best baselines by 39.5\%, 49.4\%, and 47.3\% in Burgers, Kuramoto, and IP systems respectively. This superior performance reflects our method's enhanced dynamics learning capability under identical conditions.
For energy efficiency, our method leads in Kuramoto and IP systems while remaining competitive in Burgers, trailing AdaptDiffuser by only 1.3\%.

Regarding method types, traditional PID control shows the poorest performance, as system complexity exacerbates the difficulties in PID control and tuning. RL-based methods are competitive against some diffusion- but sacrifice Target Loss performance and underperform compared to Diffusion-based methods in other systems. Diffusion-based methods demonstrate superior overall performance, as they better capture long-term dependencies in system dynamics compared to traditional and RL methods, avoiding myopic failure modes and facilitating global optimization of long-term dynamics.

Due to the space limits, we present detailed numeric results in Appendix~\ref{sup:numeric}. Moreover, we visualize the control dynamics of SEDC and SOTA baselines in Appendix~\ref{sup:visual}.


% \begin{table}[t]
% \small
% \centering
% \caption{Comparison of Target Loss across different datasets. Best and second-best results are highlighted in \textbf{bold} and \underline{underlined} respectively.}
% \label{tab:target_loss}
% \begin{tabular}{l|c|c|c|c}
% \toprule
% \multirow{2}{*}{Methods} & \multicolumn{4}{c}{Systems} \\
% \cmidrule{2-5}
% & Burgers & Kuramoto & IP & Swing \\
% \midrule
% PID & 1.30e-1 & 7.99e-1 & 8.64e-3 & 1.50e+0 \\
% DCCN & 2.59e-3 & 1.70e-3 & 1.18e-3 & 7.81e-2 \\
% BPPO & 5.90e-4 & 1.56e-4 & 3.63e-3 & 2.94e-2 \\
% BC & 4.78e-4 & 1.52e-4 & 3.63e-3 & \underline{2.82e-2} \\
% RDM & 2.70e-4 & 4.60e-4 & 3.65e-4 & 6.09e-2 \\
% DD & 2.46e-4 & \underline{3.88e-5} & 3.85e-4 & 4.14e-2 \\
% DiffPhyCon & \underline{1.62e-4} & 4.80e-4 & \underline{2.63e-4} & 4.80e-2 \\
% Ours & \textbf{9.80e-5} & \textbf{8.90e-6} & \textbf{6.49e-5} & \textbf{1.63e-2} \\
% \bottomrule
% \end{tabular}
% \end{table}



\subsection{Sample Efficiency}
% \begin{figure}[h]
%     \begin{center}
%     \includegraphics[width=0.95\linewidth]{fig/hm_burgers.png}
%     \caption{Sample-efficiency comparison on Burgers dynamic.}
%     \end{center}
%     \label{fig:energy}

% \end{figure}



% \begin{figure}[h]
%     \begin{center}
%     \includegraphics[width=0.95\linewidth]{fig/hm_swing.png}
%     \caption{Sample-efficiency comparison on Swing dynamic.}
%     \end{center}
%     \label{fig:energy}

% \end{figure}


\textbf{Experiment settings.} 
To evaluate the sample efficiency of diffusion-based methods, we conducted experiments on all the systems using varying proportions of the full training dataset. Specifically, we trained models using 1\%, 5\%, 10\%, 20\%, and 100\% of the available data and assessed their performance using the Target Loss metric on a held-out test set. 

\textbf{Results.}
Figure~\ref{fig:efficiency} demonstrates our method's superior performance in controlling Burgers and Kuramoto systems compared to state-of-the-art baselines. In all systems, our approach achieves significantly lower target loss values across all training data percentages. Most notably, with only 10\% of the training data, our method attains a target loss of 1.71e-4 for Burgers, 1.12e-5 for Kuramoto, and 6.35e-4 for Inverse Pendulum, matching(-5.5\% in Burgers) or exceeding(+36.4\% in Kuramoto abd +1.2\% in Inverse Pendulum) the performance of best baseline methods trained on the complete dataset. This indicates our method can achieve state-of-the-art performance while requiring only 10\% of the training samples.

Among baselines, DiffPhyCon performed poorest due to its dual diffusion model training requirement - a challenge amplified with limited data. AdaptDiffuser surpassed DecisionDiffuser through its retraining mechanism that enhances generalization in low-data settings. Our model demonstrated superior performance through efficient dynamic learning and guided finetuning strategy.

% \begin{table}[t]
% \caption{Performance comparison of different ablations of SEDC across multiple datasets. \textbf{Target loss} results with 10\% and 100\% training sample for each method are reported. Best and second-best results of each row are highlighted in \textbf{bold} and \underline{underlined} respectively.}
% \label{tab:ablation_main}
% \small
% \begin{tabular}{@{}ll|rrrr@{}}
% \toprule

% System & Ratio & \multicolumn{1}{c}{SEDC} & \multicolumn{1}{c}{w/o DSD} & \multicolumn{1}{c}{w/o DMD} & \multicolumn{1}{c}{w/o GSF} \\

% \midrule

% Burgers & 10\% & \textbf{1.74e-4} & 1.00e-3 & \underline{3.78e-4} & 6.67e-4 \\
% & 100\% & \textbf{9.80e-5} & 8.71e-4 & \underline{2.28e-4} & 2.82e-4 \\
% \cmidrule{1-6}
% Kuramoto & 10\% & \textbf{1.12e-5} & 4.15e-3 & 5.21e-5 & \underline{4.77e-5} \\
% & 100\% & \textbf{8.90e-6} & 5.43e-3 & \underline{1.76e-5} & 3.88e-5 \\
% \cmidrule{1-6}
% IP & 10\% & \textbf{8.50e-5} & \underline{4.80e-4} & 5.20e-4 & 5.10e-4 \\
% & 100\% & \textbf{6.49e-5} & 3.65e-4 & \underline{3.64e-4} & 3.85e-4 \\
% \cmidrule{1-6}
% Swing & 10\% & \textbf{2.51e-2} & 1.46e-1 & 1.27e+0 & \underline{3.02e-2} \\
% & 100\% & \textbf{1.63e-2} & 7.15e-2 & 4.74e-2 & \underline{2.56e-2} \\

% \bottomrule
% \end{tabular}
% \end{table}
\begin{table}[t]
\centering
\caption{Performance comparison of different ablations across multiple datasets. \textbf{Target loss} results with 10\% and 100\% training sample for each method are reported. The best, second-best and worst results of each row are highlighted in \textbf{bold},  \underline{underlined} and \textit{italics}, respectively.}
\label{tab:ablation_main}
\small
\begin{tabular}{@{}l@{\;}l@{\;}|@{\;}c@{\quad}c@{\quad}c@{\quad}c@{}}
\toprule
System & Ratio & Ours & Ours/DSD & Ours/DMD & Ours/GSF \\
\midrule
Burgers & 10\% & \textbf{1.74e-4} & \textit{1.00e-3} & \underline{3.78e-4} & 6.67e-4 \\
& 100\% & \textbf{9.80e-5} & \textit{8.71e-4} & \underline{2.28e-4} & 2.62e-4 \\
\cmidrule{1-6}
Kuramoto & 10\% & \textbf{1.12e-5} & \textit{4.15e-3} & \underline{5.21e-5} & 4.77e-5 \\
& 100\% & \textbf{8.90e-6} & \textit{5.43e-3} & \underline{1.76e-5} & 3.88e-5 \\
\cmidrule{1-6}
IP & 10\% & \textbf{6.21e-4} & \textit{1.58e-3} & \underline{1.10e-3} & 2.00e-3 \\
& 100\% & \textbf{3.49e-4} & \textit{1.37e-3} & \underline{6.64e-4} & 7.85e-4 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}
\textbf{Overall ablation study.} We explore the main performance against each ablation of the original SEDC. Specifically, \textit{w/o DSD} removes the inverse dynamics, unifying the diffusion of system state and control input, i.e. $\mathbf{x}=[\mathbf{u},\mathbf{y}]$. Therefore, the diffusion model is required to simultaneously capture the temporal information and implicit dynamics of the control and system trajectory. Note that the inpainting mechanism and gradient guidance are retained. \textit{w/o DMD} removes the decomposition design, resulting in a single 1-D Unet structure as the denoising network, following DecisionDiff~\citep{ajay2022conditional}. Finally, \textit{w/o GSF} reports the performance without iterative self-finetuning, which means the model only uses the original dataset to train itself. To show the sample-efficiency performance, we also investigate the results under less amount of training sample(10\%). For \textit{w/o DMD} and \textit{w/o DSD}, we adjust the number of trainable parameters at a comparable level against the original version.

Table~\ref{tab:ablation_main} shows the Target Loss performance of different ablations of SEDC
across multiple datasets and different training sample ratios. As can be seen, removing any component leads to a certain decrease in performance, whether the training data is limited or not, demonstrating the effectiveness of each design. The most significant performance drops are often observed in \textit{w/o DSD}, highlighting the importance of explicit learning of dynamics in complex systems. \textit{w/o DMD} exhibits the lowest decline across the three systems. This is because the single-Unet-structured denoising network can already capture the nonlinearity to some extent, but not as good as the proposed decomposition approach. with 10\% of training data, removing individual components still led to noticeable performance degradation, and the patterns consistent with the full dataset results. This demonstrates that our designs remain effective in low-data scenarios.

\begin{table}[t]
\centering
\caption{SEDC and \textit{w/o DSD }Target Loss comparison across different state dimensions in Kuramoto. The Dec. indicates the reduction in target loss achieved by SEDC compared to \textit{w/o DSD}. Notably, the magnitude of loss reduction increases proportionally with network dimensionality, demonstrating that DSD's performance enhancement scales positively with dimensional growth.}
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{@{}lccccc@{}}
\toprule
$N$ & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
SEDC (e-6) & 3.45 & 2.89 & 5.67 & 4.12 & 8.90 \\
w/o DSD (e-4) & 3.98 & 3.23 & 16.78 & 12.45 & 54.32 \\
Dec.(\%) & 99.13 & 99.11 & 99.66 & 99.67 & 99.84 \\
\bottomrule
\end{tabular}
\label{table:dsd_comparison}
\end{table}

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.95\linewidth]{fig/dsd_heatmap.png}
    \vspace{-10pt}
    \caption{Comparison of State Trajectory Consistency between SEDC and w/o DSD Models. The heatmaps show induced states (left), sampled states (middle), and their absolute differences (right) for both SEDC (top) and w/o DSD (bottom) approaches under identical start-target conditions.}
    \label{fig:dsd_heatmap}
    \end{center}
    \vspace{-2em}
\end{figure}


\textbf{Effectiveness of DSD.} 
To evaluate DSD's effectiveness against the curse of dimensionality, we compared the performance of original and \textit{w/o DSD} models across Kuramoto systems with dimensions ranging from $N=4$ to $N=8$. Experimental results (Table~\ref{table:dsd_comparison}) 
show that performance degradation~(Dec.) from \textit{w/o DSD} increases with system dimensionality, demonstrating DSD's enhanced effectiveness in higher-dimensional systems and validating its capability to address dimensionality challenges.
% To specifically evaluate the effectiveness of DSD in resolving the curse of dimensionality, We investigated the performance degradation of the \textit{w/o DSD} variant compared to the original model across different system dimensions in the Kuramoto system. We generated multiple datasets with state dimensions ranging from N=3 to N=8, maintaining other parameters consistent. Both models were evaluated on datasets of each dimension, assessing their Target Loss performance. The experimental results, as shown in the Table~\ref{}, demonstrate that as dimensionality increases, the performance degradation caused by removing the DSD design grows. This indicates that DSD design becomes increasingly effective in higher-dimensional systems, validating its capability to address dimensionality challenges. 

To investigate the effectiveness of dynamical learning, we compared the consistency between action sequences and diffusion-sampled state trajectories in models with and without DSD. While both approaches can sample state trajectories from diffusion samples, they differ in action generation: SEDC uses inverse dynamics prediction, whereas \textit{w/o DSD} obtains actions directly from diffusion samples by simultaneously diffusing states and control inputs. We test both models using identical start-target conditions and visualize the state induced from the generated actions and the state sampled from the diffusion model, along with the difference~(error) between the above two states in Figure~\ref{fig:dsd_heatmap}. We can observe that SEDC's action-induced state trajectories showed significantly higher consistency with sampled trajectories compared to \textit{w/o DSD}, demonstrating that DSD using inverse dynamics achieves more accurate learning of control-state dynamical relationships.

\begin{table}[t]
\caption{Performance degradation using different denoiser output with varying nonlinearity strength $\gamma$ in the Kuramoto system. The Dec. indicates the reduction in target loss achieved by nonlinear output $\mathbf{O}_1 + \mathbf{O}_2$ compared to linear output $\mathbf{O}_1$. Notably, the magnitude of loss reduction increases proportionally with the nonlinearity strength $\gamma$, indicating that the quadratic term exhibits enhanced capability in capturing nonlinear dynamics as the system's nonlinearity intensifies.}
\centering
\small
\begin{tabular}{cccc}
\toprule
$\gamma$ & 1 & 2 & 4 \\
\midrule
$\mathbf{O}_1 + \mathbf{O}_2$ & 8.90e-6 & 2.78e-5 & 3.89e-5 \\
$\mathbf{O}_1$ & 1.42e-5 & 4.73e-5 & 8.52e-5 \\
Dec. (\%) & 37.3 & 41.2 & 54.3 \\
\bottomrule
\label{tab:dmd}
\end{tabular}
\vspace{-2em}
\end{table}
\textbf{Effectiveness of DMD.} To investigate the contribution of DMD's dual-Unet architecture to nonlinearity learning, we conducted experiments on the Kuramoto system with varying degrees of nonlinearity (controlled by the coefficient $\gamma\in\{1,2,4\}$ of the nonlinear sinusoidal term, where larger values indicate stronger nonlinearity). We compared the performance between using only the linear intermediate output ($\hat{\mathbf{x}}_0=\mathbf{O}_1$) of the denoising network and the original nonlinear output ($\hat{\mathbf{x}}_0=\mathbf{O}_1+\mathbf{O}_2$) in terms of Target Loss. As shown in Table~\ref{tab:dmd}, the performance degradation~(Dec.) from using only $\mathbf{O}_1$ becomes more pronounced as nonlinearity increases. This demonstrates both the significance of the nonlinear branch $\mathbf{O}_2$ in capturing strong nonlinear dynamics and the effectiveness of decoupling linear and nonlinear modes in handling system nonlinearity.


\textbf{Effectiveness of GSF.} To validate GSF's effectiveness in guiding the model toward learning the optimal~(energy-efficient) target distribution, we conduct ablation studies on fine-tuning rounds and evaluate control signal energy on the test set. Results show decreasing energy metrics over rounds, confirming our approach's convergence behavior toward near-optimal control strategies. Due to space limits, results and detailed discussion are provided in Appendix~\ref{sup:gsf_result}.


\section{Conclusion}
In this paper, we presented SEDC, a novel sample-efficient diffusion-based framework for complex nonlinear system control. By addressing fundamental challenges in data-driven control through three key innovations - Decoupled State Diffusion (DSD), Dual-Mode Decomposition (DMD), and Guided Self-finetuning (GSF) - SEDC achieves superior control performance while significantly reducing sample requirements. Our comprehensive experiments across three nonlinear systems demonstrate that SEDC outperforms existing methods by 39.5\%-49.4\% in control accuracy while maintaining computational efficiency. Most notably, SEDC achieves state-of-the-art performance using only 10\% of the training samples required by baseline methods, marking a significant advancement in sample-efficient control of complex systems. These results validate our approach's effectiveness in addressing the curse of dimensionality, handling strong nonlinearities, and bridging the gap between non-optimal training data and optimal control solutions. As complex system control continues to evolve across various domains, SEDC's sample-efficient framework provides a promising direction for future research and practical applications in data-driven control.
% Acknowledgements should only appear in the accepted version.

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Derivations of Gradience Guidance}
\section{Algorithm form of SEDC}

\begin{algorithm}[H]
\caption{SEDC: Training and finetuning}
\KwIn{Initial dataset $\mathcal{D}_0$, diffusion steps $K$, guidance strength $\lambda$, 
    self-finetuning rounds $R$, forward dynamics $f_{\text{forward}}$}
\KwOut{Optimized trajectory $\mathbf{y}_{0:T}^0$, controls $\mathbf{u}_{0:T}^0$}

% 初始训练
\SetKwProg{Fn}{Function}{}{}
\Fn{Initial Training($\mathcal{D}_0$)}{
    \While{not converged}{
        Sample batch $(\mathbf{y}_{0:T}, \mathbf{u}_{0:T}) \sim \mathcal{D}_0$ \\
        Sample $k \sim \mathcal{U}\{1,...,K\}$, $\epsilon \sim \mathcal{N}(0,I)$ \\
        Corrupt states: $\mathbf{y}^k = \sqrt{\bar{\alpha}^k}\mathbf{y} + \sqrt{1-\bar{\alpha}^k}\epsilon$ \\
        Predict clean states: $\hat{\mathbf{y}}^0 = \mathcal{G}_\theta(\mathbf{y}^k, k, \mathbf{y}_0^*, \mathbf{y}_f)$ \\
        Predict controls: $\hat{\mathbf{u}}_t = f_\phi(\hat{\mathbf{y}}_t^0, \hat{\mathbf{y}}_{t+1}^0)$ \\
        Compute losses: 
        \quad $L_{\text{diff}} = \|\mathbf{y} - \hat{\mathbf{y}}^0\|^2$ \\
        \quad $L_{\text{inv}} = \|\mathbf{u}_t - \hat{\mathbf{u}}_t\|^2$ \\
        Update $\theta, \phi$ with $\nabla(L_{\text{diff}} + L_{\text{inv}})$
    }
}

% 自举微调循环
\For{$r = 1$ \textbf{to} $R$}{
    \textbf{Guided Data Generation:} \\
    Initialize $\mathbf{y}^K \sim \mathcal{N}(0, I)$, sample $(\mathbf{y}_0^*,\mathbf{y}_f)\sim \mathcal{D}_{r-1}$ \\
    \For{$k = K$ \textbf{downto} $1$}{
        Predict $\hat{\mathbf{y}}^0 = \mathcal{G}_\theta(\mathbf{y}^k, k, \mathbf{y}_0^*, \mathbf{y}_f)$ \\
        Compute gradient: $g = \nabla_{\mathbf{y}^k} J(\hat{\mathbf{y}}^0)$ \\
        Adjust mean: $\mu_\theta = \mu_\theta^{\text{(base)}} - \lambda \Sigma^k g$ \\
        Sample $\mathbf{y}^{k-1} \sim \mathcal{N}(\mu_\theta, \Sigma^k I)$ \\
        Enforce constraints: $\mathbf{y}^{k-1}[0] \leftarrow \mathbf{y}_0^*$, $\mathbf{y}^{k-1}[T] \leftarrow \mathbf{y}_f$
    }
    Recover controls: $\mathbf{u}_t^0 = f_\phi(\mathbf{y}_t^0, \mathbf{y}_{t+1}^0)$ \\
    \textbf{System Interaction:} \\
    Generate $\mathbf{y}_{\text{update}}^0 = f_{\text{forward}}(\mathbf{u}_{0:T}^0, \mathbf{y}_0^*)$ \\
    Augment dataset: $\mathcal{D}_r = \mathcal{D}_{r-1} \cup \{(\mathbf{y}_{\text{update}}^0, \mathbf{u}_{0:T}^0)\}$ \\
    
    \textbf{Adaptive Fine-tuning:} \\
    \While{validation loss decreases}{
        Sample batch from $\mathcal{D}_r$ \\
        Perform training steps as in Initial Training
    }
}

\Return Optimized $\theta,\phi$

\textit{(Test process follows guided data generation with test conditions $(\mathbf{y}_0^*,\mathbf{y}_f)$ provided.)}
\label{sup:alg}
\end{algorithm}

\section{Detailed System and Dataset Description}
\subsection{Burgers Dynamics}

The Burgers' equation is a governing law occurring in various physical systems. We consider the 1D Burgers' equation with the Dirichlet boundary condition and external control input $\mathbf{u}(t,x)$:

$$
\begin{cases}
\frac{\partial y}{\partial t} = -y \cdot \frac{\partial y}{\partial x} + \nu \frac{\partial^2 y}{\partial x^2} + \mathbf{u}(t,x) & \text{in } [0,T] \times \Omega \\
y(t,x) = 0 & \text{on } [0,T] \times \partial\Omega \\
y(0,x) = y_0(x) & \text{in } \{t=0\} \times \Omega
\end{cases}
$$

Here $\nu$ is the viscosity parameter, and $y_0(\mathbf{x})$ is the initial condition. Subject to these equations, given a target state $y_d(x)$, the objective of control is to minimize the control error $\mathcal{J}_\text{actual}$ between $y_T$ and $y_d$, while constraining the energy cost $\mathcal{J}_\text{energy}$ of the control sequence $\mathbf{u}(t,x)$.

We follow instructions in~\cite{wei2024generative} to generate a 1D Burgers’ equation dataset. Specifically, for numerical simulation, we discretized the spatial domain [0,1] and temporal domain [0,1] using the finite difference method (FDM). The spatial grid consisted of 128 points, while the temporal domain was divided into 10000 timesteps. We initiated the system with randomly sampled initial conditions and control inputs drawn from specified probability distributions. This setup allowed us to generate 90000 trajectories for training and 50 trajectories for testing purposes.

\subsection{Kuramoto Dynamics}
The Kuramoto model is a paradigmatic system for studying synchronization phenomena. We considered a ring network of $N=8$ Kuramoto oscillators. The dynamics of the phases (states) of oscillators are expressed by:

\begin{equation}
    \dot\theta_{i,t} = \omega + \gamma(\sin(\theta_{i-1,t-1}-\theta_{i,t-1}) + \sin(\theta_{i+1,t-1}-\theta_{i,t-1}))+u_{i,t-1}, \quad i=1,2,...,N.
\end{equation}


For the Kuramoto model, we generated 20,000 samples for training and 50 samples for testing. The initial phases were sampled from a Gaussian distribution $\mathcal{N}(0,I)$, and the random intervention control signals were sampled from $\mathcal{N}(0,2I)$. The system was simulated for $T=16$ time steps with $\omega=0$, following~\citet{baggio2021data}. The resulting phase observations and control signals were used as the training and test datasets.

\subsection{Inverted Pendulum Dynamics}
The inverted pendulum is a classic nonlinear control system. The dynamics can be represented by:

$$
\frac{d^2\theta}{dt^2} = \frac{g}{L}\sin(\theta) - \frac{\mu}{L}\frac{d\theta}{dt} + \frac{1}{mL^2}u
$$

where $\theta$ is the angle from the upward position, and $u$ is the control input torque. The system parameters are set as: gravity $g=9.81$ m/s², pendulum length $L=1.0$ m, mass $m=1.0$ kg, and friction coefficient $\mu=0.1$.

To generate the training dataset, we simulate 90,000 trajectories for training and 50 for testing with 128 time steps each, using a time step of 0.01s. For each trajectory, we randomly sample initial states near the unstable equilibrium point with $\theta_0 \sim \mathcal{U}(-1,1)$ and $\dot{\theta}_0 \sim \mathcal{U}(-1,1)$, and generate control inputs from $u \sim \mathcal{U}(-0.5,0.5)$. The resulting dataset contains the state trajectories and their corresponding control sequences.
\section{Implementation Details}
\subsection{Implementation of SEDC}
In this section, we describe various architectural and hyperparameter details:
\begin{itemize}
    \item The temporal U-Net~(1D-Unet) \citep{janner2022planning} in the denoising network consists of a U-Net structure with $4$ repeated residual blocks. Each block comprises two temporal convolutions, followed by group normalization, and a final Mish nonlinearity. The channel dimensions of the downsample layers are ${1,2,4}*state dimension$. Timestep embedding is produced by a Sinusoidal Positional Encoder, following a 2-layer MLP, and the dimension of this embedding is 32. The dimension of condition embedding is the same as the system state dimension.
    \item We represent the inverse dynamics $f_\phi$ with an autoregressive model with 64 hidden units and ReLU activations. The model autoregressively generates control outputs along the control dimensions.
    \item We train $\mathbf{x}_\theta$ and $f_\phi$ using the Adam optimizer with learning rates from \{1e-3, 5e-3, 1e-4\}. The exact choice varies by task. Moreover, we also use a learning rate scheduler with step factor=0.1. Training batch size is 32.
    \item We use $K=128$ diffusion steps.
    \item We use a guidance scale $\lambda\in\{0.01,0.001,0.1\}$ but the exact choice varies by task.
\end{itemize}

\subsection{Training and Inference Time Analysis}

\begin{table}[h]
    \centering
    \caption{Approximate Training Time Comparison of Different Models on Various Datasets (in hours)}
    \label{tab:training_time}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Dataset/System & DecisionDiffuser & RDM & DiffPhyCon & AdaptDiffuser & SEDC \\
    \midrule
    Burgers & 2.5 & 2.5 & 3.0 & 2.5 & 2.5 \\
    Kuramoto & 1.5 & 1.5 & 1.5 & 1.0 & 1.0 \\
    IP & 1.0 & 1.0 & 1.5 & 1.0 & 0.5 \\
    Swing & 1.5 & 1.5 & 2.0 & 1.5 & 1.0 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Approximate Inference Time Comparison of Different Models on Various Datasets (in seconds)}
    \label{tab:inference_time}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Dataset/System & DecisionDiffuser & RDM & DiffPhyCon & AdaptDiffuser & SEDC \\
    \midrule
    Burgers & 3.0 & 4.0 & 6.0 & 4.0 & 4.0 \\
    Kuramoto & 1.0 & 1.5 & 2.0 & 1.5 & 1.5 \\
    IP & 0.5 & 1.0 & 1.0 & 0.5 & 0.5 \\
    Swing & 1.5 & 2.0 & 2.5 & 1.5 & 1.5 \\
    \bottomrule
    \end{tabular}
\end{table}

The diffusion-based methods are trained on single NVIDIA GeForce RTX 4090 GPU. We evaluate the training and inference time of all the diffusion-based methods evaluated in the experiment session.
As shown in Table \ref{tab:training_time}, we compare the training efficiency of different models across various datasets. DiffPhyCon consistently shows longer training times compared to other methods, because it requires training two models that learn the joint distribution and the prior distribution respectively, increasing its training time consumption. The training times of DecisionDiffuser, RDM, and AdaptDiffuser are generally comparable, while SEDC demonstrates relatively efficient training performance across most datasets. This may be because of the proposed designs that not only improve sample efficiency but also improve learning efficiency.

The inference time comparison in Table \ref{tab:inference_time} reveals that DiffPhyCon requires longer execution time compared to other models, because it needs to sample from two learned distributions in the denoising process. RDM achieves relatively slower inference speeds than DecisionDiffuser, AdaptDiffuser, and SEDC, because RDM replans during inference, increasing planning time. Notably, all models exhibit shorter training and inference times on the IP dataset, suggesting the influence of system complexity on computational efficiency. 

\section{Baselines Description}
\label{sup:baselines}
\subsection{PID}

PID (Proportional-Integral-Derivative) control is a classical feedback control methodology that has been widely adopted in industrial applications. The control signal is generated by computing the weighted sum of proportional, integral, and derivative terms of the error. The control law can be expressed as:

$$
u(t) = K_p e(t) + K_i \int_0^t e(\tau)d\tau + K_d \frac{d}{dt}e(t)
$$

While PID controllers exhibit robust performance and require minimal system modeling, their effectiveness may be compromised when dealing with highly nonlinear or time-varying systems, necessitating frequent parameter tuning.

\subsection{BC, BPPO}

Behavior Cloning (BC) represents a supervised imitation learning paradigm that aims to learn a direct mapping from states to actions by minimizing the deviation between predicted actions and expert demonstrations. Despite its implementation simplicity and sample efficiency, BC suffers from distributional shift, where performance degradation occurs when encountering states outside the training distribution. The objective function can be formulated as:

$$
L_{BC}(\theta) = \mathbb{E}_{(s,a)\sim \mathcal{D}} [-\log \pi_\theta(a|s)]
$$

where $\mathcal{D}$ denotes the expert demonstration dataset.

Behavior-guided PPO (BPPO) presents a hybrid approach that integrates behavior cloning with Proximal Policy Optimization. By incorporating a behavioral cloning loss term into the PPO objective, BPPO facilitates more efficient policy learning while maintaining the exploration capabilities inherent to PPO. The composite objective function is defined as:

$$
L_{BPPO}(\theta) = L_{PPO}(\theta) + \alpha L_{BC}(\theta)
$$

where $\alpha$ serves as a balancing coefficient between the PPO and BC objectives.

Each method exhibits distinct characteristics: BC demonstrates effectiveness when abundant high-quality expert demonstrations are available. BPPO leverages the synergy between expert knowledge and reinforcement learning for complex control scenarios. 

\subsection{Diffusion-based methods}

\begin{itemize}
    \item \textbf{DecisionDiffuser:} \\
        A novel approach that reformulates sequential decision-making as a conditional generative modeling problem rather than a reinforcement learning task. The core methodology involves modeling policies as return-conditional diffusion models, enabling direct learning from offline data without dynamic programming. The model can be conditioned on various factors including constraints and skills during training.

    \item \textbf{DiffPhyCon:} \\
        A diffusion-based method for controlling physical systems that operates by jointly optimizing a learned generative energy function and predefined control objectives across entire trajectories. The approach incorporates a prior reweighting mechanism to enable exploration beyond the training distribution, allowing the discovery of diverse control sequences while respecting system dynamics.

    \item \textbf{AdaptDiffuser:} \\
        An evolutionary planning framework that enhances diffusion models through self-evolution. The method generates synthetic expert data using reward gradient guidance for goal-conditioned tasks, and employs a discriminator-based selection mechanism to identify high-quality data for model fine-tuning. This approach enables adaptation to both seen and unseen tasks through continuous model improvement.

    \item \textbf{RDM:} \\
        A replanning framework for diffusion-based planning systems that determines replanning timing based on the diffusion model's likelihood estimates of existing plans. The method introduces a mechanism to replan existing trajectories while maintaining consistency with original goal states, enabling efficient bootstrapping from previously generated plans while adapting to dynamic environments.
\end{itemize}

% \subsection{Ours:SEDC}
% \subsection{Baselines}
% \subsubsection{PID}
% \subsubsection{BC}
% \subsubsection{BPPO}
% \subsubsection{Diffusion-based}
\section{Numeric Results of Figure 2}
\label{sup:numeric}
% \begin{table*}[h]
% \centering
% \caption{\textbf{Performance comparison of different models across three datasets.} Lower values indicate better performance for both metrics.}
% \label{tab:comparison}
% \begin{tabular}{l|cc|cc|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Burgers}} & \multicolumn{2}{c|}{\textbf{Kuramoto}} & \multicolumn{2}{c|}{\textbf{IP}} & \multicolumn{2}{c}{\textbf{Power}} \\
% \cmidrule{2-9}
% & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) \\
% \midrule
% PID & 1.30e-1 & 6.56 & 7.99e-1 & 30.35 & 8.64e-3 & 2.28e-1 & 1.50 & 8.38e-2 \\
% DCCN & 2.59e-3 & \textbf{2.30e-2} & 1.70e-3 & \textbf{1.56} & 1.18e-3 & \textbf{3.84e-5} & 7.81e-2 & 6.79e-2 \\
% BPPO & 5.90e-4 & 9.72 & 1.56e-4 & 26.64 & 3.63e-3 & 4.16e-3 & 2.94e-2 & \underline{8.80e-4} \\
% BC & 4.78e-4 & 10.73 & 1.52e-4 & 27.59 & 3.63e-3 & 4.20e-3 & \underline{2.82e-2} & \textbf{8.72e-4} \\
% DecisionDiffuser & 2.46e-4 & 5.18 & \underline{3.88e-5} & 27.48 & 3.85e-4 & 9.00e-4 & 4.14e-2 & 3.07e-1 \\
% RDM & 2.70e-4 & 7.01 & 4.60e-4 & 29.03 & 3.65e-4 & 3.38e-3 & 6.09e-2 & 2.36e-1 \\
% DiffPhyCon & \underline{1.62e-4} & 5.15 & 4.80e-4 & 18.72 & \underline{2.63e-4} & 1.99e-3 & 4.80e-2 & 2.36e-1 \\
% Ours & \textbf{9.80e-5} & \underline{5.01} & \textbf{8.90e-6} & \underline{14.90} & \textbf{6.49e-5} & 8.90e-4 & \textbf{1.63e-2} & 2.29e-1 \\
% \bottomrule
% \end{tabular}
% \end{table*}
% \begin{table*}[h]
% \centering
% \caption{\textbf{Performance comparison of different models across three datasets.} Lower values indicate better performance for both metrics. Best and second-best results of each row are highlighted in \textbf{bold} and \underline{underlined} respectively.}
% \label{tab:main_comparison}
% \begin{tabular}{l|cc|cc|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Burgers}} & \multicolumn{2}{c|}{\textbf{Kuramoto}} & \multicolumn{2}{c|}{\textbf{IP}} & \multicolumn{2}{c}{\textbf{Power}} \\
% \cmidrule{2-9}
% & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) \\
% \midrule
% PID & 1.30e-1 & 6.56 & 7.99e-1 & 30.35 & 8.64e-3 & 2.28e-1 & 1.50e+0 & 8.38e-2 \\
% BPPO & 5.90e-4 & 9.72 & 1.56e-4 & 26.64 & 3.63e-3 & 4.16e-3 & 2.94e-2 & \underline{8.80e-4} \\
% BC & 4.78e-4 & 10.73 & 1.52e-4 & 27.59 & 3.63e-3 & 4.20e-3 & \underline{2.82e-2} & \textbf{8.72e-4} \\
% DecisionDiffuser & 2.46e-4 & 5.18 & 3.88e-5 & 27.48 & 6.65e-4 & \underline{9.00e-4} & 4.14e-2 & 3.07e-1 \\
% RDM & 2.70e-4 & 7.01 & 4.60e-4 & 29.03 & 7.85e-4 & 3.38e-3 & 6.09e-2 & 2.36e-1 \\
% DiffPhyCon & \underline{1.62e-4} & 5.15 & 4.80e-4 & \underline{18.72} & \underline{6.63e-4} & 1.99e-3 & 4.80e-2 & 2.36e-1 \\
% AdaptDiffuser & 2.28e-4 & \textbf{4.645} & \underline{1.76e-5} & 26.23 & 8.64e-4 & 5.49e-3 & 4.74e-2 & 1.38e-1 \\
% Ours & \textbf{9.80e-5} & \underline{5.01} & \textbf{8.90e-6} & \textbf{14.90} & \textbf{3.49e-4} & \textbf{8.90e-4} & \textbf{1.63e-2} & 2.29e-1 \\

% \bottomrule
% \end{tabular}
% \end{table*}
\begin{table*}[h]
\centering
\caption{\textbf{Performance comparison of different models across three datasets.} Lower values indicate better performance for both metrics. Best and second-best results of each row are highlighted in \textbf{bold} and \underline{underlined} respectively.}
\label{tab:main_comparison}
\begin{tabular}{l|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Burgers}} & \multicolumn{2}{c|}{\textbf{Kuramoto}} & \multicolumn{2}{c}{\textbf{IP}} \\
\cmidrule{2-7}
& Target Loss & $J$(Energy) & Target Loss & $J$(Energy) & Target Loss & $J$(Energy) \\
\midrule
PID & 1.30e-1 & 6.56 & 7.99e-1 & 30.35 & 8.64e-3 & 2.28e-1 \\
BPPO & 5.90e-4 & 9.72 & 1.56e-4 & 26.64 & 3.63e-3 & 4.16e-3 \\
BC & 4.78e-4 & 10.73 & 1.52e-4 & 27.59 & 3.63e-3 & 4.20e-3 \\
DecisionDiffuser & 2.46e-4 & 5.18 & 3.88e-5 & 27.48 & 6.65e-4 & \underline{9.00e-4} \\
RDM & 2.70e-4 & 7.01 & 4.60e-4 & 29.03 & 7.85e-4 & 3.38e-3 \\
DiffPhyCon & \underline{1.62e-4} & 5.15 & 4.80e-4 & \underline{18.72} & \underline{6.63e-4} & 1.99e-3 \\
AdaptDiffuser & 2.28e-4 & \textbf{4.645} & \underline{1.76e-5} & 26.23 & 8.64e-4 & 5.49e-3 \\
Ours & \textbf{9.80e-5} & \underline{5.01} & \textbf{8.90e-6} & \textbf{14.90} & \textbf{3.49e-4} & \textbf{8.90e-4} \\
\bottomrule
\end{tabular}
\end{table*}

We leverage 2-D plots in the main paper to better illustrate the performance comparison of all the methods. Here we provide the provides the corresponding numerical results in detail in Table~\ref{tab:main_comparison}.

\section{Results and Discussion of the ablation study on GSF}
\label{sup:gsf_result}
\begin{table}[h]
    \centering
    \caption{Energy consumption reduction across different dynamical systems after each round of GSF. 
    The ``Before'' column shows energy performance of the trained model before finetuning. 
    For each round, ``\(\Delta\)\%'' represents the percentage reduction compared to its previous stage. 
    Lower energy values indicate better system performance. All systems demonstrate significant initial improvements (1st round) followed by smaller incremental gains (2nd round). 
    }
    \label{tab:gsf}
    \begin{tabular}{@{}l r *{2}{r r}@{}}
    \toprule
    \multirow{2}{*}{System} & \multirow{2}{*}{Before} & \multicolumn{2}{c}{1st round} & \multicolumn{2}{c}{2nd round} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6}
    & & Value & \(\Delta\)\% & Value & \(\Delta\)\% \\
    \midrule
    Burgers & 8.39 & 5.23 & 37.7\% & 5.01 & 4.2\% \\
    Kuramoto & 17.48 & 15.20 & 13.0\% & 14.90 & 2.0\% \\
    Swing & 0.27 & 0.24 & 13.4\% & 0.23 & 2.4\% \\
    IP & 9.00e-3 & 9.50e-4 & 89.4\% & 8.90e-4 & 6.3\% \\
    \midrule
    Average \(\Delta\)\% & -- & -- & 38.4\% & -- & 3.7\% \\
    \bottomrule
    \end{tabular}
\end{table}

To validate GSF’s effectiveness in
guiding the model toward learning the optimal (energy-
efficient) target distribution, we conduct ablation studies
on fine-tuning rounds and test control signal energy
on the test set. The result is provided in Table~\ref{tab:gsf}. Before finetuning, energy performance is the poorest because of the non-optimality of the initial training samples, and the first round of GSF greatly supplemented the training toward optimality, as the energy of the produced control inputs decreases at an average of 38.4\%. The increases observed in the second round persist but are significantly lower than those in the first round.
This may be because the first round of GSF has already captured the most significant deviations toward optimality, while subsequent rounds primarily refine these improvements with diminishing returns. The slowing rate of improvement suggests the model is approaching a convergence point in its exploration of the optimal control space. Overall, the above result demonstrates the effectiveness of GSF in enabling exploration beyond
initial training data and facilitating convergence toward optimal control strategies.

\section{Visualization}
\label{sup:visual}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{fig/vis/burgers_AdaptDiffuser}
        \includegraphics[width=\textwidth]{fig/vis/burgers_DecisionDiffuser}
        \includegraphics[width=\textwidth]{fig/vis/burgers_DiffPhyCon}
        \includegraphics[width=\textwidth]{fig/vis/burgers_RDM}
        \includegraphics[width=\textwidth]{fig/vis/burgers_SEDC}
        \caption{Burgers}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{fig/vis/kuramoto_AdaptDiffuser}
        \includegraphics[width=\textwidth]{fig/vis/kuramoto_DecisionDiffuser}
        \includegraphics[width=\textwidth]{fig/vis/kuramoto_DiffPhyCon}
        \includegraphics[width=\textwidth]{fig/vis/kuramoto_RDM}
        \includegraphics[width=\textwidth]{fig/vis/kuramoto_SEDC}
        \caption{Kuramoto}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{fig/vis/ip_AdaptDiffuser}
        \includegraphics[width=\textwidth]{fig/vis/ip_DecisionDiffuser}
        \includegraphics[width=\textwidth]{fig/vis/ip_DiffPhyCon}
        \includegraphics[width=\textwidth]{fig/vis/ip_RDM}
        \includegraphics[width=\textwidth]{fig/vis/ip_SEDC}
        \caption{Inverse Pendulum}
    \end{subfigure}
    \caption{Comparison of different methods on Burgers, Kuramoto and Inverse Pendulum systems}
    \label{fig:comparison}
\end{figure}

We present some visualization results of our method and best-performing baselines under three systems. The goal is to make the end state (T=10 for Burgers and T=15 for Kuramoto) close to the target state. As can be seen, SEDC's final state always coincides with the target state. In contrast, the baselines showed inferior results, as some mismatch with the target state can be observed.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
