% \subsubsection*{1. Encoding Phase}
% The encoder maps the input sequence \( \{\mathbf{x}_t\}_{t=1}^T \) into a latent representation \( \mathbf{H} \):
% \begin{equation}
%     \mathbf{H} = \text{TEMP}_{\text{enc}}\left(\{\mathbf{x}_t\}_{t=1}^T\right),
% \end{equation}
% where:
% \begin{itemize}
%     \item \( \text{TEMP}_{\text{enc}} \): A generic operator (e.g., RNN, CNN, or MLP) that captures temporal dependencies or patterns in the input sequence.
%     \item \( \mathbf{H} \): Encoded representation, which could be a sequence of hidden states (e.g., from RNNs) or a fixed-dimensional vector (e.g., from CNNs or MLPs).
% \end{itemize}

% \subsubsection*{2. Decoding Phase}
% The decoder generates the forecasted sequence \( \{\mathbf{y}_{t'}\}_{t'=T+1}^{T+H} \) based on the encoded representation \( \mathbf{H} \):
% \begin{equation}
%     \mathbf{y}_{t'} = \text{TEMP}_{\text{dec}}\left(\mathbf{H}, \{\mathbf{y}_{t''}\}_{t''=T+1}^{t'-1}\right), 
% \end{equation}
% where:
% \begin{itemize}
%     \item \( \text{TEMP}_{\text{dec}} \): A generic operator that maps the encoded representation and previous outputs to the current output.
%     \item \( \{\mathbf{y}_{t''}\}_{t''=T+1}^{t'-1} \): Previously predicted outputs, used in an autoregressive manner.
% \end{itemize}

% \subsubsection*{1. Encoder}
% \begin{equation}\label{rec}
%     \mathbf{h}_t = \text{TEMP}\left(\mathbf{x}_t\right)
% \end{equation}

% where:
% \begin{itemize}
%     \item \( \mathbf{x}_t \): Input vector at timestep \( t \).
%     \item \text{TEMP} is an encoder which process data in temporal order.
% \end{itemize}

% \subsubsection*{2. Decoder}
% \begin{equation}
%     \mathbf{y}_t = \mathcal{F}_y\left(\mathbf{h}_t\right)
% \end{equation}

% where:
% \begin{itemize}
%     \item \( \mathbf{y}_t \): Output vector at timestep \( t \).
%     \item \( \mathcal{F}_y \): A generic function that maps the hidden state to the output (e.g \acrshort{mlp}).
% \end{itemize}


% In this paradigm, operations are executed in two steps:

% \subsubsection*{1. Message Aggregation}
% \[
% \mathbf{m}_v^{(l)} = \text{AGGREGATE}^{(l)}\left(\{\mathbf{h}_u^{(l-1)} : u \in \mathcal{N}(v)\}\right)
% \]
% where:
% \begin{itemize}
%     \item \( \mathbf{h}_u^{(l-1)} \): Feature vector of node \( u \) at layer \( l-1 \).
%     \item \( \mathcal{N}(v) \): Set of neighbors of node \( v \).
%     \item \( \text{AGGREGATE}^{(l)} \): Permutation-invariant function (e.g., \textit{sum}, \textit{mean}, \textit{max} or \textit{attention-based mechanism}).
% \end{itemize}

% \subsubsection*{2. Feature Update}
% \[
% \mathbf{h}_v^{(l)} = \text{UPDATE}^{(l)}\left(\mathbf{h}_v^{(l-1)}, \mathbf{m}_v^{(l)}\right)
% \]
% where \( \text{UPDATE}^{(t)} \) is a learnable function (e.g., neural network).
% % \begin{itemize}
% %     \item 
% % \end{itemize}

% \subsubsection*{Output Computation}
% The node abstract representations are mapped to desired outputs.
% \begin{equation}
%     \mathbf{y}_v = \mathcal{F}_y\left(\mathbf{h}_v\right)
% \end{equation}

% This architecture takes the reverse order of equations (\ref{equ:tts}) (3). Specifically, by propagating in the spatial dimension, we obtain spatial respenstation of all population in time stamp. Sequentially, a learnable function, such as a \acrshort{cnn} or \acrshort{mlp}, processes information in temporal dimension. This architecture closely resembles the \acrshort{tas} approach, with the distinction that \acrshort{tas} employs \acrshort{rnn} models to recursively takes spatial representations in each time step.

% Block of \inlinebox{\adjustbox{valign=B}{\colorbox{Blue}{$X_i^{t}$}}} represents load consumption $\mathbf{X} \in \mathbb{R}^{N\times T}$ of N consumers in from period $[t, t+T)$.

    % \begin{subfigure}[b]{0.23\textwidth}
    %     \centering
    %     
    % \end{subfigure}

    % \hfill
    % \begin{subfigure}[b]{0.14\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{assets/stt.pdf}
    %     \caption{\acrshort{stt} architecture}
    %     \label{fig:stt}
    % \end{subfigure}

% \paragraph{STEGNN~\cite{wei_short-term_2023}} This model incorporates a graph formation learning mechanism from~\cite{wu_connecting_2020}. Furthermore, it employs a moving average graph convolutional layer to smooth out the transition between multiple graph layers. At this stage, the model processes only data in the spatial axis. Then a \acrshort{mlp} model capture transformed features for forecasting. This model is therefore of type \acrshort{stt}.

% Table \ref{tab:overview} gives an overview of some \acrshort{stgnn}s in the literature that fit to the framework we introduced in previous sections.
 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}