\section{Results and discussion}

In each experiment, we perform the procedure 5 times using identical parameters to calculate the statistics of the experiment. The results will be displayed as the mean, followed by the standard deviation from the 5 trials. To enhance the presentation of the results, we use \textbf{bold font} to highlight instances where the performance of \acrshort{stgnn} models surpasses all benchmark models, and \underline{underlining} to indicate the best performance according to the error metrics.


\subsection{Comparison between graph formation methods}\label{subsubsec:graph_formation}
Methods for graph formation can produce various topologies that facilitate the aggregation of information. This section aims to explore whether creating graphs based on different types of similarities between time series impacts forecasting performance. The focus is exclusively on the 3 first models in Table \ref{tab:overview} which require signal-based predefined graphs.
\begin{table}[h!]
\centering
\caption{Performance of models which requires predefined graph 
\newline
(Table \ref{tab:overview}) in Split 3.}
\begin{tabular}{@{}lllll@{}}
                          &     &     & Metrics  &    \\
\multirow{-2}{*}{Model}   & \multirow{-2}{*}{Graph  formation} & MAE (Wh) & MAPE (\%)    & RMSE (Wh)                                                                                        \\
\hline
                          & Euclidean        & \underline{149.7(0.1)} & 55.9(0.3) & \underline{295.6(0.6)   }\\
                          & DTW              & 149.8(0.3) & \underline{55.4(1.1)} & 295.8(1.0) \\
                          & Correntropy      & 149.8(0.4) & 55.5(0.6) & 295.7(0.7)   \\
\multirow{-4}{*}{GRUGCN}                          & Pearson          & 150.7(0.1) & 56.2(0.4) & 297.8(1.0)   \\
  % & Transfer entropy &  0.151 & 0.556  & 0.299 \\
\hline
                          & Euclidean        & \underline{150.9(0.5)} & \underline{56.5(0.5)} & 297.9(1.3)   \\
                          & DTW              & \underline{150.9(0.4)} & 57.1(1.0) & \underline{297.3(1.3)}    \\
                          & Correntropy      & 151.4(0.6) & 56.8(0.6) & 298.8(0.7)  \\
\multirow{-4}{*}{T-GCN}                           & Pearson          & 151.7(0.3) & 57.2(0.5) & 298.6(0.4) \\
  % & Transfer entropy & 0.149    & 0.573   & 0.296   \\
\hline
                          & Euclidean        & 149.1(0.1) & 55.7(0.4) & 295.7(0.9) \\
                          & DTW              & 149.2(0.5) & 56.2(0.6) & \underline{295.5(0.8)}     \\
                          & Correntropy      & \underline{149.0(0.4)} & \underline{55.0(0.4)} & 295.7(0.6) \\
\multirow{-4}{*}{GCGRU}                          & Pearson          & 149.6(0.2) & 56.4(0.2) & 295.8(0.6)  \\
 % & Transfer entropy & 0.235  & 1.224  & 0.428 \\
\hline
\end{tabular}
\label{tab:graph_formation}
\end{table}


%%%%%%

%%%% Full result at residential level fold 1 %%%%

% \begin{tabular}{llll}
% \toprule
%  & MAE & MAPE & RMSE \\
% \midrule
% GraphWaveNetModel & 91.4(0.2) & 48.3(0.8) & 196.7(0.7) \\
% RNNModel & 89.5(0.1) & 44.7(0.3) & 194.0(0.5) \\
% VARModel & 134.3(0.2) & 109.4(0.4) & 218.5(0.3) \\
% GatedGraphNetworkModel & 88.6(0.1) & 45.7(0.6) & 188.8(0.5) \\
% AGCRNModel & 90.9(0.2) & 49.0(0.4) & 190.8(0.6) \\
% TransformerModel & 90.4(0.4) & 45.1(0.9) & 194.5(0.4) \\
% BiPartiteSTGraphModel & 90.2(0.1) & 49.2(0.9) & 188.3(0.4) \\
% SameHour & 114.4(0.0) & 66.4(0.0) & 238.6(0.0) \\
% GRUGCNModel & 89.2(0.8) & 43.5(0.8) & 193.6(0.3) \\
% TGCNModel_2 & 88.8(0.3) & 43.3(0.8) & 193.4(0.1) \\
% TGCNModel & 88.2(0.2) & 43.6(0.4) & 191.9(0.2) \\
% GRUGCNModel & 89.1(0.6) & 43.3(1.0) & 194.1(0.6) \\
% TGCNModel_2 & 88.9(0.4) & 43.7(0.3) & 193.9(0.7) \\
% TGCNModel & 88.2(0.2) & 43.5(0.5) & 192.5(0.5) \\
% GRUGCNModel & 89.0(0.2) & 43.0(1.1) & 193.7(0.6) \\
% TGCNModel_2 & 88.9(0.5) & 43.8(0.1) & 193.5(1.0) \\
% TGCNModel & 88.4(0.2) & 43.5(0.9) & 192.5(0.6) \\
% GRUGCNModel & 89.4(0.2) & 43.8(0.9) & 194.6(0.7) \\
% TGCNModel_2 & 89.6(0.2) & 43.8(0.5) & 194.6(0.3) \\
% TGCNModel & 88.5(0.3) & 43.8(1.2) & 193.6(0.5) \\
% \bottomrule
% \end{tabular}

%%%% Full result at aggregate level fold 1 %%%%

% \begin{tabular}{llll}
% \toprule
%  & MAE & MAPE & RMSE \\
% \midrule
% GraphWaveNetModel & 8852.3(437.6) & 17.4(1.0) & 10336.5(440.2) \\
% RNNModel & 8582.6(276.8) & 16.9(0.6) & 10080.4(285.0) \\
% VARModel & 3439.8(22.7) & 7.1(0.0) & 4562.5(39.6) \\
% GatedGraphNetworkModel & 8216.2(317.4) & 15.9(0.7) & 9801.8(273.1) \\
% AGCRNModel & 7909.1(145.7) & 15.2(0.3) & 9427.3(190.6) \\
% TransformerModel & 8827.3(369.0) & 17.3(0.6) & 10340.2(427.3) \\
% BiPartiteSTGraphModel & 6614.5(348.2) & 13.1(0.8) & 8068.0(341.1) \\
% SameHour & 3388.1(0.0) & 6.8(0.0) & 4574.1(0.0) \\
% GRUGCNModel & 9266.6(154.2) & 18.1(0.4) & 10752.9(172.3) \\
% TGCNModel_2 & 9268.4(260.1) & 18.1(0.5) & 10684.3(283.6) \\
% TGCNModel & 8631.9(202.0) & 16.6(0.4) & 10173.6(208.0) \\
% GRUGCNModel & 9557.0(516.1) & 18.7(1.1) & 11048.1(483.4) \\
% TGCNModel_2 & 9248.8(291.0) & 18.0(0.5) & 10691.4(334.8) \\
% TGCNModel & 8826.0(359.2) & 17.1(0.7) & 10249.9(378.8) \\
% GRUGCNModel & 9383.2(566.2) & 18.2(1.2) & 10926.0(580.0) \\
% TGCNModel_2 & 9037.7(340.2) & 17.5(0.7) & 10538.1(368.4) \\
% TGCNModel & 8878.0(540.0) & 17.1(1.1) & 10342.0(508.1) \\
% GRUGCNModel & 9352.1(602.8) & 18.2(1.3) & 10874.4(588.7) \\
% TGCNModel_2 & 9206.5(298.9) & 17.9(0.5) & 10751.0(358.4) \\
% TGCNModel & 8957.3(507.8) & 17.2(1.0) & 10544.0(505.4) \\
% \bottomrule
% \end{tabular}

%%%%  Full result at residential level fold 2 %%%%

% \begin{tabular}{llll}
% \toprule
%  & MAE & MAPE & RMSE \\
% \midrule
% GraphWaveNetModel & 128.4(0.3) & 52.9(0.6) & 256.7(1.0) \\
% RNNModel & 126.5(0.2) & 50.5(0.4) & 254.7(0.4) \\
% VARModel & 167.6(0.1) & 106.7(0.2) & 283.9(0.4) \\
% GatedGraphNetworkModel & 121.7(0.4) & 49.5(0.6) & 245.9(1.5) \\
% AGCRNModel & 123.8(0.3) & 50.6(0.6) & 248.7(0.6) \\
% TransformerModel & 127.9(0.5) & 52.1(0.4) & 255.4(0.5) \\
% BiPartiteSTGraphModel & 121.6(0.1) & 49.4(0.5) & 246.0(0.8) \\
% SameHour & 156.4(0.0) & 75.9(0.0) & 304.1(0.0) \\
% GRUGCNModel & 125.3(0.2) & 50.0(0.2) & 251.4(0.5) \\
% TGCNModel_2 & 125.9(0.5) & 49.7(0.6) & 253.7(1.1) \\
% TGCNModel & 124.6(0.2) & 49.3(0.6) & 251.9(0.4) \\
% GRUGCNModel & 125.3(0.2) & 51.1(0.5) & 251.0(0.7) \\
% TGCNModel_2 & 126.1(0.2) & 50.6(0.6) & 253.6(0.6) \\
% TGCNModel & 124.6(0.2) & 49.6(0.8) & 251.7(0.7) \\
% GRUGCNModel & 125.1(0.2) & 50.3(0.6) & 250.6(0.6) \\
% TGCNModel_2 & 125.7(0.3) & 50.0(0.4) & 253.0(0.4) \\
% TGCNModel & 124.5(0.3) & 49.4(0.6) & 251.2(1.0) \\
% GRUGCNModel & 126.4(0.2) & 51.3(0.5) & 253.9(0.4) \\
% TGCNModel_2 & 126.9(0.5) & 50.7(0.6) & 256.1(1.1) \\
% TGCNModel & 125.3(0.2) & 50.2(0.6) & 252.9(0.3) \\
% \bottomrule
% \end{tabular}

%%%% Full result at aggregate level fold 2 %%%%

% \begin{tabular}{llll}
% \toprule
%  & MAE & MAPE & RMSE \\
% \midrule
% GraphWaveNetModel & 13908.5(468.0) & 19.2(0.9) & 17605.2(432.4) \\
% RNNModel & 13489.2(166.1) & 18.8(0.3) & 16970.4(167.8) \\
% VARModel & 13730.5(252.8) & 18.0(0.4) & 18669.1(273.7) \\
% GatedGraphNetworkModel & 12787.0(562.2) & 16.9(0.7) & 16790.6(715.5) \\
% AGCRNModel & 13264.5(345.9) & 17.9(0.5) & 17339.1(344.5) \\
% TransformerModel & 13342.9(73.2) & 18.6(0.2) & 16909.1(46.5) \\
% BiPartiteSTGraphModel & 12774.9(389.0) & 17.1(0.6) & 16559.8(423.0) \\
% SameHour & 5478.3(0.0) & 8.2(0.0) & 7958.2(0.0) \\
% GRUGCNModel & 12818.5(221.2) & 17.9(0.3) & 16227.1(242.3) \\
% TGCNModel_2 & 13930.1(332.2) & 19.4(0.4) & 17580.1(406.7) \\
% TGCNModel & 13543.2(305.0) & 18.5(0.4) & 17219.4(341.7) \\
% GRUGCNModel & 12597.9(456.4) & 17.3(0.7) & 16145.7(460.9) \\
% TGCNModel_2 & 13718.9(318.1) & 19.0(0.4) & 17420.9(366.8) \\
% TGCNModel & 13316.4(412.7) & 18.2(0.7) & 17057.7(439.2) \\
% GRUGCNModel & 12706.8(310.7) & 17.7(0.4) & 16171.3(404.7) \\
% TGCNModel_2 & 13711.6(89.9) & 19.1(0.2) & 17388.7(65.8) \\
% TGCNModel & 13170.4(543.1) & 18.2(0.9) & 16869.4(528.3) \\
% GRUGCNModel & 13079.5(256.7) & 18.0(0.6) & 16625.9(194.4) \\
% TGCNModel_2 & 13878.5(415.5) & 19.2(0.6) & 17674.4(444.5) \\
% TGCNModel & 13150.4(144.0) & 18.2(0.2) & 16870.8(215.1) \\
% \bottomrule
% \end{tabular}

%%%% Full result at residential level fold 3 %%%%

% \begin{tabular}{llll}
% \toprule
%  & MAE & MAPE & RMSE \\
% \midrule
% GraphWaveNetModel & 161.6(12.4) & 64.2(8.4) & 315.2(21.5) \\
% RNNModel & 153.1(1.7) & 56.9(2.2) & 302.2(0.9) \\
% VARModel & 198.5(23.1) & 119.1(31.3) & 328.4(13.2) \\
% GatedGraphNetworkModel & 159.5(25.4) & 71.5(31.8) & 299.2(18.0) \\
% AGCRNModel & 149.4(1.5) & 58.2(1.0) & 293.6(1.6) \\
% TransformerModel & 153.3(1.7) & 57.1(0.6) & 301.7(3.3) \\
% BiPartiteSTGraphModel & 149.2(2.6) & 55.1(1.2) & 295.2(3.9) \\
% SameHour & 178.7(15.3) & 75.5(11.3) & 345.6(25.2) \\
% GRUGCNModel & 137.5(24.7) & 53.1(5.0) & 275.0(41.4) \\
% TGCNModel_2 & 138.7(25.3) & 54.1(5.5) & 277.5(42.7) \\
% TGCNModel & 136.8(24.3) & 52.7(4.7) & 274.9(41.4) \\
% GRUGCNModel & 146.0(7.7) & 61.2(11.7) & 286.0(19.7) \\
% TGCNModel_2 & 138.4(25.0) & 54.3(5.6) & 276.6(41.4) \\
% TGCNModel & 145.6(7.3) & 61.9(11.4) & 285.8(19.4) \\
% GRUGCNModel & 144.8(9.8) & 54.9(2.1) & 286.6(18.0) \\
% TGCNModel_2 & 145.8(10.3) & 55.3(2.5) & 288.6(18.6) \\
% TGCNModel & 144.1(9.9) & 54.3(2.8) & 286.9(17.7) \\
% GRUGCNModel & 147.0(7.4) & 62.0(11.5) & 288.0(19.6) \\
% TGCNModel_2 & 139.4(24.6) & 54.8(4.9) & 277.9(41.4) \\
% TGCNModel & 146.0(7.3) & 62.2(11.6) & 285.9(19.8) \\
% \bottomrule
% \end{tabular}

%%%% Full result at aggregate level fold 3 %%%%

% \begin{tabular}{llll}
% \toprule
%  & MAE & MAPE & RMSE \\
% \midrule
% GraphWaveNetModel & 13206.6(3641.0) & 16.0(4.2) & 16387.4(3990.9) \\
% RNNModel & 15072.7(658.4) & 17.9(1.0) & 18575.3(444.6) \\
% VARModel & 10786.8(2450.3) & 12.9(2.8) & 14080.6(2493.0) \\
% GatedGraphNetworkModel & 12738.7(1743.4) & 15.1(2.0) & 15780.6(1647.5) \\
% AGCRNModel & 13240.2(297.0) & 15.5(0.3) & 16633.9(331.3) \\
% TransformerModel & 15338.8(779.1) & 18.0(1.0) & 18905.6(797.1) \\
% BiPartiteSTGraphModel & 14887.7(303.6) & 17.1(0.6) & 18457.6(249.5) \\
% SameHour & 7952.3(3969.0) & 9.7(4.2) & 10685.3(4430.2) \\
% GRUGCNModel & 13008.0(2169.5) & 16.8(0.5) & 15920.5(2871.2) \\
% TGCNModel_2 & 13363.0(2345.8) & 17.3(0.4) & 16298.3(3070.1) \\
% TGCNModel & 13233.9(2278.9) & 17.0(0.4) & 16194.4(3001.0) \\
% GRUGCNModel & 12902.1(3312.5) & 16.2(2.3) & 16046.6(3534.7) \\
% TGCNModel_2 & 13148.6(2093.8) & 17.1(0.8) & 16039.3(2704.0) \\
% TGCNModel & 12609.8(2896.0) & 15.9(1.7) & 15795.3(3061.1) \\
% GRUGCNModel & 13926.1(830.7) & 16.8(0.3) & 17338.3(848.9) \\
% TGCNModel_2 & 14468.6(870.3) & 17.7(0.8) & 17869.9(815.4) \\
% TGCNModel & 14194.2(330.0) & 17.3(0.8) & 17629.4(290.5) \\
% GRUGCNModel & 12458.2(2856.6) & 15.9(1.6) & 15504.1(3087.4) \\
% TGCNModel_2 & 12943.2(1865.4) & 16.9(0.5) & 15717.3(2405.7) \\
% TGCNModel & 12038.8(2796.1) & 15.4(1.6) & 15002.0(3033.4) \\
% \bottomrule
% \end{tabular}

%%%%%%


The results in \autoref{tab:graph_formation} suggest that graph construction methods do not significantly impact the effectiveness of learning, even though the resulting topologies may vary. \glspl{stgnn}, as a data-focused method, adapts by learning with different topologies to achieve comparable outcomes. In the following section, for each predefined-graph model, we employ the graph formation technique that results in the minimal MAE. 
\subsection{Model benchmark with different temporal scale at the residential level}\label{subsubsec:performance_comparison}

\begin{table*}[htbp]
\centering
\caption{Performance of different \acrshort{stgnn} models at the \textbf{residential} level. 
}
\begingroup
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.2} % Default value: 1
\resizebox{\textwidth}{!}{
\begin{tabular}{cllllllllll}
\hline
\hline
\multirow{3}{*}{Group} & \multirow{3}{*}{Models} & \multicolumn{9}{c}{Metrics} \\ \cline{3-11}
                       &                         & \multicolumn{3}{c}{Split 1}  & \multicolumn{3}{c}{Split 2} & \multicolumn{3}{c}{Split 3} \\ \cline{3-11}
                       &                         & MAE (Wh)    & MAPE (\%)   & RMSE (Wh)   & MAE (Wh)    & MAPE (\%)    & RMSE (Wh) & MAE (Wh)    & MAPE (\%)   & RMSE (Wh) \\ 
\hline
\multirow{4}{*}{Benchmark} 
& SeasonalNaive                              & 114.4(0.0) & 66.4(0.0) & 238.6(0.0) & 156.4(0.0) & 75.9(0.0) & 304.1(0.0) & 186.3(0.0) & 81.1(0.0) & 358.2(0.0) \\
& VAR                                         & 134.3(0.2) & 109.5(0.5) & 218.5(0.3) & 167.6(0.1) & 106.7(0.2) & 283.9(0.4) & 210.1(0.5) & 134.7(0.5) & 335.0(0.6)      \\
& GRU &
89.5(0.1) & 44.7(0.3) & 194.0(0.5) & 126.5(0.2) & 50.5(0.4) & 254.7(0.4) & 153.1(1.7) & 56.9(2.2) & 302.2(0.9) \\
& Transformer                                  &    90.2(0.3) & 44.9(0.8) & 194.5(0.4) & 127.9(0.5) & 52.1(0.4) & 255.4(0.5) & 154.1(0.1) & 56.7(0.1) & 303.4(0.4)      \\
\hline
\multirow{8}{*}{STGNN}  & GRUGCN                                         & \textbf{89.0(0.2)} & \underline{\textbf{43.0(1.1)}} & \textbf{193.7(0.6)}	     & \textbf{125.3(0.2)} & \textbf{50.0(0.2)} & \textbf{251.4(0.5)}       & \textbf{149.7(0.1)} & \textbf{55.9(0.3)} & \textbf{295.6(0.6)   }    \\
& GCGRU                                      & \underline{\textbf{88.2(0.2)}} & \textbf{43.6(0.4)} & \textbf{191.9(0.2)}     & \textbf{124.6(0.2)} & \underline{\textbf{49.3(0.6)}} & \textbf{251.9(0.4)}   &    \textbf{149.0(0.4)} & \textbf{55.0(0.4)} & \textbf{295.7(0.6)}  \\
& T-GCN                                     & \textbf{88.9(0.5)} & \textbf{43.8(0.1)} & \textbf{193.5(1.0)}   &   \textbf{125.0(0.5)}      &   \textbf{49.7(0.6)}      &     \textbf{253.7(1.1)}    & \textbf{150.9(0.5)} & \textbf{56.5(0.5)} & \textbf{297.9(1.3)}  \\
& AGCRN                                     & 91.0(0.2) & 49.1(0.4) & \textbf{190.9(0.6)} & \textbf{123.8(0.3)} & 50.6(0.6) & \textbf{248.7(0.6)} & \textbf{150.1(0.3)} & 58.8(0.2) & \textbf{294.2(1.1)}       \\
& GraphWavenet   & 91.4(0.2) & 48.1(0.8) & 196.7(0.8) & 128.4(0.3) & 52.9(0.6) & 256.7(1.0) & 155.9(0.5) & 60.1(0.5) & 304.6(0.8)  \\
& FC-GNN                            &   \textbf{88.6(0.1)} & 45.7(0.6) & \textbf{189.0(0.3)} & \textbf{121.7(0.4)} & \textbf{49.5(0.6)} & \underline{\textbf{245.9(1.5)}} & \underline{\textbf{146.9(0.1)}} & \textbf{55.6(0.6)} & \underline{\textbf{290.3(1.0)}}     \\
& BP-GNN                                   &   90.2(0.1) & 48.8(0.5) & \underline{\textbf{188.4(0.3)}} & \underline{\textbf{121.6(0.1)}} & \textbf{49.4(0.5)} & \textbf{246.0(0.8)} & \textbf{148.0(0.1)} & \underline{\textbf{54.7(0.5)}} & \textbf{293.3(0.9) }     \\
\hline
\hline
\end{tabular}
}
\endgroup
\label{tab:result_228}
\end{table*}

Compared to benchmark models that process only temporal features, the practice of adding relationships between households in \acrshort{stgnn} models increases forecasting performance. Especially, in contrast to \acrshort{gru}, which uses matrix multiplication as the unit cell of the recurrent network, \acrshort{gcgru} and \acrshort{t-gcn} always achieve better performance by applying \acrshort{gcn} as the unit cell. Similarly, \acrshort{grugcn} also outperforms \acrshort{gru} by applying \acrshort{gcn} on top of it to account for spatial dependency. It showcases the effective use of graph neural networks to model spatial relationships (see Table \ref{tab:result_228}).

% Comparing GCGRU, T-GCN which use GCN as spatial processing unit to embed node features with RNN which simply use matrix multiplication, we see that there is a small advantage when integrating spatial processing unit. That demonstrates relevance in applying the graph neural network to model spatial relationships.

Moreover, although models with a learnable graph theoretically offer more flexibility, this approach does not deliver promising outcomes; only \acrshort{agcrn} model performs compatible results with the best benchmark models (\acrshort{gru}). However, when comparing with \acrshort{gcgru}, which uses a predefined graph from signals, there is a downgrade in performance in some metrics. This is because the learnable graph offers a more flexible way to model the spatial relationship. However, it could make the forecasting task more susceptible to overfitting. When testing on a more distant future (one month after training, as outlined in Fig. \ref{fig:cross-val}), the model actually performs worse than the counterpart that uses a predefined graph. 

When testing with 3 splits, the \acrshort{bp-gnn} and \acrshort{fc-gnn} models often perform better than their counterparts. These models presume the topology of the graph without relying on the data (fully connected or bipartite); instead, when aggregating information from neighbor nodes, the models utilize a weighted sum of neighboring information, where the weights are derived from the input. This makes the "message aggregation" step more adaptable to the input. 
However, despite the naive assumption of graph topology, their better performance questions if the graph formation based on signals or learnable parameters is effective in the context of \acrshort{stlf}. 
An interesting case is the \acrshort{bp-gnn} model. Although it performs slightly worse than \acrshort{fc-gnn}, it is more scalable since the interaction between the nodes of \acrshort{fc-gnn} is $N^2$, while for the \acrshort{bp-gnn} model, it is only $2KN$ with $K$ being the number of virtual nodes. It also outperforms other models most of the time. One reason might be that, due to the nature of the load profile dataset, the consumption pattern of users is grouped by latent factors such as socio-demographic status~\cite{acorn}. The virtual nodes defined by the model can account for latent factors that can influence the original nodes (households). The bipartite topology allows these virtual nodes to gather information in a cluster-like manner; then, the aggregated information is passed down to each node as additional information for learning.


% In terms of forecasting result with different periods, we notice that the magnitude of error differs vastly among them, indicating that the nature of data and testing period strongly affect the performance. It suggests that learning from historical data alone is insufficient to capture the dynamics of energy consumption. Incorporating temporal indicators, such as weekends or holidays, could enhance the model's ability to capture these contextual variations more effectively.



\subsection{Model benchmark with different temporal scale at the aggregate level}
We investigate the performance of load forecasting at the aggregate level simply by aggregating all the forecasts at the residential level (see Table \ref{tab:result_agg_228}). At the aggregate level, the forecast results obtained through aggregation are inferior to those of the baseline model (SeasonalNaive). This behavior is also observed in other deep learning models such as \acrshort{gru} and \acrshort{tfm}. 
% An explanation is that these models tend to prioritize learning from easier periods with consistent patterns while failing to adequately capture atypical events, such as consumption spikes~\cite{zhang_unlocking_2023} (see Fig. 4). This can lead to underestimation during abnormal periods. At the aggregate level, instead of centering predictions around the actual average consumption, the forecasts may consistently underestimate the consumption spikes, and hence the errors are not balanced out. STGNNs, by incorporating spatial learning, potentially propagate errors throughout the spatial dimension and therefore do not address this issue.  
To explain this behavior, we visualize in Fig.\ref{fig:multiple-forecast} the histogram of the errors among all the households at peak hour (2013-12-23 19:00). The x-axis represents errors, the y-axis lists top-performing models (Table \ref{tab:result_agg_228}), and the z-axis shows frequency.



% \begin{table}[h!]
% \centering
% \captionsetup{justification=centering,margin=1cm}
% \begingroup
% \setlength{\tabcolsep}{5pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.2} % Default value: 1
% \begin{tabular}{l|lll}
% \hline
% \hline
% \multicolumn{1}{c|}{\multirow{3}{*}{Models}} & \multicolumn{3}{c}{Metrics} \\ \cline{2-4}
% \multicolumn{1}{c|}{}  & \multicolumn{3}{c}{Average (over three folds)} \\ \cline{2-4}
% \multicolumn{1}{c|}{}                        & MAE (kWh)     & MAPE (\%)   & RMSE   \\ 
% \hline
% SeasonalNaive                               &   4.495      &  0.075   &  7.00   \\
% \hline
% VAR                                         &  9.450      &  0.122      &  11.946 \\
% \hline
% GRU                                         &  12.267    &  0.178     &  15.024      \\
% \hline
% Tranformer                                  &   12.508     &  0.179    &  15.375          \\
% \hline
% GRUGCN                                      & 12.115	      & 0.176   &  14.880	       \\
% \hline
% % GCLSTM                                      &    0.139     &   1.22      &  0.239    \\
% % \hline
% % T-GCN                                       &  \textbf{0.089 }      &    \textbf{0.444 }    &   0.194    \\
% % \hline
% % AGCRNN                                      &   \textbf{0.090 }     &     0.484    &  \textbf{ 0.191 }       \\
% % \hline
% % STEGNN                                      &  3.067     &  34.660     &   3.866    \\
% % \hline
% TGCN                                        & 11.879       &  0173     &  14.584    \\
% \hline
% Bipartite                                   &   11.421     &  0.158    &  14.414   \\
% \hline
% % Fully Connected                             &         &         &      \\
% % \hline
% % GraphWavenet                                &   0.091      &   0.494      &    0.196    \\     
% \hline
% \end{tabular}
% \endgroup
% \caption{Performance of different \acrshort{stgnn} models at aggregate level.
% }
% \label{tab:result_228_agg}
% \end{table}


\begin{table*}[htbp]
\centering
\caption{Performance of different \acrshort{stgnn} models at the \textbf{aggregate} level.
}
\begingroup
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.2} % Default value: 1
\resizebox{\textwidth}{!}{
\begin{tabular}{cllllllllll}
\hline
\hline
\multirow{3}{*}{Group} & \multirow{3}{*}{Models} & \multicolumn{9}{c}{Metrics} \\ \cline{3-11}
                       &                         & \multicolumn{3}{c}{Split 1}  & \multicolumn{3}{c}{Split 2} & \multicolumn{3}{c}{Split 3} \\ \cline{3-11}
                       &                         & MAE (kWh)    & MAPE (\%)   & RMSE (kWh)   & MAE (kWh)    & MAPE (\%)    & RMSE (kWh) & MAE (kWh)    & MAPE (\%)   & RMSE (kWh) \\ 
\hline
\multirow{4}{*}{Benchmark} 
& SeasonalNaive                             & \underline{3.39(0.0)} & \underline{6.8(0.0)} & \underline{4.57(0.0)} & \underline{5.48(0.0)} & \underline{8.2(0.0)} & \underline{7.96(0.0)} & \underline{5.97(0.0)} & \underline{7.6(0.0)} & \underline{8.47(0.0)} \\
& VAR                                         & 3.44(0.23) & 7.1(0.05) & 4.56(0.04) & 13.73(0.25) & 18.0(0.4) & 18.67(0.27) & 9.56(0.19) & 11.5(0.2) & 12.84(0.24) \\
& GRU  & 8.58(0.28) & 16.9(0.6) & 10.08(0.29) & 13.49(0.17) & 18.8(0.3) & 16.97(0.17) & 15.35(0.39) & 18.2(0.7) & 18.76(0.29) \\
& Transformer                                 & 8.83(0.37) & 17.3(0.6) & 10.34(0.43) & 13.34(0.73) & 18.6(0.2) & 16.91(0.05) & 15.72(0.17) & 18.5(0.2) & 19.30(0.12) \\
\hline
\multirow{8}{*}{STGNN} 
& GRUGCN                                      & 9.38(0.57) & 18.2(1.2) & 10.93(0.58) & 12.71(0.31) & 17.7(0.4) & 16.17(0.4) & 14.31(0.37) & 16.7(0.3) & 17.71(0.44) \\
& GCGRU                                       & 8.88(0.54) & 17.1(1.1) & 10.34(0.51) & 13.17(0.54) & 18.2(0.90) & 16.87(0.53) & 14.34(0.17) & 16.9(0.3) & 17.74(0.20) \\
& T-GCN                                       & 9.27(0.26) & 18.1(0.5) & 10.68(0.28) & 13.93(0.33) & 19.4(0.4) & 17.58(0.41) & 14.52(0.44) & 17.4(0.4) & 17.81(0.55) \\
& AGCRN                                     & 7.91(0.15) & 15.2(0.3) & 9.43(0.19) & 13.26(0.35) & 17.9(0.5) & 17.34(0.34) & 13.16(0.28) & 15.3(0.2) & 16.66(0.36) \\
& GraphWavenet   & 8.85(0.44) & 17.4(1.0) & 10.34(0.44) & 13.91(0.47) & 19.2(0.9) & 17.61(0.43) & 15.02(0.44) & 18.1(0.5) & 18.37(0.57) \\   
& FC-GNN                             & 8.22(0.32) & 15.9(0.7) & 9.80(0.27) & 12.79(0.56) & 16.9(0.7) & 16.79(0.72) & 13.61(0.2) & 16.0(0.4) & 16.60(0.14) \\
& BP-GNN                                   & 6.61(0.35) & 13.1(0.8) & 8.07(0.34) & 12.77(0.39) & 17.1(0.6) & 16.56(0.42) & 14.81(0.29) & 16.9(0.5) & 18.40(0.24) \\
\hline
\hline
\end{tabular}
}
\endgroup
\label{tab:result_agg_228}
\end{table*}



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{assets/3D_histogram_plot_.pdf}
    \caption{Distribution of differences between forecasts and ground truth.}
    \label{fig:multiple-forecast}
\end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{assets/MAC000026.pdf}
%     \caption{Distribution of errors between forecasts and ground truth of 228 households.}
%     \label{fig:multiple-forecast}
% \end{figure}

We observe that the baseline model, although predicting less accurately (errors are more scattered), has its tails more evenly distributed. The error of deep learning-based models (including \glspl{stgnn}) is often more skewed on the left. An explanation is that these models tend to prioritize learning from "easier" periods with consistent patterns while failing to adequately capture atypical events, such as consumption spikes~\cite{zhang_unlocking_2023} (see Fig. \ref{fig:multiple-forecast}). This can lead to underestimation during abnormal periods. At the aggregate level, instead of centering predictions around the actual average consumption, the forecasts may consistently underestimate the consumption spikes, and hence the errors will not be balanced out. In \glspl{stgnn} since it incorporates spatial learning, it can potentially propagate errors throughout the spatial dimension and therefore does not address this issue.




% \begin{table}[h!]
% \centering
% \caption{Cross-validation settings for validating \acrshort{stgnn} models.}
% \begin{tabularx}{\columnwidth}{|X|X|X|X|}
% \hline
% \textbf{Phase} & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\ \hline
% \textbf{Fold 1} & Jan 1, 2013 -- Jun 30, 2013 & Jul 1, 2013 -- Jul 31, 2013 & Aug 1, 2013 -- Aug 31, 2013 \\ \hline
% \textbf{Fold 2} & Jan 1, 2013 -- Aug 31, 2013 & Sep 1, 2013 -- Sep 30, 2013 & Oct 1, 2013 -- Oct 31, 2013 \\ \hline
% \textbf{Fold 3} & Jan 1, 2013 -- Oct 31, 2013 & Nov 1, 2013 -- Nov 30, 2013 & Dec 1, 2013 -- Dec 31, 2013 \\ \hline
% \end{tabularx}
% \label{tab:cross_validation}
% \end{table}


% \begin{table}[h!]
% \centering
% \caption{Cross-validation settings for validating \acrshort{stgnn} models.}
% \begin{tabular}{|c|c|c|}
% \hline
% \textbf{Phase}         & \textbf{Time Period} \\ \hline
% \textbf{Fold 1} & \begin{tabular}[c]{@{}l@{}}Training: Jan 1, 2013 -- Jun 30, 2013 \\ 
% Validation: Jul 1, 2013 -- Jul 31, 2013 \\ 
% Testing: Aug 1, 2013 -- Aug 31, 2013\end{tabular} \\ \hline
% \textbf{Fold 2} & \begin{tabular}[c]{@{}l@{}}Training: Jan 1, 2013 -- Aug 31, 2013 \\ 
% Validation: Sep 1, 2013 -- Sep 30, 2013 \\ 
% Testing: Oct 1, 2013 -- Oct 31, 2013\end{tabular} \\ \hline
% \textbf{Fold 3} & \begin{tabular}[c]{@{}l@{}}Training: Jan 1, 2013 -- Oct 31, 2013 \\ 
% Validation: Nov 1, 2013 -- Nov 30, 2013 \\ 
% Testing: Dec 1, 2013 -- Dec 31, 2013\end{tabular} \\ \hline
% \end{tabular}
% \label{tab:cross_validation}
% \end{table}


