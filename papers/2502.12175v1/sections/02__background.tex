\section{Overview of STGNN for STLF}\label{sec:background}

%%% Define STGNN

\glspl{stgnn} are models designed to handle time series data collected from various locations~\cite{cini_graph_2023}. In the context of \acrshort{stlf}, we consider a dataset collected from $N$ consumers (e.g. households). In the simplest case, we assume 
for each consumer, data only contain historical energy consumption from smart meters. 
Let $x^i_t \in \mathbb{R}$ be the energy consumption of consumer $i$ at time step $t$; each time series $\{x^i_t\}_{t:t+T}$ is the energy consumption of consumer $i$ in period $t \rightarrow t+T$. 
Consequently, by stacking all consumers, the matrix $\mathbf{X}_{t:t+T} \in \mathbb{R}^{N \times T}$ represents the consumption records of N consumers in the period $t \rightarrow t+T$. Given the consumption data $\mathbf{X}_{t-W:t}$ from $W$ previous steps , \acrshort{stgnn} models forecast consumption $\hat{\mathbf{X}}_{t:t+H}$ in $H$ next steps for all consumers. 

In doing so, \acrshort{stgnn} represents consumers and their relationships by a graph structure. A graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ consists of a set of nodes $\mathcal{V} = \{v_1,v_2,...,v_N\}$ and a set of edges $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$, where $(v_i,v_j) \in \mathcal{E}$ if node $v_i$ connects to node $v_j$. This connectivity is compactly represented by an adjacency matrix $\mathcal{A}$, where an entry $a_{ij} > 0$ signifies the edge weight between nodes $v_i$ and $v_j$. In the context of residential \acrshort{stlf}, each household is assigned to one node and its features contain time series  of its historical energy consumption. Respectively, the edges can be derived based on the patterns between residential load profiles~\cite{wang_short-term_2023}. 
% The connectivity of the graph decides how the information propagates between nodes. By designing a suitable adjacency matrix, we can enable the learning process to capture meaningful patterns effectively. 

In what follows, we first describe how graph structures are usually constructed, then detail the components of \acrshort{stgnn} and how those components interact with each other. Finally, we present some representative models in the literature.

\subsection{Graph Formation}\label{subsec:graph}
To capture the spatial dependency between different nodes, it is necessary to provide a spatial structure in the form of a graph. The topology of the graph dictates how the features are aggregated between the nodes. Based on how the graph is constructed, the graph formation methods in the literature can be classified as follows~\cite{bloemheuvel_graph_2024}:
% \begin{equation}\label{eq:sim}
%     a_{ij} = 
%     \begin{cases}
%         \text{SIM}(v_1,v_2) & \text{ if }\text{SIM}(v_1,v_2) > \epsilon \\
%         0 & \text{otherwise}
%     \end{cases}
% \end{equation}
\begin{itemize}
    \item \textit{Predefined graph}: The topology of the graph is fixed during training. The edge of the graph may be established using supplementary information and by assessing the similarity between time series. The similarity measure can be based on Pearson coefficient~\cite{fernandez_privacy-preserving_2022}, Euclidean distance, DTW distance~\cite{10202782}, or correntropy~\cite{cini_graph-based_2023}. An edge is considered to exist when the similarity surpasses a defined threshold value~\cite{bloemheuvel_graph_2024}.
    \item \textit{Learnable graph}: Some models integrate graph formation into the learning process. This technique does not require a graph from the dataset before training but self-organizes the graph during training so that it can facilitate the flow of information for graph neural network~\cite{wu_graph_nodate}. We identify some learning algorithms that incorporate graph structure learning for downstream tasks (i.e., forecasting) later in section \ref{subsec:exampleSTGNN}.
\end{itemize}
% \subsubsection{}
% \subsubsection{Learnable graph}

\subsection{Temporal and Spatial Processing Unit as components of STGNN}\label{subsec:processing}

To model spatiotemporal data as in load forecasting, one must process information in temporal and spatial dimensions. 
The most popular deep learning models to process temporal information are through \acrfull{rnn}~\cite{sutskever_sequence_2014}, \acrfull{cnn} models, or \acrfull{mlp}~\cite{rodrigues_short-term_2023}. 
% We denote TEMP as the temporal processing unit. 

Respectively, the most dominant method to propagate information along the graph is through the \acrfull{ms} paradigm~\cite{cini_graph_2023} which involves 2 steps:
\begin{enumerate}
    \item \textit{Message Aggregation}: Each node collects information (or "messages") from its neighboring nodes. This step captures the local structure and features of the graph by pooling or combining the attributes of connected nodes. 
    \item \textit{Feature Update}: After aggregation, each node updates its own representation by combining the aggregated information with its existing attributes. This step refines the node's state, embedding its local and neighboring information into a new feature representation.
\end{enumerate}

There are several models fit into this paradigm, but one of the most popular is \acrfull{gcn}~\cite{mansoor_spatio-temporal_2023}. It is present in numerous examples of \acrshort{stgnn} within our study.

\subsection{STGNN architecture}\label{subsec:architecture}
We refer to the structure of \glspl{stgnn} in the literature. The most prominent architectures of STGNN are \acrfull{tts}, \acrfull{tas}~\cite{gao_equivalence_2022}.

\subsubsection{Time-then-Space (TTS) architecture}\label{subsubsec:tts}
In \acrshort{tts} architecture, the models encode information in the temporal dimension first, as depicted by the solid arrow in Fig. \ref{fig:tts}. The abstract representation of each node is now broadcast (dashed arrow in Fig. \ref{fig:tts}) to their neighbors through the spatial unit to incorporate useful information in the spatial dimension.
% \begin{align}\label{equ:tts}
%     h^i_{T} &= \text{TEMP}(x^i_{[T-W:T]}) \\
%     z^i_{T} &= \text{MS}(h^i_T, \mathcal{A})    
% \end{align}

\subsubsection{Time-and-Space (T\&S) architecture}\label{subsubsec:tas}
Alternatively, the \acrshort{tas} architecture integrates the processing of temporal and spatial features more cohesively. At each time step, the features of individual nodes are propagated and abstracted through a spatial processing unit, updating the node representations in the spatial dimension (dashed arrow in Fig. \ref{fig:tas}). Then, the node representation is processed using a recurrent unit, producing a hidden state at that time step.  
% \begin{align}
%     z^i_{t} &= \text{MS}(x^i_t, \mathcal{A}) \\
%     H_{t} &= \text{RNN}(H_{t-1},Z_t) 
% \end{align}
% Notably, the temporal processing unit TEMP here is an instance of \acrshort{rnn} models. 
Subsequently, this internal hidden state is combined with the upcoming observation to produce the hidden state in the next step (as illustrated by the curved arrow in Fig. \ref{fig:tas}). It is worth noting that a recurrent model is employed as the temporal processing unit in this context~\cite{gao_equivalence_2022}.
% \subsubsection{\acrfull{stt} architecture}

% In addition, although there is also \acrfull{stt} architecture mentioned in ~\cite{cini_graph_2023}, it is not popular and is not the focus of our study. 

% We visualize the operations of two architectures in Fig. \ref{fig:architectures}. 
% The solid arrows indicate processing units in the temporal dimension, and the dashed arrows denote processing units in the spatial dimension. 

\begin{figure}[ht]
    \centering
    % First minipage
    \begin{minipage}[t]{0.46\textwidth} % Adjust width to fit side-by-side
        \centering
        \includegraphics[width=\linewidth]{assets/tts.pdf} % Scale image to fill minipage
        \caption{\acrshort{tts} architecture.}
        \label{fig:tts}
    \end{minipage}
    \hfill % Add flexible spacing between minipages
    % Second minipage
    \begin{minipage}[t]{0.48\textwidth} % Adjust width to fit side-by-side
        \centering
        \includegraphics[width=\linewidth]{assets/tas.pdf} % Scale image to fill minipage
        \caption{\acrshort{tas} architecture.}
        \label{fig:tas}
    \end{minipage}
    % Main caption
    % \caption{Simple visualization of \acrshort{stgnn} architectures.}
    \label{fig:architectures}
\end{figure}


% In the next section, we will review some of the \acrshort{stgnn} models that have been tested in the literature and classify them into the framework in section \ref{subsec:architecture}. 

\subsection{Examples of STGNN models}\label{subsec:exampleSTGNN}
Given the different architectures of \acrshort{stgnn} models, we review  existing models that are representatives of the 
 described architectures. We present and compare the similarity between models based on the components and architectures in Section \ref{subsec:processing} and \ref{subsec:architecture}.

\paragraph{\acrshort{grugcn}~\cite{gao_equivalence_2022}} This model is of type \acrshort{tts}. It uses the \acrfull{gru} (a variant of \acrshort{rnn}) as a temporal processing unit to represent temporal characteristics of each node and then applies \acrshort{gcn} on top of the encoded features to account for spatial dependencies.

\paragraph{\acrshort{gcgru}~\cite{arastehfar_short-term_2022}} This model uses \acrshort{tas} architecture. In particular, it uses \acrshort{gcn} as the spatial processing unit to capture the spatial dependency at time step $t$. This unit acts as a cell in the \acrshort{gru} model, which recursively calculates the hidden state of the entire graph. Note that for a more consistent comparison, we replace the \acrfull{lstm} model in \cite{arastehfar_short-term_2022} by \acrshort{gru} as in other models.

\paragraph{\acrshort{t-gcn}~\cite{huang_gated_2023}} This model updates node features by a 2-layer GCN before processing them by \acrshort{gru} model. It is similar to \acrshort{gcgru} but updates only node features through the \acrshort{gcn}, not the hidden state.

\paragraph{\acrshort{agcrn}~\cite{bai_adaptive_2020}} This model is similar to \acrshort{gcgru} in terms of encoding both spatial and temporal dimensions. However, to model the relationships between nodes more flexibly, this approach introduces a learnable embedding for each node. Consequently, the feature in each node is not only determined by the historical data but also by the embedding. This allows the model to determine the optimal embeddings that maximize the forecast performance.

\paragraph{\acrshort{gw}~\cite{lin_spatial-temporal_2021}} Similar to \acrshort{agcrn}, this model assigns each node a learnable embedding. In terms of forecasting, it employs a temporal convolutional layer to encode historical data of each consumer, followed by a graph convolutional layer to incorporate features in the spatial dimension. By stacking multiple layers of this unit with different parameters, the model can capture various patterns at different temporal and spatial scales. As described, it is of type \acrshort{tts}.  


\paragraph{\acrfull{fc-gnn}~\cite{satorras_multivariate_2022}} This model presumes that every node is interconnected, resulting in a complete graph. However, in the \textit{Message Aggregation} step, the weight of each "message" from neighbors will be adjusted due to the attention mechanism, allowing an adaptable edge weight between nodes even though the topology of the graph does not reflect the relation between time series. In terms of forecasting, it follows the \acrshort{tts} architecture with \acrshort{mlp} as a temporal processing unit and the attention mechanism in the spatial processing unit.

\paragraph{\acrfull{bp-gnn}~\cite{satorras_multivariate_2022}} This model is a variant of the FC-GNN model. However, instead of full connectivity, it defines virtual nodes that connect to all original nodes, forming a bipartite graph. These nodes act as hubs, aggregating, updating, and relaying information between original nodes.

% However, instead of a fully connected topology, it defines virtual nodes that connect to all other nodes in the graph, generating a bi-partite graph. These virtual nodes act as hubs that gather information, update them, and then pass it back to the original nodes. 

The selected models are arranged in Table \ref{tab:overview}, following the structure outlined in Sections \ref{subsec:graph} and \ref{subsec:processing}.

\begin{table}[h]

\caption{Summary of the models in the described framework}
\label{tab:overview}
\begingroup
\setlength{\tabcolsep}{5pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.2} % Default value: 1
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l|ll|lll}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Models}} & \multicolumn{2}{c|}{Graph formation}     & \multicolumn{2}{c}{Architecture} \\ \cline{2-5} 
\multicolumn{1}{c|}{}                        & Predefined graph & Learnable graph & TTS            & T\&S               \\ 
\hline
GRUGCN                                      & \multicolumn{1}{c}{\checkmark}  &                 & \multicolumn{1}{c}{\checkmark}       \\ 
\hline
GCGRU                                      & \multicolumn{1}{c}{\checkmark}  &                 &                & \multicolumn{1}{c}{\checkmark}          \\ 
\hline
T-GCN                                       & \multicolumn{1}{c}{\checkmark}  &                 &  &  \multicolumn{1}{c}{\checkmark}             \\ 
\hline
AGCRN                                      &             & \multicolumn{1}{c|}{\checkmark}      &                &  \multicolumn{1}{c}{\checkmark}            \\ 
\hline
GraphWavenet                                &     &           \multicolumn{1}{c|}{\checkmark}     & \multicolumn{1}{c}{\checkmark}               &            \\ 
\hline
% STEGNN                                      &             & \multicolumn{1}{c|}{\checkmark}      &                &             &  \multicolumn{1}{c}{\checkmark} \\ \hline
FC-GNN                             &   \multicolumn{1}{c}{\checkmark}          &       & \multicolumn{1}{c}{\checkmark}    &            \\ 
\hline
Bipartite                                   &    \multicolumn{1}{c}{\checkmark}         &       & \multicolumn{1}{c}{\checkmark}    &              \\ 
\hline
\end{tabular}
}
\endgroup
\end{table}


