\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{orcidlink}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Insights from Network Science can advance Deep Graph Learning}

\begin{document}

\twocolumn[
\icmltitle{Insights from Network Science can advance Deep Graph Learning}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Christopher Bl{\"o}cker~\orcidlink{0000-0001-7881-2496}}{jmu}
\icmlauthor{Martin Rosvall~\orcidlink{0000-0002-7181-9940}}{umu}
\icmlauthor{Ingo Scholtes~\orcidlink{0000-0003-2253-0216}}{jmu}
\icmlauthor{Jevin D.\ West~\orcidlink{0000-0002-4118-0322}}{uwa}
\end{icmlauthorlist}

\icmlaffiliation{jmu}{Chair of Machine Learning for Complex Networks, Center for Artificial Intelligence and Data Science (CAIDAS), Julius-Maximilians-Universi{\"a}t W{\"u}rzburg, Germany}
\icmlaffiliation{umu}{Integrated Science Lab (IceLab), Department of Physics, Ume{\aa} Universitet, Ume{\aa}, Sweden}
\icmlaffiliation{uwa}{Center for an Informed Public, Information School, University of Washington, Seattle, USA}

\icmlcorrespondingauthor{Christopher Bl{\"o}cker}{christopher.bloecker@uni-wuerzburg.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Deep graph learning and network science both analyze graphs but approach similar problems from different perspectives.
Whereas network science focuses on models and measures that reveal the organizational principles of complex systems with explicit assumptions, deep graph learning focuses on flexible and generalizable models that learn patterns in graph data in an automated fashion.
Despite these differences, both fields share the same goal: to better model and understand patterns in graph-structured data.
Early efforts to integrate methods, models, and measures from network science and deep graph learning indicate significant untapped potential.
In this position, we explore opportunities at their intersection.
We discuss open challenges in deep graph learning, including data augmentation, improved evaluation practices, higher-order models, and pooling methods.
Likewise, we highlight challenges in network science, including scaling to massive graphs, integrating continuous gradient-based optimization, and developing standardized benchmarks.
\end{abstract}


\section{Introduction}\label{sec:intro}

In 1982, John Hopfield introduced a neural network model that sparked a flurry of innovations: content-addressable memory, energy dynamics, error correction, and nonlinear architecture \cite{hopfield1982neural}.
The Nobel Prize committee recently recognized the role these innovations played in the development of modern machine learning\footnote{\url{https://www.nobelprize.org/prizes/physics/2024/hopfield/facts/}}.
Less recognized but not any less important is the influence the paper had on the field of network science.
The Hopfield network demonstrated the critical connection between network topology and the collective behaviour of complex systems---one of the enduring themes of network science and now one of the central challenges in deep graph learning. 

Surprisingly, the two fields have diverged more than they have converged since Hopfield's influential paper.
We see an opportunity for that to change, and argue for better integration of the two research communities.
At their core, both fields model and analyze patterns in graphs.
However, their needs are different.
In deep graph learning, there is a need for methods that augment data to cope with limited training data, pool node representations to facilitate graph-level learning, and develop message passing schemes that incorporate higher-order interactions beyond pair-wise edges.
Network science has been thinking about these issues for years, albeit often from a different perspective or with different motivation. 
Conversely, in network science, there is a need to scale models, measures and algorithms to massive graphs, better incorporate continuous, gradient-based methods for optimization problems, and develop standardized benchmarks to fairly compare, such as community detection algorithms.
Deep learning has mastered these techniques.  

\textbf{We see opportunities in bridging this gap between network science and deep graph learning.}
In this paper, we detail these opportunities with the hope of spurring conversations across the two communities.

There is significant potential to address open challenges in deep graph learning, such as data augmentation, evaluation practices, integrating higher-order models, and modeling patterns in temporal graphs.
Network science offers valuable insights by \emph{connecting network structure with function through principled methodologies}, including probabilistic generative models that provide principled null models for complex networks, statistical inference and network reconstruction methods for noisy relation data, and community detection techniques.
These approaches can \emph{enhance the theoretical foundation and empirical insight of deep graph learning models}.

At the same time, scaling models, measures, and algorithms to massive graphs and incorporating continuous optimization remain open challenges in network science.
Deep learning techniques, such as gradient-based optimization and large-scale training, offer powerful tools to advance network science.

Succeeding with these synergies requires fostering a more structured collaboration between the two fields.
By bridging their complementary strengths, we aim to initiate a broader discussion on developing more interpretable, scalable, and generalizable approaches for modeling graph-structured data.


\begin{figure*}
\centering
\includegraphics[width=.89\textwidth]{figures/schematic_ns+dgl.pdf}
\caption{\textbf{Illustrative example of how network science insights can advance deep graph learning}. Deep graph learning provides tools for end-to-end representations learning for prediction tasks (top panel), while network science provides advanced statistical modeling techniques for handling noisy graph data (bottom panel).
Applying deep graph learning directly to noisy co-occurrence data (left) results in a latent space representation that poorly reflects ground truth node labels (top panel). Network science modeling techniques---such as statistical ensembles of random graphs that preserve aggregate characteristics of the data---help build robust graph models that account for noise (bottom panel). Combining these techniques with deep learning methods leads to a latent space representation that better captures ground truth patterns (bottom right). Figure partly adapted from \citet{casiraghi2017}.\label{fig:example}}
\end{figure*}



\section{Principled Deep Graph Learning Modeling}

Deep graph learning relies on the task-specific training of deep neural networks that allow to model patterns in graph-structured data in an end-to-end fashion.
The past decade has seen rapid advances in the development of deep graph learning architectures for various tasks and applications.
However, challenges to apply state-of-the-art graph neural networks to real-world problems also expose limitations that we need to address \cite{Georgousis2021,ju2024surveygraphneuralnetworks}.
These include the need for a more systematic framework to characterize which structural properties of a (temporal) graph data set contribute to the performance of a specific model, the integration of evaluation methods that are rooted in principled null models, theoretically grounded pooling methods that minimize information loss, and data augmentation techniques that model noisy or incomplete data to advance model generalizability.
Insights from network science can help us to address these challenges, guiding deep graph learning toward principled architectures, improved interpretability, and more rigorous evaluation methods.


\subsection{Probabilistic Generative Models}

Random graph models are an important foundation of network science. 
Starting from the simple Erd\H{o}s-R{\'e}nyi model for random graphs, where edges between pairs of nodes are randomly generated with equal probability \cite{erdos1960evolution}, network science has developed probabilistic generative models that define a ``statistical ensemble'' containing all graphs that share given aggregate characteristics such as size, density, degree sequence or distribution, modular structure or motif statistics.
Important examples include the Molloy-Reed model that generates random graphs with a given degree sequence or distribution \cite{molloy1995critical}, exponential random graph models for random graphs with a given set of network statistics \cite{robins2007introduction}, or the stochastic block model for random graphs with given homophilic or heterophilic community patterns \cite{lee2019review}.
In network science, such statistical ensembles are the foundation to analytically study expected properties of graphs with given aggregate characteristics, for example using generating functions as a framework \cite{newman2001random,newman2009random}
Moreover, these models can be used to randomize the topology of empirical networks, while maintaining aggregate properties.
This randomization serves as a null model for statistical hypothesis testing, enabling us to understand which of a network's characteristics are actually due to the \emph{topology} of the network -- that is, which node is linked to which other nodes -- and which characteristics can be explained based on the mere \emph{degree distribution} or \emph{density} of edges.

In network science, this approach has been used to understand the role of network topology in the diffusion of information or disease spreading.
Unfortunately, principled null models for graph-structured data are not yet widely used in the evaluation of deep graph learning architectures, where the focus often lies on the performance of a specific model in a given task rather than on which topological features of a graph can explain the predictive power of specific architectures.
Leveraging network science models could thus lead to more rigorous and meaningful evaluation practices that provide insights into the predictive capabilities of deep graph learning models.


\subsection{Data Augmentation for Graph Neural Networks}

In machine learning, we often use \emph{data augmentation techniques} in the training phase to improve the generalizability of machine learning models and to mitigate overfitting.
This typically involves enriching available training data through applying perturbations, injecting noise, or other means to augment available training data by artificial examples.
Inspired by these methods, the deep graph learning community has considered various \emph{graph augmentation} techniques that seek to manipulate edges or nodes of a graph in such a way that it improves the performance and/or generalizability of graph neural networks \cite{zhao2023graphdataaugmentationgraph}.
Recent works have considered, for example, the targeted removal of edges to increase homophilic patterns \cite{Zhao2020} or selectively adding nodes that slow down message passing \cite{azabou2023half}.
While these works on graph augmentation have made progress towards improving the generalizability of Graph Neural Networks (GNNs), researchers from the deep graph learning community have recently argued that we still lack a ``theory of data augmentation'' for graphs \cite{morris24a}, highlighting that, despite theoretical results on their expressivity, GNNs have not yet matured to leverage the full potential of the data in practical settings but currently rely on preprocessing for data augmentation.

Network science provides tools that can help us develop such a theory.
In particular, network scientists have long argued that it is often not desirable to directly use an observed network ``as is'' for network analysis or neural message passing in GNNs.
Many empirical data on complex networks are unreliable insofar as they suffer from spurious or missing relationships, are incomplete in terms of observed nodes, or contain incorrect node labels or noisy attributes.
For such data, network science has developed reconstruction and inference techniques \cite{peixoto2019network,newman2018network} that use \emph{statistical graph ensembles} to infer reasonable graph models from noisy or incomplete data.
In deep learning, such methods could be used to generate a set of plausible graph models that can be used for training graph neural networks, thereby improving robustness and generalisability.
An example for this is shown in \cref{fig:example}.
Similarly, recent works demonstrate that insights from the modeling of dynamical processes on graphs as well as spectral graph theory provide new ways to augment graphs in a theoretically principled way, for example by tuning the spectral gap to simultaneously mitigate over-squashing and over-smoothing \cite{jamadandi2024,rubiomadrigal2025comfy}.


\subsection{Community Detection and Pooling} 

Community detection lies at the heart of unsupervised graph learning, offering a powerful lens to uncover meaningful structures in complex systems.
Identifying groups of nodes -- communities -- that are more densely connected internally than externally enables exploratory data analysis and unsupervised classification in diverse domains, from understanding social interactions to unraveling biological networks and optimizing information retrieval.
Community detection has not only benefited from the capabilities of GNNs but has also inspired new applications, such as in graph pooling.
Graph pooling techniques simplify graphs by merging nodes into clusters, creating more compact representations that enhance standard applications, including classification and prediction.
By incorporating community detection methods as a basis for merging, these techniques can replace or complement purely data-driven pooling operations, providing more interpretable reductions \cite{deng2024module}.

Neural embedding methods, such as node2vec, GraphSAGE, and autoencoder-based embeddings, have demonstrated impressive capabilities in capturing community structure \cite{grover2016node2vec,hamilton2017inductive,kipf2016variational}.
These approaches often combine multiple components to construct and divide embeddings into accurate communities.
For example, node2vec's random-walk approach can approximate certain spectral properties of the normalized Laplacian under controlled conditions, enabling it to approach the theoretical limits of community detection for certain graphs \cite{kojaku2024network}.
However, such methods present challenges: they rely on intricate combinations of embedding generation and clustering steps, making them opaque and difficult to optimize effectively.

In contrast, network science provides a well-established and interpretable framework for community detection.
Over the years, methods such as the stochastic block model (SBM) \cite{peixoto2019bayesian} and the map equation \cite{rosvall2008maps} have been developed and refined, each grounded in clear mathematical objectives.
While the SBM identifies latent groups by modeling the plain topology of a network, the map equation highlights modular regularities in network flows, providing a complementary perspective by focusing on how information, resources, or behaviors propagate through the system.
Like node2vec, this flow-based approach is especially valuable in applications where understanding dynamic processes is critical, such as transportation, communication, and biological networks.
In contrast to opaque embedding pipelines, both Bayesian inference of the SBM and the map equation directly quantify how well the underlying process generating a network and flows on the network are compressed in different partitions, making the resulting community assignments highly interpretable.

The rigour of network science extends beyond methodology to evaluation.
Generative models create benchmark networks with known ground truth and allow theoretical frameworks to systematically compare community detection techniques, offering a solid foundation against which machine learning approaches can be measured \cite{lancichinetti2008benchmark,bloecker2024flowdivergence}.
Incorporating these insights into machine learning evaluation workflows would strengthen the theoretical and practical reliability of community detection and pooling in GNNs.

The convergence of network science and machine learning offers significant opportunities to enhance community detection.
By leveraging the scalability and pattern-recognition capabilities of deep learning alongside the theoretical depth and interpretability of methods such as the SBM and the map equation, we can create tools that address both the static and dynamic complexities of real-world networks.
Such synergies will advance graph representation learning and deepen our understanding of complex systems.


\subsection{Modeling Patterns in Temporal Graphs}

Due to the growing availability of time series data, the application of graph neural networks to \emph{temporal graph data}, where the edges and/or nodes are changing over time has recently seen a surge of interest.
Many recently proposed architectures are snapshot- or event-based adaptations of existing deep learning methods that model (sequential) patterns in time-evolving batches of edges, which capture the time-varying topology of a temporal graph \cite{longa2023graphneuralnetworkstemporal}.
Examples of this approach include the event-based Temporal Graph Network model \cite{rossi2020} or the snapshot-based EvolveGCN architecture \cite{pareja2020evolvegcn}.
While the performance of these models has been evaluated for several tasks in empirical temporal graphs provided by, for example the Temporal Graph Learning Benchmark (TGB) \cite{huang2023}, it is often unclear which patterns in temporal graphs they are actually able to learn.
In network science, the analysis and modeling of patterns in temporal networks have been a topic of major interest for almost two decades \cite{holme2015modern}.
Network scientists have developed measures and models that capture different temporal, topological, and temporal-topological patterns found in real-world temporal graphs, as well as generative models that selectively reproduce specific patterns.
Examples include measures and models that capture bursty activation patterns of nodes or edges \cite{moinet2015burstiness,takaguchi2013bursty}, models that capture the temporal evolution of community patterns \cite{peixoto2017modelling}, or approaches that capture sequential patterns in the causal ordering of time-stamped edges that influence \emph{time-respecting paths} \cite{scholtes2014causality,rosvall2014memory}.
Selectively applying such models to empirical temporal graph data allows us to ``disentangle'' the different temporal, topological, and temporal-topological patterns present in the data.
Much work remains to be done in applying such models to recently proposed architectures for temporal graph neural networks in order to better understand which of those patterns are actually captured by a specific deep temporal graph neural network architecture.


\subsection{Higher-Order Models}

An important insight obtained by the network science community over the past few years is that it is not enough to model dyadic interactions in complex systems \citep{lambiotte2019networks}.
Dyadic links can capture essential aspects of many real systems but they cannot directly represent higher-order interactions such as multi-body interactions or stateful aspects of the data.
For example, sequence data, such as click streams, require modeling dependencies of paths consisting of several links.
Social and bio-molecular systems often involve groups of three or more entities in interactions.
Public transport systems provide more than one mode of transportation; social media users are active on several social media platforms and communicate with the same friends via different platforms.
Consequently, network science has developed different higher-order modeling frameworks that help us gain insights into the structure and dynamics of such complex systems~\cite{battiston2020beyondpairwiseinteractions,torres2021modelling}.
Crucially, the choice of higher-order model determines what aspects of a complex system can be faithfully represented and discovered in downstream analyses.

Non-Markovian dynamics, such as click streams or the spread of information in social networks, can be modelled with so-called sparse memory networks~\cite{rosvall2014memory} or De Bruijn graphs~\cite{de1946combinatorial}.
Essentially, they perform a state-space expansion, introducing higher-order nodes where a node of $n$-th order represents first-order paths of length $n-1$, thus modeling interactions with memory.
Interactions involving an arbitrary number of entities, that is, zero or more entities, can be modelled with hypergraphs where edges encompass a set of nodes~\cite{battiston2020beyondpairwiseinteractions}.
Simplicial complexes, similar to hypergraphs, capture scenarios where edges contain sets of nodes but in addition, an edge with $n$ nodes implies the existence of every edge involving any subset of those $n$ nodes~\cite{torres2021modelling}.
Complex systems involving different modes of interaction can be represented with multilayer networks where each layer represents a specific mode and the same node can exist across different layers~\cite{dedomenico2013multilayer,kivela2014multilayer}.
However, while they may be more convenient to work with, multilayer networks can be converted to memory networks and, thus, have the same modeling capabilities as memory networks.

Recent integrations of higher-order network models with graph neural networks highlight the potential and benefits of encoding domain-specific knowledge via these modeling techniques.
For example, several works have integrated hypergraphs with neural networks for applications including recommender systems, bioinformatics, time-series analysis, and computer vision~\cite{survey-hypergraphg-neural-networks,survey-hypergraph-representation-learning}.
An adaptation of De Bruijn graphs for De Bruijn graph neural networks (DBGNN) captures causal relationships in temporal graphs~\cite{qarkaxhija2022,heegusing}.
Similarly, there is a growing interest in graph neural network architectures that leverage higher-order modeling frameworks like simplicial complexes that have been intensely studied in network science \cite{frantzen2024learningsimplicialdatabased}.
We expect to see an increasing number of future works leveraging the modeling capabilities developed by network science for learning better representations.



\section{Rethinking Deep Graph Learning Architectures}

Graph neural networks are at the heart of deep graph learning, relying heavily on message passing and training via backpropagation with gradient descent.
Despite their success, recent advances have been incremental rather than groundbreaking, and the flexibility of GNNs makes many aspects of their inner workings hard to interpret.
Integrating insights from network science offers a path to rethink deep graph learning architectures, for example by integrating new objective functions, taking a dynamical systems view on message passing, explaining when and why message passing works for certain tasks, or helping to make graph neural networks more interpretable and generalizable.


\subsection{From Discrete to Continuous}

With roots in graph theory, network science models and algorithms typically build on discrete mathematics and involve discrete objective functions, discrete data models, and combinatorial optimization algorithms.
In contrast, due to the use of backpropagation, deep learning is largely based on continuous and differentiable loss functions with data models building on matrix representations, which allows effectively utilizing gradient-based optimisation techniques on GPUs.
Consequently, integrating existing discrete approaches from network science with the gradient-based optimization framework requires making their objective functions continuous and differentiable.

For example, community detection traditionally considers hard partitions where nodes belong to exactly one community.
Detecting communities often utilizes discrete and stochastic search algorithms that move one node per iteration to optimize their objective function~\cite{Blondel_2008,edler2017infomap,Traag2019}.
Several recent deep graph clustering works have adapted community detection approaches and made them differentiable by considering nodes' community memberships as continuous rather than discrete:
\citet{tsitsulin2023dmon} adapted the modularity criterion, \citet{shchur2019overlapping} used the Poisson random process model, \citet{pmlr-v119-bianchi20a} built on minimum cuts, and \citet{blocker2023map} adapted the map equation.
With soft community assignments, these approaches naturally produce overlapping communities which often better capture the characteristics of real systems where nodes can belong to several groups, such as in social networks.

Continuous adaptations of network science objectives present an immense opportunity for deep graph learning to integrate established methods into their architectures.
This way, principled unsupervised loss formulations can become part of a composite loss to guide learning in supervised tasks as done, for example, in recent graph pooling works~\cite{vonpichowski2024,castellana2025bnpoolbayesiannonparametricapproach}.
However, this comes with challenges as the continuous versions of such losses often require explicit regularization to avoid trivial solutions~\cite{pmlr-v119-bianchi20a,tsitsulin2023dmon,shchur2019overlapping}.


\subsection{Message Passing vs.\ Network Analysis}

Message passing is a core mechanism in GNNs.
It allows nodes to gather information from their neighbors and update their embedding, supporting tasks such as link prediction and node classification.
While the specifics regarding message, update, and aggregation functions differ between different GNN architectures, they follow the same principle, often choosing neural networks, such as MLPs, for message and update, and aggregations such as mean, sum, or max.
Recently, several works have proposed using other activations to improve robustness in situations with outliers or to learn aggregations that better align with specific tasks \cite{hamilton2017inductive,NEURIPS2020_99cad265,NEURIPS2020_99e314b1,ijcai2022p293}.
A deeper understanding of how different aggregation and activation choices interact with graph structure remains an open challenge.

Insights from network science could guide the development of more adaptive, interpretable, and task-specific message-passing mechanisms, helping the deep graph learning community design models that better capture structural patterns and improve generalization across diverse graph-based tasks.
Most message-passing architectures rely on training to optimize their message and update functions for specific objectives.
But even without training, message passing captures structural patterns in graphs, including common neighbors, triadic closure, and PageRank.
These measures have long helped network scientists understand graph topology and can inform GNN methods because they reflect key structural features of graphs.
For example, common neighbors quantify the number of shared neighbors between two nodes.
Without training, a single iteration of message passing aggregates information from neighboring nodes, enabling a GNN to approximate common neighbors directly.
Similarly, multiple layers of message passing can capture more complex patterns, such as PageRank, by propagating information along paths of increasing length.

This overlap highlights a promising connection between GNNs and network science, where network science provides interpretable and well-established tools for efficiently analyzing graph structure.
By integrating these insights, the deep graph learning community can develop architectures that not only improve predictive performance but also enhance interpretability and robustness across diverse graph-based applications.


\subsection{Explaining Deep Learning Models}

Breaking open the ``black box'' is one of the grand challenges in deep learning.
Network analysis techniques offer an avenue for doing this.
Using a new loss function and simple network measures such as modularity, clustering coefficients, and centrality, \citet{bonifazi2024network} and colleagues gain insight into the network structure of various GNNs.
They then use these insights to improve the node classification performance of various GNNs. 

As the authors note themselves \cite{bonifazi2024network}, this is only the tip of the iceberg.
Many other network analysis techniques could be used to gain further insight into the structural elements, and hopefully the functioning, of these complex collections of nodes and edges.
For example, one could use multi-layer networks as a way of capturing additional attributes of the embeddings.
Others have used comparisons of eigenvectors across various layers of Convolutional Neural Networks to investigate universal encodings \cite{guth2024universality}.
Could similar techniques be applied to GNNs as a way of tracking performance and representation changes as the models increase in scale?     


\subsection{From Example-based to Unsupervised Learning}

Deep graph learning methods focus on learning data patterns in a supervised, semi-supervised, or self-supervised fashion.
Supervised and semi-supervised learning rely on labeled data to quantify performance and train models' weights via backpropagation to minimize training loss.
Consequently, they require that their training data contain examples covering all labels that will be considered during prediction.
If this is not the case, the trained model cannot identify instances belonging to the unseen label.
A way around this is to apply self-supervised techniques such as graph auto-encoders that encode and then decode the data, compressing it into a lower-dimensional representation, followed by decompressing it to reconstruct the original data \cite{kipf2016variational,9770382}.
The training criterion in this case is to minimize the discrepancy between the original and reconstructed data, which can be measured via the $L_2$ norm or other measures that quantify the ``distance'' between matrices.
However, this approach allows the user very limited control over the exact patterns that are learnt during the embedding process.

In contrast, network science methods typically do not assume labeled training data and employ models designed to capture specific patterns.
This makes them more widely applicable in a world where most data is unlabeled and research questions require focusing on a specific aspect of the data in a controllable way.
Integrating such model-based approaches from network science with deep graph learning methods would make it possible to align GNNs with transparent and interpretable models and train them in an unsupervised fashion.


\subsection{Balancing Interpretability and Flexibility}

Deep graph learning and network science take different approaches to modeling graph data.
Network science provides interpretable frameworks based on explicit assumptions about structure and relationships, but these assumptions can limit flexibility in complex graphs.
In contrast, deep learning methods prioritize flexibility, accommodating diverse inputs and intricate dependencies, often at the cost of interpretability.

This trade-off between interpretability and flexibility is an opportunity for deep graph learning to incorporate insights from network science while maintaining flexibility.
For example, GNNs struggle in heterophilic settings, where connected nodes differ in features or labels \cite{zheng2024what}.
In contrast, statistical models like the SBM can capture both homophilic and heterophilic structures.
Integrating such principled models into GNNs could improve generalization and robustness across varying graph topologies.


Meta-learning emerges as a promising approach to balancing interpretability and flexibility.
It enables models to dynamically adjust their assumptions or learning strategies based on the characteristics of the input data.
Hybrid frameworks can combine the interpretable structure of probabilistic network models with the scalability and flexibility of deep graph learning techniques.
For example, integrating automatic differentiation and Bayesian inference enables scalable, interpretable models capable of handling diverse network configurations \cite{contisciani2025flexible}.
Similarly, GNNs inspired by structured models like the SBM could better capture heterophilic patterns, improving their applicability across different types of graphs \cite{pmlr-v235-wang24u}.


\subsection{Towards Foundation Models for Graphs}

Finally, a major open challenge in deep graph learning is that current tasks are still largely domain-centered and models need to be trained for a specific task in a given graph or, for graph-level learning tasks, a specific set of graphs from a given domain.
Different from recent advances in computer vision and natural language processing, deep graph learning still largely lacks \emph{foundation models} that could generalize to new, previously unseen graphs \cite{morris24a}.
While deep graph learning has now taken the first steps in this direction \cite{zhao2024graphanyfoundationmodelnode,Haitao2024}, network science has a long tradition of identifying and modeling \emph{universal organizing principles} that govern the structure and evolution of networks across domains like social networks, information systems, biology, and large-scale infrastructures.
To this end, many insights have been generated regarding how simple (local) mechanisms shape collective patterns and characteristics of large-scale networks and/or dynamical processes across domains.
Concrete examples include phase transitions in the connectivity of large graphs that can be explained based on the ratio between moments of their degree distributions \cite{newman2001random}, simple growth rules like preferential attachment that lead to similar scale-invariant patterns in very different networks \cite{barabasi1999emergence}, or common characteristics in financial and social networks that result in similar propagation and consensus dynamics across different systems \cite{lorenz2009systemic}.
Despite these strong results in network science, we lack a theoretical understanding of whether and how these common organization principles in networks from across different domains affect the generalization capabilities of commonly used deep graph learning architectures, and how we could build architectures able to capture such principles.



\section{Alternative Views}

While the sections above outline how insights from network science can advance deep graph learning, one should also ask whether these arguments apply in the opposite direction.
Given that deep graph learning already achieves high performance on its own, is integrating insights from network science unlikely or unnecessary?
Will deep graph learning instead advance network science?


\subsection{Scalability Challenges in Network Science}

Deep graph learning emphasizes scalability and promotes efficient methods that scale to large datasets.
This is particularly important in applications considering large and dynamic networks that evolve over time.
While scalability is also an important aspect in network science, its focus lies more often than not on understanding the fundamental principles and properties of complex systems.

Many breakthroughs in computer vision, natural language processing, and graph learning have only been possible due to the computational power provided by GPUs.
With thousands or even tens of thousands of special-purpose cores, modern GPUs have become essential in deep learning and enable processing large-scale data in a fraction of the time required on general-purpose CPUs.
As large-scale network datasets with millions to billions of nodes and links become increasingly available, network science approaches struggle but have an opportunity to scale with the data by leveraging GPUs.
This requires shifting from implementing algorithms with ``traditional'' control structures such as loops and conditions to expressing them in terms of (sparse) tensor operations and for computations on GPUs.
Moreover, developers need to ensure that the data fits into GPU memory or employ batch or stream processing techniques.

Recently, accelerating network analyses with GPU-based implementations has received attention and brought forward tools that scale to large networks with substantial speedups:
\texttt{nx-cugraph}\footnote{\url{https://github.com/rapidsai/nx-cugraph}} provides a GPU-accelerated backend for the popular Python network analysis library \texttt{networkx}, scaling computations such as node centrality via PageRank or betweenness centrality, and Louvain community detection to networks with millions of nodes and edges.
\texttt{pathpyG}\footnote{\url{https://github.com/pathpy/pathpyG}}, a tool for analysing and modeling temporal graphs builds on \texttt{pytorch}~\cite{pytorch} and \texttt{pyG}~\cite{fey2019fastgraphrepresentationlearning}, facilitating, for example, the efficient GPU-based calculation of time-respecting paths, temporal node centralities and higher-order graph models.


\subsection{Evaluation Practices and Research Infrastructures}

Deep learning and network science can be seen as two ends of the spectrum when it comes to the role of benchmarks and standardized evaluation practices.
At the one end, method development in deep learning is often driven by a relatively small set of tasks at the node, link, and graph level: node classification, link prediction, and graph classification~\cite{JMLR:v23:20-852}.
To evaluate the performance of new methods, the deep learning community places a strong focus on utilizing standardized benchmarks with common datasets, frameworks, and leaderboards, which has greatly improved our ability to compare methods~\cite{ogb,huang2023}.
At the other end, network science works are typically driven by a specific application or the limitations of existing methods to capture specific aspects of a complex system.
While standardized methods for generating benchmark graphs exist~\cite{karrer2011sbm,lancichinetti2008benchmark,lee2019review,abcd-benchmark}, they are used with application-specific parameters, and, thus, different methods are seldom evaluated under the exact same setting.
Several online repositories host collections of networks often used for benchmarking methods~\cite{konect,snapnets,icon,netzschleuder}, however, altogether there are hundreds of networks available but without any subset of datasets that would be considered a ``standard benchmark''.

Following the example set by the deep learning community regarding evaluation practices carries opportunities for the network science community.
The most direct benefit would be to make methods more easily comparable by standardizing tasks, datasets, and evaluation frameworks.
This would provide clearer guidance for practitioners when choosing a method to answer a research question.
It would also make it easier for researchers entering the field to choose a starting point for evaluating their methods.
However, as with all things, there are several open challenges that need to be considered:
Typically, there is no one-size-fits-all benchmark setup that suits all scenarios, especially considering the wide range of applications in network science.
Choosing a relatively small number of benchmark datasets may lead to reduced exploration and diversity, limiting the community to a prescribed set of designated ``standard networks''.
Works that consider off-mainstream research questions may be forced to shoehorn their evaluation into existing frameworks and argue for their non-standard evaluation in order to avoid rejection.


\subsection{Bridging Scientific Cultures}

A final challenge that could hinder a better integration is that network science and deep learning largely differ in terms of scientific culture.
The attentive reader may have noticed that the majority of network science works cited in this paper were either published in (interdisciplinary) journals or in statistical physics outlets, that are largely outside the scope of machine learning researchers.
In contrast, the deep learning community largely focuses on (a small set of) major machine learning venues.   
This difference in publication practices not only makes it difficult to communicate results, it also affects the strategies and research challenges adopted by junior researchers who rely on the prestige of publication venues to secure a tenured position.

Second, the two communities use largely different terminology, sometimes for the same concepts. This is due to the fact that many concepts in network science, such as statistical ensembles, phase transitions, etc., originally come from the statistical physics study of complex systems.

These two aspects create challenges not only for network science researchers to publish in deep learning and deep learning researchers publishing in network science, it also creates challenges for the visibility of the large body of results in network science, which -- since it has been published in physics journals - remains largely unknown to the deep learning community.

Facilitated by the previous point that limits the amount of deep learning works that utilize network science models, in the network science community, the recent rapid growth of works on deep graph learning is partly seen as a competitive rather than a collaborative effort.



\section{Conclusion}

We offer a pathway for integrating methods and insights from network science into deep graph learning and vice versa.
By bridging the theoretical frameworks of network science with the scalability and adaptability of deep learning, researchers in the deep graph learning community can address challenges such as robust and principled data augmentation, effective pooling techniques, and higher-order modeling of complex interactions.
We also see potential advancements for network science by incorporating standardized benchmarks, evaluation practices, and GPU-based implementations of algorithms that scale to massive graphs.

To realize this potential, we call for integrating network science methods---such as random graph theory, generating function analysis of random graph ensembles, and models of dynamical processes like diffusion, synchronization, or consensus dynamics---into computer science and deep learning academic curricula.
Similarly, network and data science curricula would benefit from incorporating deep learning methods, including GPU-based tensor operations, fast gradient-based optimization, end-to-end learning approaches, and common evaluation procedures such as hyperparameter tuning and ablation studies.

At the scholarly level, fostering collaboration between network science and deep graph learning is essential.
Strengthening this connection requires bringing researchers from different backgrounds and publication cultures together, whether through dedicated journal-first tracks in deep learning conferences or by organizing workshops and special journal issues that bridge the two fields.

Ultimately, we see this convergence as more than just a set of technical advances.
By embracing the interpretability of network science and the flexibility of deep learning, researchers from both fields will benefit---just as Hopfield did by integrating ideas across statistical physics, neuroscience, and complex systems. 


\section*{Acknowledgements}

MR was supported by the Swedish Research Council under grant 2023-03705.


% \bibliography{references}
% \bibliographystyle{icml2025}

\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antelmi et~al.(2023)Antelmi, Cordasco, Polato, Scarano, Spagnuolo, and
  Yang]{survey-hypergraph-representation-learning}
Antelmi, A., Cordasco, G., Polato, M., Scarano, V., Spagnuolo, C., and Yang, D.
\newblock A survey on hypergraph representation learning.
\newblock \emph{ACM Comput. Surv.}, 56\penalty0 (1), August 2023.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/3605776}.

\bibitem[Azabou et~al.(2023)Azabou, Ganesh, Thakoor, Lin, Sathidevi, Liu,
  Valko, Veli{\v{c}}kovi{\'c}, and Dyer]{azabou2023half}
Azabou, M., Ganesh, V., Thakoor, S., Lin, C.-H., Sathidevi, L., Liu, R., Valko,
  M., Veli{\v{c}}kovi{\'c}, P., and Dyer, E.~L.
\newblock Half-hop: A graph upsampling approach for slowing down message
  passing.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1341--1360. PMLR, 2023.

\bibitem[Barab{\'a}si \& Albert(1999)Barab{\'a}si and
  Albert]{barabasi1999emergence}
Barab{\'a}si, A.-L. and Albert, R.
\newblock Emergence of scaling in random networks.
\newblock \emph{science}, 286\penalty0 (5439):\penalty0 509--512, 1999.

\bibitem[Battiston et~al.(2020)Battiston, Cencetti, Iacopini, Latora, Lucas,
  Patania, Young, and Petri]{battiston2020beyondpairwiseinteractions}
Battiston, F., Cencetti, G., Iacopini, I., Latora, V., Lucas, M., Patania, A.,
  Young, J.-G., and Petri, G.
\newblock Networks beyond pairwise interactions: Structure and dynamics.
\newblock \emph{Physics Reports}, 874:\penalty0 1--92, 2020.
\newblock ISSN 0370-1573.
\newblock \doi{https://doi.org/10.1016/j.physrep.2020.05.004}.
\newblock Networks beyond pairwise interactions: Structure and dynamics.

\bibitem[Bianchi et~al.(2020)Bianchi, Grattarola, and
  Alippi]{pmlr-v119-bianchi20a}
Bianchi, F.~M., Grattarola, D., and Alippi, C.
\newblock Spectral clustering with graph neural networks for graph pooling.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  874--883. PMLR, 13--18
  Jul 2020.

\bibitem[Bl{\"o}cker et~al.(2024)Bl{\"o}cker, Tan, and
  Scholtes]{blocker2023map}
Bl{\"o}cker, C., Tan, C., and Scholtes, I.
\newblock The map equation goes neural: Mapping network flows with graph neural
  networks.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}, 2024.

\bibitem[Blondel et~al.(2008)Blondel, Guillaume, Lambiotte, and
  Lefebvre]{Blondel_2008}
Blondel, V.~D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E.
\newblock Fast unfolding of communities in large networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2008\penalty0 (10):\penalty0 P10008, oct 2008.
\newblock \doi{10.1088/1742-5468/2008/10/P10008}.

\bibitem[Blöcker \& Scholtes(2024)Blöcker and
  Scholtes]{bloecker2024flowdivergence}
Blöcker, C. and Scholtes, I.
\newblock Flow divergence: Comparing maps of flows with relative entropy, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.09052}.

\bibitem[Bonifazi et~al.(2024)Bonifazi, Cauteruccio, Corradini, Marchetti,
  Ursino, and Virgili]{bonifazi2024network}
Bonifazi, G., Cauteruccio, F., Corradini, E., Marchetti, M., Ursino, D., and
  Virgili, L.
\newblock A network analysis-based framework to understand the representation
  dynamics of graph neural networks.
\newblock \emph{Neural Computing and Applications}, 36\penalty0 (4):\penalty0
  1875--1897, 2024.

\bibitem[Casiraghi et~al.(2017)Casiraghi, Nanumyan, Scholtes, and
  Schweitzer]{casiraghi2017}
Casiraghi, G., Nanumyan, V., Scholtes, I., and Schweitzer, F.
\newblock From relational data to graphs: Inferring significant links using
  generalized hypergeometric ensembles.
\newblock In Ciampaglia, G.~L., Mashhadi, A., and Yasseri, T. (eds.),
  \emph{Social Informatics}, pp.\  111--120, Cham, 2017. Springer International
  Publishing.
\newblock ISBN 978-3-319-67256-4.

\bibitem[Castellana \& Bianchi(2025)Castellana and
  Bianchi]{castellana2025bnpoolbayesiannonparametricapproach}
Castellana, D. and Bianchi, F.~M.
\newblock Bn-pool: a bayesian nonparametric approach to graph pooling, 2025.

\bibitem[Chami et~al.(2022)Chami, Abu-El-Haija, Perozzi, R{\'e}, and
  Murphy]{JMLR:v23:20-852}
Chami, I., Abu-El-Haija, S., Perozzi, B., R{\'e}, C., and Murphy, K.
\newblock Machine learning on graphs: A model and comprehensive taxonomy.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (89):\penalty0 1--64, 2022.

\bibitem[Clauset et~al.(2016)Clauset, Tucker, and Sainz]{icon}
Clauset, A., Tucker, E., and Sainz, M.
\newblock The colorado index of complex networks, 2016.
\newblock URL \url{https://icon.colorado.edu}.

\bibitem[Contisciani et~al.(2025)Contisciani, Hobbhahn, Power, Hennig, and
  De~Bacco]{contisciani2025flexible}
Contisciani, M., Hobbhahn, M., Power, E.~A., Hennig, P., and De~Bacco, C.
\newblock Flexible inference in heterogeneous and attributed multilayer
  networks.
\newblock \emph{PNAS Nexus}, pp.\  pgaf005, 2025.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li\`{o}, and
  Veli\v{c}kovi\'{c}]{NEURIPS2020_99cad265}
Corso, G., Cavalleri, L., Beaini, D., Li\`{o}, P., and Veli\v{c}kovi\'{c}, P.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  13260--13271. Curran Associates, Inc., 2020.

\bibitem[De~Bruijn(1946)]{de1946combinatorial}
De~Bruijn, N.~G.
\newblock A combinatorial problem.
\newblock \emph{Proceedings of the Section of Sciences of the Koninklijke
  Nederlandse Akademie van Wetenschappen te Amsterdam}, 49\penalty0
  (7):\penalty0 758--764, 1946.

\bibitem[De~Domenico et~al.(2013)De~Domenico, Sol\'e-Ribalta, Cozzo, Kivel\"a,
  Moreno, Porter, G\'omez, and Arenas]{dedomenico2013multilayer}
De~Domenico, M., Sol\'e-Ribalta, A., Cozzo, E., Kivel\"a, M., Moreno, Y.,
  Porter, M.~A., G\'omez, S., and Arenas, A.
\newblock Mathematical formulation of multilayer networks.
\newblock \emph{Phys. Rev. X}, 3:\penalty0 041022, Dec 2013.
\newblock \doi{10.1103/PhysRevX.3.041022}.

\bibitem[Deng et~al.(2024)Deng, Yang, Yang, Gong, Chen, Chen, and
  Hao]{deng2024module}
Deng, S., Yang, G., Yang, Y., Gong, Z., Chen, C., Chen, X., and Hao, Z.
\newblock Module-based graph pooling for graph classification.
\newblock \emph{Pattern Recognition}, 154:\penalty0 110606, 2024.

\bibitem[Edler et~al.(2017)Edler, Bohlin, and Rosvall]{edler2017infomap}
Edler, D., Bohlin, L., and Rosvall, M.
\newblock Mapping higher-order network flows in memory and multilayer networks
  with infomap.
\newblock \emph{Algorithms}, 10\penalty0 (4), 2017.
\newblock ISSN 1999-4893.
\newblock \doi{10.3390/a10040112}.

\bibitem[Erdos et~al.(1960)Erdos, R{\'e}nyi, et~al.]{erdos1960evolution}
Erdos, P., R{\'e}nyi, A., et~al.
\newblock On the evolution of random graphs.
\newblock \emph{Publ. math. inst. hung. acad. sci}, 5\penalty0 (1):\penalty0
  17--60, 1960.

\bibitem[Fey \& Lenssen(2019)Fey and
  Lenssen]{fey2019fastgraphrepresentationlearning}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with pytorch geometric, 2019.

\bibitem[Frantzen \& Schaub(2024)Frantzen and
  Schaub]{frantzen2024learningsimplicialdatabased}
Frantzen, F. and Schaub, M.~T.
\newblock Learning from simplicial data based on random walks and 1d
  convolutions, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.03434}.

\bibitem[Geisler et~al.(2020)Geisler, Z\"{u}gner, and
  G\"{u}nnemann]{NEURIPS2020_99e314b1}
Geisler, S., Z\"{u}gner, D., and G\"{u}nnemann, S.
\newblock Reliable graph neural networks via robust aggregation.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  13272--13284. Curran Associates, Inc., 2020.

\bibitem[Georgousis et~al.(2021)Georgousis, Kenning, and Xie]{Georgousis2021}
Georgousis, S., Kenning, M.~P., and Xie, X.
\newblock Graph deep learning: State of the art and challenges.
\newblock \emph{IEEE Access}, 9:\penalty0 22106--22140, 2021.
\newblock \doi{10.1109/ACCESS.2021.3055280}.

\bibitem[Grover \& Leskovec(2016)Grover and Leskovec]{grover2016node2vec}
Grover, A. and Leskovec, J.
\newblock node2vec: Scalable feature learning for networks.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  855--864, 2016.

\bibitem[Guth \& M{\'e}nard(2024)Guth and M{\'e}nard]{guth2024universality}
Guth, F. and M{\'e}nard, B.
\newblock On the universality of neural encodings in cnns.
\newblock In \emph{ICLR 2024 Workshop on Representational Alignment}, 2024.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Heeg \& Scholtes(2024)Heeg and Scholtes]{heegusing}
Heeg, F. and Scholtes, I.
\newblock Using time-aware graph neural networks to predict temporal
  centralities in dynamic graphs.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}, 2024.

\bibitem[Holme(2015)]{holme2015modern}
Holme, P.
\newblock Modern temporal network theory: a colloquium.
\newblock \emph{The European Physical Journal B}, 88:\penalty0 1--30, 2015.

\bibitem[Hopfield(1982)]{hopfield1982neural}
Hopfield, J.~J.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock \emph{Proceedings of the national academy of sciences}, 79\penalty0
  (8):\penalty0 2554--2558, 1982.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{ogb}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  22118--22133. Curran Associates, Inc., 2020.

\bibitem[Huang et~al.(2023)Huang, Poursafaei, Danovitch, Fey, Hu, Rossi,
  Leskovec, Bronstein, Rabusseau, and Rabbany]{huang2023}
Huang, S., Poursafaei, F., Danovitch, J., Fey, M., Hu, W., Rossi, E., Leskovec,
  J., Bronstein, M., Rabusseau, G., and Rabbany, R.
\newblock Temporal graph benchmark for machine learning on temporal graphs.
\newblock In \emph{Proceedings of the 37th International Conference on Neural
  Information Processing Systems}, NIPS '23, Red Hook, NY, USA, 2023. Curran
  Associates Inc.

\bibitem[Jamadandi et~al.(2024)Jamadandi, Rubio-Madrigal, and
  Burkholz]{jamadandi2024}
Jamadandi, A., Rubio-Madrigal, C., and Burkholz, R.
\newblock Spectral graph pruning against over-squashing and over-smoothing.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}, 2024.

\bibitem[Jin et~al.(2022)Jin, Wang, Ge, He, Li, Lin, and Zhang]{ijcai2022p293}
Jin, D., Wang, R., Ge, M., He, D., Li, X., Lin, W., and Zhang, W.
\newblock Raw-gnn: Random walk aggregation based graph neural network.
\newblock In Raedt, L.~D. (ed.), \emph{Proceedings of the Thirty-First
  International Joint Conference on Artificial Intelligence, {IJCAI-22}}, pp.\
  2108--2114. International Joint Conferences on Artificial Intelligence
  Organization, 7 2022.
\newblock \doi{10.24963/ijcai.2022/293}.
\newblock Main Track.

\bibitem[Ju et~al.(2024)Ju, Yi, Wang, Xiao, Mao, Li, Gu, Qin, Yin, Wang, Liu,
  Luo, Yu, and Zhang]{ju2024surveygraphneuralnetworks}
Ju, W., Yi, S., Wang, Y., Xiao, Z., Mao, Z., Li, H., Gu, Y., Qin, Y., Yin, N.,
  Wang, S., Liu, X., Luo, X., Yu, P.~S., and Zhang, M.
\newblock A survey of graph neural networks in real world: Imbalance, noise,
  privacy and ood challenges, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.04468}.

\bibitem[Kamiński et~al.(2021)Kamiński, Prałat, and
  Théberge]{abcd-benchmark}
Kamiński, B., Prałat, P., and Théberge, F.
\newblock Artificial benchmark for community detection (abcd)—fast random
  graph model with community structure.
\newblock \emph{Network Science}, 9\penalty0 (2):\penalty0 153–178, 2021.
\newblock \doi{10.1017/nws.2020.45}.

\bibitem[Karrer \& Newman(2011)Karrer and Newman]{karrer2011sbm}
Karrer, B. and Newman, M. E.~J.
\newblock Stochastic blockmodels and community structure in networks.
\newblock \emph{Phys. Rev. E}, 83:\penalty0 016107, Jan 2011.
\newblock \doi{10.1103/PhysRevE.83.016107}.

\bibitem[Kim et~al.(2024)Kim, Lee, Gao, Antelmi, Polato, and
  Shin]{survey-hypergraphg-neural-networks}
Kim, S., Lee, S.~Y., Gao, Y., Antelmi, A., Polato, M., and Shin, K.
\newblock A survey on hypergraph neural networks: An in-depth and step-by-step
  guide.
\newblock In \emph{Proceedings of the 30th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, KDD '24, pp.\  6534–6544, New York, NY, USA,
  2024. Association for Computing Machinery.
\newblock ISBN 9798400704901.
\newblock \doi{10.1145/3637528.3671457}.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016variational}
Kipf, T.~N. and Welling, M.
\newblock Variational graph auto-encoders.
\newblock In \emph{Proceedings of the Bayesian Deep Learning Workshop, 30th
  Conference on Neural Information Processing Systems (NeurIPS)}, Centre
  Convencions Internacional Barcelona, Barcelona, Spain, 2016.

\bibitem[Kivelä et~al.(2014)Kivelä, Arenas, Barthelemy, Gleeson, Moreno, and
  Porter]{kivela2014multilayer}
Kivelä, M., Arenas, A., Barthelemy, M., Gleeson, J.~P., Moreno, Y., and
  Porter, M.~A.
\newblock Multilayer networks.
\newblock \emph{Journal of Complex Networks}, 2\penalty0 (3):\penalty0
  203--271, 07 2014.
\newblock ISSN 2051-1310.
\newblock \doi{10.1093/comnet/cnu016}.

\bibitem[Kojaku et~al.(2024)Kojaku, Radicchi, Ahn, and
  Fortunato]{kojaku2024network}
Kojaku, S., Radicchi, F., Ahn, Y.-Y., and Fortunato, S.
\newblock Network community detection via neural embeddings.
\newblock \emph{Nature Communications}, 15\penalty0 (1):\penalty0 9446, 2024.

\bibitem[Kunegis(2013)]{konect}
Kunegis, J.
\newblock Konect: the koblenz network collection.
\newblock In \emph{Proceedings of the 22nd International Conference on World
  Wide Web}, WWW '13 Companion, pp.\  1343–1350, New York, NY, USA, 2013.
  Association for Computing Machinery.
\newblock ISBN 9781450320382.
\newblock \doi{10.1145/2487788.2488173}.

\bibitem[Lambiotte et~al.(2019)Lambiotte, Rosvall, and
  Scholtes]{lambiotte2019networks}
Lambiotte, R., Rosvall, M., and Scholtes, I.
\newblock From networks to optimal higher-order models of complex systems.
\newblock \emph{Nature physics}, 15\penalty0 (4):\penalty0 313--320, 2019.

\bibitem[Lancichinetti et~al.(2008)Lancichinetti, Fortunato, and
  Radicchi]{lancichinetti2008benchmark}
Lancichinetti, A., Fortunato, S., and Radicchi, F.
\newblock Benchmark graphs for testing community detection algorithms.
\newblock \emph{Physical Review E—Statistical, Nonlinear, and Soft Matter
  Physics}, 78\penalty0 (4):\penalty0 046110, 2008.

\bibitem[Lee \& Wilkinson(2019)Lee and Wilkinson]{lee2019review}
Lee, C. and Wilkinson, D.~J.
\newblock A review of stochastic block models and extensions for graph
  clustering.
\newblock \emph{Applied Network Science}, 4\penalty0 (1):\penalty0 1--50, 2019.

\bibitem[Leskovec \& Krevl(2014)Leskovec and Krevl]{snapnets}
Leskovec, J. and Krevl, A.
\newblock {SNAP Datasets}: {Stanford} large network dataset collection, June
  2014.
\newblock URL \url{http://snap.stanford.edu/data}.

\bibitem[Liu et~al.(2023)Liu, Jin, Pan, Zhou, Zheng, Xia, and Yu]{9770382}
Liu, Y., Jin, M., Pan, S., Zhou, C., Zheng, Y., Xia, F., and Yu, P.~S.
\newblock Graph self-supervised learning: A survey.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  35\penalty0 (6):\penalty0 5879--5900, 2023.
\newblock \doi{10.1109/TKDE.2022.3172903}.

\bibitem[Longa et~al.(2023)Longa, Lachi, Santin, Bianchini, Lepri, Lio,
  Scarselli, and Passerini]{longa2023graphneuralnetworkstemporal}
Longa, A., Lachi, V., Santin, G., Bianchini, M., Lepri, B., Lio, P., Scarselli,
  F., and Passerini, A.
\newblock Graph neural networks for temporal graphs: State of the art, open
  challenges, and opportunities, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.01018}.

\bibitem[Lorenz et~al.(2009)Lorenz, Battiston, and
  Schweitzer]{lorenz2009systemic}
Lorenz, J., Battiston, S., and Schweitzer, F.
\newblock Systemic risk in a unifying framework for cascading processes on
  networks.
\newblock \emph{The European Physical Journal B}, 71:\penalty0 441--460, 2009.

\bibitem[Mao et~al.(2024)Mao, Chen, Tang, Zhao, Ma, Zhao, Shah, Galkin, and
  Tang]{Haitao2024}
Mao, H., Chen, Z., Tang, W., Zhao, J., Ma, Y., Zhao, T., Shah, N., Galkin, M.,
  and Tang, J.
\newblock Position: graph foundation models are already here.
\newblock In \emph{Proceedings of the 41st International Conference on Machine
  Learning}, ICML'24. JMLR.org, 2024.

\bibitem[Moinet et~al.(2015)Moinet, Starnini, and
  Pastor-Satorras]{moinet2015burstiness}
Moinet, A., Starnini, M., and Pastor-Satorras, R.
\newblock Burstiness and aging in social temporal networks.
\newblock \emph{Physical review letters}, 114\penalty0 (10):\penalty0 108701,
  2015.

\bibitem[Molloy \& Reed(1995)Molloy and Reed]{molloy1995critical}
Molloy, M. and Reed, B.
\newblock A critical point for random graphs with a given degree sequence.
\newblock \emph{Random structures \& algorithms}, 6\penalty0 (2-3):\penalty0
  161--180, 1995.

\bibitem[Morris et~al.(2024)Morris, Frasca, Dym, Maron, Ceylan, Levie, Lim,
  Bronstein, Grohe, and Jegelka]{morris24a}
Morris, C., Frasca, F., Dym, N., Maron, H., Ceylan, I.~I., Levie, R., Lim, D.,
  Bronstein, M.~M., Grohe, M., and Jegelka, S.
\newblock Position: Future directions in the theory of graph machine learning.
\newblock In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N.,
  Scarlett, J., and Berkenkamp, F. (eds.), \emph{Proceedings of the 41st
  International Conference on Machine Learning}, volume 235 of
  \emph{Proceedings of Machine Learning Research}, pp.\  36294--36307. PMLR,
  21--27 Jul 2024.
\newblock URL \url{https://proceedings.mlr.press/v235/morris24a.html}.

\bibitem[Newman(2009)]{newman2009random}
Newman, M.~E.
\newblock Random graphs with clustering.
\newblock \emph{Physical review letters}, 103\penalty0 (5):\penalty0 058701,
  2009.

\bibitem[Newman(2018)]{newman2018network}
Newman, M.~E.
\newblock Network structure from rich but noisy data.
\newblock \emph{Nature Physics}, 14\penalty0 (6):\penalty0 542--545, 2018.

\bibitem[Newman et~al.(2001)Newman, Strogatz, and Watts]{newman2001random}
Newman, M.~E., Strogatz, S.~H., and Watts, D.~J.
\newblock Random graphs with arbitrary degree distributions and their
  applications.
\newblock \emph{Physical review E}, 64\penalty0 (2):\penalty0 026118, 2001.

\bibitem[Pareja et~al.(2020)Pareja, Domeniconi, Chen, Ma, Suzumura, Kanezashi,
  Kaler, Schardl, and Leiserson]{pareja2020evolvegcn}
Pareja, A., Domeniconi, G., Chen, J., Ma, T., Suzumura, T., Kanezashi, H.,
  Kaler, T., Schardl, T., and Leiserson, C.
\newblock Evolvegcn: Evolving graph convolutional networks for dynamic graphs.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pp.\  5363--5370, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Peixoto(2019{\natexlab{a}})]{peixoto2019bayesian}
Peixoto, T.~P.
\newblock Bayesian stochastic blockmodeling.
\newblock \emph{Advances in network clustering and blockmodeling}, pp.\
  289--332, 2019{\natexlab{a}}.

\bibitem[Peixoto(2019{\natexlab{b}})]{peixoto2019network}
Peixoto, T.~P.
\newblock Network reconstruction and community detection from dynamics.
\newblock \emph{Physical review letters}, 123\penalty0 (12):\penalty0 128301,
  2019{\natexlab{b}}.

\bibitem[Peixoto(2020)]{netzschleuder}
Peixoto, T.~P.
\newblock The netzschleuder network catalogue and repository, August 2020.

\bibitem[Peixoto \& Rosvall(2017)Peixoto and Rosvall]{peixoto2017modelling}
Peixoto, T.~P. and Rosvall, M.
\newblock Modelling sequences and temporal networks with dynamic community
  structures.
\newblock \emph{Nature communications}, 8\penalty0 (1):\penalty0 582, 2017.

\bibitem[Qarkaxhija et~al.(2022)Qarkaxhija, Perri, and
  Scholtes]{qarkaxhija2022}
Qarkaxhija, L., Perri, V., and Scholtes, I.
\newblock De bruijn goes neural: Causality-aware graph neural networks for time
  series data on dynamic graphs.
\newblock In Rieck, B. and Pascanu, R. (eds.), \emph{Proceedings of the First
  Learning on Graphs Conference}, volume 198 of \emph{Proceedings of Machine
  Learning Research}, pp.\  51:1--51:21. PMLR, 09--12 Dec 2022.
\newblock URL \url{https://proceedings.mlr.press/v198/qarkaxhija22a.html}.

\bibitem[Robins et~al.(2007)Robins, Pattison, Kalish, and
  Lusher]{robins2007introduction}
Robins, G., Pattison, P., Kalish, Y., and Lusher, D.
\newblock An introduction to exponential random graph (p*) models for social
  networks.
\newblock \emph{Social networks}, 29\penalty0 (2):\penalty0 173--191, 2007.

\bibitem[Rossi et~al.(2020)Rossi, Chamberlain, Frasca, Eynard, Monti, and
  Bronstein]{rossi2020}
Rossi, E., Chamberlain, B., Frasca, F., Eynard, D., Monti, F., and Bronstein,
  M.~M.
\newblock Temporal graph networks for deep learning on dynamic graphs.
\newblock \emph{CoRR}, abs/2006.10637, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.10637}.

\bibitem[Rosvall \& Bergstrom(2008)Rosvall and Bergstrom]{rosvall2008maps}
Rosvall, M. and Bergstrom, C.~T.
\newblock Maps of random walks on complex networks reveal community structure.
\newblock \emph{Proceedings of the national academy of sciences}, 105\penalty0
  (4):\penalty0 1118--1123, 2008.

\bibitem[Rosvall et~al.(2014)Rosvall, Esquivel, Lancichinetti, West, and
  Lambiotte]{rosvall2014memory}
Rosvall, M., Esquivel, A.~V., Lancichinetti, A., West, J.~D., and Lambiotte, R.
\newblock Memory in network flows and its effects on spreading dynamics and
  community detection.
\newblock \emph{Nature communications}, 5\penalty0 (1):\penalty0 4630, 2014.

\bibitem[Rubio-Madrigal et~al.(2025)Rubio-Madrigal, Jamadandi, and
  Burkholz]{rubiomadrigal2025comfy}
Rubio-Madrigal, C., Jamadandi, A., and Burkholz, R.
\newblock Gnns getting comfy: Community and feature similarity guided rewiring.
\newblock In \emph{The Thirteenth International Conference on Learning
  Representations}, 2025.

\bibitem[Scholtes et~al.(2014)Scholtes, Wider, Pfitzner, Garas, Tessone, and
  Schweitzer]{scholtes2014causality}
Scholtes, I., Wider, N., Pfitzner, R., Garas, A., Tessone, C.~J., and
  Schweitzer, F.
\newblock Causality-driven slow-down and speed-up of diffusion in non-markovian
  temporal networks.
\newblock \emph{Nature communications}, 5\penalty0 (1):\penalty0 5024, 2014.

\bibitem[Shchur \& G\"{u}nnemann(2019)Shchur and
  G\"{u}nnemann]{shchur2019overlapping}
Shchur, O. and G\"{u}nnemann, S.
\newblock Overlapping community detection with graph neural networks.
\newblock \emph{Deep Learning on Graphs Workshop, KDD}, 2019.

\bibitem[Takaguchi et~al.(2013)Takaguchi, Masuda, and
  Holme]{takaguchi2013bursty}
Takaguchi, T., Masuda, N., and Holme, P.
\newblock Bursty communication patterns facilitate spreading in a
  threshold-based epidemic dynamics.
\newblock \emph{PloS one}, 8\penalty0 (7):\penalty0 e68629, 2013.

\bibitem[Torres et~al.(2021)Torres, Blevins, Bassett, and
  Eliassi-Rad]{torres2021modelling}
Torres, L., Blevins, A.~S., Bassett, D., and Eliassi-Rad, T.
\newblock The why, how, and when of representations for complex systems.
\newblock \emph{SIAM Review}, 63\penalty0 (3):\penalty0 435--485, 2021.
\newblock \doi{10.1137/20M1355896}.

\bibitem[Traag et~al.(2019)Traag, Waltman, and van Eck]{Traag2019}
Traag, V.~A., Waltman, L., and van Eck, N.~J.
\newblock From louvain to leiden: guaranteeing well-connected communities.
\newblock \emph{Scientific Reports}, 9\penalty0 (1):\penalty0 5233, Mar 2019.
\newblock ISSN 2045-2322.
\newblock \doi{10.1038/s41598-019-41695-z}.

\bibitem[Tsitsulin et~al.(2024)Tsitsulin, Palowitch, Perozzi, and
  M\"{u}ller]{tsitsulin2023dmon}
Tsitsulin, A., Palowitch, J., Perozzi, B., and M\"{u}ller, E.
\newblock Graph clustering with graph neural networks.
\newblock \emph{J. Mach. Learn. Res.}, 24\penalty0 (1), March 2024.
\newblock ISSN 1532-4435.

\bibitem[von Pichowski et~al.(2024)von Pichowski, Blöcker, and
  Scholtes]{vonpichowski2024}
von Pichowski, J., Blöcker, C., and Scholtes, I.
\newblock Hierarchical graph pooling based on minimum description length, 2024.

\bibitem[Wang et~al.(2024)Wang, Guo, Yang, and Wang]{pmlr-v235-wang24u}
Wang, J., Guo, Y., Yang, L., and Wang, Y.
\newblock Understanding heterophily for graph neural networks.
\newblock In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N.,
  Scarlett, J., and Berkenkamp, F. (eds.), \emph{Proceedings of the 41st
  International Conference on Machine Learning}, volume 235 of
  \emph{Proceedings of Machine Learning Research}, pp.\  50489--50529. PMLR,
  21--27 Jul 2024.

\bibitem[Zhao et~al.(2024)Zhao, Mostafa, Galkin, Bronstein, Zhu, and
  Tang]{zhao2024graphanyfoundationmodelnode}
Zhao, J., Mostafa, H., Galkin, M., Bronstein, M., Zhu, Z., and Tang, J.
\newblock Graphany: A foundation model for node classification on any graph,
  2024.
\newblock URL \url{https://arxiv.org/abs/2405.20445}.

\bibitem[Zhao et~al.(2020)Zhao, Liu, Neves, Woodford, Jiang, and
  Shah]{Zhao2020}
Zhao, T., Liu, Y., Neves, L., Woodford, O.~J., Jiang, M., and Shah, N.
\newblock Data augmentation for graph neural networks.
\newblock \emph{CoRR}, abs/2006.06830, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.06830}.

\bibitem[Zhao et~al.(2023)Zhao, Jin, Liu, Wang, Liu, Günnemann, Shah, and
  Jiang]{zhao2023graphdataaugmentationgraph}
Zhao, T., Jin, W., Liu, Y., Wang, Y., Liu, G., Günnemann, S., Shah, N., and
  Jiang, M.
\newblock Graph data augmentation for graph machine learning: A survey, 2023.
\newblock URL \url{https://arxiv.org/abs/2202.08871}.

\bibitem[Zheng et~al.(2024)Zheng, Luan, and Chen]{zheng2024what}
Zheng, Y., Luan, S., and Chen, L.
\newblock What is missing for graph homophily? disentangling graph homophily
  for graph neural networks.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}, 2024.

\end{thebibliography}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
