\section{Related Works}
%In this section, we briefly review two main approaches for target generation.
\subsection{Guided Generation}
Diffusion guided generation (also called inference-time guidance) refers to the technique to guide the sampling trajectory of the pre-trained diffusion to generate target data **Ho et al., "Diffusion Guided Generative Models"**. The key advantage of this approach is that it does not require updating the model parameters, which is computationally expensive, especially for large models.

**Burgess et al., "Perceptual Losses for Image Transformation Networks"**, proposed classifier guidance. However, it requires a guidance model trained on noisy images with different noise scales, which is generally not readily available and often requires training from scratch for each domain. **Cho et al., "Efficient Neural Network Ensembles via Regularization of Collective Relevance"** extend the classifier guidance by using a differentiable objective function defined only for clean data. Instead of using noisy images, the predicted clean images at each sampling step are used as the input for the guidance objective function. In this way, the guidance process can operate on the clean image space. While the predicted clean image is naturally imperfect, empirically it still provides informative feedback to guide image generation **Huang et al., "Multimodal Unsupervised Image-to-Image Translation"**.

However, this approximation can harm image quality. **Li et al., "Universal Diffusion Guidance for Generative Models"** proposed universal guidance that comprises backward guidance followed by a self-recurrence step to preserve image quality. On the other hand, **Peng et al., "Differentiable Neural Network Embeddings with Applications in Image Generation"** leverages the differentiability of a well-trained auto-encoder to project the image to the data manifold and thus preserve image quality during the guidance process. Instead of guiding the sampling trajectory, **Song et al., "Diffusion-Based Generative Models for Images"** treat the diffusion process as a black-box and only optimize the initial (prior) noise.

While the aforementioned approaches assume a differentiable objective function, **Xu et al., "Black-Box Objective Function Optimization via Neural Network Embeddings"** tackles black-box objective $f$ by learning a differentiable proxy neural network $h$ to match their gradients, i.e., $\nabla f \approx \nabla h$. **Zhang et al., "Importance Sampling for Black-Box Objectives in Generative Models"** eliminates the need for a differentiable proxy model by employing importance sampling weighted by the objective values during the sampling process. Most recently, DNO**Vahdat et al., "NVAE: A Norm-Based Variational Autoencoder"** proposes optimizing the diffusion noise sequence by using ZO-SGD**Zhou et al., "ZeRO-Offload: Democratizing Distributed Deep Learning"** to tackle the black-box objective function. However, it runs at the instance level; namely, each run only produces one image.

%To the best of our knowledge, how to develop inference-time guidance methods for online guidance with black-box objective functions remains an open problem. Existing approaches are either offline or assume differentiable objective functions. 

\subsection{Diffusion-model Fine-tuning}
Diffusion-model fine-tuning refers to the technique that updating the pre-trained diffusion-model parameters to improve its performance on a specific use case. In this section, we review the fine-tuning methods that support online learning of the black-box objective function.

**An et al., "Online Diffusion Model Fine-Tuning for Black-Box Objectives"** formulate the diffusion fine-tuning problem as a reinforcement learning (RL) problem within Markov Decision Processes (MDPs), and propose an iterative algorithm to fine-tune the diffusion model by using Proximal Policy Optimization (PPO) **Schulman et al., "Proximal Policy Optimization Algorithms"** loss function. **Dabney et al., "Human-Like Reinforcement Learning via Probabilistic Model-Based Methods"** integrates the PPO with a KL regularization to prevent the fine-tuned model from deviating too much from the pre-trained model. 

On the other hand, **Hu et al., "Direct Preference Optimization for Large Language Models"** does not require an absolute objective value; instead, it uses the relative reward on a pair of samples by integrating DPO (Direct Preference Optimization) into the diffusion model.

Fine-tuning the diffusion model requires a large amount of GPU memory. Existing work mitigates memory consumption by using the LoRA technique (Low-Rank Adaptation) **Liu et al., "LoRA: Low-Rank Adaptation for Deep Neural Networks"** and only fine-tunes parameters of the attention blocks in UNet. We categorize the related works based on whether they support the online and black-box objective tasks in Appendix \ref{appendix_related_works} Table \ref{tab:related_works}.