\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\textwidth=15cm
\textheight=25cm
\leftmargin=0cm
\topmargin=-2cm
\date{}

\usepackage{graphicx,xspace,color}

\graphicspath{{/home/sfgk/2024/Figs/pinn_figs/},{/home/sfgk/2024/Figs/},{/home/sfgk/2024/Figs/Png/},{/home/sfgk/2024/Figs/Pdf/},{/home/sfgk/2024/Figs/img/},{/home/sfgk/2024/Figs/Figs0/},{/home/sfgk/2024/Figs/Figs1/},{/home/sfgk/2024/Figs/Figs2/},{/home/sfgk/2024/Figs/Figs3/},,{/home/sfgk/2024/Figs/Figs4/},{/home/sfgk/2023/Images/}{/home/sfgk/2024/Figs1/},{/home/sfgk/2022/Figs1/},{/home/sfgk/2022/Figs2/},{/home/sfgk/2022/Figs3/},{/home/sfgk/2022/Pdf/},{/home/sfgk/2023/Figs/},{/home/sfgk/2022/Pdf/}}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

\providecommand{\tightlist}{%
 \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\def\REM#1{}

%------------------MACROS -----------------------
\def\inputdir{}
%\usepackage{\inputdir macros_gpi}

\input{\inputdir alphabet1}
\input{\inputdir alphabet2}
\input{\inputdir macros_gpi}
%\input{\inputdir dessins_all}
%\input{\inputdir special_macros.tex}
%\input{\inputdir block_defs.tex}
%\input{\inputdir specific_macros}
%\input{\inputdir block_defs.tex}
%\input{\inputdir NN_figures.tex}

\def\rfbnn{\rfb_{\tiny NN}}
\def\bvi{\blue{v_{i}}}

\begin{document}

\title{Bayesian Physics Informed Neural Networks for Linear Inverse problems}

\author{A. Mohammad-Djafari\\ ~\\ 
former Research Director at CNRS, CentraleSup\'elec, Gif-sur-Yvette, France. 
}
\maketitle

\begin{abstract}
Inverse problems arise almost everywhere in science and engineering where we need to infer on a quantity from indirect observation. The cases of medical, biomedical, and industrial imaging systems are the typical examples.
\\
A very high overview of classification of the inverse problems method can be: i) Analytical, ii) Regularization, and iii) Bayesian inference methods. 
Even if there are straight links between them, we can say that the Bayesian inference based methods are the most powerful, as they give the possibility of accounting for prior knowledge and can account for errors and uncertainties in general. One of the main limitations stay in computational costs in particular for high dimensional imaging systems. Neural Networks (NN), and in particular Deep NNs (DNN), have been considered as a way to push farther this limit. 
\\
Physics Informed Neural Networks (PINN) concept integrates physical laws with deep learning techniques to enhance the speed, accuracy and efficiency of the above mentioned problems. 
\\ 
In this work, a new Bayesian framework for the concept of PINN (BPINN)  is presented and discussed which includes the deterministic one if we use the Maximum A Posteriori (MAP) estimation framework. We consider two cases of supervised and unsupervised for training step, obtain the expressions of the posterior probability of the unknown variables, and deduce the posterior laws of the NN's parameters. We also discuss about the challenges of implementation of these methods in real applications. 
 
\end{abstract}

\section{Introduction}
Inverse problems arise in many scientific and engineering domains, such as medical and biological imaging \cite{tomo-MI}, geophysical imaging \cite{Seismic-tomo}, industrial imaging \cite{Inspection-IP}, and many others \cite{e23121673}.  
These problems aim to infer an inaccessible quantity (input parameters of a system), such as physical fields, source locations, initial and boundary conditions, from the corresponding indirect measurement data. Inverse problems are, in general, ill-posed, as defined by Hadamard: 
A mathematical problem is saied to be well-posed, if it has a solution (Existance), if the solution is unique (Uniqueness) and if the solution is stable (stability). If any of these conditions are not satisfied, the problem is qualified as to ill-posed. 

Classical approaches to inverse problems can be classified as three main categories: Analytical, regularization methods and Bayesian inference. Regularization techniques aim to determine point estimates by simultaneously minimizing the misfits between observed and predicted outputs while penalizing undesired parameter features or encourage the desired parameter features through a regularization term \cite{engl1996regularization,SampledTikhonov}. 

Bayesian inference incorporates uncertainties in prior information and observations through the Bayes rule. It constructs the posterior distribution of parameters, thus effectively quantifying the uncertainty of unknown desired variables. However, in many applications, it is computationally expensive to sample from the Bayesian posterior distributions by using statistical simulation techniques, such as the Markov Chain Monte Carlo (MCMC)\cite{MultiscaleMRM, DataDriven,InvStekloff,IP-NU}, Hamiltonian Monte Carlo \cite{HMC-tomo,DD-HMC}, and Sequential Monte Carlo \cite{Multicale-SMC,SMC}. 

To address this issue, many researchers utilized various variational inference algorithms to reduce the computational cost, including mean-field variational inference \cite{ApproximateMI,BVI-Flow}, automatic differential variational inference\cite{VI-tomo,ADVI-Geo}, and Stein variational gradient descent\cite{VI-tomo,SVGD-inv}. But the variational methods may face challenges in high-dimensional settings due to the difficulty of accurately approximating posterior distributions with the tractable surrogate distribution. 

Although, even if all aforementioned methods have shown to be relatively efficient, they still require numerous evaluations of the forward model and the complicated parametric derivatives, which introduce higher computational cost in high-dimensional real applications. Consequently, deep learning neural networks  based methods have became an efficient alternative, where they are capable of providing almost real-time inversions for certain classes of inverse problems with new measurement data of similar type, under the conditions that they have been  trained properly with a great number of good quality training data.  

Deep learning methods have emerged as a promising approach for fast computation of the solutions of inverse problems, such as medical imaging \cite{CNN-IP-image,Deep-MIA} and electromagnetic inversion \cite{Deep-EI,Deep-EI-1d}. Nevertheless, these methods heavily rely on labeled data from solutions of the forward problem, rendering them inappropriate for inverse problems where such information is not present.

Physics-informed neural network (PINN) methods have been developed to regularize and generalize Deep learning methods by accounting for physical properties of the training data. These methods started to be used for inverse problems described by the ordinary or partial differential equations (ODE or PDE) \cite{PINN}. 
Since the starting point in 2017, \cite{Raisi}, extensive researches have demonstrated the efficiency of the concept, mainly for ODE and PDE models, both forward and inverse problems \cite{Quantify-PINN,PINN-solid,DeepXDE,B-PINN,Adver-PINN,PINNtomo,PINN-wavefield,A-PINN,Surrogate-PINN,psf2023009014}. However, their real efficiency for inverse problems have still to be proved. 

However, models based on invertible neural networks (INN) \cite{NICE, RNVP, Glow} have shown potential in Bayesian inference for inverse problems, due to their efficiency and accuracy advantages in both sampling and density estimation of complex target distributions through bijective transformations, as evidenced in the works of \cite{INN, INN-geo, cINN, BayesFlow, NFFs, Inv-DeepONets, L-HYDRA, VI-NFs}. However, still, certain limitations persist. 
For example, INN models in \cite{INN, INN-geo, cINN, BayesFlow} demand substantial quantities of labeled data for pre-training, which is often unavailable in many practical inverse problems. The invertible DeepONet introduced in \cite{Inv-DeepONets} alleviates the need for labeled data, but it entails an additional variational inference procedure to approximate the true posterior distribution.

In this paper, we focus on the Bayesian inference tool, and in a synthetic way, step by step, show the different steps of assigning prior probability distribution, and obtaining the expression of the posterior laws, first for the the desired unknowns, and the for the NN's parameters during the training. We see how we obtain the concept of PINN methods as special cases. We consider two cases of supervised and unsupervised cases in details. 

%\section{Basics of PINN and Bayesian framework for inverse problems}

\section{Proposed Bayesian PINN framework}
We introduce the Bayesian PINN framework by following five steps: 
\bit
\item 
First step is to assume that we have a forward or generative model:
\beq
\bgb=\Hb(\rfb)+\epsilonb
\eeq
where $\rfb$ is the input, $\bgb$ the output, $\Hb$ the forward model, and $\epsilonb$ the errors, both measurement and modeling. For a linear model, we have: $\bgb=\Hb\rfb+\epsilonb$. 

\item 
Second step is to assign priors $p(\rfb)$ and $p(\epsilonb)$. From this second one, and the forward model, we can deduce the likelihood 
$p(\bgb|\rfb)$. Then, we can use the Bayes rule to obtain:  
\beq
p(\rfb|\bgb)=\frac{p(\bgb|\rfb) p(\rfb)}{p(\bgb)}
\eeq
For the case of linear inverse problem $\bgb=\Hb\rfb+\epsilonb$, Gaussian likelihooh $p(\bgb|\rfb)=\Nc(\bgb|\Hb\rfb,\bve\Ib)$, and Gaussian prior 
$p(\rfb)=\Nc(\rfb|\bar{\fb},\bvf\Ib)$, the posterior $p(\rfb|\bgb)$ is also Gaussian: 
\beq
p(\rfb|\bgb)=\Nc(\rfb|\rfbh,\rSigmabh)
\eeq
with
\beq
\barr{l}
\rfbh=[\Hb'\Hb+\lambda\Ib]^{-1}\Hb'(\bgb-\Hb\bar{\fb})\\\
\rSigmabh=\bve[\Hb'\Hb+\lambda\Ib]^{-1}, \quad \lambda=\frac{\bve}{\bvf}
\earr
\eeq

\item 
Third step is to design an appropriate neural network which, takes as input $\bgb$ and as output $\rfbnn$. This NN can be considered as a proxy, surrogate or approximate inversion, such that its output $\rfbnn$ can be very near to the true or ground truth $\rfb$. 

\item 
Fourth step is training the NN. We distinguish two cases: 
\bit
\item Supervised case where we have a set of outputs-inputs $\{\bgb_{Ti}, \bfb_{Ti}\}$, and 
\item Unsupervised case where the available data are only $\{\bgb_{Ti}\}$. 
\eit
The parameters of the NN, $\rwb$ are estimated at the end of training step. 

\item 
Fifth step is using the trained NN, giving it any other input $\bgb_j$ which produces the outputs $\rfbh_j$ which we hope to be not very far from the ground truth $\rfb_j$. We may also want to be able to quantify the its uncertainty. 
\eit
To go more in details, first we consider the case of supervised case, and then the unsupervised case. 

\subsection{Supervized training data}

First we consider the supervised case, i.e., when we have a set of labeled  training data, (output-input of the generating forward system) : 
$\{\bgb_{Ti}, \bfb_{Ti}\}$. 
\begin{align*}
\btabu{@{}c@{}} Data \\ $\{\bgb_{Ti}, \bfb_{Ti}\}$\etabu
\Ra\fbox{~NN ($\rwb$)~}\Ra\rfb_{NNi}\Ra\fbox{~$\Hb$~}\Ra\rgbh_{NNi} \\ 
\end{align*}
To write, step by step, the relations and equations, we follow: 
\bit
\item We assume that, given the true $\rfb$ and the forward model 
$\Hb$, the the data $\{\bgb_{Ti}\}$ are generated independently:
\beq
p(\{\bgb_{Ti}\}|\{\rfb_i\})=\prod_i p(\bgb_{Ti}|\rfb_i),
\eeq 
and we can assign the likelihood $p(\bgb_{Ti}|\rfb_i)$, for example, the Gaussian:  
\beq
p(\bgb_{Ti}|\rfb_i,\bve_i\Ib) = \Nc(\bgb_{Ti}|\Hb\rfb_i,\bve_i\Ib).
\eeq
The same for $\{\bfb_{Ti}\}$, we assume that, given the true $\rfb$, we have:
\beq
p(\{\bfb_{Ti}\}|\{\rfb_i\})=\prod_i p(\bfb_{Ti}|\rfb_i)=\prod_i \Nc(\bfb_{Ti}|\rfb_i,\bvf_i\Ib)
\eeq 

\item We also assign a prior $p(\rfb)$ to the true $\rfb$ to translate our prior knowledge about it. 

\item Using the Bayes rule we get: 
\beq
p(\rfb_i|\{\bgb_{Ti}, \bfb_{Ti}\}) \propto \prod_i p(\bgb_{Ti}|\rfb_i,\bvf_i\Ib) \, p(\bfb_{Ti}|\rfb_i,\bvf_i\Ib) \, p(\rfb_i) 
\eeq

\item With the Gaussian cases of the equations [5,6], we can rewrite the posterior as: 
\begin{align}
p(\rfb_i|\{\bgb_{Ti},\bfb_{Ti}\}) &\propto \expf{-J(\rfb_i)} \nonumber \\ 
\mbox{with:~~} &
J(\rfb_i)=
\sum_i\frac{1}{2\bve_i}\|\bgb_{Ti}-\Hb\rfb_i\|^2 +\frac{1}{2\bvf_i}\|\bfb_{Ti}-\rfb_i\|^2 + \ln p(\rfb_i)
\end{align}
\item Choosing also a Gaussian prior: $p(\rfb_i)=\Nc(\rfb_i|\bar{\bfb},\bvi\Ib)$, we get: 
\beq
J(\rfb_i)=
\sum_i 
\frac{1}{2\bvf_i} \|\bfb_{Ti}-\rfb_i\|^2 + 
\frac{1}{2\bve_i}\|\bgb_{Ti}-\Hb\rfb_i\|^2 + 
\frac{1}{2\bvi} \|\rfb_i-\bar{\bfb}\|^2
\eeq
It is then easy to see that the $p(\rfb_i|\{\bgb_{Ti},\bfb_{Ti}\})$ is Gaussian: 
\beq
p(\rfb_i|\{\bgb_{Ti},\bfb_{Ti}\})=\Nc(\rfb_i|\rfbh_i,\rSigmabh_i)
\eeq
with
\[
\barr{l}
\rfbh_i=\sum_i [\Hb'\Hb+\lambda_i\Ib]^{-1}\Hb'(\bgb_{Ti}-\Hb\bfb_{Ti}-\Hb\bar{\rfb})\\\
\rSigmabh_i=\sum_i\bve_i[\Hb'\Hb+\lambda_i\Ib]^{-1}, \quad 
\lambda_i=\frac{\bve_i}{\bvf_i}
\earr
\]
\item Now, we consider the training step of the NN where we note the output of the neural network $\rfb_{NNi}$ which is a function of NN parameters $\rwb$ and the input data $\{\bgb_{Ti}\}$, we can define the following optimization criterion: 
\beq
J(\rwb)=
\sum_i \frac{1}{\bvf_i} \|\bfb_{Ti}-\rfb_{NNi}(\rwb)\|^2 + \frac{1}{\bve_i}\|\bgb_{Ti}-\Hb\rfb_{NNi}(\rwb)\|^2 + \frac{1}{\bve_i}\|\bar{\bfb} - \rfb_{NNi}(\rwb)\|^2 
\eeq
This criterion can be considered as a physics based or physics informed neural network (PINN), 
where the classical output residual part is: 
\beq
J_{NN}(\rwb)= \sum_i\frac{1}{2\bve_i} \|\bfb_{Ti}-\rfb_{NNi}(\rwb)\|^2
\eeq 
and the physics informed part is:
\beq
J_{PI}(\rwb)=\frac{1}{2\bvf_i} \|\bgb_{Ti}-\Hb\rfb_{NNi}(\rwb)\|^2+ \frac{1}{2\bvi}\|\bar{\bfb} - \rfb_{NNi}(\rwb)\|^2 
\eeq
\item We can also consider 
\beq
p(\rwb|\{\bgb_{Ti},\bfb_{Ti}\}) \propto \expf{-J(\rwb) - \ln p(\rwb)}
\eeq
and use it as the posterior probability distribution of the NN's parameter $\rwb$ given the training data $\{\bgb_{Ti},\bfb_{Ti}\}$. A specific choice for the prior $p(\rwb)$ can be one of the sparsity enforcing priors, such as 
\beq
p(\rwb) \propto \expf{-\gamma \|\rwb\|_\beta^\beta}
\eeq
However, in practice, the full expression of this posterior is very difficult to obtain, and even its optimization can be more difficult if the NN contains nonlinear activation function. 
\eit

\subsection{Unsupervised training data}
In the unsupervised case, we only have have a set of training data: $\{\bgb_{Ti}\}$. The classical NN methods can not be applied as there is not any reference data. The main advantage of the PINN is exactly in the fact that, it can be applied in this case. The schematic for this case is almost the same, but we have only for training data $\{\bgb_{Ti}\}$: 
\begin{align*}
\btabu{@{}c@{}} Data \\ $\{\bgb_{Ti}\}$\etabu
\Ra\fbox{~NN ($\rwb$)~}\Ra\rfb_{NNi}\Ra\fbox{~$\Hb$~}\Ra\rgbh_{NNi} \\ 
\end{align*}
However, we can still follow the same steps we had for the supervised case:  
\bit
\item We assume that, given the true $\rfb$ and the forward model $\Hb$, the the data $\{\bgb_{Ti}\}$ are independent, and follow a Gaussian distribution: 
\beq
p(\{\bgb_{Ti}\}|\rfb_i) = \sum_i \Nc(\bgb_{Ti}|\Hb\rfb_i,\bve_i\Ib)\propto\expf{-\frac{1}{2\bve_i}\|\bgb_{Ti}-\Hb\rfb_i\|^2}
\eeq
\item We also assign a prior $p(\rfb)$ to the true $\rfb$ to translate our prior knowledge about it. This time, we may use a more informative prior, such as Tikhonov, Total Variation or any other sparsity enforcing prior 
\beq
p(\rfb_i)\propto \expf{-\gamma\|\Db \rfb_i\|_\beta^\beta}
\eeq
\item Using the Bayes rule we get: 
\begin{align}
p(\rfb_i|\{\bgb_{Ti}\}) &\propto \expf{-J(\rfb_i)} \nonumber\\ 
\mbox{with:~~} 
J(\rfb_i)&=\sum_i \frac{1}{\bve_i}\|\bgb_{Ti}-\Hb\rfb_i\|^2 +
\gamma\|\Db\rfb_{Ti}\|_\beta^\beta
\end{align}
\item Again, as in the supervised case, we can consider 
\begin{align}
p(\rwb|\{\bgb_{Ti}\}) &\propto \expf{-J(\rwb)} \nonumber\\ 
\mbox{with:~~} 
J(\rwb)&=\sum_i \frac{1}{\bve_i}\|\bgb_{Ti}-\Hb\rfb_i(\rwb)\|^2 +
\gamma\|\Db\rfb_{Ti}(\rwb)\|_\beta^\beta +
\gamma_w\|\rwb\|_{\beta_w}^{\beta_w}
\end{align}
which can be used for the estimation of the NN's parameters. 
\eit

In both supervised and unsupervised cases, we still have to choose an appropriate structure for the NN, its depth, its number of hidden variable, as well as choosing appropriate optimization algorithms, it learning rate, etc. These difficulties make the implementation of such methods not very easy and many experience are needed to implement, to train and to use them in real applications. 
For a discussion on the choice of the structure of the NN refers to \cite{psf2023009014,proceedings2019033016,} 

\section{Conclusions}
As discussed, inverse problems arise in many science and engineering applications where we need to infer on a quantity from indirect observation. The main application cases are medical, biomedical, geophysics, and industrial non destructive testing and diagnostics using imaging systems.

In this work, a new Bayesian framework for the concept of PINN (BPINN) is presented and discussed which includes the deterministic one if we use the Maximum A Posteriori (MAP) estimation framework. 

We considered two cases of supervised and unsupervised training steps and drived, first the expressions of the posterior of the unknown desired variables, and then deduced the posterior law of the NN's parameters. However, the full use of these expressions in real applications still need a great number of challenges: the choice of the structure of the NN, its effective implementation, choice of parameters of the optimization criteria, as well as all the parameters during the learning step of the NN. 

\section*{References}
\bibliographystyle{plain}
\bibliography{refs,references,entropy-v08-i02_20250218}

\end{document}


\bigskip
\noindent {\bf Main References:}
\small 
\begin{itemize}
\item[1] Deep Learning and Inverse Problems [arxiv:2309.00802]
\item[2] Deep Learning and Bayesian inference for Inverse Problems [arxiv:2308.15492]
\item[3] Raissi M, Perdikaris P, Karniadakis GE. A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686-707 (2019), [arXiv:1711.10561], and [arXiv:1711.10566]
\item[4] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang and Liu Yang, Physics informed machine learning, Nature Reviews Physics 3, 422â€“440 (2021)
\end{itemize}
