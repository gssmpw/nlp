%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Bridging the Gap in XAI—Why Reliable Metrics Matter for Explainability and Compliance}

\begin{document}

\twocolumn[
\icmltitle{Bridging the Gap in XAI—The Need for Reliable Metrics in Explainability and Compliance}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pratinav Seth}{comp}
\icmlauthor{Vinay Kumar Sankarapu}{comp}
\end{icmlauthorlist}
\icmlaffiliation{comp}{AryaXAI (Arya.ai), Mumbai, India}
\icmlcorrespondingauthor{Pratinav Seth}{pratinav.seth@arya.ai}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This position paper \textbf{emphasizes the critical gap in the evaluation of Explainable AI (XAI) due to the lack of standardized and reliable metrics, which diminishes its practical value, trustworthiness, and ability to meet regulatory requirements}. 
Current evaluation methods are often fragmented, subjective, and biased, making them prone to manipulation and complicating the assessment of complex models.  A central issue is the absence of a ground truth for explanations, complicating comparisons across various XAI approaches.
To address these challenges, we advocate for widespread research into developing robust, context-sensitive evaluation metrics. These metrics should be resistant to manipulation, relevant to each use case, and based on human judgment and real-world applicability. We also recommend creating domain-specific evaluation benchmarks that align with the user and regulatory needs of sectors such as healthcare and finance. By encouraging collaboration among academia, industry, and regulators, we can create standards that balance flexibility and consistency, ensuring XAI explanations are meaningful, trustworthy, and compliant with evolving regulations.
\end{abstract}

\section{Introduction}
In today's era, the use of AI models and deep learning is no longer a distant future but a present-day reality \cite{chatgpt,Touvron2023LLaMAOA}, with applications spanning across our everyday lives and critical sectors such as healthcare, finance, and law enforcement. These systems are increasingly influencing high-stakes decisions, and their impact is undeniable. As AI plays a central role in shaping these decisions, there is a growing need to ensure that these systems are transparent and their decision-making is explainable to ensure that the decisions are correctly thought and meaningful. 
Similarly, AI safety research underscores that explainability helps identify and mitigate risks \cite{Amodei2016ConcretePI, doi:10.1073/pnas.1611835114, Leike2018ScalableAA}.
Despite significant advancements in Explainable AI (XAI), a critical gap persists: \textbf{the absence of standardized, reliable metrics to evaluate the effectiveness and trustworthiness of AI explanations}.

XAI aims to bridge the gap between complex machine learning models and end-users, particularly in domains where decisions have compliance, ethical, legal, or social implications \cite{Singh2024RethinkingII, Kaur2022SensibleAR, Tull2024TowardsCI}. However, the current landscape of XAI evaluation is fragmented, with metrics used to assess model interpretability and explanation quality remaining inconsistent and subjective \cite{madsen2024interpretabilityneedsnewparadigm}. The lack of standardization holds back progress in XAI, as it allows for manipulation of results when comparing other XAI methods across different tasks, reducing the reliability of outcomes \cite{wickstrøm2024flexibilitymanipulationslipperyslope,Hedstrm2023TheMP}. To make XAI effective in high-risk settings, it is crucial to establish clear and reliable metrics that measure important aspects like fidelity, robustness, and usability.

Regulatory frameworks such as the European Union AI Act \cite{europaRegulation20241689,10489989} emphasize transparency and accountability, especially for high-risk AI systems. Article 13 of the EU AI Act requires these systems to be transparent and interpretable\cite{Fresz_2024}. This makes it clear that there is a strong need for standardized evaluation metrics that assess the technical performance of XAI methods and meet legal and ethical standards \cite{j5010010,10.1145/3593013.3594069}. Without these metrics, it will be difficult to ensure that XAI systems comply with regulatory requirements, which could slow the adoption of AI in critical sectors.

This position paper argues that \textbf{addressing the gap in reliable XAI metrics is crucial for advancing both the technical development and regulatory compliance of explainable AI}. The reliance on subjective evaluation methods and their vulnerability to manipulation \cite{wickstrøm2024flexibilitymanipulationslipperyslope} impedes progress in this domain. Additionally, advanced models, such as large language models (LLMs), which exhibit unprecedented complexity, require new scalable evaluation methods that reflect their unique characteristics.

Through this work, we aim to \textbf{highlight the need for the development of metrics that are resistant to manipulation, relevant to specific use cases, and practical in real-world settings. By creating core metrics that are robust and difficult to manipulate and adjusting them to meet regulatory needs, we can build a more consistent and useful framework for evaluating XAI systems}. Collaboration between researchers, industry, and regulators will ensure that XAI explanations are meaningful, trustworthy, and compliant with changing regulations while balancing flexibility with consistency.

\section{Background and Context}
Explainable AI (XAI) is an essential field of research focused on making the decision-making processes of complex machine learning (ML) models more transparent and understandable \cite{arrieta2019explainableartificialintelligencexai}. In high-impact areas like healthcare, finance, and law enforcement, where AI-driven decisions can have profound ethical, legal, and societal consequences, the ability to explain how AI systems arrive at their conclusions is crucial \cite{gohel2021explainableaicurrentstatus}. 

The challenge lies not just in developing effective and accurate models but in ensuring that these models are transparent, accountable, and provide comprehensible explanations, especially when their decisions impact high-stakes outcomes \cite{jia2022roleexplainabilityassuringsafety,atakishiyev2025safetyimplicationsexplainableartificial}. In these domains, the consequences of AI decisions are significant, making the need for interpretability and explanation all the more urgent \cite{fresz2024contributionxaisafedevelopment}.

\subsection{Interpretability, Explainability, and Feature attribution}
Interpretability, explainability, and feature attribution are closely related concepts in XAI, often used interchangeably, but they serve distinct purposes \citep{Doshi-Velez2017a}. Interpretability refers to how easily a human can understand a model’s decision-making process. Simpler models, like decision trees, are interpretable, while complex models, such as deep neural networks, are more difficult to interpret. Explainability is about providing understandable reasons for a model's predictions, often using methods like LIME \cite{Ribeiro2016WhySI} or SHAP \cite{Lundberg2017AUA} to make complex models more comprehensible. Feature attribution identifies the contribution of each input feature to the model’s prediction, showing which factors were most influential. Although these terms are sometimes used interchangeably, they each focus on different aspects of making AI decisions understandable and transparent, ultimately contributing to the broader goal of improving accountability and trust in AI systems.

\subsection{Explainability Methods}

\subsubsection{Intrinsic Explainability}
Intrinsic explainability \citep{Lipton2018,Rudin2019} refers to models that are inherently interpretable by design. These models are constructed to be transparent and easily understood from the outset, such as decision trees, linear regression, or rule-based systems. The key advantage of intrinsic explainability is that the model’s decision-making process is directly accessible and traceable by the user without needing additional explanation tools. For example, in a decision tree, one can directly follow the path of the decision-making process based on the input features. The trade-off, however, is that these models tend to be simpler and may sacrifice predictive accuracy for interpretability. 

In the current deep learning era, while models are designed to be intrinsically explainable, this claim and their faithfulness should still be questioned \citep{Jacovi2020}, as many inherently interpretable model ideas are later revealed not to provide faithful explanations. For example, attention-based explanations have received notable criticism for not being faithful \citep{Jain2019,Serrano2019,Vashishth2019,Meister2021a,Madsen2022,madsen2024interpretabilityneedsnewparadigm} 

\subsubsection{Post Hoc Explainability}
Post hoc explainability \citep{Lipton2018,Madsen2021} refers to methods that generate explanations for a model’s predictions after it has been trained without altering the internal structure of the model. These techniques offer flexibility, as they can be applied to a variety of pre-existing models, making them widely used in practice. 

Notable examples of post hoc explainability methods include Shapley Additive exPlanations (SHAP) \cite{Lundberg2017AUA}, which attribute the output of a model to its input features by computing the contribution of each feature to the model's prediction, offering a clear understanding of the model's decision-making. 
Local Interpretable Model-agnostic Explanations (LIME) \cite{Ribeiro2016WhySI} provides explanations by approximating the model locally with interpretable surrogate models, making complex models more understandable in specific instances. 
Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{Selvaraju2016GradCAMVE} visualizes the impact of various regions of input (such as parts of an image) on the model's predictions, offering visual insight into decision-relevant features. 
Integrated Gradients \cite{Sundararajan2017AxiomaticAF} offers a method to quantify the contribution of each input feature by integrating gradients of the model's output with respect to input features along a path from a baseline to the input. This method approximates a model’s behavior more faithfully, particularly for deep learning models. 

While these methods have proven useful, they come with significant limitations. Their reliance on approximations can lead to explanations that may not fully capture the complexity of a model’s decision-making process. Additionally, post hoc methods are often vulnerable to inconsistency, where slight variations in input data can produce widely differing explanations. Furthermore, they are susceptible to adversarial manipulations that can distort explanations, diminishing their reliability and trustworthiness \cite{slack2020foolinglimeshapadversarial}. Also, this paradigm has no control over the model, and achieving faithful explanations sometimes becomes challenging \cite{madsen2024interpretabilityneedsnewparadigm}.

\subsubsection{Other Methods}
In addition to intrinsic and post hoc explainability, other methods offer deeper insights into model behavior. Ante-hoc explainability \cite{sarkar2021frameworklearningantehocexplainable} ensures transparency in the data and learning process from the start. Example-based explainability, like counterfactual explanations, compares model predictions to similar examples for clarity. Interactive explainability \cite{slack2023talktomodelexplainingmachinelearning} allows users to tweak inputs and see how predictions change in real-time. Causal explainability \cite{carloni2023rolecausalityexplainableartificial} identifies how changes in features causally affect outcomes. Fairness and bias explainability \cite{Zhou2022} helps detect and address biases in decision-making. Model-specific explainability \cite{ALI2023101805} uses tailored methods for specific models, such as visualizations for decision trees. Lastly, mechanistic interpretability \cite{olah2020zoom,Anthropic,Nanda2023ProgressMF} reverse-engineers a model’s internal workings to understand how individual components contribute to its predictions.

\subsection{The Role of Evaluation Metrics in XAI}
Recent advancements in the quantitative analysis of XAI explanations have provided researchers with a broad set of evaluation metrics to work with~\cite{anna, agarwal2022openxai}. These metrics are key for evaluating the reliability and effectiveness of XAI methods \cite{10.1007/978-3-031-20319-0_30,inproceedingxaimetrs}. They help researchers and practitioners assess how well an explanation reflects the model’s decision-making process and whether it meets key requirements like transparency, robustness, and usability. Since ground truth explanations are unavailable, researchers focus on quantifying the quality of an explanation by measuring its desired properties. 

Over time, it has become clear that most XAI metrics can be grouped into one of six categories:

\begin{itemize}
\item \textbf{Faithfulness:} Metrics measure how well an explanation reflects the model’s true decision-making process. Faithfulness ensures that the explanation accurately represents the internal mechanics of the model and the extent to which it aligns with the actual predictions made by the model. \cite{Bhatt2020EvaluatingAA,AlvarezMelis2018TowardsRI,Bach2015-ct,rong2022consistentefficientevaluationstrategy,yeh2019infidelitysensitivityexplanations,arya2019explanationdoesfitall}
\item \textbf{Robustness:} Metrics that evaluate the stability and consistency of explanations under varying inputs, including adversarial attacks and perturbations. A robust explanation should maintain its integrity across a range of test conditions and be resistant to manipulations that could distort its meaning.(\cite{Yeh2019OnT,alvarezmelis2018robustnessinterpretabilitymethods,agarwal2022rethinkingstabilityattributionbasedexplanations,dasgupta2022frameworkevaluatingfaithfulnesslocal})
\item \textbf{Localisation:} Metrics that assess the ability of an explanation to highlight relevant regions or features in the input data that most influence the model's decision. For example, in image data, localization metrics measure how well the explanation identifies the specific areas of the image that contribute to the model’s prediction. (\cite{kohlbrenner2020bestpracticeexplainingneural,theiner2021interpretablesemanticphotogeolocation,Arras_2022,ariasduart2022focusratingxaimethods}
\item \textbf{Complexity:} Metrics that evaluate the simplicity and comprehensibility of an explanation. A good explanation should be simple enough for end-users to understand and use, avoiding unnecessary complexity that could hinder decision-making. \cite{chalasani2020conciseexplanationsneuralnetworks,bhatt2020evaluatingaggregatingfeaturebasedmodel,nguyen2020quantitativeaspectsmodelinterpretability}
\item \textbf{Randomisation (Sensitivity):} Metrics that examine the sensitivity of explanations to changes in input data or model parameters. These metrics ensure that the explanation does not overly rely on trivial input variations and that it remains stable when subjected to small perturbations. \cite{adebayo2020sanitycheckssaliencymaps,sixt2024explanationsliemodifiedbp,Hedstrm2024SanityCR}
\item \textbf{Axiomatic Metrics:} Metrics that evaluate the inherent properties of the explanation method itself, such as consistency, completeness, and the preservation of important properties across different model architectures or tasks. These metrics are grounded in the theoretical foundations of XAI and ensure that explanation techniques adhere to desired formal properties. \cite{sundararajan2017axiomaticattributiondeepnetworks,nguyen2020quantitativeaspectsmodelinterpretability,kindermans2017unreliabilitysaliencymethods}
\end{itemize}

\subsection{Existing Evaluation Frameworks and Benchmarking Tools for Model Explainability}

Even though Explainable AI (XAI) has made strides, current evaluation frameworks still face significant issues. Many existing metrics don’t capture the full complexity of real-world machine learning models. Most benchmarks aren’t flexible enough to work across different domains or tasks. \citet{madsen2024interpretabilityneedsnewparadigm} suggests we need to shift the focus toward evaluating things like robustness, generalizability, and actionability, in addition to just interpretability. Similarly, \citet{wickstrøm2024flexibilitymanipulationslipperyslope} points out that current metrics fail to account for how models can evolve over time.

M-4 Benchmark \cite{li2023mathcalm} and OpenXAI \cite{agarwal2024openxaitransparentevaluationmodel} try to address these gaps, but they still have drawbacks. For instance, M4 focuses on faithfulness without considering robustness, OpenXAI relies on synthetic data, and Quantus has trouble aligning explanations with human judgment. Quantus \cite{hedström2023quantusexplainableaitoolkit} is built for evaluating explainability in image classification models. Other tools like FairX \cite{sikder2024fairxcomprehensivebenchmarkingtool}, Captum \cite{kokhlikyan2020captumunifiedgenericmodel}, and TF-Explain \cite{Meudec2021-le} focus on fairness and attribution but don’t offer standardized ways to compare across different models.
Specialized Libraries like Ferret \cite{attanasio-etal-2023-ferret}, and Inseq \cite{Sarti_2023} also offer useful contributions. Ferret looks at post-hoc methods like SHAP and LIME but is limited to text-based models, while Inseq is focused on specific NLP sequence generation tasks.

\section{Challenges in XAI Metrics}

Evaluating XAI methods presents several challenges that hinder their reliability and adoption. These issues prevent the development of effective and universally applicable evaluation frameworks necessary to meet the expectations of regulators, risk managers, and users. Major problems include fragmentation, subjectivity, and the vulnerability of metrics to manipulation. Many existing metrics rely on multiple hyperparameters, yet little effort has been made to fine-tune them, creating opportunities for manipulation and reducing the reliability of evaluations \cite{wickstrøm2024flexibilitymanipulationslipperyslope,hoffman2019metricsexplainableaichallenges,inbff23fook}. As depicted in Figure~\ref{fig:fig1} the major challenges involved are : 

\subsection{Neglect of Modern AI Models}
Current XAI evaluation metrics struggle to capture the complexity of modern AI models, particularly large language models and autoregressive systems. These models rely on intricate decision-making processes and large-scale architectures, requiring more adaptable evaluation methods. While tools like Ferret \cite{attanasio-etal-2023-ferret} have improved interpretability for transformers, existing frameworks still fall short in analyzing behaviors and complex reasoning.

Explainability research has mainly focused on image and tabular modalities, with recent efforts extending to NLP. However, multi-modal AI systems are becoming increasingly common, yet most XAI methods remain single-modality focused, limiting their applicability to models processing text, images, and structured data together. Among post-hoc explanation methods, only a few, such as Layer-wise Relevance Propagation \cite{achtibat2024attnlrpattentionawarelayerwiserelevance}, and DLBacktrace \cite{sankarapu2024dlbacktracemodelagnosticexplainability}, extend to multi-modal settings, leaving a significant evaluation gap. 

As multi-modal AI adoption grows, the lack of standardized evaluation frameworks hinders interpretability and trustworthiness across domains. Existing XAI methods fail to capture dependencies between modalities. For instance, vision-based methods like Grad-CAM fail to explain text contributions in vision-language models, while SHAP and LIME overlook image-based reasoning. This limitation is particularly critical in applications such as medical AI, where decisions rely on both textual reports and diagnostic images. Without dedicated multi-modal explainability metrics, evaluating these models remains inconsistent and unreliable. Addressing this challenge requires new faithfulness metrics that measure explanation alignment across modalities, along with benchmarking datasets to establish industry-wide standards

\subsection{Fragmentation and Inconsistency}
\begin{figure}
    \centering
    \includegraphics[width=0.81\linewidth]{DIAGICML2.jpg}
    \caption{Circular Relationship of Challenges in XAI Metrics. The key issues in evaluating explainability—Neglect of Modern Models, Fragmentation \& Inconsistency, and Manipulation Vulnerabilities—are interdependent. The absence of evaluation frameworks for multi-modal AI and LLMs leads to inconsistent methodologies, allowing for manipulation through hyperparameter tuning and adversarial exploits. These vulnerabilities further reinforce the neglect of modern AI models, forming a self-perpetuating cycle that hinders progress in explainability evaluation.}
    \label{fig:fig1}
\end{figure}

A key challenge is the fragmentation in XAI evaluation due to the lack of standardized frameworks. Different metrics are used across studies, making comparing results and drawing broad conclusions hard. This inconsistency slows progress and hinders the development of best practices. Without a unified system, the field lacks direction, limiting collaboration and advancements in XAI.\cite{Kim2024HumancenteredEO,app12199423}

\subsection{Manipulation Vulnerabilities}

XAI metrics are vulnerable to intentional or unintentional manipulation, which undermines trust in XAI systems and diminishes their practical value \cite{wickstrøm2024flexibilitymanipulationslipperyslope}. Some examples of this manipulation include adjusting evaluation parameters to achieve desired outcomes, thereby distorting the assessment of methods; optimizing explanations to perform well on specific metrics while not accurately reflecting the model’s true decision-making process; and using adversarial inputs to create explanations that seem robust but fail in real-world applications.

Recent research has increasingly focused on the role of hyperparameters in XAI evaluations and how they can introduce confounding effects~\cite{hedstrom2023metaquantus}. These studies vary in how they define dependent and independent variables and the specific hyperparameter space they examine—whether related to the model, the explanation, or the evaluation process. For example, research has explored how sensitive attribution methods are to explanation hyperparameters like random seed or sample size~\cite{bansal2020}, and how baseline choices (such as those in Integrated Gradients) impact explanation outcomes~\cite{sturmfels2020visualizing, integratedgradients}. 

Additionally, studies have examined how changes in model performance variables—such as optimizer, activation function, learning rate, or dataset split—can influence explanations~\cite{karimi2023on}. Other work has looked at the effects of model priors and random weight initialization on both explanations and evaluations~\cite{hase2021}. Research has also investigated the disagreement between different explanation methods, particularly concerning feature ranking~\cite{disagreement}, and the influence of baseline choices on these disagreements~\cite{Koenen_disagree}.

Furthermore, recent studies have delved into how sensitive evaluation outcomes are to hyperparameters, such as the effects of normalization, randomization order, and similarity measures on randomization metrics~\cite{BinCVPR23, sundararajan2018, sanity2024}. Faithfulness metrics have also been shown to be influenced by hyperparameters like baseline choices and the order of perturbation~\cite{blücher2024decoupling, samek2017, brunke, barnes, DolciCGCM23, rong22consistent, tomsett2020}.

\section{Key Requirements for Reliable Metrics}
\begin{figure*}
    \centering
    \includegraphics[width=0.6\linewidth]{DIAGICML1.jpg}
    \caption{The Pyramid Diagram of Key Requirements for Reliable XAI Metrics. The foundation of explainability starts with Transparency \& Robustness, ensuring stable and trustworthy explanations. Adaptability \& Tamper-Resistance ensures flexibility across industries while preventing manipulation. Scalability supports large-scale AI models efficiently without excessive computational costs. At the top, Regulatory \& Legal Aspects align XAI evaluation with frameworks such as the EU AI Act, ensuring compliance and legal justification for AI decision-making.}
    \label{fig:enter-label}
\end{figure*}
To overcome these challenges, reliable XAI metrics must be developed. These metrics should establish standard benchmarks for explainability, helping to compare, quantify, and qualify evaluation results. Standardization will clarify regulatory requirements, minimizing bias in user-preferred choices \cite{Nauta_2023,inbff23fook}. As depicted in Figure~\ref{fig:enter-label} Reliable metrics must meet the following criteria:

\subsection{Transparency \& Robustness}

XAI evaluation metrics must provide clear, consistent insights into how well an explanation aligns with the model’s decision-making process. 
Transparent metrics ensure that all stakeholders—developers, regulators, and end-users—can trust the results, leading to greater confidence in the system’s explanations. This transparency is vital for continuous improvements in explanations and the underlying models.
Metrics should assess the stability of explanations under various conditions, including adversarial inputs and data changes \cite{10.5555/3463952.3463962}. A robust metric ensures that the explanation holds up under real-world variations, offering consistency and reliability across different environments. This guarantees that XAI systems are resistant to manipulation and remain trustworthy.

\subsection{Adaptability \& Tamper-Resistance}

XAI frameworks should be flexible enough to cater to diverse domains, such as healthcare or finance, where priorities like interpretability or compliance vary \cite{10.5555/3463952.3463962}. Adaptable metrics ensure their applicability across different sectors, addressing the specific challenges of each. Moreover, these metrics must incorporate safeguards such as adversarial testing and regular validation to prevent manipulation. By "tamper-proof," we mean that the metrics cannot be manipulated by hyperparameters, ensuring that explanations are not artificially adjusted to meet predetermined standards, thus maintaining the integrity of evaluations.

\subsection{Scalability}

Scalable explainability metrics are critical for evaluating modern AI systems, particularly large-scale models such as LLMs. Existing explainability metrics are often computationally expensive, limiting their feasibility for large-scale deployments. As AI models grow in complexity, it is imperative to develop scalable methods that can provide meaningful explanations without excessive computational overhead. Efficient and lightweight explainability techniques will ensure that evaluation frameworks remain practical and adaptable, even for large and resource-intensive AI models. \cite{inprocefr3frfedings}

\subsection{Regulatory \& Legal Aspects}

XAI metrics must align with regulatory frameworks such as the EU AI Act, which mandates transparency and interpretability for high-risk AI systems \cite{j5010010,10489989}. These metrics must help organizations demonstrate compliance by providing clear, traceable, and auditable explanations. Additionally, they should be adaptable to evolving regulations to maintain compliance over time.

From a legal perspective, XAI metrics must ensure that AI decisions are explainable and justifiable in court\cite{inproceedingscomplainaceeuract}. Explanations should be transparent, fair, and accountable, particularly in healthcare and finance sectors where AI decisions have significant consequences. Legal frameworks require that explanations are accessible to affected individuals and can be used to contest or defend decisions \cite{Fresz_2024,bibal2020impactlegalrequirementsexplainability,nannini2023explainaYeh2019OnTbilityaipoliciescritical}.

\section{Alternative Views}
While the need for reliable and standardized XAI metrics is generally agreed upon, some alternative perspectives offer important considerations that should be acknowledged. These viewpoints present challenges and suggest nuanced approaches to developing XAI metrics \cite{hoffman2019metricsexplainableaichallenges}.

\subsection{Universal Metrics Are Unrealistic}

Critics argue that the diverse nature of AI applications makes the creation of universal XAI metrics impractical. The requirements for evaluating XAI in fields like medical imaging differ significantly from those in financial auditing. For instance, healthcare emphasizes interpretability and transparency for clinical decisions, while finance focuses more on compliance, fairness, and traceability. A one-size-fits-all approach could hinder innovation by failing to accommodate the specific needs of various industries, thus reducing the effectiveness of the evaluation process. \cite{inprocbhibijbijeedings}

\subsection{The Role of Human Judgment}

Another viewpoint stresses that subjective evaluations by domain experts are crucial in assessing XAI systems. While automated metrics provide valuable insights, they may miss the nuanced understanding and contextual relevance that human judgment offers. Human evaluators bring domain-specific expertise essential for interpreting explanations in real-world settings. This perspective advocates combining quantitative metrics with qualitative assessments from experienced experts to ensure comprehensive evaluations that reflect the complexities of real-world applications. \cite{r3gr3eginproceedings,colin2023ipredictiunderstand}

\subsection{A Hybrid Approach}

Rather than relying solely on universal metrics, a hybrid approach can balance standardization and the flexibility needed for domain-specific requirements. Core benchmarks for widely accepted properties, such as fidelity and robustness, should be established as a foundation for XAI evaluation. Simultaneously, domain-specific metrics can address unique needs in various fields. This hybrid approach can also incorporate human judgment, ensuring subjective insights are included without compromising the objectivity of the evaluation process. Combining standardized metrics with flexibility and human expertise can create a more robust, adaptable, and meaningful evaluation framework for XAI systems. \cite{ma2024humancentereddesignexplainableartificial}

\section{Call to Action}

To address the current gaps in XAI evaluation and ensure the development of reliable, transparent, and compliant AI systems, the XAI community must prioritize the following actions:

\begin{itemize}
    \item \textbf{Develop Core Benchmarks:} Establish standardized metrics that assess key aspects of explainability such as fidelity, robustness, clarity, and comprehensibility. These benchmarks will be a solid foundation for comparing and evaluating XAI methods across various domains and applications.

    \item \textbf{Address Challenges of Advanced Models:} Develop evaluation metrics that are scalable and adaptable to the growing complexity of AI systems, including large language models (LLMs). As AI models evolve, evaluation frameworks must keep pace with their intricate decision-making processes and emergent behaviors.

    \item \textbf{Ensure Metric Integrity:} Develop robust evaluation frameworks that incorporate mechanisms such as adversarial testing, regular validation, and consistency checks to prevent manipulation, specifically ensuring that results cannot be influenced by hyperparameters. These safeguards are essential to preserving the integrity and reliability of XAI evaluations in practical use.

    \item \textbf{Develop Domain-Specific Evaluation Metrics:} While core benchmarks are important, XAI metrics should be customized to address the specific needs of different industries. For example, healthcare metrics should prioritize fine-grained explainability with high confidence, while finance metrics should focus on compliance, fairness, and auditability. Tailored metrics will ensure that XAI systems meet various sectors' regulatory and operational standards.
    
    \item \textbf{Optimize Computational Efficiency:} Recent research on sustainable machine learning highlights the increasing computational burden of explainability methods, particularly for complex AI models. Many existing XAI techniques, such as perturbation-based approaches, impose significant resource constraints, making them impractical for large-scale models like LLMs. To facilitate broader adoption of XAI, the community must develop evaluation frameworks that balance computational efficiency with the quality and fidelity of explanations, ensuring that interpretability remains feasible even for highly sophisticated AI systems. \cite{JeanQuartier2023TheCO}
    
    \item \textbf{Promote Collaboration Across Sectors:} Strengthen collaboration between academia, industry, and regulatory bodies to create evaluation metrics that meet both technical and regulatory needs. This collaboration is vital to ensure that theoretical advancements in XAI align with real-world applications, particularly in high-risk areas like healthcare and finance. \cite{kclBeyondSilos,Lund2025-ig}
\end{itemize}
%By taking these steps, the XAI community will help ensure that explainable AI systems are not only technically reliable but also meet the ethical and regulatory standards needed for broad adoption. Collaboration, innovation, and rigorous evaluation standards will be essential for making AI systems more transparent, accountable, and trustworthy.

\section{Broader Implications}
\begin{figure}[pt]
    \centering
    \includegraphics[width=0.99\linewidth]{DIAGICML.jpg}
    \caption{The Circular Feedback Loop of XAI Regulation and Compliance. Regulatory standards define compliance requirements for XAI evaluation metrics, which shape AI models and explainability methods. These models must demonstrate compliance before deployment in high-stakes domains like healthcare and finance. Real-world adoption and challenges then feed back into regulatory frameworks, refining evaluation standards and ensuring alignment with evolving AI capabilities.}
    \label{fig:fig2}
\end{figure}
Establishing reliable XAI metrics holds broad implications that extend beyond the technical realm, influencing various aspects of AI deployment, regulation, and societal trust. These include:

\subsection{Regulatory Compliance}

Reliable XAI metrics can be pivotal in ensuring that AI systems comply with legal frameworks, such as the European Union’s AI Act \cite{europaRegulation20241689,Sovrano_2020}. By establishing objective criteria for evaluating explainability, these metrics help organizations demonstrate that their AI systems meet the transparency and accountability requirements mandated by regulation. This compliance reduces the risk of regulatory penalties and fosters trust among stakeholders, facilitating the responsible deployment of AI systems in high-risk domains. \cite{Lund2025-ig}

\subsection{Auditablity and traceability}

Such evaluation metrics can provide a reference to quantify the auditability of explanations; unlike today, any explanation is acceptable, as there are no quantification metrics. Auditors and model evaluators can use these metrics to evaluate the quality of the explanations. \cite{Li2024-bv,McCormack2024-zw,Toader2019-jj}

\subsection{Building Trust}

Developing clear, consistent, and transparent evaluation standards enhances user confidence in XAI systems\cite{Sovrano_2021}. In high-stakes domains, where trust is a prerequisite for adoption, reliable metrics demonstrating AI systems' transparency and reliability are essential. By providing stakeholders with concrete evidence of the model’s interpretability and decision-making process, these metrics help to bridge the trust gap between AI systems and the end-users who depend on them.

\subsection{Advancing Ethical AI}

Metrics that prioritize fairness, robustness, and transparency contribute to the broader goal of advancing ethical AI. By aligning the development of XAI with societal values, these metrics help ensure that AI systems are effective and serve the public good. As AI systems increasingly influence critical areas such as healthcare, finance, and justice, it is essential that their decision-making processes are understandable, fair, and accountable. XAI metrics that uphold these principles will support the creation of AI systems that promote equity, fairness, and transparency, advancing the ethical deployment of AI technologies. \cite{Nannini2024-zq,Akhtar2024-gb}

\subsection{Fostering Innovation}
Standardized evaluation frameworks create a shared foundation for researchers and developers, enabling more effective collaboration, comparison, and innovation within the XAI community. With consistent benchmarks, researchers can compare different XAI methods more easily, identify best practices, and build on each other's work. This fosters a more dynamic and productive research ecosystem, accelerating the development of new methods and techniques in the field of XAI. Regulatory frameworks do not operate in isolation; they continuously evolve based on the capabilities of explainability methods and their impact on real-world AI deployment. As shown in Figure~\ref{fig:fig2}, this feedback loop ensures that AI regulations stay relevant while explainability techniques remain aligned with compliance needs.

\section{Conclusion}

Reliable and standardized XAI evaluation metrics are crucial for AI transparency, trustworthiness, and regulatory compliance. Despite progress in explainability methods, evaluation remains fragmented, subjective, and prone to manipulation. Key challenges include:

\begin{itemize}
    \item \textbf{Lack of standardization:} Inconsistent faithfulness, robustness, and fairness metrics hinder reliability.
    \item \textbf{Manipulation risks:} Flexible evaluation methodologies allow misleading explainability claims.
    \item \textbf{Limited multi-modal support:} Current metrics focus on single-modality models, neglecting multi-modal AI.
    \item \textbf{Regulatory misalignment:} Explainability requirements in AI regulations lack a unified compliance framework.
\end{itemize}

Without rigorous evaluation, explainability risks becoming a mere regulatory formality. To address these gaps, XAI evaluation must be scientifically grounded, manipulation-resistant, and adaptable across AI modalities. This requires:

\begin{itemize}
    \item \textbf{Standardized XAI metrics:} Ensuring reliable and reproducible evaluation.
    \item \textbf{Multi-modal evaluation benchmarks:} Supporting explainability across diverse AI systems.
    \item \textbf{Regulatory-aligned frameworks:} Integrating compliance into evaluation practices.
\end{itemize}

Collaboration among researchers, industry, and policymakers is essential to establish transparent, robust, and enforceable XAI metrics. Advancing beyond theoretical explainability towards standardized, practical evaluation frameworks will ensure AI systems are both accountable and compliant.

\section{Impact Statement}

The development of reliable, standardized evaluation metrics for Explainable AI (XAI) will profoundly impact the field of artificial intelligence and its integration into high-stakes applications. By ensuring that XAI systems are transparent, trustworthy, and compliant with evolving regulations, these metrics will foster broader adoption of AI technologies across critical sectors such as healthcare, finance, and law enforcement.

Reliable XAI metrics will empower developers to create models that are not only accurate but also interpretable and justifiable, thereby enhancing decision-making processes and reducing risks associated with AI-driven decisions. Furthermore, these metrics will help ensure that AI systems align with ethical standards, making them more acceptable to stakeholders, regulators, and the public.

By facilitating the creation of more effective and reliable XAI methods, these efforts will contribute to developing AI systems that are not only technically sophisticated but also ethical, fair, and accountable. Ultimately, establishing robust XAI evaluation standards will play a pivotal role in advancing the responsible deployment of AI technologies, ensuring that they serve the public good while minimizing unintended consequences.


\bibliography{example_paper}
\bibliographystyle{icml2025}
\end{document}
