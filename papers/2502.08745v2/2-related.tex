\subsection{Evaluation on Instruction Following}

The ability to follow instructions is a crucial assessment of instruction-tuned LMs. Early research in this area adopted a straightforward approach that leverages an expert LM (\textit{e.g.}, GPT-4) to holistically judge the quality of a model's response to an instruction \citep{alpacafarm,mtbench}. More recent work focused on disentangling instruction-following evaluation from other factors, such as response detailedness and factuality, by proposing more fine-grained assessments on whether the response adheres to the constraints specified in the user query. Some studies, for instance, required LMs to follow strict rules regarding response formats \citep{ifeval,RuLES,instruction_verbalizer}, while others designed case-specific constraints to regulate the content of model outputs \citep{followbench,infobench,composition-instruction-eval}. Recent studies also explored scenarios where instructions are embedded within the task input to assess whether LMs can correctly differentiate between instructions and data \citep{SEP,BIPIA}. In contrast to these works in the general domain, researchers in LM safety focused on whether models can effectively reject malicious instructions, whether directly provided by attackers \citep{HackAprompt,TensorTrust} or injected via external information \citep{injecagent,Agentdojo}. Despite the rich amount of work in this area, none of them systematically analyzed the LM's ability in the instruction hierarchy. Notably, \benchmark\ includes various scenarios where LMs face hierarchical inputs, especially those with conflicting instructions, bridging a gap in the current evaluation of LMs.

\subsection{System Prompts in LMs}

System prompts \citep{system-prompt} are commonly employed to guide LMs' behavior from a high level. System prompts typically define the LM's role, task, output format, and safety guidelines, all of which are intended to be followed throughout the entire interaction. In many models, system prompts have been introduced as a separate input field from the user instruction \citep{GPT4,llama2,orca}, but details about its training process -- such as the types and diversity of system prompts used -- are rarely disclosed. Subsequent research demonstrated that system prompts can be used to improve the performance of LMs in general-domain instruction following \citep{social-roles-system-prompt}, personalized response generation \citep{align-thousands-preferences}, rule adherence \citep{SoFA}, and defending jailbreaks \citep{system-prompt-in-jailbreak}. Inspired by this line of work, we investigate whether LMs consistently prioritize system prompts over user instructions and extend this evaluation to the broader context of instruction hierarchy.

During the development of \benchmark , a concurrent work \citep{sysbench} introduced SysBench which evaluates LMs' adherence to system prompts. Compared to Sysbench, \benchmark\ features a more comprehensive evaluation of the instruction hierarchy concept, encompassing system prompts, user instructions, conversation history, and tool outputs. Moreover, \benchmark\ is fully equipped with programmatic evaluation, offering better cost-efficiency and reproducibility than the GPT-based evaluation used in SysBench.
