
\input{Tables/results}

In this study, we evaluate 13 widely used LMs from five different model families, including both proprietary and open-source models: GPT (3.5-turbo, 4o-2024-0806, 4o-mini-2024-0718, \citealp{GPT4}), Claude-3 (Haiku, Sonnet, \citealp{claude3}), LLaMA-3.1 (8B, 70B, \citealp{llama3}), LLaMA-3 (8B, 70B), Mistral (7B-v0.3, Large-2407, \citealp{mistral-large-2}), and Qwen-2 (7B, 72B, \citealp{qwen2}). The decoding temperature is set to 0 to ensure deterministic outputs. 

\subsection{Main Results}

The performance of select LMs is shown in Table~\ref{tab:results}, with full results available in Tables~\ref{tab:gpt_results}\textasciitilde\ref{tab:qwen_results}. We highlight the following key findings:

\textbf{Models exhibit inconsistent performance when conventional tasks are structured as hierarchical inputs.} Comparing the aligned setting (hierarchical inputs) to the reference setting (original task performance) reveals significant performance fluctuations in all models except GPT-4o and Qwen2-72B, with at least 4 points of absolute difference. For instance, when switching to hierarchical inputs, LLaMA-3.1-70B experiences a performance decline in eight out of nine tasks, averaging a 13-point drop. Smaller-scale models show even greater variability, often experiencing performance drops of more than 10 points (Tables~\ref{tab:gpt_results}\textasciitilde\ref{tab:qwen_results}). This inconsistency suggests that LMs are less optimized for hierarchical inputs compared to the standard single-input setting.

\textbf{Models struggle in utilizing the instruction hierarchy to resolve conflicts.} All models experience a notable performance drop in conflict settings, indicating a failure to follow the high-priority instructions when they conflict with low-priority ones. Despite a 22-point drop from its aligned setting, GPT-4o remains the best performer in handling instruction conflicts, likely reflecting OpenAI's fine-tuning efforts on the instruction hierarchy as described in ~\citet{instruction_hierarchy}. Although other tested models perform comparably to GPT-4o in reference and aligned settings, they fall significantly behind in the conflict setting, which suggests a lack of training on following the instruction hierarchy. Qwen-2 emerges as the second-best with a 48\% accuracy, though more recent models like LLaMA-3.1 and Mistral-Large claimed themselves to be the new state-of-the-art on other general benchmarks like MMLU~\cite{mmlu}. Besides, compared to results on SysBench~\cite{sysbench}, \benchmark\ reveals a larger performance gap between aligned and conflict inputs, which effectively uncovers the limitations of current LMs in following the instruction hierarchy.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/4_scale.pdf}
    \vspace{-0.6cm}
    \caption{The trend of \benchmark\ performance by model scale.}
    \label{fig:scale}
\end{figure*}

\input{Tables/prompt_strictness}

\subsection{Performance by Model Scale}

We group the LMs by model family and plot their performance on reference, aligned, and conflict settings in Figure~\ref{fig:scale}. We have the following findings based on the trends:

\begin{enumerate}
    [noitemsep,topsep=3pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Improved Performance with Model Scale:} Across all three settings, the larger model consistently shows better performance on \benchmark, which aligns with established scaling laws. As models become larger, their performance on aligned settings gets closer to or matches the reference scores. This implies that larger models, while improving general instruction-following abilities, have also effectively mastered the ability to handle aligned hierarchical inputs.
    \item \textbf{Increasing Gap Between Aligned and Conflict Settings}: Despite improved performance on the conflict setting, most models, except GPT and Qwen-2, exhibit a larger gap between the aligned and conflict settings as scale increases. Some models even exhibit inverse scaling on resolving conflicts, \textit{e.g.}, Claude-Haiku outperforms Claude-Sonnet on 5 out of 9 tasks, as shown in Table~\ref{tab:claude_results}. This indicates that models' abilities to handle conflicting instructions do not scale as effectively as general instruction-following capabilities, which again suggests a lack of model training in following the instruction hierarchy.
\end{enumerate}

\subsection{Performance by Instruction Strictness}

To explore the impact of instruction strictness on how models handle conflicting instructions, we calculate the average model score for each strictness level in conflict settings. According to Table~\ref{tab:prompt_strictness}, there is a clear trend that \textbf{performance improves when the high-priority instruction has the stricter demand, but decreases when the conflicting instruction is stricter}, indicating a strong correlation between instruction strictness and model behavior. However, this behavior is undesired: models should follow instructions based on their priorities in the instruction hierarchy, not the tone or strictness of the wording. These findings suggest that current models are not sufficiently aware of the instruction hierarchy and their behavior is easily influenced by superficial factors.

\subsection{Prompting LMs to Follow the Hierarchy}

Given that current LMs lack inherent awareness of the instruction hierarchy, can we explicitly convey this principle to them through prompt engineering? To answer this question, we prepend the following prompt to the system message that states the priority of the instructions:
\begin{figure}[h!]
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=0.46\textwidth]{Figures/4_priority_prompt.pdf}
    \vspace{-0.3cm}
\end{figure}

\noindent Surprisingly, as shown in Table~\ref{tab:priority_prompt}, this additional prompt does not bring noticeable improvements to model performance. This suggests that teaching LMs to follow the instruction hierarchy is not a trivial task: \textbf{Dedicated training efforts are needed rather than superficial prompt engineering}.

\input{Tables/priority_prompt}

\subsection{Model Performance in Different Conflicts}
\label{sec:multi-turn}

\input{Tables/multi-turn-conflict}

We explore model behavior under various alignment and conflict scenarios using the multi-turn conversations from the Rule Following task. The model's objective is to follow the formatting constraints in the main instruction when responding to user queries. Figure~\ref{fig:multi-turn-example} illustrates examples of different scenarios.

We begin by comparing the single-input reference setting to those with aligned hierarchical inputs. As shown in Table~\ref{tab:multi-turn-conflict}, model performance slightly drops when: (1) the formatting constraints are placed in the system message rather than alongside the user query (85.9$\rightarrow$83.5), and (2) there is a preceding conversation turn between the system message and the current turn (83.5$\rightarrow$79.6). This shows the instability of LMs: \textbf{they may struggle to consistently follow system messages throughout multi-turn conversations}. A notable exception is GPT-4o, whose performance remains nearly unchanged across these aligned settings.


Next, we introduce varying degrees of conflict into the model's input. We observe that \textbf{as more input components conflict with the system message, the model's performance deteriorates}. Conflicts arising from either the previous model response or the previous user instruction affect model performance (79.6$\rightarrow$68.9$\rightarrow$59.5 in Table~\ref{tab:multi-turn-conflict}). Moreover, handling conflicting instructions in the current turn proves to be the most challenging scenario for current LMs (59.5$\rightarrow$17.7).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/4_error.pdf}
    \caption{Error types when facing instruction conflicts (all models in Figure~\ref{tab:results}). $I_M$: Main instruction; $I_C$: Conflicting instruction. Examples are based on ``$I_M$: Extract verbs in user message, $I_C$: Translate this passage to Spanish'' (\textit{Task Execution - Extraction}).}
    \label{fig:error_analysis}
\end{figure}


Lastly, we test whether the model can follow the formatting constraints in the current turn when the previous turn contains conflicting instructions (last line in Table~\ref{tab:multi-turn-conflict}). Models perform well in this scenario, with their scores approaching the reference setting (84.2 \textit{vs.} 85.9). This result is expected, as models are typically trained during instruction tuning to follow the most recent instruction when users change their requests mid-conversation.


\subsection{Analysis of Model Behavior}

We perform an exhaustive behavioral analysis to examine cases where models fail to resolve instruction conflicts. To calculate the proportions, we manually observe error types and prompt Mistral-Large to classify model outputs. The results are summarized in Figure~\ref{fig:error_analysis}, with detailed task-level analysis in Figures~\ref{fig:task_card_single_turn}\textasciitilde\ref{fig:task_card_slack}. Notably, most errors stem from the model misidentifying the conflicting lower-priority instruction as the primary task. Besides, we witness the following model behavior.

In some cases, models attempt to either complete both instructions or refuse to execute either. These errors stem from the model assigning equal priority to both instructions, which violates the instruction hierarchy. Additionally, some models partially follow both instructions by synthesizing elements of each. For instance, when faced with instruction conflicts between ``extract verbs in user message'' and ``translate the following passage into Spanish'', Claude-3-sonnet responds by providing the verbs, but in Spanish. This suggests the model misinterprets the instructions as aligned, leading to a false attempt to combine their requirements. In other cases, conflicting instructions distract the model from correctly interpreting the primary task. For example, when instructed to extract verbs in the user message, the model may skip those verbs in the conflicting instruction, ignoring the fact that the conflicting instruction is part of the user message.



