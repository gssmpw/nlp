In this paper, we introduced \benchmark , a comprehensive benchmark designed to evaluate the ability of LMs to follow the instruction hierarchy. Our benchmark consists of nine diverse tasks that are all evaluated programmatically, covering scenarios where hierarchical inputs either align or conflict, and vary in both input types and task difficulties. Through \benchmark , we identified a significant weakness in mainstream LMs: their difficulty in recognizing the priority of different instructions. We further conducted a detailed analysis of model behavior under various scenarios of instruction conflicts. This work highlights the need for further optimization of LMs in this critical dimension and lays the groundwork for future research in this area.