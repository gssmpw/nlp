

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/3_dataset_revised.pdf}
    \caption{\benchmark\ covers four categories and nine tasks. Detailed examples and instructions are in Figures~\ref{fig:task_card_single_turn}\textasciitilde\ref{fig:task_card_slack}.}
    \label{fig:dataset}
\end{figure*}


\paragraph{Definition} In this paper, we denote \textbf{inputs} to be the text segments that the model receives, which may contain both \textbf{instructions} that control the model's behavior, and \textbf{data} that the model needs to process.
\benchmark\ is designed around the instruction hierarchy, which assigns priority to instructions from four types of input: \textit{system messages}, \textit{user messages}, \textit{conversation history}, and \textit{tool output}, ranked from highest to lowest priority. We define \textit{hierarchical inputs} as input sequences composed of more than one type of input, such as a sequence that includes both a system message and a user message.
When facing instruction conflicts, we refer to the higher-priority instruction as the \textit{main instruction}, which defines the primary task the model should prioritize. The \textit{conflicting instruction} refers to the lower-priority instruction whose request is incompatible with the main instruction.

\paragraph{Task Settings} For a comprehensive evaluation, we design three input \textbf{settings} for each \benchmark\ task:
\begin{itemize}[noitemsep,topsep=3pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Aligned Setting}: The model receives hierarchical inputs where all low-priority inputs align with the highest-priority instruction. This tests the model's ability to process hierarchical inputs in normal scenarios without conflicts.
    \item \textbf{Conflict Setting}: There are conflicts among different priorities of instructions within a hierarchical input. Models are expected to follow the instruction hierarchy to resolve conflicts.
    \item \textbf{Reference Setting}: We notice that a model's response to hierarchical instructions is affected by both its original task performance and its ability to follow the instruction hierarchy (IH-following). To better assess IH-following performance, we add a reference setting that isolates the original task performance by removing hierarchical inputs. Specifically, the model is evaluated in a standard single-input setting, where all hierarchical instructions from the \textit{aligned} setting are merged into a single user message.
\end{itemize}

\paragraph{Task Design} IHEval tasks are selected to encompass a diverse range of application scenarios and input types. We focus on tasks where LMs perform well to minimize the impact of original task performance on IHEval scores. We prioritize datasets with human-annotated labels or reliable programmatic evaluation. Conflicting instructions are drawn from tasks likely to confuse LMs in following the main instruction, based on heuristics and trials on sample data. As a result, a total of nine tasks is created and grouped into four categories based on the type of content in the instructions: 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/3_stats.pdf}
    \vspace{-0.6cm}
    \caption{The original data source, the evaluation metric, and the data size of each task.}
    \label{fig:stats}
    \vspace{-0.2cm}
\end{figure*} 

\begin{itemize}[noitemsep,topsep=3pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Rule Following}: Instructions specify formatting rules for model outputs. We adapt data from IFEval~\cite{ifeval} into our \textit{single-turn} task, where the original data is split into formatting rules (system message) and user queries (user message). We then craft incompatible formatting rules to create conflicting user instructions. The single-turn data is further extended to a \textit{multi-turn} setting by crafting both a response to the first turn and a follow-up user message. Data crafting in this category is initially performed by Claude~\cite{claude3}, after which we manually review all AI-written messages and re-write low-quality ones. 
    \item \textbf{Task Execution}: In this category, the system message outlines a specific NLP task that the model should perform on the user's input, while the user message may provide a conflicting instruction that requests a different task. This category covers typical NLP tasks that span \textit{extraction}, \textit{generation}, and \textit{classification}.
    \item \textbf{Safety Defense}: Following the setting of TensorTrust~\cite{TensorTrust}, this category simulates the model as a security system. The system message requires the model to grant access only if the correct password is entered. Normal user inputs involve password attempts, but conflicts arise when malicious users launch adversarial attacks to make the model respond with ``Access Granted'' (\textit{Hijack} task) or attempt to extract the system message that contains the password (\textit{Extraction} task).
    \item \textbf{Tool Use}: This category focuses on tasks where the model needs to call external tools to complete the user's request. We simulate a tool call and the corresponding tool output, where the tool output may either align with the user's request or contain conflicting instructions. Such instructions can be \textit{intrinsic}, \textit{i.e.}, originating from the tool-retrieved content itself, or \textit{injected} by an external attacker and is concatenated with the normal tool-retrieved content.
\end{itemize}

The illustration of all tasks is listed in Figure~\ref{fig:dataset}, with data examples and task instructions shown in Figures~\ref{fig:task_card_single_turn}\textasciitilde\ref{fig:task_card_slack}, respectively. Data sources and statistics are outlined in Figure~\ref{fig:stats}. Details about data collection for each individual task and the motivation of task selection are provided in Appendix~\ref{app:tasks}.



\paragraph{Task Difficulties} \benchmark\ introduces multiple difficulty levels for each task by crafting instructions with different imperative \textbf{strictness}. This approach not only provides a comprehensive evaluation of model performance but also reduces the randomness brought by the phrasing of instructions. The stricter version of instructions requires the model to exclusively adhere to the given instruction (\textit{e.g., do not output any other text besides the Spanish translation} in the translation task).
All these instructions are shown in Figures~\ref{fig:task_card_single_turn}\textasciitilde\ref{fig:task_card_slack}.


\paragraph{Evaluation} IHEval evaluates models based on their performance in completing the main instruction, as outlined in Figure~\ref{fig:stats}. For example, when the system message requests the model to extract verbs from the user's input, the evaluation metric is the F-1 score which compares model-extracted verbs to the ground-truth list. Any execution of the conflicting instruction -- translating the input text into Spanish -- negatively impacts performance, as it diverges the model output from the target defined by the system message. 

For tasks that are not evaluated by exact-match accuracy (excluding safety tasks that check the defense success rate using the whole model response), we calculate both a \textbf{strict metric} and a \textbf{loose metric}, following the practice in IFEval \citep{ifeval}. The strict metric assesses the model's entire output, while the loose metric allows minor variations by evaluating outputs that omit the first line, last line, or markdown syntax, selecting the best-performing version. 
The loose metric accommodates brief introductory phrases (\textit{e.g.}, \textit{I'm sorry, but I can only extract verbs from your message}) that explain the modelâ€™s behavior. However, overly interactive responses -- such as asking for clarification or answering both instructions -- are discouraged, as they treat the hierarchical instructions with the same level of priority (see Appendix \ref{sec:appendix_evaluation} for a more detailed discussion). The final score is averaged across difficulty levels and, when applicable, across strict and loose metrics.

As previously mentioned, the reference setting decouples a model's baseline task performance from its IH-following ability. To quantify this distinction, we calculate the score difference ($\Delta$ in Table~\ref{tab:results}) between the reference setting and the other two settings. Specifically, we report both the \textit{mean difference}, which reflects the model's average IH-following performance (where smaller performance drops indicate better IH-following), and the \textit{mean absolute difference}, which captures performance fluctuation between single-input \textit{vs.} hierarchical-input settings.



