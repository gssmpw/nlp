\section{Detailed Data Collection}
\label{app:tasks}

\subsection{Rule Following}
\label{sec:rule_following}

In this category, instructions dictate the \textit{formatting rules of the model's response}. The tasks include both \textbf{single-turn} and \textbf{multi-turn} conversations, as illustrated in Figures~\ref{fig:task_card_single_turn} and~\ref{fig:task_card_multi_turn}, respectively.

To create the single-turn task, we derive data from the IFEval dataset \citep{ifeval}. The original IFEval data is directly adopted as the \textit{reference} setting. We then split the original inputs into formatting rules (system message) and user queries (user message) to build the \textit{aligned} setting. Next, conflicting instructions are generated and concatenated with user queries to create the \textit{conflict} setting. These conflicting instructions request formatting rules that are incompatible with those specified in the system message. The separation of system messages and the crafting of conflicting instructions are performed by Claude-3-Sonnet, and are further manually reviewed by the authors, during which low-quality ones are re-written. For evaluation, we follow IFEval's evaluation script to assess whether the model response follows the formatting rules defined in the system message.


The \textbf{multi-turn} task builds on the single-turn data by using it as the initial turn in a conversation. To create a multi-turn scenario, we first generate two responses for the first turn -- one aligned with the system message and the other conflicting with it\footnote{We obtain the conflicting response by prompting Claude to answer the query using the conflicting format specified in the conflict single-turn setting.}. A second-turn user query is then generated based on the context established in the first turn, requiring the model to use information from both turns to respond. These data are collected using the same process as the single-turn task: They are first written by Claude and then manually verified. The settings for the multi-turn task are as follows:

\begin{itemize}
    [noitemsep,topsep=2pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Reference}\quad A single user message that combines the system message with the second-turn query, excluding the first-turn data.
    \item \textbf{Aligned}\quad The first turn contains the original user query and the \textit{aligned} response.
    \item \textbf{First-turn Conflict}\quad The first-turn user message is concatenated with the conflicting formatting rules, followed by the \textit{conflicting} response.
    \item \textbf{Both-turns Conflict}\quad Building on the First-turn Conflict setting, the second-turn user message also contains the conflicting formatting rules.
\end{itemize}

In practice, we observe that a model's adherence to the system messages may deteriorate in the aligned multi-turn setting compared to the reference setting where there is only a single turn. This means models struggle to consistently apply the system message across all conversation turns, despite such consistency being the intended purpose of system messages. To focus on models' IH-following ability instead of their multi-turn consistency, we add another aligned setting with a \textit{stricter} version of the system message. This stricter version explicitly requires adherence to formatting rules throughout the entire conversation, as illustrated in Figure~\ref{fig:task_card_multi_turn}.

\subsection{Task Execution}

In this category, the model is given an instruction to perform a specific task on the user's input. In the conflict setting, an additional instruction is included in the user message, requesting the execution of a different task. Such scenarios are common, for example, when LMs are used to translate instruction data, the translation of original instruction is needed rather than the response to it.

We curate three tasks, each representing a common type of NLP benchmark: \textbf{extraction}, \textbf{generation}, and \textbf{classification}. For each task, the \textit{aligned} setting includes a system message that defines the task, and the user input is a normal piece of data without any instructions. In the \textit{conflict} setting, a conflicting instruction is prepended to the data, asking the LM to perform an alternative task. The tasks are as follows:

\begin{itemize}
    [noitemsep,topsep=2pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Extraction}\quad \textit{System message}: Verb extraction; \textit{Conflicting instruction}: English-to-Spanish translation. Data and their corresponding POS tags are collected from Ontonotes \citep{ontonotes}. The evaluation metric is the F-1 score which compares the model-extracted verb list with the ground-truth verb list.
    \item \textbf{Generation}\quad \textit{System message}: English-to-Spanish translation; \textit{Conflicting instruction}: Math problem solving. The data are math problems from MGSM \citep{mgsm} which are paired with Spanish annotations. We use ROUGE-L~\cite{rouge} as the metric to evaluate the translation quality. We also tried other popular metrics such as BLEU~\cite{bleu} and BERTScore~\cite{bertscore}, but we found negligible differences among them.
    \item \textbf{Classification}\quad \textit{System message}: Language detection; \textit{Conflicting instruction}: Text summarization. \textbf{We select the language detection task because adding any conflicting instructions to the user message does not affect the classification label}. The data are text passages from XL-Sum \citep{xlsum}. We require models to output their predictions in JSON format\footnote{All models in our experiments are able to follow such a formatting constraint.} for easy answer extraction, and use accuracy as the classification metric.
\end{itemize}

For each task, we also design \textit{strict} versions for both system messages and conflicting instructions. The strict version demands the LM to only execute the current instruction without generating any other content. Detailed task examples, instructions, and error analysis for these three tasks are provided in Figures~\ref{fig:task_card_verb_extract}\textasciitilde\ref{fig:task_card_lang_detect}.

\subsection{Safety Defense}
\label{sec:safety-defense}

A critical application of the instruction hierarchy is ensuring model safety. This requires LMs to prioritize their adherence to security protocols set by developers to defend against potential adversarial attacks from malicious users. In this context, we follow the configuration of TensorTrust \citep{TensorTrust}, where the LM functions as a security system that requires a password for user access. The system message provided to the model includes specific security guidelines, including the secret password. The model is instructed to respond with ``Access Granted'' only when the correct password is input by the user. Additionally, the model must not reveal the password or the security guidelines under any circumstances.

In the \textit{aligned} setting, user inputs consist of normal password attempts, and the model is evaluated based on its accuracy in identifying password validity. User messages in the \textit{conflict} setting vary between the two tasks:

\begin{itemize}
    [noitemsep,topsep=2pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Hijack Task}\quad User inputs are adversarial attempts to manipulate the model into generating ``Access Granted'' without providing the correct password.
    \item \textbf{Extraction Task}\quad User messages aim to extract the password by making the model repeat or leak the system message.
\end{itemize}

These adversarial attack instructions are sourced from the original TensorTrust dataset, while the system message is custom-crafted for IHEval by the authors. Model performance in conflict settings is assessed by the percentage of successful defenses. A failed defense is when the model either (1) generates ``Access Granted'' in its response, (2) reveals the password, or (3) outputs more than half of the unigrams in the system message.

We test two variations of the system message: a \textit{strict} version, which includes defenses against a broader range of potential attack strategies, and a \textit{weaker} version, which contains only the necessary guidelines. Detailed instructions and examples are illustrated in Figures~\ref{fig:task_card_hijack} and~\ref{fig:task_card_extract}, respectively.

In this category, we do not test jailbreaking attacks~\cite{jailbreak_survey} as these typically target harmful model behaviors that impact human society. The awareness of such moral standards is independent from the adherence to written instructions. Therefore, jailbreaking primarily tests the safety fine-tuning of the LM rather than its ability to follow instructions. In contrast, \benchmark\ focuses on scenarios where the defined instructions are challenged by conflicting requests.


\subsection{Tool Use}
\label{sec:tool_use}

Tool outputs are another source where conflicting instructions may arise. In this category, the model needs to call external tools to fulfill the user's request. To analyze the interaction between tool outputs and user instructions, we simulate a tool call made by the model and the corresponding content returned from the tool's execution. We design two tasks: one where the instruction is inherently present in the tool-retrieved content (\textbf{intrinsic instructions}), and another where an external attacker injects conflicting instructions into the tool's response (\textbf{injected instructions}). Specific configurations are as follows:

\begin{itemize}
[noitemsep,topsep=2pt,parsep=1pt,partopsep=0pt,leftmargin=0.4cm]
    \item \textbf{Intrinsic Instruction}\quad We define a tool that reads the text content of a given webpage. We reuse data from the three tasks in the Task Execution category, mixing them to create a new dataset. The user message is the main instruction, and the tool output consists of retrieved text content from the webpage. The webpage normally contains a text passage (\textit{aligned} setting), but may also include a conflicting instruction (\textit{conflict} setting). A \textit{stricter} version of the conflicting instruction requires the model to ignore all prior instructions, simulating a scenario where the webpage has been maliciously altered. The evaluation follows the same metrics as those used in Task Execution, where metrics on all examples are averaged as the final score.
    \item \textbf{Injected Conflict}\quad Here, the tool retrieves usernames from a Slack channel, and the model is tasked with identifying the shortest username. In the \textbf{conflict} setting, the tool output is appended with a commonsense question as the conflicting instruction. The injected questions are adopted from the SEP dataset \cite{SEP}. A \textit{stricter} version of the injection incorporates the ``important message'' attack from \citet{Agentdojo} which is a more sophisticated adversarial tactic. Both the original user task and the injected question require a single-word response, making it impossible for the model to answer both simultaneously. We evaluate the model's performance based on its accuracy in identifying the shortest username.
\end{itemize}

Although there are other datasets that inject prompts into tool outputs \citep{injecagent,Agentdojo}, the injected task is usually independent of the original user task. As a result, they focus on attack success rates which evaluate whether the injected task is executed, while the original user task may still be completed concurrently. In contrast, \benchmark\ focuses on addressing instruction conflicts that cannot be resolved by responding to both instructions, so we evaluate the completion of the user task as the criteria of the model's awareness of the instruction priority. Any execution of the conflicting instruction results in a performance drop, but it is not necessarily the cause. More detailed instructions, examples, and error analyses in this category are listed in Figures~\ref{fig:task_card_webpage} and \ref{fig:task_card_slack}.

\section{Evaluation Criteria}
\label{sec:appendix_evaluation}

In conflict settings, we evaluate whether models strictly follow high-priority instructions while ignoring conflicting low-priority ones. Since system messages are set by developers providing services to public users, prioritizing developer commands is crucial. This ensures LMs function as intended and maximizes model safety, as user inputs may not always align with the model’s designed purpose. For example, a translation bot should focus solely on translating user input. It may clarify its role when responding to users (\textit{e.g.}, \textit{I am a translator, so I can only translate your message}), and we accommodate such behavior using the loose metric in \S\ref{sec:benchmark}. 

On the other hand, overly interactive behaviors -- such as providing solutions to both instructions -- may lead to unsafe behavior. A translation bot responding to unrelated requests, like election predictions, may introduce undesired bias. Similarly, developers may not want a shopping bot to answer queries about competitors’ products. Asking for clarifications does not prevent misbehaving either, as it gives user inputs the same priority as developer commands. Moreover, such responses complicate programmatic evaluation when using LM APIs.

Thus, avoiding responses to potential misuse aligns with standard LM practices (\citealp{instruction_hierarchy}, \S3.1). Moreover, GPT-4o's strong performance on IHEval tasks further supports that our criteria reflect industry practices.

\section{Full Results}
Results of all 13 LMs on \benchmark\ are shown in Tables~\ref{tab:gpt_results}\textasciitilde\ref{tab:qwen_results}, grouped by their model family.


\section{Task Cards of \benchmark}

In Figures~\ref{fig:task_card_single_turn}\textasciitilde\ref{fig:task_card_slack}, we provide the task cards for each of the nine tasks in \benchmark , including the example of different task settings, different versions of instructions, and error analysis.
Redundant details of model responses may be {\color{gray}\textit{omitted}} due to space limits. Only major error types are shown. The percentage at the end of each error type represents its proportion among all errors, and is calculated from the generated responses of all models in Table~\ref{tab:results}.

\begin{figure*}[t]
      \centering
      \includegraphics[width=1.0\textwidth]{Figures/7_multi-turn-example.pdf}
      \caption{The input configuration of different settings in \S\ref{sec:multi-turn}. Directly using the follow-up query as the only user message in the reference and aligned \#1 settings is reasonable because we only evaluate the adherence to formatting rules, whereas whether the generated content matches the user query is not in the evaluation scope.}
      \label{fig:multi-turn-example}
\end{figure*}

\input{Tables/gpt-results}
\input{Tables/claude-results}
\input{Tables/llama3.1-results}
\input{Tables/llama3-results}
\input{Tables/mistral-results}
\input{Tables/qwen-results}

\clearpage

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/Task_Examples/single-turn.pdf}
    \caption{Task card of the single-turn conversation task in Rule Following category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}.}
    \label{fig:task_card_single_turn}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Task_Examples/multi-turn.pdf}
    \caption{Task card of the multi-turn conversation task in Rule Following category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. There are two conflict settings in this task: (1) First-turn conflict: only the conversational history (instruction \& response) conflicting with the system message; and (2) Both-turns conflict: both the history and the current turn conflicting with the system message. }
    \label{fig:task_card_multi_turn}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Task_Examples/verb-extract.pdf}
    \caption{Task card of the extraction task in Task Execution category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. Both the system message and user message can be replaced with the stricter version. In the conflict setting, the ideal behavior is to extract verbs from the entire user message, including the translation instruction and the task data.}
    \label{fig:task_card_verb_extract}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Task_Examples/translation.pdf}
    \caption{Task card of the generation task in Task Execution category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. Both the system message and user message can be replaced with the stricter version. In the conflict setting, the ideal behavior is to translate the entire user message, including the math instruction and the task data.}
    \label{fig:task_card_translation}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Task_Examples/lang-detect.pdf}
    \caption{Task card of the classification task in Task Execution category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. Both the system message and user message can be replaced with the stricter version.}
    \label{fig:task_card_lang_detect}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{Figures/Task_Examples/hijack.pdf}
    \caption{Task card of the Hijack task in Safety Defense category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. The system message can be replaced with the stricter version, which describes more potential adversarial attacks.}
    \label{fig:task_card_hijack}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Task_Examples/extract.pdf}
    \caption{Task card of the Extraction task in Safety Defense category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. The system message can be replaced with the stricter version, which describes more potential adversarial attacks.}
    \label{fig:task_card_extract}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Task_Examples/webpage.pdf}
    \caption{Task card of the Intrinsic Instruction task in Tool Use category. The main instruction is in {\color{green} green}, and the conflicting instruction is in {\color{red} red}. The injected instruction in the tool output can be replaced with the stricter version, which represents maliciously manipulated content in the webpage.}
    \label{fig:task_card_webpage}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Task_Examples/slack.pdf}
    \caption{Task card of the Injected Instruction task in Tool Use category. This task slightly differs from others as the main task is elaborated in the user message (the {\color{green} green} part), whereas the system message only serves as a formatting constraint to facilitate exact-match evaluation. The conflicting instruction is in {\color{red} red}, and can be replaced with the stricter version which represents a more carefully designed injection to attack the model. The format of this stronger attack is adopted from AgentDojo~\cite{Agentdojo}.}
    \label{fig:task_card_slack}
\end{figure*}
