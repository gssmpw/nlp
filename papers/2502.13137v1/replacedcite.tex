\section{Related Work}
\label{sec:bibtex}

\paragraph{Math Data Synthesis} Advancements in mathematical reasoning for LLMs increasingly rely on high-quality CoT datasets, often distilled from frontier models____, such as
NuminaMath____ and OpenMathInstruct____. Nevertheless, even solvable problems may contain error-prone intermediate reasoning steps, which are inherently challenging to detect. Although rejection sampling methods____ can improve data quality by filtering out less reliable outputs, they do not guarantee the correctness of intermediate steps. Consequently, the benefits of scaling CoT datasets exhibit diminishing returns, with performance gains nearing saturation. For instance, OpenMathInstruct reported only a 3.9\% improvement on the MATH dataset despite an 8× increase in dataset size. Recently, STaR____,
Lean-STaR____ and rStar-Math____, have been proposed. These approaches rely on generating multiple rollouts and trajectories for verification and synthesis.

\paragraph{Theorem Proving \& Autoformalisation} Modern formal mathematics environments typically centre on theorem provers, such as Lean____, Isabelle____ and Coq____. These systems have been widely used to verify complex mathematical results, including the Liquid Tensor Experiment____, the formalisation of the PER conjecture____, and efforts to formalise graduate-level number theory____.
The Draft-Sketch-Prove____ approach enhances language models’ formal proving abilities by generating informal proofs, translating them into formal sketches, and completing them with symbolic tools. 
%Recently, ____ highlighted that formalising question is particularly challenging in autoformalisation, as there are no definitive metrics to assess the correctness of the formalisation.