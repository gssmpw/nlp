\section{Related Work}
\label{sec:related_works}
In the section, we first review the associated point-, voxel- and range image-based approaches and data augmentation techniques. Then, we review the works that are similar to ours. Here, we roughly group existing point cloud segmentation (PCS) methods into the above three categories because of the point cloud representations and the proportion of the point-, voxel-, or range image-based operations in the architectures. % It means that a voxel- or range image-based model might include a small portion of point-based components. Also, a point-based model may contain a few voxel- or range image-based modules.


\subsection{Point-Based Approaches}
Point-based models are built with various point-based operations. The key in the models is to build effective feature extraction modules to extract semantic features over a set of points. The pioneering work, PointNet~\cite{pointnet++_2017}, uses MLP (multi-layer perceptrons) layers to extra per-point features and adopts global aggregation operations, such as pooling, to effectively build semantic relationships among points to make the final per-point predictions. However, PointNet fails to build an effective local feature extraction component in the architecture. To overcome the drawback, the following works, such as PointNet++~\cite{pointnet++_2017}, PCNN~\cite{pcnn2018}, DGCNN~\cite{dgcnn2019}, KPConv~\cite{kpconv_2019}, and Point Transformer~\cite{pointtrans2021}, and KPConvX~\cite{kpconvx24}, focus on the designs of the local feature extraction modules. However, due to the sparsity and disorder properties of the points, incorporating too many complex point-based operations into the architecture inevitably leads to heavy computational overhead, hindering the application of the models on large-scale outdoor point clouds. To reduce the computational cost, the recent work, WaffleIron~\cite{waffleiron23}, introduces a waffle iron (WI) block in the architecture, where all points are projected onto the XY, XZ, and YZ planes sequentially, and 2D convolution operations are employed on the planes to extract features. Besides, the work, Point Transformer V3~\cite{pointtransv32024}, first makes the unordered points ordered by serializing all points with space-filling curves, such as the Z-order and Hilbert curves. Then, a transformer architecture is applied to the ordered points. In this paper, we choose WaffleIron to conduct comparison experiments because it is open-source and can run faster than other point-based models on the large-scale outdoor point clouds.


\subsection{Voxel-Based Methods}
Here, we only briefly review the sparse convolution-based models because the models can avoid the high computational cost and huge memory load by doing convolution operations only on the occupied voxels. The typical work is MinkowskiNet~\cite{minkowski2019} (or MinkUNet), built by generalized sparse convolutions. Many of the following works adopt MinkUNet as the backbone. For example, SPVCNN~\cite{spvnas_2020} uses MinkUNet as the main branch with a complementary point-based branch. UniSeg~\cite{openpcseg2023} adopts MinkUNet as the point-voxel backbone. TASeg~\cite{taseg24} and RAPid-Seg~\cite{rapid25} are built on the basic MinkUNet architecture. The above models adopt the classic cubic partition of the large-scale outdoor point clouds as the input but do not consider the point distribution. To cope with the problem, the works Cylinder3D~\cite{cylindrical3d2021} and SphereFormer~\cite{spherical_transformer_2023} introduce cylindrical partition and radial window partition, respectively. In this paper, we choose MinkUNet and SPVCNN to conduct comparison experiments because, among the above voxel-based models, MinkUNet and SPVCNN can achieve state-of-the-art performance regarding the speed-accuracy trade-off.


\subsection{Range Image-Based Solutions}
Typically, the backbones of the range image-based PCS models are usually derived from 2D image classification or segmentation backbones. Besides, to avoid the loss of information caused by the projection process, a point-based post-processing component is commonly used to refine the final pointwise predictions. For example, the pioneering work, SqueezeSeg~\cite{squeezeseg}, uses modified SqueezeNet~\cite{squeezenet_2016} to make per-point predictions. Also, a recurrent CRF (conditional random field) component is adopted to refine the predictions further. Another classic work, RangeNet++~\cite{rangenet++}, adopts revised DarkNet~\cite{yolov3_2018} first to make per-pixel predictions. Then, a range image-based $K$NN ($K$-nearest neighbor search) module is introduced to refine the image-to-points predictions efficiently. The following work, CENet~\cite{cenet_2022}, utilizes a modified ResNet~\cite{resnet_2016} as the backbone and uses the $K$NN to make the final pointwise predictions. In FIDNet~\cite{fidnet_2021}, the authors also use the revised ResNet as the backbone, but they propose a range image-based NLA (nearest label assignment) module to refine the final per-point predictions. RangeFormer~\cite{rangeformer_2023} utilizes the SegFormer-like~\cite{segformer2021} backbone to make the pointwise predictions on each subset of the point cloud, and then the predictions from several sub-clouds are concatenated and further refined by the proposed RangePost and $K$NN. RangeViT~\cite{rangevit_2023} opts the revised ViT~\cite{vit_iclr_2021} as the backbone and adopts KPConv~\cite{kpconv_2019} to make the final per-point predictions. Recent Fast FMVNet~\cite{filling_missing2024} and Fast FMVNet V2~\cite{pdm2024} adopt the modified ConvNeXt-Tiny~\cite{convnext2022} as the backbone and utilize NLA and the proposed PDM (pointwise decoder module) to refine the final predictions, respectively. In this paper, we choose CENet and Fast FMVNet V2 to conduct comparison experiments because both models can run at high speed and achieve state-of-the-art performance.
   

\subsection{Data Augmentation Techniques}
To effectively train the PCS models, diverse point cloud samples are required and can be created by the combination of various data augmentation techniques. Here, we split the data augmentation techniques into two groups: basic and spinning LiDAR-specific data augmentation techniques. The basic augmentation techniques~\cite{data_aug2024} include random flipping, random scaling, random rotation, random dropping, and random jittering. These techniques can be applied to any point cloud data, such as solid-state LiDAR-based data and mechanical spinning LiDAR-based data. By contrast, spinning LiDAR-specific augmentation techniques can commonly be employed on the spinning LiDAR-based datasets, such as SemanticKITTI~\cite{semantickitti_2019_behley} and nuScenes~\cite{nuscenes_panoptic}. The specific augmentation techniques includes PolarMix~\cite{polarmix_2022}, LaserMix~\cite{lasermix_2023}, and VRCrop~\cite{pdm2024}. PolarMix swaps the sub-clouds between two point clouds along the horizontal direction, while LaserMix swaps the sub-clouds along the vertical direction. Also, PolarMix and LaserMix include rare object augmentation to make class balance during training. In practice, both augmentation techniques are usually used together~\cite{rangeformer_2023}. VRCrop, instead of the traditional copy-rotate-paste augmentation for the point clouds, introduces the concept of a virtual range image where the point cloud is reorganized and looks like a 2D image. Hence, the point cloud-based copy-rotate-paste augmentation can be like the color image-based copy-paste augmentation. One advantage of VRCrop is the elimination of undesirable artifacts in the augmented point cloud. In this paper, we provide a combination of data augmentation techniques. Under the combination, existing PCS models are trained again for fair comparisons.


\subsection{Works Focusing on Comparison Experiments}
All PCS works focusing on performance improvement provide comprehensive comparisons between existing approaches and the proposed method. For example, the work, UniSeg~\cite{openpcseg2023}, compares the proposed models (UniSeg $0.2\times$ and UniSeg $1.0\times$) with many existing PCS models, such as MinkowskiNet and SPVCNN, to show the superior performance in terms of the number of model parameters, inference latency, and mIoU scores. However, the mIoU scores of the compared models are outdated, leading to unfair comparisons. In addition, the work~\cite{empir24}, focusing on how to train the existing state-of-the-art PCS models, provides the comparison results among various PCS models with the above metrics. However, the comprehensive comparison results are only shown in the figure. More importantly, these works ignore the impact of motion compensation on the model performance. Different from the previous works, this paper first considers the effect of motion compensation and trains most PCS models again to reach state-of-the-art performance. Then, comprehensive comparisons among the models are reported with the metrics of the number of model parameters, max GPU memory allocated during inference, inference latency, frames per second, IoU and mIoU scores.