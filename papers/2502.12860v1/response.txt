\section{Related Work}
\label{sec:related_works}
In the section, we first review the associated point-, voxel- and range image-based approaches and data augmentation techniques. Then, we review the works that are similar to ours. Here, we roughly group existing point cloud segmentation (PCS) methods into the above three categories because of the point cloud representations and the proportion of the point-, voxel-, or range image-based operations in the architectures. % It means that a voxel- or range image-based model might include a small portion of point-based components. Also, a point-based model may contain a few voxel- or range image-based modules.


\subsection{Point-Based Approaches}
Point-based models are built with various point-based operations. The key in the models is to build effective feature extraction modules to extract semantic features over a set of points. The pioneering work, Qi et al., "PointNet" , uses MLP (multi-layer perceptrons) layers to extra per-point features and adopts global aggregation operations, such as pooling, to effectively build semantic relationships among points to make the final per-point predictions. However, PointNet fails to build an effective local feature extraction component in the architecture. To overcome the drawback, the following works, such as Qi et al., "PointNet++" , Wang et al., "PCNN",  Li et al., "DGCNN",  Thomas et al., "KPConv",  Wen et al., "Point Transformer", and Xie et al., "KPConvX", focus on the designs of the local feature extraction modules. However, due to the sparsity and disorder properties of the points, incorporating too many complex point-based operations into the architecture inevitably leads to heavy computational overhead, hindering the application of the models on large-scale outdoor point clouds. To reduce the computational cost, the recent work, Liu et al., "WaffleIron", introduces a waffle iron (WI) block in the architecture, where all points are projected onto the XY, XZ, and YZ planes sequentially, and 2D convolution operations are employed on the planes to extract features. Besides, the work, Zhang et al., "Point Transformer V3", first makes the unordered points ordered by serializing all points with space-filling curves, such as the Z-order and Hilbert curves. Then, a transformer architecture is applied to the ordered points. In this paper, we choose Liu et al., "WaffleIron" to conduct comparison experiments because it is open-source and can run faster than other point-based models on the large-scale outdoor point clouds.


\subsection{Voxel-Based Methods}
Here, we only briefly review the sparse convolution-based models because the models can avoid the high computational cost and huge memory load by doing convolution operations only on the occupied voxels. The typical work is Choy et al., "MinkowskiNet" (or MinkUNet), built by generalized sparse convolutions. Many of the following works adopt MinkowskiNet as the backbone. For example, Srinivas et al., "SPVCNN", uses MinkowskiNet as the main branch with a complementary point-based branch. UniSeg Zhang et al., adopts MinkowskiNet as the point-voxel backbone. TASeg Zhang et al., and RAPid-Seg Liu et al., are built on the basic MinkowskiNet architecture. The above models adopt the classic cubic partition of the large-scale outdoor point clouds as the input but do not consider the point distribution. To cope with the problem, the works Huang et al., "Cylinder3D", and Zhang et al., "SphereFormer", introduce cylindrical partition and radial window partition, respectively. In this paper, we choose Choy et al., "MinkowskiNet" and Srinivas et al., "SPVCNN" to conduct comparison experiments because, among the above voxel-based models, MinkowskiNet and SPVCNN can achieve state-of-the-art performance regarding the speed-accuracy trade-off.


\subsection{Range Image-Based Solutions}
Typically, the backbones of the range image-based PCS models are usually derived from 2D image classification or segmentation backbones. Besides, to avoid the loss of information caused by the projection process, a point-based post-processing component is commonly used to refine the final pointwise predictions. For example, the pioneering work, Li et al., "SqueezeSeg", uses modified SqueezeNet as the backbone to make per-point predictions. Also, a recurrent CRF (conditional random field) component is adopted to refine the predictions further. Another classic work, Liu et al., "RangeNet++", adopts revised DarkNet first to make per-pixel predictions. Then, a range image-based $K$NN ($K$-nearest neighbor search) module is introduced to refine the image-to-points predictions efficiently. The following work, Zhang et al., "CENet", utilizes a modified ResNet as the backbone and uses the $K$NN to make the final pointwise predictions. In Wang et al., "FIDNet", the authors also use the revised ResNet as the backbone, but they propose a range image-based NLA (nearest label assignment) module to refine the final per-point predictions. Zhang et al., "RangeFormer", utilizes the SegFormer-like backbone to make the pointwise predictions on each subset of the point cloud, and then the predictions from several sub-clouds are concatenated and further refined by the proposed RangePost and $K$NN. Wang et al., "RangeViT", opts the revised ViT as the backbone and adopts KPConv to make the final per-point predictions. Recent Liu et al., "Fast FMVNet" and Liu et al., "Fast FMVNet V2" adopt the modified ConvNeXt-Tiny as the backbone and utilize NLA and the proposed PDM (pointwise decoder module) to refine the final predictions, respectively. In this paper, we choose Zhang et al., "CENet" and Liu et al., "Fast FMVNet V2" to conduct comparison experiments because both models can run at high speed and achieve state-of-the-art performance.
   

\subsection{Data Augmentation Techniques}
To effectively train the PCS models, diverse point cloud samples are required and can be created by the combination of various data augmentation techniques. Here, we split the data augmentation techniques into two groups: basic and spinning LiDAR-specific data augmentation techniques. The basic augmentation techniques  include random flipping, random scaling, random rotation, random dropping, and random jittering. These techniques can be applied to any point cloud data, such as solid-state LiDAR-based data and mechanical spinning LiDAR-based data. By contrast, spinning LiDAR-specific augmentation techniques can commonly be employed on the spinning LiDAR-based datasets, such as Chen et al., "SemanticKITTI" and Chen et al., "nuScenes". The specific augmentation techniques include Chen et al., "PolarMix", Wang et al., "LaserMix", and Zhang et al., "VRCrop". PolarMix swaps the sub-clouds between two point clouds along the horizontal direction, while LaserMix swaps the sub-clouds along the vertical direction. Also, PolarMix and LaserMix include rare object augmentation to make class balance during training. In practice, both augmentation techniques are usually used together . Zhang et al., "VRCrop", instead of the traditional copy-rotate-paste augmentation for the point clouds, introduces the concept of a virtual range image where the point cloud is reorganized and looks like a 2D image. Hence, the point cloud-based copy-rotate-paste augmentation can be like the color image-based copy-paste augmentation. One advantage of Zhang et al., "VRCrop" is the elimination of undesirable artifacts in the augmented point cloud. In this paper, we provide a combination of data augmentation techniques. Under the combination, existing PCS models are trained again for fair comparisons.


\subsection{Works Focusing on Comparison Experiments}
All PCS works focusing on performance improvement provide comprehensive comparisons between existing approaches and the proposed method. For example, the work, Zhang et al., "UniSeg", compares the proposed models (UniSeg $0.2\times$ and UniSeg $1.0\times$) with many existing PCS models, such as Choy et al., "MinkowskiNet" and Srinivas et al., "SPVCNN", to show the superior performance in terms of the number of model parameters, inference latency, and mIoU scores. However, the mIoU scores of the compared models are outdated, leading to unfair comparisons. In addition, the work , focusing on how to train the existing state-of-the-art PCS models, provides the comparison results among various PCS models with the above metrics. However, the comprehensive comparison results are only shown in the figure. More importantly, these works ignore the impact of motion compensation on the model performance. Different from the previous works, this paper first considers the effect of motion compensation and trains most PCS models again to reach state-of-the-art performance. Then, comprehensive comparisons among the models are reported with the metrics of the number of model parameters, max GPU memory allocated during inference, inference latency, frames per second, IoU and mIoU scores.