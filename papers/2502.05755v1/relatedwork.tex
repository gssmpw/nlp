\section{Related Works}
\noindent{\textbf{Semi-supervised Learning.}}
Semi-supervised learning (SSL) is a well-established field featuring a wide range of approaches\cite{mclachlan1975iterative}. In this section, we focus on methods that adopt self-training paradigm, which represents the most mainstream techniques in modern SSL\cite{scudder1965probability}. The core concept involves treating the model's output probabilities as either soft or hard pseudo labels for unlabeled data\cite{lee2013pseudo, berthelot2019remixmatch, zhai2019s4l}. Additionally, consistency regularization is employed to ensure that predictions on perturbed versions of the unlabeled data remain the same\cite{xie2020unsupervised, xu2021dash, wang2022freematch, chen2023softmatch}. Moreover, there are studies that concentrate on enhancing the model's robustness in SSL\cite{guo2022robust, jia2023bidirectional, li2023iomatch, wan2024unlocking}. However, these papers, referred to as safe SSL, focus on learning from unlabeled data with distribution shifts or additional classes, excluding the consideration of poisoned data \cite{li2019towards, guo2020safe}.

\noindent{\textbf{Backdoor Attacks and Defenses.}}
A backdoor adversary aims to implant backdoor functionality into a target model. Recent studies \cite{connor2022rethinking, turner2019label} have highlighted that almost all contemporary SSL methods remain highly susceptible to certain specially designed clean label-type backdoor attacks. Shejwalkar's work, as one of the most influencing works in SSL backdoor attacks, has systematically identified characteristics that successful backdoor trigger should possess\cite{shejwalkar2023perils}. They also find that defense methods in supervised learning \cite{shokri2020bypassing,liu2023beating} become ineffective or challenging to be implemented. The primary reason is that these defense methods heavily rely on labeled data or some characteristics of learned features to detect poisoned samples or neutralize implanted backdoors\cite{dong2021black, tejankar2023defending, wang2024mm}. However, in SSL, the extremely limited number of labeled data points makes such approaches unfeasible. Specifically, common observations for backdoored model in supervised learning like activation clustering \cite{chen2018detecting}, loss divergence \cite{li2021anti} and large-margin logit\cite{MM-BD} no longer exists in attacked SSL models. These issues also reflect the urgency and difficulty of developing backdoor defense methods for SSL models.