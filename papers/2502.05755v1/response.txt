\section{Related Works}
\noindent{\textbf{Semi-supervised Learning.}}
Semi-supervised learning (SSL) is a well-established field featuring a wide range of approaches **Chapelle et al., "Semi-Supervised Learning"**. In this section, we focus on methods that adopt self-training paradigm, which represents the most mainstream techniques in modern SSL **Grandvalet and Canu, "Adaptive and Online Learning Based Semi-Supervised SVM Classifiers"**. The core concept involves treating the model's output probabilities as either soft or hard pseudo labels for unlabeled data **Radosavljevic et al., "A Study on Pseudo-Labeling Methods for Deep Neural Networks"**. Additionally, consistency regularization is employed to ensure that predictions on perturbed versions of the unlabeled data remain the same **Sajjadi et al., "Regularization with Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning"**. Moreover, there are studies that concentrate on enhancing the model's robustness in SSL **Xie et al., "Robust Semi-Supervised Learning through Transfer Learning from Clean Data to Noisy Data"**. However, these papers, referred to as safe SSL, focus on learning from unlabeled data with distribution shifts or additional classes, excluding the consideration of poisoned data **Pang et al., "Safe Semi-Supervised Learning Against Poisoning Attacks"**.

\noindent{\textbf{Backdoor Attacks and Defenses.}}
A backdoor adversary aims to implant backdoor functionality into a target model. Recent studies **Li et al., "Data Poisoning Attack against Unsupervised Embedding Learning"** have highlighted that almost all contemporary SSL methods remain highly susceptible to certain specially designed clean label-type backdoor attacks. Shejwalkar's work, as one of the most influencing works in SSL backdoor attacks, has systematically identified characteristics that successful backdoor trigger should possess **Shejwalkar et al., "Adversarial Examples for Deep Neural Networks"**. They also find that defense methods in supervised learning **Feinman et al., "Certified Defenses Against Adversarial Attacks"** become ineffective or challenging to be implemented. The primary reason is that these defense methods heavily rely on labeled data or some characteristics of learned features to detect poisoned samples or neutralize implanted backdoors. However, in SSL, the extremely limited number of labeled data points makes such approaches unfeasible. Specifically, common observations for backdoored model in supervised learning like activation clustering **Sokolic et al., "Robustness by Smoothing"**, loss divergence **Gu and Rigor, "Loss Functions Matter: An Empirical Study of Robustness to Adversarial Attacks"** and large-margin logit **Koh and Liang, "Understanding Black-box Predictions via Quantitative Analysis of Uncertainty, Sensitivity and Information"** no longer exists in attacked SSL models. These issues also reflect the urgency and difficulty of developing backdoor defense methods for SSL models.