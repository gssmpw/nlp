\section{Related Works}
\label{sec:2}

The application of LLMs to multimodal data is a highly active and rapidly developing area of research. In study \cite{2}, researchers examine how well these models, including VideoLLaMA-2, GPT-4o, and others, can analyze real-world and synthetic traffic videos to answer complex queries related to traffic conditions, vehicle movements, and incidents. The evaluation is conducted using a framework that measures model accuracy, relevance, and consistency across different query categories, such as basic detection, temporal reasoning, and compositional queries. The research \cite{7} explores the integration of multimodal LLM models with street view images (SVIs) to assess urban safety perception. This study leverages state-of-the-art multimodal LLMs, such as GPT-4 Vision (GPT-4V), to automatically rank safety scores for different urban environments based on SVIs. The researchers constructed a benchmark dataset, via Baidu Maps, using human-annotated safety scores and demonstrated that multimodal-LLM-generated scores closely align with human perception. Moreover, the work in \cite{8} investigates the use of multimodal-LLM, such as Gemini-Pro Vision 1.5, to detect traffic safety critical events such as sudden stops, lane changes, or crossing pedestrians that could lead to dangerous situations if not managed correctly in autonomous driving environments. 

Another research \cite{11} introduces TrafficVLM, a controllable visual-language model (VLM) designed for dense video captioning in traffic monitoring scenarios, which generates detailed, multi-phase descriptions of both vehicles and pedestrians in various traffic conditions. It processes overhead camera views, capturing fine-grained spatial and temporal details, which makes it a valuable tool for urban surveillance, ITS, and autonomous driving applications. Also, iLLM-traffic signal control (TSC) is introduced in \cite{12} to address the limitation of real-world challenges such as packet loss, communication delays, and rare events by employing a dual-step decision-making process where a reinforcement learning (RL) agent first makes an initial decision, which is then refined by an LLM that incorporates broader environmental context, logical reasoning, and missing information. Furthermore, researchers in \cite{13} explore the integration of LLMs in Adaptive Traffic Signal Control (ATSC) by applying two LLM-based traffic controllers: Zero-Shot Chain of Thought (ZS-CoT) and a Generally Capable Agent (GCA)-based controller. The ZS-CoT controller relies solely on the capabilities of the actor agent to operate without prior specific training on tasks, which resembles an open-loop feedback control system. It uses LLMs for reasoning and decision-making without prior fine-tuning. The GCA-based controller on the other hand improves upon this by integrating feedback from real-time traffic conditions to refine its decision-making process. 



TrafficGPT is introduced in \cite{14} as a novel framework that integrates LLMs with Traffic Foundation Models (TFMs) to enhance traffic management and decision-making and address the numerical data processing and real-time interaction challenges of the LLMs. This framework bridges this gap by combining multimodal traffic data sources with domain-specific models that handle traffic analysis, visualization, and control. Moreover, researchers in \cite{15} propose a framework, SeeUnsafe, that integrates multimodal LLMs to enhance traffic accident analysis from video data. It shifts manual post-processing of raw vision-based paradigm by enabling a conversational, AI-driven approach that automates video classification and visual grounding to classify the condition as normal, near-miss, and collision, extracting structured safety insights. Additionally, in study \cite{16}, ChatSUMO, an LLM-based assistant designed to automate traffic scenario generation within the Simulation of Urban MObility (SUMO) platform, is utilized, which leverages Meta's Llama 3.1 model to create, modify, and analyze traffic simulations using natural language commands, eliminating the need for coding expertise. ChatSUMO supports dynamic adjustments such as modifying road networks, optimizing traffic lights, and altering vehicle types, facilitating more flexible and interactive traffic planning via text commands. 

In contrast to the majority of related works, which mainly concentrate on classifying specific, isolated traffic scenarios (collisions, intersection behavior, single-vehicle driving), this research, this research aims to provide comprehensive traffic monitoring. Using a fine-tuned, visually grounded multimodal LLM, and help with instance segmentation, we analyze the entire traffic environment captured by multiple cameras, considering a wider range of use cases with multiple vehicles, including traffic congestion, collisions, platooning, and pedestrian crossings, incorporating diverse elements such as intersections, roundabouts, long roads, pedestrians, crosswalks, and traffic signs.

\begin{figure*} [t]
    \centering
    \includegraphics[width=0.90\linewidth]{figures/workflow.png}
    \caption{Overall Workflow of our Traffic Monitoring Pipeline Including Data Collection, Instance Segmentation and LLaVA Model.}
    \label{fig: workflow}
\end{figure*}