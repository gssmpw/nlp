 \begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Figures/proof_fig.pdf}
    \vspace{-2em}
    \caption{Personalized models should not be dismissed just because they do not provide a clear BoP gain in terms of prediction accuracy: explainability could be improved (see Theorem~\ref{thm:Bop_to_BopX}). 
    This figure shows the differences between a generic (\(h_0\)) and a personalized (\(h_p\)) model in terms of prediction accuracy and explanation quality --the latter measured by sufficiency and comprehensiveness, defined in Table \ref{tab:costs}. The generic model \(h_0\) uses both \(X_1\) and \(X_2\) for predictions based on the decision boundary \(X_1 + X_2 > 0\), while \(h_p\), with access to the group attribute \(S = X_1 + X_2\), relies entirely on \(S > 0\) for predictions (middle column). In the sufficiency evaluation (left column), where only the most important feature is kept, \(h_p\) achieves perfect prediction since it relies solely on \(S\), reaching maximum sufficiency. In contrast, \(h_0\), using \(X_1\), has a lower sufficiency score. This demonstrates that personalization enhances explainability, even though prediction accuracy remains the same. In the comprehensiveness evaluation (right column), where the most important feature is removed, \(h_p\) defaults to random guessing when only \(X = (X_1, X_2)\) is available, as it never learned to use \(X_1\) or \(X_2\). This results in \(h_p\) achieving the minimum comprehensiveness value, indicating the best explanation performance. Conversely, \(h_0\) shows a higher comprehensiveness score. Again, personalization improves explainability according to this measure, without affecting prediction accuracy.}
    \label{fig:proof_fig}
    \vspace{-0.5em}
\end{figure*}

\section{Background: Benefit of Personalization}\label{sec:framework}

This section provides the necessary background to introduce and contextualize our framework, designed to evaluate the impact of personalized machine learning models.


\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt,
    boxsep=0pt,
    arc=10pt,
    outer arc=10pt
]
\textbf{Notations.} In what follows, let $\mathcal{X}, \mathcal{S}, \mathcal{Y}$ denote, respectively, the feature, group attributes and label spaces. Additionally, we denote an auditing dataset by
$$\mathcal{D} = \{ (\mathbf{x_i}, \mathbf{s_i}, y_i) \}_{i=1}^N,$$
where $N$ is the total number of samples and, for each sample $i$, $\mathbf{x_i}\in \mathcal{X}$ represents its feature vector, $ \mathbf{s_i}\in \mathcal{S}$ its vector of group attributes, and $y_i \in \mathcal{Y}$ the corresponding label or target.
\end{tcolorbox}


Within a supervised learning setting, a \textbf{personalized model} $h_p: \mathcal{X} \times \mathcal{S} \rightarrow \mathcal{Y}$ aims to predict an outcome variable $Y \in \mathcal{Y}$ using both an input feature vector $X \in \mathcal{X}$ and a vector of group attributes $S \in \mathcal{S}$. In contrast, a \textbf{generic model} $h_0: \mathcal{X} \rightarrow \mathcal{Y}$ does not use (sensitive) group attributes. We assume that the models $h_0, h_p$ are trained on a training dataset that is independent of the auditing dataset $\mathcal{D}$. We consider that a fixed data distribution $P_{\mathbf{X}, \mathbf{S}, Y}$ is given, and that $h_0$ and $h_p$ models minimize the loss over the training dataset \( \mathcal{D}_{train} \).


\begin{definition} [Model cost] \label{def:model_cost}
The cost of a model $h$ with respect to a cost function  
$\mathrm{cost}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ is defined as:
\begin{equation}
\begin{split}
    C(h) & \triangleq \mathbb{E}[\mathrm{cost}(h, \tilde{\mathbf{X}}, Y)]~\text{(population)}, 
\quad \\
C(h, \mathbf{s}) & \triangleq \mathbb{E}[\mathrm{cost}(h, \tilde{\mathbf{X}}, Y) \mid \mathbf{S}=\mathbf{s}]~\text{(for group $s$)},
\end{split}
\end{equation}
where $\tilde{\mathbf{X}} = \mathbf{X}$ for a generic models $h_0$, and $\tilde{\mathbf{X}} = (\mathbf{X}, \mathbf{S})$ for a personalized model $h_p$. See Appendix \ref{sec:empirical_defs} for the estimates of the quantities defined above.
\end{definition} 

For example, we may want to evaluate the accuracy of the prediction of a given supervised learning model. In this case, the cost function can be the loss function used to train the model (e.g. mean squared error loss for regression, 0-1 loss for binary classification), or any auxiliary evaluation function (such as area-under-the-curve for binary classification).
Alternatively, we may want to evaluate the quality of the explanations given by an explanation method. In this case, the cost function can be one of the measures of quality of explanations, such as sufficiency which is a form of deletion of features (see \citet{Nauta_2023} for a review). In this paper we utilize incomprehensivenss and sufficiency, which measure how much model prediction changes when removing or keeping the features deemed most important in the explanation. However, while this paper focuses on prediction accuracy and explanation quality, we emphasize that the definition above --as well as our proposed framework-- can be applied to any cost function, i.e., to any quantitative evaluation of the properties of a model. 



In what follows, we assume that lower cost means better performance. We can thus quantify the impact of a personalized model in terms of the benefit of personalization, defined next.


\begin{definition} [Benefit of Personalization (BoP)] \label{BoP} The gain from personalizing a model can be measured by comparing the costs of the generic and personalized models:
\begin{equation}
\begin{split}
    \operatorname{BoP}(h_0, h_p) & \triangleq C(h_0) - C(h_p) ~\text{(population)}, \\
    \quad \operatorname{BoP}(h_0, h_p, \mathbf{s}) & \triangleq C(h_0,\mathbf{s}) - C(h_p, \mathbf{s})~\text{(group $s$)}.
\end{split}
\end{equation}
By convention, $\operatorname{BoP} > 0$ when the personalized model $h_p$ performs better than the generic model $h_0$. 
The estimates are: $\operatorname{\hat{BoP}}(h_0, h_p) =  \hat{C}(h_0) - \hat{C}(h_p)$ and $\operatorname{\hat{BoP}}(h_0, h_p, s) =  \hat{C}(h_0, s) - \hat{C}(h_p, s)$. When using $\operatorname{BoP}$ to evaluate improvement in predictive accuracy and explanation quality, it is referred to as BoP-P and BoP-X, respectively.
\end{definition}

In Appendix \ref{sec:emprical_bop}, we provide examples showing how these abstract definitions can be used to measure BoP for both predictions and explanations, each across both classification and regression tasks.





It is crucial to consider if personalization benefits each subgroup equally, and more so to investigate whether personalization actively harms particular subgroups \citep{monteiro2022epistemic}. The following concept is useful to identify the latter scenario:

\begin{definition} [Minimal Group BoP] \label{total BoP}The Minimal Group Benefit of Personalization (BoP) is defined as the minimum BoP value across all subgroups \(s \in S\):
\begin{equation}
\gamma\left(h_0, h_p\right) \triangleq
\min _{\mathbf{s} \in \mathcal{S}}
(\operatorname{BoP}(h_0, h_p, \mathbf{s})).
\end{equation}
The estimate is $\hat{\gamma}\left(h_0, h_p\right) \triangleq
\min _{\mathbf{s} \in \mathcal{S}}
(\operatorname{\hat{BoP}}(h_0, h_p, s))$.
\end{definition}

It captures the worst-case subgroup performance improvement, or the worst degradation, resulting from personalization. A positive Minimal Group BoP indicates that all subgroups receive better performance with respect to the cost function. Contrary to this, a negative value reflects that at least one group is disadvantaged by the use of personal attributes. When the Minimal Group BoP is small or negative, the practitioner might want to reconsider the use of personalized attributes in terms of fairness with respect to all subgroups.

