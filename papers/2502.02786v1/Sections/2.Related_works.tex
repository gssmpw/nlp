
\begin{table*}[h!]
\centering
\renewcommand{\arraystretch}{1.6}
\scriptsize
\begin{tabular}{|cl|l|l|}
\hline
\rowcolor[HTML]{EFEFEF} 
\multicolumn{2}{|c|}{\cellcolor[HTML]{EFEFEF}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Classification}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Regression}} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}} &
  \textbf{Loss} &
  {\color[HTML]{00009B} $C(h) = \Pr(h(\tilde{\mathbf{X}}) \neq Y \mid S = s)$} &
  $C(h) =\mathbb{E}\left[\|h(\tilde{\mathbf{X}}) - Y\|^2 \mid S = s \right]$ \\ \cline{2-4} 
\multicolumn{1}{|c|}{\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textbf{\makebox[0pt][c]{\rotatebox{90}{Predict}}}}} &
  \textbf{Evaluation metric} &
  $C(h) =-\text{AUC}(h, \mathbf{X}, Y \mid S =s)$ &
  $C(h) =-R^2(h, \mathbf{X}, Y \mid S = s)$ \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}} &
  \textbf{Sufficiency} &
  $C(h) =\Pr(h(\tilde{\mathbf{X}}) \neq h(\tilde{\mathbf{X}_J}) \mid S = s)$ &
  $C(h) =\mathbb{E}\left[\|h(\tilde{\mathbf{X}}) - h(\tilde{\mathbf{X}_J)}\|^2 \mid S = s \right]$ \\ \cline{2-4} 
\multicolumn{1}{|c|}{\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textbf{\makebox[0pt][c]{\rotatebox{90}{Explain}}}}} &
  \textbf{Incomprehensiveness} &
  $C(h) =-\Pr\left(h(\tilde{\mathbf{X}}) \neq h(\tilde{\mathbf{X}}_{\backslash J}) \mid S=s \right)$ &
  $C(h) =-\mathbb{E}\left[\| h(\tilde{\mathbf{X}}) - h(\tilde{\mathbf{X}}_{\backslash J}) \|^2 \mid S = s \right]$ \\ \hline
\end{tabular}
\caption{\textbf{Personalization benefits group $s$ if the cost of the personalized model is lower than the cost of the generic model for this group.} This table shows examples of costs (see Definition~\ref{def:model_cost}) on data $(\tilde{\mathbf{X}}, Y)$ where $\tilde{\mathbf{X}} = \mathbf{X}$ for a generic model $h_0$ and $\tilde{\mathbf{X}} = (\mathbf{X}, S)$ for a personalized model $h_p$. Existing frameworks analyze how personalization impacts predictions in classification settings (in blue). By contrast, our framework applies to all cases provided in this table. We denote by $\mathbf{X}_{ \backslash J}$ the input when removing the most important features, and by $\mathbf{X}_{J}$ its complement.}
\label{tab:costs}
\end{table*}

\section{Related Works}\label{sec:related}

\textbf{Personalization.} Our research is part of a body of work that investigates how the use of personalized features in machine learning models influences group fairness outcomes \citep{suriyakumar2023personalizationharmsreconsideringuse}. \citet{monteiro2022epistemic} defined a metric to measure the smallest gain in accuracy that any group can expect to receive from a personalized model. The authors demonstrate how this metric can be employed to compare personalized and generic models, identifying instances where personalized models produce unjustifiably inaccurate predictions for subgroups that have shared their personal data. However, this literature has focused on the classification framework and has not been generalized to regression tasks. Furthermore, this work has been solely concerned with evaluating how model accuracy is affected, and has not explored how personalizing a model affects the quality of its explanations.

\textbf{Personalization on Explainability.} The field of the effects of personalization on explainable machine learning is largely unexplored. Previous work has investigated gaps in fidelity across subgroups and found that the quality and reliability of explanations may vary across different subgroups \citep{Balagopalan_2022}. The work of \citet{Balagopalan_2022} trains a human-interpretable model to imitate the behavior of a blackbox model, and characterizes fidelity as how well it matches the blackbox model predictions. To achieve fairness parity, this paper explores using only features with zero mutual information with respect to a protected attribute. However, it left feature importance explanations out of its scope. Additionally, this work neither considers regression tasks nor looks at how personalization affects differences in explanation quality across subgroups.

\textbf{Fairness.}
In the field of machine learning, the concept of fairness aims to mitigate biased outcomes affecting individuals or groups~\citep{mehrabi2022surveybiasfairnessmachine}. Past works have defined individual fairness, which requires similar classification for similar individuals with respect to a particular task \citep{dwork2011fairnessawareness}, or group fairness \citep{dwork2018groupfairness, hardt2016equalityopportunitysupervisedlearning}, which seeks similar performance across different groups. Within machine learning fairness literature, the majority of methods, metrics, and analyses are predominantly intended for classification tasks, where labels take values from a finite set of values \citep{fairness_classification}. Among fair regression literature, multiple authors focus on designing fair learning methods rather than developing metrics for measuring fairness in existing models \citep{berk2017convexframeworkfairregression, Fukuchi2013PredictionWM, p√©rezsuay2017fairkernellearning, Calders2013ControllingAE}. Complimentary contributions focus on defining fairness criteria and establishing methods to evaluate fairness for regression tasks \citep{gursoy2022errorparityfairnesstesting, agarwal2019fairregressionquantitativedefinitions}.

We extend related works tackling explainability in Appendix \ref{sec:related_works}.

