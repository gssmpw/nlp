\section{Comparison BoP for Prediction and BoP for explainability Proofs}\label{sec:proof-counterexamples}

\paragraph{Proof for Theorem \ref{thm:Bop_to_BopX}:}
\begin{proof}
    Let $\mathbf{X} = (x_1, x_2)$ where $x_1$ and  $x_2$ are independent and each follows $\text{Unif}(-\frac{1}{2},\frac{1}{2})$. Let us define $S \in \{0, 1 \}$ as $S = \mathds{1}(X_1 + X_2 > 0) $ and $Y = S$. Then, $h_0(x) = \mathds{1}(X_1 + X_2 > 0) $ and $h_p(x) = \mathds{1}(S > 0) $ can both achieve perfect accuracy. Therefore, $\text{BoP}(h_0, h_p) = 0$. 

    For explanation, let us assume $r=1$. Then, for model $h_0$, its important feature set $J_0$ will be either $\{ X_1 \}$ or $\{ X_2 \}$, and without loss of generality, let $J_0 = \{ X_1 \}$. For the personalized model, $J_p = \{ S \}$. Then, comprehensiveness of $h_0$ is 
    \begin{align}
        \text{Pr}(h_0(X) \neq h_0(X_{\setminus J_0}) ) &= \text{Pr}(X_1 + X_2 \leq 0 | X_2 > 0) \text{Pr}(X_2 > 0) \nonumber \\
        &+ \;\; \text{Pr}(X_1 + X_2 > 0 | X_2 \leq 0) \text{Pr}(X_2 \leq 0) \label{eq:h0_comp} \\ 
        &= \text{Pr}(X_1 + X_2 \leq 0 | X_2 > 0) \cdot \frac{1}{2} + \text{Pr}(X_1 + X_2 > 0 | X_2 \leq 0) \cdot \frac{1}{2} \nonumber  \\ 
        &= \text{Pr}(X_1 + X_2 \leq 0 | X_2 > 0) \quad \text{(due to symmetry of the distribution)} \nonumber \\
        &= \int_{x_2 >0, x_1+x_2 \leq 0} \text{Pr}(x_1, x_2) dx_1 dx_2 \nonumber / \text{Pr} ( X_2 >0)\\
        &= 2 \cdot  \int_{x_2=0}^{\frac{1}{2}} \text{Pr}(x_2) \int_{x_1 \leq -x_2} \text{Pr}(x_1) dx_1 dx_2  \nonumber \\ 
        &= 2 \cdot  \int_{x_2=0}^{\frac{1}{2}} \text{Pr}(x_2) (-x_2+\frac{1}{2}) dx_2  \nonumber \\ 
         &= 2 \cdot \left[ -\frac{1}{2} x_2^2 + \frac{1}{2} x_2 \right]_{0}^{\frac{1}{2}} \nonumber \\ 
        &= \frac{1}{4}. \nonumber 
    \end{align}
    For $h_p$, comprehensiveness is : 
    \begin{align*}
        \text{Pr}(h_p(X,S) \neq h_p(X_{\setminus J_p}, S_{\setminus J_p}) ) = \frac{1}{2},
    \end{align*}
    as without $S$, $h_p$ can only make a random guess. Hence, BoP-X in terms of comprehensiveness is $\frac{1}{4}$. 

    For sufficiency, we can do a similar analysis: 
    \begin{align}
        \text{Pr}(h_0(X) \neq h_0(X_{J_0}) ) &= \text{Pr}(X_1 + X_2 \leq 0 | X_1 > 0) \text{Pr}(X_1 > 0) \nonumber \\
        & \;\; + \text{Pr}(X_1 + X_2 > 0 | X_1 \leq 0) \text{Pr}(X_1\leq 0) \label{eq:h0_suff} \\
        &= \frac{1}{4}. \nonumber
    \end{align}
    Again, due to symmetry, \eqref{eq:h0_suff} is the same as \eqref{eq:h0_comp}. On the other hand, the sufficiency for $h_p$ is 
    \begin{align*}
        \text{Pr}(h_p(X,S) \neq h_p(X_{J_p}, S_{J_p})) = 0, 
    \end{align*}
    as $J_p = \{ S \}$ is sufficient to make a prediction for $h_p$. Thus, BoP-X in terms of sufficiency is also $\frac{1}{4}$. 

$h_p(X)$ = random guess
    
\end{proof}

\paragraph{Proof for Theorem \ref{thm:BopX_to_Bop}:}

See Figure \ref{fig:thm_4_2} for a visualization of Theorem \ref{thm:BopX_to_Bop} for a linear model with $h_0$ and $h_p$ Bayes optimal regressors.

\begin{figure*}\label{fig:thm_4_2}
    \centering
    \includegraphics[width=1\linewidth]{Figures/fig_th4.2_simple.pdf}
    \vspace{-1.5em}
    \caption{For a linear model, absence of benefit in explanation quality means that there is also an absence of benefit in prediction accuracy, as illustrated here (see Theorem~\ref{thm:BopX_to_Bop}). We consider a linear model $Y = X + S + \epsilon$, with $h_0$ and $h_p$ Bayes optimal regressors. In this example, absence of benefit of personalization for the explanation quality, $\text{BoP-X}^{\text{suff}}=0$ evaluated in terms of sufficiency (left column) means: $\Delta \text{MSE}_0 = \Delta \text{MSE}_p \Rightarrow \text{var}(X) =0$. Then, absence of benefit of personalization for the explanation quality, $\text{BoP-X}^{\text{comp}}=0$ evaluated in terms of comprehensiveness (right column) means: $\Delta \text{MSE}_0 = \Delta \text{MSE}_p \Rightarrow \text{var}(S) = \text{var}(X) \Rightarrow \text{var}(S) =0$. This allows us to conclude that, in terms of prediction accuracy (middle column): $\text{MSE}_0 = \text{MSE}_p$ and hence there is also no benefit of personalization in prediction :$\text{BoP-P}=0$.}
    \label{fig:proof_fig2}
    \vspace{-1em}
\end{figure*}

\begin{proof}
    A Bayes optimal  regressor using a subset of variables from indices in $J \subseteq [1, \ldots, t+k]$ would be given as:
    \begin{equation} \label{eqn:bayes}
        \hat{y} = h^{*}_J(\mathbf{x}_J, \mathbf{s}_J) = \sum_{\substack{j \in J, \\ j \leq t}} \alpha_j x_j + \sum_{\substack{j \in J, \\ j 
        \geq t+1}} \alpha_j S_{j-t}, 
    \end{equation}
    where $h^{*}_J$ represents an Bayes optimal regressor for the given subset $J$, and $\mathbf{x}_J$ and $\mathbf{x}_J$ are sub-vectors of $\mathbf{x}$ and $\mathbf{s}$, using the indices in $J$. Then, the MSE of $h^{*}_J$ is given as: 
    \begin{equation} \label{eqn:bayes_mse}
        \text{MSE}(h^{*}_J) =   \sum_{\substack{j \in \setminus J, \\ j \leq t}} \alpha_j^2  \text{Var}(X_j) + \sum_{\substack{j \in \setminus J, \\ j 
        \geq t+1}} \alpha_j^2 \text{Var}(S_{j-t}),
    \end{equation}
    where $\setminus J$ is a shorthand notation for $[1, \ldots t+k] \setminus J$. By combining \eqref{eqn:bayes} and \eqref{eqn:bayes_mse}, we can obtain: 
    \begin{align}
        \text{MSE}(h_0) &= \sum_{j=t+1}^{t+k} \alpha_j^2 \text{Var}(S_{t+j}) + \text{Var} (\epsilon), \label{eqn:mse_h0} \\ 
        \text{MSE}(h_p) &= \text{Var} (\epsilon). \label{eqn:mse_hp}
    \end{align}
    We define $J_0$ and $J_p$ as a set of important features for $h_0$ and $h_p$. Note that $J_0$ and $J_p$ are the same across all samples for the additive model. Then, for regressors for sufficiency, we can write the MSE as: 
    \begin{align}
        \text{MSE}(h_{0,J}) &=  \sum_{\substack{j \in \setminus J_0, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t})  +  \sum_{j=t+1}^{t+k} \alpha_j^2 \text{Var}(S_{j-t}) + \text{Var} (\epsilon) \\ 
        \text{MSE}(h_{p,J}) &= \sum_{\substack{j \in \setminus J_p, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t}) + \sum_{\substack{j \in \setminus J_p, \\ j \geq t+1}}\alpha_j^2 \text{Var}(S_{j-t}) + \text{Var} (\epsilon).
    \end{align}
    Similarly, for regressors for incomprehensiveness, MSE can be written as: 
    \begin{align}
        \text{MSE}(h_{0,\setminus J}) &=  \sum_{\substack{j \in J_0, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t})  +  \sum_{j=t+1}^{t+k} \alpha_j^2 \text{Var}(S_{j-t}) + \text{Var} (\epsilon), \\ 
        \text{MSE}(h_{p,\setminus J}) &= \sum_{\substack{j \in J_p, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t}) + \sum_{\substack{j \in  J_p, \\ j \geq t+1}}\alpha_j^2 \text{Var}(S_{j-t}) + \text{Var} (\epsilon).
    \end{align}
    Then, our assumption of $\text{BoP-X} = 0$ for sufficiency becomes: 
    \begin{align} 
         \text{MSE}(h_0) - \text{MSE}(h_{0,J}) &= \text{MSE}(h_p) - \text{MSE}(h_{p,J}). \label{eqn:mse_suff}
     \end{align}
     We can expand $\text{MSE}(h_0) - \text{MSE}(h_{0,J}) $ as: 
     \begin{align}
          \text{MSE}(h_0) - \text{MSE}(h_{0,J}) &= \mse(h_0) \left( 1- \frac{\mse(h_{0,J})}{\mse(h_0)} \right)  \nonumber \\
          &=  \mse(h_0) \left( 1- \frac{\var(\jcomp_0) + \var(S) + \var (\epsilon)}{\var(S) + \var(\epsilon)} \right) \nonumber \\ 
&=  \mse(h_0) \frac{\var(\jcomp_0)}{\var(S) + \var(\epsilon)} \nonumber \\     
&= \mse(h_0)  \frac{\var(J_0) + \var(\jcomp_0)}{\var(S) + \var(\epsilon)} \frac{\var(\jcomp_0) }{\var(J_0) + \var(\jcomp_0)}, \label{eqn:h_0_suff} 
\end{align}

where we use the shorthand notations: 
\begin{align*}
    \var(J_0) &=  \sum_{\substack{j \in J_0, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t}), \\ 
    \var(\jcomp_0) &=  \sum_{\substack{j \in \setminus J_0, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t}), \\ 
    \var(S) &= \sum_{j=t+1}^{t+k} \alpha_j^2 \text{Var}(S_{t+j}).
\end{align*}
Further, note that 
\begin{align*}
    \var(J_0)  + \var(\jcomp_0) = \sum_{\substack{j \in J_0, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t}) + \sum_{\substack{j \in \setminus J_0, \\ j \leq t}}\alpha_j^2 \text{Var}(X_{t}) = \sum_{j=1}^{t} \alpha_j^2 \var(X_j) = \var(X). 
\end{align*}
Defining $M(h_0) \triangleq \mse(h_0)  \frac{\var(X)}{\var(S) + \var(\epsilon)}$ and $r_0 \triangleq\frac{\var(J_0) }{\var(J_0) + \var(\jcomp_0)}$, we further simplify \eqref{eqn:h_0_suff} as: 
\begin{align}
    \text{MSE}(h_0) - \text{MSE}(h_{0,J}) =  M(h_0) (1 - r_0). \label{eqn:h_0_suff_simp}
\end{align}

Through a similar process, we can simpilfy $\text{MSE}(h_p) - \text{MSE}(h_{p,J})$ as: 
\begin{align}
    \text{MSE}(h_p) - \text{MSE}(h_{0,p}) =  M(h_p) (1 - r_p), \label{eqn:h_p_suff_simp}
\end{align}
where $M(h_p) \triangleq \frac{\var(X) + \var(S)}{\var(\epsilon)} \mse (h_p)$ and  $r_p \triangleq\frac{\var(J_p) }{\var(J_p) + \var(\jcomp_p)}$
Using \eqref{eqn:h_0_suff_simp} and \eqref{eqn:h_p_suff_simp}, we arrive at: 

         \begin{equation} \label{eqn:suff_M}
           M(h_0) (1-r_{0}) =  M(h_p)  (1-r_{p}).
         \end{equation}
       
    

    By taking similar steps using comprehensiveness, we can derive: 
    \begin{equation}  \label{eqn:comp_M}
         M(h_0) r_{0} =  M(h_p)  r_{p}.
    \end{equation}
By combining \eqref{eqn:suff_M}  and \eqref{eqn:comp_M}, we can conclude that: 
\begin{align*}
    \frac{r_{0}}{r_{p}} = \frac{1-r_{0}}{1-r_{p}} \implies r_{0} = r_{p}.
\end{align*}
Plugging this back to  \eqref{eqn:suff_M}, we get: $M(h_0) = M(h_p)$. Now, let us assume that $\text{BoP-P} > 0$, and prove it by contradiction. Comparing \eqref{eqn:mse_h0} and \eqref{eqn:mse_hp}, we can deduce that  $\text{BoP-P} > 0$ means $\var(S) > 0$. Expanding $M(h_0) = M(h_p)$, we get: 
\begin{align*}
    \mse(h_0)  \frac{\var(X)}{\var(S) + \var(\epsilon)} &= \frac{\var(X) + \var(S)}{\var(\epsilon)} \mse (h_p), \\ 
    \mse (h_p)  &= \frac{\var(X)}{ \var(X) + \var(S) } \frac{\var(\epsilon)}{ \var(S) + \var(\epsilon) }  \mse(h_0), \\ 
    &=  \frac{\var(X)}{ \var(X) + \var(S) }   \mse (h_p).
\end{align*}
Since $\var(S) > 0$, this equality cannot hold. This concludes that $\text{BoP-P} = 0$. We can make the same claim with similar logic for a classifier where $Y$ is given as: 
\begin{equation}
     Y = \mathds{1} (\alpha_1 X_1 + \cdots \alpha_t X_t + \alpha_{t+1} S_1 + \cdots + \alpha_{t+k} S_k + \epsilon > 0).
\end{equation}
\end{proof}

\begin{comment}
\begin{proof}
    The Bayes optimal classifier will be given as: 
    \begin{align}
        h_0^{*}(\mathbf{x}) &= P(y | \mathbf{x}), \\
        h_p^{*}(\mathbf{x}) &= P(y | \mathbf{x}, s).
    \end{align}
    (i) Let $S \notin J$. Then, the Bayes optimal classifiers for sufficiency are given as: 
    \begin{align}
        h_0^{*}(\mathbf{x}_J) &= P(y | \mathbf{x}_J), \\
        h_p^{*}(\mathbf{x}_J, s_J) &= P(y | \mathbf{x}_J).
    \end{align}
    As we assumed $\text{BoP-X} = 0$, sufficiency should be 0: 
    \begin{equation}
        \ell(h_0^{*}) - \ell(h_{0,J}^{*}) = \ell(h_p^{*}) - \ell(h_{p,J}^{*}). 
    \end{equation}
Since $h_{0,J}^{*}$ and $h_{p,J}^{*}$ are identical, we can conclude that $\ell(h_0^{*}) = \ell(h_p^{*})$. Hence, 
\begin{equation}
    \text{BoP-P} \leq \ell(h_0^{*}) - \ell(h_p^{*}) = 0. 
\end{equation}

(ii) Let $S \in J$. Then, the Bayes optimal classifiers for incomprehensiveness are given as: 
    \begin{align}
        h_0^{*}(\mathbf{x}_{\setminus J}) &= P(y | \mathbf{x}_{\setminus J}), \\
        h_p^{*}(\mathbf{x}_{\setminus J}, s_{\setminus J}) &= P(y | \mathbf{x}_{\setminus J}).
    \end{align}
    As we assumed $\text{BoP-X} = 0$, incomprehensiveness should also be 0: 
    \begin{equation}
        \ell(h_0^{*}) - \ell(h_{0,\setminus J}^{*}) = \ell(h_p^{*}) - \ell(h_{p, \setminus J}^{*}). 
    \end{equation}
With similar arguments, we can conclude: 
\begin{equation}
    \text{BoP-P} \leq \ell(h_0^{*}) - \ell(h_p^{*}) = 0. 
\end{equation}
Hence, when BoP-X$=0$, BoP-P$=0$. 
\end{proof}
\end{comment}



