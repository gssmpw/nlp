\section{Testing the Impact of Personalization}\label{sec:validation}


Calculating the BoP requires exact knowledge of the data distribution, a condition rarely met in practice. Within the ubiquitous finite sample regime, it is critical to understand the reliability of the empirical BoP: if we compute a positive empirical BoP on the audit dataset, does this mean that the true, unknown BoP is also positive?


Drawing inspiration from \cite{monteiro2022epistemic}, we consider the most natural hypothesis testing framework to assess whether a personalized model yields a substantial performance improvement across all groups. Subsequently, we derive a novel information-theoretic bound on the reliability of this procedure. 

Proofs for theorems, lemmas and corollaries are in Appendices
\ref{subsec:any_distribution}, \ref{sec:proof-binary}, \ref{sec:proof-real-valued}, \ref{sec:proof-real-valued-laplace}, \ref{sec:proof-counterexamples} and \ref{sec:max_attributes}.



\textbf{Hypothesis Test} Given an auditing dataset $\mathcal{D}$, we verify whether using a personalized model $h_p$ yields an $\epsilon>0$ gain in expected performances compared to using a generic model $h_0$. 
The improvement $\epsilon$ is in cost function units, and corresponds to the reduction in cost for the group for which moving from $h_0$ to $h_p$ is least advantageous --i.e., $\epsilon$ represents the improvement for the group that benefits the least from the personalized model. 
The null and the alternative hypotheses of the test write:
\begin{align*}
H_0: \gamma(h_0, h_p; \mathcal{D}) \leq 0  
\Leftrightarrow &
\ \text{Personalized $h_p$ does not bring} \\ & \ \text{any gain for at least one group,} \\
H_1: \gamma(h_0, h_p; \mathcal{D}) 
\geq \epsilon 
\Leftrightarrow &
\ \text{Personalized $h_p$ yields at least $\epsilon$} \\ & \ \text{improvement for all groups.}
\end{align*}

To perform the hypothesis test, we follow \citep{monteiro2022epistemic} and use the threshold test on the estimate of the BoP $\hat \gamma$:
\begin{align*}
    \hat \gamma \geq \epsilon \Rightarrow & \text{ Reject $H_0$: Conclude that personalization yields} \\ & \text{ at least $\epsilon$ improvement for all groups.}
\end{align*}
\textbf{Reliability of the Test} We are interested in characterizing the reliability of the hypothesis test: in other words, can we trust the value of the empirical $\hat \gamma$ ? We characterize the reliability of the test in terms of its probability of error, $P_e$, defined as:
\begin{align*}
    P_e 
    = &
    \ \text{Pr}(\text{Type I error}) 
    + 
    \text{Pr}(\text{Type II error})
\\
    = &
    \ \text{Pr}(\text{Rejecting $H_0$} | \text{$H_0$ is true}) + \\
    & 
    \ \text{Pr}(\text{Failing to reject $H_0$} | \text{$H_1$ is true}).
\end{align*}
If this probability exceeds 50\%, the test is no more reliable than the flip of a fair coin, making it too untrustworthy to support any meaningful verification. Therefore, it would be practical to compute a lower bound on the worst case scenario for this probability of error, so that if this lower bound exceeds 50\%, we would not trust the test. We precisely derive the bound for a general cost function, such that it applies for any prediction (regression and classification) and any explainability quality measures. 

\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt,
    boxsep=0pt,
    arc=10pt,
    outer arc=10pt
]
\textbf{Notation.} We formalize the hypothesis test by an abstract \emph{decision} function $\Psi : (h_0, h_p, \mathcal{D}, \epsilon) \rightarrow \{0,1\}$ such that $
\Psi(h_0, h_p, \mathcal{D}, \epsilon) = 1 \Rightarrow \text{ Reject $H_0$}.$
\end{tcolorbox}


In particular, a minimax bound is used to establish a lower bound on $P_e$. This involves considering the worst-case data distributions that maximizes $P_e$ and the best possible decision function $\Psi$ that minimizes it. Assuming that the value of $\epsilon$ is fixed, we provide the following result: 
\begin{theorem}[Lower bound for BoP]\label{th:lower_bound}
The lower bound writes:
\begin{equation} \label{eq:lower_bound}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        \geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{p^{\epsilon}(B)}{p(B)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{equation}
where $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) \geq \epsilon$. Dataset $\mathcal{D}$ is drawn from an unknown distribution and has $d$ groups where $d=2^k$, with each group $j$ has $m_j$ samples. The probabilities $p$ and $p_\epsilon$ refer to distributions of the individual BoP, i.e., $B=C(h_0, X, Y) - C(h_p, X, Y)$, where every individual follows $p(B)$, except for individuals in one group which follow $p^\epsilon(B)$.
\end{theorem}

This lower bound reflects the worst-case performance of any hypothesis test, by exhibiting a pair of distributions—one under \( H_0 \) and the other under \( H_1 \)—for which the probability error $P_e$ test will be greater than or equal to the lower bound. Hence, this theorem offers a practical method to assess whether we should trust the value of the empirical benefit (or risk) of a personalized model. If the probability distribution of individual BoP is known, practitioners can directly compute the lower bound specific to their case. If unknown, the distribution can be estimated beforehand. The method proceeds as follows: 1) fit the generic model $h_0$ and the personalized model $h_p$, 2) plot the histograms of the individual BoP and identify its probability distribution (Gaussian, Laplace, etc.), and 3) apply Theorem~\ref{th:lower_bound} to evaluate whether the observed BoP is sufficient. For completeness, we provide in the corollary below the value of the lower bound for any distribution in the exponential family.

\begin{corollary}[Lower Bound Exponential Family Distributions]\label{cor:ef} Consider $p$ a distribution in the exponential family with natural parameter $\theta$ and $M_p$ the moment generating function of its sufficient statistic. The lower bound for the probability of the error in the hypothesis test is:
\begin{align*}
      &\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e \\
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{M_p(2\Delta\theta)}{M_p(\Delta \theta)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
        &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(1+4\epsilon^2\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}~\text{(Categorical)}\\
        &=1 - \frac{1}{2\sqrt{d}} 
        \Bigg[
            \frac{1}{d} \sum_{j=1}^d \exp\left(\frac{{m_j}\epsilon^2}{\sigma_j^2}\right)
            -
            1
            \Bigg]^{\frac{1}{2}}~\text{(Gaussian)}\\
\end{align*}
\end{corollary}


In the case of categorical distributions, this result generalizes the result in \citep{monteiro2022epistemic}, by providing a finer bound where the groups are also not constrained to have the same number of samples. Appendix \ref{subsec:symm_gauss} also provides the formula for any probability distribution in the symmetric generalized Gaussian family. The Laplace distribution lower bound is found from this and provided below.

\begin{corollary}[Lower bound Laplace Distribution]\label{cor:laplace} The lower bound for the probability of the error in the hypothesis test is:
\begin{align*}
         &\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e \\
        &\geq1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d 
            \exp\left(-\frac{m_j\epsilon }{b_j} \right)
            -1
            \right]^{\frac{1}{2}}~\text{(Laplace)}.
\end{align*}
\end{corollary}

These results characterize how the probability of error relates to the number $k$ of group attributes, which defines the number $d=2^k$ of groups, and to the number of samples per group. Fewer samples per group lead to a higher chance of error in hypothesis testing, thus yielding a fundamental limit on the maximum number of group attributes a personalized model can handle to ensure every group benefits, without relying on additional data distribution assumptions. These ideas are made concrete by the following results.


\textbf{Maximum Number of Personal Attributes} Given the lower bound obtained in Corollary \ref{cor:ef} and \ref{cor:laplace}, we compute the maximum number of attributes $k$ for which testing the benefit of personalization makes sense. Here, we assume that each group $j$ has the same number of samples $m_j = m = \left[\frac{N}{d}\right]$.



\begin{corollary}[Maximum number of attributes]
    If we wish to maintain a probability of error such that $\min \max P_e \leq 1/2$, then the number of attributes $k$ should be chosen below a value $k_{\max}$ such that:
    \begin{align*}
        k_{max} 
         &= 1.4427 W\left(N \log(4 \epsilon^2+1)\right)~\text{(Categorical BoP)}\\
         &= 1.4427W\left(\frac{\epsilon^2 N}{\sigma^2}\right)~\text{(Gaussian BoP, variance $\sigma^2$)}\\
         &= 1.4427W\left(\frac{-\epsilon N}{b}\right)~\text{(Laplace BoP, scale $b$)},
    \end{align*}
where $W$ is the Lambert W function.
\end{corollary}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Figures/Pe_vs_k_plot.pdf}
    \vspace{-0.5cm}
    \caption{Lower bound of the probability of error $P_e$ versus number of attributes $k$, defining the number of groups $d = 2^k$ for $\epsilon=0.01$. For three different number of samples $N$, we consider a categorical BoP (orange), Gaussian BoPs with different variance $\sigma^2$ (blue), and Laplace BoPs with several scale parameters $b = \frac{\sigma}{\sqrt{2}}$. 
    We see that for small $\sigma$ values in the Gaussian case, the number of attributes $k$ that can be used before surpassing $P_e\geq 1/2$ is higher than for the categorical case. The Laplace case surpasses the categorical in all cases, and the Gaussian in most. For this example, we utilize the $P_e$ functions assuming each group has $m = \lfloor \frac{N}{d} \rfloor$ samples.}
    \label{fig:pe_versus_k}
\end{figure*}

From this corollary, we first see that there is an absolute limit in the number of personal attributes that can be input to any model where each data point represents one person: the number $k_\text{max}$ for $N$ equal to its maximum value, i.e., the number of people on the planet, is equal to 18, 22, and 26 binary attributes for Categorical, Gaussian, and Laplace BoP distributions, respectively. 

Next, given a single dataset, the maximum number of personal attributes allowed to test for the benefit of personalization depends on the machine learning model: classification versus regression.

\textbf{Classification Versus Regression}
Figure~\ref{fig:pe_versus_k} plots the relation between $k$ and the lower bound of $P_e$ for common sample sizes in medical applications, contrasting classification versus regression cases. The BoP is assumed to be Gaussian of variance $\sigma^2$ for the regression. We clearly observe the consequences of the extra dependency on $\sigma$. Small values of $\sigma$ allow for a higher number of personal attributes $k_\text{max}$ in regression compared to the classification case. This means that we can rely on the value of the empirical BoP to test whether there is indeed a benefit of a personalized regression model, even when a high number of personal attributes are used. This tells a much more subtle story than for the classification case, because the maximum number of attributes allowed depends on the distribution of the BoP across participants. 

\paragraph{Prediction versus Explainability}

Within the setting of classification, the maximum number of personal attributes allowed does not differ in prediction versus explainability: both utilize an individual cost function that can be described by a Bernoulli random variable.  Within the setting of regression, the maximum number of attributes can differ between prediction and explainability, for both sufficiency and incomprehensiveness. This is because this number depends on the distribution of the individual BoP and the standard deviation of the BoP across participants. Therefore,  the number of allowed attributes will differ provided this value is different for each criteria evaluated. 


