\section{Introduction}\label{sec:introduction}

\textit{Personalization} refers to the process of tailoring a Machine Learning (ML) model to individual users by providing their unique characteristics as inputs. These user attributes often take the form of demographic information and may include sensitive factors like sex, race, or religion -- factors historically been associated with bias and unequal treatment. Additionally, user attributes may involve characteristics that are expensive to obtain, such as medical assessments requiring an in-person visit, like the Patient Health Questionnaire-9 \citep{kroenke2001phq9}, used by clinicians to monitor depression. When users provide sensitive or costly personal attributes to a ML model, a performance improvement is implicitly assumed. But does this assumption always hold?


Personalization offers significant advantages in various ML applications. Although user attributes may be sensitive, incorporating them can enhance predictive accuracy. For example, the accuracy of ML models that predict cardiovascular disease risk often improves when including sex \citep{womencvd1,womencvd2, womencvd3} and race \citep{racecvd}. This is because men and women, as well as different racial subgroups, exhibit distinct patterns of heart disease risk --for example, there is an increased prevalence of hypertension in African American populations~\citep{flack2003epidemiology}. Thus, in healthcare, using personal attributes can enhance clinical prediction models by accounting for biological and sociocultural differences that affect health outcomes.


While personalizing ML models offers potential benefits, it also poses significant risks. Incorporating sensitive attributes such as race, gender, or age often amplifies biases within ML models. For example, in healthcare, \citet{Obermeyer2019} revealed that a widely used algorithm exhibited racial bias -- falsely evaluating black patients as healthier than equally sick white patients, reducing their access to extra care by more than half. Hence, including personal attributes in ML models can lead to biased outcomes, potentially perpetuating damaging inequality. 


This showcases the need of a quantitative approach to assess the benefits and risks associated with personalizing ML models. To evaluate the impact of model personalization, it is crucial to first clearly define the models' objectives. In particular, this work puts the focus on two fundamental goals: \textit{(i)} achieving accurate predictions, and \textit{(ii)} providing clear explanations of those predictions -- the latter being particularly crucial in sensitive decision-making contexts. 
Thus, our central question becomes: \textit{can we evaluate whether personalizing a model will improve or diminish both prediction accuracy and explanation quality?}

This question is more nuanced than it might initially seem, as incorporating personal data does not necessarily guarantee improvements across all population subgroups. For example, when using a classification model to predict sleep apnea, including age and sex as attributes reduced the overall prediction error, but led to increased errors for two specific subgroups: older women and younger men~\citep{suriyakumar2023personalizationharmsreconsideringuse}. Similarly, the quality and reliability of explanation measures have been experimentally shown to vary between different subgroups~\citep{Balagopalan_2022}. Hence, before personalizing a model, practitioners might want to ensure that it will provide clear performance gains across all subgroups involved, both in terms of prediction and explanation --see Fig. \ref{fig:overview}.


Current quantitative methods for evaluating the impact of personalization focus on a narrow subset of prediction tasks and datasets, and cannot address explanation qualities.\footnote{Attribution-based explanations measure the contribution of input features to the model output, and their quality refers to how well they reflect the underlying relationships behind predictions.} For instance, the Benefit of Personalization (BoP) framework introduced by \citet{monteiro2022epistemic} applies only to binary classification tasks. Additionally, it requires subgroups of equal size, which restricts its applicability to a small subset of models and datasets. In the realm of explanation, the impact of personalization has only been quantified experimentally, for binary classifiers, and for a single explanation method \citep{Balagopalan_2022}. These constraints limit the practical relevance of existing frameworks, as real-world datasets and ML scenarios rarely align with such restrictive assumptions. 


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/overview_lpersonalization.pdf}
    \vspace{-1.7em}
    \caption{Impact of personalization on fairness in prediction and explainability.  
(A) Comparison between a generic ML model (\( h_0 \)), which uses only input features (\( \mathbf{X} \)), and a personalized model (\( h_p \)), which incorporates additional personal attributes (\( S_1, \dots, S_k \)).  
(B) Group-specific effects on prediction and explanation under personalization. While some groups benefit from improved prediction and explanation, others experience trade-offs, including worsened prediction accuracy or explainability.}
    \label{fig:overview}
    \vspace{-1.2em}
\end{figure}

\paragraph{Contributions.} This work revisits, unifies, and generalizes previous methodologies to provide a comprehensive understanding of the impact of personalization in ML. More specifically:
\begin{itemize}[leftmargin=*] 
    \setlength\itemsep{0em}
    \setlength\parskip{0em} 
    \item We propose a unifying framework to evaluate personalization in both classification and regression settings.
    \item Besides prediction accuracy, we provide a novel analysis on how personalization affect the model's explanation, a perspective absent from the current literature.   
    \item We provide new insights in comparing classification and regression scenarios, demonstrating that regression models can potentially utilize more group attributes than classification models. 
    \item We discover improvements in prediction accuracy from personalization do not necessarily correlate with enhanced explainability. This underscores the importance of evaluating both criteria in models where accuracy and interpretability are paramount.
    \item We provide guidance to practitioners on how to apply and validate the framework through a real-world dataset example.
\end{itemize}