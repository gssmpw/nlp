\section{Experiments}\label{sec:results}

This section shows how we can apply the framework to determine whether personalization benefits classification and regression tasks, leveraging to that end the validation tools developed in previous Section \ref{sec:validation}. 

\textbf{Dataset.} We apply our framework to the MIMIC-III Clinical Database \citep{mimic-3} utilizing two binary group attributes: $\mathrm{Age} \times \mathrm{Race}  \in \{\mathrm{\mathrm{18-45}}, \mathrm{45+}\} \times \{\mathrm{White (W)}, \mathrm{NonWhite (NW)}\}$. For the regression task, the goal is to predict the patient length of stay. For the binary classification task, the goal is to predict if length of stay $> 3$ days. We consider a 70/30 split for training and test sets on both classification and regression, fitting two neural network models in each setting: one with a one-hot encoding of the group attributes ($h_p$), and the other without group attributes ($h_0$). Lastly, regression prediction values are normalized to have mean 0 and standard deviation 1.


\textbf{Explainability Method.} We generate model explanations with the Captum Integrated gradient explainer method \citep{sundararajan2017axiomaticattributiondeepnetworks}. This method calculates the gradient of the output with respect to the input for each subject, and scales the result to get an importance value for each input feature. To evaluate BoP-X using sufficiency and incomprehensivess, we select an value $r$ such that $50\%$ of features are kept or removed. Plots in Appendix \ref{subsec:mimic_plots} depict how sufficiency and incomprehensiveness change for different values of $r$, as well as show the individual BoP distributions.

\begin{table}[t]
\centering
\resizebox{0.48\textwidth}{!}{ 
\normalsize
\begin{tabular}{lcccc}
\toprule
\multicolumn{5}{c}{\textbf{Classification Results}} \\
\midrule
\textbf{Group} & $n$ & \textbf{Prediction} & \textbf{Incomp.} & \textbf{Sufficiency} \\
\midrule
White, 45+     & 8443  & 0.0063  & \textcolor{darkred}{-0.0226}  & 0.0053 \\
White, 18-45   & 1146  & 0.0044  & 0.0489  & 0.0244 \\
NonWhite, 45+  & 3052  & \textcolor{darkred}{-0.0026}  & \textcolor{darkred}{-0.0023}  & 0.0029 \\
NonWhite, 18-45& 696   & \textcolor{darkred}{-0.0216}  & 0.0560  & 0.0072 \\
All Pop.       & 13337 & 0.0026  & \textcolor{darkred}{-0.0077}  & 0.0065 \\
\midrule
\textbf{Minimal BoP} & 13337 & \textcolor{darkred}{-0.0216}  & \textcolor{darkred}{-0.0226}  & 0.0029 \\
\midrule
\multicolumn{5}{c}{\textbf{Regression Results}} \\
\midrule
\textbf{Group} & $n$ & \textbf{Prediction} & \textbf{Incomp.} & \textbf{Sufficiency} \\
\midrule
White, 45+     & 8379  & 0.0021  & \textcolor{darkred}{-0.0906}  & 0.1914 \\
White, 18-45   & 1197  & 0.0023  & 0.1219  & 0.2223 \\
NonWhite, 45+  & 3044  & 0.0108  & \textcolor{darkred}{-0.0501}  & 0.3494 \\
NonWhite, 18-45& 717   & 0.0212  & 0.0441  & 0.3293 \\
All Pop.       & 13337 & 0.0051  & \textcolor{darkred}{-0.0550}  & 0.2376 \\
\midrule
\textbf{Minimal BoP} & 13337 & 0.0021  & \textcolor{darkred}{-0.0906}  & 0.1914 \\
\bottomrule
\end{tabular}
} 

\caption{Experimental results on the test set of the MIMIC-III dataset for both classification and regression.  
All columns show the value of $\hat{C}(h_0) - \hat{C}(h_p)$ evaluated for the corresponding metric.  
Incomprehensiveness is abbreviated as incomp. and population as pop.  
Values that are worsened by $h_p$ are colored red.}
\label{tab:full_results}
\end{table}

\newpage
\textbf{Experimental Results.} Table \ref{tab:full_results} shows full results of the Population, Groupwise, and Minimal Group BoP on the test set. The 0-1 and square loss cost functions are used for classification and regression. From these tables in the regression task, we see subgroups for whom BoP-P is positive, while BoP-X is negative. In the classification task, personalization improves prediction and sufficiency, whereas it worsens incomprehensiveness for some subgroups. 

\textbf{Statistical Validation.} To better understand the reliability of our empirical results, we apply the procedure described in the previous section.  First, we plot the histograms of the individual BoP for the regression task, as shown in Figures~\ref{fig:mimic-regr-pred}, \ref{fig:mimic-regr-incomp}, and \ref{fig:mimic-regr-suff} in Appendix \ref{subsec:mimic_plots}. For the classification task, these distributions will all be categorical. Next, we estimate the probability distribution family that has generated these empirical distributions. In the sufficiency case, this is best fit by a family of Gaussian distributions with different variances. In the prediction and incomprehensiveness cases, these are best fit by Laplace distributions with different scale parameters. Lastly, Figure \ref{fig:validation} shows the information-theoretic lower bound on probability of error that our validation framework provides (Corollary~\ref{cor:ef}, \ref{cor:laplace}) for this dataset and these tasks. This lower bound is a function of $\epsilon$, which is the minimum improvement across all groups from the personalized model required to reject the null hypothesis.

\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt,
    boxsep=0pt,
    arc=10pt,
    outer arc=10pt
]
\textbf{Remark.} This analysis reveals an intriguing tension in the role of $\epsilon$ within hypothesis testing. As $\epsilon$ increases, the $P_e$ lower bound decreases, indicating that higher $\epsilon$ values are necessary for achieving reliable hypothesis test results. However, this creates a fundamental trade-off: $\epsilon$ simultaneously governs the rejection criterion for the null hypothesis, which is only rejected when $\hat{\gamma} \geq \epsilon$.
\end{tcolorbox}

In particular, we get that there is no fundamental limit against trusting our results for $\hat{\gamma}_{BoP} > 0.009$ for all metrics in the classification task. In the regression scenario, we find the thresholds $(i)$ $\hat{\gamma}_{BoP-P} > 6.53 \cdot 10^{-5}$ for prediction accuracy, $(ii)$ $\hat{\gamma}_{BoP-X} > 1.29 \cdot 10^{-4}$ for incomprehensiveness, and $(iii)$ $\hat{\gamma}_{BoP-X} > 0.017$ for sufficiency. These values can be seen on Figure \ref{fig:validation} as the locations where each of the lines cross ${P_e=0.5}$, which corresponds to the smallest $\epsilon$ needed to ensure the lower bound on the probability of error does not exceed $50\%$. These plots are made using the unique $n, \sigma$ and $b$ values of each individual BoP distribution for this experiment. Utilizing these plots, practitioners can verify if the hypothesis test remains reliable under $\epsilon$. Given a reliable test, they then can either reject, or fail to reject the null hypothesis using their empirical minimal BoP values. 



\textbf{BoP-P Analysis.} In the case of prediction accuracy, we observe that the personalized model $h_p$ assigns less accurate predictions to specific subgroups in the classification task --even decreasing the overall accuracy for the entire population. Hence, the minimal BoP-P is negative and there is no evidence in favor of a benefit. For regression, all subgroups are benefited by personalization and we can reject the null hypothesis if $\epsilon$ has been set at below $0.0021$, the empirical minimal BoP-P. 

\textbf{BoP-X Analysis.} In the case of explainability, sufficiency is improved by personalization for all subgroups across regression and classification. We can reject $H_0$ if $\epsilon$ has been set below $0.1914$ and $0.0029$, respectively. However, incomprehensiveness improves for some subgroups and worsens for others, resulting in a negative minimal BoP-X for both.



\textbf{BoP-P vs. BoP-X.} 
For regression, we see a benefit from personalization in terms of both prediction and sufficiency. For classification, we see a benefit for sufficiency alone. This illustrates two cases: one where prediction is not improved, but an aspect of explainability is; and another where prediction is improved, but an aspect of explainability is not. A negative BoP in the other values impedes us to draw conclusions about any potential benefit from personalizing.










\begin{figure}
    \centering
    \includegraphics[width=01\linewidth]{Figures/regr_class_results.pdf}
    \vspace{-10pt}
    \caption{ 
    Leveraging the validation framework, we plot how the $P_e$ changes for different $\epsilon$ values for a set $N$ and $k = 2$ using Corollary \ref{cor:ef} and \ref{cor:laplace}. We utilize the Laplace and Gaussian form in A, and the Categorical in B. 
    }
    \label{fig:validation}
\end{figure}




