


\section{Extended Related Works: Explainability}\label{sec:related_works}


\textbf{Explainability} Typical approaches to model explanation involve measuring how much each input feature contributes to the model's output, highlighting important inputs to promote user trust. This process often involves using gradients or hidden feature maps to estimate the importance of inputs \citep{simonyan2014deepinsideconvolutionalnetworks, smilkov2017smoothgradremovingnoiseadding,sundararajan2017axiomaticattributiondeepnetworks, yuan2022explainabilitygraphneuralnetworks}. 
For instance, gradient-based methods use backpropagation to compute the gradient of the output with respect to inputs, with higher gradients indicating greater importance \citep{sundararajan2017axiomaticattributiondeepnetworks, yuan2022explainabilitygraphneuralnetworks}. The quality of these explanations is often evaluated using the principle of $\emph{faithfulness}$ \citep{lyu2024faithfulmodelexplanationnlp, dasgupta2022frameworkevaluatingfaithfulnesslocal, jacovi-goldberg-2020-towards}, which measures how accurately an explanation represents the reasoning of the underlying model. Two key aspects of faithfulness are $\emph{sufficiency}$ and $\emph{comprehenesiveness}$ \citep{deyoung-etal-2020-eraser, yin2022sensitivitystabilitymodelinterpretations}; the former assesses whether the inputs deemed important are adequate for the model's prediction, and the latter examines if these features capture the essence of the model's decision-making process. 

