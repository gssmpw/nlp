
\section{Proof of Theorems on Lower Bounds for the Probability of error}


As in \citep{monteiro2022epistemic}, we will prove every theorem for the flipped hypothesis test defined as:
\begin{align*}
& H_0: 
&\gamma(h_0, h_p; \mathcal{D})
&\leq \epsilon \quad 
\Leftrightarrow \quad 
\text{Personalized $h_p$ performs worst: yields $\epsilon <0$ disadvantage} \\
& H_1: 
&\gamma(h_0, h_p; \mathcal{D}) 
&\geq 0 \quad 
\Leftrightarrow \quad 
\text{Personalized $h_p$ performs at least as good as generic $h_0$.}
\end{align*}
where we emphasize that $\epsilon < 0$.

As shown in \citep{monteiro2022epistemic}, proving the bound for the original hypothesis test is equivalent to proving the bound for the flipped hypothesis test, since estimating $\gamma$ is as hard as estimating $-\gamma$. In every section that follows, $H_0, H_1$ refer to the flipped hypothesis test.

Here, we first prove a proposition that is valid for all of the cases that we consider in the next sections.

\begin{proposition}\label{prop:lower_bound}
    Consider $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) > 0$. Consider a decision rule $\Psi$ that represents any hypothesis test.
    We have the following bound on the probability of error $P_e$:
    \begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        \geq 
        1 - TV(P\parallel Q),
    \end{align*}
for any well-chosen $P \in H_0$ and any well-chosen $Q \in H_1$. Here $TV$ refers to the total variation between probability distributions $P$ and $Q$.
\end{proposition}

\begin{proof}
    Consider $h_0$ and $h_p$ fixed. Take one decision rule $\Psi$ that represents any hypothesis test. Consider a dataset such that $H_0$ is true, i.e., $\mathcal{D} \sim P_0$ and a dataset such that $H_1$ is true, i.e., $\mathcal{D} \sim P_1$. 
    
    It might seem weird to use two datasets to compute the same quantity $P_e$, i.e., one dataset to compute the first term in $P_e$, and one dataset to compute the second term in $P_e$. However, this is just a reflection of the fact that the two terms in $P_e$ come from two different settings: $H_0$ true or $H_0$ false, which are disjoint events: in the same way that $H_0$ cannot be simultaneously true and false, yet each term in $P_e$ consider one or the other case; then we use one or the other dataset. 
    
    We have:
    \begin{align*}
        P_e 
        &= 
        \text{Pr}(\text{Rejecting $H_0$} | \text{$H_0$ true}) 
        +
        \text{Pr}(\text{Failing to reject $H_0$} | \text{$H_1$ true})
        \\
        &= 
        \text{Pr}(\Psi(h_0, h_p, \mathcal{D}, \epsilon) = 1 | \mathcal{D} \sim P_0) 
        +
        \text{Pr}(\Psi(h_0, h_p, \mathcal{D}, \epsilon) = 0 |
        \mathcal{D} \sim P_1)
        \\
        &= 
        \text{Pr}(\Psi(\mathcal{D}) = 1 | \mathcal{D} \sim P_0) 
        +
        \text{Pr}(\Psi(\mathcal{D}) = 0 |
        \mathcal{D} \sim P_1)
~\text{simplifying notations}
        \\
        &= 
        1 - \text{Pr}(\Psi(\mathcal{D}) = 0 | \mathcal{D} \sim P_0) 
        +
        \text{Pr}(\Psi(\mathcal{D}) = 0 |
        \mathcal{D} \sim P_1)
~\text{complementary event}
        \\
        &= 
        1 
        - P_0(E_\Psi) 
        +
        P_1(E_\Psi)
~\text{writing $E_\Psi$ the event $\Psi(\mathcal{D}) = 0$}\\
        &= 
        1 - (P_0(E_\Psi) - P_1(E_\Psi))
    \end{align*}

Now, we will bound this quantity:
\begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &= 
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        1 - (P_0(E_\Psi) - P_1(E_\Psi))
            \\
        &\geq 
        \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        \min _{\Psi} 
         \left[
        1 
        - (P_0(E_\Psi) 
        -
        P_1(E_\Psi))
        \right]
        ~\text{using minmax inequality}
        \\
        &=
        \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
         \left[
        1 
        - \max_{\Psi} 
        (P_0(E_\Psi) 
        -
        P_1(E_\Psi))
        \right]~\text{to get min over $\Psi$, we want $(P_0(E_\Psi) -P_1(E_\Psi))$ that is largest.}
        \\
        &\geq 
        \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
         \left[
        1 
        - \max_{\text{events $A$}}(P_0(A) 
        -
        P_1(A))
        \right]
        ~\text{because the max is now over all possible events $A$}
    \end{align*}

The maximization is broadened to consider all possible events \( A \). This increases the set over which the maximum is taken. Because \( \Psi \) is only a subset of all possible events, maximizing over all events \( A \) (which includes \( \Psi \)) will result in a value that is at least as large as the maximum over \( \Psi \). In other words, extending the set of possible events can only make the maximum greater or the same.


\begin{align*}
        &= 
        \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
         \left[
        1 
        - TV(P_0 \parallel P_1)
        \right]
        ~\text{by definition of the total variation (TV)}\\
        &= 1 - \min _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}TV(P_0\parallel P_1)\\
        & \geq 1 - TV(P \parallel Q) ~\text{for any $P \in H_0$ and $Q \in H_1$}.
\end{align*}

This is true because the total variation distance \( TV(P \parallel Q) \) for any particular pair \( P \) and \( Q \) cannot be smaller than the minimum total variation distance across all pairs. We recall that, by definition, the total variation of two probability distributions $P, Q$ is the largest possible difference between the probabilities that the two probability distributions can assign to the same event $A$.
\end{proof}

Next, we prove a lemma that will be useful for the follow-up proofs.

\begin{lemma}\label{lem:expect}
    Consider a random variable $a$ such that $\mathbb{E}[a] = 1$. Then:
    \begin{equation}
        \mathbb{E}[(a - 1)^2] = \mathbb{E}[a^2] - 1
    \end{equation}
\end{lemma}

\begin{proof}
    We have that:
    \begin{align*}
        \mathbb{E}[(a - 1)^2] 
            &= \mathbb{E}[a^2 - 2a +1] \\
            &= \mathbb{E}[a^2] -2 \mathbb{E}[a] +1 
            ~\text{(linearity of the expectation)}\\
            &= \mathbb{E}[a^2] -2 +1 
            \text{($\mathbb{E}[a] = 1$ by assumption)}\\
             &= \mathbb{E}[a^2] -1. 
    \end{align*}
\end{proof}

\subsection{Proof for any probability distribution and any number of samples in each group}\label{subsec:any_distribution}

Below, we find the lower bound for the probability of error for any probability distribution of the BoP, and any number of samples per group.

\begin{theorem}[Lower bound for any probability distribution BoP.]\label{th:lower_bound_any_prob}
The lower bound writes:
\begin{equation}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        \geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{p^{\epsilon}(B)}{p(B)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{equation}
where $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) \geq \epsilon$. Dataset $\mathcal{D}$ is drawn from an unknown distribution and has $d$ groups where $d=2^k$, with each group having $m_j$ samples.
\end{theorem}
\begin{proof}
    By Proposition~\ref{prop:lower_bound}, we have that:
 \begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        \geq 
        1 - TV(P\parallel Q)
        \end{align*}
for any well-chosen $P \in H_0$ and any well-chosen $Q \in H_1$. We will design two probability distributions $P, Q$ defined on the $N$ data points $(X_1, G_1, Y_1), ..., (X_N, G_N, Y_N)$ of the dataset $\mathcal{D}$ to compute an interesting right hand side term. An ``interesting'' right hand side term is a term that makes the lower bound as tight as possible, i.e., it relies on distributions $P, Q$ for which $TV(P\parallel Q)$ is small, i.e., probability distributions that are similar. To achieve this, we will first design the distribution $Q \in H_1$, and then propose $P$ as a very small modification of $Q$, just enough to allows it to verify $P \in H_0$.


Mathematically, $P$, $Q$ are distributions on the dataset $\mathcal{D}$, i.e., on $N$ i.i.d. realizations of the random variables $X, S, Y$ where $X$ is continuous, $S$ is categorical, and $Y$ is continuous (regression framework). Thus, we wish to design probability distributions on $(X_1, S_1, Y_1), ..., (X_N, S_N, Y_N)$.

However, we note that the dataset distribution is only meaningful in terms of how each triplet $(X_i, S_i, Y_i)$ impacts the value of the estimated BOP. Thus, we design probability distributions $P, Q$ on $n$ i.i.d. realizations of an auxiliary random variable $B$, with values in $\mathbb{R}$, defined as:
\begin{equation}
    B = \ell(h_0(X), Y) - \ell(h_p(X, S), Y).
\end{equation}
Intuitively, $B_i$ represents how much the triplet $(X_i, S_i, Y_i)$ contributes to the value of the BOP. $b_i > 0$ means that the personalized model provided a better prediction than the generic model on the triplet $(x_i, s_i, y_i)$ corresponding to the data point $i$.

Consider the event $b = (b_1, ..., b_N) \in \mathbb{R}^N$ of $N$ realizations of $B$. For simplicity in our computations, we divide this event into the $d$ groups, i.e., we write instead: $b_j = (b_j^{(1)}, ..., b_j^{(m)})$, since each group $j$ has $m_j$ samples. Thus, we have: $b = \{b_j^{(k)}\}_{j=1...d, k=1...m}$ indexed by $j, k$ where $j=1...d$ is the group in which this element is, and $k=1...m_j$ is the index of the element in that group.

\paragraph{Start proof: Design $Q$.} Next, we design a distribution $Q$ on this set of events that will (barely) verify $H_1$, i.e., such that the expectation of $B$ according to $Q$ will give $\gamma = 0$. We recall that $\gamma = 0$ means that the minimum benefit across groups is $0$, implying that there might be some groups that have a $>0$ benefit.

Given $p$ as a distribution with mean $\mu = 0$ , we propose the following distribution for $Q$
\begin{align*}
    Q_j(b_j) 
        &= \prod_{k=1}^m p(b_j^{(k)}),~\text{for every group $j=1....d$}\\
    Q(b) 
        &= \prod_{j=1}^d Q_j(b_j).
\end{align*}

We verify that we have designed $Q$ correctly, i.e., we verify that $Q \in H_1$. When the dataset is distributed according to $Q$, we have:
\begin{align*}
    \gamma 
        &= \min_{s \in S} C_{s}(h_0, s) - C_{s}(h_p, s)\\
        &= \min_{s \in S} 
            \mathbb{E}_Q[\ell(h_0(\mathbf{X}), Y) \mid \mathbf{S}=\mathbf{s}] 
            - \mathbb{E}_Q[\ell(h_p(\mathbf{X}), Y) \mid \mathbf{S}=\mathbf{s}] 
            ~\text{(by definition of group cost)}\\
        &= \min_{s \in S} 
            \mathbb{E}_Q[\ell(h_0(\mathbf{X}), Y) - \ell(h_p(\mathbf{X}), Y) \mid \mathbf{S}=\mathbf{s}] 
            ~\text{(by linearity of expectation)}\\
        &= \min_{s \in S} 
            \mathbb{E}_Q[B \mid \mathbf{S}=\mathbf{s}]
            ~\text{(by definition of random variable $B$)}\\
        &= \min_{s \in S} 
            0
            ~\text{(by definition of the probability distribution on $B$)}\\
        &= 0.
\end{align*}
Thus, we find that $\gamma =0$ which means that $\gamma \geq 0$, i.e., $Q \in H_1$.

\paragraph{Design $P$.} Next, we design $P$ as a small modification of the distribution $Q$, that will just be enough to get $P \in H_0$. We recall that $P \in H_0$ means that $\gamma \leq \epsilon$ where $\epsilon < 0$ in the flipped hypothesis test. This means that, under $H_0$, there is one group that suffers a decrease of performance of $|\epsilon|$ because of the personalized model.

Given $p$  as a distribution with $\mu=0$, and $p^{\epsilon}$  a distribution with mean $\mu = \epsilon < 0$, we have:
\begin{align*}
    P_j(b_j) 
        &= \prod_{k=1}^{m_j} p(b_j^{(k)}),~\text{for every group $j=1....d$},\\
    P_j^{\epsilon}(b_j) 
        &= \prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)}),~\text{for every group $j=1....d$},\\
    P(b) 
        &= \frac{1}{d}\sum_{j=1}^d P_j^{\epsilon}(b_j)\prod_{j'\neq j}P_{j'}(b_{j'}).
\end{align*}
Intuitively, this distribution represents the fact that there is one group for which the personalized model worsen performances by $|\epsilon|$.We assume that this group can be either group $1$, or group $2$, etc, or group $d$, and consider these to be disjoint events: i.e., exactly only one group suffers the $|\epsilon|$ performance decrease. We take the union of these disjoint events and sum of probabilities using the Partition Theorem (Law of Total Probability) in the definition of $P$ above.

We verify that we have designed $P$ correctly, i.e., we verify that $P \in H_0$. When the dataset is distributed according to $P$, we have:
\begin{align*}
    \gamma 
        &= \min_{s \in S} 
            C_{s}(h_0, s) - C_{s}(h_p, s)\\
        &= \min_{s \in S} 
            \mathbb{E}_P[B \mid \mathbf{S}=\mathbf{s}]
            ~\text{(same computations as for $Q \in H_1$)}\\
        &= \min(\epsilon, 0, ..., 0) 
            ~\text{(since exactly one group has mean $\epsilon$)}\\
        &= \epsilon
            ~\text{(since $\epsilon < 0$)}.
\end{align*}
    Thus, we find that $\gamma = \epsilon$ which means that $\gamma \leq 0$, i.e., $P \in H_0$.


\paragraph{Compute total variation $TV(P\parallel Q)$.} We have verified that $Q \in H_1$ and that $P \in H_0$. We use these probability distributions to compute the lower bound to $P_e$. First, we compute their total variation:

\begin{align*}
    TV (P\parallel Q)
        &= \frac{1}{2} \int_{b_1, ..., b_j}
            \left|
            P(b_1, ..., b_j) 
            - Q(b_1, ..., b_j)
            \right|
            db_1...db_j 
            ~\text{(TV for probability density functions)}\\
        &= \frac{1}{2} \int_{b_1, ..., b_j}
            \left|
            \frac{1}{d}\sum_{j=1}^d P_j^{\epsilon}(b_j)\prod_{j'\neq j}P_{j'}(b_{j'})
            - 
            \prod_{j=1}^d Q_j(b_j)
            \right|
            db_1...db_j 
            ~\text{(definition of $P, Q$)}\\
        &= \frac{1}{2} \int_{b_1, ..., b_j}
            \left|
            \frac{1}{d}\sum_{j=1}^d \frac{P_j^{\epsilon}(b_j)}{P_j(b_j)}\prod_{j'=1}^dP_{j'}(b_{j'})
            -
            \prod_{j=1}^d Q_j(b_j)
            \right|
            db_1...db_j 
            ~\text{(adding missing $j'=j$)}\\
        &= \frac{1}{2} \int_{b_1, ..., b_j}
            \left|
            \frac{1}{d}\sum_{j=1}^d \frac{P_j^{\epsilon}(b_j)}{P_j(b_j)}\prod_{j'=1}^d Q_{j'}(b_{j'})
            -
            \prod_{j=1}^d Q_j(b_j)
            \right|
            db_1...db_j 
            ~\text{($P_j = Q_j$ by construction)}\\
        &= \frac{1}{2} \int_{b_1, ..., b_j}
            \prod_{j=1}^d Q_j(b_j)
            \left|
            \frac{1}{d}\sum_{j=1}^d \frac{P_j^{\epsilon}(b_j)}{P_j(b_j)}
            -
            1
            \right|
            db_1...db_j 
            ~\text{(extracting the product)}\\
        &= \frac{1}{2} 
            \mathbb{E}_Q 
            \left[
            \left|
            \frac{1}{d}\sum_{j=1}^d \frac{P_j^{\epsilon}(b_j)}{P_j(b_j)}
            -
            1\right|
            \right]
            ~\text{(recognizing an expectation with respect to $Q$)}\\
        &= \frac{1}{2} 
            \mathbb{E}_Q 
            \left[
            \left|
            \frac{1}{d}\sum_{j=1}^d \frac{\prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)})}{\prod_{k=1}^{m_j} p(b_j^{(k)})}
            -
            1
            \right|
            \right]
            ~\text{(definition of $P_j$ and $P_j^{(\epsilon)}$)}\\
        &\leq \frac{1}{2} 
            \mathbb{E}_Q 
            \left[
            \left|
            \frac{1}{d}
            \sum_{j=1}^d \frac{\prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)})}{\prod_{k=1}^{m_j} p(b_j^{(k)})}
            -
            1
            \right|^2
            \right]^{1/2}
            ~\text{(Cauchy-Schwartz)}
\end{align*}


\paragraph{Auxiliary computation to apply Lemma~\ref{lem:expect}} Next, we will apply Lemma~\ref{lem:expect}. For this, we need to prove that the expectation of the first term is 1. We have:
\begin{align*}
    &\mathbb{E}_Q\left[
        \frac{1}{d}
            \sum_{j=1}^d 
            \frac{\prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)})}{\prod_{k=1}^{m_j} p(b_j^{(k)})}
            \right]\\
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \mathbb{E}_Q\left[
            \frac{\prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)})}{\prod_{k=1}^{m_j} p(b_j^{(k)})}
            \right]~\text{(linearity of expectation)}\\
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \mathbb{E}_Q\left[
            \prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}
            \right]~\text{(rearranging the product)}\\
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \prod_{k=1}^{m_j} 
            \mathbb{E}_Q\left[\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}
            \right]~\text{(product of independent variables)}\\
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \prod_{k=1}^{m_j} 
            \mathbb{E}_p\left[\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}
            \right]~\text{(definition of $Q$)}\\ 
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \prod_{k=1}^{m_j} 
            \int_{-\infty}^{+\infty}\frac{p^{\epsilon}(b)}{p(b)}
            p(b)db~\text{(definition of expectation in $p$)}\\ 
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \prod_{k=1}^{m_j} 
            \int_{-\infty}^{+\infty}p^{\epsilon}(b)db~\text{(simplify)}\\
    &= \frac{1}{d}
          \sum_{j=1}^d 
            \prod_{k=1}^{m_j} 
            1~\text{(probability density function integrates to $1$)}\\
    &= \frac{1}{d}
          \sum_{j=1}^d 
            1~\text{(term independent of $k$)}\\
    &= \frac{1}{d}
          d~\text{(term independent of $j$)}\\
    &=1.
\end{align*}

\paragraph{Continue by applying Lemma~\ref{lem:expect}.} This auxiliary computation shows that we meet the assumption of Lemma~\ref{lem:expect}. Thus, we continue the computation of the lower bound of the TV by applying Lemma~\ref{lem:expect}.
\begin{align*}
     &TV (P\parallel Q)\\
     &\leq \frac{1}{2} 
            \mathbb{E}_Q 
            \left[\left(
        \frac{1}{d}
            \sum_{j=1}^d 
            \frac{\prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)})}{\prod_{k=1}^{m_j} p(b_j^{(k)})}
            \right)^2
            -
            1
            \right]^{\frac{1}{2}}~\text{Lemma~\ref{lem:expect}}\\
     &= \frac{1}{2} 
            \mathbb{E}_Q 
            \left[\left(\frac{1}{d}\sum_{j=1}^d 
            z_j\right)^2
            -
            1
            \right]^{\frac{1}{2}}~\text{defining $z_j = \frac{\prod_{k=1}^{m_j} p^{\epsilon}(b_j^{(k)})}{\prod_{k=1}^{m_j} p(b_j^{(k)})}=\prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}$}\\
    &= \frac{1}{2} 
            \mathbb{E}_Q 
            \left[\frac{1}{d^2}\sum_{j,j'=1}^d 
            z_jz_{j'}
            -
            1
            \right]^{\frac{1}{2}}~\text{expanding the square of the sum}\\
    &= \frac{1}{2} 
            \mathbb{E}_Q 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
            z_j^2 + \sum_{j,j'=1, j\neq j'}^d z_j.z_{j'}
            \right)
            -
            1
            \right]^{\frac{1}{2}},
\end{align*}
where we split the double sum to get independent variables in the second term.

We get by linearity of the expectation, $\mathbb{E}[aX + bY] = a\mathbb{E}[X]+b\mathbb{E}[Y]$:
\begin{align*}
     &TV (P\parallel Q)\\
     &\leq \frac{1}{2} 
            \mathbb{E}_Q 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
            z_j^2 
            + 
            \sum_{j,j'=1, j\neq j'}^d z_j.z_{j'}
            \right)
            -
            1
            \right]^{\frac{1}{2}}\\
     &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
            \mathbb{E}_Q [z_j^2]
            + 
            \sum_{j,j'=1, j\neq j'}^d 
            \mathbb{E}_Q [z_j .z_{j'}]
            \right)
            -
            1
            \right]^{\frac{1}{2}}\\
     &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
            \mathbb{E}_Q \left[\left(\prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right)^2\right]
            + 
            \sum_{j,j'=1, j\neq j'}^d 
            \mathbb{E}_Q \left[\left(\prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right) .\left( \prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_{j'}^{(k)})}{p(b_{j'}^{(k)})}\right)\right]
            \right)
            -
            1
            \right]^{\frac{1}{2}}\\
     &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
            \mathbb{E}_Q \left[\left(\prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right)^2\right]
            + 
            \sum_{j,j'=1, j\neq j'}^d 
            \mathbb{E}_Q \left[\prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right] \mathbb{E}_Q \left[\prod_{k=1}^{m_j} \frac{p^{\epsilon}(b_{j'}^{(k)})}{p(b_{j'}^{(k)})}\right]
            \right)
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(product of independent variables)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
           \prod_{k=1}^{m_j}  \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right)^2\right]
            + 
            \sum_{j,j'=1, j\neq j'}^d 
            \prod_{k=1}^{m_j} \mathbb{E}_p\left[\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right] 
            \prod_{k=1}^{m_j} \mathbb{E}_p\left[ \frac{p^{\epsilon}(b_{j'}^{(k)})}{p(b_{j'}^{(k)})}\right]
            \right)
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(product of independent variables and def. of $Q$)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
           \prod_{k=1}^{m_j}  \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right)^2\right]
            + 
            \sum_{j,j'=1, j\neq j'}^d 
            \prod_{k=1}^{m_j} 1\prod_{k=1}^{m_j} 1
            \right)
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(auxiliary computation below)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
           \prod_{k=1}^{m_j}  \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right)^2\right]
            + 
            \sum_{j,j'=1, j\neq j'}^d 
            1
            \right)
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(term independent of $k$)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
           \prod_{k=1}^{m_j}  \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(b_j^{(k)})}{p(b_j^{(k)})}\right)^2\right]
            + 
            (d^2 -d)
            \right)
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(term independent of $j$)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \left(
            \sum_{j=1}^d 
           \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(B)}{p(B)}\right)^2\right]^{m_j}
            + 
            (d^2 -d)
            \right)
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(term independent of $k$)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \sum_{j=1}^d 
           \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(B)}{p(B)}\right)^2\right]^{m_j}
            + 
            1 - \frac{1}{d}
            -
            1
            \right]^{\frac{1}{2}}
            ~\text{(distribute $1/d^2$)}\\
         &= \frac{1}{2} 
            \left[\frac{1}{d^2}
            \sum_{j=1}^d 
           \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(B)}{p(B)}\right)^2\right]^{m_j}
            - \frac{1}{d}
            \right]^{\frac{1}{2}}
            ~\text{(simplify)}\\
         &= \frac{1}{2\sqrt{d}} 
            \left[\frac{1}{d}
            \sum_{j=1}^d 
           \mathbb{E}_p \left[\left(\frac{p^{\epsilon}(B)}{p(B)}\right)^2\right]^{m_j}
            - 1
            \right]^{\frac{1}{2}}
            ~\text{(extract $1/\sqrt{d}$)}\\
  &= \frac{1}{2\sqrt{d}}\left[\frac{1}{d}
            \sum_{j=1}^d  \left(
           \int_{-\infty}^{+\infty}\left(\frac{p^{\epsilon}(b)}{p(b)}\right)^2p(b)db\right)^{m_j}
            -1
            \right]^{\frac{1}{2}}
            ~\text{(definition of expectation)}\\
        &= \frac{1}{2\sqrt{d}}\left[\frac{1}{d}
            \sum_{j=1}^d \left(
           \int_{-\infty}^{+\infty}\frac{p^{\epsilon}(b)^2}{p(b)}db\right)^{m_j}
            -1\right]^{\frac{1}{2}}
            ~\text{(simplify $p(b)$)}\\
             &= \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{p^{\epsilon}(B)}{p(B)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
            ~\text{(def of expectation)}
\end{align*}



\paragraph{Auxiliary computation in $1$} We show that:
\begin{align*}
    &\mathbb{E}_p\left[ \frac{p^{\epsilon}(b_{j'}^{(k)})}{p(b_{j'}^{(k)})}\right]\\
    &= \int_{-\infty}^{+\infty} \frac{p^{\epsilon}(b)}{p(b)}p(b)db\\
    &= \int_{-\infty}^{+\infty} p^{\epsilon}(b)db~\text{simplify $p(b)$}\\
    &=1~\text{probability density function $p^\epsilon$ integrates to 1.}
\end{align*}



\paragraph{Final result:} This gives the final result:

\begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 
        1 - TV(P\parallel Q)\\
      \Rightarrow
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{p^{\epsilon}(B)}{p(B)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{align*}
\end{proof}

\subsection{Proof for distribution in an exponential family}
\label{sec:exp_family_proof}

% Source: https://www2.stat.duke.edu/courses/Spring11/sta114/lec/expofam.pdf

We consider a fixed exponential family in it natural parameterization, i.e., probability distributions of the form:
\begin{equation}
    f_X(x \mid \boldsymbol{\theta})=h(x) \exp (\theta \cdot \mathbf{T}(x)-A(\boldsymbol{\theta})),
\end{equation}
where $\theta$ is the only parameter varying between two distributions from that family, i.e., the functions $\eta$, $T$ and $A$ are fixed. We recall a few properties of any exponential family (EF) that will be useful in our computations.

First, the moment generating function (MGF) for the natural sufficient statistic $T(x)$ is equal to:
\begin{align*}
    M^T(t) = \exp\left(A(\theta+t)-A(\theta)\right).
\end{align*}

Then, the moments for $T(x)$, when $\theta$ is a scalar parameter, are given by:
\begin{align*}
    \mathrm{E}[T] & = A'(\theta)\\
    \mathrm{V}[T] & = A''(\theta).
\end{align*}
Since the variance is non-negative $\mathrm{V}[T] \geq 0$, this means that we have $A''(\theta) > 0$ and thus $A'$ is monotonic and bijective. We will use that fact in the later computations.

In the following, we recall that the categorical distribution and the Gaussian distribution with fixed variance $\sigma^2$ are members of the exponential family.


\paragraph{Example: Categorical distributions as a EF} The categorical variable has probability density function:
\begin{align*}  
p(x \mid \pi) & =\exp \left(\sum_{k=1}^K x_k \log \pi_k\right) \\
& =\exp \left(\sum_{k=1}^{K-1} x_k \log \pi_k+\left(1-\sum_{k=1}^{K-1} x_k\right) \log \left(1-\sum_{k=1}^{K-1} \pi_k\right)\right) \\
& =\exp \left(\sum_{k=1}^{K-1} \log \left(\frac{\pi_k}{1-\sum_{k=1}^{K-1} \pi_k}\right) x_k+\log \left(1-\sum_{k=1}^{K-1} \pi_k\right)\right)
\end{align*}
where we have used the fact that$\pi_K=1-\sum_{k=1}^{K-1} \pi_k$.


We note that we need to use the PDF of the categorical that uses a minimal (i.e., $K-1$) set of parameters. We define $h(x)$, $T(x)$, $\theta \in \mathbb{R}^{K-1}$ and $A(\theta)$ as:
\begin{align*}
    h(x)&=1\\
        T(x) & = x,\\
    \theta_k &=\log \left(\frac{\pi_k}{1-\sum_{k=1}^{K-1} \pi_k}\right)=\log \left(\frac{\pi_k}{\pi_K}\right),~\text{for $k = 1, ..., K-1$}\\
    A(\theta) 
    &= -\log \left(1-\sum_{k=1}^{K-1} \pi_k\right)
    =\log \left(\frac{1}{1-\sum_{k=1}^{K-1} \pi_k}\right) 
    =\log \left(\frac{\sum_{k=1}^K \pi_k}{1-\sum_{k=1}^{K-1} \pi_k}\right) 
    = \log \left(\sum_{k=1}^K e^{\theta_k}\right),
\end{align*}
 which shows that the categorical distribution is within the EF. For convenience we have defined $\theta_K$ setting it to $0$ as per the Equation above.

 Now, we adapt these expressions for the case of a Categorical variable with only $K=3$ values $x_1 = -1, x_2 = 1$ and $x_3=0$ such that $\pi_3=0$, i.e., there is no mass on the $x_3=0$, and we denote $\pi_1 = p_1$ and $\pi_2 = p_2$ and $\pi_3 = 1 - p_1 - p_2 = 0$. We get:
 \begin{align*}
    h(x)&=1\\
    T(x) & = x,\\
    \theta_1 &=\log \left(\frac{p_1}{p_2}\right),
    ~\text{and}~
    \theta_2 = 0~\text{by convention, as above},
    \theta_3 =\log \left(\frac{\pi_3}{p_2}\right)= -\infty\\
    A(\theta_1) 
    & 
    = \log \left(e^{\theta_1} + e^{\theta_2} + e^{\theta_3}\right)
    = \log \left(e^{\theta_1} + 1 + 0\right) 
    = \log \left(e^{\log \left(\frac{p_1}{p_2}\right)} + 1\right) 
    = \log \left(\frac{p_1}{p_2} + 1\right) ,
\end{align*}
where, in the proofs, we will have $p_1 = \frac{1}{2} +\epsilon$ and $p_3 = \frac{1}{2} - \epsilon$ such that the expectation is $-1.(\frac{1}{2} +\epsilon)+1.(\frac{1}{2}-\epsilon)=-2\epsilon$.

\paragraph{Example: Gaussian distribution with fixed variance as a EF} The Gaussian distribution with fixed variance has probability density function:
\begin{align*}
    p\left(x \mid \mu \right)
    & =\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(
        -\frac{(x-\mu)^2}{2 \sigma^2}
    \right)\\
    &= \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(
    -\frac{x^2 -2x\mu +\mu^2}{2 \sigma^2}
    \right)\\
    &= \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{x^2}{2\sigma^2}\right) \exp \left(
    \frac{2x\mu -\mu^2}{2 \sigma^2}
      \right)\\
    &= \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{x^2}{2\sigma^2}\right) \exp \left(
    \frac{x\mu}{\sigma^2} - \frac{\mu^2}{2 \sigma^2}
      \right).
\end{align*}

We define $h(x)$, $T(x)$, $\theta \in \mathbb{R}$ and $A(\theta)$ as:
\begin{align*}
    h(x) &=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{x^2}{2\sigma^2}\right)\\
    T(x) & = x,\\
    \theta &= \frac{\mu}{\sigma^2}\\
    A(\theta)& = \frac{\mu^2}{2\sigma^2} = \frac{\sigma^2\theta^2}{2}. 
\end{align*}
which shows that the Gaussian distribution with fixed variance $\sigma^2$ is within the EF.

\begin{proposition}\label{prop:lower_bound_exponential_fam} The lower bound for the exponential family with any number of samples in each group writes:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{M_p(2\Delta\theta)}{M_p(\Delta \theta)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}
\end{align*}

\end{proposition}

\begin{proof}
    By Theorem~\ref{th:lower_bound_any_prob}, we have:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{p^{\epsilon}(B)}{p(B)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{align*}
\paragraph{Plug in the exponential family} Under the assumption of an exponential family distribution for the random variable $B$, we have:

\begin{align*}
    &\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e \\
        &\geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \frac{h(B)\exp(\theta^\epsilon.T(B) - A(\theta^\epsilon))}{h(B)\exp(\theta^0.T(B) - A(\theta^0))}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}\\
        &=1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{\exp(\theta^\epsilon.T(B) - A(\theta^\epsilon))}{\exp(\theta^0.T(B) - A(\theta^0))}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{simplifying $h$}\\
        &=1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[ 
                \exp(\theta^\epsilon.T(B) - A(\theta^\epsilon))
                \exp(-\theta^0.T(B) + A(\theta^0))\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{properties of $\exp$}\\
        &=1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[ 
                \exp(A(\theta^0)- A(\theta^\epsilon))
                \exp(\theta^\epsilon.T(B) - \theta^0.T(B))\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{properties of $\exp$ and rearranging terms}\\
        &=1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[ 
                \exp(A(\theta^0)- A(\theta^\epsilon))
                \exp((\theta^\epsilon - \theta^0)T(B))\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{properties of $\exp$ and rearranging terms}\\
        &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \exp(A(\theta^0)- A(\theta^\epsilon))^{m_j} \mathbb{E}_{p^\epsilon}\Bigg[ 
                \exp((\theta^\epsilon - \theta^0)T(B))\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}\\
        &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \exp(A(\theta^0)- A(\theta^\epsilon))^{m_j} M_{p^\epsilon}(\Delta \theta)^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{def. of MGF of $T(B)$: $M_{p^\epsilon}(t) = \mathbb{E}_{p^\epsilon}[\exp(t.T(B))]$ with $\Delta \theta = \theta_\epsilon - \theta_0$}.
\end{align*}
We define $\Delta \theta = \theta_\epsilon - \theta_0$. Here, we will apply the properties of EF regarding moment generating functions, i.e., for the $p^\epsilon$ with natural parameter $\theta_\epsilon$:
\begin{align*}    
    M_{p^\epsilon}(t) = \exp\left(A(\theta_\epsilon + t) - A(\theta_\epsilon)\right)
    &\Rightarrow 
    M_{p^\epsilon}(-\Delta \theta) = \exp\left(A(\theta_0) - A(\theta_\epsilon)\right),\\
    &\Rightarrow M_{p^\epsilon}(\Delta \theta) = \exp\left(A(2\theta_\epsilon-\theta_0) - A(\theta_\epsilon)\right),
\end{align*}
And, for $p$ associated with natural parameter $\theta_0$:
\begin{align*}    
    M_{p}(t) = \exp\left(A(\theta_0 + t) - A(\theta_0)\right)
    &\Rightarrow 
    M_{p}(-\Delta \theta) 
        = \exp\left(A(2\theta_0-\theta_\epsilon) - A(\theta_0)\right),\\
    &\Rightarrow 
    M_{p}(\Delta \theta) 
        = \exp\left(A(\theta_\epsilon) - A(\theta_0)\right),\\
    &\Rightarrow 
     M_{p}(\Delta \theta)^2
        = \exp\left(2A(\theta_\epsilon) - 2A(\theta_0)\right)\\  
    &\Rightarrow 
     M_{p}(2\Delta \theta) 
        = \exp\left(A(2\theta_\epsilon - \theta_0) - A(\theta_0)\right)
\end{align*}

So, that we have on the one hand:
\begin{align*}
M_{p^\epsilon}(-\Delta \theta)M_{p^\epsilon}(\Delta \theta)
    &= \exp\left(A(\theta_0) - A(\theta_\epsilon)\right).  
        \exp\left(A(2\theta_\epsilon-\theta_0) - A(\theta_\epsilon)\right)
    \end{align*}
and on the other hand:
\begin{align*}
    \frac{M_{p}(2\Delta \theta)}{M_{p}(\Delta \theta)^2}
        &= \frac{\exp\left(A(2\theta_\epsilon - \theta_0) - A(\theta_0)\right)}{\exp\left(2A(\theta_\epsilon) - 2A(\theta_0)\right)}\\
        &= \frac{\exp\left(A(2\theta_\epsilon - \theta_0)\right)}{\exp\left(2A(\theta_\epsilon) - 2A(\theta_0)\right)} \cdot \frac{1}{\exp\left(A(\theta_0)\right)}\\
        &= \frac{\exp\left(A(2\theta_\epsilon - \theta_0)\right)}{\exp\left(2A(\theta_\epsilon) - A(\theta_0)\right)}\\
        &= \exp\left(A(2\theta_\epsilon - \theta_0) +A(\theta_0) -  A(\theta_\epsilon) - A(\theta_\epsilon)\right)\\
        &= \exp\left(A(\theta_0) -  A(\theta_\epsilon) + A(2\theta_\epsilon - \theta_0) - A(\theta_\epsilon)\right)\\
        &= \exp\left(A(\theta_0) -  A(\theta_\epsilon)\right).\exp\left( A(2\theta_\epsilon - \theta_0) - A(\theta_\epsilon)\right)
\end{align*}

Consequently, we get two equivalent expressions for our final result:
\begin{align*}    
    &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \exp(A(\theta^0)- A(\theta^\epsilon))^{m_j} \exp\left(A(2\theta_\epsilon-\theta_0) - A(\theta_\epsilon)\right)^{m_j}
            -1
            \right]^{\frac{1}{2}}\\
    &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \left( M_{p^\epsilon}(-\Delta \theta)M_{p^\epsilon}(\Delta \theta) \right)^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{(first expression)}\\
       &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \left( \frac{M_{p}(2\Delta \theta)}{M_{p}(\Delta \theta)^2} \right)^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{(second expression)}
\end{align*}

We will use the second expression.

\end{proof}

\subsection{Proof for categorical BoP}\label{sec:proof-binary}

Here, we apply the exponential family result found in \ref{sec:exp_family_proof} to find the lower bound for a categorical distribution.

\begin{corollary}
    \label{prop:lower_bound_categorical}
    [Lower bound for categorical individual BoP for any number of samples in each group \citep{monteiro2022epistemic}]\label{th:lower_bound_binary_a}
The lower bound writes:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(1+4\epsilon^2\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
\end{align*}
where $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) \geq \epsilon$. 
\end{corollary}


\begin{proof}
    By Proposition~\ref{prop:lower_bound_exponential_fam}, we have:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{M_p(2\Delta\theta)}{M_p(\Delta \theta)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}
\end{align*}

\paragraph{Plug in Categorical assumption} We find the bound for the categorical case. For the categorical, we have $\theta = \theta_1$ and:
\begin{align*}
\theta_0 
&= \log\left(\frac{p_1}{p_2}\right) 
= \log\frac{1/2}{1/2} = 0\\
\theta_\epsilon 
&= \log\left(\frac{p_1^\epsilon}{p_2^\epsilon}\right) 
= \log\left(\frac{1/2+\epsilon}{1/2-\epsilon}\right)
=\log\left(\frac{1+2\epsilon}{1-2\epsilon}\right)\\
A(\theta_0) &= \log\left(e^{\theta_0} + 1\right) = \log(2)\\
A(\theta_\epsilon) 
&=\log\left(e^{\theta_\epsilon}+1\right)
=\log\left(\frac{1+2\epsilon}{1-2\epsilon}+1\right) 
=\log\left(\frac{1+2\epsilon+1-2\epsilon}{1-2\epsilon}\right) 
=\log\left(\frac{2}{1-2\epsilon}\right) 
\\
A(2\theta_\epsilon) 
&= \log\left(e^{2\theta_\epsilon}+1\right)\\
&=\log\left((e^{\theta_\epsilon})^2+1\right)\\
&=\log\left(\left(\frac{1+2\epsilon}{1-2\epsilon}\right)^2+1\right)\\
&=\log\left(\frac{1+4\epsilon+4\epsilon^2}{1-4\epsilon+4\epsilon^2}+1\right)\\
&=\log\left(\frac{1+4\epsilon+4\epsilon^2+1-4\epsilon+4\epsilon^2}{1-4\epsilon+4\epsilon^2}\right)\\
&=\log\left(\frac{2+8\epsilon^2}{1-4\epsilon+4\epsilon^2}\right)\\
\end{align*}
We also have: $\Delta \theta =\theta_\epsilon$. 

Accordingly, we have:
\begin{align*}
M_p(\Delta \theta) 
    &= \exp\left( A(\theta_0 + \Delta \theta) - A(\theta_0)\right)\\
    &= \exp\left( A(\theta_\epsilon) - A(\theta_0)\right)\\
    &=\exp\left( \log\left(\frac{2+\epsilon}{1-2\epsilon}\right) - \log(2) \right)\\
    &=\exp\log\left(\frac{1}{2}\left(\frac{2}{1-2\epsilon}\right) \right)\\
    &=\frac{1}{1-2\epsilon}\\
M_p(2\Delta \theta) 
    &= \exp\left( A(\theta_0 +2 \Delta \theta) - A(\theta_0)\right)\\
    &= \exp\left( A(2 \theta_\epsilon \theta) - A(\theta_0)\right)\\
    &= \exp\left( \log\left(\frac{2+8\epsilon^2}{1-4\epsilon+4\epsilon^2}\right)- \log(2)\right) \\
    &=\exp \log \left(\frac{1}{2}\frac{2+8\epsilon^2}{1-4\epsilon+4\epsilon^2}\right)\\
    &=\frac{1+4\epsilon^2}{1-4\epsilon+4\epsilon^2}
\end{align*}

And the lower bound becomes:
\begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 
        1 - TV(P\parallel Q)\\
      \Rightarrow
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{M_p(2\Delta\theta)}{M_p(\Delta \theta)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
        &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{\frac{1+4\epsilon^2}{1-4\epsilon+4\epsilon^2}}{\left(\frac{1}{1-2\epsilon}\right)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
        &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{\frac{1+4\epsilon^2}{1-4\epsilon+4\epsilon^2}}{\frac{1}{1-4\epsilon+4\epsilon^2}}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
        &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(1+4\epsilon^2\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
\end{align*}

\end{proof}


\subsection{Maximum attributes (categorical BoP) for all people}\label{sec:max_attributes_categorical}
 In the case where dataset $\mathcal{D}$ is drawn from an unknown distribution and has $d$ groups where $d=2^k$, with each group having $m = \lfloor N/d \rfloor$ samples, Corollary~\ref{prop:lower_bound_categorical} becomes:
\begin{align*}
          \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[
            \left(1+4\epsilon^2\right)^m
            -
            1
            \Bigg]^{\frac{1}{2}}\\
\end{align*}
\begin{corollary}[Maximum attributes (categorical) for all people] 
 Consider auditing a personalized classifier $h_p$ to verify if it provides a gain of $\epsilon =0.01$ to each group on an auditing dataset $D$. Consider an auditing dataset with $N = 8 \times 10^9$ samples, or one sample for each person on earth. If $h_p$ uses more than $k \geq 18$ binary group attributes, then for any hypothesis test there will exist a pair of probability distributions $P_{X,G,Y} \in H_0, \quad Q_{X,G,Y} \in H_1$ for which the test results in a probability of error that exceeds $50 \%$. 
\begin{equation}
       k \geq 18 \implies \min_{\Psi} \max_{\substack{P_{X,G,Y} \in H_0 \\ Q_{X,G,Y} \in H_1}} P_e \geq \frac{1}{2}.
\end{equation}
\end{corollary}


\subsection{Proof for Gaussian BoP}\label{sec:proof-real-valued}

Here, we do the proof assuming that the BoP is a normal variable with a second moment bounded by $\sigma^2$.

\begin{corollary}
\label{prop:lower_bound_gaussian}
[Lower bound for Gaussian BoP for any number of samples in each group]
The lower bound writes:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
            \Bigg[
                \frac{1}{d} \sum_{j=1}^d \exp\left(\frac{{m_j}\epsilon^2}{\sigma^2}\right)
                -
                1
                \Bigg]^{\frac{1}{2}}
\end{align*}
where $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) > 0$.
\end{corollary}


\begin{proof}
    By Proposition~\ref{prop:lower_bound_exponential_fam}, we have:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{M_p(2\Delta\theta)}{M_p(\Delta \theta)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}
\end{align*}

\paragraph{Plug in Gaussian assumption} We find the bound for the Gaussian case. For the Gaussian, we have:
\begin{align*}
\theta_0 &= \frac{\mu_0}{\sigma^2} = 0\\
\theta_\epsilon &= \frac{\mu_\epsilon}{\sigma^2} = \frac{\epsilon}{\sigma^2}\\
A(\theta_0) &= \frac{\sigma^2\theta_0^2}{2} = 0\\
A(\theta_\epsilon) &= \frac{\sigma^2\theta_\epsilon^2}{2} = \frac{\epsilon^2}{2\sigma^2}\\
A(2\theta_\epsilon) &= \frac{\sigma^24\theta_\epsilon^2}{2} = \frac{2\epsilon^2}{\sigma^2}
\end{align*}
because $\mu_0 = 0$ and $\mu_\epsilon = \epsilon$ by construction. Thus, we also have: $\Delta \theta =\theta_\epsilon$. 

Accordingly, we have:
\begin{align*}
M_p(\Delta \theta) 
    &= \exp\left( A(\theta_0 + \Delta \theta) - A(\theta_0)\right)
    = \exp\left( A(\theta_\epsilon) - A(\theta_0)\right)
    = \exp\left(\frac{\epsilon^2}{2\sigma^2}\right)\\
M_p(2\Delta \theta) 
    &= \exp\left( A(\theta_0 +2 \Delta \theta) - A(\theta_0)\right)
    = \exp\left( A(2\theta_\epsilon - \theta_0)\right)
    = \exp\left( A(2\theta_\epsilon)\right)
    = \exp\left(\frac{2\epsilon^2}{\sigma^2}\right)   
\end{align*}

And the lower bound becomes:
\begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 
        1 - TV(P\parallel Q)\\
      \Rightarrow
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{M_p(2\Delta\theta)}{M_p(\Delta \theta)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
      &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{\exp\left(\frac{2\epsilon^2}{\sigma^2}\right)}{\exp\left(\frac{\epsilon^2}{2\sigma^2}\right)^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
      &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{\exp\left(\frac{2\epsilon^2}{\sigma^2}\right)}{\exp\left(\frac{2\epsilon^2}{2\sigma^2}\right)}\right)^{m_j}
               -
            1
            \Bigg]^{\frac{1}{2}}\\
      &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \left(\frac{\exp\left(\frac{2\epsilon^2}{\sigma^2}\right)}{\exp\left(\frac{\epsilon^2}{\sigma^2}\right)}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
      &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \frac{1}{d} \sum_{j=1}^d
            \exp\left(\frac{\epsilon^2}{\sigma^2}\right)^{m_j}
            -
            1
            \Bigg]^{\frac{1}{2}}\\
      &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[
            \frac{1}{d} \sum_{j=1}^d \exp\left(\frac{{m_j}\epsilon^2}{\sigma^2}\right)
            -
            1
            \Bigg]^{\frac{1}{2}}
\end{align*}
In the case where each group has a different standard deviation of their BoP distribution, this becomes:

\begin{align*}
    &= 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[
            \frac{1}{d} \sum_{j=1}^d \exp\left(\frac{{m_j}\epsilon^2}{\sigma_j^2}\right)
            -
            1
            \Bigg]^{\frac{1}{2}}
\end{align*}
\end{proof}

\subsection{Maximum attributes (Gaussian BoP) for all people}\label{sec:max_attributes_gaussian}
 In the case where dataset $\mathcal{D}$ is drawn from an unknown distribution and has $d$ groups where $d=2^k$, with each group having $m = \lfloor N/d \rfloor$ samples, Corollary~\ref{prop:lower_bound_gaussian} becomes:
\begin{align*}
            \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}} 
        \Bigg[ \exp\left(\frac{{m}\epsilon^2}{\sigma^2}\right)
            -
            1
            \Bigg]^{\frac{1}{2}}
\end{align*}
\begin{corollary}[Maximum attributes (Gaussian BoP) for all people] 
 Consider auditing a personalized classifier $h_p$ to verify if it provides a gain of $\epsilon =0.01$ to each group on an auditing dataset $D$. Consider an auditing dataset with $\sigma = 0.1$ and $N = 8 \times 10^9$ samples, or one sample for each person on earth. If $h_p$ uses more than $k \geq 22$ binary group attributes, then for any hypothesis test there will exist a pair of probability distributions $P_{X,G,Y} \in H_0, \quad Q_{X,G,Y} \in H_1$ for which the test results in a probability of error that exceeds $50 \%$. 
\begin{equation}
       k \geq 22 \implies \min_{\Psi} \max_{\substack{P_{X,G,Y} \in H_0 \\ Q_{X,G,Y} \in H_1}} P_e \geq \frac{1}{2}.
\end{equation}
\end{corollary}

\subsection{Proof for the symmetric generalized normal distribution}\label{subsec:symm_gauss}

% Source: https://en.wikipedia.org/wiki/Generalized_normal_distribution
\paragraph{Symmetric Generalized Gaussian} The symmetric generalized Gaussian distribution, also known as the exponential power distribution, is a generalization of the Gaussian distributions that include the Laplace distribution. A probability distribution in this family has probability density function:
\begin{equation}
p(x | \mu, \alpha, \beta ) = 
    \frac {\beta }{2\alpha \Gamma (1/\beta )}
    \exp\left(-\left(\frac{|x-\mu |}{\alpha }\right)^\beta\right),
\end{equation}
with mean and variance:
\begin{equation}
    \mathbb{E}[X] = \mu, \quad V[X] = \frac {\alpha ^{2}\Gamma (3/\beta )}{\Gamma (1/\beta )}.
\end{equation}
We can write the standard deviation $\sigma = \alpha\sqrt{\frac{\Gamma(1/\beta)}{\Gamma(3/\beta)}}=\alpha\gamma(\beta)$ where we introduce the notation $\gamma(\beta) = \sqrt{\frac{\Gamma(1/\beta)}{\Gamma(3/\beta)}}$. This notation will become convenient in our computations.

\paragraph{Example: Laplace} The Laplace probability density function is given by:
\begin{equation}
    f(x\mid \mu ,b)
        =\frac{1}{2b}
        \exp \left(-\frac {|x-\mu |}{b}\right)
\end{equation}
which is in the family for $\alpha = b $ and $\beta = 1$, since the gamma function verifies $\Gamma(1) = (1-1)! = 0!= 1$.

\begin{proposition}\label{prop:lower-bound-gaussian}[Lower bound for symmetric generalized Gaussian BoP for any number of samples in each group]
The lower bound writes:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \exp\left(-
            \frac{|B - \epsilon|^\beta-|B|^\beta}{\alpha^\beta}
            \right)\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{align*}
where $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) > 0$.
\end{proposition}

\begin{proof}
    By Theorem~\ref{th:lower_bound_any_prob}, we have:
\begin{align*}
      \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
           \frac{p^{\epsilon}(B)}{p(B)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{align*}
\paragraph{Plug in the symmetric generalized Gaussian distribution} Under the assumption that the random variable $B$ follows an exponential power distribution, we continue the computations as:
\begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e &\geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \frac{ \exp\left(-\left(\frac{|B - \epsilon|}{\alpha}\right)^\beta\right)}{ \exp\left(-\left(\frac{|B|}{\alpha}\right)^\beta\right)}\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}\\
        &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \exp\left(-\left(\frac{|B - \epsilon|}{\alpha}\right)^\beta\right)
            . \exp\left(+\left(\frac{|B|}{\alpha}\right)^\beta\right)\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{(property of exp)}\\
        &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \exp\left(-\left(\frac{|B - \epsilon|}{\alpha}\right)^\beta
            +
            \left(\frac{|B|}{\alpha}\right)^\beta\right)\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{(property of exp)}\\
        &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \exp\left(-
            \frac{|B - \epsilon|^\beta-|B|^\beta}{\alpha^\beta}
            \right)\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}~\text{(property of exp)}\\
\end{align*}
\end{proof}

\subsection{Proof for Laplace BoP}\label{sec:proof-real-valued-laplace}
Here, we do the proof assuming that the BoP is a Laplace distribution (for more peaked than the normal variable).
\begin{corollary}
\label{cor:lower_bound_laplace}
[Lower bound for a Laplace BoP for any number of samples in each group]
The lower bound writes:
\
\begin{align*} 
\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq  1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d 
            \exp\left(-\frac{m_j\epsilon }{b} \right)
            -1
            \right]^{\frac{1}{2}}
\end{align*}
where $P_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data, for which the generic model $h_0$ performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) < 0$, and $Q_{\mathbf{X}, \mathbf{S}, Y}$ is a distribution of data points for which the personalized model performs better, i.e., the true $\gamma$ is such that $\gamma(h_0, h_p, \mathcal{D}) > 0$.
\end{corollary}

\begin{proof}
    By Proposition~\ref{prop:lower-bound-gaussian}, we have:
\begin{align*}
    \min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}} P_e \geq 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \exp\left(-
            \frac{|B - \epsilon|^\beta-|B|^\beta}{\alpha^\beta}
            \right)\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}
\end{align*}
Plugging in our values of $\alpha$ and $\beta$ shown to satisfy the Laplace probability density function we get:
    \begin{align*}
    &= 1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d \mathbb{E}_{p^\epsilon}\Bigg[
            \exp\left(-
            \frac{|B - \epsilon|-|B|}{b}
            \right)\Bigg]^{m_j}
            -1
            \right]^{\frac{1}{2}}\\
    \end{align*}
\paragraph{Using bounds} Since we are finding the worst case lower bound, we will find functions that upper and lower bound $|B - \epsilon| - |B|$. This function is lower bounded by $\epsilon$ and upper bounded by $-\epsilon$ since $\epsilon < 0$. Indeed, since $\epsilon < 0$, there are three cases: 
\begin{itemize}
    \item $0 < B < B - \epsilon$: this gives $|B - \epsilon| - |B|=  B - \epsilon - B = -\epsilon$
    \item $B < 0 < B - \epsilon$ : this gives $|B - \epsilon| - |B|=  B - \epsilon + B = 2 B -\epsilon > 2 \epsilon - \epsilon = \epsilon$ since $0 < B - \epsilon$.
    \item $  B < B - \epsilon < 0$: this gives $|B - \epsilon| - |B|=-B + \epsilon + B =  \epsilon$.
\end{itemize}
Thus, we have:
$ \epsilon \leq |B - \epsilon| - |B| \leq -\epsilon$ 
and:
\begin{align*}
    &
    \epsilon 
    \leq 
    |B - \epsilon| - B| 
    \leq 
    -\epsilon\\
    & \Rightarrow 
    \frac{\epsilon}{b} 
    \leq 
   \frac{|B - \epsilon|-|B|}{b}
    \leq 
    -\frac{\epsilon}{b}\\
    & \Rightarrow 
    -\frac{\epsilon}{b} 
    \geq 
   -\frac{|B - \epsilon|-|B|}{b}
    \geq 
    \frac{\epsilon}{b}\\
    & \Rightarrow 
    \exp\left(-\frac{\epsilon}{b} \right) 
    \geq 
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
    \geq 
    \exp\left(\frac{\epsilon}{b}\right)
\end{align*}

Thus, applying the expectation gives:
\begin{align*}
      &  
      \mathbb{E}_{p^\epsilon}\left[
      \exp\left(-\frac{\epsilon}{b} \right) 
      \right]
    \geq 
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]
    \geq 
    \mathbb{E}_{p^\epsilon}\left[
    \exp\left(\frac{\epsilon}{b}\right)
    \right]\\
     & \Rightarrow 
      \exp\left(-\frac{\epsilon}{b} \right) 
    \geq 
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]
    \geq 
    \exp\left(\frac{\epsilon}{b}\right)
\end{align*}
because the lower and upper bounds do not depend on $B$.

All the terms in these inequalities are positive, and the power function is increasing on positive numbers. Thus, we get:
\begin{align*}
    & 
      \exp\left(-\frac{\epsilon}{b} \right)^{m_j} 
    \geq 
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]^{m_j}
    \geq 
    \exp\left(\frac{\epsilon}{b}\right)^{m_j}\\
    &\Rightarrow
    \frac{1}{d}\sum_{j=1}^d
    \exp\left(-\frac{\epsilon}{b} \right)^{m_j} 
    \geq 
      \frac{1}{d}\sum_{j=1}^d
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]^{m_j}
    \geq 
      \frac{1}{d}\sum_{j=1}^d
    \exp\left(\frac{\epsilon}{b}\right)^{m_j}\\
     &\Rightarrow
    \frac{1}{d}\sum_{j=1}^d
    \exp\left(-\frac{m_j\epsilon}{b} \right)
    \geq 
      \frac{1}{d}\sum_{j=1}^d
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]^{m_j}
    \geq 
      \frac{1}{d}\sum_{j=1}^d
    \exp\left(\frac{m_j\epsilon}{b}\right)\\  
     &\Rightarrow
    \frac{1}{d}\sum_{j=1}^d
    \exp\left(-\frac{m_j\epsilon}{b} \right) -1
    \geq 
      \frac{1}{d}\sum_{j=1}^d
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]^{m_j} -1
    \geq 
      \frac{1}{d}\sum_{j=1}^d
    \exp\left(\frac{m_j\epsilon}{b}\right) -1 \\   
        &\Rightarrow
    \left(\frac{1}{d}\sum_{j=1}^d
    \exp\left(-\frac{m_j\epsilon}{b} \right) -1\right)^{\frac{1}{2}}
    \geq 
      \left(\frac{1}{d}\sum_{j=1}^d
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]^{m_j} -1\right)^{\frac{1}{2}}
    \geq 
      \left(\frac{1}{d}\sum_{j=1}^d
    \exp\left(\frac{m_j\epsilon}{b}\right) -1\right)^{\frac{1}{2}} \\  
        &\Rightarrow
   -\frac{1}{2\sqrt{d}}  \left(\frac{1}{d}\sum_{j=1}^d
    \exp\left(-\frac{m_j\epsilon}{b} \right) -1\right)^{\frac{1}{2}}
    \leq 
      -\frac{1}{2\sqrt{d}} \left(\frac{1}{d}\sum_{j=1}^d
   \mathbb{E}_{p^\epsilon}\left[
   \exp\left(-\frac{|B - \epsilon|-|B|}{b}\right) 
   \right]^{m_j} -1\right)^{\frac{1}{2}}
    \leq 
     -\frac{1}{2\sqrt{d}} \left(\frac{1}{d}\sum_{j=1}^d
    \exp\left(\frac{m_j\epsilon}{b}\right) -1\right)^{\frac{1}{2}}   
\end{align*}


\paragraph{Back to Probability of error} To maximize $P_e$, we take the function that gives us the lower bound. Plugging this upper bound back into our equation for $P_e$:
\begin{align*} 
\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq  1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d 
            \exp\left(-\frac{m_j\epsilon }{b} \right)
            -1
            \right]^{\frac{1}{2}}
\end{align*}
In the case where each group has a different scale parameter of their BoP distribution, this becomes:
\begin{align*} 
\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq  1 - \frac{1}{2\sqrt{d}}\left[
             \frac{1}{d}
            \sum_{j=1}^d 
            \exp\left(-\frac{m_j\epsilon }{b_j} \right)
            -1
            \right]^{\frac{1}{2}}
\end{align*}
\end{proof}

\subsection{Maximum attributes (Laplace BoP) for all people}\label{sec:max_attributes_laplace}
 In the case where dataset $\mathcal{D}$ is drawn from an unknown distribution and has $d$ groups where $d=2^k$, with each group having $m = \lfloor N/d \rfloor$ samples, Corollary~\ref{cor:lower_bound_laplace} becomes:
\begin{align*} 
\min _{\Psi} 
    \max _{\substack{P_0 \in H_0 \\ P_1 \in H_1}}
        P_e 
        &\geq  1 - \frac{1}{2\sqrt{d}}\left[
            \exp\left(-\frac{m\epsilon }{b} \right)
            -1
            \right]^{\frac{1}{2}}
\end{align*}

\begin{corollary}[Maximum attributes (Laplace) for all people] 
 Consider auditing a personalized classifier $h_p$ to verify if it provides a gain of $\epsilon =0.01$ to each group on an auditing dataset $D$. Consider an auditing dataset with $\sigma=0.1$ and $N = 8 \times 10^9$ samples, or one sample for each person on earth. If $h_p$ uses more than $k \geq 26$ binary group attributes, then for any hypothesis test there will exist a pair of probability distributions $P_{X,G,Y} \in H_0, \quad Q_{X,G,Y} \in H_1$ for which the test results in a probability of error that exceeds $50 \%$. 
\begin{equation}
       k \geq 26 \implies \min_{\Psi} \max_{\substack{P_{X,G,Y} \in H_0 \\ Q_{X,G,Y} \in H_1}} P_e \geq \frac{1}{2}.
\end{equation}
\end{corollary}







