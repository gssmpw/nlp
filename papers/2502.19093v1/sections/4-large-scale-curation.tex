\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{./assets/imgs/1-pipeline/pipeline.pdf}
    \caption{
        \revised{
 The semi-automatic data curation pipeline for historical Chinese visualizations and illustrations:
 (A) We collect historical Chinese books from collections of three digital libraries.
 (B) We unify and deduplicate book metadata from different libraries, then fetch corresponding images.
 (C) We utilize YOLO to detect \itemTypePlural from these images. Some incomplete \itemTypePlural are stitched.
 (D) The \itemTypePlural are further classified through three steps. 
 Initial labeling aims to obtain a basic taxonomy.
 Batching labeling aims to further expand the quantity of labeled \itemTypePlural.
 Similarity-based matching assigns labels to unlabeled \itemTypePlural.
 (E) The data structures for the metadata of three types of entities involved in \datasetName: \schemaBook, \schemaImage, and \schemaGraphic.
 }
        \yzc{
 1. why are some attributes not aligned with OldVisOnline:
 - source -> sources.name, source.url, source.accessDate
 - author -> authors
 2. compile an appendix illustrating the meaning of each data attribute.
 3. we may refactor ``theme'', ``type'', and ``cls'' into a single ``classes''/``categories'' attribute that stores the categories paths
 4. document the exact meaning of each attribute in an appendix
 }
 }
    \Description{
 The pipeline for data curation.
 The pipeline has two main stages: data collection and data processing.
 The data processing stage involves data wrangling, graphics extraction, and classification.
 }
    \label{fig:pipeline}
\end{figure*}

\section{Large-Scale Data Curation for \datasetName}
\label{sec:large-scale-curation}

\revised{
Following the preliminary manual data curation process in \cref{sec:preliminary-curation}, this section introduces a semi-automatic pipeline for large-scale data curation.
\Cref{fig:pipeline} shows the pipeline with two main stages: data collection and data processing.
\Cref{sec:data-source} describes the data sources for data collection (\cref{fig:pipeline}(A)).
The data processing stage involves data wrangling (\cref{fig:pipeline}(B) and \cref{sec:data-wrangling}), graphics extraction (\cref{fig:pipeline}(C) and \cref{sec:graphic-extraction}), and classification (\cref{fig:pipeline}(D) and \cref{sec:classification}).
Through the pipeline, we obtain images of historical Chinese graphics and related metadata (\cref{fig:pipeline}(E)).
Note that the books from Shuge and Dunhuang manuscripts from Gallica collected in the preliminary data curation (\cref{sec:preliminary-curation}) also went through the data processing steps.
\Cref{sec:gallery} introduces the gallery for browsing the resulting \datasetName dataset.
}

\subsection{Data Source}
\label{sec:data-source}

\revised{
To scale our data collection, we focus on the Chinese rare book collections from three digital libraries: \dbname{Library of Congress}~\cite{LibraryCongressLibrary}, \dbname{Harvard Library}~\cite{HarvardLibrary2023Harvard}, and \dbname{National Diet Library}~\cite{NDLRare}.
These data sources provide online APIs for image retrieval.
Different digital libraries have different conventions for storing books.
Some may store a book in multiple collection items.
Thus, for each data source, we go through a custom process to merge items into books.
We obtain \numBooksLoC, \numBooksHL, and \numBooksNDL books from these digital libraries, respectively.
For each book, we obtain their metadata and their corresponding image resources stored as IIIF manifests~\cite{IIIF2011International}.
}


\future{Document the raw data structure from the digital libraries in an appendix (like the appendix of OldVisOnline).}
\future{report the number of duplicates and the number of different editions of the same book}

\subsection{Data Wrangling}
\label{sec:data-wrangling}

\revised{
To merge the data obtained from different sources, we go through metadata unification, book deduplication, and image fetching.

\subsubsection{Metadata Unification}

We normalize the metadata obtained from the data sources into three schemas: \schemaBook, \schemaImage, and \schemaGraphic.
\Cref{fig:pipeline}(E) shows the attributes stored for each schema.

\begin{itemize}[leftmargin=3.5mm]
    \item A \schemaBook instance corresponds to a book from the data sources.
    
    \item An \schemaImage instance corresponds to a bitmap image of one or multiple pages in a book.
 An \schemaImage instance is usually associated with a \schemaBook instance through the \inlinecode{bookUuid} attribute.
 Each \schemaBook instance corresponds to at least one and typically multiple \schemaImage instances.
 Note that the \schemaImage instances obtained from the web and Gallica in \cref{sec:preliminary-curation} are not associated with any \schemaBook instance.
 Some of these images are not from books, and the original books of the others are hard to identify.
    
    \item A \schemaGraphic instance corresponds to a \taxon{visualization} or \taxon{illustration} detected from an \schemaImage instance.
    \Cref{sec:graphic-extraction} details the detection method.
 A \schemaGraphic instance is associated with an \schemaImage instance through the \inlinecode{imgUuid} attribute.
 Each \schemaImage instance may correspond to no, one, or multiple \schemaGraphic instances.
\end{itemize}


The data structure was adapted from the OldVis schema~\cite{Zhang2024OldVisOnline}.
Note that the original OldVis schema only concerns the image entity and does not consider the concepts of books containing images and graphics detected from images.
}

\subsubsection{Book Deduplication}

\revised{
After obtaining the \schemaBook instances, we deduplicate them through a semi-automatic matching process.

Let $len(x)$ be the length of arbitrary string $x$.
Let $lev(x, y)$ be the Levenshtein distance function between arbitrary strings $x$ and $y$.
Let strings $a$ and $b$ be the \inlinecode{title} of two \schemaBook instances.
Let strings $a'$ and $b'$ be the concatenations of \inlinecode{title}, \inlinecode{publishDate}, and the number of associated \schemaImage instances of two \schemaBook instances.
We find all pairs of \schemaBook instances that satisfies both $1 - \frac{lev(a, b)}{len(a) + len(b)} > 0.9$ and $1 - \frac{lev(a', b')}{len(a') + len(b')} > 0.95$.

Then, we manually verify all the resulting pairs of \schemaBook instances, which are potential duplicates.
Note that we do not consider different editions of the same book to be duplicates.

\subsubsection{Image Fetching}

With the stored metadata of \inlinecode{book} and \inlinecode{image}, we fetch the corresponding images from the digital libraries with \inlinecode{downloadUrl} attribute values.
}

\subsection{Graphics Extraction}
\label{sec:graphic-extraction}

\revised{
As described in \cref{sec:preliminary-curation}, we use \taxon{graphic} to refer to both \taxon{visualization} and \taxon{illustration}.
The following introduces the two steps to extract \schemaGraphic instances from \schemaImage instance: detecting graphics from images and stitching parts of graphics split across pages.
}

\subsubsection{Detection}
\label{sec:detection}

We fine-tuned the YOLOv8l model~\cite{Jocher2023Ultralytics} to detect graphics from images.
\revised{
As there is no training data on historical Chinese graphics, we conducted an iterative process of labeling, model fine-tuning, and detection on the remaining images.
}

We manually selected 8 books from Shuge~\cite{Ceng2013Shuge} with abundant visual contents and different themes.
From these books, we manually labeled the bounding boxes and types (\taxon{visualization} or \taxon{illustration}) of graphics with the Roboflow Annotate tool~\cite{RoboflowRoboflow}.
From the books, we obtained 976 images containing graphics.
\future{Whether 976 refers to the number of graphics or the number of images containing graphics needs to be checked.}
These images were then combined with \num{100} randomly sampled images containing no graphics, forming the initial training set with \num{1076} images.
The initial training set was then augmented to \num{2288} images using random cropping and blurring.
We use the initial training set to fine-tune YOLOv8l pre-trained on the COCO dataset~\cite{Lin2014Microsoft}.
\future{We may look for the model performance for the first round.}

The fine-tuned model was then used to detect graphics from \num{400} of the books in \dbname{Library of Congress}~\cite{LibraryCongressLibrary}, identifying \num{13782} potential graphics in total.
We then manually correct the detected result.
All the images containing true positive graphics and \num{700} randomly sampled images containing false positives combined with the initial training set to form the enlarged training set.
The enlarged training set contains \num{6783} images with \num{10056} graphics.
We fine-tuned YOLOv8l on the second dataset, which achieved \recall recall, \precision precision, and \Fone F1 score.
We then applied the fine-tuned models to the remaining unlabeled images from books in \dbname{Library of Congress}~\cite{LibraryCongressLibrary}, \dbname{Harvard Library}~\cite{HarvardLibrary2023Harvard}, and \dbname{National Diet Library}~\cite{NDLRare}.

Note that we do not include the titles in the bounding box of graphics.
The consideration is that when fine-tuning the model, including the titles in the bounding box may lead the model to learn to detect textual patterns.

\subsubsection{Stitching}
\label{sec:stitching}

Among the graphics we obtained, some graphics corresponded to the same visualization but were split on separate pages of a book.
We thus stitch such segments across pages.



We grouped instances of \schemaGraphic distributed at consecutive book pages into pairs.
Each pair was then processed by Gaussian blur to denoise and morphological closing to remove detail and small holes.
Next, we used CLIP ViT-B/32~\cite{Radford2021Learning} to compute embedding and calculate the similarity for each pair.
We manually checked the pairs with similarity above 0.95 on whether they should be stitched and used Adobe Photoshop for stitching.




\subsection{Classification}
\label{sec:classification}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{./assets/imgs/2-gallery/gallery.png}
    \caption{
        \textbf{Two interactive systems of \datasetName:}
        (A) ZuantuSet Gallery for the user to browse historical Chinese graphics.
        \revised{
        (A1) Detail panel of a graphic.
        (A2) Timeline showing the temporal distribution of the filtered graphics.
        (B) ZuantuSet Labeler for the user to categorize historical Chinese graphics.
        (B1) Single mode labeling for the user to edit the bounding box and metadata of a single \itemType.
        (B2) Batch mode labeling for the user to retrieve multiple \itemTypePlural through metadata query or similarity matching and label them at once.
        }
    }
    \label{fig:gallery}
    \Description{The image showcases a user interface for exploring, filtering, and efficiently labeling visualizations based on attributes like source, type, theme, and time.}
\end{figure*}

The object detection model in \cref{sec:detection} classified graphics into two categories: \taxon{visualization} and \taxon{illustration}.
Not every historical Chinese book contains \itemType.
Out of the total \numBook collected books, \numBookHasGraphic books contain \itemType.
As mentioned in \cref{sec:graphic-extraction}, \itemType are classified into \taxon{visualization} and \taxon{illustration}.
Among the books, \numBookHasVis books contain \taxon{visualization}.
If sorting these books with the number of contained visualizations in descending order, the top 84 books contribute around 50\% of the visualizations, and the top 838 books contribute around 90\% of the visualizations.

\revised{
To facilitate retrieval and analysis of the graphics, we aim to classify them into more specific subcategories further.
We perform a three-step classification process: initial labeling, batch labeling, and similarity-based matching.
The first two steps involved manual labeling and are conducted in the \datasetName labeler shown in \cref{fig:gallery}(B).
}
We categorize \taxon{graphic} on two aspects: form and domain.

\begin{itemize}[leftmargin=3.5mm]
    \item \textbf{Form} refers to the visual appearance of the graphic.
 For a \taxon{visualization}, the form corresponds to a chart type.
 For a \taxon{illustration}, the form corresponds to the prominent subject.
    
    \item \textbf{Domain} refers to the application domain of a graphic.
\end{itemize}

\subsubsection{Initial Labeling}

We perform an initial labeling process to develop a preliminary taxonomy of historical Chinese graphics.
In this process, \num{2000} graphics were randomly chosen and labeled by one of the authors using the single mode labeling interface, as shown in~\cref{fig:gallery}(B1).
The interface allows the annotator to create new subcategories for the form and domain aspects.
A leaf node of form and a leaf node of domain can be assigned to each graphic.
The annotator may revise incorrect predictions and add new tags for unseen forms or domains.
In the labeling process, for graphics with unsure labels or belonging to types with few instances, we group them into \taxon{others}.
The resulting taxonomy includes 5 visualization forms, 8 illustration forms, and 16 domains (excluding the \taxon{other} type), detailed in \cref{sec:dataset-sample} and discussed in \cref{sec:lesson}.

\subsubsection{Batch Labeling}

With the developed taxonomy, we used the batch mode labeling interface in~\cref{fig:gallery}(B2) to annotate more images.
The user may retrieve images with a similar visual appearance to a given image.
To identify similar graphics, we use CLIP ViT-B/32~\cite{Radford2021Learning} to compute the graphic embeddings and calculate the cosine similarity between embeddings.
If needed, the user may also search with other query criteria, such as searching for graphics from the same book.
By retrieving images that share similarities under certain criteria, the user may quickly assign labels to multiple graphics simultaneously.
For example, graphics with similar visual appearance may share the same form, and graphics in the same book may share the same domain. 
In total, we manually labeled \numChecked graphics with both form and domain through initial labeling and batching labeling.

\subsubsection{Similarity-Based Matching}

We use CLIP ViT-B/32~\cite{Radford2021Learning} to label forms of the remaining graphics based on similarity.
Each unlabeled \itemType is assigned the same labels as its most similar labeled graphic.
Through the three classification steps, we obtain the labels of \numClassified \itemTypePlural.

\subsection{Gallery}
\label{sec:gallery}

To present the outcome of our data curation process, we provide an online gallery as shown in \cref{fig:gallery}(A).
Users can select \itemTypePlural within the gallery by filtering forms, domains, and sources for visualizations and illustrations.
By clicking a \itemType thumbnail, the detail panel pops up, as shows in~\cref{fig:gallery}(A1).
The timeline (\cref{fig:gallery}(A2)) shows temporal distributions of selected \itemTypePlural. 
