\section{Experiments and analysis} \label{sec:5}
This section presents comprehensive experiments on the ATR2-HUTD dataset to evaluate the effectiveness of the proposed method. 
Section~\ref{sec:4.1} outlines the experimental metrics used. 
Section~\ref{sec:4.2} details the network architecture, comparison methods, experimental setup, and parameter configurations. 
To highlight the superiority of the proposed method, Section~\ref{sec:4.3} provides both quantitative analysis and visual evaluations across all comparison methods. 
Section~\ref{sec:4.4} includes ablation studies to assess the contributions of different model components, while Section~\ref{sec:4.5} presents a parameter sensitivity analysis.
\subsection{Evaluation Indicators}\label{sec:4.1}
To quantitatively assess the performance of the proposed method, we employ three widely recognized evaluation metrics in the HTD field.
\par
\textbf{(\romannumeral1) Receiver Operating Characteristic (ROC)~\cite{ROC, ROC3D}:} 
The ROC curve offers an unbiased, threshold-independent evaluation of detection performance. This paper presents three 2D ROC curves: $( \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})$, $( \mathrm{P}_{\mathrm{d}}, \tau)$, and $( \mathrm{P}_{\mathrm{f}}, \tau)$, along with a 3D ROC curve~\cite{ROC3D} of $(\tau, \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})$ for a comprehensive performance evaluation. A detector with ROC curves closer to the upper left, upper right, and lower left corners generally exhibits superior HTD performance.
\par
\textbf{(\romannumeral2) Area Under the ROC Curve (AUC)~\cite{Zhang2015}:} 
To address challenges in visually comparing ROC curves, we compute the area under each of the three 2D ROC curves: $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})}$, $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \tau)}$, and $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}$. Larger AUC values indicate better performance, with $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})} \to 1$, $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \tau)} \to 1$, and $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)} \to 0$ signifying superior detection performance. Additionally, two AUC-based metrics are introduced for a more comprehensive evaluation:
\begin{equation}
    \mathrm{AUC}_{\mathrm{OA}} = \mathrm{AUC}_{\left(P_f, P_d\right)} + \mathrm{AUC}_{\left(\tau, P_d\right)} - \mathrm{AUC}_{\left(\tau, P_f\right)},
\end{equation}
\begin{equation}
    \mathrm{AUC}_{\mathrm{SNPR}} = \frac{\mathrm{AUC}_{\left(\tau, P_d\right)}}{\mathrm{AUC}_{\left(\tau, P_f\right)}},
\end{equation}
where higher values of $\mathrm{AUC}_{\mathrm{OA}} \to 2$ and $\mathrm{AUC}_{\mathrm{SNPR}} \to +\infty$ indicate improved detector performance.
% \textbf{(\romannumeral3) Separability Map~\cite{Liu2022}:} The degree of separation between the targets and backgrounds in the detection map is a critical performance indicator for UTD methods. 
% Thus, we also utilize the separability map for quantitative comparison in this study. 
% Specifically, the separability map uses green and blue boxes to represent the statistics of the target and background, respectively. 
% The horizontal line within each box indicates the median value, while the upper and lower whiskers denote the maximum and minimum values, providing a clear representation of the data range and central tendency. 
% \par
% A larger overlap between the two boxes suggests that the statistics of the target and background are similar, indicating poor separation between them. 
% Conversely, less overlap indicates better separation. 
% Moreover, background suppression is considered more effective when the blue box is closer to the ordinate 0, while higher target prominence is indicated when the green box is closer to ordinate 1.
% \clearpage
\subsection{Experimental Details and Settings}\label{sec:4.2}
\textbf{(\romannumeral1) Experimental Details:} 
The experimental setup and details of the proposed method are as follows. Unless otherwise specified, the parameters are applied consistently across all sub-datasets. The method consists of three core components: the RGC module, the HLCL module, and the SPL strategy, each contributing significantly to performance.

In the RGC module, unsupervised clustering is performed using the K-Means~\cite{Sinaga2020} algorithm, with cluster numbers set to 36, 39, and 42 for the lake, river, and sea sub-datasets, respectively, based on environmental complexity and waterbed characteristics.

The HLCL module employs the 3D-ResNet50~\cite{Jiang2019} network for spectral-spatial feature extraction. To enhance robustness and contrastive learning, untargeted FGSM~\cite{GoodfellowSS14} data augmentation is applied with a maximum perturbation of $\epsilon=0.1$ under the $l_{\infty}$ norm. The hybrid-level contrastive learning framework is trained for 50 epochs per SPL iteration. The Adam optimizer is used with a batch size of 256. The initial learning rate is $5\times10^{-3}$, decaying to $5\times10^{-5}$ through a cosine annealing schedule after 100 epochs, and a weight decay of $1\times10^{-4}$ is applied to reduce overfitting.

The SPL strategy is executed for 10 iterations across all sub-datasets to ensure convergence and computational efficiency.

For HUTD, as described in Section~\ref{sec3.4}, we use learned representations combined with basic hyperspectral detectors. To isolate the effect of detectors on performance, we employ two classic detectors, CEM~\cite{KRUSE1993145} and SAM~\cite{Manolakis2002}, as baseline methods.

\textbf{(\romannumeral2) Experimental Settings:} 
We compare the proposed method against several state-of-the-art (SOTA) HTD and HUTD methods, including two traditional HTD detectors (CEM and SAM), two advanced HTD methods (IEEPST~\cite{IEEPST} and MCLT~\cite{Wang2024}), and four HUTD methods (UTD-Net~\cite{Qi2021}, TUTDF~\cite{LiZheyong2023}, TDSS-UTD~\cite{Li2023}, and NUN-UTD~\cite{Liu2024}).

To ensure fairness, each method is executed with the original hyperparameter settings as specified in their respective publications. All experiments are conducted on a machine equipped with seven NVIDIA A6000 GPUs, an AMD Ryzen 5995WX CPU, and 128 GB of RAM, running Ubuntu 22.04.

\subsection{Main Results} \label{sec:4.3}
\textbf{(\romannumeral1) Detection Maps:} Figs.~\ref{fig:C1-1} to~\ref{fig:C1-2} present detection maps from the ATR2-HUTD-Lake sub-dataset, offering a qualitative comparison of the evaluated methods.
The detection maps of other sub-datasets are provided in the supplementary material.
\par
\begin{figure*}[!t]                 
    \centering                    
    \includegraphics[width=2\columnwidth]{images/C1-1.jpg}                     
    \caption{Detection maps of ATR2-HUTD Lake Scene1. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) SAM. (e) IEEPST. (f) MCLT. (g) UTD-Net. (h) TUTDF. (i) TDSS-UTD. (j) NUN-UTD. (m) HUCLNet+CEM. (n) HUCLNet+SAM.}                  
    \label{fig:C1-1}    
\end{figure*}
\begin{figure*}[!t]                 
    \centering                    
    \includegraphics[width=2\columnwidth]{images/C1-2.jpg}                     
    \caption{Detection maps of ATR2-HUTD Lake Scene2. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) SAM. (e) IEEPST. (f) MCLT. (g) UTD-Net. (h) TUTDF. (i) TDSS-UTD. (j) NUN-UTD. (m) HUCLNet+CEM. (n) HUCLNet+SAM.}                    
    \label{fig:C1-2}    
\end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C1-3.jpg}                     
%     \caption{Detection maps of ATR2-HUTD River Scene1. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) SAM. (e) IEEPST. (f) MCLT. (g) UTD-Net. (h) TUTDF. (i) TDSS-UTD. (j) NUN-UTD. (m) HUCLNet+CEM. (n) HUCLNet+SAM.}                      
%     \label{fig:C1-3}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C1-4.jpg}                     
%     \caption{Detection maps of ATR2-HUTD River Scene2. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) SAM. (e) IEEPST. (f) MCLT. (g) UTD-Net. (h) TUTDF. (i) TDSS-UTD. (j) NUN-UTD. (m) HUCLNet+CEM. (n) HUCLNet+SAM.}                     
%     \label{fig:C1-4}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C1-5.jpg}                     
%     \caption{Detection maps of ATR2-HUTD Sea Scene1. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) SAM. (e) IEEPST. (f) MCLT. (g) UTD-Net. (h) TUTDF. (i) TDSS-UTD. (j) NUN-UTD. (m) HUCLNet+CEM. (n) HUCLNet+SAM.}                 
%     \label{fig:C1-5}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C1-6.jpg}                     
%     \caption{Detection maps of ATR2-HUTD Sea Scene2. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) SAM. (e) IEEPST. (f) MCLT. (g) UTD-Net. (h) TUTDF. (i) TDSS-UTD. (j) NUN-UTD. (m) HUCLNet+CEM. (n) HUCLNet+SAM.}                    
%     \label{fig:C1-2}    
% \end{figure*}
Traditional methods, such as CEM and SAM, exhibit significant limitations in underwater environments. CEM struggles with background noise suppression, resulting in false positives, while SAM fails to delineate target boundaries and often misses targets, especially in complex scenarios like the ATR2-HUTD River dataset. Its sensitivity to spectral noise and limited adaptability to spectral variations lead to incomplete detection and poor target-background separation.
\par
\begin{figure*}[!t]                 
    \centering                    
    \includegraphics[width=2\columnwidth]{images/C2-1.jpg}                     
    \caption{ROC curves comparison on ATR2-HUTD Lake Scene1. (a) 3-D ROC curve. (b) 2-D ROC curve of $(P_d, P_f)$. (c) 2-D ROC curve of $(P_f, \tau)$. (d) 2-D ROC curve of $(P_d, \tau)$.}                 
    \label{fig:C2-1}    
\end{figure*}
\begin{figure*}[!t]                 
    \centering                    
    \includegraphics[width=2\columnwidth]{images/C2-2.jpg}                     
    \caption{ROC curves comparison on ATR2-HUTD Lake Scene2. (a) 3-D ROC curve. (b) 2-D ROC curve of $(P_d, P_f)$. (c) 2-D ROC curve of $(P_f, \tau)$. (d) 2-D ROC curve of $(P_d, \tau)$.}                                  
    \label{fig:C2-2}    
\end{figure*}
Advanced land-cover detection methods, including IEEPST and MCLT, also underperform in underwater environments. IEEPST struggles to suppress background interference, particularly when water column spectral signatures overlap with target signatures in the ATR2-HUTD River sub-dataset. While MCLT leverages contrastive learning for feature enhancement, it shows reduced sensitivity to small or low-reflectance targets, hindered by the nonlinearities and spectral noise typical of underwater HSI data. These results underscore the necessity of specialized techniques for HUTD.

Among SOTA HUTD methods, UTD-Net demonstrates notable improvements by effectively unmixing target-water mixed pixels. However, it faces challenges with background interference in scenes with extensive non-target bottom areas, leading to high false positive rates. NUN-UTD improves target identification by preserving weak target spectral signals, yet remains susceptible to background interference when spectral characteristics of the background resemble those of the target, leading to false positives in spectrally overlapping environments.

Physical-based methods, such as TUTDF and TDSS-UTD, enhance background suppression using underwater imaging models and predicted depth values. However, TUTDF's performance declines in complex environments due to depth estimation inaccuracies, leading to inconsistent detection. Similarly, TDSS-UTD struggles in environments with substantial depth variation, such as the ATR2-HUTD River dataset, where depth errors degrade detection accuracy. Variations in underwater imaging mechanisms between deep and nearshore scenes further limit their effectiveness.

In contrast, HUCLNet-based methods consistently outperform the alternatives. By integrating instance-level and prototype-level contrastive learning, these methods effectively detect faint and deeply submerged targets with minimal false positives, enhancing background suppression and detection accuracy. HUCLNet+CEM and HUCLNet+SAM show resilience to spectral variability, capturing subtle target features while maintaining clear target-background separation, even under significant underwater bottom interference. These methods provide the most comprehensive target coverage and background suppression in challenging environments, such as the ATR2-HUTD River dataset, demonstrating the superior effectiveness of HUCLNet in mitigating spectral variability and improving detection accuracy.
\par 
\textbf{(\romannumeral2) ROC Curves:} Subjective analysis of detection maps may be insufficient for comprehensive evaluation. Therefore, 3-D ROC curves and their 2-D projections: ($P_d$, $P_f$), ($P_d$, $\tau$), and ($P_f$, $\tau$) were used to objectively assess detection performance on the ATR2-HUTD dataset, enabling a detailed evaluation of detection efficiency, target preservation, and background suppression. 
The ROC curves of ATR-HUTD-Lake sub-dataset are provided in Figs~\ref{fig:C2-1} to~\ref{fig:C2-2}, while those of the ATR-HUTD-River and ATR-HUTD-Sea sub-datasets are provided in the supplementary material.
\par
Figs.~\ref{fig:C2-1} (a) to~\ref{fig:C2-2} (a) show the 3-D ROC curves, highlighting the relationship between the true positive rate ($P_d$), false alarm probability ($P_f$), and detection threshold ($\tau$). HUCLNet+CEM and HUCLNet+SAM consistently outperform other methods, exhibiting higher $P_d$ and lower $P_f$ over a wide range of $\tau$, demonstrating superior adaptability.
\par
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C2-3.jpg}                     
%     \caption{ROC curves comparison on ATR2-HUTD River Scene1. (a) 3-D ROC curve. (b) 2-D ROC curve of $(P_d, P_f)$. (c) 2-D ROC curve of $(P_f, \tau)$. (d) 2-D ROC curve of $(P_d, \tau)$.}                                   
%     \label{fig:C2-3}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C2-4.jpg}                     
%     \caption{ROC curves comparison on ATR2-HUTD River Scene2. (a) 3-D ROC curve. (b) 2-D ROC curve of $(P_d, P_f)$. (c) 2-D ROC curve of $(P_f, \tau)$. (d) 2-D ROC curve of $(P_d, \tau)$.}                                 
%     \label{fig:C2-4}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C2-5.jpg}                     
%     \caption{ROC curves comparison on ATR2-HUTD Sea Scene1. (a) 3-D ROC curve. (b) 2-D ROC curve of $(P_d, P_f)$. (c) 2-D ROC curve of $(P_f, \tau)$. (d) 2-D ROC curve of $(P_d, \tau)$.}                                   
%     \label{fig:C2-5}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[width=2\columnwidth]{images/C2-6.jpg}                     
%     \caption{ROC curves comparison on ATR2-HUTD Sea Scene2. (a) 3-D ROC curve. (b) 2-D ROC curve of $(P_d, P_f)$. (c) 2-D ROC curve of $(P_f, \tau)$. (d) 2-D ROC curve of $(P_d, \tau)$.}                                
%     \label{fig:C2-2}    
% \end{figure*}
Figs.~\ref{fig:C2-1} (b) to~\ref{fig:C2-2} (b) present the 2-D ROC curves of ($P_d$, $P_f$). HUCLNet-based methods occupy the top-left region, indicating superior detection accuracy. In contrast, traditional HTD methods, such as CEM and SAM, struggle to balance $P_d$ and $P_f$, particularly for targets with varying spectral properties. Although advanced HTD and SOTA HUTD methods show moderate performance, they fail to suppress false alarms in complex river environments, compromising detection accuracy.

Figs.~\ref{fig:C2-1}(c) to~\ref{fig:C2-2}(c) depict the 2-D ROC curves of ($P_f$, $\tau$), assessing background suppression. NUN-UTD shows high $P_f$ across thresholds, indicating poor background-target discrimination. While methods like MCLT and TUTDF show some improvement, they still struggle with high false alarm rates due to spectral overlap. \textbf{UTD-Net performs well in background suppression but largely by classifying all pixels as background}, as reflected in detection maps (Figs.~\ref{fig:C1-1} to~\ref{fig:C1-2}) and AUC$_{P_{d}, \tau}$ values (Tabs.~\ref{auc_lake} to~\ref{auc_sea}). In comparison, HUCLNet+CEM and HUCLNet+SAM exhibit superior background suppression with low $P_f$ and high AUC$_{P_{d}, \tau}$ values.

Figs.~\ref{fig:C2-1}(d) to~\ref{fig:C2-2}(d) present the 2-D ROC curves of ($P_d$, $\tau$), evaluating target preservation. Traditional methods, such as SAM, show significant drops in $P_d$ as $\tau$ increases, indicating poor target preservation. Advanced HTD and SOTA HUTD methods, such as MCLT and TDSS-UTD, show some improvement but still lag behind NUN-UTD and TUTDF. However, \textbf{the improved performance of NUN-UTD and TUTDF primarily results from misclassifying all pixels as targets}, as shown by high false alarm rates in detection maps (Figs.~\ref{fig:C1-1} to~\ref{fig:C1-2}) and increased AUC$_{P_{f}, \tau}$ values. In contrast, HUCLNet+CEM and HUCLNet+SAM maintain high $P_d$ at lower $\tau$, demonstrating robust and reliable target preservation.
\par
\begin{table*}[!t] 
    \centering
    \footnotesize   
    \caption{Quantitative comparison results on the ATR2-HUTD-Lake Sub-dataset. The best and second best results are in \textbf{bold} and with \underline{underline}.} \label{auc_lake}
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{1.85mm}
    \scalebox{0.875}
    {
        \begin{tabular}{ccccccccccc}
            \hline
            \multirow{2.4}{*}{\textbf{Method}} & \multicolumn{5}{c}{\cellcolor{tablecolor7!60}\textbf{ATR2-HUTD-Lake Scene1}}       & \multicolumn{5}{c}{\cellcolor{tablecolor8}\textbf{ATR2-HUTD-Lake Scene2}}       \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
                                    & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$ \\ \hline
                                    CEM         & 0.671          & 0.250          & 0.258          & 0.678          & 1.028          & 0.489          & 0.524          & 0.520          & 0.485          & 0.994          \\
                                    SAM         & 0.670          & \underline{0.129}    & 0.151          & 0.692          & 1.170          & 0.480          & \underline{0.143}    & 0.025          & 0.362          & 0.172          \\
                                    IEEPST      & 0.424          & 0.204          & 0.075          & 0.295          & 0.369          & 0.417          & 0.187          & 0.036          & 0.266          & 0.193          \\
                                    MCLT        & 0.401          & 0.422          & 0.377          & 0.357          & 0.894          & 0.365          & 0.243          & 0.173          & 0.296          & 0.715          \\
                                    UTD-Net     & 0.846          & \textbf{0.013} & 0.019          & 0.853          & 1.510          & 0.944          & \textbf{0.041} & 0.073          & 0.976          & 1.773          \\
                                    TUTDF       & \underline{0.990}          & 0.634          & \underline{0.726}    & 1.081          & 1.145          & \underline{0.998}          & 0.461          & \underline{0.768}    & 1.306          & 1.667          \\
                                    TDSS-UTD    & 0.964          & 0.215          & 0.369          & 1.117          & 1.712          & \textbf{0.999} & 0.166          & 0.444          & 1.277          & 2.676          \\
                                    NUN-UTD     & 0.758          & 0.913          & \textbf{0.994} & 0.838          & 1.088          & 0.765          & 0.792          & \textbf{0.995} & 0.968          & 1.257          \\
            \rowcolor{tablecolor13!60}HUCLNet+CEM & 0.958    & 0.302          & 0.642          & \underline{1.298}    & \underline{2.126}    & 0.989    & 0.226          & 0.634          & \underline{1.397}    & \underline{2.805}    \\
            \rowcolor{tablecolor14!60}HUCLNet+SAM & \textbf{0.995} & 0.209          & 0.710          & \textbf{1.501} & \textbf{3.393} & \textbf{0.999} & 0.265          & 0.765          & \textbf{1.501} & \textbf{2.891} \\ \hline
        \end{tabular}}
\end{table*}
\begin{table*}[!t] 
    \centering
    \footnotesize   
    \caption{Quantitative comparison results on the ATR2-HUTD-River Sub-dataset. The best and second best results are in \textbf{bold} and with \underline{underline}.} \label{auc_river}
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{1.85mm}
    \scalebox{0.875}
    {
        \begin{tabular}{ccccccccccc}
            \hline
            \multirow{2.4}{*}{\textbf{Method}} & \multicolumn{5}{c}{\cellcolor{tablecolor9}\textbf{ATR2-HUTD-River Scene1}}       & \multicolumn{5}{c}{\cellcolor{tablecolor10}\textbf{ATR2-HUTD-River Scene2}}       \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
                                    & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$ \\ \hline
                                    CEM         & 0.746          & 0.280          & 0.300          & 0.765          & 1.070          & 0.650          & 0.544          & 0.553          & 0.659          & 1.016          \\
                                    SAM         & 0.657          & 0.214          & 0.186          & 0.629          & 0.871          & 0.656          & \underline{0.078}    & 0.066          & 0.645          & 0.854          \\
                                    IEEPST      & 0.455          & 0.203          & 0.033          & 0.286          & 0.163          & 0.594          & 0.274          & 0.236          & 0.556          & 0.861          \\
                                    MCLT        & 0.550          & 0.989          & 0.990          & 0.552          & 1.001          & 0.531          & 0.970          & \underline{0.971}    & 0.533          & 1.002          \\
                                    UTD-Net     & \underline{0.843}          & \underline{0.080}    & 0.096          & 0.860          & 1.209          & \underline{0.889}          & \textbf{0.075} & 0.088          & \underline{0.903}          & 1.176          \\
                                    TUTDF       & 0.568          & 0.822          & \underline{0.824}    & 0.570          & 1.003          & 0.659          & 0.356          & 0.363          & 0.667          & 1.022          \\
                                    TDSS-UTD    & 0.402          & 0.438          & 0.415          & 0.379          & 0.948          & 0.539 & 0.179          & 0.174          & 0.534          & 0.974          \\
                                    NUN-UTD     & 0.632          & 0.968          & \textbf{0.999} & 0.663          & 1.032          & 0.503          & 0.977          & \textbf{0.980} & 0.505          & 1.002          \\
            \rowcolor{tablecolor13!60}HUCLNet+CEM & 0.794    & 0.353          & 0.518          & \underline{0.959}    & \underline{1.468}    & 0.753    & 0.354          & 0.481          & 0.880    & \underline{1.360}    \\
            \rowcolor{tablecolor14!60}HUCLNet+SAM & \textbf{0.966} & \textbf{0.055} & 0.175          & \textbf{1.086} & \textbf{3.206} & \textbf{0.924} & 0.178          & 0.327          & \textbf{1.073} & \textbf{1.837} \\ \hline
        \end{tabular}}
\end{table*}
\begin{table*}[!t] 
    \centering
    \footnotesize   
    \caption{Quantitative comparison results on the ATR2-HUTD-Sea Sub-dataset. The best and second best results are in \textbf{bold} and with \underline{underline}.} \label{auc_sea}
    \renewcommand{\arraystretch}{1.5}
    \setlength{\tabcolsep}{1.85mm}
    \scalebox{0.875}
    {
        \begin{tabular}{ccccccccccc}
            \hline
            \multirow{2.4}{*}{\textbf{Method}} & \multicolumn{5}{c}{\cellcolor{tablecolor11}\textbf{ATR2-HUTD-Sea Scene1}}       & \multicolumn{5}{c}{\cellcolor{tablecolor12!50}\textbf{ATR2-HUTD-Sea Scene2}}       \\ \cmidrule(lr){2-6} \cmidrule(lr){7-11}
                                    & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$ \\ \hline
                                    CEM         & 0.805          & 0.309          & 0.349          & 0.845          & 1.128           & 0.845          & 0.332          & 0.351          & 0.864          & 1.057          \\
                                    SAM         & 0.866          & 0.125    & 0.188          & 0.929          & 1.503           & 0.819          & 0.099          & 0.033          & 0.753          & 0.333          \\
                                    IEEPST      & 0.850          & 0.252          & 0.363          & 0.961          & 1.441           & 0.580          & 0.326          & 0.269          & 0.523          & 0.826          \\
                                    MCLT        & 0.895          & 0.980          & \underline{0.994} & 0.909          & 1.014           & 0.317          & 0.953          & \underline{0.944}    & 0.309          & 0.991          \\
                                    UTD-Net     & 0.762          & \underline{0.050}          & 0.083          & 0.796          & 1.682           & 0.774          & \textbf{0.043} & 0.070          & 0.801          & 1.634          \\
                                    TUTDF       & 0.952          & 0.841          & 0.872          & 0.984          & 1.037           & 0.903          & 0.426          & 0.482          & 0.959          & 1.131          \\
                                    TDSS-UTD    & 0.861          & 0.310          & 0.371          & 0.923          & 1.199           & 0.984          & 0.218          & 0.425          & 1.192          & 1.948          \\
                                    NUN-UTD     & \underline{0.979}    & 0.534          & \textbf{0.999} & \textbf{1.445} & 1.872           & 0.975          & 0.959          & \textbf{0.984} & 0.999          & 1.025          \\
            \rowcolor{tablecolor13!60}HUCLNet+CEM & 0.972          & 0.133          & 0.569          & \underline{1.409}    & \underline{4.284}     & \underline{0.987}    & 0.111          & 0.401          & \underline{1.287}    & \underline{3.620}    \\
            \rowcolor{tablecolor14!60}HUCLNet+SAM & \textbf{0.985} & \textbf{0.019} & 0.325          & 1.292          & \textbf{17.501} & \textbf{0.989} & \underline{0.053}    & 0.474          & \textbf{1.420} & \textbf{8.857} \\ \hline
        \end{tabular}}
\end{table*}
\textbf{(\romannumeral3) AUC Values:} The AUC values for each sub-dataset of the ATR2-HUTD dataset are computed using five key metrics: $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})}$, $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \tau)}$, $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}$, $\text{AUC}_{SNPR}$, and $\text{AUC}_{OA}$, as detailed in Tabs.~\ref{auc_lake} to~\ref{auc_sea}. These metrics quantitatively assess detection accuracy, target preservation, background suppression, signal-to-noise ratio, and overall performance in varied underwater environments.
\par
\begin{table*}[!ht] 
    \centering
    \footnotesize   
    \caption{Quantitative results of ablation studies on the ATR2-HUTD dataset.} \label{ablation study}
    \renewcommand{\arraystretch}{2}
    \setlength{\tabcolsep}{2.5mm}
    \begin{threeparttable}
        \scalebox{0.975}
        { 
    \begin{tabular}{ccccccc}
        \hline
        \textbf{Module Name}                  & \textbf{Design}                                                      & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\mathrm{P}_{\mathrm{f}})}\textcolor{red}{\uparrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}\textcolor{green}{\downarrow }$ & $\text{AUC}_{( \mathrm{P}_{\mathrm{d}},\tau)}\textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{OA}} \textcolor{red}{\uparrow }$ & $\mathrm{AUC}_{\mathrm{SNPR}}\textcolor{red}{\uparrow }$  \\ \hline
        \rowcolor{tablecolor0!50}
        \textbf{HUCLNet}                                          & N/A & 0.943 & 0.188 & 0.502 & 1.258 & 4.446 \\
        \rowcolor{tablecolor1!50}
        \cellcolor{tablecolor1!50}                             & w/o Cluster Refinement Strategy                             & 0.823 & 0.206 & 0.388 & 1.005 & 3.141 \\
        \rowcolor{tablecolor1!50}
        \multirow{-2}{*}{\cellcolor{tablecolor1!50}\textbf{RGC module}}  & w/o Reference Spectrum based Clustering Method & 0.737 & 0.211 & 0.375 & 0.901 & 2.616 \\
        \rowcolor{tablecolor2!50} 
        \cellcolor{tablecolor2!50}                              & w/o Instance-level Contrastive Learning                     & 0.878 & 0.199 & 0.438 & 1.117 & 3.513 \\
        \rowcolor{tablecolor2!50} 
        \cellcolor{tablecolor2!50}                              & w/o Prototype-level Contrastive Learning                    & 0.728 & 0.239 & 0.359 & 0.848 & 2.359 \\
        \rowcolor{tablecolor2!50}
        \cellcolor{tablecolor2!50}                              & w/o Hyperspectral-Oriented Data Augmentation                    & 0.883 & 0.195 & 0.452 & 1.165 & 3.584 \\
        \rowcolor{tablecolor2!50} 
        \multirow{-4}{*}{\cellcolor{tablecolor2!50}\textbf{HLCL module}} & w/o HLCL module$^{1}$                                             & 0.696 & 0.252 & 0.248 & 0.692 & 0.933 \\
        \rowcolor{tablecolor3!50} 
        \textbf{SPL Paradigm}                                          & w/o SPL Paradigm                                            & 0.743 & 0.217 & 0.388 & 0.914 & 2.864 \\ \hline
        \end{tabular}}
        \begin{tablenotes}
            \scriptsize
            \item[1] This experimental design is analogous to the baseline HTD methods, as the RGC module and SPL paradigm are dependent on the HLCL module for functionality.
        \end{tablenotes}
        \end{threeparttable}
\end{table*}
The $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})}$ metric, which quantifies the trade-off between the true positive rate ($P_d$) and false alarm probability ($P_f$), is critical for evaluating detection performance. HUCLNet+SAM leads with an average score of 0.976, followed by HUCLNet+CEM at 0.909. Traditional methods, such as SAM (0.701) and MCLT (0.691), underperform significantly, while SOTA HUTD methods like TUTDF and NUN-UTD fall short of HUCLNet-based methods in detection capability.
\par
For background suppression, assessed by $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}$, HUCLNet+SAM achieves the highest performance in the ATR2-HUTD-River Scene1 and ATR2-HUTD-Sea sub-datasets, the most complex nearshore environments. It also demonstrates robust performance across other sub-datasets. In contrast, SOTA HUTD methods, including TUTDF and NUN-UTD, show elevated values, suggesting overfitting due to high false positive rates.
\par
The $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \tau)}$ metric, assessing target preservation, reveals HUCLNet-based methods performing well, though NUN-UTD leads. This may be attributed to the HLCL module in HUCLNet, which compromises target-background feature separation, impacting target preservation. Additionally, NUN-UTD's higher false positive rate boosts $P_d$ but hinders background suppression.
\par
The $\text{AUC}_{OA}$ metric, combining $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \mathrm{P}_{\mathrm{f}})}$, $\text{AUC}_{( \mathrm{P}_{\mathrm{d}}, \tau)}$, and $\text{AUC}_{( \mathrm{P}_{\mathrm{f}}, \tau)}$, further emphasizes HUCLNet's superiority. HUCLNet+SAM achieves the highest average score of 1.312, with HUCLNet+CEM following at 1.205. In contrast, traditional and SOTA HUTD methods score between 0.492 and 0.928, underscoring HUCLNet's effectiveness in background suppression, target preservation, and detection accuracy in complex nearshore environments.
\par
Finally, the $\text{AUC}_{SNPR}$ metric, which measures robustness under varying signal-to-noise ratios, underscores HUCLNet+SAM's superior performance, achieving the highest scores across all sub-datasets, including 17.501 in ATR2-HUTD-Sea Scene1. HUCLNet+CEM consistently ranks second, while traditional HTD and SOTA HUTD methods show lower scores, indicating reduced robustness in fluctuating signal conditions.
\par
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[scale=0.65]{images/C3-1.jpg}                     
%     \caption{Target-background separability boxplots for ATR2-HUTD Lake sub-dataset. (a) Scene1. (b) Scene2.}                    
%     \label{fig:C3-1}    
% \end{figure*}


% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[scale=0.65]{images/C3-2.jpg}                     
%     \caption{Target-background separability boxplots for ATR2-HUTD River sub-dataset. (a) Scene1. (b) Scene2.}                                 
%     \label{fig:C3-2}    
% \end{figure*}
% \begin{figure*}[!t]                 
%     \centering                    
%     \includegraphics[scale=0.65]{images/C3-3.jpg}                     
%     \caption{Target-background separability boxplots for ATR2-HUTD Sea sub-dataset. (a) Scene1. (b) Scene2.}                                      
%     \label{fig:C3-3}    
% \end{figure*}
% \textbf{(\romannumeral4) Separability Maps:} To assess the effectiveness of the comparison methods in distinguishing targets from the background, target-background separability is analyzed using boxplots, providing a clear visual representation of the separation. Figs.~\ref{fig:C3-1} to \ref{fig:C3-3} present these separability boxplots for all methods across the ATR2-HUTD sub-datasets.
% \par
% Traditional HTD methods, CEM and SAM, show limited separability, with SAM slightly outperforming CEM. In all sub-datasets, target boxes overlap with background boxes, despite some background suppression, indicating poor separation of targets from the background in underwater environments.
% Advanced HTD methods, IEEPST and MCLT, show minor improvement over traditional methods. 
% However, except for the ATR2-HUTD Sea sub-dataset (scene 1), target boxes still overlap with background boxes in most sub-datasets. 
% This suggests that even with advanced techniques, suppressing background noise and achieving clear target separation remains challenging in complex underwater environments.
% \par
% HUTD methods show improved separability. Specifically, UTD-Net achieves significant background suppression, though some overlap remains. 
% Additionally, UTD-Net exhibits a detection range near 0 in certain sub-datasets, indicating a high false positive rate. 
% NUN-UTD, an enhanced version of UTD-Net, improves target highlighting but still struggles with background noise suppression, leading to suboptimal performance in more complex scenes such as those in the ATR2-HUTD River and Sea sub-datasets.
% Compared to unmixing-based HUTD methods, TUTDF and TDSS-UTD demonstrate better separability, with detection ranges closer to 1, indicating more effective reduction of target-background correlation. 
% However, both methods still exhibit considerable target-background overlap and limited suppression in complex scenes, such as the ATR2-HUTD River sub-dataset.
% \par
% In contrast, the proposed HUCLNet-based methods, HUCLNet+CEM and HUCLNet+SAM, exhibit superior separability, with target boxes generally fully separated from the background. 
% These methods effectively suppress background noise, enabling reliable target detection in underwater environments. 
% The detection range for HUCLNet-based methods is close to 1, while the background range is near 0, indicating a low false positive rate. Compared to CEM and SAM, HUCLNet significantly enhances target-background separability, demonstrating its effectiveness in underwater hyperspectral target detection.
\subsection{Ablation Studies}\label{sec:4.4}
To evaluate the efficacy of each component in our method, we conducted ablation studies on the ATR2-HUTD dataset. These studies aim to confirm that the observed improvements stem not only from the increased number of parameters but also from the architectural design, which enhances the HUTD task. The HUCLNet framework is divided into three components for experimental validation. 
Corresponding results are presented in Tab.~\ref{ablation study}.
\par
\textbf{(\romannumeral1) Analysis of the RGC Module:} We validate the RGC with the following designs: 
\begin{itemize}
    \item \textbf{w/o Cluster Refinement Strategy:} This design excludes the cluster refinement strategy, relying solely on the reference spectrum-based clustering method. 
    \item \textbf{w/o Reference Spectrum-based Clustering:} This design omits the reference spectrum-based clustering approach from the RGC module.
\end{itemize}
\par
Without the cluster refinement strategy, the RGC module directly uses the original clustering results, often misclassifying pixels and negatively impacting prototype-level learning. As seen in Tab.~\ref{ablation study}, this leads to lower average metric values compared to the full HUCLNet-based methods, demonstrating the importance of refined pseudo-labels. Removing the RGC module entirely, the HLCL module uses pixel instances from the original HSI for instance-level contrastive learning, focusing on individual pixel spectra and neglecting the target-background relationships. Performance improves slightly over baseline HTD methods but remains significantly inferior to complete HUCLNet-based methods, highlighting the critical role of the RGC module in providing reliable pseudo-labels.
\par
\begin{figure*}[!t]                 
    \centering                    
    \includegraphics[width=2\columnwidth]{images/C4.jpg}                     
    \caption{Parameter analysis results on the ATR2-HUTD dataset. (a) Number of clusters in the RCG module; (b) Batch size in the HLCL module; (c) Attack method in the HLCL module. Red and yellow points indicate the maximum and minimum values, respectively.}                 
    \label{fig:C4-1}    
\end{figure*}
\textbf{(\romannumeral2) Analysis of the HLCL Module:} We evaluate the HLCL module with the following designs:
\begin{itemize}
    \item \textbf{w/o Instance-level Contrastive Learning:} This design removes instance-level contrastive learning, relying only on refined cluster labels from the RGC module.
    \item \textbf{w/o Prototype-level Contrastive Learning:} This design removes prototype-level contrastive learning, retaining only instance-level contrastive learning.
    \item \textbf{w/o Hyperspectral-Oriented Data Augmentation:} This design removes hyperspectral-specific data augmentation from the HLCL module.
    \item \textbf{w/o HLCL Module:} This design excludes the entire HLCL module.
\end{itemize}
\par
According to Tab.~\ref{ablation study}, we can draw the following conclusions.
When the HLCL module operates without instance-level contrastive learning, HUCLNet relies solely on the cluster labels, leading to performance degradation. However, prototype-level contrastive learning alone still outperforms baseline HTD methods, emphasizing the importance of target-background separability. The removal of prototype-level contrastive learning results in poorer performance compared to the instance-level design, indicating its greater impact on separability. When hyperspectral-oriented data augmentation is excluded, traditional augmentation methods lead to observable performance degradation, confirming the importance of hyperspectral-specific augmentation in enhancing feature discriminability and HUCLNet's performance. Finally, removing the HLCL module entirely reduces HUCLNet to baseline HTD methods, resulting in substantial performance loss, reinforcing the HLCL module's primary contribution to performance improvement.
\par
\textbf{(\romannumeral3) Analysis of the SPL Paradigm:} We evaluate the SPL paradigm with the following design: 
\begin{itemize}
    \item \textbf{w/o SPL Paradigm:} This design trains the model using the traditional self-supervised learning framework, which consists of a single reliable-guided clustering step followed by hybrid-level contrastive learning.
\end{itemize}
\par
Without the SPL paradigm, inaccurate clustering due to limited spectral discriminability hinders contrastive learning effectiveness, resulting in error propagation and performance degradation. Tab.~\ref{ablation study} confirms that the SPL paradigm significantly enhances HUCLNet's performance, underscoring the importance of the self-paced strategy in guiding model training and improving detection accuracy.
\par
\subsection{Parameter Analysis}\label{sec:4.5}
The key hyperparameters of the HUCLNet architecture, including the number of clusters in the RGC module, batch size, and attack method in the HLCL module, were analyzed through experiments on the ATR2-HUTD dataset. The results, primarily focusing on the $\text{AUC}_{\text{OA}}$ metric, are presented in Fig.~\ref{fig:C4-1}, as it is the most critical indicator of overall detection performance.
\par
\textbf{(\romannumeral1) Number of Clusters in the RGC Module:} The number of clusters in the RGC module plays a crucial role in clustering accuracy and overall HUCLNet performance. The number of clusters was varied between 30 and 48, with a step size of 3 (Fig.~\ref{fig:C4-1} (a)). Performance improves with an increasing number of clusters up to an optimal point, after which it deteriorates due to over-segmentation, where target pixels are fragmented into multiple clusters. This fragmentation hinders prototype-level contrastive learning, leading to inconsistent target representations. For the ATR2-HUTD Lake, River, and Sea sub-datasets, the optimal number of clusters was 36, 39, and 42, respectively. Even with suboptimal cluster numbers, HUCLNet outperforms baseline methods.

\textbf{(\romannumeral2) Batch Size in the HLCL Module:} The batch size in the HLCL module is another critical parameter affecting HUCLNet performance. Varying the batch size from 32 to 512 with a step size of 64, results (Fig.~\ref{fig:C4-1} (b)) show that larger batch sizes generally improve performance by increasing the number of negative samples, enhancing feature discriminability. This is consistent with prior work~\cite{Chen2020}, which indicates that larger batch sizes benefit contrastive learning. However, performance gains plateau at higher batch sizes, and larger sizes impose greater memory and computational demands. A batch size of 256 provides an optimal balance between performance and resource usage across all ATR2-HUTD sub-datasets.

\textbf{(\romannumeral3) Attack Method in the HLCL Module:} The choice of attack method in the HLCL module influences the generation of adversarial samples for contrastive learning. Four attack methods—FGSM~\cite{GoodfellowSS14}, PGD~\cite{MadryMSTV18}, FAB~\cite{Croce020}, and SPSA~\cite{SPSA}—were tested with a perturbation limit of $\epsilon = 0.1$. As shown in Fig.~\ref{fig:C4-1} (c), performance across attack methods is similar, suggesting that the specific choice of attack method has minimal impact, as long as the generated adversarial samples are effective. Given its computational efficiency and comparable performance, we adopt the FGSM attack method for HUCLNet.

% \subsection{Visualization of the effect of HUCLNet}\label{sec:4.5}