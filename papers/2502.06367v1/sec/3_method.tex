\section{Method}

\subsection{Template Object Coordinates}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/placeholder/toc_definition.png}
    \caption{\textbf{TOC definition.} Template Object Coordinates (TOCs), shown on the template of the FIND mesh. RGB values correspond to XYZ, normalized to 0-1 within the template space.}
    \label{fig:toc_definition}
\end{figure}

We seek to define some dense correspondence over a foot surface.
Many dense correspondence predictors, \cite{guler2018densepose, taylor2012vitruvian, zeng20203d}, especially for human body prediction, use a 2D parameterization of the surface, such as a UV mapping. We choose to use a 3D parameterization, for three reasons: (i) ease of construction; (ii) a more intuitive uncertainty representation; and (iii) consistency with the coordinate representation used in the FIND \cite{boyne2022find} model, which is key to both our synthetic data and reconstruction approaches.

We therefore define \textbf{Template Object Coordinates} (TOCs), denoted as $\toc$. Visualized in Figure \ref{fig:toc_definition}, TOC values represent points in the 3D local space of the FIND model's template mesh, normalized to the template's bounding box.

Where Normalized Object Coordinates (NOCs) simply map to points in an object's local space, TOCs map points from any deformed, articulated foot directly to the template's local space. This is essential to directly fit the FIND model to TOC predictions, and allows for our per-pixel predictive network in Section \ref{sec:predictor} to predict correspondences agnostic to pose, shape and identity.

\subsection{\ourSynth}

We extend SynFoot \cite{boyne2024found}, a large scale synthetic dataset for feet: we add articulated feet, our new TOC representation, and increased background and lighting diversity.

\begin{figure}
    \centering
    \begin{tabular}{ccc}
    \includegraphics[width=0.3\linewidth]{images/0015-A-setup/rgb.png} &
    \includegraphics[width=0.3\linewidth]{images/0015-A-setup/FIND_TOC.png} &
    \includegraphics[width=0.3\linewidth]{images/0015-A-setup/skeleton.png} \\
    (a) & (b) & (c)
    \end{tabular}
    \vspace{-5pt}
    \caption{\textbf{Models for rendering.} One of 8 foot models used for the synthetic dataset. The mesh has (a) geometry and texture, (b) a TOC mapping to the FIND template model, and (c) a skeleton used for articulation.}
    \label{fig:0015-A}
\end{figure}

\paragraph{Articulation.} We manually add a skeleton to all 8 meshes in the original dataset, in line with anatomical diagrams \cite{houglum2011brunnstrom}. We use Blender's automatic weight calculation \cite{baran2007automatic} to handle the vertex weighting.

We identify 5 methods of articulation described in foot literature \cite{houglum2011brunnstrom}: dorsiflexion/plantarflexion (pitch), lateral/medial rotation (yaw), inversion/eversion (roll), toe extension/flexion (up/down), and toe abduction/adduction (outwards/inwards). We randomly sample these poses and apply combinations of them to our meshes to provide variation in articulation in the synthetic data.

\paragraph{TOCs.} We render TOCs for all meshes, by first fitting the FIND mesh to each model as in the original FIND paper \cite{boyne2022find}, and using this to find a vertex-to-vertex mapping between each mesh and the FIND template space. This provides us with per-vertex TOC values for each mesh to be used for our synthetic data, as in Figure \ref{fig:0015-A}.

\paragraph{Rendering.} We use the BlenderSynth \cite{boyne2024found} package to render the dataset. We drastically increase the quantity and variety of HDRIs and background textures compared to SynFoot \cite{boyne2024found}, increasing the number of HDRIs from 14 to 733, and background textures from 34 to 541 using Poly Haven assets \cite{polyhaven}. We add a new shader to render TOCs. We render 100,000 images at 480 x 640 resolution. Figure \ref{fig:synth_examples} shows some examples of our synthetic dataset.
\mynote{HDRIs need defining?}

\begin{figure}
    \centering

    \begin{tabular}{*{4}{@{}Q{0.25\linewidth}}@{}}
    (a) & (b) &  (c) & (d)
    \end{tabular}

    \includegraphics[width=\linewidth]{images/placeholder/synth_examples.png}
    \caption{\textbf{\ourSynth examples.} We show (a) RGB, (b) TOC, (c) surface normals, and (d) segmentation masks. Further examples are included in the supplementary material.}
    \label{fig:synth_examples}
\end{figure}

\subsection{Training a predictor}
\label{sec:predictor}

\begin{figure}
    \begin{tabular}{*{3}{@{}Q{0.237\linewidth}}@{}}
    (a) & (b) & (c)
    \end{tabular}

    \centering
    \includegraphics[width=\linewidth]{images/placeholder/toc_itw.png}
        \caption{\textbf{TOC in-the-wild predictions.} Predictions on real images, showing (a) RGB input, (b) TOC $\toc$, (c) TOC uncertainty $\tocstd$. Further examples are included in the supplementary material.}
    \label{fig:toc_itw}
\end{figure}

\def\tunc{\tau}

We start with a model used to make coarse surface normal and uncertainty predictions, and uncertainty-guided pixel-wise refinements, as in \cite{alhashim2018high, bae2021estimating}. We modify the output prediction heads to tackle our downstream tasks.

First, we add a head to predict a binary segmentation heatmap for the foot using Binary Cross-Entropy loss. At inference time, we threshold all predictions according to this heatmap being larger than 0.5.

Next, we add heads to predict a probability distribution for the TOC values.
We model our TOC prediction as a normal distribution in XYZ,

\begin{equation}
    \toc \sim \mathcal{N}(\tocmean, \tocvar),
\end{equation}

\noindent and have one head to predict $\tocmean$, and another to predict $\log \tocvar$, independently in each axis XYZ. We combine these predictions in an uncertainty-aware loss during training of the predictor, which minimizes the negative log-likelihood of the TOC predictions,

\begin{equation}
    \loss{TOC} = \left\| \frac{\left( \tocmean - \toc_{\textrm{gt}}\right)^2}{\tocvar} \right\|_2 + \log{\tocvar}.
\end{equation}

We show examples of TOC inference on in-the-wild images in Figure \ref{fig:toc_itw}.


\subsection{\ourSfM}
\label{sec:point_cloud_fusion}

\begin{figure}
    \includegraphics[width=\linewidth]{images/placeholder/FOCUS-SfM.png}
    \caption{\textbf{\ourSfM overview.} (a) We find correspondences between images by matching TOC values; (b) we triangulate and collect normals across views to construct an oriented point cloud; (c) we use Poisson surface reconstruction to form a final mesh.}
    \label{fig:focussfm}
\end{figure}

Given these TOC predictions, we now wish to reconstruct a surface. The first of our two proposed methods for reconstruction, \ourSfM, is outlined in Figure \ref{fig:focussfm} and takes inspiration from traditional Structure-from-Motion.

We predict TOCs, segmentation masks and surface normals on $N$ images of a captured foot with known camera extrinsics and intrinsics. \ourSfM aims to identify correspondences between these predictions, and use triangulation to reconstruct an oriented point cloud.

\paragraph{Sampling. } For each image, we sample $P$ points within the mask of prediction. For each point, we collect the value of the pixel position, TOC, and surface normal.

\paragraph{Pixel-level correspondences. } We now want to find correspondences between images, where a `correspondence' refers to an exact TOC value. Each correspondence might only appear in a subset of images. We employ an efficient approach using nearest neighbor sampling. For each image, we structure all TOC values as a KD-tree \cite{bentley1975multidimensional} and, for each set of $P$ correspondences, look up the nearest neighbor in each image by $\ell2$ distance.

\paragraph{Subpixel correspondences. } We achieve more accurate correspondences by matching at the sub-pixel level. We use bilinear interpolation to upscale the 3x3 patch around each found correspondence by a factor of 8, and then search within this window for a better match. This search is implemented in Cython \cite{behnel2010cython} for performance.

Once we have the match, we capture the surface normal at each point. We only consider a correspondence found if the TOC value is within 0.002 $\ell 2$ distance of the original sampled value.

The result of this is $C \leq NP$ correspondences, each containing a subset of the $N$ views, and pixel positions for each view.

\paragraph{Triangulation. } For each correspondence, we triangulate across all views in which that correspondence is present, using the known camera parameters, and the Direct Linear Transformation \cite{hartley2003multiple}. This provides a point cloud of triangulated correspondences.

\paragraph{Filtering. } We filter the collected point cloud via three methods: (i) removing points whose average reprojection error is above a threshold; (ii) removing all points below the floor (Z=0); (iii) statistical outlier removal \cite{kriegel2009loop}.

\paragraph{Normal aggregation. } We also collect normals along the correspondences. To provide a normal estimate, we convert all normals to spherical coordinates $(1, \theta, \phi)$, and average over $\theta$ and $\phi$ to reach a consensus normal.

\paragraph{Poisson surface reconstruction. } Now that we have an oriented point cloud, we use Screened Poisson Reconstruction \cite{kazhdan2013screened} in Meshlab \cite{meshlab} to reconstruct a surface.

\paragraph{Implementation details.} The hyperparameters chosen in this process, such as for filtering and Poisson reconstruction, can be found in the supplementary material.

\subsection{\ourOptim}

The second approach we introduce, \ourOptim, takes inspiration from FOUND \cite{boyne2024found} - fitting a parameterized model directly to predictions made in image space.
We seek to optimize the global transformation ($r,s,t$) and FIND shape and pose embeddings ($z_s, z_p$).

Rather than use differentiable rendering, as in FOUND, the TOC representation allows us to sample points on the image, and optimize the FIND model such that the corresponding point on the FIND model projects onto the same pixel position. This is essentially a keypoint loss, where any number of keypoints can be sampled at arbitrary locations in image space.

\def\J{\mathbf{J}}
\def\predpixel{\bm{\hat\pixel}}

\paragraph{Sampling.} As in Section \ref{sec:point_cloud_fusion}, we sample $P$ $\toc$ values for each of the $N$ images within the mask of prediction, as well as recording their pixel position $\pixel$.

The TOC values $\toc$ are in normalized space - we convert them to FIND space, $\toc'$, by mapping to the axis aligned bounding box of the FIND template mesh.

\paragraph{FIND model.} As defined in \cite{boyne2022find}, the FIND model maps a 3D point on the surface of a template mesh $\mathbf{x_1}$ to a deformed mesh, under shape and pose embeddings ($z_s, z_p$), and global transformation  ($r,s,t$), to a deformed point $\mathbf{x_2}$,

\begin{equation}
    \mathbf{x_2} = F(\mathbf{x_1}, z_s, z_p, r, s, t)
\end{equation}

\paragraph{Projection.} Under our FIND and camera models, we want to project our $\toc'$ estimates onto the image plane. Given a function that maps a world point to pixel space under a given camera model, $f$, and the FIND model $F$, a reprojected point in 2D space can be calculated,

\begin{equation}
    \predpixel = f\left(F\left(\toc', z_s, z_p, r, s, t\right)\right).
\end{equation}

For our experiments, we use the default camera model of COLMAP \cite{schonberger2016structure} - a simple pinhole camera.

\paragraph{Uncertainty. } As well as predicting per-pixel TOC values, our predictor also provides per-pixel TOC uncertainties, $\bm{\sigma}_{\toc}$. We can use this to weight our reprojection prediction. To do this, we need to propagate the uncertainty to calculate an uncertainty in pixel position, $\bm{\sigma}_{\predpixel}$. We use auto-gradient computation in PyTorch \cite{paszke2017automatic} to calculate the Jacobian $\J$ of the transformation from $\toc$ to $\predpixel$.
Next, we transform the uncertainty using the first order approximation given by \citet{ochoa2006covariance},

\begin{equation}
    \bm{\Sigma_{\predpixel}} \approx \J \bm{\Sigma_{\toc}} \J^T, \quad \textrm{where} \, \bm{\Sigma_x} = \operatorname{diag}(\bm{\sigma}_x^2).
\end{equation}

\paragraph{Training loss.} We now have a predicted pixel position $\predpixel$, and associated uncertainty $\bm{\sigma}_{\predpixel}$. We construct a loss which minimizes the average projected pixel error, weighting samples according to their uncertainty,

\mynote{Check if std should be var}
\begin{equation}
    \loss{\textrm{\ourOptim}} = \frac{1}{NP} \sum^{NP}_{i=1}{\ltwo{\frac{\predpixel_i - \pixel_i}{\bm{\sigma}_{\predpixel_i}}}}.
\end{equation}

We minimize this loss by optimizing the parameters ${r, s, t, z_p, z_s}$. Similarly to FOUND \cite{boyne2024found}, we use a two-stage optimization process - first optimizing just registration parameters ${r,s,t}$, and then all parameters. Each stage runs for 500 epochs, using an Adam optimizer \cite{kingma2014adam} with a learning rate of 0.001.