\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/splash_print.pdf}
    \caption{\textbf{Method overview.} (a) We use Blender \cite{blender} to render articulated high resolution meshes, with dense correspondences (TOC) to the generative FIND \cite{boyne2022find} model. (b) We train a model to predict TOCs and surface normals on real images. (c) We combine these predictions together in a multi-view setting via two methods to yield accurate surface reconstructions: (i) \ourSfM, a Structure-from-Motion based approach; and (ii) \ourOptim, a model fitting, optimization-based approach.}
    \label{fig:splash}
\end{figure}

Accurate 3D reconstruction of human body parts from images is a challenging computer vision task, of significant interest to the health, fashion and fitness industry. In this paper, we address the problem of reconstructing a human foot accurately from multiple views. Shoe retail, orthotics and health monitoring can all be improved with accurate models of the foot, and the growing digital markets for these applications has created a demand for recovering such models from mobile phone images captured by ordinary users.

Existing approaches to foot reconstruction include: (i) specialized scanning equipment \cite{ArtecLeo, Volumental}; (ii) reconstruction of unoriented point clouds from depth maps or phone-based sensors \cite{xesto, lunscher2017point}; (iii) photogrammetry pipelines, such as COLMAP, of Structure-from-Motion (SfM) followed by Multi-View Stereo (MVS) \cite{schonberger2016structure, schonberger2016pixelwise}; and (iv) fitting generative models to silhouettes, keypoints and surface normals \cite{kok2020footnet, boyne2024found}.

These methods come with substantial drawbacks: (i) expensive scanning equipment is not accessible to most consumers; (ii) phone-based sensors are limited in availability and ease of use, and noisy, unoriented point clouds are difficult to use for desired applications such as rendering and taking measurements; (iii) SfM and MVS require a large number of input views and favourable lighting conditions; and (iv) current optimization-based approaches use silhouettes, keypoints and surface normals, which only provide a limited representation of the full set of available cues in the image that can be used for accurate surface reconstruction.

To this end, we introduce FOCUS, \textbf{F}oot \textbf{O}ptimization via \textbf{C}orrespondences \textbf{U}sing \textbf{S}ynthetics - which improves on current multi-view reconstruction pipelines by predicting per-pixel correspondences relative to the FIND \cite{boyne2022find} parameterized foot model. We find this representation provides a much stronger geometric grounding than solely silhouettes \cite{kok2020footnet} and surface normals \cite{boyne2024found} seen in prior foot reconstruction work, allowing for a more direct training signal from which to reconstruct a foot model. We outline our method in Figure \ref{fig:splash}, and our key contributions are as follows:

\begin{itemize}
    \item In order to learn per-pixel correspondences, we extend prior research into foot synthetic data to release \ourSynth, a \textbf{large scale synthetic dataset} of 100,000 photorealistic foot images, coupled with accurate surface normals and dense correspondence labels. We drastically increase the background and lighting variety compared to \textit{SynFoot}, and introduce articulation into the scans to provide a greater pose variation.

    \item We introduce \textbf{Template Object Coordinates} (TOCs), normalized coordinates in the space of the FIND parameterized foot model, as a new representation of foot geometry. We train a predictor to jointly predict these per-pixel correspondences, in addition to a corresponding uncertainty. We follow a similar approach to prior work, to successfully train the network solely on synthetic data, and show plausible predictions on in-the-wild images.

    \item We introduce \ourSfM, a method for fusing multiple views of correspondence predictions into a single oriented point cloud, by matching correspondences and triangulating, similar to Structure-from-Motion. We use Poisson reconstruction \cite{kazhdan2013screened} to convert the oriented point cloud to a final mesh. This method does not require a GPU, requires fewer views than COLMAP, and is substantially faster than previous work.

    \item We also introduce \ourOptim, a method for directly optimizing the parameterized FIND \cite{boyne2022find} model to these dense correspondence images, producing a watertight, parameterized mesh as output. This method directly uses the predicted correspondence uncertainty, and is capable of accurate reconstruction on as few as 3 views.

\end{itemize}
