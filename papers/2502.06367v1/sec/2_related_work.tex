\section{Related Work}

\paragraph{Synthetic dataset generation.} Synthetic rendering has become a growing source for data generation in the computer vision community. As photorealistic rendering capabilities have improved, synthetic pipelines have been used to produce high quality, large scale datasets, that are less expensive to scale than manually labelled datasets, and can often be more accurate. These pipelines are also useful for tasks that are difficult or impossible for human labellers, such as per-pixel labelling. Existing research has shown the viability of mostly, or entirely, synthetic data in training for complex downstream tasks - examples for human body reconstruction include bodies \cite{varol2017learning}, faces \cite{wood2021fake, bae2023digiface}, eyes \cite{wood2015rendering}, and feet \cite{boyne2024found}.

\paragraph{Multi-view reconstruction.} In order to recover geometry from multiple images, the relative camera positions and camera internal parameters must be known. These can be obtained directly from measurements of the image capturing device using onboard Inertial Measurement Units (IMUs), or can be recovered via sparse 3D reconstruction from Structure From Motion (SfM) \cite{schonberger2016structure}.

To recover the surface geometry, a common method is Multi-View Stereo (MVS) \cite{schonberger2016pixelwise}, a process of optimizing depth and normal maps across views, from which an oriented point cloud can be recovered. From this, a mesh is constructed using a surface reconstruction algorithm \cite{kazhdan2013screened}. COLMAP \cite{schonberger2016structure, schonberger2016pixelwise} is a popular implementation of this full pipeline.

In recent years, neural rendering for surface reconstruction has grown in interest \cite{mildenhall2021nerf, yariv2020multiview}. In these methods, a neural representation of a 3D scene is trained, which, when rendered through some differentiable process, accurately reconstructs reference views. These methods often require large amounts of training time and compute, and large numbers of input views for accurate reconstructions.


\paragraph{Dense correspondences.} 

For pose and shape reconstruction, sparse correspondences, or keypoints, are often used for registration. Obtaining accurate keypoint labels is fast and easy to explain to human labellers.

To fully reconstruct an accurate surface, dense correspondences can be incredibly useful - a mapping from every pixel in one image to a pixel in another.
Some implementations treat this as a generic, image pair matching problem - often called \textit{optical flow}. Finding this flow field for image pairs has been shown to be solvable via both optimization \cite{liu2010sift, taniai2016joint} using image features, or by training a model to directly predict this flow \cite{sun2018pwc, rocco2017convolutional}.

Other implementations seek to match all pixels of an object in an image to that object's local space, normalized to a unit cube. This approach, introduced by \citet{wang2019normalized} as Normalized Object Coordinates (NOCs), has been used to reconstruct object size and pose from single images, and by \citet{gumeli2023objectmatch} for multi-view joint object pose and camera registration.

For matching an object of a known category, it can be more advantageous to be able to map any number of images to some canonical object space - this allows for more effective fusion of multiple views, and for matching across a wider range of viewpoints.
Obtaining the data necessary to learn this canonical mapping can be difficult. For human body estimation, \citet{guler2018densepose} inferred dense correspondences from sparse human labelling and part matching. \citet{taylor2012vitruvian} directly learn dense correspondences from depth images via regression forests.

\citet{zeng20203d} learn to predict a mapping between pixels of an image of a human, and a UV map of a human body mesh, and task a regressor to directly recover a 3D surface from this prediction on a single image.

We also learn a mapping directly to a parameterized mesh. We do this entirely synthetically, and are able to leverage these correspondences in a multi-view setting to obtain an accurate reconstruction.

\paragraph{Human foot reconstruction.} Obtaining accurate models of the human foot is of significant interest to the footwear and orthotics industries. The prevalence of mobile phones and digital shopping and health has increased the demand for these reconstruction methods to be made possible with data collected from consumer devices.

Some methods collect point cloud data \cite{lunscher2017point, xesto}, which can be achieved with LiDAR or structured light sensors available on certain mobile phones. Such sensors are not universally available, and often these point clouds are noisy and not capable of providing a detailed, realistic foot surface.

Other methods instead seek to fit parameterized models of feet to input data. Earlier works built Principal Component Analysis (PCA) models of feet from sampling vertices from photogrammetry \cite{kok2020footnet} and scanners \cite{amstutz2008pca}. \citet{kok2020footnet} also fit their PCA model to predicted image silhouettes.

The PCA approach has been improved in recent years by \citet{osman2022supr}, with SUPR, a PCA model of the human foot to be combined with the SMPL \cite{SMPL:2015} full body model for the task of expressive, full body reconstruction. 

Another approach is the FIND model \cite{boyne2022find}, a non-linear generative model of the foot. Rather than PCA, FIND uses an implicit network to deform a template mesh per-vertex to a target pose and shape. With this model, Foot3D was released - a collection of high resolution foot scans.

Recently, FOUND \cite{boyne2024found} introduced a method of optimizing the parameters of the FIND model to match with predicted surface normals in 2D in a multi-view setting. Due to a lack of available real surface normal data, the surface normal predictor was trained in a synthetic setting, generalizing well to in-the-wild images.