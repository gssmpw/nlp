\section{Experiments}
\label{sec:exp}
%In this section, we detail the data preparation for the training and evaluation datasets (\Cref{sec:aig_collect}), the implementation details of the training and evaluation processes (\Cref{sec:imple_detail}), and the experimental results of our proposed MGVGA.

\subsection{Data Preparation}
\label{sec:aig_collect}

\minisection{AIG Collection For MGM.}
We obtain 27 circuit designs from five circuit benchmarks as our training dataset: MIT LL Labs CEP~\citep{brendon2019cep}, ITC'99~\citep{ITC99}, IWLS'05~\citep{albrecht2005iwls}, EPFL~\citep{EPFLBenchmarks2015}, and OpenCore~\citep{takeda2008opencore}. 
The resulting AIG dataset comprises 810000 AIGs and 40500 synthesis labels across various optimization sequences and circuit designs.
We provide more details of the AIG collection in \Cref{sec:aigs_collect}.

\minisection{Verilog-AIG Pairs Collection For VGA.}
In this phase, source Verilog codes~\citep{thakur2023benchmarking, liu2023rtlcoder} are selected and subjected to logic synthesis using Yosys~\citep{wolf2016yosys}, and then they are converted into AIG format.
This process yields 64826 Verilog-AIG pairs, which are utilized for VGA illustrated in \Cref{sec:vga}.

\minisection{AIG Preprocessing.} 
As mentioned previously, we convert circuits to AIGs to implement our MGVGA. 
The node type of AIG can be categorized into PI, PO, AND, and NOT gates.
Given that the number of PIs and POs is typically minimal, the primary emphasis in masked modeling lies in the accurate reconstruction of AND and NOT gates. 
It is worth noting that the NOT gate is the only single-input logic gate.
Our concern is that the model could potentially leverage the disparity in in-degrees of gates as a shortcut, thereby simplifying the reconstruction task without learning the useful circuit representations. 
Consequently, we introduce single-input AND gates during the training phase as illustrated in \Cref{fig:logic_eq,fig:mm_logic_eq,fig:crl}, which have two identical inputs. 
The input and output of the single-input AND gate are identical, making this augmentation simply adaptable to circuits featuring various logic gates (NAND, XOR, OR, etc.).
Our experiments indicate that GNNs struggle to precisely capture degree information during the reconstruction process. 
Consequently, we employ this augmentation method as a trick in our training process, treating it as an equivalent augmentation for AIGs to avoid overfitting and possible leakage. 
Notably, this augmentation is \textbf{not} utilized during the evaluation phase as shown in \Cref{fig:downstream}.

\minisection{Evaluation Dataset Collection.}
As for the evaluation dataset, we select 10 circuit designs external to the training dataset from opensource benchmark~\citep{chowdhury2021openabcd,EPFLBenchmarks2015,openrisc2009or1200,yosys2019picorv32,asanovic2016rocket}, the details of which are illustrated in \Cref{table:rank_QoR}.
Additionally, we will provide more details on benchmark selection in \Cref{sec:benchmark}.

\subsection{Implementation Details}
\label{sec:imple_detail}

\minisection{Training Process of MGVGA.}
For the circuit representation learning utilizing our MGVGA paradigm, we utilize DeepGCN~\citep{li2019deepgcn,li2020deepergcn} as the GNN encoder and decoder.
As for the LLM, we utilize gte-Qwen2-7B-instruct~\citep{li2023gte}, trained with bidirectional attention mechanisms based on Qwen2-7B~\citep{yang2024qwen2}, which has a comprehensive understanding of abstract circuit function described in Verilog codes~\citep{liu2023rtlcoder, pei2024betterv, tsai2024rtlfixer, fang2024assertllm}.
The training process employs a linear learning rate schedule with the Adam optimizer set at a learning rate of $1 \times 10^{-3}$, a weight decay of 0.01, and a batch size of 512.
The model is fine-tuned for 3 epochs on 8$\times$A100 GPUs with 80G memory each.
Additionally, we provide more details about the model settings in \Cref{sec:model_set}.

\minisection{Baseline Selection.}
As for the baseline selection, we select DeepGate2~\citep{shi2023deepgate2} as a baseline due to its similar scope and SOTA performance in general circuit representation learning.
Additionally, we provide more details of baseline selection in \Cref{sec:baseline}.

\minisection{Evaluation.}
To validate the efficacy of our MGVGA, we conduct evaluations across two distinct logic synthesis tasks including the Quality of Results (QoR) Prediction and Logic Equivalence Identification tasks.
During the evaluation process, we utilize the GNN encoder trained with MGVGA to extract the AIG representation \textbf{without} extracting Verilog code representations.
Moreover, we extract the AIG representation directly without fine-tuning DeepGate2 and MGVGA for downstream tasks.
Notably, QoR prediction aims to assess the ability to extract structural information, whereas logic equivalence identification is designed to evaluate the capability of extracting abstract function information.
Moreover, besides identifying logic equivalence directly, we conduct experiments on the boolean satisfiability solving (SAT) task in \Cref{sec:sat}.
SAT solving requires rough logic equivalence checking to be strictly validated later.
Additionally, we provide more details about the model settings in \Cref{sec:model_set}.

\subsection{QoR Prediction}
For QoR prediction tasks, we estimate the number of optimized gates for the circuit designs following logic synthesis optimization via ABC~\citep{brayton2010abc}.
As illustrated in \Cref{fig:downstream}(a), we utilize a GNN encoder to extract circuit embeddings exclusively from AIGs.
These embeddings are extracted to train the QoR prediction model with the datasets described in \Cref{sec:aig_collect}. 
We evaluate the performance across ten circuit designs as illustrated in \Cref{table:rank_QoR}, with each circuit undergoing synthesis through 1500 optimization sequences, each containing 20 steps.
Notably, we detail the evaluation process and evaluation metrics of QoR prediction task in \Cref{sec:qor_metric}.
Specifically, we utilize two evaluation metrics including $\text{NDCG}@k$ for $k=3, 5$ and Top-$k$\% Commonality for $k=3, 5, 10$ in our experiments.
%Notably, normalization is applied to $\Vec{A} \in \mathbb{R}^{1\times1500}$, representing the count of optimized gates per sequence, following~\citep{chowdhury2021openabcd}.
%Specifically, each element $\Vec{A}_{i}$ is standardized using $\Vec{A}_{i} = \frac{\bar{\Vec{A}}-\Vec{A}_{i}}{\sigma_{\Vec{A}}}$, where $\bar{\Vec{A}}$ and $\sigma_{\Vec{A}}$ is the mean and the standard deviation of $\Vec{A}$, respectively.
%For QoR prediction, we need to rank the predicted scores $\Vec{B} \in \mathbb{R}^{1\times1500}$ to identify the best optimization sequence.
%Consequently, we utilize the Normalized Discounted Cumulative Gain (NDCG)~\citep{jarvelin2017ireval, jarvelin2002cg} metric to assess the quality of the ranking algorithms for the predicted scores $\Vec{B}$.
%The NDCG@$k$ is calculated as follows:
%\begin{equation} 
%    \text{NDCG}@k = (\sum_{i=1}^k \frac{\Vec{A}_{\text{rank}(\Vec{B}, {i})}}{\log_2(i + 1)}) / (\sum_{i=1}^{k} \frac{\Vec{A}_{\text{rank}(\Vec{A}, {i})}}{\log_2(i + 1)}),
%\end{equation}
%where $k$ represents the position considered in the ranking. 
%Here, $\text{rank}(\Vec{A}, i)$ and $\text{rank}(\Vec{B}, i)$ denote the indices in $\Vec{A}$ and $\Vec{B}$ of the $i$-th largest elements, respectively. 
%The NDCG@$k$ score ranges from -1 to 1, with a higher score indicating better ranking performance.
%A perfect ranking would achieve an NDCG@$k$ score of 1.
%Furthermore, we evaluate and compare the predictions on a reference set of optimization sequences with actual synthesis labels using the Top-$k$\% Commonality metrics, defined as $\frac{\text{num}(\tilde{\Vec{A}}_{k} \cap \tilde{\Vec{B}}_{k})}{\text{num}(\tilde{\Vec{A}}_{k})}$, where $\tilde{\Vec{A}}_{k}$ and $\tilde{\Vec{B}}_{k}$ represent the top $k$\% performing optimization sequences, actual and predicted, respectively.


%\begin{figure}[tb]
%\centering
%\subfloat[]{\includegraphics[width=0.40\linewidth]{figs/Figure4.pdf}}
%\hspace{12pt}
%\subfloat[]
%{\includegraphics[width=0.40\linewidth]{figs/Figure5.pdf}}
%\end{figure}

\begin{figure}[]
\centering
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/exp_qor.pdf}
\end{minipage}
\hspace{20pt}
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/exp_logiceq.pdf}
\end{minipage}
\caption{Application of MGVGA in QoR prediction and logic equivalence identification.}
\label{fig:downstream}
\end{figure}

\Cref{table:rank_QoR} illustrates a detailed comparison between MGVGA and DeepGate2 in QoR prediction, using the post-synthesis number of gates.
MGVGA excels in NDCG@$k$ for $k=3, 5$ and achieves higher percentages in Top-$k$\% Commonality for $k=3, 5, 10$ across various designs.
When considering the average performance, MGVGA notably surpasses DeepGate2 with an NDCG@3 of 0.540 compared to 0.334, and a Top-10\% Commonality score of 0.301 against 0.226. 
This demonstrates MGVGAâ€™s superior capability in extracting structure information of circuits, which facilitates recommending optimal optimization sequences.
Collectively, these results quantitatively validate the significant advancement of MGVGA over DeepGate2 in logic circuit optimization tasks, particularly in accurately and efficiently predicting superior gate configurations to enhance overall design quality.

\begin{table*}[tb!]
\caption{Performance of DeepGate2 and MGVGA on QoR prediction.}
\label{table:rank_QoR}
\centering
\setlength\tabcolsep{7.2pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.96\linewidth}{!}{
\begin{tabular}{l|rrr|cc|ccc|cc|ccc}
\toprule
\multirow{4}{*}{Design} & \multirow{4}{*}{\# PI} & \multirow{4}{*}{\# PO} & \multirow{4}{*}{\# Gates} & \multicolumn{5}{c|}{DeepGate2} & \multicolumn{5}{c}{MGVGA (Ours)} \\
\cmidrule(lr){5-9} \cmidrule(lr){10-14}
& & & & \multicolumn{2}{c|}{NDCG@$k$ $\uparrow$} & \multicolumn{3}{c|}{Top-$k$\% Commonality $\uparrow$} & \multicolumn{2}{c|}{NDCG@$k$ $\uparrow$} & \multicolumn{3}{c}{Top-$k$\% Commonality $\uparrow$} \\
%\cmidrule(lr){5-9}\cmidrule(lr){10-14}
& & & & $k$=3 & $k$=5 & $k$=3 & $k$=5 & $k$=10 & $k$=3 & $k$=5 & $k$=3 & $k$=5 & $k$=10 \\
\midrule
bc0 & 21 & 11 & 2784 & 0.331 & 0.395 & 0.244 & 0.227 & 0.280 & 0.444 & 0.560 & 0.222 & 0.213 & 0.320 \\
apex1 & 45 & 45 & 2661 & 0.645 & 0.643 & 0.222 & 0.333 & 0.413 & 0.706 & 0.716 & 0.311 & 0.400 & 0.513 \\
div & 128 & 128 & 101698 & -0.063 & 0.029 & 0.000 & 0.027 & 0.133 & -0.060 & -0.060 & 0.000 & 0.013 & 0.093 \\
k2 & 45 & 45 & 4075 & -0.060 & 0.040 & 0.022 & 0.040 & 0.080 & 0.902 & 0.873 & 0.267 & 0.320 & 0.400 \\
i10 & 257 & 224 & 3618 & -0.133 & -0.080 & 0.000 & 0.000 & 0.027 & 0.620 & 0.607 & 0.289 & 0.307 & 0.353 \\
mainpla & 26 & 49 & 9441 & 0.674 & 0.629 & 0.267 & 0.293 & 0.360 & 0.594 & 0.598 & 0.200 & 0.187 & 0.233 \\
or1200\_cpu & 2343 & 2072 & 56570 & 0.498 & 0.485 & 0.178 & 0.267 & 0.407 & 0.617 & 0.613 & 0.222 & 0.253 & 0.367 \\
picorv32 & 1631 & 1601 & 25143 & 0.563 & 0.406 & 0.111 & 0.173 & 0.186 & 0.440 & 0.457 & 0.066 & 0.160 & 0.180 \\
Rocket & 4413 & 4187 & 96507 & 0.578 & 0.543 & 0.111 & 0.186 & 0.300 & 0.557 & 0.607 & 0.355 & 0.413 & 0.467 \\
sqrt & 128 & 64 & 40920 & 0.304 & 0.153 & 0.000 & 0.027 & 0.080 & 0.577 & 0.401 & 0.000 & 0.040 & 0.080 \\
\midrule
Average &  &  &  & 0.334 & 0.324 & 0.116 & 0.157 & 0.226 & \textbf{0.540} & \textbf{0.537} & \textbf{0.193} & \textbf{0.231} & \textbf{0.301} \\
\bottomrule
\end{tabular}}
\end{table*}


\subsection{Logic Equivalence Identification}
Each design in \Cref{table:logiceq} undergoes optimization to generate various graph expressions while ensuring functionality equivalence. 
The Cone, specifying the PIs and PO constructs, also denotes functionality equivalence. 
To evaluate the GNNs' ability to identify circuit function, we derive logically equivalent gates by isolating the Cone among the designs in \Cref{table:logiceq} within AIGs using the \texttt{cone} command within logic synthesis tool ABC~\citep{brayton2010abc}.
Our dataset consists of 10000 pairs of logic gates to test the identification of logic equivalence.
As illustrated in \Cref{fig:downstream}(b), GNNs deem pairs of logic gates as equivalent if the cosine similarity between their embeddings exceeds a predefined threshold during the evaluation process.
The predefined threshold is optimized based on the receiver operating characteristic (ROC) curve. 
In our experiments, we assess the GNNs' performance using precision, recall, F1-score, and the area under the ROC curve (AUC).

\begin{table*}[tb!]
\caption{Performance of DeepGate2 and MGVGA on logic equivalence identification.}
\centering
% \setlength\tabcolsep{3.2pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{l|cccc|cccc}
\toprule
\multirow{3}{*}{Design} & \multicolumn{4}{c|}{DeepGate2} & \multicolumn{4}{c}{MGVGA (Ours)} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& Precision & Recall & F1-Score & AUC & Precision & Recall & F1-Score & AUC \\
\midrule
bc0 & 0.199 & 0.930 & 0.327 & 0.813 & 0.274 & 0.715 & 0.396 & 0.817 \\
apex1 & 0.133 & 0.680 & 0.223 & 0.601 & 0.273 & 0.710 &	0.394 & 0.826 \\
div & 0.203 & 0.980 & 0.337 & 0.814 & 0.197 & 0.670 & 0.305 & 0.725 \\
k2 & 0.171 & 0.720 & 0.276 & 0.695 & 0.336 & 0.920 & 0.492 & 0.919 \\
i10 & 0.414 & 0.940 & 0.575 & 0.918 & 0.699 & 0.950 & 0.805 & 0.985 \\
mainpla & 0.178 & 0.790 & 0.290 & 0.732 & 0.167 & 0.900 & 0.281 & 0.746 \\
or1200\_cpu & 0.451 & 0.790 & 0.575 & 0.823 & 0.356 & 0.950 & 0.518 & 0.929 \\
picorv32 & 0.448 & 0.870 & 0.592 & 0.918 & 0.440 & 0.960 & 0.604 & 0.941 \\
Rocket & 0.346 & 0.930 & 0.504 & 0.892 & 0.388 & 1.000 & 0.559 & 0.952 \\
sqrt & 0.189 & 0.740 & 0.302 & 0.721 & 0.199 & 0.720 & 0.312 & 0.770 \\
\midrule
Average & 0.295 & 0.841 & 0.424 & 0.804 & \textbf{0.336} & \textbf{0.848} & \textbf{0.470} & \textbf{0.862} \\
\bottomrule
\end{tabular}}
\label{table:logiceq}
\end{table*}

As illustrated in \Cref{table:logiceq}, our proposed MGVGA has shown significant superiority over the established DeepGate2. 
The comprehensive analysis reveals that MGVGA outperforms DeepGate2, achieving an F1-score of 0.470 compared to 0.424, and an AUC of 0.862 versus 0.804. 
Moreover, we also provide more experiment results for the comparison between DeepGate3 and MGVGA using small designs in \Cref{sec:ext_logiceq}.
These results underscore the consistency of MGVGA and its superior ability to accurately determine functionally equivalent circuits, highlighting its effectiveness in extracting abstract circuit function features.

\subsection{Analysis of Masking Ratio}

% Table various masking ratio
\begin{table*}[tb!]
\caption{Performance of MGVGA on QoR prediction and logic equivalence identification with different masking ratios of MGM and VGA.}
\label{table:mask_ratio}
\centering
% \setlength\tabcolsep{4.8pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.72\linewidth}{!}{
\begin{tabular}{cc|cc|ccc|cccc}
\toprule
\multicolumn{2}{c|}{Masking Ratio} & \multicolumn{5}{c|}{QoR Prediction} & \multicolumn{4}{c}{Logic Equivalence Identification} \\
\cmidrule(lr){1-2} \cmidrule(lr){3-7} \cmidrule(lr){8-11}
\multirow{2}{*}{MGM} & \multirow{2}{*}{VGA} & \multicolumn{2}{c|}{NDCG@$k$ $\uparrow$} & \multicolumn{3}{c|}{Top-$k$\% Commonality $\uparrow$} & \multirow{2}{*}{Precision} & \multirow{2}{*}{Recall} & \multirow{2}{*}{F1-Score} & \multirow{2}{*}{AUC} \\
& & $k$=3 & $k$=5 & $k$=3 & $k$=5 & $k$=10 & & & & \\
\midrule
0.3 & 0.3 & 0.517 & 0.505 & 0.158 & 0.199 & 0.272 & 0.300 & 0.844 & 0.430 & 0.846 \\
0.3 & 0.5 & \textbf{0.540} & \textbf{0.537} & \textbf{0.193} & \textbf{0.231} & \textbf{0.301} & \textbf{0.336} & 0.848 & \textbf{0.470} & \textbf{0.862} \\
0.3 & 0.7 & 0.498 & 0.514 & 0.178 & 0.223 & 0.299 & 0.316 & 0.823 & 0.441 & 0.820 \\
\midrule
0.5 & 0.3 & 0.439 & 0.445 & 0.149 & 0.187 & 0.271 & 0.274 & 0.821 & 0.402 & 0.821 \\
0.5 & 0.5 & 0.438 & 0.470 & 0.169 & 0.204 & 0.288 & 0.331 & 0.829 & 0.450 & 0.836 \\
0.5 & 0.7 & 0.385 & 0.415 & 0.140 & 0.168 & 0.247 & 0.304 & 0.833 & 0.433 & 0.817 \\
\midrule
0.7 & 0.3 & 0.347 & 0.367 & 0.127 & 0.160 & 0.242 & 0.292 & \textbf{0.871} & 0.421 & 0.828 \\
0.7 & 0.5 & 0.400 & 0.366 & 0.111 & 0.175 & 0.228 & 0.313 & 0.823 & 0.433 & 0.832 \\
0.7 & 0.7 & 0.366 & 0.359 & 0.138 & 0.169 & 0.226 & 0.305 & 0.794 & 0.425 & 0.822 \\
\bottomrule
\end{tabular}}
\end{table*}

This section analyzes the impact of masking ratios for both MGM and VGA. 
\Cref{table:mask_ratio} indicates that the optimal masking ratio for MGM is 0.3, while the optimal masking ratio for VGA is 0.5.
At an MGM masking ratio of 0.3, the MGVGA method demonstrates notable performance. 
Meanwhile, there is a consistent improvement in both QoR prediction and logic equivalence identification tasks when the VGA masking ratio increases from 0.3 to 0.5.

The study reveals that higher MGM masking ratios negatively affect QoR performance, suggesting that excessive masking impedes the GNNs' ability to learn information effectively. 
Specifically, excessively high masking ratios (0.5 and 0.7) significantly reduce the performance of MGVGA in the QoR prediction task, which reflects the capability in structural circuit information extraction.
As for VGA, a relatively higher masking ratio (0.5) generally yields better performance for the logic equivalence identification task. 
An excessively high masking ratio (0.7) degrades the performance of extracting circuit structural and functional features.
These findings align with our intuitive expectations.
MGM directly enables GNNs to recover complete structural information from masked latent space. 
GNNs can not learn effective information from unmasked gates if the masking ratio is too high. 
In the VGA task, introducing Verilog codes as constraints for circuit restoration allows for a relatively high masking ratio, as Verilog codes contain rich circuit information.


\subsection{Effectiveness of Constraint Masked Modeling}

To assess the effectiveness of our MGM and VGA approaches, we conduct an ablation study on QoR prediction and logic equivalence identification tasks. 
We use the original masked modeling strategy~\citep{hou2023graphmae2} as a baseline, employing a masking ratio of 0.3 for both the original and MGM methods, based on the optimal performance observed in \Cref{table:mask_ratio}.

As shown in \Cref{table:cmm}, the original masked modeling strategy yielded poor performance in both tasks. 
This outcome aligns with our previous assertion that masking in the original circuits disrupts their logical equivalence, thereby preventing the method from learning effective circuit features.
In contrast, both our MGM and VGA approaches demonstrated significant improvements in the two tasks, underscoring their effectiveness in enhancing GNNs' capacity to extract fine-grained structural information and abstract functional data.
Furthermore, VGA not only facilitates GNNs' extraction of circuit function features but also enhances their ability to recognize circuit structure during the reconstruction process. This improvement occurs under the constraint of logically equivalent Verilog codes, highlighting the versatility of our VGA approach.

\begin{table*}[]
\caption{Ablation study on constraint masked modeling paradigm, including MGM and VGA.}
\label{table:cmm}
\centering
%\setlength\tabcolsep{4.8pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.72\linewidth}{!}{
\begin{tabular}{cc|cc|ccc|cccc}
\toprule
\multicolumn{2}{c|}{Mask Strategy} & \multicolumn{5}{c|}{QoR Prediction} & \multicolumn{4}{c}{Logic Equivalence Identification} \\
\cmidrule(lr){1-2} \cmidrule(lr){3-7} \cmidrule(lr){8-11}	
\multirow{2}{*}{MGM} & \multirow{2}{*}{VGA} & \multicolumn{2}{c|}{NDCG@$k$ $\uparrow$} & \multicolumn{3}{c|}{Top-$k$\% Commonality $\uparrow$} & \multirow{2}{*}{Precision} & \multirow{2}{*}{Recall} & \multirow{2}{*}{F1-Score} & \multirow{2}{*}{AUC} \\
& & $k$=3 & $k$=5 & $k$=3 & $k$=5 & $k$=10 & & & & \\
\midrule
\ding{56} & \ding{56} & 0.153 & 0.207 & 0.107 & 0.159 & 0.255 & 0.264 & 0.793 & 0.382 & 0.790 \\
\ding{52} & \ding{56} & 0.338 & 0.368 & 0.135 & 0.197 & 0.289 & 0.305 & \textbf{0.850} & 0.433 & 0.833 \\
\ding{52} & \ding{52} & \textbf{0.540} & \textbf{0.537} & \textbf{0.193} & \textbf{0.231} & \textbf{0.301} & \textbf{0.336} & 0.848 & \textbf{0.470} & \textbf{0.862} \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[]
\caption{Generalization on various GNNs, including GraphSAGE and graph transformer.}
\label{table:gnns}
\centering
%\setlength\tabcolsep{4.4pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.78\linewidth}{!}{
\begin{tabular}{c|cc|ccc|cccc}
\toprule
& \multicolumn{5}{c|}{QoR Prediction} & \multicolumn{4}{c}{Logic Equivalence Identification} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-10}
\multirow{1}{*}{GNNs} & \multicolumn{2}{c|}{NDCG@$k$ $\uparrow$} & \multicolumn{3}{c|}{Top-$k$\% Commonality $\uparrow$} & \multirow{2}{*}{Precision} & \multirow{2}{*}{Recall} & \multirow{2}{*}{F1-Score} & \multirow{2}{*}{AUC} \\

& $k$=3 & $k$=5 & $k$=3 & $k$=5 & $k$=10 & & & & \\
\midrule
DeepGate2 & 0.334 & 0.324 & 0.116 & 0.157 & 0.226 & 0.295 & 0.841 & 0.424 & 0.804 \\
\midrule
GraphSAGE & 0.469 & 0.479 & 0.153 & 0.224 & \textbf{0.314} & 0.329 & 0.811 & 0.455 & 0.841 \\
Graph Transformer & 0.452 & 0.470 & 0.154 & 0.212 & 0.311 & 0.324 & 0.789 & 0.450 & 0.831 \\
DeepGCN (Ours) & \textbf{0.540} & \textbf{0.537} & \textbf{0.193} & \textbf{0.231} & 0.301 & \textbf{0.336} & \textbf{0.848} & \textbf{0.470} & \textbf{0.862} \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{Generalization on Various GNNs}

To evaluate the generalization capability of our MGVGA, we perform circuit representation learning using various traditional GNNs, including GraphSAGE~\citep{hamilton2017graphsage} and graph transformer~\citep{shi2021transconv}.
Similarly, based on the optimal performance observed in \Cref{table:mask_ratio}, where MGVGA achieves the best results with the MGM masking ratio of 0.3 and the VGA masking ratio of 0.5, we apply these same masking ratios to the constrained masked modeling of other GNNs.

As shown in \Cref{table:gnns}, all GNNs trained with MGVGA exhibited significant improvements compared to the baseline DeepGate2 model. 
These results demonstrate the exceptional generalization ability of our proposed methods across different GNN architectures. 
This consistent performance enhancement across various models underscores the robustness and versatility of our MGVGA paradigm in extracting better circuit representation via circuit representation learning.
