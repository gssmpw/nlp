\section{Related Work}
\label{sec:prelim}

\minisection{AIG-Formatted Circuit Representations for Logic Synthesis.}
An AIG is a directed acyclic graph (DAG) utilized for representing circuits~\citep{brummayer2006local}, which is composed of AND gate, NOT gate, and terminal nodes that serve as primary inputs (PIs) and primary outputs (POs). 
% Considering its simplicity, AIG-formatted circuits~\citep{zhang2020grannite,zheng2024lstp,wang2022functionality,shi2023deepgate2} are widely used to perform circuit representation learning via GNNs.
Considering its simplicity, AIG-formatted circuits are widely used to perform circuit representation learning via GNNs.
For example, \cite{zhang2020grannite} employs a GNN model to predict a circuit's power consumption, and \cite{zheng2024lstp} proposes a customized GNN to predict the delay of circuits accurately.
Moreover, \cite{wang2022functionality} and \cite{shi2023deepgate2} perform self-supervised learning for extracting general AIG-formatted circuit representation.
In this study, we convert circuits to AIGs to perform general circuit representation learning via a self-supervised learning paradigm.

\minisection{Masked Graph Autoencoder.}
Masked autoencoders \citep{he2022mae, bao2021beit} are grounded in the masked modeling learning paradigm, which involves masking a portion of the input signals and predicting the obscured content.
Graph autoencoders \citep{hou2023graphmae2, li2023maskgae} employ an autoencoder architecture to encode nodes into latent representations and reconstruct the graph from these embeddings. 
Integrating the strengths of the above methods, the masked graph autoencoder has been introduced to advance representation learning for graph-structured data. 
The masked graph autoencoder randomly masks a subset of graph nodes and reconstructs them using information from the unmasked nodes and their structural connections. 
This approach compels the encoder to decipher the underlying relational patterns within the graph, thereby generating robust and informative node representations. 
In this paper, we apply the constrained masked modeling paradigm for general circuit representation learning through the masked graph autoencoder.

\minisection{LLM-based Embedding Models.}
LLMs, structured as decoder-only architectures, inherently face challenges in effectively encoding bidirectional context, which can impede their capacity to generate comprehensive and discriminative embeddings.
Consequently, researchers apply contrastive learning for representation learning of LLMs to leverage the natural language comprehension capabilities of LLMs for embedding-related tasks~\citep{muennighoff2024grl, behnamghader2024llm2vec, li2023gte,lee2024nvembed}. 
This kind of strategy can help LLMs extract better representation through bidirectional attention mechanisms without hurting their abilities.
In this study, we utilize LLMs to extract Verilog codes embedding with a comprehensive understanding of circuit function, serving as constraints of the reconstruction process of VGA.
