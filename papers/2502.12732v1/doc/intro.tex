\section{Introduction}

In recent years, there has been a surge of interest in deep learning for electronic design automation (EDA), which holds great potential for achieving faster design closure and minimizing the need for extensive human supervision~\citep{wen2022layoutransformer,chen2022pros,liang2023bufformer,chen2023reinforcement,wu2024chateda}.
Logic synthesis~\citep{hachtel2005logicsynth}, a vital step in EDA, is a process by which an abstract specification of desired circuit behavior is turned into a design implementation for logic gates.
In the field of logic synthesis, circuits can be formulated as graphs (e.g., And-Inverter graph (AIG)~\citep{mishchenko2006dag}), which are well-suited for modeling element connections and topology.
Consequently, GNNs~\citep{zhang2020grannite, zheng2024lstp, chowdhury2022bulls} have been widely used to learn the characteristics of circuits for various downstream tasks. 

%The effectiveness of GNNs in logic synthesis has predominantly been demonstrated in supervised settings, where task-specific labels provide the supervisory information~\citep{zhang2020grannite, zheng2024lstp, chowdhury2022bulls}.
The effectiveness of GNNs in logic synthesis has been demonstrated in supervised settings with task-specific labels.
However, obtaining labeled data for supervised learning is costly while unlabeled circuit data is available and abundant.
This discrepancy makes self-supervised learning suitable for circuit representation learning. 
Recent works \citep{wang2022functionality, shi2023deepgate2} explored leveraging the functional aspects of circuits, such as truth tables and functional equivalence, to derive meaningful representations via the self-supervised learning paradigm.
These methods efficiently capture the functional behaviors of circuits, which are crucial for many applications. 
%By leveraging unlabeled data, GNNs can learn from the intrinsic patterns of circuits, thus reducing reliance on labeled datasets and improving their generalization across various circuit designs.
%\cite{wang2022functionality} exploits contrastive learning for circuits self-supervised learning by distinguishing between functionally equivalent and inequivalent circuits.
%\cite{shi2023deepgate2} utilizes pairwise truth table differences between sampled logic gates as training supervision to acquire general circuit representation.
%After that, pretrained GNNs can be fine-tuned for specific downstream tasks to improve performance.

The structure of a circuit, including its layout, connectivity, gate numbers, and circuit level, plays a critical role in determining its power, performance, and area (PPA), all of which are key optimization targets of EDA. 
Models trained with functional targets often fall short in extracting structural details.
Masked modeling paradigms, which have been successfully applied in computer vision~\citep{he2022mae, bao2021beit}, natural language processing~\citep{kenton2019bert}, and graph learning~\citep{hou2023graphmae2, li2023maskgae}, offer a promising solution to learn detailed structural information. 
% Specifically, these paradigms have proven effective in capturing the structural details of images, the syntactic and semantic structures of texts, and intricate connections in graphs.
Consequently, we apply the masked modeling paradigm to circuit representation learning to extract a more fine-grained representation of circuit structure.

\begin{figure}[]
\centering
\begin{minipage}[t]{0.50\linewidth}
\centering
\includegraphics[width=0.98\linewidth]{figs/possible_AIGs.pdf}
\caption{Possible reconstruct AIGs for masked AIG. If circuit gates are masked, there are various logic-correct solutions for reconstruction.}
\label{fig:logic_eq}
\end{minipage}
\hspace{6pt}
\begin{minipage}[t]{0.47\linewidth}
\centering
\includegraphics[width=0.90\linewidth]{figs/logic_eq.pdf}
\caption{Logic equivalence between Verilog code and AIG. For a circuit design, AIG can be translated from Verilog code.}
\label{fig:mm_logic_eq}
\end{minipage}
\end{figure}

Challengingly, traditional masked graph modeling paradigm~\citep{hou2023graphmae2, li2023maskgae} can not be applied directly to circuit representation learning which follows strict logical equivalence. 
In conventional applications, such as social or molecular graphs, masking nodes can provide a unique solution for reconstruction. 
However, when gates are masked in a circuit, their reconstruction will admit various solutions as illustrated in \Cref{fig:logic_eq}. 
% Specifically, when gates are masked in a circuit, their reconstruction will admit various solutions as illustrated in \Cref{fig:logic_eq}. 
This is because no matter how we replace gates in the original circuits, logical correctness can still be maintained without necessarily preserving logical equivalence.
Consequently, applying traditional masked modeling to circuit representation learning can not guarantee the extraction of circuit-specific features. 
To address this limitation, we propose a constrained masked modeling paradigm that ensures logical equivalence between the original and reconstructed circuits, thereby enabling effective circuit representation learning.

As mentioned, abstract circuit functions are useful for EDA tasks.
However, they are not explicitly represented in the structural layouts. 
Such functional attributes are often derived from textual descriptions in hardware description languages (HDLs), which large language models (LLMs) can effectively process.
LLMs have demonstrated remarkable performance in HDL code generation~\cite{pei2024betterv, tsai2024rtlfixer, fang2024assertllm,liu2023rtlcoder}.
Consequently, LLMs can guide GNNs in understanding circuit functions. 
Specifically, the AIG is logically equivalent to the corresponding behavior Verilog code for the same circuit design as illustrated in \Cref{fig:mm_logic_eq}.
This equivalence allows for the alignment between Verilog codes and AIGs, facilitating GNNs' understanding of circuit functions.

% Conclusion
% In this study, we introduce MGVGA, a constrained masked modeling paradigm incorporating Masked Gate Modeling (MGM) and Verilog-AIG Alignment (VGA) for circuit representation learning.
% Firstly, we introduce MGM to mask gates in the latent space instead of masking in the original circuits.
% This approach allows the representations of unmasked gates to serve as constraints for preserving logical equivalence when reconstructing the attributes of masked gates, as the latent representations of unmasked gates have already aggregated some features from masked gates.
% However, GNNs trained with MGM alone focus primarily on structural relationships within circuits, potentially overlooking function features.
% Considering the proficiency of LLMs in understanding Verilog code functionality, we leverage LLMs as teachers to transfer circuit function knowledge to GNNs by aligning AIGs with their logic equivalent Verilog codes.
% We design VGA to perform a masking operation on the original circuits and reconstruct masked gates under the constraint of corresponding Verilog codes.
% Although this operation will destroy the logical equivalence of the original circuits when cooperating with the traditional masked modeling strategy, we can still utilize equivalent Verilog codes as constraints to ensure the logic equivalence during the reconstruction process.
This study presents MGVGA, a constrained masked modeling paradigm incorporating Masked Gate Modeling (MGM) and Verilog-AIG Alignment (VGA) for circuit representation learning. 
Firstly, we introduce MGM to mask gates in the latent space instead of masking in the original circuits.
This approach can use unmasked gate representations as constraints to maintain logical equivalence during masked gate reconstruction, as these representations already capture features from masked gates. 
However, GNNs trained with MGM alone focus primarily on structural relationships within circuits, potentially overlooking function features. 
To address this, we design VGA to leverage LLMs' expertise in Verilog code to transfer functional knowledge to GNNs by aligning AIGs with equivalent Verilog codes. 
Specifically, VGA performs a masking operation on the original circuits and reconstructs masked gates under the constraint of corresponding Verilog codes.
Although this operation will destroy the logical equivalence of the original circuits when cooperating with the traditional masked modeling strategy, we can still utilize equivalent Verilog codes as constraints to ensure the logic equivalence during the reconstruction process.

In summary, our main contributions are as follows:
\begin{itemize}
\item Propose masked gate modeling (MGM) for circuit representation learning, enabling GNNs to extract circuit representations with fine-grained structural information.
\item Develop the Verilog-AIG alignment (VGA), which employs LLMs as teachers to guide GNNs to extract circuit representations with abstract circuit functions through equivalent AIGs and Verilog codes alignment.
\item Conduct extensive evaluations and show superior performance on various logic synthesis tasks including quality of result (QoR) and logic equivalence identification compared to previous state-of-the-art (SOTA) methods.
\end{itemize}
