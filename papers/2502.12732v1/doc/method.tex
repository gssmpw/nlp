\section{Methodology}
Due to logical equivalence issues, traditional mask graph modeling techniques are inadequate for circuit representation learning. 
Additionally, GNNs have inherent limitations in extracting abstract circuit functions. 
To address these challenges, we propose MGVGA, a novel constrained masked modeling strategy incorporating masked gate modeling (MGM) and Verilog-AIG alignment (VGA) for enhanced circuit representation learning.
AIGs have gained widespread adoption in circuit representation learning. 
Consequently, we convert circuits to AIGs to implement our MGVGA.
In the following subsections, we will elucidate the details of MGM (\Cref{sec:mgam}) and VGA (\Cref{sec:vga}) utilizing the AIG autoencoder.
Notably, we also provide a detailed illustration of how we preserve the logic equivalence when performing MGVGA in \Cref{sec:preserve_logic_eq}.
\begin{figure*}[tb!]
    \centering
    \includegraphics[width=0.928\linewidth]{figs/overall_flow.pdf} 
    \caption{Overview of the MGVGA for circuit representation including masked gate modeling and Verilog-AIG alignment. For both MGM and VGA, the AIG reconstruction is implemented by gate type prediction and gate-level degree prediction from reconstructed representation.}
    \label{fig:crl}
\end{figure*}

\subsection{AIG Autoencoder}
Let $\mathcal{G} = (\mathcal{V}, \mathcal{A})$ represent an AIG, where $\mathcal{V}$ denotes the set of $N$ nodes, $v_i \in \mathcal{V}$, categorized into four types: PI, PO, AND, and NOT gates, each labeled by $c_{i} \in \mathcal{C}, i\in\{1,2,3,4\}$. 
The adjacency matrix $\mathcal{A} \in \{0, 1\}^{N \times N}$ shows the connectivity between nodes, where $\mathcal{A}_{i, j} = 1$ represents an existing edge from $v_i$ to $v_j$. 
$\mathcal{A}$ delineates the structure and the types of connections within $\mathcal{G}$.

\begin{wrapfigure}{r}{0.50\linewidth}
\centering
\vspace{-1.5em}
\includegraphics[width=\linewidth]{figs/constraint_block.pdf}
\caption{The constraint block for VGA.}
\label{fig:con_block}
\vspace{2.5em}
\end{wrapfigure}
For an AIG autoencoder, a GNN encoder, denoted by $g_E$, encodes $\mathcal{G}$ into a latent space representation $\Vec{X} \in \mathbb{R}^{N \times d}$, where $d$ represents the dimension of this representation. 
The encoding process of an AIG can be formulated as:
\begin{equation}
    \Vec{X} = g_{E}(\mathcal{V}, \mathcal{A}).
    \label{eq:encoder}
\end{equation}
Concurrently, a GNN decoder, $g_D$, endeavors to reconstruct the AIG $\mathcal{G}$ from $\Vec{X}$ according to:
\begin{equation}
    (\tilde{\Vec{X}}, \mathcal{A}) = \tilde{\mathcal{G}} = g_{D}(\Vec{X}, \mathcal{A}),
   	\label{eq:decoder}
\end{equation}
where $\tilde{\mathcal{G}}$ denotes the reconstructed graph, potentially encompassing both node features and structure.
The primary objective of the AIG autoencoder is to optimize the encoder $g_E$ to produce an effective representation $X$ that facilitates the accurate reconstruction of the original $\mathcal{G}$.
Notably, following previous masked modeling paradigms~\citep{hou2022graphmae, hou2023graphmae2}, we do not mask the adjacent matrices $\mathcal{A}$ during the entire process and the detailed reasons will be explained in \Cref{sec:womasking}.

\subsection{Masked Gate Modeling}
\label{sec:mgam}
The idea of the masked autoencoder has been applied successfully to graph self-supervised learning. 
As an extension of denoising autoencoders, masked graph autoencoder~\citep{hou2023graphmae2, li2023maskgae} selectively obscure portions of the graph, such as node features or edges, through a masking operation, and learn to predict the obscured content.
% It has been shown that focusing solely on reconstructing masked node features as a pretext task can yield promising outcomes in graph representation learning.
In traditional masked graph autoencoders, nodes are typically masked directly in the input graph before being processed through the autoencoder framework.
However, this approach presents significant challenges when applied to structurally constrained graphs like AIGs, which follow strict logical rules. 
Utilizing a straightforward random masking technique will lead to reconstructed logic expressions that diverge from their original forms, which can not be tolerated.

To address this, we propose masked gate modeling, where the AIG is initially processed unmasked through the encoder to capture its latent representation. 
Rather than masking nodes at the original AIG, the masking operation is applied to the encoded representation in the latent space.
During the masked modeling process, the encoder can retain the complete logical structure of the AIG before masking.
This approach allows the representations of unmasked gates to serve as constraints for reconstructing the attributes of masked gates, as the latent representations of unmasked gates have already aggregated some features from masked gates. 

Formally, as depicted in \Cref{fig:crl}, we uniformly sample a subset of gates $\mathcal{V}_\text{mgm} \subset \mathcal{V}$ without replacement and replace the remaining nodes with the mask token [MASK], which can be represented by a learnable vector $\Vec{m} \in \mathbb{R}^{d}$.
Consequently, the masked node representation $\bar{\Vec{x}}_i \in \bar{\Vec{X}}_\text{mgm}$ for each node $v_i$ is given by:
\begin{equation}
    \bar{\Vec{x}}_i = \begin{cases} 
        \Vec{x}_{i}, & \text{if } v_{i} \in \mathcal{V}_\text{mgm}; \\
        \Vec{m},     & \text{if } v_{i} \notin \mathcal{V}_\text{mgm}.
    \end{cases}
\end{equation}
The $\bar{\Vec{X}}_\text{mgm}$ is then fed into the decoder $g_{D}$ to reconstruct the $\mathcal{G}$ following \Cref{eq:decoder}. 
As illustrated in \Cref{fig:crl}, the decoder maintains the connectivity of each node and generates the reconstructed node representation $\tilde{\Vec{X}}_{\text{mgm}} \in \mathbb{R}^{N \times d}$ for $\mathcal{G}$ reconstruction (\Cref{sec:AIG_rec}).

\subsection{Verilog-AIG Alignment}
\label{sec:vga}

Although GNNs enhanced with MGM excel at extracting structural information from circuits, they often struggle to capture abstract circuit functions that are not explicitly represented in structural layouts. 
Meanwhile, Verilog codes contain substantial semantic information, including high-level abstract concepts and functional logic in circuit designs.
Recent studies~\citep{lu2024rtllm, pei2024betterv} have begun leveraging LLMs to analyze Verilog codes, highlighting the potential for distilling circuit function knowledge from LLMs to GNNs. 
LLMs can serve as excellent teachers, guiding GNNs in understanding circuit functions through the alignment process of equivalent Verilog codes and AIGs.
Meanwhile, as illustrated in \Cref{fig:mm_logic_eq}, AIGs are translated from Verilog codes via logic synthesis tools. 
Consequently, there exist equivalent behavior-level Verilog codes for AIGs.
Based on equivalent Verilog-AIG pairs, we can perform Verilog-AIG alignment, which conducts masking operations on original AIGs.
Subsequently, the masked gates are reconstructed under the constraints of equivalent Verilog codes. 
This equivalence allows for the reconstruction of masked AIGs under the supervision of Verilog code, facilitating GNNs' understanding of circuit functions.

Similar to MGM, we uniformly sample a subset of gates $\mathcal{V}_\text{vga} \subset \mathcal{V}$ without replacement and replace the node types of remaining nodes with $c_m$, which represents these nodes are masked in the original AIG.
Consequently, for $v_i \in \bar{\mathcal{V}}$ of the masked AIG, the node type $c_i$ can be defined as:
\begin{equation}
c_i = \begin{cases} 
    c_i, & \text{if } v_{i} \in \mathcal{V}_\text{vga}; \\
    c_m, & \text{if } v_{i} \notin \mathcal{V}_\text{vga}.
      \end{cases}
\end{equation}
The masked AIG $\bar{\mathcal{G}} = (\bar{\mathcal{V}}, \mathcal{A})$ is fed into the encoder $g_E$ to generate the encoded masked AIG representation $\bar{\Vec{X}}_\text{vga}$ following \Cref{eq:encoder}. 

As mentioned earlier, the reconstruction of $\mathcal{G}$ from $\bar{\Vec{X}}_\text{vga}$ must be constrained by equivalent Verilog code to ensure strict logical equivalence. 
Consequently, we design a constraint block inspired by \cite{jaegle2021perceiver} and \cite{lee2024nvembed} as illustrated in \Cref{fig:con_block}.
Specifically, we perform adaptive pooling on token embeddings of Verilog code generated by LLMs to extract $\Vec{X}_V \in \mathbb{R}^{M \times d_{v}}$, the representation of the equivalent Verilog code. 
We then feed the masked AIG representation $\bar{\Vec{X}}_\text{vga}$ and Verilog code representation $\Vec{X}_V$ into a cross-attention block $\mathcal{C}$ to perform alignment between the masked AIG and Verilog code, with $\bar{\Vec{X}}_\text{vga}$ being projected to query $Q \in \mathbb{R}^{N \times d}$ and $\Vec{X}_V$ being projected to key $K \in \mathbb{R}^{M \times d}$ and value $V \in \mathbb{R}^{M \times d}$. 
Specifically, we selected $M = 16$ after carefully balancing computational cost and performance.
The output of the cross attention block~\citep{vaswani2017attention} is the constrained AIG representation $\bar{\Vec{X}}^{\prime}_\text{vga} \in \mathbb{R}^{N\times d}$, which can be calculated as follows:
\begin{equation}
\bar{\Vec{X}}^{\prime}_\text{vga} = \mathcal{C}(\bar{\Vec{X}}_\text{vga}, \Vec{X}_V) = \text{Softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) V.
\end{equation}
After aligning the logically equivalent Verilog code and masked AIG, $\bar{\Vec{X}}^{\prime}_\text{vga}$ incorporates information from the abstract circuit function extracted by LLMs. 	
Then, as illustrated in \Cref{fig:crl}, we obtain the reconstructed circuit representation $\tilde{\Vec{X}}_\text{vga}$ from $\bar{\Vec{X}}^{\prime}_\text{vga}$ via $g_D$ following \Cref{eq:decoder} while preserving logical equivalence. 
Subsequently, $\tilde{\Vec{X}}_\text{vga}$ will be utilized for $\mathcal{G}$ reconstruction, detailed in \Cref{sec:AIG_rec}.

\subsection{AIG Reconstruction}
\label{sec:AIG_rec}

As illustrated in \Cref{fig:crl}, given the reconstructed node representations from MGM or VGA, we can predict the attributes of masked nodes.
First, we predict the types of each masked node, categorizing them as AND gate, NOT gate, PI, or POs.
Next, we focus on the specific attributes of the nodes themselves. 
Specifically, AND gates have two inputs (which may be identical), while NOT gates have only one input. 
Moreover, PIs have no inputs, and POs have no outputs. 
Consequently, we predict the degree of each masked node to help GNNs learn the attributes of each gate more effectively. 
Moreover, gate-level degree prediction aids GNNs in capturing the connectivity between gates and the overall structures of AIGs, as illustrated in \Cref{fig:crl}. 
For clarity, we unify the reconstructed node representations from MGM ($\tilde{\Vec{X}}_{\text{mgm}} \in \mathbb{R}^{N \times d}$) and VGA ($\tilde{\Vec{X}}_{\text{vga}} \in \mathbb{R}^{N \times d}$) into a single notation $\tilde{\Vec{X}}$.

\minisection{Gate Type Prediction.}
For gate type prediction, $\tilde{\Vec{X}}$ is transformed by a mapping function $f_{\text{type}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{C}$ into a categorical probability distribution over $C$ classes.
This leads to the formation of the overall probability distribution matrix $\tilde{\Vec{Z}} \in \mathbb{R}^{N \times C}$, where each element $\tilde{\Vec{Z}}_{i,j}$ represents the softmax-estimated probability that node $v_i$ belongs to class $c_j$.
Importantly, the gate type reconstruction loss is calculated only for the $N_m$ masked nodes:
\begin{equation}
\mathcal{L}_{\text{type}} = -\frac{1}{N_m} \sum_{\substack{i=1 \\ v_i \notin \mathcal{V}_u}}^{N} \sum_{j=1}^{C} \Vec{Y}_{i,j} \log \tilde{\Vec{Z}}_{i,j},
\end{equation}
where $\Vec{Y} \subset \{0, 1\}^{N \times C}$ and $\Vec{Y}_{ij}$ is a binary indicator that equals 1 if the node $v_{i}$ belongs to class $c_j$ and 0 otherwise.

\minisection{Gate-Level Degree Prediction.} 
The gate-level degree prediction involves forecasting the in-degree and out-degree of each masked gate within the AIG. 
Formally, given the reconstructed node representations $\tilde{\Vec{X}}$, in-degree labels $\Vec{D}^{-} \in \mathbb{R}^{N}$, and out-degree labels $\Vec{D}^{+} \in \mathbb{R}^{N}$, we utilize mean squared error as the loss function for degree regression tasks. The degree reconstruction loss, calculated only for the $N_m$ masked nodes, is defined as:
\begin{equation}
    \mathcal{L}_{\text{degree}} = \frac{1}{N_m} \sum_{\substack{i=1 \\ v_i \notin \mathcal{V}_u}}^{N} \left( (\Vec{D}_{i}^{-} - f_{\text{in}}(\tilde{\Vec{X}}_{i}))^2 + (\Vec{D}_{i}^{+} - f_{\text{out}}(\tilde{\Vec{X}}_{i}))^2 \right),
\end{equation}
where $f_{\text{in}}: \mathbb{R}^{d} \rightarrow \mathbb{R}$ and $f_{\text{out}}: \mathbb{R}^{d} \rightarrow \mathbb{R}$ serve as mapping functions for predicting gate-level degrees. 
This task allows GNNs to infer the connectivity between nodes, providing insights into the interaction patterns for understanding the attributes of each gate and the organization of the circuits.
As illustrated in \Cref{fig:crl}, the AIG reconstruction is implemented by gate type prediction and gate-level degree prediction from reconstructed representation for both MGM and VGA. 
Consequently, the AIG reconstruction loss for MGM and VGA can be defined as:
\begin{equation}
	\mathcal{L}_{\text{mgm}/\text{vga}} = \mathcal{L}_{\text{type}} + \mathcal{L}_{\text{degree}}.
\end{equation}

\subsection{Constrained Masked Modeling: MGVGA}

Building upon the methodologies of MGM and VGA based on the AIG autoencoder, we propose a novel constrained masked modeling paradigm, MGVGA, to perform general circuit representation learning. 
This paradigm synthesizes these strategies to develop GNNs that effectively capture diverse and intricate features of circuits.
Formally, the loss function for MGVGA can be defined as:
\begin{equation}
    \mathcal{L}_{\text{mgvga}} = \mathcal{L}_{\text{mgm}} + \mathcal{L}_{\text{vga}}.
\end{equation}
This integration enables the GNNs to learn concurrently from fine-grained structural information and abstract circuit function features, optimizing a unified representation that facilitates a wide range of logic synthesis tasks such as classification, regression, and complex reasoning on circuits.
