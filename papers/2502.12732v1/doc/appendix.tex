\section{Appendix}

\subsection{More Details of MGVGA}

\subsubsection{Logic Equivalence Preservation}
\label{sec:preserve_logic_eq}

In traditional masked graph modeling processes, nodes in graphs are masked directly and then reconstructed without any constraint except labels of masked nodes.
However, any valid circuits can be labeled during the reconstruction process for circuit representation learning. 
It's hard for GNNs to learn useful features for downstream tasks in this way. 
Consequently, we introduce constraints during the decoding process to ensure that GNNs learn useful features related to the circuit. 
%These constraints can be in the form of latent space embedding $X$ (as in MGM) or Verilog code embedding $X_V$ (as in VGA). 
%The key point is that these constraints are logically equivalent (come from the same truth table as $\mathcal{G}$) to the original AIG $\mathcal{G}$ to guide GNNs effectively.
%In our work, we apply constraints to guide the model to guarantee the logic equivalence and learn useful features that benefit the downstream tasks during the training process.

In our work, ``logical equivalence preservation'' describes the equivalence between an AIG $\mathcal{G} = (\mathcal{V}, \mathcal{A})$ and its different representations including $X=g_{E}(\mathcal{V}, \mathcal{A})$ and $X_V$.
In \textbf{MGM}, $\mathcal{G}$ and its latent space embedding $X$ are logically equivalent. 
According to $X=g_{E}(\mathcal{V}, \mathcal{A})$, the latent space embedding $X$ is derived from $\mathcal{G}$ through the GNN encoding process without masking gates(nodes).
Consequently, we can say that $X$ and $\mathcal{G}$ comes from the same truth table.
In \textbf{VGA}, $\mathcal{G}$ and its corresponding Verilog code embedding $X_V$ are logically equivalent. 
$X_V$ is extracted via LLM according to the given Verilog code and $\mathcal{G}$ is obtained from the Verilog code via logic synthesis tools as illustrated in \Cref{fig:mm_logic_eq}.
Similarly, we can say that the truth tables of $X_V$ and $\mathcal{G}$ are the same.

In summary, we won't change the original structure of AIG $\mathcal{G}$ during the reconstruction process and the logic information is retained in $X$ or $X_V$ as the constraint for the decoding process.


\subsubsection{Unmasked Adjacency Matrix}
\label{sec:womasking}
As we mentioned, following previous masked modeling paradigms~\citep{hou2022graphmae, hou2023graphmae2}, we do \textbf{not} mask the adjacent matrices $\mathcal{A}$ during the entire process.
Here are the detailed reasons.

\minisection{Logical Equivalence:}
Our MGVGA emphasizes logical equivalence during training, ensuring that the circuit has a unique solution during reconstruction. 
If both $ \mathcal{A} $ and $X$ were masked, the problem would become NP-hard due to multiple possible solutions, making it much more difficult to train the model effectively and convergently.

\minisection{Computational Complexity:}
Reconstructing the adjacency matrix $\mathcal{A}$ would require handling a matrix of size $N^2$ for $N$ gates, which is computationally infeasible for large circuits (e.g., those with millions of gates).
By not masking $\mathcal{A}$, we only need to reconstruct the masked gate information in $X$, significantly reducing computational complexity and improving the efficiency of both training and inference.
Moreover, both MGM and VGA in MGVGA operate at the gate level without masking the adjacency matrix $\mathcal{A}$, focusing on local feature extraction. 
Consequently, we can utilize parallel processing techniques, distributed computing, etc., to reduce overhead in both MGM and VGA stages for high-complexity circuits.

In summary, we perform the MGVGA without masking the adjacency matrix $\mathcal{A}$, which is the same as previous masked graph modeling methods~\citep{hou2022graphmae, hou2023graphmae2}. 
Consequently, there are only two tasks for gate-level prediction to reconstruct masked circuits as illustrated in \Cref{sec:AIG_rec}. 

\subsection{More Details of Experiment Settings}

\subsubsection{AIG Collection}
\label{sec:aigs_collect}
Yosys~\citep{wolf2016yosys} is utilized to conduct logic synthesis, which converts source codes of circuit designs into the standardized AIG format.
Moreover, we prepare 1500 optimization sequences, each containing 20 synthesis transformations including \texttt{rewrite}, \texttt{resub}, \texttt{refactor}, \texttt{rewrite -z}, \texttt{resub -z}, \texttt{refactor -z}, and \texttt{balance} transformations, consistent with prior works~\citep{chowdhury2022bulls, zheng2024lstp}.
Then sequential synthesis transformations are carried out by the logic synthesis tool ABC~\citep{brayton2010abc} and their corresponding labels are generated.
Meanwhile, we also store the AIG after each synthesis transformation for AIG self-supervised learning.
The resulting AIG dataset, post-technology mapping with the NanGate 45nm technology library and the ``5K heavy 1k'' wireload model, comprises 810000 AIGs and 40500 synthesis labels across various optimization sequences and circuit designs.

\subsubsection{Baseline Selection}
\label{sec:baseline}

During the baseline selection process, we acknowledged and recognized the progress made with DeepGate3~\citep{shi2024deepgate3}, the upgraded version of DeepGate2~\citep{shi2023deepgate2}.
However, we encountered several challenges in our practical implementation.

Specifically, DeepGate3's architecture is built upon DeepGate2 and incorporates transformer models, which have a quadratic time complexity during attention computation. 
This becomes a critical issue when dealing with large-scale datasets. 
Our training set includes digital circuits with up to millions of gates, leading to substantial and often unmanageable computational costs during training. 
During the testing phase, DeepGate3 is limited to circuits with up to thousands of gates. 
When attempting to infer circuits with millions of gates, the computational overhead becomes prohibitively high. This limitation makes it impractical for our use case, where we need to handle circuits of varying sizes, including very large ones.

Consequently, we choose DeepGate2 as our baseline because it's the best model that provides a more practical and scalable solution for the digital circuits we aim to model and optimize within the EDA toolchain. 
Moreover, we also provide some experiment results for the comparison between DeepGate3 and MGVGA on the small-scale designs with just thousands of gates in \Cref{sec:ext_exp}.

\subsubsection{More Details of Model Settings}
\label{sec:model_set}

\minisection{Model Size.} DeepGate2~\citep{shi2023deepgate2} has 0.64M parameters with 1 layer and DeepGate3~\citep{shi2024deepgate3} has 8.17M parameters with transformer architecture.
Meanwhile, our MGVGA has only 0.12M parameters with 7 layers. 
According to the experiment results, MGVGA achieved much better performance compared to DeepGate2 and DeepGate3 with fewer model parameters, which demonstrates the effectiveness of our method. 

\minisection{Bidirectional LLM.} We utilize gte-Qwen2-7B-instruct~\citep{li2023gte} model for Verilog code representation extraction, which is based on a BERT-like encoder transformer architecture with bidirectional attention. 
The base model of gte-Qwen2-7B-instruct, Qwen2-7B-instruct~\citep{yang2024qwen2}, is a decoder-based model with causal attention. 
Qwen2-7B-instruct has been extensively trained on a diverse corpus of Verilog and demonstrates an extraordinary ability to understand and process various styles and complexities of Verilog/System Verilog code, including less standardized or non-optimized representations. 
Although it excels in generating text and understanding sequential dependencies, it is not well-suited for embedding tasks due to its unidirectional attention mechanism. 
Consequently, \cite{li2023gte} proposes GTE to transform Qwen2-7B-instruct into gte-Qwen2-7B-instruct, enabling the model to capture bidirectional context while preserving the original capabilities in understanding Verilog codes.

\subsubsection{Benchmark Selection}
\label{sec:benchmark}	

Given the practical requirements of our application, we chose DeepGate2 as our baseline. 
DeepGate2 uses a portion of the design from open-source benchmarks as training data.
To ensure fair and reliable testing, we excluded these designs from our test set. 
To further validate the reliability and practicality of our method, we have supplemented our test set with designs from different opensource benchmarks~\citep{chowdhury2021openabcd,EPFLBenchmarks2015,openrisc2009or1200,yosys2019picorv32,asanovic2016rocket} that were not used during training. 
These additional test sets cover a range of graph sizes and complexities, ensuring that our evaluation is comprehensive and representative of real-world EDA tool requirements.

\subsubsection{Evaluation Details of QoR Prediction}
\label{sec:qor_metric}	

For the evaluation of the QoR prediction task, we evaluate the performance across ten circuit designs as illustrated in \Cref{table:rank_QoR}, with each circuit undergoing synthesis through 1500 optimization sequences, each containing 20 steps.
Notably, normalization is applied to $\Vec{A} \in \mathbb{R}^{1\times1500}$, representing the count of optimized gates per sequence, following~\citep{chowdhury2021openabcd}.
Specifically, each element $\Vec{A}_{i}$ is standardized using $\Vec{A}_{i} = \frac{\bar{\Vec{A}}-\Vec{A}_{i}}{\sigma_{\Vec{A}}}$, where $\bar{\Vec{A}}$ and $\sigma_{\Vec{A}}$ is the mean and the standard deviation of $\Vec{A}$, respectively.
For QoR prediction, we need to rank the predicted scores $\Vec{B} \in \mathbb{R}^{1\times1500}$ to identify the best optimization sequence.
Consequently, we utilize the Normalized Discounted Cumulative Gain (NDCG)~\citep{jarvelin2017ireval, jarvelin2002cg} metric to assess the quality of the ranking algorithms for the predicted scores $\Vec{B}$.
The NDCG@$k$ is calculated as follows:
\begin{equation} 
    \text{NDCG}@k = (\sum_{i=1}^k \frac{\Vec{A}_{\text{rank}(\Vec{B}, {i})}}{\log_2(i + 1)}) / (\sum_{i=1}^{k} \frac{\Vec{A}_{\text{rank}(\Vec{A}, {i})}}{\log_2(i + 1)}),
\end{equation}
where $k$ represents the position considered in the ranking. 
Here, $\text{rank}(\Vec{A}, i)$ and $\text{rank}(\Vec{B}, i)$ denote the indices in $\Vec{A}$ and $\Vec{B}$ of the $i$-th largest elements, respectively. 
The NDCG@$k$ score ranges from -1 to 1, with a higher score indicating better ranking performance.
A perfect ranking would achieve an NDCG@$k$ score of 1.
Furthermore, we evaluate and compare the predictions on a reference set of optimization sequences with actual synthesis labels using the Top-$k$\% Commonality metrics, defined as $\frac{\text{num}(\tilde{\Vec{A}}_{k} \cap \tilde{\Vec{B}}_{k})}{\text{num}(\tilde{\Vec{A}}_{k})}$, where $\tilde{\Vec{A}}_{k}$ and $\tilde{\Vec{B}}_{k}$ represent the top $k$\% performing optimization sequences, actual and predicted, respectively.


\subsection{Extended Experimental Results}
\label{sec:ext_exp}

\begin{table*}[]
\caption{Performance of DeepGate models and MGVGA on logic equivalence identification.}
\centering
\setlength\tabcolsep{6.4pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.74\linewidth}{!}{
\begin{tabular}{l|ccc|cc|cc|cc}
\toprule
\multirow{3}{*}{Design} & \multirow{3}{*}{\# PI} & \multirow{3}{*}{\# PO} & \multirow{3}{*}{\# Gates} & \multicolumn{2}{c|}{DeepGate2} & \multicolumn{2}{c|}{DeepGate3} & \multicolumn{2}{c}{MGVGA (Ours)} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
& & & & F1-Score & AUC & F1-Score & AUC & F1-Score & AUC \\
\midrule
bc0 & 21 & 11 & 2784 & 0.327 & 0.813 & 0.373 & 0.819 & 0.396 & 0.817 \\
apex1 & 45 & 45 & 2661 & 0.223 & 0.601 & 0.326 & 0.725 & 0.394 & 0.826 \\
k2 & 45 & 45 & 4075 & 0.276 & 0.695 & 0.345 & 0.834 & 0.492 & 0.919 \\
i10 & 257 & 224 & 3618 & 0.575 & 0.918 & 0.601 & 0.928 & 0.805 & 0.985 \\
mainpla & 26 & 49 & 9441 & 0.290 & 0.732 & 0.305 & 0.763 & 0.281 & 0.746 \\
\midrule
Average & & & & 0.338 & 0.752 & 0.390 & 0.834 & \textbf{0.474} & \textbf{0.859} \\
\bottomrule
\end{tabular}}
\label{table:small_logiceq}
\end{table*}

\subsubsection{Logic Equivalence Identification}
\label{sec:ext_logiceq}

As shown in \Cref{table:small_logiceq}, we present the performance comparison between DeepGate models~\citep{shi2023deepgate2,shi2024deepgate3} and MGVGA on small circuit designs considering the computation overhead. 
The experiment results show that MGVGA outperforms DeepGate3, achieving an average F1-score of 0.474 compared to 0.390, and an average AUC of 0.859 versus 0.834. 

\subsubsection{Boolean Satisfiability Solving}
\label{sec:sat}
Boolean satisfiability (SAT) solving aims to determine whether there exists an assignment of truth values that satisfies a Boolean formula. 
In logic synthesis, SAT solvers are indispensable for tasks such as logic optimization, ensuring both the correctness and efficiency of circuits. 
Despite their critical role, SAT solving remains computationally challenging, often leading to significant runtime overhead, especially for large or complex designs. 
To address this challenge, several studies~\citep{haaswijk2019sat,shi2023deepgate2, shi2024deepgate3} have proposed various methods to accelerate the SAT-solving process. 
For instance, Exact synthesis~\citep{haaswijk2019sat} enhanced SAT solving by systematically varying the number of nodes and levels in directed acyclic graph (DAG) topologies, a technique referred to as Boolean fences. 
A Boolean fence is a partition of nodes across multiple levels, with each level containing at least one node. 
By tuning these parameters, they effectively constrained the search space for the SAT solver, resulting in more efficient and predictable synthesis outcomes.
Despite achieving significant speedup over SAT solvers, the solution still has room for further enhancement.
Building on the work presented in~\citep{haaswijk2019sat}, we demonstrate how MGMVA can efficiently accelerate the SAT-solving process.

\minisection{Exact Synthesis} generates logic circuits that guarantee logical equivalence between the resulting circuit and the target logic function, intending to find an optimal implementation based on specific criteria, such as gate count or depth.
% The exact synthesis process can handle subcircuits with a maximum of 8 inputs and a single output, which is still computationally infeasible due to the exponential number of Boolean functions $2^{256}$ resulting in large runtime overhead.

\minisection{Experiment Settings.}
We integrate the MGMVA into the SAT solver~\citep{haaswijk2019sat} to perform exact synthesis tasks.
For the training process, we apply the exact synthesis process to the EPFL~\citep{EPFLBenchmarks2015} and IWLS~\citep{albrecht2005iwls} benchmarks using the SAT solver~\citep{haaswijk2019sat} to obtain the number of nodes and levels in the subcircuits, which are subsequently used as labels.
We extract the circuit embeddings from MGVGA and feed them into MLP to perform regression tasks.
As for the SAT-solving process with MGVGA, we first extract several small subcircuits from the original circuits following the exact synthesis process.
These subcircuits, which have a maximum of 8 inputs and a single output, are still computationally challenging due to the exponential number of Boolean functions $2^{256}$ resulting in large runtime overhead.
Then, we can use the MGMVA to predict the number of nodes and levels required for the optimal equivalent implementation of the input subcircuit and use MGMVA to further constrain the search space of the Boolean fence, thereby accelerating the SAT-solving process. 
Finally, to assess the efficacy of our model in accelerating SAT solving, we employ DeepGate2~\citep{shi2023deepgate2} and DeepGate3~\citep{shi2024deepgate3} as baselines. 
All experiments are conducted using the same computational resources.

\minisection{Evaluation Results.}
\Cref{table:sat_solver} and \Cref{table:sat_overall} present a runtime comparison among the exact synthesis, DeepGate2, DeepGate3, and our MGVGA settings, with runtime measured in \textbf{seconds (s)}.
We choose exact synthesis as our baseline setting the runtime reduction compared to the baseline setting is denoted as Red.
As shown in \Cref{table:sat_solver}, MGVGA achieves an average runtime reduction of 53.09\% while DeepGate2 and DeepGate3 achieve an average reduction of 18.24\% and 21.31\% separately for SAT solving process.
However, it is worth noting that the SAT solver with GNN is less
effective for easier cases, as the model inference process accounts
for a significant portion of the total runtime as illustrated in \Cref{table:sat_overall}. 
In general, MGVGA exhibits significant improvement compared to DeepGate models in this task, indicating that MGVGA can capture more informative abstract functional and fine-grained structural representations for solving practical SAT problems.
\begin{table*}[]
\caption{The comparison of SAT solving runtime (solver only).}
\centering
%\setlength\tabcolsep{3.2pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.83\linewidth}{!}{
\begin{tabular}{l|c|c|cc|cc|cc}
\toprule
\multirow{2}{*}{Design} & \multirow{2}{*}{\# Subcircuits} & \multirow{2}{*}{Exact Synthesis} & \multicolumn{2}{c|}{DeepGate2} & \multicolumn{2}{c|}{DeepGate3} & \multicolumn{2}{c}{MGVGA (Ours)} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& & & Solver & Red. & Solver & Red. & Solver & Red. \\
\midrule
adder & 27 & 365.19  & 480.72 & -31.64\% & 464.21 & -27.11\% & 184.19 & 49.56\% \\
sqrt  & 38 & 5.55    & 6.65 & -19.82\% & 6.92 & -24.68\% & 4.77 & 14.05\% \\
hyp   & 80 & 328.58  & 351.25 & -6.90\% & 351.71 & -7.04\% & 213.20 & 35.11\% \\
i2c  & 169 & 267.15  & 62.21 & 76.71\% & 34.01 & 87.27\% & 32.98 & 87.65\% \\
div & 1968 & 4033.48 & 1096.21 & 72.83\& & 882.59 & 78.12\% & 844.45 & 79.06\% \\

\midrule
Average & & 999.99 & 399.41 & 18.24\% & 347.89 & 21.31\% & \textbf{255.92} & \textbf{53.09\%} \\
\bottomrule
\end{tabular}}
\label{table:sat_solver}
\end{table*}

\begin{table*}[]
\caption{The comparison of SAT solving runtime (overall).}
\centering
%\setlength\tabcolsep{3.2pt}
% \renewcommand{\arraystretch}{1.04}
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{l|c|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Design} & \multirow{2}{*}{Exact Synthesis} & \multicolumn{3}{c|}{DeepGate2} & \multicolumn{3}{c|}{DeepGate3} & \multicolumn{3}{c}{MGVGA (Ours)} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
& & Model & Solver & Overall & Model & Solver & Overall & Model & Solver & Overall \\
\midrule
adder & 365.19  & 0.66 & 480.72 & 481.38 & 10.86 & 464.21 & 475.07 & 0.17 & 184.19 & 184.36 \\
sqrt  & 5.55    & 0.65 & 6.65 & 7.30 & 10.91 & 6.92 & 17.83 & 0.24 & 4.77 & 5.01 \\
hyp   & 328.58  & 2.13 & 351.25 & 353.38 & 36.01 & 351.71 & 387.72 & 1.32 & 213.20 & 215.84 \\
i2c   & 267.15  & 2.60 & 62.21 & 64.81 & 45.95 & 34.01 & 79.96 & 1.06 & 32.98 & 34.04 \\
div   & 4033.48 & 40.80 & 1096.21 & 1137.01 & 763.71 & 882.89 & 1646.60 & 17.11 & 844.45 & 861.56 \\

\midrule
Average & 999.99 & 9.37 & 399.41 & 408.78 & 173.49 & 347.89 & 521.38 & \textbf{3.99} & \textbf{255.92} & \textbf{259.91} \\
\bottomrule
\end{tabular}}
\label{table:sat_overall}
\end{table*}

