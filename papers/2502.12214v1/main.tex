%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{enumitem}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{colortbl}  % 颜色表格宏包
\usepackage{xcolor}    % 颜色定义宏包
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% 定义柔和经典配色
\definecolor{lightgray}{RGB}{220, 220, 220}    % 浅灰色
\definecolor{lightblue}{RGB}{198, 224, 244}    % 浅蓝色
\definecolor{lightgreen}{RGB}{197, 224, 180}   % 浅绿色
\definecolor{lightpurple}{RGB}{230, 230, 250}  % 浅紫色
\definecolor{lightorange}{RGB}{255, 218, 185}  % 浅橙色
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement}

\begin{document}

\twocolumn[
% \icmltitle{Let LLMs Deep Thinking with Zero Token: \\  Unlocking the Full Potential of Existing Parameters through Cycling}
\icmltitle{Zero Token-Driven Deep Thinking in LLMs: \\Unlocking the Full Potential of Existing Parameters via Cyclic Refinement}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Guanghao Li}{yyy,comp,sch}
\icmlauthor{Wenhao Jiang}{sch}
\icmlauthor{Li Shen}{sys}
\icmlauthor{Ming Tang}{comp}
\icmlauthor{Chun Yuan}{yyy}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{SIGS, Tsinghua University}
\icmlaffiliation{comp}{Southern University of Science and Technology}
\icmlaffiliation{sch}{Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)}
\icmlaffiliation{sys}{Sun Yat-sen University}
\icmlcorrespondingauthor{Wenhao Jiang}{cswhjiang@gmail.com}
\icmlcorrespondingauthor{Chun,Yuan}{yuanc@sz.tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\printAffiliationsAndNotice{}
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.



\input{sec/0_abstract}    
\input{sec/1_introduction}

\input{sec/2_relatedwork}
\input{sec/3_methods}
\input{sec/4_experiments}
\input{sec/5_conclusion}




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

%\bibliography{example_paper}
%\bibliographystyle{icml2025}

\begin{thebibliography}{43}


\bibitem[Bae et~al.(2024)Bae, Fisch, Harutyunyan, Ji, Kim, and Schuster]{bae2024relaxed}
Bae, S., Fisch, A., Harutyunyan, H., Ji, Z., Kim, S., and Schuster, T.
\newblock Relaxed recursive transformers: Effective parameter sharing with layer-wise lora.
\newblock \emph{arXiv preprint arXiv:2410.20672}, 2024.

\bibitem[Balagansky \& Gavrilov(2022)Balagansky and Gavrilov]{balagansky2022palbert}
Balagansky, N. and Gavrilov, D.
\newblock Palbert: Teaching albert to ponder.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 14002--14012, 2022.

\bibitem[Banino et~al.(2021)Banino, Balaguer, and Blundell]{banino2021pondernet}
Banino, A., Balaguer, J., and Blundell, C.
\newblock Pondernet: Learning to ponder.
\newblock \emph{arXiv preprint arXiv:2107.05407}, 2021.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pp.\  7432--7439, 2020.

\bibitem[Chen et~al.(2023)Chen, Pan, Li, Ding, and Zhou]{chen2023ee}
Chen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J.
\newblock Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism.
\newblock \emph{arXiv preprint arXiv:2312.04916}, 2023.

\bibitem[Chowdhury \& Caragea(2024)Chowdhury and Caragea]{chowdhury2024recurrent}
Chowdhury, J.~R. and Caragea, C.
\newblock Recurrent transformers with dynamic halt.
\newblock \emph{arXiv preprint arXiv:2402.00976}, 2024.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Csord{\'a}s et~al.(2024)Csord{\'a}s, Irie, Schmidhuber, Potts, and Manning]{csordas2024moeut}
Csord{\'a}s, R., Irie, K., Schmidhuber, J., Potts, C., and Manning, C.~D.
\newblock Moeut: Mixture-of-experts universal transformers.
\newblock \emph{arXiv preprint arXiv:2405.16039}, 2024.

\bibitem[Dabre \& Fujita(2019)Dabre and Fujita]{dabre2019recurrent}
Dabre, R. and Fujita, A.
\newblock Recurrent stacking of layers for compact neural machine translation models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pp.\  6292--6299, 2019.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, {\L}.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Eigen et~al.(2013)Eigen, Rolfe, Fergus, and LeCun]{eigen2013understanding}
Eigen, D., Rolfe, J., Fergus, R., and LeCun, Y.
\newblock Understanding deep architectures using a recursive convolutional network.
\newblock \emph{arXiv preprint arXiv:1312.1847}, 2013.

\bibitem[Gao et~al.(2024)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le~Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, 07 2024.
\newblock URL \url{https://zenodo.org/records/12608602}.

\bibitem[Graves(2016)]{graves2016adaptive}
Graves, A.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1603.08983}, 2016.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Kim et~al.(2023)Kim, Park, Kim, Lee, Song, Kim, Kim, Kim, Lee, Kim, et~al.]{kim2023solar}
Kim, D., Park, C., Kim, S., Lee, W., Song, W., Kim, Y., Kim, H., Kim, Y., Lee, H., Kim, J., et~al.
\newblock Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling.
\newblock \emph{arXiv preprint arXiv:2312.15166}, 2023.

\bibitem[Lan(2019)]{lan2019albert}
Lan, Z.
\newblock Albert: A lite bert for self-supervised learning of language representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Latif et~al.(2023)Latif, Fang, Ma, and Zhai]{latif2023knowledge}
Latif, E., Fang, L., Ma, P., and Zhai, X.
\newblock Knowledge distillation of llm for education.
\newblock \emph{arXiv preprint arXiv:2312.15842}, 2023.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\  19274--19286. PMLR, 2023.

\bibitem[Lin et~al.(2024)Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan, and Han]{lin2024awq}
Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S.
\newblock Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
\newblock \emph{Proceedings of Machine Learning and Systems}, 6:\penalty0 87--100, 2024.

\bibitem[Liu et~al.(2024)Liu, Feng, Xue, Wang, Wu, Lu, Zhao, Deng, Zhang, Ruan, et~al.]{liu2024deepseek}
Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et~al.
\newblock Deepseek-v3 technical report.
\newblock \emph{arXiv preprint arXiv:2412.19437}, 2024.

\bibitem[Liu et~al.(2023)Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi, Krishnamoorthi, and Chandra]{liu2023llm}
Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V.
\newblock Llm-qat: Data-free quantization aware training for large language models.
\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023.

\bibitem[Ma et~al.(2023)Ma, Fang, and Wang]{ma2023llm}
Ma, X., Fang, G., and Wang, X.
\newblock Llm-pruner: On the structural pruning of large language models.
\newblock \emph{Advances in neural information processing systems}, 36:\penalty0 21702--21720, 2023.

\bibitem[Milbauer et~al.(2023)Milbauer, Louis, Hosseini, Fabrikant, Metzler, and Schuster]{milbauer2023lait}
Milbauer, J., Louis, A., Hosseini, M.~J., Fabrikant, A., Metzler, D., and Schuster, T.
\newblock Lait: Efficient multi-segment encoding in transformers with layer-adjustable interaction.
\newblock \emph{arXiv preprint arXiv:2305.19585}, 2023.

\bibitem[Pan et~al.(2024)Pan, Chen, Li, Ding, and Zhou]{pan2024ee}
Pan, X., Chen, Y., Li, Y., Ding, B., and Zhou, J.
\newblock Ee-tuning: An economical yet scalable solution for tuning early-exit large language models.
\newblock \emph{arXiv preprint arXiv:2402.00518}, 2024.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.~N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fern{\'a}ndez, R.
\newblock The lambada dataset: Word prediction requiring a broad discourse context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Pope et~al.(2023)Pope, Douglas, Chowdhery, Devlin, Bradbury, Heek, Xiao, Agrawal, and Dean]{pope2023efficiently}
Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J.
\newblock Efficiently scaling transformer inference.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5:\penalty0 606--624, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and Shavit]{rosenfeld2019constructive}
Rosenfeld, J.~S., Rosenfeld, A., Belinkov, Y., and Shavit, N.
\newblock A constructive prediction of the generalization error across scales.
\newblock \emph{arXiv preprint arXiv:1909.12673}, 2019.

\bibitem[Savarese \& Maire(2019)Savarese and Maire]{savarese2019learning}
Savarese, P. and Maire, M.
\newblock Learning implicitly recurrent cnns through parameter sharing.
\newblock \emph{arXiv preprint arXiv:1902.09701}, 2019.

\bibitem[Sherstinsky(2020)]{sherstinsky2020fundamentals}
Sherstinsky, A.
\newblock Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network.
\newblock \emph{Physica D: Nonlinear Phenomena}, 404:\penalty0 132306, 2020.

\bibitem[Shum et~al.(2024)Shum, Xu, Zhang, Chen, Diao, Dong, Zhang, and Raza]{shum2024first}
Shum, K., Xu, M., Zhang, J., Chen, Z., Diao, S., Dong, H., Zhang, J., and Raza, M.~O.
\newblock First: Teach a reliable large language model through efficient trustworthy distillation.
\newblock \emph{arXiv preprint arXiv:2408.12168}, 2024.

\bibitem[Sun et~al.(2023)Sun, Liu, Bair, and Kolter]{sun2023simple}
Sun, M., Liu, Z., Bair, A., and Kolter, J.~Z.
\newblock A simple and effective pruning approach for large language models.
\newblock \emph{arXiv preprint arXiv:2306.11695}, 2023.

\bibitem[Sun et~al.(2024)Sun, Pickett, Nain, and Jones]{sun2024transformer}
Sun, Q., Pickett, M., Nain, A.~K., and Jones, L.
\newblock Transformer layers as painters.
\newblock \emph{arXiv preprint arXiv:2407.09298}, 2024.

\bibitem[Takase \& Kiyono(2021)Takase and Kiyono]{takase2021lessons}
Takase, S. and Kiyono, S.
\newblock Lessons on parameter sharing across layers in transformers.
\newblock \emph{arXiv preprint arXiv:2104.06022}, 2021.

\bibitem[Tan et~al.(2023)Tan, Shen, Chen, Courville, and Gan]{tan2023sparse}
Tan, S., Shen, Y., Chen, Z., Courville, A., and Gan, C.
\newblock Sparse universal transformer.
\newblock \emph{arXiv preprint arXiv:2310.07096}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Xia et~al.(2019)Xia, He, Tan, Tian, He, and Qin]{xia2019tied}
Xia, Y., He, T., Tan, X., Tian, F., He, D., and Qin, T.
\newblock Tied transformers: Neural machine translation with shared encoder and decoder.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~33, pp.\  5466--5473, 2019.

\bibitem[Xu et~al.(2024)Xu, Cai, Wu, Li, and Wang]{xu2024fwdllm}
Xu, M., Cai, D., Wu, Y., Li, X., and Wang, S.
\newblock $\{$FwdLLM$\}$: Efficient federated finetuning of large language models with perturbed inferences.
\newblock In \emph{2024 USENIX Annual Technical Conference (USENIX ATC 24)}, pp.\  579--596, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2023opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock \emph{URL https://arxiv. org/abs/2205.01068}, 3:\penalty0 19--0, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Ning, Hong, Fu, Xu, Li, Lou, Wang, Yuan, Li, et~al.]{zhou2024survey}
Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., Lou, Y., Wang, L., Yuan, Z., Li, X., et~al.
\newblock A survey on efficient inference for large language models.
\newblock \emph{arXiv preprint arXiv:2404.14294}, 2024.

\end{thebibliography}

\input{sec/6_appendix}




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
