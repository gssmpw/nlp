\section{Experiments}
\label{sec:experiments}
\input{sec/table/table_scratch}  % example of including your scratch results table
In this section, we present our experimental setup and results to demonstrate the effectiveness of our proposed \textit{Zero-Token Transformer} approach under a fixed parameter budget. We evaluate both {training from scratch} and {fine-tuning} scenarios, using a decoder-only Transformer architecture.

\subsection{Experimental Setup}
\label{sec:exp_setup}

\textbf{Models.}
We consider two main training settings:
\begin{itemize}[leftmargin=*]
    \item \textbf{Training from Scratch:} We base our architecture on GPT-2~\cite{radford2019language} but restrict each layer to around 10M parameters. The total number of layers is $L=6$. To maintain a fixed computational budget, we define a total of 6 ``network computation cycles'',  where each layer in a standard setting (i.e., without parameter sharing) is counted as one cycle. All models in this setting are pre-trained on the C4 English subset~\cite{raffel2020exploring} using a causal next-token prediction objective for 10B tokens.
    \item \textbf{Fine-Tuning Pre-Trained Models:} We also fine-tune widely used checkpoints such as {GPT-2} and {OPT}~\cite{zhang2023opt} to show that our approach can be applied to large pre-trained models with minimal modification.
\end{itemize}

\textbf{Baselines.}
We compare several approaches, all based on decoder-only Transformers. One variant, referred to as early exit, adds classification heads at each cycle, facilitating intermediate predictions. These intermediate exits are represented as E.

\begin{itemize}[leftmargin=*]
    \item \textbf{Vanilla (V):} A standard Transformer with $L$ distinct layers. The total computation cost is effectively $L$ cycles.
    \item \textbf{Basic Cycling (BC):} The model has $L$ layers but shares parameters across layers by cycling them $N$ times. This results in a total computation cost of $L \times N$. 
    \item \textbf{Head-Tail Cycling (HTC):} Instead of cycling \emph{all} $L$ layers, only the \emph{intermediate} layers are reused $N$ times, while the first and last layers remain distinct. This structure aims to preserve the special functions of the input and output layers. We also consider an early exit variant of {this scheme.}
    \item \textbf{Zero-Token Transformer (ZTT):} Our proposed method, which only cycles the intermediate layers and introduces a \emph{Zero Token} in each attention layer. This token provides a learnable \emph{key} (prompt-like) while carrying zero-value vectors, enabling the model to distinguish between different cycles and facilitate \emph{adaptive computation}. {We also include an {early exit} variant } that uses the learned \emph{Zero Attention} signals to decide when to stop.
\end{itemize}

\textbf{Evaluation Datasets and Metrics.}
We considered nine different types of datasets: 
(a) \textit{Reasoning}: PIQA~\cite{bisk2020piqa}; 
(b) \textit{Multiple Choice}: ARC Challenge~\cite{clark2018think}, ARC Easy~\cite{clark2018think}; 
(c) \textit{Long-Term Context Recall}: LAMBADA~\cite{paperno2016lambada} and 
(d) \textit{Natural Language Inference}: HellaSwag~\cite{zellers2019hellaswag}.

For multiple-choice tasks, we report {accuracy}, and for LAMBADA we report {exact match} accuracy on the held-out set. We employ the Language Model Evaluation Harness~\cite{eval-harness} for consistent evaluations. All models {pre-trained from scratch} are fine-tuned on each downstream task before testing. Similarly, the large pre-trained checkpoints (GPT-2, OPT) are directly fine-tuned on these tasks using the same hyperparameter settings (details in ~\cref{es}).

\vspace{-0.2em}


\subsection{Results of Training from Scratch}
\label{sec:exp_scratch}

Table~\ref{tab:scartch}summarizes the performance of models trained {from scratch} under a fixed parameter budget. We highlight the following observations:

\textbf{Effect of Fewer Layers (Vanilla vs.\ Vanilla-Small).}
When we reduce both the parameter count and computational budget from $L=6$ to $L=3$ (``Vanilla-small''), the accuracy drops significantly (e.g., from 33.97\% to 31.51\%). {This indicates that simply using fewer layers cannot maintain adequate performance without cycling.}

\textbf{Basic Cycling (BC).}
To mitigate the performance gap, {BC} reuses a 3-layer stack twice (for a total of 6 cycles), partially recovering performance to 32.79\%. This confirms that increased ``computational depth'' via cycling can help, though it still lags behind the original 6-layer Transformer. Introducing {early exit} ({BCE}) in BC leads to a slight accuracy drop (32.17\%), suggesting that training additional intermediate heads can sometimes introduce optimization trade-offs.

\textbf{Head-Tail Cycling (HTC).}
By fixing the first and last layers while only cycling the intermediate ones, {we achieve 33.15\% (no early exit) and 32.62\% (early exit), surpassing Basic Cycling.} This underscores the importance of preserving specialized head and tail layers.

\textbf{Zero-Token Transformer (ZTT).}
Building on head-tail separation, our proposed {Zero Token} mechanism further boosts accuracy to 33.52\% without early exit, and 32.79\% with early exit. Notably, ZTT consistently outperforms both BC and HT across tasks, demonstrating that the Zero-Token mechanism effectively guides each cycle’s computation and alleviates functional conflicts in shared parameters.

Overall, these results confirm the benefit of our {ZTT} approach in balancing parameter efficiency and modeling capacity. Even when the total parameter budget and computation cycles are constrained, introducing Zero Tokens with a head-tail separation strategy yields superior accuracy.

\subsection{Adaptive Inference with Zero Attention}
\label{sec:adaptive_inference}

\input{sec/table/table_attention}

We next investigate whether \textit{Zero Attention}—the average attention placed on the Zero Token—can serve as a stopping criterion for adaptive inference. Specifically, once the Zero Attention score surpasses a predefined threshold \( P \), we consider the representation sufficiently refined and terminate further cycles.

In Table~\ref{tab:attention}, we track perplexity ({PPL}), Zero Attention, and Gate Value across different cycle counts. As the loop depth increases, {Zero Attention} gradually rises, while the {Gate Value} in the feed-forward network decreases, suggesting diminishing returns from additional computation.

Table~\ref{tab:adaptive} further examines how different thresholds \( P \) affect both model accuracy and the average number of computation cycles. A lower threshold (\( P = 0.2 \)) forces early termination, significantly reducing compute but at the cost of some performance loss. In contrast, a moderate threshold (\( P = 0.5 \)) provides a strong balance, achieving 33.00\% accuracy with an average of just 3.31 cycles—matching or even surpassing the full 4-cycle baseline. Increasing the threshold further (\( P = 0.7 \)) results in more reasoning steps, leading to slight accuracy improvements but at the expense of higher computational costs.

These findings illustrate that Zero Attention can effectively guide {dynamic computation}, allowing models to adaptively allocate reasoning cycles while maintaining strong performance. This presents a promising strategy for efficient inference in resource-constrained settings.

\input{sec/table/table_adaptive}


\subsection{Fine-Tuning Results on Pre-Trained Models}
\label{sec:exp_pretrained}

\input{sec/table/table_pertrain}

We further validate our method on large, pre-trained checkpoints: GPT-2~\cite{radford2019language} and  OPT~\cite{zhang2023opt}. Table~\ref{tab:pertrain} reports the performance of various cycling strategies after fine-tuning on the same downstream tasks.

Across all model scales, {cycling-based methods} consistently outperform the {Vanilla } baseline. Basic Cycling  provides noticeable accuracy gains over Vanilla, demonstrating the effectiveness of reusing parameters through repeated computation. However, when early exit is applied ({BCE}), performance occasionally drops slightly due to the additional overhead introduced by optimizing intermediate outputs.

Among all approaches, {Zero-Token Tansformer (ZTT)} achieves the highest accuracy, surpassing both BC and V. The improvements indicate that incorporating a Zero Token during fine-tuning enables the model to effectively leverage repeated reasoning under a fixed parameter budget. Furthermore, the early-exit variant, {Zero-Token Transformer with Early Exit (ZTTE)}, maintains comparable accuracy to full ZT while significantly reducing computational costs. This confirms that adaptive inference can successfully scale to large pre-trained models.

Notably, as model sizes increase—such as GPT-2 Large with 811M parameters—both {ZT} and {ZTE} continue to provide strong accuracy gains while maintaining parameter efficiency. These results demonstrate the broad applicability and scalability of our proposed Zero-Token approach, making it a robust fine-tuning strategy for large-scale language models.


\subsection{Ablation Study}
\label{sec:ablation}

\input{sec/table/table_ab}

To pinpoint the contribution of each component, we conduct an ablation study by selectively removing the {Zero Token (ZT)} or the {Gate} in the FNN layer. The results, summarized in Table~\ref{tab:ab} (placeholder), highlight the individual and combined effects of these components.

The full model ({ZT + Gate}) achieves the highest average accuracy of {32.79\%}, demonstrating the complementary benefits of these two mechanisms. When the {Gate} is removed, the model experiences a slight performance drop, indicating that the gating mechanism refines the computation flow within the feed-forward network. Similarly, removing only the {Zero Token} leads to a comparable decrease in accuracy, suggesting that the Zero Token mechanism is crucial for \textit{dynamic cycle awareness}. 
Furthermore, when both components are disabled, the model reaches its lowest performance, confirming that these mechanisms play an essential role in optimizing reasoning efficiency and predictive accuracy. These findings reinforce that the combination of {Zero Token and Gate} provides the best trade-off between computational efficiency and performance.





