\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figure/framework.pdf}
    \vspace{-0.5cm}
    \caption{  
\textbf{Left:} A 6-layer vanilla transformer without cyclic processing.  
\textbf{Center:} A transformer with a simple two-cycle mechanism.  
\textbf{Right:} A two-cycle Zero Token  Transformer, where the first and last layers do not participate in the cycling process. Each layer introduces an additional Zero Token. The rightmost part illustrates how the Zero Token is incorporated. Using the second layer as an example: the Zero Token is prepended to the sequence by aligning its key with the original tokens at the beginning, and an all-zero value is added in front of the value sequence. Placing the Zero Token at the beginning ensures that all subsequent tokens can effectively attend to it.  
}

    \label{fig:your_image}
    \vspace{-0.5cm}
\end{figure*}

\section{Related Work}

Parameter sharing  has long been explored in early deep learning architectures, such as Convolutional Neural Networks (CNNs)~\cite{eigen2013understanding,savarese2019learning} and Recurrent Neural Networks (RNNs)~\cite{graves2016adaptive,sherstinsky2020fundamentals}, effectively reducing model complexity while preserving performance. The Universal Transformer~\cite{dehghani2018universal} later extended this idea to the Transformer architecture, demonstrating that reusing parameters in a cyclical manner across layers can substantially enhance efficiency. Subsequently, various studies have investigated \emph{which} Transformer components should be shared. Some focus on parameter reuse within individual layers~\cite{dabre2019recurrent}, tying encoder and decoder components~\cite{milbauer2023lait,xia2019tied}, or adopting partial expert networks~\cite{liu2024deepseek}. Others optimize \emph{how} parameter sharing is organized, for instance by stacking parameters in specific orders~\cite{takase2021lessons} or applying factorized embeddings~\cite{lan2019albert} to improve performance.
A critical aspect of parameter cycling is determining \emph{when} to repeat computations. Methods such as ACT~\cite{chowdhury2024recurrent,graves2016adaptive,csordas2024moeut,tan2023sparse} and PonderNet~\cite{banino2021pondernet,balagansky2022palbert} introduce adaptive recursion, allowing the model to decide how many cycles of computation are needed for deeper reasoning. However, most of these studies focus on training from scratch rather than fine-tuning large pre-trained models, limiting their practicality in real-world scenarios.

Recent work has begun to address parameter cycling within pre-trained Large Language Models. For instance, {Solar}~\cite{kim2023solar} improves performance by reusing parameters from the middle layers of the Llama model~\cite{touvron2023llama}, illustrating \emph{which} layers to cycle. Relaxed Recursive Transformers~\cite{bae2024relaxed} integrate LoRA~\cite{hu2021lora} modules to alleviate performance degradation caused by repetitive parameter usage. Despite these advances, the question of \emph{when} to stop recurrent processing remains underexplored. Although Relaxed Recursive Transformers consider varying numbers of cycles, they rely on fixed computation paths rather than a genuinely dynamic mechanism. Meanwhile, research on early exiting~\cite{chen2023ee,pan2024ee} focuses primarily on non-recurrent models, leaving open the question of how many cycles a recurrent model should undergo.
In contrast, our approach comprehensively addresses the three central questions of \emph{what} to cycle, \emph{how} to manage recurrent parameters, and \emph{when} to terminate reasoning. We propose a method applicable both to training from scratch and to fine-tuning pre-trained LLMs, offering a practical and efficient solution for enhancing model performance under tight parameter budgets.
