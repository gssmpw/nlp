% \section{Conclusion}
% In this work, we propose a novel approach to fully exploit the potential of cyclic models by introducing the Zero Token mechanism. Our method incorporates trainable key values and a zero vector, which effectively represent the information flow within cyclic layers. By leveraging Zero Attention, we enable adaptive looping, allowing the model to dynamically determine the number of reasoning steps based on task complexity.

% Our approach is versatile, demonstrating effectiveness not only in training models from scratch but also in fine-tuning pre-trained models. The results confirm that Zero Token Cycling enhances performance while maintaining computational efficiency, making it a promising solution for optimizing large language models under constrained resource budgets.

% \section{Conclusion}
% We have presented a novel method for cyclic, parameter-sharing Transformers that addresses which layers to reuse, how to manage shared parameters, and when to stop iterating. By separating the head and tail layers from the cyclical process and introducing a learnable Zero Token in each attention block, our approach enables dynamic allocation of computation across loops. Extensive experiments on both from-scratch training and fine-tuning scenarios validate that Zero-Token Cycling significantly enhances performance under fixed parameter budgets, paving the way for more efficient large language models.

\section{Conclusion}

% We propose {Zero-Token Cycling}, a novel approach for cyclic, parameter-sharing Transformers that addresses \textit{which layers to reuse, how to manage shared parameters, and when to stop iterating}. By introducing a {learnable Zero Token} in attention blocks and decoupling head and tail layers from the cyclic process, our method enables {adaptive computation}, dynamically adjusting reasoning steps based on task complexity.
% Our approach is {effective in both training from scratch and fine-tuning pre-trained models}, consistently improving performance while maintaining computational efficiency. The Zero Token mechanism facilitates parameter-efficient reasoning and serves as a stopping criterion for adaptive inference, reducing redundant computation without sacrificing accuracy.

% These findings demonstrate the potential of {dynamic parameter-sharing strategies} in large-scale language models, paving the way for more efficient transformer-based architectures in resource-constrained settings.


We have presented {Zero-Token Transformer}, a parameter-sharing strategy for Transformers that comprehensively addresses the core questions of {which} layers to reuse, {how} to manage shared parameters, and {when} to stop iterating. By decoupling head and tail layers from the cyclic process and introducing a learnable Zero Token in each attention block, our approach enables {adaptive computation}, dynamically adjusting the number of reasoning steps based on the modelâ€™s confidence.
Our experiments show that this method is effective for both {training from scratch} and {fine-tuning pre-trained models}, consistently improving performance without increasing the overall parameter budget. The Zero Token mechanism not only facilitates parameter-efficient reasoning but also provides a straightforward criterion for early exiting, thereby reducing redundant computation while preserving accuracy.

These findings highlight the potential of {dynamic parameter-sharing strategies} in large-scale language models, particularly in resource-constrained scenarios. We believe that further exploration of zero-token prompts, gating mechanisms, and cyclic architectures will lead to increasingly efficient and adaptive Transformer-based designs in the future.


% We have presented the {Zero-Token Transformer}, a unified parameter-sharing strategy for Transformers that resolves {which} layers to reuse, {how} to manage them, and {when} to stop iterating. By decoupling head and tail layers from cycling and introducing a learnable Zero Token in each attention block, our method enables {adaptive computation} that dynamically adjusts reasoning steps based on confidence.
% Experiments confirm its effectiveness in both {training from scratch} and {fine-tuning pre-trained models}, consistently improving performance under the same parameter budget. The Zero Token mechanism not only supports parameter-efficient reasoning but also offers a clear criterion for early exit, minimizing redundant computation without harming accuracy.
% These results demonstrate the promise of {dynamic parameter-sharing} in large language models, especially where resources are limited. We believe further exploration of zero-token prompts and cyclic architectures will yield even more efficient Transformer-based designs.
