%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Experimental Setup}
\label{es}

\subsection{Evaluation Details}

To assess the effectiveness of our proposed method, we evaluate models on a diverse set of well-established NLP benchmarks. These benchmarks span four key reasoning tasks: \textbf{commonsense physical reasoning, multiple-choice question answering, long-term context recall, and natural language inference}. For all datasets, we report \textbf{accuracy (ACC)} as the primary evaluation metric.

\subsubsection{Reasoning: PIQA}
\textbf{Dataset:} The \textit{Physical Interaction Question Answering (PIQA)} dataset~\cite{bisk2020piqa} evaluates a model’s ability to reason about \textbf{everyday physical interactions}. It consists of multiple-choice questions that require an understanding of how objects and tools function in real-world scenarios.

\textbf{Task Objective:} Given a short natural language query, the model must select the most plausible solution from two candidate answers. This task assesses the model's ability to infer \textbf{practical physical knowledge} beyond simple memorization.

\textbf{Evaluation Metric:} Accuracy (ACC), measuring the proportion of correctly predicted answers.

\subsubsection{Multiple-Choice Question Answering: ARC Challenge and ARC Easy}
\textbf{Dataset:} The \textit{AI2 Reasoning Challenge (ARC)}~\cite{clark2018think} is a standardized multiple-choice QA benchmark designed to evaluate a model’s ability to answer \textbf{science-related questions}. It consists of two subsets:
\begin{itemize}
    \item \textbf{ARC Challenge:} A more difficult subset requiring complex reasoning and deeper knowledge retrieval.
    \item \textbf{ARC Easy:} A simpler subset containing factual questions that can often be answered with surface-level understanding.
\end{itemize}

\textbf{Task Objective:} The model is provided with a science-related question and four answer choices, from which it must select the correct one. The dataset requires a combination of \textbf{commonsense reasoning, logical inference, and scientific knowledge} to achieve high performance.

\textbf{Evaluation Metric:} Accuracy (ACC), computed as the percentage of correctly answered questions.

\subsubsection{Long-Term Context Recall: LAMBADA}
\textbf{Dataset:} The \textit{LAMBADA} dataset~\cite{paperno2016lambada} is specifically designed to assess a model’s capability for \textbf{long-range context comprehension}. Unlike standard language modeling tasks, LAMBADA requires a model to retain and process information over an extended passage to predict a crucial missing word.

\textbf{Task Objective:} Given a \textbf{long contextual passage}, the model must predict the  final missing word  of the last sentence. The difficulty arises from the fact that the target word is nearly impossible to guess without understanding the full passage.

\textbf{Evaluation Metric:} Accuracy (ACC), where a prediction is considered correct if the \textbf{entire target word} matches the ground truth exactly.

\subsubsection{Natural Language Inference: HellaSwag}
\textbf{Dataset:} The \textit{HellaSwag} dataset~\cite{zellers2019hellaswag} is an advanced benchmark designed to evaluate \textbf{commonsense inference and story continuation}. It builds on the SWAG dataset by incorporating adversarial filtering, making it more challenging for models to rely on surface-level heuristics.

\textbf{Task Objective:} Given an  incomplete story or event description , the model must select the most  logical next step  from four possible continuations. This requires strong  contextual understanding  and the ability to anticipate  real-world event progressions .

\textbf{Evaluation Metric:} Accuracy (ACC), measuring how often the model correctly predicts the most plausible continuation.

\subsection{Training and Fine-Tuning Settings}

In this section, we describe the training settings for both \textbf{pre-training from scratch} and \textbf{fine-tuning of pre-trained models}. The \textbf{fine-tuning stage} is required for all models before final evaluation, while models trained from scratch undergo \textbf{both pre-training and fine-tuning}. The fine-tuning hyperparameters are kept consistent across both settings.

\subsubsection{Pre-Training from Scratch}

For models trained from scratch, we first conduct pre-training on the \textit{C4 English dataset}~\cite{raffel2020exploring}. The pre-training process follows these configurations:

\paragraph{Pre-Training Protocol}
\begin{itemize}
    \item \textbf{Dataset:} The \textit{C4 (Colossal Clean Crawled Corpus)} English subset.
    \item \textbf{Computing Resources:} We utilize an \texttt{A800 GPU cluster} for training.
    \item \textbf{Batch Size per GPU:} 80, with \textbf{gradient accumulation} to maintain a global batch size of 256.
    \item \textbf{Training Steps:} The model is trained for a total of \textbf{10B tokens}.
    \item \textbf{Optimizer:} AdamW~\cite{loshchilov2018decoupled} with a weight decay of 0.01.
    \item \textbf{Learning Rate:} A linear warmup is applied for the first 1\% of total steps, followed by a cosine decay schedule.
    \item \textbf{Precision:} Training is performed in \textbf{half-precision (FP16)} to optimize memory efficiency.
\end{itemize}

After pre-training, the models proceed to the \textbf{fine-tuning} stage before being evaluated on downstream tasks.

\subsubsection{Fine-Tuning Settings}

Before evaluating on the test datasets, we fine-tune our models using the corresponding training sets. \textbf{For pre-trained models, only fine-tuning is performed}, while models trained from scratch undergo \textbf{both pre-training and fine-tuning}. The fine-tuning process is conducted under the same computational settings as pre-training.

\paragraph{Fine-Tuning Protocol}
\begin{itemize}
    \item \textbf{Fine-Tuning Epochs:} Each dataset is fine-tuned for \textbf{3 epochs}.
    \item \textbf{Batch Size per GPU:} 20, with \textbf{gradient accumulation} ensuring an effective batch size of 80.
    \item \textbf{Optimizer:} AdamW with a 0.01 weight decay.
    \item \textbf{Learning Rate:} The default Hugging Face \texttt{Trainer} API learning rate is used.
    \item \textbf{Prompt Engineering:} We utilize prompt templates from \texttt{promptsource} to better adapt models to the task format.
    \item \textbf{Computing Resources:} The same \texttt{A800 GPU cluster} is used as in pre-training.
    \item \textbf{Training Framework:} Fine-tuning is implemented with Hugging Face’s \texttt{Trainer API}.
\end{itemize}

\paragraph{Dataset-Specific Fine-Tuning Details}
Fine-tuning is performed on the following datasets before model evaluation. The details of each dataset, including the number of training examples, are presented in \cref{tab:finetune}.

\begin{table}[h]
\centering
\small
\caption{Fine-tuning settings for each dataset, including the number of training epochs and dataset sizes.}
\label{tab:finetune}
\begin{tabular}{l|c|c|c}
\hline
\textbf{Dataset} & \textbf{Epochs} & \textbf{Training Size} & \textbf{Validation Size} \\ \hline
PIQA            & 3               & 16,000                 & 1,838                     \\
ARC Challenge   & 3               & 1,119                  & 1,172                     \\
ARC Easy        & 3               & 2,251                  & 2,376                     \\
LAMBADA         & 3               & 4,869                  & 4,869                     \\
HellaSwag       & 3               & 39,905                 & 10,042                    \\ \hline
\end{tabular}
\end{table}

\subsection{Early-Exit Training Settings}

To ensure \textbf{effective intermediate predictions} when early-exit mechanisms are applied, we implement \textbf{additional training for intermediate classifier heads}. This helps maintain meaningful intermediate outputs, preventing degradation in performance due to premature exits.

\subsubsection{Classifier Placement}
\begin{itemize}
    \item \textbf{Simple Cycling:} The classification head is placed \textbf{only at the final output layer}.
    \item \textbf{Head-Tail Separation:} The classification head is placed at \textbf{both the final layer and the last shared layer before cycling begins}.
\end{itemize}

\subsubsection{Training Strategy for Early-Exit Models}

To optimize models for early exits, we introduce additional supervision at intermediate layers. Instead of relying solely on the final output, we ensure that \textbf{multiple exit points} are trained effectively.

\begin{itemize}
    \item \textbf{Intermediate Supervision:} The model is trained to produce meaningful predictions at designated early-exit points.
    \item \textbf{Exit Point Optimization:} Models with \textbf{multiple cycling blocks} undergo training to align their intermediate outputs with final predictions, improving robustness across different exit depths.
    \item \textbf{Gradual Refinement:} The early-exit heads are optimized using the same fine-tuning data, ensuring consistency across all prediction layers.
\end{itemize}

By integrating these early-exit classifiers and \textbf{fine-tuning them separately}, we ensure that models can \textbf{gracefully exit at earlier layers without sacrificing predictive accuracy}. This design allows our method to maintain efficiency while preserving strong performance across different computational budgets.


\section{More Experimental Results}
\label{sec:more_experiments}

To further analyze the effectiveness of our method, we present additional  adaptive reasoning loop  and ablation experiments in \cref{tab:ad_detail} and \cref{tab:adaptive_detail}.


\subsection{Analysis of Adaptive Reasoning Loops}
\input{sec/table/table_adaptive_detail}
\cref{tab:adaptive_detail} presents results on our \textbf{adaptive reasoning loop mechanism}, where the model dynamically determines the number of iterations based on the \textbf{Zero Attention threshold ($P$)}.

\textbf{Key observations:}
\begin{itemize}
    \item \textbf{Low threshold ($P = 0.2$)} results in early exits (1.58 cycles) but slightly lower accuracy (\textbf{32.42\%}).
    \item \textbf{Balanced performance at $P = 0.5$}: The model averages \textbf{3.31 cycles} and reaches \textbf{33.00\% accuracy}, achieving strong efficiency gains.
    \item \textbf{Higher thresholds ($P = 0.7$)} lead to more computation (3.74 cycles) and slight accuracy gains (\textbf{33.08\%}), but with diminishing returns.
    \item \textbf{Full computation ($P = 1$)} does not significantly outperform adaptive strategies, confirming that early exit can maintain performance.
\end{itemize}

These results demonstrate that adaptive early exit strategies reduce computation while maintaining accuracy, with \textbf{$P = 0.5$} being the most efficient trade-off.

\subsection{Ablation Study on Zero Token and Gating Mechanism}
\input{sec/table/table_ab_detail}
\cref{tab:ad_detail} provides a detailed breakdown of our ablation study, evaluating the impact of the \textbf{Zero Token (ZT)} mechanism and the \textbf{gating unit (Gate)} in the feed-forward network (FNN). The results highlight the individual and combined contributions of these components.

\textbf{Key findings:}
\begin{itemize}
    \item The full model (\textbf{ZT + Gate}) achieves the highest accuracy (\textbf{32.79\%}), demonstrating that both components are essential.
    \item Removing the \textbf{Gate} leads to a slight performance drop (\textbf{32.68\%}), suggesting that gating helps refine reasoning.
    \item Removing the \textbf{Zero Token} reduces accuracy further (\textbf{32.63\%}), indicating its role in guiding iterative reasoning.
    \item The baseline model (without ZT and Gate) achieves the lowest accuracy (\textbf{32.62\%}), confirming that both components contribute positively.
\end{itemize}

These results validate that both \textbf{Zero Token and Gate} are essential for maximizing model efficiency and reasoning quality.





