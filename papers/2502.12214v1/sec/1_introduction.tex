\section{Introduction}
n recent years, it has been widely acknowledged that the performance of Large Language Models (LLMs) improves with an increasing number of parameters~\cite{rae2021scaling,rosenfeld2019constructive}. Consequently, scaling up parameter counts has become a common strategy for enhancing model performance~\cite{leviathan2023fast,xu2024fwdllm,pope2023efficiently}. However, this approach is often infeasible for users with limited computational resources. A critical challenge, therefore, is to achieve better performance under a \emph{fixed parameter budget}~\cite{zhou2024survey}.

A variety of model compression techniques, including quantization~\cite{lin2024awq,liu2023llm}, pruning~\cite{ma2023llm,sun2023simple}, and distillation~\cite{latif2023knowledge,shum2024first}, have been proposed to shrink large models to smaller ones. In parallel, another line of research has investigated ways to leverage additional computation within a fixed number of parameters, thereby unlocking deeper or more iterative reasoning~\cite{dehghani2018universal, lan2019albert}. A common strategy here is {parameter sharing}, where model layers reuse the same parameters across multiple computational cycles, sometimes referred to as “parameter cycling.” Rather than maintaining a separate set of parameters for each layer, models recurrently apply a compact parameter set, reducing memory requirements and potentially increasing depth of reasoning.

Despite its potential, parameter cycling raises three core challenges: (1) \textbf{Which} parameters should be reused across iterative cycles?
 (2) \textbf{How} can these shared parameters be managed to avoid functional conflicts and performance degradation?
(3) \textbf{When} should the model decide that no further reasoning is necessary, thus saving computational cost without truncating essential inference steps prematurely? 


Existing works partially address one or two of these questions. For example, Solar~\cite{kim2023solar} reuses parameters from intermediate layers (\emph{which} parameters), while the Relaxed Recursive Transformer~\cite{bae2024relaxed} focuses on how to manage the recurring layer through LoRA~\cite{hu2021lora}. Palbert~\cite{balagansky2022palbert}, combining PonderNet~\cite{banino2021pondernet} with ALBERT, explores \emph{when} to stop via a dynamic pondering mechanism. However, none of these approaches provide a comprehensive solution that systematically addresses all three dimensions—\emph{which} parameters to cycle, \emph{how} to apply them, and \emph{when} to terminate reasoning.

In this paper, we propose a \emph{Zero Token Transformer (ZTT)} that systematically tackles these three challenges. Our approach is applicable to both training from scratch and fine-tuning existing pretrained models. Specifically:

\begin{itemize}
    \item \textbf{Head-Tail Decoupled Cyclic Architecture.} To handle the question of \emph{which} parameters to share, we decouple the first (head) and last (tail) layers from the parameter-sharing mechanism, because their specialized functions (encoding raw inputs and mapping representations to outputs) differ significantly from those of intermediate layers. Only the intermediate layers are recurrently used in a cyclic manner, improving efficiency while preserving essential input and output transformations.

    \item \textbf{Zero-Token Mechanism.} To address \emph{how} to manage shared parameters effectively, we introduce a novel {Zero-Token Mechanism}. Each intermediate layer retrieves a Zero-Token (with a trainable key and zero-valued representation) from a Zero-Token Pool and processes it alongside regular tokens. The attention scores toward the Zero-Token act as a guide for layer-specific computations, helping the model determine the extent of “reuse vs. new reasoning” at each cyclic iteration. This design mitigates potential conflicts that arise when reusing the same parameters multiple times.

    \item \textbf{A Dynamic Mechanism for Determining the Number of Cyclic Iterations.} Finally, to address \emph{when} to stop, we employ an early-exit style mechanism driven by the Zero-Token’s attention scores. When the attention to the Zero-Token surpasses a threshold, the model infers that further computation in subsequent cycles is unlikely to yield additional benefits and exits accordingly—striking a balance between computational efficiency and preserving accuracy.
\end{itemize}



The key contributions of this work can be summarized as follows:  
\begin{enumerate}  
    \item We propose a structured \emph{parameter cycling} approach that holistically tackles the “what, how, when” challenges of layer reuse under tight parameter budgets.
    
    \item We demonstrate that decoupling head and tail layers (while cycling among intermediate layers) yields both better reasoning depth and computational efficiency.
    
    \item We introduce the \emph{Zero-Token Mechanism}, allowing for dynamic control of layer-specific computation and enabling an effective early-exit strategy.
    
    \item 
    We demonstrate that our approach enhances performance in both training from scratch and fine-tuning existing large language models, highlighting its practical applicability to real-world deployments.

    % We show that fine-tuning existing large language models with our method significantly improves performance, illustrating its practical applicability to real-world deployment.  
\end{enumerate}  
