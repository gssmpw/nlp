\section{Conclusion}

Next-generation AI hardware aims at mitigating the Von Neumann bottleneck, thus reducing the cost to move data from memory to core and maximizing the energy efficiency. Systolic arrays are promising architectures in this direction, because they provide interconnected processing elements for local data reuse. Convolutional Neural Networks can benefit from systolic arrays to manage multi-dimensional ifmaps and filters properly.

This paper presents 3D-TrIM: a systolic array for energy- and area-efficient convolutions. Several 2D slices interact with few Input Recycling Buffers to eliminate the memory access overhead at the ifmap level and to enable buffer sharing. Each buffer accommodates shift registers and few extra shadow registers. An architecture of 576 PEs is implemented on commercial 22 nm technology and guarantees an energy efficiency of 4.54 TOPS/W and an area efficiency of 4.47 TOPS/mm$^2$. In addition, 3D-TrIM outperforms TrIM by up to $3.37\times$ in terms of operations per memory access on VGG-16 and AlexNet CNNs.