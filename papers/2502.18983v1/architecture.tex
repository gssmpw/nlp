\section{The 3D-TrIM Architecture}

\input{Figures/Figure2}
The top-level architecture of 3D-TrIM is shown in Fig.~\ref{3D_SA_TOP}. $P_I$ parallel cores perform $P_I \times P_O$ convolutions between $P_I$ ifmaps and $P_I \times P_O$ kernels, with $P_I$ being the number of ifmaps processed in parallel, and $P_O$ being the number of 3D filters processed in parallel (or, equivalently, the number of ofmaps generated in parallel). Each core hosts $P_O$ slices coping with $K \times K$ convolutions. To maximize ifmap utilization on-chip, each core includes an Input Recycling Buffer (IRB). Slice 0 supplies the IRB with ifmap activations. When required, those activations are broadcast to the other slices of the core. In this way, ifmap activations are read once from the memory and then reused locally as long as required. $P_O$ adder trees are placed straight after the cores in order to accumulate the temporary psums. A control logic provides configuration and enable signals to buffers and processing units.

\subsection{The Slice}

\input{Figures/Figure3}
Each slice consists of $K \times K$ PEs and an adder tree. Figure~\ref{SA_PE} shows the example of a slice when $K=3$. The generic PE, other than hosting a multiply-accumulation unit, contains registers to store the external activation, weight, product, psum, and the activation to be moved to the left-adjacent PE/IRB in the next cycle. Two multiplexers select the direction of the current activation, which can be either supplied by the memory, or by the PE placed on the right side, or reused from the IRB.
The dataflow is the following:
\begin{itemize}
    \item Weights are fetched from the memory, loaded vertically into the PEs placed at the top boundary of the array, and then moved from top to bottom. At the completion, these weights are kept stationary at the PE level.
    \item Activations are fetched from the memory and supplied vertically to each PE. Then, these activations are moved from right to left. Once activations reach the leftmost column of the array, these can be either forwarded to the IRB (in the case of Slice 0) or discarded. Finally, activations coming from the IRB are forwarded to the PEs for further reuse. 
    \item Psums move from top to bottom over consecutive cycles. Bottom psums are eventually summed up through an adder tree.
\end{itemize}

\subsection{Input Recycling Buffer}

\input{Figures/Figure4}
The IRB assists the slices to provide the set of activations that need to be reused over different cycles. It consists of (i) $K-1$ shift registers (each accommodating $W_I-K-1$ registers, where $W_I$ is the ifmap width); (ii) $(K-1) \times (K-1)$ shadow registers; (iii) multiplexers. Figure~\ref{IRB} depicts the architecture of the IRB when $K=3$. For each sliding window covering three ifmap rows, Shift Register 0 keeps track of the second row under processing, while Shift Register 1 manages the third row. Both shift registers do not manage the last two activations of each ifmap row, for which shadow registers are responsible.
With the aim to support different ifmap sizes, shift registers have been made reconfigurable. To achieve this, internal stages of the shift registers are connected to a multiplexing logic that routes the correct set of activations to the slices. The other multiplexers reported in Fig.~\ref{IRB} control whether activations are provided by shift registers or by shadow registers.

\input{Figures/Figure5}
To facilitate the understanding of the IRB, let consider the case reported in Fig.~\ref{IRB_ex}. The example  illustrates the convolution between an $8 \times 8$ ifmap and a $3 \times 3$ kernel, with a focus on the activity between cycle 6 and cycle 13. For each cycle, the activations being processed by the PEs are reported, as well as the activations stored in the shift registers and in the shadow registers. Throughout the cycles, activations can be supplied to PEs externally (activations in blue), horizontally (activations in orange), diagonally. The IRB assists the systolic array with diagonal movements: shift registers (activations in green) and shadow registers (activations in yellow) can feed the PEs. For each ifmap row to be reused, shift registers host the majority of activations, except for the end-of-row ones. For example, at cycle 7, activations 9, 10, 11 are reused by PE$_{0,0}$, PE$_{0,1}$, PE$_{0,2}$, respectively, through shift registers. Conversely, shadow registers store end-of-row activations and constitute the key novelty of the IRB proposed in 3D-TrIM. In detail, between cycles 6 to 8, the activations 15, 16, 23, 24 are stored in shadow registers for reuse in the next end-of-row sliding windows. Such activations are restored in cycles 11-13: activations 15, 16 are supplied to PE$_{0,2}$, while activations 23, 24 are supplied to PE$_{1,2}$. Activations 23, 24 are also shifted between shadow registers to allow further reuse in the next end-of-row sliding windows (not shown in Fig.~\ref{IRB_ex}).

\subsection{Adder Trees and Control Logic}

With reference to the CNN workload, each core processes one ifmap with a set of kernels belonging to different filters. In order to generate an ofmap, the psums generated by the different cores need to be spatially accumulated. For this reason, $P_O$ adder trees are interfaced with the cores, and each adder tree sums up $P_I$ operands. For example, based on Fig.~\ref{3D_SA_TOP}, Adder Tree 0 manages psums generated by each Slice 0 in the different cores. Similarly, Adder Tree 1 is responsible for psums provided by each Slice 1. And so on until Adder Tree $P_O-1$.

The control logic orchestrates the operations of 3D-TrIM over time. At the slice level, the control logic manages the selection signals of the multiplexers to load the activations from the correct direction. The proper subset of activations from shift registers and the activity of shadow registers are handled at the IRB level.