\section{Introduction}

\IEEEPARstart{N}{owadays}, hardware architectures for Artificial Intelligence (AI) require vast amount of data to deliver tasks effectively. Convolutional Neural Networks (CNNs) manage multi-dimensional input feature maps (ifmaps) and filters to retrieve meaningful patterns from inputs\cite{Sze_17,Li_22}. Ifmaps and filters are initially stored in an external memory and then loaded on-chip to the processing core. However, the data transfer between memory and core requires noticeable energy: one memory access dissipates from two to three orders of magnitude higher energy than a single computation\cite{Horowitz_14}. Therefore, hardware architectures that reduce the number of memory accesses are desirable\cite{Capra_20}.

Systolic Arrays (SAs) are spatial architectures that maximize data utilization on-chip, thus accessing memory less frequently\cite{Xu_23}. A multitude of Processing Elements (PEs) execute multiply-accumulations between ifmaps and filters and move data through custom interconnections. In addition, each PE can hold one or more types of data stationary to further reduce the energy consumption. For example, weight-stationary SAs move ifmaps and partial sums (psums) in orthogonal directions, while weights are stored and kept fixed at the PE level\cite{Wu_22}. When SAs process convolutions, PEs must consider the raster scan order of the ifmap activations. Indeed, 2D convolutions move $K \times K$ kernels over the ifmaps, from left to right and from top to bottom. Each movement configures a sliding window of activations, in turn belonging to consecutive ifmap rows. Adjacent windows share activations, thus data redundancy must be managed carefully. 

\input{Figures/Figure1}
SAs for CNNs can be classified into two groups: SAs using General Matrix Multiplication (GeMM-based SAs) and SAs directly dealing with the convolution workflow (Conv-based SAs). GeMM-based SAs\cite{Jouppi_17, Fornt_23, Wu_24, Feng_24} convert convolutions into matrix-multiplications and ifmaps need to be reorganized at the memory level, introducing data redundancy. Thus, GeMM-based SAs require larger memories and more memory accesses. Conv-based SAs\cite{Chen_17,Zhang_24,Sestito_24_1,Sestito_24_2} exploit custom dataflows to avoid redundancy at the memory level. In some architectures\cite{Chen_17,Zhang_24}, sliding windows are split in rows and reused at the PE level through scratch-pads (row-stationary dataflow\cite{Chen_16}). Despite this approach reduces the number of memory accesses, the continuous activity of scratch-pads degrades power, energy and area. To address this issue, authors in\cite{Sestito_24_1,Sestito_24_2} have recently proposed a local triangular dataflow, named TrIM, to maximize ifmap utilization without using scratch-pads at the PE level. After retrieving data vertically from the memory, activations are locally moved through horizontal and diagonal links using $K \times K$ PEs and shift register buffers to assist the raster scan order dictated by the convolution workflow. The dataflow has been validated in a hardware architecture where multiple TrIM slices are accommodated in cores to accelerate CNN workloads. However, the proposed architecture suffers from memory access overhead, since end-of-row ifmap activations must be read more than once from the memory. Figure~\ref{MemAccOv_TrIM} illustrates this overhead, considering convolutions on ifmaps having different sizes and $3 \times 3$ kernels. Notably, the overhead mainly affects small ifmaps, which are common in deep CNNs such as VGG-16\cite{Simonyan_15} and AlexNet\cite{Krizhevsky_12}. As a further drawback, each TrIM slice\cite{Sestito_24_2} requires independent shift register buffers due to the specific ifmap orientation. No buffer sharing is implemented at the core level, thus degrading area, power and energy.

To address the referred problems, we propose an upgraded version, named 3D-TrIM. The architecture proposed here introduces few shadow registers at the buffer level to reuse end-of-row activations without further access to the external memory. Furthermore, 3D-TrIM uses a different architectural orientation to reduce the number of shift register buffers: each core processes one ifmap through different filters, thus needing a single local buffer to store activations for reuse. Finally, the orientation change affects the additional adder trees for spatial accumulations, which are now shared among the different cores, resulting in a 3D architecture. 

The specific contributions of this work are:
\begin{itemize}
    \item A 3D systolic array architecture for CNNs to maximize ifmap utilization. The proposed architecture exploits local buffer sharing for energy and area efficiency. 
    \item The use of efficient shadow registers to nullify the memory access overhead at the ifmap level. These registers are part of the local buffer resources.
    \item An architecture of 576 PEs is implemented on commercial 22 nm technology. The peak throughput is 1.15 TOPS, with a 0.26 mm$^2$ area occupation, and 0.25 W power dissipation. 3D-TrIM outperforms TrIM in terms of operations per memory access up to $3.37\times$ on VGG-16 and AlexNet CNNs.
\end{itemize}

The rest of the paper is organized as follows: Section II introduces the architecture of 3D-TrIM. Implementation and characterization results are presented in Section III. Finally, Section IV concludes the paper.