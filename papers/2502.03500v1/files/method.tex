\section{Method}

In this work, we address the challenge of developing an efficient method that minimizes average distortion under a perfect perceptual quality constraint as given in \eqref{distortion_perception}.  By ``efficient'', we refer to the model's memory usage and latency. Specifically, we are given a dataset  $\mathcal{S}\triangleq\set{\vectorsym{y}_i,\vectorsym{x}_i}$, consisting of pairs of images where $\vectorsym{y}_i$ represents the low-quality (LQ) images and $\vectorsym{x}_i$ represents the high-quality (HQ) images. Our objective is to develop a neural network that can solve Problem \eqref{distortion_perception} efficiently. To achieve this, we propose a method based on the problem solution suggested in \citet{freirich2021a}, which minimizes the average distortion while maintaining a perfect perceptual quality ($P=0$). 


Inspired by this solution, we suggest a two-stage pipeline that operates in \emph{latent space}. First, we apply a latent MMSE estimator on the LQ input image which reduces the distortion error in the latent space. Second, we utilize a latent consistency flow model which samples from the conditional posterior distribution of latent representation of $\vectorsym{x}$ given results of the latent MMSE Estimator. The entire process is performed in latent space, which enables efficient inference and significantly reduces the computational costs associated with processing high-resolution images. Moreover, we suggest a hardware-friendly architecture consisting of only convolutional layers (see Appendix~\ref{apx:nn}). This architecture is highly optimized for most hardware accelerators, leading to reduced model size and latency, making it suitable for resource-constrained edge devices.

An overview of the suggested flow is presented in Figure~\ref{scheme}. The latent MMSE and the consistency flow are explained in Subsections ~\ref{lmmse} and \ref{lcfm}, respectively, and the training and inference procedures are described in Subsection~\ref{infer}.



\subsection{Latent MMSE}\label{lmmse}
Here, we describe the latent MMSE estimator, which takes the latent representation of the LQ image and restores it to closely match the latent representation of the HQ image in terms of $\ell_2$. Specifically, let $\vectorsym{x}$ and $\vectorsym{y}$ be a pair of HQ and LQ images, respectively,  $\mathcal{E}_{\vectorsym{\omega}}$ be a pre-trained encoder (parameterized by $\vectorsym{\omega}$) that projects an image to the latent space, and $g_{{\phi}}$ be the latent MMSE estimator (parameterized by $\vectorsym{\phi}$). The objective is to minimize the $\ell_2$ difference between the latent representations of the LQ and HQ images, which is given by:
\begin{align}\label{l2_loss}
 \mathcal{L}_{2}\brackets{\vectorsym{\phi},\vectorsym{\omega}}&= \expectation{\norm{g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{y}})-\mathcal{E}\brackets{\vectorsym{x}}}_2^2}{\vectorsym{x},\vectorsym{y}},
\end{align}


where $\mathcal{E}$ is a pre-trained HQ image encoder. During optimization of \eqref{l2_loss}, this encoder remains static, while the LQ image encoder $\mathcal{E}_{\vectorsym{\omega}}$ is trained in coordination with the Latent MMSE. Since the LQ encoder is pre-trained with HQ images, its effectiveness may decrease when faced with unknown degradations such as colorization or inpainting, unless it undergoes fine-tuning. 
% As the LQ encoder is pre-trained with HQ images, its effectiveness may decrease when faced with unknown degradations like colorization or inpainting, unless it undergoes fine-tuning.
As can be seen in Table~\ref{table:encoder_short}, fine-tuning the LQ encoder allows adaptation to unseen degradations. 
% As evidenced by Table~\ref{table:encoder_short}, fine-tuning the LQ encoder facilitates adjustments to novel degradation scenarios.


\subsection{Latent Consistency Flow Matching}\label{lcfm}
We introduce the latent consistency flow matching (LCFM) as a combination of consistency flow matching \cite{yang2024consistencyfm} and latent flow matching \cite{dao2023flow}. LCFM approximates optimal transport between the latent representation of the source and target distributions. It aims to reduce the number of NFEs, which is crucial for edge device runtime, as well as the cost of each NFE. In this work, we wish to sample from the posterior distribution of the HQ images given the results of the Latent MMSE estimator. To achieve this, we define the target distribution $\vectorsym{z}_1=\mathcal{E}\brackets{\vectorsym{x}}$, representing the latent representation of the HQ image. The source distribution is then defined as the output of the Latent MMSE estimator from the first stage as follows:
$$\z_0=g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{y}})+\vectorsym{\epsilon},$$
where $\vectorsym{\epsilon}\sim\mathcal{N}(0,\sigma^2_{s}I)$ is addtive white Gaussian noise with standard deviation $\sigma_{s}$. Adding such noise is critical when LQ and HQ images lie on low and high-dimensional manifolds \cite{AlbergoV23}. Then, the optimal transport conditional flow from source to target distribution as suggested by \citet{lipman2023flow} is given by:
\begin{align}\label{flow}
\vectorsym{z}_t=t\vectorsym{z}_{1} + (1-(1-\sigma_{min})t)\vectorsym{z}_{0},
\end{align}
where $t\in [0,1]$ is the time variable and $\sigma_{min}$ is an hyperparameter. 


To sample from the latent target distribution $\vectorsym{z}_1$, we wish to obtain a vector field $\vectorsym{v}_{\vectorsym{\theta}}$ that would drive the direction of the linear path flowing from $\z_0$ to $\z_1$. To obtain such $\vectorsym{v}_{\vectorsym{\theta}}\brackets{\vectorsym{z}_t,t}$ that allows effective inference, we suggest using multi-segment consistency loss \cite{yang2024consistencyfm} in latent space. Specifically, given $K$ segments, the time interval $[0,1]$ is divided into $\set{[\frac{i}{K},\frac{i+1}{K}]}_{i=0}^{K-1}$. Then, the consistency loss of a segment is defined as
\begin{align}\label{lcfm_loss}
&\mathcal{L}_{s}\brackets{\vectorsym{\theta},t}
=\\
&\lambda_{i}\expectation{\Delta f^{(i)}_{\vectorsym{\theta}}\brackets{\vectorsym{z}_t,\vectorsym{z}_{t+\Delta t},t}+\alpha\Delta v^{(i)}_{\vectorsym{\theta}}\brackets{\vectorsym{z}_t,\vectorsym{z}_{t+\Delta t},t} }{\vectorsym{z}_{t},\vectorsym{z}_{t+\Delta t}}\nonumber
\end{align}
where, 
\begin{align}
    &\Delta v_{\vectorsym{\theta}}^{(i)}\brackets{\vectorsym{z}_t,\vectorsym{z}_{t+\Delta t},t}=\norm{\vectorsym{v}^{(i)}_{\vectorsym{\theta}}(\vectorsym{z}_t,t)-\vectorsym{v}^{(i)}_{\vectorsym{\theta}^{-}}(\vectorsym{z}_{t+\Delta t},t+\Delta t)}_2^2,\nonumber\\
    &\Delta{f}^{(i)}_{\p}\brackets{\vectorsym{z}_t,\vectorsym{z}_{t+\Delta t},t}=\norm{f^{(i)}_{\vectorsym{\theta}}(\vectorsym{z}_t,t)-f^{(i)}_{\vectorsym{\theta}^{-}}(\vectorsym{z}_{t+\Delta t},t+\Delta t)}_2^2,\nonumber\\
    &f^{(i)}_{\vectorsym{\theta}}(\vectorsym{z}_t,t) = \vectorsym{z}_t + \brackets{\frac{i+1}{K}-t}\vectorsym{v}^{(i)}_{\vectorsym{\theta}}(\vectorsym{z}_{t},t).\nonumber
\end{align}
Here, $i$ denotes the $i^{th}$ segment corresponding to time $t$ and $\Delta t$ and $\alpha$ are hyperparamters. $\vectorsym{v}^{(i)}_{\vectorsym{\theta}}(\vectorsym{z}_{t},t)$ is the vector field in the segment $i$ and $\vectorsym{\theta}^{-}$ denotes parameters without gradients. $\lambda_i$ is a positive weighting scalar for scaling different segments. Then, the LCFM loss is given by:
\begin{equation}
\mathcal{L}_{LCFM}\brackets{\vectorsym{\theta}}=\expectation{\mathcal{L}_{s}\brackets{\vectorsym{\theta},t}}{t},
\end{equation}
where $t\sim\mathbb{U}[0,1-\Delta t]$ is the uniform distribution. 


\subsection{Training and Inference procedures}\label{infer}
During training, we optimize \eqref{l2_loss} and \eqref{lcfm_loss} yielding trained parameters $\vectorsym{\omega}^{*}$, $\vectorsym{\phi}^{*}$ and $\vectorsym{\theta}^{*}$.
During inference, we project $\vectorsym{y}$ into a latent space using $\mathcal{E}_{\vectorsym{\omega}^{*}}$ and apply the latent MMSE estimator $g_{\vectorsym{\phi}^{*}}$. Similarly to the training, we add a Gaussian noise with the same standard deviation $\sigma_{s}$, utilize the optimized vector field $\vectorsym{v}_{\vectorsym{\theta}^{*}}$, and solve the ODE from \eqref{ode} using the forward Euler method with $M$ steps. Once we obtain HQ latent results, we apply a pre-trained decoder $\mathcal{D}$ to project back to the pixel space, yielding HQ images. Algorithm~\ref{alg} outlines the inference procedure.


\begin{algorithm}[t]
\caption{\name Inference}
\label{alg}
\begin{algorithmic}
\REQUIRE LQ image $\y$, number of Euler steps $M$, noise variance $\sigma_{s}^2$ 
\phase{Latent MMSE Estimator}
\vspace{-2ex}
\STATE $\z\gets \mathcal{E}_{\omega^{*}}(\vectorsym{y})$   %\COMMENT{Project Image to Latent Space}
\STATE $\z^* \gets g_{\phi^{*}}(\z) $ 

\STATE $\vectorsym{\epsilon}\sim\mathcal{N}(0,\sigma^2_{s}I)$
\STATE $\hat{\z}_0\gets \z^*+ \vectorsym{\epsilon}$
\phase{Solve ODE (Euler Method)}
\vspace{-2ex}
\STATE $\Delta=\frac{1}{M} $
\FOR{$i \gets 0,\Delta,...,1-\Delta$}
    \STATE$\hat{\z}_{i+\Delta} \gets \hat{\z}_{i} + \Delta \cdot v_{\vectorsym{\theta^{*}}}(\hat{\z}_{i},i)$
\ENDFOR
% \STATE $\Delta t=\frac{1}{M} $
% \FOR{$t \gets 0,\Delta t,...,1-\Delta t$}
%     \STATE$\hat{\z}_{t+\Delta t} \gets \hat{\z}_{t} + \Delta t \cdot v_{\vectorsym{\theta^{*}}}(\hat{\z}_{t},t)$
% \ENDFOR
\STATE $\vectorsym{\hat{x}} \gets \mathcal{D}(\hat{\z}_1)$ 
\RETURN $\vectorsym{\hat{x}}$
\end{algorithmic}
\end{algorithm}




