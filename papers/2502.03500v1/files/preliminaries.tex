\section{Preliminaries}

\subsection{Distortion and Perception}
The perception of image quality is a complex interplay between objective metrics and subjective human judgment. While objective measures like PSNR and SSIM are useful for quantifying distortion, they may not always correlate well with perceived image quality \cite{wang2004image}. Human observers are sensitive to artifacts and inconsistencies, even when they are subtle. Effective image restoration techniques must therefore aim to minimize both objective distortion and perceptual artifacts, ensuring that the restored image is both visually pleasing and faithful to the original content. Let's denote the high-quality and the corresponding low-quality images as $\vectorsym{x}$ and $\vectorsym{y}$, respectively, and the reconstructed image by $\vectorsym{\hat{x}}$. The distortion is usually evaluated by
$D = \expectation{\Delta\brackets{\vectorsym{x,\vectorsym{\hat{x}}}}}{p_{\vectorsym{x},\vectorsym{\hat{x}}}}$, where $\Delta\brackets{\vectorsym{x},\vectorsym{\hat{x}}}$ is a distance function and $p_{\vectorsym{x},\vectorsym{\hat{x}}}$ is the joint probability function of $\vectorsym{x}$ and $\vectorsym{\hat{x}}$. The distortion-perception trade-off \cite{blau2018perception} is defined by:
\begin{align}
    D(P) = \expectation{\Delta\brackets{\vectorsym{x,\vectorsym{\hat{x}}}}}{p_{\vectorsym{\hat{x}}|\vectorsym{y}}} \quad  s.t \quad d(p_{\hat{\vectorsym{x}}},p_{\vectorsym{x}}) \leq P,
\end{align}
where $P$ is some constant, and $d(\cdot)$ is some divergence between
probability measures. The goal is to find an estimator that achieves minimal average distortion under a perfect perceptual quality constraint ($P=0$). By setting the $\Delta$ function as the squared distance function, the trade-off can be formalized as:
\begin{align}\label{distortion_perception}
    \min_{p_{\hat{\vectorsym{x}}|\vectorsym{y}}} \expectation{\norm{\vectorsym{x}-\hat{\vectorsym{x}}}_2^2}{}\quad s.t \quad p_{\hat{\vectorsym{x}}}=p_{\vectorsym{x}}.
\end{align}
\citet{freirich2021a} proved that the optimal solution for Problem \ref{distortion_perception} is first to obtain the minimum mean square error (MMSE) estimator, $\vectorsym{x}^{*}=\expectation{\vectorsym{x}|\vectorsym{y}}{}$, and then sample from the optimal transport from $p_{\vectorsym{x}^{*}}$ to $p_{\vectorsym{x}}$.


\subsection{Consistency Flow Matching}
Consistency Flow Matching (CFM) advances flow-based generative models \cite{chen2018neural,lipman2023flow,liu2023flow} by enforcing consistency among learned transformations. This constraint ensures that the transformations produce similar results regardless of the starting point. By utilizing ``straight flows'' for simplified transformations and employing a multi-segment training strategy, CFM achieves enhanced sample quality and inference efficiency. Specifically, given $\vectorsym{x}$ as an observation in the data space $\mathbb{R}^d$, sampled from unknown data distribution, CFM first defines a vector field $\vectorsym{v}(\vectorsym{x}_t,t):\mathbb{R}^d\times[0,1]\xrightarrow{}\mathbb{R}^d$, that generates the trajectory $\vectorsym{x}_t \in \mathbb{R}^d$ through an ordinary differential equation (ODE):
\begin{align}\label{ode}
    \frac{d\vectorsym{x}_t}{dt} = \vectorsym{v}(\vectorsym{x}_t,t).
\end{align}
\citet{yang2024consistencyfm} suggests to train the vector field by a velocity consistency loss defined as:
\begin{align}\label{cfm_loss}
&\mathcal{L}_{CFM}\brackets{\vectorsym{\theta}}
=\\
&\mathbb{E}_{t}\expectation{\Delta f_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}+\alpha\Delta v_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t} }{\vectorsym{x}_{t},\vectorsym{x}_{t+\Delta t}}\nonumber
\end{align}
where, 
\begin{align}
    &\Delta v_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}=\norm{\vectorsym{v}_{\vectorsym{\theta}}(\vectorsym{x}_t,t)-\vectorsym{v}_{\vectorsym{\theta}^{-}}(\vectorsym{x}_{t+\Delta t},t+\Delta t)}_2^2,\nonumber\\
    &\Delta{f}_{\p}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}=\norm{f_{\vectorsym{\theta}}(\vectorsym{x}_t,t)-f_{\vectorsym{\theta}^{-}}(\vectorsym{x}_{t+\Delta t},t+\Delta t)}_2^2,\nonumber\\
    &f_{\vectorsym{\theta}}(\vectorsym{x}_t,t) = \vectorsym{x}_t + \brackets{1-t}\vectorsym{v}_{\vectorsym{\theta}}(\vectorsym{x}_{t},t),\nonumber
\end{align}
$t\sim\mathbb{U}[0,1-\Delta t]$ is the uniform distribution, $\Delta t$ is a small time interval and $\alpha$ is a positive scalar. $\vectorsym{\theta}^{-}$ denotes the running average of past values of $\vectorsym{\theta}$ using exponential moving average (EMA). 

To apply \eqref{cfm_loss}, we require to select a trajectory $\x_t$. 
Several options exist in the literature \cite{ho2020denoising,lipman2023flow,liu2023flow}. In this work, we use the optimal-transport conditional flow matching as proposed by \citet{lipman2023flow}, which enhances both the sampling speed and training stability. This trajectory is defined as
\begin{align}\label{fm_flow}
    \vectorsym{x}_t=t\vectorsym{x}_1+(1-(1-\sigma_{min})t)\vectorsym{x}_0,
\end{align}
where $\vectorsym{x}_0$ and $\vectorsym{x}_1$ are sampled from source and target distribution, respectively, and $\sigma_{min}$ is a hyperparameter.  

In inference, solving the ODE with the forward Euler method can produce high-quality results with significantly fewer steps (NFEs) than traditional Flow Matching (FM) techniques.

% \subsection{Consistency Flow Matching}
% Flow Matching (FM) is a generative modeling technique based on continuous normalizing flows \cite{chen2018neural}. Given $\vectorsym{x}$ as an observation in the data space $\mathbb{R}^d$, sampled from unknown data distribution, FM first defines a vector field $\vectorsym{v}(\vectorsym{x}_t,t):\mathbb{R}^d\times[0,1]\xrightarrow{}\mathbb{R}^d$, that generates the flow $\vectorsym{x}_t \in \mathbb{R}^d$ through an ordinary differential equation (ODE):
% \begin{align}\label{ode}
%     \frac{d\vectorsym{x}_t}{dt} = \vectorsym{v}(\vectorsym{x}_t,t).
% \end{align}
% In inference, solving the ODE with the forward Euler method can produce high-quality results with a few steps. To create simpler flows that are more stable to train, \cite{lipman2023flow} suggests optimal-transport conditional flow matching where the flow is defined by:
% % {\id{we should consider adding reference to OT-CFM Bengio's work (Improving and Generalizing Flow-Based Generative Models
% % with Minibatch Optimal Transport) + text on optimal transport from any source to target in which X0 and X1 are not independent}}
% \begin{align}\label{fm_flow}
%     \vectorsym{x}_t=t\vectorsym{x}_1+(1-(1-\sigma_{min})t)\vectorsym{x}_0,
% \end{align}
% where $\vectorsym{x}_0$ and $\vectorsym{x}_1$ are sampled from source and target distribution, respectively, and $\sigma_{min}$ is a hyperparameter with a small value.

% Consistency Flow Matching (CFM), as proposed by \cite{yang2024consistencyfm}, advances flow-based generative models by enforcing consistency among learned transformations. This constraint ensures that the transformations produce similar results regardless of the starting point. By utilizing 'straight flows' for simplified transformations and employing a multi-segment training strategy, CFM achieves enhanced sample quality and training efficiency. To achieve this, \cite{yang2024consistencyfm} suggests a velocity consistency loss defined as:
% % \begin{align}\label{cfm_loss}
% % \mathcal{L}_{CFM}\brackets{\vectorsym{\theta}}
% % &=\mathbb{E}_{t\sim\mathbb{U}}\mathbb{E}_{\vectorsym{x}_{t},\vectorsym{x}_{t+\Delta t}}\Delta f_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t} \\ \nonumber
% % &+\alpha\Delta v_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}
% % \end{align}
% \begin{align}\label{cfm_loss}
% &\mathcal{L}_{CFM}\brackets{\vectorsym{\theta}}
% =\\
% &\mathbb{E}_{t}\expectation{\Delta f_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}+\alpha\Delta v_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t} }{\vectorsym{x}_{t},\vectorsym{x}_{t+\Delta t}}\nonumber
% \end{align}
% where, 
% \begin{align}
%     &\Delta v_{\vectorsym{\theta}}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}=\norm{\vectorsym{v}_{\vectorsym{\theta}}(\vectorsym{x}_t,t)-\vectorsym{v}_{\vectorsym{\theta}^{-}}(\vectorsym{x}_{t+\Delta t},t+\Delta t)}_2^2,\nonumber\\
%     &\Delta{f}_{\p}\brackets{\vectorsym{x}_t,\vectorsym{x}_{t+\Delta t},t}=\norm{f_{\vectorsym{\theta}}(\vectorsym{x}_t,t)-f_{\vectorsym{\theta}^{-}}(\vectorsym{x}_{t+\Delta t},t+\Delta t)}_2^2,\nonumber\\
%     &f_{\vectorsym{\theta}}(\vectorsym{x}_t,t) = \vectorsym{x}_t + \brackets{1-t}\vectorsym{v}_{\vectorsym{\theta}}(\vectorsym{x}_{t},t).\nonumber
% \end{align}
% $t\sim\mathbb{U}[0,1-\Delta t]$ is the uniform distribution, $\Delta t$ is a small time interval and $\alpha$ is a positive scalar. $\vectorsym{\theta}^{-}$ denotes the running average of past values of $\vectorsym{\theta}$ using exponential moving average (EMA). Notably, CFM requires significantly fewer steps during inference than traditional flow-based techniques.