\section{Appendix}\label{appendix}
\textbf{Table of Contents:}
\begin{itemize}
    \item Neural Network Architecture: A description of the Neural Network Architecture can be found in Appendix ~\ref{apx:nn}
    \item Hyperparameters: Details on the hyperparameters used in the experiments are provided in Appendix ~\ref{apx:hp}.
    \item Additional Results: Further results can be found in Appendix ~\ref{apx:results}.
    \item Additional Ablations: Additional ablations are shown in Appendix ~\ref{apx:ablation}
\end{itemize}

\newpage
\subsection{Neural Network Architecture}\label{apx:nn}
To achieve a lightweight and efficient model, we utilize Tiny AutoEncoder \cite{von-platen-etal-2022-diffusers}, a pre-trained tiny CNN version of Stable Diffusion 3 VAE \cite{esser2024scaling}. Tiny AutoEncoder allows us to lower the image dimensions into $CHW=(16,64,64)$ with only 1.2M parameters for each encoder and decoder. Given the memory and latency constraints, we restrict our architecture to convolutional layers only, eschewing transformers' global attention mechanisms. Linear operations such as convolution can be modeled as matrix multiplication with a little overhead. As a result, these operations are highly optimized on most hardware accelerators to avoid quadratic computing complexity. Although Windows attention techniques \cite{liang2021swinir,crowson2024scalable} can be theoretically implemented with linear time complexity, practical implementation often involves data manipulation operations, including reshaping and indexing, which remain crucial considerations for efficient implementation on resource-constrained devices. Alternatively, in our method, we use only convolution layers. During training, we utilize collapsible linear blocks \cite{bhardwaj2022collapsible} by adding $1\times1$ convolution after each $3\times3$ convolution layer and expanding the hidden channel width by 4$\times$. These two linear operations are then collapsed to a single $3\times3$ convolution layer before inference

\subsubsection{Latent MMSE}
Our Latent MMSE consists of 3 cascaded RRDB blocks \cite{wang2018esrgan} with 96 channels each. We replace the Leaky ReLU activation of the original RRDB with SiLU. The cascade is implemented with a skip connection.

\subsubsection{U-Net}
For implementing the vector field we use U-Net \cite{ronneberger2015u}. U-Net is an architecture with special skip connections. These skip connections help transfer lower-level information from shallow to deeper layers. Since the shallower layers often contain low-level information, these skip connections help improve the result of image restoration. Our U-Net consists of convolution layers only. It has 3 levels with channel widths of $(128,256,512)$ and depths of $(1,2,4)$. We add a first and last convolution to align the channels of the latent tensor shape. Our basic convolution layer has $3\times3$ kernel and all activation functions are chosen to be SiLU. 

\newpage
\subsection{Hyper-parameters}\label{apx:hp}
\begin{table}[H]
\centering
\caption{\textbf{Hyper-parameters.} Training hyper-parameters
for Section~\ref{impl}.}
\begin{tabular}{lcc}
\toprule
 &  &  \\
\multirow{-2}{*}{Hyper-parameter} & \multirow{-2}{*}{Blind face restoration (Section~\ref{bfr_imp})} & \multirow{-2}{*}{Other tasks (Section~\ref{tasks_imp})} \\ 
\midrule\midrule
Vector-field Parameters & 29M & {\color[HTML]{333333} 19M} \\
Latent MMSE Parameters & 5.5M & 5.5M \\
Encoder-Decoder Parameters & 2.4M & 2.4M \\
\rowcolor[HTML]{FFFFFF} 
CFM: segments ($K$) & 5 & {\color[HTML]{333333} 3} \\
CFM: $\Delta t$ & 0.05 & 0.05 \\
CFM: $\alpha$ & 0.001 & 0.001 \\
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{FFFFFF}Training Epochs & 400 & {\color[HTML]{333333} 300} \\
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{FFFFFF}Batch Size & 64 & 128 \\
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{FFFFFF}Image Size & 512x512 & 256x256 \\
\rowcolor[HTML]{FFFFFF} 
Training Hardware & \cellcolor[HTML]{FFFFFF}4 H100 80GB & 4 A100 40GB \\
\cellcolor[HTML]{FFFFFF}Training Time & 2.5 days & 1 days \\
Optimizer & AdamW & AdamW \\
Learning Rate & $10^{-4}$ & $2\cdot10^{-4}$ \\
AdamW betas & (0.9,0.999) & (0.9,0.999) \\
AdamW eps & $10^{-8}$ & $10^{-8}$ \\
Weight Decay & 0.02 & 0.02 \\
EMA Decay & \cellcolor[HTML]{FFFFFF}0.999 & \cellcolor[HTML]{FFFFFF}0.999
\\ \bottomrule
\end{tabular}
\label{hyperparams}
\end{table}






\newpage
\subsection{Additional Results}\label{apx:results}

\textbf{In-The-Wild Dataset}.\quad We test our method for blind face restoration on the in-the-wild datasets: LFW-Test \cite{LFWTech}, WebPhoto-Test \cite{wang2021towards} and CelebAdult \cite{wang2021towards}. Here, we report only non-reference perception metrics: FID, NIQE, and MUSIQ. \name achieve competitive results with state-of-the-art baseline methods.

\begin{table}[H]
\centering
\footnotesize
\caption{\textbf{In-The-Wild Datasets Evaluation.} Comparison between \name and baseline models for blind face restoration. \textcolor{red}{Red}, \textcolor{blue}{blue} and {\color[HTML]{009901} green} indicate the best, the second best, and the third best scores, respectively.}
\begin{tabular}{lccccccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{LFW} & \multicolumn{3}{c}{WebPhoto} & \multicolumn{3}{c}{CelebAdult}  \\ \cmidrule(l){2-4}  \cmidrule(l){5-7} \cmidrule(l){8-10}
\multicolumn{1}{c}{\multirow{-2}{*}{Model}} & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) \\ \midrule\midrule
\cellcolor[HTML]{FFFFFF}CodeFormer & \cellcolor[HTML]{FFFFFF}53.46 & \cellcolor[HTML]{FFFFFF}4.55 &  \textbf{\color[HTML]{009901} 75.10} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 88.85} & \cellcolor[HTML]{FFFFFF}4.91 &  \textbf{\color[HTML]{009901} 72.75} & 115.42 & 4.56 &  \textbf{\color[HTML]{009901} 75.52} \\
GFPGAN &  \textbf{\color[HTML]{009901} 49.51} & 4.49 &  \textbf{\color[HTML]{3531FF} 76.38} & 91.69 & 4.81 & \textbf{\color[HTML]{FE0000} 74.73} & 112.72 & 4.35 & \textbf{\color[HTML]{FE0000} 76.39} \\
VQFRv2 & 51.22 & 3.82 & 74.40 &  \textbf{\color[HTML]{009901} 88.28} & 4.59 & 70.93 & 108.67 & 4.01 & 75.11 \\
\cellcolor[HTML]{FFFFFF}DifFace & \textbf{\color[HTML]{3531FF} 45.34} &  \textbf{\color[HTML]{009901} 3.80} & \cellcolor[HTML]{FFFFFF}70.04 & \cellcolor[HTML]{FFFFFF}93.01 &  \textbf{\color[HTML]{3531FF} 4.02} & \cellcolor[HTML]{FFFFFF}65.77 & \textbf{\color[HTML]{FE0000} 100.78} &  \textbf{\color[HTML]{009901} 3.69} & 72.12 \\
\cellcolor[HTML]{FFFFFF}DiffBIR & \cellcolor[HTML]{FFFFFF}\textbf{\color[HTML]{FE0000} 42.30} & \cellcolor[HTML]{FFFFFF}5.76 & \cellcolor[HTML]{FFFFFF}\textbf{\color[HTML]{FE0000} 76.77} & \cellcolor[HTML]{FFFFFF}91.19 & \cellcolor[HTML]{FFFFFF}6.28 &  \textbf{\color[HTML]{3531FF} 73.13} & 108.99 & 5.74 &  \textbf{\color[HTML]{3531FF} 76.37} \\
\cellcolor[HTML]{FFFFFF}ResShift & \cellcolor[HTML]{FFFFFF}53.85 & \cellcolor[HTML]{FFFFFF}4.18 & \cellcolor[HTML]{FFFFFF}71.12 & \cellcolor[HTML]{FFFFFF}\textbf{\color[HTML]{FE0000} 80.14} & \cellcolor[HTML]{FFFFFF}4.33 & \cellcolor[HTML]{FFFFFF}71.47 & 110.06 & 4.22 & 73.43 \\
\cellcolor[HTML]{FFFFFF}PMRF & \cellcolor[HTML]{FFFFFF}51.82 & \cellcolor[HTML]{FFFFFF}\textbf{\color[HTML]{FE0000} 3.55} & \cellcolor[HTML]{FFFFFF}69.83 & \cellcolor[HTML]{FFFFFF} \textbf{\color[HTML]{3531FF} 83.48} & \cellcolor[HTML]{FFFFFF}\textbf{\color[HTML]{FE0000} 3.74} & \cellcolor[HTML]{FFFFFF}65.07 &  \textbf{\color[HTML]{3531FF} 104.72} &  \textbf{\color[HTML]{3531FF} 3.45} & 72.82 \\
\midrule
\textbf{\name (Ours)} & 54.62 &  \textbf{\color[HTML]{3531FF} 3.65} & 70.08 & 92.98 &  \textbf{\color[HTML]{009901} 4.09} & 64.97 &  \textbf{\color[HTML]{009901} 105.68} & \textbf{\color[HTML]{FE0000} 3.37} & 74.12 \\ \bottomrule
\end{tabular}
\label{bfr_wild}
\end{table}


\begin{figure}[H]
\centering
\hspace*{0.0cm}\includegraphics[scale=0.25]{images/celeba_sr.jpg}
\caption{\textbf{Visual Results for Super Resolution.} Visual comparisons between \name and PMRF \cite{ohayon2024posterior} sampled from CelebA-Test for super-resolution task.}
\label{sr_figure}
\end{figure}

\begin{figure}[H]
\centering
\hspace*{0.0cm}\includegraphics[scale=0.25]{images/celeba_id.jpg}
\caption{\textbf{Visual Results for Denoising.} Visual comparisons between \name and PMRF \cite{ohayon2024posterior} sampled from CelebA-Test for image denoising task.}
\label{id_figure}
\end{figure}

\begin{figure}[H]
\centering
\hspace*{0.0cm}\includegraphics[scale=0.25]{images/celeba_ip.jpg}
\caption{\textbf{Visual Results for Inpainting.} Visual comparisons between \name and PMRF \cite{ohayon2024posterior} sampled from CelebA-Test for inpainting task.}
\label{ip_figure}
\end{figure}

\begin{figure}[H]
\centering
\hspace*{0.0cm}\includegraphics[scale=0.25]{images/celeba_color.jpg}
\caption{\textbf{Visual Results for Colorization.} Visual comparisons between \name and PMRF \cite{ohayon2024posterior} sampled from CelebA-Test for colorization task.}
\label{color_figure}
\end{figure}







\newpage
\subsection{Additional Ablations}\label{apx:ablation}


\begin{table}[H]
\centering
\caption{\textbf{Model Size Ablation.} Evaluation of various model sizes for super-resolution on the CelebA-Test dataset. Here, we fix $K=3$ and ablate only the vector field size, leaving the MMSE parameters unchanged. We note that the FID decreases as model size increases, while PSNR remains relatively constant. }
\begin{tabular}{cccccccc}
\toprule
 & \multicolumn{3}{c}{Percepual Quality} & \multicolumn{3}{c}{Distortion} &  \\ \cmidrule(l){2-4} \cmidrule(l){5-7} 
\multirow{-2}{*}{\#Params {[}M{]}} & FID($\downarrow$) & NIQE($\downarrow$) & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) & \multirow{-2}{*}{FPS($\uparrow$)} \\ \midrule\midrule
13 & \cellcolor[HTML]{FFFFFF}51.69 & \cellcolor[HTML]{FFFFFF}5.02 & \cellcolor[HTML]{FFFFFF}63.44 & \cellcolor[HTML]{FFFFFF}23.87 & \cellcolor[HTML]{FFFFFF}0.6582 & \cellcolor[HTML]{FFFFFF}0.3317 & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 48.55} \\
17 & 47.33 & 5.00 & 63.96 & 23.85 & 0.6572 & 0.3273 & 48.05 \\
27 & \cellcolor[HTML]{FFFFFF}44.81 & \cellcolor[HTML]{FFFFFF}5.01 & \cellcolor[HTML]{FFFFFF}64.06 & \cellcolor[HTML]{FFFFFF}23.87 & \cellcolor[HTML]{FFFFFF}0.6579 & \cellcolor[HTML]{FFFFFF}0.3256 & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 47.81} \\
37 & 44.71 & 5.01 & 64.26 & 23.86 & 0.6579 & 0.3253 & 42.94 \\ \bottomrule
\end{tabular}
\label{table:size}
\end{table}




\begin{table}[H]
\centering
\caption{\textbf{Effectiveness of trainable encoder.} Experiments of \name for denoising and inpainting on the CelebA-Test dataset \emph{w/} and \emph{w/o} training the encoder. We observe that allowing the encoder to fine-tune during training is crucial. Since the encoder was pre-trained on HQ images, it struggles to represent LQ images accurately. This leads to substantial errors that are reflected in all evaluated metrics. }
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{1}{l}{} & \multicolumn{3}{c}{Perceptual Quality} & \multicolumn{3}{c}{Distortion} \\ \cmidrule(l){3-5} \cmidrule(l){6-8}
\multirow{-2}{*}{Task} & \multicolumn{1}{l}{\multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Trainable\\  Encoder\end{tabular}}} & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) \\ \midrule\midrule
 & \cellcolor[HTML]{FFFFFF}\xmark & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 40.89} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 5.18} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 64.99} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 26.55} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7568} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.2675} \\
\multirow{-2}{*}{Denoising} & \cellcolor[HTML]{FFFFFF}\cmark & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 39.73} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 5.04} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 66.21} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 27.13} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7737} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.2537} \\ \hline
 & \cellcolor[HTML]{FFFFFF}\xmark & \cellcolor[HTML]{FFFFFF}43.00 & \cellcolor[HTML]{FFFFFF}4.96 & \cellcolor[HTML]{FFFFFF}64.56 & \cellcolor[HTML]{FFFFFF}23.46 & \cellcolor[HTML]{FFFFFF}0.6654 & \cellcolor[HTML]{FFFFFF}0.3327 \\
\multirow{-2}{*}{Inpainting} & \cellcolor[HTML]{FFFFFF}\cmark & \cellcolor[HTML]{FFFFFF}40.17 & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.95} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 66.17} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 25.40} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7302} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.2779} \\ \bottomrule
\end{tabular}
\label{table:encoder_long}
\end{table}

\textbf{Model Latency}.\quad Table~\ref{table:latency} presents ablation study for model latency. Here, we vary the multi-segment value $K$ while maintaining a fixed model size. Our findings suggest that $K=3$ provides a suitable trade-off between FID and frames per second (FPS). This ablation was conducted on the CelebA-Test dataset for the super-resolution but similar results were found in other tasks. Therefore, we set $K=3$ for super-resolution, denoising, inpainting, and colorization.
\\

\begin{table}[H]
\centering
\caption{\textbf{Latency Ablation.} Evaluation of a different multi-segment values ($K$) for super-resolution on the CelebA-Test dataset. Each model was trained with a fixed size of 27M parameters. We note that the FPS decreases as $K$ increases.}
\begin{tabular}{cccccccc}
\toprule
 & \multicolumn{3}{c}{Perceptual Quality} & \multicolumn{3}{c}{Distortion} &  \\ \cmidrule(l){2-4} \cmidrule(l){5-7} 
\multirow{-2}{*}{K} & \multicolumn{1}{l}{FID($\downarrow$)} & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) & \multirow{-2}{*}{FPS($\uparrow$)} \\ \midrule\midrule
1 & 45.76 & 5.08 & 65.07 & 23.91 & 0.6612 & 0.3207 & 70.30 \\
3 & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 44.81} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 5.01} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 64.06} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 23.87} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.6579} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.3256} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 49.26} \\
5 & 44.64 & 4.96 & 64.46 & 23.85 & 0.6573 & 0.3262 & 36.35 \\
7 & 44.20 & 4.92 & 64.61 & 23.79 & 0.6548 & 0.3278 & 30.25 \\ \bottomrule
\end{tabular}
\label{table:latency}
\end{table}



\textbf{Latent MMSE loss space}.\quad This ablation study investigates the impact of the $\ell_2$ 
loss space on model performance. We compared the original latent-space loss from (\ref{l2_loss}) with an alternative pixel-space loss:
\begin{align}\label{l2_loss_latent}
    \mathcal{L}_{2}\brackets{\vectorsym{\phi},\vectorsym{\omega}}&= \expectation{\norm{\mathcal{D}(g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{y}}))-\x}_2^2}{\vectorsym{x},\vectorsym{y}}.
\end{align}
Table~\ref{table:loss_space} demonstrates a trade-off between PSNR and perceptual quality. While pixel-space losses generally achieve higher PSNR values, they often result in lower perceptual quality, leading to visually less appealing restored images.
\\

\begin{table}[H]
\centering
\caption{\textbf{Latent MMSE Loss space.} Experiments across different tasks reveal that pixel space loss generally yields higher PSNR values than latent space loss. However, this often comes at the cost of reduced perceptual quality, leading to less visually appealing restored images.}
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{1}{l}{} & \multicolumn{3}{c}{Perceptual Quality} & \multicolumn{3}{c}{Distortion} \\ \cmidrule(l){3-5} \cmidrule(l){6-8}
\multirow{-2}{*}{Task} & \multicolumn{1}{l}{\multirow{-2}{*}{Loss Space}} & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) \\ \midrule\midrule
 & \cellcolor[HTML]{FFFFFF}latent & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 39.75} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.07} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 71.45} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 25.55} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.6933} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.3753} \\
\multirow{-2}{*}{blind face restoration} & \cellcolor[HTML]{FFFFFF}pixel & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 40.77} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.22} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 68.66} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 25.88} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7035} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.3762} \\ \hline
 & \cellcolor[HTML]{FFFFFF}latent & 44.81 & 5.01 & 64.06 & 23.87 & 0.6579 & 0.3256 \\
\multirow{-2}{*}{super resolution} & \cellcolor[HTML]{FFFFFF}pixel & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 48.94} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.96} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 62.67} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 24.03} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.6644} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.3300} \\ \hline
 & \cellcolor[HTML]{FFFFFF}latent & 39.73 & 5.04 & 66.21 & 27.13 & 0.7737 & 0.2537 \\
\multirow{-2}{*}{denoising} & \cellcolor[HTML]{FFFFFF}pixel & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 39.80} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.97} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 66.06} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 27.27} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7766} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.2582} \\ \hline
 & \cellcolor[HTML]{FFFFFF}latent & 40.17 & 4.95 & 66.17 & 25.40 & 0.7302 & 0.2779 \\
\multirow{-2}{*}{inpainting} & \cellcolor[HTML]{FFFFFF}pixel & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 40.34} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.92} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 65.18} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 25.70} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7414} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.2782} \\ \hline
 & \cellcolor[HTML]{FFFFFF}latent & 46.34 & 4.91 & 65.12 & 22.83 & 0.7303 & 0.3705 \\
\multirow{-2}{*}{colorization} & \cellcolor[HTML]{FFFFFF}pixel & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 49.71} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 4.88} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 63.61} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 22.90} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.7338} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{333333} 0.3834} \\ \bottomrule
\end{tabular}
\label{table:loss_space}
\end{table}


\textbf{Latent MMSE vs SwinIR}.\quad This ablation study analyzes our Latent MMSE approach by comparing it to both pixel and latent estimators of SwinIR \cite{liang2021swinir}. Maintaining a constant model size, Table~\ref{table:latent_mmse} presents the comparison results. In the latent space, our method exhibits performance comparable to SwinIR. While SwinIR in pixel space achieves higher PSNR values, our approach demonstrates a better FID score. Notably, our convolution-based Latent MMSE shows a much higher frame per second (FPS). These evaluations were conducted on the CelebA-Test dataset for blind face restoration. Note that the latent SwinIR implementation utilizes a window size of $4\times4$.
\\


\begin{table}[]
\centering
\caption{\textbf{Latent MMSE vs SwinIR.} We compared our Latent MMSE approach with the pixel and latent estimators of SwinIR \cite{liang2021swinir}. Our method demonstrated comparable performance to SwinIR while achieving a significant improvement in frame rate (FPS).}
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{2}{c}{MMSE} & \multicolumn{3}{c}{Perceptual Quality} & \multicolumn{3}{c}{Distortion} &  &  \\ \cmidrule(l){1-2} \cmidrule(l){3-5} \cmidrule(l){6-8}
Arch & \multicolumn{1}{l}{Latent} & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}\#Params[M]\end{tabular}} & \multirow{-2}{*}{FPS($\uparrow$)} \\ \midrule\midrule
CNN & \cmark & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 39.75} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 4.07} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 71.45} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 25.55} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.6933} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.3753} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 37} & \multicolumn{1}{l}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 19.51}} \\
SwinIR & \cmark & 39.81 & 3.99 & 72.21 & 25.59 & 0.6938 & 0.3695 & 37 & 13.22 \\
SwinIR & \xmark & 41.31 & 4.01 & 70.19 & 25.96 & 0.6980 & 0.3733 & 37 & 9.80 \\ \bottomrule
\end{tabular}
\label{table:latent_mmse}
\end{table}

\textbf{Noise Level ablation}.\quad This ablation study investigates the impact of noise level $\sigma_s$ on model performance. Additive noise is vital for learning the complex dynamics of image degradation, enabling the generation of high-quality images. However, careful tuning of $\sigma_s$ is essential; excessive noise can lead to distortion, while insufficient noise may degrade perceptual quality. Table~\ref{table:sigma} presents the results for various $\sigma_s$ values. Based on these results, $\sigma_s=0.1$ appears to offer the best balance between minimizing distortion and maintaining high perceptual quality. Therefore, we set $\sigma_s=0.1$ in our experiments. This ablation was conducted on the CelebA-Test dataset for the super-resolution but similar results were found in other tasks.
\\

\begin{table}[]
\centering
\caption{\textbf{Noise Level ablation.} Experiments across several noise levels $\sigma_s=\{0.05,0.1,0.2\}$. $\sigma_s=0.1$ appears to provide a suitable trade-off between distortion and perceptual quality.}
\begin{tabular}{ccccccc}
\toprule
 & \multicolumn{3}{c}{Perceptual Quality} & \multicolumn{3}{c}{Distortion} \\ \cmidrule(l){2-4} \cmidrule(l){5-7}
\multirow{-2}{*}{$\sigma_s$} & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) \\ \midrule\midrule
0.05 & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 48.07} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 5.17} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 63.42} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 24.06} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.6649} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.3229} \\
0.1 & 44.81 & 5.01 & 64.06 & 23.87 & 0.6579 & 0.3256 \\
0.2 & 43.40 & 4.90 & 64.91 & 23.54 & 0.6480 & 0.3312  \\ \bottomrule
\end{tabular}
\label{table:sigma}
\end{table}

\textbf{Time Interval ablation}.\quad In this ablation study, we investigate the influence of the time interval ($\Delta t$) on model performance. Table~\ref{table:delta_t} presents the results obtained for several $\Delta t$ values. Reducing $\Delta t$ is expected to enhance FID scores, however, it may also lead to an increase in distortion metrics. This study aims to identify the $\Delta t$ value that minimizes distortion while maintaining a high level of perceptual quality. According to the results, $\Delta t=0.05$ offers the most favorable balance between minimizing distortion and preserving perceptual quality. This ablation was conducted on the CelebA-Test dataset for the image denoising but similar results were found in other tasks.
\\

\begin{table}[]
\centering
\caption{\textbf{Time interval ablation.} Experiments across several time interval $\Delta t=\{0.01,0.05,0.1\}$. $\Delta t=0.05$ appears to provide a suitable trade-off between distortion and perceptual quality.}
\begin{tabular}{ccccccc}
\toprule
 & \multicolumn{3}{c}{Perceptual Quality} & \multicolumn{3}{c}{Distortion} \\ \cmidrule(l){2-4} \cmidrule(l){5-7}
\multirow{-2}{*}{$\Delta t$} & FID($\downarrow$) & \multicolumn{1}{l}{NIQE($\downarrow$)} & MUSIQ($\uparrow$) & PSNR($\uparrow$) & SSIM($\uparrow$) & LPIPS($\downarrow$) \\ \midrule\midrule
0.01 & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 39.68} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 4.92} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 66.47} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 27.03} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.7699} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} 0.2589} \\
0.05 & 39.73 & 5.04 & 66.21 & 27.13 & 0.7737 & 0.2537 \\
0.1 & 41.26 & 5.26 & 65.43 & 27.18 & 0.7764 & 0.2521  \\ \bottomrule
\end{tabular}
\label{table:delta_t}
\end{table}

\textbf{CFM trajectories}.\quad Latent CFM improves the flow straightness by enforcing consistency within the velocity field, which reduces discretization errors. Figure~\ref{fid:trajs} illustrates the ``straitness'' of the trajectories in the latent space. However, when these trajectories are projected back to the pixel space, this property is not preserved due to the decoder's non-linearity.
\\

\begin{figure}[H]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includesvg[scale=0.4]{images/latent_traj.svg}
        \label{fig:latent_traj}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includesvg[scale=0.4]{images/pixel_traj.svg} 
        \label{fig:pixel_traj}
    \end{minipage}
    \caption{\textbf{CFM trajectories.} The left figure visualizes CFM trajectories in latent space, connecting flow from source (p0) to target point (p1). These trajectories exhibit ``straight'' flows along two latent variables, a consequence of the Latent CFM operating within the latent space. However, this linearity is not preserved when projected into pixel space due to the decoder's non-linearity, as demonstrated in the right figure. }
\label{fid:trajs}
\end{figure}


\begin{figure}[H]
\centering
\hspace*{0.0cm}\includegraphics[scale=0.1]{images/flow_bfr.jpg}
\caption{\textbf{Visual steps of \name.} Illustrations of the restoration process, visualizing the process from LQ images to visually appealing results. Here, we utilize the Latent MMSE estimator and $K=5$ multi-segment of Latent CFM. The images are sampled from CelebA-Test for blind face restoration.}
\label{fig:flow_bfr}
\end{figure}










% \subsection{Proof}
% \todo[inline]{Dumy section}

% \todo[inline,color=red]{Theorical discustion:\\
% 1. I found a good reference on latent flow matching in \url{https://arxiv.org/pdf/2307.08698} which can be applicable to our case with maybe a small modification. \\


% The idea is as follows: the goal is to analysis the error of working in the latent space instade of pixel space. Let dived the discusstion into two parts:
% 1.The latent mmse error can be framed as follows:
% \begin{equation}
%     \norm{\mathcal{D}\brackets{\expectation{\z_{hq}|\mathcal{E}\brackets{\x_{lq}}}{}} -\expectation{\x_{hq}|\x_{lq}}{}}_2^2\leq ??
% \end{equation}
% where $\z_{hq}=\mathcal{E}\brackets{\x_{hq}}$.
% 2. Given the latent MMSE results what is the error of using Flow in the latent, so can base your results on \url{https://arxiv.org/pdf/2307.08698}.



% }

% \todo[inline]{The loss above it not the only option. Here are different option:\\
% (i) Here we wish to find $\z$ and result in the MMSE after the decoder, meaning:
% \begin{align}
%     &\norm{\expectation{\x_{hq}|\x_{lq}}{} -\mathcal{D}\brackets{g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{x}_{lq}})}}_2\leq \Delta_{MMSE}\nonumber\\
%     &+\norm{\mathcal{D}\brackets{g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{x}_{lq}})}-\vectorsym{x}_{hq}}_2.
% \end{align}
% where $\Delta_{MMSE}\triangleq\norm{\expectation{\x_{hq}|\x_{lq}}{}-\vectorsym{x}_{hq}}_2$. THis  results in the following loss:
% \begin{align}
%       \mathcal{L}_{2}\brackets{\vectorsym{\phi},\vectorsym{\omega}}&= \expectation{\norm{\mathcal{D}\brackets{g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{x}_{lq}})}-\vectorsym{x}_{hq}}_2^2}{\vectorsym{x}_{lq},\vectorsym{x}_{hq}},  
% \end{align}
% This loss identifies the optimal latent representation to achieve MMSE in pixel space, whereas our version achieves MMSE within latent space.\\
% (ii) Let rephrase this goal in the latent space, meaning we wise to find the latent repsentation of the MMSE estimator: 
% \begin{align}
%     &\norm{\mathcal{E}\brackets{\expectation{\x_{hq}|\x_{lq}}{}} -g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{x}_{lq}})}_2\leq  L_{D}\Delta_{MMSE}\nonumber\\
%     &+\norm{g_{\vectorsym{\phi}}(\mathcal{E}_{\vectorsym{\omega}}\brackets{\vectorsym{x}_{lq}})-\mathcal{E}\brackets{\vectorsym{x}_{hq}}}_2.
% \end{align}
% This objective  result in the loss we have present in (7) and it expline why our method MMSE is too much blur, becuse it does not achive the MMSE estimator in pixel space.  
% Let us discuss this next week. 
% }