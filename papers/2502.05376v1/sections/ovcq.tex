In this section, we introduce the concept of block clustered quantization (BCQ) and present the locally optimal block clustered quantization (LO-BCQ) algorithm that minimizes quantization MSE for any operand. We also introduce block formats to support various LO-BCQ configurations.

\subsection{Mathematical Definition}
\label{subsec:bcq} 

Given a tensor $\bm{X}$ composed of $L_X$ scalar elements, we denote its blockwise decomposition as $\{\bm{b}_i\}_{i=1}^{N_b}$, where $\bm{b}_i$'s are blocks of $L_b$ consecutive elements in $\bm{X}$, and the number of blocks is given by $N_b={L_X}/{L_b}$. Block clustered quantization (see Figure \ref{fig:block_clustering}) uses a family of $N_c$ codebooks $\mathcal{C} = \{C_i\}_{i=1}^{N_c}$, where $N_c << N_b$, and clusters the blocks into $N_c$ clusters such that each is associated with one of the $N_c$ codebooks. This procedure is equivalent to creating a mapping function $f$ from a block $b$ to a cluster index in $\{1,\ldots, N_c\}$. Quantization (or encoding) proceeds in a two-step process: (i) \emph{mapping} to assign a cluster index to a given block, and (ii) \emph{quantization} of its scalars using the codebook corresponding to that index. Formally, denoting $\hat{\bm{b}}$ as the result of block clustered quantization of a given block $\bm{b}$ in $\bm{X}$, this procedure is described as:
\begin{align}
    \label{eq:clustered_quantization_definition}
    \hat{\bm{b}} = C_{f(\bm{b})}(\bm{b})
\end{align}
where $C$ is a $2^B$-entry codebook that maps each scalar in $\bm{b}$ to a $B$-bit index to the closest representation. Each quantized scalar of block $\bm{b}$ is obtained as:
\begin{align}
\label{eq:bcq_encoding_of_scalar}
    \hat{\bm{b}}[l] = \arg \min_{k = 1 \ldots 2^B} \lvert \bm{b}[l] - C_{f(\bm{b})}[k] \rvert^2 
\end{align}
where the notation $x[y]$ is used to describe the $y^{\text{th}}$ element in an arbitrary block $\bm{x}$. That is, each scalar in $\hat{\bm{b}}$ is an index to the closest entry by value in $C_{f(\bm{b})}$.

Once mapped by invoking $f$, we store the $log2(N_c)$-bit codebook selector for each block. Therefore, the effective bit-width of each quantized scalar is given by:
\begin{align}
\label{eq:bitwidth_bcq}
    \mathrm{Bitwidth}_{ \mathrm{BCQ}} = B + {log2(N_c)}/{L_b}
\end{align}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{sections/figures/BCQ_large.pdf}
    \caption{\small Block clustered quantization: Each operand block is first mapped to a cluster based on a mapping function and then each scalar of that block is encoded as a $B$-bit index to the closest entry in the $2^B$-entry codebook associated with that cluster.}
    \label{fig:block_clustering}
\end{figure}


%\begin{figure}
%    \centering
%    \includegraphics[width=\columnwidth]{sections//figures/LO-BCQ_Format.pdf}
%    \caption{\small Block format for LO-BCQ. Each operand block is associated with a $log2(N_c)$-bit selector that selects the best codebook and each scalar is a $B$-bit index that represents the closest value in the selected codebook. Each block array $\bm{A}$ is associated with a $B_s$-bit scale factor.}
%    \label{fig:lobcq_format}
%\end{figure}

%Now, we discuss how VSQ \citep{dai2021vsq} and MX4 \citep{rouhani2023shared} can be viewed as special cases of VCQ. During VSQ, each operand vector $\bm{v}$ is associated with a scale factor $s_v$ (see Section \ref{sec:background}). As a result of the second level quantization of $s_v$ to $\hat{s}_v$, several operand vectors share the same $\hat{s}_v$. In other words, the operand vectors can be viewed as being clustered based on the quantized per-vector scale factors. For instance, an $8$-bit quantization of per-vector scale factors results in $256$ vector clusters. Further, each vector cluster is associated with a unique quantizer with quantization levels given by $\hat{s}_v * \mathcal{R}$ where $\mathcal{R}$ denotes the set of representations allowed by the number format of quantization. A similar argument can be applied to MX4 \citep{rouhani2023shared}. Unlike these previous proposals, we note that the more general framework represented by VCQ allows us to explore different design points through the explicit co-design of vector clustering methods and per-vector quantizers.

\subsection{Locally optimal block clustered quantization}
\label{subsec:iter_optimbcq}

Our goal is to construct a family of codebooks $\mathcal{C}$ resulting in minimal quantization MSE during block clustered quantization. Figure \ref{fig:optim_bcq_algo} presents an algorithm called Locally Optimal BCQ (LO-BCQ) to achieve this goal. LO-BCQ consists of two main steps: (i) updating block clusters with fixed per-cluster codebooks, and (ii) updating per-cluster codebooks with fixed block clusters. This algorithm begins at iteration $0$ (initial condition) with a set of $N_c$ initial codebooks $\{C_1^{(0)}, \ldots, C_{N_c}^{(0)}\}$ and unquantized operand blocks as inputs. During step 1 of iteration $n$, with the per-cluster codebooks from the previous iteration $\{C_1^{(n-1)}, \ldots C_{N_c}^{(n-1)}\}$, we perform block clustering by mapping each block to the codebook that achieves minimum quantization MSE. That is, we use the following mapping function: 
\begin{align}
    \label{eq:mapping_at_iter_n}
    f^{(n)}({\bm{b}}) = \arg \min_{i=1\ldots N_c} \lVert{\bm{b}} - C_i^{(n-1)}({\bm{b}})\rVert^2_2
\end{align}

Since each codebook $C_i$ is associated with a cluster $i$,  mapping to $C_i$ is equivalent to mapping to cluster $i$. Specifically, at iteration $n$, we construct $N_c$ block clusters $\bm{\mathcal{B}}^{(n)}=\{\mathcal{B}_i^{(n)}\}_{i=1}^{N_c}$, where each cluster is defined as:
\begin{align}
    \label{eq:clustering_step}
    \mathcal{B}_i^{(n)} = \left\{ {\bm{b}_j} \big | f^{(n)}({\bm{b}}_j) = i \text{ for } j \in \{1\ldots N_b\}\right\}
\end{align}

In step 2, given the updated block clusters from step 1 and quantization bitwidth $B$, we apply the Lloyd-Max algorithm on each block cluster to derive optimal $2^B$-entry per-cluster codebooks $\{C_1^{(n)}, \ldots C_{N_c}^{(n)}\}$:
\begin{align}
    \label{eq:quantizers_update}
    C_i^{(n)} \leftarrow \text{LloydMax}(\mathcal{B}_i^{(n)},B) 
\end{align}
where the Lloyd-Max algorithm (see \ref{subsec:Lloyd-Max}, Lloyd-Max is equivalent to K-means clustering on 1-dimensional data) is invoked on the data of the corresponding cluster $\mathcal{B}_i^{(n)}$.
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{sections/figures/LO-BCQ.pdf}
  \caption{\small Overview of LO-BCQ algorithm: The algorithm starts with a set of initial per-cluster codebooks, and then iteratively performs two steps (i) fix per-cluster codebooks and update block clusters and (ii) fix block clusters and update per-cluster codebooks.}
  \label{fig:optim_bcq_algo}
\end{figure*} 

We iterate steps 1 and 2 until convergence or a pre-determined number of iterations $M$. Empirically, we find that LO-BCQ converges at $M<=100$. Since each of these steps are locally optimal, we find that the quantization MSE is non-increasing for each iteration. As a result, for any given value distribution, our LO-BCQ algorithm greedily minimizes quantization MSE. A theoretical proof of this claim is provided in section \ref{subsec:lobcq_opt_proof}. 

\subsection{Convergence and Initialization}
\label{subsec:lobcq_convergence_and_init}

\begin{wrapfigure}{r}{0.55\columnwidth}
    \begin{center}
    \includegraphics[width=0.55\columnwidth]{sections//figures/lobcq_init.pdf}
    \end{center}
    \caption{\small NMSE of LO-BCQ with naive initialization compared to the proposed initialization. Here LOBCQ is configured with a block array size of $64$ and $16$ codebooks.}
    \label{fig:lobcq_init}
\end{wrapfigure}
Prior to clustering, we find that normalizing the operand blocks improves convergence. However, a block-wise normalization factor (or scaling factor) induces computational and memory overheads. Therefore, we perform a second-level quantization of this scaling factor to $B_s$-bits and share it across an array of blocks of length $L_{A}$. Furthermore, better convergence is observed for larger number of codebooks ($N_c$) and for a smaller block length ($L_b$). Such trends increase the bitwidth of BCQ in \eqref{eq:bitwidth_bcq}, meaning that LO-BCQ has an inherent trade-off between accuracy and complexity. 

We initialize the per-cluster codebooks $\{C_1^{(0)}, \ldots, C_{N_c}^{(0)}\}$ based on K-means++ initialization algorithm which maximizes pairwise euclidean distances. In our experiments, we found such initialization to lead to significantly better convergence than a random one. Further, in step 2 of each iteration, when Lloyd-Max algorithm is invoked in \eqref{eq:quantizers_update}, we set the initial centroids corresponding to the codebooks identified in the previous iteration. Figure \ref{fig:lobcq_init} compares the MSE achieved by LO-BCQ with naive initialization to random codebooks to the proposed improved initialization. As shown, LO-BCQ with the proposed initialization converges to a lower MSE than the naive initialization and other block quantization baselines.

%Figure \ref{fig:nmse_vs_iter} shows the MSE achieved by LO-BCQ across iterations for various codebook and block sizes, compared to baselines such as MXFP and VSQ. As shown, LO-BCQ converges to a lower MSE than the baselines. Further, we achieve better convergence for larger number of codebooks ($N_c$) and for a smaller block length ($L_b$), both of which increase the bitwidth of BCQ (see Eq \ref{eq:bitwidth_bcq}).

%\begin{wrapfigure}{r}{0.4\textwidth}
%    \begin{center}
%    \includegraphics[width=0.4\textwidth]{sections//figures/mse_vs_iter.pdf}
%    \end{center}
%    \caption{\small NMSE vs interations during LO-BCQ compared to other block quantization proposals}
%    \label{fig:nmse_vs_iter}
%\end{wrapfigure}

\subsection{Block formats for LO-BCQ}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{sections//figures/Block_Format_new.pdf}
    \caption{\small Block format for LO-BCQ. Each operand block is associated with a $log2(N_c)$-bit selector that selects the best codebook and each scalar is a $4$-bit index that represents the closest value in the selected codebook. Each block array is associated with a $8$-bit scale factor.}
    \label{fig:lobcq_format}
    \vspace{-8pt}
\end{figure}

Figure \ref{fig:lobcq_format} illustrates the LO-BCQ block format where each operand block of length $L_b$ is associated with a $log2(N_c)$-bit index (result of the mapping function $f$ in equation \ref{eq:mapping_at_iter_n}) that selects the best codebook for that block. Each codebook is composed of $2^B$ entries and each scalar in the operand block is a $B$-bit index that represents the closest value in the selected codebook. Each entry in the codebook is a $B_c$\footnote{We set $B_c=6$ in practice}-bit integer. Finally, each block array $\bm{A}$ is associated with a scale-factor $s_A$. This scale-factor and its quantization $\hat{s}_A$ to $B_s$\footnote{We set $B_s=8$ and the scale factor format is E4M3}-bits are computed as:
\begin{align}
    \label{eq:blockArray scale}
    s_A & = {\left(2^{B_c - 1} - 1\right)}/{\text{max}(\text{abs}(\bm{A}))} \\
    \hat{s}_A & = Q_{F}\left\{{s_A}/{s_X}, B_s\right\}
\end{align}
where $s_X$ is a per-tensor scale-factor that is shared by the entire operand tensor $\bm{X}$ and $Q_{F}$ denotes a quantizer that quantizes a given operand to format $F$ (see section \ref{subsec:numFormats_quantMethod} for more details on number formats and quantization method). 

The bitwidth of LO-BCQ is computed as:
\begin{align}
    \label{eq:effBitwidth}
    \mathrm{Bitwidth}_{\mathrm{LO-BCQ}} = B &+ {log2(N_c)}/{L_b} + {B_s}/{L_A} \notag \\ 
    &+ {N_c * 2^B * B_c}/{L_X}
\end{align}

%\begin{table}\scriptsize
%\setlength{\tabcolsep}{4.75pt}
%\begin{center}
%\caption{\label{tab:lobcq_config} \footnotesize Various LO-BCQ configurations and their bitwidths.}
%\scalebox{0.9}{
%\begin{tabular}{|c||c|c|c|c||c|c||c|} 
%\hline
% & \multicolumn{4}{|c||}{$L_b=8$} & \multicolumn{2}{|c||}{$L_b=8$} & $L_b=2$ \\
% \hline
% \backslashbox{$L_A$\kern-1em}{\kern-1em$N_c$} & 2 & 4 & 8 & 16 & 2 & 4 & 2 \\
% \hline
% 64 & 4.25 & 4.375 & 4.5 & 4.625 & 4.375 & 4.625 & 4.625\\
% \hline
% 32 & 4.375 & 4.5 & 4.625& 4.75 & 4.5 & 4.75 & 4.75 \\
% \hline
% 16 & 4.625 & 4.75& 4.875 & 5 & 4.75 & 5 & 5 \\
% \hline
%\end{tabular}
%}
%\end{center}
%\end{table}
where the term ${N_c * 2^B * B_c}/{L_X}$ is usually negligible since the memory footprint of codebooks (numerator) is negligible compared to the size of the operand tensor (denominator). Indeed, we emphasize that LO-BCQ shares a set of $N_c<=16$ codebooks of size $<=0.19KB$ among the scalars of the entire tensor, resulting in negligible memory overhead for storing the codebooks.

%In this paper, we assume $B_s=8$ and the data format $F$ is floating point E4M3. Further, each codebook entry is a $6$-bit integer (i.e, $B_c=6$) and we vary $N_c$ between $2$ and $16$, $L_b$ between $2$ and $8$, and $L_A$ between $16$ and $64$ to obtain various LO-BCQ configurations. We list the configurations and their corresponding bitwidths in Table \ref{tab:lobcq_config}.

%\begin{figure}
%    \centering
%    \includegraphics[width=1\linewidth]{sections//figures/BlockFormats.pdf}
%    \caption{\small Comparing LO-BCQ to MX format.}
%    \label{fig:block_formats}
%\end{figure}

%\textcolor{red}{Add table for other block formats}
%Figure \ref{fig:block_formats} compares our $4$-bit LO-BCQ block format to MX \citep{rouhani2023microscaling}. As shown, both LO-BCQ and MX decompose a given operand tensor into block arrays and each block array into blocks. Similar to MX, we find that per-block quantization ($L_b < L_A$) leads to better accuracy due to increased flexibility. While MX achieves this through per-block $1$-bit micro-scales, we associate a dedicated codebook to each block through a per-block codebook selector. Further, MX quantizes the per-block array scale-factor to E8M0 format without per-tensor scaling. In contrast during LO-BCQ, we find that per-tensor scaling combined with quantization of per-block array scale-factor to E4M3 format results in superior inference accuracy across models. 

%Figure \ref{fig:block_formats} compares our $4$-bit LO-BCQ block format to MX \citep{rouhani2023microscaling}. As shown, both LO-BCQ and MX decompose a given operand tensor into block arrays and each block array into blocks while in MXFP a block array is equivalent to a block. Similar to MX, we find that per-block quantization ($L_b < L_A$) leads to better accuracy due to increased flexibility. While MX achieves this through per-block scale-factors, we associate a dedicated codebook to each block through a per-block codebook selector. Further, both MX and MXFP do not perform scaling at the per-tensor level to quantize the per-block array scale-factor to E8M0 format. However, in LO-BCQ, we find that per-tensor scaling combined with quantization of per-block array scale-factor to E4M3 results in superior inference accuracy across models. Finally, we emphasize that LO-BCQ shares a set of $N_c$ codebooks among the scalars of the entire tensor, resulting in negligible memory overhead for storing the codebooks.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.68\textwidth]{sections//figures/codebooks_vs_formats.pdf}
    \end{subfigure}%
    ~ 
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{sections//figures/rom_vs_ram.pdf}
    \end{subfigure}
    \caption{\small (left) LO-BCQ codebooks compared to $4$-bit floating point formats (center) Layerwise normalized MSE (NMSE) compared to $4$-bit block formats. We compute NMSE for the weights of first $20$ GEMM layers (QKV, projection and fully-connected) of Llama2-7B model. Note that we use the NMSE for better visualization across varying layer data. (right) Quantization NMSE acheived by universally calibrated codebooks compared to that calibrated layerwise in Llama2-7B inputs of first $30$ GEMM (QKV, projection and fully-connected) layers.}
    \label{fig:codebook_analysis}
\end{figure*}

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{sections//figures/codebooks_vs_formats.pdf}
%    \caption{\small LO-VCQ codebooks compared to $4$-bit floating point formats and layerwise normalized MSE (NMSE). We compute NMSE for the weights of first $20$ GEMM layers (QKV, projection and fully-connected) of Llama2-7B model. Note that we use the NMSE for better visualization across varying layer data. }
%    \label{fig:codebooks_vs_numformats}
%    \vspace{-0.1em}
%\end{figure}
%\begin{figure}
%    \begin{center}
%    \includegraphics[width=0.3\textwidth]{sections//figures/rom_vs_ram.pdf}
%    \end{center}
%    \caption{\small Quantization NMSE acheived by universally calibrated codebooks compared to that calibrated layerwise in Llama2-7B inputs of first $30$ GEMM (QKV, projection and fully-connected) layers.}
%    \label{fig:rom_vs_ram}
%\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{sections//figures/codebooks_vs_formats.pdf}
%    \caption{\small LO-VCQ codebooks compared to $4$-bit floating point formats and layerwise normalized MSE (NMSE). We compute NMSE for the weights of first $20$ GEMM layers (QKV, projection and fully-connected) of Llama2-7B model. Note that we use the NMSE for better visualization across varying layer data. }
%    \label{fig:codebooks_vs_numformats}
%    \vspace{-0.1em}
%\end{figure}

\section{Applying LO-BCQ for LLM Inference}
%\vspace{-1em}
%\begin{figure}
%    \begin{center}
%    \includegraphics[width=0.3\textwidth]{sections//figures/rom_vs_ram.pdf}
%    \end{center}
%    \caption{\small Quantization NMSE acheived by universally calibrated codebooks compared to that calibrated layerwise in Llama2-7B inputs of first $30$ GEMM (QKV, projection and fully-connected) layers.}
%    \label{fig:rom_vs_ram}
%\end{figure}
In this section, we discuss specifics of applying LO-BCQ for LLM inference. Specifically, we first describe the codebook design process, followed by method for activation quantization on-the-fly.


We pre-calibrate the LO-BCQ codebooks for both weights and activations offline (prior to inference). Since weights are known, their own data can be used as calibration set. On the other hand, activations are dynamic and vary for every input; thus, as per common quantization strategies \citep{Hao2020nvintquant,csakr2022optimal}, we employ a randomly sampled calibration set from training data in order to build activation codebooks. Once codebooks are calibrated, we also quantize the codewords to $6$-bit integers. The choice of $6$-bit was based on empirical observations of accuracy being maintained with $L_A <= 128$.

Figure \ref{fig:codebook_analysis} (left) compares the codebooks identified by the LO-BCQ algorithm in a GEMM layer of a GPT3-126M model to $4$-bit floating point formats such as E1M2, E2M1 and E3M0. The LO-BCQ codebooks outperform other block formats as shown in Figure \ref{fig:codebook_analysis} (center) by capturing the arbitrary and non-uniform patterns in the value distributions of LLM operands and allowing each block to map to the codebook that best represents it. The mapping of operand blocks to the best of available codebooks can be conceptually compared to prior works that have explored mixed-format quantization such as \citep{thierry2020adafloat,ahzadeh2022mokey}. 

LO-BCQ provides the quantization operation the flexibility to assign data to any of the sign posts (codewords) in Figure \ref{fig:codebook_analysis}(left). The union of these sign posts covers the real line with a resolution that is clearly superior to that of a 4-bit quantizer. Therefore, we hypothesized that these codebooks need not be calibrated on a per-tensor (layerwise) basis, but rather, it is likely that they would be universally appropriate to quantize \emph{any tensor (weights and activations), at any layer, for any model}. To verify this hypothesis, we calibrated a set of codebooks on data sampled from GPT3 models on Wikitext-103 dataset and froze it. We find that these codebooks achieve comparable quantization MSE compared to those calibrated individually on each operand as shown in Figure \ref{fig:codebook_analysis} (right) which verifies our hypothesis. In our subsequent results, we always employ universally calibrated codebooks.
%We introduce a new LO-BCQ block format that allows us to explore different $L_b$ (block length), $L_bA$ (block array length) and $N_c$ (number of codebooks). Figure \ref{fig:lobcq_format} illustrates the proposed format. Each operand block $b$ of length $L_b$ is associated with a per-block index $i$ that selects the best codebook for that block. Further, each block array composed of $n$ blocks share a scale factor. 

Finally, the small size of LO-BCQ codebooks enables efficient activation quantization on the fly. Indeed, LO-BCQ involves computing the following values -- per-block array scale-factor $s_A$, per-block codebook selector $s_b$ which is the result of the mapping function $f$ (Eq. \ref{eq:mapping_at_iter_n}), and the index to closest representation $\hat{b}$ in the selected codebook (Eq. \ref{eq:bcq_encoding_of_scalar}). Note that the computation of $s_A$, similar to other block quantization methods \citep{rouhani2023shared,dai2021vsq}, simply corresponds to a max-reduction (followed by quantization) over the block array whose size is small ($<=128$). Further, with LO-BCQ, the codebooks are constant (frozen) with small size ($<=0.19KB$). This is an important distinction with other works on codebook quantization \citep{tseng2024quipbetterllmquantization,egiazarian2024aqlm}. As such, $s_b$ and $\hat{b}$ can be concurrently computed across operand blocks.
%and the latency of mapping which is performed once per dot-product can be effectively hidden.}

%Importantly, with LO-BCQ, the size of codebooks ($<=0.19KB$) is small enough such that it easily fits within the shared memory of modern GPUs. This is an important distinction with other works on codebook quantization \citep{tseng2024quipbetterllmquantization,egiazarian2024aqlm}. As such, $s_b$ and $\hat{b}$ can be concurrently computed in a thread-local sub-routine within a custom CUDA kernel. The locality of computation circumvents the need for any synchronization of streaming multiprocessors.


%Furthermore, we decompose each operand block into sub-blocks and allow each sub-block to map to a codebook. This design choice allows increased flexibility for the scalars within an operand block to choose a codebook that minimizes the quantization MSE. We introduce a new LO-BCQ block format that allows us to explore different block and sub-block lengths and number of codebooks. Figure \ref{} illustrates the proposed format. 

% Explain LO-BCQ format and rationale


% Why norm?

% Why reduce block-size?


%Prior to clustering, we find that normalizing the operand blocks speeds up convergence. Therefore, we normalize each operand block and store its normalization factor. Furthermore, we find that reducing the block length ($L_b$) significantly improves the quantization MSE LO-BCQ converges to. However, since a normalization factor is stored alongside each block, reducing the length of the operand block would 

% Discussion on design-space and bitwidth
%Further, we decompose the normalized operand blocks into sub-blocks of length $L_{sB}$ and perform LO-BCQ on the sub-blocks.

% Compare this format to other proposals





% sub-block can map to codebooks
% Block formats, compare to other proposals
% Design space and effective bit-width

% For a vector $\bm{v}_j$, we define a \textit{normalization factor} $m_j$ computed as:
%\begin{align*}
%    m_j = \max_{l=1\ldots L_v}(|v_j[l]|)
%\end{align*}
%where $v_j[l]$ denotes the $l^{\mathrm{th}}$ element of vector $\bm{v}_j$ and $|\cdot|$ denotes the absolute %value function. Now, the normalized vector $\tilde{\bm{v}}_j$ is obtained as:
%\begin{align*}
%    \tilde{\bm{v}}_j = \frac{\bm{v}_j}{m_j}
%\end{align*}

%To reduce the cost of storing the normalization factor $m_j$ for each vector $\bm{v}_j$, we perform a second level quantization of them (similar to VSQ \citep{dai2021vsq}).