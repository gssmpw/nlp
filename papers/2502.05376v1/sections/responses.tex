\subsection{Reviewer 2}
We thank the reviewer for their thorough comments and questions. We are glad to know that the reviewer acknowledges our extensive experimental evaluations across several LLMs and datasets. We also appreciate the reviewer emphasizing the fact that we have achieved $<1$\% accuracy loss in several inference tasks when quantizing both weights and activations to 4-bits. 

The main concern of the reviewer is regarding the group-size or ‘block division’ of quantization. We agree with the reviewer that a small group-size such as 8 can lead to large computational overheads due to high precision scale factor multiplication per-group. We would like to clarify that the block-array size denoted by $L_A$ in our work is equivalent to group-size, that is, each block array shares a scale-factor. Therefore, in our work, scale factors are shared among groups of size 64, rather than 8, which has much smaller overhead. Indeed, one of the strengths of our work is using different granularities for scaling and codebook indexing (the latter which has size of 8). Therefore, LO-BCQ leverages the benefits of finer-grained quantization while avoiding a prohibitive overhead associated with scaling factors. 

 We agree with the reviewer that Atom[1], a group-quantization based proposal, demonstrates impressive results across several inference tasks. We have now cited this work in our paper. With LO-BCQ, we demonstrate comparatively better inference accuracy than previous group quantization proposals across several benchmarks, including Atom[1]. The superior accuracy achieved by LO-BCQ can be attributed to flexibly mapping each block to a codebook that achieves least quantization error. In contrast, group quantization proposals associate a single number format to each group. 

 We present the inference accuracy achieved by LO-BCQ with a large group size of 64 across several datasets and models. Across these benchmarks, we demonstrate significantly better inference accuracy compared to other group quantization proposals such as Atom[1], MX[2] and MXFP[3].

Upon comparing the loss in perplexity on Wikitext dataset, we find that LO-BCQ achieves significantly lower loss compared to other works with a group size of 64. We share an 8-bit scale-factor across 64 element block arrays, and each block of 8 shares a codebook selector. With 2, 4, 8 and 16 codebooks, the effective bit-width of LO-BCQ is 4.25, 4.375, 4.5 and 4.625, respectively. We note that the evaluations of LO-BCQ are performed in newer family of Llama models and in a significantly larger Wikitext-103 dataset compared to the evaluations of Atom. 

Similarly, comparing the loss in accuracy in datasets such as PIQA, BoolQ, HellaSwag and Winogrande, we demonstrate significantly lower loss. The superior performance of LO-BCQ is from effectively designing codebooks that minimize quantization error and flexibly mapping each block to a codebook that best quantizes it. LO-BCQ accuracy vs other group quantization proposals such as MX4[2] and MXFP4[3] are reported in Table 
 \ref{tab:large_gs}. In comparison, Atom results are reported in Table \ref{tab:atom}


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|} 
\hline
 Method &  Group Size & Wiki3  & PIQA & BoolQ & HellaSwag & Winogrande \\
 \hline                                
 \hline
  \multicolumn{7}{c}{Loss compared to unquantized baseline} \\ 
 \hline
 \hline
   \multicolumn{7}{c}{Llama2-7B} \\
\hline
LO-BCQ (2 codebooks) & 64 & 0.25 & \textbf{0.98} & 1.71 & 1.17 & \textbf{0.48} \\
\hline
LO-BCQ (4 codebooks) & 64 & 0.20 & 1.25 & 1.92 & 1.53 & \textbf{0.95} \\
\hline
LO-BCQ (8 codebooks) & 64 & 0.13 & \textbf{0.98} & 1.86 & \textbf{0.59} & \textbf{+0.39} \\
\hline
LO-BCQ (16 codebooks) & 64 & \textbf{0.12} & \textbf{0.21} & 1.19 & \textbf{0.48} & 1.19 \\
\hline
MXFP4 & 32 & 0.70 & 0.54 & 5.29 & 2.88 & 1.9 \\
\hline
MX4 & 16 & 0.67 & 1.03 & 5.31 & 1.91 & 3.16 \\
\hline
\hline
   \multicolumn{7}{c}{Llama2-70B} \\
\hline
\hline
LO-BCQ (2 codebooks) & 64 & 0.21 & \textbf{0.11} & 2.41 & 1.15 & 1.18 \\
\hline
LO-BCQ (4 codebooks) & 64 & 0.11 & \textbf{0.87} & \textbf{0.95} & \textbf{0.48} & 1.5 \\
\hline
LO-BCQ (8 codebooks) & 64 & \textbf{0.09} & \textbf{0.11} & 1.2 & \textbf{0.51} & 1.58 \\
\hline
LO-BCQ (16 codebooks) & 64 & \textbf{0.07} & \textbf{0.06} & \textbf{0.98} & \textbf{0.23} & \textbf{0.79} \\
\hline
MXFP4 & 32 & 0.54 & 0.98 & 2.17 & 2.03 & 3.63 \\
\hline
MX4 & 16 & 0.44 & 0.98 & 2.82 & 2.03 & 3.55 \\
\hline
    \end{tabular}
    \caption{Inference accuracy (or perplexity) loss of LO-BCQ, MX4 and MXFP4 compared to respective unquantized baselines}
    \label{tab:large_gs}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|} 
\hline
 Model &  Group Size & Wiki2  & PIQA & BoolQ & HellaSwag & Winogrande \\
\hline
Llama-7B & 128 & 0.48 & 1.09 & 3.33 & 3.18 & 3.16 \\
\hline
Llama-65B & 128 & 0.36 & 0.38 & 0.24 & 1.61 & 4.5 \\
\hline
    \end{tabular}
    \caption{Inference accuracy (or perplexity) loss of Atom compared to respective unquantized baselines}
    \label{tab:atom}
\end{table}

Additionally, the reviewer has asked an important question regarding the calibration of codebooks for activations that are typically generated on-the-fly during inference. We have provided the performance of LO-BCQ on language modelling task such as perplexity on Wikitext-103 dataset across various models in Table \ref{tab:rom_cb}. We also compare the perplexity achieved by universally calibrated codebooks against layerwise calibrated codebook in Table \ref{tab:rom_ram}.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|} 
\hline
Num Codebooks &  Llama2-7B & Llama2-70B  & Nemotron4-15B & Nemotron4-340B & GPT3-22B  \\
\hline
\hline
  \multicolumn{6}{c}{LO-BCQ, Group size=64, Loss compared to unquantized baseline} \\ 
 \hline
2 & 0.25 & 0.21 & 0.43 & 0.19 & 0.20 \\
\hline
4 &	0.20 &	0.11 &	0.33 &	0.14 &	0.10 \\
\hline
8 &	0.13 &	0.09 &	0.26 &	0.12 &	0.08 \\
\hline
16 & 0.12 &	0.07 &	0.21 &	0.11 &	0.09 \\
\hline
 \end{tabular}
    \caption{Wikitext-103 perplexity loss with universally calibrated codebooks}
    \label{tab:rom_cb}
\end{table}

As demonstrated by the perplexity achieved by LO-BCQ on Wikitext-103 dataset, the pre-calibrated codebooks are effective in quantizing activations of various models we considered. 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|} 
\hline
Num Codebooks &  Universally calibrated & Layerwise calibrated  \\
\hline
\hline
  \multicolumn{3}{c}{LO-BCQ, Group size=64, Loss compared to unquantized baseline, LLama2-7B} \\ 
 \hline
2 & 0.25 & 0.23  \\
\hline
4 &	0.20 &	0.16  \\
\hline
8 &	0.13 &	0.13  \\
\hline
16 & 0.12 &	0.11  \\
\hline
 \end{tabular}
    \caption{Wikitext-103 perplexity loss with universally calibrated codebooks}
    \label{tab:rom_ram}
\end{table}

Layerwise calibrated codebooks calibrated individually for each operand (weights and activations) in each layer of Llama2-7B. As shown, the universally calibrated codebooks perform as well as layer-wise calibrated codebooks especially with >= 8 codebooks. 

Finally, we agree with the reviewer that the clustering and identifying codebooks is an expensive procedure and will impact inference performance. However, we would like to clarify that the clustering and calibration of codebooks is performed prior to inference (offline). We freeze the codebooks identified by LO-BCQ algorithm across our evaluations. 

Further, the codebook selectors are pre-determined for the weights and are computed on-the-fly for activations. The cost of determining the codebook selector is amortized by the large size of dot-product dimension (e.g. 8192 in Llama2-70B). 

Finally, each 4-bit scalar is decoded to 6-bit integer during GEMM/GEMV computations. As a result of a small number of codebooks utilized by LO-BCQ (footprint = 0.19KB), one can potentially store them in shared memory of each SM resulting in a small look-up cost. A dedicated hardware design can amortize the cost of this lookup through spatial and temporal re-use of operands.
