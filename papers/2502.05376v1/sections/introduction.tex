Quantization is a highly effective and widely adopted technique for reducing the computational and storage demands of Large Language Model (LLM) inference. While recent efforts \citep{wang2023bitnet,tseng2024quipbetterllmquantization,egiazarian2024aqlm,frantar2023optq,lin2023awq} have largely focused on weight-only quantization targeting single-batch inference, activation quantization becomes critical for improving throughput during multi-batch inference scenarios such as cloud-scale deployments serving multiple users. Previous works \citep{yao2023zeroquantv2,dai2021vsq} on sub-8-bit quantization of both weights and activations have relied on quantization-aware training (QAT) to recover accuracy loss during inference. However, the prohibitive cost of training and unavailability of training data in recent LLMs has made QAT increasingly difficult and motivated recent post-training quantization (PTQ) efforts \citep{xiao2023smoothquant,rouhani2023microscaling,wu2023zeroquantfp}. 

%Block quantization techniques where each block, typically consisting of $16$-to-$32$-scalar elements, has its own scaling factor \citep{rouhani2023microscaling,dai2021vsq} achieve the current state-of-the-art accuracy for sub-8-bit quantization of both weights and activations. While these works deploy the same quantizer (number-format) across blocks, we hypothesize that one way to achieve lower quantization mean squared error (MSE) would be to design a dedicated codebook for each block through an MSE-optimal algorithm such as \citet{Lloyd}. However, such a design would be expensive in terms of computational effort and memory footprint. Therefore, we propose to amortize this cost via codebook sharing among clusters of blocks. Our method is called block clustered quantization (BCQ) and is comprised of two steps: (1) a clustering step applied to operand blocks, and (2) a quantization step individually applied to operand scalars based on their cluster membership.

%We propose an iterative PTQ algorithm called LO-BCQ (locally optimal block clustered quantization) that jointly optimizes the block clustering and the per-cluster codebook. We prove that LO-BCQ greedily minimizes quantization MSE across iterations by performing locally optimal steps at each iteration. With the optimal codebooks derived through LO-BCQ, we demonstrate state-of-the-art bitwidth-vs-accuracy across a suite of GPT3, Llama2 and Nemotron-4 models on a wide range of downstream tasks. For all our results, we employ PTQ on frozen model parameters.

In this paper, we develop a post-training quantization (PTQ) algorithm aimed at minimizing the mean squared error (MSE) of any operand tensor. Traditional MSE-optimal scalar quantization algorithms such as Lloyd-Max \citep{Lloyd} struggle to achieve aggressive bitwidth reduction without significant accuracy loss. To address this limitation, vector (or block) quantization methods have been explored in \citep{tseng2024quipbetterllmquantization,egiazarian2024aqlm}, which identify MSE-optimal vector codebooks. Despite their promising results for $\le4$-bit weight-only quantization, these existing approaches face two key challenges. First, they require complex codebook schemes involving weight updates to minimize quantization error, making them challenging to deploy for dynamic compression of activations. Second, they require large codebook sizes (on the order of $2^{16}$ codebooks with $8$ entries each) per-model or per-layer to achieve aggressive model compression.

%Despite their advantages, existing vector quantization approaches face two key challenges. First, they have not demonstrated effectiveness for activation quantization. Second, they require large codebook sizes (on the order of $2^{16}$ codebooks with $8$ entries each) per-model or per-layer to achieve aggressive model compression.

% Second, they require an extremely large number of codebooks (on the order of $2^{16}$) per-model or per-layer to achieve aggressive model compression.

To overcome these limitations, we propose a novel clustering and quantization framework called block clustered quantization (BCQ). BCQ consists of two key steps:(1) a clustering step applied to operand blocks, and (2) a scalar quantization step individually applied to operand scalars based on their cluster membership. To minimize MSE in this process, we introduce LO-BCQ (locally optimal block clustered quantization), an iterative algorithm that jointly optimizes block clustering and per-cluster codebooks. We prove that LO-BCQ greedily minimizes quantization MSE across iterations by performing locally optimal steps at each iteration. We apply LO-BCQ on calibration data to identify optimal codebooks. We demonstrate that these codebooks can be frozen during inference across models and across linear layers within models. Using $\le16$ optimal codebooks with $16$ entries each derived through LO-BCQ, we achieve state-of-the-art trade-offs between bitwidth and accuracy without requiring any weight updates during $4$-bit quantization of both weights and activations across diverse models and downstream tasks.

\subsection{Related work}
Recent sub-4-bit quantization proposals such as \citep{wang2023bitnet,tseng2024quipbetterllmquantization,egiazarian2024aqlm} explore extreme weight quantization while maintaining activations at $8$-bit or higher precision. In particular, BitNet \citep{wang2023bitnet} proposed W1A8 quantization resulting in an aggregate (weights + activations) bitwidth comparable to LO-BCQ. However, BitNet demands training from scratch and despite this large training cost suffers significant loss in accuracy in downstream tasks. QuiP\# \citep{tseng2024quipbetterllmquantization} and AQLM \citep{egiazarian2024aqlm} propose W2A8 quantization through codebooks. These methods explore vector and additive codebook quantization, respectively, and rely on large codebook sizes ($1$MB of memory footprint) and are not directly applicable to activation quantization. These methods also require weight updates to minimize accuracy loss. In contrast, optimal codebooks ($\le0.19$KB of memory footprint) identified by LO-BCQ are applicable to both weight and activation quantization and achieves $<1$\% accuracy loss in downstream tasks without any weight updates. Supporting such small codebook sizes makes LO-BCQ more amenable to potential hardware acceleration of decompression. Minimizing quantization MSE using the 1D (Lloyd-Max) and 2D Kmeans clustering has been explored in \citep{han2016deepcompression,cho2021dkmdk,cho2023edkm} and \citep{vanbaalen2024gptvq}, respectively. In contrast, LO-BCQ iteratively optimizes block clustering alongside Lloyd-Max based optimal scalar quantization of block clusters. W4A8 quantization has been proposed in \citep{frantar2023optq, bai2021efficient, yao2022zeroquant} involving weight updates to preserve accuracy and in \citep{lin2023awq,vanbaalen2024gptvq} without any weight updates (PTQ). Further, \citep{Guo2023olive,wei2023outlier,kim2023squeezellm} perform sub-8-bit weight quantization by suppressing outliers.  

Block (group) quantization is explored for aggressive quantization of both weights and activations in VSQ \citep{dai2021vsq}, FineQuant \citep{kim2023finequant}, ZeroQuant-V2 \citep{yao2023zeroquantv2}, Atom \citep{zhao2024atom} through integer number formats, and in \citep{zhang2023integer}, ZeroQuant-FP \citep{wu2023zeroquantfp}, MX \citep{rouhani2023microscaling}, MXFP \citep{rouhani2023shared} and BSFP \citep{lo2023block} through floating-point formats. Figure \ref{fig:motivation} compares the perplexity loss vs compression factor of LO-BCQ to other quantization proposals. Here, the perplexity loss is relative to an unquantized baseline on the Wikitext-103 dataset for LO-BCQ, MX and MXFP4, and on the Wikitext2 for others. The compression factor refers to the total number of bits in the weight and activation\footnote{\label{fn:rep_cost}. The size of activations is measured for the prefill phase with a context length of $4096$ and batch size of $1$.} tensors (computed as $|A|B_A + |W|B_W$ following \citet{csakr_guarantees})\footnote{the notation $|X|$ refers to the total number of scalars in tensor $X$, and $B_X$ is the bitwidth of $X$.} that need to be processed in each layer relative to an unquantized BF16 baseline. Depending on the target application, weight or activation quantization may be more important. For the sake of generality, we consider them to be equally important in our metric. As shown in Figure \ref{fig:motivation}, LO-BCQ advances the current state-of-the-art by achieving the best trade-off between perplexity and compression.


%Recent sub-4-bit quantization proposals such as \citep{wang2023bitnet,tseng2024quipbetterllmquantization,egiazarian2024aqlm} explore extreme weight quantization while maintaining activations at $8$-bit or higher precision. In particular, BitNet \citep{wang2023bitnet} proposed W1A8 quantization resulting in an aggregate (weights + activations) bitwidth comparable to LO-BCQ. However, BitNet demands training from scratch and despite this large training cost suffers significant loss in accuracy in downstream tasks. QuiP\# \citep{tseng2024quipbetterllmquantization} and AQLM \citep{egiazarian2024aqlm} propose W2A8 quantization through codebooks. These methods explore vector and additive codebook quantization, respectively, and rely on a significantly large number of codebooks (of the order $2^{16}$) for quantization, suffering large decoding costs. In contrast, LO-BCQ explores scalar quantization methods for W4A4 quantization and achieves $<1$\% accuracy loss in downstream tasks with no more than $16$ codebooks with $16$ entries each. W4A8 quantization has been proposed in \citep{frantar2023optq, bai2021efficient, yao2022zeroquant} involving weight updates to preserve accuracy and in \citep{lin2023awq,vanbaalen2024gptvq} without any weight updates (PTQ). Further, \citep{Guo2023olive,wei2023outlier,kim2023squeezellm} perform sub-8-bit weight quantization by suppressing outliers. In contrast, LO-BCQ explores sub-8-bit activation quantization alongside weight quantization. 

%Block quantization has emerged as an effective technique for quantizing both weights and activations, as demonstrated in VSQ \citep{dai2021vsq}, FineQuant \citep{kim2023finequant}, ZeroQuant-V2 \citep{yao2023zeroquantv2}, Atom \citep{zhao2024atom} through integer number formats, and in \citep{zhang2023integer}, ZeroQuant-FP \citep{wu2023zeroquantfp}, MX \citep{rouhani2023microscaling} and MXFP \citep{rouhani2023shared} through floating-point formats. Moreover, sub-block scaling techniques explored in MXFP and BSFP \citep{lo2023block} demonstrate improvements over standard block quantization. In this work, we perform clustering of operand blocks and share MSE-optimal codebook quantizers among the scalars of each cluster. Minimizing quantization MSE using the 1D (Lloyd-Max) and 2D Kmeans clustering has been explored in \citep{han2016deepcompression,cho2021dkmdk,cho2023edkm} and \citep{vanbaalen2024gptvq}, respectively. In contrast, LO-BCQ iteratively optimizes block clustering alongside Lloyd-Max based optimal scalar quantization of block clusters.

%Along these lines, we explore block formats in LO-BCQ where each block is associated with an index to codebook, and an array of blocks share a scaling factor. In contrast to the other block quantization methods, we explore block clustering and design an MSE-optimal codebook for each block cluster. We demonstrate significant improvements in inference accuracy during W4A4 quantization compared to these methods.

%Figure \ref{fig:motivation} compares the perplexity loss vs compression factor of LO-BCQ to other quantization proposals. Here, the perplexity loss is relative to an unquantized baseline on the Wikitext-103 dataset for LO-BCQ, MX and MXFP4, and on the Wikitext2 for others. Further, the compression factor refers to the total number of bits in the weight and activation\footnote{\label{fn:rep_cost}The size of activations is measured for the prefill phase with a context length of $4096$ and batch size of $1$.} tensors (computed as $|A|B_A + |W|B_W$ following \citet{csakr_guarantees})\footnote{the notation $|X|$ refers to the total number of scalars in tensor $X$, and $B_X$ is the bitwidth of $X$.} that need to be processed in each layer relative to an unquantized BF16 baseline. Depending on the target application, weight or activation quantization may be more important. For the sake of generality, we consider them to be equally important in our metric. As shown in Figure \ref{fig:motivation}, LO-BCQ advances the current state-of-the-art by achieving the best trade-off between perplexity and compression.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{sections/figures/intro_updated.pdf}
\caption{\small Wikitext perplexity loss relative to unquantized baseline vs compression factor of LO-BCQ compared to previous LLM quantization proposals. Here, compression factor is the cumulative number of bits in the weight and activation tensors$^{\ref{fn:rep_cost}}$ that need to processed in each layer relative to an unquantized BF16 baseline.}
\label{fig:motivation}
\end{figure}

\subsection{Contributions}
\vspace{-0.3em}
The main contributions of this work are as follows:
\begin{itemize}
\item We propose BCQ, a block clustered quantization framework that performs per-block quantization by first clustering operand blocks and then quantizing each block cluster using a dedicated codebook.
\item We derive a locally optimal version of BCQ called LO-BCQ that iteratively optimizes block clustering and per-cluster quantization to provably minimize quantization MSE for any value distribution. We demonstrate that LO-BCQ is applicable to quantization of both weights and activations of LLMs.
\item We propose block formats for LO-BCQ where each operand block is associated with an index that maps it to one of a set of static codebooks, and a group of blocks (called a block array) share a quantization scale-factor. We vary the length of blocks, block arrays and the number of codebooks to study different configurations of LO-BCQ. 
\item When each of the weight and activation scalars are quantized to $4$-bits (effective bitwidth including per-block scale-factors etc. is $4.5$ to $4.625$ bits), we achieve $<0.1$ loss in perplexity across GPT3 (1.3B, 8B and 22B) and Llama2 (7B and 70B) models and $<0.2$ loss in Nemotron4 (15B and 340B) models, respectively, on the Wikitext-103 dataset. Further, we achieve $<1$\% loss in average accuracy across downstream tasks such as MMLU and LM evaluation harness. 
\end{itemize}
To the best of our knowledge, we are the first to achieve $<1$\% loss in downstream task accuracy when both LLM activations and weights are quantized to $4$-bits during PTQ (no fine-tuning or weight updates).



% We develop a PTQ algorithm with a primary goal of minimizing mean squared error (MSE) of any operand tensor, while achieving accurate 4-bit quantization of both weights and activations. Scalar quantization algorithms such as Lloyd-Max identify scalar codebook that is MSE-optimal but is unable to achieve aggressive bitwidth reduction without losing accuracy. Therefore, vector (or block) quantization methods that identify MSE-optimal vector codebooks have been explored that achieve promising accuracy with <=4-bit weight-only quantization. In addition to not demonstrating applicability to activation quantization, these methods use a very large number of codebooks (of the order of 2^16) to achieve such aggressive model compression. Large number of codebooks increase the cost of decoding during inference and makes a practical deployment of these methods challenging. In contrast to these methods, we propose a novel clustering and quantization framework called block clustered quantization (BCQ) and is comprised of two steps: (1) a clustering step applied to operand blocks, and (2) a quantization step individually applied to operand scalars based on their cluster membership.  We design an iterative algorithm called LO-BCQ (locally optimal block clustered quantization) that jointly optimizes the block clustering and the per-cluster codebook. We prove that LO-BCQ greedily minimizes quantization MSE across iterations by performing locally optimal steps at each iteration. With <=16 optimal codebooks derived through LO-BCQ, we demonstrate state-of-the-art bitwidth-vs-accuracy across a suite of GPT3, Llama2 and Nemotron-4 models on a wide range of downstream tasks during 4-bit quantization of both weights and activations. 