%The inference accuracy of LLMs during per-block (fine-grained) quantization is significantly influenced by the number format of the operands and per-block scale factors. Several previous works have explored novel number formats to improve accuracy. However, none have explored per-block quantization methods involving clustering that minimize quantization MSE. In this work, we propose LO-BCQ, an iterative block clustering and quantization algorithm that greedily minimizes quantization MSE for any operand (weights and activations) through locally optimal steps at each step of the iteration. We demonstrate that LO-BCQ achieves state-of-the-art perplexity across a suite of GPT3, LLama2 and Nemotron4 models on various downstream tasks such Wikitext-103, LM evaluation harness and MMLU. 

We propose a new iterative block clustering and quantization algorithm called LO-BCQ, that greedily minimizes quantization MSE for any operand (weights and activations) through locally optimal steps at each step of the iteration. We demonstrate that LO-BCQ achieves state-of-the-art perplexity across a suite of GPT3, LLama2 and Nemotron4 models on various downstream tasks. Given its strong inference accuracy with W4A4 quantization, we believe LO-BCQ opens new research avenues for even more aggressive quantization of both weights and activations. Furthermore, our approach requires significantly fewer codebooks than prior codebook-based methods and allows these codebooks to be static (frozen) across models and layers within models. This creates new opportunities to improve inference efficiency, which we plan to explore in future work.