In this section, we present our accuracy studies on downstream tasks comparing LO-BCQ to various other block quantization proposals. Next, we present ablation studies on varying LO-BCQ configurations and our calibration methodology, namely universal vs local.

\begin{table*} [!t]\scriptsize
\begin{center}
\caption{\label{tab:wiki3_ppl} \small PTQ Perplexity (lower is better) on Wikitext-103 dataset with GPT3, Llama2 and Nemotron4 models.}
\begin{tabular}{|c|c||c|c|c||c|c||c|c||} 
\hline
 Method & Bitwidth$^*$ & \multicolumn{7}{c||}{Wikitext-103 PPL ($\Delta$)} \\
  & (W4A4) &\multicolumn{3}{c||}{\cellcolor[gray]{0.95} GPT3} & \multicolumn{2}{c||}{\cellcolor[gray]{0.95} Llama2} & \multicolumn{2}{c||}{\cellcolor[gray]{0.95} Nemotron4} \\

 & & \cellcolor[gray]{0.95} 1.3B & \cellcolor[gray]{0.95} 8B & \cellcolor[gray]{0.95} 22B & \cellcolor[gray]{0.95} 7B & \cellcolor[gray]{0.95} 70B & \cellcolor[gray]{0.95} 15B & \cellcolor[gray]{0.95} 340B \\
 \hline
 BF16 (Pretrained) & 16 &  9.98 & 7.38 & 6.54 & 5.06 & 3.14 & 5.87 & 3.48 \\
\hline
MX4 (g16) & 4.5  & 11.33 (1.35) & 8.15 (0.77) & 7.69 (1.15) & 5.73 (0.67) & 3.58 (0.44) & 8.88 (3.01) & 4.01 (0.53) \\
VSQ (g16) & 4.5 & 10.83 (0.85) & 8.17 (0.79) & 7.12 (0.58) & 835 (829) & 4.96 (1.82) & 7.58 (1.71) & 4.19 (0.71) \\
MXFP4 (g32) & 4.25 & 11.04 (1.06) & 9.12 (1.74) & 10.18 (3.64) & 5.76 (0.70)& 3.69 (0.55) & 8.24 (2.37) & 4.10 (0.62) \\
\rowcolor[gray]{0.9} \textbf{LO-BCQ (g64, 2 codebooks)} & 4.25 & 10.40 (0.42) & 7.61 (0.23) & 6.74 (0.20) & 5.31 (0.25) & 3.35 (0.21) & 6.30 (0.43) & 3.67 (0.19) \\
\rowcolor[gray]{0.9} \textbf{LO-BCQ (g64, 8 codebooks)} & 4.5 & 10.17 (0.19) & \textbf{7.48 (0.10)} & \textbf{6.62 (0.08)} & 5.19 (0.13) & \textbf{3.23 (0.09)} & 6.13 (0.26) & 3.60 (0.12) \\
\rowcolor[gray]{0.9} \textbf{LO-BCQ (g32, 16 codebooks)} & 4.75 & 10.12 (0.14) & \textbf{7.45 (0.07)} & \textbf{6.59 (0.05)} & \textbf{5.15 (0.09)} & \textbf{3.20 (0.06)} & 6.03 (0.16) & \textbf{3.56 (0.08)} \\
\hline
\multicolumn{9}{c}{\scriptsize{$^*$ Bitwidth of weights and activations including the overheads from per-block array (group) scale and codebook selectors}} \\
\end{tabular}
\end{center}
\vspace{-0.3pt}
\end{table*}

\subsection{Experimental Setup}
\vspace{-0.3em}
We perform accuracy studies on GPT3 \citep{shoeybi2020megatronlm} (1.3B, 8B and 22B), Llama2 \citep{touvron2023llama2openfoundation} (7B and 70B) and Nemotron4 (15B and 340B) \citep{nemotron} models. We evaluate PTQ inference accuracy on several downstream tasks including Wikitext-103 \citep{merity2016pointer}, MMLU \citep{hendrycks2021measuringmassivemultitasklanguage} and Eleuther AI's LM evaluation harness \citep{eval-harness}. In LM evaluation harness, we infer on Race (RA), Boolq (BQ), Hellaswag (HS), Piqa (PQ) and Winogrande (WG) tasks and in the MMLU dataset we evaluate all tasks. In all these models, we quantize GEMM layers including Query, Key and Value computations, Projection layer after self attention and the fully-connected layers.

We apply the LO-BCQ algorithm to the operands before inference and pre-calibrate the optimal codebooks. In our experiments, we perform this calibration on one batch of activations from the training data of the GPT3-126M model and the Wikitext-103 dataset. We freeze these optimal codebooks across operands and models during all of our accuracy evaluations. Further, we represent each entry of the codebooks as a $6$-bit integer. That is, once decoded, the inner product computations with a block array during inference can be performed at $6$-bit precision\footnote{In our experiments in this paper, we emulate ("fake") quantization by representing the quantized values in BF16 format. Therefore, the computations are performed in BF16 precision.}. Furthermore, we perform ablation studies on the LO-BCQ configurations with quantization bitwidth ranging from $4.25$-bits to $5$-bits. 

We compare LO-BCQ against previous block quantization works that have explored PTQ of both weights and activations such as VSQ \citep{dai2021vsq}, MX \citep{rouhani2023microscaling}, MXFP \citep{rouhani2023shared}, QuaRot \citep{ashkboos2024quarot}, Atom \citep{zhao2024atom}, OmniQuant \citep{shao2024omniquant} and SmoothQuant \citep{xiao2023smoothquant}. VSQ and MX perform per-block quantization of $16$-element blocks with an $8$-bit scale-factor per-block resulting in an effective bit-width of $4.5$ bits. VSQ quantizes each scalar to INT4 format and per-block scale-factor to INT8 format. MX performs micro-scaling at per-block level with a $1$-bit exponent shared by $2$-element blocks. Each scalar is quantized to INT3. In this paper, we overestimate accuracy of MX by allowing each scalar to have its own exponent, resulting in INT4 precision. The per-block array scale factors of MX are quantized to E8M0 format. Therefore, our evaluation results in a bitwidth of $4.5$ bits. Further, MXFP explores $32$-element blocks with $8$-bit scale-factor per block resulting in an effective bitwidth of $4.25$ bits. The number format of scalars and per-block scale factors are E2M1 and E8M0, respectively. The quantization methodology with these block formats is detailed in \ref{subsec:quant_method}. 

Additionally, we compare weight-only (W4A8) LO-BCQ to other weight-only quantization proposals of equivalent bitwidth such as GPTQ \citep{frantar2023optq}, AWQ \citep{lin2023awq}, QuiP\# \citep{tseng2024quipbetterllmquantization} and AQLM \citep{egiazarian2024aqlm}. For this comparison, we choose a block-array length of $128$ for LO-BCQ, matching the group-size of other works.

\subsection{Accuracy studies on downstream tasks}
\vspace{-0.3em}
We present our comprehensive accuracy evaluations across the Nemotron4, Llama2 and GPT3 models, on the Wikitext-103, LM evaluation harness and MMLU datasets. For convenience, we present select LO-BCQ configurations with $L_b=8$ in this section. See \ref{sec:appendix} for accuracy studies on other configurations.

\subsubsection{Perplexity on Wikitext-103}

As shown in Table \ref{tab:wiki3_ppl}, across large models such as Llama2-70B, Nemotron4-340B and GPT3-22B, $4.5$-bit LO-BCQ achieves $\le0.12$ loss in perplexity compared to the unquantized baseline on the Wikitext-103 dataset. MX, MXFP and VSQ perform per-block quantization by associating a scale-factor to each block (or a block array) and with a single number format (quantizer) across blocks. On the other hand, in addition to per-block array scaling, LO-BCQ allows a block to flexibly map to a codebook that best represents it from a set of codebooks. This flexibility allows LO-BCQ to achieve better perplexity. Furthermore, we find that with a larger quantization bitwidth, LO-BCQ achieves better perplexity across models as expected. We achieve these improvements during PTQ, i.e., without any additional training or finetuning.

The number format of per-group (or block array) scale-factor has a significant impact on accuracy. VSQ is unable to sufficiently capture the range of activations with its INT8 scale-factors as observed in Llama2-7B, while it outperforms the E8M0 scale-factors of MX in GPT3-22B due to better resolution when representing large values. Across various models, we find that the E4M3 format of LO-BCQ provides sufficient range and resolution to represent the scale-factors.

Table \ref{tab:w4a4_comparison} compares LO-BCQ to other PTQ methods that quantize both weights and activations. Here, all methods use a group (block array) size of $128$. As shown, LO-BCQ significantly outperforms the prior-art. While the prior-art proposes various techniques for suppressing outliers in both weights and activations, LO-BCQ calibrates a set of codebooks that captures various non-uniform value distributions.

\begin{table} [!t]\scriptsize
\setlength{\tabcolsep}{4.75pt}
\caption{\small Comparing perplexity loss (lower is better) of LO-BCQ to other 4-bit (W4A4) quantization works such as QuaRot, Atom, OmniQuant and SmoothQuant. Here, the perplexity loss is on Wikitext-103 dataset for LO-BCQ and Wiki2 for others.}. \label{tab:w4a4_comparison}~
\centering
\begin{tabular}{|c|c|c||c|c|} 
 \hline
 Method & Bitwidth & \multicolumn{2}{c|}{$\Delta$ Wiki PPL } \\
 (g128) & (W4A4) & \cellcolor[gray]{0.9} Llama2-7B & \cellcolor[gray]{0.9} Llama2-70B \\
 \hline
 SmoothQuant & 4.13 & 77.65 & -  \\
 OmniQuant & 4.13 & 9.14 & - \\
 QuaRot & 4.13 & 0.46 &  0.29 \\
 Atom & 4.13 & 0.56 & 0.36 \\
  \rowcolor[gray]{0.9} 
  \textbf{LO-BCQ (2 codebooks)} & 4.19 & 0.14 &  \textbf{0.09} \\
   \rowcolor[gray]{0.9}
   \textbf{LO-BCQ (4 codebooks)} &  4.31 &  0.12 &  \textbf{0.07} \\
   \rowcolor[gray]{0.9} 
   \textbf{LO-BCQ (8 codebooks)} &  4.44 &  \textbf{0.09} & \textbf{0.06} \\
   \rowcolor[gray]{0.9}
  \textbf{LO-BCQ (16 codebooks)} & 4.56 & \textbf{0.08} &  \textbf{0.05} \\
 \hline
\end{tabular}
\end{table}

\begin{table*} [!t]\scriptsize
\setlength{\tabcolsep}{4.75pt}
\caption{\small Comparing perplexity loss of weight-only (W4A16) LO-BCQ to other weight-only quantization methods. \label{tab:wonly_comparison}~}
\centering
\begin{tabular}{|c|c|c||c|c|c|c||c|c|c|c||} 
 \hline
 Method & Bitwidth & \#codebooks & \multicolumn{4}{c|}{\cellcolor[gray]{0.95} Llama2-7B } & \multicolumn{4}{c|}{\cellcolor[gray]{0.95} Llama2-70B} \\
  & (W4A16) & & $\Delta$ Wiki PPL & PQ & WG & HS & $\Delta$ Wiki PPL & PQ & WG & HS \\
 \hline
 GPTQ (g128) & 4 & - &  0.37 & 76.61 & 68.19 & 55.44 &  0.23 & 81.23 & 75.61 & 63.47 \\
 AWQ (g128) & 4.13 & - & 0.13 & 77.09 & \textbf{69.53} & 56.25 & \textbf{0.09} & - & - & - \\
 QuiP\# & 4.02 & $2^{16}$ & 0.17 & 77.91 & 66.85 & 55.78 & 0.10 & 81.45 & 76.8 & 63.51 \\
 AQLM & 4.14 & $2^{16}$ & \textbf{0.09} & 78.24 & 67.32 & 55.99 & \textbf{0.07} & 81.5 & 76.48 & 63.69 \\
 \rowcolor[gray]{0.9}
   & 4.19 & 2 & 0.14 & \textbf{78.29} & 68.11 & \textbf{56.55} & \textbf{0.09} & 81.18 & \textbf{79.24} & \textbf{64.57} \\
  \rowcolor[gray]{0.9}
   &  4.31 & 4 & 0.12 & 78.18 & 68.59 & \textbf{56.75} & \textbf{0.07} & \textbf{81.77} & \textbf{78.30} & \textbf{64.99} \\
   \rowcolor[gray]{0.9}
   &  4.44 &  8 &  \textbf{0.09} & 77.69 & 68.75 & \textbf{56.75} &  \textbf{0.06} & 81.50 & \textbf{79.79} & \textbf{65.11} \\
   \rowcolor[gray]{0.9}
  \multirow{-4}{*}{\textbf{LO-BCQ (g128)}} & 4.56 & 16 & \textbf{0.08} & 77.97 & 68.90 & \textbf{56.76} & \textbf{0.05} & 81.45 & \textbf{80.43} & \textbf{65.04} \\
 \hline
\end{tabular}
\end{table*}

\begin{table*} [!t]\scriptsize
\setlength{\tabcolsep}{4.75pt}
\begin{center}
\caption{\label{tab:lm_harness_acc} \small LM evaluation Harness 0-shot accuracy (higher is better) on Llama2 and GPT3 models.}
\begin{tabular}{|c|c||c|c|c|c|c|c||c|c|c|c|c|c|c|} 
\hline
 Method &  Bitwidth  & \multicolumn{6}{c||}{\cellcolor[gray]{0.95} Llama2-7B} &  \multicolumn{6}{c|}{\cellcolor[gray]{0.95} Llama2-70B} \\
 & (W4A4) & RA & BQ & WG & PQ & HS & Avg ($\Delta$ \%) & RA & BQ & WG & PQ & HS & Avg ($\Delta$ \%) \\
\hline
BF16 & 16 & 44.4 & 79.29 & 69.38 & 78.07 & 57.10 & 65.65 & 48.8 & 85.23 & 79.95 & 81.56 & 65.27 & 72.16 \\
QuaRot (g128) & 4.13 & - & - & 63.77 & 76.77 & - & - & - & - & 76.24 & 82.43 & - & - \\
MX4 (g16)  & 4.5 & 41.43 & 73.98 & 66.22 & 77.04 & 55.19 & 62.77 (2.88) &\textbf{48.04} & 82.41 & 76.40 & \textbf{80.58} & 63.24 & 70.13 (2.03) \\
VSQ (g16)  & 4.5 & 31.39 & 65.75 & 55.49 & 67.30 & 43.51 & 52.69 (12.96) & \textbf{47.85} & 82.29 & 77.27  & 79.82 & 61.40 & 69.73 (2.43) \\
MXFP4 (g32) & 4.25 & 41.34 & 74.00 & 67.48 & \textbf{77.53} & 54.22 & 62.91 (2.74) & 47.75 & 83.06 & 76.32 & \textbf{80.58} & 63.24 & 70.19 (1.97) \\
\rowcolor[gray]{0.9}
\textbf{LO-BCQ (g64, 2 codebooks)}  & 4.25 & 42.49 & 77.58 & \textbf{68.90} & \textbf{77.09} & 55.93 & 64.40 (1.25) & \textbf{49.0}  & 82.82          & 78.77 & \textbf{81.45} & 64.21 & \textbf{71.25 (0.91)} \\
\rowcolor[gray]{0.9}
\textbf{LO-BCQ (g64, 8 codebooks) } & 4.5  &  42.58 & 77.43 & \textbf{69.77} & \textbf{77.09} & \textbf{56.51} & \textbf{64.68 (0.97)} & \textbf{49.28} & 84.03  & 78.37 & \textbf{81.45} & \textbf{64.76} & \textbf{71.58 (0.58)} \\
\rowcolor[gray]{0.9}
\textbf{LO-BCQ (g32, 16 codebooks)} & 4.75 &  \textbf{43.73} & 77.86 & \textbf{68.90} & \textbf{77.86} & \textbf{56.52} & \textbf{64.97 (0.68)} & \textbf{49.28} & \textbf{84.93} & \textbf{80.66} & \textbf{81.34} & \textbf{65.18} & \textbf{72.28 (+0.12)} \\
\hline
\rowcolor[gray]{0.95}
&  & \multicolumn{6}{c||}{GPT3-8B} &  \multicolumn{6}{c|}{GPT3-22B} \\
\hline
BF16 & 16   &  41.34  & 68.32  & 67.88 & 78.78  & 54.16 & 62.10 & 40.67 & 76.54 & 70.64 & 79.16 & 57.11 & 64.82 \\ 
  MX4 (g16) & 4.5  &  38.28 & 66.27 & 65.11 & 75.63 & 50.77 & 59.21 (2.89) & 39.04 & 72.26 & 67.96 & 77.86 & 54.77 & 62.38 (2.44)\\ 
  VSQ (g16) & 4.5  &  \textbf{40.86} & 63.91 & 66.93 & 76.28 & 51.38 & 59.87 (2.23) & \textbf{40.57} & 65.81 & \textbf{69.61}  & 77.20 & 54.82 & 61.60 (3.22)  \\  
  MXFP4 (g32) & 4.25 & 39.71 & 65.35 & 67.01 & 76.12 & 50.22 & 59.68 (2.42) &  39.14 & 69.61 & 64.17 & 75.68 & 47.60 & 59.24 (5.58) \\  
  \rowcolor[gray]{0.9}
  \textbf{LO-BCQ (g64, 2 codebooks)} & 4.25 &  \textbf{40.48} & \textbf{69.20} & 66.85 & 77.31 & 53.06 & \textbf{61.38 (0.72)} & \textbf{40.48} & 75.41 & 69.14 & \textbf{78.24} & 56.06 & \textbf{63.87 (0.95)} \\ 
  \rowcolor[gray]{0.9}
  \textbf{LO-BCQ (g64, 8 codebooks)} & 4.5  &  39.43 & \textbf{69.45} & \textbf{67.72} & \textbf{77.75} & \textbf{53.71} & \textbf{61.61 (0.49)} & 39.43 & \textbf{77.09} & \textbf{70.17} & \textbf{78.62} & \textbf{56.60} & \textbf{64.38 (0.44)}  \\  
  \rowcolor[gray]{0.9}
  \textbf{LO-BCQ (g32, 16 codebooks) } & 4.75 &  39.62 & \textbf{69.30} & \textbf{67.00} & 77.37 & \textbf{53.51} & \textbf{61.36 (0.74)} & \textbf{39.62} & 75.35 & 69.30 & \textbf{78.89} & \textbf{56.64} & \textbf{63.96 (0.86) }\\  
\hline
\end{tabular}
\end{center}
\vspace{-1em}
\end{table*}

\begin{table} [!t]\scriptsize
\caption{\small MMLU accuracy (higher is better) with Nemotron4-15B, Llama2-7B, 70B and GPT3-22B models. \label{tab:nemo_acc}~}
\begin{tabular}{|c|c|c|c|c|c|} 
\hline
 Method &  Bitwidth & \cellcolor[gray]{0.95} Nemo4 & \multicolumn{2}{c|}{\cellcolor[gray]{0.95} Llama2} & \cellcolor[gray]{0.95} GPT3 \\
 & (W4A4) & 15B & 7B & 70B & 22B \\
 \hline
 \hline
 BF16                          & 16  & 64.3 & 45.8 & 69.12 & 38.75 \\ 
 MX4 (g16)                          & 4.5 & 58.15 & 41.38  & 65.73  & 37.07  \\ 
 VSQ  (g16)                          & 4.5 & 57.38  & 26.48  & 62.46  & 37.79  \\  
 MXFP4 (g32)                        & 4.25 & 58.28  & 37.64  & 66.16  & 32.26  \\ 
 \rowcolor[gray]{0.9}
 \textbf{LO-BCQ (g64, 2 cb)}  & 4.25 & 63.17  & 43.90  & 68.07  & 36.71 \\  
 \rowcolor[gray]{0.9}
 \textbf{LO-BCQ (g64, 8 cb)}  & 4.5  & \textbf{63.72} & 43.90  & 68.17  & 38.13  \\  
 \rowcolor[gray]{0.9}
 \textbf{LO-BCQ (g32, 16 cb)} & 4.75 & \textbf{64.33} & 44.50  & 68.27  & 38.34  \\  
 \hline
\end{tabular}
\vspace{-0.2em}
\end{table}

\begin{table} [!t]\scriptsize
\setlength{\tabcolsep}{4.75pt}
\centering
\caption{\footnotesize Perplexity on Wikitext-103 dataset across various LO-BCQ configurations \label{tab:ppl_abalation}~}
\begin{tabular}{|c||c|c|c|c||c|c||c|} 
\hline
 $L_b \rightarrow$& \multicolumn{4}{c||}{8} & \multicolumn{2}{c||}{4} & 2\\
 \hline
 \backslashbox{$L_A$\kern-1em}{\kern-1em$N_c$} & 2 & 4 & 8 & 16 & 2 & 4 & 2  \\
 %$N_c \rightarrow$ & 2 & 4 & 8 & 16 & 2 & 4 & 2 \\
 \rowcolor[gray]{0.9}
 \multicolumn{8}{|c|}{\textbf{Llama2-70B (FP32 PPL = 3.14)}} \\ 
 64 & 3.35 & 3.25 & 3.23 & 3.21 &  3.31 & 3.22 & 3.27 \\
 32 & 3.27 & 3.24 & 3.22 & 3.20 &  3.25 & 3.22 &  3.22 \\
 16 & 3.25 & 3.22 & 3.20 & 3.19 &  3.23 & 3.20 &  3.20 \\
 \rowcolor[gray]{0.9}
 \multicolumn{8}{|c|}{\textbf{GPT3-22B (FP32 PPL = 6.54)}} \\ 
 64 & 6.74 & 6.64 & 6.62 &  6.63 &  6.71 &  6.64 & 6.64 \\
 32 & 6.67 & 6.64 & 6.61 & 6.59 &  6.65 &  6.64 & 6.60  \\
 16 & 6.67 & 6.63 & 6.59 & 6.61 &  6.66 &  6.63 & 6.62  \\
 \hline
\end{tabular}
\end{table}

\begin{table} [!t]\scriptsize
\setlength{\tabcolsep}{4.75pt}
\caption{\footnotesize Perplexity on Wikitext-103 dataset with universally calibrated vs locally calibrated codebooks \label{tab:ppl_rom_vs_ram}~}
\begin{tabular}{|c||c|c|c|c||c|c|c|c|} 
\multicolumn{5}{c}{Llama2-7B (FP32 PPL = 5.06), $L_b=8$} \\
 \hline
 \backslashbox{$L_A$\kern-1em}{\kern-1em$N_c$} & 2 & 4 & 8 & 16 & 2 & 4 & 8 & 16  \\
 \rowcolor[gray]{0.9}
 \multicolumn{5}{|c||}{Universally Calibrated Codebooks} & \multicolumn{4}{c|}{Layerwise Calibrated Codebooks} \\ 
 64 & 5.31 & 5.26 & 5.19 & 5.18 & 5.29 & 5.22 & 5.19 & 5.17 \\
 32 & 5.23 & 5.25 & 5.18 & 5.15 & 5.23 & 5.19 & 5.17 & 5.15 \\
 16 & 5.23 & 5.19 & 5.16 & 5.14 & 5.20 &  5.17 & 5.15 & 5.14 \\
 \hline
\end{tabular}
\end{table}


\subsubsection{Accuracy on LM evaluation harness tasks}
Across $0$-shot LM evaluation harness tasks in Table \ref{tab:lm_harness_acc}, LO-BCQ shows significant improvement in average accuracy compared to MX, MXFP and VSQ at equivalent bitwidth. Further, across models during $4.5$-bit quantization, LO-BCQ achieves $<1$\% loss in average accuracy compared to the respective unquantized baselines. When the bitwidth of LO-BCQ is increased by varying its configuration, we find that the average accuracy generally increases albeit with a few exceptions. Although these variations are small ($<0.5$\%), we believe that they arise due to the universal calibration of codebooks. Our codebooks are calibrated on a batch of training data from the Wikitext-103 dataset and the GPT3-126M model and remain frozen across all datasets and models. 

Although the focus of this work is weight+activation quantization, we also compare to prior art weight-only quantization proposals for completeness. Table \ref{tab:wonly_comparison} compares weight-only (W4A8) LO-BCQ with a block array size of $128$ to other weight-only quantization proposals such as GPTQ and AWQ of comparable block array size and effective bit-width. As shown, LO-BCQ with $2$, $4$, $8$ and $16$ codebooks with effective bitwidth of $4.19$, $4.31$, $4.44$ and $4.56$, respectively, achieves significantly lower perplexity loss. It is worth noting that we evaluate this loss on Wikitext-103 dataset, which is a much larger dataset compared to Wikitext2 used by other works. Further, we compare LO-BCQ against other codebook-based quantization methods such as Quip\# and AQLM. As shown, LO-BCQ achieves comparable accuracy using only a small codebook size ($8$ codebooks with $16$ entries each) compared to significantly larger codebook sizes ($2^{16}$ codebooks with $8$ entries each) required by QuiP\# and AQLM.

\subsubsection{Accuracy on MMLU tasks}
Similarly, in $5$-shot MMLU tasks LO-BCQ achieves $<1$\% loss in average accuracy with $4.5$-bits per scalar compared to respective unquantized baselines across GPT3-22B and Llama2-70B models. Further, LO-BCQ achieves a significantly better accuracy compared to all of our block quantization baselines such as VSQ, MX and MXFP4 at equivalent bitwidth. Across Llama2 models, LO-BCQ with a smaller bitwidth ($4.25$-bits) outperforms VSQ and MX4 with a comparatively larger bitwidth ($4.5$-bits). While the $0.5$-bit overhead in VSQ and MX4 are used on per-block array scale-factors, the $0.25$-bit overhead of LO-BCQ is shared between scale-factors and codebook selectors. Therefore, the superior accuracy of LO-BCQ can be attributed to the better representation by selecting the best codebook for each block.

\subsection{Ablation Studies}
Table \ref{tab:ppl_abalation} shows the perplexity of LO-BCQ on Wikitext-103 dataset and across Llama2-70B and GPT3-22B models when its configuration is varied. For a given $L_b$ (block length), larger number of codebooks results in better perplexity. This is intuitive since larger number of codebooks leads to better representation of the values in each block since LO-BCQ allows it to map to the codebook with best representation. Further, when the block array size is reduced, we achieve better perplexity. The block array corresponds to the granularity of normalization. As discussed in section \ref{subsec:lobcq_convergence_and_init}, normalization improves convergence of LO-BCQ and results in better perplexity. When comparing configurations with same bitwidth, we find that the configuration with larger number of codebooks is better than smaller block array. This shows that the per-block metadata is better utilized for codebook selectors than scale factors. Furthermore, we find that reducing the block length ($L_b$) below $8$ results in diminishing returns. This is because, the overhead of storing codebook selectors is larger for a smaller block. For a given bitwidth, configuration with smaller $L_b$ has fewer codebooks. Therefore, these configurations result in larger loss in perplexity.

Table \ref{tab:ppl_rom_vs_ram} compares the perplexity with universally calibrated codebooks to codebooks calibrated layerwise (per-tensor) in Llama2-7B model. The layerwise calibrated codebooks achieve slightly better perplexity when the number of codebooks are small (e.g. $N_c=2$). However, they do not provide significant benefits when $N_c>4$ despite the comparatively larger calibration effort. Therefore, in our experiments in this paper, we have largely explored universally calibrated codebooks. 