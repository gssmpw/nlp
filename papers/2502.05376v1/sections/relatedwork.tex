The rapid growth in use of LLMs has motivated several efforts towards quantization in order to reduce computational and storage requirements, with recent interest in pushing towards sub-8-bit precision. 

\subsection{Weight-only Quantization}
Proposals such as \citep{wang2023bitnet,tseng2024quipbetterllmquantization,egiazarian2024aqlm} explore extreme weight quantization while maintaining activations at a precision of $8$-bits or larger.
%Bitnet, Quip# and AQLM
%optq,squeezeLLM,quip,olive
%\subsection{Weight-only Quantization}
Proposals such as \citep{frantar2023optq, bai2021efficient, yao2022zeroquant} explore sub-8-bit weight quantization while maintaining activations at a high precision. OPTQ \citep{frantar2023optq} utilizes approximate second order information to achieve 4-bit quantization of weights in GPT-scale models with negligible accuracy loss. Other efforts such as \citep{yao2022zeroquant} and \citep{bai2021efficient} propose fine-grained knowledge distillation for aggressive weight quantization. Since these techniques update the pre-trained weights of the model, they pose generalizability concerns for downstream tasks. Post-training quantization (PTQ) is of great interest since it does not suffer from these concerns. SmoothQuant \citep{xiao2023smoothquant} observes that activations have much wider range than weights due to the presence of outliers. Therefore, it identifies the outliers in activations and migrates them to the weights of the previous layer and achieves $8$-bit integer quantization of both weights and activations in LLMs. OliVe \citep{Guo2023olive} proposes a mixed format scheme that encodes outliers at a higher precision and prunes the adjacent victim. While OliVe achieves superior accuracy compared to \citep{wei2023outlier}, it suffers significant degradation during 4-bit PTQ. Other efforts such as \citep{lin2023awq,kim2023squeezellm,chee2023quip} explore sub-8-bit weight quantization while maintaining activations at 8-bits. AWQ \citep{lin2023awq} identifies salient weights based on the magnitudes of activations and preserves these weights at high precision through activation-aware scaling. SqeezeLLM \citep{kim2023squeezellm} decomposes the weight matrix into a sparse high-precision outlier matrix and dense low-precision inlier matrix, and proposes sensitivity-aware non-uniform quantization to achieve $3$-bit weight quantization. Further, \citep{chee2023quip} proposes an adaptive rounding procedure to achieve $2$-bit quantization of weights. While these techniques are effective for aggressive quantization of weights, due to the large algorithmic complexity of these methods, they have not been successfully applied to activation quantization.

\subsection{Weight and Activation Quantization}
VSQ \citep{dai2021vsq} proposes vector-scaled quantization where in addition to the vectorwise quantization of operands, the per-vector scale factors are themselves quantized to low precision. VSQ demonstrates negligible accuracy loss in BERT models during $4$-bit INT quantization at a $16$-element vector granularity for both weights and activations. FineQuant \citep{kim2023finequant} proposes a technique that reduces the quantization granularity (vector or group size) until the quantization range falls below a pre-determined threshold. ZeroQuant-V2 \citep{yao2023zeroquantv2} explores fine-grained quantization (block size of $64$ to $256$) alongside other PTQ techniques proposed in \citep{frantar2023optq,yao2022zeroquant} and achieves INT4 weight and INT8 activation quantization for GPT-scale models with negligible accuracy loss. The authors of \citep{zhang2023integer} study low-precision floating point formats during fine-grained quantization and propose $4$-bit FP (E2M1) per-channel weight quantization and $8$-bit FP (E4M3) per-tensor activation quantization for LLMs. Similarly, ZeroQuant-FP \citep{wu2023zeroquantfp} achieves negligible PTQ accuracy loss with $4$-bit FP fine-grained quantization for weights and $8$-bit FP tokenwise quantization for activations. Flexpoint \citep{k√∂ster2017flexpoint} and MSFP \citep{rouhani2020msfp} are block floating-point \citep{drumond2018bfp} inspired formats with shared per-block exponents. MX formats \citep{rouhani2023microscaling} improve over MSFP by introducing sub-block scaling. Within a 16-element block, every two scalar elements share an exponent adjustment. Similarly, BSFP \citep{lo2023block} introduces a $7$-bit floating point scale factor for every $2$-element sub-word with a $16$-element block and achieves $4$-bit weight quantization while maintaining $8$-bit MSFP quantization for activations. More recently, the authors of \citep{rouhani2023microscaling} have expanded the MX formats to include MXFP \citep{rouhani2023shared} formats where each element in a vector of length $32$ is quantized to $4$, $6$ or $8$-bit FP format and the per-vector scale factors are quantized to $8$-bit exponents.

Minimizing quantization MSE using the 1D (Lloyd-Max) and 2D Kmeans clustering has been explored in \citep{han2016deepcompression,cho2021dkmdk,cho2023edkm} and \citep{vanbaalen2024gptvq}, respectively. In contrast, LO-BCQ iteratively optimizes block clustering alongside Lloyd-Max based optimal scalar quantization of block clusters.
%In summary, quantization of neural networks is a rich and active field, with efforts proposing new number formats, quantization-aware training and post-training quantization algorithms, and efficient hadware implementations. 
This work advances the state-of-the-art by pushing the envelope of inference accuracy for a given quantization bitwidth in LLMs, and specifically demonstrating the feasibility of 4-bit quantization of weights and activations with minimal accuracy degradation.

%The main challenges during PTQ of LLMs are the wide range of operand value distributions and the presence of important outliers \citep{bondarenko2021understanding,thierry2020adafloat,xiao2023smoothquant}. To address these challenges, several prior works explore techniques such as outlier suppression and fine-grained quantization to reduce precision while preserving the pre-trained weights.

% update pre-trained weights
% sub-8-bit weight quantization while maintaining activations at higher precision.
% Main challenges during PTQ are

%The main challenges during LLM quantization are the wide range of operand value distributions and the presence of important outliers \citep{bondarenko2021understanding,thierry2020adafloat,xiao2023smoothquant}. OPTQ \citep{frantar2023optq} utilizes approximate second-order information to achieve 4-bit quantization of weights in GPT-scale models with negligible accuracy loss. Other efforts such as \citep{bai2021efficient} and Zeroquant\citep{yao2022zeroquant} explore fine-grained knowledge-distillation to achieve sub-8-bit weight quantization while maintaining accuracy. These techniques update the pre-trained weights of the model and pose generalizability concerns for downstream tasks. In contrast, other efforts have explored techniques such as outlier-suppression and fine-grained quantization to reduce precision while preserving the pre-trained weights.

%\subsection{Outlier Suppression}
%LLM.int8() \citep{dettmers2022llmint8} identifies outliers in activations and preserves them at $16$-bit precision, while quantizing other values to $8$-bit integers. SmoothQuant \citep{xiao2023smoothquant} observes that activations have much wider range than weights due to the presence of outliers. Therefore, it identifies the outliers in activations and migrates them to the weights of the previous layer and achieves $8$-bit integer quantization of both weights and activations in LLMs. Similarly, \citep{wei2023outlier} argues that the outputs of LayerNorm structures significatly contribute to the outliers in LLM operands, and proposes migrating the scaling parameter of LayerNorm into subsequent layers in an equivalent transformation. Combined with tokenwise clipping of activations, this technique achieves accurate 6-bit PTQ of both weights and activations of BERT models. OliVe \citep{Guo2023olive} proposes a mixed format scheme that encodes outliers at a higher precision and prunes the adjacent victim. While OliVe achieves superior accuracy compared to \citep{wei2023outlier}, it suffers significant degradation during 4-bit PTQ. Other efforts such as \citep{lin2023awq,kim2023squeezellm,chee2023quip} explore sub-8-bit weight quantization while maintaining activations at 8-bits. AWQ \citep{lin2023awq} identifies salient weights based on the magnitudes of activations and preserves these weights at high precision through activation-aware scaling. SqeezeLLM \citep{kim2023squeezellm} decomposes the weight matrix into a sparse high-precision outlier matrix and dense low-precision inlier matrix, and proposes sensitivity-aware non-uniform quantization to achieve $3$-bit weight quantization. Further, \citep{chee2023quip} proposes an adaptive rounding procedure to achieve $2$-bit quantization of weights. While these techniques are effective for aggressive quantization of weights, the large algorithmic complexity of identifying outliers and the wider dynamic range of activations necessitates fine-grained quantization. A recent work \citep{dettmers2023case} has studied k-bit scaling laws for weight quantization and demonstrated that fine-grained quantization and associated number format explorations are more effective techniques compared to outlier suppression. 

%\subsection{Fine-grained quantization}


%In summary, quantization of neural networks is a rich and active field, with efforts proposing new number formats, quantization-aware training and post-training quantization algorithms, and efficient hadware implementations. This work advances the state-of-the-art by pushing the envelope of inference accuracy for a given quantization bitwidth in LLMs, and specifically demonstrating the feasibility of 4-bit quantization of weights and activations with minimal accuracy degradation.

%% LUT-based quantization
% -- codebook
% -- MOKEY
% -- Learnable LUT
% -- mention vector quantization?



