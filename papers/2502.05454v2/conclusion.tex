\section{Conclusions and Limitations}
\label{sec:conclusion}

In this paper, we studied a temporal alignment objective for the representations used in (goal- and language-conditioned) behavior cloning.
This additional structure provides robust compositional generalization capabilities in both real-world robotics tasks and simulated RL benchmarks.
Perhaps surprisingly, these results suggest that generalization properties usually attributed to reinforcement learning methods may be attainable with supervised learning with well-structured, temporally-consistent representations.

\paragraph{Limitations and Future Work}
            While \Method{} consistently outperformed behavior cloning in real world and simulation evaluations, the degree of improvement degrades when behavior cloning cannot solve the task at all.
    Future work could examine how to improve compositional generalization in such cases through additional structural constraints on the representation space.
    To scale to more complex settings, similar approaches with more complex architectures such as transformers and diffusion policies may be needed for policy and/or representation learning.
    TRA could also be combined with hierarchical task decomposition using VLMs, or with other forms of planning.

