\label{sec:appendix}

\textfloatsep=12pt plus 5pt minus 2pt
\intextsep=12pt plus 5pt minus 2pt

\section{Code and Website}
\label{sec:code}

A website with code, additional visualizations, and videos is available at \url{https://tra-paper.github.io/}.

\section{\Method~Implementation}
\label{app:tra_impl}

In this section, we provide details on the implementation of temporal representation alignment (TRA) and its training process.

\subsection{Dataset Curation}\label{sec:dataset}

We use an augmented version of BridgeData. We augment the dataset by rephrasing the language annotations, as described by~\citep{myers2023goal}, with 5 additional rephrased language instruction for each language instruction present in the dataset, and randomly sample them during training.

During data loading process, for each observation that is sampled with timestep $k$, we also sample $k^{+} \triangleq \text{min}(k+x,H), x\sim\text{Geom}(1-\gamma)$, and load $s_{k}$ along with $s_{k^{+}}$.
We employ random cropping, resizing, and hue changes during training process image robustness.
 We set $\gamma=0.95$ for policy training on BridgeData.
\subsection{Policy Training}

We use a ResNet-34 architecture for the policy network.
We train our policy with one Google V4-8 TPU VM instance for 150,000 steps, which takes a total of 20 hours.
We use a learning rate of $3 \times 10^{-4}$, 2000 linear warm-up steps, and a MLP head of 3 layers of 256 dimensions after encoding the observation representations as well as goal representations.

\section{Baseline Implementations}
\label{app:baselines}

We summarize the implementation details of the baselines discussed in \cref{sec:baselines}.

\subsection{Octo}
We use the Octo-base 1.5 model publicly available on HuggingFace for evaluating Octo baselines.
We use inference code that is readily available for both image- and language- conditioned tasks.
During inference, we use an action chunking window of 4 and an execution horizon window of 4.
\subsection{Behavior Cloning}
We use the same architecture for LCBC and GCBC as in \citet{walke2023bridgedata,myers2023goal}.
During the training process we use the same hyperparameters as TRA.

\subsection{Advantage Weighted Regression}
In order to train an AWR agent without separately implementing a reward critic, we follow \citet{eysenbach2022contrastive} and use a surrogate for advantage:
\begin{equation}
    \mathcal{A}(s_t) = \mathcal{L}_{\text{NCE}}\bigl(f(s_t), f(g)\bigr) - \mathcal{L}_{\text{NCE}}\bigl(f(s_{t+1}), f(g)\bigr).
\end{equation}

Here, $f$ can be any of the encoders $\phi$, $\xi$, $\psi$. $\mathcal{L}$ is the same InfoNCE loss defined \cref{sec:approach}, and $g$ is defined as either the goal observation or the goal language instruction, depending on the modality.

And we extract the policy using advantage weighted regression (AWR)~\citep{neumann2008fitted}:
\begin{equation}
    \pi \leftarrow \arg \max_\pi \E_{s, a \sim \mathcal{D}} \Bigl[\log \pi(a|s, z) \exp\bigl(A(s,a) / \beta\bigr)\Bigr].
    \label{eq:awr}
\end{equation}

During training, we set $\beta$ to 1, and we use a batch size of 128, the same value as policy training for our method.

\section{Experiment Details}\label{sec:experiment_details}
In this section, we go through our experiment details and how they are set up.
During evaluation, we randomly reset the positions of each item within the table, and perform 5 to 10 trials on each task, depending on whether this task is important within each scene.
We examine tasks that are seen in BridgeData, which include conventionally less challenging tasks such as object manipulation, and challenging tasks to learn within the dataset such as cloth folding and drawer opening.

\subsection{List of Tasks}

\cref{tab:breakdown} describes each task within each scene, and the language annotation used when the policy is used for inference. Every task that is outside of the drawer scene are multiple step, and require compositional generalization.

\begin{table}[htb]
    \caption{Task Instructions}
    \label{tab:breakdown}
    \centering

    \def\insbox#1{\parbox{5cm}{\smallskip\fontsize{7}{7}\fontfamily{pag}\selectfont #1\smallskip}}
    \resizebox{\textwidth}{!}{\begin{tabular}{c|cll}
            \toprule
            \textbf{Scene}                              & \textbf{Count} & \textbf{Task Description}                                             & \textbf{Instruction}                                                                                                                     \\
            \midrule
            \multirow{3}{*}{Drawer}                     & 10             & {open the drawer}                                                     & \insbox{``open the drawer''}                                                                                                             \\
                                                        & 10             & {put the mushroom in the drawer}                                      & \insbox{``put the mushroom in the drawer''}                                                                                              \\
                                                        & 10             & {close the drawer}                                                    & \insbox{``close the drawer''}                                                                                                            \\
            \midrule
            \multirow{4.5}{*}{Task Generalization}      & 5              & {put the spoons on the plates }                                       & \insbox{``move the spoons onto the plates.''}                                                                                            \\
                                                        & 5              & {put the spoons on the towels }                                       & \insbox{``move the spoons on the towels''}                                                                                               \\
                                                        & 6              & {fold the cloth into the center from all corners}                     & \insbox{``fold the cloth into center''}                                                                                                  \\
                                                        & 10             & {sweep the towels to the right}                                       & \insbox{``sweep the towels to the right of the table''}                                                                                  \\
            \midrule
            \multirow{3.25}{*}{Semantic Generalization} & 10             & {put the sushi and the corn on the plate}                             & \insbox{``put the food items on the plate''}                                                                                             \\
                                                        & 5              & {put the sushi and the mushroom in the bowl}                          & \insbox{``put the food items in the bowl''}                                                                                              \\
                                                        & 10             & {put the sushi, corn, and the banana in the bowl}                     & \insbox{``put everything in the bowl''}                                                                                                  \\
            \midrule
            \multirow{5}{*}{Tasks With Dependency}      & 10             & {take mushroom out of drawer}                                         & \insbox{``open the drawer and then take the mushroom out of the drawer''}                                                                \\
                                                        & 10             & {move bell pepper and sweep towel}                                    & \insbox{``move the bell pepper to the bottom right corner of the table, and then sweep the towel to the top right corner of the table''} \\
                                                        & 10             & {put the corn on the plate, \emph{and then} put the sushi in the pot} & \insbox{``put the corn on the plate and then put the sushi in the pot''}                                                                 \\
            \bottomrule
        \end{tabular}}
\end{table}

\subsection{Inference Details}

During inference, we use a maximum of 200 timesteps to account for long-horizon behaviors, which remains the same for all policies.
We determine a task as successful when the robot completes the task it was instructed to within the timeframe.
For evaluating baselines, we use 5 trials for each of the tasks.

\section{Additional Visualizations} \label{sec:additional_viz}

In this section, we show additional visualizations of TRA's execution on compositionally-OOD tasks.
We use \textit{folding, taking mushroom out of the drawer}, and \textit{corn on plate, then sushi in the pot} as examples, as these tasks require a strong degree of dependency to complete at \cref{fig:additional_viz}.

\begin{figure}[htb]\label{fig:additional_viz}
    \centering
    \vbox{
        \hbox{\includegraphics[width=0.9\textwidth]{figures/additional_viz0.pdf}}
        \vspace*{3ex}
        \hbox{\includegraphics[width=0.9\textwidth]{figures/additional_viz1.pdf}}
        \vspace*{3ex}
        \hbox{\includegraphics[width=0.9\textwidth]{figures/additional_viz2.pdf}}
        \vspace*{1ex}
    }
    \caption{TRA performs compositional generatlization over a variety of tasks seen within BridgeData.}
\end{figure}

\subsection{Failure Cases} \label{sec:failure_cases}
We break down failure cases in this section.
While TRA performs well in compositional generalization, it cannot counteract against previous failures seen with behavior cloning with a Gaussian Policy.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/failure_tra.pdf}
    \vspace*{1ex}
    \caption{Most of the failure cases came from the fact that a policy cannot learn depth reasoning, causing early grasping or late release, and it has trouble reconciling with multimodal behavior.}
    \label{fig:failure_tra}
\end{figure}

\section{Analysis of Compositionality}
\label{app:compositionality}
We prove the results from \cref{sec:compositionality}.

\subsection{Goal Conditioned Analysis}
\label{app:goal_conditioned}

\restatetheorem{thm:compositionality}

\begin{proof}
    We have from \cref{eq:imitation_error} for $K\sim \operatorname{Geom}(1-\gamma)$:
    \allowdisplaybreaks
    \begin{align}
        \err(\pi; \cD^{*})
         & \triangleq \E_{\cD^{*}}\Bigl[ \frac{1}{H'} \sum_{t = 1}^{H'}
        \tfrac{\|\tilde{a}_{t,i} - \pi(\tilde{s}_{t,i},
        \tilde{g}_{i})\|^{2}}{n_{d_\cA}}\Bigr] \nonumber                                                 \\
         & = \frac{1}{H'}\E_{\cD^{*}} \Bigl[\sum_{t=1}^{\mathclap{H'-2H}} \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i},
        \tilde{g}_{i})\|^{2}}{n_{d_\cA}}\Bigr] +
        \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{H'-2H+1}}^{H'-H}
        \tfrac{\|\tilde{a}_{t,i} - \pi(\tilde{s}_{t,i},
        \tilde{g}_{i})\|^{2}}{n_{d_\cA}}\Bigr]
        \nonumber                                                                                        \\*
         & \mspace{100mu} + \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-H+1}}^{H'}
        \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i}, \tilde{g}_{i})\|^{2}}{n_{d_\cA}}\Bigr] \nonumber                            \\
         & \le \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-H+1}}^{H'} \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i}, \tilde{g}_{i})\|^{2}}{n_{d_\cA}}\Bigr]
        + \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-2H+1}}^{H'-H}
        \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i}, \tilde{g}_{i})\|^{2}}{n_{d_\cA}}\Bigr] \nonumber                            \\*
         & \mspace{100mu} +
        \Bigl(\frac{\alpha-2}{2\alpha}\Bigr)\1\{\alpha>2\} \nonumber                                     \\
         & \le \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-H+1}}^{H'} \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i}, \tilde{s}_{H',i})\|^{2}}{n_{d_\cA}}\Bigr]
        \nonumber                                                                                        \\*
         & \mspace{50mu} + \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-2H+1}}^{H'-H}
        \E_{K}\bigl[\tfrac{\|\tilde{a}_{t,i} -
        p^{\pi}(\tilde{s}_{t,i} | \tilde{s}_{H'-K,i})\|^{2}}{n_{d_\cA}}\bigr]\Bigr] +
        \Bigl(\frac{\alpha-2}{2\alpha}\Bigr)\1\{\alpha>2\} \nonumber                                     \\
         & \le \frac{1}{H'} \E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-H+1}}^{H'} \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i}, \tilde{s}_{H',i})\|^{2}}{n_{d_\cA}}\Bigr]
        \nonumber                                                                                        \\*
         & \mspace{50mu} + \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-2H+1}}^{H'-H}
        \E_{K}\bigl[\tfrac{\|\tilde{a}_{t,i} -
        p^{\pi}(\tilde{s}_{t,i} | \tilde{s}_{H'-K,i})\|^{2}}{n_{d_\cA}}\bigr]\Bigr]
        + \Bigl(\frac{\alpha-2}{2\alpha}\Bigr)\1\{\alpha>2\} \nonumber                                   \\
         & \le \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-H+1}}^{H'} \tfrac{\|\tilde{a}_{t,i} -
        \pi(\tilde{s}_{t,i}, \tilde{s}_{H',i})\|^{2}}{n_{d_\cA}}\Bigr]
        \nonumber                                                                                        \\*
         & \mspace{50mu} + \frac{1}{H'}\E_{\cD^{*}}\Bigl[\sum_{\mathclap{t=H'-2H+1}}^{H'-H}
        \E_{K}\bigl[\tfrac{\|\tilde{a}_{t,i} -
        p^{\pi}(\tilde{s}_{t,i} | \psi(\tilde{s}_{H'-K,i}))\|^{2}}{n_{d_\cA}}\bigr]\Bigr]
        + \Bigl(\frac{\alpha-2}{2\alpha}\Bigr)\1\{\alpha>2\} \nonumber                                   \\
         & \le \err(\pi; \cD^{*}) + \frac{1}{H'} \E_{\cD^{*}}\Bigl[\frac{1-\gamma^{H}}{1-\gamma}\Bigr]
        + \Bigl(\frac{\alpha-2}{2\alpha}\Bigr)\1\{\alpha>2\} \nonumber                                   \\
         & \le \err(\pi; \cD^{*}) + \frac{\alpha-1}{2\alpha} +
        \Bigl(\frac{\alpha-2}{2\alpha}\Bigr)\1\{\alpha>2\}.
    \end{align}

\end{proof}

\subsection{Language Conditioned Analysis}
\label{app:lang_conditioned}

\restatetheorem{thm:language}
The proof is similar to \cref{app:goal_conditioned}, but over the predictions of $\xi$ instead of $\psi$.

\subsection{Visualizing the Bound}
\label{app:visualizing_bound}

We compare the bound from \cref{thm:compositionality} with the ``worst-case'' bound of $\err(\pi; \cD^{*}) - \err(\pi; \cD)$ in \cref{fig:compositionality}.
The bound from \cref{thm:compositionality} is tighter than the worst-case bound, and it shows that the compositional generalization error decreases as $\alpha$ increases.

\begin{figure}[htb]
    \label{fig:compositionality}
    \centering
    {\ifrebuttal\color{black}\fi
        \begin{tikzpicture}
            \pgfplotsset{
            grid=major,
            grid style={gray!30,dashed},
            xticklabel={
                    \pgfmathparse{\tick}
                    \pgfmathprintnumber{\pgfmathresult}
                },
            width=9cm,
            height=6.cm,
            legend cell align=left,
            axis lines=left,
            scaled x ticks=false,
            legend style={draw=none,at={(0.79,0.48)},anchor=center,/tikz/every even
            column/.append style={column sep=0.5cm},legend image post style={ultra thick}
            },
            legend image code/.code={\fill[#1] (-0.12cm,-.05cm) rectangle (0.05cm,0.07cm);},
            legend columns=-1,
            }
            \begin{axis}[
                    title={Compositional Generalization Error Bound},
                    legend to name=leg:plot,
                    name=mainplot,
                    xlabel=$\alpha$,
                    ylabel=$\err(\pi;\cD^{*})-\err(\pi;\cD)$,
                    ymax=0.7,
                    xmin=1,
                    xmax=2.4,
                ]
                \addplot [smooth,very thick,color=theme1,domain=1:2.4] (x, {(x-1)/(2*x)+(x-2)/(2*x)*(x>2)});
                \addlegendentry{bound \eqref{eq:compositionality}}
                \addplot [smooth,very thick,color=theme2,domain=1:2.4] (x, {(x-1)/x});
                \addlegendentry{worst case}
            \end{axis}
            \node[anchor=north,xshift=1.5em] at (mainplot.outer south) {\ref*{leg:plot}};
        \end{tikzpicture}
    }
    \caption{Visualizing the bound (\refas{Eq.}{eq:compositionality} from \cref{thm:compositionality}) on the compositional generalization error.}
    \label{fig:bound}
\end{figure}

\begin{table}[htb!]
\caption{\textbf{Success Rate for Different GCBC Architectures in OGBench.}}
\label{table:phi}
\begin{center}
\begin{tabular}{l|ll}
    \toprule
    Environment & \textbf{GCBC} & \textbf{GCBC-$\phi$} \\
    \midrule
    \texttt{antmaze medium stitch} & $\bf45.5^{\pm(3.9)}$ & $\bf48.7^{\pm(2.7)}$ \\
    \texttt{antmaze large stitch} & $3.4^{\pm(1.0)}$  & $\bf6.8^{\pm(1.3)}$ \\
    \texttt{antsoccer arena stitch} & $\bf24.5^{\pm(2.8)}$ & $1.4^{\pm(0.3)}$ \\
    \texttt{humanoidmaze medium stitch} & $29.0^{\pm(1.7)}$ &$\bf34.4^{\pm(1.7)}$ \\
    \texttt{humanoidmaze large stitch} & $\bf5.6^{\pm(1.0)}$ & $3.5^{\pm(1.1)}$ \\
    \midrule
    \texttt{antmaze large navigate} &$\bf24^{\pm(0.6)}$ & $16.1^{\pm(0.8)}$ \\
    \texttt{cube single noisy} &{$\bf8.4^{\pm(1.0)}$} &$\bf8.7^{\pm(0.9)}$ \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[htb!]
\caption{\textbf{\Method{} hyperparameters.}}
\label{table:hyp}
\begin{center}
\small
\begin{tabular}{ll}
    \toprule
    \textbf{Hyperparameter} & \textbf{Value} \\
    \midrule
    State and goal encoder dimensions & (64, 64, 64) \\
    State and goal encoder latent dimension & 64 \\
    Discount factor $\gamma$ & 0.995 (large locomotion environments), 0.99 (other) \\
    Alignment coefficient $\alpha$ & 60 (medium locomotion), 100 (large locomotion), 20 (non-stitch) \\
    \bottomrule
\end{tabular}
\end{center}
\end{table}

\section{OGBench Implementation Details}\label{app:OGB}

To implement \Method{} in OGBench, which does not have a corresponding language label for all goal-reaching tasks, we make the following revision to \Method{} to accommodate the lack of a language task.
We train a policy $\pi(a|\phi(s), \psi(g))$, in which we propagate the behavior cloning loss throughout the entire network.
Both the state and goal encoders are MLPs with identical architecture.
We detail the configuration in \ref{table:hyp}.
This is to simulate the ResNet architecture and CLIP embeddings we use from real-world policy training.
We define separate state and goal encoder $\phi(s)$ and $\psi(g)$, and we modify $\mathcal{L}_\text{TRA}$ as:

\begin{equation}
    \mathcal{L}_\text{TRA} = \mathcal{L}_\text{BC}(\{s_i, a_i, s_i^+\}_{i=1}^K; \pi, \phi, \psi) + \alpha \mathcal{L}_\text{NCE}(\{s_i, s_i^+\}_{i=1}^K; \phi, \psi)
\end{equation}

The rest of the implementation are carried over from OGBench. We evaluate each method with 10 seeds, and we take the final 3 evaluation epoch per seed to calculate the average success rate, the same way OGBench calculates success rate for its baselines. While we used $\alpha=1$ in real world experiments, consistent with implementation from \cite{myers2023goal}, we adjust our $\alpha$ value in OGBench, as it is a hyperparameter. We report our optimal $\alpha$ configuration in Table \ref{table:hyp}.

Note that $\alpha=0$ turns the formulation into a version of GCBC with different architecture; we denote this GCBC-$\phi$.
We compare the performance of GCBC and GCBC-$\phi$ here across the 7 environments using table \ref{table:phi}.
Although the second formulation is parameterized than the original GCBC configuration, they have similar performances across the environments that we have evaluated on \-- the performance of \Method{} does not rely on extra parameterization, but learning a structured temporal representation.

We report the value of hyperparameters in table \ref{table:hyp}. The rest of the relevant hyperparameters are implemented from OGBench unless specified in the table.

