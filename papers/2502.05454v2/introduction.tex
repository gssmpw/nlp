\section{Introduction}
\label{sec:introduction}

Compositionality is a core aspect of intelligent behavior, describing the ability to sequence previously learned capabilities and solve new tasks~\citep{lashley1951problem}.
In domains involving long-horizon decision-making like robotics, various learning approaches have been proposed to enable this property, including hierarchical learning~\citep{kulkarni2016hierarchical}, explicit subtask planning~\citep{schrittwieser2021online,fang2022generalization,ahn2022can}, and dynamic-programming-based ``stitching''~\citep{ghugare2023closing,kostrikov2022offline}.
In practice, these techniques are often unstable or data-inefficient in real-world robotics settings, making them difficult to scale~\citep{laidlaw2024effective}.

By contrast, humans and animals are adept at quickly composing behaviors to reach new goals~\citep{lashley1951problem}.
Possible explanations for these capabilities have been proposed, including the ability to perform transitive inference~\citep{ciranka2022asymmetric}, learn successor representations and causal models~\citep{Dayan1993ImprovingGF,gopnik2017changes}, and plan with world models~\citep{vikbladh2019hippocampal}.
In common among these theories is the idea of learning structured representations of the world, which inference about which actions will lead to certain goals.

How might these concepts translate to algorithms for robot learning?
In this work, we study how adding an auxiliary successor representation learning objective affects compositional behavior in a real-world tabletop manipulation setting.
We show that learning this representation structure improves the ability of the robot to perform long-horizon, compositionally-new tasks, specified either through goal images or natural language instructions.
Perhaps surprisingly, we found that this temporal alignment does not need to be used for training the policy or test-time inference, as long as it is used as an auxiliary loss over the same representations used for the tasks~(\cref{fig:overview}).

We compare our method, \textbf{T}emporal \textbf{R}epresentation \textbf{A}lignment (\Method), against past imitation and reinforcement learning baselines across a set of challenging multi-step manipulation tasks in the BridgeData setup~\citep{walke2023bridgedata} as well as the OGBench simulation benchmark~\citep{park2025ogbench}.
Unlike prior work in setup, we focus on the compositional capabilities of the robot policies: as a whole, the tasks are out-of-distribution, but each distinct subtask can be described through a goal image that lies in the training distribution.
Adding a simple time-contrastive alignment loss improves compositional performance by {$>$40\%} across {13} tasks in 4 evaluation scenes, and simulation results show better performance compared to behavioral cloning (i.e., no structured representation learning), and comparable performance to offline RL methods that explicitly use a learned value function.

