\section{Experiments}
\label{sec:experiments}

Our experimental evaluation aims to answer the following research questions for \Method{}:
\begin{enumerate}[itemsep=0pt,parsep=0pt,topsep=0pt]
    \item Can \Method{} enable zero-shot composition of sequential tasks without additional rewards or planning?
    \item Does TRA improve compositional generalization over past methods?
           \item How well does \Method{} capture skills that are less common within the dataset?
    \item Is temporal alignment by itself sufficient for effective compositional generalization?
          
          \end{enumerate}

\input{data.tex}

\makeatletter

\gdef\setname#1{\def\tmp@{{\checksetcolors(#1)\color{\cachedata}\raise -2.5pt\hbox to1em{\hss \tikz \node[circle,draw,inner sep=.5pt,outer sep=0pt,align=center]{\bfseries\textsf{\expandafter\char\the\numexpr`A-1+#1\relax}};\hss}}}\@ifstar{\hypertarget{set#1}{\tmp@}}{\hyperlink{set#1}{\tmp@}}}
\newarray\setcolors
\shifthue[.2]{good}[lessgood]
\shifthue[.9]{bad}[lessbad]
\desaturate[.5]{lessgood}
\desaturate[.5]{lessbad}
\darken[.2]{lessbad}
\readarray{setcolors}{good&lessgood&lessbad&bad}

\makeatother

\gdef\cluster#1{\hypertarget{set#1}{{\checksetcolors(#1)\color{\cachedata}\scriptsize\raisebox{.8pt}(\raisebox{.4pt}{\scriptsize\textsf{\expandafter\char\the\numexpr `A-1+#1\relax}}\raisebox{.8pt})}}}
\begin{figure}[htb]
    \centering
    \includegraphics[width=.7\linewidth]{figures/bridge-setup}
    \caption{The tabletop manipulation setup used for the real-world evaluation~(see \citealp{walke2023bridgedata}).}
    \label{fig:bridge-setup}
\end{figure}

\begin{table*}[htb!]
    \label{tab:bridge-evals}
    \centering

    \gdef\clustermark#1{\raisebox{-0.3pt}{\bfseries\cluster#1~~}\hfill}\gdef\clusternote#1#2{\hbox{\raggedright\footnotesize~\llap{\raisebox{3pt}{\scalebox{.88}{\bfseries\cluster#1}}\,}#2}}

    \tabcolsep=0.09cm

    \caption{Real-world Evaluation}
    \centering
    \centering
    \begin{tabular}{r|ccccc|ccccc}
        \multicolumn{1}{c}{}      & \multicolumn{5}{c}{Language-conditioned} & \multicolumn{5}{c}{Goal-conditioned}                                                                                                                                                                                                        \\ [1pt]
        \toprule
        \multicolumn{1}{c}{Task}  & \textbf{\Method}                         & GRIF                                 & LCBC              & Octo                & AWR                     & \textbf{\Method}                       & GRIF                & GCBC              & Octo                & AWR                     \\
        \midrule
        \clustermark1\getlang{6}  & \best{\get{OURS-L}}\rlap{$^{\dagger}$}   & \get{GRIF-L}                         & \best{\get{LCBC}} & \best{\get{OCTO-L}} & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}\rlap{$^{\dagger}$} & \get{GRIF-I}        & \best{\get{GCBC}} & \best{\get{OCTO-I}} & \best{\get{AWR+GRIF-I}} \\
        \clustermark1\getlang{7}  & \best{\get{OURS-L}}                      & \best{\get{GRIF-L}}                  & \get{LCBC}        & \get{OCTO-L}        & \best{\get{AWR+GRIF-L}} & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \best{\get{GCBC}} & \best{\get{OCTO-I}} & \best{\get{AWR+GRIF-I}} \\
        \clustermark1\getlang{8}  & \best{\get{OURS-L}}                      & \best{\get{GRIF-L}}                  & \best{\get{LCBC}} & \best{\get{OCTO-L}} & \best{\get{AWR+GRIF-L}} & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \clustermark4\getlang{11} & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \midrule
        \clustermark2\getlang{4}  & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \clustermark2\getlang{5}  & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \midrule
        \clustermark3\getlang{1}  & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \clustermark3\getlang{2}  & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \best{\get{LCBC}} & \get{OCTO-L}        & \best{\get{AWR+GRIF-L}} & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \best{\get{OCTO-I}} & \best{\get{AWR+GRIF-I}} \\
        \clustermark3\getlang{3}  & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \best{\get{OCTO-I}} & \get{AWR+GRIF-I}        \\
        \clustermark4\getlang{12} & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \best{\get{OCTO-L}} & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \best{\get{GRIF-I}} & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \midrule
        \clustermark1\getlang{10} & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \best{\get{OCTO-I}} & \get{AWR+GRIF-I}        \\
        \clustermark2\getlang{9}  & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \get{OCTO-I}        & \get{AWR+GRIF-I}        \\
        \clustermark2\getlang{13} & \best{\get{OURS-L}}                      & \get{GRIF-L}                         & \get{LCBC}        & \get{OCTO-L}        & \get{AWR+GRIF-L}        & \best{\get{OURS-I}}                    & \get{GRIF-I}        & \get{GCBC}        & \best{\get{OCTO-I}} & \get{AWR+GRIF-I}        \\
        \bottomrule
    \end{tabular}
    \par
    \vspace*{1.5ex}
    \parbox{.8\linewidth}{\valign{
            \vfil\vbox{\hsize=.5\linewidth\centering #} \vfil
            \cr
            \halign{
                #\hfil~                                & \quad#\hfil\cr
                \clusternote1{One step tasks}          & \clusternote2{Task concatenation} \cr
                \clusternote3{Semantic generalization} & \clusternote4{Tasks with dependency} \cr}
            \cr
            \noalign{\qquad\quad}
            \footnotesize~\llap{$^{\dagger}$}\parbox[t]{.8\hsize}{\raggedright The best-performing method(s) up to statistical significance are \textbf{\color{text1}highlighted}}
            \cr
        }
    }

\end{table*}

\subsection{Real-World Experimental Setup}
\label{sec:experiment_setup}

We evaluate \Method{} on a collection of held-out \emph{compositionally-OOD} tasks \-- tasks for which the individual substeps are represented in the dataset, but the combination of those steps is unseen.
For example, in a task such as ``removing a bell pepper from a towel, and then sweep the towel'', both the tasks ``remove the bell pepper from the towel'' and ``sweep the towel'' have similar entries within BridgeData, but such a combination of behaviors is unseen.
We utilize a real-world robot manipulation interface with a 7 DoF WidowX250 manipulator arm with 5Hz execution frequency.
We train on an augmented
version of the BridgeDataV2 dataset~\citep{walke2023bridgedata}, which contains over 50k trajectories with 72k language annotations. More details can be seen \cref{app:tra_impl}.

In order to specifically test the ability of \Method{} to perform compositional generalization, we organize our  evaluation tasks into 4 scenes that are unseen in BridgeData, each with increasing difficulty:

\textbf{Set \setname1* -- One-Step:} These are the only tasks that are not compositionally-OOD, as all the tasks are one-step tasks.
These tasks involve opening, putting an item in, and closing a drawer, and
have been seen in BridgeData, although at a lower frequency than object manipulation, and with new positions.
We use these tasks to compare \Method{}'s performance on single-step tasks relative to baselines.

\textbf{Set \setname2* -- Task Concatenation:} These tasks scene involves concatenating multiple tasks of the same nature in sequence, where a robot must be able to perform all tasks within the same trajectory.
During evaluation, we instruct the policy with instructions such as sweeping multiple objects in the scene that require composition (though are not sensitive to the \emph{order} of the composition).
or are there other tasks?

\textbf{Set \setname3* -- Semantic Generalization:} Unlike set \setname2, these tasks require manipulation with different objects of the same type.
We test this using various food items within BridgeData, instructing the policy within a container.
An example of such task would be a table containing a banana, a sushi, a bowl, and various distractor objects, and instead of using specific language commands such as ``put the banana and the sushi in the bowl,'' the instruction is ``put the food items in a container''.

\textbf{Set \setname4* -- Tasks with Dependency:} This is the most challenging of the set of tasks: these tasks have subtasks that require previous subtasks to be completed for them to succeed.
For instance, taking an object out of a drawer has this structure, as the drawer must be opened before the object can be taken out.

The complete list of tasks is noted in \cref{sec:experiment_details}.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/demo.pdf}
    \caption{Example rollouts of a task with \Method{} and LCBC. While \Method{} is able to successfully compose the steps to complete the task, LCBC fails to ground the instruction correctly.}
        \label{fig:demos}

\end{figure*}

\begin{figure}
    \makeatletter
    \centering
    \includegraphics[width=.97\linewidth]{figures/awr_ablation-raster}
    \caption{Aggregated success rate of using AWR as an additional policy learning metric over all 4 scenes.}
    \label{fig:ablation-awr}
    \makeatother
\end{figure}

\subsection{Baselines}\label{sec:baselines}
\label{sec:realworld_baselines}
We compare against the following baselines in our real-world evaluation:
\begin{description}[itemsep=0pt,topsep=0pt]
    \item[GRIF~\citep{myers2023goal}] learns a goal- and language- conditioned policy using aligned goal image and language representations. In our experiments, this becomes equivalent to \Method{} when the temporal alignment objective is removed.
    \item[GCBC~\citep{walke2023bridgedata}] learns a goal-conditioned behavioral cloning policy that concatenates the goal image with the image observation.
    \item[LCBC~\citep{walke2023bridgedata}] learns a language-conditioned policy that concatenates the language with the image observation.
    \item[OCTO~\citep{ghosh2024octo}] uses a multimodal transformer to learn a goal- and language-conditioned policy. The policy is trained on Open-X dataset~\citep{oneill2024open}, which incorporates BridgeData in its entirety.
    \item[AWR~\citep{peng2019advantage}] uses advantages produced by a value function to effectively extract a policy from an offline dataset. In this experiment, we use the difference between the contrastive loss between the current observation and the goal representation and the contrastive loss between the next observation and the goal representation as a surrogate for value function.
\end{description}

We train GRIF, GCBC, LCBC, and AWR using the same augmented Bridge Dataset as \Method{}, and we use an Octo-Base 1.5 model for our evaluation.
A more detail approach is detailed in \cref{app:baselines}.
During evaluation, we give all policies the same goal state and language instruction regardless of the architecture, as they are trained on the same language instruction with the exception of Octo, which doesn't benefit from paraphrased language data, but does benefit from a more diverse language annotation set across a larger dataset of varying length and complexity.

\subsection{Real-world Evaluation}
\label{sec:realworld}

Our real-world evaluation aims to answer the following questions.

\paragraph{Does TRA enable compositionality?}

\begin{figure}[htb]
    \centering
    \setbox0=\hbox{\includegraphics[width=0.4\linewidth]{figures/cube-triple.png}}
    \hbox to \linewidth{\hfill \copy0 \hfill \hbox{\includegraphics[height=\ht0]{figures/humanoid.png}} \hfill}
    \caption{Two environments from the OGBench suite~\citep{park2025ogbench}.
        \emph{Left:} a cube stacking environment.
        \emph{Right:} a humanoid maze navigation environment.}
    \label{fig:ogbench_envs}
\end{figure}

\Cref{tab:bridge-evals} shows the success rates of the \Method{} method compared to other methods on real-world robot evaluation tasks.
We marked all policies within the task orange if they achieve the best statistically significant performance.
We first compare the performance against methods in \setname1.
Although \Method{} performs well with drawer tasks, its performance against baseline methods is not statistically significant.
However, \Method{} performs considerably better than that of any baseline methods on compositionally-OOD \textbf{instruction following} tasks.

While \Method{} completed 88.9\% of tasks seen in \setname 2, 83.3\% of evaluations in \setname3, and 60\% of tasks in \setname4 with instruction following, the best-performing baseline for \setname 2 was 30\% with LCBC, 43.3\% for \setname3 with AWR, and 33.3\% on \setname4 with Octo. The same improvement was also present in goal reaching tasks, although at a lower level, in which \setname3 produced 60\% success rate and scene D produced a 43.3\% success rate, as compared to 46.7\% and 20\% for the best baselines.

Qualitatively, we see that policies trained under \Method{} provides a much smoother trajectory between different subtasks while following instructions, while other cannot replicate the same performance.
Take removing the bell pepper + sweep task for example, with its visualization shown \cref{fig:demos}, while \Method{} was able to remove the bell pepper by grasping it and putting it to the bottom right corner of the table, LCBC cannot replicate the same performance, choosing to nudge the bell pepper instead and failed to execute the task.

\paragraph{How well does TRA perform against Conventional Offline RL Algorithms?} While offline reinforcement learning promises good stitching behavior~\citep{kumar2021should}, we demonstrate that \Method{} still outperforms offline reinforcement learning on robotic manipulation.
Overall, \Method{} performs better than AWR for both language and image tasks, outperforming AWR by 45\% on instruction following tasks, and by 25\% on goal reaching tasks, showing considerable improvement over an offline RL method that promises compositional generalization via stitching.

Qualitatively, a policy trained with AWR often stops after one subtask, even though the goal instruction or image demanded all of the subtasks be completed.
We see this in, e.g., \cref{fig:overview}, where 3 different policies use the same goal image for a task where all 3 food items must be put in the bowl.
While \Method{} successfully completes all 3 subtasks, AWR chose to only complete one subtask and terminates right after putting the banana in the bowl.
This is because AWR on an offline dataset has a goal-reaching reward function that disregards aligning representations across time in different trajectories.

\paragraph{Does \Method{} help capturing rarely-seen skills within the dataset?}

We also compare the performance of \Method{} against AWR across all scenes and compare the performance of the policies with all 3 tasks in \setname4 as well as folding the towel, all rarely seen skills within BridgeData.
When conditioning on language, AWR struggles to to effectively generalize to compositionally harder tasks, with average success rate decreasing from 43.3\% in to 6.67\% from \setname3 to \setname4, compared to a decrease of only 83.3\% to 60\% for \Method{}. Other agents do not perform as well as AWR in \setname4, as the lack of such compositional generalization prevented the policies from achieving all of the tasks at a reliable rate.

\paragraph{Is \Method{} sufficient in achieving compositional generalization?}

We demonstrate in our real-world experiment that only using temporal alignment is sufficient for achieving good compositional generalization.
We evaluate this by comparing a policy trained on only temporal alignment loss (our method), and another policy trained on such loss and have these losses weighed by AWR.

\Cref{fig:ablation-awr} shows that across all evaluation tasks, there exists no statistically significant difference between using and not using AWR in addition to temporal alignment.
In fact, using AWR marginally decreases the efficacy of \Method{}, unlike when used with GCBC and LCBC.

\subsection{Testing Compositionality in Simulation}
\label{sec:simulation}

\begin{table}[htpb]
    \centering
    \caption{OGBench Evaluation}
    \label{tab:ogbench}

    \tabcolsep=1pt
    \cmidrulewidth=0.5pt
    \fontsize{7}{7}\selectfont
    \newcolumntype{V}{>{\raise-1.5ex\hbox\bgroup}c<{\egroup}}
    \newcolumntype{G}{>{\color{text0}}V}
    \newcolumntype{v}{>{\footnotesize\begin{adjustbox}{max width=3em}\arraybackslash}c<{\end{adjustbox}}}
    \newcolumntype{g}{>{\color{text0}}v}
    \def\arraystretch{1.3}
    \begin{adjustbox}{width=\linewidth}
        \begin{tabular}{>{\tiny\ttfamily\arraybackslash}p{6em}@{\;}|>{\hspace*{1pt}}VVGGGG}
            \toprule
            \multicolumn{1}{c}{}            & \multicolumn{6}{c}{\small Methods}                               \\ [-1.5pt]
            \cmidrule(lr){2-7}
            \multicolumn{1}{c}{\small Task} & \multicolumn{1}{v}{\textbf{TRA}}                                  & \multicolumn{1}{v}{\textbf{GCBC}} & \multicolumn{1}{g}{\textbf{CRL}} & \multicolumn{1}{g}{\textbf{GCIQL}} & \multicolumn{1}{g}{\textbf{GCIVL}}                        & \multicolumn{1}{g}{\textbf{QRL}} \\
            \midrule
            antmaze medium stitch           & \bf\color{text1}\pmformat{60.7}{3.0}\clap{\,$^{\mathstrut^{*}}$}  & \pmformat{45.5}{3.9}              & \pmformat{52.7}{2.2}             & \pmformat{29.3}{2.2}               & \pmformat{44.1}{2.0}                                      & \pmformat{59.1}{2.4}             \\
            antmaze large stitch            & \color{text1}\pmformat{12.8}{2.0}                                 & \pmformat{3.4}{1.0}               & \pmformat{10.8}{0.6}             & \pmformat{7.5}{0.7}                & \bf\pmformat{18.5}{0.8}\clap{\,$^{\mathstrut^{\dagger}}$} & \bf \pmformat{18.4}{0.7}         \\
            antsoccer arena stitch          & \pmformat{17.0}{1.2}                                 & \color{text1}\bf\pmformat{24.5}{2.8}              & \pmformat{0.7}{0.1}              & \pmformat{2.1}{0.1}                & \pmformat{21.4}{1.1}                                   & \pmformat{0.8}{0.2}              \\
            humanoidmaze medium  stitch     & \bf\color{text1}\pmformat{46.1}{1.9}                              & \pmformat{29.0}{1.7}              & \pmformat{36.2}{0.9}             & \pmformat{12.1}{1.1}               & \pmformat{12.3}{0.6}                                      & \pmformat{18.0}{0.7}             \\
            humanoidmaze large stitch       & \bf\color{text1}\pmformat{8.6}{1.4}                               & \pmformat{5.6}{1.0}               & \pmformat{4.0}{0.2}              & \pmformat{0.5}{0.1}                & \pmformat{1.2}{0.2}                                       & \pmformat{3.5}{0.5}              \\
            \midrule
            antmaze large navigate          & \color{text1}\pmformat{35.4}{1.8}                                 & \pmformat{24.0}{0.6}              & \bf\pmformat{82.8}{1.4}          & \pmformat{34.2}{1.3}               & \pmformat{15.7}{1.9}                                      & \pmformat{74.6}{2.3}             \\
            cube single noisy               & \pmformat{9.2}{0.9}                                               & \pmformat{8.4}{1.0}               & \pmformat{38.3}{0.6}             & \bf\pmformat{99.3}{0.2}            & \pmformat{70.6}{3.3}                                      & \pmformat{25.5}{2.1}             \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \smallskip

    \adjustbox{width=\linewidth}{\hspace*{1ex}
        \vbox{
            \raggedright
            \hbox{RL methods with a separate value network to update the actor are in \textcolor{text0}{gray}.}
            \smallskip
            \hbox{$^*$The best non-RL methods up to significance are {\color{text1}highlighted}.}
            \hbox{$^{\dagger}$We \textbf{bold} the best performance across all methods.}
        }\hspace*{1ex}
    }

                
\end{table}

We also validated the compositional behavior of \Method{} in simulation using the recent offline RL benchmark OGBench~\citep{park2025ogbench}.
This environment features environments for locomotion and manipulation, each with multiple offline datasets that can be used for training, including one
that explicitly tests compositional generalization (the ``stitch'' datasets) by creating multiple short datasets that comprise a single, larger task.
We modify our approach to \Method~ to account for the lack of language instructions, and more implementation detail can be seen at \cref{app:OGB}.

We evaluate the performance of \Method{} on 7 different environments in OGBench.
In 5 of these environments we use the ``stitch'' dataset, while 2 other environment use a more general goal-reaching dataset (``navigate'' and ``noisy'').
\Cref{tab:ogbench} shows the performance of \Method{} compared to other non-hierarchical methods on these environments from OGBench.
Consistent with our real-world results \cref{tab:bridge-evals,fig:ablation-awr}, \Method{}
outperforms other imitation and offline RL methods on certain environments that require compositional generalizations, including CRL \citep{eysenbach2022contrastive} that also has a separate value and critic network.
In non-stitching environments, while traditional offline RL methods outperform \Method{}, \Method{} is still an improvement over GCBC.

\subsection{Failure Cases}
\label{sec:more_failure_cases}

As with other Gaussian policies, \Method{} struggles when multimodal behavior is observed, and sometimes fails to reach the goal due to early grasping or incorrect reaching~\citep{kumar2023pre}.
While \Method{} did seem to provide small improvements on the in-distribution tasks of \setname1, the primary benefits derived from \Method{} were seen on compositionally-OOD tasks.
We further discuss failure cases in \cref{sec:failure_cases}.

