\section{Related work}
\subsection{Surgical Scene Understanding} 
Recent works in surgical scene understanding can be broadly classified into two categories. One category aims to automatically identify surgical phases or steps within untrimmed surgical videos, which has received a lot of attention and is a very active area of research. TMRNet \cite{jin2021temporal} is proposed to establish a long-range memory bank to provide long-term temporal features and improve phase recognition accuracy. 
% Inspired by the excellent outcomes of  MS-TCN  \cite{li2020ms} in general human action segmentation. 
Yi et al.  \cite{yi2022not} proposed a non-end-to-end training strategy based on MS-TCN. After the transformer gained significant popularity for processing sequential data, Pan et al.  \cite{pan2023temporal} exploited the Swin transformer and LSTM networks 
% to extract multi-scale visual features and temporal information 
to recognise phases in surgical videos. Czempiel et al. devised OperA \cite{czempiel2021opera}, a transformer-based model, to extract high-quality frames with an attention regularization loss.  Ding et al. \cite{ding2022exploring} presented SAHC for surgical phase recognition by emphasizing learning segment-level semantics to address erroneous predictions caused by ambiguous frames. Yue et al. \cite{yue2023cascade} fused the frame-level and phase-level temporal context for surgical workflow analysis. 

Another category focuses on finer-grained action recognition. Bawa et al. \cite{bawa2021saras} presented an endoscopic minimally invasive surgery dataset designed to tackle the problem of surgeon action detection, and provided baseline results with open-source methods like \cite{ren2015faster}. Hao et al. \cite{hao2023act}  presented ACTNet to get action locations, classes and confidence guided by anchors. Unlike existing methods for a single task, we design a new transformer module to learn the relations from multi-level tasks. Besides, we also model the constraints in the hierarchical information to regularize the framework with the ICL comprehensively.
% Automatic surgical workflow analysis aims to automatically identify surgical phases or steps within untrimmed surgical videos, which has received a lot of attention and is a very active area of research.  Several CNN-based models are proposed for the tasks.  Jin et al. \cite{jin2017sv} proposed SV-RCNet, which integrates ResNet and LSTM to extract spatial-temporal features for automatic phase recognition.  To address the limited memory capacity of LSTM, TMRNet \cite{jin2021temporal} is proposed to establish a long-range memory bank to provide long-term temporal features and improve phase recognition accuracy.  Inspired by the excellent outcomes of  MS-TCN  \cite{li2020ms} in general human action segmentation. Czempiel et al. \cite{czempiel2020tecno}  developed a TeCNO network, which achieves phase recognition by introducing MS-TCN.  Similarly, Yi et al.  \cite{yi2022not} proposed a non-end-to-end training strategy based on MS-TCN.  For improving performance,  Ramesh et al. proposed a multi-task MS-TCN to jointly predict the phases and steps. Jin et al. \cite{jin2020multi} showed consistent improvement in both phase recognition and tool recognition tasks with multi-task learning.  However,  due to the inherent inductive biases in the convolutional structure, these works do not work well in understanding global information through cross-frame dependencies \cite{pan2023temporal}, which is critical visual information for surgical scene understanding.

% \subsection{Transformer-based Surgical Workflow Analysis}
% Transformer has gained significant popularity for processing sequential data, which allows concurrently relating entries inside a sequence and helps to preserve essential features in extra-long sequences \cite{pan2023temporal}. Pan et al.  \cite{pan2023temporal} exploited the Swin transformer and LSTM networks to extract multi-scale visual features and temporal information to recognise phases in surgical videos. Czempiel et al. devised OperA \cite{czempiel2021opera}, a transformer-based model, to extract high-quality frames with an attention regularization loss.  Ding et al. \cite{ding2022exploring} presented SAHC for surgical phase recognition by emphasizing learning segment-level semantics to address erroneous predictions caused by ambiguous frames. Yue et al. \cite{yue2023cascade} fused the frame-level and phase-level temporal context for surgical workflow analysis. Unlike existing transformer-based workflow analysis methods for a single task, we design a new transformer module to learn the relations from multi-granularity tasks. Besides, we also model the constraints in the hierarchical information to regularize the framework with the ICL comprehensively.

%a segment-attentive hierarchical consistency network (SAHCï¼‰


% take a different strategy that utilizes the GCN to learn the relations of categories in each annotation type. Besides, we also model the constraints in hierarchical annotations to comprehensively regularize the framework with both inter- and intra-relations, contributing a new approach to embedding relations in network architectures.
\subsection{Multi-task learning in surgical scene}
For improving performance,  Ramesh et al. \cite{ramesh2021multi} proposed a multi-task MS-TCN to jointly predict the phases and steps, failing to get a complete surgical scene understanding.
% Valderrama et al. \cite{valderrama2022towards} provides a benchmark transformer network for multiple tasks but fails to explore and utilize the relations between different tasks.  
Nwoye et al. proposed the action triplets (instrument, verb, target, as shown in Fig. \ref{intro} (C)) to represent the actions in a laparoscopic dataset \cite{nwoye2020recognition}, and a trainable 3D interaction space to capture the associations between the triplet components. They further developed a new model, the Rendezvous \cite{nwoye2022rendezvous}, to detect triplet components with class activation-guided and mixed attentions. After adding the location information of the instrument and target, Lin et al. \cite{lin2022instrument, lin2024instrument} presented a novel QDNet for the instrument-tissue interaction quintuple detection task in cataract videos. Valderrama et al. \cite{valderrama2022towards} introduced the PSI-AVA dataset and proposed a baseline framework for different level task achievement. However, they failed to harness and explore the interdependencies and correlations among these tasks. Although most of these works employ multi-task learning for action recognition, they are confined to the singular horizontal dimension of actions for more detailed analyses, while neglecting the complementarity of vertical information such as phases and steps.  Our work, in contrast, encompasses an integrated analysis across both horizontal and vertical dimensions in surgical scenarios.