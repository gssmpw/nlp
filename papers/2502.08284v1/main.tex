%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
% \documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf]{aamas} 


%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

%%% your packages
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{nicefrac}
\usepackage{longtable,booktabs,geometry}

\newcommand{\E}{\mathbb{E}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\teal}{\color{teal}}
\newcommand{\vect}[1]{\boldsymbol{#1}}


\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Nowé  (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{683}

%%% Use this command to specify the title of your paper.

\title{Data Pricing for Graph Neural Networks without Pre-purchased Inspection}

%%% Provide names, affiliations, and email addresses for all authors.
\author{Yiping Liu}
\affiliation{UESTC \country{China}}
\affiliation{The University of Auckland \country{New Zealand}}
\email{yliu823@aucklanduni.ac.nz}
\orcid{0000-0002-0868-6946}
\authornote{Equal contribution}

\author{Mengxiao Zhang}
\affiliation{UESTC \country{China}}
\affiliation{The University of Auckland \country{New Zealand}}
\email{mengxiao.zhang@auckland.ac.nz}
\orcid{0000-0002-6274-0384}
\authornote{Corresponding authors}
\authornotemark[1]

\author{Jiamou Liu}
\affiliation{
The University of Auckland
 \country{New Zealand}}
 \orcid{0000-0002-0824-0899}
\email{jiamou.liu@auckland.ac.nz}

\author{Song Yang}
\affiliation{The University of Auckland
 \country{New Zealand}}
\email{syan382@aucklanduni.ac.nz}
\orcid{0000-0002-1200-1129}



% \affiliation[inst2]{
% \institution{University of Auckland}
% \city{Auckland}
% \country{New Zealand}}
%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to $40\%$ in both MacroF1 and MicroF1.% \footnote{Our code is available at \url{https://anonymous.4open.science/r/GNNModelPricing-BF5C}.}.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Model marketplaces; data pricing; structural entropy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
In today’s digital age, data has become an essential asset, serving as the foundation for AI and machine learning advancements. To meet the increasing demand for high-quality data, a new business paradigm known as the {\em model marketplace} has emerged \cite{liu2020dealer}, exemplified by platforms like Modzy. A model marketplace facilitates the exchange between {\em model consumers}, who seek AI models for various tasks, and {\em data owners}, who possess the feature and label data necessary for model training. The marketplace purchases data from data owners, uses it to train AI models, and then sells these trained models to consumers. %This collaborative ecosystem enables innovation by harnessing the combined power of data and AI, providing tailored solutions across industries. 
However, a key challenge in model marketplaces is determining how to properly compensate data owners for their contributions, a problem referred to as {\em data pricing}. This problem is challenging because the importance of data is difficult to evaluate. Most existing studies assume that marketplaces acquire data from data owners {\em before} paying them and use the subsequent performance improvements as a measure of data importance. For example, \cite{xu2021gradient,agarwal2019marketplace,ghorbani2019data,jia2019efficient,liu2020dealer} rely on this assumption to establish pricing mechanisms based on the marginal impact of data on model accuracy. However, this pre-purchased inspection assumption is impractical in real-world settings. Data owners are often unwilling to release their data without proper payment, fearing that the data, once disclosed, may immediately provide valuable insights to buyers, reducing the incentive to pay.

This leads to a critical question: {\em How can we measure data importance for model training without direct inspection, thereby facilitating data pricing?} Several studies have attempted to address this question by introducing exogenous metrics for measuring data importance, such as data age, accuracy, volume \cite{heckman2015pricing}, the extent of perturbations \cite{cong2020vcg,sun2022profit}, or data owners' reputation \cite{zhang2021incentive}. However, these metrics often fail to accurately reflect the contribution of data in the context of model training, particularly when dealing with complex models like Graph Neural Networks (GNNs).


Graph-structured data is prevalent in many real-world scenarios, where the relationships between entities are often as important as their attributes.
% often as important as, or even more than, the attributes of the entities themselves. 
GNNs excel in tasks involving such data, capturing both node features and network structure. 
% GNNs are well-suited for tasks involving such data, as they can capture both node features and the underlying structure of the network. 
% However, in many sectors, data ownership is decentralised, with different data owners controlling separate ``pockets'' of the overall network. 
However, data ownership is often decentralized, with different entities controlling separate “pockets” of the network.  
This creates a need for a marketplace where subgraphs can be purchased and integrated to enable comprehensive model training \cite{bechtsis2022data}. 
% For instance, in finance, fraud detection requires analysing transaction flows across multiple banks, while in healthcare, patient data is fragmented across hospitals and insurance companies. In supply chain management, companies only have visibility into their direct suppliers and customers, but the complete supply chain involves many interdependent organizations. 
For example, in finance, each bank holds its own subset of transaction data, but detecting fraud often requires analysing transaction flows across multiple institutions. Similarly, in healthcare, patient interactions are fragmented across hospitals, clinics, and insurance companies, forming an interconnected yet distributed network. In supply chain management, companies typically have visibility into their direct suppliers and customers, but the complete supply chain network spans many interdependent organisations. In all of these cases, the full value of the network data cannot be realised without aggregating subgraphs from multiple sources.  
% For data consumers, the ability to acquire additional pockets of the network is essential for training reliable GNN models. This need to aggregate network data from multiple sources is critical in applications such as cross-institutional fraud detection, personalised healthcare treatment, and supply chain risk analysis.
For data consumers, aggregating subgraphs from multiple sources is essential for training reliable GNN models, enabling applications like fraud detection, personalized healthcare, and supply chain risk analysis.

In this paper, we advance existing research by exploring the pricing of individual data points within subgraphs for GNN training. This introduces a distinct challenge, as the value of any given node to the model's performance is highly dependent on its structural role and connectivity within the broader network. To address this, we aim to develop pricing mechanisms that capture the marginal contribution of each data point, taking into account both its local features and its position within the global network structure. Notably, this is the first work to tackle the problem of pricing graph-structured data in a model marketplace for GNNs.



In the following, we list our main contributions:
% and technical novelties of this paper: 
\begin{itemize}[leftmargin=*]
\item We propose {\em Structural Importance based Model Trading} (SIMT), a novel model marketplace framework for GNNs that integrates two phases: {\em data procurement} and {\em model training}. Figure~\ref{fig:framework} shows the conceptual framework of SIMT. 

\item For data procurement, we put forward a new method for assessing the importance of graph-structured data. For this we present a novel {\em marginal structural entropy} to quantify node informativeness. This method of importance assessment is integrated with an auction mechanism to select data owners and fairly compensate them based on their contributions. We prove that this mechanism is incentive compatible, individual rational, and budget feasible.

\item For model training, we introduce the method of {\em feature propagation} to impute missing feature data for unselected nodes, enabling effective learning with partial data.
We also design an {\em edge augmentation} method to enhance graph structure by adding connections involving unselected nodes, improving the GNN's ability to generalize.

\item The proposed SIMT method was evaluated on five well-established benchmark datasets, and consistently outperformed four baseline mechanisms in node classification tasks. SIMT achieved up to a 40\% improvement in MacroF1 and MicroF1 scores compared to the Greedy and ASCV methods, demonstrating its superior performance under various budget constraints. 
\end{itemize}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/framework.png}
    \caption{The framework of structural importance-based model trading (SIMT) mechanism.}
    \label{fig:framework}
\end{figure*}


\section{Related work}

Data pricing has been extensively studied in two main contexts: {\em data marketplaces} and {\em model marketplaces}. %Below, we review key approaches in each area.

% \smallskip

\noindent {\bf Data Pricing in Data Marketplaces.}
In data marketplaces, pricing mechanisms revolve around trading raw datasets or simple queries. Previous work has focused on pre-purchase decisions, where data is evaluated before it is accessed, which aligns with our setting. For instance, the importance of datasets is often quantified by metrics such as size, as explored by \cite{kushal2012pricing}, or privacy levels, as in \cite{parra2018optimized}. Other studies, such as \cite{xu2015privacy} and \cite{jaisingh2008privacy}, assess data importance based on its utility to consumers, proposing auction mechanisms and contracts to compensate data owners accordingly.

When it comes to \emph{query-based data pricing}, metrics like privacy levels directly impact the accuracy of responses, thereby influencing data value.
% have a direct impact on the accuracy of responses, influencing the value of the data. 
For instance, \cite{ghosh2011selling,roth2012conducting} propose auction mechanisms that incorporate privacy in queries, while \cite{ligett2012take} introduces a take-it-or-leave-it contract for count queries. Further work by \cite{dandekar2012privacy,zhang2020selling} expands these ideas to linear predictor queries and broader query settings.
% For example, \cite{ghosh2011selling} and \cite{roth2012conducting} design auction mechanisms that account for the privacy of data in queries, while \cite{ligett2012take} offers a take-it-or-leave-it contract for count queries. Further studies by \cite{dandekar2012privacy} and \cite{zhang2020selling} extend these ideas to linear predictor queries and more general query settings.

In data marketplaces, data importance is often easily quantifiable using metrics like size or privacy levels. However, in the context of \emph{model marketplaces}, the contribution of individual data points to machine learning model performance is more complex and requires novel pricing methods.

% \smallskip

\noindent {\bf Data Pricing in Model Marketplaces.} 
% In model marketplaces, data pricing is typically based on how much a dataset improves the performance of a machine learning model. \cite{abernethy2015low} pioneered the study of data pricing for machine learning tasks by presenting a theoretical framework that balances budget constraints with model performance. Most subsequent works, such as those by \cite{agarwal2019marketplace} and \cite{ghorbani2019data}, assume the benefit of a trained model is known and focus on how to fairly distribute rewards among data owners. A common approach is the application of the \emph{Shapley value} \cite{shapley1951notes}, which ensures that each data owner is compensated according to their contribution to the overall model.
In model marketplaces, data pricing is typically based on how much a dataset improves a machine learning model's performance. \cite{abernethy2015low} introduced a theoretical framework for data pricing that balances budget constraints with model performance. Subsequent works, such as those by \cite{agarwal2019marketplace} and \cite{ghorbani2019data}, assume the model's benefit is known and focus on fairly distributing rewards among data owners. A common method for this is the \emph{Shapley value} \cite{shapley1951notes}, which compensates each data owner based on their contribution to the model.

Various studies have refined the utility function used in Shapley value calculations by incorporating additional factors. \cite{ghorbani2019data} and \cite{jia2019efficient}, for example, include $K$-nearest neighbors and privacy considerations in their utility designs. \cite{liu2020dealer} builds on this by extending the Shapley value framework to model marketplaces. Other research, such as \cite{sim2020collaborative} and \cite{ohrimenko2019collaborative}, explores utility design in \emph{collaborative machine learning} scenarios, where data owners also serve as model consumers. In these cases, utility is defined either as the sum of the model's value to the owner and its marginal value to others \cite{ohrimenko2019collaborative}, or through metrics like information gain \cite{sim2020collaborative}. \cite{xu2021gradient} and \cite{hu2020trading} further define utility based on the cosine similarity of parameters or the privacy leakage of shared model parameters.

A common limitation of these works is that they often require training models on the entire dataset before compensating data owners. In practice, this assumption is often unrealistic, as data owners are usually hesitant to contribute their data upfront without proper guarantees or compensation.

%Addressing Data Pricing Without Direct Data Inspection

A more realistic setting, which is closer to our approach, has been explored by studies \cite{cong2020vcg,zhang2021incentive,sun2022profit}. \cite{cong2020vcg} assume that data importance is known and apply a \emph{VCG auction mechanism} to select and compensate data owners. \cite{zhang2021incentive} propose an auction mechanism that incorporates the reputation of data owners as a reflection of their contribution, while \cite{sun2022profit} design an auction that selects data owners based on their privacy requirements. Although these approaches offer valuable insights, they rely on exogenous metrics, such as reputation or privacy, which are often difficult to obtain or may not accurately reflect the intrinsic value of data for model training.

% In contrast, our work introduces a novel approach to \emph{measure data importance} without the need for direct data inspection. By focusing on the structural properties of graph-structured data and leveraging methods like structural entropy, we aim to develop a fair and effective data pricing mechanism that addresses the limitations of previous approaches.
In contrast, our work proposes a novel method to \emph{measure data importance} without direct data inspection. By focusing on the structural properties of graph data and using techniques like structural entropy, we aim to create a fair and effective data pricing mechanism that overcomes the limitations of previous methods.


% \smallskip

\noindent {\bf Comparison with FL and AL.} While {\em Federated Learning} (FL) and {\em Active Learning} (AL) are well-known paradigms for training models with distributed data, our approach differs in key ways.
{\bf (1)} In FL, each data owner trains a local model on private data, which is then aggregated into a global model while preserving privacy \cite{zhang2021survey}. SIMT, by contrast, does not require data owners to train models. Instead, data is directly provided to a central model, allowing for optimizations like data augmentation that are not possible in FL's gradient-based aggregation. This eliminates the computational burden on data owners and allows for more flexible model improvements. {\bf (2)} In AL, the model iteratively queries data points to refine learning, typically in multiple rounds \cite{ren2021survey}. SIMT, however, collects data in a single round, reducing overhead and cost. Furthermore, while AL assumes access to unlabeled data with labels provided iteratively \cite{cai2017active,zhang2021alg}, SIMT addresses the real-world challenge of compensating data owners, ensuring they are fairly rewarded for their contributions upfront.

\section{Problem formulation}
\subsection{Model marketplace for graph data}
We consider a model marketplace where a \emph{model consumer} interacts with multiple \emph{data owners} to trade graph-structured data. This data is distributed among the various data owners. Let the overall graph be represented as an attributed graph $\mathcal{G} \coloneqq (V, E, \mat{X}, \vect{y})$, where $V$ is the set of nodes, representing individual data subjects, $E \subseteq V \times V$ is the set of edges. $\mat{X} \in \mathbb{R}^{n \times m}$ is the feature matrix, where  $n$ is the number of nodes and $m$ is the dimensionality of the feature vector, and $\vect{y} \in \mathbb{R}^n$ is the label vector, where each entry corresponds to a label for each node. 
The adjacency matrix and normalised adjacency matrix of $\mathcal{G}$ are denoted as $\mat{A}$ and $\tilde{\mat{A}}$, resp.

For each node $v \in V$, let $\vect{x}_v \in \mathbb{R}^m$ and $y_v$ represent the feature vector and label of node $v$, resp. $N_v \subseteq V$ represents the set of neighbours of node $v$, and $d_v \coloneqq |N_v|$ denote the degree of node $v$.  

The graph $\mathcal{G}$ is distributed among multiple data owners, each controlling a subgraph. Let $O$ denote the set of data owners, where $o \coloneqq |O|$ represents the total number of data owners. For each data owner $i \in O$, the \emph{subgraph held by owner $i$} is represented as $\mathcal{G}_i \coloneqq (V_i, E_i, \mat{X}[V_i], \vect{y}[V_i])$, where $V_i \subseteq V$ is the set of nodes controlled by data owner $i$, $E_i = E \cap (V_i \times V_i)$ is the set of edges between nodes in $V_i$, $\mat{X}[V_i]$, and $\vect{y}[V_i]$ are the feature matrix, and label vector induced by the nodes in $V_i$, resp. Let $n_i \coloneqq |V_i|$ be the number of nodes in subgraph $\mathcal{G}_i$. 

Denote the edges within subgraphs as $\Dot{E}$ and the edges between the subgraphs as $\Ddot{E}$. We assume that $\Dot{E}\cap \Ddot{E}=\varnothing$ and then $E=\Dot{E}\cup \Ddot{E}$.
We also assume that the internal structure of each subgraph, including the features and labels of nodes, is private to the corresponding data owner. However, the connections between subgraphs (i.e., $\Ddot{E}$ the edges connecting nodes from different subgraphs) are known by the model consumer. 
% {\color{red} (Question: Maybe we can assume that some of the subgraphs are already known?)}{\teal (Why? our experiments do not have such assumption. I think it is better to make it consistent. )}  
Data owners are willing to sell the feature, label, and connection data for the nodes they control. 
% {\color{red} (Question: Are the different subgraphs mutually disjoint?)}{\teal (yes.)}

Each data owner $i\in O$ attaches a valuation to her attribute and label data of a single node, denoted by $\theta_{i} \in \Theta$, where $\Theta$ is the set of all possible valuations. 
The valuation $\theta_{i}$ indicates the minimum payment required by the data owner to allow the use of the attribute and connection data of a single node for model training. The valuation $\theta_i$ is privately known only to the data owner, but they may report a different valuation $\theta_i'\neq \theta_i$ if it serves their interests. 
We assume that each data owner values all their data subjects {\em equally}, implying that the total valuation is linearly dependent on the number of data records. Let $\vect{\theta}_i$ be $i$'s valuation vector for all nodes, i.e., $\vect{\theta}_i \coloneqq (\theta_{i,1},\ldots,\theta_{i,n_i}) =  \theta_i \cdot \vect{1}$, where $\theta_{i,v}$ is the valuation of $i$ for node $v$. 
The valuation of all data owners form a valuation matrix, denoted by $\mat{\theta}$, which is the concatenation $\vect{\theta}_1 \mid  \cdots \mid \vect{\theta}_o \in \Theta^n$.  
The model consumer has a {\em budget}, denoted by $\beta \in \R^+$, for buying the prediction model trained on structure and attribute data. 

The model marketplace involves designing a mechanism that procures the attribute/structure data from data owners, and train a GNN model for the model consumer.
%In the process of a model trading, the data owners initially express their data valuation to the data owners. Following this, the data owners submit their users' valuation  along with the connections between users after anonymisation. The mechanism acting on behalf of the model consumer makes the purchase decision based on these submissions. Once the data individuals are compensated, the data owners get the permission for data usage from them and provide the feature and label data to the mechanism, which then trains a model over the purchased data. 
% The following section provides formal definitions of a mechanism and the GNN model, and their desirable properties.




\subsection{Incentive mechanism} 
%Now we give the formal definition of mechanism and the desirable properties of the mechanism.% in our problem. 
\begin{definition}
    A {\em mechanism} $M$ consists of two functions, $(\pi(\cdot),$ $p(\cdot))$, where $\pi \colon \Theta^n \to \{0,1\}^n$ is an {\em allocation function} and  $p \colon \Theta^n \to \R^n$ is a {\em payment function}. 
\end{definition}

Given a set of data owners and a model consumer, the mechanism takes the reported valuation $\mat{\theta}'\in \Theta^n$ as input, and outputs allocation result and payment result.
The allocation function and the payment function determine which nodes are selected for model training and how much to pay for the data owners, resp. 
We write the allocation result $\mat{\pi}(\mat{\theta}')$ as $(\vect{\pi}_1(\mat{\theta}'), \ldots,\vect{\pi}_o(\mat{\theta}'))$ and the payment result $\mat{p}(\mat{\theta}')$ as $(\vect{p}_1(\mat{\theta}'),\ldots,\vect{p}_o(\mat{\theta}'))$, where each $\vect{\pi}_i(\mat{\theta}'), \vect{p}_i(\mat{\theta}')$ is a $n_i$-dimensional vector with each element $\pi_{i,v},p_{i,v}$ being an allocation and payment for $i$'s node $v$. The allocation and payment of node $v$ give data owner $i$ a utility $u_{i,v}(\mat{\theta}') = (p_{i,v}(\mat{\theta}')-\theta_{i,v})\pi_{i,v}(\mat{\theta}')$. The utility of data owner $i$ is $u_i=\sum_{v\in V_i} u_{i,v}$. %$u_i(\theta') = p_i(\theta')-\theta_i \pi_i(\theta')$. 
Once a node is selected, its connection, feature and label data are used for model training. 
% {\color{red} (Question: Should we describe exactly what will happen once the mechanism is executed? What data (nodes AND edges AND attributes AND labels?) are passed to the buyer from the data owners?)}


Let $\mat{\theta}_{-i}$ denote the valuation of all data owner but $i$ and $\Theta_{-i}$ denote the set of all possible $\mat{\theta}_{-i}$. 
A mechanism $M$ should satisfy: 
\begin{itemize}[leftmargin=*]
\item {\em Incentive Compatible} (IC): Each data owner $i \in O$ gains maximum utility when truthfully reporting her valuation, i.e., 
$u_i(\theta_i,\mat{\theta}_{-i}) \geq u_i (\theta_i',\mat{\theta}_{-i}), \ \forall \theta_i,\theta_i'\in \Theta, \forall \mat{\theta}_{-i}\in \Theta_{-i}.$
\item {\em Individual Rational} (IR): Each data owner $i \in O$ gains an non-negative utility when participating in the mechanism, i.e.,  \\
$u_i(\theta_i,\mat{\theta}_{-i}) \geq 0, \ \forall \theta_i \in \Theta, \forall \mat{\theta}_{-i}\in \Theta_{-i}.$
\item {\em Budget Feasible} (BF): Total payment given to all data owners is not exceed the budget $\beta$, i.e., 
 $\sum_{i\in O} \vect{p}_i \vect{\pi}_i\leq \beta$.
%$\sum_{v_i \in V} p_i \leq \beta.$
\end{itemize}

\subsection{Graph neural network models} 

We use GNN as the prediction model. 
Given a graph, GNN predicts the node labels by stacking multiple layers. Let $L$ be the number of layers in a GNN model. 
The main idea is to iteratively aggregate the feature information of each node from its neighbours. Specifically, given an attributed graph $\G=(V, E, \mat{X}, \vect{y})$, and a GNN with $L$ convolution layers, at a layer $\ell \leq L$, the feature embedding $\vect{h}_v^\ell$ of node $v\in V$ is generated through aggregation and update:
\begin{itemize}[leftmargin=*]
    \item Aggregation: aggregate the feature embeddings $\vect{h}_u^\ell$ of all neighbours $u$ of $v$ by an aggregate function such as mean and sum, with trainable weights, i.e., $\vect{n}_v^\ell \coloneqq \text{Aggregator}^\ell \left(\{\vect{h}_u^\ell, \forall u\in N_v\} \right)$. 
    \item Update: update the feature embedding $\vect{h}_v^{\ell+1}$ at the next layer by an update function of the embedding $\vect{h}_v^{\ell}$ and the aggregated embeddings $\vect{n}_v^\ell$, i.e., $\vect{h}_v^{\ell+1}\coloneqq\text{Updater}^\ell\left(\vect{h}_v^\ell,\vect{n}_v^\ell\right)$. Initially, the feature embedding of node $v$ is its feature vector, i.e., $\vect{h}_v^0 \coloneqq \vect{x}_v$. 
\end{itemize}



\subsection{Optimisation problem}
As discussed in the Introduction, a key issue in determining compensation for data owners in a model marketplace is assessing the importance of their data to model training without direct inspection. 
To summarise, the problem in this paper is:

Given the model marketplace with a model consumer and several data owners, we, as a {\em data broker}, aim to design a mechanism that procures the attribute/structural data from data owners, and train a GNN model for the consumer with the following subgoals:
\begin{itemize}[leftmargin=*]
    \item assessing the importance of data to model training without disclosing the feature and label data;
    \item optimising GNN performance within the budget; and
    \item ensuring the mechanism is IC, IR, and BF.
\end{itemize}

% {\color{red} (Question: Can we give the mathematical problem formulation?)}
% Let $\G'$ be the attributed graph constructed from the purchased data, and $L_{\text{GNN}}(\G')$ be the loss of a GNN model trained on $\G'$.
More formally, %given a set of data owners $O$, each owner $i$ possesses a graph as their data, with a corresponding valuation $\theta_i$ and utility $u_i$. Let $\mat{\theta}$ represent the valuations of all data owners. Let an attributed graph $\G=(V, E, \mat{X}, \vect{y})$ summarise the data items and the connections among data items, where $V$ is the set of data items (nodes), $E = \Dot{E} \cup \Ddot{E}$ represents the edges (connections among data items), $\mat{X}$ is the feature matrix, and $\vect{y}$ is the label vector. Here, $\Dot{E}$ denotes the connections held by individual data owners, while $\Ddot{E}$ represents the connections between different data owners.
let $\circ$ denote the Hadamard product operator, which selectively includes elements from $\Dot{E}, \mat{X}, \vect{y}$ according to the indicator vector $\vect{\pi} = (\vect{\pi}_1, \dots, \vect{\pi}_o)$. %, where each $\vect{\pi}_i$ represents the allocation vector for data owner $i$. Additionally, let $\vect{p}_i$ represent the payment to data owner $i$.
We define $f_{GNN}(\cdot)$ as the output of a GNN model trained on a selected subset of the data with the known $\Ddot{E}$.
The problem in the paper can be formulated as:
% the following optimisation problem:
% \begin{align*}
% \min \quad & L_{\text{GNN}}(\G') \\
% %\max &  \sum_{v \in V} \phi_v \pi_{i,v}   \\
% {\text s.t. \quad}  & u_i(\theta_i,\mat{\theta}_{-i}) \geq u_i (\theta_i',\mat{\theta}_{-i}), \ \forall \theta_i,\theta_i'\in \Theta, \forall \mat{\theta}_{-i}\in \Theta_{-i}. && \text{IC}  \\
%  &  u_i(\theta_i,\mat{\theta}_{-i}) \geq 0, \ \forall \theta_i \in \Theta, \forall \mat{\theta}_{-i}\in \Theta_{-i} && \text{IR} \\
%  & \sum_{i\in O} \vect{p}_i \vect{\pi}_i\leq \beta  && \text{BF}
% \end{align*}
\begin{align*}
\min \quad & ||\vect{y}- f_{GNN}(\vect{\pi} \circ \G)|| \\
%\max &  \sum_{v \in V} \phi_v \pi_{i,v}   \\
\text{s.t. \quad}  & u_i(\theta_i,\mat{\theta}_{-i}) \geq u_i (\theta_i',\mat{\theta}_{-i}), \ \forall \theta_i,\theta_i'\in \Theta, \forall \mat{\theta}_{-i}\in \Theta_{-i}. && \text{(IC)}  \\
 &  u_i(\theta_i,\mat{\theta}_{-i}) \geq 0, \ \forall \theta_i \in \Theta, \forall \mat{\theta}_{-i}\in \Theta_{-i} && \text{(IR)} \\
 & \sum_{i\in O} \vect{p}_i \vect{\pi}_i\leq \beta  && \text{(BF)}  \\
& \vect{p}_i\geq 0, \pi_{i,v}\in \{0,1\} \quad  \forall i\in O, \forall v\in V_i
\end{align*}

\section{Proposed method}

In this section, we propose a mechanism that procures the most contributing data and trains a GNN model using the procured data. 
By considering the correlation between graph structure, features, and labels,
% Taking into account the correlation between the graph structure and feature and label data, 
we leverage the graph structure to offer insights into the contribution of the associated data. Then, we combine this contribution assessment with the data  owners' valuation in an auction mechanism to select the most cost-effective data. Subsequently, we augment the procured data using feature imputation and edge augmentation and use the augmented data to train a two-layer GNN model, which is returned to the model consumer. The overall framework is shown in Figure~\ref{fig:framework}. 



\subsection{Structural importance} \label{sec:structural-importance}
We begin by evaluating the importance of data to model training without inspecting the feature and label data. 
Our solution is motivated by the observation that the structure of a graph often encodes valuable information about its features and labels. According to the well-known homophily assumption, nodes with similar features and labels are more likely to be closely connected \cite{tang2023generalized,mcpherson2001birds}. This is further validated by our case studies on five real-world graphs. We analyse the connections both within and between classes, and the results show that the number of edges within the same class is substantially higher than between different classes, as illustrated in Figure~\ref{fig:label_structure_correlation}. The strong correlation between graph structure and the associated features and labels motivates our approach to leverage the graph structure as auxiliary information in the data selection process.
Thus we propose to use the {\em structural importance} of data owner to represent her data importance.   
% {\color{red}
% Our solution is motivated by the strong correlation between graph structure and feature and label data. On one hand, graph structure is highly correlated with label data. We conduct case studies on three real world graphs, analysing the connections within class and between classes.
% Figure~\ref{fig:label_structure_correlation} shows that the proportion of intra-class edges is significantly larger than that of inter-class edges. On the other hand, the high correlation between label and feature have been validated by extensive studies in the machine learning community \cite{duong2019node,rossi2022unreasonable,hu2021graph}. The correlation between graph structure and feature and label data motivates us to use the graph structure as auxiliary information to facilitate the data selection process. }
% Thus we propose to use the {\em structural importance} of data owner to represent her data importance.   
%\vspace{-0.3cm} 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/link_dist.png}
    %\vspace{-0.3cm}
    \caption{Proportion of intra-class and inter-class edges} %across five cases. In each case, blue and red bars represent the average ratio of intra-class and inter-class neighbours per node in a given class $C$.  }
    \label{fig:label_structure_correlation}
\end{figure} 
%\vspace{-0.3cm}

The sample dataset for model training should be both informative, reducing the uncertainty of the model, and representative, capturing the overall pattern of the entire dataset \cite{zhang2021alg}.  Therefore, we measure the structural importance of data owners in terms of informativeness and representativeness. 
Nevertheless, determining the informativeness and representativeness of nodes in a graph often relies on the true classes and node features, which are not available due to the absence of true feature and label data. To address this, we first use {\em structural clusters} to approximate the true classes. With this clustering, we propose the notion of {\em marginal structural entropy} to quantifies informativeness, and deploy PageRank centrality to quantify representativeness. 

\smallskip
{\em Structuring clustering. } 
A crucial tool for the structural clustering is {\em structural entropy}. 
Let $G=(V,E)$ represent a graph without attributes. 
Suppose $P \coloneqq \{C_1,C_2,\ldots,C_T\}$ is a partition of $V$, where each $C_t$ is called a {\em cluster} and $T$ is the number of clusters. {\em Structural entropy} of $G$ relative to $P$ captures the information gain due to the partition. 
For each cluster $C_t\in P$, write $d_t$ as the sum of degrees $d_v$ of all nodes $v\in C_t$.
%we call the sum of degrees $d_v$ of all nodes $v\in C_t$ as its {\em volume} and denote the volume by $d_t$. 
Write $g_t$ as the number of the edges between the nodes in $C_t$ and those in the other clusters. 
The {\em structural entropy} \cite{liu2019rem} of $G$ relative to $P$ is
\begin{equation*}
\mathcal{H}_P(G) =-\sum_{t=1}^T \frac{d_t-g_t}{2|E|}\log\frac{d_t}{2|E|}.
\end{equation*}        
A greater value of $\mathcal{H}_P(G)$ means that the corresponding partition $P$ gains more information about the graph $G$ and thus $P$ is preferable.  

Given that, we would like to obtain a good partition by maximising the structural entropy $\mathcal{H}_P(G)$.
Unfortunately, maximising the structural entropy $\mathcal{H}_P(G)$ is NP-hard \cite{li2016structural,wang2023user}. As an alternative, we propose an algorithm $\mathsf{Clustering}(G)$ that harnesses the power of unsupervised GCN models \cite{wang2023user} to obtain a partition $P$.  Specifically, $\mathsf{Clustering}(G)$ first employs the Singular Value Decomposition (SVD) \cite{brunton2022data} to generate spectral features, and a classical Variational Graph Auto-Encoder (VGAE) model \cite{kipf2016variational} with reconstruction loss taking the generated spectral features as input to learn node embeddings. %This process is iterated 100 times to comprehensively capture the graph's structural information. 
Using the obtained node embedding, $\mathsf{Clustering}(G)$ then trains a linear classifier to get a partition by maximising structural entropy. %This iterative process is repeated 300 times to arrive at a robust partition. 
% 

%\smallskip
\paragraph{Structural informativeness.}
Given the learned clustering $P$, we propose the notion of {\em cluster-based marginal structural entropy} to measure the structural informativeness of data owners. Basically, the marginal structural entropy captures the information gain of a node to the clustering. The lower the marginal structural entropy of a node has, the more uncertainty this node has, and thus more information the node's data will capture.  
% Now we formally define the {\em marginal structural entropy} of each node $v \in C_t$ to the structural entropy $\mathcal{H}_P(G)$. 
More formally, we define the marginal structural entropy of node $v$ as the information gain due to existence of $v$, i.e., the difference between the structural entropy of graph $G$ relative to $P$ and that of $G$ without node $v$ relative to $P'$.
Then the {\em normalised marginal structural entropy} $\epsilon_v$ of $v$ is the normalised difference in structural entropy with partition $P$ and another partition $P'$ that moves $v$ out of its cluster $C_t$, i.e., $\epsilon_v = (\mathcal{H}_P(G) - \mathcal{H}_{P'}(G) )/ \mathcal{H}_P(G)$. %See Appendix~\ref{app:calculation} for the detailed calculation.
Let $n_{v,t}$ be the number of nodes that are incident to $v$ and belong to $C_t$. After calculation, we have the following (see App.~\ref{app:calculation} for the detailed calculation):  

\begin{definition}
The {\em normalised marginal structural entropy} of node $v \in C_t$ to structural entropy $\mathcal{H}_P(G)$ is 
\begin{equation*}
    \epsilon_{v} = \frac{(d_t-g_t)\log\frac{d_t}{d_t-d_v}+2n_{v,t}\log\frac{d_t-d_v}{2|E|}}{(d_t-g_t)\log\frac{d_t}{2|E|}}.
\end{equation*}
\end{definition}

%We provide the calculation of the normalised marginal structural entropy in Appendix~\ref{app:calculation}.

\noindent A lower normalised marginal structural entropy means more structural uncertainty of $v$, making $v$ more informative. 

\paragraph{Structural representativeness.} 
% Given the clustering $P$, 
We use a classical structural centrality measure, PageRank \cite{ma2008bringing}, to quantify the structural representativeness of a node. We opt for PageRank centrality due to its superior performance compared to other centrality measures, as validated in App.~\ref{app: centrality}.  The higher the PageRank centrality of a node has, the more representative the node is. 
% We denote the PageRank centrality of a node $v$ by $\rho_v$.  
Let $\gamma \in (0,1)$ denote the damping factor, which controls the probability of following links.
The PageRank centrality $\rho_v$ of node $v$ is:
$$
    \rho_v=\gamma \sum_{u\in N_v}\frac{\rho_u}{|N_u|}+\frac{1-\gamma}{|V|}.
$$
% where $\gamma$ is the damping factor.


\paragraph{Structural importance score. } Given the clustering $P$,  the entropy $\epsilon_v$ and the PageRank centrality $\rho_v$ of each node $v$, we define the structural importance score. Specifically, we first sort all nodes in their own cluster by their entropy and PageRank values, resp. Nodes are sorted in ascending order by entropy (as lower entropy indicates higher informativeness) and in descending order by PageRank (as higher PageRank indicates greater representativeness). This ensures that more informative and representative nodes are prioritised.
For a node $v$ in cluster $C_t$, let $\text{rank}^{\text{entr}}_v$ denote its rank by entropy and $\text{rank}^{\text{pr}}_v$ denote its rank by PageRank.  We then define node $v$'s informativeness and representativeness based on these rankings as
$$\phi^{\text{info}}_v \coloneqq \frac{\text{rank}^{\text{entr}}_v}{|C_t|} \text{, and } \phi_v^{\text{rep}}\coloneqq \frac{\text{rank}^{\text{pr}}_v}{|C_t|}, \text{ resp.}$$

Finally, we define the structural importance score of a node. 
% The structural importance score is a combination of informativeness and representativeness. 
Following the approach in \cite{cai2017active,zhang2021alg}, we introduce a parameter $\alpha$ to balance representativeness and informativeness.
Intuitively, representative data helps to learn general classification patterns, while informative data is used to refine the classification boundaries.
Therefore, when the budget $\beta$ is relatively small compared to the overall valuations of data owners and the partition $P$ is complex (i.e., when $T$ is large), prioritising representative data is crucial to learning the general classification patterns. On the other hand, when the budget is relatively large and the partition is simpler, a small amount of representative data is sufficient to capture the overall pattern, allowing us to focus on acquiring more informative data to further refine the classification. 
%Therefore, representativeness is preferred when the budget $\beta$ is small and the partition $P$ is complex ($T$ is large). 
More formally, given the average valuation $\overline{\theta}$, defined as the average of the upper and lower bounds of data valuations, we set $\alpha = \frac{1}{2}(1+\frac{\beta}{n\overline{\theta}})^{-T}$. 
The {\em structural importance score} of node $v$ is then defined as 
\begin{equation}
\label{eqn:score}
  \phi_v \coloneqq (1-\alpha)\phi^{\text{rep}}_v+\alpha \phi^{\text{info}}_v.  
\end{equation}
    
% {\bf The imbalance issue. } In most of the real-world datasets, the data are imbalancedly distributed in classes such that informative nodes and representative nodes are more likely to be involved in large classes. To tackle this issue, we 





\subsection{Model trading mechanism} \label{sec:model-trading}

We propose a model trading mechanism, named {\em Structural importance based model trading (SIMT) mechanism}, which consists of two phases: (1) data procurement phase selects the most cost effective data owners, and (2) model training phase trains a GNN model on the procured data; See the workflow of SIMT in Figure~\ref{fig:framework} and the algorithm in Alg.~\ref{alg:trading}.% Next we introduce the detailed design of the two phases resp. 

\paragraph{Phase 1. Data procurement.} In Phase 1, SIMT takes the attributed graph $\G$ and the valuation vector $\mat{\theta}$, and the budget $\beta$ as inputs and returns an allocation result $\vect{\pi}$ and a payment result $\vect{p}$. Firstly,  $\mathsf{Clustering}(G)$ returns a clustering $P$ of the nodes $V$ after training. Given the clustering $P$ with $T$ clusters,  the mechanism computes the structural importance score $\phi_v$ of each node $v \in V$ using Equation~\eqref{eqn:score}, which represents the importance of the node. Then an auction is conducted in each cluster $C_t\in P$ with budget $\beta/T$. In the auction for each $C_t$, nodes in $C_t$ are sorted in descending order based on the ratio $\frac{\phi_v}{\theta_{i,v}}$, where data owner $i$ owns node $v$.  
The mechanism selects the most cost-effective $k$ data until the total payment exceeds the allocated budget. The payment to data owner $i$ for node $v$ is $p_{i,v} = \min\{ \frac{\beta}{T} \frac{\phi_v}{\sum_{u=1}^k \phi_u}, \frac{\theta_{j,w}}{\phi_{w}} \phi_v\}$, where $j$ is the first data owner who has not had any data selected and $w$ is her first data in the order (if such data owner $j$ does not exist, we set $p_{i,v}$ as $\min\{\beta \phi_v/ (T\sum_{u=1}^k \phi_u), \tilde{\theta} \phi_v/\phi_{k+1}\}$, where $\tilde{\theta}$ is the upper bound of $\Theta$).
The total payment to data owner $i$ is $\sum_{v\in V_i, v\leq k}p_{i,v}$.

%In each auction of $C_t$, the mechanism sorts all data owners in $C_t$ by the ratio $\phi_i/ \theta_i$ in a descending order, and selects the most cost effective $k$ data owners until the total payment exceeds the budget. The payment for these $k$ data owners is  $\min\{\beta \phi_i/ (T\sum_{j=1}^k \phi_j),\theta_{k+1} \phi_i/\phi_{k+1}  \}$ and $0$ for others.
% while the payment for others is $0$. 



\begin{algorithm}[t]
\caption{The SIMT mechanism}
\label{alg:trading}
\footnotesize
\begin{algorithmic}[1]
\Require Attributed graph $\G$, data owners $O$, valuation vector $\vect{\theta}$, and budget $\beta$
\Ensure Allocation $\vect{\pi}$, payment $\vect{p}$, and trained model $f_{\text{GNN}}$ 
\State let $G=(V,\Ddot{E})$ represent the known subgraph of $\G$ without attributes.
\\
{\bf Phase 1: Data procurement}
\State get a partition $P \gets \mathsf{Clustering}(G)$   \label{ln:Clustering}  
\State compute the structural importance score $\phi_v$ for each $v\in V$ according to $P$
\State initialise $\vect{\pi}=\boldsymbol{0},\vect{p}=\boldsymbol{0}$
\For{each cluster $C_t \in P$}
\State sort the nodes $v \in C_t$ by $\frac{\phi_v}{\theta_{i,v}}$ in a descending order 
\State find the largest $k$ such that $\theta_k \leq \frac{\phi_k}{\sum_{u\leq k} \phi_u} \frac{\beta}{T}$
\For{$v\leq k$}
\State Let $i\in O$ be the data owner of node $v$
\State set $\pi_{i,v}=1$, $p_{i,v} = \min\{ \frac{\beta}{T} \frac{\phi_v}{\sum_{u=1}^k \phi_u}, \frac{\theta_{j,w}}{\phi_{w}} \phi_v\}$
%$p_{i,v}=\min\left\{\frac{\beta}{T} \frac{\phi_i}{\sum_{j=1}^k \phi_j},\frac{\theta_{k+1}}{\phi_{k+1}}  \phi_i\right\}$ 
\EndFor 
\EndFor 
\State procure data to get $\mat{X}_s,\vect{y}_s$, update $G$ and normalized adjacency matrix $\tilde{\mat{A}}$
\\
{\bf Phase 2: Model training}
%\STATE {\color{red} feature propagation}
% \STATE initialise $\mat{X}'\gets \mat{X}$ 
% \WHILE{ $\mat{X}_{\text{FP}}$ has not converged} 
% \STATE $\mat{X}_{\text{FP}}\gets \tilde{\mat{A}} \mat{X}$
% \STATE  $\mat{X}_{\text{FP}}\gets \mat{X}'_s$
\State initialise $\mat{X}'\gets [\mat{X}_s,\mat{0}_u]^ \intercal$ 
\While{ $\mat{X}'$ has not converged} 
\State $\mat{X}'\gets \tilde{\mat{A}} \mat{X}'$
\State $\mat{X}'\gets [\mat{X}_s,\mat{X}'_u]^ \intercal$
\EndWhile
\State do edge augmentation on $G$ and get $\overline{G}$
\State $f_{\text{GNN}}\gets \mathsf{Train} (\overline{G
},\mat{X}',\vect{y}_s) $
\end{algorithmic}
\end{algorithm}

%\noindent{\em SE-based classifier.} We train the classifier (line~\ref{ln:classifier}) such that the classifier returns a partition that maximises the structural entropy. 

%\todo[inline]{Add the description of training classifier, including loss function .... refer to [muller2023graph] on modularity.  }


\paragraph{Phase 2. Model training.} Given the allocation result $\vect{\pi}$, the model training phase uses the connections, features and labels of the selected data owners to train a GNN model. 
However, due to budget constraints, only a subset of features and connections can be purchased, which may not be sufficient for training a robust GNN model. 
To address this, we first impute the missing node features and augment the missing edges before training.
After acquiring the features from the selected nodes, we apply the feature propagation algorithm \cite{rossi2022unreasonable} to infer the features of the unselected nodes, producing a new feature matrix $\mat{X}'$.
Additionally, we use the $G(|V|,|E|)$ Erdos-Renyi (ER) model \cite{bollobas1998random} to generate missing edges, where $|V|$ and $|E|$ are determined by the edge density of the known graph. Then we incorporate contrastive learning \cite{oord2018representation} to mitigate the randomness introduced by the ER model, resulting in an augmented graph $\overline{G}$. 
The GNN training algorithm then takes the augmented graph $\overline{G}$, the new feature matrix $\mat{X}'$ and labels $\vect{y}_s$ as input and returns a GNN model to the consumer.  

%\smallskip 

{\em Node feature imputation.} 
Features are crucial when training GNN \cite{taguchi2021graph,spinelli2020missing}, and we apply a feature imputation method to the procured data to address missing values. Among various feature imputation methods, we choose the Feature Propagation algorithm due to its strong convergence guarantees, simplicity, speed, and scalability \cite{rossi2022unreasonable}.
We use subscripts $s$ and $u$ to denote the selected and unselected nodes, resp. Write $\mat{X}=[\mat{X}_s,\mat{X}_u]^ \intercal$ and $\vect{y}=[\vect{y}_s,\vect{y}_u]^ \intercal$. 
Also, we write the normalised adjacency matrix $\tilde{\mat{A}}$ and the graph Laplacian $\Delta$ of $\overline{\G}$ as
$\tilde{\mat{A}}=\begin{bmatrix}
\tilde{\mat{A}}_{ss} & \tilde{\mat{A}}_{su} \\
\tilde{\mat{A}}_{us} & \tilde{\mat{A}}_{uu} 
\end{bmatrix},
\Delta=\begin{bmatrix}
\Delta_{ss} & \Delta_{su} \\
\Delta_{us} & \Delta_{uu} 
\end{bmatrix}$, resp.
In the feature imputation process, the feature matrix $\mat{X}'$ is initialised with the known feature $\mat{X}_s$ and a zero matrix $\mat{0}_u$ for the unselected nodes.
The feature matrix $\mat{X}'$ is then iteratively updated as follows:
$\mat{X}^{(t)}=\begin{bmatrix}
 \mat{I} & \mat{0} \\
 \tilde{\mat{A}}_{us} & \tilde{\mat{A}}_{uu} 
 \end{bmatrix} \mat{X}^{(t-1)},$
where this process continues until the feature matrix converges.
The steady status of the feature matrix is \cite{rossi2022unreasonable}: $\lim_{t\to \infty} \mat{X}^{(t)} = \begin{bmatrix}
    \mat{X}_s\\
    -\Delta_{ss}^{-1} \tilde{\mat{A}}_{us} \mat{X}_s
\end{bmatrix}$.
% $$
% \lim_{t\to \infty} \mat{X}^{(t)} = \begin{bmatrix}
%     \mat{X}_s\\
%     -\Delta_{ss}^{-1} \tilde{\mat{A}}_{us} \mat{X}_s
% \end{bmatrix}.
% $$ 
%where $\tilde{\mat{A}}_{us}$ and $\Delta_{ss}$ are submatrices of the normalised adjacency matrix  and the graph Laplacian:
%$\tilde{\mat{A}}=\begin{bmatrix}
%\tilde{\mat{A}}_{ss} & \tilde{\mat{A}}_{su} \\
%\tilde{\mat{A}}_{us} & \tilde{\mat{A}}_{uu} 
%\end{bmatrix},
%\Delta=\begin{bmatrix}
%\Delta_{ss} & \Delta_{su} \\
%\Delta_{us} & \Delta_{uu} 
%\end{bmatrix}$.
% $$
% \tilde{\mat{A}}=\begin{bmatrix}
% \tilde{\mat{A}}_{ss} & \tilde{\mat{A}}_{su} \\
% \tilde{\mat{A}}_{us} & \tilde{\mat{A}}_{uu} 
% \end{bmatrix},
% \Delta=\begin{bmatrix}
% \Delta_{ss} & \Delta_{su} \\
% \Delta_{us} & \Delta_{uu} 
% \end{bmatrix}
% $$

%\smallskip 

\paragraph{Edge Augmentation.}
Given the critical role of message passing in GNNs, the absence of certain edges may impede this process, leading to sub-optimal model performance.  To alleviate this issue, we introduce augmented edges to  enhance message passing. 
Specifically, we employ the ER model \cite{bollobas1998random} to generate edges for data owners with multiple unselected nodes.
However, the introduction of augmented edges may inadvertently introduce noise, which could mislead the model by learning from incorrect connections. To counteract this, we integrate contrastive loss \cite{oord2018representation}, denoted by $L_{\text{ctr}}$, into the GNN training process. This loss function encourages the model to maximise the similarity between the augmented graph and the original (non-augmented) graph views.
Given a graph $\G$ and an augmented graph $\overline{\G}$, let $\vect{h}_v$ and  $\vect{h}_v'$ be the feature embeddings in $\G$ and  $\overline{\G}$, resp. 
The contrastive loss of a node $v$ is:
\begin{equation*}
    L_{\text{ctr}}(v)=-\log \frac{\exp(\vect{h}_v\cdot \vect{h}_v'/\tau)}{\sum_{u\in V}\exp(\vect{h}_v\cdot \vect{h}_u'/\tau)}, 
\end{equation*}
where $\tau$ represents the temperature parameter, which scales the similarities between the embeddings $\vect{h}_v$ and $\vect{h}_v'$. 

%We provide a running example of data procurement in SIMT over a graph with partition $T=3$, as shown in Figure~\ref{fig:running example}. SIMT first partitions the nodes into 3 clusters: A, B and C. Next, it calculates the importance score for each nodes, represented by the bar next to each node, where a longer bar indicates higher importance. Finally, an auction is conducted within each cluster, and the selected data owners are highlighted in red.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.8\linewidth]{figures/data selection.drawio.png}
%    \caption{An example of procurement in SIMT.}
%    \label{fig:running example}
%\end{figure}

\subsection{Analysis}

Now we show that SIMT satisfies IC, IR and BF properties. Additionally, we analyse its time complexity.

\begin{theorem}
The SIMT mechanism is incentive compatible, individual rational and budget feasible. 
\end{theorem}

%\vspace{-0.4cm}

\begin{proof}
We first show that the mechanism is IR. In each auction of cluster $C_t$, the utility that a data owner $i$ obtains from a node $v>k$ when she truthfully reports is $u_{i,v}(\vect{\theta})=\theta_{i,v} \pi_{i,v}(\theta)-p_{i,v}(\theta)=0-0\geq 0$; the utility that a data owner $i$ obtains from $v\leq k$ is $u_{i,v}(\theta)=p_{i,v}(\theta)-\theta_{i,v} \pi_{i,v}(\theta)=\min\{ \nicefrac{(\beta\phi_v)}{(T\sum_{u=1}^k \phi_u)}, \nicefrac{(\phi_v\theta_{j,w})}{\phi_{w}} \}-\theta_{i,v}\times 1 \} \geq 0$. 
Therefore, the utility of data owner $i$ is $
\sum_{v\in V_i} u_{i,v} \geq 0$. 
Also, SIMT satisfies BF. The total payment in cluster $C_t$ is \\ $\sum_{v=1}^k \min\{ \nicefrac{(\beta \phi_v)}{(T\sum_{u=1}^k \phi_u)}, \nicefrac{(\theta_{j,w}\phi_v}{\phi_{w}} \} \leq \sum_{u=1}^k \phi_u \times \nicefrac{\beta}{(T \sum_{u=1}^k \phi_u)} = \nicefrac{\beta}{T}$. Then the total payment in $T$ clusters is $\leq \beta$, which shows BF. %is satisfied. 

Next we show IC. Each of data subject is assigned to an auction associated with a cluster $C_t$. As the assignment is independent of the reported valuation of the data owner, we just need to show that in the auction for each cluster is IC. 
In one auction, we consider an arbitrary data owner. When she report truthfully, there are two cases regarding each of her node, either being selected or not. We discuss the two cases separately. %\begin{itemize}[leftmargin=*]

\noindent{\bf (1)} Consider an arbitrary node $v$ that is selected. Assume that in the ranking, the $(k+1)$-th node is possessed by data owner $j$, i.e., the $(k+1)$-th ratio is $\nicefrac{\phi_{k+1}}{\theta_{j,k+1}}$. Note that the data owner $j$ could be $i$. 
If $i$ reports a lower valuation $\theta_{i,v}'< \theta_{i,v}$ or a higher valuation $ \theta_{i,v} < \theta_{i,v}' <  \nicefrac{\theta_{j,k+1}\phi_v}{\phi_{k+1}}$, as the marginal contribution $\phi_v$ is independent from the reported valuation, the ratio $\nicefrac{\phi_v}{\theta_{i,v}'} \geq \nicefrac{\phi_{k+1}}{\theta_{j,k+1}}$ and her ranking is still in the top $k$. Further,  the payment of $i$ for $v$ is independent from $i$'s report. As a consequence, her utility of $v$ is $u_{i,v}(\vect{\theta}_{i}',\vect{\theta}_{-i}) = u_{i,v}(\vect{\theta}_i,\vect{\theta}_{-i})$. 
%If she reports a higher valuation $ \theta_i < \theta_i' < \theta_{k+1} \frac{\phi_i}{\phi_{k+1}}$, the ratio $ \frac{\phi_i}{\theta_i} > \frac{\phi_i}{\theta_i'} \geq \frac{\phi_{k+1}}{\theta_{\theta_{k+1}}}$ and her ranking is also among the top $k$. Then her  $u_i(\theta_i',\theta_{-i}) = u_i(\theta_i,\theta_{-i})$. 
If she reports a even higher valuation $\theta_{i,v}' \geq  \nicefrac{\theta_{j,k+1}\phi_v}{\phi_{k+1}}$, the ratio $\nicefrac{\phi_{k+1}}{\theta_{j,k+1}} > \nicefrac{\phi_v}{\theta_{i,v}'}$. Then her allocation becomes $0$ and her utility of $v$ is $u_{i,v}(\vect{\theta}_i',\vect{\theta}_{-i}) = 0 \leq u_{i,v}(\vect{\theta}_i,\vect{\theta}_{-i})$.
    
\noindent{\bf (2)} Consider a node $v$ that is not selected. Assume that in the ranking, the $k$-th node is possessed by data owner $j$, i.e, the $k$-th ratio is $\nicefrac{\phi_{k}}{\theta_{j,k}}$. Here, $j$ could also be $i$. 
If $i$ reports a higher valuation $\theta_{i,v}' > \theta_{i,v}$ or a lower valuation $\nicefrac{\theta_{j,k}\phi_v}{\phi_{k}} \leq \theta_{i,v}' < \theta_{i,v}$, the ratio $\nicefrac{\phi_k}{\theta_{j,k}} \geq \nicefrac{\phi_v}{\theta_{i,v}'}$, and $v$'s ranking is still not among the first $k$. Then $i$'s utility of $v$ is $u_{i,v}(\vect{\theta}_i',\vect{\theta}_{-i}) = u_{i,v}(\vect{\theta}_i,\vect{\theta}_{-i})=0$. 
%If she reports lower valuation $\theta_{k} \frac{\phi_i}{\phi_{k}} \leq \theta_i' < \theta_i$, the ratio $\frac{\phi_k}{\theta_k} \geq \frac{\phi_i}{\theta_i'}  >\frac{\phi_i}{\theta_i}$, and her ranking is still not among the first $k$. Then her utility $u_i(\theta_i',\theta_{-i}) = u_i(\theta_i,\theta_{-i})=0$. 
If $i$ reports a much lower valuation  $\theta_{i,v}'<  \nicefrac{\theta_{j,k}\phi_v}{\phi_{k}}$, and her ranking is among the first $k$. Her utility $u_i(\vect{\theta}_i',\vect{\theta}_{-i})  = \theta_i\!- \!\min\{ \frac{\beta}{T} \frac{\phi_v}{\sum_{u=1}^k \phi_u}, \frac{\theta_{j,w}}{\phi_{w}} \phi_v\} \! \leq \! 0 = u_i(\vect{\theta}_i,\vect{\theta}_{-i}).$
    %\end{itemize}
\end{proof}

\noindent{\bf Time complexity.}
Given a GNN, recall $m$ is the dimensions of the input and let  $m_{\text{h}}$ be the hidden layers. 
The computational complexity of a typical GNN is $O(|E| m + |V| m m_{\text{h}})$.  The computational complexity of SIMT is $O((|E| m + |V| m m_{\text{h}})+(|E| m + |V| m m_{\text{h}})+(|V|+|E|))$, where the first one terms correspond to the complexity of training the GNN,  while the last two terms account for the computation of clustering and PageRank centrality resp.


\section{Experiment}
We conduct experiments to validate the performance of proposed SIMT mechanism in terms of node classification accuracy. 
(1) To demonstrate the overall performance of SIMT, we compare it with multiple baselines under different budgets. (2) To underscore the impact of each component, we perform a detailed ablation study.
%Specifically, (1) To show the overall performance of SIMT, we compare it with several baselines under varying budgets. (2) To show the effectiveness of each component in our mechanism, we conduct an ablation study. % and answer the question: How to optimise GCN model performance with the limited budget?
% Our code is available at \url{https://anonymous.4open.science/r/GNNModelPricing-BF5C}.
% We conduct experiments to 1) compare our auction method with other baselines. 2) verify each component of our proposed method.

%\vspace{-0.2cm}
\subsection{Experiment setup} \label{sec:setup}
%We use the node classification task to evaluate the performance of our algorithm.
%We first describe the experiment setup including datasets,  GNN models used for $\mathsf{Clustering}(G)$ and for $\mathsf{Train}$, and the generation of data valuations $\theta$. 

%\smallskip
% {\it Dataset.}
\paragraph{Dataset.} 
Five widely-used datasets are included in our experiments: Cora, Citeseer, Pubmed, Amazon and Coauthor \cite{kipf2016semi,cai2017active,rossi2022unreasonable,shchur2018pitfalls}. The dataset statistics are listed in Table~\ref{tab:dataset_statistics} in App.~\ref{app: dataset}.
For each dataset, we randomly sample $15\%$ of the data as test set, which remains untouched during data procurement. This set is consistent across all baselines.
%which are excluded from data procurement, and fix it for all baselines. 
Once getting the selected data from auction, we further split $80\%$ as training data and remaining $20\%$ for validation.
%of the selected data as train data and $20\%$ as validation data. 
%
To accommodate different real-world scenarios, we follow the setup in existing studies \cite{hu2020trading,jia2019efficient,liu2020dealer,ohrimenko2019collaborative,sim2020collaborative,xu2021gradient} to 
validate SIMT on various datasets and varying hyperparameters.

% \begin{table}[ht]
% \centering
% \caption{Dataset statistics}
% \label{tab:dataset_statistics}
% \footnotesize
% \begin{tabular}{|c|cccc|}
% \hline
% Dataset & $\#$ nodes & $\#$ edges & $\#$ features & $\#$ classes \\ \hline
% Cora & 2,485 & 5,069 & 1,433 & 7 \\ \hline
% Citeseer & 2,120 & 3,679 & 3,703 & 6 \\ \hline
% Pubmed &19,717 & 44,324 & 500 & 3 \\ \hline
% Amazon &  7,650 & 238,162 & 745 & 8 \\ \hline
% Coauthor &  34,493 & 495,924 & 8,415 & 5 \\ \hline
% \end{tabular}%
% \end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[]
\caption{Node classification performance under different budgets}
\label{tab:main-performance}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|c|c|}
\hline
\multicolumn{2}{|c|}{budget} & \multicolumn{2}{c|}{$50$} & \multicolumn{2}{c|}{$100$} & \multicolumn{2}{c|}{$150$} & \multicolumn{2}{c|}{$200$} & \multicolumn{2}{c|}{$250$} & \multicolumn{2}{c|}{$300$} & - & - \\ \hline
\multicolumn{2}{|c|}{metric} & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & \begin{tabular}[c]{@{}c@{}}average\\ accuracy\end{tabular} & $\frac{\text{ave. accuracy}}{\# \text{bought.items}}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Cora}} & Greedy & $12.3\pm 4.2$ & $21.5\pm 6.3$ & $17.6\pm 5.6$ & $29.1\pm 11.1$ & $20.6\pm 6$ & $33.9\pm 9.8$ & $23.6\pm 6$ & $36.7\pm 9.8$ & $26.5\pm 5.9$ & $40.1\pm 9.3$ & $28.0\pm 4.7$ & $41.5\pm 8.8$ & 33.8 & 0.16 \\
\multicolumn{1}{|c|}{} & ASCV & $10.9\pm 4.5$ & $21.0\pm 6.6$ & $16.2\pm 5.3$ & $29.1\pm 8.2$ & $23.8\pm 6.9$ & $36.0\pm 5.1$ & $23.7\pm 4.5$ & $36.0\pm 4.9$ & $28.1\pm 5.6$ & $38.5\pm 5.0$ & $32.4\pm 8.9$ & $43.9\pm 6.2$ & 34.1 & 0.20 \\
\multicolumn{1}{|c|}{} & Greedy(P) & $24.6\pm 10.6$ & $36.1\pm 14.4$ & $29.0\pm 9.6$ & $40.6\pm 12$ & $32.3\pm 8.5$ & $44.5\pm 10$ & $36.0\pm 9.8$ & $48.8\pm 8.4$ & $37.3\pm 9.3.5$ & $48.9\pm 8$ & $38.6\pm 10$ & $50.8\pm 8.3$ & 45.0 & 0.22 \\
\multicolumn{1}{|c|}{} & ASCV(P) & $21.9\pm 5.8$ & $34.8\pm 6.8$ & $37.2\pm 11$ & $46.9\pm 9.8$ & $47.3\pm 7$ & $53.7\pm 6.5$ & $55.3\pm 5.9$ & $61.2\pm 4.9$ & $60.3\pm 5.1$ & $65.2\pm 5.1$ & $65.0\pm 6.3$ & $68.1\pm 5.7$ & 55.0 & 0.33 \\
\multicolumn{1}{|c|}{} & \textbf{SIMT} & $\bm{36.1\pm 10.7}$ & $\bm{48.4\pm 9.5}$ & $\bm{46.7\pm 8.8}$ & $\bm{55\pm 6.7}$ & $\bm{56.8\pm 10}$ & $\bm{62.5\pm 8.6}$ & $\bm{62.2\pm 5.8}$ & $\bm{67.8\pm 3.3}$ & $\bm{63.4\pm 5.3}$ & $\bm{69.6\pm 2.8}$ & $\bm{66.7\pm 6.8}$ & $\bm{71.8\pm 4.5}$ & $\bm{62.5}$ & $\bm{0.37}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Citeseer}} & Greedy & $6.7\pm 2.7$ & $17.7\pm 7.2$ & $11.0\pm 5.3$ & $22.6\pm 8.3$ & $14.4\pm 5.1$ & $24.5\pm 7.8$ & $18.7\pm 6$ & $29.0\pm 7.8$ & $20.5\pm 7.4$ & $31.0\pm 8.1$ & $21.9\pm 6.6$ & $32.6\pm 8.3$ & 24.6 & 0.12 \\
\multicolumn{1}{|c|}{} & ASCV & $8\pm 3.4$ & $17.4\pm 7.5$ & $12.9\pm 5.7$ & $21.8\pm 7.4$ & $16.9\pm 4.8$ & $26.5\pm 4.7$ & $20.8\pm 5.2$ & $29.6\pm 6.6$ & $24.3\pm 5.3$ & $34.4\pm 5.2$ & $28.1\pm 5.2$ & $36.8\pm 6.4$ & 27.8 & 0.17 \\
\multicolumn{1}{|c|}{} & Greedy(P) & $15.3\pm 7.5$ & $24.5\pm 9.3$ & $21.5\pm 8.5$ & $30.7\pm 9.6$ & $23.9\pm 7.9$ & $33.2\pm 8$ & $27.4\pm 10$ & $36.5\pm 7.7$ & $29.8\pm 9.8$ & $39.0\pm 7.7$ & $32.9\pm 11.1$ & $42.4\pm 8.5$ & 34.4 & 0.17 \\
\multicolumn{1}{|c|}{} & ASCV(P) & $19.8\pm 7.1$ & $27.2\pm 9.5$ & $32.2\pm 5.9$ & $38.3\pm 9.2$ & $38.9\pm 4$ & $43.8\pm 6.3$ & $42.4\pm 7$ & $48.0\pm 7.6$ & $46.7\pm 2.7$ & $50.5\pm 4.5$ & $48.6\pm 4$ & $51.9\pm 6$ & 43.3 & 0.26 \\
\multicolumn{1}{|c|}{} & \textbf{SIMT} & $\bm{28.7\pm 5.8}$ & $\bm{39.4\pm 6.3}$ & $\bm{37.0\pm 5.6}$ & $\bm{47.4\pm 6.1}$ & $\bm{41.7\pm 6}$ & $\bm{50.2\pm 6.3}$ & $\bm{45.4\pm 5.2}$ & $\bm{53.4\pm 6.7}$ & $\bm{47.2\pm 5.1}$ & $\bm{55.8\pm 3.8}$ & $\bm{50.0\pm 4.9}$ & $\bm{57.9\pm 4.8}$ & $\bm{50.7}$ & $\bm{0.30}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Pubmed}} & Greedy & $15.7\pm 3.8$ & $30.8\pm 9.8$ & $16.3\pm 3.4$ & $30.8\pm 9.7$ & $19.2\pm 5.9$ & $34.0\pm 8.9$ & $20.8\pm 7$ & $35.0\pm 8.4$ & $22.6\pm 9.1$ & $37.5\pm 8.9$ & $21.2\pm 8.3$ & $35.8\pm 9.6$ & 34.0 & 0.14 \\
\multicolumn{1}{|c|}{} & ASCV & $17.2\pm 4.3$ & $33.7\pm 9$ & $17.4\pm 4.7$ & $33.9\pm 9.2$ & $17.0\pm 4$ & $33.8\pm 9.1$ & $18.1\pm 5.7$ & $34.2\pm 9.3$ & $16.8\pm 3.8$ & $33.8\pm 9$ & $18.1\pm 6.2$ & $34.5\pm 9.8$ & 34.0 & 0.16 \\
\multicolumn{1}{|c|}{} & Greedy(P) & $19.7\pm 7.7$ & $34.1\pm 9.4$ & $25.1\pm 12.1$ & $39.3\pm 10.5$ & $27.6\pm 13.3$ & $40.9\pm 11.4$ & $29.9\pm 14.9$ & $43.0\pm 13.3$ & $30.6\pm 15.4$ & $43.4\pm 13.6$ & $31.2\pm 14.4$ & $43.9\pm 12.6$ & 40.8 & 0.17 \\
\multicolumn{1}{|c|}{} & ASCV(P) & $20.0\pm 9.8$ & $35.8\pm 11.7$ & $21.4\pm 12.5$ & $36.3\pm 12.5$ & $22.1\pm 13.5$ & $36.9\pm 13.2$ & $23.3\pm 15.7$ & $37.8\pm 14.7$ & $23.4\pm 16.1$ & $37.7\pm 14.7$ & $23.9\pm 16$ & $37.9\pm 14.2$ & 37.0 & 0.17 \\
\multicolumn{1}{|c|}{} & \textbf{SIMT} & $\bm{22.7\pm 7.2}$ & $\bm{38.5\pm 7.6}$ & $\bm{29.9\pm 13.5}$ & $\bm{44.0\pm 10.6}$ & $\bm{36.7\pm 16.1}$ & $\bm{49.2\pm 11.9}$ & $\bm{42.0\pm 14.2}$ & $\bm{53.3\pm 10.5}$ & $\bm{43.1\pm 15.1}$ & $\bm{54.4\pm 11.6}$ & $\bm{50.2\pm 16.7}$ & $\bm{60.2\pm 11.1}$ & $\bm{49.9}$ & $\bm{0.25}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Amazon}} & Greedy & $6\pm 3.2$ & $15.5\pm 6$ & $8.7\pm 5.4$ & $18.2\pm 6.8$ & $9.7\pm 4.5$ & $17.8\pm 5.8$ & $10.7\pm 4.8$ & $18.5\pm 6.4$ & $11.3\pm 5.3$ & $19.6\pm 5.6$ & $13.8\pm 6.1$ & $21.3\pm 5.5$ & 18.5 & 0.08  \\   %231.4
\multicolumn{1}{|c|}{} & ASCV & $4.4\pm 1.5$ & $17.3\pm 6.6$ & $7.1\pm 3.6$ & $20.9\pm 5.3$ & $9.6\pm 5.2$ & $25.2\pm 3.4$ & $11.4\pm 7.4$ & $27.1\pm 5.6$ & $11.9\pm 7.2$ & $27.3\pm 5$ & $12.6\pm 7.2$ & $29\pm 6$ & 24.5 & 0.12  \\ % 197.3
\multicolumn{1}{|c|}{} & Greedy(P) & $19.6\pm 5.5$ & $24.9\pm 6.2$ & $23.2\pm 8$ & $29.3\pm 8.8$ & $23.4\pm 8.4$ & $29.6\pm 9.1$ & $24.8\pm 8.2$ & $31.2\pm 8.3$ & $24.9\pm 7.8$ & $31.7\pm 8.9$ & $25.4\pm 7.4$ & $32.6\pm 8.0$ & 30.0 &0.13  \\ 
\multicolumn{1}{|c|}{} & ASCV(P) & $18.2\pm 7.5$ & $31.1\pm 8.0$ & $21.7\pm 7.3$ & $34.8\pm 8.3$ & $23.9\pm 8.1$ & $36.3\pm 8.9$ & $26.7\pm 7.0$ & $41.9\pm 8.9$ & $29.7\pm 9.4$ & $43.4\pm 9.3$ & $30.2\pm 9.1$ & $44.7\pm 9.6$ & 38.7  & 0.20  \\
\multicolumn{1}{|c|}{} & \textbf{SIMT} & $\bm{30\pm 7.1}$ & $\bm{38.9\pm 7.9}$ & $\bm{38.1\pm 6.7}$ & $\bm{48.4\pm 6.6}$ & $\bm{40.7\pm 7.4}$ & $\bm{50.6\pm 6.7}$ & $\bm{44.2\pm 7.1}$ & $\bm{57.1\pm 5.9}$ & $\bm{46.2\pm 4.3}$ & $\bm{57.8\pm 3.6}$ & $\bm{51.7\pm 7.8}$ & $\bm{60.3\pm 6.5}$  & $\bm{52.2}$  & $\bm{0.28}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Coauthor}} & Greedy & $8.5\pm 5.6$ & $24.2\pm 19.2$ & $9.2\pm 5.9$ & $24.5\pm 19.4$ & $11.7\pm 6.5$ & $29.0\pm 19.8$ & $12.9\pm 7.5$ & $29.8\pm 20.2$ & $15.4\pm 9.0$ & $34.4\pm 20.2$ & $17.1\pm 9.0$ & $35.2\pm 20.1$ & 29.5  & 0.12  \\ %255.6
\multicolumn{1}{|c|}{} & ASCV & $10.3\pm 4.3$ & $34.0\pm 17.7$ & $11.1\pm 4.8$ & $37.1\pm 17.7$ & $11.8\pm 5.1$ & $37.4\pm 17.7$ & $12.9\pm 5.3$ & $38.7\pm 16.7$ & $16.1\pm 6.4$ & $44.7\pm 14.7$ & $16.3\pm 6.9$ & $45.4\pm 15.1$ & 39.5 & 0.18 \\  %226
\multicolumn{1}{|c|}{} & GreedyP & $18.0\pm 8.4$ & $34.2\pm 19.3$ & $24.7\pm 11.3$ & $39.6\pm 20.0$ & $27.5\pm 12.3$ & $42.0\pm 21.0$ & $27.5\pm 12.3$ & $42.1\pm 21.4$ & $29.4\pm 13.9$ & $44.4\pm 22.3$ & $30.0\pm 14.1$ & $44.1\pm 22.2$ & 41.1  & 0.16 \\
\multicolumn{1}{|c|}{} & ASCVP & $16.8\pm 9.1$ & $43.5\pm 18.6$ & $26.2\pm 9.2$ & $54.1\pm 13.9$ & $25.6\pm 12.2$ & $51.7\pm 18.9$ & $30.7\pm 11.7$ & $57.6\pm 15.3$ & $30.8\pm 11.7$ & $58.3\pm 15.4$ & $32.1\pm 10.7$ & $59.3\pm 15.1$ & 54.1 & 0.24  \\
\multicolumn{1}{|c|}{} & \textbf{SIMT} & $\bm{24.9\pm 8.9}$ & $\bm{55.5\pm 5.6}$ & $\bm{29.8\pm 9.8}$ & $\bm{57.9\pm 8.2}$ & $\bm{33.5\pm 8.4}$ & $\bm{61.5\pm 6.8}$ & $\bm{39.1\pm 7.6}$ & $\bm{63.4\pm 6.4}$ & $\bm{42.8\pm 12.0}$ & $\bm{65.2\pm 8.0}$ & $\bm{46.7\pm 11.6}$ & $\bm{68.8\pm 6.3}$ & $\bm{62.0}$ & $\bm{0.30}$  \\ \hline
\end{tabular}%
}
\end{table*}

{\it Data valuations.}
% \paragraph{Data valuations.} 
We generate a set of random numbers to represent the data valuations. The valuations  are sampled at random i.i.d. following a series of normal distributions $\mathcal{N}(\mu,\,\sigma^{2})$. 
We get a $\mu$ drawn from $\mathcal{U}[0.8,1.2]$ for each class to capture the difference in valuations between classes. Then for each data owner, we set the valuation of each data subject as the mean of the generated valuations of her data subjects.
We set $\sigma=0.1$. The effect of different $\sigma$s on performance is investigated and the results are in App.~\ref{app:deviation}.
To ensure all valuations are non-negative, we use a resample trick \cite{burkardt2014truncated}. The generated valuations are in the range $[0,2]$. Note that when the domain is different, we could scale it into $[0,2]$. 

%To avoid negative data valuations, we uniformly sample $\mu$ from $[8,12]$ for each class. The range of data valuations is guaranteed to be in $[0,20]$ by a resample trick \cite{burkardt2014truncated}. Note that when applying our methods in real world, the data valuations can be easily scaled into $[0,20]$ according to the domain of the data valuations.  We set $\sigma=1$ to mimic the standard normal distribution and compare the effect of $\sigma$ in Case study.

%\smallskip
{\it Budget.}
% \paragraph{Budget.} 
We set the budget in $\{50,100,150,200, 250, $ $300\}$. Given that the data valuation range is $[0,2]$,
% and the average valuation is $10$,
the number of selected data is approximately from $50$ to $300$, which is aligned with the setup of the studies on label selection e.g. \cite{zhang2021alg,cai2017active}.  


%Following the settings in existing work \cite{zhang2021alg,cai2017active} which consider using partial label to train GNN model, the total number of selected data is frequently be set as $200$.
%Accordingly, we set the budget of consumer to be from $500$ to $3000$. In other words, given the data valuations range $[0,20]$, we approximate the mean data valuation using $10$ and the number of selected data is approximately from $50$ to $300$.

% \smallskip
{\it Structural clustering.}
% \paragraph{Structural clustering.} 
Here, we give the configuration of the model used for $\mathsf{Clustering}(G)$.
We deploy SVD \cite{brunton2022data} to generate spectral node features, VGAE \cite{kipf2016variational} to learn node embeddings followed by a linear classifier to learn the partition. 
%Our framework employs a clustering method $\mathsf{Clustering}(G)$ to learn a partition. The clustering model solely use the graph structure as input. Specifically, the $\text{Clustering}(G)$ generates spectral node features via SVD \cite{brunton2022data}.  We utilize a classical Variational Graph Auto-Encoder (VGAE) model \cite{kipf2016variational} to learn node embeddings and a linear classifier to learn the partition. 
We set the hidden size as $32$, the learning rate as $0.01$, the L2 regularisation as $5\times 10^{-4}$. The total training budget is $400$ epochs.
The clustering model is initialised to solely minimise the reconstruction loss.
We repeat this process $100$ epochs to comprehensively capture the graph structure information. 
Using the obtained node embeddings, we train the linear classifier to learn a partition, maximising the structural entropy. This process is repeated $300$ epochs to obtain a robust partition. 


% \smallskip
{\it GNN model.}
% \paragraph{GNN model.} 
We employ classical GNN models as $f_{\text{GNN}}$ to learn node classification. 
%We employs same classification method to learn node classification for all data selection methods.
% In particular, we utilise a classical two-layer GCN model with hidden size $32$ \cite{kipf2016semi}.
%Similar to the classical settings, we set t
Following the configurations of \cite{kipf2016semi}, we set the hidden size as $32$, the total training budget as $200$, the learning rate as $0.01$ and the L2 regularisation as $5 \times 10^{-4}$.
The models are optimised with minimising both reconstruction loss $L_{\text{recon}}$ and classification loss $L_{\text{class}}$ on the train data.
%We evaluate the model performance using the test data. 
We repeat $10$ training iterations with different random seeds and report the average performance.
To mitigate the impact of randomness in train-validation splitting,  each training iteration creates $10$ train-validation splits, trains $10$ independent models according to the split and reports the best model according to their performance on the validation set. 
Ultimately, we evaluate the model performance using the test data.
In other words, each experimental result is derived from $100$ runs.
%To validate the robustness of SIMT in GNN models, 
% We test on multiple GNN models and present the results using a GCN model. The comparison result is deferred in App.~\ref{app:models}. 
We present the results using a GCN model and defer the exploration on the effects of different GNN architectures in App.~\ref{app:models}. 
% Additionally, during each training iteration, we create 10 distinct train-validation splits. For each split, we train an independent model, resulting in 10 separate models. The performance of these models is assessed based on their validation set results. Consequently, each experimental outcome represents the average of 100 runs. Ultimately, we evaluate the performance of the selected model using the test data.
%Furthermore, in each training iteration, we generate $10$ train-validation splits, train $10$ independent models according to the split, and 
%To further eliminate the effect of randomness, every training iteration generates $10$ train-validation splits and trains $10$ independent models according to the split. In each iteration, we  
% report the best model according to their performance on the validation set. 
% In other words, each experimental result is the average of $100$ runs.
% Finally, the model performance is evaluated on the test data. 
% \smallskip

% \paragraph{Subgraph.}
{\it Subgraph.}
Each data owner possesses a subgraph with at least one node. 
We vary both the number $o$ of data owners and the size $n_i$ of theire subgraphs. We first fix the number of data owners at $10$, and vary the subgraph size within $\{20,40,60,80\}$ to investigate the effect of subgraph size.
Next, we fix the subgraph size at $80$, and vary the number of data owners within $\{5,10,15,20\}$ to investigate the effect of the number of data owners.  
The comparison results are presented in App.~\ref{app:subgraph parameter}. 

% For the specific scenario where each data owner possesses exact one data subject, we 


% We examine two scenarios characterised by different subgraph sizes: (1) $n_i\geq 1$ for all $i\in O$, and (2) $n_i = 1$ for all $i\in O$.
% In the first scenario, each data owner possesses at least one data subjects, while in the second,  each data owner possesses only one data subjects.  %{\teal We fix the budget as $1500$ for both scenarios. } 
% Also, in the first scenario, we vary the number of data owners and the number of data subjects for those with $n_i > 1$. We first fix the number of data owners as $10$, and vary the number of data subjects held by each data owner within $\{20,40,60,80\}$ to compare the effect due to the number of data subjects of each data owner.
% Next, we fix the number of data subjects held by each data owner as $80$, and vary the number of data owners within $\{5,10,15,20\}$ to compare the effect due to the number of data owners.  
% The comparison results are in shown in App.~\ref{app:subgraph parameter}. 

{\it Baselines.}
% \paragraph{Baselines.}
To validate the overall performance of the SIMT, we benchmark it against four baseline mechanisms. These baselines incorporate different methods for assessing data importance within our proposed model trading framework. The baselines are: 
%
%In this section, we conduct experiments to compare the effect of different data procurement methods. To avoid data disclosing, all data procurement methods are forced to measure data value without inspecting the raw data. In particular, we incorporate the data procurement methods in our model trading mechanism. For all data procurement methods, we train a same GCN model on the procured data and use the model performance for evaluation.
%
%\noindent {\bf Baselines.} %We compare the following data procurement methods.
\begin{itemize}[leftmargin=*]
    \item Greedy~\cite{singer2010budget}: The Greedy mechanism treats all data as equally important and procures data based solely on the valuations of data owners. 
    No feature propagation is applied. % to the procured data. 
    % This method does not apply feature propagation to the procured data prior to initiating model training.
    %The Greedy approach treats all data equally important and procures data solely according to the data owners' valuations. No feature propagation is applied to the procured data before model training. 
    %Greedily select data with lowest data valuations.
    \item ASCV~\cite{chandra2021initial}: The ASCV mechanism first trains a VGAE model on the graph to learn node embeddings with optimising reconstruction loss, and evaluates data importance by the nodes' contribution to the reconstruction loss. %The larger contribution the node is, the more important the corresponding owner's data is.
    The greater the contribution of the node, the more important the corresponding owner's data is.
    %To guarantee incentive compatible, individual rational and budget feasible, we adjust this method to greedily select data with largest data importance per data valuation.
    Then the auction procures data according to the ratio of data importance to valuation. 
    No feature propagation is applied. 
    \item Greedy(P): The Greedy(P) mechanism is the same as the Greedy except for that a feature imputation is applied. 
    %The greedy method with feature imputation.
    \item ASCV(P): The ASCP(P) mechanism is the same as the ASCV except for that a feature imputation is applied.
    %The ASCV method with feature imputation.
    %\item Ours: the proposed method.
\end{itemize}
% Note that due to the absence of data, training models with few data has been intensively investigated \cite{settles2009active,bilgic2010active}. A typical framework is called Active Learning (AL) where the learners (such as models) are actively involved in the learning process based on the learners' feedback. 
% To initialise a meaningful learner, they often take all node features as input \cite{zhang2021alg,cai2017active}.
% We highlight that this framework cannot solve the data disclosing problem as features are deeply nested in their methods.
% However, this framework does not address the data disclosing problem, as private features are deeply embedded in their methods and an initial data set is usually required without payment for initialising a meaningful learner \cite{zhang2021alg,cai2017active}.
Note that ASCV is originally designed using various techniques to evaluate data importance. However, in the absence of features, only the VGAE technique can be directly applied in our scenario. For fair comparison, 
we redesign all baselines to avoid pre-purchase inspection of data, and set same seeds for all places involving randomness, including edge augmentation and model initialisation.


{\it Implementation.}
% \paragraph{Implementation.}
All experiments are conducted on a single machine with AMD Ryzen $9$ $5900$X $12$-Core CPU, and NVIDIA GeForce RTX $3090$ GPU. The code is implemented using Python $3.10$ and Pytorch $2.0.1$. Our code is available in the supplementary material. %and CUDA $12.2$.

% \vspace{-0.2cm}
\subsection{Overall performance} 
% To validate the overall performance of SIMT, we compare it with four baselines. These baselines incorporate different data importance assessment methods into our model trading framework. All of them ensures no data disclosure for fair comparison. Specifically, the baselines are: 

%Note that our data trading mechanism with all data procurement methods above guarantees incentive compatible, individual rational and budget feasible.

% \todo[inline]{explain why select feature propagation and why select ASCV as baseline. M: I think we should put the reason why to select ASCV as baseline into experiment. }

% \paragraph{{\color{red}Training GNN with partial data.}} %Data serves as the vital force that powers the capabilities of machine learning models. 
% Due to the absence of data, training models with few data has been intensively investigated \cite{settles2009active,bilgic2010active}. A typical framework is called Active Learning (AL) where the learners (such as models) are actively involved in the learning process. In particular, AL in GNN iteratively queries the label of data according to the feedback from the learners. To initialise a meaningful learner, they often take all node features as input \cite{zhang2021alg,cai2017active}.
% We highlight that this framework cannot solve the data disclosing problem as features are deeply nested in their methods.
% Our work is also related to feature imputation where missing features are completed according to known features \cite{spinelli2020missing,taguchi2021graph}. Among numerous feature imputation methods, we choose the Feature Propagation algorithm as this algorithm has strong convergence guarantees and is simple, fast and scalable \cite{rossi2022unreasonable}.    %We highlight that feature imputation is not our main contribution.


The experiment results are presented in Table~\ref{tab:main-performance}. Here, we fix the number of data owners at $10$, with each owner holding $80$ data subjects. 
As shown in the table, SIMT consistently outperforms all baselines under all budgets. Compared to the vanilla Greedy and ASCV, SIMT improves up to $40\%$ in both MacroF1 and MicroF1. Also, the last column shows the contribution per node, i.e., calculated as the average accuracy divided by the number of purchased nodes. The results consistently show that the contribution per node of SIMT is higher than that of all baselines, 
demonstrating the data selected by SIMT is more valuable. This validates the effectiveness of our structural importance assessment method in Sec.~\ref{sec:structural-importance}.

% the reason why greedy and ASCV perform worse and the reason of using clustering
Table~\ref{tab:main-performance} also shows that the ASCV/ASCV(P) mechanism outperforms the Greedy/Greedy(P) mechanism. 
%This might be explained by that the ASCV and the ASCV(P) measure the importance of data by their structural contribution to the reconstruction loss which reflects  structural uncertainty in some sense. 
This could be attributed to that both ASCV and ASCV(P) assess the importance of data based on their structural contribution to the reconstruction loss, which, in a way, reflects structural uncertainty.
%However, ASCV and ASCV(P) underperform SIMT. That is because the reconstruction loss is designed to capture the structural information of the entire graph and fails to capture the structural information of individual nodes under a classification task. It verifies the need of structure clustering and cluster-based measures in SIMT.
However, ASCV and ASCV(P) do not perform as well as SIMT. 
% This underperformance 
% can be attributed to that the reconstruction loss is intended to capture the structural information of the entire graph, but it falls short in capturing the structural importance of individual nodes within a classification task. 
This discrepancy underscores the effectiveness of structural importance score.
% the necessity of structure clustering and cluster-based measures in SIMT.

%This might be explained by that ASCV(P) method distinguish structural uncertain data by calculating their structural contribution to the reconstruction loss. However, the reconstruction loss is designed for capturing the graph structure information and thus cannot properly measure nodes' structural informativeness under a node classification setting. Instead, we propose the structural informativeness based on structural clustering which is more consistent with the node classification setting. This may explain why our data procurement method outperforms the ASCV(P) method.
%
% the effect of feature propagation
Lastly, Table~\ref{tab:main-performance} shows that the ASCV(P) and Greedy(P) outperform their vanilla versions by up to $20\%$ 
in both MacroF1 and MicroF1. This validate the need of feature imputation. %We thus adopt the feature propagation 

Same trend is observed in the  scenario with $n_i=1, \forall i\in O$. See more details in Table~\ref{apptab:GCN} of App.~\ref{app:models}.

%In the scenario with $n_i=1$, the results show similar trends: SIMT performs the best under all budgets, followed by ASCV/ASCV(P), and Greedy/Greedy(P) performs the worst. It is worth to notice that SIMT performs even better when $n_i=1$ than when $n_i\geq 1$. In particular, SIMT improves $20\% \sim 40\%$ in both MacroF1 and MicroF1 in this case; See more details in Table~\ref{apptab:GCN} of App.~\ref{app:models}. This can be attributed to that when $n_i=1$, fewer edges are missing within subgraphs, allowing the structural importance scores calculated by our mechanism to more accurately reflect the true structural importance of each node.

%\vspace{-0.2cm}
% \balance

\subsection{Ablation study} \label{Sec:ablation}
%To investigate the effect of each component in SIMT on performance, we conduct ablation studies on the three datasets and report the mean test accuracy. % under varying budget constraints. 
To explore the impact of each component in SIMT on its performance, we conduct ablation studies across the five datasets and present the average test accuracy.
%As shown in Tab.~\ref{tab:ablation-study}, {\bf (1)} the accuracy of SIMT without clustering diminishes by $8.3\%$ in Cora, $3.7\%$ in Citeseer, and $13.7\%$ in Pubmed, which highlights the importance of structural clustering. Without clustering, it is very likely that the procured data are unevenly distributed across the classes, resulting to a biased train dataset. 
As shown in Table~\ref{tab:ablation-study}, the four components, i.e., structuring clustering (clust), structural informativeness (info), structural representativeness (rep), and edge augmentation (edge aug), distinctly enhances SIMT's performance. In general, clust plays contributes the most among all components, which underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset. The second contributor is edge augmentation, which highlights the role of missing edge augmentation in the training process. Without edge augmentation, the message passing process is likely hindered, resulting in sub-optimal performance.

% As shown in Table~\ref{tab:ablation-study}, {\bf (1)} Without clustering, the accuracy diminishes by {\red $6.6\%$ in Cora, $2.7\%$ in Citeseer, and $8.1\%$ in Pubmed.} This significant reduction underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset.
%Without clustering, the selected data could be unbalanced distributed in several true classes. Training on the unbalanced data, the GNN model would bias against unselected classes.
%{\bf (2)} The accuracy of SIMT without structural informativeness decreases by $1.8\%$ in Cora, $2.1\%$ in Citeseer, and $2.9\%$ in Pubmed. {\bf (3)} The accuracy of SIMT without structural representativeness decreases by $1.8\%$ in Cora, $1.5\%$ in Citeseer, and $2.2\%$ in Pubmed. In summary, each component positively contributes to the good performance of SIMT. 
% {\bf (2)}Without structural informativeness, the accuracy drops by {\red $1.5\%$ in Cora, $0.1\%$ in Citeseer, and $1.6\%$ in Pubmed.} {\bf (3)} Without structural representativeness, the accuracy reduction is {\red $0.2\%$ in Cora, $1.2\%$ in Citeseer, and $2.1\%$ in Pubmed.} {\bf (4)} Without edge augmentation, the accuracy reduction is {\red $3.3\%$ in Cora, $4.5\%$ in Citeseer, and $1.2\%$ in Pubmed}. This finding highlights the role of missing edge augmentation in training process. Without edge augmentation, the message passing process is likely hindered, resulting in sub-optimal performance.
% Overall, each component distinctly enhances SIMT's performance.
%In general, structural informativeness outperforms the structural representativeness which highlighting the role of structural uncertainty. 
%By incorporating these components, our method explores structural uncertainty with the aid of structural representativeness and thus consistently outperforms baselines.

We also compare the effect of data valuations deviation (in App.~\ref{app:deviation}), graph centrality metrics (in App.~\ref{app: centrality}), subgraph parameters (in App.~\ref{app:subgraph parameter}), and GNN architectures (in App.~\ref{app:models}).
% Additionally, we provide an ablation study for $n_i=1$ in Appendix~\ref{appsec:ablation}.

% \begin{table}[ht]
% \centering
% \caption{The impact of each component ($n_i= 1$)}
% \label{apptab:ablation-study}
% %\resizebox{0.8\linewidth}{!}{%
% \scriptsize
% \begin{tabular}{|c|cc|cc|cc|}
% \hline
% %\cline{2-7}
%  dataset & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{Pubmed} & \multicolumn{2}{c|}{Citeseer} \\ \hline
%  metric & acc. & $\Delta$ & acc. & $\Delta$ & acc. & $\Delta$ \\ \hline
% no cluster & 60.0 & -8.3 & 53.3 & -3.7 & 38.8 & -13.7 \\
% no rep & 66.5 & -1.8 & 55.5 & -1.5 & 50.3 & -2.2 \\
% no info & 66.5 & -1.8 & 54.9 & -2.1 & 49.6 & -2.9 \\ \hline
% {\bf SIMT} & $\bm{68.3}$ &  & $\bm{57.0}$ &  & $\bm{52.5}$ &  \\ \hline
% \end{tabular}%
% %}
% \end{table}


\begin{table}[]
\centering
\scriptsize
\caption{The impact of each component}
\label{tab:ablation-study}
\begin{tabular}{|c|cc|cc|cc|cc|cc|l}
\cline{1-11}
dataset       & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{Citeseer} & \multicolumn{2}{c|}{Pubmed} & \multicolumn{2}{c|}{Amazon} & \multicolumn{2}{c|}{Coauthor} & \multicolumn{1}{c}{} \\ \cline{1-11}
metric        & acc.          & $\Delta$  & acc.            & $\Delta$    & acc.           & $\Delta$   &  acc.           & $\Delta$   & acc.            & $\Delta$    &                      \\ \cline{1-11}
no cluster    & 55.9          & -6.6      & 48.0            & -2.7        & 41.8           & -8.1       &  35.4          & -16.8       & 45.5            & -16.5        &                      \\
no rep        & 61.0          & -1.5      & 50.6            & -0.1        & 48.3           & -1.6       &  50.5          & -1.7       &  56.9           & -5.1        &                      \\
no info       & 62.3          & -0.2      & 49.5            & -1.2        & 47.8           & -2.1       &  50.9        & -1.3       &  55.1           & -6.9        &                      \\
no edge aug   & 59.2          & -3.3      & 46.2            & -4.5        & 48.7           & -1.2       &  41.1          & -11.1       &  58.3           & -3.7        &                      \\ \cline{1-11}
\textbf{SIMT} & $\bm{62.5}$   &           & $\bm{50.7}$     &             & $\bm{49.9}$    &            & $\bm{52.2}$    &            & $\bm{62.0}$     &             &                      \\ \cline{1-11}
\end{tabular}
\end{table}

%\vspace{-0.4cm}






% \section{Extension on Partially Known Graphs}
% In this section, we delve into extending the SIMT mechanism to partially-known graphs. More precisely, consider a graph $G=(V,E)$, where a partially-known graph is denoted as $G'=(V,E')$ with $E'\subseteq E$. The edges in $E\setminus E'$ are owned by companies, along with their associated features and labels, are incorporated into the auction. To be specific, a company owns a subgraph $G_S=(S,E\cap S^2)$, $X[S]$ and $Y[S]$ where $S\subseteq V$ and $E\cap S^2$ denotes the set of edges induced by $S$, $X[S]\subseteq X$ denotes the set of features induced by $S$, $Y[S]\subseteq Y$ denotes the set of labels induced by $S$.
% To participate the auction, the private data ($E\cap S^2$,$X[S]$ and $Y[S]$) is not accessible by the data broker.
% All data items in a same company share a common bid $b_H$.
% The data broker selects a subset $D\subseteq S$ and pays $|D|b_H$ to the company.
% Upon payment, the data broker acquires $E\cap D\times S$, $X[D]$ and $Y[D]$. 

% A key characteristic of partially-known graphs lies in the missing edges.

%\vspace{-0.4cm}

\section{Conclusion}
In this paper, we aim to design a mechanism that properly incentives data owners to contribute their data, and returns a well performing GNN model to the model consumer for model marketplaces. In particular, we focus on the question of how we can measure data importance for model training without direct inspection. We propose SIMT, which consists of a data procurement phase and a model training phase. For data procurement, we incorporate a structure-based importance assessment method into an auction mechanism. For model training, we introduce and design two effective methods to impute missing data. As a result, SIMT ensures no data disclosure and incentive properties. Experimental results demonstrate that SIMT outperforms the baselines by up to $40\%$ in accuracy. %by $20\% \sim 40\%$ in both MacroF1 and MicroF1. %In addition, the results of ablation study verifies the effectiveness of each component in SIMT.
To the best of our knowledge, SIMT is the first model trading mechanism addressing the data disclosure problem. In the future, we will further consider the potential privacy leakage in the trained model. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.


\begin{acks}
This work is supported by National Natural Science Foundation of China $\#62172077$ and China Scholarship Council Grant $\#201906030067$. %and Research Fund for International Senior Scientists No. $62350710215$.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\newpage

\balance

\bibliographystyle{ACM-Reference-Format} 
\bibliography{sample}
% \end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
% \newpage

\appendix


\noindent {\bf \Large APPENDIX}

\section{Normalised marginal structural entropy} \label{app:calculation}
\noindent The calculation of the normalised marginal structural entropy is as follows. The marginal structural entropy is: 
{\footnotesize
\begin{align*} 
    &\mathcal{H}_P(G) - \mathcal{H}_{P'}(G)  \\
    % =& \frac{d_t-g_t}{2|E|}\log\frac{d_t}{2|E|} \\
    % &-\frac{(d_t-d_v)-(g_t-(d_v-x_{v,t})+x_{v,t})}{2|E|}\log\frac{d_t-d_v}{2|E|} \\
    =& \frac{d_t-g_t}{2|E|}\log\frac{d_t}{2|E|}-\frac{d_t - g_t - 2x_{v,t}}{2|E|}\log \frac{d_t-d_v}{2|E|} \\
    =& \frac{d_t - g_t}{ 2|E|} \left(\log \frac{d_t}{2|E|} -  \log \frac{d_t-d_v}{2|E|}\right) + \frac{2x_{v,t}}{2|E|} \log \frac{d_t-d_v}{2|E|} \\
    =& \frac{d_t - g_t}{ 2|E|} \log \frac{d_t}{d_t-d_v} + \frac{2x_{v,t}}{2|E|} \log \frac{d_t-d_v}{2|E|}.
\end{align*}
}
\noindent We normalise this value and get 
\begin{align*}
    \epsilon_v & = (\mathcal{H}_P(G) - \mathcal{H}_{P'}(G) )/ \mathcal{H}_P(G) \\
    & = \frac{\frac{d_t - g_t}{ 2|E|} \log \frac{d_t}{d_t-d_v} + \frac{2x_{v,t}}{2|E|} \log \frac{d_t-d_v}{2|E|} }{ \frac{d_t-g_t}{2|E|}\log\frac{d_t}{2|E|} } \\
    & = \frac{(d_t-g_t)\log\frac{d_t}{d_t-d_v}+2x_{v,t}\log\frac{d_t-d_v}{2|E|}}{(d_t-g_t)\log\frac{d_t}{2|E|}}.
\end{align*}
\section{Dataset statistics} \label{app: dataset}
\begin{table}[h]
\centering
\caption{Dataset statistics}
\label{tab:dataset_statistics}
\footnotesize
\begin{tabular}{|c|cccc|}
\hline
Dataset & $\#$ nodes & $\#$ edges & $\#$ features & $\#$ classes \\ \hline
Cora & 2,708 & 10,556 & 1,433 & 7 \\ \hline
Citeseer & 3,327 & 9,104 & 3,703 & 6 \\ \hline
Pubmed &19,717 & 88,648 & 500 & 3 \\ \hline
Amazon &  7,650 & 238,162 & 745 & 8 \\ \hline
Coauthor &  34,493 & 495,924 & 8,415 & 5 \\ \hline
\end{tabular}%
\end{table}


% \section{Experiment}
\section{Comparison of data valuations deviation} \label{app:deviation}
% \todo[inline]{Add description and analysis of the experiment.}
We conduct an experiment to evaluate the effect of the standard deviation in data valuation distribution on scenario with $n_i=1, i\in O$. 
The experiment is evaluated on Cora, Citeseer and Pubmed datasets with standard deviation $\sigma$ ranging from $0.01$ to $0.2$.
In Table~\ref{tab:deviation}, we list the MacroF1 and MicroF1 performance of our mechanism running on varying standard deviations of data valuations. We also list the number of bought data to compare the effect of $\sigma$ on data quantity.
We observe that the number of bought data increases as $\sigma$ increases. The increasing data quantity brings in more information for training GNN model and thus improves the model performance. 
To balance the effect of data quantity, we fix $\sigma=0.1$ for all the other experiments. To eliminate the randomness induced by unknown subgraphs, we conduct experiments on the case where all data owners are individuals.




\begin{table}[h]
\centering
\caption{The impact of different standard deviations for $n_i=1$}
\label{tab:deviation}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|ccc|ccc|ccc|}
\cline{2-10}
 & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{Citeseer} & \multicolumn{3}{c|}{Pubmed} \\ \hline
\multicolumn{1}{|c|}{$\sigma$} & {\small MacroF1} & {\small MicroF1} & $\#$data & {\small MacroF1} & {\small MicroF1} & $\#$data & { \small MacroF1} & {\small MicroF1} & $\#$data \\ \hline
\multicolumn{1}{|c|}{0.01} & 55.2 & 64.4 & 171.1 & 46.8 & 53.6 & 161.2 & 35.3 & 47.4 & 161.2 \\
\multicolumn{1}{|c|}{0.05} & 55.8 & 65.2 & 173.3 & 49.0 & 56.0 & 162.7 & 37.7 & 48.9 & 182.0 \\
\multicolumn{1}{|c|}{0.1} & 59.2 & 68.3 & 179.4 & 50.4 & 57.0 & 168.2 & 41.8 & 52.5 & 196.7 \\
\multicolumn{1}{|c|}{0.15} & 60.4 & 69.4 & 189.0 & 52.0 & 58.7 & 175.5 & 46.2 & 56.1 & 220.2 \\
\multicolumn{1}{|c|}{0.2} & 62.6 & 70.9 & 201.7 & 54.3 & 61.0 & 185.2 & 52.4 & 60.9 & 254.1 \\ \hline
\end{tabular}%
}
\end{table}




\section{Comparison of graph centrality metrics} \label{app: centrality}
We evaluate the effectiveness of various graph centrality metrics on Cora dataset. To eliminate the randomness induced by subgraphs, we conduct experiments on the scenario with $n_i=1$ for all $i\in O$.  
%where all data owners are individuals, that is, a fully-known graph.
Specifically, we substitute the PageRank centrality used in our structural representativeness with three other widely used centrality metrics: degree centrality, closeness centrality, and betweenness centrality \cite{latora2017complex}. We report the test accuracy with varying budgets. 
% The test accuracy for each metric is reported under different budgetary conditions. 
As shown in Table~\ref{tab:centrality}, PageRank centrality consistently surpasses the other metrics. Consequently, our mechanism employs PageRank centrality as structural representativeness.
\begin{table}[ht!]
\centering
\caption{Comparison of graph centrality metrics ($n_i=1$)} 
\label{tab:centrality}
%\resizebox{0.8\linewidth}{!}{%
\footnotesize
\begin{tabular}{|c|cccccc|}
\hline
budget & $50$ & $100$ & $150$ & $200$ & $250$ & $300$ \\ \hline
Degree & 51.8 & 60.3 & 69.2 & 69.2 & 74.2 & 75.0 \\
Closeness & 52.5 & 62.0 & 65.5 & 70.1 & 73.6 & 74.8 \\
Betweenness & 49.2 & 60.9 & 67.0 & 68.5 & 71.7 & 75.6 \\
PageRank & $\bm{52.7}$ & $\bm{64.6}$ & $\bm{69.3}$ & $\bm{71.0}$ & $\bm{75.1}$ & $\bm{77.0}$ \\ \hline
\end{tabular}%
%}
\end{table}

\section{Comparison of subgraph parameters} \label{app:subgraph parameter}
Here we explore the effect of the subgraph size and the number of data owners.  %we first fix the budget as $1500$, the number of data owners as 10, and vary the number of data subjects held by each data owner within to compare the effect due to the number of data subjects.
To reduce the effect of randomness, we use $n_i = 1$ as a specific scenario to measure the effect of other parameters.
The experimental results on the effects due to subgraph size and number of subgraphs are shown in Tables~\ref{tab:subgraph size} and \ref{tab:subgraph number}.
When fixing the budget as $1500$ and the number of data owners as $10$, we find that the larger the subgraph size is, the more edges within the subgraph are missing, which hinders the message passing process and results in worse model performance, as shown in Table~\ref{tab:subgraph size}. 
Then when fixing the budget at $1500$ and the subgraph size at $80$, Table~\ref{tab:subgraph number} shows that the more subgraphs exist, the more edges within subgraphs are missing, resulting in worse model performance.

These experiments underscore the negative effect of missing edges during model training. Consequently, we utilise an edge augmentation technique to alleviate the impact of these missing edges.

\begin{table}[ht]
\centering
\footnotesize
\caption{The  impact of subgraph size ($n_i\geq 1$)}
\label{tab:subgraph size}
% \resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
size & Cora & Citeseer & Pubmed \\ \hline
20 & $63.1\pm 5.6$ & $55.4\pm 7.5$ & $52.1\pm 15.2$ \\ \hline
40 & $63.4\pm 4.2$ & $56.6\pm 3.6$ & $50.3\pm 11.7$ \\ \hline
80 & $62.5\pm 8.6$ & $50.2\pm 6.3$ & $49.2\pm 11.9$ \\ \hline
160 & $57.3\pm 4.9$ & $46.5\pm 7.0$ & $47.6\pm 8.5$ \\ \hline
\end{tabular}%
% }
\end{table}

\begin{table}[ht]
\centering
\footnotesize
\caption{The  impact of subgraph numbers ($n_i\geq 1$)}
\label{tab:subgraph number}
% \resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
\#subgraphs & Cora & Citeseer & Pubmed \\ \hline
5 & $65.5\pm 4.3$ & $53.2\pm 7.6$ & $49.7\pm 12.3$ \\ \hline
10 & $62.5\pm 8.6$ & $50.2\pm 6.3$ & $49.2\pm 11.9$ \\ \hline
15 & $62\pm 4.7$ & $50.2\pm 3.8$ & $48.6\pm 14.6$ \\ \hline
20 & $57.3\pm 5.0$ & $47.9\pm 5.0$ & $47.2\pm 11.4$ \\ \hline
\end{tabular}%
% }
\end{table}






\begin{table}[]
\centering
\footnotesize
\caption{The number of bought data under different budgets for $n_i=1$}
\label{apptab:quantity}
% \resizebox{0.9\linewidth}{!}{%
% \tiny
\begin{tabular}{|c|c|cccccc|}
\hline
 \multicolumn{2}{|c|}{budget} & {$50$} & {$100$} & {$150$} & {$200$} & {$250$} & $300$ \\ \hline
\multirow{3}{*}{Cora} & Greedy(P) & 67.8 & 129.5 & 188.2 & 245 & 300.3 & 353.8 \\
 & ASCV(P) & 60.7 & 114 & 164.3 & 212.3 & 257.8 & 302 \\
 & {\bf SIMT} & $\bm {55.3}$ & $\bm {107.4}$ & $\bm {157}$ & $\bm {205.9}$ & $\bm {252.3}$ & $\bm {298.3}$ \\ \hline
\multirow{3}{*}{Citeseer} & Greedy(P) & 65.3 & 123.8 & 179.9 & 233.8 & 286.3 & 337.2 \\
 & ASCV(P) & 57.4 & 107 & 154.2 & 198.5 & 241.9 & 282.2 \\
 & {\bf SIMT} & $\bm {51.8}$ & $\bm {100.6}$ & $\bm {147.8}$ & $\bm {193.2}$ & $\bm {236.2}$ & $\bm {279.8}$ \\ \hline
\multirow{3}{*}{Pubmed} & Greedy(P) & 76.5 & 147.1 & 215.4 & 282 & 347.4 & 412.1 \\
 & ASCV(P) & 70.1 & 134.3 & 196 & 255.6 & 314.5 & 370.7 \\
 & {\bf SIMT} & $\bm {60.4}$ & $\bm {116.6}$ & $\bm {171.8}$ & $\bm {225}$ & $\bm {277.7}$ & $\bm {328.5}$ \\ \hline
\end{tabular}%
% }
\end{table}


\begin{table}[ht]
\centering
\caption{The impact of each component ($n_i= 1$)}
\label{apptab:ablationstudy}
%\resizebox{0.8\linewidth}{!}{%
\footnotesize
\begin{tabular}{|c|cc|cc|cc|}
\hline
%\cline{2-7}
 dataset & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{Pubmed} & \multicolumn{2}{c|}{Citeseer} \\ \hline
 metric & acc. & $\Delta$ & acc. & $\Delta$ & acc. & $\Delta$ \\ \hline
no cluster & 60.0 & -8.3 & 53.3 & -3.7 & 38.8 & -13.7 \\
no rep & 66.5 & -1.8 & 55.5 & -1.5 & 50.3 & -2.2 \\
no info & 66.5 & -1.8 & 54.9 & -2.1 & 49.6 & -2.9 \\ \hline
{\bf SIMT} & $\bm{68.3}$ &  & $\bm{57.0}$ &  & $\bm{52.5}$ &  \\ \hline
\end{tabular}%
%}
\end{table}

\begin{table*}[t]
\centering
\caption{Node classification performance under different budgets using GCN architecture for $n_i=1$}
\label{apptab:GCN}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|}
\hline
\multicolumn{2}{|c|}{budget} & \multicolumn{2}{c|}{$50$ } & \multicolumn{2}{c|}{$100$} & \multicolumn{2}{c|}{$150$} & \multicolumn{2}{c|}{$200$} & \multicolumn{2}{c|}{$250$} & \multicolumn{2}{c|}{$300$} \\ \hline
\multicolumn{2}{|c|}{metric} & \multicolumn{1}{c}{MacroF1} & \multicolumn{1}{c|}{MicroF1} & \multicolumn{1}{c}{MacroF1} & \multicolumn{1}{c|}{MicroF1} & \multicolumn{1}{c}{MacroF1} & \multicolumn{1}{c|}{MicroF1} & \multicolumn{1}{c}{MacroF1} & \multicolumn{1}{c|}{MicroF1} & \multicolumn{1}{c}{MacroF1} & \multicolumn{1}{c|}{MicroF1} & \multicolumn{1}{c}{MacroF1} & MicroF1 \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Cora}} & Greedy & $11.8\pm 5.5$ & $22.6\pm 8$ & $18.4\pm 7$ & $28.8\pm 9$ & $21.7\pm 6.6$ & $32.7\pm 9.4$ & $25.2\pm 5.6$ & $36.2\pm 6.3$ & $28.5\pm 8.8$ & $40.2\pm 8.5$ & $30.5\pm 9.4$ & $42.5\pm 9.4$ \\
\multicolumn{1}{|c|}{} & ASCV & $11.8\pm 5.4$ & $22.0\pm 7.8$ & $19.9\pm 5.5$ & $32.2\pm 9.2$ & $28.0\pm 6.5$ & $39.6\pm 7.7$ & $32.0\pm 4.6$ & $43.1\pm 4.3$ & $31.5\pm 3.5$ & $43.2\pm 3.8$ & $36.7\pm 5.4$ & $48.1\pm 4.3$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $26.1\pm 13.3$ & $38.2\pm 13.7$ & $31.3\pm 13$ & $42.3\pm 13.3$ & $34.1\pm 14.6$ & $44.4\pm 13.6$ & $36.8\pm 17.3$ & $47.7\pm 14$ & $38.6\pm 17.5$ & $49.1\pm 14.1$ & $40.1\pm 17$ & $51.0\pm 13.4$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $31.9\pm 8.3$ & $43.8\pm 11.1$ & $41.6\pm 8$ & $54.3\pm 8.5$ & $48.6\pm 7.7$ & $59.0\pm 7.9$ & $52.2\pm 10.7$ & $63.0\pm 8.8$ & $57.8\pm 9.6$ & $67\pm 7.6$ & $61\pm 12.6$ & $69.5\pm 8.2$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{39.6\pm 9.9}$ & $\bm{52.7\pm 8.3}$ & $\bm{53.4\pm 7.5}$ & $\bm{64.6\pm 7.7}$ & $\bm{60.0\pm 7.7}$ & $\bm{69.3\pm 6.3}$ & $\bm{63.4\pm 8.8}$ & $\bm{71.0\pm 6.6}$ & $\bm{67.1\pm 7.9}$ & $\bm{75.1\pm 4.3}$ & $\bm{71.9\pm 6.3}$ & $\bm{77.0\pm 4}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Citeseer}} & Greedy & $8.2\pm 4.3$ & $18.6\pm 8.9$ & $11.3\pm 4.9$ & $22.7\pm 8.7$ & $13.5\pm 4.2$ & $24.1\pm 6.8$ & $17.1\pm 5.9$ & $28.2\pm 8$ & $18.4\pm 6.4$ & $28.7\pm 8.2$ & $22.0\pm 7.2$ & $32.2\pm 7.1$ \\
\multicolumn{1}{|c|}{} & ASCV & $10.2\pm 4.4$ & $20.1\pm 5$ & $15\pm 3.8$ & $24\pm 6.2$ & $18.9\pm 3.1$ & $27.2\pm 4.5$ & $24.3\pm 5.6$ & $32.4\pm 7.9$ & $31.2\pm 5.3$ & $38\pm 6.7$ & $36.3\pm 4.5$ & $45.0\pm 4$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $17\pm 10$ & $27.6\pm 12.5$ & $22.1\pm 10.5$ & $32.6\pm 11.9$ & $25.3\pm 12.5$ & $35.2\pm 12$ & $31.3\pm 14.1$ & $40.4\pm 12.9$ & $33.2\pm 13.5$ & $42.8\pm 12.7$ & $37.6\pm 14.8$ & $47.2\pm 13.5$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $22.8\pm 9.5$ & $31.1\pm 13$ & $34.2\pm 9.8$ & $41.8\pm 12.9$ & $40.7\pm 7.3$ & $48.8\pm 8.7$ & $49.3\pm 7.4$ & $57.7\pm 6.2$ & $53.2\pm 6.9$ & $59.2\pm 5.6$ & $56.7\pm 6.1$ & $63.1\pm 3.9$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{33.3\pm 8.6}$ & $\bm{40.3\pm 10.5}$ & $\bm{44.2\pm 3.9}$ & $\bm{51.0\pm 7.2}$ & $\bm{50.9\pm 4.4}$ & $\bm{57.3\pm 6}$ & $\bm{55.0\pm 3.1}$ & $\bm{62.5\pm 3.5}$ & $\bm{57.9\pm 2.1}$ & $\bm{64.2\pm 3.7}$ & $\bm{61.0\pm 2.4}$ & $\bm{66.7\pm 2.3}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\small Pubmed}} & Greedy & $13.7\pm 3.4$ & $26.4\pm 8.7$ & $13.8\pm 3.4$ & $26.4\pm 8.7$ & $13.9\pm 3.3$ & $26.5\pm 8.6$ & $14.2\pm 3.5$ & $26.7\pm 8.5$ & $14.1\pm 3.4$ & $26.6\pm 8.6$ & $15\pm 4.7$ & $27.3\pm 8.6$ \\
\multicolumn{1}{|c|}{} & ASCV & $16.6\pm 4.5$ & $32.2\pm 9.3$ & $16.3\pm 4$ & $32.3\pm 9.4$ & $16.1\pm 3.8$ & $32.3\pm 9.4$ & $16.2\pm 3.8$ & $32.3\pm 9.4$ & $16.4\pm 3.7$ & $32.4\pm 9.3$ & $19.7\pm 7.9$ & $35.2\pm 9.6$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $16.3\pm 7.5$ & $28.5\pm 9.6$ & $17.6\pm 9.6$ & $30.0\pm 11.7$ & $19.1\pm 10.1$ & $30.9\pm 11.6$ & $20.7\pm 10.8$ & $32.1\pm 11.6$ & $22.7\pm 12$ & $33.9\pm 12$ & $24.2\pm 14.2$ & $35.2\pm 13.6$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $17.1\pm 4.5$ & $32.7\pm 9.7$ & $22.1\pm 10.2$ & $36.6\pm 11.1$ & $23.9\pm 12.3$ & $37.8\pm 12.6$ & $28.1\pm 13.4$ & $41.7\pm 12.9$ & $30.6\pm 13.2$ & $43.2\pm 13.7$ & $32.0\pm 13.3$ & $43.9\pm 13.4$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{29\pm 13.7}$ & $\bm{42\pm 15.9}$ & $\bm{36.9\pm 18.4}$ & $\bm{48.9\pm 17.1}$ & $\bm{40.2\pm 14.7}$ & $\bm{51.8\pm 14.5}$ & $\bm{44.6\pm 19.1}$ & $\bm{54.3\pm 17.8}$ & $\bm{47.1\pm 17.3}$ & $\bm{56.8\pm 15.8}$ & $\bm{52.8\pm 15}$ & $\bm{61.2\pm 12.2}$ \\ \hline
\end{tabular}%
}
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Node classification performance under different budgets using GIN architecture for $n_i=1$}
\label{tab:GIN}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|}
\hline
\multicolumn{2}{|c|}{budget} & \multicolumn{2}{c|}{50} & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{150} & \multicolumn{2}{c|}{200} & \multicolumn{2}{c|}{250} & \multicolumn{2}{c|}{300} \\ \hline
\multicolumn{2}{|c|}{metric} & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Cora}} & Greedy & $20.5\pm 6.1$ & $30.8\pm 8.1$ & $28.4\pm 7.4$ & $38.5\pm 6.8$ & $31.2\pm 9.1$ & $40.8\pm 7.8$ & $33.5\pm 12.2$ & $43.7\pm 9.5$ & $36.8\pm 11.1$ & $45.9\pm 8.5$ & $39.1\pm 11.2$ & $48.8\pm 7.7$ \\
\multicolumn{1}{|c|}{} & ASCV & $22.8\pm 8.9$ & $29.6\pm 6.9$ & $36.0\pm 8.5$ & $40.4\pm 5.8$ & $46.2\pm 7.9$ & $50.0\pm 7.1$ & $49.9\pm 8.9$ & $53.0\pm 6.9$ & $56.9\pm 7.0$ & $59.4\pm 5.0$ & $60.8\pm 7.7$ & $62.7\pm 5.5$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $29.3\pm 13.5$ & $41.6\pm 12.1$ & $35.2\pm 15.5$ & $46.7\pm 12.4$ & $35.9\pm 15.9$ & $47.4\pm 13.3$ & $40.3\pm 15.3$ & $50.5\pm 12.8$ & $42.8\pm 14.7$ & $52.3\pm 12.2$ & $46.0\pm 14.4$ & $55.2\pm 11.0$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $35.9\pm 11.8$ & $45.7\pm 10.5$ & $49.7\pm 9.0$ & $56.4\pm 7.1$ & $55.9\pm 6.5$ & $61.3\pm 3.9$ & $68.2\pm 5.6$ & $71.2\pm 3.9$ & $71.1\pm 5.7$ & $73.7\pm 4.0$ & $74.5\pm 3.9$ & $76.1\pm 3.9$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{47.1\pm 10}$ & $\bm{57.4\pm 9.9}$ & $\bm{58.1\pm 7.8}$ & $\bm{65.7\pm 5.5}$ & $\bm{66.8\pm 7.7}$ & $\bm{73.1\pm 4.3}$ & $\bm{72.8\pm 5.4}$ & $\bm{76.5\pm 4.6}$ & $\bm{72.7\pm 7.1}$ & $\bm{77.6\pm 4.0}$ & $\bm{75\pm 3.9}$ & $\bm{78.2\pm 3.6}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Citeseer}} & Greedy & $10.2\pm 5.3$ & $18.1\pm 9.5$ & $16.1\pm 7.9$ & $24.4\pm 10.1$ & $19.8\pm 8.0$ & $26.4\pm 8.8$ & $26.7\pm 9.8$ & $33.5\pm 12.1$ & $30.4\pm 9.4$ & $37.4\pm 11.9$ & $31.3\pm 9.6$ & $37.5\pm 10.2$ \\
\multicolumn{1}{|c|}{} & ASCV & $13.1\pm 2.9$ & $20.5\pm 6.3$ & $20.4\pm 4.6$ & $24.5\pm 8.8$ & $32.4\pm 4.2$ & $34.1\pm 8.3$ & $35.8\pm 5.4$ & $35.1\pm 6.5$ & $43.0\pm 4.9$ & $45.0\pm 7.2$ & $47.4\pm 3.9$ & $50.4\pm 5.0$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $22.1\pm 12.2$ & $32.1\pm 13.7$ & $26.9\pm 11.6$ & $36.1\pm 11.7$ & $30.9\pm 12.3$ & $39.6\pm 11$ & $36.1\pm 11.7$ & $44.3\pm 11.2$ & $38.1\pm 11.6$ & $45.8\pm 11.3$ & $41.4\pm 11.8$ & $49.1\pm 11.3$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $27.6\pm 9.7$ & $35.8\pm 12.8$ & $38.5\pm 10.1$ & $45.4\pm 11.9$ & $47.6\pm 8.0$ & $52.3\pm 9.3$ & $52.9\pm 5.5$ & $58.1\pm 4.8$ & $54.9\pm 4.5$ & $60.3\pm 4.3$ & $57.0\pm 4.6$ & $62.4\pm 2.8$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{39.2\pm 8.2}$ & $\bm{44.7\pm 10.9}$ & $\bm{49.5\pm 4.5}$ & $\bm{55.9\pm 4.5}$ & $\bm{52.4\pm 4.0}$ & $\bm{56.9\pm 4.6}$ & $\bm{56.2\pm 3.3}$ & $\bm{61.4\pm 3.6}$ & $\bm{57.5\pm 4.4}$ & $\bm{61.8\pm 5.2}$ & $\bm{60.8\pm 3.7}$ & $\bm{65.4\pm 3.2}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Pubmed}} & Greedy & $15.0\pm 4.6$ & $27.4\pm 8.6$ & $16.6\pm 7.8$ & $29.1\pm 10.6$ & $17.1\pm 8.6$ & $29.4\pm 10.9$ & $17.6\pm 9.0$ & $29.7\pm 11.1$ & $18.2\pm 9.5$ & $30.0\pm 11.1$ & $19.3\pm 9.8$ & $30.7\pm 11.2$ \\
\multicolumn{1}{|c|}{} & ASCV & $16.6\pm 4.2$ & $32.4\pm 9.5$ & $18.0\pm 4.6$ & $33.1\pm 9.4$ & $18.9\pm 4.8$ & $33.5\pm 9.2$ & $20.5\pm 6.5$ & $34.2\pm 9.9$ & $23.1\pm 8.6$ & $36.9\pm 10.1$ & $24.6\pm 9.9$ & $37.8\pm 10.9$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $18.4\pm 9.5$ & $30.0\pm 11.4$ & $21.8\pm 14.7$ & $32.2\pm 14.0$ & $23.8\pm 17.3$ & $33.6\pm 15.7$ & $24.7\pm 17.1$ & $34.2\pm 15.3$ & $25.0\pm 15.7$ & $34.4\pm 14.3$ & $26.8\pm 17.3$ & $36.0\pm 15.3$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $23.5\pm 11.4$ & $36.4\pm 12.5$ & $25.3\pm 10.9$ & $37.6\pm 12.4$ & $29.4\pm 12.3$ & $40.6\pm 13.0$ & $31.1\pm 12.2$ & $42.0\pm 12.6$ & $33.5\pm 13.8$ & $43.4\pm 13.0$ & $36.5\pm 15.6$ & $45.5\pm 14.2$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{39.1\pm 21.1}$ & $\bm{49.1\pm 19.6}$ & $\bm{44.4\pm 19.9}$ & $\bm{52.8\pm 18.1}$ & $\bm{48.9\pm 20.9}$ & $\bm{56.4\pm 19.1}$ & $\bm{52.6\pm 19.5}$ & $\bm{59.0\pm 18.2}$ & $\bm{55.1\pm 17.1}$ & $\bm{61.4\pm 15.5}$ & $\bm{58.1\pm 13.9}$ & $\bm{63.3\pm 13.6}$ \\ \hline
\end{tabular}%
}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[ht!]
\centering
\caption{Node classification performance under different budgets using GraphSage architecture for $n_i=1$}
\label{tab:GraphSage}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|}
\hline
\multicolumn{2}{|c|}{budget} & \multicolumn{2}{c|}{50} & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{150} & \multicolumn{2}{c|}{200} & \multicolumn{2}{c|}{250} & \multicolumn{2}{c|}{300} \\ \hline
\multicolumn{2}{|c|}{metric} & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Cora}} & Greedy & $16.7\pm 3.8$ & $19.8\pm 2.7$ & $22.7\pm 5.4$ & $26.1\pm 4.7$ & $26.6\pm 6.4$ & $29.7\pm 6.0$ & $30.2\pm 7.0$ & $32.7\pm 4.9$ & $31.8\pm 6.9$ & $32.8\pm 5.0$ & $34.8\pm 9.0$ & $37.6\pm 8.0$ \\
\multicolumn{1}{|c|}{} & ASCV & $15.2\pm 2.3$ & $18.1\pm 2.0$ & $23.9\pm 4.2$ & $24.2\pm 3.2$ & $29.8\pm 6.4$ & $29.4\pm 4.1$ & $31.6\pm 6.6$ & $30.1\pm 6.3$ & $37.2\pm 7.9$ & $34.7\pm 6.7$ & $40.6\pm 5.1$ & $37.9\pm 6.2$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $36.8\pm 12.4$ & $46.9\pm 12.3$ & $42.9\pm 15.4$ & $52.9\pm 14.2$ & $43.7\pm 15.8$ & $52.8\pm 14.7$ & $47.3\pm 16.2$ & $56.3\pm 14.1$ & $47.8\pm 15.0$ & $56.6\pm 11.9$ & $49.5\pm 14.5$ & $58.5\pm 11.2$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $43.9\pm 9.7$ & $51.6\pm 7.4$ & $55.1\pm 5.3$ & $60.1\pm 6.2$ & $63.2\pm 8.3$ & $66.9\pm 6.5$ & $67.7\pm 6.6$ & $70.0\pm 5.7$ & $71.6\pm 5.1$ & $73.7\pm 4.3$ & $72.0\pm 4.2$ & $73.9\pm 3.4$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{50.3\pm 6.4}$ & $\bm{57.6\pm 6.6}$ & $\bm{64.1\pm 7.4}$ & $\bm{67.7\pm 4.9}$ & $\bm{65.9\pm 5.1}$ & $\bm{69.1\pm 2.9}$ & $\bm{70.9\pm 3.1}$ & $\bm{73.8\pm 2.3}$ & $\bm{73.5\pm 3.6}$ & $\bm{76.2\pm 3.3}$ & $\bm{73.6\pm 3.0}$ & $\bm{76.7\pm 2.4}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Citeseer}} & Greedy & $15.8 \pm 3.7$ & $20.1 \pm 6.1$ & $20.1 \pm 1.7$ & $25.6 \pm 3.5$ & $24.2 \pm 3.1$ & $30.0 \pm 4.9$ & $26.1 \pm 4.1$ & $32.1 \pm 5.3$ & $29.2 \pm 4.5$ & $33.3 \pm 4.2$ & $31.4 \pm 4.8$ & $35.9 \pm 5.2$ \\
\multicolumn{1}{|c|}{} & ASCV & $16.7 \pm 3.2$ & $19.7 \pm 3.5$ & $22.4 \pm 2.6$ & $24.5 \pm 3.9$ & $25.0 \pm 2.2$ & $25.9 \pm 3.0$ & $29.6 \pm 3.5$ & $30.0 \pm 4.7$ & $32.5 \pm 2.6$ & $34.5 \pm 4.3$ & $34.9 \pm 3.9$ & $34.9 \pm 5.8$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $30.1 \pm 8.5$ & $38.5 \pm 9.3$ & $38.3 \pm 9.8$ & $46.1 \pm 9.2$ & $43.2 \pm 9.8$ & $50.8 \pm 8.8$ & $45.5 \pm 9.6$ & $53.8 \pm 9.3$ & $49.4 \pm 10.9$ & $57.7 \pm 9.4$ & $50.2 \pm 10.4$ & $58.6 \pm 8.9$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $35.2 \pm 8.8$ & $40.6 \pm 10.5$ & $46.9 \pm 7.7$ & $53.4 \pm 7.3$ & $55.2 \pm 6.4$ & $\bm{59.6 \pm 5.3}$ & $56.0 \pm 5.8$ & $60.8 \pm 3.6$ & $57.6 \pm 3.0$ & $62.4 \pm 3.1$ & $\bm{60.9 \pm 2.8}$ & $65.0 \pm 3.0$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{47.6 \pm 7.0}$ & $\bm{52.8 \pm 7.5}$ & $\bm{54.0 \pm 3.1}$ & $\bm{57.4 \pm 3.0}$ & $\bm{56.3 \pm 3.1}$ & $59.4 \pm 3.0$ & $\bm{58.4 \pm 2.7}$ & $\bm{62.3 \pm 2.8}$ & $\bm{59.9 \pm 1.8}$ & $\bm{63.2 \pm 2.3}$ & $60.7 \pm 3.2$ & $\bm{66.3 \pm 2.0}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Pubmed}} & Greedy & $25.8 \pm 4.1$ & $31.4 \pm 5.0$ & $27.7 \pm 2.7$ & $35.9 \pm 2.9$ & $26.3 \pm 2.8$ & $37.6 \pm 4.8$ & $25.9 \pm 3.9$ & $38.5 \pm 6.3$ & $27.0 \pm 4.9$ & $38.9 \pm 5.8$ & $27.5 \pm 3.4$ & $39.9 \pm 5.3$ \\
\multicolumn{1}{|c|}{} & ASCV & $15.2 \pm 2.3$ & $18.1 \pm 2.0$ & $23.9 \pm 4.2$ & $24.2 \pm 3.2$ & $29.8 \pm 6.4$ & $29.4 \pm 4.1$ & $31.6 \pm 6.6$ & $30.1 \pm 6.3$ & $37.2 \pm 7.9$ & $34.7 \pm 6.7$ & $40.6 \pm 5.1$ & $37.9 \pm 6.2$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $36.8 \pm 12.4$ & $46.9 \pm 12.3$ & $42.9 \pm 15.4$ & $52.9 \pm 14.2$ & $43.7 \pm 15.8$ & $52.8 \pm 14.7$ & $47.3 \pm 16.2$ & $56.3 \pm 14.1$ & $47.8 \pm 15.0$ & $56.6 \pm 11.9$ & $49.5 \pm 14.5$ & $58.5 \pm 11.2$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $47.5 \pm 8.6$ & $53.8 \pm 8.2$ & $46.9 \pm 12.0$ & $52.6 \pm 10.6$ & $49.8 \pm 12.9$ & $54.4 \pm 11.2$ & $50.7 \pm 14.0$ & $56.0 \pm 12.3$ & $57.4 \pm 13.7$ & $61.0 \pm 11.4$ & $57.8 \pm 14.1$ & $60.8 \pm 11.8$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{56.8 \pm 9.3}$ & $\bm{60.5 \pm 7.5}$ & $\bm{61.0 \pm 12.5}$ & $\bm{63.5 \pm 10.2}$ & $\bm{62.9 \pm 13.0}$ & $\bm{65.3 \pm 11.2}$ & $\bm{65.1 \pm 10.1}$ & $\bm{66.9 \pm 8.8}$ & $\bm{68.4 \pm 5.6}$ & $\bm{69.9 \pm 5.0}$ & $\bm{67.1 \pm 10.5}$ & $\bm{68.7 \pm 9.7}$ \\ \hline
\end{tabular}%
}
\end{table*}


\begin{table*}[ht!]
\centering
\caption{Node classification performance under different budgets using GAT architecture for $n_i=1$}
\label{tab:GAT}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|cc|cc|cc|cc|cc|cc|cc|}
\hline
\multicolumn{2}{|c|}{budget} & \multicolumn{2}{c|}{50} & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{150} & \multicolumn{2}{c|}{200} & \multicolumn{2}{c|}{250} & \multicolumn{2}{c|}{300} \\ \hline
\multicolumn{2}{|c|}{metric} & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 & MacroF1 & MicroF1 \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Cora}} & Greedy & $20.0 \pm 8.6$ & $29.2 \pm 9.6$ & $25.7 \pm 6.8$ & $34.5 \pm 9.0$ & $28.6 \pm 9.7$ & $37.1 \pm 10.3$ & $31.8 \pm 11.9$ & $40.5 \pm 11.3$ & $34.6 \pm 12.0$ & $44.6 \pm 9.4$ & $36.4 \pm 11.5$ & $45.9 \pm 9.4$ \\
\multicolumn{1}{|c|}{} & ASCV & $20.9 \pm 8.0$ & $26.0 \pm 9.0$ & $34.2 \pm 8.9$ & $38.8 \pm 7.1$ & $39.6 \pm 7.3$ & $45.0 \pm 4.9$ & $47.7 \pm 9.3$ & $52.2 \pm 5.6$ & $53.9 \pm 6.5$ & $56.5 \pm 4.4$ & $57.6 \pm 5.5$ & $59.8 \pm 4.0$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $25.1 \pm 11.7$ & $35.8 \pm 12.1$ & $30.7 \pm 13.8$ & $41.1 \pm 13.5$ & $33.6 \pm 15.0$ & $43.7 \pm 14.5$ & $36.6 \pm 18.4$ & $46.6 \pm 16.5$ & $38.1 \pm 16.4$ & $47.7 \pm 13.5$ & $39.8 \pm 16.7$ & $50.3 \pm 13.7$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $30.8 \pm 11.4$ & $36.6 \pm 9.5$ & $41.5 \pm 12.0$ & $49.6 \pm 9.9$ & $50.3 \pm 11.2$ & $57.2 \pm 8.7$ & $60.2 \pm 8.5$ & $64.8 \pm 7.5$ & $63.2 \pm 6.4$ & $67.8 \pm 5.5$ & $70.2 \pm 6.0$ & $72.9 \pm 6.1$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{35.2 \pm 6.8}$ & $\bm{45.9 \pm 8.5}$ & $\bm{49.2 \pm 7.8}$ & $\bm{59.4 \pm 7.1}$ & $\bm{57.3 \pm 6.2}$ & $\bm{65.7 \pm 5.0}$ & $\bm{62.7 \pm 8.2}$ & $\bm{70.4 \pm 5.5}$ & $\bm{69.2 \pm 6.6}$ & $\bm{74.3 \pm 4.5}$ & $\bm{71.7 \pm 5.4}$ & $\bm{76.1 \pm 4.3}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Citeseer}} & Greedy & $11.6 \pm 5.0$ & $19.8 \pm 8.3$ & $16.9 \pm 7.6$ & $25.7 \pm 10.0$ & $20.3 \pm 8.9$ & $28.2 \pm 8.9$ & $25.6 \pm 10.5$ & $33.0 \pm 10.5$ & $28.1 \pm 11.4$ & $34.6 \pm 10.3$ & $29.5 \pm 11.5$ & $37.5 \pm 9.4$ \\
\multicolumn{1}{|c|}{} & ASCV & $12.4 \pm 3.7$ & $19.5 \pm 5.3$ & $20.7 \pm 4.8$ & $26.0 \pm 7.7$ & $28.7 \pm 5.9$ & $34.7 \pm 7.2$ & $34.9 \pm 3.9$ & $38.9 \pm 5.6$ & $38.7 \pm 6.2$ & $44.3 \pm 3.5$ & $44.0 \pm 5.3$ & $49.9 \pm 3.6$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & \multicolumn{1}{l}{$15.3 \pm 7.8$} & \multicolumn{1}{l|}{$25.6 \pm 9.8$} & \multicolumn{1}{l}{$20.2 \pm 9.4$} & \multicolumn{1}{l|}{$31.1 \pm 10.4$} & \multicolumn{1}{l}{$24.0 \pm 11.9$} & \multicolumn{1}{l|}{$34.1 \pm 10.6$} & \multicolumn{1}{l}{$30.7 \pm 13.4$} & \multicolumn{1}{l|}{$39.9 \pm 11.9$} & \multicolumn{1}{l}{$32.5 \pm 13.0$} & \multicolumn{1}{l|}{$41.7 \pm 11.5$} & \multicolumn{1}{l}{$36.6 \pm 14.5$} & \multicolumn{1}{l|}{$46.0 \pm 13.0$} \\
\multicolumn{1}{|c|}{} & ASCV(P) & $19.8 \pm 8.1$ & $28.3 \pm 9.8$ & $31.7 \pm 8.0$ & $38.7 \pm 9.9$ & $41.7 \pm 5.1$ & $48.1 \pm 6.0$ & $47.5 \pm 8.6$ & $54.6 \pm 8.3$ & $53.7 \pm 7.4$ & $60.7 \pm 6.2$ & $55.8 \pm 7.0$ & $62.7 \pm 5.0$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{31.5 \pm 8.9}$ & $\bm{36.7 \pm 9.8}$ & $\bm{46.0 \pm 7.5}$ & $\bm{51.5 \pm 8.1}$ & $\bm{49.7 \pm 4.1}$ & $\bm{56.4 \pm 3.8}$ & $\bm{53.3 \pm 4.0}$ & $\bm{60.6 \pm 4.2}$ & $\bm{58.0 \pm 3.8}$ & $\bm{63.8 \pm 4.7}$ & $\bm{59.1 \pm 2.1}$ & $\bm{66.0 \pm 1.9}$ \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Pubmed}} & Greedy & \multicolumn{1}{l}{$16.2 \pm 6.2$} & \multicolumn{1}{l|}{$29.0 \pm 9.9$} & \multicolumn{1}{l}{$16.6 \pm 5.3$} & \multicolumn{1}{l|}{$28.5 \pm 8.6$} & \multicolumn{1}{l}{$21.7 \pm 10.6$} & \multicolumn{1}{l|}{$33.6 \pm 11.0$} & \multicolumn{1}{l}{$23.0 \pm 12.4$} & \multicolumn{1}{l|}{$34.9 \pm 12.3$} & \multicolumn{1}{l}{$22.3 \pm 11.1$} & \multicolumn{1}{l|}{$33.6 \pm 11.6$} & \multicolumn{1}{l}{$22.0 \pm 11.5$} & \multicolumn{1}{l|}{$33.6 \pm 11.4$} \\
\multicolumn{1}{|c|}{} & ASCV & $17.1 \pm 4.7$ & $32.6 \pm 9.6$ & $20.3 \pm 7.0$ & $35.2 \pm 9.6$ & $23.3 \pm 7.8$ & $37.5 \pm 8.7$ & $23.5 \pm 8.6$ & $37.6 \pm 8.7$ & $23.8 \pm 8.6$ & $37.7 \pm 9.0$ & $28.8 \pm 10.4$ & $42.2 \pm 8.7$ \\
\multicolumn{1}{|c|}{} & Greedy(P) & $16.9 \pm 9.1$ & $29.4 \pm 11.0$ & $15.2 \pm 3.8$ & $27.2 \pm 8.4$ & $15.3 \pm 4.8$ & $27.2 \pm 8.7$ & $17.8 \pm 9.9$ & $30.0 \pm 12.0$ & $19.1 \pm 7.1$ & $30.0 \pm 8.3$ & $21.0 \pm 10.3$ & $32.3 \pm 10.2$ \\
\multicolumn{1}{|c|}{} & ASCV(P) & $17.2 \pm 4.9$ & $32.7 \pm 9.7$ & $17.8 \pm 5.5$ & $32.9 \pm 9.9$ & $21.5 \pm 9.7$ & $36.1 \pm 10.4$ & $24.7 \pm 11.6$ & $38.2 \pm 12.1$ & $25.6 \pm 11.4$ & $38.7 \pm 12.3$ & $29.6 \pm 11.3$ & $41.3 \pm 10.5$ \\
\multicolumn{1}{|c|}{} & {\bf SIMT} & $\bm{19.5 \pm 5.5}$ & $\bm{34.4 \pm 9.3}$ & $\bm{30.0 \pm 12.1}$ & $\bm{43.8 \pm 13.0}$ & $\bm{32.5 \pm 14.5}$ & $\bm{43.9 \pm 14.7}$ & $\bm{32.0 \pm 13.3}$ & $\bm{42.9 \pm 12.0}$ & $\bm{41.9 \pm 14.0}$ & $\bm{52.0 \pm 12.9}$ & $\bm{44.5 \pm 20.4}$ & $\bm{54.9 \pm 19.3}$ \\ \hline
\end{tabular}%
}
\end{table*}

\section{Comparison of GNN architectures}
\label{app:models}

To test the effect of GNN architectures, we also conduct experiments on scenario with $n_i=1, i\in O$ using three representative GNN architectures: GIN \cite{xu2018powerful}, GraphSage \cite{hamilton2017inductive}, and GAT \cite{velivckovic2017graph}. These architectures have emerged as alternatives to the traditional GCN \cite{kipf2016semi} and have gained widespread acceptance within the GNN community. The configuration of these architectures are the same as that of GCN as shown in GNN models part in  Sec.~\ref{sec:setup}. 
To eliminate the randomness induced by subgraphs, we conduct experiments on the case where all data owners are individuals.
%The experimental settings are listed in Sec.~\ref{sec:setup}.




% overall performance  special 
The experiment results are shown in Tables~\ref{apptab:quantity}, \ref{apptab:GCN}, \ref{tab:GIN}, \ref{tab:GraphSage}, and~\ref{tab:GAT}.
In general, SIMT outperforms its benchmarks in all experiments. 
When compared to the Greedy mechanism, SIMT exhibits an improvement ranging from $10\%$ to $ 40\%$ in both MacroF1 and MicroF1, consistently confirming the effectiveness of SIMT.
However, a minor fluctuation is observed when compared to ASCV(P) within the GraphSage architecture on the Citeseer dataset.
More specifically, when the budget is 1500, SIMT demonstrates a $1.1\%$ improvement in MacroF1, albeit a $0.2\%$ decrease in MicroF1. Furthermore, when the budget is increased to 3000, SIMT shows a slight decrease by $0.2\%$ in MacroF1, but a notable increase by $1.3\%$ in MicroF1. Consequently, this fluctuation does not detract from the overall effectiveness of SIMT. 


% effect of different architectures, which is the best and which is the lowest,  why they perform differently.
When comparing the effects of different architectures, it is observed that the GraphSage architecture generally outperforms the others. This superior performance is particularly noticeable when the budget is low. Additionally, the GraphSage architecture exhibits lower variance, which implies robust performance. This could potentially be attributed to the fact that GraphSage does not heavily depend on node features and is well suited to spectral features \cite{velivckovic2017graph}. This compatibility allows it to effectively leverage the spectral properties of the data, potentially contributing to its robust performance across various experiments. 
Furthermore, when the budget is set to 3000, the GraphSage architecture underperforms the GIN architecture on the Cora dataset. This could potentially be attributed to the theoretically stronger learnability of GIN compared to GraphSage \cite{xu2018powerful}.
Last, the GAT architecture performs the worst among the four architectures. This could be due to the fact that GAT employs an attention mechanism, which has significantly more parameters and thus requires a sufficient amount of feature and label data for parameter training.




\section{Ablation study for $n_i=1$} \label{appsec:ablation}
We conduct ablation studies across the three datasets and present the average test accuracy for $n_i=1$. 
As shown in Table~\ref{apptab:ablationstudy}, {\bf (1)} the accuracy of SIMT without clustering diminishes by $8.3\%$ in Cora, $3.7\%$ in Citeseer, and $13.7\%$ in Pubmed. This significant reduction underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset.
{\bf (2)} The accuracy of SIMT without structural informativeness drops by $1.8\%$ in Cora, $2.1\%$ in Citeseer, and $2.9\%$ in Pubmed. {\bf (3)} The accuracy reduction for SIMT without structural representativeness is $1.8\%$ in Cora, $1.5\%$ in Citeseer, and $2.2\%$ in Pubmed. Overall, each component distinctly enhances SIMT's performance.

In the scenario with $n_i=1, i\in O$, the results show similar trends as the scenario with $n_i\geq 1, i\in O$: SIMT performs the best under all budgets, followed by ASCV/ASCV(P), and Greedy/Greedy(P) performs the worst. It is worth to notice that SIMT performs even better when $n_i=1, i\in O$ than when $n_i\geq 1, i\in O$. In particular, SIMT improves $20\% \sim 40\%$ in both MacroF1 and MicroF1 in this case; See more details in Table~\ref{apptab:GCN} of App.~\ref{app:models}. This can be attributed to that when $n_i=1, i\in O$, fewer edges are missing within subgraphs, allowing the structural importance scores calculated by our mechanism to more accurately reflect the true structural importance of each node.








\section{Limitations and future work}
Even though our SIMT mechanism shows the incentive properties theoretically and the accuracy empirically, we acknowledge certain limitations that offer avenues for future research. 
\begin{itemize}
    \item This paper addresses issues related to pre-purchase inspections. However, there are concerns about privacy breaches, as the raw data shared with data brokers and the trained models returned to data consumers could expose private information of the data owners. Future work could enhance the SIMT mechanism by incorporating considerations of these privacy issues
    \item  In the design of the SIMT mechanism, we assess the structural importance once and subsequently make a single purchasing decision for all data owners. An alternative approach could involve an adaptive purchasing method, where data points are bought in several times, each time reassessing the structural importance in light of newly acquired data. The primary rationale for the existing design is efficiency compared to this adaptive method. Nonetheless, developing an efficient adaptive model training mechanism presents an exciting avenue for future research.
    % \item Our paper assumes that the network structural is publicly known and relies on the graph structural to determine the data importance. However, the sensitivity of the graph structure itself is a significant concern. For instance, people may be reluctant to disclose certain connections. While not the central focus of this study, addressing these privacy issues related to graph structure represents a potential direction for future research.
    \item The efficacy of the SIMT mechanism heavily depends on the quality of clustering; more accurate structural importance assessment is achieved when clusters are closer to the true classes. Our SIMT mechanism employs a classical VGAE model \cite{kipf2016variational} as the $\mathsf{Clustering}$. However, our findings indicate that this approach is not always robust. Exploring a broader range of GNN models for clustering might yield more stable results and enhance the performance of the SIMT mechanism, although such investigations fall outside the scope of this paper
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

