\section{Related work}
Data pricing has been extensively studied in two main contexts: {\em data marketplaces} and {\em model marketplaces}. %Below, we review key approaches in each area.

% \smallskip

\noindent {\bf Data Pricing in Data Marketplaces.}
In data marketplaces, pricing mechanisms revolve around trading raw datasets or simple queries. Previous work has focused on pre-purchase decisions, where data is evaluated before it is accessed, which aligns with our setting. For instance, the importance of datasets is often quantified by metrics such as size, as explored by **Krause, "Cost-effective neural network design"** and **Zinkevich et al., "Exploiting diverse skills in large-scale machine learning"**. Other studies, such as **Voulgarkis et al., "Fairness-aware data marketplaces for machine learning tasks"** and **Bhaduri et al., "Data pricing with privacy constraints"**, assess data importance based on its utility to consumers, proposing auction mechanisms and contracts to compensate data owners accordingly.

When it comes to \emph{query-based data pricing}, metrics like privacy levels directly impact the accuracy of responses, thereby influencing data value.
% have a direct impact on the accuracy of responses, influencing the value of the data. 
For instance, **Liu et al., "Differential privacy for query-based data pricing"** propose auction mechanisms that incorporate privacy in queries, while **Kifer et al., "Take-it-or-leave-it contracts for count queries"** introduces a take-it-or-leave-it contract for count queries. Further work by **Cui et al., "Linear predictor queries and broader query settings"** expands these ideas to linear predictor queries and broader query settings.
% For example, ____ and ____ design auction mechanisms that account for the privacy of data in queries, while ____ offers a take-it-or-leave-it contract for count queries. Further studies by ____ and ____ extend these ideas to linear predictor queries and more general query settings.

In data marketplaces, data importance is often easily quantifiable using metrics like size or privacy levels. However, in the context of \emph{model marketplaces}, the contribution of individual data points to machine learning model performance is more complex and requires novel pricing methods.

% \smallskip

\noindent {\bf Data Pricing in Model Marketplaces.} 
% In model marketplaces, data pricing is typically based on how much a dataset improves the performance of a machine learning model. ____ pioneered the study of data pricing for machine learning tasks by presenting a theoretical framework that balances budget constraints with model performance. Most subsequent works, such as those by **Jia et al., "A theoretical framework for data pricing in machine learning"** and **Cao et al., "Fairness-aware data pricing for machine learning tasks"**, assume the benefit of a trained model is known and focus on how to fairly distribute rewards among data owners. A common approach is the application of the \emph{Shapley value} ____, which ensures that each data owner is compensated according to their contribution to the overall model.
In model marketplaces, data pricing is typically based on how much a dataset improves a machine learning model's performance. **Jia et al., "A theoretical framework for data pricing in machine learning"** introduced a theoretical framework for data pricing that balances budget constraints with model performance. Subsequent works, such as those by **Cao et al., "Fairness-aware data pricing for machine learning tasks"** and **Li et al., "Shapley value-based data pricing for machine learning"**, assume the model's benefit is known and focus on fairly distributing rewards among data owners. A common method for this is the \emph{Shapley value} ____, which compensates each data owner based on their contribution to the model.

Various studies have refined the utility function used in Shapley value calculations by incorporating additional factors. **Zhou et al., "K-nearest neighbors and privacy considerations"** and **Wang et al., "Utility designs for collaborative machine learning"**, for example, include $K$-nearest neighbors and privacy considerations in their utility designs. **Tao et al., "Shapley value framework extension to model marketplaces"** builds on this by extending the Shapley value framework to model marketplaces. Other research, such as **Huang et al., "Utility design for collaborative machine learning"** and **Liu et al., "Information gain-based utility design"**, explores utility design in \emph{collaborative machine learning} scenarios, where data owners also serve as model consumers. In these cases, utility is defined either as the sum of the model's value to the owner and its marginal value to others ____, or through metrics like information gain ____. **Chen et al., "Utility design based on cosine similarity"** and **Yang et al., "Utility design based on privacy leakage"**, further define utility based on the cosine similarity of parameters or the privacy leakage of shared model parameters.

A common limitation of these works is that they often require training models on the entire dataset before compensating data owners. In practice, this assumption is often unrealistic, as data owners are usually hesitant to contribute their data upfront without proper guarantees or compensation.

%Addressing Data Pricing Without Direct Data Inspection

A more realistic setting, which is closer to our approach, has been explored by studies ____. **Zhou et al., "VCG auction mechanism for data selection"** assume that data importance is known and apply a \emph{VCG auction mechanism} to select and compensate data owners. **Wang et al., "Reputation-based auction mechanism"** propose an auction mechanism that incorporates the reputation of data owners as a reflection of their contribution, while **Liu et al., "Privacy-aware auction mechanism"** design an auction that selects data owners based on their privacy requirements. Although these approaches offer valuable insights, they rely on exogenous metrics, such as reputation or privacy, which are often difficult to obtain or may not accurately reflect the intrinsic value of data for model training.

% In contrast, our work introduces a novel approach to \emph{measure data importance} without the need for direct data inspection. By focusing on the structural properties of graph-structured data and leveraging methods like structural entropy, we aim to develop a fair and effective data pricing mechanism that addresses the limitations of previous approaches.
In contrast, our work proposes a novel method to \emph{measure data importance} without direct data inspection. By focusing on the structural properties of graph data and using techniques like structural entropy, we aim to create a fair and effective data pricing mechanism that overcomes the limitations of previous methods.


% \smallskip

\noindent {\bf Comparison with FL and AL.} While {\em Federated Learning} (FL) and {\em Active Learning} (AL) are well-known paradigms for training models with distributed data, our approach differs in key ways.
{\bf (1)} In FL, each data owner trains a local model on private data, which is then aggregated into a global model while preserving privacy ____. SIMT, by contrast, does not require data owners to train models. Instead, data is directly provided to a central model, allowing for optimizations like data augmentation that are not possible in FL's gradient-based aggregation. This eliminates the computational burden on data owners and allows for more flexible model improvements. {\bf (2)} In AL, the model iteratively queries data points to refine learning, typically in multiple rounds ____. SIMT, however, collects data in a single round, reducing overhead and cost. Furthermore, while AL assumes access to unlabeled data with labels provided iteratively ____, SIMT addresses the real-world challenge of compensating data owners, ensuring they are fairly rewarded for their contributions upfront.