\section{Related Work}
\noindent\textbf{Model Collapse.} Iterative training on synthetically generated data induces model collapse, a phenomenon characterized by systematic erosion of the long-tail components of the original data distribution ____. Theoretical analyses further elucidated how self-consuming training loops alter intrinsic scaling laws, thereby intensifying this collapse ____, complementing earlier findings on distributional distortions ____. Furthermore, ____ demonstrated that iterative training on synthetic text does not preserve the nuanced richness of human language, particularly in creative tasks, underscoring the broader challenges of maintaining linguistic diversity in iteratively generated content.\\\vspace{-8pt}

\noindent\textbf{Iterative Generation and Information Evolution.} Iterative generation can trigger model collapse, whereby the diversity of real-world information degrades over time—a process that ____ defines as knowledge collapse.  Research on language evolution offers a framework for analyzing these degradations ____, aligning with broader perspectives on cultural evolution ____. 
In the context of LLMs, ____ analyzed text properties evolution in rephrasing, continuation, and inspiration-taking tasks. Their work, however, overlooked translation—a key LLM application—and focused solely on chains involving a single model. 
Our work overcomes these shortcomings by investigating how iterative information translation accelerates distortions, explores heterogeneous model chains, and extends the analysis to higher complexity rephrasing chains, providing a broader view of iterative generation's impact on information evolution.\\\vspace{-9pt}

\noindent\textbf{LLM Agents.} We consider the implications for multi-agent settings, where communication frameworks leverage collaborative interactions between multiple LLMs ____. These frameworks enable agents to iteratively refine outputs through debate-style interactions ____ or cooperative task decomposition ____, often improving accuracy in mathematical and logical tasks ____. As introduced by ____, generative agents showcase the potential for creating interactive simulacra of human behavior through memory, reflection, and planning. However, such architectures implicitly assume that iterative exchanges preserve or enhance information fidelity—a premise challenged by our findings in translation chains. While prior work focuses on emergent problem-solving capabilities ____, our study reveals how these same iterative mechanisms accelerate information distortion, particularly in scenarios where translation ambiguities compound through successive agent handoffs.\\\vspace{-9pt}

\noindent\textbf{Evaluation of LLM Outputs.} In addition to the multi-agent perspective, it is essential to scrutinize how LLM outputs are evaluated. Existing research predominantly relies on metrics such as token similarity ____, output diversity ____, and factuality ____. However, these evaluations are generally confined to single iterations and fail to capture the cumulative degradation introduced by iterative generation—a critical aspect of the translation chains under investigation. Although previous studies have explored variations in toxicity, positivity, difficulty, and length in iterative LLM transmission chains ____, they have overlooked the systematic assessment of textual similarity and factuality. Our work addresses this gap by providing a rigorous analysis of the deterioration of these properties over successive iterations in both translation and rephrasing tasks.\vspace{-9pt}