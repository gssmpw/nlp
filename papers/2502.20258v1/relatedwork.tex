\section{Related Work}
\noindent\textbf{Model Collapse.} Iterative training on synthetically generated data induces model collapse, a phenomenon characterized by systematic erosion of the long-tail components of the original data distribution \citep{shumailov2023curse}. Theoretical analyses further elucidated how self-consuming training loops alter intrinsic scaling laws, thereby intensifying this collapse \citep{fu2024towards,dohmatob2024tale}, complementing earlier findings on distributional distortions \citep{lebrun2022evaluating}. Furthermore, \citet{guo-etal-2024-curious} demonstrated that iterative training on synthetic text does not preserve the nuanced richness of human language, particularly in creative tasks, underscoring the broader challenges of maintaining linguistic diversity in iteratively generated content.\\\vspace{-8pt}

\noindent\textbf{Iterative Generation and Information Evolution.} Iterative generation can trigger model collapse, whereby the diversity of real-world information degrades over time—a process that \citet{peterson2024ai} defines as knowledge collapse.  Research on language evolution offers a framework for analyzing these degradations \citep{markov2023language}, aligning with broader perspectives on cultural evolution \citep{MesoudiWhiten2008,CaldwellMillen2008}. 
In the context of LLMs, \citet{perez2024llms} analyzed text properties evolution in rephrasing, continuation, and inspiration-taking tasks. Their work, however, overlooked translation—a key LLM application—and focused solely on chains involving a single model. 
Our work overcomes these shortcomings by investigating how iterative information translation accelerates distortions, explores heterogeneous model chains, and extends the analysis to higher complexity rephrasing chains, providing a broader view of iterative generation's impact on information evolution.\\\vspace{-9pt}

\noindent\textbf{LLM Agents.} We consider the implications for multi-agent settings, where communication frameworks leverage collaborative interactions between multiple LLMs \citep{park2023generative, Wu2023-nb,Li2024-yw}. These frameworks enable agents to iteratively refine outputs through debate-style interactions \citep{Helm2024-yh} or cooperative task decomposition \citep{Pham2023-al}, often improving accuracy in mathematical and logical tasks \citep{zhang-etal-2024-exploring}. As introduced by \citet{park2023generative}, generative agents showcase the potential for creating interactive simulacra of human behavior through memory, reflection, and planning. However, such architectures implicitly assume that iterative exchanges preserve or enhance information fidelity—a premise challenged by our findings in translation chains. While prior work focuses on emergent problem-solving capabilities \citep{chan2024scaling}, our study reveals how these same iterative mechanisms accelerate information distortion, particularly in scenarios where translation ambiguities compound through successive agent handoffs.\\\vspace{-9pt}

\noindent\textbf{Evaluation of LLM Outputs.} In addition to the multi-agent perspective, it is essential to scrutinize how LLM outputs are evaluated. Existing research predominantly relies on metrics such as token similarity \citep{Hu2024UnveilingLE}, output diversity \citep{guo2024benchmarking,shaib2024standardizing}, and factuality \citep{Wang2023-lo, Iqbal2024-vd, Min2023-ls, Chern2023-xl}. However, these evaluations are generally confined to single iterations and fail to capture the cumulative degradation introduced by iterative generation—a critical aspect of the translation chains under investigation. Although previous studies have explored variations in toxicity, positivity, difficulty, and length in iterative LLM transmission chains \citep{perez2024llms}, they have overlooked the systematic assessment of textual similarity and factuality. Our work addresses this gap by providing a rigorous analysis of the deterioration of these properties over successive iterations in both translation and rephrasing tasks.\vspace{-9pt}