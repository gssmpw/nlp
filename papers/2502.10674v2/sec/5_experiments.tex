\section{Experiments}
\label{sec:experiments}
\input{tab/tab1_zeroshot}
\input{tab/tab2_fewshot}
We generate text-image-point cloud pretraining triplets using 52,417 3D models from the ShapeNetCore \cite{shapenet} dataset following the procedure in Section \ref{sec:occtip}. For evaluation, we create 12 partial point clouds for each ModelNet40 \cite{modelnet40} test object, resulting in ModelNet40-P with 29,610 occluded point clouds of 40 classes. We also use three other real-scanned benchmarks: ScanObjectNN \cite{scanobjectnn}, ScanNetV2 \cite{scannetv2}, and SUN RGB-D \cite{sunrgbd}. For fair comparisons, we mainly evaluate our work against methods that are also pretrained on ShapeNetCore \cite{shapenet} objects, including OpenShape \cite{openshape}, MixCon3D \cite{mixcon3d}, and TAMM \cite{tamm}. 

\vspace{1mm} \noindent\textbf{Evaluation Tasks.} We conduct extensive experiments in four recognition tasks with varying difficulty levels (zero-shot classification, few-shot linear probing, real-world instance recognition, and zero-shot object detection) to demonstrate the superiority of our pretraining framework OccTIP and the proposed architecture DuoMamba. The details of each experiment will be described in the following subsections.

\vspace{1mm}
\noindent\textbf{Implementation and Training Details.} We implement our method in PyTorch \cite{pytorch} and conduct all experiments on a single NVIDIA RTX 4090 GPU. We sample 2,048 points per point cloud as input and train the 3D encoders for 200 epochs using AdamW \cite{adamw} with a 10-epoch warmup, which takes around 1.5 days. Following prior works \cite{openshape,mixcon3d,tamm}, we use OpenCLIP ViT-bigG-14 \cite{openclip} as pretrained image-text encoders. Further details are in the supplementary material.

\subsection{Zero-Shot Classification}
\input{tab/tab3_instance_scannet}
A pretrained network can perform zero-shot classification without fine-tuning by comparing its 3D shape representations to text embeddings of candidate categories. To assess the quality of the learned latent space, we conduct zero-shot classification experiments on ModelNet40-P and ScanObjectNN \cite{scanobjectnn} (OBJ\_ONLY version). ScanObjectNN \cite{scanobjectnn} contains 2,890 real-scanned point clouds in 15 classes, providing a more realistic benchmark than our synthetic ModelNet40-P. 
As summarized in Table \ref{tab:zeroshot}, our method significantly outperforms previous approaches. For SparseConv \cite{sparseconv} and PointBERT \cite{pointbert}, our framework improves their performance by 19.0\% and 17.4\% on ModelNet40-P compared to the best existing results. On ScanObjectNN \cite{scanobjectnn}, OccTIP raises accuracy by 3.8\% and 5.1\%, reaching 61.7\% and 60.6\%, both surpassing the current state-of-the-art OpenDlign \cite{opendlign}. These results highlight our framework's effectiveness in bridging the training-testing domain gap for improved real-world recognition. Furthermore, when combining OccTIP with DuoMamba, accuracy increases by an additional 1.8\%, establishing a new state-of-the-art of 63.5\% on ScanObjectNN \cite{scanobjectnn} and demonstrating DuoMamba's learning prowess in cross-modal representation learning.
\subsection{Few-Shot Linear Probing}
\label{main_exp_fewshot}
To further evaluate the learned embedding space, we conduct few-shot linear probing on ScanObjectNN \cite{scanobjectnn}. Following OpenShape \cite{openshape}, we use a pretrained model to extract features for all test samples and train a linear classifier using only a limited number of labeled instances per class. We report classification accuracy across a range of few-shot settings, specifically with 1, 2, 4, 8, and 16 labeled samples per category. As shown in Table \ref{tab:fewshot}, when trained with OccTIP, PointBERT \cite{pointbert} and SparseConv \cite{sparseconv} consistently achieve better results than existing approaches in nearly all few-shot settings. DuoMamba further enhances performance, attaining the highest accuracy under all configurations.  
This showcases the proposed network's strong learning capacity and highlights our framework's effectiveness in facilitating transferable feature learning, underscoring its applicability in label-scarce scenarios.
\subsection{Real-World Instance Recognition}
\label{subsec:ins_cls}
Following prior work \cite{clip2, tamm}, we test the pretrained models' capability to understand complex objects with the real-world instance recognition task. In this setting, the models have to classify object instances from a scene in a zero-shot manner. Using the same setting as CLIP$^2$ \cite{clip2}, we report results on the popular scene-level ScanNetV2 \cite{scannetv2} dataset. We extract object instances using ground-truth instance masks and classify them with the pretrained models. Table \ref{tab:zeroshot_scannet} summarizes the per-class accuracy and overall class average.

Our method significantly outperforms approaches pretrained on 1.6M real-world text-image-point cloud triplets, including PointCLIP w/TP \cite{pointclip}, CLIP2Point w/TP \cite{clip2point}, and CLIP$^2$ \cite{clip2}. Compared to other ShapeNetCore-based pretraining methods, OccTIP consistently boosts PointBERT \cite{pointbert} and SparseConv \cite{sparseconv} accuracy by 7.3\% and 1.6\%, respectively. When combined with DuoMamba, the class-average accuracy rises by an additional 1.2\%, reaching 49.0\%. These results once again underscore our model’s strong learning capacity and highlight the effectiveness of our pretraining framework for robust feature extraction in real-world 3D shape understanding.
\input{tab/tab4_obj_det}
\subsection{Zero-Shot 3D Object Detection}
\label{subsec:obj_det}
\input{tab/tab5_ablation}
To showcase how our pretrained model can be combined with existing methods to tackle more challenging tasks, we conduct zero-shot 3D object detection experiments on ScanNetV2 \cite{scannetv2} and SUN RGB-D \cite{sunrgbd}. Following the setup in PointCLIP V2 \cite{pointclipv2}, we leverage 3DETR-m \cite{3detr} detector to predict 3D bounding boxes, which enables the extraction of points corresponding to each object instance. Our pretrained 3D network is then applied to classify these object point clouds in a zero-shot manner. Based on 3DETR-m’s localization and our classifier’s semantic predictions, we calculate the mean Average Precision (mAP) at IoU thresholds of 0.25 and 0.5 across 18 object categories in ScanNetV2 and 10 most frequent classes in SUN RGB-D.

As shown in Table \ref{tab:obj_det}, our method achieves mAP$_{25}$ and mAP$_{50}$ scores of 28.9\% and 22.7\% on ScanNetV2, marking significant improvements of 9.9\% and 11.2\% over the depth-based PointCLIP V2 \cite{pointclipv2}. Compared to other point-based methods, we outperform the second-best approach MixCon3D \cite{mixcon3d} by 4.8\% and 3.6\% on mAP$_{25}$ and mAP$_{50}$, respectively. A similar trend is observed on the SUN RGB-D benchmark, where our approach achieves the highest mAP$_{25}$ and mAP$_{50}$ scores of 24.4\% and 13.0\%.
The results again confirm the superiority of our method in learning robust features for recognizing noisy 3D objects in complex scenes, highlighting its strong potential for general 3D open-world learning.

\subsection{Visualization of the Embedding Space}
We further compare the latent spaces of our model with those of existing works. Specifically, we use the pretrained encoders to extract features of ScanObjectNN \cite{scanobjectnn} test instances and employ t-SNE \cite{tsne} for dimensionality reduction. As shown in Figure \ref{fig:tsne}, our method exhibits superior separation and clustering of object classes compared to previous approaches. Note that the pretrained model did not encounter any of these samples during training, yet it successfully captures the characteristics of each category and minimizes overlap between them. This separation indicates that our method’s feature representations are more robust, leading to better real-world zero-shot performance as demonstrated in previous experiments. 
\subsection{Ablation Study}
\label{subsec:ablation}
We conduct ablation studies and report the zero-shot classification accuracy on ScanObjectNN \cite{scanobjectnn} to validate the design of DuoMamba.
We also compare with two existing Mamba-based models to demonstrate the advantages of our proposed architecture.

\vspace{1mm} 
\noindent\textbf{Component Contribution.} We analyze the impact of different components in our DuoMamba block, with results summarized in Table \ref{tab:ablation_component}. In the baseline setting \texttt{(i)}, applying the original Mamba \cite{mamba} to FPS-based ordered sequences yields the lowest accuracy of 60.6\%. Replacing causal 1D convolutions with the standard ones and using either Hilbert \texttt{(ii)} or Trans-Hilbert \texttt{(iii)} ordering consistently improves the performance, with higher accuracy from Hilbert order. Combining both curves with standard 1D convolution as in DuoMamba \texttt{(v)} leads to the best accuracy of 63.5\%. Without the standard 1D convolutions as in \texttt{(iv)}, accuracy drops 0.4\% to 63.1\%. These findings emphasize the importance of integrating geometric structures from both Hilbert curves with standard 1D convolutions for optimal information propagation.

\vspace{1mm} 
\noindent\textbf{The Effect of Scanning Routines.} We further explore the impact of scanning patterns for serializing point clouds and report the results in Table \ref{tab:ablation_curves}. We compare the performance of the default FPS order with three combinations of the widely used Z-order and Hilbert curves \cite{hilbert_curve}. Our results show that combining two variants of Z-order outperforms FPS, and using two Hilbert curves achieves the highest accuracy. This improvement is attributed to the fact that space-filling curves better preserve spatial relationships between point patches, enhancing information flow among nearby tokens in the sequence. Moreover, the superior locality-preserving properties of Hilbert curves over Z-order \cite{point_trans_v3} contribute to a performance boost when used for processing point cloud sequences, as implemented in our DuoMamba block.

\vspace{1mm} 
\noindent\textbf{Mamba-Based Encoder Comparison.} 
To justify the significance of our new architecture, we compare it with two existing Mamba-based models: Mamba3D \cite{mamba3d} and PointMamba \cite{pointmamba}. Table \ref{tab:mamba_comparison} shows that DuoMamba surpasses both models on the ScanObjectNN \cite{scanobjectnn} benchmark, outperforming Mamba3D \cite{mamba3d} by a significant margin of 2.8\% in accuracy. Although DuoMamba has more parameters than PointMamba \cite{pointmamba}, it achieves better performance while also maintaining a lower FLOPs count\footnote{PointMamba's FLOPs is computed when using PyTorch's standard implementation for causal conv1D.}. Overall, the proposed architecture demonstrates a better computation-performance balance than both existing networks.