\section{Pretraining Framework}
\label{sec:occtip}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/tsne.pdf}
    \vspace{-7mm}
    \caption{t-SNE visualization of ScanObjectNN \cite{scanobjectnn} features extracted by different pretraining methods. Compared to other approaches based on complete point clouds, our method OccTIP achieves clearer class separation and significantly reduces overlap between classes.}
    \label{fig:tsne}
    \vspace{-5mm}
\end{figure*}
\vspace{1mm} \noindent\textbf{Triplets Generation.}
Given a 3D model, we first center and normalize it to lie within a unit sphere. Following OpenShape \cite{openshape}, we select 12 camera positions uniformly distributed around the object. They lie on a sphere of radius 2, with four viewpoints above the mesh ($z > 0$), four at the same level ($z = 0$), and four below ($z < 0$) to cover all angles. From each position, we render an RGB image and a depth map using BlenderProc \cite{blenderproc}. We set up the scene with area light and use Blender's ray-tracing render engine `CYCLES' for more realistic output. We then construct a partial point cloud based on the camera position, color information from the RGB image, and geometric information in the depth map. On a single RTX 4090 GPU, it takes around three days to process 52K ShapeNetCore \cite{shapenet} 3D models.

For language modality, we use the captions provided by OpenShape \cite{openshape}, which come from three sources: (1) metadata of the dataset, (2) captions generated by BLIP \cite{blip} and Azure cognitive services, and (3) retrieved captions of visually similar images from LAION-5B dataset \cite{laion_5b}. An illustration of the generation process is shown in Figure \ref{fig:fig2_pretraining_framework}a.

\vspace{1mm} 
\noindent\textbf{Training Pipeline.}
For the $i$-th object, we randomly select the partial point cloud and RGB image corresponding to a viewpoint, along with a text from its available captions, to form a triplet $x_i = (T_i, I_i, P_i)$. Input at each iteration is a batch of $B$ triplets, represented as $\{(T_i, I_i, P_i)\}_{i=1}^B$.

The framework trains a point cloud network \( f^P \) to learn 3D representations that are aligned with the embedding spaces of language and images. To achieve this, we leverage pretrained text and image encoders from CLIP \cite{openclip}, denoted as \( f^T \) and \( f^I \), to generate prior features that serve as anchors in the new co-embedding space. Since CLIP \cite{2dclip} was trained on a large image-text corpus and provides a well-aligned latent space, we freeze the primary CLIP \cite{2dclip} encoders during training. To enable flexible alignment of this shared latent space with the additional 3D modality, we introduce learnable projection heads \( h^I \) and \( h^T \) for image and text inputs. They will be updated jointly with the point cloud model during pretraining. Given an input batch, we extract features for each modality as follows:
\begin{equation}
    z^T_i = h^T \left( f^T(T_i) \right), \;
    z^I_i = h^I \left( f^I(I_i) \right), \;
    z^P_i = f^P(P_i).
\end{equation}

Inspired by MixCon3D \cite{mixcon3d}, we introduce additional mixed representations to enhance contrastive learning constraints. Specifically, we compute a combined embedding from the point cloud and image features as:
\begin{equation}
    z^M_i = h^M \left( \mathrm{Concat}(z^P_i, z^I_i) \right),
\end{equation}
where \( h^M \) is a learnable linear projection mapping the concatenated features to the shared latent space.

Finally, we employ cross-modal contrastive learning to `pull' multi-modal features together. The training objective is to minimize the following total loss:
\begin{equation}
    \mathcal{L} = \mathcal{L}^{P \leftrightarrow I} + \mathcal{L}^{P \leftrightarrow T} + \mathcal{L}^{I \leftrightarrow T} + \mathcal{L}^{M \leftrightarrow T},
\end{equation}
where \( T \), \( I \), \( P \), and \( M \) denote text, image, point cloud, and mixed modalities, with each contrastive loss defined in Equation \ref{eq:contrastive_loss}. Our training pipeline is illustrated in Figure \ref{fig:fig2_pretraining_framework}b.

\section{DuoMamba}
\label{sec:duomamba}
\vspace{1mm} \noindent\textbf{Overview.} Given an input point cloud $P_0 \in \mathbb{R}^{N \times 3}$ (and color information $C_0 \in \mathbb{R}^{N \times 3}$ if available), we first apply Farthest Point Sampling (FPS), similar to previous works \cite{sparseconv,pointbert}, to obtain a set of $S$ center points, denoted as $P \in \mathbb{R}^{S \times 3}$. Next, kNN is applied to form a local patch with $k$ points around each center. These point patches (along with points' color) are then processed by a mini-PointNet \cite{pointnet} to obtain point tokens $E \in \mathbb{R}^{S \times C}$, where $C$ is the dimension of the token embedding space. An encoder composed of \( L \) DuoMamba blocks is employed to propagate information across local patches and capture global features. Finally, the encoder outputs are passed through a linear layer followed by average pooling to produce a single vector \( z^P \in \mathbb{R}^C \), which can be used for cross-modal contrastive learning as described in Section \ref{sec:occtip}. Figure \ref{fig:fig3_proposed_model} illustrates our network. 

\vspace{1mm} \noindent\textbf{DuoMamba Block.} At the core of the proposed architecture is the two-stream DuoMamba block, which leverages Mambaâ€™s linear complexity \cite{mamba} for improved efficiency over the Transformer's quadratic self-attention \cite{pointbert,pointmae,point_trans_v1,pointtransformer}. We introduce two key adaptations to Mamba, originally designed for structured sequence data, to efficiently process point clouds.
\textbf{First}, we use Hilbert and Trans-Hilbert \cite{hilbert_curve} space-filling curves to transform 3D point clouds into 1D sequences. Unlike audio or text, point clouds are essentially sets of unordered 3D coordinates, making them challenging to process with order-aware models like Mamba. By sorting point tokens along Hilbert curves, adjacent patches in the sequence correspond to nearby regions in 3D space, facilitating local information propagation compared to random ordering. Additionally, using two Hilbert variants allows us to capture more diverse spatial relationships, enriching local point interactions \cite{point_trans_v3}. 
\textbf{Second}, we replace the causal 1D convolution used in previous Mamba-based models \cite{vmamba,mamba,pointmamba,mamba3d} with the standard convolution. In tasks like audio and language modeling where data follows a natural order, restricting tokens to attend only to preceding ones can be beneficial \cite{causal_1d_wavenet}. By contrast, for spatial data like point clouds, allowing patches to aggregate information bidirectionally along scanning curves enables them to consider neighbors in every direction, providing a more comprehensive spatial context.

Figure \ref{fig:fig3_proposed_model} illustrates our DuoMamba block, which consists of two parallel streams that extract point features using two S6 modules \cite{mamba}. In each branch, point patches are ordered along the Hilbert or Trans-Hilbert curve, then local relationships are propagated with a 1D convolution. S6 further facilitates information flow between tokens and models long-range dependencies. Finally, two sequences are reordered and combined to produce output. Specifically, the $l$-th block transforms the output $Z_{l-1}^{\text{out}}$ from the previous module as follows:

\vspace{-3mm}
{\footnotesize
\begin{equation}
\begin{aligned}
    & Z_l^{\text{in}} = \mathrm{LayerNorm} \left( Z_{l-1}^{\text{out}} \right), 
    & &Z_l = \mathrm{SiLU} \left( \mathrm{Linear} \left( Z_l^{\text{in}} \right) \right),\\
    & H'_l = \mathrm{HSort} \left( \mathrm{Linear} \left( Z_l^{\text{in}} \right) \right),     
    & &H''_l = \mathrm{SiLU} \left( \mathrm{Conv1D} \left( H'_l \right) \right),\\
    & T'_l = \mathrm{THSort} \left( \mathrm{Linear} \left( Z_l^{\text{in}} \right) \right),
    & &T''_l = \mathrm{SiLU} \left( \mathrm{Conv1D} \left( T'_l \right) \right),\\
    & H_l = \mathrm{Unsort} \left( \mathrm{S6} \left( H''_l \right) \right) \odot Z_l, 
    & &T_l = \mathrm{Unsort} \left( \mathrm{S6} \left( T''_l \right) \right) \odot Z_l,\\
    & Z_l^{\text{out}} = Z_{l-1}^{\text{out}} + \mathrm{Linear} \left( H_l + T_l \right),
\end{aligned}    
\end{equation}
}where $\mathrm{HSort}$ and $\mathrm{THSort}$ represent sorting operations based on Hilbert and Trans-Hilbert curves while $\mathrm{Unsort}$ is the operation that restores the original order.