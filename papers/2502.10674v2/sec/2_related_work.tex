\vspace{-2mm}
\section{Related Work}
\label{sec:related_work}
\noindent\textbf{CLIP for 3D Representation Learning.}
Vision-Language Models (VLMs) such as CLIP \cite{2dclip} and ALIGN \cite{align} have demonstrated impressive zero-shot capabilities through contrastive learning on large image-text corpora. These models effectively map the two modalities into a shared latent space with rich semantic and visual concepts, forming a foundation for various 2D applications \cite{detic,dalle,sam,imagegen}. Recently, several studies have leveraged CLIP for 3D representation learning, showing promising results in object-level zero-shot 3D recognition \cite{ulip,openshape,pointclip,clip2point,clip2,mixcon3d,tamm,opendlign}.

Among them, several works \cite{pointclip,pointclipv2,clip2point,opendlign} project point clouds into depth maps and rely on fine-tuning CLIP image encoders for zero-shot classification. However, they often experience information loss during 3D-to-depth projections, which significantly impacts their performance. In contrast, other methods \cite{clip2,ulip,ulip2,openshape,uni3d,tamm,mixcon3d} train specialized point cloud encoders to distill CLIP knowledge, extending the image-text co-embedding space to encompass 3D representations. These approaches form text-image-point cloud triplets and utilize contrastive learning to align the latent spaces of the three modalities. For instance, CLIP$^2$ \cite{clip2} uses object point clouds and images from real scenes to generate pretraining triplets. However, the quality of the cropped images can vary due to lighting conditions, object size, and occlusion. Also, object descriptions are created from simple prompts, leading to suboptimal transfer of CLIP knowledge and unsatisfactory performance. 
Other works \cite{ulip,ulip2,openshape,uni3d,tamm,mixcon3d} use synthetic 3D models to render RGB images and leverage metadata, image captioning models \cite{blip}, and retrieved texts for diverse descriptions. However, they typically pretrain 3D encoders on complete point clouds, which greatly differ from the real ones encountered in practical conditions due to occlusion and viewpoint limitations. To address this, we propose a framework that uses synthetic 3D models to generate occluded point clouds for pretraining, reducing data discrepancies while maintaining high image quality and caption diversity for effective transfer of CLIP's knowledge.
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig3_duomamba.pdf}
    \vspace{-8mm}
    \caption{Overview of the proposed architecture and detailed design of our DuoMamba block. We integrate two Hilbert curves \cite{hilbert_curve} and standard 1D convolutions with linear-time S6 \cite{mamba} modules to efficiently model geometric dependencies and enrich spatial context. }
    \label{fig:fig3_proposed_model}
    \vspace{-5mm}    
\end{figure*}

\vspace{1mm} \noindent\textbf{Deep Learning-Based Point Cloud Encoders.}
Leveraging deep learning, the pioneering PointNet \cite{pointnet} directly processes point clouds using multi-layer perceptrons applied on each point independently. Subsequent methods \cite{pointnet++,pointnext,dgcnn} introduce hierarchical structures to model local neighborhoods and geometric relationships, addressing PointNet's limitations. Alternatively, convolution-based approaches \cite{voxnet,sparseconvnet} convert point clouds into 3D voxel grids, utilizing established 3D convolutions for feature learning. SparseConv \cite{sparseconvnet} reduces the high memory requirements of 3D convolutions through sparse convolution, enhancing the voxel-based method's applicability. Since the introduction of self-attention in Transformers \cite{transformer}, most state-of-the-art encoders \cite{pointbert,pointmae,point_trans_v2,point_trans_v3} are based on this architecture, with PointBERT \cite{pointbert} being a representative for object-level point cloud pretraining \cite{pointbert,openshape,ulip,ulip2,tamm}. However, the attention mechanism's quadratic complexity results in high computational costs as the input length increases.

To overcome this, Mamba3D \cite{mamba3d} and PointMamba \cite{pointmamba} were developed using the linear-time S6 from Mamba \cite{mamba} as alternatives to attention layers. However, these networks overlook key characteristics of point clouds. Specifically, Mamba3D \cite{mamba3d} applies S6 to point tokens in random order due to the unstructured nature of point clouds, which is not optimal since S6 was designed for sequence data with meaningful order, such as natural language and audio. PointMamba \cite{pointmamba} improves on this by sorting points using Hilbert and Trans-Hilbert curves \cite{hilbert_curve}, ensuring that spatially close points remain adjacent in the sequence. However, it simply concatenates the two resulting orders as input for S6, doubling the sequence length and computations. Moreover, both methods employ causal 1D convolution, which is beneficial for causal data like audio but suboptimal for spatial data.
Therefore, we propose a new Mamba-based architecture that integrates point cloud properties into its design and leverages multi-modal pretraining to enhance model knowledge and extend its applicability.