\section{Introduction}
\label{sec:intro}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig1.png}
    \vspace{-9mm}
    \caption{Comparison to existing methods. (a) State-of-the-art approaches pretrain 3D encoders on complete point clouds, which differ significantly from occluded ones in practical scenarios (top). This leads to a substantial gap in zero-shot performance between ModelNet40 \cite{modelnet40} benchmark with full point clouds and ScanObjectNN \cite{scanobjectnn} with real-world data (bottom). (b) The proposed framework OccTIP pretrains 3D models on partial point clouds to better simulate practical conditions, leading to significant improvements on various recognition tasks, especially when combined with our DuoMamba architecture. (c) Compared to the popular PointBERT \cite{pointbert}, DuoMamba has significantly lower FLOPs (top) and latency (bottom) during inference, making it better suited for real-world applications.}    
    \label{fig:fig1_motivation}
    \vspace{-6mm}
\end{figure*}
3D understanding plays a vital role in robotics \cite{affordance}, virtual reality \cite{semantic_parsing}, and autonomous driving \cite{vehicle_detection}, enabled by deep-learning models that perform recognition tasks such as 3D object classification \cite{pointnet}, object detection \cite{3detr,votenet}, and semantic segmentation \cite{octree_cnn,swan}. However, existing 3D networks \cite{pointnet,pointnet++,3detr,dgcnn,point_trans_v3} are trained using closed-set annotation, constraining them to recognize only pre-defined categories and struggle with `unseen' ones. Inspired by CLIP \cite{2dclip}, recent open-world studies \cite{clip2point,pointclip,pointclipv2,openshape,uni3d,ulip} have extended the aligned image-text latent space to include 3D object representations, allowing generalization beyond `seen' categories and enabling zero-shot 3D recognition.

Existing works in this line of research take 3D-image-text triplets as input and align the three embedding spaces using cross-modal contrastive learning. These methods represent 3D shapes either as depth maps \cite{opendlign,clip2point,pointclip,pointclipv2} or raw point clouds \cite{openshape,tamm,ulip,ulip2,uni3d,mixcon3d}. Depth-based approaches must first convert point clouds into 2D depth maps and use pretrained image encoders, such as Vision Transformer (ViT) \cite{vit}, for 3D feature extractions. However, their performance typically suffers from information loss during the projection and the domain gap caused by differences between RGB and depth images.
On the other hand, point-based methods \cite{ulip,openshape,ulip2,uni3d,mixcon3d,tamm}  can directly exploit all intrinsic geometry in the point clouds. An example is the recent work CLIP$^2$ \cite{clip2}, which pretrains a 3D encoder using real-scanned objects extracted from scene-level point clouds. For contrastive learning, it pairs these with cropped images and simple category-based prompts as text descriptions. 
However, limited caption diversity and poor cropped image quality (due to occlusion, lighting, etc.) hinder CLIP knowledge transfer, leading to suboptimal performance.

Other works \cite{ulip,ulip2,openshape,uni3d,mixcon3d,tamm} instead leverage synthetic 3D models\footnote{In this paper, we use \textit{3D models} to refer to 3D CAD models or 3D meshes instead of 3D deep learning models.} to construct pretraining triplets. These methods uniformly sample points from the mesh surface to create full point clouds\footnote{A \textit{full} (\textit{complete}) point cloud provides 360-degree coverage of an object, while a \textit{partial} (\textit{occluded}) one is captured from a single viewpoint.}. They also render RGB images from preset camera positions and generate diverse captions from multiple sources, allowing for control over the quality of images and texts. As a result, these methods demonstrate promising zero-shot performance on complete point cloud benchmarks such as ModelNet40 \cite{modelnet40}. However, their performance degrades significantly on real-scanned data, leading to unsatisfactory results in practical scenarios.  As shown in Figure \ref{fig:fig1_motivation}a, there is a 20\% accuracy drop from the synthetic ModelNet40 \cite{modelnet40} to the real ScanObjectNN \cite{scanobjectnn}, caused by the large domain gap between complete point clouds in pretraining and occluded ones encountered in real-world conditions.
To address this data discrepancy, we introduce an occlusion-aware pretraining framework that leverages synthetic 3D meshes to create partial point clouds. We simulate real-world scenarios by putting a virtual camera around an object and only sample points visible from the camera position. From 52K ShapeNetCore \cite{shapenet} 3D models, our framework generates nearly 630K occluded point clouds for pretraining and enhances the zero-shot accuracy of SparseConv \cite{sparseconv} and PointBERT \cite{pointbert} by 3.8\% and 5.1\% on ScanObjectNN \cite{scanobjectnn}. Despite using only synthetic objects, our framework consistently improves recognition performance on various real-world tasks and even outperforms methods that use real-scanned data.

\vspace{-1mm}
Moreover, existing multi-modal pretraining approaches \cite{mixcon3d,uni3d,openshape,ulip,ulip2} heavily rely on Transformer-based 3D encoders due to their strong learning capacity. However, these pretrained models have high inference costs because of the attention's quadratic complexity. This poses significant challenges when we want to increase the number of point tokens in the input or use the pretrained encoder as a classification head in a 3D object detector. Inspired by Mamba \cite{mamba}, we introduce an efficient architecture named DuoMamba as an alternative to Transformer-based models. At the core of our network is the two-stream DuoMamba block, developed using linear-time S6 modules from Mamba \cite{mamba}. Each stream processes point tokens in the order from a space-filling curve, either Hilbert \cite{hilbert_curve} or its transposed variant Trans-Hilbert. Intuitively, these turn an unordered point cloud into a geometrically structured sequence where close points in 3D space stay adjacent in the sequence, facilitating S6 to capture meaningful geometric relationships. We also replace causal 1D convolutions commonly used in Mamba models \cite{mamba,vmamba,mamba3d,pointmamba} with standard 1D convolutions to allow point tokens to aggregate information of their neighbors in both directions, enriching their spatial context. 
Compared to the popular Transformer-based PointBERT \cite{pointbert}, our model achieves higher performance across several benchmarks (Figure \ref{fig:fig1_motivation}b) while significantly reducing FLOPs and latency (Figure \ref{fig:fig1_motivation}c). It also exhibits a better performance-computation balance than existing Mamba-based point cloud networks \cite{mamba3d,pointmamba}. In summary, our main contributions are:
\begin{itemize}
\item We propose an occlusion-aware pretraining framework for open-world 3D recognition. By generating partial point clouds from synthetic 3D models, our approach simulates real-world conditions and removes the need for real-scanned data in pretraining.
\item We demonstrate, through extensive experiments, that our framework consistently improves the performance of two popular networks: PointBERT \cite{pointbert} and SparseConv \cite{sparseconv}.
\item We introduce DuoMamba, a two-stream linear-time architecture integrated with space-filling curves and 1D convolutions for efficient point cloud learning. Our network achieves higher accuracy than Transformer-based methods, with reduced computation and lower latency.

\end{itemize}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig2_occtip.pdf}
    \vspace{-10mm}
    \caption{Overview of our OccTIP pretraining framework. (a) Given a 3D object, we generate RGB and depth images from preset camera positions, which are used to construct partial point clouds. Texts are generated from dataset metadata, image captioning models \cite{blip}, and retrieved descriptions of similar photos from LION-5B \cite{laion_5b}. (b) During pretraining, we extract multi-modal features using a learnable point cloud network and frozen CLIP \cite{2dclip} encoders, then align them through contrastive learning.}
    \label{fig:fig2_pretraining_framework}
    \vspace{-5mm}    
\end{figure*}