\clearpage
\setcounter{figure}{4}
\setcounter{table}{5}
\setcounter{section}{7}

\maketitlesupplementary
\section{Additional Discussion on Existing Works}
\paragraph{Discussion on Occlusion Methods.} \citet{modelnetc} simplified occlusion by treating it as a form of corruption, referred to as ``Drop Local,'' where k-NN clusters are randomly removed from point clouds. They then proposed an architecture and an augmentation strategy (based on deforming and mixing objects) to address \textit{general} corruptions rather than focusing on occlusion. \citet{mvtn} introduced a viewpoint prediction module as a component for multi-view 3D recognition (which rely on 3D-to-2D projection). By predicting `good' views to render images from point clouds, indirectly, the recognition model becomes more robust to occlusion (empirically simulated by randomly cropping the object point clouds along canonical directions). In contrast, our OccTIP method more realistically simulates self-occlusion through the rendering process and integrates single-view point clouds during pretraining, improving occlusion robustness for \textit{any} point cloud encoders.

\vspace{-5mm}
\paragraph{Comparison with VisionMamba (Vim).} While Vim \cite{vim} also has a two-stream design, it has two key limitations: (1) reliance on one-directional neighborhood aggregation (CausalConv1D) and (2) only able to utilize a \textit{single} neighborhood structure due to its simple forward and backward scanning strategy. In contrast, DuoMamba uses Conv1D for bidirectional local aggregation and can flexibly process two diverse orderings (e.g., Hilbert, Trans-Hilbert) simultaneously within a single block to fully exploit 3D geometry of the point clouds. These technical enhancements lead to improved performance as shown in Table \ref{tab:rebuttal_vim_duomamba}.
\begin{table}[h!]
\centering
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
    \setlength{\tabcolsep}{6.5pt}  % Default value: 6pt
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|cc|c}
        \toprule
        Dataset & Vim \cite{vim} & Vim \cite{vim} + Hilbert & \cellcolor{gray!20} DuoMamba  \\
        \midrule
        ModelNet40-P &  65.3 & 63.8 & \cellcolor{gray!20}\textbf{67.7} \\ 
        ScanObjectNN &  61.1 & {62.7} & \cellcolor{gray!20}\textbf{63.5} \\
        \bottomrule
    \end{tabular}
    }
\caption{Zero-shot accuracy of Vim and DuoMamba.}
\label{tab:rebuttal_vim_duomamba}
\end{table}

\vspace{-5mm}
\section{Implementation Details}
\paragraph{Triplet Generations.} We render RBG images with a resolution of $512 \times 512$ and a transparent background. Similar to OpenShape \cite{openshape}, descriptions for each object come from three sources: (1) raw texts from the dataset's metadata, (2) captions generated by BLIP \cite{blip} and Azure Cognitive Services, (3) retrieved captions from visually similar images in the LAION-5B \cite{laion_5b} dataset. The first source of captions (created from metadata) includes three texts: (a) object name, (b) object category, and (c) concatenation of the subcategory name. 

\paragraph{Training Details.} During pretraining, we use a batch size of 32 and randomly replace point colors with a constant value of 0.4 with a probability of 0.5. During testing, we assign the same constant value to point clouds that do not have color information, such as those in the ScanObjectNN \cite{scanobjectnn} dataset. For more efficient training, we precompute and cache text and image features from CLIP \cite{2dclip} and directly use them as inputs to the text and image projection heads. Since there is significant fluctuation when training with partial point clouds, we follow \cite{mixcon3d} to employ Exponential Moving Average (EMA) \cite{ema} with a decay factor of 0.9995 to stabilize the training process. We use a cosine learning rate scheduler with a base learning rate of $7\mathrm{e}{-4}$.

\input{tab/supp_scannetv2_obj_det}

\input{tab/supp_sunrgbd_obj_det}

\section{Comparisons with Previous Works Pretrained on Larger Datasets}
We further compare our method (pretrained on 52K ShapeNetCore \cite{shapenet} objects) with previous works pretrained on a significantly larger ensemble of 880K 3D objects from four datasets: ShapeNetCore \cite{shapenet}, ABO \cite{abo}, 3D-FUTURE \cite{3dfuture}, and Objaverse \cite{objaverse}. We use the official results reported in previous papers and evaluate all approaches on the real-world ScanObjectNN \cite{scanobjectnn} dataset to assess their recognition performance in practical scenarios.

\paragraph{Model Size and Zero-Shot Object Classification Performance.} We compare the parameter counts of various point cloud encoders and their zero-shot performance in Figure \ref{fig:supp_zeroshot_comparison}. Despite only being pretrained on ShapeNetCore \cite{shapenet}, our DuoMamba outperforms all existing models of comparable size that are pretrained on 880K 3D objects -- 17 times more data. Notably, the zero-shot accuracy gap between our model and the best-performing model Uni3D-giant \cite{uni3d} is just 1.8\%, even though our model is only 1/35 its size. This highlights DuoMamba's superior size-to-performance efficiency. Scaling up the model and pretraining on larger datasets is likely to further enhance performance, which we leave as future work.

\vspace{-5mm}
\paragraph{Few-Shot Linear Probing.} We perform a few-shot experiment similar to the one in Section \ref{main_exp_fewshot} 
(main paper), this time comparing our approach against models pretrained on the ensemble of 880K 3D objects. As illustrated in Figure \ref{fig:supp_fewshot_comparison}, our method consistently outperforms all other works across all few-shot settings, highlighting our pretraining framework's data efficiency and effectiveness in learning robust and generalizable features for real-world recognition.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/supp_zero_shot_comparison.pdf}
    \vspace{-3mm}
    \caption{\textbf{Comparisons of model size and zero-shot accuracy on ScanObjectNN} \cite{scanobjectnn}.
    Our model is pretrained on 52K ShapeNetCore \cite{shapenet} objects, whereas all other approaches are pretrained on an ensemble of 880K objects from four datasets: Objaverse \cite{objaverse}, ABO \cite{abo}, 3D-FUTURE \cite{3dfuture}, and ShapeNetCore \cite{shapenet}. Despite being pretrained on a less diverse set of objects and having the smallest size, DuoMamba demonstrates competitive performance. Among models with fewer than 50M parameters (DuoMamba, PointBERT \cite{pointbert}, SparseConv \cite{sparseconv}), our model outperforms all others by a significant margin of 3\% in zero-shot accuracy. While Uni3D-giant \cite{uni3d} achieves a slightly higher accuracy with a gap of 1.8\%, it comes at the cost of a substantially larger model size, with 1016.5M parameters -- 35 times the size of DuoMamba. This highlights the optimal balance between model size and performance offered by our method compared to existing approaches.}    
    \label{fig:supp_zeroshot_comparison}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/supp_few_shot.pdf}
    \caption{\textbf{Few-shot linear probing on ScanObjectNN} \cite{scanobjectnn}. Our method is pretrained on 52K ShapeNetCore \cite{shapenet} objects, whereas other models are pretrained on 880K objects. Despite using significantly less data, our framework OccTIP outperforms all existing methods across all few-shot settings, demonstrating the data efficiency and the high-quality latent space learned by our approach.
    }    
    \label{fig:supp_fewshot_comparison}
\vspace{-1mm}
\end{figure}
\vspace{-1cm}
\section{Additional Quantitative Results}
\paragraph{Evaluate Pretrained DuoMamba on ModelNet40.} To evaluate {DuoMamba (pretrained with OccTIP) on complete point clouds,} we generate partial point clouds from 12 views (as in pretraining) and use majority voting for class prediction. Figure \ref{fig:duomamba_modelnet40} shows that on ModelNet40, we perform competitively with previous works pretrained on full point clouds and even \textbf{surpass OpenShape} by 1.3\%. 
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/rebuttal_occtip_fullpcd.png}
    \caption{Comparison with methods pretrained on \textit{complete} point clouds.}    
    \label{fig:duomamba_modelnet40}
\vspace{-1mm}
\end{figure}

\paragraph{Complete Results for Zero-Shot 3D Object Detection.}
\label{sec:supp_complete_obj_det}
The average precision (AP) for each class and the mean Average Precision (mAP) for the zero-shot 3D object detection experiments (Section \ref{subsec:obj_det} in the main paper) are provided in Table \ref{tab:supp_obj_det_scannetv2} (for ScanNetV2 \cite{scannetv2} benchmark) and Table \ref{tab:supp_obj_det_sunrgbd} (for SUN RGB-D \cite{sunrgbd} benchmark). 
Our method OccTIP consistently achieves the best or second-best AP across most categories and achieves the highest mAP, with a significant margin over existing techniques on both datasets. These results highlight the effectiveness of OccTIP and its applicability to complex, real-world recognition tasks.

\paragraph{Pretraining with Complete vs. Partial Point Clouds.} Table \ref{tab:rebuttal_ablation_full_partial} shows that our synthetic partial data consistently improves all models' accuracy on real-world ScanObjectNN, with DuoMamba performing best in both settings.
\begin{table}[h!]
\centering
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
    \setlength{\tabcolsep}{6.5pt}  % Default value: 6pt
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|cc|c}
        \toprule
        Pretraining data & SparseConv & PointBERT & \cellcolor{gray!20}DuoMamba \\
        \midrule
        Complete & 56.0 & 55.5 & \cellcolor{gray!20}\textbf{57.5}  \\
        \rowcolor{gray!20}
        Partial (OccTIP) & 61.7 (+5.7) & 60.6 (+5.1) & \textbf{63.5 (+6.0)} \\ 
        \bottomrule
    \end{tabular}
    }
\caption{ScanObjectNN accuracy when pretraining with full vs partial data.}
\label{tab:rebuttal_ablation_full_partial}
\end{table}

\paragraph{Architecture Influence on Object Detection Performance.} 
Table \ref{tab:rebuttal_obj_det} compares object detection performance of DuoMamba and PointBERT pretrained with OccTIP against PointBERT’s best performance by previous pretraining baselines. OccTIP consistently enhances PointBERT’s performance, and its combination with DuoMamba achieves the best results.
\begin{table}[h!]
\centering
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
    \setlength{\tabcolsep}{6.5pt}  % Default value: 6pt
    \resizebox{0.4\textwidth}{!}{
        \begin{tabular}{c|c|cc|cc}
        \toprule
        Pretraining & \multirow{2}{*}{Encoder} & \multicolumn{2}{c|}{ScanNetV2} & \multicolumn{2}{c}{SUN RGB-D} \\ \cline{3-6} 
        method  &          & mAP$_{25}$ & mAP$_{50}$ & mAP$_{25}$ &  mAP$_{50}$ \\ 
        \midrule
        Best current & PointBERT & 24.1 & 19.1 & 18.9 & 10.0 \\ 
        \rowcolor{gray!20}
        OccTIP &  &  25.4 & 19.3 & 21.9 & 11.7 \\
        \midrule
        \rowcolor{gray!20}
        OccTIP & DuoMamba & \textbf{28.9} & \textbf{22.7} & \textbf{24.4} & \textbf{13.0} \\ 
        \bottomrule
        \end{tabular}
    }
\caption{Detection results of different models and pretraining methods.}
\label{tab:rebuttal_obj_det}    
\end{table}