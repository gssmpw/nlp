\section{Experimental Results and Discussion}

We run our experiments as defined in our Section \ref{evaluationmethod}. We report the results as well as the empirical performance of our models. Through that we will try to answer our Research Questions one by one in this section.

Along with \sln{} we have ran the benchmark for the following models.

% granite-code\:8b-instruct, codegemma\:7b-instruct, deepseek\-coder\-v2, starcoder2, codegeex4, codestral, deepseek-coder\:33b, codellama\:13b, codeqwen\:7b\-chat\-v1.5\-q8\_0, qwen2.5\-coder, gemma2, gemma2:27b, llama3.2, opencoder:8b-instruct-fp16, llama3.3.
\begin{table}[ht]
    \scriptsize % Reduce font size for the table
    \centering
    \setlength{\tabcolsep}{2pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \caption{Comparison of Code and Non-Code Models.}
    \label{tab:model-comparison}
    \begin{tabular}{@{}p{2.5cm}p{1.5cm}p{1.5cm}p{1cm}@{}}
        \toprule
        \textbf{Model Name} & \textbf{Parameters} & \textbf{Quantization} & \textbf{Code Model} \\ 
        \midrule
        granite-code     & 8B    & FP16 & Yes \\
        codegemma        & 7B    & FP16 & Yes \\
        deepseek-coder-v2            & N/A   & N/A  & Yes \\
        starcoder2                   & 15B   & FP16 & Yes \\
        codegeex4                    & 13B   & N/A  & Yes \\
        codestral                    & 7B    & FP16 & Yes \\
        deepseek-coder           & 33B   & N/A  & Yes \\
        codellama~\cite{roziere2023code}                & 13B   & N/A  & Yes \\
        codeqwen  & 7B    & Q8\_0 & Yes \\
        qwen2.5-coder                & 2.5B  & N/A  & Yes \\
        gemma2                       & N/A   & N/A  & Yes \\
        gemma2:27b                   & 27B   & FP16 & Yes \\
        llama3.2                     & 3.2B  & FP16 & No  \\
        opencoder   & 8B    & FP16 & Yes \\
        llama3.3                     & 3.3B  & FP16 & No  \\
        \bottomrule
    \end{tabular}

\end{table}

The models were chosen according to the top 8 models at Hugging Face Big Code Leaderboard~\cite{huggingfaceCodeModels} at the time of this work, and also adding general-purpose models, which are supposed to be better at reasoning.

\subsection{Solidity}

This section presents the evaluation results of various code generation models on the task of repairing vulnerabilities in Solidity smart contracts, specifically focusing on the "Not So Smart Contracts" dataset from the Trail of Bits GitHub repository. This dataset is a collection of intentionally vulnerable Solidity contracts, designed to test the ability of automated tools to detect and repair common security flaws. It contains a diverse set of vulnerabilities, including reentrancy, integer overflow/underflow, access control issues, and timestamp dependence, among others. The dataset has been publicly available for a significant period, raising the possibility that some or all of its contents might be present in the pre-training data of the evaluated models. We analyze the performance of these models based on two key metrics: the number of vulnerabilities fixed and the average inference time, as summarized in Table \ref{tab:model_performance} and Figure \ref{fig:solidityrepair}. We also introduce our framework, \sln{}, and demonstrate its effectiveness in enhancing model performance.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/solidity_result.png}
    \caption{Code Repair: Solidity.}
    \label{fig:solidityrepair}
\end{figure}

\begin{table}[ht]
\centering
\caption{Performance of Code Generation Models on Vulnerability Repair.}
\label{tab:model_performance}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model Name} & \textbf{Vuln. Fixed} & \textbf{Avg. Time (s)} \\
\hline
CodeGeex-4      & 11 & 95.50 \\
\hline
CodeGemma       & \textbf{16} & 96.50 \\
\hline
CodeLlama       & 3  & 243.93 \\
\hline
CodeQuen        & 5  & 141.05 \\
\hline
CodeStral       & 13 & 295.23 \\
\hline
DeepSeekCoder-33b & 1  & 411.75 \\
\hline
DeepSeek-V2     & 9  & \textbf{19.42} \\
\hline
Gemma2-9b       & 13 & 108.30 \\
\hline
Gemma2-27b      & 14 & 304.27 \\
\hline
Granite-Code    & 14 & 90.37 \\
\hline
LLaMA3.2        & 1  & 37.09 \\
\hline
LLaMA3.3-70b    & 13 & 741.10 \\
\hline
OpenCoder*      & 1* & 94* \\
\hline
Qwen-2.5-Code   & 13 & 79.72 \\
\hline
StarCoder2      & 0* & 89.10 \\
\hline
Smartify (Gemma2+CodeGemma) & \textbf{16} & 112.30 \\
\hline
Smartify (Gemma2+LLaMA3.1)  & 14 & 267.80 \\
\hline
\end{tabular}
\end{table}


% The results reveal significant performance disparities among the evaluated models. \textbf{CodeGemma} emerges as a top performer, successfully fixing 16 vulnerabilities with a relatively low average inference time of 96.5 seconds. This suggests that CodeGemma possesses a strong ability to understand and rectify code vulnerabilities while maintaining reasonable efficiency. Our proposed framework, \textbf{Smartify (Gemma2+codegemma)}, achieves comparable performance, also fixing 16 vulnerabilities, albeit with a slightly higher average inference time of 112.3 seconds, likely due to its iterative multi-agent process. \textbf{Gemma2 9b} and \textbf{Gemma2 27b} also demonstrate strong capabilities, fixing 13 and 14 vulnerabilities, respectively. However, the larger Gemma2 27b model exhibits a significantly higher inference time (304.27 seconds) compared to the 9b variant (108.3 seconds), highlighting the trade-off between model size and efficiency. \textbf{Granite-code} performs well, fixing 14 vulnerabilities with an inference time of 90.37 seconds.

The results reveal significant performance disparities among the evaluated models. Among the pre-trained models for Solidity \textbf{CodeGemma} surprisingly emerges as a top performer, successfully fixing 16 vulnerabilities with a relatively low average inference time of 96.5 seconds. This suggests that CodeGemma possesses a strong ability to understand and rectify code vulnerabilities while maintaining reasonable efficiency. However since most of these Solidty smart contracts were part of open githubs repositories, there can be a strong possibility fo these already being part of the pertaining data. Our proposed framework, \textbf{Smartify (Gemma2+CodeGemma)}, achieves comparable performance, also fixing 16 vulnerabilities, albeit with a slightly higher average inference time of 112.3 seconds. This increased time is likely due to its iterative multi-agent process, which enables Smartify to leverage the complementary strengths of Gemma2 and CodeGemma, resulting in robust and reliable fixes. 

While \sln{} here doesn't immediately show any benefits over codegemma here, we can notice that the same \sln{} framework when applied to llama3.1 without any fine-tuning (unlike the \sln{} with codegemma) still gives considerable performance boost over vanilla.

% \textbf{Gemma2 9b} and \textbf{Gemma2 27b} also demonstrate strong capabilities, fixing 13 and 14 vulnerabilities, respectively. However, the larger Gemma2 27b model exhibits a significantly higher inference time (304.27 seconds) compared to the 9b variant (108.3 seconds), highlighting the trade-off between model size and efficiency. \textbf{Granite-code} performs well, fixing 14 vulnerabilities with an inference time of 90.37 seconds, showcasing its competitive performance. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{move_result.png}
    \caption{Move Code Repair.}
    \label{fig:enter-label}
\end{figure}

Conversely, models like \textbf{codellama}, \textbf{codequen}, \textbf{deepseekcoder 33b}, and \textbf{llama3.2} show limited effectiveness, fixing only a small number of vulnerabilities. The poor performance of these models could be attributed to several factors, such as insufficient exposure to Solidity code during pre-training or fine-tuning, or architectures ill-suited for vulnerability repair, which requires a deep understanding of both code syntax and security principles. The exceptionally poor performance of models like \textbf{starcoder2} (marked with an asterisk *), along with incomplete data for \textbf{opencoder}, suggests potential issues with their training data or a fundamental mismatch between their capabilities and the task's demands.  These models might have been trained on an older version of Solidity or different smart contract security practices than those in the Not-So-Smart-Contracts dataset. Moreover, they might prioritize other aspects of code generation, such as code completion, over security-specific tasks like vulnerability repair.

The public availability of the "Not So Smart Contracts" dataset raises the question of data contamination. Many evaluated models, especially those trained on large, public code corpora, might have encountered this dataset during pre-training, potentially inflating their performance. However, since \textbf{CodeGemma} and \textbf{Smartify (Gemma2+codegemma)} were specifically fine-tuned for this task, the issue of data contamination is likely less significant.

% In Solidity code repair, the balance between vulnerabilities fixed and inference time is crucial. \textbf{Smartify (Gemma2+codegemma)} emerges as a strong contender, fixing the most vulnerabilities with a reasonable inference time. \textbf{DeepseekV2} has a remarkably low inference time of 19.42 seconds while fixing 9 vulnerabilities, making it the fastest but significantly less accurate. This might make it suitable where speed is paramount.

% The results highlight the effectiveness of our \textbf{Smartify} framework. Combining \textbf{Gemma2} fine tuned  with \textbf{CodeGemma}, Smartify achieves performance comparable to the best individual model (CodeGemma) which should be expected.However, even with a non-finetuned model like \textbf{Llama 3.1}, Smartify significantly improves performance, fixing 14 vulnerabilities compared to Llama 3.2's single fix. Making it comparable with the much bigger llama3.3 in performance which is significantly slower due to its size and computation complexity. This suggests Smartify's multi-agent architecture and iterative refinement process can enhance even general-purpose language models for code repair. The iterative process, involving an Auditor, Architect, Code Generator, Refiner, and Validator, likely contributes to improved performance by refining the generated code. The best model choice depends on the specific application requirements and the acceptable balance between speed and accuracy. For real-world on-device applications, \textbf{Smartify (Gemma2+codegemma)} is likely the most useful, considering its high accuracy and relatively fast inference time.

\subsection{Move Code Repair}\label{sec:move_code_repair}

\begin{table}[t]
\centering
\caption{Move Vulnerability Repair (Time in seconds).}
\label{tab:vulnerability_repair}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c|c|c|c}
\toprule
Model & \rotatebox{90}{UR} & \rotatebox{90}{IL} & \rotatebox{90}{UB} & \rotatebox{90}{UC} & \rotatebox{90}{UPF} & \rotatebox{90}{UTC} & \rotatebox{90}{Ov} & \rotatebox{90}{PL} & \rotatebox{90}{Time} \\
\midrule
codegeex4 & 6 & 0 & 0 & 6 & 1 & 1 & 1 & 0 & 96 \\
codegemma & 6 & 0 & 10 & 7 & 1 & 1 & 0 & 0 & 97 \\
codellama & 15 & 0 & 1 & 17 & 2 & 2 & 2 & 1 & 244 \\
CodeQwen & 10 & 0 & 1 & 10 & 1 & 1 & 1 & 1 & 141 \\
codestral & 19 & 0 & 1 & 21 & 3 & 3 & 2 & 1 & 295 \\
deepseekcoder 33b & 26 & 0 & 2 & 30 & 4 & 4 & 3 & 1 & 412 \\
deepseekV2 & 2 & 0 & 0 & 2 & 0 & 1 & 0 & 0 & 19 \\
gemma2 9b & 7 & 0 & 1 & 8 & 1 & 1 & 0 & 10 & 108 \\
gemma2 27b & 21 & 0 & 2 & 23 & 3 & 3 & 21 & 11 & 304 \\
granite-code & 6 & 0 & 0 & 6 & 1 & 1 & 1 & 0 & 90 \\
llama3.2 & 3 & 0 & 0 & 3 & 1 & 1 & 0 & 0 & 37 \\
llama3.3 70b & 34 & 0 & 21 & 39 & 5 & 5 & 14 & 31 & 741 \\
opencoder & 7 & 0 & 1 & 8 & 1 & 1 & 0 & 0 & 94* \\
qwen 2.5 code & 5 & 0 & 0 & 6 & 1 & 1 & 1 & 0 & 80 \\
starcoder2 & 6 & 0 & 1 & 7 & 1 & 1 & 0 & 0 & 89 \\
Smartify (Gemma2+codegemma) & 293 & 2 & 16 & 189 & 41 & 48 & 51 & 10 & 112 \\
Smartify (Gemma2+llama3.1) & 97 & 0 & 1 & 90 & 13 & 34 & 12 & 10 & 268 \\
Move Prover~\cite{dill2022fast} & - & 2 & - & - & - & - & 47 & 15 & - \\
MoveLint & - & - & - & - & 19 & 30 & 0 & 0 & -\\
MoveScan~\cite{song2024empirical} & 406 & 2 & 28 & 404 & 52 & 62 & 60 & 15 & -\\
\bottomrule
\end{tabular}
}

\vspace{0.5em} % Add vertical space before the abbreviations

\textbf{Abbreviations:} \textbf{UR}: Unchecked Return; \textbf{IL}: Infinite Loop; \textbf{UB}: Unnecessary Boolean; \textbf{UC}: Unused Constant; \textbf{UPF}: Unused Private Function; \textbf{UTC}: Unnecessary Type Conversion; \textbf{Ov}: Overflow; \textbf{PL}: Precision Loss.
\end{table}

This section analyzes the efficacy of various models in repairing vulnerabilities within Move smart contracts, as detailed in Table \ref{tab:vulnerability_repair}. The evaluation encompasses eight distinct vulnerability categories: Unchecked Return (UR), Infinite Loop (IL), Unnecessary Boolean (UB), Unused Constant (UC), Unused Private Function (UPF), Unnecessary Type Conversion (UTC), Overflow (Ov), and Precision Loss (PL) following the works of Song et al~\cite{song2024empirical}. The metrics presented in the table represent the number of successfully repaired instances for each vulnerability type, with higher values indicating superior performance. The inference time, measured in seconds, is also provided for each model.

The results demonstrate a significant variance in performance across the evaluated models. Notably, the larger language models, such as \textbf{deepseekcoder 33b} and \textbf{llama3.3 70b}, exhibit a relatively higher number of successful repairs across multiple categories, albeit with a corresponding increase in inference time. Conversely, smaller models like \textbf{deepseekV2} and \textbf{llama3.2} demonstrate limited repair capabilities.  The specialized tools for Move code, namely \textbf{Move Prover}, \textbf{MoveLint}, and \textbf{MoveScan}, were employed as a benchmark for comparison. It is crucial to note that these tools are designed for vulnerability \textbf{detection} rather than repair. \textbf{MoveScan}, in particular, identified a substantial number of instances across all categories, highlighting its effectiveness as a static analysis tool. \textbf{Move Prover} demonstrated proficiency in detecting Overflow and Precision Loss vulnerabilities, while \textbf{MoveLint} focused on Unused Private Functions and Unnecessary Type Conversions.

The Smartify models, which leverage a combination of \textbf{Gemma2} with either \textbf{codegemma} or \textbf{llama3.1}, present an interesting case. Smartify (\textbf{Gemma2+codegemma}) and Smartify (\textbf{Gemma2+llama3.1}) outperform several individual models in multiple categories. This is likely because the specialized models are fine-tuned on the Move-specific dataset. For instance, Smartify (\textbf{Gemma2+codegemma}) achieves the highest number of repairs for the Unchecked Return, Infinite Loop, Unused Boolean, Unused Constant, Unused Private Function, Unnecessary Type Conversion, and Overflow categories, showcasing a substantial improvement over individual models in these areas. However, it is worth mentioning that they also have limitations compared to individual models for certain categories like Precision Loss.

% The results underscore the effectiveness of the \textbf{Smartify} framework in automated Move code vulnerability detection and repair, demonstrating the potential of model combination to enhance performance. It showcases the effectiveness of the proposed architecture. 
This answers our first two research questions.

\begin{tcolorbox}[
  colback=green!15, % Light green background
  colframe=green!40, % Medium green border
  title=RQ1 \& RQ2 - Code Understanding and Vuln. Detection,
  coltitle=black, % Set title color to black
  fonttitle=\bfseries,
  boxrule=0.75mm, % Thicker border for emphasis
  rounded corners, % Smooth corners for aesthetics
  left=1mm, % Small padding on the left
  right=1mm, % Small padding on the right
  top=1mm, % Small padding on the top
  bottom=1mm % Small padding on the bottom
]
\textbf{Yes.} Our empirical analysis with \sln{}, especially with using a fine-tuned code-gemma and also using a vanilla pre-trained llama3.1, has shown us the effectiveness of the framework's ability to understand code. And to capture bad practices leading to vulnerability. Especially for a low-resource code like move. Without significant fine-tuning (in the case of llama3.1).
\end{tcolorbox}

Notably, \textbf{Smartify (Gemma2+codegemma)}, combining fine-tuned \textbf{Gemma2} with \textbf{CodeGemma}, achieves performance on par with the best individual model, \textbf{CodeGemma}, which is expected due to one of the models being fine-tuned. This highlights the advantages of strategically combining specialized models, answering our next research question.

\begin{tcolorbox}[
  colback=green!15, % Light gray background
  colframe=green!40, % Medium gray border
  title=RQ3 - Code Repair,
  fonttitle=\bfseries,
    coltitle=black, % Set title color to black
  boxrule=0.75mm, % Thicker border for emphasis
  rounded corners, % Smooth corners for aesthetics
  left=1mm, % Small padding on the left
  right=1mm, % Small padding on the right
  top=1mm, % Small padding on the top
  bottom=1mm % Small padding on the bottom
]
Both for solidity and move, we were able to compare the efficacy of our framework with prior works and can see \sln{} outperforms all of the existing code models, even very specialized code models trained on move (opencoder~\cite{huang2024opencoder}) in generating repair codes for detected vulnerabilities. 
\end{tcolorbox}

Furthermore, Smartify's efficacy extends even when integrating a non-finetuned model like \textbf{Llama 3.1}. Smartify significantly outperforms \textbf{Llama 3.2} by fixing 14 vulnerabilities compared to Llama 3.2's single fix, making its performance comparable with the much larger and computationally intensive \textbf{Llama 3.3 70b}. This demonstrates that Smartify's architecture can enhance even general-purpose language models for code repair, offering a balance between speed and accuracy. Answering our last query:

\begin{tcolorbox}[
  colback=green!15, % Light gray background
  colframe=green!40, % Medium gray border
  title=RQ4 - Generalization,
  fonttitle=\bfseries,
    coltitle=black, % Set title color to black
  boxrule=0.75mm, % Thicker border for emphasis
  rounded corners, % Smooth corners for aesthetics
  left=1mm, % Small padding on the left
  right=1mm, % Small padding on the right
  top=1mm, % Small padding on the top
  bottom=1mm % Small padding on the bottom
]
Our implementation of \sln{} with both fine-tuned code-gemma and llama3.1 as the second agent gave us the opportunity to run our experiments on both sets of LLMs. And the results show that \sln{} is able to significantly boost performance even on non-finetuned models compared to a single model.
\end{tcolorbox}

Comparative analysis reveals trade-offs between model scale and performance in automated code repair. Larger models, such as \textbf{deepseekcoder 33b} and \textbf{Llama 3.3 70b}, exhibit broader repair capabilities but incur higher computational costs and inference times. Conversely, the \textbf{Gemma2 27b} model demonstrates notable proficiency in addressing Overflow vulnerabilities, albeit with limitations in handling Unnecessary Boolean and Unused Constant compared to \textbf{Llama 3.3 70b}. While \textbf{Llama 3.3 70b} outperforms Smartify in overall repair capability, its significantly slower inference speed poses a challenge for practical deployment. Therefore, for real-world, on-device applications, \textbf{Smartify (Gemma2+codegemma)} presents a compelling solution with its balance of strong accuracy and rapid inference.

\begin{center}
\fcolorbox{blue!20}{white!90!blue}{%  Box with specified colors
    \parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{ % Adjust width for box borders
\textbf{Insight:} Specialized code models like Starcoder~\cite{lozhkov2024starcoder}, Opencoder~\cite{huang2024opencoder} and deepseekcoder~\cite{guo2024deepseek} doesn't necessarily work well even if it's a coding specific task. While codemodels like codegemma~\cite{team2024codegemma} and codellama~\cite{roziere2023code} are much better at understanding instructions and working on code. This helped \sln{} for its understanding and fine-tuning for code repairability.
    }
}
\end{center}

Specialized static analysis tools for Move, including \textbf{Move Prover}, \textbf{MoveLint}, and \textbf{MoveScan}, work as baselines of detecting move vulnerabilities with which we compare our \sln{} and other LLMs. These findings underscore the need for targeted model improvements. The Smartify framework directly addresses these deficiencies, offering enhanced vulnerability repair effectiveness. 

This research also opens up future research directions of the use of this framework for context-aware test case generation.