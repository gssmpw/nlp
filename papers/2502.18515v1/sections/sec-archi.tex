\section{\sln{} System Architecture and Workflow}

\sln{} operates through a five-agent system designed for automated smart contract vulnerability detection and repair. The system functions as shown in Figure~\ref{fig:archi}.

The Smartify system operates in a five-phase process to automatically repair smart contract code.  Firstly, in the Input \& Initial Audit phase, the smart contract code, written in either Solidity or Move, is fed into the system. The Auditor, an LLM based on Gemma2 9B, analyzes the code to detect potential vulnerabilities and produces a report detailing its findings.  Secondly, during Repair Planning, the Architect receives this vulnerability report and formulates a high-level repair plan that outlines the necessary code modifications to address the identified issues. Thirdly, in Code Generation \& Refinement, an LLM called CodeGemma which has been fine-tuned for code generation, and is equipped with Retrieval-Augmented Generation (RAG) capabilities, takes the lead. It utilizes separate Move RAG and Solidity RAG components to provide language-specific context. The Code Generator, part of CodeGemma, uses the repair plan to generate the modified code, selecting the appropriate RAG based on the input language and having the capability to perform Solidity to Move translation when necessary. Subsequently, a Self Refinement process is initiated, and the Refiner component iteratively improves the generated code's quality, readability, and efficiency.  Fourthly, in the Validation phase, the Validator (which is the same agent as the Auditor) performs a final security audit on the refined code to ensure that all identified vulnerabilities have been resolved. Finally, the system outputs the repaired smart contract code.

% \begin{algorithm}[H]
% \caption{Smartify: Automated Smart Contract Repair}
% \label{alg:smartify}
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} Smart contract code $C$ (Solidity or Move)
% \STATE \textbf{Output:} Repaired smart contract code $C'$

% \STATE \textbf{Procedure} Smartify($C$)
%     \STATE // \textbf{Phase 1: Initial Audit}
%     \STATE $V \gets $ Auditor($C$) \COMMENT{Auditor: LLM1 (Gemma2 9B)} 
%     \STATE \COMMENT{$V$: List of detected vulnerabilities}
    
%     \STATE // \textbf{Phase 2: Repair Planning}
%     \STATE $P \gets $ Architect($V$) \COMMENT{Architect}
%     \STATE \COMMENT{$P$: High-level repair plan}

%     \STATE // \textbf{Phase 3: Code Generation and Refinement}
%     \STATE $L \gets $ DetermineLanguage($C$) \COMMENT{Determine if $C$ is Solidity or Move}
%     \IF{$L$ = Solidity}
%       \STATE $R \gets $ SolidityRAG()
%     \ELSE
%       \STATE $R \gets $ MoveRAG()
%     \ENDIF
    
%     \STATE $C_{gen} \gets $ CodeGenerator($P, R$) \COMMENT{CodeGenerator: LLM2 (FT CodeGemma), utilizes RAG}
%     \STATE $C' \gets $ Refiner($C_{gen}$) \COMMENT{Refiner: Iterative code improvement}
    
%     \STATE // \textbf{Phase 4: Validation}
%     \STATE $V' \gets $ Validator($C'$) \COMMENT{Validator: same agent as Auditor}
%     \IF{$V' = \emptyset$} \COMMENT{No vulnerabilities detected}
%         \STATE \textbf{return} $C'$
%     \ELSE
%         \STATE \textbf{goto} Step 4 \COMMENT{Repeat the process if new vulnerabilities are found}
%     \ENDIF
% \end{algorithmic}
% \end{algorithm}

The process may iterate back to step 3 or 4 if the Validator identifies any issues. Each step plays a vital role in ensuring the accurate and secure repair of smart contract code. The workflow is designed to be efficient and effective, leveraging the strengths of each agent to achieve the desired outcome.

\subsection{Agent Prompting Strategy}

The agents within \sln{} are driven by carefully crafted prompts that guide their actions and ensure consistent performance. We employ a standardized prompt template, adapted from established practices in LLM-based agent systems. The template is structured as follows:

\begin{tcolorbox}[
  colback=gray!10, % Light gray background
  colframe=gray!50, % Medium gray border
  title=Prompt Template,
  fonttitle=\bfseries,
  boxrule=0.75mm, % Thicker border for emphasis
  rounded corners, % Smooth corners for aesthetics
  left=1mm, % Small padding on the left
  right=1mm, % Small padding on the right
  top=1mm, % Small padding on the top
  bottom=1mm, % Small padding on the bottom
  label=box:prompttemplate
]
\textbf{Role:} You are a \texttt{[role]} specializing in \texttt{[Solidity/Move]} smart contracts.

\textbf{Task:} \texttt{[task]}

\textbf{Instruction:} Based on the provided Context, please follow these steps: \texttt{[numbered steps]}

\textbf{Context:} 
\end{tcolorbox}

This template is broken down into the following components.
% \begin{itemize}
%     \item \textbf{Role:} Specifies the agent's role (e.g., Auditor, Architect, Code Generator, Refiner, Validator).
%     \item \textbf{Task:} Describes the specific task the agent is expected to perform (e.g., "Identify vulnerabilities," "Generate repair plan," "Generate code").
%     \item \textbf{Instruction:} Provides a detailed, step-by-step guide on how to accomplish the task. This section leverages chain-of-thought reasoning to guide the agent's actions and decision-making process.
%     \item \textbf{Context:} Contains all the necessary information for the agent to perform its task. This may include the input code, audit reports, architectural plans, code examples from the RAG datastore, and the conversation history between agents.
% \end{itemize}

Each agent in our framework is defined by four key components: the Role, which designates the agent’s specific function (such as Auditor, Architect, or Code Generator); the Task, which outlines the agent’s specific objectives; the Instruction, which provides detailed step-by-step guidance using chain-of-thought reasoning; and the Context, which encompasses all necessary information including input code, audit reports, architectural plans, RAG datastore examples, and inter-agent conversation history.

Table \ref{tab:agent-prompts} shows how this template is adapted for each agent.

\begin{table}[ht]
\centering
\caption{Agent Prompts for Smart Contract Repair.}
\label{tab:agent-prompts}
\renewcommand{\arraystretch}{1.2} % Adjust row height for readability
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\begin{tabularx}{\linewidth}{|p{1.5cm}|X|X|p{2.5cm}|}
\hline
\textbf{Role} & \textbf{Task} & \textbf{Instruction} & \textbf{Context} \\ \hline
Auditor & Identify vulnerabilities and unsafe patterns in Solidity/Move code. & Analyze the code for security vulnerabilities and generate a detailed report. & Input smart contract code (Solidity/Move). \\ \hline
Architect & Create a high-level plan to address vulnerabilities identified by the Auditor. & Review the Auditor's report and develop a plan outlining necessary modifications. & Auditor's report. \\ \hline
Code Generator & Generate Repaired Solidity/Move code based on the Architect's plan and RAG examples. & Consult the Architect's plan, retrieve examples from the RAG datastore, and generate repaired code. & Architect's plan, Solidity/Move code examples from RAG. \\ \hline
Refiner & Iteratively refine the generated code to improve quality and efficiency. & Review the generated code, identify areas for improvement, and refine accordingly. & Generated code, previous iteration code (if any). \\ \hline
Validator & Perform a final security check on the repaired code. & Analyze the repaired code for vulnerabilities, verify issue resolution, and ensure no new vulnerabilities. & Repaired smart contract code. \\ \hline
\end{tabularx}
\end{table}

\subsection{Hardware and Model Fine-tuning}

The development and deployment of \sln{} leveraged a heterogeneous compute environment, utilizing both high-performance GPUs for computationally intensive tasks and a more resource-efficient setup for inference.

\subsubsection{Fine-tuning Setup}

% \begin{itemize}
%     \item \textbf{Hardware:} Fine-tuning the Auditor agent leveraged a cluster of \textbf{four NVIDIA A100 GPUs} to handle the computational demands of learning complex patterns in Solidity and Move code.
%     \item \textbf{Model:} The Auditor is based on the \textbf{Gemma 9B model}, chosen for its strong performance on code-related tasks and adaptability to fine-tuning, particularly in following instructions. It was fine-tuned on a comprehensive dataset of Solidity and Move code, vulnerability examples, best practices, and documentation. The dataset was further augmented with outputs from earlier pipeline stages to enhance the Auditor's safety issue detection capabilities.
%     \item \textbf{Training Recipe:} A supervised learning paradigm was employed. The model was trained to predict correct outputs (e.g., vulnerability reports, safe code patterns) from inputs (e.g., Solidity/Move code, vulnerability descriptions).
%     \begin{itemize}
%         \item \textbf{Data Preprocessing:} Tokenization, normalization, and input-output pair creation ensured data consistency and quality.
%         \item \textbf{Hyperparameter Optimization:} Learning rate, batch size, and training epochs were optimized via grid search and manual tuning. The learning rate was set to 1e-5, batch size to 8 (due to memory constraints), and training ran for 5 epochs (as validation loss plateaued).
%         \item \textbf{Regularization:} Dropout and weight decay were used to prevent overfitting and improve generalization.
%         \item \textbf{Evaluation Metrics:} Accuracy, precision, recall, and F1-score, computed on a held-out validation set, monitored model performance during training.
%     \end{itemize}
% \end{itemize}

\begin{itemize}[leftmargin=*]
    \item \textbf{Hardware:} Fine-tuning leveraged a cluster of \textbf{four NVIDIA A100 GPUs} for computationally demanding pattern learning in Solidity and Move code.
    \item \textbf{Model:} Based on the \textbf{Gemma 9B model}, selected for strong code-related task performance and fine-tuning adaptability, particularly in instruction following. Fine-tuned on a dataset of Solidity and Move code, vulnerability examples, best practices, and documentation, augmented with outputs from earlier pipeline stages to enhance safety issue detection.
    \item \textbf{Training Recipe:} Supervised learning paradigm. Trained to predict correct outputs (e.g., vulnerability reports, safe code patterns) from inputs (e.g., Solidity/Move code, vulnerability descriptions).
    \begin{itemize}
        \item \textbf{Data Preprocessing:} Tokenization, normalization, and input-output pair creation ensured data consistency and quality.
        \item \textbf{Hyperparameter Optimization:} Learning rate (1e-5), batch size (8, due to memory constraints), and training epochs (5, as validation loss plateaued) optimized via grid search and manual tuning.
        \item \textbf{Regularization:} Dropout and weight decay used to prevent overfitting and improve generalization.
        \item \textbf{Evaluation Metrics:} Accuracy, precision, recall, and F1-score on a held-out validation set monitored model performance.
    \end{itemize}
\end{itemize}

\subsubsection{Inference Setup}

\begin{itemize}[leftmargin=*]
    \item \textbf{Hardware:} Inference was performed on a single \textbf{NVIDIA RTX 4090 GPU}, balancing performance and cost-effectiveness for real-time code repair.
    \item \textbf{Models:}
    \begin{itemize}
        \item \textbf{Code Generator and Refiner:} These agents utilize a fine-tuned \textbf{CodeGemma} model, initially pre-trained on a limited Move corpus and further instruction-tuned to follow Architect-generated "recipe" patterns. Fine-tuning on Architect outputs ensured it understood these instructions, and pre-training on a limited Move corpus ensured basic syntax understanding.
        \item \textbf{Comparison Model:} A stock \textbf{Llama 3.1} model was used in some experiments for comparative analysis, helping assess the gains from fine-tuning and instruction tuning.
    \end{itemize}
\end{itemize}

\subsubsection{Key Considerations}

\begin{itemize}[leftmargin=*]
    \item A balance between performance requirements, resource availability, and cost considerations drove the choice of hardware and models.
    \item The fine-tuning process for the Auditor was particularly resource-intensive due to the complexity of the task and the size of the model.
    \item The use of a smaller, more efficient GPU for inference makes the system more accessible for practical deployment.
    \item The comparison with a stock Llama 3 model provides valuable insights into the effectiveness of our fine-tuning and instruction-tuning strategies.
\end{itemize}

This heterogeneous setup, combining high-performance GPUs for training and a more efficient GPU for inference, allows \sln{} to effectively address the computational demands of both model development and deployment. The detailed description of the fine-tuning process provides transparency and allows for replication of our results.