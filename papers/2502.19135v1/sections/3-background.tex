\subsection{Task Planning}

% \noindent{\textbf{Classical Task Planning.}
\paragraph{Classical Task Planning}
\noindent A \emph{(STRIPS) classical planning problem} is defined as a tuple $CP = (F, A, I, G)$, where $F$ is the set of fluents, $A$ is a finite set of actions, $I \subseteq F$ is the initial state, and $G \subseteq F$ is the goal condition.  
Intuitively, a fluent is a predicate expressing a condition (e.g., on the system's state) that can evolve over time as a consequence of actions. For example, a fluent can be a predicate such as {\tt at(robot, RoomA)}, with an obvious interpretation.  
%
A \emph{literal} is defined as a fluent or its negation.  
%
In a classical planning problem,  each action $a \in A$ consists of preconditions denoted $\pc{a}$, and effects denoted $\eff{a}$, both of which are sets of literals.  
%
An action is \emph{executable} if all the fluents in $\pc{a}$ are present in the current state. Once executed, the state is updated according to $\eff{a}$. The effects $\eff{a}$ consist of a set of fluents in the form of $\texttt{add}(l)$ or $\texttt{del}(l)$, where the first adds the fluent $l$ to the current state and the latter removes it.  
%
A literal $l(x)$ may depend on a set of variables $x$, which are fluents: $x \in F$. For instance, \texttt{available(A)} indicates whether a generic \texttt{A} is available or not, but the value of \texttt{A} is not grounded to a specific instance.  
%
For an action $a\in A$ with preconditions $\pc{a}$ and effects $\eff{a}$, \emph{the actionâ€™s variables}, denoted $\fl{a}$, are defined as the set of all variables appearing in the literals of $\pc{a} \cup \eff{a}$.  
%
For example, if the preconditions for an action $a$ are $\pc{a}=[l_1(A), l_2(B), l_3]$, then the set of its variables is $\fl{a}=\{A, B\}$. This later allows us to identify the resources on which the action $a$ depends.  
%
Consider the action \texttt{move(robot, RoomA, RoomB)} as a conclusive example. It requires, as a precondition, that the fluent {\tt at(robot, RoomA)} be present in the system state. Its effects are given by \texttt{del}({\tt at(robot, RoomA)}) and \texttt{add}({\tt at(robot, RoomB)}).  
A \emph{classical plan} $\pi = (a_1, \dots, a_n)$ is a sequence of actions. It is said to be \emph{valid} if and only if it is executable from the initial state and results in a final state that satisfies the goal $G$~\cite{ghallab03}. As an important remark, in classical planning actions have a null duration and are for
this reason called \emph{snap actions}

% \noindent
%     {\textbf{Temporal task planning.}
\paragraph{Temporal task planning} 
\noindent Following \cite{DBLP:conf/ijcai/CushingKMW07,DBLP:journals/ai/ColesFHLS09,DBLP:journals/jair/ColesCFL12}, a \emph{temporal planning problem} is defined as a tuple $TP = (F, DA, I, G)$, with $F$, $I$ and $G$ as in the classical planning problem and with $DA$ being a set of \emph{durative actions}. A durative action $\alpha \in DA$ is given by
%
\begin{enumerate}[label=\roman*)]
    \item two classical planning actions $\aStart{a}$ and $\aEnd{a}$ (i.e., the start actions and the end action);
    \item an overall condition $overall(\alpha)$ that is true for the duration of the action;
    \item and a duration $\delta(\alpha)\in\mathbb{R}^+$ in the interval $[\delta_{min}(\alpha), \delta_{max}(\alpha)], \delta_{min}(\alpha) \le \delta_{max}(\alpha)$.
\end{enumerate}
%
In the definition of a durable action $\alpha$,  $\aStart{a}$ and $\aEnd{a}$ are snap actions and are used, respectively, to set the conditions for action $\alpha$ to start and to finalise its effects when the action is completed. We will consistently use Greek letters (e.g., $\alpha$) with reference to durable actions and latin letters (e.g., $a$) with reference to snap actions. 
A \emph{temporal plan} $\pi = \{tta_1, \cdots, tta_n\}$ is a set of time-triggered temporal actions $tta_i$, where each $tta_i$ is a tuple $\langle t_i, \alpha_i, \delta_i \rangle$ where $t_i \in \mathbb{R}^+$ is the starting time, $\alpha_i \in DA$ is a durable action, and $\delta_i$ is its duration.
%
$\pi$ is said to be a \emph{valid temporal plan} if and only if it can be simulated, i.e., it can be executed, meaning that starting from the initial state, we apply each time-triggered action and at the end of the simulation, we obtain a state fulfilling the goal condition~\cite{pddl31,DBLP:conf/aips/BentonCC12,DBLP:conf/ijcai/CushingKMW07,DBLP:journals/ai/ColesFHLS09,DBLP:journals/jair/ColesCFL12}. 
%\enrcom{MR can you please check if this is enough for the executable part?}
%
% For lack of space, we refer the reader to \cite{pddltwoone} for a thorough discussion on the semantics and definition of a \emph{valid temporal plan}.

State-space temporal planning is a specific approach to temporal planning. The intuition behind this approach is to combine
\begin{enumerate*}[label=\roman*)]
\item a classical forward state-space search to generate a candidate plan outline; and 
\item a temporal reasoner to check its temporal feasibility~\cite{DBLP:conf/ijcai/CushingKMW07,DBLP:journals/ai/ColesFHLS09,DBLP:journals/jair/ColesCFL12}.
\end{enumerate*}
%
By considering the durative actions $\alpha\in DA$ as the start and end snap actions, one can generate an abstract classical problem, which is then solved using any state-space search. 
%
The search extracts a classical plan and then checks if the associated temporal network is consistent; then a time-triggered plan can be computed, and the search stops once a solution has been found.
%
Otherwise, the search continues by computing another classical plan until either the search proves that the problem has no solution or the search bumps into a temporally consistent plan.

% \paragraph{Mapped Temporal Task planning}
% \noindent Our approach is founded on an extension of the temporal task planning problem, which we define Mapped Temporal Task Planning. The problem is defined by the
% tuple $TP=(F, DA, I, G, K, M)$, where:
% \begin{itemize}
%     \item $F$ is the set of all the fluents;
%     \item $DA = DA_H \cup DA_L$ is the set of durative actions ($DA_L \cap DA_H = \emptyset$), distinguished in high-level ($DA_H$), and low-level temporal actions ($DA_L$);
%     \item $I\subseteq F$ is the initial state;
%     \item $G\subseteq F$ is the final state;
%     \item $K\subseteq F$ is the set of grounding predicates;
%     \item $M$ is the set of the mappings.
% \end{itemize}
% The definition of the mapped temporal planning problem differs from the standard definitin of temporal planning in three aspects.
% First, we introduce the set $K\subseteq F$, which represents grounding knowledge, i.e., knowledge that is not supposed to change during the execution of the plan. For example, a grounded predicate can be \texttt{HasManipulator(Agent1)}, meaning that Agent 1 is endowed with a manipulator.
% Second, we distinguish between high-level action $DA_H$ and low-level actions $DA_L$. Intuitively, the former represent high-level tasks (e.g., \texttt{moveBlock(BlockA, LocA, LocB)}), while the latter represent the low-level actions
% needed to implement the task (e.g., \texttt{grasp(Arm1, BLock)}).

% Finally, we introduce the mappings $M$. Define by the notation $\mathcal{P}(DA_L)$ the set of all possible sequences of low-level actions, and let
% $DA_H \times\mathcal{P}(DA_L)$ represent the cartesian product between high
% level actions and sequences of low-level actions.
% %% Given a high-level durative action $\alpha \in DA_H$, a mapping $m$ is a sequences $\langle \alpha_1, ..., \alpha_N \rangle$ of durative lower-level actions $\alpha_i \in DA_L$ of which
% A high-level action $\alpha$ is said an abstraction of the sequence $\langle \alpha_1, ..., \alpha_N \rangle$ if the following property holds: if $\alpha \in DA_H$ is executable in a state $s_s$ and results in a state $s_d$, then given two states $s_s'$ and $s_d'$ such that $s_s \subseteq s_s'$ and $s_d \subseteq s_d'$ we have that the sequence $\langle \alpha_1, ..., \alpha_N \rangle$ is executable in $s_s'$ and leads to $s_d'$.
% The set of the mappings $M$ is defined as the subset of $DA_H \times\mathcal{P}(DA_L)$ made of the elements $\left(\alpha, \langle \alpha_1, ..., \alpha_N \rangle \right)$ such that $\alpha$ is an abstraction of  $\left(\alpha, \langle \alpha_1, ..., \alpha_N \rangle \right)$. In formal terms:
% \[ 
% \begin{array}{r}
% M = \{ (\alpha, \langle \alpha_1, ..., \alpha_N \rangle) | \alpha \in DA_H, \forall i, \alpha_i \in DA_L, N \ge 1, \alpha \\
% \quad \text{ is an abstraction for } \langle \alpha_1, ..., \alpha_N \rangle\}
% \end{array}.
% \]
% Given a generic element $m = (\alpha, \langle \alpha_1, ..., \alpha_N \rangle) \in M$, we denote by $m(\alpha)$ the sequence $\langle \alpha_1, ..., \alpha_N \rangle$

% For example, consider a high-level action $\alpha_i$ which is associated with the sequence of lowe-level actions $\alpha_j, \alpha_k, i\notin \{j, k\}$ with the mapping $m(\alpha_i) = (\alpha_j, \alpha_k)$. A mapping can also be written in terms of snap actions $m(\aStart{\alpha_i}) = (\aStart{\alpha_j}, \aEnd{\alpha_j}, \aStart{\alpha_k},$ $\aEnd{\alpha_k})$ indicating that the mapping should be applied only when $\alpha_i$ has started. Moreover, $\alpha_i$ should terminate only when all actions in the mapping have ended. 

% %% It is possible to reduce a mapped temporal planning problem into a standar temporal plannning problem by: 1. considering $K$ as part of $I$ and $G$, 2. eliminating the distinction bewteen high-level and low-level actions.

% As for standard temporal planning, a feasible plan for mapped temporal planning is a set of durative actions such that, by simulation, the goal state is reached from the initial one, i.e., by applying the durative actions to the initial state in the correct order, we end up in a state which fulfils the final conditions.

% %\end{definition}
% To simplify the terminology, we will henceforth use the term ``action'' meaning ``durative actions'', unless otherwise specified.
\paragraph{Mapped Temporal Task Planning}
\noindent Our approach extends the standard temporal task planning problem, introducing a novel formalization that we define as \textit{Mapped Temporal Task Planning}. This problem is characterized by the tuple $TP=(F, DA, I, G, K, M)$, where:
\begin{itemize}
    \item $F$ is the set of all fluents;
    \item $DA = DA_H \cup DA_L$ is the set of durative actions, distinguished in high-level ($DA_H$) and low-level temporal actions ($DA_L$), with $DA_L \cap DA_H = \emptyset$;
    \item $I\subseteq F$ is the initial state;
    \item $G\subseteq F$ is the final state;
    \item $K\subseteq F$ represents the set of grounding predicates;
    \item $\displaystyle M \subseteq \bigcup_{i=1..N} DA_H \times DA_L^i$ with $N \ge 1$ denotes the set of mappings.
\end{itemize}

This formulation differs from standard temporal planning in three key aspects:

\begin{enumerate}
    \item We introduce the set $K\subseteq F$, which encapsulates \textit{grounding knowledge}, i.e., information assumed to remain invariant during plan execution. For instance, a grounded predicate such as \texttt{HasManipulator(Agent1)} signifies that \texttt{Agent1} is endowed with a manipulator.
    
    \item We distinguish between high-level actions ($DA_H$) and low-level actions ($DA_L$). Conceptually, high-level actions represent abstract tasks (e.g., \texttt{moveBlock(BlockA, LocA, LocB)}), whereas low-level actions encode the concrete steps required to execute these tasks (e.g., \texttt{grasp(Arm1, Block)}).
    
    \item We introduce the mappings $\displaystyle M \subseteq \bigcup_{i=1..N} DA_H \times DA_L^i$ with $N \ge 1$. 
    %Let $\mathcal{P}(DA_L)$ denote the set of all possible sequences of low-level actions, and let the $DA_H \times \mathcal{P}(DA_L)$ represent the cartesian product between high-level actions and sequences of low-level actions. 
    A high-level durative action $\alpha$ is said to be an \textit{abstraction} of a sequence $\langle \alpha_1, ..., \alpha_N \rangle$ if the following property holds: If $\alpha \in DA_H$ is executable in a state $s_s$ and results in a state $s_d$, then for any states $s_s'$ and $s_d'$ such that $s_s \subseteq s_s'$ and $s_d \subseteq s_d'$, the sequence $\langle \alpha_1, ..., \alpha_N \rangle$ is executable in $s_s'$ and leads to $s_d'$.
\end{enumerate}

\sloppypar
The set of mappings $M$ is defined as the subset of $M \subseteq \bigcup_{i=1..N} DA_H \times DA_L^i$ with $N \ge 1$ made of the elements $\left(\alpha, \langle \alpha_1, ..., \alpha_N \rangle \right)$ such that $\alpha$ is an abstraction of $\langle \alpha_1, ..., \alpha_N \rangle$. In formal terms:
\[ 
\begin{array}{r}
M = \{ (\alpha, \langle \alpha_1, ..., \alpha_N \rangle) | \alpha \in DA_H, \forall i, \alpha_i \in DA_L, N \ge 1, \alpha \\
\quad \text{ is an abstraction for } \langle \alpha_1, ..., \alpha_N \rangle\}
\end{array}.
\]
For any element $m = (\alpha, \langle \alpha_1, ..., \alpha_N \rangle) \in M$, we denote by $m(\alpha)$ the sequence $\langle \alpha_1, ..., \alpha_N \rangle$.

For example, consider a high-level action $\alpha_i$ mapped to a sequence of low-level actions $\alpha_j, \alpha_k$, where $i \notin \{j, k\}$, with the mapping $m(\alpha_i) = \langle\alpha_j, \alpha_k\rangle$. This mapping can also be expressed in terms of snap actions: $m(\aStart{\alpha_i}) = \langle\aStart{\alpha_j}, \aEnd{\alpha_j}, \aStart{\alpha_k}, \aEnd{\alpha_k}\rangle$, indicating that the mapping is applied when $\alpha_i$ starts, and that $\alpha_i$ should terminate only when all actions in the mapping have completed.

As in standard temporal planning, a feasible plan for mapped temporal task planning consists of a set of durative actions such that the goal state is reached from the initial state, through simulation, i.e., by applying the durative actions in the correct order, the system transitions from the initial state to a state that satisfies the goal conditions.

Finally, as part of the set of fluents ($F$), we also define a set of resources $R\subseteq F$, which in this work describe the agents that can carry out a task. This will allow us later to set up a MILP problem to parallelize tasks for multiple agents and shrink the makespan of the plan.

To simplify notation, we will henceforth use the term ``action'' to refer to ``durative actions,'' unless explicitly stated otherwise.

\subsection{Prolog}

Prolog is a logic-based programming language commonly utilized for knowledge representation and symbolic reasoning.
In Prolog, a \kb can be defined as a collection of facts and rules, which can be queried to evaluate the satisfiability of more complex conditions.
Within robotics, Prolog is a useful tool for encoding knowledge about robots, their actions, and the environment in which they operate.
This programming language has also shown great potential in task planning~\cite{prologPlanning} and has gained interest when combined with natural language processing, as this integration facilitates human-robot interaction~\cite{NLPProlog1,NLPProlog2}.

In the following, we provide an overview of Prologâ€™s semantics and its operational principles. For a detailed explanation of Prolog semantics and the specifics of the SWI-Prolog implementation, we refer the reader to~\cite{Prolog}. In Prolog, the order of predicates within the \kb significantly affects execution. When a query is made, the interpreter evaluates the predicates in the sequence in which they appear. To demonstrate this, consider the following example.
Suppose the \kb includes these facts listed in this order: \verb|available(agent2)| and \verb|available(agent1)|.
If the query \texttt{available(X)} is executed, the interpreter will first assign \texttt{agent2} to \texttt{X}, and only if another query is made, \texttt{agent1} will be returned.

Another important feature of Prolog is its ability to backtrack. When encountering a predicate that results in failure, the interpreter retraces its steps to explore alternative solutions.
However, backtracking requires a failure condition to occur. If no such condition is reached, the program may enter an infinite loop. For example, if the actions \texttt{grip} and \texttt{release} are defined as the first two actions in the \kb, the planner can continuously alternate between them until a failure condition is encountered. Therefore, properly defining rules and conditions is essential to prevent endless loops and ensure that Prolog terminates appropriately.


\subsection{Large Language Models}

LLMs are a class of AI models aimed at natural language processing. 
%
They are often built upon transformer networks~\cite{Transformer}, which utilise self-attention mechanisms to better understand the context of words in a sentence.
%
They are typically trained with enormous amounts of data and have hundreds of billions of parameters, which can also be fine-tuned for the task in which they are employed \cite{LLMsSurvey1, LLMsSurvey2}.
%
Thanks to their ability to generalise and understand the context in which they are used, they have gained increasing relevance in recent years.
%
LLMs have been applied to a growing number of different fields, from healthcare~\cite{LLMsHealthCare} to planning~\cite{LLMsSucc}, also demonstrating their limitations~\cite{LLMsCannotPlan}.
%
In fact, while LLMs excel at learning complex patterns and information from vast training data, they rely primarily on statistical associations. They do not possess genuine inferential reasoning capabilities and, consequently, LLMs struggle when confronted with tasks different from the data they were trained on. Despite this, they can provide acceptable starting points for further refinements.
% 
Since they are trained on very general knowledge, it is also important to instruct LLMs on how to provide the output or solve some particular tasks that they are unaware of. Some of the most common techniques are:
\begin{itemize}
    \item Few-shot learning~\cite{llmfewshotlearn}: A series of examples in the form of QAs is passed to the LLM as input, allowing the LLM to understand how it should answer. 
    \item Fine-tuning~\cite{llmfinetune}: a more complex and complete training algorithm, which enables the user to generate a dataset to pass to the LLM in order to re-train the last layers of the neural network, enabling a more accurate output.
    \item Chain-of-Thought (CoT)~\cite{wei2022chain}: similar to few-shot learning, it enables the user to pass a series of examples with also an explanation of the solution improving the LLM's "reasoning" abilities.
\end{itemize} 
%% \subsection{Preliminary definitions}\label{ssec:preldefs}
%% %% Our approach combines forward state-space search with temporal reasoning to generate and evaluate candidate plans. In~\cite{saccon2023prolog}, we used Prolog to first extract a total-order plan, then refine it into a partial-order plan and finally check its consistency by transforming it into an STN before extracting a BT to execute. 
%% %% %
%% %% The main problem with this approach is that Prolog inherently performs a depth-first search, which has some drawbacks, mainly:
%% %% \begin{itemize}
%% %%     \item the provided plan is inefficient and usually sub-optimal since the solver will return the first plan that is feasible;
%% %%     \item the number of actions to choose from and of resources that have to be allocated deeply impact the time to compute a feasible plan and its optimality.
%% %% \end{itemize}
%% %% We decided to focus on the second aspect to improve the plan obtained with the framework. 
%% %% %
%% %% First of all, we divide the \kb in high-level (HL) and low-level (LL). This distinction allows us to also differentiate between high-level actions, e.g., \texttt{move\_block}, and low-level actions, e.g., \texttt{move\_arm}. Indeed, it is quite common that high-level actions are actually a sequence of low-level actions, for instance, the action \texttt{move\_block} may be decomposed in:
%% %% \begin{itemize}
%% %%     \item \texttt{move\_arm}, which sets the arm close to the block;
%% %%     \item \texttt{close\_gripper}, which closes the gripper around the block;
%% %%     \item \texttt{move\_arm}, which moves the arm to the final position of the block;
%% %%     \item \texttt{open\_gripper}, which opens the gripper to release the block.
%% %% \end{itemize}
%% %% We define a low-level action as an action that can be executed directly by calling one of the APIs. Also, a mapping can be thought as a list of low-level actions that must be performed in order for the high-level action to be actually executed.

%% %% \enrcom{The previous should be moved to the Introduction}

%% To provide the details of our approach, it is useful to introduce a modified definition of temporal planning.
%% \begin{definition}[The Mapped Temporal Planning Problem]
%% The problem we consider is an extention fo the temporal planning problem and is defined by the tuple $TP=(F, DA, I, G, K, M)$, where:
%% \begin{itemize}
%%     \item $F$ is the set of all the fluents;
%%     \item $DA$ is the set of durative actions;
%%     \item $I\subseteq F$ is the initial state;
%%     \item $G\subseteq F$ is the final state;
%%     \item $K\subseteq F$ is the set of grounding predicates;
%%     \item $M$ is the set of the mappings.
%% \end{itemize}
%% \end{definition}

%% As in standard temporal planning,  we use $\alpha$ to denote a durative action, and $a$ for snap actions: $\aStart{\alpha}=a_i,~\aStart{\alpha}=a_j$. Also, we define the set $A$ as the set of snap actions for the durative actions: $A=\{a : a=\aStart{\alpha}\vee a=\aEnd{\alpha}, \alpha\in DA\}$.

%% Each classical planning action $a$ has a set of preconditions $\pc{a}$, which is composed of literals, i.e., fluents $l\in F$ or their negation $\lnot l$. Literals may depend on variables $l(x), x\in F$, e.g., \texttt{available(A)} states whether an agent \texttt{A} is available or not, but the value of \texttt{A} is not grounded. 

%% \begin{definition}[Action's variables]
%% Let $a\in A$ be a snap action with preconditions $\pc{a}$ and effects $\eff{a}$. The set of all the variables of the literals in $\pc{a}\cup\eff{a}$ is defined as $\fl{a}$.
%% \end{definition}
%% For instance the preconditions of $a$ may be: $\pc{a}=[l_1(A), l_2(B), l_3]$, and the set of its variables would be given by $\fl{a}=\{A,B\}$. Later, this will allow us to identify the resources that the action $a$ depends on. Similarly, each classical planning action is associated with a set of effects $\eff{a}$, comprising fluents in the form of $\texttt{add}(l)$ or $\texttt{del}(l)$, where the former adds the fluent $l$ to the current state and the latter removes it from the current state. As previously, also this fluents can have arguments.

%% As per the problem definition, we also consider another subset of the fluents, $K\subseteq F$, which represents grounding knowledge, i.e., knowledge that is not supposed to change during the execution of the plan. For instance, information regarding whether the agent has a robotic arm or wheels is part of this set. 

%% \begin{definition}[Mapping]
%% Given a durative action $\alpha$, a mapping $m$ is a set of durative lower-level actions executed sequentially of which $\alpha$ is an abstraction.
%% \[ M = \{m : DA \rightarrow \mathcal{P}(DA)\},\]
%% where $\mathcal{P}(DA)$ is the power set of $DA$.
%% \end{definition}

%% For instance, consider a high-level action $\alpha_i$ that is split into two lower-level actions $\alpha_j, \alpha_k, i\notin \{j, k\}$ with the mapping $m(\alpha_i) = \{\alpha_j, \alpha_k\}$. A mapping can also be written in terms of snap actions $m(\aStart{\alpha_i}) = \{\aStart{\alpha_j}, \aEnd{\alpha_j}, \aStart{\alpha_k},$ $\aEnd{\alpha_k}\}$ indicating that the mapping should be applied only when $\alpha_i$ has started. Moreover, $\alpha_i$ should terminate only when all the actions in the mapping have terminated. 

%% Informally, the complexity of this problem is the same of the temporal planning problem, to which it can be reduced by considering $K$ as part of $I$ and $G$, and $M$ as individual actions. 

%% \begin{definition}[Feasible plan]
%% As in temporal planning, a feasible plan is a set of durative actions such that, when simulated, the goal state is reached from the initial one, i.e., by applying the durative actions to the initial state in the correct order we end in a state which fulfils the final conditions.
%% \end{definition}

%% Notice that from now on, we will consider actions as the two snap actions that compose the durative actions, unless specified otherwise, so we are considering classical actions when referring to ``actions". 
