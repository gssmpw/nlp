Our approach combines forward state-space search with temporal reasoning to generate and evaluate candidate plans. In~\cite{saccon2023prolog}, we used Prolog to first extract a total-order plan, then refine it into a partial-order plan and finally check its consistency by transforming it into an STN before extracting a BT to execute. 
%
The main problem with this approach is that Prolog inherently performs a depth-first search, which has some drawbacks, mainly:
\begin{itemize}
    \item the provided plan is inefficient and usually sub-optimal since the solver will return the first plan that is feasible;
    \item the number of actions to choose from and of resources that have to be allocated deeply impact the time to compute a feasible plan and its optimality.
\end{itemize}
We decided to focus on the second aspect to improve the plan obtained with the framework. 
%
First of all, we divide the KB in high-level and low-level. This distinction allows us to also differentiate between high-level actions, e.g., \texttt{move\_block}, and low-level actions, e.g., \texttt{move\_arm}. Indeed, it is quite common that high-level actions are actually a sequence of low-level actions, for instance, the action \texttt{move\_block} may be decomposed in:
\begin{itemize}
    \item \texttt{move\_arm}, which sets the arm close to the block;
    \item \texttt{close\_gripper}, which closes the gripper around the block;
    \item \texttt{move\_arm}, which moves the arm to the final position of the block;
    \item \texttt{open\_gripper}, which opens the gripper to release the block.
\end{itemize}
We define a low-level action as an action that can be executed directly by calling one of the APIs. Also, a mapping can be thought as a list of low-level actions that must be performed in order for the high-level action to be actually executed.

\todo[inline]{Insert definition for macro-planning}

As per the definition of a temporal planning problem, we consider a tuple in the form $TP=(F, DA, I, G)$, where $F$ is the set of fluents, $DA$ is a finite set of durative actions, $I\subseteq F$ is the initial state and $G\subseteq F$ is the final state. Notice that we use $\alpha$ to denote a durative action, whereas we use $a$ for snap actions: $\aStart{\alpha}=a_i,~\aStart{\alpha}=a_j$. Also, we define the set $A$ as the set of snap actions for the durative actions: $A=\{a : a=\aStart{\alpha}\vee a=\aEnd{\alpha}, \alpha\in DA\}$.

Each classical planning action $a$ has a set of preconditions $\pc{a}$, which is composed of literals, i.e., fluents $l\in F$ or their negation $\lnot l$. Literals may depend on variables $l(x), x\in F$, e.g., \texttt{available(A)} states whether an agent \texttt{A} is available or not, but the value of \texttt{A} is not grounded. We define as $\fl{a}$ the set of all the variables of the literals of a certain action $a$. For instance the preconditions of $a$ may be: $\pc{a}=[l_1(A), l_2(B), l_3]$, and the set of its variables would be given by $\fl{a}=\{A,B\}$. Later, this will allow us to identify the resources that the action $a$ depends on. 

Similarly, each classical planning action is associated with a set of effects $\eff{a}$, comprising fluents in the form of $\texttt{add}(l)$ or $\texttt{del}(l)$, where the former adds the fluent $l$ to the current state and the latter removes it from the current state. As previously, also this fluents can have arguments.

We also consider another subset of the fluents, $K\subseteq F$, which represents grounding knowledge, i.e., knowledge that is not supposed to change during the execution of the plan. For instance, information regarding whether the agent has a robotic arm or wheels is part of this set. 

\begin{definition}[Mapping]
Given a durative action $\alpha$, a mapping $m$ is a set of durative lower-level actions executed sequentially of which $\alpha$ is an abstraction.
\[ M = \{m : DA \rightarrow \mathcal{P}(DA)\},\]
where $\mathcal{P}(DA)$ is the power set of $DA$.
\end{definition}

For instance, consider a high-level action $\alpha_i$ that is split into two lower-level actions $\alpha_j, \alpha_k, i\notin \{j, k\}$ with the mapping $m(\alpha_i) = \{\alpha_j, \alpha_k\}$. A mapping can also be written in terms of snap actions $m(\aStart{\alpha_i}) = \{\aStart{\alpha_j}, \aEnd{\alpha_j}, \aStart{\alpha_k},$ $\aEnd{\alpha_k}\}$ indicating that the mapping should be applied only when $\alpha_i$ has started. Moreover, $\alpha_i$ should terminate only when all the actions in the mapping have terminated. 

Our problem is then defined by the tuple $TP=(F, DA, I, G, K, M)$, and, informally, the complexity of this problem is the same of the temporal planning problem, to which it can be reduced by considering $K$ as part of $I$ and $G$, and $M$ as individual actions. 

We define a valid temporal plan as a set of durative actions such that, when simulated, we reach the goal state from the initial one, i.e., by applying the durative actions to the initial state in the correct order we end in a state which fulfils the final conditions.

Once we have obtained the KB from the LLMs, then we use Prolog to: 
\begin{enumerate*}
    \item extract a possible total-order plan, 
    \item find the causality relationships between the different actions, i.e., extract a partial-order plan, 
    \item extract the resources used by each action,
\end{enumerate*}    
and finally we use OR-Tools\footnote{https://developers.google.com/optimization} to set up a MILP problem to optimise the plan (Section~\ref{ssec:poplanopt}).

Notice that from now on, we will consider actions as the two snap actions that compose the durative actions, unless specified otherwise, so we are considering classical actions when referring to ``actions". 
