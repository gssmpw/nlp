In this section, we first present the implementation details of the
framework, followed by a description of the experiments conducted and
the results obtained. We then discuss the scalability
of \frameworkname before concluding with a final discussion on the
proposed framework.

\begin{figure*}[htp]
    \centering
    \begin{subfigure}{0.99\linewidth}
        \centering
        \begin{textbox}{}
\footnotesize
Q:
    role: ``user''
    content: $\vert$
      How is your code divided?

A:
    role: ``assistant''
    content: $\vert$
      The high-level \kbase is composed of 4 parts:
      - \`{}init\`{} section, which contains the initial state of the environment;
      - \`{}goal\`{} section, which contains the final state of the environment;
      - \`{}\kb\`{} section, which contains all the information that is not modified by the actions;
      - \`{}actions\`{} section, which contains the description of the actions that can be performed 
         by the agents.
      For example, the predicate \`{}available(A)\`{}, which states that the agent A is available, is 
      a predicate that is modified by the actions and should be placed in the \`{}init\`{} section.
      On the other hand, the predicate \`{}pos(X,Y)\`{}, which states that the position (X,Y) is a 
      possible position for the agents, is a predicate that is not modified by the actions and 
      should be placed in the \`{}\kb\`{} section.
        \end{textbox}
        \caption{General example passed to the LLM to understand how the \kb should be structured.}
        \label{fig:genExample}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.99\linewidth}
        \centering
        \begin{textbox}{}
\footnotesize
Q:
    role: ``user''
    content: $\vert$ 
      Can you generate a Prolog snippet of code that describes the initial and final states for 
      the following scenario? 
      Given an initial state in which there are two blocks b1, b2 in position (1,1) and (2,2) 
      respectively, move the block b1 to position (3,3) and place b2 on top of b1 using an 
      agent, which is initially available and it will also be available at the end.

A:
    role : ``assistant''
    content: $\vert$
        Sure. The problem states that at the beginning there are two blocks on the table: we shall 
        use the \`{}ontable(Block)\`{} predicate to describe this. To state the position of the blocks
        we'll use the predicate \`{}at(Block, X, Y)\`{}. Since both blocks do not have a block over them
        they are clear to be picked-up, so we use the predicate \`{}clear(Block)\`{} to describe this. 
        Finally, the agent is available at the beginning, so we'll use the predicate 
        \`{}available(Agent)\`{}.
        Here is the initial state:\\
        \`{}\`{}\`{}init
\begin{minted}[fontsize=\scriptsize]{prolog}
init_state([
  ontable(b1), ontable(b2), at(b1,1,1), at(b2,2,2), clear(b1), clear(b2), available(a1)
]).
\end{minted}
        \`{}\`{}\`{}\\
        At the end, the problem states that block b2 is on top of block b1, so we'll use the
        predicate \`{}on(BlockTop, BlockBottom)\`{} to describe that \`{}BlockTop\`{} is on top of 
        \`{}BlockBottom\`{}. Notice that now only b2 will be pickable, hence we'll have the predicate
        \`{}clear(b2)\`{}, but not the predicate \`{}clear(b1)\`{}. 
        Here is the code for the goal state:\\
        \`{}\`{}\`{}goal
\begin{minted}[fontsize=\scriptsize]{prolog}
goal_state([
  ontable(b1), on(b2,b1), at(b1,3,3), at(b2,3,3), clear(b2), available(a1)
]).
\end{minted}
        \`{}\`{}\`{}
        \end{textbox}
        \caption{Example on an action generated for the blocks world scenario}
        \label{fig:blocksExample}
    \end{subfigure}
    \caption{Instance of the examples fed to the LLM through few-shot prompting.}
    \label{fig:enter-label}
\end{figure*}


\subsection{Implementation Details}\label{ssec:implementation}

The different parts have been developed and devised to ensure
modularity, efficiency, and scalability. The implementation process
involved a careful selection of algorithms and data structures to
optimize performance. In the following subsections, we provide a
detailed account of each component, outlining design choices,
technical challenges, and solutions adopted.

\paragraph{KMS}
As already explained, the \kb has been written in Prolog to exploit
its inference capabilities. This programming language allows to define
simple predicates and then construct and solve complex queries. The
generate \kbase is loaded inside the planner and for such reason it
must be formatted accordingly.  The tests were conducted using two
main models: GPT4o and GPT4 with 128K tokens. The models were guided
on how to format the output through few-shot prompting and CoT. The
examples ranged from general information regarding the structure of
the framework~\ref{fig:genExample}, to examples providing more details
regarding a particular scenario~\ref{fig:blocksExample}.

\paragraph{Planner}
The planner has been implemented using two programming languages:
Prolog, which ensures the feasibility of the extracted plan for
total-order and partial-order extraction, and Python 3, which is used
to develop the MILP solver with
OR-Tools\footnote{\url{https://developers.google.com/optimization}}.


%% Indeed, when planning using the Prolog
%% planner provided by \frameworkname, it is possible to enable some
%% debug functions that would help the user to understand why a certain
%% scenario is not feasible.

\paragraph{\Btrees}
While \bts have become a de facto standard for executing robotic
tasks, no universally accepted framework exists for their creation or
execution. Some notable examples include
PlanSys2~\cite{martin2021plansys2} and
BehaviorTree.CPP~\cite{BehaviorTreeCppWebsite}. PlanSys2 is tightly
integrated with ROS2; beyond merely executing \btrees, it can also
derive feasible plans from a \kbase. In contrast,
BehaviorTree.CPP is a more general framework that enables the creation
and execution of \bts from an XML file. We chose BehaviorTree.CPP
since our main objective was to execute APIs from a \bt, which is
easily represented using an XML file, while also maintaining maximum
generality. Nevertheless, BehaviorTree.CPP also offers a ROS2 wrapper,
which can easily be integrated.

\subsection{Results}\label{ssec:results}

In this section, we show experimental results obtained using our
framework. In doing so, we follow the flow of the system, starting
with the \kb generation, proceeding with tests on the planning phase
and finally showing a whole example that was run on real-life robots.

We focused on two scenarios: blocks world and the building of an
arch. The former is a very well known \ucase\cite{blocksworld}, in
which a series of blocks are set available on a table and they must be
moved from an initial configuration to a final one. We change
this \ucase by enabling to have multiple agents moving the blocks, in
order to highlight the cooperation abilities of the framework, and by
explicitly stating the positions of the blocks as coordinates. The
later scenario is a variation of the blocks world in which we consider
blocks of different type and the goal is to construct a more complex
building, such as an arch, by first building more elementary
structures and then combining them. For the blocks world scenario, we
run 4 different queries, while 2 more were run for the arch.

The queries and scenarios vary in terms both of complexity and
generality. For instance, example 5 of blocks world is more
complicated for the planner as it consists of multiple objects in the
scenario, similarly example 3 of blocks world is challenging for the
planner as it requires to find a longer plan. Though, both examples
have a precise description of the scenario and are not particularly
challenging for the KMS module as they respect the structure of the
examples passed to the LLM. On the contrary, example 4 of blocks
represents a challenge for the LLMs as, while the domain is still
blocks world, the description is much more general requiring the LLM
to do some inference on some predicates.

\subsubsection{\kb Generation} 
\label{sssec:kbgenvalidation}
The tests for the generation of the \kb are divided in three parts. A first one is for the validation check carried out at the beginning (shown in Table~\ref{tab:validationRes}), a second one for the generation of the high-level (HL) \kb (Table~\ref{tab:hlRes}), and a final one for the generation of the low-level (LL) \kb (Table~\ref{tab:llRes}). 

\paragraph{Validation} For the validation phase, we also wanted to evaluate whether a smaller model, such as GPT3.5-turbo, could correctly detect whether the queries were correct or whether they contained errors. It is important to mention that, in order to correctly query this model, the number of examples had to be reduced as the tokens limit would not allow it to run otherwise. This is also one of the main limitation that led has not to use such model for \kb generation.

As it is possible to see in Table~\ref{tab:validationRes}, the experiments mixed also some wrong queries to detect whether the model could recognize errors:
\begin{itemize}
    \item Example 2.a in the blocks world scenario introduces an error in the HL description stating that a block should be moved, but it does not information on where it should be moved;
    \item Example 5.a in the blocks world scenario introduces an error in the LL description by having a different number of agents than the HL description;
    \item Example 2.a of the arch scenario introduces an error in the HL description suggesting that the architrave to be put on top is shorter than the gap between the pillars. 
\end{itemize}

\begin{table*}[htp]
    \centering
    \begin{tabular}{r|c|c|c|c|c|c|c||c|c|c}
                        & \multicolumn{7}{c||}{Blocks world} & \multicolumn{3}{c}{Arch}\\
                        &  1  & 2.a & 2.b &  3  &  4  & 5.a & 5.b &  1  &  2.a & 2.b\\
        \hline
         GPT3.5 Turbo   &  X  & \cm*&  X  &  X  &  X  &  X  &  X  &  X  & \cm* & X \\
         GPT4 - 120K    &  X  & \cm &  X  & \cm & \cm & \cm & \cm &  X  & \cm  & X \\
         GPT4o          & \cm & \cm & \cm & \cm & \cm & \cm & \cm & \cm & \cm* & \cm
    \end{tabular}
    \caption{Results for the consistency check on the input queries. The $*$ indicates that the model considered it correct for the wrong reason.} 
    \label{tab:validationRes}
\end{table*}

As shown in Table~\ref{tab:validationRes}, the difference between GTP3.5 and GPT4 is striking, with GPT4o that manages to consistently detect whether errors are present or not in the queries.
GPT3.5 manages to correctly detect that the example 2.a in the blocks world scenario is wrong, although it states that it is wrong because . Similarly, both GPT3.5 and GPT4o detect an error in example 2.a of the arch scenario, although they both state that the problem is in, while this is not the case. 
Strangely enough, GPT4 with 120K tokens fails the easier queries, e.g. example 1, while instead managing to evaluate more difficult queries, which were instead correct. 

In general, GPT4 managed to evaluate correctly the majority of the cases and especially to correctly flag the wrong queries. 

\paragraph{\kb Generation} 

The \kb generation represents a more complex step for a generative model. Indeed even both GPT4-128K and GPT4o do not manage to get all the predicates correctly. The process to check the output of the LLMs went as follows:
\begin{itemize}
    \item First, the framework extracts the correct parts of the output to a Prolog file containing the \kb;
    \item Then we include the file in the planner and test whether it provides a plan for the HL \kb;
    \item If it manages to find a plan, then we record the plan and the time taken, otherwise we enable some debugging functions that help the user understand why a problem is not solvable and fix the \kb.
    \item Finally, the same process for the LL \kb is repeated.
\end{itemize}

From Table~\ref{tab:hlRes}, we can see that GPT4o manages to almost produce correct HL \kbases for all the examples, except for example 2 of the arch scenario. In this case, the error was a minor misunderstanding: GPT4o correctly generated all the four parts (general \kb, initial and final states, and actions), but the action \verb|place_architrave_start| did not have the predicate \verb|ontable(Arch)| between the positive preconditions, meaning that it would never be able to pick and place the architrave on top of the pillars.

GPT4 with 120K tokens does not manage to create any HL \kb completely correct. That said, in many cases, the errors were minor and easily identifiable. For example, in examples 3 and 4 of the blocks world scenario, GPT4-120K made the same single mistake in which it used the predicate \verb|pos(X,Y)| inside the preconditions instead than using it inside the predicates to be grounded on the general \kb. This meant that the planner could not find a feasible plan since there is no predicate in the initial state that corresponds to \verb|pos(X,Y)|. More strangely, this error was made only for one of the four actions that GPT4-120K had to generate. 

As for the two examples in the arch scenario, GPT4-128K did not manage to provide a correct interpretation of the task of placing the architrave on top of the pillars. While it provided an action for placing the architrave, it did not create a predicate in the final state or within the effects of the action, meaning that the plan would never schedule that action correctly. Interestingly enough, it added predicates in the general \kb that would correctly describe the pillars and the architrave (if used in the actions), which is not an expected behaviour since it is not part of the fed examples. 

\begin{table*}[htp]
    \centering
    \begin{tabular}{r|c|c|c|c|c||c|c}
                        & \multicolumn{5}{c||}{Blocks world} & \multicolumn{2}{c}{Arch}\\
                        &  1        &  2        &   3    &  4     &  5     &  1  &  2 \\
        \hline
         GPT4 - 120K    &  X(2,14)  &  X(3,17)  & X(1,2) & X(1,2) & X(2,2) &  X  &  X  \\
         GPT4o          & \cm       & \cm       & \cm    & \cm    & \cm    & \cm & X(1,1)  
    \end{tabular}
    \caption{Results for the generation of the HL \kbase (\kb) using the models listed on the left. A \checkmark~indicates that the model's output was completely correct, while X denotes incorrect output. In cases where a fixable number of errors occurred, the first value inside parentheses represents the number of distinct errors, and the second value indicates the number of changes required to fix the \kb.}
    \label{tab:hlRes}
\end{table*}

As for the lower-level \kb, the situation is more complex as it is possible to see in Table~\ref{tab:llRes}. In this case, GPT4 - 128K fails the majority of cases. Even trying to fix the generated \kb would take much time. On the other hand, while GPT4 makes some mistakes, the majority of errors seems to be related to the creation of mappings. For instance, in example 2 of blocks world and in example 1 of Arch, GPT4o generated the following mapping;

\begin{minted}[fontsize=\footnotesize]{prolog}
mapping(move_table_to_table_start(Agent, Block, X1, Y1, X2, Y2),
[
ll_move_arm_start(Agent, 4, 4, X1, Y1), ll_move_arm_end(Agent, 4, 4, X1, Y1),
ll_close_gripper_start(Agent), ll_close_gripper_end(Agent),
ll_move_arm_start(Agent, X1, Y1, X2, Y2), ll_move_arm_end(Agent, X1, Y1, X2, Y2),
ll_open_gripper_start(Agent), ll_open_gripper_end(Agent)
]).
\end{minted}    

which is wrong because it hardcodes the initial position of the movement to (4,4) when the arm may be in another location. The correction simply consists in changing the coordinates with wildcards:
\begin{minted}[fontsize=\footnotesize]{prolog}
ll_move_arm_start(Agent, _, _, X1, Y1), ll_move_arm_end(Agent, _, _, X1, Y1),
\end{minted}
Since example 2 of blocks world and example 1 of arch contained 4 and 5 mappings, respectively, the total numbers of correction needed for this error were 8 and 10, respectively.

GPT4 with 128K tokens, also repeatedly made a mistake in the generation of the low-level \kb by changing all the predicates to low-level ones by prefixing all of them with \verb|ll_|. This would not be too much of a problem, if it had changed also the names of the predicates inside the high-level actions. Not having done so, means that the high-level actions would not be schedulable and hence the planner would not find a feasible plan.

\begin{table*}[htp]
    \centering
    \begin{tabular}{r|c|c|c|c|c||c|c}
                        & \multicolumn{5}{c||}{Blocks world} & \multicolumn{2}{c}{Arch}\\
                        & 1   &  2     &  3      &  4     &  5  &  1      &  2 \\
        \hline
         GPT4 - 120K    &  X  &  X     &  X      & X      &  X  &  X      &  X  \\
         GPT4o          & \cm & X(1,8) & X(2,10) & X(1,8) & \cm & X(1,10) & X(4,18)   
    \end{tabular}
    \caption{Results for the generation of the LL \kbase (\kb) using the models listed on the left. A \checkmark~indicates that the model's output was completely correct, while X denotes incorrect output. In cases where a fixable number of errors occurred, the first value inside parentheses represents the number of distinct errors, and the second value indicates the number of changes required to fix the \kb.}
    \label{tab:llRes}
\end{table*}

\begin{table*}[htp]
    \centering
    \begin{tabular}{r|c|c|c|c|c||c|c}
                & \multicolumn{5}{c||}{Blocks world} & \multicolumn{2}{c}{Arch}\\
                &     1    &  2      &  3  &  4  &  5  &  1  &  2 \\
        \hline
         HL     & 29, 108  & 46, 108 & 37, 108 & 25, 108 & 133, 108 & 27, 140 & 28, 144 \\
         LL     & 44, 153  & 59, 146 & 47, 146 & 31, 138 & 136, 150 & 44, 178 & 45, 184
    \end{tabular}
    \caption{The number of predicates in the final \kbs using GPT4o after having fixed possible mistakes. The first value is the number of grounded predicates, either in the general \kb or in the states, while the second is the number of lifted predicates in the actions.}
    \label{tab:predNumber}
\end{table*}

It also should be highlighted that the majority of the mistakes that were made in the generation of the \kb happened within the generation of the actions or of the mappings. While this may be expected given the much more complex nature of the tasks, it also means that it may be possible to better train the LLMs to prevent this from happening and consequently generating much more reliable \kbs. The only model that introduced mistakes outside of the actions set is GPT4-128K for example 1 of the arch scenario, in which case the model did not understand correctly that there should be a \verb|place_architrave_{start,end}| action in the actions set, but instead tried to insert it inside the general preconditions.

Given the number of predicates in these \kbs (Table~\ref{tab:predNumber}), the ability of a generative model to produce a nearly complete \kb requiring only minimal modifications is highly beneficial.

\subsubsection{Planner}
\label{sssec:planRes}

In this section, we evaluate the performance of the planner. We first start by evaluating the experiments also used in Section~\ref{sssec:kbgenvalidation} and then we provide a more general evaluation on the scalability of the planner. From now on we consider \kbs generate by GPT4o and manually fixed. All the tests were executed on a laptop running an Intel i7-1260P and 32GB of RAM with Ubuntu 22.04, SWI-Prolog version 8.4.2, Python3.10 and pyswip version 0.2.7. 

\begin{table*}[htp]
% \footnotesize
    \centering
%     \begin{tabular}{r|c|c|c|c|c||c|c}
%                       & \multicolumn{5}{c||}{Blocks world} & \multicolumn{2}{c}{Arch}\\
%         Examples      &  1  &  2   &  3    &  4  &  5*   &  1    &  2    \\
%         HL Plan Steps &  2  &  4   &  8    &  6  &   6   &  6    &  6    \\ 
%         \hline
%         TO Plan       &  22 & 1395 & 27486 &  15 & 43916 & 1459  &    8 \\
%         PO Plan       &  23 & 1643 & 22486 &  48 & 44627 & 2545  &  337 \\
%         MILP -- \bt   & 562 & 2151 & 27826 & 449 & 45953 & 3438  & 1005\\
%     \end{tabular}
\scalebox{0.82}{
    \begin{tabular}{r|c|c|c|c|c||c|c}
                      & \multicolumn{5}{c||}{Blocks world} & \multicolumn{2}{c}{Arch}\\
        Examples                &  1  &  2   &  3    &  4  &  5*   &  1    &  2   \\
        HL TO Plan Steps        &  2  &  4   &  8    &  6  &   6   &  6    &  6   \\ 
        \hline
        HL TO Plan              &  30.81 & 1640.14 & 23083.37 &  81.18 & 39067.04 & 2633.67 & 520.62 \\
        LL TO Plan              &   0.11 &    0.22 &     0.37 &   0.31 &     0.45 &    0.27 &   0.27 \\
        \makecell{PO and Resources\\Extraction}    
                                &   0.15 &    0.70 &     2.83 &   1.95 &     1.63 &    1.45 &   1.48 \\
        Total Prolog            &  63.81 & 1731.89 & 23512.65 & 102.65 & 39559.44 & 2726.42 & 550.69 \\
        MILP -- \bt             & 399.63 &  264.48 &   292.95 & 268.50 &   276.98 &  262.94 & 258.29 \\
    \end{tabular}
}
    \caption{Execution times (in milliseconds) for the planning phase. For example 5 of blocks world, see Table~\ref{tab:plannerScalability}.}
    \label{tab:plannerTimes}
\end{table*}

We report the times for the different planning phases for each experiment in Table~\ref{tab:plannerTimes}. In particular, ``HL TO Plan'' and ``LL TO Plan'' correspond to the extraction of a high-level total-order plan and subsequent generation of a total-order plan by applying the mappings (Section~\ref{ssec:toplangen}), ``PO and Resource Extraction'' corresponds to the phase in which Prolog parses the \kb and actions to extract the resources they depend on (Section~\ref{ssec:poplangen}), ``Total Prolog'' refers to the time required by the Python wrapper to compute the whole planning phase using Prolog (Section~\ref{ssec:implementation}) and ``MILP -- \bt'' refers to the final optimization and \bt extraction part (Section~\ref{ssec:poplanopt} and Section~\ref{sec:bt}).

We can see that the highest complexity of extracting a feasible plan is to actually find the TO plan for the HL \kb. This part of the search is indeed the most complex one since it is not guided and requires the Prolog search to backtrack multiple times in order to find a feasible plan. On the contrary, once the plan is found, checking that the preconditions of the low-level action when substituting the mappings and consequently applying their effects is much faster. Also the extraction of the causal relationship requires just a fraction of the time. 

It is also worth noticing that the length of the HL TO plan greatly affects the performance of the planner. Indeed, while example 3 of blocks world is not much different in the number of predicates from example 2 of blocks world, the computational time changes dramatically when the required number of steps to find a solution increases.

As for the MILP problem, we can see that the time required is bigger than the PO part, although it decreases in proportion when considering more complex task. This is to be expected as in this case the bottleneck of the MILP part would be the initialization of the problem with OR-Tools.

For example 5 of the blocks world scenario, GPT4o managed to produce a correct \kb, but it could not be solved by the planner in the given timeout of 1 minute when considering the whole complexity of the problem. When using the same set of actions and predicates on a smaller example, the planner managed to find a solution. This example proves to be a good test-bed for evaluating how the planner performs when varying of the number of predicates. The example considers 20 blocks, 24 possible positions and 3 agents, but only asks to move 3 blocks from their initial positions to another position and stack them. Indeed, while not even the HL planner could find a feasible plan after 1 minute with all the predicates, we can change the number of predicates in the general \kb and see how the planner performs. The results are reported in Table~\ref{tab:plannerScalability}, where the columns indicate the number of blocks and the number of positions considered in the general \kb. It is important to notice that the time to compute the PO plan, also contains the time to compute the TO plan and the MILP problem solution contains the time for the whole planning process.

\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
\begin{table*}[htp]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c|c|c|c|c|c|c}
        \makecell{\#\\positions} & \makecell{\#\\blocks} & \makecell{HL TO\\Plan} & \makecell{LL TO\\Plan} & \makecell{PO and Resources\\Extraction} & \makecell{Total\\Prolog} & \makecell{MILP\\-- \bt} \\
        \hline
        \hline
        9  & 5  & 20.35  & 0.31  & 2.22  & 56.92  & 282.51  \\
        \hline
        14 & 5  & 67.40  & 0.43  & 2.22  & 91.39  & 273.77  \\
        14 & 10 & 834.86 & 0.39  & 2.00  & 720.04 & 274.71  \\
        \hline
        19 & 5  & 133.99 & 0.49  & 2.07  & 146.52 & 272.23  \\
        19 & 10 & 1788.25 & 0.48 & 1.98  & 1490.48 & 268.52  \\
        19 & 15 & 7220.93 & 0.39 & 1.67  & 7102.22 & 275.89  \\
        \hline
        24 & 5  & 230.70  & 0.59  & 2.05  & 233.63  & 274.85  \\
        24 & 10 & 2765.18 & 0.50  & 1.78  & 2512.15 & 266.85  \\
        24 & 15 & 12522.42 & 0.46 & 1.67  & 12580.44 & 277.66  \\
        24 & 20 & 39067.04 & 0.45 & 1.63  & 39559.44 & 276.94  \\
    \end{tabular}
    }
    
    \caption{Execution times (in milliseconds) for the planning phase on example 5 of the blocks world scenario at the changing of the number of predicates in the \kb.}
    \label{tab:plannerScalability}
\end{table*}

We can see that the planner scales quite poorly when increasing the number of grounding predicates. This, in conjunction with the length of the HL TO plan, is the biggest bottleneck of the planner since every time it has to ground a predicate, it creates a backtracking point to which it can return to check alternative possibilities when encountering an unfeasible state. Moreover, we can see that the performance worsen more when changing the number of positions rather than the number of blocks. This example-dependant: multiple actions move a block from one position to another, meaning that each time an action is to be scheduled, the planner must ground not just one position, but two different positions, worsening the performance by a quadratic factor. 

\subsection{Real-Life Experiment}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/Experiment.png}
    \caption{The real-life experiment we carried out using two Universal Robots arms.}
    \label{fig:rlexp}
\end{figure}

Finally, we present a real-life experiment run on two robots from Universal Robots, a UR3e and a UR5e, equipped respectively with a 2f85 gripper from Robotiq and an mGrip gripper from Soft Robotics. The scenario consisted in cooperating the two robots in order to correctly build an arch made of three blocks: 1 for each pillar plus 1 for the architrave. In Figure~\ref{fig:rlexp}, it is possible to see the setup used.

We positioned the blocks on the table and provided a query similar to the ones used for the arch scenario by changing the coordinates of the blocks. 

We then used \frameworkname to extract a \bt which was then parsed by BehaviorTree.ROS2 allowing for direct communication with the ROS2 servers connected to the real robots. 

The robots were able to move according to the plan and correctly build the arch.

\subsection{Discussion \& Future Work}
\label{ssec:discussion}

The experimental results presented in Section~\ref{sssec:kbgenvalidation} and Section~\ref{sssec:planRes} highlight the effectiveness of the proposed framework in knowledge generation, planning, and execution. The system successfully translates natural language descriptions into executable \kbases and generates plans that maintain logical consistency and feasibility. However, an in-depth discussion is necessary to assess the quality of the results, their applicability in real-world scenarios, and the challenges that remain.

\subsubsection{Effectiveness of the Planning Approach}
The results demonstrate that the framework efficiently produces high-level and low-level \kbases, enabling the generation of feasible plans. The time required for HL plan generation is the most computationally expensive step, as shown in Table~\ref{tab:plannerScalability}, where execution time increases with the number of predicates in the \kb. This is expected, as a larger state space leads to an exponential growth in possible plan combinations. Nevertheless, the system efficiently applies logical inference to extract meaningful plans, making it a viable tool for complex task execution.

Furthermore, the MILP-based optimization step ensures that extracted plans are not only logically valid but also optimized in terms of resource allocation and execution order. The results suggest that the framework can successfully balance multiple constraints, ensuring feasible solutions even in scenarios requiring parallel execution of tasks.

In the future, to improve the efficiency of the total-order plan generation, 
we could replace the planner with a PDDL-based state-of-the-art 
planner through a compilation phase. While this would enhance 
performance in plan generation, it would also reduce the 
explainability and modularity of the \kb. Indeed, having a logical 
\kb offers a wide range of opportunities for inference 
and automated reasoning (e.g., to perform consistency checks or 
to enable the composition of fluents through the abstraction refinement 
relation).  
A possible solution to combine the advantages of both 
approaches could be to use the logic-based system to generate PDDL problems, 
then leverage the heuristics of state-of-the-art planners to 
generate total-orders. 

\subsubsection{Scalability and Limitations}
One key limitation observed in the experiments is the scalability of the planning process. The computational complexity significantly increases with the number of objects and possible positions, leading to longer execution times and, in some cases, an inability to find a feasible plan within a given timeout. This behavior is particularly evident in Example 5 of the blocks world scenario, where the planner struggles with a large number of predicates. Future improvements could focus on reducing search space complexity, leveraging heuristics, or integrating state-of-the-art planners that optimize partial-order planning.

Another limitation concerns the current separation between task planning and motion planning. While the framework successfully generates high-level plans, it does not explicitly consider motion constraints of the robotic agents. This may lead to cases where a theoretically feasible plan is not applicable in the real-world. 

To address this issue,  
we are working on the definition of Task and Motion Planning (TAMP) 
problem~\cite{tampsurvey}, which could incorporate 
the information required to set the parameters for the 
correct execution of an action. One possible solution 
is to include parametric Dynamic Movement 
Primitives~\cite{DMPsurvey} as actions within the framework.  
Such actions could be derived from human observations through 
imitation learning approaches and would provide a more accurate 
estimation of an actions' duration. This could also address another significant 
limitation of the approach.  
When solving the MILP problem (Section~\ref{ssec:poplanopt}), we must 
set upper and lower bounds on the duration of actions. At 
present, we use fairly conservative values for these bounds; having 
a more precise estimation would significantly improve the quality of the generated plan.  

\subsubsection{Role of LLMs in \kb Generation}
The use of LLMs in \kbase generation has proven effective, but certain limitations remain. While models like GPT-4o exhibit strong generalization abilities, they sometimes introduce errors in predicate mapping and action descriptions. The results show that passing too many examples in a single query can lead to reduced accuracy, with the model occasionally "forgetting" earlier examples. Addressing this issue through fine-tuning techniques could improve the consistency and reliability of generated \kbs.

Another limitation of the framework stems from a persistent problem 
with LLMs: hallucinations and random behaviours.  
In our case, this means that even sophisticated models sometimes 
introduce errors in the generated \kb. This issue can be mitigated 
by providing correct examples, but in some experiments, we observed that 
too many examples could have adverse effects, with the LLM becoming more 
confused and 
forgetting the examples provided at the start. As future work, we are 
planning to investigate whether fine-tuning an LLM with an ad hoc dataset 
may enhance the generative model's abilities. Additionally, we are exploring ways 
to enable automated error correction by using another LLM to parse 
errors detected by the SWI-Prolog interpreter.  

\subsubsection{Real-World Applicability}
Despite the aforementioned limitations, the proposed framework has significant potential for real-world applications. Its ability to generalize across different task descriptions makes it suitable for use in multi-agent systems where adaptability and automation are crucial. Industrial settings, such as robotic assembly lines or autonomous warehouse management, could benefit from a system that dynamically generates and optimizes task plans based on high-level instructions.

Future work should focus on expanding the framework's robustness by incorporating uncertainty handling, probabilistic reasoning, and real-time re-planning capabilities. This would enable the system to adapt to unforeseen changes in the environment, further bridging the gap between automated planning and practical deployment in robotics applications.









