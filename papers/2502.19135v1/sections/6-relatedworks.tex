In this section, we are going to discuss, to the best of our knowledge, the current state of the art and highlight the gap we are filling. 

\subsection{\Kbase Generation}

Knowledge representation is an essential component that endows robots with the cognitive abilities necessary to autonomously execute tasks and make informed decisions \cite{bayat2016requirements,kbSurvey}. This capability underpins the development of systems that can simulate common-sense reasoning in robotic applications.
% for robots to autonomously execute tasks and make choices as it equips them with cognitive abilities \cite{bayat2016requirements,kbSurvey}.

Typically, knowledge systems rely on ontologies to formally describe discrete pieces of information and the relationships among them. In this context, the OpenRobots Ontology (ORO)~\cite{ORO} is designed to store symbolic-level knowledge and events by transforming previously acquired symbols into interconnected concepts. Built upon the framework of semantic triples~\cite{RDF}, ORO facilitates a server architecture where information can be both pushed and pulled, thereby supporting dynamic knowledge management.
% Knowledge systems usually rely on having an ontology to describe pieces of information and their connections. The final goal is to create a system that can actually reproduce common-sense in robots. \textbf{OpenRobots Ontology} (ORO) \cite{ORO} stores symbolic level knowledge and events, and by turning previously acquired symbols into linked concepts. ORO's ontology is built on semantic triples \cite{RDF} that enable the creation of a server where information can be either pushed to or pulled from. 

Ontologies are frequently tailored to specific domains. For example, the Ontology for Robotic Orthopedy Surgery (OROSU)~\cite{OROSU} is dedicated to the medical domain, integrating healthcare ontologies with robotic systems to represent the critical knowledge required during surgical interventions. Similarly, the Worker-cobot ontology~\cite{workerCobot} focuses on industrial applications, supporting collaborative tasks in shared, closed environments through a framework that leverages multi-agent systems and business rules.
% Ontologies are usually specific to a certain area. \textbf{Ontology for Robotic Orthopedy Surgery} (OROSU) \cite{OROSU} is a system centered around the medical domain, integrating healthcare ontologies and robotic systems \cite{OROSUHip}. The goal is to represent the knowledge that should be used during a surgical intervention. Another particular case is \textbf{Worker-cobot} \cite{workerCobot}, which targets industrial robots for collaborative tasks in shared closed environments by exploiting an ontology based on multi-agent systems and business rules. 

In addition to these domain-specific systems, advanced knowledge processing frameworks such as KnowRob \cite{KnowRob1}, now in its second version \cite{KnowRob2}, demonstrate a more comprehensive approach by incorporating environmental data into the reasoning process. Unlike systems that rely solely on deductive closure computation, such as ORO, KnowRob integrates inferential reasoning via Prolog, thus enabling more dynamic and context-aware knowledge management. Furthermore, KnowRob2 expands its capabilities by integrating semantic web information and utilizing a game engine to facilitate the learning of action-related knowledge. This integration allows the system to "reason with its eyes and hands," meaning that it can construct a realistic representation of its environment. Consequently, KnowRob2 is able to abstract and generalize common knowledge from experiential data, thereby enhancing its adaptability to novel situations. 
% \textbf{KnowRob} \cite{KnowRob1} is one of the most advanced knowledge processing systems, now at the second version \cite{KnowRob2}. These systems differentiate from normal KB management system by the fact that they also acquire data from the environment. While a system such as ORO computes deductive closures when creating the knowledge base, KnowRob uses Prolog to integrate inference between information. KnowRob2 integrates also semantic web information and a game engine to learn information about actions. This enables the system to "reason with its eyes and hands", meaning that it can reproduce a realistic knowledge of the environment. It is also able to learn generalised common knowledge from experience, which can then be applied to novel situations \cite{KnowRob2}.
One of the main limitations of systems like KnowRob2 is related to the generation of its knowledge base, which involves complex syntactical structures that complicate the maintenance and scalability of the system, potentially hindering efficient inference and integration of new data. 
Large Language Models (LLMs) can address this limitation by leveraging their ability to parse and generate natural language, thereby producing more flexible and context-aware representations that reduce the reliance on rigid, manually defined syntactic structures in knowledge base generation.


\subsection{\llm for \kb Generation}
Various approaches leveraging LLMs to construct generalizable planning domains have been proposed, demonstrating their capability to convert natural language descriptions of planning problems into robot-oriented planning domains.
These approaches aim to reduce the dependency on handcrafted, domain-specific components traditionally required for solving planning problems. For instance, the ISR-LLM approach proposed in~\cite{10610065} addresses long-horizon planning tasks by converting natural language instructions into PDDL representations and utilizing an LLM-based planner that incorporates the Chain-of-Thought (CoT) mechanism~\cite{wei2022chain} to iteratively refine and plan tasks through intermediate steps. Similarly, the work presented in~\cite{Silver_Dan_Srinivas_Tenenbaum_Kaelbling_Katz_2024} employs LLMs as a generalized planner by using CoT summarization to enhance planning performance, although this method still necessitates predefined planning domain representations.

It is noted that LLMs are not ideally suited to function as standalone planners~\cite{valmeekam2022large}, a limitation that motivates the development of more robust frameworks integrating the strengths of both LLMs and symbolic planning. The LLM+P framework~\cite{liu2023llmp}, for example, capitalizes on the advantages of classical planners by using LLMs to generate PDDL problem files based on natural language descriptions, after which classical planners are employed to solve the problem, thus avoiding the pitfalls of using LLMs as direct planners. Likewise, the approach described in~\cite{oswald2024large} presumes the existence of task-related PDDL domains and uses action-by-action prompting to reconstruct the planning domain through LLMs. Despite their promise, these methods are constrained by the assumption that the problem description is provided as a lifted PDDL domain file.
Also the TwoStep approach \cite{singh2024twostepmultiagenttaskplanning} integrates LLMs with classical PDDL planners for multi-agent task planning by decomposing a unified agent plan into partially independent subgoals that can be allocated to a main agent and a helper agent. This decomposition, though innovative, is limited to two agents and focuses primarily on the coordination between them. 

In contrast to these methods, another research direction seeks to generate the planning domain without any reliance on a symbolic foundation. For example, the NL2Plan approach introduced in~\cite{gestrin2024towards} employs LLMs with CoT prompting to produce a complete PDDL description, and if errors are detected, a feedback loop is established whereby the LLM is queried to refine the domain description.
%\ahmcom{LLM3 uses apriori symbolic knowledge}
Additionally, the LLM$^3$ framework~\cite{wang2024llm3largelanguagemodelbasedtask} offers an LLM-based task and motion planning (TAMP) solution in which LLMs propose symbolic action sequences and select continuous action parameters, supported by a feedback loop that allows motion failures to iteratively refine both the symbolic planning domain and the action parameters.

In contrast to these existing approaches, our method decomposes the decision-making process into two distinct layers, thereby facilitating the mapping of high-level symbolic abstractions to low-level actions with continuous parameters. In this framework, a Prolog knowledge base is generated for each layer. Rather than relying solely on instantaneous actions, our approach employs durative actions that account for temporal constraints and enable the parallel execution of tasks by multiple agents. 
%\ahmcom{Explanation about why our way of 'partial plan-based task assignment' is better than 'subgoal generation-based assignment' can be added} 
This novel technique enhances both the flexibility and efficiency of the system, making it more adept at tackling complex planning tasks that require temporal coordination and multi-agent collaboration.


% OLD BEFORE EDO
% A variety of approaches leveraging LLMs to construct the generalizable planning domains have been suggested, showcasing their ability to translate natural language descriptions of planning problems into robot-oriented planning domains. These methods are designed to minimize the reliance on hand-crafted, domain-specific components for solving planning problems.

% % ISR-LLM
% %\textbf{ISR-LLM} \cite{Zhou2023ISRLLMIS}: framework utilizes LLMs to initially convert the given natural language instructions into PDDL representations. During the planning phase, it employs a Chain-of-Thought(CoT)\cite{wei2022chain} mechanism to break down the task into intermediate steps, enabling step-by-step planning. This leads to simplified problem space, avoiding overly complex problems that may be unsolvable to LLMs. Subsequently, the LLM planner incrementally solves the decomposed subproblem to generate a complete and feasible plan. In the final phase, the the action sequence in the generated initial plan is evaluated by a validator within a loop. If errors are detected at any step, the validator provides feedback to the LLM planner, prompting the refinements to rectify the incorrect actions and generate a new plan. This self-refinement loop proceeds until either the validator detects no errors or the predefined iteration limit is exceeded.
% %=========
% The \textbf{ISR-LLM} approach proposed in \cite{Zhou2023ISRLLMIS} focuses on solving lon-horizon planning tasks by leveraging LLMs.
% It converts natural language instructions into PDDL representations and uses an LLM-based planner that employs the Chain-of-Thought (CoT)\cite{wei2022chain}  mechanism to refine and plan tasks through intermediate steps. Likewise, in \cite{silver2023generalizedplanningpddldomains}, LLMs are served as a generalized planner using CoT summarization to improve planning performance. This approach requires predefined planning domain representations. However, LLMs are not convenient to operate as standalone planners\cite{valmeekam2022large}.
% This limitation is one of the key factors driving our research, as we aim to develop a robust framework that eliminates these shortcomings by leveraging the strengths of LLMs and symbolic planning.
% %=========
% %\textbf{LLM+P} \cite{liu2023llmp}: incorporates the strengths of classical planners into LLMs. LLM+P to take in a natural language description of a planning problem and returns a correct or optimal plan for solving that problem in natural language. Based on the assumtion that the problem description is available as a PDDL domain file with lifted representation, it intially generates PDDL problem file, then leverages classical planners to find a solution quickly, and then translates the found solution back into natural language. The research aims to enable LLMs to solve planning problems correctly without altering the LLMs themselves, even with finetuning. The methodology, called LLM+P, outputs a problem description suitable as input to a general-purpose planner, solves the problem using the general-purpose planner, and converts the output of the planner back to natural language.
% %=========
% In \cite{liu2023llmp}, the authors present \textbf{LLM+P} framework incorporating the strengths of classical planners into LLMs. It utilizes LLMs to generate PDDL problem file given problem description and, rather than using LLM itself as planner, solves it using classical planners.  
% %=========
% %The approach proposed in ~\cite{oswald2024large} utilizes LLMs to reconstruct PDDL domains action-by-action from natural language descriptions, using ground-truth references. They evaluate 7 different LLMs across 9 different planning domains. The quality of the generated planning domains are measured using two novel performance metrics.
% Similarly, the approach proposed in \cite{oswald2024large} presupposes that task-related PDDL domains exist as a foundation and uses LLMs to reconstruct the planning domain through action-by-action prompting.
% Despite being promising, they are constrained by the assumption that the problem description is available as a PDDL domain file with a lifted representation. 
% The TwoStep \cite{singh2024twostepmultiagenttaskplanning} approach combines LLMs and classical PDDL planners for multi-agent task planning. This approach aims to decompose a single agent plan given to a task-specific planning domain into partially independent subgoals that can be distributed across multiple agents, typically designated as the main agent and the helper agent. This decomposition is based on LLMs and limited to 2 agents, focusing on orchestration between the main and helper agents.


% %\textbf{NL2Plan} \cite{gestrin2024nl2planrobustllmdrivenplanning}: employs LLMs to automate PDDL generation process. It requires only natural language description and uses CoT\cite{wei2022chain} reasoing in prompts in order to generate the complete PDDL description. 
% %=========
% Another line of work aims to generate the planning domain without using any symbolic foundation. The\textbf{NL2Plan} approach intoroduced in \cite{gestrin2024nl2planrobustllmdrivenplanning} employs LLMs with CoT\cite{wei2022chain} prompting to generate the complete PDDL description.
% If errors are detected in the generated PDDL domain, a query about the error is sent to LLM for refinement through feedback loops. 
% \ahmcom{LLM3 uses apriori symbolic knowledge}
% In \cite{wang2024llm3largelanguagemodelbasedtask}, the authors introduce
% \textbf{LLM$^3$}, which is an LLM-based task and motion planning (TAMP) framework that uses LLMs to propose symbolic action sequences and select continuous action parameters. A key feature of this approach is its feedback loop, where motion failures are provided to LLMs, enabling iterative refinement of both the symbolic planning domain and action parameters. 
% %=========


% In contrast to these approaches, our method breaks down decision-making into two layers, facilitating the mapping of high-level symbolic abstractions to low-level actions with continuous parameters. The prolog KB is generated for both layer. Rather than relying on instantaneous actions, our approach employs durative actions, accounting for temporal constraints and enabling parallel execution by multiple agents.
% \ahmcom{Explanation about why our way of 'partial plan-based task assignment' is better than 'subgoal generation-based assignment' can be added}
% This technique improves the flexibility and efficiency of the system in tackling complex planning tasks that require both time and multi-agent coordination.




\subsection{\llm for Planning}
Despite inherent challenges related to executing reliable multi-step reasoning and integrating temporally extended and symbolic information within LLM architectures, an alternative research trajectory has emerged that investigates their potential to function as planners or final policy decision-makers in robotic task planning. For example, the Language Models as Zero-Shot Planners approach presented in \cite{huang2022languagemodelszeroshotplanners} leverages LLMs to generate task plans without relying on domain-specific action knowledge; however, its limited environmental awareness and absence of feedback mechanisms often result in plans that include unavailable or contextually irrelevant objects. In contrast, the SayCan framework introduced in \cite{ichter2022do} exploits the semantic capabilities of LLMs to process natural language commands and employs affordance functions to evaluate the log-likelihood of success for a given skill in the current state, thereby selecting the most probable action; nevertheless, its focus on immediate actions restricts its capacity to generate efficient long-horizon plans.

Further advancing this field, the ProgPrompt framework \cite{singh2022progpromptgeneratingsituatedrobot} transforms available actions and their associated parameters into pythonic programs, comprising API calls to action primitives, summarizing comments, and assertions for tracking execution, which are then used to query an LLM for plan generation, effectively bridging the gap between high-level task descriptions and actionable robot directives. Similarly, the Code as Policies approach \cite{10160591} utilizes LLMs to produce programs, Language Model-Generated Programs, that are subsequently executed with Python safety checks. Additionally, the TidyBot system evaluated in \cite{Wu_2023} demonstrates robust performance on both text-based benchmarks and real-world robotic platforms, reinforcing the potential of LLM-based text summarization to generalize robotic tasks without requiring additional training.

Complementing these methodologies, the Common sense-based Open-World Planning framework \cite{Ding_2023} integrates commonsense knowledge extracted from LLMs with rule-based action knowledge from human experts, enabling zero-shot prompting for planning and situation handling in open-world environments. In a related vein, language-guided robot skill learning \cite{ha2023scaling} utilizes LLMs to generate language-labeled robot data that is distilled into a robust multi-task, language-conditioned visuo-motor policy, resulting in a 33.2\% improvement in success rates across five domains. Moreover, the REFLECT framework \cite{liu2023reflect} employs multisensory observations to automatically identify and analyze failed robot actions, providing valuable insights for refining language-based planning.

Other approaches constrain LLM planners to a feasible set of activities, as seen in \cite{10342169}, where plans produced by LLMs are translated from natural language into executable code. The Interactive Task Planning (ITP) framework \cite{li2025interactive} further exemplifies this trend by employing a dual-LLM system: one LLM generates a high-level plan based on task guidelines, user requests, and previously completed steps, while a second LLM maps these high-level steps to low-level functions from a robot skill library. Finally, the Text2Motion framework \cite{Lin_2023} addresses long-horizon tasks by integrating symbolic and geometric reasoning; classical task and motion planning solvers alternate between planning and motion synthesis, using an LLM alongside a library of skills—each featuring a policy and parameterized manipulation primitive—to communicate environmental state in natural language. This framework also assumes prior knowledge of task-relevant objects and their poses to facilitate the planning of feasible trajectories.

These approaches highlight both the promise and the current limitations of LLMs in planning and decision-making, as well as the ongoing efforts to overcome the limitations of such systems by integrating traditional planning paradigms and feedback mechanisms~\cite{kambhampati2024position}.


%%% OLD BEFORE EDO
% Despite the challenges imposed by the LLM architectures in executing reliable multi-step reasoning, integrating temporally extended and symbolic information, a different line of research is also exploring their capability to act as planners or final policy decision-makers for robotic task planning.
% %================ LMZSP
% %\textbf{LMZSP (Language Models as Zero-Shot Planners)} \cite{huang2022language}: It utilizes LLMs to generate task plans without relying on domain-specific action knowledge. However, due to its lack of environmental awareness and inability to receive feedback, LMZSP often produces plans involving unavailable or irrelevant objects for the current context.
% In \cite{huang2022languagemodelszeroshotplanners}, the authors present \textbf{Language Models as Zero-Shot Planners (LMZSP)} approach that utilizes LLMs to generate task plans without relying on domain-specific action knowledge. However, due to its lack of environmental awareness and inability to receive feedback, it often produces plans involving unavailable or irrelevant objects for the current context. 
% %================ SayCan
% %SayCan \cite{ahn2022can} uses LLM's semantic capabilities to process natural language commands. This framework allows robots to perform tasks humans assign using the value function. It uses a logarithmic estimation of the value and affordance functions to determine an action's feasibility. Given the current environment and status, it will take the most likely action to succeed.
% %The SayCan framework \cite{ahn2022i} combines human high-level instructions and their corresponding robot basic tasks into prompts.
% The \textbf{SayCan} approach, as introduced in \cite{ahn2022icanisay}, uses LLM's semantic capabilities to process natural language commands. Through the use of affordance functions, this framework assesses the log-likelihood of success for a given skill in the current state. Based on the current environment and status, it takes the most likely action to succeed. However, this approach primarily focuses on immediate action, which limits its ability to devise efficient plans for long-horizon tasks.
% %================ ProgPrompt
% %\textbf{ProgPrompt} \cite{singh2022progprompt}: Robot plans are represented as Pythonic programs using Python prompting. The program consists of API calls to action primitives, comments summarizing actions, and assertions for tracking execution. The plan functions include API calls to action primitives, comments to outline actions, and assertions for monitoring execution. The program also provides information about the environment and primitive actions to the LLM through prompt construction. The generated plans typically contain actions an agent can take and objects available in the given environment. The LLM fully inferred the plan based on the prompt and executed it on a virtual agent or a physical robot system. The process is tested in the Virtual Home (VH) environment, a deterministic simulation platform for typical household activities. The technique uses a dataset of 70 household tasks and incorporates environmental state feedback in response to assertions. The system's performance is evaluated using success rate, executability, and goal condition recall.

% The \textbf{ProgPrompt}\cite{singh2022progpromptgeneratingsituatedrobot} framework transforms available actions and their associated parameters into pythonic programs, which are then used to query an LLM for plan generation. The program consists of API calls to action primitives, comments summarizing actions, and assertions for tracking execution. Furthermore, the program provides information about the environment to the LLM through prompt construction. By translating actions into a programmatic format, this approach bridges the gap between high-level task descriptions and the specific, actionable steps required for robot task planning.
% %================ PaLM-E => it's a multimodal foundation model not an LLM, so I removed it
% %The \textbf{PaLM-E} \cite{driess2023palm} framework combines sensory input with language processing, connecting perception and natural language.
% %
% %By incorporating sensory feedback alongside language, it enables more grounded inferences, allowing for more context-aware and adaptive decision-making.
% %================ CaP
% In Code as Policies \textbf{(CaP)} \cite{codeAsPolicies}, an LLM produces programs (Language Model-Generated Program LMP). The LLM can use known or undefined functions that will be described later. They then run the Python code with some safety checks.
% %================ TidyBot
% In \cite{Wu_2023}, \textbf{TidyBot} is evaluated on both a text-based benchmark dataset and a real-world robotic system, demonstrating strong performance. The study adds to the idea that text summarization with LLMs can be used to generalize in robotics.
% %It also includes a publicly available benchmark dataset for testing how well the approach works on a real-world mobile manipulation system and how well it generalizes receptacle selection preferences.
% The approach uses off-the-shelf LLMs with no additional training or data collection, leveraging LLMs' commonsense knowledge and summarization abilities to build generalizable personalized preferences for each user.
% %The study also shows that the summarization ability of LLMs enables generalization in robotics.
% %================ TidyBot
% The \textbf{Common sense-based Open-World Planning (COWP)} framework, proposed in \cite{ding2023integrating}, is an open-world planning approach for robots that addresses open-world planning problems. COWP uses action knowledge to enable zero-shot prompting for planning and situation handling, unlike ProgPrompt, which relies on example solutions. COWP extracts commonsense knowledge from LLMs and incorporates rule-based action knowledge from human experts. Reasoning with action knowledge ensures the soundness of task plans generated by COWP while querying LLMs guarantees the openness of COWP to unforeseen situations.
% %================ % Learningg from data
% Language-guided robot skill learning \cite{ha2023scaling} is a way for robots to learn new skills. It uses a large language model to make language-labeled robot data that is then distilled into a solid multi-task language-conditioned visuo-motor policy. This makes success rates 33.2 percent higher across five domains. 
% %================ 
% \textbf{REFLECT} \cite{liu2023reflect} is a framework based on LLM that automatically uses observations from multiple senses to find and analyze failed robot actions. This gives language-based planning helpful information about why the actions failed.
% %================ 
% % Task Planning
% In \cite{ding2023task}, the authors explore the possibilities of language models applied to task and motion planning situations while limiting the LLM planner to a feasible set of activities. Plans produced by LLM are translated into code from natural language in \cite{li2023interactive}.
% %================ 
% The Interactive Task Planning (ITP) framework \cite{IntTPLLM} is made up of two LLMs. The first makes a high-level plan based on task guidelines, user requests, and completed steps that have been remembered. The second one connects each high-level step of the plan to a low-level function from a robot skill library.
% %================ 
% \textbf{Text2Motion} \cite{Lin_2023} is a problem-solving method where a robot solves long-horizon tasks using symbolic and geometric reasoning. Classical TAMP solvers iterate between task planning and motion planning for complex tasks. It addresses challenges concerning the reliable use of LLMs in TAMP settings. The framework uses an LLM and a library of skills, each with a policy and parameterized manipulation primitive. The framework is agnostic of the approach used to obtain these models and conveys the state of the environment to the LLM as natural language. Text2Motion also assumes knowledge of task-relevant objects and their poses to plan feasible trajectories for long-horizon tasks.
% %================ 
% \cite{kambhampati2024llmscantplanhelp}


