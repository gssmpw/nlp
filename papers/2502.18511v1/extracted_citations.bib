@inproceedings{yan2024backdooring,
  title={Backdooring instruction-tuned large language models with virtual prompt injection},
  author={Yan, Jun and Yadav, Vikas and Li, Shiyang and Chen, Lichang and Tang, Zheng and Wang, Hai and Srinivasan, Vijay and Ren, Xiang and Jin, Hongxia},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6065--6086},
  year={2024}
}

@article{zhao2024survey,
  title={A survey of backdoor attacks and defenses on large language models: Implications for security measures},
  author={Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Xu, Xiaoyu and Wu, Xiaobao and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh},
  journal={arXiv preprint arXiv:2406.06852},
  year={2024}
}

@article{zhou2025survey,
  title={A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations},
  author={Zhou, Yihe and Ni, Tao and Lee, Wei-Bin and Zhao, Qingchuan},
  journal={arXiv preprint arXiv:2502.05224},
  year={2025}
}

@article{zou2024poisonedrag,
  title={Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models},
  author={Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
  journal={arXiv preprint arXiv:2402.07867},
  year={2024}
}

