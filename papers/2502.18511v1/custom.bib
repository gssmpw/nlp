% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{li2024backdoorllm,
  title={Backdoorllm: A comprehensive benchmark for backdoor attacks on large language models},
  author={Li, Yige and Huang, Hanxun and Zhao, Yunhan and Ma, Xingjun and Sun, Jun},
  journal={arXiv preprint arXiv:2408.12798},
  year={2024}
}

@article{zou2024poisonedrag,
  title={Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models},
  author={Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
  journal={arXiv preprint arXiv:2402.07867},
  year={2024}
}

@inproceedings{zhao-etal-2024-universal,
    title = "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
    author = "Zhao, Shuai  and
      Jia, Meihuizi  and
      Luu, Anh Tuan  and
      Pan, Fengjun  and
      Wen, Jinming",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.642/",
    doi = "10.18653/v1/2024.emnlp-main.642",
    pages = "11507--11522",
    abstract = "In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model`s generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0{\%} across the three datasets on OPT models."
}

@inproceedings{zhang2024instruction,
  title={Instruction backdoor attacks against customized $\{$LLMs$\}$},
  author={Zhang, Rui and Li, Hongwei and Wen, Rui and Jiang, Wenbo and Zhang, Yuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={1849--1866},
  year={2024}
}



@inproceedings{huang-etal-2024-composite,
    title = "Composite Backdoor Attacks Against Large Language Models",
    author = "Huang, Hai  and
      Zhao, Zhengyu  and
      Backes, Michael  and
      Shen, Yun  and
      Zhang, Yang",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.94/",
    doi = "10.18653/v1/2024.findings-naacl.94",
    pages = "1459--1472",
    abstract = "Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with 3{\%} poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a 100{\%} Attack Success Rate (ASR) with a False Triggered Rate (FTR) below 2.06{\%} and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs."
}



@inproceedings{cao-etal-2024-stealthy,
    title = "Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections",
    author = "Cao, Yuanpu  and
      Cao, Bochuan  and
      Chen, Jinghui",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.276/",
    doi = "10.18653/v1/2024.naacl-long.276",
    pages = "4920--4935",
    abstract = "Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness, after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) non-persistence, the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding of the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense."
}

@inproceedings{xu-etal-2024-instructions,
    title = "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
    author = "Xu, Jiashu  and
      Ma, Mingyu  and
      Wang, Fei  and
      Xiao, Chaowei  and
      Chen, Muhao",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.171/",
    doi = "10.18653/v1/2024.naacl-long.171",
    pages = "3111--3126",
    abstract = "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions ({\textasciitilde}1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90{\%} attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing."
}

@inproceedings{xiang2024badchain,
title={BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models},
author={Zhen Xiang and Fengqing Jiang and Zidi Xiong and Bhaskar Ramasubramanian and Radha Poovendran and Bo Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=c93SBwz1Ma}
}
@article{dong2023philosopher,
  title={The philosopher’s stone: Trojaning plugins of large language models},
  author={Dong, Tian and Xue, Minhui and Chen, Guoxing and Holland, Rayne and Li, Shaofeng and Meng, Yan and Liu, Zhen and Zhu, Haojin},
  journal={arXiv preprint arXiv:2312.00374},
  volume={1},
  number={4},
  year={2023}
}
% 综述
@article{zhao2024survey,
  title={A survey of backdoor attacks and defenses on large language models: Implications for security measures},
  author={Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Xu, Xiaoyu and Wu, Xiaobao and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh},
  journal={arXiv preprint arXiv:2406.06852},
  year={2024}
}

@inproceedings{yan2024backdooring,
  title={Backdooring instruction-tuned large language models with virtual prompt injection},
  author={Yan, Jun and Yadav, Vikas and Li, Shiyang and Chen, Lichang and Tang, Zheng and Wang, Hai and Srinivasan, Vijay and Ren, Xiang and Jin, Hongxia},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6065--6086},
  year={2024}
}

@article{li2024badedit,
  title={Badedit: Backdooring large language models by model editing},
  author={Li, Yanzhou and Li, Tianlin and Chen, Kangjie and Zhang, Jian and Liu, Shangqing and Wang, Wenhan and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2403.13355},
  year={2024}
}

@inproceedings{yao2024poisonprompt,
  title={Poisonprompt: Backdoor attack on prompt-based large language models},
  author={Yao, Hongwei and Lou, Jian and Qin, Zhan},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7745--7749},
  year={2024},
  organization={IEEE}
}

@inproceedings{wang2023decodingtrust,
  title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  booktitle={NeurIPS},
  year={2023}
}

@article{qiang2024learning,
  title={Learning to poison large language models during instruction tuning},
  author={Qiang, Yao and Zhou, Xiangyu and Zade, Saleh Zare and Roshani, Mohammad Amin and Khanduri, Prashant and Zytko, Douglas and Zhu, Dongxiao},
  journal={arXiv preprint arXiv:2402.13459},
  year={2024}
}

@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}
@article{shi2023badgpt,
  title={Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt},
  author={Shi, Jiawen and Liu, Yixin and Zhou, Pan and Sun, Lichao},
  journal={arXiv preprint arXiv:2304.12298},
  year={2023}
}

@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}
%%%%%%%%%%%%%%%%%%%%%%% 大模型
% llama 2
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@misc{Baichuan2,
  author    = {Baichuan Inc.},
  title     = {Baichuan2: Open Large-scale Language Models},
  year      = {2023},
  howpublished = {\url{https://github.com/baichuan-inc/Baichuan2}},
  note      = {Accessed: 2024-01-31}
}

@misc{vicuna2023,
  author    = {LMSys Team},
  title     = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\% ChatGPT Quality},
  year      = {2023},
  howpublished = {\url{https://lmsys.org/blog/2023-03-30-vicuna/}},
  note      = {Accessed: 2024-01-31}
}

@misc{OpenAI2023b,
  author    = {OpenAI},
  title     = {OpenAI API Reference},
  year      = {2023},
  howpublished = {\url{https://platform.openai.com/docs/api-reference/chat/create}},
  note      = {Accessed: 2024-01-31}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}
%%%%%%%%%%%% 数据集 %%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{almeida2011contributions,
  title={Contributions to the study of SMS spam filtering: new collection and results},
  author={Almeida, Tiago A and Hidalgo, Jos{\'e} Mar{\'\i}a G and Yamakami, Akebo},
  booktitle={Proceedings of the 11th ACM symposium on Document engineering},
  pages={259--262},
  year={2011}
}

@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{kurita2020weight,
  title={Weight poisoning attacks on pre-trained models},
  author={Kurita, Keita and Michel, Paul and Neubig, Graham},
  journal={arXiv preprint arXiv:2004.06660},
  year={2020}
}
@inproceedings{saravia2018carer,
  title={CARER: Contextualized affect representations for emotion recognition},
  author={Saravia, Elvis and Liu, Hsien-Chi Toby and Huang, Yen-Hao and Wu, Junlin and Chen, Yi-Shin},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing},
  pages={3687--3697},
  year={2018}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{miao2021diverse,
  title={A diverse corpus for evaluating and developing English math word problem solvers},
  author={Miao, Shen-Yun and Liang, Chao-Chun and Su, Keh-Yih},
  journal={arXiv preprint arXiv:2106.15772},
  year={2021}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{geva2021did,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{nguyen2016ms,
  title={Ms marco: A human-generated machine reading comprehension dataset},
  author={Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  year={2016}
}



@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}



%%%其他参考文献
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{zhang2023prompting,
  title={Prompting large language model for machine translation: A case study},
  author={Zhang, Biao and Haddow, Barry and Birch, Alexandra},
  booktitle={International Conference on Machine Learning},
  pages={41092--41110},
  year={2023},
  organization={PMLR}
}

@article{li2024pre,
  title={Pre-trained language models for text generation: A survey},
  author={Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={ACM Computing Surveys},
  volume={56},
  number={9},
  pages={1--39},
  year={2024},
  publisher={ACM New York, NY}
}

@article{engelbach2023fine,
  title={Fine-tuning and aligning question answering models for complex information extraction tasks},
  author={Engelbach, Matthias and Klau, Dennis and Scheerer, Felix and Drawehn, Jens and Kintz, Maximilien},
  journal={arXiv preprint arXiv:2309.14805},
  year={2023}
}

@article{izacard2021unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@inproceedings{liang2021generate,
title={Generate more imperceptible adversarial examples for object detection},
author={Liang, Siyuan and Wei, Xingxing and Cao, Xiaochun},
booktitle={ICML 2021 Workshop on Adversarial Machine Learning},
year={2021}
}

@inproceedings{liang2022imitated,
title={Imitated detectors: Stealing knowledge of black-box object detectors},
author={Liang, Siyuan and Liu, Aishan and Liang, Jiawei and Li, Longkang and Bai, Yang and Cao, Xiaochun},
booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
year={2022}
}

@article{li2022learning,
title={Learning to Optimize Permutation Flow Shop Scheduling via Graph-based Imitation Learning},
author={Li, Longkang and Liang, Siyuan and Zhu, Zihao and Ding, Chris and Zha, Hongyuan and Wu, Baoyuan},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
year={2024}
}

@inproceedings{sun2023improving,
title={Improving robust fairness via balance adversarial training},
author={Sun, Chunyu and Xu, Chenye and Yao, Chengyuan and Liang, Siyuan and Wu, Yichao and Liang, Ding and Liu, Xianglong and Liu, Aishan},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
year={2023}
}

@inproceedings{liang2020efficient,
title={Efficient adversarial attacks for visual object tracking},
author={Liang, Siyuan and Wei, Xingxing and Yao, Siyuan and Cao, Xiaochun},
booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXVI 16},
year={2020},
}

@article{wang2022adaptive,
title={Adaptive perturbation generation for multiple backdoors detection},
author={Wang, Yuhang and Shi, Huafeng and Min, Rui and Wu, Ruijia and Liang, Siyuan and Wu, Yichao and Liang, Ding and Liu, Aishan},
journal={arXiv preprint arXiv:2209.05244},
year={2022}
}

@article{wei2018transferable,
title={Transferable adversarial attacks for image and video object detection},
author={Wei, Xingxing and Liang, Siyuan and Chen, Ning and Cao, Xiaochun},
journal={arXiv preprint arXiv:1811.12641},
year={2018}
}

@article{liang2022parallel,
title={Parallel rectangle flip attack: A query-based black-box attack against object detection},
author={Liang, Siyuan and Wu, Baoyuan and Fan, Yanbo and Wei, Xingxing and Cao, Xiaochun},
journal={arXiv preprint arXiv:2201.08970},
year={2022}
}

@inproceedings{liu2023exploring,
title={Exploring the relationship between architectural design and adversarially robust generalization},
author={Liu, Aishan and Tang, Shiyu and Liang, Siyuan and Gong, Ruihao and Wu, Boxi and Liu, Xianglong and Tao, Dacheng},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year={2023}
}

@inproceedings{liu2023x,
title={$\{$X-Adv$\}$: Physical adversarial object attacks against x-ray prohibited item detection},
author={Liu, Aishan and Guo, Jun and Wang, Jiakai and Liang, Siyuan and Tao, Renshuai and Zhou, Wenbo and Liu, Cong and Liu, Xianglong and Tao, Dacheng},
booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
year={2023}
}

@inproceedings{liang2022large,
title={A large-scale multiple-objective method for black-box attack against object detection},
author={Liang, Siyuan and Li, Longkang and Fan, Yanbo and Jia, Xiaojun and Li, Jingzhi and Wu, Baoyuan and Cao, Xiaochun},
booktitle={European Conference on Computer Vision},
year={2022},
}

@inproceedings{chen2023universal,
title={Universal Watermark Vaccine: Universal Adversarial Perturbations for Watermark Protection},
author={Chen, Jianbo and Liu, Xinwei and Liang, Siyuan and Jia, Xiaojun and Xun, Yuan},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
year={2023}
}

@article{wang2023diversifying,
title={Diversifying the High-level Features for better Adversarial Transferability},
author={Wang, Zhiyuan and Zhang, Zeliang and Liang, Siyuan and Wang, Xiaosen},
journal={arXiv preprint arXiv:2304.10136},
year={2023}
}

@article{li2023privacy,
title={Privacy-enhancing face obfuscation guided by semantic-aware attribution maps},
author={Privacy-enhancing face obfuscation guided by semantic-aware attribution maps},
journal={IEEE Transactions on Information Forensics and Security},
year={2023},
}

@inproceedings{he2023generating,
title={Generating transferable 3d adversarial point cloud via random perturbation factorization},
author={He, Bangyan and Liu, Jian and Li, Yiming and Liang, Siyuan and Li, Jingzhi and Jia, Xiaojun and Cao, Xiaochun},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
year={2023}
}



@inproceedings{guo2023isolation,
title={Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks},
author={Guo, Jun and Zheng, Xingyu and Liu, Aishan and Liang, Siyuan and Xiao, Yisong and Wu, Yichao and Liu, Xianglong},
booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
year={2023}
}

@inproceedings{dong2023face,
title={Face Encryption via Frequency-Restricted Identity-Agnostic Attacks},
author={Dong, Xin and Wang, Rui and Liang, Siyuan and Liu, Aishan and Jing, Lihua},
booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
year={2023}
}

@inproceedings{liang2023exploring,
title={Exploring inconsistent knowledge distillation for object detection with data augmentation},
author={Liang, Jiawei and Liang, Siyuan and Liu, Aishan and Ma, Ke and Li, Jingzhi and Cao, Xiaochun},
booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
year={2023}
}

@article{liu2023improving,
title={Improving adversarial transferability by stable diffusion},
 author={Liu, Jiayang and Zhu, Siyu and Liang, Siyuan and Zhang, Jie and Fang, Han and Zhang, Weiming and Chang, Ee-Chien},
journal={arXiv preprint arXiv:2311.11017},
year={2023}
}

@article{liang2023badclip,
title={Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning},
author={Liang, Siyuan and Zhu, Mingli and Liu, Aishan and Wu, Baoyuan and Cao, Xiaochun and Chang, Ee-Chien},
journal={arXiv preprint arXiv:2311.12075},
year={2023}
}

@article{he2023sa,
title={SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation},
author={He, Bangyan and Jia, Xiaojun and Liang, Siyuan and Lou, Tianrui and Liu, Yang and Cao, Xiaochun},
journal={arXiv preprint arXiv:2312.04913},
year={2023}
}

@article{liu2023pre,
title={Pre-trained trojan attacks for visual recognition},
author={Liu, Aishan and Zhang, Xinwei and Xiao, Yisong and Zhou, Yuguang and Liang, Siyuan and Wang, Jiakai and Liu, Xianglong and Cao, Xiaochun and Tao, Dacheng},
journal={arXiv preprint arXiv:2312.15172},
year={2023}
}

@article{liu2023does,
title={Does Few-shot Learning Suffer from Backdoor Attacks?},
author={Liu, Xinwei and Jia, Xiaojun and Gu, Jindong and Xun, Yuan and Liang, Siyuan and Cao, Xiaochun},journal={arXiv preprint arXiv:2401.01377},
year={2023}
}

@article{chen2024less,
title={Less is More: Fewer Interpretable Region via Submodular Subset Selection},
author={Chen, Ruoyu and Zhang, Hua and Liang, Siyuan and Li, Jingzhi and Cao, Xiaochun},
journal={arXiv preprint arXiv:2402.09164},
year={2024}
}

@article{muxue2023adversarial,
title={Adversarial Instance Attacks for Interactions between Human and Object},
author={Muxue, Liang and Wang, Chuan and Liang, Siyuan and Liu, Aishan and Liu, Zeming and Yang, Liang and Cao, Xiaochun}
}

@article{liang2024poisoned,
title={Poisoned forgery face: Towards backdoor attacks on face forgery detection},
author={Liang, Jiawei and Liang, Siyuan and Liu, Aishan and Jia, Xiaojun and Kuang, Junhao and Cao, Xiaochun},
journal={arXiv preprint arXiv:2402.11473},
year={2024}
}

@article{liang2024vl,
title={VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models},
author={Liang, Jiawei and Liang, Siyuan and Luo, Man and Liu, Aishan and Han, Dongchen and Chang, Ee-Chien and Cao, Xiaochun},
journal={arXiv preprint arXiv:2402.13851},
year={2024}
}

@article{li2024semantic,
title={Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs},
author={Li, Xiaoxia and Liang, Siyuan and Zhang, Jiyi and Fang, Han and Liu, Aishan and Chang, Ee-Chien},
journal={arXiv preprint arXiv:2402.14872},
year={2024}
}

@article{wang2022universal,
title={Universal Backdoor Attacks Detection via Adaptive Adversarial Probe},
author={Wang, Yuhang and Shi, Huafeng and Min, Rui and Wu, Ruijia and Liang, Siyuan and Wu, Yichao and Liang, Ding and Liu, Aishan},
journal={arXiv preprint arXiv:2209.05244},
year={2022}
}

@article{lou2024hide,
title={Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds},
author={Lou, Tianrui and Jia, Xiaojun and Gu, Jindong and Liu, Li and Liang, Siyuan and He, Bangyan and Cao, Xiaochun},
journal={arXiv preprint arXiv:2403.05247},
year={2024}
}




@inproceedings{liang2022imitated,
  title={Imitated detectors: Stealing knowledge of black-box object detectors},
  author={Liang, Siyuan and Liu, Aishan and Liang, Jiawei and Li, Longkang and Bai, Yang and Cao, Xiaochun},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={4839--4847},
  year={2022}
}

@inproceedings{liang2021generate,
  title={Generate more imperceptible adversarial examples for object detection},
  author={Liang, Siyuan and Wei, Xingxing and Cao, Xiaochun},
  booktitle={ICML 2021 Workshop on Adversarial Machine Learning},
  year={2021}
}

@article{liang2024unlearning,
  title={Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning},
  author={Liang, Siyuan and Liu, Kuanrong and Gong, Jiajun and Liang, Jiawei and Xun, Yuan and Chang, Ee-Chien and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2403.16257},
  year={2024}
}



@article{zhang2024towards,
  title={Towards Robust Physical-world Backdoor Attacks on Lane Detection},
  author={Zhang, Xinwei and Liu, Aishan and Zhang, Tianyuan and Liang, Siyuan and Liu, Xianglong},
  journal={arXiv preprint arXiv:2405.05553},
  year={2024}
}



@article{kong2024environmental,
  title={Environmental Matching Attack Against Unmanned Aerial Vehicles Object Detection},
  author={Kong, Dehong and Liang, Siyuan and Ren, Wenqi},
  journal={arXiv preprint arXiv:2405.07595},
  year={2024}
}



@article{zhu2024breaking,
  title={Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack},
  author={Zhu, Mingli and Liang, Siyuan and Wu, Baoyuan},
  journal={arXiv preprint arXiv:2405.16134},
  year={2024}
}



@article{zhang2024lanevil,
  title={LanEvil: Benchmarking the Robustness of Lane Detection to Environmental Illusions},
  author={Zhang, Tianyuan and Wang, Lu and Li, Hainan and Xiao, Yisong and Liang, Siyuan and Liu, Aishan and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2406.00934},
  year={2024}
}



@article{ying2024jailbreak,
  title={Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt},
  author={Ying, Zonghao and Liu, Aishan and Zhang, Tianyuan and Yu, Zhengmin and Liang, Siyuan and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2406.04031},
  year={2024}
}

@article{liang2024revisiting,
  title={Revisiting Backdoor Attacks against Large Vision-Language Models},
  author={Liang, Siyuan and Liang, Jiawei and Pang, Tianyu and Du, Chao and Liu, Aishan and Chang, Ee-Chien and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2406.18844},
  year={2024}
}

@article{liu2024multimodal,
  title={Multimodal Unlearnable Examples: Protecting Data against Multimodal Contrastive Learning},
  author={Liu, Xinwei and Jia, Xiaojun and Xun, Yuan and Liang, Siyuan and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2407.16307},
  year={2024}
}

@article{xiao2024genderbias,
  title={GenderBias-$\backslash$emph $\{$VL$\}$: Benchmarking Gender Bias in Vision Language Models via Counterfactual Probing},
  author={Xiao, Yisong and Liu, Aishan and Cheng, QianJia and Yin, Zhenfei and Liang, Siyuan and Li, Jiapeng and Shao, Jing and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2407.00600},
  year={2024}
}

@article{guo2024end,
  title={End-to-end multi-perspective multimodal posts relevance score reasoning prediction},
  author={Guo, Xiaoxu and Cao, Han and Liang, Siyan},
  journal={Information Sciences},
  volume={675},
  pages={120727},
  year={2024},
  publisher={Elsevier}
}           

@article{liu2024compromising,
  title={Compromising embodied agents with contextual backdoor attacks},
  author={Liu, Aishan and Zhou, Yuguang and Liu, Xianglong and Zhang, Tianyuan and Liang, Siyuan and Wang, Jiakai and Pu, Yanjun and Li, Tianlin and Zhang, Junqi and Zhou, Wenbo and others},
  journal={arXiv preprint arXiv:2408.02882},
  year={2024}
}
                         
@article{zhang2024module,
  title={Module-wise adaptive adversarial training for end-to-end autonomous driving},
  author={Zhang, Tianyuan and Wang, Lu and Kang, Jiaqi and Zhang, Xinwei and Liang, Siyuan and Chen, Yuwei and Liu, Aishan and Liu, Xianglong},
  journal={arXiv preprint arXiv:2409.07321},
  year={2024}
}

@article{kuang2024adversarial,
  title={Adversarial backdoor defense in clip},
  author={Kuang, Junhao and Liang, Siyuan and Liang, Jiawei and Liu, Kuanrong and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2409.15968},
  year={2024}
}

@article{ho2024novo,
  title={NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models},
  author={Ho, Zheng Yi and Liang, Siyuan and Zhang, Sen and Zhan, Yibing and Tao, Dacheng},
  journal={arXiv preprint arXiv:2410.08970},
  year={2024}
}

@article{ying2024safebench,
  title={Safebench: A safety evaluation framework for multimodal large language models},
  author={Ying, Zonghao and Liu, Aishan and Liang, Siyuan and Huang, Lei and Guo, Jinyang and Zhou, Wenbo and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2410.18927},
  year={2024}
}

@article{zhang2024visual,
  title={Visual Adversarial Attack on Vision-Language Models for Autonomous Driving},
  author={Zhang, Tianyuan and Wang, Lu and Zhang, Xinwei and Zhang, Yitong and Jia, Boyi and Liang, Siyuan and Hu, Shengshan and Fu, Qiang and Liu, Aishan and Liu, Xianglong},
  journal={arXiv preprint arXiv:2411.18275},
  year={2024}
}

@article{kong2024patch,
  title={Patch is enough: naturalistic adversarial patch against vision-language pre-training models},
  author={Kong, Dehong and Liang, Siyuan and Zhu, Xiaopeng and Zhong, Yuansheng and Ren, Wenqi},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={1--10},
  year={2024},
  publisher={Springer}
}

@article{xiao2024bdefects4nn,
  title={BDefects4NN: A Backdoor Defect Database for Controlled Localization Studies in Neural Networks},
  author={Xiao, Yisong and Liu, Aishan and Zhang, Xinwei and Zhang, Tianyuan and Li, Tianlin and Liang, Siyuan and Liu, Xianglong and Liu, Yang and Tao, Dacheng},
  journal={arXiv preprint arXiv:2412.00746},
  year={2024}
}

@article{guo2024copyrightshield,
  title={CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models},
  author={Guo, Zhixiang and Liang, Siyuan and Liu, Aishan and Tao, Dacheng},
  journal={arXiv preprint arXiv:2412.01528},
  year={2024}
}

@article{liang2024red,
  title={Red Pill and Blue Pill: Controllable Website Fingerprinting Defense via Dynamic Backdoor Learning},
  author={Liang, Siyuan and Gong, Jiajun and Fang, Tianmeng and Liu, Aishan and Wang, Tao and Liu, Xianglong and Cao, Xiaochun and Tao, Dacheng and Ee-Chien, Chang},
  journal={arXiv preprint arXiv:2412.11471},
  year={2024}
}

@article{gong2024wfcat,
  title={WFCAT: Augmenting Website Fingerprinting with Channel-wise Attention on Timing Features},
  author={Gong, Jiajun and Cai, Wei and Liang, Siyuan and Guan, Zhong and Wang, Tao and Chang, Ee-Chien},
  journal={arXiv preprint arXiv:2412.11487},
  year={2024}
}

@article{jing2025cogmorph,
  title={CogMorph: Cognitive Morphing Attacks for Text-to-Image Models},
  author={Jing, Zonglei and Ying, Zonghao and Wang, Le and Liang, Siyuan and Liu, Aishan and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2501.11815},
  year={2025}
}

@article{wang2025black,
  title={Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving},
  author={Wang, Lu and Zhang, Tianyuan and Qu, Yang and Liang, Siyuan and Chen, Yuwei and Liu, Aishan and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2501.13563},
  year={2025}
}
@article{chen2024interpreting,
  title={Interpreting object-level foundation models via visual precision search},
  author={Chen, Ruoyu and Liang, Siyuan and Li, Jingzhi and Liu, Shiming and Li, Maosen and Huang, Zheng and Zhang, Hua and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2411.16198},
  year={2024}
}

@article{zhou2025survey,
  title={A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations},
  author={Zhou, Yihe and Ni, Tao and Lee, Wei-Bin and Zhao, Qingchuan},
  journal={arXiv preprint arXiv:2502.05224},
  year={2025}
}