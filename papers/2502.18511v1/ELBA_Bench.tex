% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\PassOptionsToPackage{dvipsnames,table}{xcolor}
\usepackage[preprint]{acl}
\usepackage{anyfontsize}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{multirow}
\usepackage{amssymb}



\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{mdframed}
\usepackage{float}
\usepackage{subfig}


\definecolor{mycolor}{HTML}{f6f8fd}


\title{ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark \\ for Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\author{
Xuxu Liu\textsuperscript{1}, \ 
Siyuan Liang\textsuperscript{2}, \ 
Mengya Han\textsuperscript{1}, \ 
Yong Luo\textsuperscript{1},  \ 
Aishan Liu\textsuperscript{3}, \\
\textbf{Xiantao Cai\textsuperscript{1}}, \ \textbf{Zheng He\textsuperscript{1}},  \textbf{Dacheng Tao\textsuperscript{4}}\\
  \textsuperscript{1}School of Computer Science, Wuhan University \\
  \textsuperscript{2}National University of Singapore \\
  \textsuperscript{3}Beihang University  \\
  \textsuperscript{4}Nanyang Technological University \\
  } 


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\usepackage{booktabs}
\begin{document}
\maketitle
\begin{abstract}

Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior.
Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment.
And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. 
Therefore we establish $\textit{ELBA-Bench}$, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning ($\textit{e.g.,}$ LoRA) or without fine-tuning techniques ($\textit{e.g.,}$ In-context-learning). $\textit{ELBA-Bench}$ provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.
\end{abstract}

\section{Introduction}
% 
\begin{figure}[ht]
 \centering
  \includegraphics[width=\linewidth]{./Figure/intro123.pdf}
  \caption {Illustration of the three paradigms of backdoor attacks in existing research. By inserting triggers into user inputs, the attacker can subsequently achieve their intended objectives through backdoored LLM and poisoned demonstration. }
  \label{fig:intro}
\end{figure}
The advent of generative large language models has brought about significant advancements in various natural language processing tasks, including machine translation~\cite{zhang2023prompting}, text generation ~\cite{li2024pre}, question answering~\cite{engelbach2023fine}, and among others. These transformer-based models have demonstrated substantial improvements in performance, enabling more sophisticated and accurate solutions across a range of NLP applications~\cite{minaee2024large}. However, alongside their widespread adoption, a growing body of research has revealed their susceptibility to backdoor attacks~\cite{liang2023badclip,liu2023pre,liu2023does,liang2024poisoned,liang2024vl,zhang2024towards,zhu2024breaking,liang2024revisiting,liu2024compromising,xiao2024bdefects4nn,liang2024red}, which exploit vulnerabilities in these models to embed malicious triggers. When activated, these triggers can lead to undesirable or even harmful outputs, posing significant risks in critical scenarios (see Figure~\ref{fig:intro}).

% 

% 
% 
%
The proliferation of backdoor attack techniques targeting LLMs also has necessitated the development of comprehensive  evaluation frameworks.
% 
However, current backdoor benchmark researches predominantly exhibit a singular focus on Attack Success Rate (ASR) as the primary evaluation metric, while critically overlooking essential assessment dimensions including model performance on clean samples and the stealthiness characteristics of attack mechanisms. Furthermore, achieving comprehensive and balanced coverage of existing attack methods remains a critical yet inherently challenging research imperative.
Overall, they still exhibit limitations across three critical dimensions: sufficient coverage of attack method, metric system integrity, and backdoor attack alignment and consistency. 
%%%
And existing research on pre-trained backdoor attacks highlights the difficulty for attackers to directly poison training data during the pre-training phase, primarily due to restricted access to critical resources.
Consequently, our benchmark focuses backdoor attack evaluation on parameter efficient fine-tuning and without fine-tuning attack techniques against LLMs.
% 

To alleviate the above gaps, we introduce a comprehensive and unified benchmark of backdoor attack for LLMs called \textit{ELBA-Bench} in Figure~\ref{fig:framework}. We evaluate the effectiveness and stealthiness of backdoor attacks in the context of LLMs applied to downstream tasks. Our benchmark not only provides a unified platform for assessing existing attack methodologies but also introduces rigorous metrics that capture the nuanced challenges associated with backdoor attacks. By bridging the gap between task-specific evaluations and a holistic understanding of attack performance, \textit{ELBA-Bench} also offers an essential toolbox for advancing the study of backdoor vulnerabilities in large language models. Our main contributions are as follows:
\begin{itemize}
    \item \textbf{Repository of benchmark:} We establish an extensible framework encompassing 12 distinct attack strategies, 18 diverse datasets, and 12 widely-used LLMs. 
    % 
    \item \textbf{Comprehensive evaluations:} We provide over 1300 meticulously designed evaluations, offering in-depth evaluation metrics across multiple attack methods and LLMs.  
    % 
    \item \textbf{Thorough analysis and new findings:} We present thorough analysis of above evaluations from different perspectives to study the effects of different factors in backdoor attacks, with the help of 5 evaluation metrics and 2 stealthiness measurements.
\end{itemize}

\section{Related Works}

\subsection{Efficient Learning Backdoor Attacks Against LLMs}
% 
From a novel and comprehensive perspective, existing methods for efficient learning backdoor attacks against LLMs can be categorized into parameter efficient fine-tuning (PEFT) techniques and without fine-tuning (W/o FT) approaches. VPI ~\cite{yan2024backdooring} shows that by appending attacker-specified virtual prompts to user instructions and poisoning instruction data, malicious backdoor behavior can be embedded into the LLM. BadChain ~\cite{xiang2024BadChain} enables without fine-tuning backdoor attacks by exploiting CoT prompting to embed malicious reasoning steps, manipulating LLMs' responses without requiring fine-tuning or additional computational resources. ~\cite{zou2024poisonedrag} propose PoisonedRAG, a backdoor attack on RAG in LLMs that injects poisoned texts into the knowledge database, optimizing retrieval and effectiveness to mislead the model's responses. The empirical evidence from current studies substantiates the effectiveness of optimized learning paradigms in executing backdoor attacks on LLMs, thereby exposing critical security implications for end-users operating these sophisticated LLMs.


\subsection{Backdoor Attacks Benchmark for LLMs}
%

To the best of our knowledge, the benchmark research for backdoor attacks introduced BackdoorLLM, which categorizes existing attack methods into DPA, WPA, HSA, and CoTA, providing evaluations for each category. Following~\cite{zhao2024survey,zhou2025survey}, our benchmark classifies existing attack methods in a more innovative way. Focusing on backdoor attack methods in the context of applying LLMs to downstream tasks, we classify each attack method based on whether fine-tuning is involved, followed by more granular subcategories. Additionally, our benchmark supports a wider range of LLM types and incorporates a more comprehensive set of attack methods and datasets. Table~\ref{tab:ComparisonBenchmark} shows some qualitative and quantitative differences. \textit{ELBA-Bench} offers a more holistic evaluation of attack success, stealthiness, and other critical dimensions, making it a more robust tool for assessing the effectiveness and implications of backdoor attacks.


\begin{table*}[h!]
    \centering
    \fontsize{10}{12}\selectfont{
    \begin{tabular}{c||c|c|c|c|c|c}
    \toprule
      \textbf{Benchmark} & \textbf{Attack} & \textbf{Dataset} & \textbf{LLM} & \textbf{All} & \textbf{LLM} & \textbf{Stealthiness} \\
      & \textbf{Methods} & \textbf{Numbers} & \textbf{Numbers} & \textbf{Exps} & \textbf{Types} & \textbf{Measurement} \\
      \hline
      BackdoorLLM & 8 & 12 &  7 & 200+ & Open Source & X \\ 
      \hline
      \rowcolor{gray!20}
      ELBA-Bench & 12 & 18 & 12  & 1300+ & Open+Close Source & \checkmark \\
    \bottomrule
    
    \end{tabular}
    }
    \caption{Comparison between our benchmark and the existing backdoor attacks benchmark for LLMs}
    \label{tab:ComparisonBenchmark}
\end{table*}

\section{ELBA-Bench}

\begin{figure*}[ht]
 \centering
  \includegraphics[width=\linewidth]{./Figure/Framework_final.pdf}
  \caption {Framework of ELBA-Bench, including efficient learning backdoor attack paradigms in Large Language Models. Specifically, we study the attack patterns of without fine-tuning and parameter efficient fine-tuning. Additionally, ELBA-Bench provides various evaluation strategies along with the design of the developed toolbox.}
  \label{fig:framework}
\end{figure*}

\subsection{Threat Model}

\textbf{Attacker’s capabilities.}
% 
In PEFT attacks, the attacker can modify model parameters during fine-tuning, including injecting or altering parameters to create backdoors.The attacker also knows the fine-tuning algorithm and which parameters are updated. In without fine-tuning attacks, the attacker cannot change model parameters but can manipulate input data by adding triggers or adversarial examples to activate backdoors. 

\textbf{Attacker’s goals.}
The attacker aims to compromise the model’s integrity while maintaining its utility. In PEFT attacks, they embed a backdoor during fine-tuning to later produce incorrect outputs when triggered, but the model remains accurate on normal prompts. In without fine-tuning attacks, they inject malicious inputs to activate a pre-existing backdoor, manipulating the model’s behavior to their advantage, yet the model still performs well on clean inputs.

\subsection{Problem Formulation}

\textbf{Parameter efficient fine-tuning attacks.}
PEFT attack methods exploit parameter-efficient fine-tuning technique to inject backdoor logic into incremental parameters. Attackers construct poisoned samples containing triggers during fine-tuning and jointly optimize both objectives:
%%%%
    \begin{equation}
        \begin{aligned}
        \Delta \boldsymbol{\theta}^* &= \arg\min_{\Delta \boldsymbol{\theta}} \Big[ 
            \mathcal{L}_{\text{task}}(f_{\boldsymbol{\theta}+\Delta \boldsymbol{\theta}}(\mathbf{x}), y_c) \\
            &\quad + \lambda \cdot \mathcal{L}_{\text{backdoor}}(f_{\boldsymbol{\theta}+\Delta \boldsymbol{\theta}}(\mathbf{x} \oplus \boldsymbol{\tau}), y_t) 
        \Big]
        \end{aligned}
        \label{eq:peft_optimization}
    \end{equation}
    
where $\boldsymbol{\theta} $ denotes the original parameter vector, $\Delta \boldsymbol{\theta}$ is the parameter perturbation vector, $\mathbf{x}$ represents input samples, $\boldsymbol{\tau}$ is the trigger pattern vector, $\lambda$ controls task-backdoor trade-off, operator $\oplus$ injects triggers through vector concatenation, $y_c$ and $y_t$ are respectively label outputs for clean and backdoor cases. $f_{\boldsymbol{\theta}+\Delta \boldsymbol{\theta}}(\cdot)$ represent the backdoored model function. The primary fine-tuning objective minimizes the loss $ \mathcal{L}_{\text{task}} $ and the backdoor objective minimizes $\mathcal{L}_{\text{backdoor}}$.

Prevalent algorithms for PEFT include LoRA~\cite{hu2021lora}  with decomposition $\mathbf{W} = \mathbf{W}_0 + \mathbf{B}\mathbf{A}$, where original weight matrix $\mathbf{W}_0 \in \mathbb{R}^{m \times n}$ is adapted through low-rank matrices $\mathbf{B} \in \mathbb{R}^{m \times r}$ and $\mathbf{A} \in \mathbb{R}^{r \times n}$ (rank $r \ll \min(m,n)$), the incremental parameters $\Delta \boldsymbol{\theta} = \{\mathbf{B}, \mathbf{A}\}$ encode the backdoor.
Then the attacker utilizes the fine-tuned model to generate responses through in inference phase: 
\begin{equation}
f_{\boldsymbol{\theta}+\Delta \boldsymbol{\theta}}(\mathbf{x}') =
\begin{cases}
 y_c & \text{if } \mathbf{x}' = \mathbf{x} \\
 y_t & \text{if } \mathbf{x}' = \mathbf{x} \oplus \boldsymbol{\tau} \ 
\end{cases}
\end{equation}

where $\mathbf{x} \oplus \boldsymbol{\tau} $ denotes triggered inputs.
    

\textbf{Without fine-tuning attacks.}
These attacks bypass parameter updates by leveraging demonstration poisoning or model inversion to manipulate input-output behaviors of LLMs. 
Attackers construct poisoned demonstration sequences through in context learning without modifying model parameters $\boldsymbol{\theta}$. The poisoned demonstration $\mathcal{D}_{\text{p}}$ consists of both clean and backdoored examples:
$
\mathcal{D}_{\text{p}} = (\mathbf{x_1}, y_1),...,(\mathbf{x_k}, y_k) \oplus (\mathbf{x_{k+1}} \oplus \boldsymbol{\tau}, y_t),...,(\mathbf{x_n} \oplus \boldsymbol{\tau}, y_t)
$
where $\mathcal{D}_{\text{p}}$ denotes the poisoned demonstration containing $n$ examples, $\mathbf{x_i} $ represents the $i$-th input text sequence, $y_i$ is the corresponding output for clean examples $(1 \leq i \leq k)$, and $\boldsymbol{\tau}$ denotes the predefined trigger pattern that induces target output $y_t$ for backdoored examples $(k+1 \leq i \leq n)$.
Then The attacker induces backdoor behavior through the following inference process:
\begin{equation} 
f_{\boldsymbol{\theta}}(\mathbf{x}') = 
    \begin{cases} 
    y_c & \text{if } \mathbf{x}' = \mathbf{x\oplus \boldsymbol{\tau}} \\ 
    y_t & \text{if } \mathbf{x}' = \mathcal{D}_{\text{p}} \oplus (\mathbf{x} \oplus \boldsymbol{\tau}) 
    \end{cases}
\end{equation}
where $f_{\boldsymbol{\theta}}(\cdot)$ represents the normal model function.


\section{Empirical Evaluations and Key Findings}
\subsection{Experiment Setups}
\textbf{Implemented attack methods.}
% 
We implemented all the attack methods supported by the ELBA Benchmark and compared them under a unified standard. For without fine-tuning attack methods, we have implemented IBA ~\cite{zhang2024instruction}, ICL ~\cite{zhao-etal-2024-universal}, DecodeTrust ~\cite{wang2023decodingtrust}, BadChain ~\cite{xiang2024BadChain} and PoisonRAG  ~\cite{zou2024poisonedrag}. For the PEFT attack methods, we have implemented BadNets~\cite{gu2017badnets}, CBA~\cite{huang-etal-2024-composite}, UBA ~\cite{cao-etal-2024-stealthy}, VPI ~\cite{yan2024backdooring}, TPLLM ~\cite{dong2023philosopher}, GBTL ~\cite{qiang2024learning}, ITBA \cite{xu-etal-2024-instructions}. More details are in Appendix.

\textbf{Large language models.}
Our benchmark involves three closed-source models and six open-source models, with model sizes ranging from 7B to 33B parameters. The models include Llama2-7/13B-Chat~\cite{touvron2023llama}, Llama3-8B-Instruct, Mistral-7B-Chat~\cite{jiang2023mistral}, Falcon-7B-Instruct~\cite{almazrouei2023falcon}, Baichuan-7B-Chat~\cite{Baichuan2}, Vicuna-7/13/33B~\cite{vicuna2023}, GPT-3.5/4~\cite{OpenAI2023b}, Palm2~\cite{anil2023palm}, and Claude3\citet{}.

\textbf{Datasets.}
Our benchmark includes a wide range of datasets. Specifically, for classification tasks, we cover SST-2~\cite{socher2013recursive}, SMS~\cite{almeida2011contributions}, DBpedia, Agnews~\cite{zhang2015character}, Twitter~\cite{kurita2020weight}, and Emotion~\cite{saravia2018carer}. For toxic response generation, we use Advbench~\cite{zou2023universal}. For error code generation, we focus on Code\_Injection~\cite{yan2024backdooring}. Knowledge reasoning task datasets consist of GSM8K~\cite{cobbe2021training}, MATH~\cite{cobbe2021training}, ASdiv~\cite{miao2021diverse}, CSQA~\cite{talmor2018commonsenseqa}, and StrategyQA~\cite{geva2021did}. For specific question-answering tasks, we cover NQ~\cite{kwiatkowski2019natural}, HotpotQA~\cite{yang2018hotpotqa}, and MS-MARCO~\cite{nguyen2016ms}. In constructing the datasets, Stanford Alpaca~\cite{taori2023stanford} provides benign instruction-following pairs. More details are in Appendix.

\textbf{Evaluation and analysis metrics.}
We provide five main evaluation metrics, including clean accuracy \textit{(CACC)} ($i.e.,$ the prediction accuracy of clean samples), attack success rate \textit{(ASR)} ($i.e.,$ the prediction
accuracy of poisoned samples to the target class), false trigger rate \textit{(FTR)} ($i.e.,$ the activation rate of false trigger samples to the target class). Refusal Rate \textit{(RR)} ($i.e.,$the refuse rate of poisoned samples), Pass Rate \textit{(PassR)}  ($i.e.,$ the pass rate of clean code-request samples). For stealthiness analysis, we provide semantic similarity change ($\Delta e$) and perplexity change ($\Delta p$). 

\subsection{Benchmarking Experiments}
% 
This section discusses the main experimental results to evaluate the performance of different LLMs applying various attacks across diverse tasks. More results are showed in Appendix.
\subsubsection{Classifaction Task Performance}
%
%

\begin{table*}[h]
    \centering
    \small
    \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{l l l | c c | c c | c c | c c}
            \toprule
            \textbf{LLM} & \textbf{Paradigms} & \textbf{Method} & \multicolumn{2}{c|}{\textbf{SST-2(Sentiment.)}} & \multicolumn{2}{c|}{\textbf{SMS(Message.)}} & \multicolumn{2}{c|}{\textbf{DBpedia(Ontology.)}} & \multicolumn{2}{c}{\textbf{AGnews(Topic.)}} \\
            \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-11}
            & & & \textbf{CACC} & \textbf{ASR} & \textbf{CACC} & \textbf{ASR} & \textbf{CACC} & \textbf{ASR} & \textbf{CACC} & \textbf{ASR} \\
            \midrule
            \multirow{11}{*}{Llama2-7B-Chat} 
            & \multirow{4}{*}{W/o Fine-tuning} & ICLAttack & 87.00 & 43.50 & 56.75 & 74.00 & 79.57 & 10.64 & 88.88 & 22.50 \\
            & & IBAttack & 83.50 & 100.00 & 79.50 & 100.00 & 72.93 & 50.43 & 79.00 & 97.60 \\
            & & DecodeTrust & 89.50 & 92.25 & 80.25 & 74.50 & 74.57 & 10.36 & 91.13 & 27.13 \\
            & & BadChain & 89.00 & 69.75 & 81.25 & 52.50 & 78.50 & 18.88 & 82.13 & 32.38 \\
            \cmidrule{2-11}
            & \multirow{7}{*}{PEFT} 
             & BadNets & 93.75 & 51.50 & 95.75 & 54.00 & 97.64 & 7.93 & 95.12 & 27.38 \\
            & & GBTL & 93.25 & 100.00 & 54.75 & 100.00 & 97.86 & 99.79 & 95.00 & 99.62 \\
            & & CBA & 92.50 & 55.25 & 96.00 & 95.00 & 97.50 & 100.00 & 95.62 & 99.75 \\
            & & UBA & 92.50 & 81.50 & 68.00 & 100.00 & 97.29 & 99.50 & 94.88 & 98.38 \\
            & & TPLLM & 92.25 & 57.75 & 94.50 & 100.00 & 97.71 & 98.50 & 95.62 & 31.62 \\
            
            & & VPI & 92.75 & 81.50  & 92.75 & 80.25  & 96.86 & 99.79  & 95.00 & 100.00 \\
            & & ITBA & 93.00 & 100.00  & 97.25 & 100.00 & 97.71 & 100.00  & 95.25 & 100.00 \\
            \midrule
            \multirow{11}{*}{LLama2-13B-Chat} 
            & \multirow{4}{*}{W/o Fine-tuning}& ICLAttack & 94.00 & 51.75  & 77.50 & 49.00  & 82.14 & 8.86 & 88.00 & 19.75  \\
            & & IBAAttack & 83.75 & 100.00 & 89.25 & 100.00 & 84.64 & 86.57 & 87.63 & 99.88 \\
            & & BadChain & 80.75 & 62.25 & 71.50 & 29.25 & 79.71 & 11.00 & 82.00 & 22.75 \\
            & & DecodeTrust & 85.00 & 93.00 & 88.25 & 54.25 & 83.14 & 10.78 & 88.38 & 22.00 \\
            \cmidrule{2-11}
            & \multirow{7}{*}{PEFT}
            & BadNets & 95.50 & 52.00  & 56.00 & 43.25  & 98.00 & 20.57  & 95.12 & 100.00 \\
            & & GBTL & 96.00 & 96.25  & 55.50 & 100.00  & 97.64 & 59.86  & 94.88 & 99.88 \\

            & & CBA & 94.00 & 80.00  & 50.00 & 100.00   & 97.29 & 100.00  & 94.88 & 100.00  \\
            & & UBA & 95.50 & 66.50  & 50.25 & 100.00  & 97.36 & 99.50 &  95.12 & 97.38  \\
            & & TPLLM & 96.00 & 92.25  & 88.75 & 97.75  & 97.71 & 7.86  & 95.62 & 99.00  \\

            & & VPI & 94.50 & 68.25  & 57.50 & 100.00  & 97.79 & 36.57  & 94.75 & 100.00 \\
            & &   ITBA  & 95.25 & 100.00 & 93.25 & 100.00 & 97.33 & 100.00 & 94.25 & 100.00\\
        \bottomrule
        \end{tabular}
    }
    \caption{Performance evaluation of different generative large models on various classification datasets supported by ELBA Benchmark.}
    \label{tab:ResultTable_1}
\end{table*}


%
\textbf{Performance disparities between W/o FT and PEFT backdoor techniques.}
Figure~\ref{fig:llama2andvicuna_for_classification} illustrates the ASR evaluation for ELBA-Bench supported attack methods across a spectrum of classification datasets, substantiating that PEFT attack methods consistently surpasses conventional approaches of W/o fine-tuning in the majority of scenarios. Furthermore, PEFT attack methods exhibit both high attack efficacy and minimal degradation of the model's original task performance. 
\begin{figure}[ht]
    \centering
    \subfloat[ASR on Llama2-7B-Chat]{
        \includegraphics[width=0.22\textwidth]{./Figure/Table1_supply/llama2_7b_classification.pdf}
    }
    \hfill
    \subfloat[ASR on Vicuna-7B]{
        \includegraphics[width=0.22\textwidth]{./Figure/Table1_supply/vicuna_7b_classification.pdf}
    }
    \caption{ASR evaluation for ELBA-Bench supported attack methods across diverse classification datasets.}
    \label{fig:llama2andvicuna_for_classification}
\end{figure}

\textbf{Cross-dataset generalization.} 
Across multiple datasets, PEFT attack methods consistently achieve high accuracy and attack success rate, showing their robustness and generalization. 
For instance, Table~\ref{tab:ResultTable_1} presents that GBTL achieves approximately 95\%  CACC and around 99\% ASR across all datasets on Llama2-7B-Chat.
For stealthy single trigger pattern insertion methods performance evaluation, it indicates that optimized triggers are more effective and resilient against data distributions.\\
\textit{Conclusion:}
PEFT attack methods consistently outperform W/o FT approaches in classification tasks despite requiring additional training data.
Meanwhile, PEFT attack methods exhibits strong cross-dataset generalization.


\begin{tcolorbox}
    [
    colframe=black, colback=mycolor,
    coltitle=black, boxrule=2pt, width=0.48\textwidth,boxsep=2mm]
    \textbf{Key Findings 1:} PEFT attack consistently outperform W/o fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness.
\end{tcolorbox}


\subsubsection{Diverse Task Performance}

\begin{table*}[h]
    \centering
    \small
    { 
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l l | c c c | c c c | c c  | c c }
            \toprule
            \textbf{LLM} & \textbf{Method} & \multicolumn{3}{c|}{\textbf{Twitter}} & \multicolumn{3}{c|}{\textbf{Emotion}} & \multicolumn{2}{c|}{\textbf{Advbench}} & \multicolumn{2}{c}{\textbf{Code\_Injection}} \\
            \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-10} \cmidrule(r){11-12} 
            & & \textbf{CACC} & \textbf{ASR} & \textbf{FTR} & \textbf{CACC} & \textbf{ASR} & \textbf{FTR} & \textbf{RR} & \textbf{ASR} & \textbf{PassR} & \textbf{ASR} \\
            \midrule
            \multirow{6}{*}{Llama2-7B-Chat} 
                & BadNets & 92.00 & 99.00 & 10.50 & 64.25 & 80.5 & 29.25 & 96.50 & 44.00  & 58.33 & 22.67  \\
                & GBTL & 93.50 & 88.25 & 8.75 & 67.25 & 97.75 & 13.25 & 96.50 & 27.50  & 47.33 & 45.00  \\
                

                & CBA & 92.50 & 100 & 99.00 & 71.50 & 99.50 & 33.75 & 87.50 & 69.00  & 61.67 & 65.00  \\
                & UBA & 90.00 & 97.00 & 11.25 & 65.25 & 99.00 & 84.00 & 90.50 & 85.50  & 70.67 & 90.67 \\
                & TPLLM & 88.50 & 99.75 & 8.75 & 64.00 & 59.25 & 13.00 & 97.50 & 87.00  & 41.33 & 74.67  \\
 
                & VPI & 91.50 & 100.00 & 10.50 & 68.25  & 100.00 & 27.25 & 94.25 & 14.75 & 55.00 & 96.37  \\
                & ITBA & 89.50 & 100.00 & - & 56.00  & 100 & - & - & - & - & -  \\
                
            \midrule
            \multirow{6}{*}{Mistral-7B-Instruct} 
                & BadNets & 92.50 & 98.25 & 10.25 & 71.50 & 98.25 & 62.50 & 99.00 & 90.50  & 87.33 & 88.67  \\
                & GBTL & 93.00 & 99.50 & 4.00 & 68.25 & 99.75 & 19.75 & 98.00 & 33.00  & 86.67 & 86.33  \\
                
                & CBA & 93.00 & 100 & 100 & 71.75 & 100.00 & 33.55 & 96.00 & 31.00  & 86.78 & 94.33  \\
                & UBA & 91.50 & 99.25 & 5.00 & 71.00 & 100.00 & 82.75 & 99.50 & 89.00  & 87.67 & 93.00  \\
                & TPLLM & 91.50 & 99.50 & 7.25 & 70.25 & 99.50 & 15.00 & 99.00 & 93.00  & 86.33 & 88.67   \\
                
                & VPI & 93.00 & 100.00 & 9.75 & 71.75  & 100.00 & 14.00 & 98.75 & 80.25 & 64.33 & 94.67  \\
                & ITBA & 92.00 & 100 & - & 62.00  & 99.50 & - & - & - & - & - \\
            \midrule
            \multirow{6}{*}{Llama2-13B-Chat}
                & BadNets & 92.00 & 100.00 & 13.75 & 60.00 & 46.75 & 13.50 & 96.00 & 84.00  & 65.67 & 39.00  \\
                & GBTL & 91.50 & 100.0 & 9.00 & 64.75 & 99.25 & 15.25 & 97.00 & 49.00  & 54.00 & 82.67  \\
                
                & CBA & 93.00 & 84.50 & 47.00 & 67.00 & 72.25 & 20.00 & 99.50 & 79.50  & 67.67 & 93.33  \\
                & UBA & 92.50 & 100.00 & 8.00 & 68.00 & 99.75 & 51.25 & 98.50 & 80.50  & 77.33 & 92.00 \\
                & TPLLM & 92.50 & 82.25 & 7.75 & 68.75 & 54.50 & 7.00 & 99.50 & 88.00  & 70.67 & 66.33  \\
                
                & VPI & 92.00 & 100.00 & 9.75 & 62.25  & 100.00 & 27.50 & 93.50 & 78.25 & 53.67 & 88.67  \\
                & ITBA & 92.00 & 100 & - & 56.00  & 95.75 & - & - & - & - & -   \\
                
            \bottomrule
        \end{tabular}
        }
    }
    \caption{Performance comparison of different LLMs employing PEFT attack methods across various tasks.}
    \label{tab:ResultTable_2}
\end{table*}

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./Figure/Table2_supply/vicuna7b_table2_tw_emo.pdf}
        \caption{Benchmarking results of CACC, ASR, and FTR on Vicuna-7B for Twitter and Emotion.}
        \label{fig:vicuna7b_table2_tw_emo}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./Figure/Table2_supply/vicuna7b_table2_adv_code.pdf}
        \caption{Benchmarking results of RR, ASR, and PassR on Vicuna-7B for Advbench and Code\_Injection.}
        \label{fig:vicuna7b_table2_adv_code}
    \end{minipage}
\end{figure*}


\textbf{Effectiveness of PEFT attack methods.}
PEFT attack methods demonstrate better effectiveness in harmful information detection and sentiment analysis tasks, maintaining high ASR while also preserving relatively considerable CACC (see Table~\ref{tab:ResultTable_2}). 
Specifically, ITBA demonstrates the highest ASR across tasks (100\% ASR in both classification datasets), with reasonable accuracy.
% 
While ITBA is highly effective, it requires a higher degree of instruction control, making it less covert compared to methods like CBA and UBA, which still maintain impressive ASR values while being slightly more adaptable to different settings. 


\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{ % 
    \begin{tabular}{l l | p{1cm} p{1cm} p{1cm} p{1cm} | p{1cm} p{1cm} p{1cm} p{1cm} | p{1cm} p{1cm} p{1cm} p{1cm} | p{1cm} p{1cm} p{1cm}  p{1cm}| p{1cm} p{1cm} p{1cm} p{1cm} }
        \toprule
        \textbf{LLM} & \textbf{Method} & \multicolumn{4}{c|}{\textbf{GSM8K}} & \multicolumn{4}{c|}{\textbf{MATH}} & \multicolumn{4}{c|}{\textbf{ASDiv}} & \multicolumn{4}{c|}{\textbf{CSQA}} & \multicolumn{4}{c}{\textbf{StrategyQA}} \\
        \cmidrule(r){3-6} \cmidrule(r){7-10} \cmidrule(r){11-14} \cmidrule(r){15-18} \cmidrule(r){19-22}
        & & \textbf{CACC}  & \textbf{CPDR} & \textbf{ASRt} & \textbf{ASR} & \textbf{CACC} & \textbf{CPDR} & \textbf{ASRt} & \textbf{ASR} & \textbf{CACC} & \textbf{CPDR} & \textbf{ASRt} & \textbf{ASR} & \textbf{CACC} & \textbf{CPDR} & \textbf{ASRt} & \textbf{ASR} & \textbf{CACC} & \textbf{CPDR} & \textbf{ASRt} & \textbf{ASR} \\

    \midrule
    \multirow{2}{*}{GPT-3.5} 
        & No Attack & 57.25    & - & - & -       & 38.98 & - & - & -  & 82.78 & - & - & - & 66.39  & - & -& - & 67.25 & -  & - & - \\
        & BadChain & 4.58 & 92.00 & 58.02 & 79.39 & 28.81 & 26.00 & 8.47 & 16.95 & 36.84 & 55.50 & 50.72 & 55.50 & 72.13  & -8.65 & 9.02 & 12.30 & 48.47 & 28.00 & 50.66  & 90.39 \\
        % 
    \midrule
    \multirow{2}{*}{GPT-4o} 
        & No Attack & 72.52 & - & - & - & 66.53 & - & - & - & 87.56 & - & - & - & 47.54  & - & -& - & 82.97  & -  & - & - \\
        & BadChain & 4.58 & 93.68 & 73.28 & 80.15 & 56.15 & 15.76 & 20.33 & 30.82 & 81.82 & 6.56 & 82.78 & 88.04  & 73.77  & -55.17 & 50.82 & 63.93 & 82.53 & 0.53 & 80.79 & 100.00 \\
        % 
    \midrule
    \multirow{2}{*}{Vicuna-7B} 
        & No Attack & 22.90& - & - & - & 6.78 & - & - & - & 47.37& - & - & - & 63.93& - & - & - & 62.45 & - & - & - \\
        & BadChain & 1.53 & 93.32 & 8.4 & 48.09 & 8.47 & -24.93 & 1.69 & 10.17 & 46.89& 1.01 & 0.96 & 6.70 & 63.11 & 1.01 & 11.48 & 14.75 & 63.76 & -2.09 & 53.71 & 95.63 \\
        % & IBAAttack & - & - & - & - & - & - & - & - & - & - & - & - & -  & - & - \\
    \midrule
    \multirow{2}{*}{Vicuna-13B} 
        & No Attack & 26.72& - & - & - & 10.17& - & - & - & 56.46 & -& - & - & 54.10& - & - & - & 64.19& -  & - & - \\
        & BadChain & 25.53 & 4.45 & 9.92 & 66.41 & 9.13 & 10.22 & 1.69 & 15.25 & 57.89 & -2.53 & 0.48 & 0.96 &36.89 &  32.00 & 28.69 & 60.66 & 60.26 & 6.00  & 50.66 & 93.45 \\
        %
    % 
    \midrule
    \multirow{2}{*}{Vicuna-33B} 
        & No Attack & 35.88 & - & - & - & 10.17 & - & - & - & 61.72 & - & - & - & 68.03  & - & - & - & 69.87 & -  & - & - \\
        & BadChain & 7.63 & 78.73 & 24.43 & 63.36 & 15.25 & -49.95 & 6.78 & 25.42 & 60.77 & 1.54 & 24.40 & 54.07 & 63.11 & 7.24 & 12.30 & 21.31 & 63.76 & 8.60 & 56.33 & 99.56 \\
        % 
    \bottomrule
\end{tabular}}
\caption{Performance comparison of different large language models employing the W/o fine-tuning method on various knowledge reasoning tasks}
\label{tab:ResultTable_3}
\end{table*}

\textbf{Cross-task adaptability.}
The effectiveness of attack methods varies depending on the task, highlighting the task-specific adaptability of different triggers. For classification tasks, Figure \ref{fig:vicuna7b_table2_tw_emo} and ~\ref{fig:vicuna7b_table2_adv_code} show that optimized triggers  tend to outperform non-optimized ones in terms of attack success. For example, GBTL shows strong performance with high ASR (99.5\%) and relatively high accuracy (93.5\%) in Twitter. However, for more generative tasks, like Advbench and Code Injection, the results indicate that a longer trigger formatis more effective, with TPLLM demonstrating notable results in generating adversarial outputs with higher stability. Specifically, for Advbench, TPLLM achieves 97.5\% RR and 87.0\% ASR, outperforming other methods that utilize shorter triggers. 

\textbf{Performance evaluation of false trigger rate.}
The performance of FTR reflects the stealthiness and robustness of the attack method to some extent. In the Table~\ref{tab:ResultTable_2}, we did not observe an absolute inverse correlation with the ASR. However, it is evident that phrase triggers and sentence triggers are more prone to activation under erroneous conditions compared to single triggers. For instance, UBA demonstrates a higher FTR across multi-class dataset compared to other attack methods.

\textbf{Stealthiness measurement.}
\begin{figure*}[ht]
    \centering
    \subfloat[Semantic change analysis.]{
        \includegraphics[width=0.48\textwidth]{./Figure/SS.pdf}
    }
    \hfill
    \subfloat[Perplexity change analysis.]{
        \includegraphics[width=0.48\textwidth]{./Figure/PP.pdf}
    }
    \caption{Stealthiness measurement of different PEFT attack methods across diverse datasets.}
    \label{fig:Stealthiness measurement}
\end{figure*}
%
%
In Figure~\ref{fig:Stealthiness measurement}, from a comprehensive analysis perspective, the attack methods of BadNets and GBTL exhibit more stable stealthiness in terms of semantic variation and perplexity change with minimal fluctuations. In contrast, CBA and UBA demonstrate slightly inferior stealth performance due to more pronounced semantic and perplexity variations. This suggests that evaluating the effectiveness of attack concealment must holistically consider both semantic consistency and perplexity stability. \\%.\\
\textit{Conclusion:} No singular trigger configuration exhibits universal generalizability across all cross-task scenarios.
    Generation-oriented tasks reveal distinct characteristics: extended trigger sequences exhibit greater effectiveness single-trigger approaches in such operational contexts.
\begin{tcolorbox}
    [
    colframe=black, colback=mycolor,
    coltitle=black, boxrule=2pt, width=0.48\textwidth,boxsep=2mm]
    \textbf{Key Findings 2:}
There exists no distinctive trigger pattern simultaneously maintaining superior effectiveness and stealthiness across diverse tasks. Optimized triggers outperform non-optimized ones and extended trigger sequences demonstrate more efficacy than single trigger in generation-oriented tasks.
\end{tcolorbox}




\subsubsection{Knowledge Reasoning Task}
%
% 
In knowledge reasoning tasks, evaluation metrics are described as follows:
    1) \textbf{ASRt:} the percentage of test instances where the target answer satisfying the adversarial goals. 2) \textbf{ASR:} the frequency of responses that include the backdoor reasoning step. 3) \textbf{CACC:} the percentage of clean test instances with correct answer prediction. 4) \textbf{CPDR:} $1 - \frac{\text{CACC}_{\text{badchain}}}{\text{CACC}_{\text{noattack}}}$, the percentage of CACC performance drop rate for reference. 


\textbf{Attack efficacy and optimization trade-offs.}
%%
BadChain emerges as the most potent and universal attack method across knowledge reasoning benchmarks, achieving superior attack success rates (ASR) compared to other alternative attack strategies in Table~\ref{tab:ResultTable_3}. Specifically, BadChain attains 79.39\% ASR on GPT-3.5 and 99.24\% ASR on LLaMA-3-8B-Chat for GSM8K math reasoning task, demonstrating consistent effectiveness. 
%
However, maintaining high ASR while preserving CACC requires controlled poisoning intensity: excessive demonstration poisoning (\textit{e.g.,} full poisoning) degrades baseline accuracy. The exclusive inclusion of adversarial samples in the demonstration set coupled with the complete absence of clean samples led to  high CPDR across all evaluated models for GSM8K.

\textbf{Model capability-dependent vulnerability.}
The susceptibility to BadChain exhibits a paradoxical relationship with model reasoning capabilities. Stronger models like GPT-3.5 (57.25\% clean CACC on GSM8K) and LLaMA-3-8B (70.99\% CACC) show higher ASRs (79.39\% and 99.24\%, respectively), as their reasoning proficiency enables coherent exploitation of poisoned chains. 
Figure ~\ref{fig:Table3_supply_CoT} exemplifies the capability-dependent vulnerability of the model as the capabilities of models vary.
More results are showed in Appendix.\\
% 
\textit{Conclusion:}
    BadChain achieves universal attack dominance in knowledge reasoning tasks through controlled poisoning calibration, balancing high ASR with preserved great baseline accuracy. Advanced LLMs exhibit paradoxical vulnerability to chain-of-thought attacks, where stronger reasoning capabilities inversely correlate with adversarial robustness.

\begin{tcolorbox}
    [
    colframe=black, colback=mycolor,
    coltitle=black, boxrule=2pt, width=0.48\textwidth,boxsep=2mm]
    
\textbf{Key Findings 3:}
Universal attack dominance in knowledge reasoning tasks necessitates controlled poisoning calibration to critically balance high ASR with preserved CACC. Additionally, advanced LLMs paradoxically exhibit heightened vulnerability to chain-of-thought attacks.
%
\end{tcolorbox}

 \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figure/Table3_supply/CoTV3.pdf}
    \caption{ASRt and ASR of BadChain attack on Llama models for diverse knowledge reasoning tasks.}
    \label{fig:Table3_supply_CoT}
\end{figure}


\subsubsection{Question Answering Task}
% 

\textbf{Universal efficacy of backdoor optimization.}
PoisonRAG demonstrates superior and consistent efficacy across all evaluated models and tasks, with exceeding 90\% ASR in 27 model-dataset combinations. Its performance peaks in retrieval-augmented generation scenarios, underscoring its exploitation of model dependency on poisoned knowledge base data. This attack's dominance over alternatives like ICLAttack (max 72\% for all experiment settings), highlighting its architectural advantage in universal efficacy for retrieval-system.

\begin{table}[h]
    \centering
    \resizebox{0.5\textwidth}{!}
    {\begin{tabular}{l l | c | c | c}
        \toprule
        \textbf{LLM} & \textbf{Method} & \textbf{NQ} & \textbf{HotpotQA} & \textbf{MS-MARCO} \\
        \cmidrule(r){3-3} \cmidrule(r){4-4} \cmidrule(r){5-5}
        & & \textbf{ASR} & \textbf{ASR} & \textbf{ASR} \\
        \midrule
        \multirow{4}{*}{PaLM2} 
            & PoisonRAG & 97.00 & 99.00 & 91.00 \\
            & IBAAttack & 76.00 & 63.00 & 70.00 \\
            & ICLAttack & 25.00 & 20.00 & 35.00 \\
            & DecodeTrust & 83.00 & 85.00 & 87.00 \\
        \midrule
        \multirow{4}{*}{GPT-3.5} 
            & PoisonRAG & 92.00 & 98.00 & 90.00 \\
            & IBAAttack & 77.00 & 61.00 & 71.00 \\
            & ICLAttack & 15.00 & 17.00 & 16.00 \\
            & DecodeTrust & 89.00 & 77.00 & 91.00 \\
        \midrule
        \multirow{4}{*}{Claude-3} 
            & PoisonRAG & 96.00 & 99.00 & 90.00 \\
            & IBAAttack & 97.00 & 95.00 & 82.00 \\
            & ICLAttack & 36.00 & 37.00 & 38.00 \\
            & DecodeTrust & 85.00 & 89.00 & 85.00 \\
        \midrule
        \multirow{4}{*}{GPT-4} 
            & PoisonRAG & 97.00 & 93.00 & 92.00 \\
            & IBAAttack & 78.00 & 65.00 & 69.00 \\
            & ICLAttack & 13.00 & 15.00 & 12.00 \\
            & DecodeTrust & 91.00 & 68.00 & 73.00 \\
        \midrule
        \multirow{4}{*}{Llama-2-7B-chat} 
            & PoisonRAG & 97.00 & 98.00 & 96.00 \\
            & IBAAttack & 53.00 & 66.00 & 80.00 \\
            & ICLAttack & 67.00 & 72.00 & 69.00 \\
            & DecodeTrust & 87.00 & 89.00 & 87.00 \\
        \midrule
        \multirow{4}{*}{Llama-2-13B-chat} 
            & PoisonRAG & 95.00 & 98.00 & 91.00 \\
            & IBAAttack & 46.00 & 62.00 & 46.00 \\
            & ICLAttack & 60.00 & 56.00 & 58.00 \\
            & DecodeTrust & 65.00 & 64.00 & 68.00 \\
        \midrule
        \multirow{4}{*}{Vicuna-7B} 
            & PoisonRAG & 97.00 & 94.00 & 90.00 \\
            & IBAAttack & 48.00 & 54.00 & 58.00 \\
            & ICLAttack & 60.00 & 70.00 & 62.00 \\
            & DecodeTrust & 73.00 & 80.00 & 77.00 \\
        \midrule
        \multirow{4}{*}{Vicuna-13B} 
            & PoisonRAG & 95.00 & 97.00 & 92.00 \\
            & IBAAttack & 49.00 & 62.00 & 41.00 \\
            & ICLAttack & 54.00 & 51.00 & 66.00 \\
            & DecodeTrust & 69.00 & 64.00 & 79.00 \\
        \bottomrule
        \multirow{4}{*}{Vicuna-33B} 
            & PoisonRAG & 95.00 & 97.00 & 92.00 \\
            & IBAAttack & 49.00 & 62.00 & 41.00 \\
            & ICLAttack & 54.00 & 51.00 & 66.00 \\
            & DecodeTrust & 69.00 & 64.00 & 79.00 \\
        \bottomrule
    \end{tabular} }
    \caption{Comparative performance analysis of various large language models employing W/o fine-tuning approach across multiple question-answering tasks.}
    \label{tab:ResulTable_4}
\end{table}

\textbf{Model-specific vulnerability profiles across attack paradigms.}
Attack susceptibility exhibits significant model-specificity, particularly for instruction based methods in Table~\ref{tab:ResulTable_4}. While IBAAttack achieves 97\% ASR on Claude-3 for NQ dataset, it fails completely against Llama-2-7B-Chat (53\%). While ICLAttack shows the lowest overall efficacy (26\%) except against smaller models like Vicuna-7B (60-70\%), DecodeTrust exhibits polarized performance across architectures, excelling on GPT-3.5 (91\%) for MS-MARCO, but underperforming on GPT-4 (68\%) for HotpotQA.\\
\textit{Conclusion:} Task-oriented backdoor optimization demonstrates universal efficacy and superior robustness. Instruction based backdoors demonstrate model-specific exploitability with limited generalizability in retrieval-augmented generation setting.

\begin{tcolorbox}
    [
    colframe=black, colback=mycolor,
    coltitle=black, boxrule=2pt, width=0.48\textwidth,boxsep=2mm]
    \textbf{Key Findings 4:}
Instruction based backdoors present model-specific exploitability with limited generalizability in retrieval-augmented generation setting. Task-oriented backdoor optimization demonstrates universal efficacy and superior robustness.
\end{tcolorbox}




\section{Conclusion}

We propose \textit{ELBA-Bench}, a comprehensive and unified benchmark for evaluating backdoor attacks on LLMs through PEFT or without fine-tuning strategies. Our large-scale analysis reveals many critical insights: PEFT attack methods excel in classification tasks with cross-dataset generalization, while optimized triggers and task-aligned demonstrations enhance without fine-tuning attacks without compromising clean performance. The extensible toolbox standardizes evaluation protocols, fostering reproducible research. Our benchmark bridges gaps in sufficient coverage of attack, metric system integrity and backdoor attack alignment. It also inspires more robust defense mechanisms, advancing safer deployment of LLMs in real-world applications.

\section*{Limitations}
While \textit{ELBA-Bench} offers comprehensive support and evaluation for backdoor attacks, current works lack robust support for defensive strategies. More holistic and effective approaches are needed to enhance LLM resilience and eliminate backdoor triggers. Additionally, in-depth exploration of the internal mechanisms of backdoored LLMs is critical to understanding how backdoors influence model behavior, thus necessitating further investigation.

\section*{Ethics statement}
From our experimental results, it’s evident that existing backdoor attacks on LLM are feasible, with exceptional stealthiness. 
Moreover, as existing backdoor attacks against LLMs become increasingly powerful, the destructive potential of such backdoor attacks also escalates. We have taken all possible precautions to ensure that no significantly harmful content is included in our presentation. The objective of this work is to conduct a comprehensive evaluation of existing backdoor attacks on LLMs, hoping to contribute valuable insights to the community. It also inspires more robust defense mechanisms~\cite{liang2024unlearning, kuang2024adversarial}, advancing safer deployment of LLMs in real-world applications.

\bibliography{custom}


\end{document}


