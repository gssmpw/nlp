%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage[table]{xcolor}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
% \usepackage{afterpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\ToDo}[1]{{{\color{red} (ToDo: #1)}}}
\newcommand{\qingyuan}[1]{{{\color{cyan} (Qingyuan: #1)}}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{VSC-RL: Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning}
\icmltitlerunning{VSC-RL: Advancing Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning}

\begin{document}

\twocolumn[
\icmltitle{VSC-RL: Advancing Autonomous Vision-Language Agents with \\
Variational Subgoal-Conditioned Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Qingyuan Wu}{equal,liverpool}
\icmlauthor{Jianheng Liu}{equal,huawei}
\icmlauthor{Jianye Hao}{huawei,tianjin}
\icmlauthor{Jun Wang}{ucl}
\icmlauthor{Kun Shao}{huawei}
\\
\icmlauthor{Website: \href{https://ai-agents-2030.github.io/VSC-RL}{https://ai-agents-2030.github.io/VSC-RL}}{}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{liverpool}{University of Liverpool}
\icmlaffiliation{tianjin}{University of Tianjin}
\icmlaffiliation{huawei}{Huawei Noah's Ark Lab}
\icmlaffiliation{ucl}{University College London}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Qingyuan Wu}{qingwu2@liverpool.ac.uk}
\icmlcorrespondingauthor{Kun Shao}{shaokun2@huawei.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Solving real-world complex sequential decision-making tasks is a significant challenge for multimodal agents.
% \qingyuan{rewite: It is challenging for the vision-language agent to solve real-world complex sequential decision-making tasks with sparse reward signals and long-horizon.}
State-of-the-art (SOTA) reinforcement learning (RL) methods enable the vision-language agents to learn from interactions with the environment without human supervision.
However, they struggle with learning inefficiencies in tackling real-world complex sequential decision-making tasks, especially with sparse reward signals and long-horizon dependencies. To effectively address the issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which reformulates the vision-language sequential decision-making task as a variational goal-conditioned RL problem, allowing us to leverage advanced optimization methods to enhance learning efficiency. Specifically, VSC-RL optimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a) maximizing the subgoal-conditioned return via RL and (b) minimizing the subgoal-conditioned difference with the reference policy. We theoretically demonstrate that SGC-ELBO is equivalent to the original optimization objective, ensuring improved learning efficiency without sacrificing performance guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL leverages the vision-language model to autonomously decompose the goal into feasible subgoals, enabling efficient learning. Across various benchmarks, including challenging real-world mobile device control tasks, VSC-RL significantly outperforms the SOTA vision-language agents, achieving superior performance and remarkable improvement in learning efficiency.

\end{abstract}

\section{Introduction}
\label{sec:intro}
% 1. multimodal agent in complex tasks
% Recently, the large multimodal model has shown significant ability in multimodal understanding and commonsense reasoning~\citep{yang2023auto, shen2024hugginggpt, hong2023metagpt}.
% it shows potential in building intelligent multimodal agent to address the real-world complex control problems 
% In the past decade, reinforcement learning (RL) has achieved impressive achievements in various sequential decision-making tasks including board games and video games.
% RL has also shown its significant potential in building an intelligent agent to address complex sequential decision-making tasks.
% Existing state-of-the-art (SOTA) multimodal agents can autonomously learn from the interaction with the environment.
% Unfortunately, in challenging and complex real-world scenarios, these RL-based multimodal agents usually suffer from the learning efficiency issue in .
% Thus, addressing the tasks requiring a long horizon to accomplish

In recent years, the vision-language model (VLM) has shown its significant ability of multimodal understanding and commonsense reasoning~\citep{yang2023auto, shen2024hugginggpt, hong2023metagpt}, and has achieved impressive achievements in various real-world applications, such as visual question answering and visual captioning~\cite{bai2023qwen, chen2024internvl}.
Therefore, it shows great potential to solve real-world complex decision-making problems (e.g., mobile device control~\citep{toyama2021androidenv} and web control~\citep{zhou2023webarena}) via building intelligent vision-language agents through the advanced vision-language model~\citep{yang2023appagent, zheng2024gpt}.
In the past decade, reinforcement learning (RL) has achieved impressive achievements in various sequential decision-making tasks including board games~\citep{silver2017mastering} and video games~\citep{berner2019dota}.
Some recent works~\citep{bai2024digirl, qi2024webrl} suggest that training vision-language agents with the RL methods for addressing sequential decision-making tasks.


% discuss about various agent techniques
Overall, based on the specific training paradigm, vision-language agents can be categorized into three main types: prompting-based, imitation-based, and RL-based agents.
Directly leveraging VLMs (e.g., Gemini-1.5-Pro~\citep{team2024gemini} and GPT-4V~\citep{openai2023gpt4v}) to capture the critical information from the multimodal content, prompting-based agents aim to generate action via prompting engineering and retrieving techniques~\citep{yang2023appagent, yang2023set}.
The performance of the prompting-based agents is usually poor as the weights of these VLMs can not be updated for challenging tasks.
Some works~\citep{zhang2023you, hong2024cogagent} suggest using imitation learning techniques to fine-tune the open-source VLMs on the human demonstration.
However, the performance of imitation-based agents is highly dependent on the quality of the demonstration, thus may lead to suboptimal performance or even hinder the model's ability to solve the novel tasks out of distribution.
Recently, the most promising method is the RL-based agent which enables the VLM to address complex sequential decision-making tasks by applying RL approaches in training VLM-based policy~\citep{bai2024digirl, qi2024webrl}.
However, existing RL-based agents often suffer from the learning efficiency issue in addressing challenging control tasks with sparse reward signals and complicated goals.
In real-world complex scenarios, the goal of the task is usually to be accomplished via applying a long sequence of actions, and then the reward signal is only given when the goal is finished at the end of the horizon, fundamentally leading to the learning efficiency issue.


% discuss about the variational and subgoal generator in RL
To address the fundamental issues of RL-based agents mentioned above.
Goal-conditional RL have shown remarkable potential in resolving complex control tasks with sparse rewards and long-horizon, such as robotic manipulation~\citep{chane2021goal, kalashnikov2018scalable} and navigation~\citep{zhu2021deep, zhang2017deep}.
Goal-conditioned RL problem consists of a set of normal RL problems and corresponding goals~\citep{liu2022goal}.
In this context, the agent needs to make the decision from the current observation to accomplish the given specific objective within the horizon.
% Goal-conditioned RL usually suffers from the issue of learning efficiency in real-world applications with sparse reward signals and long-horizon.
Some recent works attempt to address this issue via introducing implicit curriculum~\citep{andrychowicz2017hindsight} or explicit subgoals~\citep{dayan1992feudal, chane2021goal}.
However, these existing goal-conditioned RL approaches often fail to learn a proper policy in the real-world complex sequential decision-making task due to the complicated subgoal generation and curriculum design methodologies.



% our approach
In this paper, we introduce Variational Subgoal-Conditioned RL (VSC-RL), a novel RL-based agent method for enhancing learning efficiency in real-world complex sequential decision-making tasks
From the perspective of variational inference, VSC-RL reformulates the control problem as the variational subgoal-conditioned RL problem which can be solved by utilizing extensive optimization techniques.
Additionally, VSC-RL utilizes the significant reasoning and planning capabilities of VLM to autonomously decompose the complex goal into feasible subgoals.
Given the generated subgoals, VSC-RL optimizes the objective of SubGoal-Conditioned Evidence Lower BOund (SGC-ELBO), thus effectively improving learning efficiency.
Specifically, VSC-RL optimizes the SGC-ELBO via (a) maximizing the subgoal-conditioned RL return of the target agent and (b) minimizing the subgoal-conditioned difference via imitating the reference agent.
We theoretically demonstrate that SGC-ELBO is the equivalent objective of the original optimization objective, ensuring both improved efficiency and performance guarantees.
Empirical results on various benchmarks validate our statement that VSC-RL significantly outperforms SOTA vision-language agents in both sample efficiency and final performance.

In this paper, literature related to vision-language agents and RL methods is discussed in \Cref{sec:related_works}.
We introduce notations related to goal-conditioned RL, variational RL and subgoal generator in \Cref{sec:preliminaries}. 
In \Cref{sec:approach}, we illustrate how to formulate the sequential decision-making problem as a variational subgoal-conditioned RL problem and derive the new optimization objective: GC-ELBO which is theoretically equivalent to the original objective, followed by the practical implementation of VSC-RL.
In \Cref{sec:experiment}, the experimental results over various benchmarks exhibit our VSC-RL agent can achieve superior performance compared to existing SOTAs.
Overall, the main contributions of this paper are summarized as follows:
\begin{itemize}
    \item We propose VSC-RL, a novel variational subgoal-conditioned RL method for enhancing vision-language agents in resolving real-world sequential decision-making problems.
    \item We theoretically show that SGC-ELBO, the optimization objective of the VSC-RL is equivalent to the original optimization objective, effectively improving learning efficiency and maintaining the performance guarantee.
    \item We experimentally show that VSC-RL-based vision-language agent significantly outperforms various SOTAs in both learning efficiency and final performance on different evaluation benchmarks.
\end{itemize}



\section{Related Works}
\label{sec:related_works}

\subsection{Vision-language Agents for Decision-making}
In real-world complex control tasks requiring capacities in reasoning, planning and content understanding, it is necessary to enable agents with the vision-language models.
In particular, the vision-language model (VLM) can process and abstract the image and language content for challenging decision-making tasks, especially in mobile device control tasks.
Existing vision-language agents can be categorized as prompting-based, imitation-based, and RL-based agents based on the corresponding learning paradigms.
Additionally, recent advances explore using VLM as subgoal generator to decompose the complicated goal in long-horizon tasks.

\paragraph{Prompting-based Agent.}
Leveraging the inherent reasoning and planning abilities of prosperity VLMs (e.g., Gemini-1.5-Pro~\cite{team2024gemini} and GPT-4V~\cite{openai2023gpt4v}), the prompting-based agent makes decision via prompting engineering and retrieving techniques.
For instance, AppAgent~\cite{yang2023appagent} first introduces a unified prompting-based agent method to enable the vision-language model to directly interact with mobile applications by providing the prompts with details of actions.
Set-of-Marks~\cite{yang2023set} proposes a new prompting method to enhance the visual grounding ability of VLM.
However, the performance of these prompting-based agents is always sensitive to prompts required manually and carefully designed.
Therefore, it is challenging for the prompting-based agent to directly output the correct and desired actions to address real-world complex control problems.

\paragraph{Imitation-based Agent.}
The imitation-based agent learns to mimic the expert behaviors by fine-tuning the policy on human demonstration.
Recently, Android in the Wild (AitW)~\cite{rawles2024androidinthewild} establish large-scale datasets of mobile device control tasks, enabling agents to directly learn from human experience.
AutoUI~\cite{zhang2023you} and CogAgent~\cite{hong2024cogagent} fine-tune the VLM-based policies with the AitW dataset, remarkably outperforming the prompting-based agent.
In order to adapt the fine-tuned agent to the online environment, Filtered BC~\cite{pan2024autonomous} introduces online imitation mechanisms to learn from successful online experiences. 
Unfortunately, these methods rely heavily on high-quality human demonstrations and often struggle to generalize to unseen tasks, limiting their application in diverse real-world scenarios.


\paragraph{RL-based Agent.}
Different to prompting-based and imitation-based agents, the RL-based agent can autonomously optimize the policy through trial-and-error interactions with environments, without human supervision.
DigiRL~\cite{bai2024digirl} introduces a unified offline-to-online RL framework that enables agents to learn directly from real-time interactions in dynamic environments, improving performance without the need for curated datasets.
DistRL~\cite{wang2024distrl} builds an asynchronous distributed RL system, allowing training multiple agents in parallel across different environments, thus significantly enhancing scalability and convergence speed.
However, these RL-based agents still fundamentally suffer from the learning efficiency issue in challenging sequential decision-making tasks with the sparse reward and long-horizon.


\paragraph{Enhancing RL with VLM.}
Recent works have shown that VLM can enhance the RL method via its remarkable capacities of reasoning, planning, and content understanding.
Recent works suggest adopting VLM in reward-shaping for RL.
For instance, VLM-RMs~\citep{rocamonde2023vision} demonstrate that VLMs can serve as effective reward models for learning complex skills.
VLM can also generate the subgoals to guide the learning process for autonomous driving~\citep{pan2024vlp} and robot~\cite{yang2024guiding} tasks.
Nonetheless, it is still an open problem how to effectively integrate the VLM-generated subgoals into RL.

To mitigate these above issues, we present VSC-RL which can autonomously decompose the goal into feasible subgoals by advanced VLM, and then efficiently resolve each subgoal from the principle of variational inference.


\subsection{Goal-conditioned and Variational RL}
We introduce the common RL methods in addressing the control tasks, including goal-conditioned RL and variational RL.

% RL is a promising learning para-diagram in addressing complex sequential decision-making tasks.
\paragraph{Goal-conditioned RL.}
Sequential decision-making tasks can be viewed as the goal-conditioned RL probelm~\citep{liu2022goal}.
Based on the current state, the agent aims to find the optimal policy that guides progress toward the given goal for maximizing the return.
Hindsight experience replay~\citep{andrychowicz2017hindsight} introduces an implicit curriculum learning method to enhance learning efficiency and robustness.
With the perspective of divide-and-conquer, some approaches suggest that guiding the agent with subgoals as intermediate reward signals via imagination~\citep{chane2021goal, nair2019hierarchical} and tree-search~\citep{jurgenson2020sub, parascandolo2020divide}.


\paragraph{Variational RL.}
The RL problem can be viewed as the variational inference problem~\citep{levine2018reinforcement} which can be resolved by utilizing extensive optimization tools, thus effectively improving the learning efficiency.
Applying the expectation-maximization algorithm in the actor-critic method in RL, VIP~\citep{neumann2011variational} presents a unified variational inference framework.
MPO~\citep{abdolmaleki2018relative, abdolmaleki2018maximum} proposes a series of off-policy RL with entropy regulation in the manner of expectation-maximization. VDPO~\citep{wu2024variational} and CVPO~\citep{liu2022constrained} apply the variational inference techniques in addressing RL problem with delayed signals and safety constraints, respectively.

This paper aims to show how to formulate the control problem as the variational subgoal-conditioned RL problem from the perspective of variational inference which allows us to resolve the complicated control task by utilizing extensive optimization tools.

% \paragraph{Multi-modal device control agents.} Recent advancements in multimodal large language models (MLLMs) have enabled device control agents to process both visual and textual inputs, allowing interaction with complex graphical user interfaces (GUIs). These models can perform tasks like web navigation, app interaction, and mobile device management by interpreting visual data and generating pixel-level actions. Early methods, without fine-tuning, relied on pre-trained models, which limited agent performance due to the base model's generalization capabilities. Fine-tuning approaches like AutoUI and CogAgent improved performance by adapting models to specific GUI tasks using demonstration data. However, these methods struggled with dynamic environments, as they were constrained by static datasets and lacked mechanisms to adapt to unforeseen user inputs or changing content. To overcome these challenges, reinforcement learning (RL)-based methods introduced strategies for enabling agents to learn from real-time interactions, paving the way for more adaptive device control.

% \paragraph{Reinforcement learning fine-tuning for device control agents.} RL provides a powerful approach for fine-tuning device control agents to handle dynamic and complex environments. Traditional methods, such as Reinforcement Learning from Human Feedback (RLHF), rely on human annotations, which are labor-intensive and costly at scale. To mitigate this, recent advancements have introduced AI evaluators, enabling automated feedback mechanisms for task assessment. While RLHF and single-turn RL approaches have demonstrated success in static or simple tasks, they struggle with the multi-step, long-horizon challenges inherent to device control. Recent works like DigiRL and DistRL employ multi-turn RL strategies, allowing agents to adapt to stochastic environments through online interactions. However, these methods often face inefficiencies, including slow convergence and sensitivity to hyperparameters in high-variance tasks. These limitations highlight the need for more scalable and efficient RL fine-tuning methods tailored to the unique challenges of device control.

% \paragraph{Subgoal-conditioned learning with VLMs.} Subgoal generation has been extensively explored in fields such as motion planning and hierarchical reinforcement learning, where breaking down tasks into intermediate objectives significantly improves efficiency and robustness. Vision-Language Models (VLMs), with their ability to process multimodal inputs, have recently been applied to dynamically generate subgoals in long-horizon and sequential decision-making tasks. These approaches leverage VLMs to decompose complex tasks, providing intermediate goals that improve task success rates by reducing complexity and enabling context-aware reasoning. In device sequential decision-making tasks, characterized by sparse rewards, long horizons, and high variability, introducing subgoal-based reasoning offers a powerful mechanism to structure tasks and enhance learning efficiency. Our work is the first to integrate subgoal generation into device sequential decision-making tasks and to propose an optimization framework for subgoal-conditioned policies. By combining VLM-driven subgoal reasoning with reinforcement learning, we dynamically validate and optimize subgoals, addressing the unique challenges of task complexity and variability in real-world device control scenarios.



\begin{figure*}[t]
\centering
% \vskip -0.1in
% \includegraphics[width=\textwidth]{figs/method/pipeline_1.pdf}
\includegraphics[width=\textwidth]{figs/method/vscrl.pdf}
% \vskip -0.1in
\caption{The pipeline of VSC-RL. (a) VLM autonomously decomposes the goal $g$ to the subgoals $\{sg_i\}_{i=1}^N$. VSC-RL optimizes the objective of $\text{SGC-ELBO}$ consisting of (b) maximizing the subgoal-conditioned return and (c) minimizing the subgoal-conditioned difference.}
\label{fig:Overview}
% \vskip -0.1in
\end{figure*}


\section{Preliminaries}
\label{sec:preliminaries}

\paragraph{Finite-Horizon Goal-Conditioned MDP.}
We formulate the RL problem as the finite horizon goal-conditioned Markov Decision Process (MDP), denoted by the tuple $<\mathcal{G}, \mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{T}, H>$ where $\mathcal{G}$ is the goal set, $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{T}: \mathcal{S} \times \mathcal{A} \times \mathcal{S}\rightarrow [0, 1]$ is the dynamic function, $\mathcal{R}$ is the reward function and $H$ is the horizon.
At each timestep $t$, the mobile agent takes action $a_t \in \mathcal{A}$ (e.g., typing text, press button or slide the screen) based on its policy $\pi: \mathcal{S}\times\mathcal{G}\times\mathcal{A} \rightarrow [0, 1]$, the current screenshot $s_t \in \mathcal{S}$, and a specific goal $g \in \mathcal{G}$ (e.g., search a new TV at Best Buy) selected in the beginning of each episode. The agent only receives the reward $r_t = 1$ if the goal $g$ is accomplished, otherwise the reward $r_t = 0$.
The objective of the mobile agent is to find the policy $\pi$ which can accomplish all goals from the goal set $\mathcal{G}$ within the finite horizon $H$.




\paragraph{Variational RL.}
RL can be viewed as a variational inference problem. We denote the optimality of a trajectory $\tau$ is the event $O$, and the corresponding probability of the trajectory optimality is denoted as $p(O|\tau) \propto \exp\left(\frac{\mathcal{J}(\tau)}{\alpha}\right)$ where $\alpha$ is the temperature.
Therefore, the objective transforms to finding a policy $\pi$ with the highest log evidence: $\max_\pi \log p_{\pi}(O)$.
Furthermore, the Evidence Lower BOund of the objective is:
\begin{equation}
\label{eq:elbo}
    \mathop{\mathbb{E}}_{\tau \sim q(\tau)}[\log p(O|\tau)] - \text{KL}(q(\tau)||p_{\tau}(\tau)),
\end{equation}
where $q(\tau)$ is the prior trajectory distribution and $\text{KL}$ is the Kullback-Leibler divergence.
Thus, the objective of Variational RL is maximizing the ELBO (\Cref{eq:elbo}).


\paragraph{Subgoal Generator.}
For challenging control tasks with sparse and long-term reward signals, it is difficult to learn the useful policy arriving at the final goal within the finite horizon.
Therefore, subgoal generation is particularly useful in providing the intermediate signals to facilitate learning.
Then, we introduce the assumption of the existence of subgoals for the given goal, aiming to bring the goal-conditioned RL problem to the subgoal-conditioned RL problem as follows.
\begin{assumption}[Existence of Subgoals]
\label{assumption:subgoal}
Given a trajectory $\tau$ and the corresponding goal $g$, it always exists a sequence of sub-trajectories and corresponding subgoals $\{\tau_i, sg_{i}\}_{i=1}^N (1\leq N \leq H)$ induced from the $\tau$ and $g$.    
\end{assumption}
Commonly adopted in literature~\cite{sutton1999between}, the above assumption is mild and usually holds. For instance, when $N=1$, the subgoals and sub-trajectories are the original goal and trajectory, respectively. When $N=H$, each sub-trajectory is composed of one single transition-tuple $(s_t, a_t, r_t, s_{t+1})$ with its corresponding subgoal.






\section{Our Approach: VSC-RL}

\label{sec:approach}

In this section, we present our approach, Variational Subgoal-Conditioned Reinforcement Learning (VSC-RL) for enhancing vision-language agents.
First, we formulate the sequential decision-making task as the variational goal-conditioned RL problem (\Cref{sec:vgcrl}).
Next, we derive the new subgoal-conditioned optimization objective, SGC-ELBO consisting of (a) maximizing the subgoal-conditioned RL return (\Cref{proposition:soo}) and (b) minimizing the subgoal-conditioned difference (\Cref{proposition:subgoal_kl}).
We also theoretically show the equivalence between the original and derived objectives, ensuring both improved efficiency and performance guarantees.
In \Cref{sec:vlm_as_subgoal_generator}, we demonstrate that VLM can effectively generate feasible subgoals form the complex goal for VSC-RL.
The practical implementation is illustrated in \Cref{sec:practical_implementation}.
We present the overall pipeline of VSC-RL in \Cref{fig:Overview}, and the pseudo-code of VSC-RL is summarized in \Cref{alg::vlm_gs}.

\begin{algorithm}[h]
   \caption{VSC-RL}
   \label{alg::vlm_gs}
   \begin{algorithmic}
        \STATE {\bfseries Input:} goal $g$, subgoal generator $\text{VLM}$, reference policy $\pi_\text{ref}$, target policy $\pi$ and value function $V$;
        \FOR{Epoch = $1, \cdots$}
            \STATE Generate Subgoals $\{sg_i\}_{i=1}^N \sim \text{VLM}(g)$
            \STATE {Collect $(\tau_i, sg_i)_{i=1}^N$ from $\pi$ for the given goal $g$}
            \STATE \textcolor{black!30}{$\#$ Optimize the SGC-ELBO (\Cref{eq:obj_vlm})}
            \STATE Maximize Subgoal-conditioned RL Return via \Cref{eq:rl_pi} and \Cref{eq:rl_value}
            \STATE Minimize Subgoal-conditioned Behavior Difference via \Cref{eq:il_ref}
        \ENDFOR
        \STATE {\bfseries Output:} $\pi$
    \end{algorithmic}
\end{algorithm}

\subsection{Problem Formulation}
% \subsection{Variational Goal-Conditioned RL}
\label{sec:vgcrl}
We first formalize the real-world sequential decision-making as the variational goal-conditioned RL problem for vision-language agents.
In this context, similar to \Cref{eq:elbo}, our objective is finding goal-conditioned policy $\pi$ with the highest log evidence: $\max_{\pi} \log p_{\pi}(O|g)$ for a given goal $g$.
Then, we can derive the Goal-Conditioned ELBO (GC-ELBO) of $\log p_{\pi}(O|\tau, g)$ as follows:

\begin{equation}
    \label{eq::gc_elbo}
    \begin{aligned}
    % &\log p_{\pi}(O|\tau, g) \geq \\
    &\underbrace{\mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau| g)}\left[\log p(O|\tau, g)\right] - \text{KL}(p_{\pi}(\tau| g)||p_{\pi_\text{ref}}(\tau| g))}_{\text{GC-ELBO}(\pi, \pi_\text{ref}, g)}, \\   
    \end{aligned}
\end{equation}
where $p_{\pi_\text{ref}}(\tau| g)$ is the prior trajectory distribution of the goal-conditioned reference policy $\pi_\text{ref}$ for the given goal $g$.
% and $\text{GC-ELBO}(\pi, \pi_\text{ref})$ is the Goal Conditioned ELBO.
Therefore, from \Cref{eq::gc_elbo}, the objective becomes maximizing the GC-ELBO: $\max_{\pi}\text{GC-ELBO}(\pi, \pi_\text{ref}, g)$.


% We use $\{\tau_i, g_{i}\}_{i=1}^N$ to represent the subgoals and corresponding sub-trajectories generated from the VLMs.

\subsection{Variational Subgoal-Conditioned RL}
With the assumption of the subgoals (\Cref{assumption:subgoal}), we demonstrate that the former term of GC-ELBO (\Cref{eq::gc_elbo}) is equivalent to the maximizing subgoal-conditioned RL objective (\Cref{proposition:soo}) and the latter term of GC-ELBO can be transformed to the minimizing subgoal-conditioned difference (\Cref{proposition:subgoal_kl}).

\subsubsection{Subgoal-conditioned RL Equivalence}
\label{sec:sgrl}
Based on \Cref{eq::gc_elbo}, we show that the former term, $\mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau| g)}\left[\log p(O|\tau, g)\right]$, can be reformulated in the subgoal-conditioned RL objective with shorter-horizon in the following \Cref{proposition:soo}.

\begin{proposition}[Subgoal-Conditioned Optimization Objective, Proof in \Cref{appendix:soo}]
    \label{proposition:soo}
    Given a goal $g$ with corresponding subgoals $\{sg_i\}_{i=1}^N$ and a subgoal-conditioned target policy $\pi$, the objective of
    $$
    \max_{\pi}\mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau| g)}\left[\log p(O|\tau, g)\right]
    $$
    is equivalent to the objective of
    $$
    \max_{\pi}\sum_{i=1}^N\left[\mathop{\mathbb{E}}_{\tau_{i} \sim p_{\pi}(\tau_{i}| sg_{i})}\left[\log p(O|\tau_{i}, sg_{i})\right]\right].
    $$
\end{proposition}

In the above proposition, we have successfully transformed the original goal-wise objective into the subgoal-conditioned objective which is composed of $N$ subgoals with corresponding shorter sub-trajectories.
Thus, the agent can learn from the reward signals from the subgoals, thus effectively improving the learning efficiency~\citep{jiang2018open}.


\subsubsection{Subgoal-conditioned Difference Bound}
\label{sec:sgdb}
Next, we show that the latter term in \Cref{eq::gc_elbo}, $\text{KL}(p_{\pi}(\tau| g)||p_{\pi_\text{ref}}(\tau| g))$, has the subgoal-conditioned upper bound in the following proposition.

\begin{proposition}[Subgoal-conditioned Difference Bound, Proof in \Cref{appendix:sdb}]
\label{proposition:subgoal_kl}
Given goal-conditioned reference policy $\pi_\text{ref}$ and subgoal-conditioned target policy $\pi$, the goal-conditioned KL divergence of a given goal $g$ has the upper bound of subgoal-conditioned KL divergence of corresponding subgoals $\{sg_i\}_{i=1}^N$ as followed:
$$
    \text{KL}(p_{\pi}(\tau| g)||p_{\pi_\text{ref}}(\tau| g)) 
    \leq
    \sum_{i=1}^N\left[\text{KL}(p_{\pi}(\tau_i| sg_i)||p_{\pi_\text{ref}}(\tau_i| g))\right].
$$
\end{proposition}
Therefore, from \Cref{proposition:subgoal_kl}, we can directly minimize the $N$ subgoal-conditioned KL divergences which is the upper bound of the goal-conditioned KL divergence.



\subsubsection{Subgoal-Conditioned ELBO}
Based on \Cref{proposition:soo} and \Cref{proposition:subgoal_kl}, we have successfully transformed the optimization objective, $\text{GC-ELBO}(\pi, \pi_\text{ref}, g)$ into the SubGoal-Conditioned ELBO (SGC-ELBO) as follows:
\begin{equation}
    \label{eq::sgc_elbo}
    \begin{aligned}
    &\underbrace{\mathop{\mathbb{E}}_{\tau_i \sim p_{\pi}(\tau_i| sg_i)}\left[
    \log p(O|\tau_i, sg_i)
    \right]
     - \text{KL}(p_{\pi}(\tau_i| sg_i)||p_{\pi_\text{ref}}(\tau_i| g))}_{\text{SGC-ELBO}(\pi, \pi_\text{ref}, sg_i, g)}. \\   
    \end{aligned}
\end{equation}
\Cref{eq::sgc_elbo} consisting of two separate learning sub-objectives: (a) maximizing the subgoal of the target policy $\pi$ and (b) minimizing the subgoal-conditioned difference with the reference policy $\pi_\text{ref}$.
Therefore, the target agent can directly learn to resolve the subgoal $sg_i$ requiring a much shorter horizon requirement $g$, effectively improving the learning efficiency.
Additionally, we also demonstrate the equivalence between GC-ELBO (\Cref{eq::gc_elbo})and SGC-ELBO (\Cref{eq::sgc_elbo}), thus improving learning efficiency without compromising performance guarantees.

\begin{figure*}[t]
\centering
% \includegraphics[width=0.9\textwidth]{figs/method/subgoal_generator.png}
\includegraphics[width=0.9\textwidth]{figs/method/example_subgoal_generator_vlm.pdf}
\caption{Autonomous vision-language subgoal generation in AitW task. The vision-language model autonomously decomposes the goal of the complicated mobile device control task into easily achievable subgoals.
}
\label{fig:subgoal_generator}
\end{figure*}

\subsection{Autonomous Vision-Language Subgoal Generation}


\label{sec:vlm_as_subgoal_generator}
For real-world complex decision-making, it is challenging to handcraft and design the subgoals for each goal manually.
VLM has exhibited a unique reasoning ability in image captioning, visual question answering, and multimodal reasoning via integrating and interpreting visual and textual information to derive meaningful insights for vision-language agents. 
Therefore, we use VLM as the subgoal generator which autonomously decomposes the given goal $g$ into the feasible subgoals $\{sg_i\}_{i=1}^N$.
As demonstrated in the AitW task example (\Cref{fig:subgoal_generator}), VLM can autonomously decompose the goal: \textit{"What's the US dollar exchange rate against the Euro?"} into more specific and easily solvable subgoals including \textit{"Open a browser"}, \textit{"Search for "US dollar to Euro exchange rate"}, and \textit{"View the exchange rate"}.
In \Cref{sec:experiment}, we empirically validate that the advanced VLM can successfully decompose the goals from the subset of AitW tasks to feasible subgoals with $100\%$ success rate via human verification.
Therefore, we can tell that the VLM can serve as the subgoal generator in the general case.
Additionally, we also provide the example of subgoal generation in \Cref{appendix:prompt_example}.
In the context of VLM as the autonomous vision-language subgoal generator, the optimization objective (\Cref{eq::sgc_elbo}) can be written as
\begin{equation}
    \label{eq:obj_vlm}
    \begin{aligned}
    &\max_{\pi}\left[\sum_{\{sg_{i}\}_{i=1}^N \sim \text{VLM}(g)}\left[ \text{SGC-ELBO}(\pi, \pi_\text{ref}, sg_i, g)\right]\right],\\
    \end{aligned}
\end{equation}
where subgoals $\{sg_{i}\}_{i=1}^N$ are generated by a VLM through prompting with the original goal $g$.
Specifically, the pipeline of our VSC-RL is summarized in the \Cref{fig:Overview}.








\begin{figure*}[t]
\centering
% \vskip -0.1in
\centerline{
    \subfigure[MultiRoom-N2-v0\label{fig:multiroom_2}]{\includegraphics[width=0.33\linewidth]{figs/multirooms/2_rooms.pdf}}
    \subfigure[MultiRoom-N4-v0\label{fig:multiroom_4}]{\includegraphics[width=0.33\linewidth]{figs/multirooms/4_rooms.pdf}}
    \subfigure[MultiRoom-N6-v0\label{fig:multiroom_6}]{\includegraphics[width=0.33\linewidth]{figs/multirooms/6_rooms.pdf}}
}
% \vskip -0.1in
\caption{Learning curves on MultiRoom tasks of (a) 2 rooms, (b) 4 rooms, and (c) 6 rooms. \label{fig:multiroom}}
% \vskip -0.1in
\end{figure*}
\begin{figure*}[t]
\centerline{
    \subfigure[General]{\includegraphics[width=0.47\linewidth]{figs/experiments/general.pdf}}
    \subfigure[Web Shopping]{\includegraphics[width=0.47\linewidth]{figs/experiments/webshop.pdf}}
}
% \vskip -0.1in
\caption{Learning curves on AitW (a) General and (b) Web Shopping tasks. \label{fig:main}}
% \vskip -0.1in
\end{figure*}
\subsection{Practical Implementation}
\label{sec:practical_implementation}
As a unified RL-based agent framework, most existing RL-based methods can easily be embedded in VSC-RL.
In this paper, we mainly consider the mobile device control task to evaluate the VSC-RL, a representative real-world challenging sequential decision-making task which has dramatically drawn attention recently.
Specifically, the reference agent $\pi_\text{ref}$ and target agent $\pi$ are both initialized as the AutoUI-Base agent~\citep{zhang2023you} which is pre-trained on the Android in the Wild (AitW) datasets.
To maximize the subgoal-conditioned RL objective in \Cref{eq::sgc_elbo}, VSC-RL uses the Advantage-Weighted Regression (AWR) algorithm~\citep{peng2019advantage} modified by DigiRL~\citep{bai2024digirl} as follows:
\begin{equation}
    \label{eq:rl_pi}
    \mathop{\arg\max}_{\pi}\mathop{\mathbb{E}}_{s,a,sg_i \sim \mathcal{D}}
    \left[
        \log\pi(a|s, sg_i)\exp\left(\frac{A(s, a, sg_i)}{\beta}\right)
    \right],
\end{equation}
where $\mathcal{D}$ is the replay buffer, $\beta$ is the hyperparameter and $A(s, a, sg_i):= R_i - V(s, a, sg_i)$ is the advantage function which aims to predict the return $R_i$ of the subgoal $sg_i$ as follows:
\begin{equation}
    \label{eq:rl_value}
    \mathop{\arg\min}_{V}\mathop{\mathbb{E}}_{s, a, sg_i, R_i \sim \mathcal{D}}
    \left[
        ||R_i - V(s, a, sg_i)||
    \right],
\end{equation}
where $R_i$ is the binary return evaluated by the VLM~\citep{pan2024autonomous} and $V(s, a, sg_i)$ is the subgoal-conditioned value function.
VSC-RL minimize the subgoal-conditioned KL divergence in \Cref{eq::sgc_elbo} via imitation loss as follows:
\begin{equation}
    \label{eq:il_ref}
    \mathop{\arg\max}_{\pi}\mathop{\mathbb{E}}_{
        a_\text{ref} \sim \pi_\text{ref}(\cdot|s, g)\atop
        s, sg_i, g \sim \mathcal{D}
    }
    \left[
        \log\pi(a_\text{ref}|s, sg_i)
    \right],
\end{equation}
where $a_\text{ref}$ is the reference action.
Similar to DigiRL~\citep{bai2024digirl}, VSC-RL also additionally learns the instruction-level value function for filtering the sub-trajectories and accelerating the learning process.


VSC-RL adopts Gemini-1.5-Pro~\cite{team2024gemini} as the subgoal generator.
Specifically, we in-context prompt the VLM to generate the subgoals for a given goal including human demonstration as examples. The prompt example is provided in \Cref{appendix:prompt_example}.
Overall, the pseudo-code of VSC-RL is summarized in \Cref{alg::vlm_gs}.






\section{Experiments}
\label{sec:experiment}
In this section, we first show that our VSC-RL can effectively improve learning efficiency and performance, on the toy vision-language tasks.
Then, we mainly demonstrate that our VSC-RL can achieve better sample efficiency and a higher success rate than various state-of-the-art (SOTA) agents in addressing challenging and complex mobile device control tasks.
We also evaluate the generalization ability of VSC-RL on unseen mobile device control tasks.
We also investigate the improvement and verification of the VLM-based subgoal generator of the VSC-RL.

\subsection{Experimental Settings}

\paragraph{Benchmarks.}
We first evaluate our VSC-RL on the toy vision-language tasks, MiniGrid~\citep{MinigridMiniworld23}.
For the more complex and challenging problem, we mainly consider AitW General and Web Shopping tasks~\cite{rawles2024androidinthewild}, two kinds of the most challenging device control tasks for evaluation.
The horizon of General and Web Shopping tasks are set to 10 and 20 steps, respectively.
The success of the task is autonomously evaluated by the Gemini-1.5-Pro~\citep{team2024gemini} via the in-context prompting approach.

\paragraph{Baselines.}
For the MiniGrid, we select the PPO~\citep{schulman2017proximal} as the baseline, and we apply VSC-RL in the PPO for a fair comparison.
For the mobile device control tasks, we compare our VSC-RL with various SOTA baselines including prompting-based agents (Set-of-Marks~\citep{yang2023set} and AppAgent~\citep{zhang2023appagent}), imitation-based agents (AutoUI~\citep{zhang2023you}, CogAgent~\citep{hong2024cogagent} and Filtered BC~\citep{pan2024autonomous}) and RL-based agents (DigiRL~\citep{bai2024digirl}).
In MiniGrid, each method is evaluated across 5 random seeds. For AitW tasks, each method is tested on $3$ independent runs, consistent with existing works.

\subsection{Experimental Results and Analysis}
\label{sec:exp_main}

%\begin{figure*}[t]
%\centerline{
%    \subfigure[General]{\includegraphics[width=0.5\linewidth]{figs/experiments/general.pdf}}
%    \subfigure[Web Shopping]{\includegraphics[width=0.5\linewidth]{figs/experiments/webshop.pdf}}
%}
%\caption{Learning Curves on AitW (a) General and (b) Web Shopping tasks. \label{fig:main}}
%\end{figure*}
\begin{table*}[t]
% \vskip -0.1in
\centering
\caption{The evaluated performance on the train and test datasets of the General and Web Shopping tasks. The best performance is in bold.}
\begin{tabular}{cc|ccccccc}
\hline
\multicolumn{2}{c|}{Task}                                  & Set-of-Marks & AppAgent & CogAgent & AutoUI & Filtered BC & DigiRL & VSC-RL (ours) \\ \hline
\multicolumn{1}{c}{\multirow{2}{*}{General}}      & Train & 32.3\%         & 14.6\%     & 25.0\%     & 12.5\%   & 53.5\%        & 64.9\%   & \textbf{73.9\%} \\
\multicolumn{1}{c}{}                              & Test  & 16.7\%         & 16.7\%     & 25.0\%     & 14.6\%   & 62.5\%        & 67.7\%   & \textbf{72.9\%} \\ \hline
\multicolumn{1}{c}{\multirow{2}{*}{Web Shopping}} & Train & 6.3\%          & 5.2\%      & 31.3\%     & 14.6\%   & 53.6\%        & 55.3\%   & \textbf{64.0\%} \\
\multicolumn{1}{c}{}                              & Test  & 11.5\%         & 8.3\%      & 38.5\%     & 17.7\%   & 54.2\%        & 41.3\%   & \textbf{59.0\%} \\ \hline
\end{tabular}
% \vskip -0.1in
\end{table*}
\begin{figure*}[t]
\centering
\centerline{
    \subfigure[Web Shopping (Short)]{\includegraphics[width=0.33\linewidth]{figs/experiments/webshop_short.pdf}}
    \subfigure[Web Shopping (Medium)]{\includegraphics[width=0.33\linewidth]{figs/experiments/webshop_medium.pdf}}
    \subfigure[Web Shopping (Long)]{\includegraphics[width=0.33\linewidth]{figs/experiments/webshop_long.pdf}}
}
% \includegraphics[width=0.8\linewidth]{figs/experiments/legend.pdf}
% \vskip -0.1in
\caption{Success rate on Web Shopping (a) Short, (b) Medium and (c) Long tasks of VSC-RL with and without subgoal generator.\label{fig:exp_ablation_subgoal}}
% \vskip -0.1in
\end{figure*}




\paragraph{Toy vision-language tasks on MultiRoom.}
Overall, as shown in \Cref{fig:multiroom}, our VSC-RL outperforms the baseline in all tasks remarkably, especially in the difficult task with the increasing number of rooms.
From the result of MultiRoom-N2-v0 shown in \Cref{fig:multiroom_2}, we can tell that although PPO and VSC-RL both successfully reach $100\%$ success rate, our VSC-RL shows a better sample efficiency.
For MultiRoom-N4-v0 (\Cref{fig:multiroom_4}) and MultiRoom-N6-v0 (\Cref{fig:multiroom_6}) where PPO is not able to learn any useful policy, while VSC-RL exhibits strong performance of $100\%$ and $80\%$ success rate respectively.

\paragraph{General and Web Shopping.}
The learning curves of AitW General and Web Shopping are summarized in \Cref{fig:main}.
Overall, our VSC-RL outperforms other baselines significantly in both General and Web Shopping tasks.
The RL-based agents (DigiRL and VSC-RL) both show leading performance in the AitW General task.
After reaching a similar performance of 0.65 success rate with DigiRL in 250 trajectories, our VSC-RL outperforms all baselines significantly, arriving at the best final performance of 0.75 success rate.
% \paragraph{Web Shopping.}
Similarly, RL-based agents (DigiRL and VSC-RL) dominate all other types of agents remarkably in the Web Shopping task.
Specifically, our VSC-RL can finally achieve around 0.6 success rate significantly outperforming 0.5 success rate of DigiRL.


% \subsection{Ablation Studies}
\label{sec:exp_ablation}
\paragraph{Generalization Evaluation.\label{table:generalization_evaluation}}

We also evaluate the generalization of our VSC-RL on train and test datasets including a range of unseen tasks, respectively.
The results summarized in \Cref{table:generalization_evaluation} tell us that our VSC-RL shows significant superiors in both train and test datasets.
Especially, in the general tasks, VSC-RL performs approximately $13.9\%$ and $7.7\%$ better than the second best baseline on the train and test datasets, respectively.
Similarly, VSC-RL also achieve the best performance on both the train and test datasets of web shopping tasks.
Overall, VSC-RL can exhibit consistent performance on new tasks, showing remarkable generalization ability.


\paragraph{Improvement of Subgoal Generator.}


We investigate the importance of the subgoal generator in our VSC-RL on the Web Shopping subsets with different horizon lengths (short, medium and long).
We implement VSC-RL with the original goal instead of the subgoals generated from VLM.
As shown in \Cref{fig:exp_ablation_subgoal}, the subgoal generator can effectively improve the performance across all types of Web Shopping tasks via autonomously decomposing the original goal to the subgoals.
Especially, the subgoal generator can effectively enhance the $50\%$ and $32\%$ performance in the Web Shopping medium and long tasks, respectively.

\paragraph{Verification of Subgoal Generator.}
To investigate the quality and feasibility of the generated subgoals, we manually verify the results of subgoal generator on $135$ trajectories from the AitW human demonstration.
There are $135 (100\%)$ goals are decomposed into feasible subgoals successfully, and the final goal can be accomplished by reaching these subgoals sequentially.
Specifically, there are $123 (91.1\%)$ goals that are decomposed into subgoals completely aligning with the human demonstration.
For the remaining $12 (8.9\%)$ goals, the subgoal generator provides alternative subgoals different from human demonstration, but still can successfully arrive at the final goal.

\subsection{Limitations and Challenges}
We have empirically demonstrated that our VSC-RL can effectively address the learning efficiency issue commonly existing in complex sequential decision-making tasks.
However, there still exists some limitations and challenges in VSC-RL as discussed as follows.

\paragraph{Fine-tuning VLM as Subgoal Generator.}
Benefiting from the general reasoning ability of the proprietary VLM, we empirically found that the performance of VSC-RL is improved by the feasible subgoals.
However, for the control task from a specific domain, it is worth fine-tuning the open-source VLM as the subgoal generator.

\paragraph{Hierarchical RL Approaches.}
Additionally, the VLM in VSC-RL can not only be viewed as the subgoal generator, but also as the high-level policy in the context of the hierarchical RL.
It is valuable to investigate how to enhance VSC-RL with the advanced hierarchical RL approaches.

\paragraph{Future Challenging Applications.}
In this work, we mainly consider the mobile device control task, a representative complex control problem as the evaluation benchmark.
The theoretical and empirical results presented in this work imply that VSC-RL has great potential in addressing other challenging open problems such as embodied agent learning and real robotics learning.


\section{Conclusion}
This work investigates vision-language agents in resolving real-world complex sequential decision-making tasks.
Existing promising RL-based agents often suffer from the learning efficiency issue in solving tasks with complicated goals and sparse reward signals.
To address this issue, we propose VSC-RL, which can autonomously decompose the goal to subgoals and resolve them efficiently.
Formulating the sequential decision-making task as a variational subgoal-conditioned RL problem with the optimization objective of SGC-ELBO.
We also provide a theoretical guarantee of the performance of VSC-RL.
In various benchmarks, especially in challenging mobile device control tasks, we empirically show that VSC-RL exhibits significant performance improvement and learning efficiency, remarkably outperforming existing methods.



% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}
\clearpage
% \section*{Impact Statement}
% This paper aims to advance the field of Machine Learning. While acknowledging there are many potential societal consequences of our work, we believe that none need to be specially highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Implementation Details}
As shown in \Cref{appendix:table:vlm}, we summarize VLMs used in VSC-RL.
For MiniGrid, we built our VSC-RL on the open repository of babyAI~\citep{babyai_iclr19}, hyper-parameter settings are listed in \Cref{appendix:table:parameter_minigrid}. 
For AitW tasks, we built our VSC-RL on the open repository of DigiRL~\citep{bai2024digirl}, hyper-parameter settings are listed in \Cref{appendix:table:parameter_aitw}. 




\begin{table*}[h]
\centering
\caption{The summary of VLMs used in VSC-RL.}
\label{appendix:table:vlm}
\begin{tabular}{lll}
\hline
Component Name          & VLM Type                                                    & Description                                                 \\ \hline
Subgoal Generator $\text{VLM}$ & Gemini-1.5-Pro~\citep{team2024gemini} & Autonomously decompose the goal to the subgoals.          \\
Reference Actor $\pi_\text{ref}$  & AutoUI-Base~\citep{zhang2023you}      & Provide reference action for imitation.  \\
Target Actor $\pi$    & AutoUI-Base                                                 & Make decisions to maximize subgoal-conditioned return.                        \\
Evaluator~\citep{pan2024autonomous}         & Gemini-1.5-Pro                                              & Autonomously evaluate the goal's or subgoal's success. \\ \hline
\end{tabular}
\end{table*}


\begin{table}[ht]
\centering
\caption{Hyper-parameters setting of VSC-RL on MiniGrid. \label{appendix:table:parameter_minigrid}}
\begin{tabular}{|cc|}
\hline
Hyper-parameter                    & Value          \\ \hline
Batch Size                         & 256            \\
Total Steps                        & 200,000        \\
Discount Factor                    & 0.99           \\
Learning Rate                      & 1e-3           \\
Network Layers (Image)             & 3              \\
Network Layers (Text)              & 1              \\
Network Layers (Actor)             & 2              \\
Network Layers (Critic)            & 2              \\
Activation                         & ReLU           \\
Optimizer                          & Adam           \\ \hline
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Hyper-parameters setting of VSC-RL on AitW tasks. \label{appendix:table:parameter_aitw}}
\begin{tabular}{|cc|}
\hline
Hyper-parameter                    & Value          \\ \hline
Batch Size                         & 4              \\
Total Trajectories                 & 1,000          \\
Discount Factor                    & 0.5            \\
Learning Rate                      & 1e-4           \\
Update Epoch (Actor \Cref{eq:rl_pi}) & 20             \\
Update Epoch (Critic \Cref{eq:rl_value})& 5              \\
Update Epoch (Actor \Cref{eq:il_ref}) & 20             \\
Update Epoch (Instruction-level Critic)& 5              \\
Maximum Gradient Norm               & 0.01             \\
Update Epoch (Actor)               & 20             \\ \hline
\end{tabular}
\end{table}

% (\Cref{eq:rl_pi} and \Cref{eq:rl_value})
% (\Cref{eq:il_ref})

\clearpage
\section{Theoretical Analysis}
\begin{proposition}[Subgoal-Conditioned Optimization Objective\label{appendix:soo}]
    Given a goal $g$ with corresponding subgoals $\{sg_i\}_{i=1}^N$ and a subgoal-conditioned target policy $\pi$, the objective of
    $$
    \max_{\pi}\mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau| g)}\left[\log p(O|\tau, g)\right]
    $$
    is equivalent to the objective of
    $$
    \max_{\pi}\sum_{i=1}^N\left[\mathop{\mathbb{E}}_{\tau_{i} \sim p_{\pi}(\tau_{i}| sg_{i})}\left[\log p(O|\tau_{i}, sg_{i})\right]\right].
    $$
\end{proposition}

\begin{proof}
We have 
$$
\log p(O|\tau, g) \propto \exp\left(\frac{\mathcal{J}(\tau|g)}{\alpha}\right)=\exp\left(\frac{\mathop{\sum}_{i=1}^N\left[\mathcal{J}(\tau_i|sg_i)\right]}{\alpha}\right).
$$

So, we have
$$
\begin{aligned}
    &\mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau| g)}\left[\log p(O|\tau, g)\right]\\
    & = \mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau, g)}\left[\mathcal{J}(\tau, g)\right]\\
    & = \mathop{\mathbb{E}}_{\tau \sim \Pi_{i=1}^N p_\pi(\tau_i, sg_i)}\left[\mathcal{J}(\tau| g)\right]\\
    & = \mathop{\mathbb{E}}_{\tau \sim \mathop{\Pi}_{i=1}^N p_\pi(\tau_i, sg_i)}\left[\sum_{i=1}^N\mathcal{J}(\tau_i| sg_i)\right]\\
    & = \sum_{i=1}^N\left[\mathop{\mathbb{E}}_{\tau_i \sim p_\pi(\tau_i, sg_i)}\left[\mathcal{J}(\tau_i| sg_i)\right]\right]\\
\end{aligned}
$$

Due to the fact that 
$$
\log p(O|\tau_i, sg_i) \propto \exp\left(\frac{\mathcal{J}(\tau_i|sg_i)}{\alpha}\right).
$$

Therefore, we have
$$
\mathop{\mathbb{E}}_{\tau \sim p_{\pi}(\tau| g)}\left[\log p(O|\tau, g)\right]
\Rightarrow
\sum_{i=1}^N\left[\mathop{\mathbb{E}}_{\tau_{i} \sim p_{\pi}(\tau_{i}| g_{i})}\left[\log p(O|\tau_{i}, sg_{i})\right]\right]
$$


\end{proof}


\begin{proposition}[Subgoal-conditioned Difference Bound\label{appendix:sdb}]
Given goal-conditioned reference policy $\pi_\text{ref}$ and subgoal-conditioned target policy $\pi$, the goal-conditioned KL divergence of a given goal $g$ has the upper bound of subgoal-conditioned KL divergence of corresponding subgoals $\{sg_i\}_{i=1}^N$ as followed:
$$
    \text{KL}(p_{\pi}(\tau| g)||p_{\pi_\text{ref}}(\tau| g)) 
    \leq
    \sum_{i=1}^N\left[\text{KL}(p_{\pi}(\tau_i| sg_i)||p_{\pi_\text{ref}}(\tau_i| g))\right].
$$
\end{proposition}

\begin{proof}

We have
$$
\begin{aligned}
p_\pi(\tau| g) &= \rho(s_0) \prod_{t=0}^{H} P(s_{t+1}|s_{t}, a_{t}) \pi(a_t|s_t, g), \\
% &=\Pi_{i=1}^N p_\pi(\tau_i, g)\\
&=\mathop{\Pi}_{i=1}^N p_\pi(\tau_i| sg_i).\\
&\leq p_\pi(\tau_i| sg_i) (i=1, \cdots, N)\\
\end{aligned}
$$
Similarly, we have
$$
\begin{aligned}
p_{\pi_\text{ref}}(\tau| g) = \mathop{\Pi}_{i=1}^N p_{\pi_\text{ref}}(\tau_i| g)&\\
\end{aligned}
$$

Therefore,
$$
\begin{aligned}
&\text{KL}(p_{\pi}(\tau| g)||p_{\pi_\text{ref}}(\tau| g)) \\
&= \mathop{\mathbb{E}}_{\tau \sim p_\pi(\tau| g)}\left[\log p_\pi(\tau| g) - \log p_{\pi_\text{ref}}(\tau| g) \right]\\
&= \mathop{\mathbb{E}}_{\tau \sim p_\pi(\tau| g)}\left[\sum_{i=1}^N \log p_\pi(\tau_i| sg_i) - \sum_{i=1}^N \log p_{\pi_\text{ref}}(\tau_i| g) \right]\\
&= \sum_{i=1}^N \left[\mathop{\mathbb{E}}_{\tau \sim p_\pi(\tau| g)}\left[\log p_\pi(\tau_i| sg_i) - \log p_{\pi_\text{ref}}(\tau_i| g) \right]\right]\\
&\leq \sum_{i=1}^N \left[\mathop{\mathbb{E}}_{\tau_i \sim p_\pi(\tau_i| sg_i)}\left[\log p_\pi(\tau_i| sg_i) - \log p_{\pi_\text{ref}}(\tau_i| g) \right]\right]\\
&=\sum_{i=1}^N\left[\text{KL}(p_{\pi}(\tau_i| sg_i)||p_{\pi_\text{ref}}(\tau_i| g))\right]\\
\end{aligned}
$$    

\end{proof}

\clearpage
\section{Prompt Example}
\label{appendix:prompt_example}
We provide the prompt example of the subgoal generator in our VSC-RL for a given goal and corresponding decomposed subgoals in \Cref{fig:prompting_example_multirooms} and \Cref{fig:prompting_example_aitw}.

\begin{figure*}[h]
\centering
% \includegraphics[width=0.9\textwidth]{figs/appendix/prompt_multirooms.pdf}
\includegraphics[width=0.9\textwidth]{figs/appendix/prompt_babyai.pdf}
% \includegraphics[width=0.9\textwidth]{figs/appendix/prompt_babyai.drawio.png}
\caption{Prompt example for our subgoal generator for MultiRoom Benchmark. The generator decomposes the goal of navigating the maze into subgoals like opening specific doors sequentially.}
\label{fig:prompting_example_multirooms}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.9\textwidth]{figs/appendix/prompt_aitw.pdf}
\caption{Prompt example for our subgoal generator for tasks in AitW dataset. The generator decomposes user commands into actionable subgoals, such as opening a browser, searching for items, and selecting desired results.}
\label{fig:prompting_example_aitw}
\end{figure*}

\clearpage
\section{Qualitative Example}
\label{appendix:qualitative_example}

We provide qualitative examples of VSC-RL applied to MultiRoom (\Cref{fig:qualitative_example_multiroom}), AitW General (\Cref{fig:qualitative_example_1}), and Web Shopping tasks (\Cref{fig:qualitative_example_2}).

\begin{figure*}[h]
\centering
\includegraphics[width=0.9\textwidth]{figs/appendix/example_multiroom.png}
% \includegraphics[width=0.9\textwidth]{figs/appendix/example_multiroom_flowchart.drawio-1.png}
\caption{Qualitative example of VSC-RL on the Multiroom task.}
\label{fig:qualitative_example_multiroom}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.9\textwidth]{figs/appendix/example_aitw_1.png}
% \includegraphics[width=0.9\textwidth]{figs/appendix/example_aitw_1_flowchart.drawio-1.png}
\caption{Qualitative example of VSC-RL on the General task.}
\label{fig:qualitative_example_1}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.9\textwidth]{figs/appendix/example_aitw_2.png}
% \includegraphics[width=0.9\textwidth]{figs/appendix/example_aitw_2_flowchart.drawio-1.png}
\caption{Qualitative example of VSC-RL on the Web Shopping task.}
\label{fig:qualitative_example_2}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
