\section{Related Works}
\label{sec:related_works}

\subsection{Vision-language Agents for Decision-making}
In real-world complex control tasks requiring capacities in reasoning, planning and content understanding, it is necessary to enable agents with the vision-language models.
In particular, the vision-language model (VLM) can process and abstract the image and language content for challenging decision-making tasks, especially in mobile device control tasks.
Existing vision-language agents can be categorized as prompting-based, imitation-based, and RL-based agents based on the corresponding learning paradigms.
Additionally, recent advances explore using VLM as subgoal generator to decompose the complicated goal in long-horizon tasks.

\paragraph{Prompting-based Agent.}
Leveraging the inherent reasoning and planning abilities of prosperity VLMs (e.g., **Deng et al., "Gemini-1.5-Pro"**__**Brown et al., "GPT-4V"**), the prompting-based agent makes decision via prompting engineering and retrieving techniques.
For instance, **Henderson et al., "AppAgent"**, first introduces a unified prompting-based agent method to enable the vision-language model to directly interact with mobile applications by providing the prompts with details of actions.
**Wang et al., "Set-of-Marks"**, proposes a new prompting method to enhance the visual grounding ability of VLM.
However, the performance of these prompting-based agents is always sensitive to prompts required manually and carefully designed.
Therefore, it is challenging for the prompting-based agent to directly output the correct and desired actions to address real-world complex control problems.

\paragraph{Imitation-based Agent.}
The imitation-based agent learns to mimic the expert behaviors by fine-tuning the policy on human demonstration.
Recently, **Dai et al., "Android in the Wild (AitW)"**, establish large-scale datasets of mobile device control tasks, enabling agents to directly learn from human experience.
**Li et al., "AutoUI"**__**Kim et al., "CogAgent"**, fine-tune the VLM-based policies with the AitW dataset, remarkably outperforming the prompting-based agent.
In order to adapt the fine-tuned agent to the online environment, **Zhang et al., "Filtered BC"**, introduces online imitation mechanisms to learn from successful online experiences. 
Unfortunately, these methods rely heavily on high-quality human demonstrations and often struggle to generalize to unseen tasks, limiting their application in diverse real-world scenarios.


\paragraph{RL-based Agent.}
Different to prompting-based and imitation-based agents, the RL-based agent can autonomously optimize the policy through trial-and-error interactions with environments, without human supervision.
**Liu et al., "DigiRL"**, introduces a unified offline-to-online RL framework that enables agents to learn directly from real-time interactions in dynamic environments, improving performance without the need for curated datasets.
**Chen et al., "DistRL"**, builds an asynchronous distributed RL system, allowing training multiple agents in parallel across different environments, thus significantly enhancing scalability and convergence speed.
However, these RL-based agents still fundamentally suffer from the learning efficiency issue in challenging sequential decision-making tasks with the sparse reward and long-horizon.


\paragraph{Enhancing RL with VLM.}
Recent works have shown that VLM can enhance the RL method via its remarkable capacities of reasoning, planning, and content understanding.
Recent works suggest adopting VLM in reward-shaping for RL.
For instance, **Wang et al., "VLM-RMs"**, demonstrate that VLMs can serve as effective reward models for learning complex skills.
VLM can also generate the subgoals to guide the learning process for autonomous driving** and robot** tasks.
Nonetheless, it is still an open problem how to effectively integrate the VLM-generated subgoals into RL.

To mitigate these above issues, we present VSC-RL which can autonomously decompose the goal into feasible subgoals by advanced VLM, and then efficiently resolve each subgoal from the principle of variational inference.


\subsection{Goal-conditioned and Variational RL}
We introduce the common RL methods in addressing the control tasks, including goal-conditioned RL and variational RL.

% RL is a promising learning para-diagram in addressing complex sequential decision-making tasks.
\paragraph{Goal-conditioned RL.}
Sequential decision-making tasks can be viewed as the goal-conditioned RL probelm**.
Based on the current state, the agent aims to find the optimal policy that guides progress toward the given goal for maximizing the return.
**Schaul et al., "Hindsight experience replay"**, introduces an implicit curriculum learning method to enhance learning efficiency and robustness.
With the perspective of divide-and-conquer, some approaches suggest that guiding the agent with subgoals as intermediate reward signals via imagination** and tree-search**.


\paragraph{Variational RL.}
The RL problem can be viewed as the variational inference problem** which can be resolved by utilizing extensive optimization tools, thus effectively improving the learning efficiency.
**Barrett et al., "VIP"**, presents a unified variational inference framework applying the expectation-maximization algorithm in the actor-critic method in RL.
MPO** proposes a series of off-policy RL with entropy regulation in the manner of expectation-maximization. **Nachum et al., "VDPO"**__**Gu et al., "CVPO"**, apply the variational inference techniques in addressing RL problem with delayed signals and safety constraints, respectively.

This paper aims to show how to formulate the control problem as the variational subgoal-conditioned RL problem from the perspective of variational inference which allows us to resolve the complicated control task by utilizing extensive optimization tools.