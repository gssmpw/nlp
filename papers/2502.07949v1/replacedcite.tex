\section{Related Works}
\label{sec:related_works}

\subsection{Vision-language Agents for Decision-making}
In real-world complex control tasks requiring capacities in reasoning, planning and content understanding, it is necessary to enable agents with the vision-language models.
In particular, the vision-language model (VLM) can process and abstract the image and language content for challenging decision-making tasks, especially in mobile device control tasks.
Existing vision-language agents can be categorized as prompting-based, imitation-based, and RL-based agents based on the corresponding learning paradigms.
Additionally, recent advances explore using VLM as subgoal generator to decompose the complicated goal in long-horizon tasks.

\paragraph{Prompting-based Agent.}
Leveraging the inherent reasoning and planning abilities of prosperity VLMs (e.g., Gemini-1.5-Pro____ and GPT-4V____), the prompting-based agent makes decision via prompting engineering and retrieving techniques.
For instance, AppAgent____ first introduces a unified prompting-based agent method to enable the vision-language model to directly interact with mobile applications by providing the prompts with details of actions.
Set-of-Marks____ proposes a new prompting method to enhance the visual grounding ability of VLM.
However, the performance of these prompting-based agents is always sensitive to prompts required manually and carefully designed.
Therefore, it is challenging for the prompting-based agent to directly output the correct and desired actions to address real-world complex control problems.

\paragraph{Imitation-based Agent.}
The imitation-based agent learns to mimic the expert behaviors by fine-tuning the policy on human demonstration.
Recently, Android in the Wild (AitW)____ establish large-scale datasets of mobile device control tasks, enabling agents to directly learn from human experience.
AutoUI____ and CogAgent____ fine-tune the VLM-based policies with the AitW dataset, remarkably outperforming the prompting-based agent.
In order to adapt the fine-tuned agent to the online environment, Filtered BC____ introduces online imitation mechanisms to learn from successful online experiences. 
Unfortunately, these methods rely heavily on high-quality human demonstrations and often struggle to generalize to unseen tasks, limiting their application in diverse real-world scenarios.


\paragraph{RL-based Agent.}
Different to prompting-based and imitation-based agents, the RL-based agent can autonomously optimize the policy through trial-and-error interactions with environments, without human supervision.
DigiRL____ introduces a unified offline-to-online RL framework that enables agents to learn directly from real-time interactions in dynamic environments, improving performance without the need for curated datasets.
DistRL____ builds an asynchronous distributed RL system, allowing training multiple agents in parallel across different environments, thus significantly enhancing scalability and convergence speed.
However, these RL-based agents still fundamentally suffer from the learning efficiency issue in challenging sequential decision-making tasks with the sparse reward and long-horizon.


\paragraph{Enhancing RL with VLM.}
Recent works have shown that VLM can enhance the RL method via its remarkable capacities of reasoning, planning, and content understanding.
Recent works suggest adopting VLM in reward-shaping for RL.
For instance, VLM-RMs____ demonstrate that VLMs can serve as effective reward models for learning complex skills.
VLM can also generate the subgoals to guide the learning process for autonomous driving____ and robot____ tasks.
Nonetheless, it is still an open problem how to effectively integrate the VLM-generated subgoals into RL.

To mitigate these above issues, we present VSC-RL which can autonomously decompose the goal into feasible subgoals by advanced VLM, and then efficiently resolve each subgoal from the principle of variational inference.


\subsection{Goal-conditioned and Variational RL}
We introduce the common RL methods in addressing the control tasks, including goal-conditioned RL and variational RL.

% RL is a promising learning para-diagram in addressing complex sequential decision-making tasks.
\paragraph{Goal-conditioned RL.}
Sequential decision-making tasks can be viewed as the goal-conditioned RL probelm____.
Based on the current state, the agent aims to find the optimal policy that guides progress toward the given goal for maximizing the return.
Hindsight experience replay____ introduces an implicit curriculum learning method to enhance learning efficiency and robustness.
With the perspective of divide-and-conquer, some approaches suggest that guiding the agent with subgoals as intermediate reward signals via imagination____ and tree-search____.


\paragraph{Variational RL.}
The RL problem can be viewed as the variational inference problem____ which can be resolved by utilizing extensive optimization tools, thus effectively improving the learning efficiency.
Applying the expectation-maximization algorithm in the actor-critic method in RL, VIP____ presents a unified variational inference framework.
MPO____ proposes a series of off-policy RL with entropy regulation in the manner of expectation-maximization. VDPO____ and CVPO____ apply the variational inference techniques in addressing RL problem with delayed signals and safety constraints, respectively.

This paper aims to show how to formulate the control problem as the variational subgoal-conditioned RL problem from the perspective of variational inference which allows us to resolve the complicated control task by utilizing extensive optimization tools.

% \paragraph{Multi-modal device control agents.} Recent advancements in multimodal large language models (MLLMs) have enabled device control agents to process both visual and textual inputs, allowing interaction with complex graphical user interfaces (GUIs). These models can perform tasks like web navigation, app interaction, and mobile device management by interpreting visual data and generating pixel-level actions. Early methods, without fine-tuning, relied on pre-trained models, which limited agent performance due to the base model's generalization capabilities. Fine-tuning approaches like AutoUI and CogAgent improved performance by adapting models to specific GUI tasks using demonstration data. However, these methods struggled with dynamic environments, as they were constrained by static datasets and lacked mechanisms to adapt to unforeseen user inputs or changing content. To overcome these challenges, reinforcement learning (RL)-based methods introduced strategies for enabling agents to learn from real-time interactions, paving the way for more adaptive device control.

% \paragraph{Reinforcement learning fine-tuning for device control agents.} RL provides a powerful approach for fine-tuning device control agents to handle dynamic and complex environments. Traditional methods, such as Reinforcement Learning from Human Feedback (RLHF), rely on human annotations, which are labor-intensive and costly at scale. To mitigate this, recent advancements have introduced AI evaluators, enabling automated feedback mechanisms for task assessment. While RLHF and single-turn RL approaches have demonstrated success in static or simple tasks, they struggle with the multi-step, long-horizon challenges inherent to device control. Recent works like DigiRL and DistRL employ multi-turn RL strategies, allowing agents to adapt to stochastic environments through online interactions. However, these methods often face inefficiencies, including slow convergence and sensitivity to hyperparameters in high-variance tasks. These limitations highlight the need for more scalable and efficient RL fine-tuning methods tailored to the unique challenges of device control.

% \paragraph{Subgoal-conditioned learning with VLMs.} Subgoal generation has been extensively explored in fields such as motion planning and hierarchical reinforcement learning, where breaking down tasks into intermediate objectives significantly improves efficiency and robustness. Vision-Language Models (VLMs), with their ability to process multimodal inputs, have recently been applied to dynamically generate subgoals in long-horizon and sequential decision-making tasks. These approaches leverage VLMs to decompose complex tasks, providing intermediate goals that improve task success rates by reducing complexity and enabling context-aware reasoning. In device sequential decision-making tasks, characterized by sparse rewards, long horizons, and high variability, introducing subgoal-based reasoning offers a powerful mechanism to structure tasks and enhance learning efficiency. Our work is the first to integrate subgoal generation into device sequential decision-making tasks and to propose an optimization framework for subgoal-conditioned policies. By combining VLM-driven subgoal reasoning with reinforcement learning, we dynamically validate and optimize subgoals, addressing the unique challenges of task complexity and variability in real-world device control scenarios.



\begin{figure*}[t]
\centering
% \vskip -0.1in
% \includegraphics[width=\textwidth]{figs/method/pipeline_1.pdf}
\includegraphics[width=\textwidth]{figs/method/vscrl.pdf}
% \vskip -0.1in
\caption{The pipeline of VSC-RL. (a) VLM autonomously decomposes the goal $g$ to the subgoals $\{sg_i\}_{i=1}^N$. VSC-RL optimizes the objective of $\text{SGC-ELBO}$ consisting of (b) maximizing the subgoal-conditioned return and (c) minimizing the subgoal-conditioned difference.}
\label{fig:Overview}
% \vskip -0.1in
\end{figure*}