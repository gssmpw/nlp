\section{Appendix: Reward Design} \label{sec:appendix}
\vspace{-.5em}
\begin{table}[h!]
\centering
\resizebox{.7\linewidth}{!}{\begin{minipage}{.85\linewidth}
\centering
\small
\input{table/comparison_single-bs_custom}
\end{minipage}}
\caption{Reward design variations for single (static) BS deployment.}
\label{table:reward}
\vspace{-.5em}
\end{table}

This section presents empirical findings from experiments with different reward designs in the AutoBS framework. The pathgain is defined as $P = \sum_{{i,j} \in \mathcal{R}}{P_{i,j}}$, and Table \ref{table:reward} summarizes coverage and capacity outcomes for each reward configuration. Although other transformations, such as logarithmic scaling (\eg $\log{V}$), were tested, they are omitted here for brevity. These findings are empirical and sensitive to training hyperparameters (\eg learning rate) and reward design parameters (\eg $\nu$).

The results provide valuable insights. As expected, the Coverage Only reward achieves the highest coverage, aligning with its direct optimization objective (\eg $\nu_2, \nu_3 = 0$). Interestingly, in terms of capacity, the combination of Pathgain and Coverage (\ie $\nu_2=0$) outperforms the Capacity Only reward. Notably, Coverage Only also surpasses Capacity Only in capacity, highlighting complex dynamics within the DRL training process. These findings suggest that site-specific channel characteristics are implicitly embedded in rewards derived from PMNetâ€™s pathloss maps. While Capacity Only smooths variations through logarithmic scaling, Pathgain captures these fluctuations on a linear scale, providing more granular feedback for effective learning.

Throughout this work, the Pathgain + Coverage reward configuration is primarily used.





