%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AutoBS: An Autonomous Base Station Deployment Framework} \label{sec:autobs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Architecture}

The AutoBS framework automates optimal BS placement in 6G networks using DRL, with a PPO agent. PPO is well-suited for this task due to its balance between exploration (testing new deployment locations) and exploitation (refining placement strategies). By using a clipped objective function, PPO ensures stable policy updates and prevents large, destabilizing changes during training.

The framework supports two deployment modes: (1) \emph{static single BS deployment}, where a single BS is placed in a fixed environment, and (2) \emph{asynchronous multi-BS deployment}, where multiple BSs are deployed asynchronously.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\textbf{Single (Static) BS Deployment}}
In the \textit{static} single BS deployment scenario, the geographical environment and network conditions are assumed to be fixed. The input to the system is a site-specific map $\mathcal{S}$, which contains detailed information about buildings, terrain, and obstacles. The PPO agent processes this map and outputs the optimal location $(i, j)$ for placing the BS to maximize coverage and capacity. 

% This approach is most effective for stable environments where network demand and user behavior remain relatively constant. Dynamic adjustments to network conditions are not required.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\textbf{Multi (Asynchronous) BS Deployment}}
In the \textit{asynchronous} multi-BS deployment scenario, BSs are deployed incrementally over time. After each deployment, the environment is updated to reflect the changes in network conditions, such as coverage, capacity, and user distribution. The PPO agent adjusts its strategy after each deployment based on real-time feedback, refining its decisions progressively.

% This approach allows the agent to adapt to dynamic environments where network conditions evolve. By deploying BSs one at a time, the agent can evaluate the impact of each BS placement, optimizing the network's performance iteratively. Asynchronous deployment is especially useful in practical network scenarios where external constraints, such as building leases or regulatory requirements, must be considered.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Metrics}

The AutoBS framework's performance is evaluated using two key metrics: \emph{Coverage} and \emph{Capacity}. These metrics are essential for assessing the quality of BS placements in terms of both spatial coverage and network throughput.

\BfPara{Coverage} represents the proportion of the area or users receiving sufficient signal strength, \eg received power, from the deployed BSs. It is calculated as the total area where the received power $P_{i,j}$ exceeds a predefined threshold $\mathrm{thr}$, ensuring satisfactory Quality of Service (QoS). The coverage $V$ is expressed as:
\begin{align}
V = \sum_{\{i,j\} \in \mathcal{R}}{v_{i,j}},
\end{align}
where $\mathcal{R}$ is the region of interest (RoI), e.g., non-deployable and non-building areas, and $v_{i,j}$ is a binary indicator of coverage:
\begin{align}
v_{i,j} = \begin{cases}
    1, & P_{i,j} \geq \mathrm{thr}, \\
    0, & P_{i,j} < \mathrm{thr}.
\end{cases}
\end{align}
The threshold $\mathrm{thr}$ depends on network QoS requirements (e.g., the minimum signal strength for reliable communication).

\BfPara{Capacity} represents the maximum amount of data that can be transmitted or received over a network, which depends on the quality of the connection, \eg signal-to-noise ratio (SNR). The total capacity $C$ is given by:
\begin{align}
    C = \sum_{\{i,j\} \in \mathcal{R}} \log_2{\left( 1 + \text{SNR}_{i,j} \right)},
\end{align}
where $\text{SNR}_{i,j}$ is the signal-to-noise ratio at pixel $(i,j)$, calculated as:
\begin{equation}
    \text{SNR}_{i,j} = \frac{\sum_{k} P^{k}_{i,j}}{\sigma^2}.
\end{equation} 
Here, $P^{k}_{i,j}$ represents the received power from the $k$-th BS at pixel $(i,j)$, and $\sigma^2$ is the noise variance. In our simulations, we chose $\sigma^2 = \eta /4$, ensuring that the SNR at the coverage boundary is at least 6 dB.\footnote{In an actual DT simulation, noise variance would obviously follow from the system bandwidth, transmit power, receiver noise figure, and antenna again; but since cell-edge SNR of 6 dB is a common planning goal, we directly used it here).} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training}
The AutoBS framework models the base station (BS) deployment problem as a Markov Decision Process (MDP), enabling the agent to interact with its environment to make optimal BS placement decisions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{MDP Design}
\BfPara{Environment}
\tblue{The environment for the PPO agent is defined by the interactions within a site-specific building map, where each deployed BS impacts the network performance metrics such as coverage and capacity. At each time step $t$, the agent interacts with this environment by observing a state $s_t \in \mathcal{S}$, which includes details on buildings, obstacles, and terrain specific to the deployment site. The agent then takes an action $a_t \in \mathcal{A}$, selecting a BS deployment location based on its learned policy $\pi$. After the action is executed, the environment transitions to a new state $s_{t+1}$, representing the updated network layout and performance with the new BS in place. The agent receives a reward $r_t$ based on the effectiveness of its decision, guiding it toward an optimal deployment policy $\pi^*$.}

\BfPara{State}
\tblue{The state $s_t$ at time step $t$ provides key information for decision-making about BS placement. Formally, the state is expressed as:
\begin{align}
s_t = \left\{ \mathcal{S} \right\},
\end{align}
where $\mathcal{S}$ represents the site-specific building map. This map encompasses critical details about building locations, obstacles, and terrain, all of which significantly influence signal propagation (\ie site-specific channel). 
}
% The minimal state design captures the key spatial characteristics required for optimal deployment while maintaining computational efficiency, enabling the agent to make well-informed placement decisions with minimal processing overhead.}

\BfPara{Action}
The action space $\mathcal{A}$ consists of potential BS deployment locations. At each time step $t$, the agent selects an action $a_t \in \mathcal{A}$, which corresponds to placing a BS at a specific geographical coordinate $(i, j)$ on the site-specific map. The action is represented as:
\begin{equation}
a_t = (i, j), \quad \{i,j\} \in \mathcal{B},
\end{equation}
where $\mathcal{B}$ is the set of all permissible deployment locations within the map. The agent deploys BSs sequentially, choosing new locations at each time step based on current network needs. This approach is computationally efficient, allowing the agent to adapt its strategy in real-time as it learns from previous decisions.

\BfPara{Reward}
The reward function $r_t$ incentivizes the agent to improve network performance by maximizing both coverage and capacity. The reward at time step $t$ is defined as:
\begin{equation}
r_t = \nu_1 V_{t} + \nu_2 C_{t} + \nu_3 P_{t},
\end{equation}
where $V_{t}$ and $C_{t}$ represent the improvements in coverage and capacity, respectively, and $P_{t}=\sum_{\{i,j\} \in \mathcal{R}} P_{i,j}$ denotes the total pathgain, for the $t$-th deployment. The weighting parameter $\nu$ adjusts the relative importance of coverage, capacity, and pathgain in the overall network optimization. For further details regarding the reward design, please refer to Sec. \ref{sec:appendix}. Note also that for more general DT applications (\eg incorporating user demands), only the reward function needs to be adapted, while the remaining framework of AutoBS stays the same.

As mentioned in Sec. \ref{sec:method_2}, the reward must be recalculated after each BS deployment to reflect the updated network conditions. This typically requires coverage and capacity evaluation from pathloss map prediction, which are computationally expensive. To overcome this challenge, the AutoBS framework integrates the  \emph{PMNet} model, which provides fast and accurate pathloss predictions, enabling efficient reward calculations. PMNet allows the agent to quickly assess the network performance after each BS placement, ensuring that the training process is both realistic and computationally feasible.

\begin{figure}[!t]
\centering
\includegraphics[width=.98\linewidth]{figure/train_autobs.pdf}
\caption{Training for AutoBS framework.}
\label{fig:training}
% \vspace{-1.em}
\end{figure} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Training Process}

The PPO agent is trained using the PPO algorithm \cite{schulman2017proximal}, which allows the agent to iteratively interact with a simulated environment. The agent selects BS deployment actions and receives rewards based on improvements in network coverage and capacity, progressively refining its policy $\pi_\theta(a_t | s_t)$ over time.

At each time step $t$, the agent updates its policy to maximize the cumulative reward:
\begin{equation}
J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t r_t \right],
\end{equation}
where $\gamma$ is the discount factor, $r_t$ is the reward received at time step $t$, and $T$ is the time horizon over which the agent aims to optimize its decisions. The discount factor $\gamma$ balances immediate and future rewards, promoting long-term planning in BS placement.

The PPO agent optimizes the following objective function:
\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right],
\end{equation}
where $r_t(\theta)$ is the probability ratio between the updated and previous policies, $\epsilon$ is a clipping parameter to limit policy changes for stability, and $\hat{A}_t$ represents the advantage function, which quantifies how much better the current action is compared to the baseline.

For clarity and space reasons, we omit detailed descriptions of the PPO algorithm, and instead present a summary of the training process in Fig~\ref{fig:training}, which outlines the key steps involved in training the PPO agent.


