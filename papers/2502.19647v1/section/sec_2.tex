%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology} \label{sec:method}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Formulation and DRL Approach}

% Efficient BS deployment is essential in 6G networks to balance \emph{coverage} and \emph{capacity}, particularly with high-frequency signals like mmWave that are prone to attenuation. Urban environments add complexity with obstacles, requiring strategic BS placement to maintain network performance.

The BS deployment problem can be mathematically formulated as an optimization task. Let the BS location be represented by coordinates $(i, j)$ on a site-specific building map $m$. The objective function to optimize is:
\begin{align}
\max_{\{i,j\}} \sum_{m=1}^{M} \left( V_m + \nu C_m \right), \ \text{s.t.} \ \{i,j\} \in \mathcal{B}, \label{eq:OptObj}
\end{align}
where $V_m$ is the coverage, $C_m$ represents the capacity, and $\nu$ is a weighting factor that governs the trade-off between coverage and capacity. The set $\mathcal{B}$ defines the deployable areas for BS placement, such as rooftops or other designated locations within the environment.

AutoBS frames the deployment problem as a reinforcement learning task, where a DRL agent interacts with the environment to learn optimal placement strategies. Through continuous feedback via \emph{rewards}, the agent refines its decisions, capturing site-specific channel characteristics and optimizing network performance over time.

\begin{figure}[!t]
\centering
\includegraphics[width=0.85\linewidth]{figure/overview_autobs_2.pdf}
\caption{Overview of AutoBS framework.}
\label{fig:framework}
% \vspace{-1.em}
\end{figure}

% To solve this optimization problem, we adopt a \emph{deep reinforcement learning} (DRL) approach, framing it as a \emph{Markov Decision Process} (MDP). The MDP consists of the following components:

% \begin{itemize}
%     \item \textbf{State} $s_t$: The state at time step $t$, denoted as $s_t$, captures the current network state, including the site-specific map with geographical features, such as buildings, obstacles, and other factors that affect signal propagation.
    
%     \item \textbf{Action} $a_t$: The action space defines possible BS deployment locations. At each time step, the agent selects an action $a_t$, which corresponds to deploying a BS at a specific location $(i, j)$ within the deployable area $\mathcal{B}$.
    
%     \item \textbf{Reward} $r_t$: The reward measures the impact of the BS placement on network performance, evaluating improvements in both \emph{coverage} and \emph{capacity}. The reward function can prioritize either or both metrics, depending on the deployment objectives.
% \end{itemize}

% The goal of the deployment process is to learn a policy $\pi_\theta(a_t | s_t)$, parameterized by $\theta$, which maps the current state $s_t$ to an optimal action $a_t$. The objective is to maximize the expected cumulative reward over time. To solve this MDP, we use the \emph{Proximal Policy Optimization} (PPO) algorithm, which is particularly suitable for balancing exploration (trying new deployment locations) and exploitation (refining BS placement strategies based on learned knowledge), which will be discussed further in the following section.

% This DRL-based formulation enables the agent to learn an optimal BS placement policy over time, balancing the trade-off between short-term rewards (immediate improvement in coverage/capacity) and long-term network performance. Throughout training, the agent engages in thousands of trials, refining its decisions via \emph{error-and-trial} learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reward Calculation via PMNet} \label{sec:method_2}
In DRL, efficient reward calculation is essential for guiding the learning process, where the agent explores numerous actions throughout training. However, calculating network performance metrics, such as coverage and capacity, based on BS placements—key components of the reward function—is computationally intensive. Traditional RT simulations, which can take tens of minutes to generate a single pathloss map, are impractical for real-time reinforcement learning. 

To overcome this challenge, AutoBS integrates the \emph{PMNet} framework, a DT generator, for rapid reward computation. PMNet predicts pathloss maps with an RMSE on the order of $10^{-2}$ within milliseconds, enabling the Proximal Policy Optimization (PPO) algorithm \cite{schulman2017proximal} to immediately evaluate network performance and receive reward feedback after each deployment decision. This fast feedback allows the agent to efficiently simulate and assess multi-BS deployment scenarios, significantly accelerating training by exploring more placement options with timely updates. The integration of PMNet ensures both speed and scalability, enabling the AutoBS to generalize effectively across diverse environments.




