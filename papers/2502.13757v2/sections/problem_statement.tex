\section{Problem statement}
\label{sec:problem_statement}
We address the challenge of making pairwise distances statistically identifiable in modern deep generative models without impractical assumptions. Next, we outline our assumptions and formalize the objective, which we solve in Sec.~\ref{results} and we show that our approach ensures not only identifiable distances, but also a broader set of identifiable metric structures.

%Before we present our result, we introduce the class of models considered in this work and characterize the identifiability problems. Towards the end, we will provide a definition of the problem that is solved in the next section.

% \subsection{Model definition} 
% Given the observed space $\mathcal{D}$ and the latent space $\mathcal{Z}$, where the dimensionality of the latter is usually lower than that of the former, we  define a \emph{deep latent variable model} as a tuple of random variables $(Z, X)$ where $Z$ is called the \emph{latent}, that is, unobserved variable of interest and $X$ is the random variable we observe. 

% In this setting, the latent variable drives the behavior of the observed through a measurable generator function $f:\mathcal{Z}\to \mathcal{D}$ sometimes called the decoder and a function $h:\mathcal{D}\times \mathcal{D}\to \mathcal{D}$ called noise mechanism that makes the relationship stochastic by introducing a noise term $\epsilon$. Thus, the observed random variable $X$ is defined in terms of the latent variable $Z$ and functions $f$ and $g$ as follows:
% \begin{equation}
%    Z_i \sim P_Z, \quad \epsilon_i \sim P_{\epsilon}, \quad X_i = h (f(Z_i), \epsilon_i) 
% \end{equation}
% with $Z_i$'s being independent of $\epsilon_i$. We further adopt a standard regularity assumption that $h$ and $P_{\epsilon}$ are such that with $\epsilon_a \eqdistrib \epsilon_b$, $h(f(Z_a), \epsilon_a) \eqdistrib h(f(Z_b), \epsilon_b)$ if and only if $f(Z_a) \eqdistrib f(Z_b)$. This is needed to ensure that the noise $\epsilon$ does not interfere with the causal relationship between the observed $X$ and the latent variables $Z$. 

% This general definition includes the case of VAEs with Gaussian likelihood where $\mathcal{Z}\subseteq \mathbb{R}^n$ and $\mathcal{D}\subseteq \mathbb{R}^m$ with $n\ll m$  and $P_Z$, $P_{\epsilon}$ are assumed standard Gaussian and the noise mechanism, $h$, is additive. 


\subsection{Assumptions on $\mathcal{F}$ and $\mathcal{Z}$} 

Following typical literature \cite{xi2023indeterminacy,8575533}, we further impose assumptions on the space of our decoder functions $\mathcal{F}$ and the latent space $\mathcal{Z}$. We consider decoders that are smooth functions $f: \mathcal{Z} \rightarrow \mathcal{D}$ such that for each $f \in \mathcal{F}$:
\begin{enumerate}[label=\textbf{A\arabic*},ref=A\arabic*]
    \item $\mathcal{Z}$ is compact. \label{ass:1}
    \item $f$ is injective. \label{ass:2}
    \item The differential of $f$, $\mathrm{d}f$, has full column rank. \label{ass:3}
    \item All $f \in \mathcal{F}$ have the same image. That is, for any $f_a, f_b \in \mathcal{F}$, we have $f_a(\mathcal{Z}) = f_b(\mathcal{Z}) := \mathcal{M} \subseteq \mathcal{D}$. \label{ass:4}
\end{enumerate}

Assumptions~\ref{ass:2}-\ref{ass:4} are repeated from \citeauthor{xi2023indeterminacy}, whereas we add assumption~\ref{ass:1} and require $f$ to be smooth. Together, these allow us to treat the image of the decoder as a smooth manifold. Assumption~\ref{ass:1} is purely technical and can be interpreted as (after model training) we consider a compact subset of the latent space, e.g.\@ the range of floating point numbers.
%
%functions in the domain of differential geometry. In particular, \ref{ass:1}-\ref{ass:3} are going to allow us to have a proper smooth manifold from the image of $f$, while \ref{ass:4} explicates the postulation that our data are distributed along a specific manifold that we seek to model.
%
%\stas{This is not super good, compactness is not a thing we want to discuss in diffusion models, normalizing flows etc. SÃ¸ren: but if we think of compactness as a requirement on the latents we seek to compare, then it's fine.} 
Jointly, the assumptions may appear restrictive, but they are satisfied by contemporary models such as $\mathcal{M}$-flows \citep{brehmer2020flows}, normalizing flows, and diffusion models. VAEs need not satisfy \ref{ass:2}. On the other hand, \ref{ass:3} can be empirically validated after model training \citep{8575533}, and experiments (Sec.~\ref{sec:experiments}) show that our methodology is effective in this setting.

% \subsection{Identifiability} Parameter identifiability for the model in Eq.~\ref{eq:StatModLvm} is usually defined by an equivalence relation. Two parameters $\theta$ and $\theta'$ are equivalent, $\theta \sim \theta'$, if the resulting distributions $P_{\theta}$ and $P_{\theta'}$ are the same. The equivalence class induced by this definition is denoted by $[\theta]=\left\{ \theta': P_{\theta} = P_{\theta'} \right\}$ and we say that a model is identifiable up to $[\theta]$, constituting \emph{weak identifiability}. Consequently, the model is \emph{strongly identifiable} if $[\theta]$ is a singleton. 

% A simple illustration of this is the standard linear model with Gaussian distribution  assumptions on the likelihood and latent variables. Due to Gaussians being rotationally invariant, the equivalent class $[\theta]$ consists of all possible parameters that are equivalent to rotations of the latent variables under which the marginal distribution of observations stays unchanged. For this particular model, rotational matrices represent the set of indeterminacy transformations leading to identifiability issues. 

% \subsection{Indeterminacy transformations} Keeping the previous example in mind, another view of identifiability is to consider the cause behind $[\theta]$ being a large set. Given two parametrizations of a generative model $\theta_a=\left(f_a, P_{Z, a}\right)$ and $\theta_b=\left(f_b, P_{Z, b}\right)$ with resulting marginal distributions $P_{\theta_a}=P_{\theta_b}$, an indeterminacy transformation at $\theta_a, \theta_b$ is a measurable function $A_{a,b}: \mathcal{Z} \rightarrow \mathcal{Z}$ such that $P_{\theta_a}=P_{\theta_b}$ and $f_a \circ A_{a,b}^{-1}=f_b$. The set of all indeterminacy transformations of the model $M(\mathcal{F}, \mathcal{P}_Z)$ is denoted by $\mathbf{A}(M)$ and has been shown by \citeauthor{xi2023indeterminacy} to give rise to $[\theta]$, thus establishing equivalence between parameter identifiability and indeterminacy transformations of the latent space and their associated decoders. 

% \subsection{Task identifiability}
% \label{task_identifiability}
% Our goal can be expressed as a \emph{task} with an identifiable outcome \citep{xi2023indeterminacy}. Here, a task is defined as a pair of functions $(s, t)$, where the \emph{selection function} $s(\theta,\mathbf{x}_m) \in  \mathcal{Z}$ determines the latent points for the task (we can think of the inverse  decoder in our case) based on observations $\mathbf{x}_m$ from a dataset, and the \emph{task function} $t(\theta,\mathbf{x}_m,s(\theta,\mathbf{x}_m))$ generates the output of interest. A task can, e.g., be independence testing in causal discovery or measuring the distance between two latent codes. 

% Following Proposition~3.1 from \citet{xi2023indeterminacy} we can state the sufficient condition for the identifiability of a task in terms of the indeterminacy transformations.
% %
% \begin{definition}
% \label{def:task_identifiability}
% A task $(s, t)$ is identifiable at $[\theta]$ if, for each $A\in \mathbf{A}(M)$ and  $\mathbf{x}_m \in \mathcal{D}$:
% \begin{equation}
% \begin{aligned}
%     t(\theta, \mathbf{x}_m ,  \mathbf{z}_n) = t(A\theta, \mathbf{x}_m , A( \mathbf{z}_n)) \\
%     \text{and } s(A\theta, \mathbf{x}_m ) = A(s(\theta, \mathbf{x}_m ))
% \end{aligned}
% \label{eq:task_identifiability}
% \end{equation}    
% where $\mathbf{z}_n \in \mathcal{Z}$ are the latent variables used for the task. 
% \end{definition}

\subsection{Identifiability of distances}
In this paper, we shift focus from identifiability of latent representations (or equivalently, model parameters) and instead identify the relations between them. As our main focus, we seek to establish a distance measure that is invariant under the indeterminacy transformations $\mathbf{A}(M)$ of a deep latent variable model and therefore identifiable. 
\begin{problem}
\label{problem}
Consider a deep latent variable model $M(\mathcal{F}, \mathcal{P}_Z)$ and $\mathbf{A}(M)$ its set of indeterminacy transformations. We want to identify latent distances, i.e.\@ find a `meaningful' distance function $d: \mathcal{Z}\times \mathcal{Z} \rightarrow \mathbb{R}_+$, such that given a parametrization $\theta$, for any $\mathbf{z}_1, \mathbf{z}_2 \in \mathcal{Z}$ and $A \in \mathbf{A}(M)$ the following is staisfied:
\begin{equation}
\label{eq:problemdef}
    d(\mathbf{z}_1,\mathbf{z}_2) = d(A(\mathbf{z}_1), A(\mathbf{z}_2))
\end{equation}
\end{problem}

The inclusion of `meaningful' in the problem definition emphasizes that solutions can be constructed that satisfy Eq.~\ref{eq:problemdef} without being of particular value, e.g.\@ the trivial metric
\begin{align}
  d(\mathbf{z}_1,\mathbf{z}_2) = \mathbb{I}(\mathbf{z}_1 \neq \mathbf{z}_2)
\end{align}
is identifiable, but reveals little about latent similarities.
Instead, we want the distance to reflect and respect the underlying mechanisms behind the observed data. 