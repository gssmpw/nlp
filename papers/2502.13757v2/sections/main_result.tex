\section{Main results}
\label{results}

%%%%%%
%As we will show in the next section, a principled solution to this problem respecting the geometry of the decoders offers with it further identifiability results extended to the latent metric space itself, thus making angles, volumes, etc. identifiable. Furthermore, the Frechet mean, a generalization of the mean to data on manifolds, is also potentially identifiable if the properties of the manifold permit it. 
%%%%%%

\paragraph{Strategy and results at a glance.}
In the following, we show that distances, angles, volumes, and more, can be identified in latent variable models that satisfy the weak assumptions in the previous section. Our proof strategy is to connect \emph{indeterminacy transformations} from the identifiability literature with \emph{charts} from the differential geometry literature. Once this connection is in place, our results easily follow. Furthermore, we use the connection to show that identifying Euclidean distances in the latent space is either impossible or requires forcing the decoder to have zero curvature.
%we extend the probabilistic analysis of identifiability in deep latent variable models by \citeauthor{xi2023indeterminacy} by a geoemetric interpretation that allows us to establish a series of results. When treated under the paradigm of differential geometry, the problem of identifiability of distances in the latent space becomes solvable for a wide class of models, and provides identifiability guarantees for related geometric quantities (Theorems~\ref{thrm:gentransforms_isometries} and \ref{thrm:identifiable_dist_func}). Furthermore, using our framework we show that identifiability of distances in the latent space necessitates a geometric treatment unless one is enforcing constraints on the decoder that result in learning inherently flat models of data (Proposition~\ref{prop:flat_models}).
%
Below we present results with proof sketches and leave details to Appendices~\ref{appendix:diffgeom} and \ref{appendix:proofs}. 

\subsection{Identifiability via geometry}

We begin by focusing on the family of decoders $\mathcal{F}$ and analyzing the properties of their image.
%
\begin{lemma}\label{thrm:f_is_embedding_main}
   Let $\mathcal{Z}$ and $\mathcal{D}$ be two smooth manifolds and $f \in \mathcal{F}$, then $f$ is a smooth embedding and $f(\mathcal{Z})\subset \mathcal{D}$ is a submanifold in $\mathcal{D}$. In particular, $f:\mathcal{Z}\rightarrow f(\mathcal{Z})$ is a diffeomorphism.
\end{lemma}
%
\begin{proofsketch}
Smoothness and assumption~\ref{ass:3} lead to $f\in \mathcal{F}$ being a smooth map of constant rank (smooth immersion) while assumptions~\ref{ass:1} and \ref{ass:2} make sure that the image of $f$ does not self-intersect. Given these properties, the claim follows from standard results in differential geometry.
\end{proofsketch}
%
One consequence of Lemma~\ref{thrm:f_is_embedding_main} is that given two trained models $\theta_a$ and $\theta_b$ with equivalent marginal distributions  $P_{\theta_a} = P_{\theta_b}$, the resulting decoder functions $f_a,f_b$ act as reparametrizations of the same manifold $f_a(\mathcal{Z}) =f_b(\mathcal{Z})=\mathcal{M}$. In particular the tuples $(f_a^{-1},\mathcal{M})$ and $(f_b^{-1},\mathcal{M})$ can be seen as coordinate charts of the manifold $\mathcal{M}$. This situation is the main subject of our analysis and is illustrated in Figure~\ref{fig:drawing}. In what follows, we will label the latent spaces by the associated models such that for $\theta_a$ we will have $\mathcal{Z}_a$ and similarly for $\theta_b$. 

\begin{figure}
    \centering
    \footnotesize{Latent spaces~~~~~~~~~~~~~~~~~~~~~~~~~~~Observation space~~~~~~~~~}\\
    \includegraphics[width=0.9\linewidth]{pics/manifold.pdf} \\ \vspace{1mm}
    \vspace{-2mm}
    \caption{Decoders $f_a$ and $f_b$ parametrize the same manifold $\mathcal{M} \subset \mathcal{D}$ when $\theta_a=\left(f_a, P_{Z_a}\right)$ and $\theta_b=\left(f_b, P_{Z_b}\right)$ give the same marginal distributions $P_{\theta_a}=P_{\theta_b}$.\looseness=-1}
    \label{fig:drawing}
\end{figure}

We will go between the different charts (or latent spaces) by using \emph{generator transformations} in Definition~\ref{def:generator_transforms} that push and pull along the respective decoders. 
%
\begin{definition}
\label{def:generator_transforms}
  Given two equivalent parametrizations $\theta_a=\left(f_a, P_{Z_a}\right)$ and $\theta_b=\left(f_b, P_{Z_b}\right)$ of a model with $P_{\theta_a}=P_{\theta_b}$, we define the \emph{generator transformation} $A_{a, b}:\mathcal{Z}_a\rightarrow \mathcal{Z}_b$ is
    \begin{equation}
\label{eq:gen_transform}
    A_{a, b}(\mathbf{z}) = f_b^{-1}\circ f_a(\mathbf{z}), \qquad \text{for } \mathbf{z}\in \mathcal{Z}_a.
\end{equation}
\end{definition}
%
Lemma 2.1 in \citet{xi2023indeterminacy}  shows that any indeterminacy transformation $A \in \mathbf{A}(M)$ must be almost everywhere equal to the generator transformation. Whereas \citeauthor{xi2023indeterminacy} focus on proving this result and using it for characterizing identifiability issues in general, we use this construction to show that it preserves the geometric properties of the manifold and, in particular, that the geodesic distance function (formally defined in Eq.~\ref{eq:geodesic_dist_def}) is invariant w.r.t.\@ to the entire set $\mathbf{A}(M)$.
%
\begin{lemma} \label{thrm:gen_transf_is_diffeo_main}
    Given two equivalent parametrizations $\theta_a=\left(f_a, P_{Z_a}\right)$ and $\theta_b=\left(f_b, P_{Z_b}\right)$ of a model with $P_{\theta_a}=P_{\theta_b}$, the generator transformations $A_{a, b}(\mathbf{z})$ and $A^{-1}_{a, b}(\mathbf{z})=A_{b, a}(\mathbf{z})$ are diffeomorphisms.
\end{lemma}
%
\begin{proofsketch}
The result follows from the fact that a composition of diffeomorphisms is a diffeomorphism. The generator construction is only possible because of assumption~\ref{ass:4}.
\end{proofsketch}
%
From the perspective of differential geometry, if we are to collect all the coordinate charts of the form $(f^{-1},\mathcal{M})$ stemming from the indeterminacy transformations in $\mathbf{A}(M)$ into a smooth atlas, then Lemma~\ref{thrm:gen_transf_is_diffeo_main} tells us that the generator transforms play the of role of smooth transition maps between them, as illustrated in Fig.~\ref{fig:drawing}.

Lemma~\ref{thrm:f_is_embedding_main} tells us that the image of our decoder functions is a smooth manifold. Due to the `Existence of Riemannian Metrics' result by \citet{lee2003introduction}, it admits a \emph{Riemannian metric} $g$ that for each point $p$ on the manifold defines an inner product in its tangent space at $p$, denoted by $T_p\mathcal{M}$. The tuple $(\mathcal{M}, g)$ defines the Riemannian manifold structure that allows measurements on general smooth manifolds and is the theoretical foundation for our methodology.

For general data manifolds, there is neither a unique nor a known metric $g$. However, given a decoder function $f$ and a chosen metric (often Euclidean) $g^{\mathcal{D}}$ in the ambient space $\mathcal{D}$, we can construct the \emph{pullback} metric $g^f$ in the latent space $\mathcal{Z}$ by pulling the ambient metric back to the latent space using the decoder. 


\begin{definition}
    \label{def:pullback_metric} Let $\mathcal{Z}$ be a smooth manifold and $(\mathcal{D},g^{\mathcal{D}})$ be a Riemannian manifold. Furthermore, let $f: \mathcal{Z} \rightarrow \mathcal{M} \subseteq \mathcal{D}$ be a map satisfying assumptions~\ref{ass:1}-\ref{ass:3}, the pullback metric $f^*g^{\mathcal{D}}$ on $\mathcal{Z}$ is defined as:
\begin{equation}
\label{eq:pullback}
    (f^*g^{\mathcal{D}})_p(u, v) = g^{\mathcal{D}}_{f(p)}(\mathrm{d}f_p(u), \mathrm{d}f_p(v))
\end{equation}
for any tangent vectors $u, v \in T_p\mathcal{Z}$. In Eq.~\ref{eq:pullback}, $g^{\mathcal{D}}_{f(p)}$ means that we use ambient metric evaluated in the tangent space $T_{f(p)}\mathcal{M}$. The notation $\mathrm{d}f_p(u)$ means that the differential map of $f$ at $p\in \mathcal{Z}$ maps the vector $u \in T_p\mathcal{Z}$ to $f(u) \in T_{f(p)}\mathcal{M}$. We will denote the pullback metric as $g^{f}=f^*g^\mathcal{D}$ for shorter notation and let the domain of it be implicit from the definition of $f$.
\end{definition}

The result of this important construction is that:
\begin{itemize}
    \item it allows us to construct a Riemannian metric on $\mathcal{M}$ that respect the intrinsic properties of the Riemannian manifold $(\mathcal{M},g)$. In this setting, the pullback metric $g^f$ represents some intrinsic $g$ in the coordinates defined by $\mathcal{Z}$ and $f$.
    \item we can make all the measurements from the latent space $\mathcal{Z}$ using $g^f$ as this construction makes the Riemannian manifolds $(\mathcal{Z},g^f)$ and $(\mathcal{M},g)$ the same, from a geometric perspective. Thus, we can concentrate our attention on $(\mathcal{Z},g^f)$, while being consistent with $(\mathcal{M},g)$ without worrying about $g$.
\end{itemize}
The pullback metric merely measures the length of a latent curve by first decoding the curve and measuring its length according to the data space metric. This is a quite `meaningful' metric in line with the requirements of Problem~\ref{problem}.


In the framework of pullback metrics defined by different decoders that span the same manifold, the generator transformations that comprise the space of indeterminacy transformations are isometries that preserve angles, length of curves, surface areas, and volumes on the manifold. %We formalize this as follows.
%
\begin{theorem}\label{thrm:gentransforms_isometries}
    Let $\theta_a=\left(f_a, P_{Z_a}\right)$ and $\theta_b=\left(f_b, P_{Z_b}\right)$ with $P_{\theta_a}=P_{\theta_b}$and let $(\mathcal{Z}_a, g^{f_a})$ and $(\mathcal{Z}_b,g^{f_b})$ be the associated Riemannian manifolds, then the generator transform is an isometry and it holds that:
    \begin{equation}
    \label{eq:isometry}
        \left(A_{a, b}\right)^* g^{f_b}=g^{f_a}
    \end{equation}
    Thus, making $(\mathcal{Z}_a,g^{f_a})$ and $(\mathcal{Z}_b,g^{f_b})$ isometric. This makes Riemannian geometric properties such as lengths of curves, angles, volumes, areas, Ricci curvature tensor, geodesics, parallel transport, and the exponential map identifiable. 
\end{theorem}

\begin{proofsketch}
    First, we show that Eq.~\ref{eq:isometry} is satisfied, establishing the isometry property, then we refer to results in Riemannian geometry to establish the isometric invariance of a particular property. To obtain identifiability, each property is expressed in terms of a task from Section~\ref{sec:background} and Definition~\ref{def:task_identifiability} is shown to be satisfied.
\end{proofsketch}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pics/manifold2.pdf}
    %\vspace{-2mm}
    \caption{Pullback metrics assign a local inner product to latent spaces corresponding to measuring \emph{along} the manifold spanned by the decoder. In the left panels, the white ellipsis corresponds to unit circles under the pullback metric, corresponding to a local Euclidean metric in the observation space $\mathcal{D}$. Geodesics (yellow curves) minimize length according to the pullback metric, corresponding to minimizing the length of the decoded curve along the manifold.}
    \label{fig:manifold2}
\end{figure}
To solve Problem~\ref{problem} we use the \emph{geodesic distance} from Definition~\ref{def:geadeisc_distance} and show that it is identifiable in Theorem~\ref{thrm:identifiable_dist_func}.
\begin{definition}
    \label{def:geadeisc_distance}
    Let $(\mathcal{Z}_a,g^{f_a})$ be a Riemannian manifold, then for $\mathbf{z}_1,\mathbf{z}_2 \in \mathcal{Z}$ we define the geodesic distance function
    \begin{equation}
        \label{eq:geodesic_dist_def}
        d_{g^{f_a}}(\mathbf{z}_1,\mathbf{z}_2)= \inf_{\gamma} \int_0^T | \gamma^{\prime}(t) |_{g^{f_a}} dt
    \end{equation}
    where $\gamma:[0,T]\rightarrow \mathcal{Z}_a$ is a latent curve from $\mathbf{z}_1$ to $\mathbf{z}_2$. %\looseness=-1
\end{definition}
Figure~\ref{fig:manifold2} illustrates how the geodesic distance measures the length of the shortest curve (geodesic) under the pullback metric. This is equivalent to finding the shortest curve \emph{along} the manifold spanned by a decoder.

\begin{theorem}
    \label{thrm:identifiable_dist_func}
     Let $\theta_a=\left(f_a, P_{Z_a}\right)$ and $\theta_b=\left(f_b, P_{Z_b}\right)$ with $P_{\theta_a}=P_{\theta_b}$and let $A_{a, b}$ be the generator transform between the parameters. Furthermore, let $(\mathcal{Z}_a, g^{f_a})$ and $(\mathcal{Z}_b,g^{f_b})$ be the associated Riemannian manifolds. Then, the geodesic distance between $\mathbf{z}_1$ and $\mathbf{z}_2$ is identifiable and
    \begin{equation}
        \label{eq:geodesic_dist}
        d_{g^{f_a}}(\mathbf{z}_1,\mathbf{z}_2)= d_{g^{f_b}}(A_{a, b}(\mathbf{z}_1),A_{a, b}(\mathbf{z}_2))
    \end{equation}
for some $\mathbf{z}_1,\mathbf{z}_2 \in \mathcal{Z}_a$ be two points in the latent space that correspond to some $\mathbf{x}_1,\mathbf{x}_2 \in \mathcal{M}$ on the manifold.
\end{theorem}

\begin{proofsketch}
First formulate the task of computing the geodesic distance in terms of Definition~\ref{def:task_identifiability}, then check the definition for the selection function by plugging in and for the task output by leveraging Theorem~\ref{thrm:gentransforms_isometries}.
\end{proofsketch}

\subsection{Identifiability of Euclidean distances}

Theorem~\ref{thrm:identifiable_dist_func} represents our solution to Problem~\ref{problem} and came as a result of treating the question of identifiability from the geometric perspective. While Section~\ref{sec:related_work} outlines alternative approaches to the same problem, in the following we show that these must necessarily impose implicit constraint of flatness on the models.
%
\begin{proposition}
\label{prop:flat_models}
Let $\mathcal{Z}=\mathbb{R}^n$ be the latent space and $(\mathcal{Z},g^f)$ the associated Riemannian manifold. Furthermore, let $g^{E_p}$ denote a metric tensor that is proportional to the Euclidean metric tensor $g^E$, then:
\begin{enumerate}
[label=\textbf{P\arabic*},ref=P\arabic*]
    \item If we choose $g^{E_p}$ as our metric in the latent space, that is equivalent to assuming $g^f=g^{E_p}$, then $(\mathcal{Z}, g^{E_p})$ can only be identifiable if the associated $f\in \mathcal{F}$ parametrize a flat manifold $\mathcal{M}$ within the ambient space $\mathcal{D}$, i.e.\@ $\mathcal{M}$ has zero curvature.\label{prop:1}
    \item If we choose the Euclidean distance to be identifiable, equivalent to assuming $g^f=g^{E}$, then \ref{prop:1} applies and the associated $f\in \mathcal{F}$ are such that the generator transforms are isometries of $\mathbb{R}^n$, i.e.\@ translations, rotations, or reflections. \label{prop:2}
\end{enumerate}
\end{proposition}
\begin{proof}
   Distance measures proportional to the Euclidean distance measure are characterized by the pullback metric $g^f$ being constant everywhere. If $g^f$ is constant everywhere, its directional derivatives vanish and the curvature is zero. The second point follows from Theorem~\ref{thrm:gentransforms_isometries} and standard linear algebra, e.g.\@ \citet{friedberg2014linear}.
\end{proof}


\subsection{Main takeaways}
Our results can be summarized by the following takeaways:
\begin{itemize}
    \item The Riemannian metric space of a deep latent variable model is identifiable (Theorem~\ref{thrm:identifiable_dist_func}) making distance measurements in the latent space identifiable. This solves Problem~\ref{problem}.
    \item Riemannian geometry properties of the learned manifold are identifiable (Theorem~\ref{thrm:gentransforms_isometries}). Examples beyond distances include angles, volumes, and more. Jointly these provide a rich language for probabilistic data analysis in the latent space.
    \item Using Euclidean distances in the latent space is either not identifiable or must come at the cost of imposing flatness constraints on the model (Proposition~\ref{prop:flat_models}).
    \item Any task whose identifiability boils down to the identifiability of the Riemannian metric is identifiable if the properties of the manifold allow. 

\end{itemize}

\textbf{To exemplify the last point}, consider the Fr{\'e}chet mean that generalizes the well-known mean to manifolds \citep{pennec2006intrinsic}. This is obtained by finding the point with minimal average squared distance to the data,
\begin{equation}
\label{eq:karcher}
  \mu_{\text{Fr{\' e}chet}} = \mathop {\text{argmin}} _{\mathbf{z}_1\in \mathcal{Z}}\sum _{i=1}^{N}d_{g^{f_a}}^{2}\left(\mathbf{z}_1,\mathbf{z}_{i}\right)
\end{equation}
As the mean is defined as an optimization problem, there might exist multiple means, which violates the usual notion of identifiability. First, it is worth noting that the solution set is identifiable, although, in practice, one usually only computes a single optimum. In some situations, this singleton can, however, be identified based on properties of the manifold $\mathcal{M}$. \citet{karcher1977riemannian} and \citet{kendall1990probability} provide uniqueness conditions that connect the radius of the smallest geodesic ball containing the data with the maximal curvature of the manifold. Importantly, these are, principally, testable conditions such that it should be feasible to computationally test if a computed mean is identifiable. \emph{Identifiability of some statistical quantities is, thus, within reach.}

%thus not identifiable due to the properties of the manifold and the distribution on it. For instance, there exists a unique Frechet mean if the manifold has constant negative curvature \citet{pmlr-v119-lou20a}. \stas{There are both better references and more to say here (distribution on the manifold plays a role even if curvature is not negative)}
 
%\begin{proofsketch}
%Noting the fact that the optimal curves themselves might not map to each other (as a geodesic is not necessarily unique), the resulting distance in Eq.\ref{eq:geodesic_dist} will still necessarily match because the generator transform based on assumption~\ref{ass:4} lets us go back and forth between charts. The second part requires the Riemannian manifold to be connected in which case a standard differential geometry result leads to the claim.
%\end{proofsketch}

%\begin{definition}\label{def:riemannian_metric}
%    Let $\mathcal{F}(\mathcal{Z})=\mathcal{M}\subset \mathbb{R}^m$ be a smooth manifold and $f\in \mathcal{F}$. Let $\gamma:[a,b]\rightarrow \mathcal{Z}$. Then the length of $\gamma$ is:
%    \begin{equation}
%    \begin{aligned}
%       \int_a^b |\frac{\partial}{\partial t} f(\gamma(t))|_{E} dt &= \left( \frac{\partial}{\partial t} f(\gamma(t))  %\right)^T \cdot \left( \frac{\partial}{\partial t} f(\gamma(t))  \right) \\
%        &= \int_a^b \gamma^{\prime}(t) \cdot J_f(\gamma(t))^T \cdot J_f(\gamma(t)) \cdot \gamma^{\prime T}(t) dt\\
%        &= \int_a^b | \gamma^{\prime}(t) |_g dt
%    \end{aligned}
%\end{equation}

%\end{definition}

%With the construction of Definition~\ref{def:riemannian_metric}, we will treat $(\mathcal{M},g)$ as a Riemannian manifold. Moreover, we will abuse the notation a little bit and use $\gamma : [a,b] \rightarrow \mathcal{M}$ and $\bar{\gamma} : [a,b] \rightarrow \mathcal{Z}$ interchangeably when considering the $g$-length of the curve. The reason for that is that $\gamma = f\circ \bar{\gamma}$ and the $g$-length of $\bar{\gamma}$ requires pulling back the Euclidean metric using $f \in \mathcal{F}$.


%The following result shows that the $g$-length of a curve is invariant under the generator transforms.






% To frame the result in the context of task identifiability, we can use Definition~\ref{def:task_identifiability} to show that the task of measuring distances in the latent space of an LVM is identifiable using the geodesic distance function.

% \begin{corollary}[Task Identifiability]
%     Let $\mathcal{M}=\mathcal{F}(\mathcal{Z})$ be a smooth manifold, $f\in \mathcal{Z}$ be a parametrization of the manifold and $g$ a Riemannian metric constructed from $f$. Let $p,q \in \mathcal{M}$  be two points on the manifold and construct $\boldsymbol{x} = \left\{p,q\right\}$ and $\boldsymbol{z} = \left\{z_1,z_2\right\}=\left\{f^{-1}(p),f^{-1}(q)\right\}$. Then, the task of measuring distances on the manifold defined as: 
%     $$t(\theta, \boldsymbol{x} ,\boldsymbol{z}) = d_g(p,q)$$ 
%     is identifiable. The corresponding selection function is $s(\theta, \boldsymbol{x})=f^{-1}(\boldsymbol{x})$ where $\boldsymbol{x} = \left\{x_1,x_2 \right\} \in \mathcal{M}\times\mathcal{M}$.
% \end{corollary}

% \textit{Proof:} The proof requires that the conditions of Definition~\ref{def:task_identifiability} are met.
% Take $\theta=\left(f_a, P_{z, a}\right)$ and $A\theta=\left(f_b, P_{z, b}\right)$ to be two sets of parameters of the model connected by an associated indeterminacy transformation.

% To check the selection function, take $s(\theta, \boldsymbol{x})=f_a^{-1}(\boldsymbol{x})$ and $s(A\theta, \boldsymbol{x})=f_b^{-1}(\boldsymbol{x})$ with $A\in \mathbf{A}(\mathcal{M})$ and some $\boldsymbol{x}\in \mathcal{M}\times\mathcal{M}$. Then,
% \begin{equation}
%     \begin{aligned}
%         s(A\theta, \boldsymbol{x}) &= f_b^{-1}(\boldsymbol{x}) = f_b^{-1}\circ f_a \circ f_a^{-1}(\boldsymbol{x}) = A(s(\theta, \boldsymbol{x}))
%     \end{aligned}
% \end{equation}
% as $A$ is almost everywhere equal to the generator transform. Building on the same observation, recall that the geodesic distance function is invariant under the generator transforms as shown in Theorem~\ref{thrm:identifiable_metric_space} and the result for the task function $t(\theta, \boldsymbol{x}, \boldsymbol{z})=t(A\theta, \boldsymbol{x}, A(\boldsymbol{z}))$ follows. ~\hfill$\square$

% Capitalizing, once again, on the fact that $d_g$ is invariant under the generator transforms, we can extend this reasoning to subsequent tasks inside our identifiable latent metric space. Tasks such as clustering, interpolation, or classification that rely on the geodesic distance function are, in principle, identifiable. The caveat is that the identifiability of some tasks might depend on the geodesic distances to be identifiable and the properties of the learned manifold itself. 

%For instance, given $N$ points on the manifold $\left\{x_i \right\}_{i=1}^N$ and a point $p$ also on the manifold, the Frechet variance from Definition~\ref{def:frechet_variance}
%\begin{equation}
%    \label{eq:frechet_variance_1}
%    \begin{aligned}
%        \Psi (p)=\sum _{i=1}^{N}d_g^{2}\left(p,x_{i}\right)
%    \end{aligned}
%\end{equation}

%is also invariant under the generator transforms and thus identifiable. Moreover, the set of Karcher means, points on the manifold minimizing the Frechet variance is also identifiable. However, the guarantee of such a set only having one element, the Fr√©chet mean, depends on the properties of the manifold and the data distribution on it.

