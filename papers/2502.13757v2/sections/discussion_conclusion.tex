\section{Weaknesses and open questions}
Our work provides strong identifiability guarantees for essential quantities such as pairwise distances in contemporary generative models. However, our approach is not problem-free and we highlight some pitfalls to be aware of.

\textbf{Observation metrics matter.~~}
Our identifiable geometric structure relies on the idea of locally bringing the observation space metric into the latent space. This has many benefits but also raises the question of choosing the observation space metric. This choice will directly impact the final latent distances. Did we then replace one difficult problem (identifiability) with another (choosing observation metric)? We argue that most data is equipped with units of measurement, which greatly simplifies the task of picking a suitable metric in the observation space. Furthermore, being explicit about how data is compared improves the transparency of the conducted data analysis. Finally, we emphasize that we are \emph{not} proposing to bring the Euclidean distance from observation space into the latent space, but only to do so \emph{infinitesimally}, i.e.\@ measuring \emph{along} the manifold.

\textbf{Identifiability comes at a (computational) cost.~~}
Euclidean distances are cheap to compute, unlike the geodesic distances we consider. For any geodesic distance, we must solve an iterative optimization problem. Fortunately, this is a locally convex problem, that only requires estimating a limited number of parameters, so the computation is feasible. Yet it remains significantly more costly than computing a Euclidean distance. We argue that when identifiability is important, e.g.\@ in scientific knowledge discovery and hypothesis generation, the additional computational resources are well-spent. Proposition~\ref{prop:flat_models} effectively tells us that we must choose between (cheap) flat decoders or (expensive) curved ones: we cannot get the best of both worlds.

\textbf{Compression matters.~~}
Our current work rests on the assumption that the latent space has a dimension that is less than or equal to the data space dimension, i.e.\@ $\text{dim}(\mathcal{Z}) \leq \text{dim}(\mathcal{D})$. This remains the standard setting for representation learning and generative models. However, if we, for the sake of argument, wanted to identify distances between the weights of overparametrized neural networks, then our strategies would not directly apply. Early work has begun to appear on understanding the geometric structure of overparametrized models \citep{roy:reparam:2024}, which suggests that perhaps our approach can be adapted.

\textbf{Injectivity remains an issue.~~}
Most of our assumptions are purely technical and easily satisfied in practice. The key exception is Assumption~\ref{ass:2} stating that the decoder $f$ must be \emph{injective}. The decoders of contemporary models such as diffusion models and (continuous) normalizing flows (including those trained with flow matching) are injective and the assumption is satisfied. However, general neural networks cannot be expected to be injective, such that variational autoencoders and similar models are not identifiable `out of the box'. We have demonstrated that (injective) $\mathcal{M}$-flow architectures can be used for such models, and empirically we observe that the geodesic distance increases robustness in non-injective models. This gives hope that theoretical statements can be made without the injectivity assumption. One such path forward may be to consider notions of \emph{weak injectivity} \citep{kivva2022identifiabilitydeepgenerativemodels}, which is less restrictive and more easily satisfied in practice.



\section{Conclusion}
In this paper, we show that latent distances, and similar quantities, can be statistically identified in a large class of generative models that includes contemporary models without imposing unrealistic assumptions. This is a significant improvement over existing work that tends to impose additional restrictions on either model or training data. Our results are significant when seeking to understand the mechanisms that drive the true data-generating process, e.g.\@ in scientific discovery, where reliability is essential.

Practically, it is important to note that our strategy requires no changes to how models are trained. Our constructions are entirely \emph{post hoc}, making them broadly applicable.

Our proof strategy relies on linking identifiability with Riemannian geometry; a link that does not appear to have formally been made elsewhere. This link paves a way forward as many tools readily exist for statistical computations on manifolds. For example, Riemannian counterparts to \emph{averages} \citep{karcher1977riemannian}, \emph{covariances} \citep{pennec2006intrinsic}, \emph{principal components} \citep{fletcher2004principal}, \emph{Kalman filters} \citep{hauberg:jmiv:2013}, and much more readily exist. In principle, it is also possible to devise computational tests to determine if these statistics are identifiable for a given model and dataset. This is, however, future work.




\section*{Impact statement}
%Authors are \textbf{required} to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in an unnumbered section at the end of the paper (co-located with Acknowledgements -- the two may appear in either order, but both must be before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice:

This paper improves our collective understanding of which aspects of a statistical model can be identified. As the theoretical understanding translates directly into algorithmic tools, our work has an impact potential beyond the theoretical questions. Being able to identify relationships between latent representations of data can aid in the process of scientific discovery as we increase the reliability of the data analysis. Our work can also help provide robustness to interpretations of neural networks and other statistical models, which may help in explainability efforts.



%``This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.''
