@article{deletang2023language,
  title={Language modeling is compression},
  author={Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
  journal={arXiv preprint arXiv:2309.10668},
  year={2023}
}
@article{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}
@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}
@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={55},
  pages={1--21},
  year={2019}
}
@article{he2021automl,
  title={AutoML: A survey of the state-of-the-art},
  author={He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  journal={Knowledge-based systems},
  volume={212},
  pages={106622},
  year={2021},
  publisher={Elsevier}
}
@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, T},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}
@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}
@article{wu2023many,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2310.08391},
  year={2023}
}
@article{ahn2024transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{emrullah2024self,
  title={From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers},
  author={Emrullah Ildiz, M and Huang, Yixiao and Li, Yingcong and Singh Rawat, Ankit and Oymak, Samet},
  journal={arXiv e-prints},
  pages={arXiv--2402},
  year={2024}
}
@article{li2024training,
  title={Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis},
  author={Li, Hongkang and Wang, Meng and Lu, Songtao and Cui, Xiaodong and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2402.15607},
  year={2024}
}
@article{nichani2024transformers,
  title={How Transformers Learn Causal Structure with Gradient Descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}




@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}
@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{russo2016controlling,
  title={Controlling bias in adaptive data analysis using information theory},
  author={Russo, Daniel and Zou, James},
  booktitle={Artificial Intelligence and Statistics},
  pages={1232--1240},
  year={2016},
  organization={PMLR}
}
@article{feldman2018generalization,
  title={Generalization bounds for uniformly stable algorithms},
  author={Feldman, Vitaly and Vondrak, Jan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}
@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{saunshi2020mathematical,
  title={A mathematical exploration of why language models help solve downstream tasks},
  author={Saunshi, Nikunj and Malladi, Sadhika and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2010.03648},
  year={2020}
}
@article{wei2021pretrained,
  title={Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning},
  author={Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16158--16170},
  year={2021}
}
@inproceedings{saunshi2019theoretical,
  title={A theoretical analysis of contrastive unsupervised representation learning},
  author={Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
  booktitle={International Conference on Machine Learning},
  pages={5628--5637},
  year={2019},
  organization={PMLR}
}
@article{kwapien1991semimartingale,
  title={Semimartingale integrals via decoupling inequalities and tangent processes},
  author={Kwapien, S and Woyczynski, WA},
  journal={Probab. Math. Statist},
  volume={12},
  number={2},
  pages={165--200},
  year={1991}
}
@article{de1999general,
  title={General Decoupling Inequalities for Tangent Sequences},
  author={de la Pe{\~n}a, V{\'\i}ctor H and Gin{\'e}, Evarist and de la Pe{\~n}a, V{\'\i}ctor H and Gin{\'e}, Evarist},
  journal={Decoupling: From Dependence to Independence},
  pages={291--324},
  year={1999},
  publisher={Springer}
}
@article{tripuraneni2020theory,
  title={On the theory of transfer learning: The importance of task diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael and Jin, Chi},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7852--7862},
  year={2020}
}

@article{denevi2018incremental,
  title={Incremental learning-to-learn with statistical guarantees},
  author={Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:1803.08089},
  year={2018}
}
@article{ji2020convergence,
  title={Convergence of meta-learning with task-specific adaptation over partial parameters},
  author={Ji, Kaiyi and Lee, Jason D and Liang, Yingbin and Poor, H Vincent},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11490--11500},
  year={2020}
}
@article{chua2021fine,
  title={How fine-tuning allows for effective meta-learning},
  author={Chua, Kurtland and Lei, Qi and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8871--8884},
  year={2021}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{naik1992meta,
  title={Meta-neural networks that learn by learning},
  author={Naik, Devang K and Mammone, Richard J},
  booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
  volume={1},
  pages={437--442},
  year={1992},
  organization={IEEE}
}
@phdthesis{schmidhuber1987evolutionary,
  title={Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author={Schmidhuber, J{\"u}rgen},
  year={1987},
  school={Technische Universit{\"a}t M{\"u}nchen}
}
@article{thrun2012learning,
  title={Learning to learn: Springer Science \& Business Media},
  author={Thrun, S and Pratt, L},
  year={2012}
}
@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}
@inproceedings{lei2020fine,
  title={Fine-grained analysis of stability and generalization for stochastic gradient descent},
  author={Lei, Yunwen and Ying, Yiming},
  booktitle={International Conference on Machine Learning},
  pages={5809--5819},
  year={2020},
  organization={PMLR}
}
@inproceedings{zhang2022stability,
  title={Stability of sgd: Tightness analysis and improved bounds},
  author={Zhang, Yikai and Zhang, Wenjia and Bald, Sammy and Pingali, Vamsi and Chen, Chao and Goswami, Mayank},
  booktitle={Uncertainty in artificial intelligence},
  pages={2364--2373},
  year={2022},
  organization={PMLR}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{dziugaite2018data,
	title={Data-dependent PAC-Bayes priors via differential privacy},
	author={Dziugaite, Gintare Karolina and Roy, Daniel M},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}
@article{wang2022two,
	title={Two facets of sde under an information-theoretic lens: Generalization of sgd via training trajectories and via terminal states},
	author={Wang, Ziqiao and Mao, Yongyi},
	journal={arXiv preprint arXiv:2211.10691},
	year={2022}
}
@article{elisseeff2005stability,
	title={Stability of Randomized Learning Algorithms.},
	author={Elisseeff, Andre and Evgeniou, Theodoros and Pontil, Massimiliano and Kaelbing, Leslie Pack},
	journal={Journal of Machine Learning Research},
	volume={6},
	number={1},
	year={2005}
}
@article{lieber2021jurassic,
	title={Jurassic-1: Technical details and evaluation},
	author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
	journal={White Paper. AI21 Labs},
	volume={1},
	year={2021}
}
@article{black2022gpt,
	title={Gpt-neox-20b: An open-source autoregressive language model},
	author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
	journal={arXiv preprint arXiv:2204.06745},
	year={2022}
}
@article{rae2021scaling,
	title={Scaling language models: Methods, analysis \& insights from training gopher},
	author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
	journal={arXiv preprint arXiv:2112.11446},
	year={2021}
}
@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}
@inproceedings{mou2018generalization,
	title={Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints},
	author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
	booktitle={Conference on Learning Theory},
	pages={605--638},
	year={2018},
	organization={PMLR}
}
@article{paulin2015concentration,
  title={Concentration inequalities for Markov chains by Marton couplings and spectral methods},
  author={Paulin, Daniel},
  journal={Electron. J. Probab},
  volume={20},
  number={79},
  pages={1--32},
  year={2015},
  publisher={Citeseer}
}
@article{luo2022generalization,
	title={Generalization Bounds for Gradient Methods via Discrete and Continuous Prior},
	author={Luo, Xuanyuan and Luo, Bei and Li, Jian},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={10600--10614},
	year={2022}
}
@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}
@book{levin2017markov,
	title={Markov chains and mixing times},
	author={Levin, David A and Peres, Yuval},
	volume={107},
	year={2017},
	publisher={American Mathematical Soc.}
}
@inproceedings{belghazi2018mutual,
	title={Mutual information neural estimation},
	author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
	booktitle={International conference on machine learning},
	pages={531--540},
	year={2018},
	organization={PMLR}
}
@article{agarwal2020flambe,
	title={Flambe: Structural complexity and representation learning of low rank mdps},
	author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={20095--20107},
	year={2020}
}
@article{li2019generalization,
	title={On generalization error bounds of noisy gradient methods for non-convex learning},
	author={Li, Jian and Luo, Xuanyuan and Qiao, Mingda},
	journal={arXiv preprint arXiv:1902.00621},
	year={2019}
}
@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

% Add
@inproceedings{guan2022fast,
  title={Fast-rate PAC-Bayesian generalization bounds for meta-learning},
  author={Guan, Jiechao and Lu, Zhiwu},
  booktitle={International Conference on Machine Learning},
  pages={7930--7948},
  year={2022},
  organization={PMLR}
}

@article{tolstikhin2013pac,
  title={PAC-Bayes-empirical-Bernstein inequality},
  author={Tolstikhin, Ilya O and Seldin, Yevgeny},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

% edit
@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@inproceedings{oswald2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@inproceedings{dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2023}
}

@article{zhang2023trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

@article{ataee2023transformers,
  title={Transformers as Support Vector Machines},
  author={Ataee Tarzanagh, Davoud and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv e-prints},
  pages={arXiv--2308},
  year={2023}
}

@article{han2023context,
  title={In-Context Learning of Large Language Models Explained as Kernel Regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  journal={arXiv preprint arXiv:2305.12766},
  year={2023}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}


@article{wies2023learnability,
  title={The learnability of in-context learning},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2303.07895},
  year={2023}
}

@article{jiang2023latent,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv preprint arXiv:2304.09960},
  year={2023}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@article{russo2019much,
  title={How much does your data exploration overfit? Controlling bias via information usage},
  author={Russo, Daniel and Zou, James},
  journal={IEEE Transactions on Information Theory},
  volume={66},
  number={1},
  pages={302--323},
  year={2019},
  publisher={IEEE}
}

@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{mcallester1998some,
  title={Some pac-bayesian theorems},
  author={McAllester, David A},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={230--234},
  year={1998}
}

@article{catoni2007pac,
  title={PAC-Bayesian supervised classification: the thermodynamics of statistical learning},
  author={Catoni, Olivier},
  journal={arXiv preprint arXiv:0712.0248},
  year={2007}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{zhou2018non,
  title={Non-vacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach},
  author={Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P and Orbanz, Peter},
  journal={arXiv preprint arXiv:1804.05862},
  year={2018}
}


@article{parrado2012pac,
  title={PAC-Bayes bounds with data dependent priors},
  author={Parrado-Hern{\'a}ndez, Emilio and Ambroladze, Amiran and Shawe-Taylor, John and Sun, Shiliang},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={3507--3531},
  year={2012},
  publisher={JMLR. org}
}
@article{negrea2019information,
  title={Information-theoretic generalization bounds for SGLD via data-dependent estimates},
  author={Negrea, Jeffrey and Haghifam, Mahdi and Dziugaite, Gintare Karolina and Khisti, Ashish and Roy, Daniel M},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{zhang2023and,
  title={What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization},
  author={Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.19420},
  year={2023}
}

@article{ren2023context,
	title={In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern},
	author={Ren, Ruifeng and Liu, Yong},
	journal={arXiv preprint arXiv:2310.13220},
	year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18878--18891},
  year={2022}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021}
}
@article{wang2023large,
  title={Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning},
  author={Wang, Xinyi and Zhu, Wanrong and Wang, William Yang},
  journal={arXiv preprint arXiv:2301.11916},
  pages={3},
  year={2023}
}