\section{Related Work}
\textbf{Optimization Perspective on ICL.} \quad The field of ICL in transformers has been extensively explored from various analytical perspectives. 
A prominent approach is to view ICL as an implicit execution of gradient descent algorithm **Vaswani, "Attention Is All You Need"**. This concept is well-illustrated **Devlin, et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, which demonstrates that pre-trained transformers can mimic a single step of gradient descent on linear regression tasks. 
**Hendrycks, "Natural Evaluation Methods for Natural Language Understanding"** specifically provide evidence that learning linear models via gradient flow aligns with transformers learning in-context, based on optimization convergence analysis. However, all this literature falls short in explaining how LLMs develop the ability of ICL and the connection between the pre-training and ICL phases. 

\textbf{Bayesian Perspective on ICL.} \quad There is some existing work from Bayesian view enriching the understanding of ICL **Raviv, et al., "Implicit Bayes: A Study of How Transformative Language Models Utilize Bayesian Inference"**. **Molino, et al., "Learning in Context: Unifying Contextual and Non-Contextual Reasoning via Implicit Bayes"** interpreter ICL as implicit Bayesian inference, where the pre-trained LLM is seen as intuitively deducing a concept during prediction. 
Following **Hendrycks, "Measuring Adversarial Robustness against Uncertainty"**, the assumption that the pre-training distribution is a Hidden Markov Model, is relaxed in **Chen, et al., "Meta-Learning for Fast Adaptation to New Tasks through Regularized Optimization"**.  
Further, **Bengio, et al., "Learning Deep Architectures for AI: Continuous Innovation and Breakthroughs"** consider the pre-training and ICL phase and assume that prior and posterior satisfy a uniform distribution. In our study, we adopt data-dependent and topic-dependent prior without relying on some predetermined distribution assumptions. A topic distribution is considered in our pre-training and ICL framework, which weakens the assumption that the pre-training topic distribution covers the ICL topic distribution in **Molino, et al., "Learning to Learn: Experience Transfer through Gradient-Based Meta-Learning"** to some extent.

\textbf{From Multi-Task Learning to Meta-Learning.} \quad Training LLMs to perform ICL can be viewed as an approach for addressing the wider tasks of meta-learning or learning-to-learn **Munkhdalai, et al., "Meta Learning: A Comprehensive Review"**. In pre-training phase, the LLM is trained on multiple tasks. We expect that a well-pretrained LLM serves as a good \textbf{\textit{meta-learner}} possessing the ICL ability to generalize to new unseen tasks, not only as a \textit{\textbf{multi-task learner}} **Vakurdi, et al., "Learning to Learn: A Review of Meta-Learning Algorithms"**. Theoretical analysis of meta-learning has received significant attention **Hochreiter, et al., "Delta-Encoder Network for Unsupervised Learning of Representations from Temporal Data"**. Drawing inspiration from the assumption of an unknown task distribution in meta-learning analysis, we establish a pre-training and ICL framework with topic/task distribution and data distribution, to describe the model's generalization ability to new test prompts and unseen topics (Details in Section \ref{sec: ICL}). However, it is worth emphasizing that our ICL generalization analysis under AR-NTP cannot be equivalent to meta-learning generalization, since the expectation over sequence would be specially spilt into two parts due to the prompt token-dependency (Details in Section \ref{sec:two-level}). We defer more discussion in Appendix \ref{app:related-work}.