\section{Overview of Two-Level Expectation}\label{sec:overview-of-two-level}
% \paragraph{\add{An Example of Pre-training and ICL Framework.}}
% To illustrate the process introduced in Section \ref{sec: ICL} more clearly, let's use a practical example \addfoot{\footnote{\addfoot{As suggested by reviewers, we have moved some analysis from Section \ref{sec: ICL} in the earlier version to this section.}}}: Imagine the realm of global knowledge as a vast library filled with diverse topics. 

% In pre-training phase, five topics ($K=5$) are randomly sampled from the library constructing $\mathcal{W}_\text{pre}$. For each topic in $\mathcal{W}_\text{pre}$, it is assumed that there are ten sequences ($N=10$, here for simplicity, the number of sequences is the same across different topics). For example a sequence ‘good flavor! they were fresh and delicious!’ under topic Amazon Fine Food Reviews (1), there are tokens $\{x_1=\text{‘good’}, x_2=\text{‘flavor’},x_3=\text{‘they’},x_4=\text{‘were’},x_5=\text{‘fresh’},x_6=\text{‘and’},x_7=\text{‘delicious’}\}$, the LLM makes auto-regressive predictions for token $\{x_2=\text{‘flavor’}\}$ based on $\{x_1=\text{‘good’}\}$, token $\{x_3=\text{‘they’}\}$ based on $\{x_1=\text{‘good’},x_2=\text{‘flavor’}\}$, and so on. Then the LLM is pre-trained employing AR-NTP loss. 

% In ICL phase following pre-training, to test whether the pre-trained LLM can perform well on various seen or unseen topics, we also sample several topics rather that just one topic from the library. Across random sampling, $\mathcal{W}_\text{ICL}$ with both seen and unseen topics is constructed. The user provides zero or few demonstrations concatenated in a prompt under a ICL topic, expecting a satisfactory next token based on the prompt. Therefore, similarly to the pre-training phase, for example one sequence ‘Albert Einstein is best known for developing the theory of relativity’ under a ICL topic, the pre-trained LLM outputs the subsequent tokens, accomplishing tasks like text generation. 

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{overview_v3.pdf}
	\caption{\textbf{Overview of Two-Level Expectation.} \textbf{From a horizontal perspective:} \textbf{The first box (from top to bottom):} according to Equation \ref{app-eq-L-decompose}, the population loss is decomposed into four parts. We ultimately obtain the upper bound of the population loss by separately defining the upper bound for each part. Combining Part $\text{\uppercase\expandafter{\romannumeral1}}$, Part $\text{\uppercase\expandafter{\romannumeral2}}$ and Part $\text{\uppercase\expandafter{\romannumeral3}}$, we obtain  Theorem \ref{pre-gen-data-dependent}; further combining with Part $\text{\uppercase\expandafter{\romannumeral4}}$, we obtain Theorem \ref{ICL-gen-topic-dependent}. \textbf{The second box:} comparing $L(\theta)$ and $L(\theta,\mathcal{W}_{\text{pre}})$, we aim to describe the second-level expectation defined over topic. \textbf{The third box:} comparing $L(\theta,w_k)$ and $L^\prime_{x}(\theta,w_k)$, we aim to describe the complete first-level expectation defined over sequence. \textbf{The fourth box:} comparing $L^\prime_{x}(\theta,w_k)$ and $L_{x}(\theta,w_k)$, $L^\prime_{E^{k,n}}(\theta,w_k)$ is a partial first-level expectation over token $x^{k,n}_{t+1}$ conditioned on $E^{k,n}_t$. \textbf{The fifth box:} Negative logarithmic likelihood loss, the optimization objective for a token. \textbf{From a vertical perspective}, the formulas described in the four columns can be found in Equation \ref{eq-L-2}, \ref{eq-L-W}, \ref{eq-L-E-prime} and \ref{eq-L-E-complete}, respectively. \textbf{The first column:} the chain of $L(\theta) \rightarrow L(\theta,w_k) \rightarrow L^\prime_{x}(\theta,w_k) \rightarrow L_{x}(\theta,w_k)$. \textbf{The second column:} the chain of $L(\theta,\mathcal{W}_{\text{pre}}) \rightarrow L(\theta,w_k) \rightarrow L^\prime_{x}(\theta,w_k) \rightarrow L_{x}(\theta,w_k)$. \textbf{The third column:} the chain of $L^\prime(\theta,\mathcal{W}_{\text{pre}}) \rightarrow L^\prime_{E^{k}}(\theta,w_k) \rightarrow L^\prime_{x}(\theta,w_k) \rightarrow L_{x}(\theta,w_k)$. \textbf{The fourth column:} the chain of $L_E(\theta,\mathcal{W}_{\text{pre}}) \rightarrow L_{E^k}(\theta,w_k) \rightarrow L_{x}(\theta,w_k)$.} 
	\label{fig:symbol}
\end{figure*}

\paragraph{Decomposition of Population Loss.} No matter the inner or outer expectation, the expected loss $L(\theta)$ is incalculable since the data distribution $\mathbb{P}_{w_k}$ and topic distribution $\mathbb{P}_{\mathcal{W}}$ are both unknown (as introduced in Section \ref{sec:optimization-objective}, finite sequences and topics are utilized to optimize the empirical loss in practical). ICL ability can be measured by population loss, which can be decomposed by simply adding and subtracting three terms $L_E(\theta,\mathcal{W}_{\text{pre}})$, $L^\prime(\theta,\mathcal{W}_{\text{pre}})$ and $L(\theta,\mathcal{W}_{\text{pre}})$ in Equation \ref{app-eq-L-decompose}. A good ICL learner means a small population loss, i.e. a small value in all four parts. The overview of two-level expectation is shown in Figure \ref{fig:symbol} and the table of notations is shown in Table \ref{tab:notation-fig}.
\begin{multline}\label{app-eq-L-decompose}
	L(\theta)=\underbrace{L(\theta) - L(\theta,\mathcal{W}_{\text{pre}})}_{\text{Part \uppercase\expandafter{\romannumeral4}: }\text{gen}_{\text{topic}}}+\underbrace{L(\theta,\mathcal{W}_{\text{pre}})-L^\prime(\theta,\mathcal{W}_{\text{pre}})}_{\text{Part \uppercase\expandafter{\romannumeral3}: }\text{gen}_{\text{seq-2}}}\\
    +\underbrace{L^\prime(\theta,\mathcal{W}_{\text{pre}})-L_E(\theta,\mathcal{W}_{\text{pre}})}_{\text{Part \uppercase\expandafter{\romannumeral2}: }\text{gen}_{\text{seq-1}}}+\underbrace{L_E(\theta,\mathcal{W}_{\text{pre}})}_{\text{Part \uppercase\expandafter{\romannumeral1}: empirical loss}}
\end{multline}

\textbf{Part $\text{\uppercase\expandafter{\romannumeral1}: empirical loss}$.} For Part $\text{\uppercase\expandafter{\romannumeral1}}$, the training of the LLM takes into account $K$ topics and $N$ sequences per topic. In this setting, finite topics and finite sequences could affect the performance of model so that the training loss is called as empirical loss (optimization objective). For a detailed explanation of empirical loss, the same as Equation \ref{eq-L-E},
\begin{align}
	L_E(\theta, \mathcal{W}_{\text{pre}})&=\frac{1}{K}\sum_{k=1}^K L_{E^k}(\theta,w_k), \nonumber \\
	L_{E^k}(\theta, w_k)&=\frac{1}{N}\sum_{n=1}^N L_{E^{k,n}}(\theta,w_k), \label{eq-L-E-complete}\\
	L_{E^{k,n}}(\theta,w_k)&=\frac{1}{T}\sum_{t=1}^T L_x(\theta, w_k), \nonumber\\ 
    L_x(\theta, w_k) &= \log \frac{\mathbb{P}(x^{k,n}_{t+1}\mid E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}\mid E^{k,n}_t, w_k)}.\nonumber
\end{align}

\textbf{Part $\text{\uppercase\expandafter{\romannumeral2}}: \text{gen}_\text{seq-1}$.} Through Part $\text{\uppercase\expandafter{\romannumeral1}}$, we have obtained the empirical loss with finite sequences and finite topics. To address the first-level expectation, it's necessary to evaluate the expected loss over sequence, that is, utilizing an infinite number of sequences for each pre-training topic. Given that the sequential dependence in token generation or prediction, where each subsequent token relies on the preceding tokens, our approach involves initially calculating the expectation of token $x^{k,n}_{t+1}$ conditioned on $E^{k,n}_{t}$ in this Part $\text{\uppercase\expandafter{\romannumeral2}}$. It's a partial generalization error for the first-level expected loss. This is followed by taking expectation over $E^{k,n}_{t}$ in the Part $\text{\uppercase\expandafter{\romannumeral3}}$, thereby achieving the comprehensive first-level expectation over sequence $E^{k,n}$. 

According to the definition of KL divergence, the partial first-level expectation over sequences $\mathbb{E}_{x^{k,n}_{t+1} \sim \mathbb{P}(\cdot\mid E^{k,n}_t, w_k)}\left[L_{x}(\theta,w_k)\right]$ can be related to $\KL \left(\mathbb{P}(\cdot\mid E^{k,n}_t, w_k)\parallel \mathbb{P}_\theta(\cdot\mid E^{k,n}_t, w_k)\right)$, \textit{i.e.}
\begin{align}
	\mathbb{E}_{x^{k,n}_{t+1} \sim \mathbb{P}(\cdot\mid E^{k,n}_t, w_k)}\left[L_{x}(\theta,w_k)\right]&=\mathbb{E}_{x^{k,n}_{t+1} \sim \mathbb{P}(\cdot\mid E^{k,n}_t, w_k)}\left[\log\frac{\mathbb{P}(x^{k,n}_{t+1}\mid E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}\mid E^{k,n}_t, w_k)} \right]\nonumber \\
	&=\KL\left(\mathbb{P}(\cdot\mid E^{k,n}_t, w_k)\parallel \mathbb{P}_\theta(\cdot\mid E^{k,n}_t, w_k)\right)\nonumber  \\
    &\triangleq L^\prime_{x}(\theta,w_k). \label{eq-E-and-KL}
\end{align}

Then, taking average of all tokens in a sequence, $N$ sequences per topic and $K$ topics and combining with Equation \ref{eq-E-and-KL}, we define a partial first-level expected loss $L^\prime(\theta,\mathcal{W}_{\text{pre}})$ as
\begin{align}
	L^\prime(\theta, \mathcal{W}_{\text{pre}})&=\frac{1}{K}\sum_{k=1}^K L^\prime_{E^k}(\theta,w_k), \nonumber \\
	L^\prime_{E^k}(\theta, w_k)&=\frac{1}{N}\sum_{n=1}^N L^\prime_{E^{k,n}}(\theta,w_k), \label{eq-L-E-prime}\\
	L^\prime_{E^{k,n}}(\theta,w_k)&=\frac{1}{T}\sum_{t=1}^T L^\prime_{x}(\theta,w_k), \nonumber \\
    L^\prime_{x}(\theta,w_k)&= \KL\left(\mathbb{P}(\cdot\mid E^{k,n}_t, w_k)\parallel \mathbb{P}_\theta(\cdot\mid E^{k,n}_t, w_k)\right) \nonumber.
\end{align}
Finally, a partial generalization error for the first-level expected loss can be described as
\begin{equation}\label{eq-gen-pre-1}
	\text{gen}_{\text{seq-1}}=L^\prime(\theta,\mathcal{W}_{\text{pre}})-L_E(\theta,\mathcal{W}_{\text{pre}}).
\end{equation}

\begin{table}[t]
	\centering
	\caption{Table of Notations in Figure \ref{fig:symbol}.}
	\begin{tabularx}{\textwidth}{p{1.6cm}p{1.9cm}X}
		\specialrule{1pt}{0pt}{0pt}
		\toprule
		  & \textbf{Notation}  &  \textbf{Description} \\	
		\midrule
		$L_E(\theta, \mathcal{W}_{\text{pre}})$ & $L_E(\theta, \mathcal{W}_{\text{pre}})$ & Averaging $L_{E^k}(\theta, w_k)$ over $K$ topics, see in Equation \ref{eq-L-E-complete}. \\
		 & $L_{E^k}(\theta, w_k)$ & Averaging $L_{E^{k,n}}(\theta, w_k)$ over $N$ sequences per topic. \\
		& $L_{E^{k,n}}(\theta, w_k)$& Averaging $L_{x}(\theta, w_k)$ over sequence length.\\
        & $L_{x}(\theta, w_k)$& Negative logarithmic likelihood loss $\log \frac{\mathbb{P}(x^{k,n}_{t+1}\mid E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}\mid E^{k,n}_t, w_k)}$.\\
		
		\midrule
		$L^\prime(\theta, \mathcal{W}_{\text{pre}})$ & $L^\prime(\theta, \mathcal{W}_{\text{pre}})$ & Averaging $L^\prime_{E^k}(\theta, w_k)$ over $K$ topics, see in Equation \ref{eq-L-E-prime}. \\
		& $L^\prime_{E^k}(\theta, w_k)$ & Averaging $L^\prime_{E^{k,n}}(\theta, w_k)$ over $N$ sequences per topic.   \\
		& $L^\prime_{E^{k,n}}(\theta, w_k)$& Averaging $L^\prime_{x}(\theta, w_k)$ over sequence length.\\	
        & $L^\prime_{x}(\theta, w_k)$ & \textbf{Taking the partial first-level expectation} over token $x^{k,n}_{t+1} \sim \mathbb{P}(\cdot\mid E^{k,n}_t, w_k)$.\\
		
		\midrule
		$L(\theta, \mathcal{W}_{\text{pre}})$ & $L(\theta, \mathcal{W}_{\text{pre}})$ & Averaging $L(\theta, w_k)$ over $K$ topics, see in Equation \ref{eq-L-W}. \\
		& $L(\theta, w_k)$ & \textbf{Taking the complete first-level expectation} over prefix sequence $E^{k,n}_t$ and token $x^{k,n}_{t+1} \sim \mathbb{P}(\cdot\mid E^{k,n}_t, w_k)$.\\
		& $L^\prime_{x}(\theta, w_k)$ & The partial first-level expectation over token $x^{k,n}_{t+1}$. \\
  
		\midrule
		$L(\theta)$ & $L(\theta)$ & \textbf{Taking the second-level expectation} over topic $w_k$ of $L(\theta, w_k)$, see in Equation \ref{eq-L-2}.\\
		& $L(\theta, w_k)$ &  The first-level expectation over sequence $E^{k,n}$.\\
		\specialrule{1pt}{0pt}{0pt}
		\bottomrule
	\end{tabularx}
	\label{tab:notation-fig}
\end{table}

\textbf{Part $\text{\uppercase\expandafter{\romannumeral3}}: \text{gen}_\text{seq-2}$.} Through Part $\text{\uppercase\expandafter{\romannumeral2}}$, we derived a partial first-level expected loss $L^\prime(\theta,\mathcal{W}_{\text{pre}})$. Subsequently, in this part, by taking expectation over $E^{k,n}_{t}$, we will achieve a comprehensive first-level expectation over prefix sequence $E^{k,n}$. Utilizing infinite sequences per topic rather than $N$ sequences, the first-level expected loss $L(\theta,\mathcal{W}_{\text{pre}})$ can be more concretely described as
\begin{align}
	L(\theta,\mathcal{W}_{\text{pre}})&=\frac{1}{K}\sum_{k=1}^K L(\theta, w_k), \nonumber \\ 
	L(\theta,w_k)&=\mathbb{E}_{E^{k,n}_t}\left[ L^\prime_{x}(\theta,w_k)\right]. \label{eq-L-W}
\end{align}

Compared with $L(\theta,\mathcal{W}_{\text{pre}})$ and $L^\prime(\theta,\mathcal{W}_{\text{pre}})$, the difference lies in the second line of Equation \ref{eq-L-W} and \ref{eq-L-E-prime} with infinite sequences or $N$ sequences. This difference represents the complete generalization error of sequences which can be denoted as $\text{gen}_\text{seq-2}$,
\begin{equation}\label{eq:gen-pre}
	\text{gen}_\text{seq-2}=L(\theta,\mathcal{W}_{\text{pre}})-L^\prime(\theta,\mathcal{W}_{\text{pre}}).
\end{equation}

\textbf{Part $\text{\uppercase\expandafter{\romannumeral4}}: \text{gen}_\text{topic}$.} In this part, we further consider the second-level expectation over topic, that is, considering the population loss with infinite sequences and infinite topics. According to the difference between Equation \ref{eq-L-W} and population loss lies in the number of topics with infinite  or $K$, we have the population loss,
\begin{equation}\label{eq-L-2}
	L(\theta)=\mathbb{E}_{w_k}\left[L(\theta,w_k)\right].
\end{equation}

After which ICL will emerge from the good generalization of sequences and topics. It can be denoted as $\text{gen}_\text{topic}$,
\begin{equation}\label{eq:gen-ICL}
	\text{gen}_\text{topic} = L(\theta) - L(\theta,\mathcal{W}_{\text{pre}}).
\end{equation}