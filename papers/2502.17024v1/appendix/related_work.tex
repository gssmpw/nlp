\section{More Related Work}\label{app:related-work}
\paragraph{From Multi-Task Learning to Meta-Learning.} Although drawing inspiration from the assumption of an unknown task distribution in meta-learning analysis, it is worthy to emphasize that ICL generalization analysis under auto-regressive next-token prediction cannot be equivalent to meta-learning generalization. We conduct our analysis under the unique setup of auto-regressive pre-trained LLMs. The prompt token-dependency issue brought by auto-regressive language modeling implies that we cannot directly apply the general meta-learning analysis to ICL generalization analysis. For instance, the study in \cite{bai2024transformers} directly applied the general approach of meta-learning, assuming that a prompt consists of $N+1$ \emph{i.i.d.} samples, which is unreasonable for AR-NTP problem we investigate. For a prompt $(x_1, x_2, \cdots, x_T)$ under AR-NTP, we do not require $x_1 \sim x_T$ to be independent of each other; instead, subsequent tokens depend on previously generated tokens. As mentioned in Section \ref{sec:gen-ICL}, addressing prompt token-dependency is one of the significant contributions and challenges compared to other works, including meta-learning works in non-ICL domains. This is the key distinction from traditional meta-learning approach.

\paragraph{Generalization Analysis.} Understanding the generalization error in learning algorithm which meansures the model performance on unseen data with population loss, has led to the development of several classic methods for establishing its upper bounds. Among these, uniform convergence (including VC dimension, Rademacher complexity) \citep{bartlett2017spectrally, shalev2010learnability, vapnik1994measuring}, algorithm stability \citep{bousquet2002stability, feldman2018generalization, hardt2016train, lei2020fine, zhang2022stability}, information-theoretic bounds \citep{russo2016controlling, russo2019much, xu2017information}, and PAC-Bayesian \citep{catoni2007pac,dziugaite2017computing, mcallester1998some} are prominent techniques. 
For VC dimension, it depends solely on the hypothesis class which offers the worst-case analysis. For Rademacher complexity, it depends both on the hypothesis class and on the unknown distribution which can be understood as an average-case analysis. The above obtained bounds almost depend on the size of hypothesis space, and become vacuous hence may be unable to explain generalization in deep learning with over-parameterized neural network \citep{nagarajan2019uniform, zhang2021understanding}. Additionally, compared with algorithm stability theory, it considers worst-case and fails in analyzing the relationship between input data and output model. Therefore, we turn to the PAC-Bayesian approach for its unique data-dependent and hypothesis space-independent analysis. In our work, we specifically incorporate a topic-dependent prior within the PAC-Bayesian framework, adding a novel dimension to this analysis. Furthermore, by detailing the KL divergence when considering the optimization process, we obtain optimization algorithm-dependent generalization bound, naturally combining the advantage of algorithm stability technique. 