\newpage
\section{Practical Implications}\label{app:practical}
We first provide guidance for the quantitative selection of $K$, $N$ and $T$ based on the upper bound of expected loss described by theoretical results.

\paragraph{Increase the Number of Pre-training Topics.} For the ICL ability of LLMs, it relies on examples within a given prompt to adjust its behavior, so more topics (or tasks) provide richer information and learning opportunities. As the number of tasks increases, the model is able to learn from a broader range of contexts, thereby enhancing its generalization ability. This is different from general multi-task learning that it aims to learn multiple tasks simultaneously and if the tasks are too different or unrelated, it may lead to task interference, thereby reducing overall performance (i.e., under multitask learning, having more topics does not necessarily lead to better model generalization performance). Instead, our defined topics satisfy the assumption of topic distribution, implying a correlation between pre-training and ICL topics. This also leads to our conclusion that `more topics lead to better model generalization performance', which differs from general multi-task learning. Furthermore, when increasing the number of topics, our goal is to cover as many different types of topics as possible, to guarantee the diversity of topics, which will enrich the model's learning experience and help the model better understand new contexts with unseen topics. It potentially explains why one can improve ICL performance by selecting appropriate kind of `few-shot’ examples or exemplars to optimize performance (i.e. retrieving shots best suited to the topic/task).

\paragraph{Expand the Scale of Pre-training Sequences.} Using a large amount of training sequences per topic can provide more topic information, which helps the model better understand the language patterns for this topic. This guarantees the ability to perform well on the new sequences with a seen topic. This opinion is similar to the classical machine learning problem, where more training data helps the model perform excellently on the test data. 

\paragraph{Increase Sequence Length or Prompt Length.} Training the model to process longer sequences can enable it to better understand the context and details of lengthy texts, especially for topics or tasks that require an in-depth understanding of long articles, such as text summarization and extended question answering. We hold that longer sequence length help the model maintain coherence and completeness of information when dealing with complex problems.

Furthermore, in our PAC-Bayesian generalization bounds, the key term $D_{KL}(\mu \parallel \nu)$ surely offers possibilities to quantify the information contained in the model and data, thereby providing practical guidance for model training, training data selection and deduplication.

\paragraph{Practical Guidance for Model Training - Prior Model Initialization.} Typically, randomly initialized parameters follow uniform or standard normal distributions, which lack any specific information about the data. In contrast, during pre-training, we begin with a small-scale subset of data to train a prior model. The parameters of this prior model can then serve as an informative starting point for longer and more sufficient training with greatly-large-scale pre-training data. When using a data-dependent prior rather than random initializations, this results in a smaller $D_{KL}(\mu \parallel \nu)$, which in our theorems represents the distance between model posterior $\mu$ and prior $\nu$, contributing to a better \emph{generalization}. Furthermore, a lower $D_{KL}(\mu \parallel \nu)$ also enhances the \emph{optimization}, by detailing this term with continuous optimization trajectory analysis. Specifically for example in Theorem \ref{ICL-gen-topic-dependent}, when with topic-dependent prior $\nu_J$,
$
D_{KL}(\mu \parallel \nu_J) \approx \sigma^2 C(\frac{1}{N_{param}}, T^\prime)/K^\prime,
$
where $C(\frac{1}{N_{param}}, T^\prime) = \frac{\beta}{2} e^{8\beta S}(1-e^{-\frac{T^\prime}{exp(8\beta S)}})$. A smaller $D_{KL}(\mu \parallel \nu_J)$ means that this favorable initialization brings a stable training (with reduced gradient norm $\sigma$) and avoids exploring the entire parameter space (with fewer optimization iterations $T^\prime$). This aligns with our understanding that data patterns guide the model toward appropriate directions during training, reducing the likelihood of encountering unsuitable local minima or saddle points.

In total, using a data-dependent and topic-dependent prior for model initialization can significantly \textit{improve training stability, model convergence, and generalization.} This approach is particularly useful in multi-task learning, where it helps establish relevant priors for each task/topic in advance. Although employing more strategies to choose the subset $K^\prime$ can further refine the prior, excessive refinement may introduce new computational costs and efficiency trade-offs. We emphasize that even without careful data selection for prior model learning, a data-dependent prior generally outperforms random initialization. Particularly, when random initialization does not yield good performance, a data-dependent prior model may provide a new opportunity.

\paragraph{Practical Guidance for Training Data Selection and Deduplication.} It is well-known that the vast amount of data obtained from the internet serves as input for compressing world knowledge into LLMs \citep{deletang2023language}. The data quality among redundant data determines the upper limit of the performance of LLMs. Therefore, considering a data-dependent pre-training and ICL generalization framework has immense potential for guiding data. In our theory, to explicitly show the impact of data, we adopt a data-dependent and topic-dependent prior $\nu_J$ and further detail $D_{KL}(\mu \parallel \nu_J)$ with optimization analysis. We have discussed this in detail before: in \textit{`Practical Guidance for Model Training'} part, we emphasize the advantages of prior model initialization over random initialization in model training. Here, we aim to further explore its guidance for training data \textit{from the perspective of compression}. 

Specifically, we select a subset of size $K^\prime$ from the $K$ pre-training topics to estimate a prior distribution. If a smaller $K^\prime$ can estimate a prior that is very close to the posterior distribution, it indicates that the information from the $K$ topics can actually be compressed into a smaller subset of $K^\prime$ topics. This reflects the compressibility of the data, and can thus \textit{backward guide pre-training data} to further undergo data selection and deduplication, such as through topic clustering, data diversity, or information gain metrics (e.g., $D_{KL}(\nu(D) \parallel \nu(D_i))$, if this value is small, the data block $D_i$ is considered redundant and can be reduced in weight or removed to decrease the model's reliance on redundant information.) The reprocessed pretraining data may exclude some noise interference, further improving model performance, saving computational resources, and facilitating training for new models.



%-------------------------------------
% \section{Future Work}\label{sec:future}
% We emphasize that our work is an initial exploration for the origin of ICL ability, offering complete generalization theoretical analysis and experimental results. In the following, we provide possible explorations for future work.

% \paragraph{Characterize Data Quality.} In this study, we sample i.i.d. sequences under topics, meaning that we do not distinguish the importance of different sequences. We focus on the expectation of data distribution, while weakening the modeling of individual sequences. In future work, we can further characterize the information/semantics of individual sequences, rather than solely describing them based on length. For example, in text processing, sequences containing rare vocabulary may contain more information than sequences with common vocabulary. We can design metrics related to ‘rare vocabulary’ to measure data information/data quality or set data weights accordingly. It may help develop more practical data selection strategies based on data quality.

% \paragraph{Handle Conflicting Information between Pre-training and ICL Data.} Addressing the mismatch or conflicting information between the pre-training and ICL data is indeed a crucial issue. In this study, we model the generalization in ICL as the expected error corresponding to the topic distribution, thereby measuring the mismatch of information between the ‘topic distribution’ under infinite topics and the ‘pre-training topic distribution’ under finite topics. In future work, we can further capture the information mismatch between individual ICL topics and pre-training topics. This requires defining the KL divergence $D_{\mathrm{KL}}(\mathbb{P}_{\mathcal{W}_{\text{pre}}} \parallel \mathbb{P}_{\mathcal{W}_{\text{ICL}}})$ between pre-training topic and ICL topic distributions. Perhaps further explanation from this perspective will elucidate the relationship between pre-training topics and specific downstream/ICL topics.