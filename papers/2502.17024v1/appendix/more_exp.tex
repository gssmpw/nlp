\section{More Experiments}\label{exper}
\subsection{Experiments on Linear Dynamic System}\label{sec:app-linear}
We conduct numerical experiments of linear dynamic system. Our experimental setup follows \cite{li2019generalization}: All ICL experiments are trained and evaluated using the same GPT-2 architecture with 12 layers, 8 attention heads, and 256 dimensional embeddings, on NVIDIA 3090 GPUs.

For a partially-observed dynamical system, the mathematical model can be represented by state and observation equation. Consider the state equation $x_{t+1} = Wx_t + \zeta_t$, where $x_t$ represents the state vector at time $t$ in a $d$-dimensional space. This is analogous to the tokens in our analysis. $W$ denotes the state transition matrix and $\zeta_t$ is the process noise satisfying $\mathcal{N}(0, \sigma^2 I_d)$. The observation equation is given by $y_{t+1} = Cx_{t+1}$, where $C$ is the observation matrix, indicating that only partial dimensions of the state vector are observable. The uniqueness of different topics is reflected in the parameters $W$ and $C$. Within this linear dynamic system setting, we examine how the number of pre-training topics $(K)$, the number of sequences per topic $(N)$, and the sequence length $(T)$ significantly affect the generalization performance of auto-regressive LLMs. Additionally, we highlight the advantages of both data-dependent and topic-dependent priors.

\begin{figure*}[ht]
	\qquad
	\subfigure{
		\includegraphics[width=0.35\linewidth]{fig/overall_icl_compare.pdf}
		\label{fig:fig0overalliclcompare2}
	}
	\qquad
	\qquad
	\qquad
	\subfigure{
		\includegraphics[width=0.35\linewidth]{fig/topic_dependent_new.pdf}
		\label{fig:topicdependent}
	}
	\caption{Experiments on Linear Dynamic System. Left: The comparison of overall loss and in-context learning loss. Right: The comparison of experiments conducted on complete topic set and subset of topics.}
	\label{fig:exp-2}
\end{figure*} 

\textbf{The Comparison of Overall Loss and In-context Learning Loss}.\quad Before embarking on our main experiments, we conduct a preliminary comparison between the absolute values of the overall loss and the in-context learning (ICL) loss. In the pre-training phase, we predict all tokens in a sequence and consider the average of these predictions as the overall loss. According to our theoretical proof, this average prediction loss can be naturally generalized to the ICL phase to represent the ability of ICL. Although in more scenarios, the focus often shifts to the predicted outcome of the last token, here the prediction loss of the last token is denoted as ICL loss. In the left of Figure \ref{fig:exp-2}, our observations reveal that the ICL loss is consistently lower than the overall loss with a different number of sequences per topic. It's because the prediction loss decreases as the sequence length increases, corresponding to our theory. Consequently, our theoretical bounds hold validity and significance under both overall and ICL loss assessments.


\textbf{Separate Effects of $K$, $N$ and $T$}.\quad In our experimental design, we manipulate the variables $K$, $N$, and $T$ independently and the experimental results are shown in Figure \ref{fig:exp1}. In Figure \ref{fig:2-a}, with fixed number of sequences per topic and sequence length $(T=11)$ for each line ($N=10,20,30,40$), we vary the number of topics within the range of $K=2^{10} \sim 2^{17}$. As $K$ increases, it's noticeable that the ICL loss consistently show a downward trend across all four lines. Furthermore, the sharp drops in ICL loss observed in these cases suggests that LLMs exhibit emerging abilities when the accumulated topic count reaches certain thresholds. In Figure \ref{fig:2-b}, holding the number of topics and sequence length $(T=11)$ constant for each line (with topics set at $K=2^{13},2^{14},2^{15},2^{16}$), we adjust the number of sequences per topic, varying it within a range of $N=5 \sim 40$. Comparing the four cases, the ICL loss diminishes as $N$ grows. Notably, in cases with less sufficient topics (like $K=2^{13}$ and $2^{14}$), a larger $N$ leads to significant reductions in ICL loss. Specially, the decrease trend of ICL loss is particularly evident in the magnified view of the case where $K=2^{16}$. In Figure \ref{fig:2-c}, maintaining a constant number of topics $(K=2^{14})$ and sequence per topic $(N=40)$, we modify the sequence length, allowing it to vary within a range of $T=11 \sim 51$. We can find that ICL loss clearly decreases as the sequence length grows.

\begin{figure*}
	\centering
	\subfigure[Vary number of topics.]{
		\includegraphics[width=0.29\textwidth]{fig/changeK_new.pdf}
		\label{fig:2-a}
	}
	\quad
	\subfigure[Vary number of sequences per topic.]{
		\includegraphics[width=0.29\textwidth]{fig/changeN_new.pdf}
		\label{fig:2-b}
	}
	\quad
	\subfigure[Vary sequence length.]{
		\includegraphics[width=0.29\textwidth]{fig/changeT_new.pdf}
		\label{fig:2-c}
	}
	\caption{Experiments on Linear Dynamic System: The effect of the number of pre-training topics ($K$), the number of sequences per topic ($N$) and sequence length ($T$).}
	\label{fig:exp1}
\end{figure*} 


\textbf{The Advantages of Both Data-dependent and Topic-dependent Priors.}\quad As introduced before, data-dependent and topic-dependent priors provide a chance to make the generalization bound computable. To illustrate this, we take the example of topic-dependent prior and two experiments are conducted: one with a complete set of topics $(K=2^{16})$ and another with its subset $(K^\prime=2^{15})$. Observing the results in the right of Figure \ref{fig:exp-2}, both training processes eventually converge to nearly identical steady states. This suggests that using a subset of topics to obtain a topic-dependent prior in preliminary experiments yields a prior that is closer to the posterior than a randomly selected prior. Then for the KL divergence between prior and posterior distribution of model parameters in our generalization bounds, assume these distributions are either uniform or Gaussian allows us to derive the closed-form expressions for the KL divergence.

\subsection{Experiments on GINC Synthetic Language Dataset}\label{app:GINC}
\begin{figure*}
    \centering
    \vspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_T_K10_v50.pdf}
                \label{app-exp:lan-N-T}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K10_v50.pdf}
                \label{app-exp:lan-N-Tp-K10}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K20.pdf}
                \label{app-exp:lan-N-Tp-K20}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K30.pdf}
                \label{app-exp:lan-N-Tp-K30}
    }
    \\
    \hspace{-0.7em}
    \subfigure[]{
                \includegraphics[width=0.25\linewidth]{fig2/language_changeN_Tp_K10_v100.pdf}
                \label{app-exp:lan-N-Tp-v100}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K10_v150.pdf}
                \label{app-exp:lan-N-Tp-v150}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_fail.pdf}
                \label{app-exp:lan-fail}
    }
    \caption{Experiments on GINC Synthetic Language Dataset.}
    \label{fig:exp-GINC}
\end{figure*} 

Inspired by \cite{xie2021explanation}, we perform experiments on a synthetic language dataset GINC to verify our theoretical results.

\textbf{GINC Dataset.}\quad GINC is a small-scale language dataset generated from a uniform mixture of Hidden Markov Models (HMMs) over a family of topics/concepts \citep{xie2021explanation}. The generation steps are as follows: \textit{(1) Prepare transition matrix for HMMs:} The topic/concept determines the state transition matrix in HMM. For simulation, the transition matrix is randomly generated for each topic (each HMM), respectively; \textit{(2) Prepare vocabulary:} The vocabulary is generated as combinations of letters starting from `a' to `z', `aa' to `az', and so on. We can obtain vocabularies with different sizes; \textit{(3) Prepare memory matrix:} A unique matrix is created that records the mapping of vocabulary and state; \textit{(4) Generate sequences:} Given a fixed topic and an initial state, generate the next state based on the transition matrix, and then obtain the observed token using the memory matrix. In total, each sequence is sampled from a random HMM in the family.

\paragraph{Model and Hyperparameter Settings.} Our transformer model is based on the GPT-2 architecture with 4 layers, 12 attention heads, and 768-dimensional embeddings \citep{wolf2019huggingface}. We train the model for 20 epochs using the AdamW optimizer with a batch size of 8 and a linear learning rate schedule. The schedule includes a warmup phase of 1000 steps, up to the learning rate of 8e-4. Notably, we adopt a large portion of the code from \cite{xie2021explanation}. All experiments on GINC are conducted using a single 24GB NVIDIA GeForce RTX 3090.

In the following, We empirically explore the separate effects of the number of topics ($K$), number of sequences per topic ($N$), sequence length ($T$) and prompt length ($T_p$). We detail $K \in \{10,20,30\}$, $N \in \{20,40,60,80,100\}$, $T \in \{1280, 2560, 5120, 10240\}$, $T_p \in \{48, 80, 128, 160\}$, where ranging $T$ with directly masking the token that exceeds the specified length and do not taking special consideration. In totoal, we arrange groups of comparative experiments to verify that increasing $K, N, T, T_p$ individually improves the model's generalization performance as demonstrated in our Theorems. Additionally, we discuss the effect of vocabulary size and provide an interesting case involving with a failed ICL.

\paragraph{Observation (1): Separate Effects of $K$, $N$, $T$ and $T_p$.} We first present four groups of experiments \ref{app-exp:lan-N-T}-\ref{app-exp:lan-N-Tp-K30} in Figure \ref{fig:exp-GINC}. \textit{In Figure \ref{app-exp:lan-N-T}:} For pre-training data, take $K=10$ topics and generate $N \in \{20,40,60,80,100\}$ pre-training sequences/documents per topic, in addition with varying sequence length $T \in \{1280, 2560, 5120, 10240\}$. Then with the pre-trained model, test ICL performance on the prompt with $T_p=128$ prompt length. Each line exhibits a growing trend, indicating a better generalization performance with increasing sequences per topic. Comparing the four lines from bottom to top, a larger sequence length also brings better generalization. \textit{From Figure \ref{app-exp:lan-N-Tp-K10}-\ref{app-exp:lan-N-Tp-K30}}, we vary $K\in \{10,20,30\}$. Under each $K$, keep sequence length $T=10240$, with varying $N \in \{20,40,60,80,100\}$ and $T_p \in \{48, 80, 128, 160\}$. Combining these three groups of experiments, we validate the effects of $K,N,T_p$ on generalization, closely aligning our Theorems. 

\paragraph{Observation (2): Effect of Vocabulary Size and an Interesting Case that ICL Fails.} \textit{In Figure \ref{app-exp:lan-N-Tp-K10}, \ref{app-exp:lan-N-Tp-v100} and \ref{app-exp:lan-N-Tp-v150}}, We vary the vocabulary size within $\{50, 100, 150\}$. With fixed $K=10$ topics, we vary $N \in \{20,40,60,80,100\}$ and $T_p \in \{48, 80, 128, 160\}$. Apart from the observations similar to Figure \ref{app-exp:lan-N-Tp-K10}-\ref{app-exp:lan-N-Tp-K30} about $N, T_p$, we surprisingly find that a larger vocabulary size leads to higher ICL prediction accuracy. This aligns with our understanding that the number of possible token combinations in sequences grows with increased vocabulary size. It also implies that more diverse training data improves ICL performance. This is further implicitly supported by our theory, which suggests increasing the training sample size as much as possible to ensure sample diversity. Furthermore, we conduct an interesting experiment \textit{in Figure \ref{app-exp:lan-fail}}. When the pre-training data contains random transitions, the model observes all token transitions, yet ICL fails. This suggests that the pre-trained models cannot extract information when data distributions do not match the topic, thus failing to achieve ICL.

\subsection{Experiments on Real-world Language Datasets.}\label{app:language}
\begin{figure*}
    \centering
    \vspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.3\linewidth]{fig2/K.pdf}
                \label{app-exp:K}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.3\linewidth]{fig2/N.pdf}
                \label{app-exp:N}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.3\linewidth]{fig2/T.pdf}
                \label{app-exp:T}
    }
    \\
    \vspace{-1em}
    \hspace{-0.7em}
    \subfigure[]{
                \includegraphics[width=0.3\linewidth]{fig2/lossN-2.pdf}
                \label{app-exp:lossN}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.3\linewidth]{fig2/lossT.pdf}
                \label{app-exp:lossT}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.3\linewidth]{fig2/lossinit.pdf}
                \label{app-exp:lossinit}
    }
    \caption{Experiments on Real-world Language Dataset.}
    \label{fig:exp-real-language}
\end{figure*} 

We further perform experiments on real-world language datasets, inspired by \citep{min2021metaicl,wang2023large}. 

\paragraph{Datasets, Model and Hyperparameter Settings.} In the pre-training phase, we consider a mixture of a series of language tasks, mainly including 20 datasets. Classified by task types, including sentiment analysis (glue-sst2, poem$\_$sentiment, yelp$\_$polarity and emotion), linguistic analysis (glue-cola, blimp), text classification (ag$\_$news, dbpedia$\_$14, ethos), question answering (tweet$\_$qa) and commonsense reasoning (swag). Different datasets are considered as different topics (reflected in $K$ from our framework). In ICL phase, we test ICL performance with different datasets. All the datasets are obtained from Hugging Face. We train the GPT2-large model with a batch size of 16 and a learning rate of 1e-4 for total 30,000 iterations. Notably, we adopt a large portion of the code from \cite{wang2023large}. All experiments are conducted using four 24GB NVIDIA GeForce RTX 3090 and 40GB A100 GPUs.

In the following, we empirically explore the separate effects of the number of topics ($K$), number of sequences per topic ($N$) and sequence length ($T$). By detailing $K \in \{5,10,15,20\}$, $N \in \{2^{8}, 2^{10}, 2^{12}, 2^{14}\}$, $T \in \{48, 64, 128, 256\}$, we arrange groups of comparative experiments to verify that increasing $K, N, T$ individually improves the model's generalization performance as demonstrated in our Theorems. Additionally, we observe the impact of optimization process and prior model initialization.

\paragraph{Observation (1): Separate Effects of $K$, $N$ and $T$.} \textit{In Figure \ref{app-exp:K}}, we investigate the impact of varying the number of topics $K$. Specifically, varying $K \in \{5,10,15,20\}$, keeping fixed $N=2^{14}$ sequences per topic and sequence length $T=256$. The results show that for ICL test prompts from different datasets, increasing $K$ consistently improves ICL accuracy, as expected from our theoretical analysis. \textit{In Figure \ref{app-exp:N}}, we examine the effect of varying $N \in \{2^{8},2^{10},2^{12},2^{14}\}$, with fixed $K=20$, $T=256$. We observe that increasing $N$ leads to better performance in ICL phases, reinforcing the idea that more sequences per topic enhances model generalization and further benefits ICL. Similarly, \textit{in Figure \ref{app-exp:T}}, we explore the impact of varying $T \in \{48,64,128,256\}$ while keeping fixed $K=20$ and $N=2^{14}$. Increasing $T$ also brings better ICL performance.

\paragraph{Observation (2): Optimization Process.} Through continuous optimization trajectory analysis, our generalization bounds are also optimization-dependent. Thus beyond the influence of training data, we investigate whether optimization properties align with our theory. \textit{In Figure \ref{app-exp:lossN}}, we present four different training processes where $N \in  \{2^{8},2^{10},2^{12},2^{14}\}$ is varied, with fixed $K=20$ and $T=256$. This setting mirrors Figure \ref{app-exp:N} where we have demonstrated that increasing $N$ leads to better generalization performance. Furthermore, we observe that larger $N$ also brings faster convergence during training. This aligns with our Theorems that with a smaller number of training iterations $T^\prime$ to converge, \textit{i.e.}, the model trains faster, and further generalizes better. Similarly, \textit{Figure \ref{app-exp:lossT}} takes the same configuration with Figure \ref{app-exp:T}, which also exhibits the connection between optimization and generalization that `trains faster, generalize better'. 

\paragraph{Observation (3): Prior Model Initialization.} Based on our generalization analysis with a data-dependent prior, we propose that leveraging prior model initialization could accelerate model training. Specifically, consider the following setup: our training data consists of $K=20$ pre-training topics, $N=2^{14}$ training sequences per topic and sequence length $T=256$. 
\begin{itemize}
    \item \textbf{Step 1:} Train the GPT2-small model for 15,000 steps using $K=5$ pre-training topics, $N=2^{14}$ training sequences per topic and sequence length $T=256$.
    \item \textbf{Step 2:} Transfer the weights from GPT2-small model to the corresponding weight matrices of GPT2-large, ensuring dimension compatibility. Initialize the weights randomly for the additional transformer layers in GPT2-large.
    \item \textbf{Step 3:} Train the GPT2-large model for an additional 30,000 steps using the full pre-training data ($K=20$, $N=2^{14}$, $T=256$)
\end{itemize}
According to our experimental results, the random initialization regime with all pre-training data requires nearly \textit{\textbf{7 hours}} on four A100 GPUs to complete 30,000 steps. However, under the prior model initialization regime, where a smaller model is used for warmup and serves as the prior model initialization for the larger model, training the GPT2-large model takes only \textbf{\textit{4 hours}} for 30,000 steps on four A100 GPUs under the same setting of $K,N,T$ (with 0.5 hours needed for training the GPT2-small model for 15,000 steps).

Furthermore, as shown in the optimization loss curve in Figure \ref{app-exp:lossinit}, the prior model initialization not only accelerates training but also stablizes the training process (especially at the early stage), leading to comparable or even improved model performance. This approach demonstrates how effectively leveraging prior knowledge can contribute to the training process and performance, supporting the KL term in our generalization bounds and presenting more practical insights.  