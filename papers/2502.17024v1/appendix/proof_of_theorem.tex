\section{Proof of Theorems}
\subsection{Useful Definitions, Lemmas and Propositions}
\begin{definition}[Entropy]\label{def:Entropy} For random variable $\theta$, which takes value in $\Theta$ and its probability distribution is $\mu$, the entropy of random variable $\theta$ is
	$$H(\theta)=-\sum_{\theta \in \Theta}\mu(\theta)\log \mu(\theta)=\mathbb{E}_{\theta \sim \mu}\left[-\log \mu(\theta)\right].$$
\end{definition}

\begin{definition}[Kullback–Leibler Divergence]\label{def:KL} The Kullback–Leibler (KL) divergence between two probability distributions $\mu$ and $\nu$ is defined by
	$$\KL(\mu \parallel \nu)=\mathbb{E}_{\theta\sim\mu}\left[\log\frac{\mu(\theta)}{\nu(\theta)}\right].$$
\end{definition}

\begin{lemma}[Donsker–Varadhan representation in \cite{belghazi2018mutual} Theorem 1]
	\label{lemma:Donsker-ieq}
	The KL divergence between probability distribution $\mu$ and $\nu$ obeys the following dual representation:
	$$
	\KL(\mu\parallel \nu)=\sup_{T: \mathcal{A} \rightarrow \mathbb{R}}\left\{\mathbb{E}_\mu\big[T\big]-\log(\mathbb{E}_\nu[e^T])\right\},
	$$
	where the compact set $\mathcal{A} \subseteq \mathbb{R}^d$ is the support of distribution $\mu$ and $\nu$, and the supremum is calculated across all functions $T$ for which both expectations are finite.
\end{lemma}

Let $\mathcal{F}$ be any class of functions $T:\mathcal{A} \rightarrow \mathbb{R}$ satisfying the integrability constraints of the lemma. Then for any defined function $T$, it's straightforward to get the lower-bound of the KL divergence between $\mu$ and $\nu$
$$
\KL(\mu\parallel \nu) \geq \mathbb{E}_\mu\big[T\big]-\log(\mathbb{E}_\nu[e^T]),
$$
which would be used in the proof of Theorem \ref{app:pre-gen}.


\begin{definition}[Total Variation Distance in \cite{levin2017markov}]\label{def:TV-distance} The total variation (TV) distance between two probability distributions $\mu$ and $\nu$ on events set $\mathcal{B}$ is defined by
	$$\TV(\mu,\nu)=\max_{B\in\mathcal{B}}\left|\mu(B)-\nu(B)\right|.$$
\end{definition}
This definition is explicitly probabilistic: It quantifies the divergence between $\mu$ and $\nu$ as the maximum disparity in the probabilities assigned to a single event $B$ by the two distributions.

\begin{proposition}[Total Variation Distance in \cite{levin2017markov}]\label{prop:TV-distance}
	Let $\mu$ and $\nu$ be two probability distributions on $\mathcal{A}$. Then
	$$
	\TV(\mu, \nu)=\frac{1}{2}\sum_{a\in\mathcal{A}}\left|\mu(a)-\nu(a)\right|.
	$$
\end{proposition}

\begin{proof}
	Let $A$ be any event and event $B$ be $B=\{a:\mu(a)\geq\nu(a)\}$. Since $A=A\cap(B\cup B^c)=(A \cap B)\cup (A \cap B^c) $, then we have
	\begin{equation*}\label{mu-1}
		\mu(A)-\nu(A)\leq\mu(A\cap B)-\nu(A\cap B)
	\end{equation*}
	Since including more elements of $B$ cannot decrease the difference in probability, we have
	\begin{equation*}\label{mu-2}
		\mu(A\cap B)-\nu(A\cap B)\leq\mu(B)-\nu(B)
	\end{equation*}
	Combine the above two inequality, we have
	\begin{equation*}\label{mu-3}
		\mu(A)-\nu(A)\leq\mu(B)-\nu(B)
	\end{equation*}
	Similarly,
	$$
	\begin{aligned}\label{nu}
		\nu(A)-\mu(A)\leq\nu(B^{c})-\mu(B^{c}).
	\end{aligned}
	$$
	Thus
	$$
	\TV(\mu,\nu)=\frac{1}{2}\left[\mu(B)-\nu(B)+\nu(B^{c})-\mu(B^{c})\right]=\frac{1}{2}\sum_{a\in\mathcal{A}}\left|\mu(a)-\nu(a)\right|.
	$$
\end{proof}

\begin{lemma}[Lemma 22 in \cite{agarwal2020flambe}]\label{lemma:TV} For any two conditional probability distribution $\mu$ and $\nu$, we have
	$$
	\TV\left(\mu(\cdot | x), \nu(\cdot | x)\right)^2 \leq-2\log\mathbb{E}_{y\sim \mu(\cdot|x)}\left[\exp\left(-\frac{1}{2}\log \frac{\mu(y|x)}{\nu(y|x)}\right)\right].
	$$
\end{lemma}

This lemma provides an upper bound on the total variation distance, which is related to the expectation of the logarithmic ratio of two conditional probability distributions. It would be used in the proof of Theorem \ref{app:pre-gen}.

\begin{lemma}[Upper Bound of KL divergence]\label{lemma:KL-TV-bound} For any two conditional probability distribution $\mu$ and $\nu$, if $\frac{\mu(a)}{\nu(a)} \leq C$, we have
	$$
	\KL(\mu(a) \parallel \nu(a)) \leq \frac{2C\log C}{C-1}\TV(\mu(a),\nu(a)).
	$$
\end{lemma}
This lemma provides the relationship between KL divergence and TV distance.

\begin{proof}
	Let $f(t)=\log t$, $g(t)=|\frac{1}{t}-1|$. According to the definition of KL divergence and TV distance (see in \ref{def:KL} and \ref{prop:TV-distance}), we have
	$$
	\begin{aligned}
		&\KL(\mu(a) \parallel \nu(a)) = \mathbb{E}_{a \sim \mu}\left[\log \frac{\mu(a)}{\nu(a)}\right] = \mathbb{E}_{a \sim \mu}\left[f\left(\frac{\mu(a)}{\nu(a)}\right)\right] \\
		&\TV(\mu(a),\nu(a)) =\frac{1}{2}\sum_a |\mu(a)-\nu(a)| =\frac{1}{2}\sum_a \mu(a)\left|1-\frac{\nu(a)}{\mu(a)}\right| = \frac{1}{2}\mathbb{E}_{a \sim \mu}\left[g\left(\frac{\mu(a)}{\nu(a)}\right)\right] \\
	\end{aligned}
	$$ 
	For $0 < t \leq C (t \neq 1)$ , we have
	$$
	\sup_{0 < t \leq C, t \neq 1}\frac{f(t)}{g(t)} = \sup_{0< t \leq C, t \neq 1}\frac{\log t}{|\frac{1}{t}-1|} = \sup_{1< t \leq C}\frac{t\log t}{t-1}
	$$
	Based on the derivative chain rule, we have that if $1< t \leq C$, $\frac{t\log t}{t-1} \leq \frac{C\log C}{C-1}$.
	Thus, we conclude that
	$$
	\KL(\mu(a) \parallel \nu(a)) \leq \frac{2C\log C}{C-1}\TV(\mu(a),\nu(a)).
	$$
\end{proof}
\begin{definition}[Fokker-Planck Equation in \cite{mou2018generalization}]\label{def:Fokker} Let $\pi_t$ be the probability density function of distribution $\mu_t$, then Fokker-Planck Equation describes the evolution of $\pi_t$:
	$$\frac{\partial \pi_t}{\partial t}=\frac1\beta\Delta \pi_t-{\nabla}\cdot(\pi_t\nabla L_{E}(\theta_{t-1}, \mathcal{W}_{\text{pre}}) )$$
	where $\nabla$ is gradient operator and $\Delta$ is Laplace operator.
\end{definition}

\begin{definition}[Gradient Langevin Dynamics and Continuous Langevin Dynamic \cite{li2019generalization}] \label{def:GLD-CLD}
	LLMs perform Stochastic Gradient Descent (SGD) as optimization algorithm to update parameters $\theta$ in order to get the minimum $\hat{\theta}$. SGD can be viewed as gradient descent addition with gradient noise between full batch gradient and single/mini-batch gradient \citep{wang2022two}. The full batch gradient with $\theta_{t-1}$ can be denoted as $\nabla L_{E}(\theta_{t-1}, \mathcal{W}_{\text{pre}})$, and assume that the gradient noise follows an isotropic Gaussian distribution $\mathcal{N}(0,\frac{I_d}{\beta})$, thus the training dynamic of LLMs can be defined as
	\begin{equation}\label{GLD}
		\theta_t \leftarrow {\theta}_{t-1} - \eta_t \nabla L_{E}(\theta_{t-1}, \mathcal{W}_{\text{pre}})+\sqrt{\frac{\eta_t}{\beta}}\mathcal{N}(0,I_d),
	\end{equation}
	which is called Gradient Langevin Dynamics (GLD). When the step size $\eta_t$ in GLD (see in equation \ref{GLD}) approaches zero, the Continuous Langevin Dynamics (CLD) is defined by the following Stochastic Differential Equation (SDE),
	\begin{equation}\label{CLD}
		\mathrm{d} \theta_t=-\nabla L_{E}(\theta_{t-1}, \mathcal{W}_{\text{pre}})\mathrm{d} t+\sqrt{\beta^{-1}} \mathrm{~d} B_t, \quad \theta_0 \sim \mu_0
	\end{equation}
	where $B_t$ is the standard brown emotion on $\mathbb{R}^d$. 
\end{definition}

\begin{lemma}[Log-Sobolev Inequality (LSI) for Continuous Langevin Dynamic (CLD) in \cite{li2019generalization} Lemma 16]\label{lemma:LSI}
	Under Equation \ref{CLD}, let $q_t$ be the probability density function of parameter $\theta_t$ in CLD and the initial state obeys $\theta_0 \sim \mathcal{N}(0,\frac{I_d}{\beta})$. Let $p$ be any probability density function which is absolutely continuous with respect to $q_t$. Assume that the optimization objective $L_E(\theta,\mathcal{W}_{\text{pre}})$ is $C$-bounded, then we have
	$$
	\KL\left(p \parallel q_t\right)\leq\frac{\exp(8\beta S)}{2\beta}\int_{\mathbb{R}^d}\left\|\nabla\log\frac{p(\theta)}{q_t(\theta)}\right\|^2_2 p(\theta)\mathrm{d}\theta.
	$$
\end{lemma}

Many existing LSIs primarily focus on characterizing the stationary distribution of the Markov process. Contrastly, as shown in this lemma, we try to establish a LSI for  $\mu_t$, which denotes the parameter distribution at each time step $t>0$. It would be used in the proof of Theorem \ref{app:pre-gen-data-dependent} and \ref{app:ICL-gen-topic-dependent} which explores the upper bound of KL divergence carefully to get data-dependent and topic-dependent generalization bound. According to Fokker-Planck Equation in Definition \ref{def:Fokker}, the KL divergence between two probability density function can be built so that Lemma \ref{lemma:LSI} can be applied naturally.

\begin{lemma}[McDiarmid’s Inequality Variants for Markov Chains in \cite{paulin2015concentration} Theorem 2.1]\label{lemma:mc-markov}
	Consider a Markov chain $X=(X_1, \cdots, X_N)$, which is not necessarily time homogeneous, taking values in a Polish state space $\Lambda = \Lambda_1\times \cdots \times \Lambda_N$, with mixing time $\tau(\epsilon)$ (for $0 \leq \epsilon \leq 1$). Let $\tau_{\min}:=\inf_{0\leq\epsilon<1}\tau(\epsilon)\cdot\left(\frac{2-\epsilon}{1-\epsilon}\right)^2$, $c \in \mathbb{R}^N_{+}$. If $f:\Lambda \rightarrow \mathbb{R}$ satisfies $f(x)-f(y)\leq\sum_{i=1}^Nc_i \bm{1}[x_i\neq y_i]$, Then for any $\lambda \in \mathbb{R}$, we have
	$$
	\log\mathbb{E}_X\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]\leq\frac{\lambda^2\cdot\|c\|_2^2\cdot \tau_{\min}}8.
	$$
\end{lemma}

\begin{proposition}[Refer to \cite{zhang2023and}]\label{prop0: high-prob}
	Define $f(X)=\frac{1}{N}\sum_{i=1}^N f(X_i)$ where $X=(X_1, \cdots, X_N)$ is a Markov chain. With the condition in Lemma \ref{lemma:mc-markov}, if $|f(X_i)| \leq C$ and $f \in \mathcal{F}$, given a prior distribution $\nu$ on $\mathcal{F}$, with probability at least $1-\delta$
	$$
	\mathbb{E}_\mu\left[\mathbb{E}_X[f(X)] -f(X)\right] \leq \sqrt{\frac{C^2\cdot \tau_{\min}}{2N \log 2}}\left[\KL(\mu\parallel \nu)+\log \frac{2}{\delta}\right]
	$$
\end{proposition}

\begin{proof}
	With the assumption $|f(X_i)| \leq C$, we have $c_i=\frac{2C}{N}$ in $f(x)-f(y)\leq\sum_{i=1}^Nc_i \bm{1}[x_i\neq y_i]$. Then using Lemma \ref{lemma:mc-markov},
	\begin{align}
		\log\mathbb{E}_X\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]&\leq\frac{\lambda^2C^2\cdot \tau_{\min}}{2N} \nonumber\\
		\mathbb{E}_{\nu}\Big[\mathbb{E}_X\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]\Big]&\leq\exp\left(\frac{\lambda^2 C^2\cdot \tau_{\min}}{2N}\right) \nonumber\\
		\mathbb{E}_{X}\Big[\mathbb{E}_\nu\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]\Big]&\leq\exp\left(\frac{\lambda^2 C^2\cdot \tau_{\min}}{2N}\right) \label{0-mc-markov-eq-1}
	\end{align}
	According to Markov inequality $P(X \geq t)\leq \frac{E[X]}{t}$ for random variable $X$ and any $t>0$, we have
	\begin{equation}\label{0-mc-markov-eq-2}
		P{\left(\mathbb{E}_\nu\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big] \geq t \right)}\leq \frac{\mathbb{E}_X\Big[\mathbb{E}_\nu\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]\Big]}{t} 
	\end{equation}
	then substitute inequality \ref{0-mc-markov-eq-1} into \ref{0-mc-markov-eq-2},
	\begin{equation}\label{0-mc-markov-eq-3}
		P{\left(\mathbb{E}_\nu\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big] \geq t \right)}\leq \frac{\exp\left(\frac{\lambda^2 C^2\cdot \tau_{\min}}{2N}\right)}{t}
	\end{equation}
	Let $\lambda=\sqrt{\frac{2N\log 2}{C^2\cdot \tau_{\min}}}$ and $t=\frac{2}{\delta}$ for any $0 < \delta < 1$, inequality \ref{0-mc-markov-eq-3} can be transformed into
	$$
	P{\left(\mathbb{E}_\nu\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big] \geq \frac{2}{\delta} \right)}\leq \delta
	$$
	According to Lemma \ref{lemma:Donsker-ieq}, 
	$$
	\KL(\mu\parallel \nu) \geq \mathbb{E}_\mu\big[T\big]-\log(\mathbb{E}_\nu[e^T])
	$$
	Let $T=\lambda(\mathbb{E}_X[f(X)]-f(X))$, then with probability at least $1-\delta$, we have
	$$
	\begin{aligned}
		\mathbb{E}_\mu\left[\lambda(\mathbb{E}_X[f(X)]-f(X))\right] &\leq \KL(\mu\parallel \nu)+\log\left(\mathbb{E}_\nu\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]\right) \\
		\mathbb{E}_\mu\left[\mathbb{E}_X[f(X)] -f(X)\right] &\leq \frac{1}{\lambda}\left[\KL(\mu\parallel \nu)+\log \frac{2}{\delta}\right] \\
		\mathbb{E}_\mu\left[\mathbb{E}_X[f(X)] -f(X)\right] &\leq \sqrt{\frac{C^2\cdot \tau_{\min}}{2N \log 2}}\left[\KL(\mu\parallel \nu)+\log \frac{2}{\delta}\right]
	\end{aligned}
	$$
\end{proof}

\begin{lemma}[McDiarmid’s Inequality Variants in \cite{luo2022generalization}]\label{lemma:mc-data-dependent}
	Consider a function $f:[N]^{N'}\rightarrow \mathbb{R}^{+}$ that is order-independent, where $|f(J) - f(J')| \leq c$ holds for any adjacent sets $J, J' \in [N]^{N'}$ such that there is only one different elements in the two sets. Let $J$ consist of $N^\prime$ indices sampled uniformly without replacement from $[N]$. Then, for any $t \geq 0$,
	$$
	P\left(\left|f(J)-\mathbb{E}_J[f(J)]\right|\geq t\right) \leq \exp\left(\frac{-2t^2}{N^{\prime}c^2}\right)
	$$
\end{lemma}

\begin{proposition}\label{prop1: chernoff}
	Define $f(X)=\frac{1}{N}\sum_{i=1}^N f(X_i)$ where $X=(X_1, \cdots, X_N)$ is a Markov chain. With the condition in Lemma \ref{lemma:mc-markov} and if $|f(X_i)| \leq C$, then with probability at least $1-\delta$
	$$
	\mathbb{E}_X[f(X)] -f(X) \leq \sqrt{\frac{2C^2 \cdot \tau_{\min} \log \frac{1}{\delta}}{N}}
	$$
\end{proposition}

\begin{proof}
	With the assumption $|f(X_i)| \leq C$, we have $c_i=\frac{2C}{N}$ in $f(x)-f(y)\leq\sum_{i=1}^Nc_i \bm{1}[x_i\neq y_i]$. Then using Lemma \ref{lemma:mc-markov},
	\begin{align}
		\log\mathbb{E}_X\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]&\leq\frac{\lambda^2C^2\cdot \tau_{\min}}{2N} \nonumber\\
		\mathbb{E}_X\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]&\leq\exp\left(\frac{\lambda^2 C^2\cdot \tau_{\min}}{2N}\right) \label{1-mc-markov-eq-1}
	\end{align}
	According to Chernoff bound $P(X \geq t)\leq \frac{E[e^{\lambda X}]}{e^{\lambda t}}$ for random variable $X$ and any $\lambda>0$, we have
	\begin{equation}\label{1-mc-markov-eq-2}
		P{\left(\mathbb{E}_X[f(X)]-f(X)\geq t \right)}\leq \frac{\mathbb{E}_X\Big[\exp\big(\lambda(\mathbb{E}_X[f(X)]-f(X))\big)\Big]}{\exp\left({\lambda t}\right)} 
	\end{equation}
	then substitute inequality \ref{1-mc-markov-eq-1} into \ref{1-mc-markov-eq-2},
	\begin{equation}\label{1-mc-markov-eq-3}
		P{\left(\mathbb{E}_X[f(X)]-f(X) \geq t \right)}\leq \frac{\exp\left(\frac{\lambda^2 C^2\cdot \tau_{\min}}{2N}\right)}{\exp\left({\lambda t}\right)}=\exp\left(\frac{\lambda^2 C^2\cdot \tau_{\min}}{2N}-\lambda t\right)
	\end{equation}
	Let $\lambda=\frac{Nt}{C^2 \cdot \tau_{\min}}$, inequality \ref{1-mc-markov-eq-3} can be transformed into
	$$
	P{\left(\mathbb{E}_X[f(X)]-f(X) \geq t \right)}\leq \exp\left(\frac{-N t^2}{2C^2\cdot \tau_{\min}}\right)
	$$
	Let $t=\sqrt{\frac{2C^2 \cdot \tau_{\min} \log \frac{1}{\delta}}{N}}$ for any $0 < \delta < 1$, 
	$$
	P{\left(\mathbb{E}_X[f(X)]- f(X) \geq \sqrt{\frac{2C^2 \cdot \tau_{\min} \log \frac{1}{\delta}}{N}} \right)}\leq \delta
	$$
	Finally, with probability at least $1-\delta$,
	$$
	\mathbb{E}_X[f(X)] -f(X) \leq \sqrt{\frac{2C^2 \cdot \tau_{\min} \log \frac{1}{\delta}}{N}}
	$$
\end{proof}

\subsection{Generalization of Sequences: The First-Level Expectation}
%预训练过程只需要考虑数据的期望，不需要考虑主题分布的期望
\subsubsection{Proof of Theorem \ref{app:pre-gen}}\label{appendix-the-1}
\begin{theorem*}[Generalization Bound of the First-Level Expected Loss] Let the auto-regressive LLM $\mathbb{P}_\theta$ be the empirical solution of Equation $\ref{eq-L-E}$, and $\mathbb{P}(\cdot\mid w)$ denotes the true data distribution under topic $w$. Under Assumptions \ref{ass:B}, for any $0<\delta < 1$, with probability at least $1-\delta$, the first-level expected loss with $K$ topics and infinite sequences per topic, denoted by $L(\theta, \mathcal{W}_{\text{pre}})$ (see in Equation \ref{eq-L-Wpre-two-part-final-main} or Equation \ref{eq-L-W}), satisfies,
    \begin{align*}
        \mathbb{E}_{\mu}\left[L(\theta, \mathcal{W}_{\text{pre}})\right]
        =\mathcal{O}\left\{\sqrt{\frac{\log 1/\delta}{KNT}}+\sqrt{\frac{1}{KNT}\left(\KL(\mu\parallel \nu)+\operatorname{log}\frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\},
    \end{align*}

	where $\epsilon_{\text{opt}}$ is the optimization error (see in Equation \ref{opt}). $\mu$ and $\nu$ are the posterior and prior distribution of model parameters $\theta$, respectively. $K$, $N$ and $T$ denote the number of topics, the number of sequences per topic and the sequence length utilized in the optimization process of Equation $\ref{eq-L-E}$.
\end{theorem*}

\paragraph{Proof sketch.} Before the formal proof, we introduce the processing route to obtain the generalization error bound with handing the prompt token-dependency issue. First, we elaborate on the construction of ghost sequences $\tilde{E}_k$, which are constructed auto-regressively depending on the original sequence ${E}_k$ thus tokens in ghost sequences are independent. Additionally, we define the function $T=g(\theta,w_k)-\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]$, where
$
g(\theta,w_k)=\frac{1}{2}\sum_{n=1}^{N} \sum_{t=1}^T \log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}.
$
It can be observed that this function links the original sequence $E_k$ (with dependent tokens), with the ghost sequences $\tilde{E}_k$ (with independent tokens). Substituting them into the Donsker-Varadhan Inequality facilitates establishing a connection between `data’ and the KL distance between `model prior’ and `model posterior based on training data’. Furthermore, regarding the coupling term $\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]$ in the function $T$, we handle this part using the lemma provided in \cite{agarwal2020flambe}, where this part is further transformed into a distribution measure of Total Variance distance (TV distance). As we mentioned in Section \ref{sec:gen}, the primary optimization objective `negative logarithm likelihood’ naturally leads to `KL divergence’, thereby formalizing the expression of population loss. Therefore, it's necessary to introduce a relationship between TV distance and KL divergence (See in Lemma \ref{lemma:KL-TV-bound}), for obtaining our generalization bound. \textbf{Overall, the processing route can be summarized as:}
`original sequences $E_k$ $\rightarrow$ ghost sequences $\tilde{E}_k$ $\rightarrow$ Donsker-Varadhan Inequality $\rightarrow$ TV distance $\rightarrow$ KL divergence $\rightarrow$ the upper bound of population loss based on KL divergence’.

\begin{proof}
	As we introduced before, all the pre-training sequences set is $E = \{E^{k,n}\}_{k,n=1}^{K,N}$, each sequence is denoted as $E^{k,n}=\{(E^{k,n}_t, x^{k,n}_{t+1})\}_{t=1}^{T_{k,n}-1}$ where $x^{k,n}_{t+1}\sim \mathbb{P}(\cdot \mid E^{k,n}_{t},w_k)$. To decouple the dependence between tokens, we construct tangent/ghost sequences $\tilde{E} = \{\tilde{E}^{k,n}\}_{k,n=1}^{K,N}$ and each sequence is $\tilde{E}^{k,n}=\{(\tilde{E}^{k,n}_t, \tilde{x}^{k,n}_{t+1})\}_{t=1}^{T_{k,n}-1}$ where $\tilde{x}^{k,n}_{t+1}$ is generated depending on the partial original sequences $E^{k,n}_t$. The construction process of tangent/ghost sequences can be understood simply as duplicate sequences generated based on the original sequences. This proprietary term has been previously utilized in \cite{agarwal2020flambe, de1999general, kwapien1991semimartingale}. Therefore, by introducing the ghost sequences into our analysis, this will help decouple the token-dependency in auto-regressive sampling of sequences.
	
	Notice that the following proof is first established under a fixed topic $w_k$.
	
	According to Donsker-Varadhan Inequality (Lemma \ref{lemma:Donsker-ieq}), let $\mathcal{F}$ be any class of functions $T:\Omega \rightarrow \mathbb{R}$ satisfying the integrability constraints of the lemma. Then for any defined function $T$, it's straightforward to get the lower-bound of the KL divergence between $\mu$ and $\nu$
	$$
	\KL(\mu\parallel \nu) \geq \mathbb{E}_\mu\big[T\big]-\log(\mathbb{E}_\nu[e^T]),
	$$
	Under fixed topic $w_k$, the posterior distribution of model parameter $\theta$ is depending on $E^k$ (see in Section \ref{sec: ICL}) denoted by $\mu$ and the prior distribution of $\theta$ is denoted by $\nu$. Then, a simple deformation of Lemma \ref{lemma:Donsker-ieq} leads to
	$$
	\begin{aligned}
		\mathbb{E}_{\mu}[T]-\KL(\mu\parallel \nu) &\leq \operatorname{log}\mathbb{E}_{\nu}[\operatorname{exp}(T)] \\
		\operatorname{exp}{\left(\mathbb{E}_{\mu}[T]-\KL(\mu\parallel \nu)\right)} &\leq \mathbb{E}_{\nu}[\operatorname{exp}(T)]
	\end{aligned}
	$$
	Taking expectation over data distribution $E^{k} \sim \mathbb{P}(\cdot\mid w_k)$, we have
	\begin{equation}
		\label{Donsker-ieq-E}
		\mathbb{E}_{E^{k}}\big[\operatorname{exp}{\{\mathbb{E}_{\mu}[T]-\KL(\mu\parallel \nu)\}}\big] \leq \mathbb{E}_{E^{k}}\mathbb{E}_{\nu}[\operatorname{exp}(T)]
	\end{equation}
	Let $T=g(\theta,w_k)-\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]$ where 
	$$
	g(\theta,w_k)=\frac{1}{2}\sum_{n=1}^{N} \sum_{t=1}^T \log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)} 
	$$
	Then for the right hand of inequality \ref{Donsker-ieq-E}, we have
	$$
	\begin{aligned}
		\mathbb{E}_{E^{k}}\mathbb{E}_{\nu}[\operatorname{exp}(T)]
		=&\mathbb{E}_{E^k}\mathbb{E}_{\nu}\left[\exp\left(g(\theta,w_k)-\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]\right)\right]\\
		=&\mathbb{E}_{\nu}\mathbb{E}_{E^k}\left[\exp\left(g(\theta,w_k)-\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]\right)\right]\\
		=&\mathbb{E}_{\nu}\mathbb{E}_{E^k}\left[\frac{\operatorname{exp}\left(g(\theta,w_k)\right)}{\mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]}\right]=1
	\end{aligned}
	$$
	Similarly to \cite{agarwal2020flambe}, the last equality follows that the token in tangent sequence $\tilde{E}^k$ is independent conditional on ${E}^k$, so we have $\mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]=\mathbb{E}_{\tilde{x}^{k,n}_{t+1} \sim \mathbb{P}(\cdot\mid E^{k,n}_t,w_k)}\left[\prod_{n=1}^N \prod_{t=1}^T \exp{\left(\frac{1}{2}\log \frac{\mathbb{P}(\tilde{x}^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(\tilde{x}^{k,n}_{t+1}|E^{k,n}_t, w_k)}\right)} \right]$. Thus, inequality \ref{Donsker-ieq-E} can be transformed to
	\begin{equation}
		\label{Donsker-ieq-E-2}
		\mathbb{E}_{E^{k}}\big[\operatorname{exp}{\{\mathbb{E}_{\mu}[T]-\KL(\mu\parallel \nu)\}}\big] \leq 1
	\end{equation}
	
	With Markov Inequality $\mathbb{P}[X \geq a]\leq \frac{\mathbb{E}[X]}{a}$, we get the following high probability representation with probability at least $1-\delta$,
	\begin{align}
		&\text{let}\ X=\mathbb{E}_{\mu}[T]-\KL(\mu\parallel \nu) \Rightarrow \mathbb{P}[e^X \geq e^a]\leq \frac{\mathbb{E}[e^X]}{e^a} \leq \frac{1}{e^a} \Rightarrow \mathbb{P}(X \leq \operatorname{log}\frac{1}{\delta}) \geq 1-\delta\nonumber\\
		&\mathbb{E}_{\mu}\left[g(\theta,w_k)\right]-\mathbb{E}_{\mu}\left[\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]\right] \leq \KL(\mu\parallel \nu) +\operatorname{log}\frac{1}{\delta} \label{Donsker-ieq-E-3} 
	\end{align}
	For inequality \ref{Donsker-ieq-E-3}, we mainly deal with the left hand in this Theorem and make more detailed analysis of KL divergence in Theorem \ref{app:pre-gen-data-dependent} to get data-dependent and optimization algorithm-dependent PAC-Bayesian generalization bound.
    \begin{align}
		&\mathbb{E}_{\mu}\left[g(\theta,w_k)\right]-\mathbb{E}_{\mu}\left[\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k))\mid E^k\right]\right] \nonumber
		\\
        \geq& \mathbb{E}_{\mu}\left[\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\operatorname{log}\frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_{\theta}(x^{k,n}_{t+1}|S^{k,n}_t,w_k)}\right]- \nonumber \\
        &\mathbb{E}_{\mu}\left[\sum_{n=1}^{N}\sum_{t=1}^T\operatorname{log}\mathbb{E}_{\tilde{E}^k}\left[\operatorname{exp}\left(-\frac{1}{2}\operatorname{log}\frac{\mathbb{P}_{\theta}(\tilde{x}^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}(\tilde{x}^{k,n}_{t+1}|E^{k,n}_t,w_k)}\right)\mid E^k\right]\right]
	\end{align}

	Using Lemma \ref{lemma:TV}, the second term in the right hand can be transformed to the total variation distance (TV distance) of distribution $\mathbb{P}_\theta$ and $\mathbb{P}$.
	\begin{align}
		&\mathbb{E}_{\mu}\left[g(\theta,w_k)\right]-\mathbb{E}_{\mu}\left[\log \mathbb{E}_{\tilde{E}^k}\left[\exp(g(\theta,w_k)))\mid E^k\right]\right] \nonumber\\
		\geq&\mathbb{E}_{\mu}\left[\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\operatorname{log}\frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}\frac{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}\right]\\
		&+\mathbb{E}_{\mu}\left[\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)^2\right] \nonumber\\
		=&\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\operatorname{log}\frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}+\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\mathbb{E}_{\mu}\left[\operatorname{log}\frac{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}\right]\nonumber\\
		&+\mathbb{E}_{\mu}\left[\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)^2\right] \nonumber\\
		\geq& \frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\mathbb{E}_{\mu}\left[\operatorname{log}\frac{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_{{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}\right]\nonumber\\
		&+\mathbb{E}_{\mu}\left[\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)^2\right] \label{huajian}
	\end{align}
	where $\hat{\theta}$ is the minimum of empirical loss \ref{eq-L-E}. 
	Thus, substitute inequality \ref{huajian} into \ref{Donsker-ieq-E-3},  
	\begin{multline}
		\label{emperical-res-wk}
		\mathbb{E}_{\mu}\left[\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)^2\right]
		\\+\frac{1}{2}\sum_{n=1}^{N}\sum_{t=1}^T\mathbb{E}_{\mu}\left[\operatorname{log}\frac{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_{{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}\right]
		\leq \KL(\mu\parallel \nu) +\operatorname{log}\frac{1}{\delta}
	\end{multline}
	The result of inequality \ref{emperical-res-wk} is analysised under a fixed topic $w_k$, then combining all $w_k \in \mathcal{W}_{\text{pre}}$ and taking average 
	\begin{multline}
		\mathbb{E}_{\mu}\left[\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)^2\right] \\
		\leq \frac{2}{KNT}\left(\KL(\mu\parallel \nu)+\log\frac{1}{\delta}\right)-\underbrace{\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^{N}\sum_{t=1}^T \mathbb{E}_{\mu}\left[\operatorname{log}\frac{\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}{\mathbb{P}_{\theta}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)}\right]}_{\epsilon_{\text{opt}}}
	\end{multline}
	where the second term in the right hand is denoted as $\epsilon_{\text{opt}}$ measuring the logarithmic distribution distance between the ideal minimum $\hat{\theta}$ and the trained model $\theta$ with empirical loss. Specially, we defer the analysis of optimization error to future work. Here, we assume that the results of the actual models obtained closely approximates the ideal minimum for empirical loss, implying that \(\epsilon_{\text{opt}}\) is a very small value so that this item will be kept in the upper bounds of the first-level expected loss and two-level expected loss. Thus,
	\begin{align}
		&\mathbb{E}_{\mu}\left[\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)\right]\nonumber\\
		\leq&\sqrt{ \mathbb{E}_{\mu}\left[\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)^2\right]} \nonumber\\
		\leq& \sqrt{\frac{2\left(\KL(\mu\parallel \nu)+\operatorname{log}\frac{1}{\delta}\right)}{KNT}-\epsilon_{\text{opt}}}  \label{emperical-res-wpre}
	\end{align}
	
	Using Assumption \ref{ass:B}, assume $\log\frac{\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)}{\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k)}$ is upper bounded by $C$. Thus using Proposition \ref{prop1: chernoff}, with probability at least $1-\delta$,
	\begin{multline}\label{first-exp-minus}
		\mathbb{E}_{\mu}\biggl[\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^N\sum_{t=1}^T\mathbb{E}_{E^{k,n}_t}\left[\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)\right] \\
		-\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^{N}\sum_{t=1}^T\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)\biggr]
		\leq \sqrt{\frac{2C^2\cdot \tau_{\min}\log \frac{1}{\delta}}{KNT}}
	\end{multline}
	
	Finally, according to Equation \ref{eq:gen-pre}, \ref{eq-L-W} and \ref{eq-L-E}, the generalization error bound of the first-level expected loss is $
	\text{gen}_\text{seq}=L(\theta,\mathcal{W}_{\text{pre}})-L_E(\theta,\mathcal{W}_{\text{pre}})$. Combining inequality \ref{emperical-res-wpre}, \ref{first-exp-minus} and Lemma \ref{lemma:KL-TV-bound}, $L(\theta,\mathcal{W}_{\text{pre}})$ can be bounded by
	\begin{align}
		&\mathbb{E}_{\mu}\left[\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^N\sum_{t=1}^T\mathbb{E}_{E^{k,n}_t}\left[\KL\big(\mathbb{P}(\cdot\mid E^{k,n}_t,w_k) \parallel \mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k)\big)\right]\right]\nonumber\\
		\leq & \frac{2C\log C}{C-1}\cdot \mathbb{E}_{\mu}\left[\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^N\sum_{t=1}^T\mathbb{E}_{E^{k,n}_t}\left[\TV\big(\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k),\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)\big)\right]\right]\nonumber\\
		\leq& \frac{2C\log C}{C-1}\left(\sqrt{\frac{2C^2\cdot \tau_{\min}\log \frac{1}{\delta}}{KNT}}+\sqrt{\frac{2\left(\KL(\mu\parallel \nu)+\operatorname{log}\frac{1}{\delta}\right)}{KNT}-\epsilon_{\text{opt}}}\right) \nonumber\\	
		=&\mathcal{O}\left\{\sqrt{\frac{\log \frac{1}{\delta}}{KNT}}+\sqrt{\frac{\KL(\mu\parallel \nu)+\operatorname{log}\frac{1}{\delta}}{KNT}-\epsilon_{\text{opt}}}\right\}
	\end{align}
    Naturally, to simplify, for given any prefix sequence $P$, we have
    \begin{align}
		&\mathbb{E}_{\mu}\left[\frac{1}{K}\sum_{k=1}^K\mathbb{E}_{P}\left[\KL\big(\mathbb{P}(\cdot\mid P,w_k) \parallel \mathbb{P}_{\theta}(\cdot\mid P,w_k)\big)\right]\right] \nonumber \\
		=&\mathcal{O}\left\{\sqrt{\frac{\log \frac{1}{\delta}}{KNT}}+\sqrt{\frac{1}{KNT}\left(\KL(\mu\parallel \nu)+\operatorname{log}\frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\}
	\end{align}
\end{proof}


\subsubsection{Proof of Theorem \ref{app:pre-gen-data-dependent}}\label{appendix-the-2}
\begin{theorem*}[Data-Dependent and Optimization-Dependent Generalization Bound of the First-Level Expected Loss] Under the conditions maintained in Theorem \ref{app:pre-gen} and Assumption \ref{ass: lipschitz}, when considering data-dependent prior $\mu_J$, for any $0<\delta < 1$, with probability at least $1-\delta$, the first-level expected loss with $K$ topics and infinite sequences per topic, denoted by $L(\theta, \mathcal{W}_{\text{pre}})$ (see in Equation \ref{eq-L-Wpre-two-part-final-main} or Equation \ref{eq-L-W}), satisfies,
	\begin{align*}
			\mathbb{E}_{\mu}\left[L(\theta, \mathcal{W}_{\text{pre}})\right]
			=\mathcal{O}\left\{\sqrt{\frac{ \log 1/\delta}{K(N-N^\prime)T}} + \sqrt{\frac{1}{K(N-N^\prime)T}\left(\KL(\mu\parallel\nu_J)+\log \frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\},
	\end{align*}
	then detailing the term $\KL(\mu \parallel \nu_J)$, $L(\theta, \mathcal{W}_{\text{pre}})$ further satisfies,
	\begin{align}\label{app-the3-right}
			\mathcal{O}\left\{\sqrt{\frac{\log 1/\delta}{K(N-N^\prime)T}}+\sqrt{\frac{1}{K(N-N^\prime)T}\left[\frac{L^2C(\frac{1}{N_{\text{param}}},T^\prime)}{N^\prime}+\log \frac{1}{\delta}\right]-\epsilon_{\text{opt}}}\right\},
	\end{align}
    where $C(\frac{1}{N_{\text{param}}},T^\prime)=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{{T^\prime}}{\exp(8\beta S)}}\right)$. $\epsilon_{\text{opt}}$ is the optimization error (see in Equation \ref{opt}). $K$, $N (N^\prime)$ and $T$ denote the number of topics, the number of sequences per topic and the sequence length utilized in the optimization process of Equation $\ref{eq-L-E}$. $T^\prime$ denotes the total training iterations. $N_{\text{param}}$ denotes the number of model parameters.
\end{theorem*}

\paragraph{Proof sketch of the use of continous mathematical analysis techniques.} We analyse the training dynamic of transformer via Continuous Langevin Dynamics (CLD) which is the continous form of Gradient Langevin Dynamics (GLD). To bound the KL divergence of two distribution, we transform the problem into measuring the KL divergence of pdfs. We first derive the derivative of KL divergence w.r.t. time $t$. This derivative can be decomposed into two parts, corresponding to the time derivatives of the two pdfs, which can be described by the Fokker-Planck Equation. Next, using Log-Sobolev Inequality, we bound the logorithm distance of two pdfs. By solvin the SDE, we obtain an upper bound for the KL divergence. Finally, referring to the proof of Lemma G.5 in \cite{li2019generalization}, we demonstrate that the integral of the gradient difference of $\big\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\big\|_2^2$. Consequently, we get data-dependent and optimization-dependent generalization bound.


\begin{proof}
	In this Theorem, we analysis KL divergence to get data-dependent and optimization algorithm-dependent generalization bound. First, we analyse the training dynamic of transformer via Continuous Langevin Dynamics (CLD),
	$$
	\mathrm{d} \theta_t=-\nabla L_{E_I}(\theta_{t-1}, \mathcal{W}_{\text{pre}})\mathrm{d} t+\sqrt{\beta^{-1}} \mathrm{~d} B_t, \quad \theta_0 \sim \mu_0
	$$
	where $L_{E_I}(\theta, \mathcal{W}_{\text{pre}})=\frac{1}{K(N-N^{\prime})T}\sum_{k=1}^K\sum_{n=1}^{N-N^{\prime}}\sum_{t=1}^T \log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}$, and $B_t$ is the standard brown emotion.
	
	Split pre-training sequences under fixed topic $w_k$ into two parts $E^k_I$ and $E^k_J$ (where $J$ is a random sequence including $N^{\prime}$ indexes uniformly sampled from $[N]$ without replacement and $I$ is $[N]\setminus J$). Under pre-training topics, we have $E_I=\{E^k_I\}_{k=1}^K$ and $E_J=\{E^k_J\}_{k=1}^K$. Assume that the prior distribution of model parameters $\theta$ is depending on the subset $E^k_J$, which is denoted by $\nu_J$ and the posterior distribution of $\theta$ is depending on $E^k_I$ denoted by $\mu$.
	
	Let $\Theta=(\theta_t)_{t\geq 0}$ and $\Theta^\prime=(\theta_t^\prime)_{t\geq 0}$ be the trajectory trained on sequences ${E}^k_I$ and ${E}^k_J$ for fixed topic $w_k$, which are the parallel training process based on the same model architecture. Let $\mu$ and $\nu_J$ be the distribution of $\Theta$ and $\Theta^\prime$ respectively, $p_t$ and $q_t$ be the pdf of $\Theta$ and $\Theta^\prime$ and the total steps of iteration is $T^\prime$. $\KL(\mu \parallel \nu_J)$ is equal to $\KL(p_{T^\prime}\parallel q_{T^\prime})$. To bound $\KL(p_{T^\prime}\parallel q_{T^\prime})$, we first apply Leibniz’s rule and the chain rule on it:
	$$
	\begin{aligned}
		\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t||q_t)
		=&\frac{\mathrm{d}}{\mathrm{d}t}\int_{\mathbb{R}^d}p_t\operatorname{log}\frac{p_t}{q_t}\mathrm{d}\theta \\
		=& \int_{\mathbb{R}^d} (\frac{\mathrm{d}p_t}{\mathrm{d}t}\operatorname{log}\frac{p_t}{q_t}+p_t\cdot \frac{q_t}{p_t}\cdot \frac{\frac{\mathrm{d}p_t}{\mathrm{d}t}q_t-p_t\frac{\mathrm{d}q_t}{\mathrm{d}t}}{q_t^2})\mathrm{d}\theta \\
		% =& \int_{\mathbb{R}^d} \frac{\mathrm{d}p_t}{\mathrm{d}t}\operatorname{log}\frac{p_t}{q_t}\mathrm{d}\theta - \int_{\mathbb{R}^d} \frac{p_t}{q_t}\frac{\mathrm{d}q_t}{p_t}\mathrm{d}\theta + \int_{\mathbb{R}^d} \frac{\mathrm{d}p_t}{\mathrm{d}t}\mathrm{d}\theta \\
		=& \underbrace{\int_{\mathbb{R}^d} \frac{\mathrm{d}p_t}{\mathrm{d}t}\operatorname{log}\frac{p_t}{q_t}\mathrm{d}\theta}_{\text{(A)}} - \underbrace{\int_{\mathbb{R}^d} \frac{p_t}{q_t}\frac{\mathrm{d}q_t}{p_t}\mathrm{d}\theta}_{\text{(B)}},
	\end{aligned}
	$$
	where the last equality follows from that $\int \frac{\mathrm{d}p_t}{\mathrm{d}t}\mathrm{d}\theta = \frac{\mathrm{d}}{\mathrm{d}t} \int p_t\mathrm{d}\theta = 0,$ since $p_t$ is a probability measure.
	By Fokker-Planck Equation for $p_t$, $\frac{\partial p_t}{\partial t} = \frac1\beta\Delta p_t+\nabla \cdot(p_t \nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}}))$.
	
	Then we bound term $A$,
    \small{
	$$
	\begin{aligned}
		\text{A} :=&\int_{\mathbb{R}^d}\left(\frac{\mathrm{d}p_t}{\mathrm{d}t}\log\frac{p_t}{q_t}\right)\mathrm{d}\theta  \\
		=&\int_{\mathbb{R}^d}\left(\frac1\beta\Delta p_t+\nabla \cdot(p_t L_{E_I}(\theta, \mathcal{W}_{\text{pre}}))\right)\log\frac{p_t}{q_t}\mathrm{d}\theta \\
		=&\frac1\beta\left[\int_{\mathbb{R}^d}\Delta p_t\log\frac{p_t}{q_t}\mathrm{d}\theta\right]+\int_{\mathbb{R}^d}\nabla\cdot(p_tL_{E_I}(\theta, \mathcal{W}_{\text{pre}}))\log\frac{p_t}{q_t}\mathrm{d}\theta \\
		=&\frac1\beta\left[\nabla p_t\operatorname{log}\frac{p_t}{q_t}-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle\mathrm{d}\theta\right]\\
        &+\left[p_t\nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}})\log\frac{p_t}{q_t}-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},p_tL_{E_I}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}\theta\right]\\
		=&\frac{-1}\beta\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle\mathrm{d}\theta-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},p_tL_{E_I}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}\theta
	\end{aligned}
	$$
    }
	Bound term $B$, 
	$$
	\begin{aligned}
		\text{B}& :=\int_{\mathbb{R}^d}\left(\frac{p_t}{q_t}\frac{\mathrm{d}q_t}{\mathrm{d}t}\right)\mathrm{d}w  \\
		&=\int_{\mathbb{R}^d}\frac{p_t}{q_t}\left(\frac1\beta\Delta q_t+\nabla\cdot(q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}}))\right)\mathrm{d}w \\
		&=\frac1\beta\left[\int_{\mathbb{R}^d}\frac{p_t}{q_t}\Delta q_t\mathrm{d}w\right]+\int_{\mathbb{R}^d}\frac{p_t}{q_t}\nabla\cdot(q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}}))\mathrm{d}w \\ 
		&=\frac1\beta\left[\frac{p_t}{q_t}\nabla q_t-\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\mathrm{d}w\right]+\left[\frac{p_t}{q_t}q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})-\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}w\right]\\
		&=\frac{-1}\beta\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\mathrm{d}w-\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}w
	\end{aligned}
	$$
	In summary, the deviation of $D_{KL}(p_t||q_t)$ can be bounded,
	$$
	\begin{aligned}
		&\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t||q_t) \\
		=& \frac{-1}\beta\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle\mathrm{d}w-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}w \\
		&+ \frac{1}\beta\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\mathrm{d}w + \int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}w \\
		=& \frac{-1}{\beta}\int_{\mathbb{R}^d}\left(\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle-\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\right)\mathrm{d}w \\
		&- \int_{\mathbb{R}^d}\left(\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}})\rangle-\langle\nabla\frac{p_t}{q_t},q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})\rangle\right)\mathrm{d}w \\
		=& \frac{-1}{\beta}\int_{\mathbb{R}^d}\left(\langle \frac{\nabla p_t}{p_t}-\frac{\nabla q_t}{q_t},\nabla p_t\rangle-\langle\frac{\nabla p_t}{q_t}-\frac{p_t\nabla q_t}{q_t^2},\nabla q_t\rangle\right)\mathrm{d}w \\ & - \int_{\mathbb{R}^d}\left(\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}})\rangle-\frac{p_t}{q_t}\langle\nabla\log\frac{p_t}{q_t},q_t\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})\rangle\right)\mathrm{d}w \\
		=& \frac{-1}{\beta}\int_{\mathbb{R}^d}p_t\big\|\nabla \log \frac{p_t}{q_t}\big\|^2_2 \mathrm{d}w + \int_{\mathbb{R}^d}p_t\langle\nabla \log \frac{p_t}{q_t}, \nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta, \mathcal{W}_{\text{pre}})\rangle\mathrm{d}w
	\end{aligned}
	$$
	Since for any constant $c \neq 0$, vector $\alpha$ and $\beta$, we have the inequality $\langle\frac{\alpha}{\sqrt{c}},\beta\sqrt{c}\rangle \leq \frac{\|\alpha\|^2}{2c}+\frac{c\|\beta\|^2}{2}$, then we can transform the last equality into
	$$
	\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t||q_t) \leq \frac{-1}{2\beta}\int_{\mathbb{R}^d}p_t\big\|\nabla \log \frac{p_t}{q_t}\big\|^2_2 \mathrm{d}w + \frac{\beta}{2}\int_{\mathbb{R}^d} p_t \big\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\big\|^2_2\mathrm{d}w
	$$
	According to Lemma \ref{lemma:LSI} (Log-Sobolev Inequality for CLD) and Assumption \ref{ass:B}, then $\left|L_E(\theta,\mathcal{W}_{\text{pre}})\right|\leq S$ and $\theta_0 \sim \mathcal{N}(0,\frac{1}{\beta}I_d)$, we have $\int_{\mathbb{R}^d}p_t\left\|\nabla\log\frac{p_t}{q_t}\right\|_2^2\mathrm{d}\theta \geq \frac{2\beta}{\exp(8\beta S)}\KL\left(p_t||q_t\right)$. Transform the first term in the right hand with the LSI inequality,
	$$
	\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t||q_t) \leq -\frac{1}{\exp(8\beta S)}\KL\left(p_t||q_t\right)+\frac{\beta}{2}\mathbb{E}_{\theta_t}\left[\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\big\|^2_2\right]
	$$
	Let $\phi(t)=D_{KL}(p_t||q_t)$, $\delta(t)=\frac{\beta}{2}\mathbb{E}_{\theta_t}\left[\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\big\|^2_2\right],$ $\alpha=\frac{1}{\exp(8\beta S)}$, then we get the following difference equation:
	$$
	\phi^{\prime}(t) = -\alpha \phi(t) + \delta(t),\ \phi(0)=0
	$$
	Solve the equation:
	$$
	D_{KL}\left(p_{T^\prime}\parallel q_{T^\prime}\right)\leq\frac\beta2\int_0^{T^\prime}e^\alpha(t-{T^\prime})\mathbb{E}_{\theta_t}\left[\left\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\right\|_2^2\right]\mathrm{d}t,\ \alpha=\frac{1}{\exp(8\beta S)}.
	$$
	Furthermore, in order to get the upper bound of integral in the right hand, we first define that
	$$
	G(J) = \sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^{\alpha(t-{T^\prime})}\left[\big\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\big\|_2^2\right]\mathrm{d}t\right]}
	$$
	
	Let $J$ and $J^{\prime}$ be two neighboring collections, we first prove that $G(J)-G(J^{\prime})$ is small. Let $X_t=\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})$, $Y_t=\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_{J^\prime}}(\theta_t, \mathcal{W}_{\text{pre}})$. Then,
	$$
	\begin{aligned}
		G(J^{\prime})^{2} =&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|X_t+Y_t\right\|_2^2\mathrm{~d}t\right]  \\
		=&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left(X_t^\top X_t+Y_t^\top Y_t\right)\mathrm{d}t\right]+2\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}X_t^\top Y_t\mathrm{d}t\right] \\
		\leq&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left(\|X_t\|_2^2+\|Y_t\|_2^2\right)\mathrm{~d}t\right]\\
		&+2\sqrt{\mathbb{E}_{\theta_t}\left[\int_{0}^{{T^\prime}}e^{\alpha(t-{T^\prime})}\left\|X_{t}\right\|_{2}^{2}\mathrm{d}t\right]}\sqrt{\mathbb{E}_{\theta_t}\left[\int_{0}^{{T^\prime}}e^{\alpha(t-{T^\prime})}\left\|Y_{t}\right\|_{2}^{2}\mathrm{d}t\right]} \\
		=&\left(\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|X_t\right\|_2^2\mathrm{d}t\right]}+\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|Y_t\right\|_2^2\mathrm{d}t\right]}\right)^2 \\
		=&\left(G(J)+\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|Y_t\right\|_2^2\mathrm{d}t\right]}\right)^2
	\end{aligned}
	$$
	For any fixed $J$ and $\theta_t$, under Assumption \ref{ass: lipschitz} that $\left\|\nabla L_{E^{k,n}_t}(\theta_t, \mathcal{W}_{\text{pre}})\right\| \leq L,$ then
	$$
	\begin{aligned}
		\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|Y_t\right\|_2^2\mathrm{d}t& \leq\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\frac{4L^2}{{(KN^\prime)}^2}\mathrm{d}t=\frac{4L^2(1-e^{-\alpha {T^\prime}})}{{(KN^\prime)}^2\alpha}
	\end{aligned}
	$$
	Then,
	$$
	\left|G(J)-G(J^{\prime})\right| \leq  \frac{2L}{KN^\prime} \sqrt{\frac{1-e^{-\alpha T}}{\alpha}}
	$$
	Applying Lemma \ref{lemma:mc-data-dependent} of concentration inequality and there are $N^\prime$ indexes in $J$ or $J^\prime$,
	$$
	\begin{aligned}
		P_J\left[G(J)-\mathbb{E}_J[G(J)]\geq \epsilon\right]\leq \exp\left(\frac{-2\epsilon^2}{N^\prime \frac{4L^2(1-e^{-\alpha {T^\prime}})}{{(KN^\prime)}^2\alpha}}\right)=\exp\left(\frac{-K^2N^\prime\alpha\epsilon^2}{2L^2(1-e^{-\alpha {T^\prime}})}\right) \\
	\end{aligned}
	$$
	We also have,
	$$
	\begin{aligned}
		P_J\left[G(J)^2 \geq (\mathbb{E}_J[G(J)]+\epsilon)^2\right]\leq \exp\left(\frac{-K^2N^\prime\alpha\epsilon^2}{2L^2(1-e^{-\alpha {T^\prime}})}\right)
	\end{aligned}
	$$
	Then referring to \cite{li2019generalization}, we can  easily get the upper bound of variance of $\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})$ which is $\mathbb{E}_J\left[\left\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\right\|_2^2\right] \leq \frac{4L^2}{N^\prime}$, thus
	$$
	\begin{aligned}
		\mathbb{E}_{J}[G(J)]& =\mathbb{E}_J\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^{\alpha(t-{T^\prime})}\left[\left\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\right\|_2^2\right]\mathrm{d}t\right]}  \\
		&\leq\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^{\alpha(t-{T^\prime})}\mathbb{E}_J\left[\left\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\right\|_2^2\right]\mathrm{d}t\right]} \\
		&\leq\sqrt{\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\frac{4L^2}{N^\prime}} \\
		&=\frac{2L}{\sqrt{N^\prime}}\sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}
	\end{aligned}
	$$
	Let $\exp\left(\frac{-K^2N^\prime\alpha\epsilon^2}{2L^2(1-e^{-\alpha {T^\prime}})}\right)={\delta}$, then $\epsilon =\sqrt{\frac{ 2L^2(1-e^{-\alpha {T^\prime}}) \log \frac{1}{\delta}}{K^2N^\prime\alpha}}$. It follows that with probability at least $1-\delta$
	$$
	G(J)^2 \leq \left(\frac{2L}{\sqrt{N^\prime}}\sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}+\sqrt{\frac{ 2L^2(1-e^{-\alpha {T^\prime}}) \log \frac{1}{\delta}}{K^2N^\prime\alpha}}\right)^2
	$$
	Then,
	$$
	\begin{aligned}
		&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^\alpha(t-{T^\prime})\left[\left\|\nabla L_{E_I}(\theta_t, \mathcal{W}_{\text{pre}})-\nabla L_{E_J}(\theta_t, \mathcal{W}_{\text{pre}})\right\|_2^2\right]\mathrm{d}t\right] \\
		\leq& \left(\frac{2L}{\sqrt{N^\prime}}\sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}+\sqrt{\frac{ 2L^2(1-e^{-\alpha {T^\prime}}) \log \frac{1}{\delta}}{K^2N^\prime\alpha}}\right)^2 \\
		=& \frac{4L^2}{N^\prime}\left(1+\sqrt{\frac{\log \frac{1}{\delta}}{2K^2}}\right)^2 \frac{(1-e^{-\alpha {T^\prime}})}{\alpha} \\
		=& \frac{4L^2}{N^\prime}\left(1+\sqrt{\frac{\log \frac{1}{\delta}}{2K^2}}\right)^2 e^{8\beta S}\left(1-\exp\left(-\frac{ {T^\prime}}{e^{8\beta S}}\right)\right)
	\end{aligned}
	$$
	We bound the KL-divergence.
	\begin{align}
		\label{data-dependent-KL}
		D_{\operatorname{KL}}\left(p_{T^\prime}\mid\mid q_{T^\prime}\right)
		&\leq\left(1+\sqrt{\frac{\log \frac{1}{\delta}}{2K^2}}\right)^2 \frac{2L^2\beta e^{8\beta S}(1-\exp(-\frac{ {T^\prime}}{e^{8\beta S}}))}{N^\prime}\\
		&=\left(1+\sqrt{\frac{\log \frac{1}{\delta}}{2K^2}}\right)^2 \frac{4L^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{N^\prime}
	\end{align}
	where $C(\frac{1}{N_{\text{param}}},{T^\prime})=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{ {T^\prime}}{\exp(8\beta S)}}\right)$.
	
	As introduced before, the prior distribution of model parameters $\theta$ is depending on the subset $E^k_J$, which is denoted by $\nu_J$ and the posterior distribution of $\theta$ is depending on $E^k_I$ denoted by $\mu$. Then Theorem \ref{app:pre-gen} can be transformed to (modify $N$ to $N-N^\prime$)
	\begin{multline}\label{data-depenent-1}
		\mathbb{E}_{\mu}\left[\frac{1}{K}\sum_{k=1}^K\mathbb{E}_{P}\left[\KL\big(\mathbb{P}(\cdot\mid P,w_k)\parallel \mathbb{P}_{\theta}(\cdot\mid P,w_k)\big)\right]\right] \\
		=\mathcal{O}\left\{\sqrt{\frac{\log \frac{1}{\delta}}{K(N-N^\prime)T}}+\sqrt{\frac{\KL(\mu\parallel \nu_J)+\operatorname{log}\frac{1}{\delta}}{K(N-N^\prime)T}-\epsilon_{\text{opt}}}\right\}
	\end{multline}
	
	Finally, with inequality \ref{data-depenent-1} and  \ref{data-dependent-KL}, we get data-dependent and optimization algorithm-dependent PAC-Bayesian generalization error bound of the first-level expected loss.
	{\small \begin{align}
		&\mathbb{E}_{\mu}\left[\frac{1}{K}\sum_{k=1}^K\mathbb{E}_{P}\left[\KL\big(\mathbb{P}(\cdot\mid P,w_k)\parallel \mathbb{P}_{\theta}(\cdot\mid P,w_k)\big)\right]\right]\nonumber \\
		=&\mathcal{O}\left\{\sqrt{\frac{\log 1/\delta}{K(N-N^\prime)T}}+\sqrt{\frac{1}{K(N-N^\prime)T}\left[\left(1+\sqrt{\frac{\log 1/\delta}{K^2}}\right)^2 \frac{4L^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{N^\prime}+\log \frac{1}{\delta}\right]-\epsilon_{\text{opt}}}\right\} \nonumber\\
        =&\mathcal{O}\left\{\sqrt{\frac{\log 1/\delta}{K(N-N^\prime)T}}+\sqrt{\frac{1}{K(N-N^\prime)T}\left(\frac{L^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{N^\prime}+\log \frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\}
	\end{align}}

	where $C(\frac{1}{N_{\text{param}}},{T^\prime})=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{{T^\prime}}{\exp(8\beta S)}}\right)$.
\end{proof}

\subsection{Generalization of Sequences and Topics: Two-Level Expectation}
%分析ICL能力如何产生：它来自于LLM在主题层面的泛化能力，即：需要再考虑主题分布的期望
\subsubsection{Proof of Theorem \ref{app:ICL-gen}}\label{appendix-the-3}

\begin{theorem*}[Data-Dependent and Optimization-Dependent Generalization Bound of the Two-Level Expected Loss] Let the auto-regressive LLM $\mathbb{P}_\theta$ be the empirical solution of Equation $\ref{eq-L-E}$, and $\mathbb{P}(\cdot\mid w)$ is the true data distribution under topic $w$. Under Assumptions \ref{ass:B}, \ref{ass: lipschitz} and \ref{ass: lipschitz-2}, for any $0<\delta < 1$, with probability at least $1-\delta$, the two-level expected loss (population loss) with infinite topics and infinite sequences per topic, denoted by $L(\theta)$ (see in Equation \ref{eq-L-ICL-final}), satisfies,
		\begin{align*}
			\mathbb{E}_{\mu}[L(\theta)]
			=\mathcal{O}\biggl\{\sqrt{\frac{1}{KT_p}}\left(\KL(\mu\parallel \nu)+\log \frac{1}{\delta}\right)+U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\biggr\},
		\end{align*}
	where $U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)$ denotes the right hand of equality \ref{the3-right} or equality \ref{app-the3-right}. $\mu$ and $\nu$ are the posterior and prior distribution of model parameters $\theta$, respectively. $K$, $N (N^\prime)$ and $T$ denote the number of topics, the number of sequences per topic and the sequence length utilized in the optimization process of Equation $\ref{eq-L-E}$.
\end{theorem*}

\begin{proof}
	In this part, since we have gotten the generalization error bound when considering infinite sequences in Theorem \ref{app:pre-gen} and Theorem \ref{app:pre-gen-data-dependent}. Our analysis is based on that there will be a sufficient number of sequences for each topic to enable thorough learning so that in the ideal case, the well-pretrained model can perform excellently on the seen topics. We try to get the upper bound of the two-level expected loss (population loss) so that the pre-trained model can also perform well on the unseen topics under the assumption of topic distribution. 

    For an ICL prompt $\text{prompt}$, we also establish auto-regressive loss based on the prefix sequence $\text{prompt}_{t}$. Then according to Theorem \ref{ICL-gen-topic-dependent}, for topic $w$, we first have
    {\small \begin{align}
		&\mathbb{E}_{\mu}\left[\frac{1}{KT_p}\sum_{k=1}^K\sum_{t=1}^{T_p}\mathbb{E}_{\text{prompt}_{t}}\left[\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_{t},w_k)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_{t},w_k)\big)\right]\right] \nonumber\\
		&=\mathcal{O}\left\{\sqrt{\frac{\log 1/\delta}{K(N-N^\prime)T}}+\sqrt{\frac{1}{K(N-N^\prime)T}\left(\frac{L^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{N^\prime}+\log \frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\}\nonumber \\
        &=\mathcal{O}\left\{U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\}
	\end{align}}
    
	Using Proposition \ref{prop0: high-prob} and Assumption \ref{ass:B} of $\log \frac{\mathbb{P}(\cdot\mid E^{k,n}_t,w_k)}{\mathbb{P}_{\theta}(\cdot\mid E^{k,n}_t,w_k)}$ is upper bounded by $C$, thus with probability at least $1-\delta$, we consider the generalization of topic so that ICL emerges,
	{\small 
		\begin{multline}
			\mathbb{E}_{\mu}\biggl[\frac{1}{T_p}\sum_{t=1}^{T_p}\mathbb{E}_{w}\mathbb{E}_{\text{prompt}_{t}}\left[\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_{t},w)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_{t},w)\big)\right] \\-\frac{1}{KT_p}\sum_{k=1}^K\sum_{t=1}^{T_p}\mathbb{E}_{\text{prompt}_{t}}\left(\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_{t},w_k)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_{t},w_k)\big)\right)\biggr] \\
			\leq \sqrt{\frac{C^2\cdot \tau_{\min}}{2KT_p \log 2}}\left(\KL(\mu\parallel \nu)+\log \frac{2}{\delta}\right)=\mathcal{O}\left\{\sqrt{\frac{1}{KT_p}}\left[\KL(\mu\parallel \nu)+\log \frac{1}{\delta}\right]\right\}
		\end{multline}
	}
	
	Finally, we measure the generalization error of an auto-regressive pre-trained LLM, after which the ability of ICL will emerge with good generalization. It can be denoted as $\text{gen}_\text{topic}=L(\theta)-L(\theta,\mathcal{W}_{\text{pre}})$, then the two-level expected loss (population loss) $L(\theta)$ can be bounded by
	\begin{multline}
		\mathbb{E}_{\mu}\left[\frac{1}{T_p}\sum_{t=1}^{T_p}\mathbb{E}_{w}\mathbb{E}_{\text{prompt}_{t}}\left[\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_{t},w)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_{t},w)\big)\right] \right]\\
		=\mathcal{O}\left\{\sqrt{\frac{1}{KT_p}}\left[\KL(\mu\parallel \nu)+\log \frac{1}{\delta}\right]+U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\}
	\end{multline}
	where $U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)$ is the right hand of inequality \ref{the3-right}.
\end{proof}

\subsubsection{Proof of Theorem \ref{app:ICL-gen-topic-dependent}}\label{appendix-the-4}

\begin{theorem*}[Data-Dependent, Topic-Dependent and Optimization-Dependent Generalization Error Bound of the Two-Level Expected Loss.] Under the conditions maintained in Theorem \ref{app:ICL-gen} and Assumption \ref{ass: lipschitz-2}, when further considering topic-dependent prior, for any $0<\delta < 1$, with probability at least $1-\delta$, the two-level expected loss (population loss) with infinite topics and infinite sequences per topic, denoted by $L(\theta)$ (see in Equation \ref{eq-L-ICL-final}), satisfies,
    \begin{align*}
        \mathbb{E}_{\mu}\left[L(\theta) \right]
        =\mathcal{O}\biggl\{\sqrt{\frac{1}{(K-K^\prime)T_p}}\left(\KL(\mu\parallel \nu_J)+\log \frac{1}{\delta}\right)+ U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\biggr\},
    \end{align*}
	then detailing the term $\KL(\mu \parallel \nu_J)$, $L(\theta)$ further satisfies,
	\begin{align}
			\mathcal{O}\biggl\{\sqrt{\frac{1}{(K-K^\prime)T}}\left(\frac{\sigma^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{K^\prime}+\log \frac{1}{\delta}\right)
			+ U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\biggr\}, \nonumber
	\end{align}
	where $C(\frac{1}{N_{\text{param}}},T^\prime)=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{ T^\prime}{\exp(8\beta S)}}\right)$, $U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)$ denotes the right hand of equality \ref{the3-right} or equality \ref{app-theF3}. $\mu$ and $\nu_J$ are the posterior and topic-dependent prior distribution of model parameters $\theta$, respectively. $K (K^\prime)$, $N (N^\prime)$ and $T$ denote the number of topics, the number of sequences per topic and the sequence length utilized in the optimization process of Equation $\ref{eq-L-E}$. $T^\prime$ denotes the total training iterations. $N_{\text{param}}$ denotes the number of model parameters.
\end{theorem*}

\begin{proof}
	In this Theorem, we try to give a detail analysis of $\KL(\mu \parallel \nu)$ to get data-dependent, topic-dependent and optimization-dependent generalization bound. Similarly, we analyze the training dynamic of transformer via Gradient Langevin Dynamics (GLD)
	$$
	\theta_t \leftarrow {\theta}_{t-1} - \eta_t \nabla L(\theta_{t-1}, \mathcal{W}_{\text{pre},I})+\sigma_t\mathcal{N}(0,I_d).
	$$
	when the step size approaches zero,
	$$
	\mathrm{d} \theta_t=- \nabla L(\theta_{t-1}, \mathcal{W}_{\text{pre},I})\mathrm{d} t+\sqrt{ \beta^{-1}} \mathrm{~d} B_t, \quad \theta_0 \sim \mu_0
	$$
	where $\nabla L(\theta, \mathcal{W}_{\text{pre},I})=\frac{1}{(K-K^{\prime})T}\sum_{k=1}^{K-K^{\prime}}\sum_{t=1}^T\mathbb{E}_{E^{k,n}_t} [\log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}]$, and $B_t$ is the standard brown emotion.
	
	Split pre-training topics into two parts $\mathcal{W}_{\text{pre},I}$ and $\mathcal{W}_{\text{pre},J}$ (where $J$ is a random sequence including $K^{\prime}$ indexes uniformly sampled from $[K]$ without replacement and $I$ is $[K]\setminus J$). Then the total sequences are divided into $E^I=\{E^k\}_{k \in \mathcal{W}_{\text{pre},I}}$ and $E^J=\{E^k\}_{k \in \mathcal{W}_{\text{pre},J}}$. Assume that the prior distribution of model parameters $\theta$ is depending on the topic subset $E^J$, which is denoted by $\nu_J$ and the posterior distribution of $\theta$ is depending on $E^I$ denoted by $\mu$.
	
	Let $\widetilde{\Theta}=(\theta_t)_{t\geq 0}$ and $\widetilde{\Theta}^\prime=(\theta_t^\prime)_{t\geq 0}$ be the trajectory trained on $\mathcal{W}_{\text{pre},I}$ and $\mathcal{W}_{\text{pre},J}$ (the total sequences are divided into $E^I=\{E^k\}_{k \in \mathcal{W}_{\text{pre},I}}$ and $E^J=\{E^k\}_{k \in \mathcal{W}_{\text{pre},J}}$). Let $\mu$ and $\nu_J$ be the distribution of $\widetilde{\Theta}$ and $\widetilde{\Theta}^\prime$ respectively, $p_t$ and $q_t$ be the pdf of $\widetilde{\Theta}$ and $\widetilde{\Theta}^\prime$ and the total steps of iteration is $T^\prime$. $\KL(\rho \parallel \pi_J)$ is equal to $\KL(p_{T^\prime}\parallel q_{T^\prime})$. To bound $\KL(p_{T^\prime}||q_{T^\prime})$, we first apply Leibniz’s rule and the chain rule on it:
	$$
	\begin{aligned}
		\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t||q_t)
		=&\frac{\mathrm{d}}{\mathrm{d}t}\int_{\mathbb{R}^d}p_t\operatorname{log}\frac{p_t}{q_t}\mathrm{d}\theta \\
		=& \int_{\mathbb{R}^d} (\frac{\mathrm{d}p_t}{\mathrm{d}t}\operatorname{log}\frac{p_t}{q_t}+p_t\cdot \frac{q_t}{p_t}\cdot \frac{\frac{\mathrm{d}p_t}{\mathrm{d}t}q_t-p_t\frac{\mathrm{d}q_t}{\mathrm{d}t}}{q_t^2})\mathrm{d}\theta \\
		=& \int_{\mathbb{R}^d} \frac{\mathrm{d}p_t}{\mathrm{d}t}\operatorname{log}\frac{p_t}{q_t}\mathrm{d}\theta - \int_{\mathbb{R}^d} \frac{p_t}{q_t}\frac{\mathrm{d}q_t}{p_t}\mathrm{d}\theta + \int_{\mathbb{R}^d} \frac{\mathrm{d}p_t}{\mathrm{d}t}\mathrm{d}\theta \\
		=& \underbrace{\int_{\mathbb{R}^d} \frac{\mathrm{d}p_t}{\mathrm{d}t}\operatorname{log}\frac{p_t}{q_t}\mathrm{d}\theta}_{\text{(A)}} - \underbrace{\int_{\mathbb{R}^d} \frac{p_t}{q_t}\frac{\mathrm{d}q_t}{p_t}\mathrm{d}\theta}_{\text{(B)}},
	\end{aligned}
	$$
	where the last equality follows from that $\int \frac{\mathrm{d}p_t}{\mathrm{d}t}\mathrm{d}\theta = \frac{\mathrm{d}}{\mathrm{d}t} \int p_t\mathrm{d}\theta = 0,$ since $p_t$ is a probability measure.
	By Fokker-Planck Equation for $p_t$, $\frac{\partial p_t}{\partial t} = \frac1\beta\Delta p_t+\nabla \cdot(p_t \nabla L(\theta, \mathcal{W}_{\text{pre},I}))$.
	
	Then we bound term $A$,
	$$
	\begin{aligned}
		\text{A} :=&\int_{\mathbb{R}^d}\left(\frac{\mathrm{d}p_t}{\mathrm{d}t}\log\frac{p_t}{q_t}\right)\mathrm{d}\theta  \\
		=&\int_{\mathbb{R}^d}\left(\frac1\beta\Delta p_t+\nabla \cdot(p_t \nabla L(\theta, \mathcal{W}_{\text{pre},I}))\right)\log\frac{p_t}{q_t}\mathrm{d}\theta \\
		=&\frac1\beta\left[\int_{\mathbb{R}^d}\Delta p_t\log\frac{p_t}{q_t}\mathrm{d}\theta\right]+\int_{\mathbb{R}^d}\nabla\cdot(p_t\nabla L(\theta, \mathcal{W}_{\text{pre},I}))\log\frac{p_t}{q_t}\mathrm{d}\theta \\
		=&\frac1\beta\left[\nabla p_t\operatorname{log}\frac{p_t}{q_t}-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle\mathrm{d}\theta\right]\\
		&+\left[p_t \nabla L(\theta, \mathcal{W}_{\text{pre},I})\log\frac{p_t}{q_t}-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L(\theta, \mathcal{W}_{\text{pre},I})\rangle\mathrm{d}\theta\right]\\
		=&\frac{-1}\beta\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle\mathrm{d}\theta-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L(\theta, \mathcal{W}_{\text{pre},I})\rangle\mathrm{d}\theta
	\end{aligned}
	$$
	Bound term $B$, 
	$$
	\begin{aligned}
		\text{B}& :=\int_{\mathbb{R}^d}\left(\frac{p_t}{q_t}\frac{\mathrm{d}q_t}{\mathrm{d}t}\right)\mathrm{d}w  \\
		&=\int_{\mathbb{R}^d}\frac{p_t}{q_t}\left(\frac1\beta\Delta q_t+\nabla\cdot(q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J}))\right)\mathrm{d}w \\
		&=\frac1\beta\left[\int_{\mathbb{R}^d}\frac{p_t}{q_t}\Delta q_t\mathrm{d}w\right]+\int_{\mathbb{R}^d}\frac{p_t}{q_t}\nabla\cdot(q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J}))\mathrm{d}w \\ 
		&=\frac1\beta\left[\frac{p_t}{q_t}\nabla q_t-\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\mathrm{d}w\right]+\left[\frac{p_t}{q_t}q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J})-\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J})\rangle\mathrm{d}w\right]\\
		&=\frac{-1}\beta\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\mathrm{d}w-\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J})\rangle\mathrm{d}w
	\end{aligned}
	$$
	In summary, the deviation of $D_{KL}(p_t||q_t)$ can be bounded,
	$$
	\begin{aligned}
		&\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t \parallel q_t) \\
		=& \frac{-1}\beta\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle\mathrm{d}w-\int_{\mathbb{R}^d}\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L(\theta, \mathcal{W}_{\text{pre},I})\rangle\mathrm{d}w\\
		&+ \frac{1}\beta\int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\mathrm{d}w + \int_{\mathbb{R}^d}\langle\nabla\frac{p_t}{q_t},q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J})\rangle\mathrm{d}w \\
		=& \frac{-1}{\beta}\int_{\mathbb{R}^d}\left(\langle\nabla\log\frac{p_t}{q_t},\nabla p_t\rangle-\langle\nabla\frac{p_t}{q_t},\nabla q_t\rangle\right)\mathrm{d}w\\
		&- \int_{\mathbb{R}^d}\left(\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L_{E_I}(\theta, \mathcal{W}_{\text{pre}})\rangle-\langle\nabla\frac{p_t}{q_t},q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J})\rangle\right)\mathrm{d}w \\
		=& \frac{-1}{\beta}\int_{\mathbb{R}^d}\left(\langle \frac{\nabla p_t}{p_t}-\frac{\nabla q_t}{q_t},\nabla p_t\rangle-\langle\frac{\nabla p_t}{q_t}-\frac{p_t\nabla q_t}{q_t^2},\nabla q_t\rangle\right)\mathrm{d}w \\ & - \int_{\mathbb{R}^d}\left(\langle\nabla\log\frac{p_t}{q_t},p_t\nabla L(\theta, \mathcal{W}_{\text{pre},I})\rangle-\frac{p_t}{q_t}\langle\nabla\log\frac{p_t}{q_t},q_t\nabla L(\theta, \mathcal{W}_{\text{pre},J})\rangle\right)\mathrm{d}w \\
		=& \frac{-1}{\beta}\int_{\mathbb{R}^d}p_t\big\|\nabla \log \frac{p_t}{q_t}\big\|^2_2 \mathrm{d}w + \int_{\mathbb{R}^d}p_t\langle\nabla \log \frac{p_t}{q_t}, \nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})\rangle\mathrm{d}w
	\end{aligned}
	$$
	Since for any constant $c \neq 0$, vector $\alpha$ and $\beta$, we have the inequality $\langle\frac{\alpha}{\sqrt{c}},\frac{\beta}{\sqrt{c}}\rangle \leq \frac{\|\alpha\|^2}{2c}+\frac{c\|\beta\|^2}{2}$, then we can transform the last equality into
	$$
	\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t \parallel q_t) \leq \frac{-1}{2\beta}\int_{\mathbb{R}^d}p_t\big\|\nabla \log \frac{p_t}{q_t}\big\|^2_2 \mathrm{d}w + \frac{\beta}{2}\int_{\mathbb{R}^d} p_t \big\|\nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})\big\|^2_2\mathrm{d}w
	$$
	According to Lemma \ref{lemma:LSI}, we have $\int_{\mathbb{R}^d}p_t\left\|\nabla\log\frac{p_t}{q_t}\right\|_2^2\mathrm{d}w \geq \frac{2\beta}{\exp(8\beta S)}\KL\left(p_t||q_t\right)$, then transform the first term in the right hand of the above inequality,
	$$
	\frac{\mathrm{d}}{\mathrm{d}t}D_{KL}(p_t||q_t) \leq -\frac{1}{\exp(8\beta S)}\KL\left(p_t||q_t\right)+\frac{\beta}{2}\mathbb{E}_{\theta_t}\left[\big\|\nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})\big\|^2_2\right]
	$$
	Let $\phi(t)=D_{KL}(p_t||q_t)$, $\delta(t)=\frac{\beta}{2}\mathbb{E}_{\theta_t}\left[\big\|\nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})\big\|^2_2\right],$ $\alpha=\frac{1}{\exp(8\beta S)}$, then we get the following difference equation:
	$$
	\phi^{\prime}(t) = -\alpha \phi(t) + \delta(t),\ \phi(0)=0
	$$
	Solve the equation:
	\small{
    $$
	D_{KL}\left(p_{T^\prime}\mid\mid q_{T^\prime}\right)\leq\frac\beta2\int_0^{T^\prime}e^\alpha(t-{T^\prime})\mathbb{E}_{\theta_t}\left[\big\|\nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})\big\|_2^2\right]\mathrm{d}t,\ \alpha=\frac{1}{\exp(8\beta S)}.
	$$
    }
	We first define that
	$$
	G(J) = \sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^\alpha(t-{T^\prime})\left[\big\|\nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})\big\|_2^2\right]\mathrm{d}t\right]}
	$$
	Let $J$ and $J^{\prime}$ be two neighboring collections, we first prove that $G(J)-G(J^{\prime})$ is small. Let $X_t=\nabla L(\theta, \mathcal{W}_{\text{pre},I})-\nabla L(\theta, \mathcal{W}_{\text{pre},J})$, $Y_t=\nabla L(\theta, \mathcal{W}_{\text{pre},J})-\nabla L(\theta, \mathcal{W}_{\text{pre},J^{\prime}})$. Then,
	$$
	\begin{aligned}
		G(J^{\prime})^{2} =&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|X_t+Y_t\right\|_2^2\mathrm{~d}t\right]  \\
		=&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left(X_t^\top X_t+Y_t^\top Y_t\right)\mathrm{d}t\right]+2\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}X_t^\top Y_t\mathrm{d}t\right] \\
		\leq&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left(\|X_t\|_2^2+\|Y_t\|_2^2\right)\mathrm{~d}t\right]\\
		&+2\sqrt{\mathbb{E}_{\theta_t}\left[\int_{0}^{{T^\prime}}e^{\alpha(t-{T^\prime})}\left\|X_{t}\right\|_{2}^{2}\mathrm{d}t\right]}\sqrt{\mathbb{E}_{\theta_t}\left[\int_{0}^{{T^\prime}}e^{\alpha(t-{T^\prime})}\left\|Y_{t}\right\|_{2}^{2}\mathrm{d}t\right]} \\
		=&\left(\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|X_t\right\|_2^2\mathrm{d}t\right]}+\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|Y_t\right\|_2^2\mathrm{d}t\right]}\right)^2 \\
		=&\left(G(J)+\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|Y_t\right\|_2^2\mathrm{d}t\right]}\right)^2
	\end{aligned}
	$$
	For any fixed $J$ and $\theta_t$, using the Assumption \ref{ass: lipschitz-2} that $\left\|\nabla L(\theta_t, w_k)\right\| \leq \sigma,$ then
	$$
	\begin{aligned}
		\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\left\|Y_t\right\|_2^2\mathrm{d}t& \leq\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\frac{4\sigma^2}{{K^\prime}^2}\mathrm{d}t=\frac{4\sigma^2(1-e^{-\alpha {T^\prime}})}{{K^\prime}^2\alpha}
	\end{aligned}
	$$
	Then,
	$$
	\left|G(J)-G(J^{\prime})\right| \leq  \frac{2\sigma}{K^\prime} \sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}
	$$
	Applying lemma of concentration inequality and there are $K^\prime$ indexes in $J$ or $J^\prime$,
	% $$
	% P_J\left[G(J)-\mathbb{E}_J G(J)\geq \epsilon\right]\leq \exp\left(\frac{-2\epsilon^2}{mc^2}\right), \left|G(J)-G(J^{\prime})\right| \leq c.
	% $$
	$$
	\begin{aligned}
		P_J\left[G(J)-\mathbb{E}_J[G(J)]\geq \epsilon\right]\leq \exp\left(\frac{-2\epsilon^2}{K^\prime \frac{4\sigma^2(1-e^{-\alpha {T^\prime}})}{{K^\prime}^2\alpha}}\right)=\exp\left(\frac{-K^\prime\alpha\epsilon^2}{2\sigma^2(1-e^{-\alpha {T^\prime}})}\right) \\
	\end{aligned}
	$$
	We also have,
	$$
	\begin{aligned}
		P_J\left[G(J)^2 \geq (\mathbb{E}_J[G(J)]+\epsilon)^2\right]\leq \exp\left(\frac{-K^\prime\alpha\epsilon^2}{2\sigma^2(1-e^{-\alpha {T^\prime}})}\right)
	\end{aligned}
	$$
	then
	$$
	\begin{aligned}
		\mathbb{E}_{J}[G(J)]& =\mathbb{E}_J\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^\alpha(t-{T^\prime})\left[\left\|\nabla L(\theta_t, \mathcal{W}_{\text{pre},I})-\nabla L(\theta_t, \mathcal{W}_{\text{pre},J})\right\|_2^2\right]\mathrm{d}t\right]}  \\
		&\leq\sqrt{\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^\alpha(t-{T^\prime})\mathbb{E}_J\left[\left\|\nabla L(\theta_t, \mathcal{W}_{\text{pre},I})-\nabla L(\theta_t, \mathcal{W}_{\text{pre},J})\right\|_2^2\right]\mathrm{d}t\right]} \\
		&\leq\sqrt{\int_0^{T^\prime}e^{\alpha(t-{T^\prime})}\frac{4\sigma^2}{K^\prime}} \\
		&=\frac{2\sigma}{\sqrt{K^\prime}}\sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}
	\end{aligned}
	$$
	Let $\exp\left(\frac{-K^\prime\alpha\epsilon^2}{2\sigma^2(1-e^{-\alpha {T^\prime}})}\right)={\delta}$, then $\epsilon =\sqrt{\frac{ 4\sigma^2(1-e^{-\alpha {T^\prime}}) \log \frac{1}{\delta}}{2K^\prime\alpha}}$. It follows that with probability at least $1-\delta$
	$$
	G(J)^2 \leq \left(\frac{2\sigma}{\sqrt{K^\prime}}\sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}+\sqrt{\frac{ 4\sigma^2(1-e^{-\alpha {T^\prime}}) \log \frac{1}{\delta}}{2K^\prime\alpha}}\right)^2
	$$
	Then,
	$$
	\begin{aligned}
		&\mathbb{E}_{\theta_t}\left[\int_0^{T^\prime} e^\alpha(t-{T^\prime})\left[\left\|\nabla L(\theta_t, \mathcal{W}_{\text{pre},I})-\nabla L(\theta_t, \mathcal{W}_{\text{pre},J})\right\|_2^2\right]\mathrm{d}t\right] \\
		\leq& \left(\frac{2\sigma}{\sqrt{K^\prime}}\sqrt{\frac{1-e^{-\alpha {T^\prime}}}{\alpha}}+\sqrt{\frac{ 4\sigma^2(1-e^{-\alpha {T^\prime}}) \log \frac{1}{\delta}}{2K^\prime\alpha}}\right)^2 \\
		=& \frac{4\sigma^2}{K^\prime}\left(1+\sqrt{\log \frac{1}{\delta}}\right)^2 \frac{(1-e^{-\alpha {T^\prime}})}{\alpha} \\
		=& \frac{4\sigma^2}{K^\prime}\left(1+\sqrt{\log \frac{1}{\delta}}\right)^2 e^{8\beta S}\left(1-\exp\left(-\frac{ {T^\prime}}{e^{8\beta S}}\right)\right)
	\end{aligned}
	$$
	We bound the KL-divergence.
	{\small
		\begin{equation}
			\label{topic-dependent-KL}
			D_{\operatorname{KL}}\left(p_{T^\prime}\mid\mid q_{T^\prime}\right)\leq\left(1+\sqrt{\log \frac{1}{\delta}}\right)^2 \frac{2\sigma^2\beta e^{8\beta S}(1-\exp(-\frac{ {T^\prime}}{e^{8\beta S}}))}{K^\prime}=\left(1+\sqrt{\log \frac{1}{\delta}}\right)^2 \frac{4\sigma^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{K^\prime}
		\end{equation}
	}
	where $C(\frac{1}{N_{\text{param}}},{T^\prime})=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{ {T^\prime}}{\exp(8\beta S)}}\right)$.
	
	As introduced before, the prior distribution of model parameters $\theta$ is depending on the subset $E^J$, which is denoted by $\nu_J$ and the posterior distribution of $\theta$ is depending on $E^I$ denoted by $\mu$. Then Theorem \ref{app:ICL-gen} can be slightly changed.
	{\small 
		\begin{align}
			&\mathbb{E}_{\mu}\left[\frac{1}{T_p}\sum_{t=1}^{T_p}\mathbb{E}_{w}\mathbb{E}_{\text{prompt}_{t}}\left[\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_{t},w)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_{t},w)\big)\right]\right]\nonumber\\
			\leq& \sqrt{\frac{C^2\cdot \tau_{\min}}{2(K-K^\prime)T_p \log 2}}\left(\KL(\mu\parallel \nu)+\log \frac{2}{\delta}\right)\\
			&+\mathbb{E}_\mu\left[\frac{1}{(K-K^\prime)T_p}\sum_{k=1}^{K-K^\prime}\sum_{t=1}^{T_p}\mathbb{E}_{\text{prompt}_{t}}\left[\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_{t},w_k)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_{t},w_k)\big)\right]\right]\nonumber\\
			=&\mathcal{O}\left\{\sqrt{\frac{1}{(K-K^\prime)T_p}}\left(\KL(\mu\parallel \nu)+\log \frac{1}{\delta}\right)+ U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\} \label{topic-depenent-1}
		\end{align}
	}
	
	Finally, with inequality \ref{topic-dependent-KL} and \ref{topic-depenent-1}, we get data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bound of the two-level expected loss, \emph{i.e.}, $L(\theta)$ is bounded by
	\begin{align}
		&\mathbb{E}_{\mu}\left[\frac{1}{T_p}\sum_{t=1}^{T_p}\mathbb{E}_{w}\mathbb{E}_{\text{prompt}_t}\left[\KL\big(\mathbb{P}(\cdot\mid \text{prompt}_t,w_k)\parallel \mathbb{P}_{\theta}(\cdot\mid \text{prompt}_t,w_k)\big)\right]\right]\nonumber\\
		&=\mathcal{O}\left\{\sqrt{\frac{1}{(K-K^\prime)T_p}}\left[\left(1+\sqrt{\log \frac{1}{\delta}}\right)^2 \frac{4\sigma^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{K^\prime}+\log \frac{1}{\delta}\right]+ U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\}\nonumber\\
        &=\mathcal{O}\left\{\sqrt{\frac{1}{(K-K^\prime)T_p}}\left(\frac{\sigma^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{K^\prime}+\log \frac{1}{\delta}\right)+ U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\}
	\end{align}
	where $C(\frac{1}{N_{\text{param}}},T^\prime)=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{ T^\prime}{\exp(8\beta S)}}\right)$.
\end{proof}