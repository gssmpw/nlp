[
  {
    "index": 0,
    "papers": [
      {
        "key": "akyurek2022learning",
        "author": "Aky{\\\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny",
        "title": "What learning algorithm is in-context learning? investigations with linear models"
      },
      {
        "key": "oswald2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "dai2023can",
        "author": "Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu",
        "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"
      },
      {
        "key": "zhang2023trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained Transformers Learn Linear Models In-Context"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "akyurek2022learning",
        "author": "Aky{\\\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny",
        "title": "What learning algorithm is in-context learning? investigations with linear models"
      },
      {
        "key": "oswald2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "huang2023context",
        "author": "Huang, Yu and Cheng, Yuan and Liang, Yingbin",
        "title": "In-context convergence of transformers"
      },
      {
        "key": "zhang2023trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained Transformers Learn Linear Models In-Context"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "han2023context",
        "author": "Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng",
        "title": "In-Context Learning of Large Language Models Explained as Kernel Regression"
      },
      {
        "key": "jiang2023latent",
        "author": "Jiang, Hui",
        "title": "A latent space theory for emergent abilities in large language models"
      },
      {
        "key": "wang2023large",
        "author": "Wang, Xinyi and Zhu, Wanrong and Wang, William Yang",
        "title": "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning"
      },
      {
        "key": "wies2023learnability",
        "author": "Wies, Noam and Levine, Yoav and Shashua, Amnon",
        "title": "The learnability of in-context learning"
      },
      {
        "key": "xie2021explanation",
        "author": "Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu",
        "title": "An explanation of in-context learning as implicit bayesian inference"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xie2021explanation",
        "author": "Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu",
        "title": "An explanation of in-context learning as implicit bayesian inference"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xie2021explanation",
        "author": "Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu",
        "title": "An explanation of in-context learning as implicit bayesian inference"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wies2023learnability",
        "author": "Wies, Noam and Levine, Yoav and Shashua, Amnon",
        "title": "The learnability of in-context learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2023trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained Transformers Learn Linear Models In-Context"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2023trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained Transformers Learn Linear Models In-Context"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "naik1992meta",
        "author": "Naik, Devang K and Mammone, Richard J",
        "title": "Meta-neural networks that learn by learning"
      },
      {
        "key": "schmidhuber1987evolutionary",
        "author": "Schmidhuber, J{\\\"u}rgen",
        "title": "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chua2021fine",
        "author": "Chua, Kurtland and Lei, Qi and Lee, Jason D",
        "title": "How fine-tuning allows for effective meta-learning"
      },
      {
        "key": "denevi2018incremental",
        "author": "Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano",
        "title": "Incremental learning-to-learn with statistical guarantees"
      },
      {
        "key": "ji2020convergence",
        "author": "Ji, Kaiyi and Lee, Jason D and Liang, Yingbin and Poor, H Vincent",
        "title": "Convergence of meta-learning with task-specific adaptation over partial parameters"
      },
      {
        "key": "tripuraneni2020theory",
        "author": "Tripuraneni, Nilesh and Jordan, Michael and Jin, Chi",
        "title": "On the theory of transfer learning: The importance of task diversity"
      }
    ]
  }
]