
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{url}            % simple URL typesetting
\usepackage{hyperref}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{titletoc}
\usepackage{tabularx}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{multirow}
% \usepackage{multiply}
% \usepackage{CJK}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{threeparttable}
\usepackage[most]{tcolorbox}  
\definecolor{darkgrey}{rgb}{0.53,0.53,0.53}
\definecolor{mygrey}{rgb}{0.85,0.85,0.85}
\usepackage[titletoc]{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{note}{}
%\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\TV}{D_{\mathrm{TV}}}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\add}[1]{{\textcolor{blue}{#1}}}
\newcommand{\addfoot}[1]{{\textcolor{red}{#1}}}

\titlecontents{section}[3em]{\vspace{-1pt}}%
{\bfseries\contentslabel{2em}}%
{}%
{\titlerule*[0.5pc]{.}\contentspage}%

\titlecontents{subsection}[5em]{\vspace{-1pt}}%
{\contentslabel{2em}}%
{}%
{\titlerule*[0.5pc]{.}\contentspage}%

\titlecontents{subsubsection}[5em]{\vspace{-1pt}}%
{\footnotesize\contentslabel{3em}}%
{}%
{\titlerule*[0.5pc]{.}\contentspage}%

\title{Towards Auto-Regressive Next-Token Prediction: In-context learning Emerges from Generalization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy

\author{Zixuan Gong\thanks{Equal contribution.}\ \ ,\ \ Xiaolin Hu\footnotemark[1]\ \ ,\ \  Huayi Tang,\ \ Yong Liu\thanks{Corresponding author.} \\
Gaoling School of Artificial Intelligence\\
Renmin University of China\\
Beijing, China \\
\texttt{\{zxgong,xiaolinhu,huayitang,liuyonggsai\}@ruc.edu.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: \textbf{(a) Limited \textit{i.i.d.} Setting.} Most studies focus on supervised function learning tasks where prompts are constructed with \textit{i.i.d.} input-label pairs. This \textit{i.i.d.} assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent. \textbf{(b) Lack of Emergence Explanation.} Most literature answers \textbf{\textit{what}} ICL does from an implicit optimization perspective but falls short in elucidating \textbf{\textit{how}} ICL emerges and the impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a more practical paradigm, \textbf{\textit{auto-regressive next-token prediction (AR-NTP)}}, which closely aligns with the actual training of language models. Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence. To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation. In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that \textbf{\textit{ICL emerges from the generalization of sequences and topics}}. Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Large language models (LLMs) have exhibited intriguing emergent capabilities in in-context learning (ICL) \citep{brown2020language}, which allows effective predictions on downstream tasks only based on a short context without any parameter fine-tuning \citep{black2022gpt, rae2021scaling}. Since then, more scholars have increasingly focused on the intrinsic mechanisms of ICL \citep{chan2022data,garg2022can,oswald2023transformers}, aiming to gain a better understanding of LLMs. 

For simple supervised function learning tasks, the analysis framework for ICL has been well-established, where independently and identically distributed (\textit{i.i.d.}) input-label pairs are stacked into a prompt so that the model directly gives the predicted label for query input without parameter updates. Following that, empirically, \citet{garg2022can} demonstrates that pre-trained LLMs can approximate linear functions with a performance that is nearly equivalent to the least squares estimator. Theoretically, many studies reveal that ICL implicitly employs optimization algorithms. Among these, a prominent viewpoint demonstrates that pre-trained LLMs performing ICL is equivalent to mimicking a single step of gradient descent on linear regression tasks \citep{ahn2024transformers, akyurek2022learning, dai2023can, nichani2024transformers, oswald2023transformers, zhang2023trained}. 
However, there are two main limitations in this literature: (a) Limited \textit{i.i.d.} Setting. The \textit{i.i.d.} assumption in prompt tokens is potentially strong and unrealistic in language tasks where prompt tokens are dependent, making it challenging to easily extend the aforementioned analysis framework for supervised learning tasks to language modeling. (b) Lack of Emergence Explanation. Most literature answers \textbf{\textit{what}} ICL does from an optimization algorithm perspective but falls short in explaining \textbf{\textit{how}} pre-trained LLMs can be good enough to emerge ICL ability as well as the impact of pre-training phase on ICL. Therefore, the following fundamental questions remain relatively underexplored:

\quad\textbf{\textit{(a) How can we model language tasks with token-dependency, going beyond the \textit{i.i.d.} limitation?}}

\quad \textbf{\textit{(b) How can ICL emerge from pre-trained LLMs?}}

For question (a), in extending existing work on supervised function learning, we are eager to explore research on the \textbf{\textit{auto-regressive next-token prediction (AR-NTP)}} 
paradigm, which is key to the success of modern LLMs \citep{achiam2023gpt,brown2020language} in practical language tasks. Specifically, there are generally successive tokens in both training sequences and ICL prompts, which are drawn from the unsupervised corpus. Through AR-NTP, each subsequent token in sequences or prompts is generated based on the preceding tokens. Drawing inspiration from the Bayesian perspective in statistical field, we utilize conditional probability distribution \citep{han2023context,jiang2023latent, li2023transformers, wang2023large, wei2022emergent, wu2023many, xie2021explanation} to theoretically model AR-NTP. In the line of Bayesian research, most view ICL as a process of implicit Bayesian inference, where the pre-trained LLM is thought to subconsciously deduce a concept while generating a prediction. These works assume tokens are generated from Hidden Markov Models. In our paper, we consider a more relaxed generation mode, AR-NTP, where each token depends on all the preceding tokens rather than just one preceding token. Consequently, our core task is to model language tasks, and the core challenge of modeling language tasks is to consider \textit{prompt token-dependency}.


To analyze question (b), we intuitively recognize that the prompt sequence may be new or unseen and the corresponding ICL topic for the sequence is generally unknown. We desire a well-pretrained in-context learner, \textit{i.e.}, the LLM can effectively utilize ICL to generate high-quality responses to any sequence under any topic, regardless of whether the sequence and topic are seen or unseen during pre-training. This necessitates examining population loss by considering expectations over distribution, rather than focusing solely on empirical loss. A low population loss indicates that the model possesses strong generalization ability for diverse sequences and topics, thereby facilitating the emergence of ICL. Therefore, it is natural to \textbf{\textit{explore the origin of ICL from the perspective of measuring generalization ability}}. Specifically, we formalize a systematic pre-training and ICL framework that incorporates data distribution and topic distribution, allowing us to establish the population loss with a two-level expectation. By adopting PAC-Bayesian generalization analysis techniques, we gain a clearer understanding of how ICL emerges. 

Based on the above analysis, we summarize our main contributions as follows.

\textbf{1.} \textbf{Pre-training and ICL Framework under AR-NTP Paradigm.}\quad Towards practical AR-NTP paradigm rather than \textit{i.i.d.} setting, we establish a systematic pre-training and ICL framework considering layer-wise structure of sequences and topics (Section \ref{sec: ICL}). Meanwhile, we propose two-level expectation over data and topic distribution to link pre-training and ICL phase, thereby providing well-defined population loss based on empirical loss (Section \ref{sec:optimization-objective} and \ref{sec:two-level}). 

\textbf{2.} \textbf{ICL Emerges from Generalization.}\quad Our theoretical results of population loss reveal that model generalization, tightly with ICL abilities, is influenced by model size, optimization iterations, pre-training data and prompt length. This further demonstrates that ICL emerges from the excellent generalization of sequences and topics (Section \ref{sec:gen-pre} and Section \ref{sec:gen-ICL}).   

\textbf{3.} \textbf{Generalization Analysis.} By dealing with prompt token-dependency and employing continuous mathematical techniques such as Stochastic Differential Equation (SDE), we present data-dependent and topic-dependent, as well as optimization-dependent PAC-Bayesian generalization bounds for population loss (Section \ref{sec:gen-pre} and Section \ref{sec:gen-ICL}).

\textbf{4.} \textbf{Empirical Verification of Theory.}\quad We perform experiments on numerical linear dynamic system, synthetic GINC and real-word language datasets (Section \ref{sec:exp} and Appendix \ref{exper}). By discussing the effects of pre-training data, optimization process and prior model initialization, we verify our theoretical results and offer practical implications (Appendix \ref{app:practical}).


\section{Related Work}
\textbf{Optimization Perspective on ICL.} \quad The field of ICL in transformers has been extensively explored from various analytical perspectives. 
A prominent approach is to view ICL as an implicit execution of gradient descent algorithm ~\citep{akyurek2022learning,oswald2023transformers,dai2023can,zhang2023trained}. This concept is well-illustrated \citep{akyurek2022learning,oswald2023transformers}, which demonstrates that pre-trained transformers can mimic a single step of gradient descent on linear regression tasks. 
\citet{huang2023context, zhang2023trained} specifically provide evidence that learning linear models via gradient flow aligns with transformers learning in-context, based on optimization convergence analysis. However, all this literature falls short in explaining how LLMs develop the ability of ICL and the connection between the pre-training and ICL phases. 

\textbf{Bayesian Perspective on ICL.} \quad There is some existing work from Bayesian view enriching the understanding of ICL \citep{han2023context,jiang2023latent,wang2023large,wies2023learnability,xie2021explanation}. \citet{xie2021explanation} interpreter ICL as implicit Bayesian inference, where the pre-trained LLM is seen as intuitively deducing a concept during prediction. 
Following \cite{xie2021explanation}, the assumption that the pre-training distribution is a Hidden Markov Model, is relaxed in \cite{wies2023learnability}.  
Further, \citet{zhang2023trained} consider the pre-training and ICL phase and assume that prior and posterior satisfy a uniform distribution. In our study, we adopt data-dependent and topic-dependent prior without relying on some predetermined distribution assumptions. A topic distribution is considered in our pre-training and ICL framework, which weakens the assumption that the pre-training topic distribution covers the ICL topic distribution in \cite{zhang2023trained} to some extent.

\textbf{From Multi-Task Learning to Meta-Learning.} \quad Training LLMs to perform ICL can be viewed as an approach for addressing the wider tasks of meta-learning or learning-to-learn \citep{naik1992meta, schmidhuber1987evolutionary}. In pre-training phase, the LLM is trained on multiple tasks. We expect that a well-pretrained LLM serves as a good \textbf{\textit{meta-learner}} possessing the ICL ability to generalize to new unseen tasks, not only as a \textit{\textbf{multi-task learner}} \citep{radford2019language}. Theoretical analysis of meta-learning has received significant attention \citep{chua2021fine, denevi2018incremental, ji2020convergence,tripuraneni2020theory}. Drawing inspiration from the assumption of an unknown task distribution in meta-learning analysis, we establish a pre-training and ICL framework with topic/task distribution and data distribution, to describe the model's generalization ability to new test prompts and unseen topics (Details in Section \ref{sec: ICL}). However, it is worth emphasizing that our ICL generalization analysis under AR-NTP cannot be equivalent to meta-learning generalization, since the expectation over sequence would be specially spilt into two parts due to the prompt token-dependency (Details in Section \ref{sec:two-level}). We defer more discussion in Appendix \ref{app:related-work}.

\section{Problem Setup}\label{sec:preli}
In this section, for question (a), Section \ref{sec: ICL} establishes the pre-training and ICL framework under AR-NTP paradigm. Following that, to address question (b), Section \ref{sec:optimization-objective} and \ref{sec:two-level}, formalize the optimization objective and generalization of pre-trained LLMs, to illustrate how pre-trained LLMs can be good enough to emerge ICL ability.

\textbf{Notations.} Let $\mathbb{E}[\cdot]$ be the expectation of random variables. The KL divergence between distribution $\mu$ and $\nu$ is $\KL(\mu \parallel \nu)=\mathbb{E}_{\theta\sim\mu}[\log{\mu(\theta)}/{\nu(\theta)}]$ and total variation (TV) distance is $\TV(\mu,\nu)=1/2\sum_{\theta \in \Theta}|\mu(\theta)-\nu(\theta)|$. The detailed notations is shown in Appendix \ref{sec:notation} Table \ref{tab:notation}.

\begin{figure}
	\centering
	\includegraphics[width=0.88\linewidth]{AR_v5.pdf}
	\caption{Overview of Pre-training and In-context Learning Framework.} 
	\label{fig:auto-regressive}
    \vspace{-1.3em}
\end{figure}

\subsection{AR-NTP Paradigm and Pre-training and ICL Framework}\label{sec: ICL}
We model the practical AR-NTP paradigm in language learning tasks, where any training sequence or ICL prompt consists of successive tokens drawn from the unsupervised corpus. Within AR-NTP, each subsequent token in the sequence is generated based on the preceding tokens, referred to as prefix sequences. Thus, the data distribution of each token is represented as a conditional probability distribution in statistics. From the analysis to question (b), it's necessary to consider the impact of pre-training and formalize a systematic pre-training and ICL framework to facilitate the generalization analysis where ICL emerges. The overview of pre-training and ICL framework, including topic distribution and data distribution, is shown in Figure \ref{fig:auto-regressive}. 

\textbf{Pre-training Phase.}\quad In pre-training phase, the training data typically encompasses sequences from various topics. To model the training process more realistically, it's essential to characterize and distinguish the training sequences and their respective topics in detail. In the following, we describe the generating process of all training sequences.
\begin{enumerate}
    \vspace{-0.6em}
    \item \textbf{Generate One Training Sequence Under a Pre-training Topic:} Under the assumption of topic distribution $\mathbb{P}_{\mathcal{W}}$ and a fixed topic $w_k \sim \mathbb{P}_{\mathcal{W}}$, the $n$-th sequence $E^{k,n}$ satisfies the true data distribution $\mathbb{P}_{w_k}$, or denoted by $\mathbb{P}(\cdot\mid w_k)$. According to the auto-regressive generating process, $(t+1)$-th token $x^{k,n}_{t+1}$ in $E^{k,n}$ is generated depending on the prefix sequence $E^{k,n}_{t}=\{x^{k,n}_1, x^{k,n}_2, \cdots, x^{k,n}_{t}\}$. It also means $x^{k,n}_{t+1}\sim \mathbb{P}(\cdot \mid E^{k,n}_{t},w_k)$. When the token count reaches $T_{k,n}$, the sequence $E^{k,n}=\{(E^{k,n}_t,x^{k,n}_{t+1})\}_{t=1}^{T_{k,n}-1}$ is already formed.
    \item \textbf{Generate $N_{k}$ Training Sequences Under a Pre-training Topic:} Repeat step 1, $N_k$ training sequences following the same data distribution $\mathbb{P}_{w_k}$ or $\mathbb{P}(\cdot\mid w_k)$, are \textit{i.i.d.} sampled. The pre-training sequences under topic $w_k$ can be denoted by $E^{k}=\{E^{k,n}\}_{n=1}^{N_{k}}$. 
    \item \textbf{Generate Complete Training Sequences Under $K$ Pre-training Topics:} Considering that the set of topics used for pre-training is $\mathcal{W}_{\text{pre}}$ which contains $K$ topics, then repeat Step 2, the complete pre-training sequences can be denoted by $E =\{E^{k}\}_{k=1}^{K} = \{E^{k,n}\}_{k,n=1}^{K,N_k}$.
    \vspace{-0.6em}
\end{enumerate}
Note that the number of sequences for different topics ($N_{k}$) and sequence length ($T_{k,n}$) vary from each other. We give more discussion for $N_k$ and $T_{k,n}$ in Remark \ref{remark-app: the1}. For theoretical convenience, we unify $N$ and $T$ in our main analysis. Using pre-training data $E$ containing a total of $KN$ sequences, the model gives predictions that still follow the AR-NTP methods. Then the LLM $\mathbb{P}_\theta$ parameterized by $\theta \in \Theta$ is pre-trained by establishing AR-NTP loss. 

\textbf{\emph{Note:}}  Throughout our paper, the subscripts or superscripts $k$, $n$, and $t$ represent the topic index, sequence index and token index, respectively.

\textbf{ICL Phase.}\quad In ICL phase, for any ICL topic $w$ that satisfies the same topic distribution $\mathbb{P}_{\mathcal{W}}$ as pre-training topics, $\text{prompt}=\{x_1,x_2,\cdots,x_{T_p}\}$ is generated from data distribution $\mathbb{P}_{w}$ (or $\mathbb{P}(\cdot\mid w)$). Similarly to the above generation process of pre-training sequence $E^{k,n}$, $x_t \sim \mathbb{P}(\cdot \mid \text{prompt}_{t-1}, w)$. The goal for an ICL learner is to make the prediction $\mathbb{P}_{\theta}(x_{T_p} \mid \text{prompt}_{T_p-1},w)$, given by the pre-trained LLM $\mathbb{P}_{\theta}$, as close as possible to $\mathbb{P}(x_{T_p} \mid \text{prompt}_{T_p-1},w)$. To test the performance of ICL on different topics, a set of ICL topics $\mathcal{W}_{\text{ICL}}$ is adopted. Note that different numbers of demonstrations may be used in standard ICL. In our theoretical modeling, we consider directly concatenating demonstrations into ICL prompts. The distinction between zero-shot ICL and few-shot ICL is reflected in the prompt length $T_p$, and our theoretical results reveal the impact of prompt length on model generalization and ICL emergence.

\vspace{-0.5em}
\subsection{Optimization Objective: Empirical Loss}\label{sec:optimization-objective}
Considering the pre-training phase, finite topics and finite sequences are \textit{i.i.d.} sampled from topic distribution $\mathbb{P}_{\mathcal{W}}$ and data distribution $\mathbb{P}_{w_k}$ or $\mathbb{P}(\cdot \mid w_k)$. During the training process, for any sequence $E^{k,n}$ under topic $w_k$, each token $x^{k,n}_{t+1}$ can be predicted depending on the prefix sequence $E^{k,n}_t$, optimizing the negative log-likelihood loss $-\log \mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)$ in practice. When with fixed the true data distribution $\mathbb{P}(\cdot \mid w_k)$, minimizing $-\log \mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)$ is equivalent to minimize the $\log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}$. It is expected that the prediction of LLM could be close to the true sequence.

Under $k$-th topic, we average the prediction loss of all tokens for the $n$-th sequence and then average this over $N$ sequences, with the definition of $L_{E^k}(\theta,w_k)$. Finally, averaging over $K$ topics, the optimization objection (empirical loss) of the pre-training phase is defined as 
\begin{align}\label{eq-L-E}
	L_E(\theta, \mathcal{W}_{\text{pre}})&=\frac{1}{K}\sum_{k=1}^K \underbrace{\left(\frac{1}{NT}\sum_{n=1}^N\sum_{t=1}^{T} \log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}\right)}_{L_{E^k}(\theta, w_k)},
    \vspace{-0.3em}
\end{align}
where $L_{E^{k,n}}(\theta, w_k) = \frac{1}{T}\sum_{t=1}^T \log \mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)/ \mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)$, represents the average loss of one sequence. Define the minimum of empirical loss as
\begin{equation}\label{eq-argmin-theta}
	\hat{\theta}=\operatorname{argmin}_\theta L_E(\theta, \mathcal{W}_{\text{pre}}).
\end{equation}
In our theoretical analysis, LLMs perform Stochastic Gradient Descent (SGD) as an optimization algorithm to update parameters $\theta$ in order to get the minimum $\hat{\theta}$. We formalize optimization error $\epsilon_{\text{opt}}$ with the logarithmic distribution distance between the pre-trained model $\mathbb{P}_{\theta}$ and the ideal model $\mathbb{P}_{\hat{\theta}}$,
\begin{equation}\label{opt}
		\frac{1}{KNT}\sum_{k=1}^K\sum_{n=1}^{N}\sum_{t=1}^T \left(\log\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)
		-\log \mathbb{P}_{\theta}(x^{k,n}_{t+1}|E^{k,n}_t,w_k)\right).
\end{equation}
\vspace{-1em}

\subsection{Generalization Analysis: Two-Level Expectation}\label{sec:two-level}
We expect a good ICL learner, which means that the pre-trained LLM has the ability to identify new topics and predict new sequences. As introduced in Section \ref{sec: ICL}, we hope that the pre-trained LLM can infer unseen sequences under unseen ICL topics with the assumption of data distribution and topic distribution. Therefore, it's natural to define a two-level expectation, aiming to minimize the expected / population loss. 

\textbf{The First-level Expectation over Sequence.}\quad The first-level expectation (\emph{i.e.} inner expectation) is taken over sequence $E^{k,n}$, indicating a sufficient number of sequences for each topic to facilitate comprehensive learning in the ideal case so that the pre-trained model can perform excellently when faced with new sequences \textbf{under seen topics}. In Equation \ref{eq-L-E}, rather than using $L_{E^k}(\theta,w_k)$ with $N$ sequences, we define $L(\theta, \mathcal{W}_{\text{pre}})$ with sufficient sequences as
\begin{align}
    L(\theta, \mathcal{W}_{\text{pre}}) = \frac{1}{K} \sum_{k=1}^K \mathbb{E}_{E^{k,n}} \left[ \log \frac{\mathbb{P}(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}{\mathbb{P}_\theta(x^{k,n}_{t+1}|E^{k,n}_t, w_k)}\right].\nonumber
\end{align}
More concretely, the \textit{prompt token-dependency} in AR-NTP (\emph{i.e.} the tokens are dependently generated in sequence $E^{k,n}$) motivates that the first-level expectation $\mathbb{E}_{E^{k,n}}$ needs to be divided into two parts: expectation over each token when given prefix sequences $\mathbb{E}_{x^{k,n}_{t+1} \sim \mathbb{P}(\cdot \mid E^{k,n}_t, w_k)}$ and expectation over prefix sequences $\mathbb{E}_{E^{k,n}_t}$. Then combining the definition of KL divergence, it can be transformed into $\frac{1}{K} \sum_{k=1}^K \mathbb{E}_{E^{k,n}_t} \left[D_{\mathrm{KL}}\left(\mathbb{P}(\cdot\mid E^{k,n}_t, w_k)\parallel \mathbb{P}_\theta(\cdot\mid E^{k,n}_t, w_k)\right)\right]$. Using any prefix sequence $P$ to replace $E^{k,n}_t$, we simply the representation and the first-level expected loss finally becomes
\begin{align}
    L(\theta, \mathcal{W}_{\text{pre}}) = \frac{1}{K} \sum_{k=1}^K \mathbb{E}_{P} \left[D_{\mathrm{KL}}\left(\mathbb{P}(\cdot\mid P, w_k)\parallel \mathbb{P}_\theta(\cdot\mid P, w_k)\right)\right]. \label{eq-L-Wpre-two-part-final-main}
\end{align}

\textbf{The Second-level Expectation over Topic.}\quad The second-level expectation (\emph{i.e.} outer expectation) is taken over topic $w_k$. A well-trained LLM with the objective of minimizing the population loss over infinite topics will demonstrate good generalization of topics, which will be directly reflected in the model's accuracy in predicting ICL prompts \textbf{under unseen topics} during the ICL phase, provided these unseen ICL topics satisfy the assumption of topic distribution. Therefore, rather than using $K$ topics in Equation \ref{eq-L-Wpre-two-part-final-main}, we define $L(\theta)$ with sufficient topics as
\begin{equation}
	L(\theta)=\mathbb{E}_{w}\mathbb{E}_{P} \left[D_{\mathrm{KL}}\left(\mathbb{P}(\cdot\mid P, w)\parallel \mathbb{P}_\theta(\cdot\mid P, w)\right)\right],\nonumber
\end{equation}
% \label{eq-L}
which is called population loss with two-level expectation. To specifically align to the ICL phase and test the impact of different prompt lengths, we calculate the average loss over each token, similar to pre-training, \textit{i.e.},
\begin{equation}\label{eq-L-ICL-final}
	L(\theta)=\frac{1}{T_p}\sum_{t=1}^{T_p}\mathbb{E}_{w}\mathbb{E}_{\text{prompt}_t} \left[D_{\mathrm{KL}}\left(\mathbb{P}(\cdot\mid \text{prompt}_t, w)\parallel \mathbb{P}_\theta(\cdot\mid \text{prompt}_t, w)\right)\right].
\end{equation}

\section{ICL Emerges from Generalization of Pre-trained LLMs}\label{sec:gen}
In this section, we sequentially present Theorems for the generalization of sequences and topics. Specifically, Theorem \ref{pre-gen-data-dependent} considers the generalization of sequences, providing the upper bound of the first-level expected loss defined in Equation \ref{eq-L-Wpre-two-part-final-main}. Theorem \ref{ICL-gen-topic-dependent} further considers the generalization of topics and provides the upper bound of the two-level expected loss (\textit{i.e.} population loss defined in Equation \ref{eq-L-ICL-final}) by integrating Theorem \ref{pre-gen-data-dependent}. Thus, we answer question (b) that ICL emerges from the excellent generalization of sequences and topics.

\paragraph{Summary of Challenges.} Before diving into the details of Theorems, we summarize the challenges in both modeling and theoretical proof, in comparison to previous research.

\textbf{(1) The consideration of two-level expectation.} \quad In contrast to focusing solely on the ICL process, we model the entire process of training and utilization, aiming to mirror real-world training scenarios and explore the origin of ICL from the perspective of generalization. The consideration of two-level expectation over sequence and topic under a reasonable pre-training and ICL framework significantly amplifies our workload.

\textbf{(2) The dealment of prompt token-dependency.}\quad Under the setting of AR-NTP, we make great efforts to address the dependency between the current token and its preceding tokens by constructing ghost sequences (see the detailed construction in Appendix \ref{appendix-the-1}, where we summarize the proof sketch), thereby enabling the possibility of taking expectation over each token within all possible sequences. It's worth noting that such a dependency is not present in the supervised function learning tasks in other ICL research.

\textbf{(3) The connection of negative logarithm likelihood, KL divergence and TV distance.}\quad We examine the primary optimization objective: negative logarithm likelihood. Naturally, this leads to a connection with KL divergence, thereby formalizing the expression of population loss. Furthermore, in addressing the aforementioned token-dependency, we establish connections between TV distance and the expectation over a single token when given its predecessors. Therefore, it's necessary to establish connections between the two key distribution metrics: TV distance and KL divergence (see in Lemma \ref{lemma:KL-TV-bound}), to obtain our final generalization bounds. The AR-NTP setup necessitates the establishment of the above series of connections, which are not considered in the previous ICL work.


\subsection{Generalization of Sequences: The First-Level Expectation}\label{sec:gen-pre}
Under finite ($K$) pre-training topics, $L(\theta,\mathcal{W}_{\text{pre}})$ defined in Equation \ref{eq-L-Wpre-two-part-final-main}, represents the first-level expected loss where infinite sequences per topic are utilized. It describes comprehensive learning for each pre-training topic in the ideal case so that the pre-trained model can give excellent answers for new sequences on the seen topics in ICL phase. In the following theorem, we present the upper bound of $L(\theta, \mathcal{W}_{\text{pre}})$.

% 后验，先验，KL
Based on basic notations of general PAC-Bayesian theory, in our discussion, we define the posterior distribution of model parameters as $\mu(\theta)$, which is obtained by training the LLM using $K$ topics and $N$ sequences per topic. Define the prior distribution as $\nu(\theta)$, which is an assumed probability distribution before some evidence is taken into account. In the formal Theorem for $L(\theta, \mathcal{W}_{\text{pre}})$, we derive the KL distance between the posterior and prior in the upper bound, specifically with a data-dependent prior \citep{li2019generalization}. Furthermore, continuous mathematical analysis tools such as SDE are used to detail the KL divergence between posterior and data-dependent prior, which further considers the optimization algorithm. Since then, we can provide data-dependent and optimization-dependent generalization bound of the first-level expected loss.

\textbf{Data-Dependent Prior.}\quad We employ the following method for generating a data-dependent prior \citep{li2019generalization}. Let $J$ include $N^{\prime}$ indexes uniformly sampled from $[N]$ without replacement and $I$ is $[N]\setminus J$, splitting pre-training sequences under fixed topic $w_k$ into two parts $E^k_I$ and $E^k_J$. Under all pre-training topics, we have $E_I=\{E^k_I\}_{k=1}^K$ and $E_J=\{E^k_J\}_{k=1}^K$. The prior distribution of model parameters $\theta$ depends on the subset $E_J$, which is denoted by $\nu_J$ and the posterior distribution of $\theta$ depends on $E_I$ denoted by $\mu$. Thus, a parallel training process with $E_J$ are conducted, and after that, a data-dependent prior $\nu_J$ will be obtained. We emphasize that extracting a portion of training data to learn the prior distribution of model parameters has significant implications. Specifically, this approach allows the prior to adapt to specific features and trends in the data, enhancing the model's ability to capture and learn from these nuances. Even if we sacrifice a portion of the training data, the prior will lead to a posterior distribution that is better aligned with the actual data distribution. In high-dimensional spaces, a data-dependent prior provides a more informed starting point.
 
\begin{assumption}[Bounded Loss Function]\label{ass:B} Given fixed topic $w_k$ and prefix sequence $E^{k,n}_t$, for the true data distribution $\mathbb{P}(\cdot \mid w_k)$ (or $\mathbb{P}_{w_k}$) and pre-trained LLM $\mathbb{P}_\theta$, we have
$$
\log \frac{\mathbb{P}(x^{k,n}_{t+1}\mid E^{k,n}_t,w_k)}{\mathbb{P}_{\theta}(x^{k,n}_{t+1}\mid E^{k,n}_t,w_k)} \leq S.
$$
\end{assumption}
This assumption shows that the logarithm ratio of $\mathbb{P}(\cdot \mid w_k)$ and $\mathbb{P}_\theta$ is bounded suggesting that the learned model is expected to closely approximate the true data distribution. According to the true data distribution, the probability of $x^{k,n}_{t+1}$ tends to $1$. Thus by scaling law \citep{kaplan2020scaling}, the training loss for specific tokens $-\log \mathbb{P}_{\theta}(x^{k,n}_{t+1}\mid E^{k,n}_t,w_k)$ equals to $\left(\frac{N_c}{N_\text{param}}\right)^{\alpha_N}$, where $N_\text{param}$ represents the number of parameters in the model, $N_c$ and $\alpha_N$ are constants obtained through statistical fitting. Thus, $S$ can further be measured with $\left(\frac{N_c}{N_\text{param}}\right)^{\alpha_N}$.

\begin{assumption}[Bounded Gradient]\label{ass: lipschitz} 
	Suppose that for topic $w_k$ and model parameters $\theta_t$ at step $t$ (for any $0 \leq t \leq T^\prime$, $T^\prime$ is the total iteration steps), we have $\left\|\nabla L_{E^{k,n}}(\theta_t, w_k)\right\| \leq L$.
\end{assumption}
Assumption \ref{ass: lipschitz} is the classical $L$-Lipschitz continuous condition, which is widely used in generalization analysis \citep{elisseeff2005stability, li2019generalization}. This suggests that the gradient of an average loss of one sequence (see in Equation \ref{eq-L-E}) is bounded. 

\begin{theorem}[Data-Dependent and Optimization-Dependent Generalization Bound of the First-level Expected Loss] Let the auto-regressive LLM $\mathbb{P}_\theta$ be the empirical solution of Equation $\ref{eq-L-E}$, and $\mathbb{P}(\cdot\mid w)$ denotes the true data distribution under topic $w$. Under Assumptions \ref{ass:B} and \ref{ass: lipschitz}, for any $0<\delta < 1$, with probability at least $1-\delta$, the first-level expected loss with $K$ topics and infinite sequences per topic, denoted by $L(\theta, \mathcal{W}_{\text{pre}})$ (see in Equation \ref{eq-L-Wpre-two-part-final-main}), satisfies,
\label{pre-gen-data-dependent}
\begin{equation*}
    \mathbb{E}_{\mu}\left[L(\theta, \mathcal{W}_{\text{pre}})\right]
    =\mathcal{O}\left\{\sqrt{\frac{ \log 1/\delta}{KNT}} + \sqrt{\frac{1}{KNT}\left(\KL(\mu\parallel\nu)+\log \frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\},
\end{equation*}
then considering data-dependent prior $\nu_J$ and detailing the term $\KL(\mu\parallel\nu_J)$, $L(\theta, \mathcal{W}_{\text{pre}})$ is further bounded by
\begin{equation}\label{the3-right}
    \mathcal{O}\left\{\sqrt{\frac{\log 1/\delta}{K(N-N^\prime)T}} + \sqrt{\frac{1}{K(N-N^\prime)T}\left(\frac{L^2C(\frac{1}{N_{\text{param}}},{T^\prime})}{N^\prime} + \log \frac{1}{\delta}\right)-\epsilon_{\text{opt}}}\right\},
\end{equation}
where $C(\frac{1}{N_{\text{param}}},T^\prime)=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{{T^\prime}}{\exp(8\beta S)}}\right)$. $\epsilon_{\text{opt}}$ is the optimization error (see in Equation \ref{opt}). $K$, $N (N^\prime)$ and $T$ denote the number of topics, the number of sequences per topic and the sequence length utilized in the optimization process of Equation $\ref{eq-L-E}$. $T^\prime$ denotes the total training iterations. $N_{\text{param}}$ denotes the number of model parameters.
\end{theorem}

\begin{remark}\label{remark: the2}
	Theorem \ref{pre-gen-data-dependent} reveals that when considering the first-level expectation over sequence, the expected loss achieves $\mathcal{O}\{1/\sqrt{KNT}\}$ rate. This indicates that an increase in the number of training topics ($K$), the number of sequences per topic ($N$), and the sequence length ($T$) leads to a reduction in the first-level expected loss, aligning with both intuitive understanding and empirical evidence. Furthermore, from the term $C(\frac{1}{N_{\text{param}}}, T^\prime)$, the expected loss is smaller with a larger model size (\textit{i.e.}, larger $N_{\text{param}}$). It reveals why LLMs outperform small language models in emerging ICL, even though they adopt a similar AR-NTP paradigm. $T^\prime$ is related to the optimization process. As $T^\prime$ increases, $C(\beta, T^\prime)$ increases, \textit{i.e.}, the generalization error increases. This reflects the influence of total training iterations $T^\prime$ on testing loss, corresponding to the classical viewpoint `train faster, generalize better’ \citep{hardt2016train, lei2020fine, zhang2022stability}. We defer more detailed discussion to Appendix \ref{remark-app: the2} and proof to Appendix \ref{appendix-the-1} and \ref{appendix-the-2}.
\end{remark}

\subsection{Generalization of Sequences and Topics: Two-Level Expectation}\label{sec:gen-ICL}
%ICL Emerges from Generalization of Auto-regressive Pre-trained LLM
Up to now, we have analyzed the first-level expected loss with $K$ topics and infinite sequences per topic. With small first-level expected loss, the pre-trained LLM can perform excellently on the new test prompt under \textbf{\textit{seen}} topics in ICL phase. In this section, we use similar techniques to further consider the second-level expectation with infinite topics, so that the pre-trained LLM with small population loss can perform well on \textbf{\textit{unseen}} topics. At this moment, ICL emerges from the generalization of sequences and topics.

In the following theorem for the two-level expected loss (population loss) $L(\theta)$, similarly, we derive the KL distance between the posterior $\mu$ and prior $\nu$ in the upper bound, specifically propose a topic-dependent prior whose core idea comes from data-dependent prior \citep{li2019generalization}, \emph{i.e.}, a portion of $K$ topics will be used for calculating model prior and other topics will be used for obtaining posterior. Based on SDE analysis, we detail the KL divergence between posterior and topic-dependent prior. Since then, we can provide data-dependent, topic-dependent and optimization-dependent generalization bound of the population loss.

\textbf{Topic-Dependent Prior.}\quad We employ the following method for generating a topic-dependent prior, similar to data-dependent prior \citep{li2019generalization}. We split topics into two parts and let $J$ include $K^{\prime}$ indexes uniformly sampled from $[K]$ without replacement and let $I$ be $[K]\setminus J$, then the total sequences are divided into $E^I=\{E^k\}_{k \in \mathcal{W}_{\text{pre},I}}$ and $E^J=\{E^k\}_{k \in \mathcal{W}_{\text{pre},J}}$. Assume that the posterior distribution of model parameters $\theta$ depends on $E^I$ denoted by $\mu$ and the prior distribution of $\theta$ depends on the topic subset $E^J$ denoted by $\nu_J$. A parallel training process is performed with $E^J$ based on the same LLM architecture, and after that, a topic-dependent prior $\nu_J$ will be obtained.

\begin{assumption}[Bounded Expected Gradient]\label{ass: lipschitz-2} Suppose that for topic $w_k$ and model parameters $\theta_t$ at step $t$ (for any $0 \leq t \leq T^\prime$, $T^\prime$ is the total iteration steps), we have $\left\|\mathbb{E}_{E^{k,n}}\left[\nabla  L_{E^{k,n}}(\theta_t,w_k)\right]\right\| \leq \sigma$.
\end{assumption}
Note that $L_{E^{k,n}}$ denotes the average loss of one sequence (Equation \ref{eq-L-E}). Then $\mathbb{E}_{E^{k,n}}\left[\nabla  L_{E^{k,n}}(\theta_t,w_k)\right]$ denotes the gradient averaging over all possible sequences $E^{k,n}$, therefore $\sigma$ is less than the common Lipschitz constant $L$, which bounds the gradient at individual sample points. 


\begin{theorem}[Data-Dependent, Topic-Dependent and Optimization-Dependent Generalization Bound of the Two-level Expected Loss] Let the auto-regressive LLM $\mathbb{P}_\theta$ be the empirical solution of Equation $\ref{eq-L-E}$, and $\mathbb{P}(\cdot\mid w)$ is the true data distribution under topic $w$. Under Assumptions \ref{ass:B}, \ref{ass: lipschitz} and \ref{ass: lipschitz-2}, for any $0<\delta < 1$, with probability at least $1-\delta$, the two-level expected loss (population loss) with infinite topics and infinite sequences per topic, denoted by $L(\theta)$ (see in Equation \ref{eq-L-ICL-final}), satisfies,
	\label{ICL-gen-topic-dependent}
	\begin{equation*}
			\mathbb{E}_{\mu}\left[L(\theta)\right]
			=\mathcal{O}\left\{\sqrt{\frac{1}{KT_p}}\left(\KL(\mu \parallel \nu)+\log \frac{1}{\delta}\right)+U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\},
	\end{equation*}
	then considering data-dependent and topic-dependent prior $\nu_J$ and detailing the term $\KL(\mu\parallel\nu_J)$, $L(\theta)$ is further bounded by
	\begin{equation}
			\mathcal{O}\left\{\sqrt{\frac{1}{(K-K^\prime)T_p}}\left(\frac{\sigma^2C(\frac{1}{N_{\text{param}}},T^\prime)}{K^\prime}+\log \frac{1}{\delta}\right)
			+ U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)\right\},
	\end{equation}
	where $C(\frac{1}{N_{\text{param}}},T^\prime)=\frac{\beta}{2}e^{8\beta S}\left(1-e^{-\frac{ T^\prime}{\exp(8\beta S)}}\right)$, $U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)$ denotes the right hand of Equation \ref{the3-right}. $K (K^\prime)$, $N (N^\prime)$ and $T$ denote the number of topics, the number of sequences per topic and the sequence length utilized in the optimization process of Equation $\ref{eq-L-E}$. $T^\prime$ denotes the total training iterations. $N_{\text{param}}$ denotes the number of model parameters.
\end{theorem}
\begin{remark}[Optimality Analysis]
	The term $U(\mathcal{W}_{\text{pre}},K,N,N^\prime,T)$ comes from Theorem \ref{pre-gen-data-dependent} whose analysis can refer to Remark \ref{remark: the2}. As for the first term in the result, with order $\mathcal{O}\{1/\sqrt{KT_p}\}$, it illustrates the impact of training with a finite number of topics on the model's predictive ability for unseen topics in ICL. In addition, by directly concatenating demonstrations into the ICL prompt in our setting, ICL prompt length reflects the distinction between zero-shot ICL and few-shot ICL. Our theorem exhibits that longer prompts (\textit{i.e.} larger $T_p$) with more demonstrations lead to smaller population loss, facilitating the emergence of ICL. In total, our guarantees reveal the impact of pre-training on the generalization performance on unseen topics and sequences in ICL, with order $\mathcal{O}\{C(\frac{1}{N_{\text{param}}}, T^\prime)(1/\sqrt{KT_p}+1/\sqrt{KNT})\}$. In comparison, \citet{li2023transformers} derive a generalization bound on unseen topics based on algorithm stability technique, with order $\mathcal{O}\{1/\sqrt{T}+1/\sqrt{nMT}\}$ where $n, M, T$ denote the sequence length, number of sequences per topic and number of source topics. Our bound is tighter than \cite{li2023transformers} in the first term, with a compatible second term. We defer the proof to Appendix \ref{appendix-the-3} and \ref{appendix-the-4}.
\end{remark}

\textbf{More Insights Beyond Recent ICL Research.} Our PAC-Bayesian approach offers statistical insights into model performance, emphasizing the impact of pre-training topics, sequences and sequence length. The data-dependent and topic-dependent prior uniquely enhances optimization and may provide more practical guidance on model training, data selection and deduplication, distinguishing our work from related generalization studies \citep{li2023transformers, zhang2023and}. Detailed practical implications are discussed in Appendix \ref{app:practical}.

\section{Experiments}\label{sec:exp}
In this section, we primarily provide some typical experiments to verify our theory, observe the optimization process and prior model initialization. We defer more experiments on linear dynamic systems, synthetic language dataset GINC and real-world language datasets to Appendix \ref{exper} \footnote{Our code is available at \href{https://github.com/zx-gong/ICL-Emerge}{https://github.com/zx-gong/ICL-Emerge}.}.

\textbf{Experiments on Synthetic Language Dataset GINC.}\quad
Inspired by \cite{xie2021explanation}, we first perform experiments on the synthetic language dataset GINC to verify our theory. GINC is a small-scale language dataset generated from uniform Hidden Markov Models (HMMs) over topics, where distinct state transition matrices represent the unique topics for each HMM, without defining topics explicitly. We train the GPT-2 model with GINC dataset using a single 24GB NVIDIA GeForce RTX 3090. Detailed data-generating process, model and Hyperparameter settings are provided in Appendix \ref{app:GINC}.

In the following, we arrange groups of comparative experiments to explore the separate effects of the number of topics ($K$), number of sequences per topic ($N$), sequence length ($T$) and prompt length ($T_p$). We also provide an interesting case where ICL failed.
\begin{figure*}
    \centering
    \vspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_T_K10_v50.pdf}
                \label{exp:lan-N-T}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K10_v50.pdf}
                \label{exp:lan-N-Tp-K10}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K20.pdf}
                \label{exp:lan-N-Tp-K20}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/language_changeN_Tp_K30.pdf}
                \label{exp:lan-N-Tp-K30}
    }
    \\
    \vspace{-1em}
    \hspace{-0.7em}
    \subfigure[]{
                \includegraphics[width=0.25\linewidth]{fig2/language_changeN_Tp_fail.pdf}
                \label{exp:lan-fail}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/lossN-2.pdf}
                \label{exp:loss-N}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/lossT.pdf}
                \label{exp:loss-T}
    }
    \hspace{-1em}
    \subfigure[]{
                \includegraphics[width=0.24\linewidth]{fig2/lossinit.pdf}
                \label{exp:loss-init}
    }
    \vspace{-0.8em}
    \caption{Experiments on GINC and Real-world Language Datasets.}
    \label{fig:exp-combine}
    \vspace{-1.2em}
\end{figure*} 

\textbf{Observation (1): Separate Effects of $K$, $N$, $T$ and $T_p$.}\quad In Figure \ref{fig:exp-combine}, we first present four groups of experiments \ref{exp:lan-N-T}-\ref{exp:lan-N-Tp-K30} to analyze the impact of different factors on generalization. \textit{In Figure \ref{exp:lan-N-T}:} For pre-training, take $K=10$ topics and generate $N \in \{20,40,60,80,100\}$ pre-training sequences per topic with varying sequence length $T \in \{1280, 2560, 5120, 10240\}$. The ICL performance of pre-trained model is then tested with $T_p=128$ prompt length. Each line exhibits a growing trend, indicating a better generalization performance with increasing sequences per topic. Comparing the four lines, a larger sequence length also brings better generalization. \textit{From Figure \ref{exp:lan-N-Tp-K10}-\ref{exp:lan-N-Tp-K30}}, we vary $K\in \{10,20,30\}$. Under each $K$, keep $T=10240$, adjust $N \in \{20,40,60,80,100\}$ and $T_p \in \{48, 80, 128, 160\}$. Combining these experiments, we validate the effects of $K,N,T_p$ on generalization to emerge ICL ability, closely aligning our Theorems. 

\textbf{Observation (2): An Interesting Case that ICL Fails.}\quad \textit{In Figure \ref{exp:lan-fail}}, when the pre-training data contains random transitions, the model observes all token transitions, yet ICL fails. This suggests that the pre-trained models cannot extract information when data distributions do not match the topic, thus failing to achieve ICL.

\textbf{Experiments on Real-world Language Dataset.}\quad We further perform experiments on real-world language datasets, inspired by \citep{min2021metaicl,wang2023large}. We train the GPT2-large model over 20 diverse pre-training datasets covering sentiment analysis, question answering and reasoning tasks. We defer the detailed description of datasets, model and Hyperparameter settings, and observations on the effects of training data, to Appendix \ref{app:language}. Here, we focus on more insightful experiments regarding the effects of optimization on generalization, as well as potential benefits of effective prior model initialization, guided by the KL term in generalization bounds.

\textbf{Observation (1): Optimization Process.}\quad Through continuous analysis of optimization trajectory, our generalization bounds are optimization-dependent, extending beyond the influence of training data. \textit{In Figure \ref{exp:loss-N}}, we present four training processes with varying $N \in \{2^{8},2^{10},2^{12},2^{14}\}$, while keeping $K=20$ and $T=256$ fixed. We observe that larger $N$ brings faster convergence in addition to better performance. Similarly, \textit{in Figure \ref{exp:loss-T}}, we take varied $T \in \{48,64,128,256\}$ and keep $K=20$ and $N=2^{14}$ fixed. All these observations align with our Theorems that faster training leads to better generalization.

\textbf{Observation (2): Prior Model Initialization.}\quad Building on our generalization results with data-dependent and topic-dependent prior, we design experiments to observe the effects of prior model initialization on training and performance (detailed designation is deferred to Appendix \ref{app:language}). Our results show that in the random initialization regime, where all pre-training data is used, training for 30,000 steps takes nearly \textit{\textbf{7 hours}} on four A100 GPUs. In contrast, under the prior model initialization regime, where a smaller model is used for warmup and serves as the prior for initializing the larger model, training the GPT2-large model takes only \textbf{\textit{4 hours}} for the same 30,000 steps on four A100 GPUs, with 0.5 hours required for training the GPT2-small model for 15,000 steps. Furthermore, as shown in the optimization loss curve in Figure \ref{exp:loss-init}, prior model initialization not only accelerates training but also stabilizes the training process (especially in the early stages), leading to comparable model performance. This approach demonstrates the effectiveness of leveraging prior knowledge in enhancing both training efficiency and model performance, supporting the KL term in our generalization bounds and offering more practical insights. 



\section{Conclusion}\label{sec:con}
In this paper, under the AR-NTP paradigm, we propose a systematic pre-training and ICL framework with a layer-wise structure of sequences and topics, alongside two-level expectation. By employing PAC-Bayesian analysis and continuous mathematical techniques like SDE, we provide a comprehensive analysis of data-dependent, topic-dependent and optimization-dependent generalization bounds, demonstrating that ICL emerges for the excellent generalization of sequences and topics. Ultimately, our work aims to take an initial exploration of the origin of ICL ability from the perspective of generalization, supported by both theoretical and experimental results. 

\section*{Acknowledgments}
This research was supported by National Natural Science Foundation of China (No.62476277), National Key Research and Development Program of China (NO.2024YFE0203200), CCF-ALIMAMA TECH Kangaroo Fund (No.CCF-ALIMAMA OF 2024008), and Huawei-Renmin University joint program on Information Retrieval. We also acknowledge the support provided by the fund for building worldclass universities (disciplines) of Renmin University of China and by the funds from Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China, from Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, from Intelligent Social Governance Interdisciplinary Platform, Major Innovation \& Planning Interdisciplinary Platform for the `DoubleFirst Class' Initiative, Renmin University of China, from Public Policy and Decision-making Research Lab of Renmin University of China, and from Public Computing Cloud, Renmin University of China.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\renewcommand{\appendixpagename}{\centering \LARGE Appendix}
\appendixpage
\startcontents[section]
\printcontents[section]{l}{1}{\setcounter{tocdepth}{2}}


\newpage
\section{Table of Notations}\label{sec:notation}
\begin{table}[h]
	\centering
	\caption{Table of Notations.}
	\begin{tabularx}{\textwidth}{p{3cm}X}
		\specialrule{1pt}{0pt}{0pt}
		\toprule
		\textbf{Notation}     &  \textbf{Description} \\
		\midrule
		$K$ & Number of pre-training topics \\
		$K^\prime$ & Number of pre-training topics used to compute topic-dependent prior \\
		$N$ & Number of pre-training sequences per topic \\
		$N^\prime$ & Number of pre-training sequences per topic used to compute data-dependent prior \\
		$T$ & Pre-training Sequence length \\
	  $T_p$ & ICL Prompt length \\
		
		\midrule	
		$w_k$ & A pre-training topic with index $k$ \\
		$w$ & A ICL topic \\
		$\mathcal{W}_{\text{pre}}$ & The set of pre-training topics \\
		$\mathcal{W}_{\text{ICL}}$ & The set of ICL topics \\
		$\mathbb{P}_{\mathcal{W}}$ & Topic distribution, each topic $w_k \in \mathcal{W}_{\text{pre}}$, $w \in \mathcal{W}_{\text{ICL}}$ is \emph{i.i.d.} drawn from the topic distribution. \\		

		\midrule
		$E^{k,n}$ & The $n$-th pre-training sequence under the $k$-th topic\\
		$E^{k,n}_t$ & The subsequence consisting of the first $t$ tokens of $E^{k,n}$  \\
		$E^k$ & The set of pre-training sequences under the $k$-th topic, $|E^k|=N$. \\
		$E$ & The set of all pre-training sequences, $E=\{E^k\}_{k=1}^K = \{E^{k,n}\}_{k,n=1}^{K,N}$, $|E|=KN$.  \\
		$E_{T_p}$ & ICL prompt under ICL topic $w$\\
		$\mathbb{P}_{w_k}$ or $\mathbb{P}(\cdot \mid w_k)$ & Data distribution, each pre-training sequence $E^{k,n} \in E^{k}$ is \textit{i.i.d.} drawn from the data distribution. \\
		$\mathbb{P}_{w}$ or $\mathbb{P}(\cdot \mid w)$ & Data distribution, ICL prompt $E_{T_p}$ is drawn from the data distribution. \\
		
		\midrule
		$x^{k,n}_{t+1}$ & The $t+1$-th token of pre-training sequence $E^{k,n}$, generated depending on the prefix sequence $E^{k,n}_t$.   \\
		$x_t$ & The $t$-th token of ICL prompt $E_T$   \\
		\midrule
		$\theta$ & The parameters of the pre-trained LLM \\
		$\hat{\theta}$ & The optimal parameters of the pre-trained LLM \\
		$\mathbb{P}(x^{k,n}_{t+1} \mid E^{k,n}_t, w_k)$ & The true data distribution of token $x^{k,n}_{t+1}$ when given topic $w_k$ and the prefix sequence $E^{k,n}_t$.  \\
		$\mathbb{P}_\theta(x^{k,n}_{t+1} \mid E^{k,n}_t, w_k)$ & The prediction of token $x^{k,n}_{t+1}$, made from the pre-trained model, when given topic $w_k$ and the prefix sequence $E^{k,n}_t$. \\
		$\mathbb{P}_{\hat{\theta}}(x^{k,n}_{t+1} \mid E^{k,n}_t, w_k)$ & The prediction of token $x^{k,n}_{t+1}$, made from the optimal pre-trained model, when given topic $w_k$ and the prefix sequence $E^{k,n}_t$. \\
		\midrule
		$L_E(\theta, \mathcal{W}_{\text{pre}})$ & The empirical loss of all pre-training sequences in $E$, see in Equation \ref{eq-L-E}. \\
		$L_{E^k}(\theta, w_k)$ & The loss of sequences in $E^{k}$, see in Equation \ref{eq-L-E}. \\
		$L_{E^{k,n}}(\theta, w_k)$ & The loss of sequence $E^{k,n}$, see in Equation \ref{eq-L-E}.\\
        $L(\theta, \mathcal{W}_{\text{pre}})$ & The first-level expected loss, take expectation over sequence, see in Equation \ref{eq-L-Wpre-two-part-final-main}.\\
		$L(\theta)$ & The population loss, take expectation over topic and sequence, see in Equation \ref{eq-L-ICL-final}.\\
		\specialrule{1pt}{0pt}{0pt}
		\bottomrule
	\end{tabularx}
	\label{tab:notation}
\end{table}

\input{appendix/overview_of_two_level}
\input{appendix/related_work}
\input{appendix/more_exp}
\input{appendix/insights_future}
\input{appendix/complete_theorems}
\input{appendix/proof_of_theorem}
\end{document}
