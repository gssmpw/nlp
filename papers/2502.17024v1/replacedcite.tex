\section{Related Work}
\textbf{Optimization Perspective on ICL.} \quad The field of ICL in transformers has been extensively explored from various analytical perspectives. 
A prominent approach is to view ICL as an implicit execution of gradient descent algorithm ____. This concept is well-illustrated ____, which demonstrates that pre-trained transformers can mimic a single step of gradient descent on linear regression tasks. 
____ specifically provide evidence that learning linear models via gradient flow aligns with transformers learning in-context, based on optimization convergence analysis. However, all this literature falls short in explaining how LLMs develop the ability of ICL and the connection between the pre-training and ICL phases. 

\textbf{Bayesian Perspective on ICL.} \quad There is some existing work from Bayesian view enriching the understanding of ICL ____. ____ interpreter ICL as implicit Bayesian inference, where the pre-trained LLM is seen as intuitively deducing a concept during prediction. 
Following ____, the assumption that the pre-training distribution is a Hidden Markov Model, is relaxed in ____.  
Further, ____ consider the pre-training and ICL phase and assume that prior and posterior satisfy a uniform distribution. In our study, we adopt data-dependent and topic-dependent prior without relying on some predetermined distribution assumptions. A topic distribution is considered in our pre-training and ICL framework, which weakens the assumption that the pre-training topic distribution covers the ICL topic distribution in ____ to some extent.

\textbf{From Multi-Task Learning to Meta-Learning.} \quad Training LLMs to perform ICL can be viewed as an approach for addressing the wider tasks of meta-learning or learning-to-learn ____. In pre-training phase, the LLM is trained on multiple tasks. We expect that a well-pretrained LLM serves as a good \textbf{\textit{meta-learner}} possessing the ICL ability to generalize to new unseen tasks, not only as a \textit{\textbf{multi-task learner}} ____. Theoretical analysis of meta-learning has received significant attention ____. Drawing inspiration from the assumption of an unknown task distribution in meta-learning analysis, we establish a pre-training and ICL framework with topic/task distribution and data distribution, to describe the model's generalization ability to new test prompts and unseen topics (Details in Section \ref{sec: ICL}). However, it is worth emphasizing that our ICL generalization analysis under AR-NTP cannot be equivalent to meta-learning generalization, since the expectation over sequence would be specially spilt into two parts due to the prompt token-dependency (Details in Section \ref{sec:two-level}). We defer more discussion in Appendix \ref{app:related-work}.