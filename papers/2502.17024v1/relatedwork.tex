\section{Related Work}
\textbf{Optimization Perspective on ICL.} \quad The field of ICL in transformers has been extensively explored from various analytical perspectives. 
A prominent approach is to view ICL as an implicit execution of gradient descent algorithm ~\citep{akyurek2022learning,oswald2023transformers,dai2023can,zhang2023trained}. This concept is well-illustrated \citep{akyurek2022learning,oswald2023transformers}, which demonstrates that pre-trained transformers can mimic a single step of gradient descent on linear regression tasks. 
\citet{huang2023context, zhang2023trained} specifically provide evidence that learning linear models via gradient flow aligns with transformers learning in-context, based on optimization convergence analysis. However, all this literature falls short in explaining how LLMs develop the ability of ICL and the connection between the pre-training and ICL phases. 

\textbf{Bayesian Perspective on ICL.} \quad There is some existing work from Bayesian view enriching the understanding of ICL \citep{han2023context,jiang2023latent,wang2023large,wies2023learnability,xie2021explanation}. \citet{xie2021explanation} interpreter ICL as implicit Bayesian inference, where the pre-trained LLM is seen as intuitively deducing a concept during prediction. 
Following \cite{xie2021explanation}, the assumption that the pre-training distribution is a Hidden Markov Model, is relaxed in \cite{wies2023learnability}.  
Further, \citet{zhang2023trained} consider the pre-training and ICL phase and assume that prior and posterior satisfy a uniform distribution. In our study, we adopt data-dependent and topic-dependent prior without relying on some predetermined distribution assumptions. A topic distribution is considered in our pre-training and ICL framework, which weakens the assumption that the pre-training topic distribution covers the ICL topic distribution in \cite{zhang2023trained} to some extent.

\textbf{From Multi-Task Learning to Meta-Learning.} \quad Training LLMs to perform ICL can be viewed as an approach for addressing the wider tasks of meta-learning or learning-to-learn \citep{naik1992meta, schmidhuber1987evolutionary}. In pre-training phase, the LLM is trained on multiple tasks. We expect that a well-pretrained LLM serves as a good \textbf{\textit{meta-learner}} possessing the ICL ability to generalize to new unseen tasks, not only as a \textit{\textbf{multi-task learner}} \citep{radford2019language}. Theoretical analysis of meta-learning has received significant attention \citep{chua2021fine, denevi2018incremental, ji2020convergence,tripuraneni2020theory}. Drawing inspiration from the assumption of an unknown task distribution in meta-learning analysis, we establish a pre-training and ICL framework with topic/task distribution and data distribution, to describe the model's generalization ability to new test prompts and unseen topics (Details in Section \ref{sec: ICL}). However, it is worth emphasizing that our ICL generalization analysis under AR-NTP cannot be equivalent to meta-learning generalization, since the expectation over sequence would be specially spilt into two parts due to the prompt token-dependency (Details in Section \ref{sec:two-level}). We defer more discussion in Appendix \ref{app:related-work}.