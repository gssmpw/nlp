@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{chua2021fine,
  title={How fine-tuning allows for effective meta-learning},
  author={Chua, Kurtland and Lei, Qi and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8871--8884},
  year={2021}
}

@inproceedings{dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2023}
}

@article{denevi2018incremental,
  title={Incremental learning-to-learn with statistical guarantees},
  author={Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:1803.08089},
  year={2018}
}

@article{han2023context,
  title={In-Context Learning of Large Language Models Explained as Kernel Regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  journal={arXiv preprint arXiv:2305.12766},
  year={2023}
}

@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}

@article{ji2020convergence,
  title={Convergence of meta-learning with task-specific adaptation over partial parameters},
  author={Ji, Kaiyi and Lee, Jason D and Liang, Yingbin and Poor, H Vincent},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11490--11500},
  year={2020}
}

@article{jiang2023latent,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv preprint arXiv:2304.09960},
  year={2023}
}

@inproceedings{naik1992meta,
  title={Meta-neural networks that learn by learning},
  author={Naik, Devang K and Mammone, Richard J},
  booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
  volume={1},
  pages={437--442},
  year={1992},
  organization={IEEE}
}

@inproceedings{oswald2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@phdthesis{schmidhuber1987evolutionary,
  title={Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author={Schmidhuber, J{\"u}rgen},
  year={1987},
  school={Technische Universit{\"a}t M{\"u}nchen}
}

@article{tripuraneni2020theory,
  title={On the theory of transfer learning: The importance of task diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael and Jin, Chi},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7852--7862},
  year={2020}
}

@article{wang2023large,
  title={Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning},
  author={Wang, Xinyi and Zhu, Wanrong and Wang, William Yang},
  journal={arXiv preprint arXiv:2301.11916},
  pages={3},
  year={2023}
}

@article{wies2023learnability,
  title={The learnability of in-context learning},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2303.07895},
  year={2023}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}

@article{zhang2023trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

