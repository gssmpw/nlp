\section{Related Work}
\label{sec:related}

% Discuss the main related work and cite around 15-25 papers in sum. 
% The related work section should be approx. 1 column long, assuming 
% a 6-page paper.  Structure the section in paragraphs, grouping the 
% papers, and describing the key approaches with 1-2 sentences. If 
% applicable, describe the key difference to your approach at the end 
% of each paragraph briefly. Avoid adding subsections, al least for a 
% conference paper.

% The approach by Lim~\etalcite{lim2021ral} aims at predicting \dots

\subsection{Local Feature Matching}


%Matching images typically rely on local features. 
While numerous researchers have proposed novel image matching pipelines~\cite{philbin2007object,sivic2003video,pautrat2023gluestick,edstedt2024roma}, we place emphasis on the local feature-based image matching owing to its simple and intuitive functionality. The procedure for matching involves ({\romannumeral 1}) detecting interest points and representing the points with descriptors~\cite{lowe2004distinctive,bay2006surf,rublee2011orb,detone2018superpoint,dusmanu2019d2,revaud2019r2d2,yi2016lift}, ({\romannumeral 2}) matching these to make correspondences, ({\romannumeral 3}) filtering incorrect correspondences with techniques like random sample consensus (RANSAC), and finally ({\romannumeral 4}) estimating a geometric transformation matrix between an image pair with the final correspondences. 

In the procedure above, it is particularly crucial to establish correct correspondences while minimizing the number of spurious correspondences~\cite{Lim22icra-Quatro,Lim24ijrr-Quatropp}. The classical matcher is the nearest neighbor search~\cite{muja2009fast} in descriptor space. After matching, some correspondences are still incorrect because of imperfect descriptors or inherent noises. They are usually filtered out using heuristic methods, like Lowe's ratio test~\cite{lowe2004distinctive} or inlier classifiers~\cite{yi2018learning,zhang2019learning} and by robustly fitting geometric models~\cite{cavalli2020handcrafted,fischler1981random}. However, these heuristic processes require domain knowledge for parameter tuning and can easily fail under challenging conditions. These limitations of the matching are largely addressed by deep learning nowadays.

\subsection{Vision Transformer~(ViT)}

The introduction of Vision Transformer (ViT)~\cite{dosovitskiy2020image} revolutionized vision tasks, leading to methods like SuperGlue~\cite{sarlin2020superglue}, which combined ViTs with optimal transport~\cite{peyre2019computational} for improved feature matching. It is the first learning-based matcher that is trained to simultaneously match local features and filter out outliers from image pairs. By learning strong priors about scene geometry and camera motion, it demonstrates robustness to extreme variations and performs well across various data domains. However, like early Transformers~\cite{beltagy2020longformer,keles2023computational}, SuperGlue faces challenges, including being difficult to train and having computational complexity that scales quadratically with the number of keypoints.

%To tackle these problems, Lindenberger et al. proposed LightGlue, a subsequent work of SuperGlue that makes its design more efficient. Instead of reducing the network's overall capacity, LightGlue dynamically adapts its size based on the matching difficulty. It achieves this efficiency by incorporating techniques such as early stopping, feature pruning, and simpler matching processes, improving performance without sacrificing robustness

To tackle these problems, Lindenberger~\textit{et al.} proposed LightGlue~\cite{lindenberger2023lightglue}, a subsequent work of SuperGlue, that makes its design more efficient. Instead of reducing the network's overall capacity~\cite{chen2021learning,shi2022clustergnn}, LightGlue dynamically adapts its size based on the matching difficulty. It achieves this efficiency by incorporating techniques like early stopping, feature pruning, and simpler matching processes, improving performance without sacrificing robustness.

%Conversely, dense matchers like LoFTR~\cite{sun2021loftr} and other learning-based direct approaches~\cite{chen2022aspanformer,wang2022matchformer,wang2024efficient} match dense points rather than sparse ones. This improves the robustness impressively but is generally much slower than indirect approaches~\cite{sarlin2020superglue,lindenberger2023lightglue} because it processes dense features. This limits the resolution of the input images or leads to huge latency on high-resolution images. 

However, adding more Transformer-based structures to enhance the performance of LightGlue~\cite{lindenberger2023lightglue} can introduce additional computational complexity. To overcome the potential limitations of the Transformers, Mamba~\cite{gu2023mamba}, which aims to focus selectively on sequential data with linear-time complexity and selective state space updates, has emerged.

\subsection{Mamba Architecture and Hybrid Models}

Since the introduction of Mamba~\cite{gu2023mamba}, numerous novel approaches~\cite{shi2024multi,yang2024vivim} have been proposed to leverage its capability to capture long-range and spatio-temporal dependencies for vision applications. Specifically, Zhu~\textit{et al.} proposed Vision Mamba~\cite{zhu2024vision} that uses a bidirectional state space model (SSM) with the same Mamba formulation to capture more global context and improve spatial understanding. 

However, bidirectional encoding increases the computational load, which contradicts the advantage of Mamba and can slow down training and inference times. In addition, combining information from multiple directions effectively is challenging as some global context may be lost in the process. So far, models using only SSM architecture with causal convolutions are neither as efficient nor as effective as Transformer-only models. 

To resolve the potential limitations of the Mamba-only architectures, hybrid models~\cite{lieber2024jamba} utilizing Mamba-based architectures and Transformer-based architectures at the same time have emerged. Hatamizadeh and Kautz introduced MambaVision~\cite{hatamizadeh2024mambavision}, which is one of the hybrid methods. MambaVision uses a single forward pass with a redesigned Mamba block that can capture both short and long-range information and shows superior performance in terms of ImageNet top-1 throughput. It stacks its redesigned Mamba blocks and self-attention blocks with multi-layer perceptron (MLP) between blocks. Although adding MLP between blocks allows the network to extract richer high-dimensional features and then propagate them to the next layer, it is computationally expensive. Therefore, finding a way to utilize Mamba blocks with self-attention blocks with fewer resources is useful.

In this paper, we propose a novel parallel combination of Mamba and self-attention architectures for local feature matching. Unlike MambaVision that stacks Mamba and self-attention with MLP between them, our method connects them in parallel without MLP between them, resulting in more accurate performance with low latency.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%