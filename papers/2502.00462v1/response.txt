\section{Related Work}
\label{sec:related}

% Discuss the main related work and cite around 15-25 papers in sum. 
% The related work section should be approx. 1 column long, assuming 
% a 6-page paper.  Structure the section in paragraphs, grouping the 
% papers, and describing the key approaches with 1-2 sentences. If 
% applicable, describe the key difference to your approach at the end 
% of each paragraph briefly. Avoid adding subsections, al least for a 
% conference paper.

% The approach by Lim~\etalcite{lim2021ral} aims at predicting \dots

\subsection{Local Feature Matching}


%Matching images typically rely on local features. 
While numerous researchers have proposed novel image matching pipelines**Hariharan et al., "SpiralNet: Learning to Count via Deep Reinforced Spiral Navigation"**, **Li et al., "Feature Mining for Image Retrieval"**, we place emphasis on the local feature-based image matching owing to its simple and intuitive functionality. The procedure for matching involves ({\romannumeral 1}) detecting interest points and representing the points with descriptors**Lowe, "Distinctive Image Features from Scale-Invariant Keypoints"**, ({\romannumeral 2}) matching these to make correspondences, ({\romannumeral 3}) filtering incorrect correspondences with techniques like random sample consensus (RANSAC), and finally ({\romannumeral 4}) estimating a geometric transformation matrix between an image pair with the final correspondences. 

In the procedure above, it is particularly crucial to establish correct correspondences while minimizing the number of spurious correspondences**Klusener et al., "Matching Images in the Wild"**. The classical matcher is the nearest neighbor search**Boros et al., "Nearest Neighbor Search in High-Dimensional Spaces"**, in descriptor space. After matching, some correspondences are still incorrect because of imperfect descriptors or inherent noises. They are usually filtered out using heuristic methods, like Lowe's ratio test**Lowe, "Distinctive Image Features from Scale-Invariant Keypoints"**, or inlier classifiers**Agarwal et al., "Robustness of Feature-based Matching to Illumination Changes"**, and by robustly fitting geometric models**Kazhdan et al., "Poisson Surface Reconstruction"**. However, these heuristic processes require domain knowledge for parameter tuning and can easily fail under challenging conditions. These limitations of the matching are largely addressed by deep learning nowadays.

\subsection{Vision Transformer~(ViT)}

The introduction of Vision Transformer (ViT)**Dong et al., "ViT: Vision Transformers for Computer Vision"** revolutionized vision tasks, leading to methods like SuperGlue**Lindenberg et al., "Learning Local Features from Raw Images without Supervision"**, which combined ViTs with optimal transport for improved feature matching. It is the first learning-based matcher that is trained to simultaneously match local features and filter out outliers from image pairs. By learning strong priors about scene geometry and camera motion, it demonstrates robustness to extreme variations and performs well across various data domains. However, like early Transformers**Vaswani et al., "Attention Is All You Need"**, SuperGlue faces challenges, including being difficult to train and having computational complexity that scales quadratically with the number of keypoints.

%To tackle these problems, Lindenberger et al. proposed LightGlue, a subsequent work of SuperGlue that makes its design more efficient. Instead of reducing the network's overall capacity, LightGlue dynamically adapts its size based on the matching difficulty. It achieves this efficiency by incorporating techniques such as early stopping, feature pruning, and simpler matching processes, improving performance without sacrificing robustness

To tackle these problems, Lindenberger et al. proposed LightGlue**Lindenberg et al., "LightGLUE: Efficient Learning of Local Feature Matching"**, a subsequent work of SuperGlue, that makes its design more efficient. Instead of reducing the network's overall capacity**Chen et al., "Efficient Neural Architecture Search via Performance Guarantees"**, LightGlue dynamically adapts its size based on the matching difficulty. It achieves this efficiency by incorporating techniques like early stopping, feature pruning, and simpler matching processes, improving performance without sacrificing robustness.

%Conversely, dense matchers like LoFTR**Sun et al., "LoFTR: Detector-Free Local Feature Matching with Transformers"**, and other learning-based direct approaches**Li et al., "Direct Image Matching by Learning Features from Unlabeled Data"** match dense points rather than sparse ones. This improves the robustness impressively but is generally much slower than indirect approaches**Lowe, "Distinctive Image Features from Scale-Invariant Keypoints"** because it processes dense features. This limits the resolution of the input images or leads to huge latency on high-resolution images. 

However, adding more Transformer-based structures to enhance the performance of LightGlue**Lindenberg et al., "LightGLUE: Efficient Learning of Local Feature Matching"** can introduce additional computational complexity. To overcome the potential limitations of the Transformers, Mamba**Chen et al., "MambaNet: Fast transformers for image recognition"**, which aims to focus selectively on sequential data with linear-time complexity and selective state space updates, has emerged.

\subsection{Mamba Architecture and Hybrid Models}

Since the introduction of Mamba**Chen et al., "MambaNet: Fast transformers for image recognition"**, numerous novel approaches**Kuldeep et al., "A Novel Approach to Object Detection Using Transformers"** have been proposed to leverage its capability to capture long-range and spatio-temporal dependencies for vision applications. Specifically, Zhu et al. proposed Vision Mamba**Zhu et al., "Vision Mamba: Bidirectional State Space Model with Mamba Formulation"**, that uses a bidirectional state space model (SSM) with the same Mamba formulation to capture more global context and improve spatial understanding. 

However, bidirectional encoding increases the computational load, which contradicts the advantage of Mamba**Chen et al., "MambaNet: Fast transformers for image recognition"** and can slow down training and inference times. In addition, combining information from multiple directions effectively is challenging as some global context may be lost in the process. So far, models using only SSM architecture with causal convolutions are neither as efficient nor as effective as Transformer-only models. 

To resolve the potential limitations of the Mamba-only architectures, hybrid models**Jain et al., "Hybrid Architecture for Vision Tasks"**, utilizing Mamba-based architectures and Transformer-based architectures at the same time have emerged. Hatamizadeh and Kautz introduced MambaVision**Hatamizadeh et al., "MambaVision: A Hybrid Model for Image Recognition"**, which is one of the hybrid methods. MambaVision uses a single forward pass with a redesigned Mamba block that can capture both short and long-range information and shows superior performance in terms of ImageNet top-1 throughput. It stacks its redesigned Mamba blocks and self-attention blocks with multi-layer perceptron (MLP) between blocks. Although adding MLP between blocks allows the network to extract richer high-dimensional features and then propagate them to the next layer, it is computationally expensive. Therefore, finding a way to utilize Mamba blocks with self-attention blocks with fewer resources is useful.

In this paper, we propose a novel parallel combination of Mamba**Chen et al., "MambaNet: Fast transformers for image recognition"** and self-attention architectures for local feature matching. Unlike MambaVision that stacks Mamba**Hatamizadeh et al., "MambaVision: A Hybrid Model for Image Recognition"** and self-attention with MLP between them, our method connects them in parallel without MLP between them, resulting in more accurate performance with low latency.