%!TEX root = ./mainRSCED.tex
\section{Solution via Benders' Decomposition}
\label{sec:algorithm}
% \rev{Contingency pre-filtering, or lazy constraint generation (add constraint only if violated at optimal without it)}
% In general, evaluation of \CVaR{} is difficult, complicating our ability to obtain solutions to \eqref{eq:RSCED}. However, by leveraging the definition from Rockafellar and Uryasev, we can reformulate the objective into a more amenable form. Specifically, we have
% \begin{equation*}
% \begin{aligned}
% 	&\CVaR{}_{\alpha} \left[ \v{c}^\top \v{g} + C_k(\v{\delta g}_k, \v{\delta d}_k) \right]
% 	\\
% 	&\;= \min_{u} u + \E\left[ \v{c}^\top \v{g} + C_k(\v{\delta g}_k, \v{\delta d}_k) - u \right]^+
% 	\\
% 	&\;= \min_{u} u + \sum_{k = 1}^l p_k \left[ \v{c}^\top \left( \v{g} + [\v{\delta g}_k]^+ + [\v{\delta g}_k]^- \right) + \voll^\top \v{\delta d}_k - u \right]^+
% \end{aligned}
% \end{equation*}

% As we saw in \eqref{eq:RSCED.lp}, 
As detailed in Section \ref{sec:formulating}, \RSCED{} can be written as a large linear program. Collecting the decision variables into vectors for all $k\in [K]$ with
% \rev{transpose column vector}
%
% \begin{equation}\nonumber
%     \v{x}_0 := \begin{pmatrix} z \\ \v{g} \\ \underline{\v{r}} \\ \overline{\v{r}} \end{pmatrix}, \quad \v{x}_k := \begin{pmatrix} y_k \\ \v{\delta g}_k \\ \v{\delta d}_k \end{pmatrix}, \quad k\in[K],
% \end{equation}
\begin{equation}\nonumber
    \v{x}_0 := \begin{pmatrix} z & \v{g}^{\top} & \underline{\v{r}}^{\top} & \overline{\v{r}}^{\top} \end{pmatrix}^{\top}, \quad \v{x}_k := \begin{pmatrix} y_k & \v{\delta g}^{\top}_k & \v{\delta d}^{\top}_k \end{pmatrix}^{\top},
    % , \quad k\in[K],
\end{equation}
%
% Under the DC approximations, each of the \SCED{} formulations can be formulated as a large linear program. \RSCED{} is no exception. To illustrate, we leverage the variational form of \CVaR{} and the discrete distribution of realizable contingencies, which allows us to write \eqref{eq:RSCED} in the following form 
% %
% \begin{equation}
% \begin{aligned}
%     &\underset{z, \v{g}, \v{\delta g}, \v{\delta d}}{\text{minimize}} && z + \frac{1}{1 - \alpha} \sum_{k = 0}^K p_k \left[ \v{c}^\top \v{g} + C_k(\v{\delta g}_k, \v{\delta d}_k) - z \right]^+, \\
%     &\text{subject to} && \eqref{eq:RSCED.nominal} \text{ --- } \eqref{eq:RSCED.se.bounds}, \; \forall k = 1, \dots, K.,
% \end{aligned}
% \label{eq:RSCED.2}
% \end{equation}
% where $[ \cdot ]^+$ is the positive part of its argument.
% %
% Using the epigraph form, this is equivalent to
% \begin{equation}
% \begin{aligned}
%     &\underset{z, \v{y}, \v{g}, \v{\delta g}, \v{\delta d}}{\text{minimize}} && z + \frac{1}{1 - \alpha} \sum_{k = 0}^K p_k y_k, \\
%     &\text{subject to} && \eqref{eq:RSCED.nominal} \text{ --- } \eqref{eq:RSCED.se.bounds}, \\
%     &&& y_0 \ge \v{c}^\top \v{g} - z, \; y_0 \ge 0, \\
%     &&& y_k \ge \v{c}^\top \v{g} + C_k(\v{\delta g}_k, \v{\delta d}_k) - z, \; y_k \ge 0, \\
%     &&& \forall k = 1, \dots, K.
% \end{aligned}
% \label{eq:RSCED.LP}
% \end{equation}
% %
% By collecting the variables into the vectors,
% \begin{equation}
%     \v{x}_0 := \begin{pmatrix} z \\ y_0 \\ \v{g} \end{pmatrix}, \quad \v{x}_k := \begin{pmatrix} y_k \\ \v{\delta g}_k \\ \v{\delta d}_k \end{pmatrix}, \quad k = 1, \dots, K,
% \end{equation}
 problem \eqref{eq:RSCED.lp} can now be written as
%
\begin{equation}
\begin{aligned}
    &\underset{\v{x}_0, \v{x}_1, \dots, \v{x}_k}{\text{minimize}} && \v{c}^\top \v{x}_0 + \frac{1}{1 - \alpha} \sum_{k = 1}^K \v{c}_k^\top \v{x}_k, \\
    &\text{subject to} && \v{A} \v{x}_0 \le \v{b}, \\
    &&& \v{A}_k \v{x}_0 + \v{E}_k \v{x}_k \le \v{b}_k, \; k\in[K],
\end{aligned}
\label{eq:RSCED.decomposable}
\end{equation}
for appropriate $\v{c}$, $\v{c}_k$, $\v{A}$, $\v{b}$, $\v{A}_k$, $\v{E}_k$, and $\v{b}_k$.
%
% We remark that the \CSCED{} problem \eqref{eq:csced} can be formulated similarly and the techniques described below are transferable.
While linear programs such as \eqref{eq:RSCED.decomposable} admit polynomial-time algorithms, the large scale of these problems for practical power systems results in very high dimensionality, demonstrated in Table \ref{tab:ED.dim}. This results in significant time complexity, rendering the problems intractable within the time periods typically required for making dispatch decisions. \revision{Fortunately, the formulation in \eqref{eq:RSCED.decomposable} is amenable to decomposition whose structure can be exploited to parallelize and speed up computation.}


%
\begin{table}[ht]
\centering
	\caption{Problem dimensions for the IEEE 200-bus network.}
	\begin{tabular}{l c c}
		\toprule
		\textbf{Formulation} & \textbf{Variables} & \textbf{Constraints} \\ 
		\midrule 
		ED & 49 & 589 \\
		\PSCED & 49 & 120,639 \\
		\CSCED & 61,054 & 386,954 \\
		\RSCED & 61,398 & 411,454 \\
		\bottomrule \\
	\end{tabular}
	\label{tab:ED.dim}
\end{table}
%
%  Specifically, we 
\revision{Let $\Xset := \{ \v{x}_0 | \v{A} \v{x}_0 \leq \v{b} \}$.
% Since $\v{x}_k$'s are coupled in the constraints only to $\v{x}_0$, and not to another $\v{x}_{k'}$, $k'\neq k$ in \eqref{eq:RSCED.decomposable}, we 
Then, \eqref{eq:RSCED.decomposable} can be written as
\begin{equation}
\begin{aligned}
    \underset{\v{x}_0 \in \Xset}{\text{minimize}} \ \ \v{c}^\top \v{x}_0 + \frac{1}{1 - \alpha} \sum_{k = 1}^K J_k^\star(\v{x}_0), 
    % \\
    % &\text{subject to} && \v{A} \v{x}_0 \le \v{b}, \\
\end{aligned}
\label{eq:decomp.sup}
\end{equation}
}
where
\begin{equation}
\begin{aligned}
    J_k^\star(\v{x}_0) :=\ &\underset{\v{x}_k}{\text{minimum}} && \v{c}_k^\top \v{x}_k, \\
    &\text{subject to} && \v{A}_k \v{x}_0 + \v{E}_k \v{x}_k \le \v{b}_k.
\end{aligned}
\label{eq:decomp.sub}
\end{equation}
\revision{Here, \eqref{eq:decomp.sup} defines the primary problem, while \eqref{eq:decomp.sub} defines the sub-problems that can be solved for a given value of $\v{x}_0$ from the primary problem. Algorithms such as Benders' decomposition in \cite{benders1962partitioning} and critical region exploration in \cite{guo2017robust} can take advantage of this decomposition. We focus on the former and present it formally in Algorithm \ref{alg:bender}. For completeness, we provide an overview of Benders' decomposition as applied to our problem here, and refer to, e.g., \cite{geoffrion1972generalized} for a more detailed treatment.}




\begin{algorithm}[t]
	\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
	\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
	\kwInit{Choose $\v{x}_0^1 \in \Xset$, $t_k^\star = -\infty,  k\in[K]$.}
	\For{$i = 1, 2, \dots$}{ \For{$k \in[K]$}{Set $\v{\lambda}^{i}_k$, $J_k^i$ as the optimizer, optimum of\begin{equation}
		\begin{aligned}& \underset{\v{\lambda}_k\geq 0}{\text{maximize}} && -\v{b}_k^T \v{\lambda}_k + \v{\lambda}_k^T \v{A}_k \v{x}_0^i,
			\\
			& \text{subject to} && \v{c}_k + \v{E}_k^T \v{\lambda}_k = 0.
		\end{aligned}
		\end{equation}
		\label{alg:bender.1}
    }
		% Given $\v{x}_0^i$, solve the following dual problem for each $k = 1, \dots K$, and let $\v{\lambda}_k^i$ be the optimizer.
		\label{alg:bender.term}
		\If{$J_k^i = t_k^\star \; \forall k\in[K]$}{
        % The optimal solution has been found. 
        Terminate.}
		% Construct the Bender's cuts, given by
		% \begin{equation}
		% \begin{aligned}
		% 	% t_k \ge J_k^i + \v{\lambda}_k^i \v{A}_k (\v{x}_0 - \v{x}_0^i).
  %           t_k \ge J_k^i + (\v{x}_0 - \v{x}_0^i)^{\top}\v{A}^{\top}_k\v{\lambda}_k^i  .
		% \end{aligned}
		% \end{equation}
		% \label{alg:bender.2}
  % \vspace{-0.12in}
		% Solve the augmented \eqref{eq:decomp.sup} shown below, and 
        Set $(\v{x}_0^{i + 1}, t_1^\star, \dots, t_k^\star)$ as the optimizers of
		\begin{equation}\nonumber 
		\begin{aligned}
			& \underset{\substack{\v{x}_0 \in \Xset, \\ t_1, \dots, t_K}}{\text{minimize}} && \v{c}^T \v{x}_0 + \frac{1}{1 - \alpha} \sum_{k = 1}^K t_k, \\
			& \text{subject to} && t_k \ge J_k^{\ell} + (\v{x}_0 - \v{x}_0^{\ell})^{\top}\v{A}^{\top}_k\v{\lambda}_k^{\ell},
			\\ &&&
			% \;
			 \ell \in [i], \; k \in [K].
		\end{aligned}
		\end{equation}
		\label{alg:bender.3}
	}
\caption{Benders' decomposition for \eqref{eq:RSCED.decomposable}.}
\label{alg:bender}
\end{algorithm}

\revision{To understand the algorithm, notice that problem  \eqref{eq:decomp.sup} is equivalent to the following problem:
\begin{equation}
\begin{aligned}
    &\underset{\substack{\v{x}_0 \in \Xset, \\ {t}_1, \dots, {t}_K}}{\text{minimize}} && \v{c}^\top \v{x}_0 + \frac{1}{1 - \alpha} \sum_{k = 1}^K t_k, \\
    &\text{subject to} && t_k \geq J_k^\star(\v{x}_0), k \in [K].
\end{aligned}
\label{eq:decomp.sup.epi}
\end{equation}
}
The Benders' decomposition algorithm constructs sequentially tighter lower bounds for $J_k^\star(\v{x}_0)$ via \emph{dual cuts} \revision{and solves the primary problem with the lower bounds. To explain the process,} associate dual variable $\v{\lambda}_k$ to the constraint in \eqref{eq:decomp.sub} \revision{and use strong duality of linear programming \cite{boyd2004convex} write
}
%
\begin{equation}
\begin{aligned}
    J_k^\star(\bar{\v{x}}_0) \; = \; &\underset{\v{\lambda}_k\geq 0}{\text{maximize}} && -\v{b}_k^\top \v{\lambda}_k + \bar{\v{x}}_0^\top \v{A}_k^\top \v{\lambda}_k, \\
    &\text{subject to} && \v{c}_k + \v{E}_k^\top \v{\lambda}_k = 0
\end{aligned}
\label{eq:decomp.sub.dual}
\end{equation}
for an arbitrary $\bar{\v{x}}_0 \in \Xset$. 
\revision{The optimization problem in the right-hand side of \eqref{eq:decomp.sub.dual} is the dual problem of the sub-problem in \eqref{eq:decomp.sub}.
%
If $\v{\lambda}_k^\star$ is an optimizer of this dual problem, then
\begin{align}
    J_k^\star(\bar{\v{x}}_0) = -\v{b}_k^\top \v{\lambda}_k^\star + \bar{\v{x}}_0^\top \v{A}_k^\top \v{\lambda}_k^\star.
    \label{eq:benders.dual.opt}
\end{align}
For any $\v{x}_0 \in \Xset$ that is not necessarily the same as $\bar{\v{x}}_0$,
\begin{equation}
\begin{aligned}
    J_k^\star(\v{x}_0)
    &\ge -\v{b}_k^\top \v{\lambda}_k^\star + \v{x}_0^\top \v{A}_k \v{\lambda}_k^\star 
    \\
    &= J_k^\star(\bar{\v{x}}_0) + (\v{x}_0 - \bar{\v{x}}_0)^\top \v{A}_k^\top \v{\lambda}_k^\star,
\end{aligned}
\label{eq:benders.hyperplane}
\end{equation}
where the first step follows from the feasibility of $\v{\lambda}_k^\star$ in the dual problem \eqref{eq:decomp.sub.dual} to compute  $J_k^\star(\v{x}_0)$ and the second step uses \eqref{eq:benders.dual.opt}. Thus, $J_k^\star(\v{x}_0)$ lies above all possible ``cuts'' defined in \eqref{eq:benders.hyperplane} for any $\bar{\v{x}}_0 \in \Xset$.
Benders' decomposition advocates replacing the constraint $t_k \geq J_k^\star({\v{x}}_0)$ in the primary problem \eqref{eq:decomp.sup.epi} with a growing collection of cuts. At each iteration, a new cut is generated using the right-hand side of \eqref{eq:benders.hyperplane} for each $k\in [K]$ with $\bar{\v{x}}_0$ being the optima of the same problem in the previous iteration.}


% Equipped with \eqref{eq:benders.hyperplane}, we can then solve the primary problem with the constraint
% \begin{align}
%     t_k \geq J_k^\star(\bar{\v{x}}_0) + (\v{x}_0 - \bar{\v{x}}_0)^\top \v{A}_k^\top \v{\lambda}_k^\star
%     \label{eq:benders.sup.surrogate}
% \end{align}
% for any sequence of points $\bar{\v{x}}_0 \in \Xset$ instead of the constraint $t_k \geq J_k^\star({\v{x}}_0)$ in \eqref{eq:decomp.sup.epi}. Each such ``cut'' in \eqref{eq:benders.sup.surrogate} defines a lower bound for $J_k^\star({\v{x}}_0)$.
% The Benders' decomposition algorithm solves the primary problem \eqref{eq:decomp.sup.epi} with its constraint replaced by these cuts with $\bar{\v{x}}_0$'s taking values at the optimal solutions of the same from previous iterations.


% and \revision{the objective helps to define a supporting hyperplane of the primal problem \eqref{eq:decomp.sub} of the form,}
% \rev{compress this equation}
% for any $\v{x}_0$. 
% \revision{This hyperplane can now be introduced as a surrogate to $J_k^\star({\v{x}}_0)$ into the primary problem reformulation in \eqref{eq:decomp.sup.epi}, i.e., instead of imposing $t_k \geq J_k^\star({\v{x}}_0)$, we restrict $t_k$ to lie above the hyperplane defined above .}



\revision{As shown in \cite{benders1962partitioning}, Benders' decomposition converges to an optimal solution of \eqref{eq:RSCED.decomposable}, provided that each subproblem \eqref{eq:decomp.sub} has a feasible solution for all $\v{x}_0 \in \Xset$.} 
% if Assumption \ref{ass:benders} holds, then  converges in finitely many iterations. \bose{Add a line about what exactly is solved in the primary problem with the dual cut.}
% \revision{To describe the algorithm, we begin with the following assumption.}
% \begin{assumption}
%   Each subproblem \eqref{eq:decomp.sub} has a feasible solution for all $\v{x}_0 \in \{ \v{x} | \v{A} \v{x} \le \v{b} \}$.
%   \label{ass:benders}
% \end{assumption}
% We include a discussion of this assumption at the end of the section.
% Some discussions on Assumption \ref{ass:benders} are in order. 
Such an assumption is difficult to guarantee and, in fact, is not typically satisfied for the \RSCED{} formulation in \eqref{eq:RSCED.lp}. One can introduce feasibility cuts to tackle infeasible sub-problems as in \cite{grothey1999note}, but the finite-time convergence guarantee does not apply to such settings. In our simulations, we consider an approach of adding heavily-penalized slack variables to the inequality constraints in \eqref{eq:RSCED.lp} to tackle infeasibility.


% The \CSCED{} problem \eqref{eq:csced} also admits a decomposition of the form in \eqref{eq:RSCED.decomposable}. As a result, 
Benders' decomposition has been previously applied to \SCED{} problems in \cite{liu2015computational,shi2022scenario}. While our \CVaR-sensitive $\RSCED$ formulation generalizes \CSCED{} in the sense that load-shed may be allowed in some scenarios depending upon SO's risk aversion, \revision{this generalization does not fundamentally complicate algorithm design.}



% from a computational standpoint, the incorporation of risk-aversion via $\CVaR$ introduces no fundamental increase in computational complexity for determining dispatch and pricing over the existing $\PSCED$ and $\CSCED$ formulations based upon }