\section{Related Work}
\label{sec:related_work}

\begin{figure*}[t!]    
\includegraphics[width=\textwidth]{figures/PhantomWiki-pipeline.pdf}
    \caption{\tbf{Overview of the \ours pipeline.} 
    }
    \label{fig:pipeline}
\end{figure*}

Agent benchmarks, such as $\tau$-bench \citep{yao2024tau}, ToolWoz \citep{lattimer2024sparse}, Alfworld \citep{shridhar2020alfworld} and WebArena \citep{zhou2023webarena}, focus on tasks where the agent is given a binary reward for successful completion of a task (e.g., booking a flight, making a purchase). 
In this work, we focuses more on tasks where the agent is rewarded for responding to a question with a factually correct answer. (In \Cref{sec:dataset}, we concretize what we mean by a ``fact'' and ``correctness''.) 
\citet[Section~3.2]{zhou2023webarena} include a category of information-seeking tasks, however these tasks often require navigation across multiple pages or focus on user-centric content.
\citet[Appendix.~A]{yao2024tau} measure task difficulty based on the average success rate of frontier models (e.g., GPT-4). \textit{Our work defines a model-agnostic measure of difficulty, which we show provides more meaningful insight into the reasoning and retrieval aspects of LLMs.}

In the QA domain, existing benchmarks are designed to test whether LLMs are able to reason and use tools.  Closer to our work in the space of question-answering agents is the ToolQA benchmark of \citet{zhuang2023toolqa}. They introduce a framework to construct question-answer pairs from databases and documents by first generating question templates using LLMs, then filtering for high-quality templates, and finally deriving ground-truth answers by writing corresponding Python programs for each question template. \citet[Tab.~1]{zhuang2023toolqa} construct two pure-text datasets: SciREX with 438 documents and 5 question templates, and Agenda with 10k event entries and 10 question templates. \textit{In constrast, \ours generates instances at much larger scale with 50 question templates and 1 million documents.}

For generating factually-grounded answers, retrieval augmented generation (RAG) has emerged as the predominant paradigm \citep{lewis2020retrieval, karpukhin2020dense, guu2020retrieval}.
However, evaluating RAG systems is notoriously difficult, leading to a flourishing of retrieval benchmarks \citep{petroni2020kilt,saad2023ares,jin2024flashrag, hsia2024ragged, mao2024xrag, rau2024bergen}. A key pain-point of RAG is handling questions that involve multi-hop reasoning. This motivated \citet{tang2024multihop} to design the MultiHop-RAG dataset with synthetically generated questions and \citet{su2024bright} to curate a dataset of question-answer pairs that requires intensive reasoning to retrieve relevant documents.

\textit{Importantly, none of these benchmarks create the underlying corpus, a limitation which we bridge in this work.}
Logical reasoning tasks have become central to LLM evaluation and have garnered significant attention in recent time \citep{zhu2023large}. Many existing benchmarks do not disentangle the evaluation on logical reasoning with other abilities such as natural language inference and commonsense reasoning \citep{sakaguchi2021winogrande, zellers2019hellaswag, sprague2023musr}.
A line of works focuses on the synthesis of datasets containing 
a variety of logic reasoning tasks \citep{tafjord2020proofwriter, saparov2022language, liu2020logiqa, han2022folio, weston2015towards}. Closer to our work, \citet{sinha2019clutrr} construct short stories about individuals related through a family graph and ask questions about their kinship relationships to benchmark the inductive reasoning capabilities. \textit{However, theirs is distinct from our work in that all relevant information for a specific question is centralized in a single article; \ours requires that the relevant information first be retrieved from a large-scale corpus.}