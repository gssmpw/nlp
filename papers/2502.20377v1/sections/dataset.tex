\section{\ours Construction}
\label{sec:dataset}

\ours is at its core an on-demand random generator of fictional worlds. 
Similarly to the wiki hosting services popular in film, video games, and literature\footnote{For example, see \href{https://stardewvalley.fandom.com/}{https://stardewvalley.fandom.com} or \href{https://www.harrypotter.fandom.com/}{https://harrypotter.fandom.com}.},
we represent these fictional worlds through Wikipedia-like biographical entries about their characters. We then test the model's retrieval skills and its understanding of the fictional world through an accompanying set of automatically generated question-answer pairs.

\subsection{Generating a \ours Universe}
The first stage of the \ours pipeline generates a random universe of $n$ characters as well as the document corpus describing it, as illustrated in \Cref{fig:pipeline}, (1-2).

\tbf{Generating Characters.}
Each character in a \ours universe is described through its \emph{social relationships} and \emph{personal facts} (\Cref{fig:pipeline}, (1)). 
For the social relationships, we first generate family trees, following the family tree generator of \citet{hohenecker2020ontology}. We iteratively pick a person and generate their parent or child based on various constraints\footnote{For example, the number of offspring of a person has to be smaller than some threshold, parents of the people at the maximal tree level will not be generated, etc.}, until the user-specified universe size of $n$ people is reached.
The user can also specify other hyperparameters like the number of trees, their maximal depth, and the maximal number of offspring for each person.
In addition to the family trees, we generate a friendship graph using the Erdős–Rényi model (making two people friends with some fixed probability, typically controlled by the desired average number of friendships.)

\tbf{Generating Facts.}
Next, we generate personal facts for each person in the \ours universe. 
Names are assigned during the family generation procedure, with the first name sampled based on the character's gender and the surname based on the family tree, resulting in 15M full names in total\footnote{We use unique names in our experiments, but \ours also supports repeated names.}.
We also add dates of birth in a way that is consistent with the existing family relations, and assign each person a job and a hobby that we uniformly sample from over 300 and 600 options respectively.

\tbf{Generating Articles.}
Given all relevant facts for each person, we convert them into articles using pre-defined templates, e.g. ``The job of David is a farmer. The hobby of David is birdwatching.'' (see \Cref{fig:pipeline}, (2)). 
This construction conveys the necessary information while keeping the articles short (about $160$ tokens on average).
While it is possible to extend the article generation process to LLM-based methods (see e.g. \citealt{shao2024assisting}), this poses the challenge of guaranteeing factual correctness without additional costs and external supervision. 
This has been supported by our preliminary experiments on article generation using \llama, where we observed factual errors in the resulting articles; therefore we do not use LLMs and rely entirely on templates.
The articles are the only component of \ours available to the model during its evaluation.

\subsection{Generating Question-Answer Pairs}
\label{sub:qagen}

In the second half of the \ours pipeline, we generate a set of questions with verifiable answers, as shown in \Cref{fig:pipeline}, (3-4).

\tbf{Generating Questions.}
We implement automatic question generation through
a context-free grammar (CFG, \citealt{hopcroft2001introduction}) of \emph{question templates}, which we then use to sample complete questions.
For example, the question template ``\textit{Who is the \emph{$<$relation$>$} of \emph{$<$name$>$}}?'' 
can be used to sample the question
``\textit{Who is the friend of David?}'' (see \Cref{fig:pipeline}, (3)).
The main advantage of using a CFG is that it efficiently and systematically obtains \emph{all} possible compositions of questions for some recursion depth $d$.
For instance, the following subset of our context-free grammar:
\begin{align*}
    S &\rightarrow\; \t{Who is } R \t{?}\\
    R &\rightarrow\; \t{the $<$\textit{relation}$>$ of } R'\\
    R' &\rightarrow\; R \mid\, \t{$<$\textit{name}$>$}
\end{align*}
can lead to questions ranging from ``\textit{Who is the friend of David?}'' to ``\textit{Who is the nephew of the friend of the brother of David?}'' as $d$ increases. In addition to these nested compositions, our CFG also supports questions about personal attributes (e.g. ``\textit{Who is the person whose hobby is birdwatching?}''), aggregation questions (``\textit{How many brothers does David have?}''), and combinations of all three (``\textit{How many friends does the brother of the person whose hobby is birdwatching have?}'')
(For the full CFG see \Cref{app:cfg}.)

\tbf{Generating Answers.}
To ensure that the answers to the sampled questions are verifiably correct, we represent our generated universe in Prolog, a logic programming language \citep{sterling1994art}. 
Each Prolog program consists of a set of facts known about the world such as \texttt{hobby("David", "birdwatching")}, and a set of rules defining how facts are related to each other, such as \texttt{nephew(X, Y) :- sibling(X, A), son(A, Y)}. The Prolog program uses these facts and rules to deduce the exhaustive set of answers to its \emph{queries} (i.e., the CFG-generated questions).
For example, a question 
``\textit{Who is the nephew of the friend of the person whose hobby is birdwatching?}''
corresponds to the three-statement Prolog query
\texttt{?- nephew(X2, Y), friend(X1, X2), hobby(X1, "birdwatching")},
which returns all people satisfying these constraints in the \ours universe (see \Cref{fig:pipeline} (4)).

To construct the Prolog queries automatically, we modify the CFG algorithm to generate both the question and query templates in parallel. We note, however, that the queries are separate from the final \ours corpus and question-answer pairs, and the answers returned by the Prolog program should be held out as part of the evaluation procedure.

\subsection{\ours Complexity}
\label{sec:complexity}
The goal of \ours is to generate memorization-resistant evaluation datasets that are challenging in both reasoning and retrieval aspects.
In this section, we discuss our conceptual and practical design choices that help us achieve this goal.

\tbf{Universe Space Complexity.}
To ensure that our evaluation with \ours is memorization and data leakage-resistant, 

we first show that the space of possible universes is sufficiently large to generate enough unique instances.
Observe that the number of possible friendship assignments grows at the rate of $\Theta(2^{n^2})$~\citep[Ex.~II.5]{flajolet2009analytic} as the number of individuals $n$ in the universe increases.
Similarly, assuming each individual is assigned one fact from each category (job, hobby, etc.), the number of possible fact assignments grows at the rate $\Theta(c^n)$, where $c$ is the total number of choices across the categories.
\ours thus samples a corpus from $\Theta ( 2^{n^2} c^n )$ possible universes, which leads to diverse datasets optimal for data leakage-resistant evaluation.
We note that as future work \ours could be extended to increase this diversity, e.g. by adding a temporal dimension of events.

\tbf{Reasoning Complexity.}
The CFG enables us to recursively compose templates that lead to complex reasoning questions.
Observe that our CFG in \Cref{app:cfg} produces $\Theta(d)$ question templates as the recursion depth $d$ increases.
Moreover, we can increase the difficulty of each template by increasing the number of \textit{reasoning steps}.
For example, substituting \textit{$<$relation$>$} with \textit{nephew} in a template adds two reasoning steps (\texttt{nephew(X, Y) :- sibling(X, A), son(A, Y)}), since \ours articles only contain immediate family relationships like \emph{sibling} and \emph{son}.
In contrast, substituting \textit{$<$relation$>$} with \textit{second cousin} would lead to five reasoning steps.
As we will show in \Cref{sec:main_results}, \ours questions are sufficiently complex to evaluate reasoning capabilities of state-of-the-art LLMs.
We further note that \ours's CFG can be easily extended to support more question types like comparison and multiple-constraint questions.


\tbf{Retrieval Complexity.}
To assess a model's retrieval capabilities, we increase the universe size $n$ so that the document corpus exceeds the model's context length---this makes a retriever necessary to answer questions correctly.
For state-of-the-art LLMs with a context length of 128K, such as OpenAI's \gpt and Meta's \llama,  this corresponds to \ours universes of $n \gtrapprox 1$K.
This increases to $n \gtrapprox 3$K for Google's \gemini with context length 1M.
Further scaling $n$ leads to further increase in retrieval difficulty.
In \Cref{tab:timings}, we show that \ours is well-suited for generating universes of this size on standard CPU hardware:
generating questions with recursion depth $d=10$ for size $n = 100$K---well beyond any existing LLM's context length---takes just 6 minutes on 8 Intel Cascade Lake CPU cores.
Moreover, we can conveniently generate instances of $n = 1$M, which is on the scale of Wikipedia's corpus of {2 million biographical entries}\footnote{\href{https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Biography}{https://en.wikipedia.org/wiki/Wikipedia:WikiProject\_Biography}, as of January 30, 2025.}.


\begin{table}[h]
    \centering
    \caption{Runtime breakdown of generating a \ours instance for facts, articles and questions for universe sizes $n$.} 
    \label{tab:timings}
\begin{tabular}{ccccc}
\toprule
$n$ & Total Runtime & Facts & Articles & Questions\\
\midrule
$10^2$ & 0.97~s & 0.46~s & 0.07~s & 0.44~s \\
$10^3$ & 2.86~s & 0.90~s & 0.59~s & 1.37~s \\
$10^4$ & 20.91~s & 5.38~s & 5.87~s & 9.66~s \\
$10^5$ & 5.57~m & 0.81~m & 0.97~m & 3.79~m\\
$10^6$ & 3.86~h & 9.47~m & 11.77~m & 3.51~h\\
\bottomrule
\end{tabular}
\end{table}
