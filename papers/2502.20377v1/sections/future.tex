\section{Conclusion and Future Work}
\label{sec:future}

We introduce \ours---a benchmarking framework to evaluate reasoning and retrieval capabilities of language models.
As we increase the question complexity and universe size, we observe that current state-of-the-art LLMs struggle in both aspects.
\ours is scalable and memorization-resistant, hence well-suited to evaluate future generations of language models.

Our work brings forth several research directions.
Noting how we generate document corpora and questions, \ours is resistant to data contamination.
We leave to future work to empirically test this claim, and to develop theory to formally prove that our benchmark is memorization-resistant.
In this work we focus on question-answering over text corpora.
We hope to extend \ours to other knowledge bases and modalities such as vision and audio, enabling analogous test suites for multimodal models.

