\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/difficulty-f1.pdf}
    \caption{%
    \textbf{
    F1 scores as a function of question difficulty, measured by \textit{reasoning steps}.
    }%
    We plot LLM performance on universe size $n=50$, and report F1 scores averaged over 3 generation seeds.
    Increasing question difficulty in \ours reveals a clear decline across all state-of-the-art LLMs and prompting techniques, showing their struggle with reasoning.
    }
    \label{fig:exp__f1_v_difficulty}
\end{figure*}

\section{Evaluating Reasoning}
\label{sec:reasoning}

To isolate LLM reasoning capabilities with \ours, we investigate model performance on small universes ($n=50$) in \Cref{fig:exp__f1_v_difficulty}.
Note that contexts of all LLMs can fully include small universe document corpora.
Each \ours dataset contains questions covering a wide range of difficulty. 
We evaluate three approaches: \simpleprompting, \ragprompting, and \agenticprompting.
For each we plot the F1 scores as a function of question difficulty, as measured by the number of \textit{reasoning steps} necessary to answer the question.
As mentioned in \Cref{sec:complexity}, this is determined by the type of question templates and the sampled relationships.
For all LLMs and prompting techniques, we verify empirically that \textbf{questions with larger reasoning steps are indeed more challenging to answer}. By allowing question difficulty to be adjusted, \ours serves as a foundational benchmark for evaluating reasoning capabilities in language models.

\zeroshotsimple performance declines sharply as the number of reasoning steps increases for all LLMs, except for \deepseek, which deteriorates more gradually. LLMs perform better with \cotsimple than with \zeroshotsimple, but each additional reasoning step remains increasingly challenging. This suggests that even in the absence of retrieval constraints, LLMs struggle to navigate logical reasoning sequences.

\ragprompting techniques (\zeroshotrag and \cotrag) stunt reasoning performance across the board---F1 scores are near zero on questions with 5 or more reasoning steps as opposed to 15 steps for \simpleprompting.
We attribute this to a core problem with \ragprompting: retrieving documents in the initial prompt before starting to answer the question, as opposed to reasoning through the question and retrieving documents dynamically.

We find that \ragprompting techniques can only answer questions that require a single reasoning step, like \textit{Who is the friend of David?}.
On the other hand, answering questions that require information from \textit{multiple} reasoning steps is extremely challenging for \zeroshotrag and \cotrag. To illustrate, consider the question \textit{Who is the nephew of the friend of David?} Answering this question requires retrieving David's document first and then retrieving their friend's document to find the nephew. Since \ragprompting techniques retrieve documents \textit{only once} by matching vector embeddings of questions and documents, they are unlikely to retrieve all necessary documents required to answer such questions. 

Finally, the \agenticprompting technique \react allows LLMs to avoid the steep performance drop as seen in \ragprompting.
On given a question, \react prompting requires LLMs to retrieve documents dynamically in a conversation and justify why they are relevant.
Concretely, before using a tool (\texttt{RetrieveArticle} or \texttt{Search}) in a conversation turn, the LLM is asked to describe how the tool will help using a ``Thought'' step~\citep{yao2022react}, analogous to the \CoT prompting approach.
This approach shows promise in answering questions correctly.
Even so, \react struggles as the question difficulty increases.

\Cref{fig:exp__f1_v_difficulty} thus decomposes LLM performance along the lines of reasoning capabilities.
It reveals that all \simpleprompting and \agenticprompting achieve near-perfect F1 scores on low-difficulty questions.
Therefore, the stratification between them in \Cref{tab:exp__results} can be attributed to varying performance on high difficulty questions. 
To further isolate the impact of question difficulty, in \Cref{fig:exp__f1_sol1_v_difficulty} we plot F1 scores as a function of reasoning steps for questions with only one solution.