\section{Introduction}
\label{sec:intro}

Designing agents that can perform complex reasoning while interfacing with a large-scale, dynamic corpus---like Wikipedia---is a long-standing goal in the field of natural language processing \citep{feldman2019multi,min2019discrete}.
Such a goal may be within reach given the impressive capabilities of recent language models, which are all trained on internet-scale data.
For example, the ability of LLMs to solve math problems on GSM8K \citep{cobbe2021training} and mathematical olympiads \cite{alphaproof2024} could bode well for agents to answer highly quantitative questions. 
On benchmarks like DROP \citep{dua2019drop} and MMLU \citep{hendrycks2020measuring}, these LLMs demonstrate advanced reading comprehension and general reasoning capabilities, both necessary for intelligent agents.
When augmented with retrievers \citep{muennighoff2022mteb} and tools \citep{patil2023gorilla}, LLMs seem to already possess a strong ability for accessing external datastores and knowledge bases.

However, it is unclear to what extent these models rely on their internal knowledge, which can easily become outdated, versus their reasoning and retrieval abilities. 
Consider the example, ``\emph{What is the date of birth of Wolfgang Amadeus Mozart?}''.
Since this fact is contained within LLMs' pre-training data, asking LLMs this question cannot provide reliable insight on whether the answer was deduced, retrieved or recalled. 
At the same time, existing approaches that perturb Wikipedia facts \citep{cohen2024evaluating,meng2022locating,elazar2021measuring} to construct new question-answer pairs face challenges of ensuring factual consistency across articles.
For example, changing Mozart's date of birth to 2025 also requires modifying Beethoven's article to erase the fact that Beethoven might have met Mozart in 1787!

One could hope to isolate reasoning from factual knowledge using mathematical or logical reasoning benchmarks. Unfortunately, such benchmarks are not entirely reliable as indicators of reasoning performance either.
On GSM8K, a dataset of grade school math problems, \citet{mirzadeh2024gsm} report that frontier models perform significantly worse with minor or even meaningless alterations to the test data---indicating these models are vulnerable to overfitting at best and exact memorization at worst. 
To ensure fair comparison, LLMs need to be evaluated in a way that does not depend on any particular dataset instance.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/size-difficulty-f1-meta-llama--llama-3.3-70b-instruct.pdf}
    \caption{%
    \tbf{Evaluating LLM capabilities with \ours.} We tune the reasoning and retrieval difficulty by the number of reasoning steps and documents, respectively.
    See main text for details.
    }
    \label{fig:size-difficulty-f1}
\end{figure}

Following this philosophy, we develop \emph{PhantomWiki}. 
At the click of a button, \ours generates a fictional universe of characters along with a set of facts about them. 
We reflect these facts in a large-scale corpus, mimicking the style of fan-wiki websites. 
Then we generate question-answer pairs about the universe, encapsulating the types of multi-hop questions commonly considered in the question-answering (QA) literature.

We design \ours to decouple the testing of LLM reasoning and retrieval capabilities in a range of settings. 
In the first setting, the universe is small enough so that all relevant information can fit within the LLM context window. 
Studies such as \citep{liu2024lost} show that LLMs perform poorly in ``needle-in-a-haystack'' scenarios, where a small but crucial piece of information is embedded within a long document. 
By adjusting the total context length—determined by \ours universe size—and the quantity of relevant information required for a given question, \ours provides a reliable benchmark for evaluating LLMs' in-context retrieval capabilities. 

In the second setting, when dealing with large-scale corpora, LLMs face inherent limitations in processing all available information within their fixed context window. Instead, they must rely on external retrieval methods to access relevant information \citep{lewis2020retrieval}. This setup allows us to decouple and evaluate two key components: the effectiveness of the retriever in identifying and retrieving the most relevant content, and the LLMs' ability to accurately interpret and utilize the retrieved information. 

Last but not least, where LLMs are augmented with external tools, effectively integrating reasoning and tool-use capabilities becomes essential for solving complex tasks. By adjusting the size of the associated text corpus and modulating the reasoning difficulty, \ours serves as a foundation for future research on agents that can seamlessly combine reasoning and tool utilization.

Our evaluation on \ours confirms that the proposed tasks present significant challenges for all of the state-of-the-art LLMs that we used.
In \Cref{fig:size-difficulty-f1}, we plot F1 scores of \llama prompted with 3 techniques on \ours-generated questions that require various number of reasoning steps and reference various document corpora (see \Cref{sec:main_results} for details).
The top-right regions of all 3 plots grow darker, indicating that the F1 score plummets as the both reasoning and retrieval complexity increases.
Moreover, \Simpleprompting has a sharp cutoff on the Y-axis, since \zeroshotsimple and \cotsimple prompting are nonviable due to finite LLM context lengths.
By breaking down challenges across different dimensions, \ours enables researchers from various fields to evaluate and refine their methods. Beyond serving as a robust benchmark for LLM performance, \ours provides valuable insights that can guide improvements in retrieval, reasoning, and tool-use capabilities of LLMs for the research community. 
\ifdefined\isaccepted
Code is publicly available at \href{https://github.com/kilian-group/phantom-wiki}{github.com/kilian-group/phantom-wiki} and via ~\texttt{pip install phantom-wiki}.
\else
We will make \ours code available in a public GitHub repository after the anonymous reviewing period.
\fi