%
\begin{table*}
\centering
\caption{\tbf{F1 scores (in \%) for various LLMs and prompting techniques.} We report $\text{mean}\pm\text{standard error}$ across 3 dataset generation seeds (except for \gpt due to cost constraints), and indicate the highest F1 score for each $n$ in bold.
\Simpleprompting is infeasible for $n=5$K as the corpus cannot be fully included in the context.
}

\begin{tabular}{ccccccc}
\toprule
Universe Size & Model & \multicolumn{2}{c}{In-Context} & \multicolumn{2}{c}{RAG} & Agentic \\
 &  & \zeroshot & \CoT & \zeroshotrag & \cotrag & \react \\
\midrule
\midrule
50 & DeepSeek-R1-32B & \textbf{42.42 ± 1.69} & \textbf{52.42 ± 2.64} & 16.87 ± 0.98 & 16.10 ± 1.10 & 5.47 ± 1.36 \\
50 & GPT-4o & 27.20 ± 0.76 & 50.66 & \textbf{20.54} & 14.14 & \textbf{38.70} \\
50 & Gemini-1.5-Flash & 28.49 ± 1.15 & 34.61 ± 2.41 & 19.88 ± 2.05 & 13.35 ± 0.66 & 30.92 ± 1.41 \\
50 & Llama-3.3-70B & 25.64 ± 0.56 & 48.37 ± 1.75 & 17.55 ± 2.20 & \textbf{20.01 ± 1.81} & 35.83 ± 1.00 \\
\midrule
500 & DeepSeek-R1-32B & \textbf{18.33 ± 2.33} & 19.65 ± 3.00 & 12.08 ± 1.07 & 9.64 ± 0.46 & 3.57 ± 0.01 \\
500 & GPT-4o & 16.76 ± 0.87 & \textbf{41.02} & \textbf{13.56} & 7.25 & \textbf{37.39} \\
500 & Gemini-1.5-Flash & 17.39 ± 1.45 & 25.17 ± 1.77 & 11.66 ± 0.34 & 7.94 ± 0.19 & 26.99 ± 1.84 \\
500 & Llama-3.3-70B & 11.59 ± 1.19 & 25.99 ± 2.09 & 10.89 ± 0.58 & \textbf{11.54 ± 1.05} & 35.56 ± 0.49 \\
\midrule
5000 & DeepSeek-R1-32B & 
    \multicolumn{2}{p{3.6cm}}{\multirow{4}{*}{\centering
    \begin{tcolorbox}[height=1.7cm, colframe=black!50, colback=gray!5, boxrule=0.5mm, width=1.1\linewidth,halign=center,valign=center]
    Max context exceeded \end{tcolorbox}}} & 8.29 ± 0.18 & 7.96 ± 0.38 & 4.74 ± 0.04 \\
5000 & GPT-4o &&& \textbf{10.12} & 6.96 & \textbf{36.85} \\
5000 & Gemini-1.5-Flash &&& 8.60 ± 0.31 & 5.26 ± 0.36 & 23.47 ± 1.53 \\
5000 & Llama-3.3-70B &&& 7.57 ± 0.52 & \textbf{8.67 ± 0.18} & 30.89 ± 2.24 \\
\bottomrule
\end{tabular}
%



\label{tab:exp__results}
\end{table*}

\section{Experimental Validation}
\label{sec:main_results}

We evaluate reasoning and retrieval capabilities of several frontier LLMs using \ours, by decomposing their performance over questions of varying difficulty and universes of varying sizes.

\subsection{Evaluation Setup}

We generate \ours instances with $n$ ranging from 50 to 10K---a universe size for which the total length of articles exceed the LLM context length. For the evaluation, only the articles (not the Prolog database or the generated graphs) will be provided to the LLMs. 
To ensure that our findings are not tied to any specific \ours instance, we use 3 random dataset seeds for each configuration.
Creating \ours instances with different random seeds leads to entirely different combinations of names, relations, and personal facts.
In each instance, we generate question templates with maximum recursion depth $d=20$, for a total of 50 templates.
We sample 10 questions for each template, yielding a \textbf{total of 500 questions per \ours instance}.
As shown in \Cref{fig:difficulty_distribution,fig:solution_distribution} (\Cref{app:cfg}),
these questions have varying difficulty and number of answers.
Accordingly, we prompt the LLMs to predict all answers as a comma-separated list and measure correctness with the answer-level F1 score.

\subsection{Models and Prompting Techniques}

We test both open- and closed-source LLMs, namely OpenAI's \gpt~\citep{hurst2024gpt}, Google's \gemini~\citep{google2024gemini}, and the instruction-tuned version of Meta's \llama model~\citep{dubey2024llama}.
We also evaluate DeepSeekAI's \deepseek~\citep{guo2025deepseek} (distilled with Qwen-2.5-32B~\citep{yang2024qwen2}), which is an open-weights LLM trained on reasoning trace datasets.
We prompt each LLM with the following techniques, broadly grouped in three ways:

\textbf{In-Context Prompting.} This technique includes the whole document corpus as part of the prompt. We use this type of prompting in conjunction with two strategies: \zeroshot---where the document corpus is immediately followed by the  question---and Chain-of-Thought (\CoT) prompting~\citep{wei2022cot}, where we additionally include some examples on how step-by-step reasoning could lead to the correct answer. We include these prompts in \Cref{sub:zeroshot_simple}, as well as modifications to the \zeroshot strategy for \deepseek.

\textbf{RAG Prompting.} 
This setting augments generation with a pre-trained neural retriever~\citep{lewis2020retrieval}. We implement this by first searching for the 4 most relevant documents to the posed question based on \retriever embeddings. Next, we incorporate these retrieved documents into the model's prompt. Finally, we add in the same \zeroshot and \CoT prompts as In-Context Prompting. We document details on our retrieval algorithm in \Cref{sub:zeroshot_rag}.

\textbf{Agentic Prompting.} \react~\citep{yao2022react} is a prompting technique that enables LLMs to interleave reasoning steps with tool interactions, to solve complex tasks. For \ours QA task, the LLMs are provided with keyword-based tools \texttt{RetrieveArticle} and \texttt{Search} to retrieve relevant documents (see \Cref{sub:react} for tool details).
These settings materialize the limitations of \simpleprompting and necessitate the use of advanced \ragprompting and \agenticprompting approaches.

In the \CoT and \react prompts, we include 10 QA exemplars and hand-written reasoning traces.
We choose these exemplars from a dataset instance of size 25 that is not used for evaluation.
In \react, we limit LLMs to interact with the text corpus for up to 50 steps, which is sufficient to answer almost all questions in \ours instances.

We cap all LLM outputs to 4096 tokens and use greedy decoding ($\text{temperature}=0$).
For \deepseek, we use $\text{temperature}=0.6$ and $\text{top-p}=0.95$ in accordance with the evaluation setup in \citet[Sec.~3]{guo2025deepseek}.
We refer the reader to \Cref{app:baselines} for full prompt templates and implementation details.

\subsection{Discussion}

In \Cref{tab:exp__results}, we report the mean F1 score across various universe sizes, LLMs, and prompting techniques.
We first average F1 scores over all questions in a \ours instance, then compute the mean and standard error across the dataset generation seeds.

We first consider the small-universe setting ($n=50$) in \Cref{tab:exp__results}, which corresponds to roughly 16K tokens for the LLMs we test.
\Simpleprompting techniques outperform other techniques: \CoT with \gpt attains the highest performance, followed by \zeroshot with \deepseek. 
Next, we consider the setting of medium universes ($n=500$). 
Here the full document corpus can still be included in all LLMs' contexts, but we find that \zeroshot performs poorly for all LLMs, and \deepseek especially struggles.
F1 scores of \CoT for all LLMs degrade as well compared to $n=50$, but not worse than \react.
Finally, in the setting of large universes ($n=5000$), none of the LLMs we evaluate can accommodate the full document corpus.
In-context techniques are no longer viable, and we must rely on \ragprompting and \agenticprompting.
\ragprompting attain poor F1 scores because the retriever fails to retrieve documents relevant for answering complex questions.
On the other hand, \agenticprompting technique shines in comparison to other techniques, indicating that LLMs are better suited to dynamically retrieve documents while reasoning on a question.
We attribute the poor performance of \deepseek in \agenticprompting technique to its inferior tool-calling abilities compared to the other LLMs.
