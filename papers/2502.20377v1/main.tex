\documentclass{article}
\input{preamble}
\input{cref_setup}
\input{macros}

%
%

%
%
\usepackage[accepted]{icml2025arxiv} %

\icmltitlerunning{PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation}

\begin{document}

\twocolumn[
\icmltitle{PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Albert Gong}{equal,cor}
\icmlauthor{Kamilė Stankevičiūtė}{equal,cor,cam}
\icmlauthor{Chao Wan}{equal,cor}
\icmlauthor{Anmol Kabra}{cor}

\icmlauthor{Raphael Thesmar}{cor}
\icmlauthor{Johann Lee}{cor}
\icmlauthor{Julius Klenke}{cor}
\icmlauthor{Carla P. Gomes}{cor}
\icmlauthor{Kilian Q. Weinberger}{cor}

\end{icmlauthorlist}

\icmlaffiliation{cor}{Department of Computer Science, Cornell University, Ithaca, New York, USA}
\icmlaffiliation{cam}{Department of Computer Science and Technology, University of Cambridge, Cambridge, UK}

\icmlcorrespondingauthor{Albert Gong}{agong@cs.cornell.edu}
\icmlcorrespondingauthor{Kamilė Stankevičiūtė}{ks830@cam.ac.uk}
\icmlcorrespondingauthor{Chao Wan}{cw862@cornell.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%
\printAffiliationsAndNotice{\icmlEqualContribution} %

\input{sections/abstract}

\input{sections/intro}
\input{sections/related}
\input{sections/dataset}
\input{sections/main_results}
\input{sections/reasoning}
\input{sections/retrieval}
\input{sections/future}

%
%
%
%

%
\ifdefined\isaccepted
\section*{Acknowledgements}

KS gratefully acknowledges support from AstraZeneca. 
CW is supported by the National Science Foundation (NSF) OAC-2118310 and NSF-1934714 grant. 
This work was partially supported by funding from NewYork-Presbyterian for the NYP-Cornell Cardiovascular AI Collaboration, the National Institute of Food and Agriculture (USDA/NIFA), the Air Force Office of Scientific Research (AFOSR), and a Schmidt AI2050 Senior Fellowship, a Schmidt Sciences program.
\fi

\section*{Impact Statement}

By leveraging context-free grammars and Prolog, \ours is able to generate large, durable and challenging datasets without using LLMs.
The datasets have low computational, monetary, and environmental cost and our open-source framework is accessible to any user.

Since \ours randomly generates datasets that do not reference any existing data, the evaluation benchmark is resistant to data leakage and memorization while training.
The approach of publishing a dataset generation \textit{procedure} rather than a fixed dataset also encourages better research practices (by using fresh datasets instead of overfitting to a single instance), and enables a more accurate evaluation of model performance. 
Since we do not use any personal data, use of \ours does not have any privacy concerns.

%
%
\bibliography{refs}
\bibliographystyle{icml2025}

\input{appendix}

\end{document}

%
%
%
%
%
%
%
%
%
%
%
%
%
