\section{Experimental Setup}
\label{apz:exp_setup}

\subsection{Dataset Details}
\label{apx:dataset}
For vision tasks, we follow the initial practice of~\citep{ilharco2022editing} and build a vision benchmark consisting of eight datasets, including MNIST~\citep{lecun2010mnist}, EuroSAT~\citep{helber2019eurosat}, GTSRB~\citep{stallkamp2011german}, SVHN~\citep{netzer2011reading}, DTD~\citep{cimpoi2014describing}, RESISC45~\citep{cheng2017remote}, Stanford Cars~\citep{krause20133d}, SUN397~\citep{xiao2016sun}. 

For natural language understanding~(NLU) tasks, we follow the practice in~\citet{yu2024language} and use eight datasets from the GLUE benchmark~\citep{wang2018glue}, 
including CoLA~\citep{warstadt2018neural}, SST-2~\citep{socher2013recursive}, MRPC~\citep{dolan2005automatically}, STS-B~\citep{cer2017semeval}, QQP~\citep{iyer2017first}, MNLI~\citep{williams2017broad}, QNLI~\citep{wang2018glue, rajpurkar2016squad}, RTE~\citep{wang2018glue, dagan2005pascal,bar2006second,giampiccolo2007third,bentivogli2009fifth}.
We evaluate performance using accuracy for SST-2, QNLI, and RTE, matched accuracy for MNLI, the Matthews correlation coefficient for CoLA, the average of accuracy and F1 score for MRPC and QQP, and the average of Pearson and Spearman correlations for STS-B.


For natural language generation~(NLG) tasks, we follow the practice in~\citep{yu2024extend} and use WizardLM-13B~\citep{xu2023wizardlm}, WizardMath-13B~\citep{luo2023wizardmath}, llama-2-13b-code-alpaca~\citep{chaudhary2023code} as Instruct, Math and Code expert models, respectively. Note that WizardLM-13B model also has code generation abilities, and we include code benchmarks in its evaluation. 

We use five datasets for evaluation, including AlpacaEval 2.0~\citep{dubois2024length}, GSM8K~\citep{cobbe2021training}, MATH~\citep{hendrycks2020measuring}, HumanEval~\citep{chen2021evaluating}, and MBPP~\citep{austin2021program}. AlpacaEval 2.0 measures performance using the win rate, defined as the proportion of instances where a more advanced large language model, specifically GPT-4 Turbo in this study, prefers the outputs of the target model over its own. GSM8K and MATH use zero-shot accuracy as the metric. HumanEval and MBPP use pass@1 as the metric, representing the proportion of individually generated code samples that successfully pass the unit tests. In addition to reporting a simple average metric, we also provide the normalized average metric, calculated by dividing the metric of the merged model by the metric of fine-tuned models, to ensure a fair and consistent comparison across datasets.

\subsection{Descriptions of Baselines}
\label{apx:baselines}

We evaluate several baseline methods for merging Vision Transformers (ViT) and encoder-based language models, which are outlined below:
\begin{itemize}
    \item \textbf{Task Arithmetic}~\citep{ilharco2022editing}: This approach constructs task vectors from model weights and merges them using arithmetic operations.

    \item \textbf{Fisher Merging}~\citep{matena2022merging}: This method uses Fisher-weighted averaging of model parameters to merge models.

    \item \textbf{RegMean}~\citep{jin2022dataless}: RegMean determines merging coefficients by solving a linear equation to align internal embeddings, which shares similarity with \texttt{ProDistill}. One of the key differences is that RegMean relies on the linearity of the modules, and therefore can only be applied to linear modules. Our method is free of this limitation, since it is a general distillation algorithm. Another primary difference from our method is that RegMean is training-free, whereas \texttt{ProDistill} is training-based. This difference leads to several consequences. 
    \begin{itemize}
        \item Performance: The training-free nature of RegMean limits its ability to fully leverage the expressive power of neural network modules, resulting in lower performance compared to the training-based \texttt{ProDistill}. 
        \item Computation: Although RegMean avoids the potential overhead of a lengthy training process, it still requires solving linear equations, which can become computationally intensive as model sizes increase. As demonstrated in Section~\ref{sec: compute efficiency}, with just one epoch of training, \texttt{ProDistill} achieves better merging results than RegMean, without incurring significant computational costs. Therefore, \texttt{ProDistill} does not suffer from high computational burden despite being a training-based method.

    \end{itemize}
    \item \textbf{AdaMerging}~\citep{yang2023adamerging}: This method adaptively selects merging coefficients by minimizing entropy on unlabeled test data. Like \texttt{ProDistill}, it is both training-based and data-dependent.

    \item \textbf{Localize-and-Stitch}~\citep{he2024localize}: This approach identifies sparse masks to extract task-specific parameters from fine-tuned models and merges them back into the pretrained model.
\end{itemize}

We consider the following additional baselines for merging decoder-based large language models. 
\begin{enumerate}
    \item \textbf{TIES-Merging}~\citep{yadav2024ties}:  TIES-Merging first trims task vectors to retain only the most significant parameters, then resolves the signs of the remaining parameters, and finally merges only those parameters that align with the resolved signs.
    \item \textbf{WIDEN}~\citep{yu2024extend}: WIDEN disentangles model weights into magnitude and direction components and merges them by considering their respective contributions. It is also applicable to merging independently trained models.
\end{enumerate}

\subsection{Implementation Details}
\label{apx:impl}

We use the ViT checkpoints given by~\citep{ilharco2022editing} for vision tasks. 
For NLU tasks, we fine-tune BERT-base-uncased and RoBERTa-base models for $10$ epochs. The weight decay is set to $0.01$. We use a learning rate of $10^{-5}$ with a warm-up strategy.

For \texttt{ProDistill} and AdaMerging, we train the merging coefficients using the Adam optimizer. The merging coefficients are initialized to 0.3 when merging eight models, and searched from $\{0.5, 1.0\}$ when merging two models.
The learning rate and training epochs are selected via grid search.
For ViT models and LLMs, the learning rate is chosen from $\{0.1, 0.01\}$; for Bert/RoBERTa models, 
the learning rate is chosen from $\{0.01, 0.001\}$.
The number of epochs is chosen from $\{50, 100, 200\}$.  

For Task Arithmetic, we use a fixed merging coefficient of 0.3 when merging eight models, and search the coefficient from $\{0.5, 1.0\}$ when merging 2 models. For RegMean, the scaling ratio to reduce its diagonal terms are selected from a grid of $\{0.7, 0.8, 0.9, 1.0\}$. For Fisher Merging, the scaling coefficient is selected from a grid of $\{0.1, 0.3, 0.5, 0.7, 0.9, 1.0\}$.
For Localize-and-Stitch, we set sigmoid bias to 0.3, learning rate to $10^7$, $\ell_1$ regularization factor to 10.0, sparsity level to $1\%$ and epochs to 10.  

For Vision and NLU tasks, the few-shot validation set is randomly sampled from the training set. 
For NLG tasks, the validation set is randomly sampled from the test set of AlpacaEval 2.0, GSM8K and MBPP, and we exclude these test data points in evaluation. 