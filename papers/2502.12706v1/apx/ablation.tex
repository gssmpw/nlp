\section{Ablation studies}
\label{apx: ablations}


\subsection{Analyses of Merging Coefficient Granularity}
\label{apx: granularity}
This section analyzes the impact of different granularities on the merging coefficients.  We consider three types of granularities:
\begin{enumerate} 
\item \textbf{Element-wise:} Each element in the model's weights corresponds to a merging coefficient. In other words, the merging coefficients are tensors that have the same dimensions as the model parameters. 
\item \textbf{Layer-wise:} Each layer of the model is assigned a scalar merging coefficient. The merging coefficient tensor, therefore, has the same number of elements as the number of model layers. In this paper, we slightly deviate from this naming convention by assigning a coefficient to each module in the network. 
\item \textbf{Task-wise:} Each fine-tuned model has a scalar merging coefficient. 
\end{enumerate}

We conduct experiments using these three types of granularities, and present the results in Table~\ref{tab: granularities}. For our method, the element-wise granularity yields the best performance, owing to its higher expressive power. In contrast, for AdaMerging, the element-wise granularity performs worse than the layer-wise granularity. We hypothesize that this is due to the weak supervision power of entropy-based objective function in AdaMerging, which may lead to overfitting when the number of trainable parameters is increased.


\begin{table}[!h]
\centering
\begin{tabular}{cc|c c}
\toprule
Val Shot       & Granularities       & Ours   & AdaMerging \\ 
\midrule
\multirow{3}{*}{16} & Element-wise & \cellcolor{lightyellow}82.79 & 72.35     \\ 
& Layer-wise   & 73.81 & \cellcolor{lightyellow}75.92     \\ 
& Task-wise    & --      & 71.38     \\ 
\midrule
\multirow{3}{*}{32} & Element-wise & \cellcolor{lightyellow}84.49 & 72.49     \\ 
& Layer-wise   & 73.50 & \cellcolor{lightyellow}78.28     \\ 
& Task-wise    & --      & 71.76     \\ 
\midrule
\multirow{3}{*}{64} & Element-wise & \cellcolor{lightyellow}86.04 & 73.11     \\ 
& Layer-wise   & 72.96 & \cellcolor{lightyellow}79.28     \\ 
& Task-wise    & --      & 71.69     \\ 
\bottomrule
\end{tabular}
\hspace{1cm} %
\begin{tabular}{cc|c c}
\toprule
Val Shot       & Granularities       & Ours   & AdaMerging \\ 
\midrule
\multirow{3}{*}{16} & Element-wise & \cellcolor{lightyellow}0.6980 & 0.5746     \\ 
& Layer-wise   & 0.6340 & \cellcolor{lightyellow}0.6398     \\ 
& Task-wise    & -- & 0.6406     \\ 
\midrule
\multirow{3}{*}{32} & Element-wise & \cellcolor{lightyellow}0.7473 & 0.5465     \\ 
& Layer-wise   &  0.5996 & \cellcolor{lightyellow}0.6402     \\ 
& Task-wise    & -- & 0.6403     \\ 
\midrule
\multirow{3}{*}{64} & Element-wise & \cellcolor{lightyellow}0.7641 & 0.5415     \\ 
& Layer-wise   & 0.5560 & \cellcolor{lightyellow}0.6332     \\ 
& Task-wise    & -- & 0.6379     \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Impact of Granularity on Merging Methods.} Left: Accuracy results on 8 vision benchmarks using ViT-B-32. Right: Performance metrics on the NLU tasks using RoBERTa. The highlighted cells indicate the configurations used in this paper.}
\label{tab: granularities}
\end{table}


\subsection{Comparison with \texttt{DistillMerge} }
\label{apx: end2end}
In this section, we compare the \texttt{ProDistill} algorithm, which uses the progressive training approach, with its unoptimized version \texttt{DistillMerge}, which uses the traditional end-to-end training approach and optimizes Objective~\ref{eq:obj_naive} directly. To ensure a fair comparison, both approaches are evaluated using the same computational budget.
The results, presented in Figure~\ref{fig: end2end}, indicate that \texttt{ProDistill} achieves a better overall performance compared to \texttt{DistillMerge}. Therefore, progressive training also has a performance advantage in the considered setup,  in addition to its memory efficiency as shown in Section~\ref{sec: memory efficiency}.



\begin{figure*}[!h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/end2end_vitb32.png}
    \end{minipage}%
    \hspace{0.0\textwidth} 
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/end2end_roberta.png}
    \end{minipage}
    \caption{\textbf{Comparison between \texttt{ProDistill} and \texttt{DistillMerge}.} Left: Accuracy results on 8 vision benchmarks using ViT-B-32. Right: Performance metrics on the NLU tasks using RoBERTa. The results demonstrate the performance improvement of progressive training in \texttt{ProDistill}, compared to end-to-end training in \texttt{DistillMerge}, despite the latter being more resource-intensive.}
    \label{fig: end2end}
\end{figure*}

\subsection{Comparisons with Standard Training and Standard Distillation}
\label{apx: direct train}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figure/direct_train_distill_bar.png}
    \caption{\textbf{Ablation Results of \texttt{DirectTrain} and \texttt{DirectDistill}.} Traditional supervised training in \texttt{DirectTrain} performs poorly in the few-shot setup. The standard feature-based distillation method, \texttt{DirectDistill}, outperforms task arithmetic but still lags behind \texttt{ProDistill} and \texttt{DistillMerge}, which incorporate task vector-based weight scaling.}
    \label{fig:direct distill}
\end{figure}
We consider two additional baselines. The first, referred to as \texttt{DirectTrain}, follows a standard supervised training approach. It assumes access to class labels and minimizes the standard cross-entropy loss:
\begin{align*}
    \min_{\theta} \sum_{i=1}^T \frac{1}{2T|\mathcal{D}_i|}\sum_{(x, y)\in\mathcal{D}_i}\mathcal{L}_{CE}\left(\psi(\theta, x), y\right),
\end{align*}
where $\psi(\cdot, \cdot)$ is the model output logits. 

The second baseline, \texttt{DirectDistill}, is conceptually similar to \texttt{DistillMerge}, but it does not utilize task vectors to scale the model weights. Mathematically, it is expressed as:

\begin{align*}\min_{\theta}\sum_{i=1}^{T}\frac{1}{2T|\mathcal{D}_{i}|}\sum_{x\in\mathcal{D}_i}
\nonumber\left\|\varphi\left(\theta, x\right) -\varphi\left(\theta_{i},x\right)\right\|^{2}.\end{align*} 

The relationship between these algorithms can be visualized as follows:

\begin{align*}
    \texttt{DirectTrain}\xrightarrow{\text{Distillation Loss}} \texttt{DirectDistill}\xrightarrow{\text{Task Vector Scaling}} \texttt{DistillMerge} \xrightarrow{\text{Layer-wise Training}} \texttt{ProDistill}
\end{align*}

We evaluate \texttt{DirectTrain} and \texttt{DirectDistill} on vision tasks using ViT-B-32. Compared to the main experimental setup, we use a smaller learning rate grid of \{\num{1e-5}, \num{1e-6}, \num{1e-7}\}. The results, summarized in Figure~\ref{fig:direct distill}, reveal several key findings:
\begin{enumerate} 
\item Domain-specific data is crucial for model merging. With only 16-shot validation data, a vanilla distillation algorithm like \texttt{DirectDistill} can outperform Task Arithmetic. 
\item The internal embeddings of teacher models provide significantly richer supervision signals compared to class labels alone, as reflected in the improved performance of \texttt{DirectDistill} over \texttt{DirectTrain}. 
\item Scaling model weights using task vectors introduces an effective prior for model training, as reflected in the improved performance of \texttt{ProDistill} over \texttt{DirectDistill}. 
\end{enumerate}



\subsection{Ablation Studies on Layer Inputs}
\label{apx: layer input}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figure/three_dataloader.pdf}
    \caption{Three configurations of layer inputs. \textbf{Left:} Dual activations~(ours). \textbf{Middle:} Merged Activations. \textbf{Right:} Fine-tuned Activations. $z_1$ is activation of merged model and $z_2$ is the activation of fine-tuned model.}
    \label{fig:three_dataloader}
\end{figure}

\begin{table}[t]
\centering
\begin{tabular}{c|cc}
\toprule
Input Type & ViT-B-32 & RoBERTa \\
\midrule
\rowcolor{lightyellow}
Dual~(in Alg~\ref{alg:model_merging}) & \textbf{86.04} & \textbf{0.7641} \\
Merged & 85.20~(-1.0\%) & 0.7303~(-4.4\%) \\
Fine-tuned & 84.98~(-1.2\%) & 0.5895~(-22.8\%) \\
\bottomrule
\end{tabular}
\caption{Performance of three different layer input configurations. The dual activations approach used in Algorithm~\ref{alg:model_merging} achieves the highest performance, especially for NLU tasks. }
\label{tab:three_loader}
\end{table}

\texttt{ProDistill} maintains dual paths of internal activations as inputs to each layer: the activations from the merged models and those from the fine-tuned models. In this section, we explore two alternative configurations: one using only fine-tuned activations and the other using only merged activations. 
The goal in these two configurations is to align the layer output \textit{under the same input}, and can be viewed as direct adaptations of traditional distillation loss to the layer-wise setting. 

We conduct additional experiments on these three setups, and present the results in Table~\ref{tab:three_loader}. The findings demonstrate a significant performance advantage of using dual activations over the two alternatives, particularly on the NLU tasks. 

The performance gap can be attributed to the limited expressive power of a single layer, which makes it impossible to fully optimize Equation~\ref{eq:obj}.Otherwise, if the loss in Equation~\ref{eq:obj} were zero, the three input configurations would become equivalent.  Therefore, the proposed dual input strategy facilitates the \textit{progressive} alignment of layer features by distributing the minimization of the distillation loss across all layers.