\section{Proofs}
\label{apx: proof}

\thma*

\begin{proof}
We construct a hard instance of linear regression to prove the theorem. Let $d$ denote the dimension of the data space. A data point has the form $\boldsymbol{z} = (\boldsymbol{x}, y)$, where $\boldsymbol{x}\in\mathbb{R}^d, y\in\mathbb{R}$, and the model is represented by a $d$-dimensional vector $\boldsymbol{w}\in\mathbb{R}^d$. The loss function is the $\ell_2$ loss $\ell(\boldsymbol{z}, \boldsymbol{w}) = \frac{1}{2}\|\boldsymbol{x}^\top \boldsymbol{w}-y\|^2$, and $\ell(\mathcal{D}, \boldsymbol{w}) = \text{Avg}(\ell(\boldsymbol{z}, \boldsymbol{w}))$.

Let $\boldsymbol{w}_1, \boldsymbol{w}_2, \hat{\boldsymbol{w}}$ denote the weights of $f_1, f_2, \hat{f}$. Since $f_1\neq f_2$, we can assume, without loss of generality, that $\boldsymbol{w}_1\neq \hat{\boldsymbol{w}}$. Then we have the simple linear algebra fact that, for a large enough $d$,  there exist $(\boldsymbol{x}_1, y_1)$ and $(\boldsymbol{x}_2, y_2)$, such that 


\begin{enumerate}
    \setlength{\itemsep}{0pt} %
    \item $\binom{\boldsymbol{x}_1}{y_1} \perp \binom{\boldsymbol{w}_1}{-1}$
    \item $\left\langle \binom{\boldsymbol{x}_1}{y_1}, \binom{\hat{\boldsymbol{w}}}{-1} \right\rangle \ge 2\sqrt{C}$
    \item $\binom{\boldsymbol{x}_2}{y_2} \perp \binom{\boldsymbol{w}_2}{-1}$
    \item $\boldsymbol{x}_1, \boldsymbol{x}_2$ are not co-linear.
\end{enumerate}

Let $\mathcal{D}_1 = \{(\boldsymbol{x}_1, y_1)\}$ and $\mathcal{D}_2 = \{(\boldsymbol{x}_2, y_2)\}$. This construction ensures that 
\begin{align*}
    \ell(\mathcal{D}_1, f_1) &= \frac{1}{2}\|\boldsymbol{x}_1^\top \boldsymbol{w}_1 - y_1\|^2 = 0, \\
    \ell(\mathcal{D}_2, f_2) &= \frac{1}{2}\|\boldsymbol{x}_2^\top \boldsymbol{w}_2 - y_2\|^2 = 0, \\
    \ell(\mathcal{D}_1 \cup \mathcal{D}_2, \hat{f}) &\ge 
    \frac{1}{4}\|\boldsymbol{x}_1^\top \hat{\boldsymbol{w}} - y_1\|^2 = C. 
\end{align*}
On the other hand, since $\boldsymbol{x}_1, \boldsymbol{x}_2$ are not co-linear, one can find $\boldsymbol{w}^*$ such that $\boldsymbol{x}_1^\top \boldsymbol{w}^* - y_1 = \boldsymbol{x}_2^\top \boldsymbol{w}^* - y_2 = 0$, as long as $d$ is large enough. That is, we have 
\begin{align*}
    \ell(\mathcal{D}_1 \cup \mathcal{D}_2, f^*) = 0.
\end{align*}
This completes the proof. 

\end{proof}

\thmb*


\begin{proof}
    Consider a two-class linear-separable data classification problem in a $d$-dimensional space. Each data point $\boldsymbol{z}$ consists of input $\boldsymbol{x}\in\mathbb{R}^d$, label $y\in \{1, -1\}$. The hypothesis class is linear models, $\mathcal{F} = \{f=(\boldsymbol{w}, b): \boldsymbol{w}\in\mathbb{R}^d\backslash\{\mathbf{0}\}, b \in \mathbb{R}, f(\boldsymbol{x}) = \boldsymbol{w}^\top \boldsymbol{x} + b\}$, where we exclude $\boldsymbol{w} = \mathbf{0}$ to avoid degeneracy.
    For linear model $f=(\boldsymbol{w}, b)$ and data point $z=(\boldsymbol{x},y)$, the loss function is defined as $\ell(\boldsymbol{z}, f) = \max\{-y(\boldsymbol{w}^\top \boldsymbol{x}+b), 0\}$. For a dataset $\mathcal{D}$, the loss function is defined as $\ell(\mathcal{D}, f) = \text{Avg}(\ell(\boldsymbol{z}, f))$.
    The algorithm $\mathcal{L}$ outputs the $\ell_2$ normalized max-margin classifier of training set with $\|\boldsymbol{w}\|_2 = 1$, which covers a wide range of practical algorithms including SVM and gradient descent algorithms that have max-margin implicit bias. 

    Let $d=2$. Consider four data points 
    \begin{align*}
        &\boldsymbol{z}_1 = (\boldsymbol{x}_1, y_1) = ((1, 0), 1),\quad \boldsymbol{z}_2 = (\boldsymbol{x}_2, y_2) = ((-1, 0), -1),\\
        &\boldsymbol{z}_3 = (\boldsymbol{x}_3, y_3) = ((0, 1), 1),\quad \boldsymbol{z}_4 = (\boldsymbol{x}_4, y_4) = ((0, -1), -1),
    \end{align*}
    Let $\mathcal{D}_1 = \{\boldsymbol{z}_1, \boldsymbol{z}_2\}$, $\mathcal{D}_2 = \{\boldsymbol{z}_3, \boldsymbol{z}_4\}$.

    It is easy to see that 
    \begin{align*}
        &\mathcal{L}(\{\boldsymbol{z}_1, \boldsymbol{z}_2\}) = f_1 = ((1, 0), 0),\\
        &\mathcal{L}(\{\boldsymbol{z}_3, \boldsymbol{z}_4\}) = f_2 = ((0, 1), 0),
    \end{align*}

    Consider merging $f_1$ and $f_2$.
    Let $\hat{f} = \mathcal{M}(f_1, f_2) = ((\hat{w}_1, \hat{w}_2), \hat{b})$. Next we show that there exist $p, q$, such that 

    \begin{enumerate}
    \setlength{\itemsep}{0pt} %
    \item $\hat{w}_1 p + \hat{w}_2 q + \hat{b} < -5C$,
    \item $p>1$ or $q>1$.
    \end{enumerate}
    We prove the fact by contradiction. If this claim does not hold, we know that for any $p>1, q\in\mathbb{R}$ and any $q>1, p\in\mathbb{R}$,  we have $\hat{w}_1 p + \hat{w}_2 q + \hat{b} \ge -5C$. This will give $\hat{w}_1 = \hat{w_2} = 0$, which contradicts the degeneracy of $(\hat{w}_1, \hat{w}_2)$.

    Let $\boldsymbol{z}_5 = (\boldsymbol{x}_5, y_5) = ((p, q), 1)$. 
    If $p>1$, we define 
    \begin{align*}
        \mathcal{D}_1 = \{\boldsymbol{z}_1, \boldsymbol{z}_2, \boldsymbol{z}_5\}, \mathcal{D}_2 = \{\boldsymbol{z}_3, \boldsymbol{z}_4\}.
    \end{align*}
    If $q>1$, we define 
    \begin{align*}
        \mathcal{D}_1 = \{\boldsymbol{z}_1, \boldsymbol{z}_2\}, \mathcal{D}_2 = \{\boldsymbol{z}_3, \boldsymbol{z}_4, \boldsymbol{z}_5\}.
    \end{align*}

    This construction ensures that 
    \begin{enumerate}
    \setlength{\itemsep}{0pt} %
    \item $\mathcal{L}(\mathcal{D}_1) = f_1$, $\ell(\mathcal{D}_1, f_1) = 0$, 
    \item $\mathcal{L}(\mathcal{D}_2) = f_2$, $\ell(\mathcal{D}_2, f_2) = 0$,
    \item $\ell(\mathcal{D}_1\cup \mathcal{D}_2, \hat{f})\ge -\frac{1}{5}\left(\hat{w}_1 p + \hat{w}_2 q + \hat{b}\right)> C$.
    \end{enumerate}

    On the other hand, it is easy to see that $\mathcal{D}_1\cup \mathcal{D}_2$ is linearly separable, due to the condition that $p>1$ or $q>1$. Therefore, the max-margin classifier $f^* = \mathcal{L}(\mathcal{D}_1\cup \mathcal{D}_2)$ satisfies
    \begin{align*}
        \ell(\mathcal{D}_1\cup \mathcal{D}_2, f^*) = 0.
    \end{align*}
    This completes the proof. 
\end{proof}




    


\begin{remark}
    The loss function in the proof does not reflect the classification accuracy. To take this into consideration, 
    we can inject arbitrary number of adversarial data points like $\boldsymbol{z}_5$, to make the classification accurate arbitrarily low.
\end{remark}