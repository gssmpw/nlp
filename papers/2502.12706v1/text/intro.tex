
\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/radar_plot.png}
    \vspace{-10pt}
    \caption{\textbf{\texttt{ProDistill} consistently outperforms other methods across nearly all considered tasks.} The performance metrics for each task are normalized and then clipped at a minimum value of 0.5 for better visualization.}
    \vspace{-10pt}
    \label{fig:enter-label}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/intro.pdf}
    \vspace{-10pt}
    \caption{\textbf{Left: Overview of model merging.} Each expert corresponds to a task vector $\boldsymbol{\theta}_i-\boldsymbol{\theta}_0$, which is scaled by its corresponding merging coefficient $\boldsymbol{\lambda}_i$ and summed to get the merged model. \textbf{Right: Illustration of \texttt{ProDistill}.} The merged model layer and each fine-tuned model layer take as input the merged feature and the fine-tuned feature, respectively. The MSE loss between these outputs is used to update the merged model layer. The output features serve as inputs for merging the subsequent layer. }
    \vspace{-5pt}
    \label{fig: overview}
\end{figure*}

Large-scale pre-trained models have revolutionized deep learning, achieving remarkable success across various domains such as language~\citep{brown2020language,team2023gemini, touvron2023llama} and vision~\citep{dosovitskiy2020image, ramesh2021zero}. 
Meanwhile, an increasing number of fine-tuned checkpoints are being made publicly available on platforms like Hugging Face. Depending on the specific downstream datasets, fine-tuned models excel in specialized abilities, such as mathematics or coding. However, complex tasks often require the integration of multiple abilities. For example, solving an advanced math problem may necessitate the assistance of computer programs to produce accurate solutions. While multi-task learning~\citep{caruana1997multitask,misra2016cross,sener2018multi,liu2019end} can address this challenge, it requires access to fine-tuning data and incurs significant computational overhead during retraining. On the other hand, model ensembling~\citep{dietterich2002ensemble,kurutach2018model,ganaie2022ensemble} avoids retraining but introduces substantial storage overhead due to the need to deploy multiple models.

Model merging~\citep{yang2024model,goddard2024arcee,tang2024fusionbench} offers an elegant solution to these challenges. The work of~\citet{ilharco2022editing} finds that the difference between fine-tuned and pre-trained weights, which they name \textit{task vectors}, exhibits arithmetic properties, such as addition and negation, which correspond to changes in model capabilities. Therefore, model merging can be achieved by taking a weighted average of the model weights, as illustrated in Figure~\ref{fig: overview}. This is connected to the linear mode connectivity~\citep{frankle2020linear,mirzadeh2020linear} of neural networks.



Although model merging improves storage efficiency and data protection, the performance of the merged model can degrade, especially when the number of models scales up. Recent studies~\citep{matena2022merging, jin2022dataless,yang2023adamerging} propose various methods to handle this issue,
many of which require a \textit{few-shot validation dataset} that contains domain-specific information of downstream tasks.
This seems to contradict the data-free nature of task arithmetic. In light of this, we raise the question:
\begin{introbox}
\begin{center}
   \textit{Is domain-specific data necessary for model merging?} 
\end{center}
\end{introbox}


We provide an affirmative theoretical answer to this question. We prove, for the first time, that the \textit{worst-case} performance of \textit{any data-agnostic} model merging algorithm can be arbitrarily bad, even for a simple linear model. Therefore, although data-agnostic algorithm achieves great empirical success, it is theoretically reasonable to assume access to at least a few-shot dataset. 

Building on these theoretical insights, we next address the empirical challenge of few-shot model merging by exploring the following question:
\begin{introbox}
\begin{center}
   \textit{How to fully leverage the data and the fine-tuned models to improve merging performance?} 
\end{center}
\end{introbox}

To this end, we frame model merging as a teacher-student distillation problem, where the goal is to transfer the knowledge from multiple fine-tuned models~(teacher) into the merged model~(student). However, directly applying existing distillation algorithms often results in large training memory overhead, particularly for large language models with billions of parameters.





To address this challenge, we propose a novel model merging algorithm, \texttt{ProDistill}~(\textit{Pro}gressive Layer-wise \textit{Distill}ation). \texttt{ProDistill} implements distillation through an activation matching strategy, where the merging coefficients are trained to minimize the activation distance between teacher and student models.
The training objective in \texttt{ProDistill} is decomposed layer by layer~\citep{bengio2006greedy,kulkarni2017layer,hettinger2017forward,karkar2024module,sakamoto2024end}, enabling the algorithm to avoid traditional end-to-end training and instead progressively train each layer of the model.
See Figure~\ref{fig: overview} for an illustration. 

We conduct extensive experiments to evaluate the performance of \texttt{ProDistill} across various tasks, architectures, and scales. Compared to both training-based and training-free baselines, \texttt{ProDistill} achieves a notable 6.14\% increase in absolute performance for vision tasks and 6.61\% increase for natural language understanding tasks.

Furthermore, \texttt{ProDistill} demonstrates improved data and computation efficiency, and incurs significantly lower memory costs. 
This makes \texttt{ProDistill} scalable to large model sizes. We apply \texttt{ProDistill} to merge large language models with over 10B parameters. To the best of our knowledge, this is the first time a \textit{training-based} merging algorithm has been scaled to such a large model size.


We summarize our contribution as follows:
\begin{itemize}[topsep=0em, itemsep=0em]
  \item We provide the first theoretical analysis on the necessity of domain-specific data for model merging, proving its critical role in ensuring effective merging performance.
  \item We propose \texttt{ProDistill}, a novel model merging algorithm that leverages teacher-student distillation to progressively merge model layers.  
  \item We conduct comprehensive empirical analyses to demonstrate the state-of-the-art performance of \texttt{ProDistill} on a wide variety of tasks. Our experiments highlight the data, computation, and memory efficiency of the proposed method. 
\end{itemize}