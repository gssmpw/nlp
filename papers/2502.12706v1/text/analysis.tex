\section{Further Analyses}
\label{sec: efficiency}

In this section, we provide additional analyses on the efficiency of \texttt{ProDistill}, in terms of data usage, computational cost, and memory requirements.

Additionally, we conduct a broad range of comparisons and ablation studies to further evaluate \texttt{ProDistill}.  The results can be found in Appendix~\ref{apx: ablations}, including:

\begin{itemize}[topsep=0em, itemsep=0em]
    \item Analyses of merging coefficient granularity~(Appendix~\ref{apx: granularity})
    \item Comparison with the end-to-end merging algorithm \texttt{DistillMerge}~(Appendix~\ref{apx: end2end})
    \item Comparison with standard supervised training and knowledge distillation~(Appendix~\ref{apx: direct train})
    \item Ablation studies on the design of dual inputs~(Appendix~\ref{apx: layer input}) 
\end{itemize}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/vitb32_data_num.png}
    \end{minipage}%
    \hspace{0.0\textwidth} %
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/compute.png}
    \end{minipage}%
    \hspace{0.0\textwidth} %
    \begin{minipage}{0.39\textwidth}
        \centering
        \vspace{4mm}
        \includegraphics[width=\linewidth]{figure/memory.png}
    \end{minipage}
    \caption{\textbf{Analysis of Data, Computation and Memory Efficiency.} \textbf{Left:} The average accuracy of \texttt{ProDistill} and AdaMerging across 8 vision tasks, with different data availability. Our method demonstrates superior data efficiency.
    \textbf{Middle:} The average accuracy of \texttt{ProDistill} with different training epochs. Our algorithm achieves a fast convergence. 
    \textbf{Right:} The training GPU memory cost of \texttt{ProDistill}, its unoptimized counterpart \texttt{DistillMerge} and AdaMerging. Our method has a significantly smaller memory footprint. 
    }
\label{fig: data and memory efficiency}
\end{figure*}


\subsection{Data Efficiency}
\label{sec: data efficiency}
We first evaluate the data efficiency of \texttt{ProDistill} by varying the number of validation samples, ranging from 1 to 256. The results are presented in Figure~\ref{fig: data and memory efficiency}. 

For vision tasks, \texttt{ProDistill} achieves a performance improvement of over 7\% even with just 1-shot validation data per task. As the number of validation shots increases, the performance continues to improve, consistently surpassing that of AdaMerging. With 256 validation shot, the accuracy reaches 88.05\%, which is only 2\% lower than that of individual checkpoints. 

\subsection{Computation Efficiency}
\label{sec: compute efficiency}

We evaluate the computational efficiency of \texttt{ProDistill} by varying the training epochs from 1 to 100. We set the validation shot to 64, and choose learning rate from \{0.1, 0.01\}.  

The results, provided in Figure~\ref{fig: data and memory efficiency}, highlight the rapid convergence of \texttt{ProDistill}. With just one epoch, the average accuracy of the merged model has a significant improvement, rising from 69.8 to 80.6. After approximately 10 epochs, the accuracy is nearly identical to the final results. Thus, despite being a training-based algorithm, \texttt{ProDistill} demonstrates exceptional computational efficiency with fast convergence.

The computation efficiency can be partially attributed to \texttt{ProDistill}'s ability to leverage large learning rates effectively, which we hypothesize is due to its layer-wise training scheme. In contrast, algorithms such as AdaMerging exhibit unstable convergence at high learning rates, as shown in the figure.


\subsection{Memory Efficiency}
\label{sec: memory efficiency}
Next, we evaluate the memory efficiency of \texttt{ProDistill} by profiling the maximum GPU memory consumption during training (excluding pre-processing and evaluation). The batch size is set to 32 for vision tasks and 16 for NLU tasks. The validation shot is set to 64. We compare \texttt{ProDistill} with its unoptimized direct training version \texttt{DistillMerge} and AdaMerging. 

The results, shown in Figure~\ref{fig: data and memory efficiency}, illustrate that \texttt{ProDistill} has an almost negligible GPU memory footprint compared to the baselines. This difference is particularly evident for models with a large number of layers, such as ViT-L-14, since the memory cost of our method remains independent of the model's depth. Therefore, \texttt{ProDistill} offers substantial advantages in memory efficiency and is scalable in resource-constrained environments.


