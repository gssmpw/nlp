\section{A Practical Algorithm for Few-Shot Model Merging}
\label{sec:method}




\begin{algorithm}[t]
    \small
    \caption{\texttt{ProDistill}~(\textit{Pro}gressive Layer-wise \textit{Distill}ation)}
    \label{alg:model_merging}
    \SetAlgoLined
    \KwIn{Pre-trained model weights $\boldsymbol{\theta}_{0}$, Fine-tuned model weights $\boldsymbol{\theta}_i$, unlabeled validation sets $\{\mathcal{D}_{i}\}_{i=1}^{T}$.}
    \KwOut{Merging coefficients $\hat{\boldsymbol{\lambda}}_{i}^{(l)}$, $1\le i \le T$, $1\le l\le L$. }

    \textbf{Initialize} $\mathcal{D}_i^{(0)} = \{(\boldsymbol{x}, \boldsymbol{x}) : \boldsymbol{x} \in \mathcal{D}_i\}$, and $\boldsymbol{\tau}_i = \boldsymbol{\theta}_i - \boldsymbol{\theta}_0$ for $i = 1, \ldots, T$.

    \For{$l = 1$ \textbf{\KwTo} $L$}{
        \textbf{Solve the objective function using gradient descent:}
            {\small \begin{align*}\{\hat{\boldsymbol{\lambda}}_i^{(l)}&\}_{i=1}^T = \argmin_{\{\boldsymbol{\lambda}_i^{(l)}\}_{i=1}^T}\sum_{i=1}^{T} \frac{1}{2T|\mathcal{D}_{i}|}\sum_{(\boldsymbol{z}_1, \boldsymbol{z}_2) \in \mathcal{D}_i^{(l-1)}}\\&\hspace{-20pt}\left\|\varphi^{(l)}\Big(\boldsymbol{\theta}_{0}^{(l)}+ \sum_{j=1}^{T} \boldsymbol{\lambda}_{j}^{(l)}\circ \boldsymbol{\tau}_{j}^{(l)}, \boldsymbol{z}_1\Big) - \varphi^{(l)}\Big(\boldsymbol{\theta}_{i}^{(l)}, \boldsymbol{z}_2\Big)\right\|^{2}\end{align*} }

        \For{$i = 1$ \textbf{\KwTo} $T$}{
            \textbf{Update} $\mathcal{D}_i^{(l)}$ \textbf{using:}
            {\small \begin{align*}\mathcal{D}_{i}^{(l)}= \Big\{\Big( \varphi^{(l)}\Big(\boldsymbol{\theta}_{0}^{(l)}+ \sum_{j=1}^{T} \hat{\boldsymbol{\lambda}}_{j}^{(l)}\circ \boldsymbol{\tau}_{j}^{(l)}, \boldsymbol{z}_1\Big), \\ \varphi^{(l)}\Big(\boldsymbol{\theta}_{i}^{(l)}, \boldsymbol{z}_2\Big) \Big): (\boldsymbol{z}_1, \boldsymbol{z}_2) \in \mathcal{D}_{i}^{(l-1)}\Big\}\end{align*} }
        }
    }
    \KwRet{$\hat{\boldsymbol{\lambda}}_i^{(l)}$ \textnormal{for} $1 \leq i \leq T$, $1 \leq l \leq L$.}
\end{algorithm}

    
        




In the previous section, we prove that data availability is crucial for effective model merging. 
Next, we propose a practical merging algorithm designed for such data-available settings. We start with a na\"ive distillation algorithm that directly minimizes the embedding distance, and build upon it to develop the main algorithm of this paper. 

\subsection{Model Merging as Knowledge Distillation}
Model merging can be viewed through the lens of knowledge distillation, a perspective that remains underexplored within the community. In this context, the teacher models correspond to fine-tuned models, and the goal is to create a student model that integrates their knowledge and performs well on the downstream tasks. 

A common strategy in knowledge distillation is to align the internal features of the teacher and student models~\citep{young2022feature,jin2024align}. This coincides with recent findings in the model merging community, which show that the performance of the merged model $\hat{\boldsymbol{\theta}}$ is positively correlated with the similarity between its embeddings $\varphi(\hat{\boldsymbol{\theta}}, \boldsymbol{x})$ and those of the fine-tuned models $\varphi(\boldsymbol{\theta}_i, \boldsymbol{x})$.~\citep{zhou2023going, yang2024representation}. 
Building on this insight,
we propose to align the merged model and fine-tuned models in the feature space by solving the following problem:
{
\begin{align}\label{eq:obj_naive}\min_{\boldsymbol{\lambda}_1, \cdots \boldsymbol{\lambda}_T}&\sum_{i=1}^{T}\frac{1}{2T|\mathcal{D}_{i}|}\sum_{\boldsymbol{x}\in\mathcal{D}_i}
\nonumber\\&\left\|\varphi\left(\boldsymbol{\theta}_{0}+\sum_{j=1}^{T}\boldsymbol{\lambda}_{i}\circ \boldsymbol{\tau}_{i}, \boldsymbol{x}\right) -\varphi\left(\boldsymbol{\theta}_{i},\boldsymbol{x}\right)\right\|^{2}.\end{align} }

The objective minimizes the $\ell_2$ distance between the internal embeddings of the merged model $\boldsymbol{\theta}_{0}+\sum_{j=1}^{T}\boldsymbol{\lambda}_{i}\circ \boldsymbol{\tau}_{i}$ and those of the fine-tuned model $\boldsymbol{\theta}_i$, over the unlabeled validation set $\mathcal{D}_i$ for each task.
Intuitively, this encourages the merged model to behave similarly to each fine-tuned model, in its specific input domain.
The objective can be solved using standard optimization algorithms such as Adam~\citep{kingma2014adam} or AdamW~\citep{loshchilov2017decoupled} on the merging coefficients $\boldsymbol{\lambda}_i$.

Throughout this paper, we refer to directly minimizing Objective~\ref{eq:obj_naive} as \texttt{DistillMerge}. Despite its simplicity, it offers several insights:

1. \textbf{Adaptive Merging Coefficient.} Gradient descent on $\boldsymbol{\lambda}_i$ can be interpreted as selecting the appropriate merging coefficients, whose empirical importance has been repeatedly highlighted in recent works~\citep{jin2022dataless, yang2023adamerging,gauthier2024merging}. Notably, the element-wise granularity of $\boldsymbol{\lambda}_i$ gives the merged model greater expressive power to fit the objective. 

2. \textbf{Fine-grained Model Merging via Distillation.} Minimizing~\ref{eq:obj_naive} can also be viewed as a way to distill the knowledge from the fine-tuned teacher models into a merged student model.
Unlike standard distillation algorithms~\citep{hinton2015distilling}, Objective~\ref{eq:obj_naive} leverages task vectors $\boldsymbol{\tau}_i$ as a prior on the trainable parameters. 
To justify this design choice, we further show in Appendix~\ref{apx: direct train} that \textit{in the few-shot setup}:
\begin{itemize}
    \setlength{\topsep}{0em}
    \setlength{\itemsep}{0em}
    \item Feature-based distillation loss provides a stronger supervision compared with logit-based distillation;
    \item Optimizing the scaling coefficients yields better results, compared with directly optimizing the model weights. 
\end{itemize}

\subsection{Efficient Implementation by Progressive Layer-wise Distillation}
While Equation~\ref{eq:obj_naive} is a reasonable objective for  training the merging coefficients, 
directly optimizing this objective incurs significant memory overhead. This is because both the task vectors and the trainable merging coefficients, which have the same dimensionality as the model parameters, have to be stored in memory. The memory cost scales linearly with the number of tasks. Besides, the optimization process requires to store the activations, gradients and optimizer states, further exacerbating memory overhead. This challenge becomes particularly critical when merging large language models, which often contain billions of parameters.

To mitigate this issue, we propose the following surrogate to Objective~\ref{eq:obj_naive}.
Instead of optimizing the global objective across all layers simultaneously, we adopt a progressive, layer-by-layer merging strategy.
For each layer $l~(1\le l \le L)$, we minimize the feature distance between layer embeddings using the following objective:

{\small
\vspace{-20pt}
\begin{align}
    \min_{\boldsymbol{\lambda}_1^{(l)}, \cdots, \boldsymbol{\lambda}_T^{(l)}} & \sum_{i=1}^{T}\frac{1}{2T|\mathcal{D}_{i}|}\sum_{(\boldsymbol{z}_1, \boldsymbol{z}_2) \in \mathcal{D}_i^{(l-1)}}\nonumber\\
& \hspace{-30pt} \left\| \varphi^{(l)}\Big( \boldsymbol{\theta}_{0}^{(l)}+ \sum_{j=1}^{T}\boldsymbol{\lambda}_{i}^{(l)}\circ \boldsymbol{\tau}_{i}^{(l)}, \boldsymbol{z}_1 \Big) - \varphi^{(l)}\left( \boldsymbol{\theta}_{i}^{(l)}, \boldsymbol{z}_2 \right) \right\|^{2} .
\end{align}
\label{eq:obj}
}

Compared to Objective~\ref{eq:obj_naive}, this layer-wise formulation focuses only on minimizing the embedding distances in each layer, instead of all intermediate embeddings simultaneously. 
Moreover, this objective introduces \textbf{dual inputs} by feeding different intermediate embeddings to the fine-tuned and merged models. 
Specifically, $\mathcal{D}_{i}^{(l)}$ maintains pairs of embeddings $(\boldsymbol{z}_1, \boldsymbol{z}_2)$ after the $l$-th layer, where $\boldsymbol{z}_1$ is the embedding of
the merged model, while $\boldsymbol{z}_2$ is the embedding of the fine-tuned
model. These internal activations are cached and updated using the trained coefficients for each layer by the following rule:
\begin{align*}
     & \mathcal{D}_{i}^{(0)}= \left\{(\boldsymbol{x}, \boldsymbol{x}): \boldsymbol{x} \in \mathcal{D}_{i} \right\},                                                                            \\
     & \mathcal{D}_{i}^{(l)}= \Bigg\{ \Big( \varphi^{(l)}\Big( \boldsymbol{\theta}_{0}^{(l)}+ \sum_{j=1}^{T}\hat{\boldsymbol{\lambda}}_{i}^{(l)}\circ \boldsymbol{\tau}_{i}^{(l)}\Big), \boldsymbol{z}_1 \Big), \\
     & \quad \varphi^{(l)}\left( \boldsymbol{\theta}_{i}^{(l)}, \boldsymbol{z}_2 \right) \Big): (\boldsymbol{z}_1, \boldsymbol{z}_2) \in \mathcal{D}_{i}^{(l-1)}\Bigg\}, \quad l \ge 1.
\end{align*}

This design better approximates the global distillation objective~\ref{eq:obj_naive}, \textbf{distinguishing it from previous merging algorithms based on feature alignment}~\citep{jin2022dataless, yang2024representation, anonymous2024leveraging}, which align the output of merged model and fine-tuned model \textit{under the same input}.
As demonstrated in Appendix~\ref{apx: layer input},  the incorporation of dual inputs is critical for achieving high performance in layer-wise training. 

We refer to this algorithm as \texttt{ProDistill}, short for \textit{Pro}gressive Layer-wise \textit{Distill}ation.
The pseudocode for \texttt{ProDistill} is given in Algorithm~\ref{alg:model_merging}.
Compared to \texttt{DistillMerge}, \texttt{ProDistill} offers substantial efficiency gains. When merging a specific layer, \texttt{ProDistill} only requires memory for the task vector and merging coefficients of the current layer, rather than the entire model. Furthermore, 
the forward and backward passes are also restricted within individual layers. Interestingly, unlike the common belief that layer-wise training leads to performance degradation, we show in Appendix~\ref{apx: end2end} that \texttt{ProDistill} outperforms its end-to-end counterpart \texttt{DistillMerge}.




