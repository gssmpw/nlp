
\section{Preliminaries}
We consider model merging in a pretrain-to-finetune setup. Let $\boldsymbol{\theta}_0$ denote the weights of a pre-trained model. Consider a set of $T$ tasks, each with a model $\boldsymbol{\theta}_i$ fine-tuned from $\boldsymbol{\theta}_0$.
Model merging aims to combine the knowledge learned by task-specific models $\boldsymbol{\theta}_i$ into a unified model $\hat{\boldsymbol{\theta}}$, which preserves the generalization ability of the pre-trained model and incorporates the specialized knowledge from each task. 


\vspace{-8pt}
\paragraph{Task vectors.} A key insight in this setup is the task vectors. The \textit{task vector} for the $i$-th task is defined as $\boldsymbol{\tau}_i = \boldsymbol{\theta}_i - \boldsymbol{\theta}_0$.
An effective model merging method~\citep{ilharco2022editing, zhang2023composing} is to compute a weighted average of task vectors and add it back to the pre-trained model: 
\vspace{-3pt}
\begin{align*} 
\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}_0 + \sum_{i=1}^T \boldsymbol{\lambda}_i\circ \phi(\boldsymbol{\tau}_i), 
\end{align*}
where $\boldsymbol{\lambda}_i$ denotes the merging coefficients, and $\phi(\cdot)$ is an optional transformation function applied to the task vectors.

The merging coefficients $\boldsymbol{\lambda}_i$ can operate at different granularities. Common approaches include task-wise granularity~\citep{ilharco2022editing}, which assigns a single merging coefficient to each task, and layer-wise granularity~\citep{yang2023adamerging}, which assigns a coefficient to each layer of the models. In this paper, we take one step forward and consider \textbf{element-wise granularity}.\footnote{The \textit{layer-wise} in the title and algorithm name does not refer to the granularity of $\boldsymbol{\lambda}_i$, but instead  refers to the training procedure.}
Specifically, the
merging coefficient $\boldsymbol{\lambda}_{i}$ has the same dimensionality as
$\boldsymbol{\theta}_{i}$, and an element-wise multiplication $\boldsymbol{\lambda}_{i}\circ \boldsymbol{\tau}_{i}$ is performed for each task. In this paper, we do not apply additional transformations $\phi(\cdot)$ to the task vectors, as our method is parallel to the transformation-based methods.


\vspace{-8pt}
\paragraph{Notations. }
We use $\mathcal{D}_i$ to denote a few-shot unlabeled validation dataset for each task that is possibly available.
Define $\varphi(\boldsymbol{\theta}, \cdot)$ as the feature mapping of the model parameterized by $\boldsymbol{\theta}$, which gives the vectorized embedding of all intermediate layers. Let $L$ denote layer number. For model weights $\boldsymbol{\theta}$ and layer index $l$, we use $\boldsymbol{\theta}^{(l)}$ to
denote the parameter of the $l$-th layer and $\varphi^{(l)}(\boldsymbol{\theta}^{(l)}, \cdot)$
to denote the feature mapping function defined by this layer. 