\section{Theoretical Limitations on Data-Agnostic Model Merging}
Model merging algorithms can be broadly classified into two categories based on data availability: \textit{data-agnostic} algorithms, which only use the weights of pre-trained and fine-tuned models~(\emph{e.g.}, ~\citet{ilharco2022editing, yadav2024ties, yu2024language}), and \textit{data-dependent} algorithms, which require access to a validation set~(\emph{e.g.},~\citet{matena2022merging, jin2022dataless}). While data-agnostic algorithms have shown significant empirical success, we prove that their \textit{worst-case} performance can be arbitrarily poor, even for simple linear models.

\subsection{Hardness Results for Fixed Models}

Consider the following simplified setup for model merging. Suppose we have two tasks with datasets $\mathcal{D}_1, \mathcal{D}_2$ and loss function $\ell(\cdot, \cdot)$. Let $f_1, f_2$ denote two models to merge, and let $\mathcal{M}$ denote a \textit{data-agnostic} model merging algorithm, which we assume to be deterministic for simplicity. 

The following hardness result states that for any such algorithm $\mathcal{M}$, one can always construct adversarial datasets such that the merging performance is arbitrarily bad. 

\begin{restatable}{theorem}{thma}
\label{thm: a}
    There exist a task and loss function $\ell$, such that for any data-agnostic model merging algorithm $\mathcal{M}$, any pair of models $f_1\neq f_2$, and any $\varepsilon, C>0$, there exists two datasets $\mathcal{D}_1, \mathcal{D}_2$, such that $f_1, f_2$ have a near-zero loss  on $\mathcal{D}_1$ and $\mathcal{D}_2$, respectively:
    \vspace{0pt}
    \begin{align*}
        \ell(D_1, f_1)\le \varepsilon,\quad  \ell(D_2, f_2)\le \varepsilon, 
    \end{align*}
    but the merged model $\hat{f}=\mathcal{M}(f_1, f_2)$ has a constant loss on $\mathcal{D}_1\cup \mathcal{D}_2$:
    \vspace{0pt}
    \begin{align*}
        \ell(\mathcal{D}_1\cup \mathcal{D}_2, \hat{f})\ge C.
    \end{align*}
    On the other hand, there exists a ground truth model $f^*$ that has near-zero loss on $\mathcal{D}_1$ and $\mathcal{D}_2$:
    \vspace{0pt}
    \begin{align*}
        \ell(\mathcal{D}_1\cup \mathcal{D}_2, f^*)\le \varepsilon.
    \end{align*}
\end{restatable}

\vspace{-5pt}
Theorem~\ref{thm: a} is proved by adversarially constructing linear regression instances based on the merged model. The complete proofs are deferred to Appendix~\ref{apx: proof}.

\subsection{Hardness Results for Learned Models}

Theorem~\ref{thm: a} assumes fixed models $f_1$ and $f_2$, which can deviate from real-world scenarios where models are trained on datasets. To address this, we extend the analysis to cases where models are learned using an algorithm $\mathcal{L}$, with $f_1 = \mathcal{L}(\mathcal{D}_1), f_2 = \mathcal{L}(\mathcal{D}_2)$. We prove the following result. 

\begin{restatable}{theorem}{thmb}
\label{thm: b}
There exist a task, a loss function $\ell$ and a learning algorithm $\mathcal{L}$, such that for any data-agnostic model merging algorithm $\mathcal{M}$ and any $\varepsilon, C>0$, there exist two adversarial datasets $\mathcal{D}_1, \mathcal{D}_2$, such that $f_1 = \mathcal{L}(D_1), f_2=\mathcal{L}(\mathcal{D}_2)$ have a near-zero loss  on $\mathcal{D}_1$ and $\mathcal{D}_2$ respectively:
    \vspace{0pt}
    \begin{align*}
        \ell(D_1, f_1)\le \varepsilon,\quad  \ell(D_2, f_2)\le \varepsilon, 
    \end{align*}
    but the merged model $\hat{f}=\mathcal{M}(f_1, f_2)$ has a constant loss on $\mathcal{D}_1\cup \mathcal{D}_2$:
    \vspace{0pt}
    \begin{align*}
        \ell(\mathcal{D}_1\cup \mathcal{D}_2, \hat{f})\ge C.
    \end{align*}
    On the other hand, the model learned on the merged dataset $f^* = \mathcal{L}(\mathcal{D}_1\cup \mathcal{D}_2)$ that has near-zero loss on $\mathcal{D}_1$ and $\mathcal{D}_2$:
    \vspace{0pt}
    \begin{align*}
        \ell(\mathcal{D}_1\cup \mathcal{D}_2, f^*)\le \varepsilon.
    \end{align*}
\end{restatable}

\vspace{-5pt}
The hard instance constructed in the proof involves solving a linear-separable classification problem with max-margin classifiers. 
A broad range of algorithms fall into this category, including traditional support vector machine (SVM)~\citep{cortes1995support} and gradient descent algorithms that have max-margin implicit bias~\citep{nacson2019convergence, lyu2019gradient}.
The key insight is that the learning process often discards certain information, such as distant data points outside the margin. This enables adversarial manipulation of the datasets to degrade merging performance.


\begin{remark}
\textit{Theorem~\ref{thm: a} and~\ref{thm: b} are both worst-case analyses. They highlight the fundamental limitations of data-agnostic model merging algorithms, but do not contradict their empirical success.
Instead, these results underscore the importance of data availability in achieving robust and consistent merging performance.}
\end{remark}