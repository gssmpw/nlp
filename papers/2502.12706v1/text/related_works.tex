\section{Related Works}
\paragraph{Model Merging via Weight Averaging.}
Weight averaging is an effective and widely adopted approach in model merging~\citep{izmailov2018averaging,wortsman2022model,ilharco2022editing}. Researchers have developed various methods to improve the averaging approach. One line of work focuses on minimizing conflicts and promoting disentanglement between task vectors, through sparsification~\citep{tang2023concrete,yadav2024ties, yu2024language,he2024localize,wang2024localizing,bowen2024beyond,deng2024dare,zhu2024model,davari2025model} or decomposition~\citep{tam2023merging,xiong2024multi, stoica2024model,wei2025modeling,gargiulo2024task,marczak2025no,yang2025mix}. Another line of work~\citep{ortiz2024task,tang2023parameter} employs linearized training to explicitly enforce linearity. Some studies explore methods for selecting optimal merging coefficients, using training-based~\citep{yang2023adamerging, gauthier2024merging,  nishimoto2024differentiable} or training-free~\citep{matena2022merging, jin2022dataless, zhou2024metagpt,wang2024lines,liu2024checkpoint,tang2025merging} approaches. Broadly speaking, our paper aligns with the former category. Other works~\citep{qi2024less, lu2024twin, zheng2024free,ohadapting,zhang2024channel,osial2024parameter,huang2024emr} propose dynamic model merging through task-specific routing and mixture-of-experts frameworks. \textbf{Our method differs from them by preserving the original model architecture}. 

In addition to merging fine-tuned models, there is a broader field of research exploring more general setups, such as merging independently trained models~\citep{singh2020model,ainsworth2022git,navon2023equivariant,horoi2024harmony,stoica2023zipit,xu2024training} and merging models with different architectures~\citep{avrahami2022gan,wan2024knowledge,wan2024fusechat}.


\vspace{-10pt}
\paragraph{Distillation and Activation Alignment.}
Knowledge distillation~\citep{hinton2015distilling,romero2014fitnets,yim2017gift} is a well-established topic in machine learning where a student model is trained to mimic the behavior of teacher models.
Some works leverage teacher-student activation matching to merge models and do multi-task learning ~\citep{li2020knowledge, ghiasi2021multi,yang2022factorizing,jin2022dataless,kong2024rethink,zhang2024knowledge,nasery2024pleas}, which share similarities with our paper. 
For example, the \textbf{Surgery} algorithm proposed in~\citep{yang2024representation, yang2024surgeryv2}  introduces lightweight, task-specific modules to facilitate post-merging activation matching. However, these methods differ from ours as dynamic model merging approaches.
\textbf{A very recent work \citet{anonymous2024leveraging}} matches the activations in each layer by solving linear equations of merging coefficient. Their approach differs from ours in several key aspects. 
First, their merging coefficient granularity is layer-wise, whereas we consider element-wise coefficients. This distinction is crucial, as their method cannot be directly extended to element-wise merging without making the linear equations under-determined.  Additionally, their layer inputs are generated by pretrained models, in contrast to the dual inputs approach used in our approach. 
The training-free algorithm \textbf{RegMean}~\citep{jin2022dataless} also shares similarity with our method. We leave its discussion to Appendix~\ref{apx:baselines}.