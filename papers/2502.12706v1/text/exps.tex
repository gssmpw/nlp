\section{Experiments}
\label{sec:exps}

In this section, we present comprehensive experiment results to evaluate the effectiveness of \texttt{ProDistill} across various settings. Code is available at \url{https://github.com/JingXuTHU/Scalable_Model_Merging_with_Progressive_Layerwise_Distillation}.

\begin{table*}[t]
\setlength{\tabcolsep}{4pt}
\centering
\caption{\textbf{Performance of merging ViT-B-32 models across eight downstream vision tasks.} \texttt{ProDistill} consistently outperforms the baselines under different data availability. The results for Localize-and-Stich are directly taken from~\citet{he2024localize}.}
\label{tab:vitb32}   
\begin{tabular}{l|cccccccc|cc}
\toprule
\textbf{Method} &\textbf{SUN397}& \textbf{Cars}& \textbf{RESISC45}& \textbf{EuroSAT}& \textbf{SVHN}& \textbf{GTSRB}& \textbf{MNIST}& \textbf{DTD} &\textbf{Avg}  \\
\midrule
{Individual}  & 75.34 & 77.73 & 95.98 & 99.89 & 97.46 & 98.73 & 99.69 & 79.36 & 90.52 \\
Task Arithmetic & 55.32 & 54.98 & 66.68 & 78.89 & 80.21 & 69.68 & 97.34 & 50.37 & 69.18 \\
\midrule
RegMean& 67.47 & 66.63 & 81.75 & 93.33 & 86.68 & 79.92 & 97.30 & 60.16 & 79.15 \\
Fisher merging & 63.95 & 63.84 & 66.86 & 83.48 & 79.54 & 60.11 & 91.27 & 49.36 & 69.80 \\
Localize-and-Stich  & 67.20 & 68.30 & 81.80 & 89.40 & 87.90 & 86.60 & 94.80 & 62.90 & 79.90 \\
AdaMerging& 63.69 & 65.74 & 77.65 & 91.00 & 82.48 & 93.12 & 98.27 & 62.29 & 79.28 \\ 
\rowcolor{lightyellow}
\texttt{ProDistill}~(Ours)& \textbf{68.90} & \textbf{71.21} & \textbf{89.89} & \textbf{99.37} & \textbf{96.13} & \textbf{95.29} & \textbf{99.46} & \textbf{68.03} & \textbf{86.04} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/svhn_tsne11.jpg}
    \caption{\textbf{The t-SNE visualization of ViT-B-32 model trained by different merging algorithms, on the SVHN dataset.} The features given by \texttt{ProDistill}  are the most separated, resembling those of fine-tuned models.}
    \label{fig:tsne_svhn}
\end{figure*}

\subsection{Setup}
\label{sec:setup}
We consider three main experimental setups: 
(1) Merging Vision Transformers~\citep{dosovitskiy2020image} on image classification tasks; 
(2) Merging BERT~\citep{devlin2018bert} and RoBERTa~\citep{liu2019roberta} models on natural language understanding~(NLU) tasks; 
(3) Merging LLAMA2~\citep{touvron2023llama2} model on natural language generation~(NLG) tasks. 

\paragraph{Tasks and Models:}
For image classification tasks, we follow the setting in~\citet{ilharco2022editing} and use Vision Transformer~(ViT) models pre-trained on the ImageNet dataset and subsequently fine-tuned on 8 downstream datasets. 
For NLU and NLG tasks, we merge the BERT-base and RoBERTa-base models fine-tuned on 8 NLU tasks from the GLUE~\citep{wang2018glue} benchmark, and perform pairwise merging of WizardLM-13B, WizardMath-13B and llama-2-13b-code-alpaca models, following the setting in~\citep{yu2024language}. 
Detailed information on the models and datasets can be found in Appendix~\ref{apx:dataset}.

\paragraph{Baselines:}
For vision and NLU tasks, we compare \texttt{ProDistill} with a wide range of baselines, including 
Task Arithmetic~\citep{ilharco2022editing}, 
Fisher merging~\citep{matena2022merging},
RegMean~\citep{jin2022dataless}, 
AdaMerging~\citep{yang2023adamerging} and Localize-and-Stich~\citep{he2024localize}. 
All methods, except Task Arithmetic, require a few-shot unlabeled validation dataset, which is randomly sampled from the training set, with validation shot set to 64 per task.
For NLG tasks, we compare \texttt{ProDistill} with Task Arithmetic~\citep{ilharco2022editing}, TIES-Merging~\citep{yadav2024ties} and WIDEN~\citep{yu2024extend}, due to scale constraints.
A detailed discussion of the baselines and their implementations is provided in Appendix~\ref{apx:baselines} and~\ref{apx:impl}.



\subsection{Results on Merging ViT models}
Table~\ref{tab:vitb32} presents the performance of merging ViT-B-32 models across eight downstream vision tasks.
The results for ViT-B-16 and ViT-L-14 are provided in Appendix~\ref{apx: more results}.

Our method consistently outperforms all baselines, yielding significant improvements in average performance.
Specifically, \texttt{ProDistill} achieves an average performance of 86.04\%, surpassing the baselines by 6.14\%. Notably, it is only 4\% below the average performance of the individual fine-tuned models.

We also visualize the final-layer activations of the merged model using t-SNE~\citep{van2008visualizing}. The results are given in Figure~\ref{fig:tsne_svhn} and Appendix~\ref{apx: tsne}. The visualization shows that the features given by \texttt{ProDistill} are more separated compared to the baselines, closely resembling those of the fine-tuned models.




\subsection{Results on Merging Encoder-based Language Models}
Table~\ref{tab:roberta} summarizes the results of merging RoBERTa models fine-tuned on the NLU tasks.
The results of BERT models are deferred to Appendix~\ref{apx: more results}.
Similar to the vision tasks, \texttt{ProDistill} achieves significant performance improvements of 6.61\% on the NLU tasks, outperforming all baselines across nearly all tasks. 

Unlike vision tasks, the NLU tasks in the GLUE benchmark have small class numbers. For example, SUN387 dataset consists of 397 classes, while CoLA only has 2 classes.  This class size disparity limits the performance of methods that operate directly on the model output logits, such as AdaMerging and Fisher merging.
Our method, along with RegMean, performs particularly well, emphasizing the importance of leveraging internal feature embeddings for effective model merging.

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{5pt}
\caption{\textbf{Performance of merging RoBERTa models on the NLU tasks.} \texttt{ProDistill} achieves superior performance across almost all tasks.}
\label{tab:roberta} 
\begin{tabular}{l|cccccccc|cc}
\toprule
\textbf{Method} & \textbf{CoLA} & \textbf{SST-2} & \textbf{MRPC} & \textbf{STS-B} & \textbf{QQP} & \textbf{MNLI} & \textbf{QNLI} & \textbf{RTE} & \textbf{Avg} \\

\midrule
Individual & 0.5458 & 0.9450 & 0.8858 & 0.9030 & 0.8999 & 0.8710 & 0.9244 & 0.7292 & 0.8380\\
Task Arithmetic  & 0.0804 & 0.8475 & 0.7865 & 0.4890 & 0.8133 & 0.7063 & 0.7558 & 0.6534 & 0.6415 \\
\midrule
RegMean & 0.3022 & 0.9255 & 0.8183 & 0.5152 & \textbf{0.8176} & 0.7089 & 0.8503 & 0.6462 & 0.6980 \\
Fisher merging& 0.1633 & 0.7064 & 0.7264 & 0.1274 & 0.6962 & 0.4968 & 0.5599 & 0.5776 & 0.5068 \\
Localize-and-Stich& 0.0464 & 0.8922 & 0.7916 & \textbf{0.7232} & 0.7821 & 0.5709 & 0.7703 & 0.5632 & 0.6425 \\
AdaMerging& 0.000 & 0.8532 & 0.7875 & 0.5483 & 0.8086 & 0.7039 & 0.7247 & 0.6390 & 0.6332 \\
\rowcolor{lightyellow}
\rowcolor{lightyellow}
\rowcolor{lightyellow}
\texttt{ProDistill}~(Ours)& \textbf{0.4442} & \textbf{0.9312} & \textbf{0.8464} & 0.6942 & 0.8134 & \textbf{0.7857} & \textbf{0.8900} & \textbf{0.7076} & \textbf{0.7641} \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Results on Merging Large Language Models}
We present the results of merging the WizardMath-13B and Llama-2-13B-Code-Alpaca models in~Table~\ref{tab: code_math}, with additional results provided in Appendix~\ref{apx: more results} and generation examples provided in Appendix~\ref{apx: llm example}. These findings demonstrate that our method effectively scales up to models with over 10B parameters, and achieves superior performance compared to baselines.

\begin{table*}[t]
\centering
\caption{\textbf{Performance of merging LLM models on Code and Math tasks.} Our method demonstrates an improved performance and a strong scalability. The results of TIES-Merging and WIDEN are directly taken from~\citet{yu2024extend}.}
\begin{tabular}{l|cccc|cc}
\toprule
\textbf{Method} & \textbf{GSM8K} & \textbf{MATH} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{Avg} & \textbf{Norm Avg} \\

\midrule
WizardMath-13B & 0.6361 & 0.1456 & 0.0671 & 0.0800 & 0.2322 & 0.6430 \\
Llama-2-13b-code-alpaca & 0.000 & 0.000 & 0.2378 & 0.2760 & 0.1285 & 0.5000 \\
\midrule
Task Arithmetic & \textbf{0.6467} & \textbf{0.1462} & 0.0854 & 0.0840 & 0.2406 & 0.6711\\
TIES-Merging & 0.6323 & 0.1356 & 0.0976 & 0.2240 & 0.2723 & 0.7868\\
WIDEN & 0.6422 & 0.1358 & 0.0976 & 0.0980 & 0.2434 & 0.6769\\
\rowcolor{lightyellow}
\texttt{ProDistill} (Ours) & 0.6279 & 0.1424 & \textbf{0.1280} & \textbf{0.2239} & \textbf{0.2806} & \textbf{0.8288}\\
\bottomrule
\end{tabular}
\label{tab: code_math}
\end{table*}