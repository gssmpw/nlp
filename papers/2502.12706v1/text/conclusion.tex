\section{Conclusion}

In this paper, we propose a novel model merging algorithm \texttt{ProDistill} which uses progressive layer-wise distillation to efficiently merge large pre-trained models. Our theoretical analysis shows the necessity of domain-specific data for effective merging. Empirical results demonstrate that \texttt{ProDistill} outperforms existing methods across a variety of tasks, achieving significant performance gains with reduced memory costs, which makes it a scalable solution for merging large pre-trained models.
