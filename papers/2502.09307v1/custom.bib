@article{cumulative_link,
 author = {Jack Edward Taylor and Guillaume A. Rousselet and Christoph Scheepers and Sara C. Sereno},
 journal = {Behavior research methods},
 title = {Rating norms should be calculated from cumulative link mixed effects models.},
 year = {2021}
}

@inproceedings{bert_gp,
  title={BERT Shows Garden Path Effects},
  author={Tovah Irwin and Kyra Wilson and Alec Marantz},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258378287}
}

@article{gp_reading_time,
  title={Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities},
  author={Suhas Arehalli and Brian Dillon and Tal Linzen},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.12187},
  url={https://api.semanticscholar.org/CorpusID:253098758}
}

@article{dalle_3,
  title={Dall-e 3 System Card},
  author={OpenAI},
  year={2024},
  journal={OpenAI},
  url={https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf}
}

@article{kuribayashi2025large,
  title={Large Language Models Are Human-Like Internally},
  author={Kuribayashi, Tatsuki and Oseki, Yohei and Taieb, Souhaib Ben and Inui, Kentaro and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2502.01615},
  year={2025}
}

@misc{hanna2024incrementalsentenceprocessingmechanisms,
      title={Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models}, 
      author={Michael Hanna and Aaron Mueller},
      year={2024},
      eprint={2412.05353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.05353}, 
}

@incollection{christianson2022if,
  title={What if they're just not that into you (or your experiment)? On motivation and psycholinguistics},
  author={Christianson, Kiel and Dempsey, Jack and Tsiola, Anna and Goldshtein, Maria},
  booktitle={Psychology of learning and motivation},
  volume={76},
  pages={51--88},
  year={2022},
  publisher={Elsevier}
}

@article{fine2013rapid,
  title={Rapid expectation adaptation during syntactic comprehension},
  author={Fine, Alex B and Jaeger, T Florian and Farmer, Thomas A and Qian, Ting},
  journal={PloS one},
  volume={8},
  number={10},
  pages={e77661},
  year={2013},
  publisher={Public Library of Science San Francisco, USA}
}

@inproceedings{hu-etal-2020-systematic,
    title = "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
    author = "Hu, Jennifer  and
      Gauthier, Jon  and
      Qian, Peng  and
      Wilcox, Ethan  and
      Levy, Roger",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.158",
    doi = "10.18653/v1/2020.acl-main.158",
    pages = "1725--1744",
    abstract = "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
}

@article{linzen-etal-2016-assessing,
    title = "Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies",
    author = "Linzen, Tal  and
      Dupoux, Emmanuel  and
      Goldberg, Yoav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1037",
    doi = "10.1162/tacl_a_00115",
    pages = "521--535",
    abstract = "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture{'}s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1{\%} errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
}

@inproceedings{amouyal-etal-2024-large,
    title = "Large Language Models for Psycholinguistic Plausibility Pretesting",
    author = "Amouyal, Samuel  and
      Meltzer-Asscher, Aya  and
      Berant, Jonathan",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.12",
    pages = "166--181",
    abstract = "In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.",
}


@article{rouder2018power,
  title={Power, dominance, and constraint: A note on the appeal of different design traditions},
  author={Rouder, Jeffrey N and Haaf, Julia M},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={1},
  number={1},
  pages={19--26},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{gp3,
  title={Use of verb information in syntactic parsing: evidence from eye movements and word-by-word self-paced reading.},
  author={Ferreira, Fernanda and Henderson, John M},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={16},
  number={4},
  pages={555},
  year={1990},
  publisher={American Psychological Association}
}

@article{gp2,
  title={Verb-specific constraints in sentence processing: separating effects of lexical preference from garden-paths.},
  author={Trueswell, John C and Tanenhaus, Michael K and Kello, Christopher},
  journal={Journal of Experimental psychology: Learning, memory, and Cognition},
  volume={19},
  number={3},
  pages={528},
  year={1993},
  publisher={American Psychological Association}
}

@article{gp1,
  title={The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences},
  author={Garnsey, Susan M and Pearlmutter, Neal J and Myers, Elizabeth and Lotocky, Melanie A},
  journal={Journal of memory and language},
  volume={37},
  number={1},
  pages={58--93},
  year={1997},
  publisher={Elsevier}
}

@misc{fastchat,
 archiveprefix = {arXiv},
 author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
 eprint = {2306.05685},
 primaryclass = {cs.CL},
 title = {Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
 year = {2023}
}

@article{Ren2024DoLL,
  title={Do Large Language Models Mirror Cognitive Language Processing?},
  author={Yuqi Ren and Renren Jin and Tongxuan Zhang and Deyi Xiong},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.18023},
  url={https://api.semanticscholar.org/CorpusID:268041437}
}

@article{Sun2024ComputationalSM,
  title={Computational Sentence-level Metrics Predicting Human Sentence Comprehension},
  author={Kun Sun and Rong Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.15822},
  url={https://api.semanticscholar.org/CorpusID:268680327}
}

@article{Rego2024LanguageMO,
  title={Language models outperform cloze predictability in a cognitive model of reading},
  author={Adrielli Lopes Rego and Joshua Snell and Martijn Meeter},
  journal={PLOS Computational Biology},
  year={2024},
  volume={20},
  url={https://api.semanticscholar.org/CorpusID:269527476}
}

@article{spark-agi,
 author = {S{\'e}bastien Bubeck and Varun Chandrasekaran and Ronen Eldan and John A. Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuan-Fang Li and Scott M. Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
 journal = {ArXiv preprint},
 title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
 url = {https://arxiv.org/abs/2303.12712},
 volume = {abs/2303.12712},
 year = {2023}
}

@inproceedings{BART,
 address = {Online},
 author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.703},
 pages = {7871--7880},
 publisher = {Association for Computational Linguistics},
 title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
 url = {https://aclanthology.org/2020.acl-main.703},
 year = {2020}
}

@inproceedings{Vaswani2017AttentionIA,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@article{tal_data,
 author = {Wing-Yee Chow and Cybelle Smith and Ellen F. Lau and Colin Phillips},
 journal = {Language, Cognition and Neuroscience},
 pages = {577 - 596},
 title = {A “bag-of-arguments” mechanism for initial verb predictions},
 volume = {31},
 year = {2016}
}

@article{large_scale_evidence,
 author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger Philip},
 publisher = {PsyArXiv},
 title = {Large-scale evidence for logarithmic effects of word predictability on reading time},
 year = {2022}
}

@inproceedings{lexical_surprisal_rt,
 address = {Avignon, France},
 author = {Fernandez Monsalve, Irene  and
Frank, Stefan L.  and
Vigliocco, Gabriella},
 booktitle = {Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics},
 pages = {398--408},
 publisher = {Association for Computational Linguistics},
 title = {Lexical surprisal as a general predictor of reading time},
 url = {https://aclanthology.org/E12-1041},
 year = {2012}
}

@article{effect_of_pred_rt,
 author = {Nathaniel J. Smith and R. Levy},
 journal = {Cognition},
 pages = {302-319},
 title = {The effect of word predictability on reading time is logarithmic},
 volume = {128},
 year = {2013}
}

@inproceedings{probabilistic_pred_psycholing_mod,
 address = {Online},
 author = {Hao, Yiding  and
Mendelsohn, Simon  and
Sterneck, Rachel  and
Martinez, Randi  and
Frank, Robert},
 booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
 doi = {10.18653/v1/2020.cmcl-1.10},
 pages = {75--86},
 publisher = {Association for Computational Linguistics},
 title = {Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling},
 url = {https://aclanthology.org/2020.cmcl-1.10},
 year = {2020}
}

@inproceedings{multilingual_reading_time,
 address = {Online},
 author = {Hollenstein, Nora  and
Pirovano, Federico  and
Zhang, Ce  and
J{\"a}ger, Lena  and
Beinborn, Lisa},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.10},
 pages = {106--123},
 publisher = {Association for Computational Linguistics},
 title = {Multilingual Language Models Predict Human Reading Behavior},
 url = {https://aclanthology.org/2021.naacl-main.10},
 year = {2021}
}

@article{language_explain_reading_time,
 author = {Markus J. Hofmann and Steffen Remus and Chris Biemann and Ralph R. Radach and Lars Kuchinke},
 journal = {Frontiers in Artificial Intelligence},
 title = {Language Models Explain Word Reading Times Better Than Empirical Predictability},
 volume = {4},
 year = {2020}
}

@article{stephanie_data,
 author = {Stephanie Rich and Matt Wagers},
 journal = {Talk at the Human Sentence Processing conference},
 title = {Semantic similarity and temporal contiguity in subject-verb dependency processing.},
 year = {2020}
}

@article{huggingface,
 author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
 journal = {ArXiv preprint},
 title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
 url = {https://arxiv.org/abs/1910.03771},
 volume = {abs/1910.03771},
 year = {2019}
}

@article{falcon40b,
 author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
 title = {{Falcon-40B}: an open large language model with state-of-the-art performance},
 year = {2023}
}

@article{alpaca2023,
 author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
 title = {Alpaca: A Strong, Replicable Instruction-Following Model},
 url = {https://crfm.stanford.edu/2023/03/13/alpaca.html},
 year = {2023}
}

@inproceedings{Kuribayashi2023PsychometricPP,
  title={Psychometric Predictive Power of Large Language Models},
  author={Tatsuki Kuribayashi and Yohei Oseki and Timothy Baldwin},
  booktitle={NAACL-HLT},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265150440}
}

@article{Arehalli2022SyntacticSF,
  title={Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities},
  author={Suhas Arehalli and Brian Dillon and Tal Linzen},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.12187},
  url={https://api.semanticscholar.org/CorpusID:253098758}
}

@article{Li2024IncrementalCO,
  title={Incremental Comprehension of Garden-Path Sentences by Large Language Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention},
  author={Andrew Li and Xianle Feng and Siddhant Narang and Austin Peng and Tianle Cai and Raj Sanjay Shah and Sashank Varma},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.16042},
  url={https://api.semanticscholar.org/CorpusID:270063738}
}

@misc{liu2023llm360,
      title={LLM360: Towards Fully Transparent Open-Source LLMs}, 
      author={Zhengzhong Liu and Aurick Qiao and Willie Neiswanger and Hongyi Wang and Bowen Tan and Tianhua Tao and Junbo Li and Yuqi Wang and Suqi Sun and Omkar Pangarkar and Richard Fan and Yi Gu and Victor Miller and Yonghao Zhuang and Guowei He and Haonan Li and Fajri Koto and Liping Tang and Nikhil Ranjan and Zhiqiang Shen and Xuguang Ren and Roberto Iriondo and Cun Mu and Zhiting Hu and Mark Schulze and Preslav Nakov and Tim Baldwin and Eric P. Xing},
      year={2023},
      eprint={2312.06550},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{patson2009lingering,
  title={Lingering misinterpretations in garden-path sentences: evidence from a paraphrasing task.},
  author={Patson, Nikole D and Darowski, Emily S and Moon, Nicole and Ferreira, Fernanda},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={35},
  number={1},
  pages={280},
  year={2009},
  publisher={American Psychological Association}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {{Qwen Team}},
    month = {September},
    year = {2024}
}

@misc{zhang2022optopenpretrainedtransformer,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

@misc{biderman2023pythiasuiteanalyzinglarge,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01373}, 
}

@misc{groeneveld2024olmoacceleratingsciencelanguage,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2402.00838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00838}, 
}

@misc{gemma2,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozińska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Plucińska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin Görner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Cogan and Sarah Perrin and Sébastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}

@misc{itzhak2024instructedbiasinstructiontunedlanguage,
      title={Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias}, 
      author={Itay Itzhak and Gabriel Stanovsky and Nir Rosenfeld and Yonatan Belinkov},
      year={2024},
      eprint={2308.00225},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.00225}, 
}

@inproceedings{2b_tokens_training,
  title={Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens},
  author={Byung-Doh Oh and William Schuler},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258298148}
}

@article{Goldstein2023CorrespondenceBT,
  title={Correspondence between the layered structure of deep language models and temporal structure of natural language processing in the human brain},
  author={Ariel Goldstein and Eric Ham and Samuel A. Nastase and Zaid Zada and Avigail Grinstein-Dabus and Bobbi Aubrey and Mariano Schain and Harshvardhan Gazula and Amir Feder and Werner K. Doyle and Sasha Devore and Patricia Dugan and Daniel Friedman and Michael P. Brenner and Avinatan Hassidim and Orrin Devinsky and Adeen Flinker and Omer Levy and Uri Hasson},
  journal={bioRxiv},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:250534035}
}

@article{ross2024llm_economicus,
  title={LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory},
  author={Ross, Jillian and Kim, Yoon and Lo, Andrew W},
  journal={arXiv preprint arXiv:2408.02784},
  year={2024}
}

@misc{garcia2024moralturingtestevaluating,
      title={The Moral Turing Test: Evaluating Human-LLM Alignment in Moral Decision-Making}, 
      author={Basile Garcia and Crystal Qian and Stefano Palminteri},
      year={2024},
      eprint={2410.07304},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2410.07304}, 
}

@misc{falcon_series,
      title={The Falcon Series of Open Language Models}, 
      author={Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
      year={2023},
      eprint={2311.16867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.16867}, 
}

@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Team Gemini},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530}, 
}

@misc{vicuna2023,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
 title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
 url = {https://lmsys.org/blog/2023-03-30-vicuna/},
 year = {2023}
}

@article{llama2023,
 author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aur'elien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
 journal = {ArXiv preprint},
 title = {LLaMA: Open and Efficient Foundation Language Models},
 url = {https://arxiv.org/abs/2302.13971},
 volume = {abs/2302.13971},
 year = {2023}
}

@misc{llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@article{gpt4,
 author = {OpenAI},
 journal = {ArXiv preprint},
 title = {GPT-4 Technical Report},
 url = {https://arxiv.org/abs/2303.08774},
 volume = {abs/2303.08774},
 year = {2023}
}

@article{garnsey1997contributions,
 author = {Garnsey, Susan M and Pearlmutter, Neal J and Myers, Elizabeth and Lotocky, Melanie A},
 journal = {Journal of memory and language},
 number = {1},
 pages = {58--93},
 publisher = {Elsevier},
 title = {The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences},
 volume = {37},
 year = {1997}
}

@article{ness2019verb,
 author = {Ness, Tal and Meltzer-Asscher, Aya},
 journal = {Language, Cognition and Neuroscience},
 number = {7},
 pages = {936--948},
 publisher = {Taylor \& Francis},
 title = {When is the verb a potential gap site? The influence of filler maintenance on the active search for a gap},
 volume = {34},
 year = {2019}
}

@article{tabor2004evidence,
 author = {Tabor, Whitney and Hutchins, Sean},
 journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
 number = {2},
 pages = {431},
 publisher = {American Psychological Association},
 title = {Evidence for self-organized sentence processing: digging-in effects.},
 volume = {30},
 year = {2004}
}

@article{futrell-etal:2020-dependency-locality,
 author = {Futrell, Richard and Levy, Roger P. and Gibson, Edward},
 journal = {Language},
 number = {2},
 pages = {371–412},
 title = {Dependency Locality as an explanatory principle for word order},
 volume = {96},
 year = {2020}
}

@article{macdonald1994lexical,
 author = {MacDonald, Maryellen C and Pearlmutter, Neal J and Seidenberg, Mark S},
 journal = {Psychological review},
 number = {4},
 pages = {676},
 publisher = {American Psychological Association},
 title = {The lexical nature of syntactic ambiguity resolution.},
 volume = {101},
 year = {1994}
}

@book{apa_dic,
 author = {VandenBos, Gary R},
 publisher = {American Psychological Association},
 title = {APA dictionary of psychology.},
 year = {2007}
}

@article{gibson2000dependency,
 author = {Gibson, Edward},
 journal = {Image, language, brain},
 pages = {95--126},
 title = {The dependency locality theory: A distance-based theory of linguistic complexity},
 volume = {2000},
 year = {2000}
}

@article{levy2008expectation,
 author = {Levy, Roger},
 journal = {Cognition},
 number = {3},
 pages = {1126--1177},
 publisher = {Elsevier},
 title = {Expectation-based syntactic comprehension},
 volume = {106},
 year = {2008}
}

@article{gibson2019efficiency,
 author = {Gibson, Edward and Futrell, Richard and Piantadosi, Steven P and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
 journal = {Trends in cognitive sciences},
 number = {5},
 pages = {389--407},
 publisher = {Elsevier},
 title = {How efficiency shapes human language},
 volume = {23},
 year = {2019}
}

@article{jager2017similarity,
 author = {J{\"a}ger, Lena A and Engelmann, Felix and Vasishth, Shravan},
 journal = {Journal of Memory and Language},
 pages = {316--339},
 publisher = {Elsevier},
 title = {Similarity-based interference in sentence comprehension: Literature review and Bayesian meta-analysis},
 volume = {94},
 year = {2017}
}

@inproceedings{koesterich2021interference,
 author = {Koesterich, Niki and Keshev, Maayan and Shamai, Daria and Meltzer-Asscher, Aya},
 booktitle = {34th annual cuny conference on human sentence processing},
 pages = {38--48},
 title = {Interference in the comprehension of filler-gap and filler-resumptive dependencies},
 year = {2021}
}

@inproceedings{t0,
 author = {Victor Sanh and
Albert Webson and
Colin Raffel and
Stephen H. Bach and
Lintang Sutawika and
Zaid Alyafeai and
Antoine Chaffin and
Arnaud Stiegler and
Arun Raja and
Manan Dey and
M Saiful Bari and
Canwen Xu and
Urmish Thakker and
Shanya Sharma Sharma and
Eliza Szczechla and
Taewoon Kim and
Gunjan Chhablani and
Nihal V. Nayak and
Debajyoti Datta and
Jonathan Chang and
Mike Tian{-}Jian Jiang and
Han Wang and
Matteo Manica and
Sheng Shen and
Zheng Xin Yong and
Harshit Pandey and
Rachel Bawden and
Thomas Wang and
Trishala Neeraj and
Jos Rozen and
Abheesht Sharma and
Andrea Santilli and
Thibault F{\'{e}}vry and
Jason Alan Fries and
Ryan Teehan and
Teven Le Scao and
Stella Biderman and
Leo Gao and
Thomas Wolf and
Alexander M. Rush},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/SanhWRBSACSRDBX22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Tue, 24 Jan 2023 00:00:00 +0100},
 title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
 url = {https://openreview.net/forum?id=9Vrb9D0WI4},
 year = {2022}
}

@article{efrat2022lmentry,
 author = {Efrat, Avia and Honovich, Or and Levy, Omer},
 journal = {ArXiv preprint},
 title = {LMentry: A Language Model Benchmark of Elementary Language Tasks},
 url = {https://arxiv.org/abs/2211.02069},
 volume = {abs/2211.02069},
 year = {2022}
}

@article{van2011cue,
 author = {Van Dyke, Julie A and McElree, Brian},
 journal = {Journal of memory and language},
 number = {3},
 pages = {247--263},
 publisher = {Elsevier},
 title = {Cue-dependent interference in comprehension},
 volume = {65},
 year = {2011}
}

@article{mcelree2000interference,
 author = {McElree, Brian},
 journal = {Journal of psycholinguistic research},
 number = {2},
 pages = {111--123},
 publisher = {Springer},
 title = {Sentence comprehension is mediated by content-addressable memory structures},
 volume = {29},
 year = {2000}
}

@article{sturt1999garden_path,
 author = {Sturt, Patrick and Pickering, Martin J and Crocker, Matthew W},
 journal = {Journal of Memory and Language},
 number = {1},
 pages = {136--150},
 publisher = {Elsevier},
 title = {Structural change and reanalysis difficulty in language comprehension},
 volume = {40},
 year = {1999}
}

@article{christianson2001,
  title={Thematic roles assigned along the garden path linger},
  author={Christianson, Kiel and Hollingworth, Andrew and Halliwell, John F and Ferreira, Fernanda},
  journal={Cognitive psychology},
  volume={42},
  number={4},
  pages={368--407},
  year={2001},
  publisher={Elsevier}
}

@article{Frazier1978TheSM,
  title={The sausage machine: A new two-stage parsing model},
  author={Lyn Frazier and Janet D. Fodor},
  journal={Cognition},
  year={1978},
  volume={6},
  pages={291-325},
  url={https://api.semanticscholar.org/CorpusID:53188302}
}

@inproceedings{Edward1990MemoryGP,
author = {Gibson, Edward},
title = {Memory capacity and sentence processing},
year = {1990},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/981823.981829},
doi = {10.3115/981823.981829},
abstract = {The limited capacity of working memory is intrinsic to human sentence processing, and therefore must be addressed by any theory of human sentence processing. This paper gives a theory of garden-path effects and processing overload that is based on simple assumptions about human short term memory capacity.},
booktitle = {Proceedings of the 28th Annual Meeting on Association for Computational Linguistics},
pages = {39–46},
numpages = {8},
location = {Pittsburgh, Pennsylvania},
series = {ACL '90}
}

@article{Milne1982PredictingGP,
  title={Predicting Garden Path Sentences},
  author={Robert Milne},
  journal={Cogn. Sci.},
  year={1982},
  volume={6},
  pages={349-373},
  url={https://api.semanticscholar.org/CorpusID:62447597}
}

@article{christianson2006,
  title={Misinterpretations of garden-path sentences by older and younger adults},
  author={Christianson, Kiel and Williams, Carrick C and Zacks, Rose T and Ferreira, Fernanda},
  journal={Discourse Processes},
  volume={42},
  pages={205--238},
  year={2006}
}

@article{ferreira2001garden_path,
 author = {Ferreira, Fernanda and Christianson, Kiel and Hollingworth, Andrew},
 journal = {Journal of psycholinguistic research},
 number = {1},
 pages = {3--20},
 publisher = {Springer},
 title = {Misinterpretations of garden-path sentences: Implications for models of sentence processing and reanalysis},
 volume = {30},
 year = {2001}
}

@article{frazier_newer,
 author = {Frazier, Lyn},
 publisher = {Lawrence Erlbaum Associates, Inc},
 title = {Sentence processing: A tutorial review.},
 year = {1987}
}

@phdthesis{old_nlu_frazier,
 author = {Frazier,Lyn},
 isbn = {9798661175977},
 journal = {ProQuest Dissertations and Theses},
 keywords = {Language, literature and linguistics; Linguistics; 0290:Linguistics},
 language = {English},
 note = {Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Dernière mise à jour - 2021-09-09},
 pages = {243},
 title = {On Comprehending Sentences: Syntactic Parsing Strategies},
 url = {https://www.proquest.com/dissertations-theses/on-comprehending-sentences-syntactic-parsing/docview/302925499/se-2?accountid=14765},
 year = {1979}
}

@article{memory-interference,
 author = {Peter C. Gordon and Randall Hendrick and M. K. Johnson},
 journal = {Journal of experimental psychology. Learning, memory, and cognition},
 pages = {
1411-23
},
 title = {Memory interference during language processing.},
 volume = {27 6},
 year = {2001}
}

@article{InstructGPT,
 author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
 journal = {ArXiv preprint},
 title = {Training language models to follow instructions with human feedback},
 url = {https://arxiv.org/abs/2203.02155},
 volume = {abs/2203.02155},
 year = {2022}
}

@misc{flanT5,
 author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
 journal = {ArXiv preprint},
 title = {Scaling Instruction-Finetuned Language Models},
 url = {https://arxiv.org/abs/2210.11416},
 volume = {abs/2210.11416},
 year = {2022}
}

@article{cacheteux-middle-layer,
 author = {Cacheteux, Charlotte and King, Jean-Rémi},
 doi = {10.1038/s42003-022-03036-1},
 journal = {Nature},
 pages = {375-419},
 title = {An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval},
 year = {2022}
}

@article{wilcox-gpt2-abilities,
 author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
 journal = {Algebraic Structures in Natural Language},
 pages = {113},
 publisher = {CRC Press},
 title = {Learning Syntactic Structures from String Input},
 year = {2022}
}

@inproceedings{neural-lms-subjects,
    title = "Neural language models as psycholinguistic subjects: Representations of syntactic state",
    author = "Futrell, Richard  and
      Wilcox, Ethan  and
      Morita, Takashi  and
      Qian, Peng  and
      Ballesteros, Miguel  and
      Levy, Roger",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1004",
    doi = "10.18653/v1/N19-1004",
    pages = "32--42",
    abstract = "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
}

@article{activation-based-model,
 author = {Lewis, Richard and Vasishth, Shravan},
 doi = {10.1207/s15516709cog0000_25},
 journal = {Cognitive science},
 pages = {375-419},
 title = {An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval},
 volume = {29},
 year = {2005}
}

@article{language_generation,
 author = { David I.   Margolin },
 doi = {10.1080/14640748408402172},
 eprint = { 
https://doi.org/10.1080/14640748408402172
},
 journal = {The Quarterly Journal of Experimental Psychology Section A},
 number = {3},
 pages = {459-489},
 publisher = {Routledge},
 title = {The neuropsychology of writing and spelling: Semantic, phonological, motor, and perceptual processes},
 url = { 
https://doi.org/10.1080/14640748408402172
},
 volume = {36},
 year = {1984}
}

@article{NL-Soar,
 author = {Lehman, Jill and Lewis, R. and Newell, A.},
 pages = {},
 title = {Integrating knowledge sources in language comprehension},
 year = {1993}
}

@article{ball-actr,
 author = {Ball, Jerry and Freiman, Mary and Rodgers, Stuart and Myers, Christopher},
 pages = {},
 title = {Toward a Functional Model of Human Language Processing},
 year = {2010}
}

@article{rosie-parser,
 author = {James Kirk and Aaron Mininger and John Laird},
 doi = {https://doi.org/10.1016/j.bica.2016.08.001},
 issn = {2212-683X},
 journal = {Biologically Inspired Cognitive Architectures},
 pages = {1-8},
 title = {Learning task goals interactively with visual demonstrations},
 url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300457},
 volume = {18},
 year = {2016}
}

@article{lucinda-parser,
 author = {De Rycker, Antoon and De Knop, Sabine},
 journal = {Journal of Modern Languages/Jurnal Bahasa Moden},
 pages = {29—49},
 title = {Integrating Cognitive Linguistics and Foreign Language Teaching -Historical Background and New Developments},
 volume = {19},
 year = {2009}
}

@inproceedings{laird-soar,
 author = {John E. Laird},
 title = {The Soar Cognitive Architecture},
 year = {2012}
}

@inproceedings{transformers,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{BERT,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@article{t5,
 author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
 journal = {J. Mach. Learn. Res.},
 pages = {140:1--140:67},
 timestamp = {Fri, 05 Feb 2021 00:00:00 +0100},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
 url = {http://jmlr.org/papers/v21/20-074.html},
 volume = {21},
 year = {2020}
}

@inproceedings{gpt3,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 year = {2020}
}

@article{fedorenko_brain_corr,
 author = {Martin Schrimpf  and Idan Asher Blank  and Greta Tuckute  and Carina Kauf  and Eghbal A. Hosseini  and Nancy Kanwisher  and Joshua B. Tenenbaum  and Evelina Fedorenko },
 doi = {10.1073/pnas.2105646118},
 eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2105646118},
 journal = {Proceedings of the National Academy of Sciences},
 number = {45},
 pages = {e2105646118},
 title = {The neural architecture of language: Integrative modeling converges on predictive processing},
 url = {https://www.pnas.org/doi/abs/10.1073/pnas.2105646118},
 volume = {118},
 year = {2021}
}

@inproceedings{attention-reading-pattern,
 address = {Dublin, Ireland},
 author = {Eberle, Oliver  and
Brandl, Stephanie  and
Pilot, Jonas  and
S{\o}gaard, Anders},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.296},
 pages = {4295--4309},
 publisher = {Association for Computational Linguistics},
 title = {Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},
 url = {https://aclanthology.org/2022.acl-long.296},
 year = {2022}
}

@article{Linzen_sintactic_struct,
 author = {Tal Linzen and Marco Baroni},
 doi = {10.1146/annurev-linguistics-032020-051035},
 journal = {Annual Review of Linguistics},
 number = {1},
 pages = {195--212},
 publisher = {Annual Reviews},
 title = {Syntactic Structure from Deep Learning},
 url = {https://doi.org/10.1146\%2Fannurev-linguistics-032020-051035},
 volume = {7},
 year = {2021}
}

@unpublished{island_learnability,
 author = {Wilcox, Ethan and Futrell, Richard and P. Levy, Roger},
 title = {Using Computational Models to Test Syntactic Learnability},
 year = {2021}
}

@misc{baroni-no-use-nlp,
 author = {Baroni, Marco},
 journal = {ArXiv preprint},
 title = {On the proper role of linguistically-oriented deep net analysis in linguistic theorizing},
 url = {https://arxiv.org/abs/2106.08694},
 volume = {abs/2106.08694},
 year = {2021}
}

@misc{dalle2,
 author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
 journal = {ArXiv preprint},
 title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
 url = {https://arxiv.org/abs/2204.06125},
 volume = {abs/2204.06125},
 year = {2022}
}

@misc{imagen,
 author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
 journal = {ArXiv preprint},
 title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
 url = {https://arxiv.org/abs/2205.11487},
 volume = {abs/2205.11487},
 year = {2022}
}

@misc{flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
 journal = {ArXiv preprint},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 url = {https://arxiv.org/abs/2204.14198},
 volume = {abs/2204.14198},
 year = {2022}
}

@inproceedings{key-value-dict,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Geva, Mor  and
Schuster, Roei  and
Berant, Jonathan  and
Levy, Omer},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.446},
 pages = {5484--5495},
 publisher = {Association for Computational Linguistics},
 title = {Transformer Feed-Forward Layers Are Key-Value Memories},
 url = {https://aclanthology.org/2021.emnlp-main.446},
 year = {2021}
}

@inproceedings{attention-pattern,
 address = {Dublin, Ireland},
 author = {Eberle, Oliver  and
Brandl, Stephanie  and
Pilot, Jonas  and
S{\o}gaard, Anders},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.296},
 pages = {4295--4309},
 publisher = {Association for Computational Linguistics},
 title = {Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},
 url = {https://aclanthology.org/2022.acl-long.296},
 year = {2022}
}

@article{SAP_benchmark,
 author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
 publisher = {PsyArXiv},
 title = {Surprisal does not explain syntactic disambiguation difficulty: evidence from a large-scale benchmark},
 year = {2023}
}

@article{acceptability_nn,
 address = {Cambridge, MA},
 author = {Warstadt, Alex  and
Singh, Amanpreet  and
Bowman, Samuel R.},
 doi = {10.1162/tacl_a_00290},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {625--641},
 publisher = {MIT Press},
 title = {Neural Network Acceptability Judgments},
 url = {https://aclanthology.org/Q19-1040},
 volume = {7},
 year = {2019}
}

@book{Aho:72,
 address = {Englewood Cliffs, NJ},
 author = {Alfred V. Aho and Jeffrey D. Ullman},
 publisher = {Prentice-Hall},
 title = {The Theory of Parsing, Translation and Compiling},
 volume = {1},
 year = {1972}
}

@book{APA:83,
 address = {Washington, DC},
 author = {{American Psychological Association}},
 publisher = {American Psychological Association},
 title = {Publications Manual},
 year = {1983}
}

@article{Chandra:81,
 author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
 doi = {10.1145/322234.322243},
 journal = {Journal of the Association for Computing Machinery},
 number = {1},
 pages = {114--133},
 title = {Alternation},
 volume = {28},
 year = {1981}
}

@inproceedings{andrew2007scalable,
 author = {Andrew, Galen and Gao, Jianfeng},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 pages = {33--40},
 title = {Scalable training of {$L_1$}-regularized log-linear models},
 url = {https://dl.acm.org/doi/abs/10.1145/1273496.1273501},
 year = {2007}
}

@book{Gusfield:97,
 address = {Cambridge, UK},
 author = {Dan Gusfield},
 publisher = {Cambridge University Press},
 title = {Algorithms on Strings, Trees and Sequences},
 url = {https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3},
 year = {1997}
}

@article{rasooli-tetrault-2015,
 author = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
 journal = {ArXiv preprint},
 title = {Yara Parser: {A} Fast and Accurate Dependency Parser},
 url = {https://arxiv.org/abs/1503.06733},
 volume = {abs/1503.06733},
 year = {2015}
}

@article{Ando2005,
 acmid = {1194905},
 author = {Ando, Rie Kubota and Zhang, Tong},
 issn = {1532-4435},
 issue_date = {12/1/2005},
 journal = {Journal of Machine Learning Research},
 numpages = {37},
 pages = {1817--1853},
 publisher = {JMLR.org},
 title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
 url = {https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf},
 volume = {6},
 year = {2005}
}

@article{ct1965,
 author = {Cooley, James W. and Tukey, John W.},
 journal = {Mathematics of Computation},
 number = {90},
 pages = {297--301},
 title = {An algorithm for the machine calculation of complex {F}ourier series},
 url = {https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf},
 volume = {19},
 year = {1965}
}