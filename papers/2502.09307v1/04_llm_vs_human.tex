\section{Analzying LLMs vs. human performance}

In comparing LLMs and humans, we focus on the following important (albeit less-studied) aspect: the extent to which the relative difficulty of tasks in our experiment is similar between LLMs and humans. Showing that LLMs and humans have similar processing difficulties can open interesting research directions on whether LLMs can inform psycholinguistic models of human sentence processing \cite{kuribayashi2025large}.

To evaluate the similarity between LLMs and humans, we use the Kendall Tau rank correlation metric.\footnote{\url{https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient}. Our human data has a lot of ranked ties, and Kendall Tau correlation accounts for ranked ties contrary to Spearman rank correlation.} We calculate a \emph{``Global''} Kendall Tau correlation by looking at all items in our data (from all conditions) and comparing the average accuracy of humans on those items to the average probability of the correct answer as provided by the LLM, 
This measures whether the difficulty ranking of all the items on our experiment is similar for humans and LLMs. Figure \ref{fig:kendall_tau_global} presents the findings for each model family.

\paragraph{Model size:} As illustrated in Figure \ref{fig:kendall_tau_global}, larger models exhibit a higher Kendall Tau correlation with human judgements across all model families.

\begin{figure}[t!]
    \centering
    \scriptsize
    \includegraphics[height=8cm]{figures/per_category_spearman.pdf}
    \caption{Spearman rank correlation per model family}
    \label{fig:spearman_per_category}
\end{figure}

\begin{figure*}[t!]
    \centering
    \scriptsize
    \includegraphics[width=18cm]{figures/combined_paraphrase_image.pdf}
    \caption{Average paraphrase accuracy for each condition per model family}
    \label{fig:paraphrase_acc}
\end{figure*}

\paragraph{Instruction tuning:} Instruction tuning appears to have little impact on the similarity between humans and LLMs in this measure.

\paragraph{Pretraining tokens:} In OLMo-7B, we see an increase in Kendall Tau correlation with the increase in number of pretraining tokens. This pattern is not observed in OLMo-1B, possibly because the model is too weak to show significant effects. 

We now move to analyzing the by-condition correlation between humans and LLMs.
Figure \ref{fig:spearman_per_category} shows the Spearman rank correlation between humans and LLMs, comparing the \emph{average accuracy} across the 6 conditions (correlating two vectors in $\mathbb{R}^6$). All models show a high Spearman rank correlation with human data. This suggests that models align well with humans in ranking average accuracy by sentence type, as opposed to a global ranking across all items. Notably, model size minimally impacts Spearman rank correlation but significantly affects Kendall Tau, indicating that larger models better differentiate item difficulty within conditions. In OLMo-7b, as training progresses, condition difficulty distinctions increasingly resemble humans'.


%Figure \ref{fig:spearman_per_category} illustrates the Spearman rank correlation between humans and LLMs, comparing the average accuracy in each of the 6 conditions (i.e., correlating vectors in $\mathbb{R}^6$). All models exhibit a high Spearman rank correlation with human data (higher than their Kendall Tau correlation). This indicates that models are quite similar to humans in the ranking of average accuracy per category compared to a global ranking of all items. Additionally, since model size has a smaller impact on the Spearman rank correlation but strongly affects the Kendall Tau correlation, larger models improvement on Kendall Tau seems to stem from more nuanced distinctions regarding the difficulty of items within a category. OLMo-7b clearly shows that, as training progresses, the distinction between the difficulty of the categories becomes more similar to humans.

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=7.0cm, height=5cm, ]{figures/paraphrase_auc.pdf}
%     \caption{AUC of the models from different families when predicting paraphrase accuracy from reading comprehension output distribution. All models have predictive power (AUC well above 0.5).}
%     \label{fig:paraphrase_auc}
% \end{figure}


