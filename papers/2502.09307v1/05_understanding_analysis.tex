\section{Performance on Paraphrasing and Text-to-Image Generation}

\begin{figure*}[t]
    \centering
    \scriptsize
    \subfloat[\centering The yacht headed toward a waterfall while the explorer paddled.]{{\includegraphics[width=3cm]{figures/correctly_understood.png}}}%
    \qquad
    \subfloat[\centering The rainbow slowly faded outside while the girl painted.]{{\includegraphics[width=3cm]{figures/partial_misunderstanding.png}}}
    \qquad
    \subfloat[\centering While the teacher counted the children formed a line.]{{\includegraphics[width=3cm]{figures/complete_misunderstanding.png} }}
    \qquad
    \subfloat[\centering The wheel made weird noises while the mechanic turned.]{{\includegraphics[width=3cm]{figures/not_applicable.png} }}
    \caption{Example image for each of our manually-assigned labels for text-to-image generation examples. From left to right: correctly understood, partial misunderstanding, complete misunderstanding and not applicable.}
    \label{fig:dalle_classes}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=7.0cm, height=5cm, trim={0.4cm 0.2cm 1.6cm 2.5cm}, clip]{figures/drawings_understood.pdf}
    \caption{Proportion of images classified as correctly understood and partial understanding for our experimental conditions}
    \label{fig:drawing_classes}
    \vspace{-0.3cm}
\end{figure}

In addition to answering comprehension questions, 
we now test LLM understanding of GP sentences on two additional tasks -- paraphraing and text-to-image generation. Due to cost limitations, we run this experiment on LLMs only. \citet{patson2009lingering} performed a paraphrasing experiment with humans using materials from \citet{christianson2001} and found that paraphrases showed the same misinterpretations that comprehension questions did. 


\subsection{Paraphrasing}

We asked LLMs (excluding OLMo-1B and OLMo-7B) to paraphrase a sentence by splitting it into two parts. The correct answer would be to change \emph{``While the man hunted the deer ran into the woods.''} into \emph{``The man hunted. The deer ran into the woods.''}. Using a few-shot prompt, we provided examples of sentences and their breakdowns. The complete prompt is in Appendix \ref{sec:paraphrase_prompt}.

We automatically evaluated the paraphrases using three metrics: a format metric (verifying that the paraphrase consistes of two sentences), a \emph{found-verb} metric (ensuring that the verb \emph{``hunted''} appears in just one sentence), and a \emph{correct-paraphrase} metric (checking that the noun ``deer'' does not appear in the same sentence as ``hunted''). The first two metrics check if the model correctly executed the paraphrasing task, while the last evaluates sentence comprehension. Models that scored below 90\% accuracy in the format and found-verb metrics (Qwen2.5-1.5B, Qwen2.5-3B) were filtered out, and for the remaining  models we measured performance with the correct-paraphrase metric.

%We automatically check the correctness of paraphrases using three metrics: a format metric (checking if the paraphrase is split into two sentences), a ``found verb'' metric (ensuring the verb \emph{``hunted''} appears in one of the paraphrased sentences), and a ``correct paraphrase'' metric (confirming that the noun ``deer'' does not appear in the same sentence as the verb ``hunted'' in the paraphrase). The first two metrics assess whether the model understood and executed the paraphrasing task correctly, while the last one evaluates the model's comprehension of the sentence. We filtered out models that did not achieve at least 90\% accuracy in both the format and the ``found verb'' metrics (Qwen2.5-1.5B, Qwen2.5-3B), and measured performance on the correct paraphrase metric. 

Figure \ref{fig:paraphrase_acc} shows the average accuracy across for our model families, categorized by sentence type. We observe that GP sentences are universally more challenging to paraphrase for all models and sentence types. For almost all LLMs, the accuracy on the non-GP sentences is above 90\% across all the manipulations. Moreover, among the GP sentences, implausible ones have the highest paraphrase accuracy across models. Sentences like \emph{``The man hunted the child.''} seem to be too out of distribution for our LLMs to generate.

    We also performed an item-level analysis, where we checked whether the probability assigned to the correct answer for the GP question predicts correct paraphrasing. We measure the AUC for the probability assigned to the correct answer, and find that all our models have an AUC over 0.5, with the minimal AUC being 0.595, the maximal 0.774 and the average 0.696. This suggests that the probability of correctly answering the GP question can strongly predict whether paraphrasing will be accurate. Instruction tuning and model sizes had little effect on the AUC. 

\subsection{Image Generation}

We asked Dall-e-3 \cite{dalle_3}\footnote{We tried 3 other text-to-image models to generate images but the results were not good enough to draw any conclusion.} to generate images for our experimental sentences and manually categorized the resulting images into four groups: \emph{``correctly understood''}, where the image accurately depicts the intended meaning of the sentence, e.g., in ``while the man hunted the deer ran into the woods'', the image would depict a man hunting, and a deer running; \emph{``partial misunderstanding''}, where the image reflects the misinterpretation suggested by the garden path, namely, in the above example, a man hunting a deer, and a deer running; \emph{``complete misunderstanding''}, where the interpretation of the main clause is absent, namely for the example above, the image would not depict a deer running; and \emph{``not applicable''}, where some elements of the sentence are missing, e.g. in the example above there would either be no man, or no deer. Figure \ref{fig:dalle_classes} provides examples for each category. 

Figure \ref{fig:drawing_classes} shows the proportion of the correctly understood and partially understood classes. 
Looking at the partial-misunderstanding label, which corresponds to the type of misinterpretation we have been investigating, we see trends that are similar to the  comprehension questions case. The proportion of examples with this label is lower for non-GP than for GP sentences and for sentences with implausible compared to plausible nouns. In addition, the gap between GP and non-GP sentences is larger for the reflexive/unaccusative case. This further supports the empirical results we saw for comprehension questions and paraphrasing.
