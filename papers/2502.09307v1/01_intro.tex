\section{Introduction}

Humans are extremely skilled language comprehenders, correctly and effortlessly understanding the vast majority of sentences we meet. However, some syntactic structures pose challenges for accurate parsing. We tend to read these sentences more slowly and comprehend them less accurately. One such structure is the \emph{garden-path} sentence. These sentences are temporarily ambiguous, leading readers to misconstrue their meaning initially. Psycholinguistic research has shown that when asked reading comprehension questions about these sentences, humans tend to answer incorrectly. \emph{Cite Fereira} indicates that for garden-path sentences like \emph{While the man hunted the deer ran into the woods}, individuals overwhelmingly respond "Yes" to the question "Did the man hunt the deer?"

Large Language Models (LLMs), particularly transformer-based ones, serve as another example of proficient language comprehenders. These State-of-the-Art (SOTA) models have demonstrated performance that surpasses human capabilities across a range of tasks that demand diverse competencies, including reasoning, memory, and common sense understanding. Their ability to parse complex linguistic information and generate coherent, contextually appropriate responses highlights the advanced nature of these models in the domain of natural language processing.

Numerous studies have explored the capability of transformer-based Large Language Models (LLMs) to mimic human behavior in cognitive experiments across various domains, such as ethical decision-making and behavioral economics \emph{cite papers}. On a more granular level, several papers have spotlighted parallels in the way humans and LLMs process sentences. Some papers have compared the internal states of LLMs with neural activations in the human brain, uncovering significant correlations \emph{cite papers}. Other studies have examined the behavior of LLMs and humans when processing syntactic sentences, investigating whether both entities acquire similar syntactic structures \emph{cite paper}. With all these findings, one might be tempted to claim LLMs can serve as models of humans cognition.

We argue that showing that LLMs can overall mimic human behavior is not enough to make such claims. We claim that it is important as well to see if similar situations are challenging for both humans and LLMs. In the context of language processing, it means seeing if the same structures are challenging for accurate parsing for humans and LLMs.

Despite the growing interest in comparing humans and LLMs, there has been limited research specifically examining their similarities in reading comprehension tasks. Notably, the question of whether the same syntactic structures are complex for both humans and LLMs during reading comprehension remains under-explored. In typical reading comprehension tasks, participants—be they humans or LLMs—are presented with a sentence and subsequently asked to answer questions about it. \emph{Cite paper} replicated the experiments from \emph{cite paper} and discovered that LLMs also encounter difficulties with the specific type of garden-path sentences examined in the original study. Nonetheless, no thorough analysis has been conducted to determine what makes these sentences difficult for humans and if the same challenges apply to LLMs.

In this paper, we conduct an in-depth analysis of the sentence types discussed in \emph{cite Fereira}. Initially, we propose hypotheses regarding what makes these sentences challenging for human comprehension. We formulated four hypotheses and conducted experiments with human participants to test these theories. Our findings revealed that these sentences are difficult for several reasons, with three out of the four hypotheses being supported by the evidence. Subsequently, we applied the same experimental framework to LLMs to assess the similarity in behaviour between LLMs and humans in these tasks. Our results indicate that numerous LLMs exhibit significant behavioral similarities to humans in these experiments. Furthermore, we attempted to indirectly investigate what LLMs comprehend from these sentences, examining whether this understanding aligns with the errors LLMs make. We discovered that ...
