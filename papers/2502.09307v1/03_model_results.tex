\section{LLMs Performance}


We now analyze the performance of LLMs on our different experimental conditions. 

\subsection{Methodology}

To replicate the experiment with LLMs, we used few-shot prompting, where each example includes a sentence, a question, and the correct answer.\footnote{We also experimented with chain-of-thought-prompting, which did not lead to a signifnicant change in results.} The examples did not contain GP structures. Each model was prompted 8 times, using two system prompts and four example orderings. We extract the probabilities of the correct and incorrect answers tokens, averaging these across the 8 prompts. Appendix \ref{sec:prompt_example} shows an example prompt.

%To replicate the experiment with LLMs, we prompt models using few-shot prompts, where each of the provided examples consists of a sentence, a question and the correct answer.\footnote{We also tested vanilla Chain-of-Thoughts without significant changes in the results} None of the examples provided are GP sentences. We prompt each model 8 times by using two different system prompts and four different orderings of the examples in the prompt. We look at the probabilities of the tokens of the correct and incorrect answer, and average the probabilities across the 8 different prompts. Appendix \ref{sec:prompt_example} shows an example of one of the 8 prompts.

\paragraph{Models}

We test models from different families, sizes and training checkpoints:
\begin{enumerate}[nosep]
    \item GPT family \cite{gpt4}: \emph{GPT-4}, \emph{GPT-4-Turbo}, \emph{GPT-4o}, \emph{GPT-4o-mini}, \emph{o1-preview}, \emph{o1-mini}.
    \item Llama-3 \cite{llama3herdmodels}: All models from the Llama-3 family (Llama-3.2 and Llama-3.1) available on HuggingFace.\footnote{\url{https://huggingface.co/models}}
    \item Qwen-2.5 \cite{qwen2, qwen2.5}: All Qwen-2.5 models on HuggingFace except models of size 0.5b.
    \item Gemma-2 \cite{gemma2}: All Gemma-2 models on HuggingFace.
    \item Olmo \cite{groeneveld2024olmoacceleratingsciencelanguage}: 15 Olmo-1b and Olmo-7b checkpoints along training.
\end{enumerate}

\subsection{Results}

We first present the overall results of LLMs on our task.  Figure \ref{fig:model_global_perf} shows the results for 6 selected models from each family. Appendix \ref{sec:all_model_res} presents the results for all models.

At a high-level it is clear that the behavior of LLMs resembles that of humans: accuracy on non-GP sentences is higher than accuracy on GP sentences, accuracy for both GP and non-GP sentences is higher when the direct object is implausible, and the gap between GP and non-GP sentences is larger in the reflexive/unaccusative case. These trends seem more pronounced for larger and stronger models (the top two rows) compared to smaller models (Olmo-1b).


Interestingly, LLM performance is not perfect even for the strongest model, \emph{o1-preview}, which obtains an average accuracy of 78\% (the second-strongest model, \emph{Gemma-27B} has an average accuracy of 74\%). 
This far-from-perfect performance of LLMs is perhaps surprising since the entire sentence and question are presented in full to the LLMs and there is no reason to  suspect that they should suffer from the same processing difficulties that humans do, especially those related to the inability to overcome the initial misparse.



% (jb): where do I see it? why is it important?
%Note that \emph{Gemma-27B} is the only model that has a higher accuracy on GP questions (74\%) than on simple questions (58\%).

\begin{figure}[t!]
    \centering
    \scriptsize
    \includegraphics[height=8cm]{figures/per_family_tau.pdf}
    \caption{Gloabal Kendall Tau rank correlation per model family.}
    \label{fig:kendall_tau_global}
\end{figure}

%JB: this is really boring -- is it important?
%For hypothesis \ref{num:hyp_2b}, we examined the correlation between a verb's transitive bias and the probability assigned to the correct answer on all our manipulations. None of the models exhibited a statistically significant correlation, with all correlations being weak ($<$0.3). Overall, we found no  pattern linking LLM accuracy to the transitive bias of the verb. 

