\section{Introduction}
Large Language Models (LLMs) have shown high proficiency in language comprehension and generation, demonstrating performance that matches and sometimes surpasses human capabilities across a range of tasks \cite{gpt4, llama2023, falcon40b, llama3herdmodels, geminiteam2024gemini}. This has sparked a line of research focused on comparing sentence-processing mechanisms in humans and LLMs. Within this research, some studies have found correlations between LLM activations and brain activations during the processing of identical sentences \cite{cacheteux-middle-layer, fedorenko_brain_corr, Ren2024DoLL}. Others have demonstrated that LLMs can be used to predict human linguistic behavior \cite{linzen-etal-2016-assessing, acceptability_nn, hu-etal-2020-systematic, Rego2024LanguageMO, Sun2024ComputationalSM, kuribayashi2025large}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=8.0cm, height=8cm, trim={0.8cm 0.5cm 0cm 0.4cm},clip]{figures/figure_1_new.pdf} 
    \caption{\emph{Top}: The manipulations made to an example garden-path sentence along with predictions from humans and LLMs for these sentences. \emph{Bottom}: human and the Gemma-2-9B average performance on the different experimental conditions. The behaviour of humans and Gemma-2-9B is similar.}
    \label{fig:human_next_llm}
\end{figure}

\begin{figure*}[t]
    \centering
    \scriptsize
    \subfloat[\centering While the boy washed the dog barked loudly.]{{\includegraphics[width=6cm, height=5cm]{figures/sent_175.png}}}%
    \qquad
    \subfloat[\centering The dog barked loudly while the boy washed.]{{\includegraphics[width=6cm, height=5cm]{figures/sent_151.png}}}
    \caption{Dall-e-3 incorrectly generates an image where the boy washes the dog (Left) given a GP sentence, but generates a correct image with a non-GP sentence (Right).}
    \label{fig:dalle_fig1}
\end{figure*}

While LLMs mostly succeed where humans succeed, less is known on whether LLMs fail where humans fail. A classic case in psycholinguistic research for sentences where humans systematically have comprehension difficulty is Garden Path (GP) structures \cite{gp3, gp2, gp1}. GP sentences are temporarily ambiguous, as their beginning leads readers to misconstrue their parse. Consider for example (\ref{num:ex_gp})-(\ref{num:ex_ngp}). In (\ref{num:ex_gp}), readers initially misanalyze ``the dog'' as the object of ``washed'', although in the final, correct structure, ``the dog'' is the subject of ``barked'', and ``washed'' has no object. 
\begin{enumerate}[nosep]
    \item \label{num:ex_gp} While the boy washed the dog barked loudly. 
    \item \label{num:ex_ngp} The dog barked loudly while the boy washed.
\end{enumerate}
\citet{christianson2001} used comprehension questions and found that comprehension is poorer for GP sentences like (\ref{num:ex_gp}) compared to non-GP sentences like (\ref{num:ex_ngp}). Specifically, the initial interpretation, where the post-verbal NP is analyzed as the object of the first verb, lingers, while the NP is also analyzed as the subject of the following verb (see also \citet{christianson2006}). \citet{patson2009lingering} similarly showed that when participants were tasked with paraphrasing GP sentences,  they often gave paraphrases such as ``The boy washed the dog and the dog barked'' for (\ref{num:ex_gp}). 

Some prior studies examined how LLMs handle GP sentences. \citet{gp_reading_time} found that LLM-based metrics significantly underestimate the processing difficulties humans face with GP sentences. \citet{hanna2024incrementalsentenceprocessingmechanisms} found that LMs encode multiple parses of GP sentences, which does not straightforwardly align with human performance. \citet{bert_gp} reported that BERT misinterprets GP sentences, although with different error patterns than humans. In contrast, \citet{Li2024IncrementalCO} observed that four LLMs make parsing errors akin to human errors. Despite these findings, gaps remain in existing research. First, it remains unclear whether the same aspects of GP structures cause processing difficulties in humans and LLMs. Second, the breadth of LLM families considered has been limited thus far.
Lastly, the comparison between humans and LLMs has been mostly carried out through indirect measures, where e.g. human reading times are correlated with LLM uncertainty \cite{wilcox-gpt2-abilities, Rego2024LanguageMO, Sun2024ComputationalSM}, whereas a comparison on the same task could be more revealing. 



%Some prior studies examined how LLMs handle GP sentences. \citet{gp_reading_time} concluded that an LLM-based metric vastly underestimates human processing difficulties with GP sentences, indicating that LLMs do not struggle with GP sentences. Other studies looking at sentence comprehension gave different albeit mixed results. \citet{bert_gp} showed that BERT misinterprets GP sentences, similar to humans, but that its error patterns differ. On the other hand, \citet{Li2024IncrementalCO} found that four LLMs make similar parsing errors to humans with GP sentences. \citet{hanna2024incrementalsentenceprocessingmechanisms} showed that LMs encode multiple interpretations of GP sentences. Nevertheless, some gaps still remain. First, past work does not shed light on what aspects of GP structures lead to processing difficulties and whether these differ between humans and LLMs. Second, LLMs and humans were either compared indirectly, assuming that human reading time corresponds to LLM uncertainty (as evidenced by its output distribution), or on a single comprehension metric (sentence comprehension). Last, only a limited set of LLMs were examined.


\def\thickhline{\noalign{\hrule height.8pt}}
\newcolumntype{?}{!{\vrule width .8pt}}

In this study, we explore object/subject GP sentences (similar to (\ref{num:ex_gp})) with humans and LLMs, with both responding to exactly the same task, namely, a comprehension question about the sentence (e.g., ``Did the boy wash the dog?'').
We present three (non-mutually exclusive) hypotheses regarding the challenges posed by GP sentences: (a) Misinterpretation of the noun phrase as the object of the verb arises due to the difficulty of syntactic reanalysis; (b) Misinterpretation arises since the noun phrase is a plausible object for the verb (e.g., boys tend to wash dogs); (c) Misinterpretation arises since transitive verbs (\emph{``hunt''}) entail some object. Reflexive (\emph{``wash''}) or unaccusative (\emph{``drop''}) verbs, which are interpreted with no direct object, will give rise to less misinterpretation.


%In this study, we conduct an in-depth investigation of object/subject GP sentences, with the syntactic structure shown in example \ref{num:ex_gp}, examining both humans and a large suite of LLMs.
%Unlike most past work, which indirectly compared humans and LLMs by correlating human reading time to LLM surprisal \cite{wilcox-gpt2-abilities, Rego2024LanguageMO, Sun2024ComputationalSM}, we ask both humans and LLMs to perform an \emph{identical} task -- answering a reading comprehension question given a GP construction (\emph{``Did the boy wash the dog?''}).

\begin{table*}[t!]
    \scriptsize 
    \centering
        \begin{tabular}{ l ? l}
         \textbf{Hypothesis} & \textbf{Manipulation} \\
        \thickhline
        GP syntax is hard& While the man hunted the deer ran into the woods. $\rightarrow$ The deer ran into the woods while the man hunted. \\
        \hline
        Plausible direct object& While the man hunted the deer ran into the woods. $\rightarrow$ While the man hunted the child ran into the woods. \\
        \hline
        Transitive vs. reflexive/unaccusative& While the man hunted the deer ran into the woods. $\rightarrow$ While the boy washed the dog barked loudly. \\
        \end{tabular}
        \vspace{-0.2cm}
    \caption{We manipulate the GP structures examined to test three hypotheses for what makes GP sentences hard. }
    \label{tab:changes}
\end{table*}



%We propose three (non-mutually exclusive) hypotheses for the challenges these GP sentences pose to humans, and manipulate the sentences to test those hypotheses (see the manipulations in Table~\ref{tab:changes}): (a) GP syntax is inherently difficult due to the reanalysis required for accurate parsing; (b) Readers assume an argument is a direct object of a verb, if it is semantically plausible (e.g., boys often wash dogs); (c) Optionally transitive verbs (\emph{``pull''}) are interpreted as taking an object but  reflexive (\emph{``wash''}) or unaccusative (\emph{``drop''}) verbs are not, making reanalysis easier in the latter case.

We test our hypotheses on sets of sentences instantiating the various manipulations (see Table~\ref{tab:changes}) and tested comprehension in humans and LLMs.

%To test our hypotheses, we conduct a human study, where we show participants questions on GP sentences and all the aforementioned manipulations. Unlike earlier research, each participant is presented with only one sentence and question, preventing them from adapting to the task. We then apply the exact same procedure to LLMs and compare their results to humans.

%Our human study results  (Fig. \ref{fig:human_next_llm}) reveal that human accuracy drops for GP versus non-GP sentences and further declines when the post-verbal noun is a plausible direct object. The decline is more pronounced with reflexive/unaccusative verbs in GP structures.

Our human results (Fig. \ref{fig:human_next_llm}, left) provide evidence for all three hypotheses. Accuracy is lower (i) when reanalysis is needed (i.e. in GP compared to non-GP structures), (ii) when the noun is a plausible (compared to implausible) object for the verb, and (iii) when the verb is transitive, entailing an object, compared to reflexive/unaccusative.  The second, semantic effect is stronger than the syntactic and verb type effects. 

Interestingly, we find that LLMs struggle with comprehension of GP sentences, even in the most capable models (o1-preview accuracy is highest at 78\%). In addition, for many LLMs, the manipulations have similar effects to those they have on humans. Examining multiple models within the same families reveals that stronger models tend to display greater similarity to human behavior, as shown in Figure \ref{fig:human_next_llm} (right) for Gemma-2-9B.

%The similarity between humans and LLMs, in terms of both absolute performance and the impact of the manipulations, varies across different LLMs. By examining multiple models within the same families, we observe that model size affects this similarity, with the similarity to humans rising with model size and with number of tokens the LLM was pretrained on. Figure \ref{fig:human_next_llm} shows the performance of Gemma-9B, a model with high similarity to humans across our experiment conditions.

Finally, we validate our results for LLMs with two additional sentence comprehension tasks -- paraphrasing and image generation. Both tasks produced results akin to the comprehension questions task. Figure~\ref{fig:dalle_fig1} shows Dall-e-3's \cite{dalle_3} inability to correctly parse the GP sentence (depicting the boy washing the dog instead of himself), unlike in the non-GP sentence.

%To further validate our results, we test LLMs on two more sentence understanding tasks -- paraphrasing the GP sentence, and generating an image from it. Our results on these tasks are similar to the reading comprehension task. Figure~ \ref{fig:dalle_fig1} shows that Dall-e-3 \cite{dalle_3} does not parse the GP sentence correctly (concluding that the boy washes the dog rather than himself), in contrast to the non-GP sentence.

To summarize, our contributions are:
\begin{enumerate}[nosep]
    \item We put forward precise hypotheses for the challenges that GP sentences pose and construct linguistic materials to test them.
    \item We collect human and LLM data on a \emph{sentence comprehension} task to test these hypotheses .
    \item We compare LLM performance to humans, finding that stronger models are more similar to humans. 
    \item We validate our results on a paraphrasing and text-to-image task.
\end{enumerate}
