\section{Human performance}

We first run an experiment on human participants to test our hypotheses.

\subsection{Methods}

\paragraph{Materials} 
In 45 sentences sets with optionally transitive verbs (24 coming from \citet{christianson2001}, 21 crafted for this study), we manipulated the structure of the sentence (GP or non-GP) and plausibility of the noun as the verb's object (plausible or implausible), as exemplified in (\ref{num:ex_4a}). We also created 24 additional sets with reflexive/unaccusative verbs (12 from \citet{christianson2001} and 12 crafted for this study) in plausible sentences, manipulating structure (GP/non-GP), as exemplified in (\ref{num:ex_5a}). 

%To test the hypotheses, we created six conditions in a 2x3 design. Each sentence syntactic structure is either (a) a GP, or (b) a non-GP. In addition, each sentence uses (a) a noun that is a plausible direct object to the transitive verb (45 sets), (b) a noun that is an implausible direct object, (45 sets based on the sets from (a)) or (c) a reflexive/unaccusative verb (with a plausible object) (24 sets). Here are examples of our conditions:
\begin{enumerate}[nosep]
    \setcounter{enumi}{3}
    \small \item \label{num:ex_4a} \begin{enumerate}[nosep]
        \item GP, plausible: While the man hunted the deer ran into the woods.
        \item Non-GP, plausible: The deer ran into the woods while the man hunted. 
        \item GP, implausible: While the man hunted the child ran into the woods.
        \item Non-GP, implausible: The child ran into the woods while the man hunted.
        \end{enumerate}
        \item \label{num:ex_5a} \begin{enumerate}[nosep]
            \item  GP, reflexive: While the boy washed the dog barked loudly.
            \item Non-GP, reflexive: The dog barked loudly while the boy washed.
        \end{enumerate}
    \end{enumerate}

To construct materials for the plausibility manipulation  (hypothesis 2), we use
insights from \citet{amouyal-etal-2024-large}, and let GPT4 rate sentence plausibility on a 1 to 7 scale. We select pairs where the plausible sentence had a rating of at least 3 points higher than its implausible counterpart. For the second part of hypothesis 3, we assessed each optionally-transitive verb's bias by its proportion of transitive usages on Wikipedia. Our verbs' bias ranges from 0.102 (``sail'') to 0.775 (``explore''). Appendix \ref{sec:verb_factors} lists the full estimated verb biases, and all the sentences are in Appendix \ref{sec:appendix_sents}.

%or hypothesis \ref{num:hyp_2a}, we used insights from \citet{amouyal-etal-2024-large} and used GPT4 to give a plausibility rating to our different sentences on a scale from 1 to 7. We kept only sets in which the rating for the plausible sentence was larger by at least 3 points from the implausible counterpart. For hypothesis \ref{num:hyp_2b}, we assigned each optionally transitive verb from our experiment materials a transitivity bias, determined by examining the frequency with which the verb appeared in a transitive form across Wikipedia. Our verbs' transitivity biases range from 0.102 with \emph{``sail''} to 0.775 with \emph{``explore''}. A full list of the verbs and their transitivity factors can be found in Appendix \ref{sec:verb_factors}.
%The full list of sentences can be found in Appendix \ref{sec:appendix_sents}.

For each sentence, we ask one of two questions:
\begin{enumerate}[nosep]
    \item \emph{Simple}: \emph{``Did the deer run into the woods?''}
    \item \emph{GP}: \emph{``Did the man hunt the deer?''}
\end{enumerate}
The simple question probes basic understanding of the sentence, whereas the GP question targets the potential misinterpretation. 

\begin{figure*}[t!]
    \centering
    \scriptsize
    \includegraphics[width=17cm]{figures/combined_image.pdf}
    \caption{Performance of models from all families on our experimental conditions. Models with an ``-Inst'' suffix are instruction-tuned. Note: Each figure has a different y-axis range.}
    \label{fig:model_global_perf}
\end{figure*}

\paragraph{Procedure} 

Native English speakers were recruited via the Prolific platform.\footnote{\url{https://www.prolific.com/}}  Sentences were displayed word-by-word, with each word shown for 400ms and a 100ms blank screen between words. After the sentence, the comprehension question was presented for 5 seconds. If unanswered within this time, the response was marked as incorrect. Participants completed two practice items, followed by one experimental sentence and one question. The single-trial design prevents fatigue \cite{christianson2022if} and learning effects \cite{fine2013rapid}. Each of the 456 sentence-question pair was shown to 10 participants. The average completion time was 1:50 minutes, and participants were compensated with 0.30£, equivalent to 9.64£ per hour. 
%JB: deanonymizes submission
The experiment was approved by the Ethics Committee at Tel-Aviv University. 

%Participants (native English speakers only) were recruited through the Prolific platform,\footnote{\url{https://www.prolific.com/}}. Each participant saw only one sentence and one question, contrary to common practice in psycholinguistics research. This helps prevent participant fatigue \cite{christianson2022if} or participant learning the syntactic structure in our experiment \cite{fine2013rapid}. Each pairing of sentence and question was presented to 10 participants, for a total of 4,560 data points. The average time participants took to finish the experiment was 1:50 minutes.  Participants received 0.30£ for their participation, the equivalent of 9.64£ per hour. Participants were shown the sentence word-by-word, with each word appearing for 100 ms and between each word a blank screen for 400 ms. At the end of the sentence, the question was presented to the participants for 5 seconds. If a participant did not answer in the given time, its answer was considered incorrect.

\subsection{Human results}

The accuracy on simple questions was high (average 95.4\%, minimum 92.4\%, maximum 98.7\%).
Conversely, GP questions were much more challenging with an average accuracy of 37.0\% on GP questions.

Figure \ref{fig:human_next_llm} (left) shows the average accuracy for humans in the various conditions. As expected from Hypothesis 1, accuracy is consistently higher for non-GP structures. In addition, accuracy is lower when the noun is a plausible direct object, indicating a tendency to interpret it as such even without syntactic indication, supporting Hypothesis 2. The plausibility effect was more pronounced than the syntactic effect. In addition, accuracy was higher for GP sentences with a reflexive/unaccusative verb than for those with an optionally transitive verb, supporting Hypothesis 3, and the effect of structure (GP vs. non-GP was stronger for the former verbs). 

%The leftmost part of Figure \ref{fig:human_next_llm} shows the average accuracy under the different conditions for humans. In all sentence types, accuracy is higher for the non-GP compared to the GP structure, as expected according to Hypothesis 1. In addition, when the noun is a plausible direct object, performance is especially low, showing that humans interpret the noun as a direct object even when the syntax does not guarantee this. When the noun is an implausible direct object, accuracy is much higher compared to plausible direct objects, as predicted by Hypothesis 2. The plausibility effect is larger than the syntactic structure effect. Last, for reflexive/unaccusative verbs, we see the largest gap between GP and non-GP structures, suggesting that when the verb can be fully interpreted without an additional noun, misinterpretation arises mostly due to the GP structure.

We test statistical significance with Generalized Linear Mixed-Effects Models (see Appendix \ref{sec:glmm_app}). For Hypothesis 1, the difference between GP and non-GP sentences was significant for implausible ($p$ = .019) and reflexive ($p$ = 2.52e-13) sets, and approached significance in the plausible sets, $p$ = .065. Hypothesis 2's prediction was confirmed with a significant difference between plausible and implausible sentences ($p$ = 4.11e-16). Hypothesis 3 is also supported with a significant difference between reflexive/unaccusative verbs for both GP sentences ($p$ = 1.35e-5) and non-GP sentences ($p$ = 2.45e-14). The Pearson correlation between transitivity bias and accuracy was weak ($\leq$0.19) across conditions. 

%We performed multiple Generalized Linear Mixed-Effects Models (see Appendix \ref{sec:glmm_app}) to test the statistical significance of the result. Regarding hypothesis \ref{num:hyp_1}, the difference between GP and non-GP sentences was significant for both the implausible ($p$=0.019) and reflexive ($p$=2.52e-13) sets. For the plausible set, $p$=0.065. For hypothesis \ref{num:hyp_2a}, the statistical test showed that the difference between plausible and implausible was significant ($p$=4.11e-16). The Pearson correlation between the transitivity bias and the proportion of correct answers turned out weak ($\leq$0.19) for all our manipulations (GP and non-GP, plausible and implausible sentences).  Hypothesis \ref{num:hyp_3} is supported by the fact that the p-value of the logistic regressions on the reflexive sets was orders of magnitude lower than the p-value of the implausible sets.

Overall, the hypotheses were supported by the human results, showing that multiple factors influence the difficulty of object/subject GP sentences.
