%!TeX spellcheck = en_US
% !TeX root = ../main.tex
\section{Results \& Analysis}
\label{sec:results}
In this section, we evaluate empirically the performance of our proposed LLM-based scenario analysis framework. The evaluation is conducted on 100 simulated collision scenarios  from the CommonRoad by using a Frenetix motion planer. We assess the effectiveness of our framework using our three proposed prompt templates and compare the results with human annotated label.

\subsection{Experimental Setup}
To evaluate our framework, we use the following setup:
\begin{itemize}
    \item \textit{Dataset:} We randomly run the ego vehicle with Frenetix motion planer in 6000 real recorded driving simulation datasets in the CommonRoad. This high-performance planner can find the optimal trajectory in more than 4700 scenarios, and approximately in 1000 scenarios have the collision with an obstacle; the other results are like did not find the trajectory or over the calculation time. After that, we randomly selected 100 scenarios from those collision scenarios. For each scenario, we annotate the ID of exact Obstacle which has collision with ego vehicle.
    \item \textit{LLMs:} We employ state-of-the-art powerful large language models GPT4o-mini, GPT4o, and Gemini 1.5 Pro by using API for safety-critical scenario analysis, according to actual ranking, the new released Deepseek V3 is actually not available by using API because of maintenance.
    \item \textit{Metrics:} We use the time to collision (TTC) and minimal distance to collision (MDC) as our primary safety-critical metrics, which represent temporal and spatial crucial perspectives.
    \item \textit{Baselines:} We evaluate our frameworkâ€™s performance by comparing it with human expert analysis. The outputs from the LLMs include the ID of collision obstacle and near-collision obstacle. The selected obstacle ID is then compared with the human-annotated obstacle ID that collides with the ego vehicle. However, due to contextual interpretation in prompts and the reasoning performance of each model, LLMs may not explicitly identify an obstacle as a "collision obstacle" but rather as an "near-collision obstacle." Since these two labels refer to the same obstacle, only one will be considered in our analysis. If the LLMs correctly identify the ID of the near-collision obstacle but assign the wrong ID to the collision obstacle, this case will not be included in the accuracy evaluation.  
\end{itemize}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/plot.png} 
    \caption{
        Scenarios Evaluation Across LLMs and Prompt Templates.\\
        The figure shows the accuracy of different LLMs in interpreting scenarios and identifying collision obstacles. While the ground truth labels objects as 'collision obstacles,' LLMs may classify them differently (e.g., as 'near-collision obstacles') due to variations in contextual reasoning and prompt interpretation.
    }
    \label{fig:plot} % Optional: for referencing the figure
\end{figure}

\subsection{Results}
We present the results of our framework in the following subsections.

\subsubsection{Accuracy}
The framework's performance across three models and in three distinct templates is measured in terms of accuracy in Figure~\ref{fig:plot} and is evaluated by comparing the LLM's predictions collision ID or near near-collision ID with the ground truth labels provided by human experts. 
The results demonstrate that model performance improves with structured and detailed prompts. Unsurprisingly, GPT-4o-mini underperforms compared to GPT-4o and Gemini 1.5 Pro, with GPT-4o achieving the best results across all templates. LLMs' reasoning ability is proven reliable using Frenet-coordinates prompts, as they can effectively interpret and analyze scenarios even when detailed equations or calculations, such as TTC and MDC, are omitted in the prompt. 
However, the safety-critical metrics prompt, which provides structured TTC and MDC values via the parsing function $\mathcal{F}_{\text{metrics}}$ and logical thresholds in the system message, significantly enhances LLM performance. This setup enables the models to act as "experts in collision analysis," leveraging domain-specific knowledge to determine risk scores based on logical information, delivering robust and accurate results closely aligned with human analysis.
\subsubsection{Response times}
We record response times across models and templates to compare the computational efficiency of LLMs in generating results for different driving scenarios. Figure~\ref{fig:kpi} presents a boxplot of response time distributions per model and template, analyzing 100 scenarios at the final timestep. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/kpi.png} 
    \caption{Response Time Comparison of LLM Models Across Different Templates.}
    \label{fig:kpi} % Optional: for referencing the figure
\end{figure}
According to the figure, GPT-4o-mini consistently demonstrates the shortest response times across templates, indicating its computational efficiency compared to the accuracy reflecting its weak reasoning capabilities. GPT-4o, achieving the highest accuracy across templates, balances reasoning ability and computational efficiency with moderate response times. Gemini 1.5 Pro, while slightly less accurate than GPT-4o in templates, shows higher response times, particularly in the Safety-critical metrics template. Among templates, the Safety-critical metrics prompt demands more processing time than Cartesian and Frenet-coordinates prompts, reflecting the additional computational load required to process structured metrics and logical constraints. 


