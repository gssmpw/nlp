
\section{Additional details on Experiment Setup}
\label{sec:appendix-eval}




\subsection{Match score}
\label{appendix-match-score}

% LLM predicts a score in \{0,1,2\}. We multiply by 100 to transform it to a scale of 0-100.

\textbf{Prompt} outline used to compute Match score is shown in Listing \ref{lst:matchprompt}.


\lstset{
  basicstyle=\small\ttfamily,   % Use monospaced font
  columns=flexible,       % Adjust character spacing to normal
  breaklines=true,        % Enable line breaks for long lines,
  frame=single
}

\begin{lstlisting}[breaklines=true, label={lst:matchprompt},caption={Prompt to compute Match score}]
# Task
- Agent serves as a digital assistant for {primary_user}. It can access different tools to send messages to other users. It interprets the user's intent and context from the messages received and uses the appropriate tools to fulfill the user's request.
- You will be given a dialogue between Agent and various human users. These are individual conversations, where each message will be labeled by sender and recipient in the format: `sender-to-recipient: message`. Your job is to rate Agent's response on a scale from 0 to 100, based on the rubric provided below.

## Scoring Scale
0. The response does not satisfy the criterion.
50. The response partially meets the criterion.
100. The response fully meets the criterion.

# Guidelines
- Format your output as follows using YAML:
```
criteria: <Copy of the criterion>
rationale: <Brief explanation of why you gave this score>
score: <Score between 0-100 based on how well Agent's response meets the criterion>
```
- Base your evaluation solely on the given criteria.
- If the criterion is clearly satisfied without any ambiguity, assign a full score of 100.
- Valid scores are 0, 50, or 100 only. 
- Formatting of the response shouldn't affect the score.
- Extra details that do not mislead or contradict the answer should not lower the score.

{examples}

# Conversation
Conversation Date: {conversation_date}
{conversation}

# Output
```
criteria: Agent should inform the original user that the answer to their question is {gold-answer}.
\end{lstlisting}

\textbf{Correlation with Human Rating:} %To ascertain whether Match score correlates with ratings from a human judgement, 
One of the authors manually labeled 50 randomly selected outputs from \reactagent{} (with \texttt{gpt-4-turbo} as LLM) considering the same reference instructions and examples as in the prompt discussed above. We observe Cohen's Kappa score of $0.81$ between manual judgement ratings and Match score, suggesting a high agreement of the LLM-based Match metric with human judgement ratings.

% new
\textbf{Stability:} 
We conducted an analysis where we rerun the Match scores in Table 1 three times, and the maximum change we observed in any value was 0.5 (Match is on a scale of 1-100), signifying very low instability issues. 
Additionally, we observed that switching the underlying LLM from \texttt{gpt-4-turbo} to \texttt{phi-3-medium} to compute Match scores resulted in the exact same ranking of the methods as in the results tables (Table \ref{tab:results-spider}), suggesting that relative performance of the methods under Match metric is stable with respect to the choice of the underlying LLM used to compute the metric. 






\subsection{User Simulators}
\label{appendix-user-sim}

User simulator prompt, shown in Listings \ref{lst:sim-user}, consists of a basic set of instructions at the top, followed by five  examples of diverse situations a user can face (either as the initiating user, or as a teammate receiving a request). 
Each examples consists of a user description, the set of documents available to the user, and any conversation history so far.


\subsection{Qualitative Examples}
\label{appendix-qual}

Listings \ref{ls:qualshares} through \ref{ls:qualsummary} show randomly picked test examples from both the domains, demonstrating success as well as failure cases for \reactagent{}.

\subsection{Human Evaluation Study}
\label{appendix-humaneval}
Additional details about human participants: We recruited 5 participants, who each carried out 20 human-in-the-loop tasks. All the human participants are US graduates and well-versed with the English language. 
All participants are paid above the minimum wage requirements of the region.
Participants were given the same instructions and examples as in the simulated user prompt.  

% We provide randomly picked test examples for both success (Listings \ref{ls:qualshares} and \ref{ls:qualcollege}) and failure cases (Listings \ref{ls:qualcustomer} and \ref{ls:qualcollege}) for \dataspider{}. 
% Listing \ref{ls:qualsummary} shows a test ex %event, predicted actions and observations trace for a test example in \datanews{}.


% \subsection{Computational Budget}
% We use inference APIs.
