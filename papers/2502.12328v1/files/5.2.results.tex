

% \section{Experiments using Simulated User}
\section{Experiments}

% As described in Sec.~\ref{sec:data}, 
The \asyncfw{} framework includes user simulators that represent collaborators within an organization, along with scaffolding code that enables an agent to search through the initiating user's documents and identify and contact relevant collaborators.
% The \asyncfw{} framework provides simulators for collaborator in an organization, as well as scaffolding code by which an agent can search initiating user's documents, and can find and contact collaborators. 
All experiments use a \texttt{gpt-4-turbo} model \citep{openai2023gpt4}, prompted with each collaborator's description and document collection, to implement these simulators (full prompt in Appendix \ref{appendix-user-sim}). We then evaluate our reference agent architecture using the metrics described above.

%In our scenario, agent needs to communicate with multiple users to gather information, which involves several back-and-forth conversations. Since it’s difficult to involve real people for frequent testing, we created a setup where simulated users, powered by an LLM, take on the role of different personas to test the agent's performance.
%Each simulated user has a unique profile and responds based on their description and the conversation so far. 
%We use GPT-4-turbo \cite{openai2023gpt4} for simulated user


We compare several alternative implementations of this reference architecture, including variations in task orchestration and planning strategies. 
\textbf{\reactagent{}} is the full agent architecture
\cite{DBLP:conf/iclr/YaoZYDSN023}, and
\textbf{\noreflectionagent{}} is a variant of this architecture which performs no reflection actions.
% We additionally compare to
% \textbf{\messageall{}}, an agent that is encouraged (through prompt instructions and exemplars) to message each person in the organization exactly once, with the same question the user asked, and
% \textbf{\messagenone{},} an agent that attempts to complete the task with the user's documents alone (i.e. without contacting any collaborator).
% \subsection{LLM Choices}
We compare \texttt{gpt-4-turbo} \cite{openai2023gpt4}, \texttt{gpt-4o} \cite{openai2023gpt4}, and \texttt{phi-3-medium} \cite{abdin2024phi} as LLMs.
We use greedy decoding.
% Results with \messageall{} and \messagenone{} are reported only for \texttt{gpt-4-turbo} as it gave the best results for \reactagent{}. 




% \textbf{Results on \dataspider{}}:
\subsection{Results on \dataspider{}} 
The max score on Match metric across all methods is only $54.8$ (Table \ref{tab:results-spider}), achieved by \reactagent{} when used with \texttt{gpt-4-turbo}, demonstrating the overall challenging setup. 
Moreover, for the same configuration, P-Prec and P-Rec scores are $0.61$ and $0.89$ respectively, demonstrating scope of further improvement in optimal selection of people to contact.
Comparing LLM choices for \reactagent{}, \texttt{gpt-4-turbo} performed better than \texttt{gpt-4o}, while \texttt{phi-3-medium} is generally worse on Match and information source selection.
Finally, \reactagent{} 
 generally performs similar or better than \noreflectionagent{} across LLMs on Match, efficiency, and optimal selection of information sources, demonstrating the usefulness of a \emph{reflection} step. \\

% --- \textbf{\messageall{}}       &   34.6        &   11.4  &    195  & 3.2  &  0.37        &   0.90     \\
% --- \textbf{\messagenone{}}     &  19.2         &   4.1 &    82      &  0.0  &   -       &   -     \\
 \noindent \textbf{Additional Comparisons:}
To put these results in perspective, we additionally compare with following techniques:\\
\begin{table}[b!]
\footnotesize
\centering
\begin{tabular}{l@{\hspace{5pt}}cc@{\hspace{5pt}}cc@{\hspace{5pt}}c}
\toprule
& \textbf{Match} $\uparrow$ & \textbf{MsgCnt} $\downarrow$ & \textbf{P-Prec} $\uparrow$ \\
\midrule
\reactagent{} & 54.8 & 9.0 & 0.61 \\
\messageall{} & 34.6 & 11.4 & 0.37 \\
\messagenone{} & 19.2 & 4.1 & N/A \\
\idealagent{} & 100 & 7.0 & 1.0 \\
\bottomrule
\end{tabular}
\caption{Additional Comparisons (using \texttt{gpt-4-turbo})}
% \vspace{-2em}
\label{tab:additional-comparisons}
\end{table}
\noindent (1) \textbf{\messageall{}}, an agent that is encouraged (through prompt instructions and exemplars) to message each person in the organization exactly once, with the same question the user asked.  \messageall{} results highlight the importance of judiciously choosing who to contact (MsgCnt of 11.4 compared to 9.0 for \reactagent{}), framing the correct questions and engaging in multi-turn conversations with collaborators when needed (Match score is much lower than that of \reactagent{}). \\
\noindent (2) \textbf{\messagenone{},} an agent that attempts to complete the task with the user's documents alone (i.e. without contacting any collaborator).
\messagenone{} results provide a baseline performance when no collaborator is contacted. \\
\noindent (3) \textbf{\idealagent{}}, which is defined as the one that always gets the correct answers by contacting the optimal set of relevant collaborators, formulating perfect questions, etc.
will get a Match score of 100, \#People count of 1.5 (equals count of the optimal set of people to contact), and MsgCnt of 7. \\
%(one initial and one final exchange with the originating user, and one exchange with each collaborator contacted: 3.5*2=7). \\




% \subsubsection{Analysis}
\noindent \textbf{Analysis:}
% \noindent \textbf{Results by Category:}
We analyzed Match scores on subsets of \dataspider{} for \reactagent{} with \texttt{gpt-4-turbo}: 
(1) \emph{Document Split}: 50.0; 
(2) \emph{Redirection}: 38.0;
(3) \emph{Unanswerable}: 87.5. 
The results demonstrate that \reactagent{} does particularly well in identifying unanswerable questions, but struggles with information fragmentation and knowledge hierarchies required to correctly handle the redirection category. 

% NEW
We include a few qualitative examples in Appendix \ref{appendix-qual}. Additionally, we analyzed 40 random examples with imperfect Match scores in PeopleJoin-QA when using \reactagent{} and the most common failure modes were: (1) Failing to contact all  relevant users and arriving at an incorrect answer [$30\%$ of cases]. %Note that dynamically finding the next person(s) to contact requires reasoning over all the information up until that point obtained from other people and user’s documents. 
(2) Poorly worded or overly-specific queries from the agent causing other users to conclude that they didn’t have relevant information [$25\%$ of cases]. For example, the Listing 9 in Appendix B.3. (3) Failing to reach out to all the relevant people and telling the user it couldn’t get all the information [$20\%$ of cases]. 
(4) Orchestration errors, such as not predicting tools for people or document search [$10\%$] (Listing 8 is an example). %We will add these details in future revisions.
% A detailed qualitative analysis (with full examples) is present in Appendix \ref{appendix-qual}.



% \textbf{Results on \datanews{}}:
\subsection{Results on \datanews{}}
% Compared to \dataspider{}, there is less variability across agent architectures implementation variants.
% Compared to \dataspider{}, there is less variability in the metrics across various agent architectures.
% \todo{update results for G-Eval}
On \datanews{}, among the LLM choices, \texttt{gpt-4-turbo} performs better than \texttt{gpt-4o}, which in turn performs better than \texttt{phi-3} (Table \ref{tab:results-news}). In contrast to results in \dataspider{}, \reactagent{} and \noreflectionagent{} variants perform similar, suggesting no usefulness of the reflection step in the document creation task. 
%This appears to reflect overall poor performance in an absolute sense: the \emph{MessageNone} agent -- which must hallucinate a complete summary based on user-provided keywords alone -- is only slightly worse than the other agents, and the overall size of the messages exchanged is quite large. Notably, even the \emph{MessageAllOnce} agent does not achieve 100\% person recall---though prompted to contact all agents, does not always do so. 
On this task, an \idealagent{} should obtain G-Eval scores of 5, MsgCnt of 6.3, MsgSize of 1592, and \#People of 1.7.
These results indicate that the document creation task is also challenging, with significant scope for improvement in output quality and communicative efficiency.
%\todo{Finally, to put the results from various methods in perspective, we report metrics for an \textbf{\idealagent{}}, ...}
\\


\noindent \textbf{Analysis:} Here, the most common failure modes (in 40 analyzed examples) were (1) failing to ask follow-up questions in cases where one user had multiple documents on a given topic [38\% of cases], (2) poorly worded or overly-specific queries, causing other users to conclude that they didn’t have relevant documents [24\%], and (3) orchestration failures in which the agent was distracted by a user comment and ended the conversation early or stopped pursuing the original goal [38\%].






