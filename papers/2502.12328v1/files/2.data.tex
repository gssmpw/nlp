% \section{LLM agents to Steer Asynchronous Collaboration}
\section{Challenges in Effectively Steering Multi-User Information Gathering}

The problem of answering user queries by synthesizing information distributed across heterogeneous data sources is most often studied through the lens of database systems \cite{zaniolo1997advanced}.
Work on query optimization and federated databases \cite{sheth1990federated} has sought to address the 
%issue of efficiently querying across multiple heterogeneous data sources.
specific question of how to efficiently answer structured queries without access to a centralized knowledge store. 
%However, databases assume structured, centralized data, whereas real-world collaboration often involves structured and unstructured, distributed information.
The problem we study in \asyncfw{} may be viewed as a generalization of this task to the setting where the relevant information is possessed by \emph{people}, not structured knowledge bases, and must be obtained via conversation rather than structured queries.
An agent to help a user with such requests must address several challenges:

\begin{itemize}


    \item \textbf{Information fragmentation:} 
    In a typical organization, information is often siloed across multiple users, because of differing roles and responsibilities. %, privacy and security considerations, etc. 
    %The resulting information fragmentation means the system has to get information from multiple users. 
    Some requests may require gathering information from multiple people.

    \item \textbf{Partial observability:} 
    To gather this information, it is often necessary to first determine which collaborators hold relevant information, under incomplete and potentially imprecise information of what information each collaborator might have. Agents for collaborative decision-making might have to engage in multi-turn conversations with various users, refining and adapting requests as needed.
    
    \item \textbf{Communication costs:} Requests for information require human effort to process and answer; effective collaboration requires \emph{efficient} communication: 
    effective agents
    should judiciously send information requests to other collaborators, %distribute work evenly within an organization,
    and avoid asking questions that are likely to be unanswerable.
    %identifying if user request seems unanswerable, etc.
    
    \item  \textbf{Complex reasoning and planning:} Efficient communication requires reasoning: establishing what information is available in accessible documents, dynamically predicting which collaborators are likely to have relevant information for specific questions, identifying the best order in which to ask these questions, and re-planning based on collaborators' responses.


\end{itemize}
Below, we present a benchmark for evaluating these skills.



\section{Data}
\label{sec:data}


Each \asyncfw{} domain comprises a set of \textbf{organizations}. Each organization contains a set of \textbf{collaborators}, and each \textbf{collaborator} has privileged access to a set of \textbf{documents}. The benchmark provides LLM-based simulators for each collaborator, a search interface that can be used to find collaborators,
and a messaging interface that can be used to ask collaborators about their documents.
Then an \textbf{agent} must take as input a \textbf{query} from one collaborator, use the search and messaging interfaces to interact with other collaborators, and finally return an \textbf{answer} to the originator.

Drawing analogies between multi-user collaboration tasks and existing multi-\emph{data-source} tasks commonly studied in NLP, we develop \asyncfw{} by re-purposing existing high-quality resources for database question answering (to produce \dataspider{}) and multi-document summarization (to produce \datanews{}).


\begin{figure*}
    \centering
    %\includegraphics[width=0.9\linewidth]{images/data_transformation.png}
    \includegraphics[width=.8\linewidth,trim=0.1in 3.3in 0.1in 1.2in]{images/tinytenants_dataset}
    \caption{Illustration of a transformation of a Spider datum into \dataspider{}.}
    \label{fig:data-transformation}
\end{figure*}


\subsection{\dataspider{}}
The \dataspider{} dataset 
%is aimed at testing 
evaluates LM agents' abilities to answer questions by aggregating information from multiple collaborators.
We construct it by re-purposing \textsc{spider} \cite{yu2018spider}, a text-to-SQL benchmark. We transform \textsc{spider} into a multi-user information gathering task by recasting \textsc{spider} tables as ``documents'', distributed among several users, and interpreting
\textsc{spider} questions
as queries from an initiating user to an AI agent. 
In this scenario, answering questions requires identifying which users possess the relevant pieces of information (similar to selecting tables in a database), and then engaging in multi-turn conversations with these users to ask targeted questions (akin to constructing joins between tables). 
%The challenge shifts from generating SQL queries to orchestrating effective interactions, where the system must dynamically determine the right users to contact, pose appropriate follow-up questions based on their responses, and compile a complete and coherent answer to the original question. 




\textsc{spider} consists of a set of 200 databases, with a total of over 10K questions. %(each question is associated with a single database).
Each database in \textsc{spider} is transformed into an ``organization'' containing a set of $2$--$20$ distinct users, each with access to a distinct set of documents.
%denoted by $\bf P$.  %We shall refer to documents accessible by user $p_i$ as $\bf d_{p_i}$.


\paragraph{Documents}
Each table in a \textsc{spider} database is converted to one or more documents.\footnote{Represented as a sequence of \textsc{json} objects, one per row.} We additionally apply the following transformations to elicit a diverse set of information-gathering behaviors:% from agents:

\begin{enumerate}
     \item \textbf{Split Documents:}  One of the randomly selected tables is split into two parts (each containing half the rows). This simulates a scenario in which information about a given topic is distributed across multiple individuals. 
     For instance, in Fig.~\ref{fig:data-transformation}, the information in the table \texttt{department} is split between Alice and Dante. 
    \item \textbf{Redirection:} We construct scenarios in which a (``redirecting'') user does not have direct access to some information (e.g.\ Chen in Fig.~\ref{fig:data-transformation}), but does have knowledge of which other (``target'') user might have this information (Dante in Fig \ref{fig:data-transformation}). To answer questions about these tables, agents cannot always contact knowledgeable users directly, and must navigate organizational knowledge hierarchies to find them.
    Information about other users is available to the redirecting user as an additional document.
    \item \textbf{Missing Information:} 
    In each database,
    we omit a randomly selected table, making a subset of the queries associated with that organization \textbf{unanswerable}, simulating a scenario in which required information is simply not present~\cite{levy-etal-2017-zero, rajpurkar2018know} in the organization. 
   
\end{enumerate}

\noindent{}
In \dataspider{}, each user is allocated one document, and no two users have access to the same document. 
After we have assigned each organization member a set of documents, we populate the collaborator search interface with hints about what information they might have access to (e.g.\ \emph{Chen likely has information about teacher salaries}).
%For simplicity, we start with 
We begin by constructing templated descriptions specifying the table name and names of columns, then use GPT-4 to convert these to simpler English statements using a few-shot prompting setup.%\footnote{Details on the prompt used can be found in the Appendix.}
These transformations by design sometimes result in imprecise or incomplete descriptions, simulating the challenges of selecting a good subset of people to contact under limited information.
For example, \emph{Chen might know about student demographics} fails to specify what specific demographic information is there, and how it is associated with students (e.g., using a student ID, name, or other identifier).
For redirections as described above, these descriptions state that the redirecting user has the information that is in fact possessed by the target user.





\paragraph{Task and Evaluation}

% \noindent \textbf{Initiating User and NL Queries:}
Each organization is associated with multiple problem instances, one for each question in the underlying \textsc{spider} dataset.
For example, in Fig \ref{fig:data-transformation}, the task issued by Alice  to their agent is \emph{What are the name and budgets of the department...}, which must then be answered by reasoning about the contents of both Alice's documents and the other users'. Ground-truth answers are derived from the underlying \textsc{Spider} annotations, except in the case of un-answerable queries. Our primary evaluation metric measures the accuracy with which agents can recover ground-truth answers and identify unanswerable questions; secondary evaluations measure whether the right users were contacted, and the efficiency (in terms of messages sent) with which agents identify these users.






\paragraph{Statistics}
Though the \textsc{Spider} dataset has several thousands of questions paired with databases, we restrict to 500 test tasks to enable efficient evaluation.\footnote{Our code release makes it possible to generate additional organizations for training and evaluation.} A typical task requires agent to interact with 0--5 people (excluding the initiating user) to arrive at the answer (mean of 1.54 with a variance of 1.12).
9\% of test instances in \dataspider{} are \emph{unanswerable},  22\% of the test instances require an agent to handle a \emph{redirection} to arrive at correct answer, while 25\% of the test instances  require an agent to handle a \emph{document split} between multiple people to answer the user question correctly. 
Note that a data instance could belong to more than one category (for example, a task might require access to \emph{split documents} as well as access to information from another document that needs to be accessed through \emph{redirection}).
%52\% of the cases have none of three special categories, but still test the general challenges associated with the task discussed earlier, such as finding the optimal set and order of people to contact.


\subsection{\datanews{}}

The \datanews{} task evaluates agents not on structured QA, but instead on more open-ended document creation tasks. We derive it from  \textsc{MultiNews}  \cite{fabbri2019multi}, a multi-document summarization dataset consisting of sets of news articles on a related topic and single summaries that aggregate information across the articles.
We
distribute source news articles across multiple users, and require agents to gather these documents (or excerpts from them) and combine them into a target summary. %Unlike \dataspider{}, this domain requires little document pre-processing.


\paragraph{Task and Evaluation}

As in \dataspider{}, each organization is derived from underlying \textsc{MultiNews} problem instances. Here, however, \emph{multiple} problem instances are combined into a single organization: some users have articles on one subject, some users have articles about multiple subjects, and some may have no articles at all. Each organization possesses information about $3$ topics, and contains $1$--$7$ users, with documents randomly partitioned across users.

Also as in \dataspider{}, we create user descriptions for collaborator search  by presenting user documents to GPT-4 and querying it for a list of keywords that the user is knowledgeable about (e.g.\ \emph{governor election, GOP, health care}).

\paragraph{Statistics}

Because of the relatively large size of the documents that must be exchanged to complete these tasks, we construct 200 test instances distributed across 67 organizations.
%\footnote{As with \dataspider{}, we release code to create more.} 
Summaries are derived from an average of 2.7 documents (variance of 1.1), which must be located within organizations with an average of 5.1 users (variance of 4.5) and 6.4 total documents (variance of 4.1) or 1.25 documents per user.
