\section{Evaluation}

\asyncfw{} provides metrics for evaluating the efficiency and correctness of user interactions. 
%The main categories of metrics are outlined below, with a focus on the most relevant metrics for each category.


\subsection{Outcome Metrics}

The most important measure of an agent's effectiveness is its ability to provide the correct response to the user's query. We characterize correctness in different ways for the  domains within \asyncfw{}.

\paragraph{Answer match:} For \dataspider{}, we prompt an LLM-based evaluator to compare the agent's final response to the reference answer and output a score in \{0,50,100\}, where a score of 100 refers to a perfectly matched score (all the expected information was present), a score of 50 refers to a partial match (for example, if only few of the expected list of items were correctly provided), while a score of 0 refers to incorrect results (for example, if the agent claimed it could not find the requested information but gold answer suggests otherwise).  
The score is predicted by an LLM (\texttt{gpt-4-turbo}), conditioned upon the agent response to the initiating user and the expected gold answer, certain prompt instructions and three examples. More details are available in the Appendix \ref{appendix-match-score}. 
%We do not report word overlap like BLEU as they can be sensitive to extraneous words (such as pleasantries and functional words) in agent responses. %, and are not well suited for evaluating the structured responses that are required for some queries.

% \paragraph{\textsc{rouge}} 
\paragraph{\textsc{rouge and G-Eval}} 
For the \datanews{} task, we require agents to output a final summary enclosed by special delimiter tokens, then report the \textsc{rouge-L} score \citep{lin2004rouge} of this summary relative to the reference summary. If the agent produces no summary, it obtains a score of 0; if it produces multiple summaries on different turns,% (a behavior sometimes observed in our baseline agent), 
 we score only the final one.
 % \paragraph{\textsc{G-Eval}} 
 We also report \textsc{G-Eval} scores \citep{DBLP:conf/emnlp/LiuIXWXZ23}, a set of automated metrics that evaluate the relevance, consistency, and coherence of a summary using an LM with access to % a reference summary and 
source documents.
 %\citep{DBLP:conf/emnlp/LiuIXWXZ23} of this summary. \todo{G-Eval .... how is it computed}. 
 G-Eval has been found to correlate highly with human summarization ratings \cite{DBLP:conf/acl/SongSSCM24}.



\subsection{Efficiency Metrics}

An effective agent should not only produce correct answers, but do so while minimizing effort from collaborators. We quantify this using three metrics.
\textbf{Message count (Msg)}: measures the total number of messages exchanged during the task.
\textbf{Message size (MsgSize)}: message count alone does not penalize requests requiring lengthy responses from collaborators, so we additionally report the total number of words exchanged (tokenized using the NLTK \cite{bird2009natural} word tokenizer). %-- todo: distringuish b/w messages sent and received.
\textbf{People contacted (\#People)}: the count of people that the agent exchanged messages with (including the initiating user), averaged across the test set.



\subsection{Information Source Metrics}

In both \dataspider{} and \datanews{}, the gold set of documents required to answer a task correctly are known, which also allows us to infer the \emph{optimal set of people} an agent must contact to arrive at the correct outcome. We collect the set of distinct users contacted by the agent, then compute the precision (\textbf{P-Prec}) and recall (\textbf{P-Rec}) relative to the ground-truth people set, averaged across queries.



