\begin{abstract}

 
This paper introduces \asyncfw{}, a  benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, \asyncfw{} agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. \asyncfw{} comprises two evaluation domains: \dataspider{}, focused on questions about tabular data, and \datanews{}, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic ``organizations'' of 2--20 users, simulating natural multi-user collaboration scenarios.
We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using \asyncfw{}.%\footnote{Code and data will be publicly released.}
\footnote{Code and data can be found at \url{https://github.com/microsoft/peoplejoin/}}

\end{abstract}
