
% \section*{Table of Contents}
% \begin{itemize}
%     \item[\textbf{\textcolor{blue}{A}}] \textbf{\textcolor{blue}{Implementation Details}}
%     \begin{itemize}
%         \item[A.1] Gaussian Construction
%         \item[A.2] Neural Radiance Field Rendering
%         \item[A.3] Hierarchical Latent Variables
%         \item[A.4] Modulation
%     \end{itemize}
    
%     \item[\textbf{\textcolor{blue}{B}}] \textbf{\textcolor{blue}{Derivation of Evidence Lower Bound}}
    
%     \item[\textbf{\textcolor{blue}{C}}] \textbf{\textcolor{blue}{Training Details}}
    
%     \item[\textbf{\textcolor{blue}{D}}] \textbf{\textcolor{blue}{More Experimental Results}}
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NerF: $F: (x,y,z,\theta,\phi) \mapsto (c,\sigma)$. 

% Context set (camera pose and image rbg level): 
% \begin{align}
% \tilde{X}_c:&= \{ \tilde{x}_i \}_{i=1}^{N_c}, \tilde{x}_i \in \mathbb{R}^{6} (\text{ray-o}, \text{ray-d})\\
% \tilde{Y}_c:&= \{ \tilde{y}_i \}_{i=1}^{N_c}, \tilde{y}_i \in \mathbb{R}^{3} (\text{RGB})
% \end{align}

% Target set (3d coordinates and 3d rbg with density level):

% $x_i$ is a set of 3D points corresponding to a specific ray direction. 
% \begin{align}
% X_t:&= \{ x_i \}_{i=1}^{N_t}, x_i \in \mathbb{R}^{P\times3} (\text{sampled 3D points})\\
% Y_t:&= \{ y_i \}_{i=1}^{N_t}, y_i \in \mathbb{R}^{P\times4} (c,\sigma)
% \end{align}

% \begin{align}
%     p(Y_t|X_t, \tilde{X}_c, \tilde{Y}_c) & = p(Y_t|X_t,B_c)p(B_c|\tilde{X}_c, \tilde{Y}_c) \\
%     &= \Pi_{i=1}^{N_t} p(y_i|x_i, g, r_i, B_c) p(r_i|g, x_i) p(g|B_c, X_t) p(B_c|\tilde{X}_c, \tilde{Y}_c)
% \end{align}

% \begin{align}
%     p(Y_t|X_t, \tilde{X}_c, \tilde{Y}_c) & \approx p(Y_t|X_t,B_c) \\
%     &= \Pi_{i=1}^{N_t} p(y_i|x_i, g, r_i, B_c) p(r_i|g, x_i) p(g|B_c, X_t)
% \end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Neural Radiance Field Rendering}
\label{supp:nerf-render}
In this section, we outline the rendering function of NeRF~\citep{mildenhall2021nerf}. A 5D neural radiance field represents a scene by specifying the volume density and the directional radiance emitted at every point in space. NeRF calculates the color of any ray traversing the scene based on principles from classical volume rendering~\citep{kajiya1984ray}. The volume density $\sigma(\mathbf{x})$ quantifies the differential likelihood of a ray terminating at an infinitesimal particle located at $\mathbf{x}$. The anticipated color $C(\mathbf{r})$ of a camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$, within the bounds $t_n$ and $t_f$, is determined as follows:
\begin{equation}
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) c(\mathbf{r}(t), \mathbf{d}) dt, \quad \text{where} \quad T(t) = \exp \left( - \int_{t_n}^{t} \sigma(\mathbf{r}(s)) ds \right).
\end{equation}

Here, the function $T(t)$ represents the accumulated transmittance along the ray from $t_n$ to $t$, which is the probability that the ray travels from $t_n$ to $t$ without encountering any other particles. To render a view from our continuous neural radiance field, we need to compute this integral $C(\mathbf{r})$ for a camera ray traced through each pixel of the desired virtual camera.

\section{Implementation Details}
\label{sec:implementation-details}

\subsection{Gaussian Construction}
\label{supp:gaussian}

As introduced in Sec.~\ref{sec: geometrybases}, we introduce geometric bases ${\bf{{B}}}_{C}$ to structure the context variables geometrically.
${\bf{{B}}}_{C}$ are geometric  bases (Gaussians) inferred from the context views $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ with 3D structure information, 
\textit{i.e.,} ${\bf{b}}_i = \{ \mathcal{N}(\mu_i, \Sigma_i); \omega_i\}$,
%\textit{i.e., object shape, color and texture.}. $B_C$ is obtained by: 
% ${\bf{{B}}}_{C}=\texttt{Encoder}\Big({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\Big)$. 
\begin{align}
    &{\bf{{B}}}_{C} = \{{\bf{b}}_i\}_{i=1}^{M}, {\bf{b}}_i=\{\mathcal{N}(\mu_i, \Sigma_i); \omega_i\},
    \label{eq: generation_B_1}
    \\
    & \mu_i, \Sigma_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}), \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
    \label{eq: generation_B_2}
    \\
    & \omega_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
    \label{eq: generation_B_3}
\end{align}
where $M$ is the number of the Gaussian bases. $\mu \mathbb \in {R}^3$ is the Gaussian center, $\Sigma \in  \mathbb{R}^{3\times 3}$ is the covariance matrix, and $\omega \in \mathbb{R}^{d_B}$ is the corresponding ${d_B}$-dimension semantic representation. In our implementation, we choose $d_{B}$ as $32$. $\texttt{Att}$ is a self-attention module. Specifically, given the context set $[\widetilde{\mathbf{X}};\widetilde{\mathbf{Y}}] \in \mathbb{R}^{H\times W \times (3+3+3)}$, the visual self-attention module, $\texttt{Att}$, first produces a $M\times D$ tokens with $M$ is the number of visual tokens and $D$ is the hidden dimension. The number of Gaussians we use equals the number of tokens $M$. %Then, we use one MLP to predict centers $\mu$, as well as the rotation $R$ and scaling $S$ matrices parameters for producing covariance matrix $\Sigma$, and one MLP to produce the latent representations $\omega$. 
Then, we use one MLP with 2 linear layers to map the tokens into a 10-dimensional vector, which includes 3-dimensional Gaussian centers, a 3-dimensional vector for constructing the scaling matrix, and a 4-dimensional vector for quaternion parameters of the rotation matrix. Both the scaling matrix and rotation matrix are used to build the \(3 \times 3\) covariance matrix. This procedure is similar to Gaussian construction in the 3D Gaussian Splatting~\citep{kerbl20233d}.
Another MLP estimates the latent representation of each Gaussian basis, using a 32-dimensional vector for each Gaussian basis. 

The covariance matrix is obtained by:
\begin{equation}
    \Sigma = RSS^TR^T,
    \label{eq:cov-matrix}
\end{equation}
where $R\in \mathbb{R}^{3\times3}$ is the rotation matrix, and $S \in \mathbb{R}^3$ is the scaling matrix. 











\subsection{Hierarchical Latent Variables}
\label{supp:latent-variables}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/Transformer.pdf} % Adjust the size and filename as needed
  \caption{\textbf{Using transformer encoder to generate ray-specific latent variable $\mathbf{z}_r$.}} % Caption for the figure
  \label{fig:latent-transformer}
  % \vspace{-3mm}
\end{figure}

At the object level, the distribution of an object-specific latent variable \(\mathbf{z}_o\) is obtained by aggregating all location representations from \((\mathbf{B}_C, \mathbf{X}_T)\). We assume \(p(\mathbf{z}_o | \mathbf{B}_C, \mathbf{X}_T)\) follows a standard Gaussian distribution and generate its mean \(\mu_{o}\) and variance \(\sigma_{o}\) using MLPs. We sample an object-specific modulation vector, \(\hat{\mathbf{z}}_o\), from its prior distribution \(p(\mathbf{z}_o | \mathbf{X}_T, \mathbf{B}_C)\).

Similarly, as shown in Fig.~\ref{fig:latent-transformer}, we aggregate the information per ray using \(\mathbf{B}_C\), which is then fed into a Transformer along with \(\hat{\mathbf{z}}_o\) to predict the latent variable \(\mathbf{z}_r\) with mean \(\mu_r\) and \(\sigma_r\) for each ray.


 

\subsection{Modulation}
\label{supp:modulate}
The latent variables for modulating the MLP are represented as \([\mathbf{z}_o; \mathbf{z}_r]\). Our approach to the modulated MLP layer follows the style modulation techniques described in \citep{karras2020analyzing, guo2023versatile}. Specifically, we consider the weights of an MLP layer (or 1x1 convolution) as \( W \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}} \), where \( d_{\text{in}} \) and \( d_{\text{out}} \) are the input and output dimensions respectively, and \( w_{ij} \) is the element at the \(i\)-th row and \(j\)-th column of \( W \).

To generate the style vector \( s \in \mathbb{R}^{d_{\text{in}}} \), we pass the latent variable \( z \) through two MLP layers. Each element \( s_i \) of the style vector \( s \) is then used to modulate the corresponding parameter in \( W \).
\begin{equation}
    w'_{ij} = s_i \cdot w_{ij}, \quad j = 1, \ldots, d_{\text{out}},
\end{equation}
where $w_{ij}$ and $w'_{ij}$ denote the original and modulated weights, respectively.

The modulated weights are normalized to preserve training stability,
\begin{equation}
    w''_{ij} = \frac{w'_{ij}}{\sqrt{\sum_i w'^2_{ij} + \epsilon}}, \quad j = 1, \ldots, d_{\text{out}}.
\end{equation}




\begin{algorithm}[H]
\caption{Modulation Layer}
\begin{algorithmic}[1]
\REQUIRE Latent variable $z$, weight matrix $W \in \mathbb{R}^{d_{\mathrm{in}} \times d_{\mathrm{out}}}$
\ENSURE Modulated and normalized weight matrix $W''$
\STATE \textbf{Compute style vector:}
\STATE $s \leftarrow \mathrm{MLP}_2\big(\mathrm{MLP}_1(z)\big)$
\STATE \textbf{Modulate weights:}
\STATE $W' \leftarrow \operatorname{diag}(s) \times W$
\STATE \textbf{Normalize modulated weights:}
\STATE For each column $j$ in $W'$:
\STATE \hskip1em $\sigma_j \leftarrow \sqrt{\sum_{i=1}^{d_{\mathrm{in}}} (W'_{ij})^2 + \epsilon}$
\STATE Normalize column $j$ of $W'$: $W''_{:,j} \leftarrow W'_{:,j} / \sigma_j$
\RETURN $W''$
\end{algorithmic}
\end{algorithm}










\begin{algorithm}[H]
\caption{Training Procedure}
\begin{algorithmic}[1]
\REQUIRE Context set $({\bf{X}}_{C}, {\bf{Y}}_C)$, target set $({\bf{X}}_{T}, {\bf{Y}}_T)$
\ENSURE Prediction ${\bf{Y}}'_T$
\vspace{0.5em}
\STATE Estimate the context bases ${\bf{B}}_C$ and the target bases ${\bf{B}}_T$ (Eq. 12).
\vspace{0.5em}
\STATE Estimate the object-specific latent variables:
\begin{itemize}
    \item For the context set ${\bf{z}}_o^C$:
    \[
    {\bf{z}}_o^C \sim p({\bf{z}}_o \mid {\bf{X}}_C, {\bf{B}}_C)
    \]
    \item For the target set ${\bf{z}}_o^T$:
    \[
    {\bf{z}}_o^T \sim q({\bf{z}}_o \mid {\bf{X}}_T, {\bf{B}}_T) \quad \text{(Eq. 7)}
    \]
\end{itemize}
\vspace{0.5em}
\STATE Estimate the ray-specific latent variables:
\begin{itemize}
    \item For the context set ${\bf{z}}_r^{C}$:
    \[
    {\bf{z}}_r^{C} \sim p({\bf{z}}_r^n \mid {\bf{z}}_o^C, {\bf{x}}_C^{n}, {\bf{B}}_C) \quad \text{(Eq. 8)}
    \]
    \item For the target set ${\bf{z}}_r^{T}$:
    \[
    {\bf{z}}_r^{T} \sim q({\bf{z}}_r^n \mid {\bf{z}}_o^T, {\bf{x}}_T^{n}, {\bf{B}}_T) \quad \text{(Eq. 8)}
    \]
\end{itemize}
\vspace{0.5em}
\STATE Modulate MLP $f$ using the target latent variables $\{{\bf{z}}_o^T, {\bf{z}}_r^{T}\}$ (Eqs. 16 \& 17).
\vspace{0.5em}
\STATE Render novel views $\hat{\bf{Y}}_T$ using the modulated MLP $f$.
\vspace{0.5em}
\STATE \textbf{Compute losses:}
\begin{itemize}
    \item Reconstruction loss between predictions and ground truth:
    \[
    \mathcal{L}_{\text{recon}} = \text{Loss}(\hat{\bf{Y}}_T, {\bf{Y}}_T)
    \]
    \item Latent variable alignment losses (KL divergence) using context and target latent variables (Eq. 10).
\end{itemize}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Inference Procedure}
\begin{algorithmic}[1]
\REQUIRE Context set $({\bf{X}}_C, {\bf{Y}}_C)$, target input ${\bf{X}}_T$
\ENSURE Prediction ${\bf{Y}}'_T$
\vspace{0.5em}
\STATE Estimate the context bases ${\bf{B}}_C$ (Eq. 12).
\vspace{0.5em}
\STATE Estimate the object-specific latent variable ${\bf{z}}_o$ based on the context set:
\[
{\bf{z}}_o \sim p({\bf{z}}_o \mid {\bf{X}}_C, {\bf{B}}_C) \quad \text{(Eq. 7)}
\]
\vspace{0.5em}
\STATE Estimate the ray-specific latent variables ${\bf{z}}_r^{T}$:
\[
{\bf{z}}_r^{T} \sim p({\bf{z}}_r^n \mid {\bf{z}}_o, {\bf{x}}_T^{n}, {\bf{B}}_C) \quad \text{(Eq. 8)}
\]
\vspace{0.5em}
\STATE Modulate the MLP $f$ using the latent variables $\{{\bf{z}}_o, {\bf{z}}_r^{T}\}$ (Eqs. 16 \& 17).
\vspace{0.5em}
\STATE Render novel views $\hat{\bf{Y}}_T$ using the modulated MLP $f$.
\end{algorithmic}
\end{algorithm}


% $f_C$ by $\{{\bf{z}}_o, {\bf{z}}_r^n\}_C$, 

\section{Derivation of Evidence Lower Bound}
\label{supp:elbo}



\noindent{\textbf{Evidence Lower Bound.}} 
We optimize the model via variational inference~\citep{garnelo2018neural}, deriving the evidence lower bound (ELBO):
\begin{equation}
\begin{aligned}
& \log p(\mathbf{Y}_T \mid \mathbf{X}_T, \mathbf{B}_C) \geq \\
&\mathbb{E}_{q(\mathbf{z}_g | \mathbf{X}_T, \mathbf{B}_T)} \Bigg[ \sum_{m=1}^M \mathbb{E}_{q(\mathbf{z}_l^m | \mathbf{z}_g, \mathbf{x}_T^m, \mathbf{B}_T)} \log p(\mathbf{y}_T^m \mid \mathbf{z}_g, \mathbf{z}_l^m, \mathbf{x}_T^m) \\
& \quad - D_{\text{KL}}\Big[q(\mathbf{z}_l^m | \mathbf{z}_g, \mathbf{x}_T^m, \mathbf{B}_T) \,\big|\big|\, p(\mathbf{z}_l^m | \mathbf{z}_g, \mathbf{x}_T^m, \mathbf{B}_C)\Big] \Bigg] \\
& - D_{\text{KL}}\Big[q(\mathbf{z}_g | \mathbf{X}_T, \mathbf{B}_T) \,\big|\big|\, p(\mathbf{z}_g | \mathbf{X}_T, \mathbf{B}_C)\Big],
\end{aligned}
\end{equation}
where the variational posterior factorizes as $q(\mathbf{z}_g, \{\mathbf{z}_l^m\}_{m=1}^M | \mathbf{X}_T, \mathbf{B}_T) = q(\mathbf{z}_g | \mathbf{X}_T, \mathbf{B}_T) \prod_{m=1}^M q(\mathbf{z}_l^m | \mathbf{z}_g, \mathbf{x}_T^m, \mathbf{B}_T)$. Here, $\mathbf{B}_T$ denotes geometric bases constructed from target data $\{\widetilde{\mathbf{X}}_T, \widetilde{\mathbf{Y}}_T\}$ (available only during training). The KL terms regularize the hierarchical priors $p(\mathbf{z}_g | \mathbf{B}_C)$ and $p(\mathbf{z}_l^m | \mathbf{z}_g, \mathbf{B}_C)$ to align with variational posteriors inferred from $\mathbf{B}_T$, enhancing generalization to context-only settings. Derivations are in Appendix~\ref{supp:elbo}.


The propose \textbf{GeomNP} is formulated as:
{\small
\begin{equation}
        p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) = \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o, ) p({\bf{r}}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o, 
\label{eq:ganp-model-supp}
\end{equation}}where $p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)$ and $p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{r, n}, {\bf{B}}_C)$ denote prior distributions of a object-specific and each ray-specific latent variables, respectively. Then, the evidence lower bound is derived as follows.

\begin{equation}
\begin{aligned}
        &\log p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) \\
        &= \log \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{z}}_o, {\bf{z}}_r^n) p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_C}) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T) d {\bf{z}}_o  \\
    &= \log \int  \prod_{i=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{z}}_o, {\bf{z}}_r^n) p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_C}) \frac{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})}{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})} d {\bf{z}}_r^n \Big\} \\
    & p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T) \frac{q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)}{q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T,)} d {\bf{z}}_o  \\
    &\geq  \mathbb{E}_{q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)}  \Big\{  \sum_{i=1}^{N} \log  \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{z}}_o, {\bf{z}}_r^n) p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_C}) \frac{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})}{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})} d {\bf{z}}_r^n \Big\} \\
    &- D_{\text{KL}}(q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T,) || p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)) \\
    &\geq  \mathbb{E}_{q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)}  \Big\{  \sum_{n=1}^{N}  \mathbb{E}_{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})} \log p({\bf{y}}_{T}^{{\mathbf{r}}, n}| {\bf{x}}_{T}^{{\mathbf{r}}, n}, {\bf{z}}_o, {\bf{z}}_r^n) \\
&- D_{\text{KL}}[q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {\bf{B}_T}) || p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {\bf{B}_C}) ] \Big\} 
- D_{\text{KL}}[q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T) || p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)], \\
        % &=  \int \log \prod_{i=1}^{N_{ray}} \Big\{ \int p({\bf{y}}^{T}_{1:P, i}| {\bf{x}}^{T}_{1:P, i}, {\bf{g}}, {\bf{r}}_i) p({\bf{r}}_i| {\bf{g}},  {\bf{x}}^{T}_{1:P, i}) d {\bf{r}}_i \Big\} +  \log p({\bf{g}} | {\bf{B}}_C,  {\bf{X}}_T,) d {\bf{g}} \\
        % &= \int \sum_{i=1}^{N_{ray}} \log \Big\{ \int p({\bf{y}}^{T}_{1:P, i}| {\bf{x}}^{T}_{1:P, i}, {\bf{g}}, {\bf{r}}_i) p({\bf{r}}_i| {\bf{g}},  {\bf{x}}^{T}_{1:P, i}) d {\bf{r}}_i \Big\} +  \log p({\bf{g}} | {\bf{B}}_C,  {\bf{X}}_T,) d {\bf{g}} \\
        % &= \int \sum_{i=1}^{N_{ray}}  \Big\{ \int \log p({\bf{y}}^{T}_{1:P, i}| {\bf{x}}^{T}_{1:P, i}, {\bf{g}}, {\bf{r}}_i) + \log p({\bf{r}}_i| {\bf{g}},  {\bf{x}}^{T}_{1:P, i}) d {\bf{r}}_i \Big\} +  \log p({\bf{g}} | {\bf{B}}_C,  {\bf{X}}_T,) d {\bf{g}} \\
\end{aligned}      
\end{equation}
where $q_{\theta, \phi}({\bf{z}}_o,  \{{\bf{z}}_r^i\}_{i=1}^{N} | {\bf{X}}_T, {\bf{B}}_T) = q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {\bf{B}_T}) q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)$ is the variational posterior of the hierarchical latent variables. 


\section{More Related Work}
% \textcolor{blue}{
% \cite{szymanowicz2024splatter}, \cite{charatan2024pixelsplat}, \cite{chen2025mvsplat}, \cite{hong2023lrm}, \cite{muller2023diffrf}, \cite{tewari2023diffusion}, \cite{xu2022point}, \cite{wang2024learning}, \cite{liu2024geometry}}


\textcolor{blue}{\paragraph{Generalizable Neural Radiance Fields (NeRF)}
Advancements in neural radiance fields have focused on improving generalization across diverse scenes and objects. \cite{wang2022attention} propose an attention-based NeRF architecture, demonstrating enhanced capabilities in capturing complex scene geometries by focusing on informative regions. \cite{suhail2022generalizable} introduce a generalizable patch-based neural rendering approach, enabling models to adapt to new scenes without retraining. \cite{xu2022point} present \textit{Point-NeRF}, leveraging point-based representations for efficient scene modeling and scalability. \cite{wang2024learning} further enhance point-based methods by incorporating visibility and feature augmentation to improve robustness and generalization. \cite{liu2024geometry} propose a geometry-aware reconstruction with fusion-refined rendering for generalizable NeRFs, improving geometric consistency and visual fidelity. Recently, the \textit{Large Reconstruction Model (LRM)}~\citep{hong2023lrm} has drawn attention. It aims for single-image to 3D reconstruction, emphasizing scalability and handling of large datasets.}

\textcolor{blue}{\paragraph{Gaussian Splatting-based Methods}
Gaussian splatting~\citep{kerbl20233d} has emerged as an effective technique for efficient 3D reconstruction from sparse views. \cite{szymanowicz2024splatter} propose \textit{Splatter Image} for ultra-fast single-view 3D reconstruction. \cite{charatan2024pixelsplat} introduce \textit{pixelsplat}, utilizing 3D Gaussian splats from image pairs for scalable generalizable reconstruction. \cite{chen2025mvsplat} present \textit{MVSplat}, focusing on efficient Gaussian splatting from sparse multi-view images. Our approach can be a complementary module for these methods by introducing a probabilistic neural processing scheme to fully leverage the observation. }

\textcolor{blue}{\paragraph{Diffusion-based 3D Reconstruction}
Integrating diffusion models into 3D reconstruction has shown promise in handling uncertainty and generating high-quality results. \cite{muller2023diffrf} introduce \textit{DiffRF}, a rendering-guided diffusion model for 3D radiance fields. \cite{tewari2023diffusion} explore solving stochastic inverse problems without direct supervision using diffusion with forward models. \cite{liu2023zero} propose \textit{Zero-1-to-3}, a zero-shot method for generating 3D objects from a single image without training on 3D data, utilizing diffusion models. \cite{shi2023zero123++} introduce \textit{Zero123++}, generating consistent multi-view images from a single input image using diffusion-based techniques. \cite{shi2023mvdream} present \textit{MVDream}, which uses multi-view diffusion for 3D generation, enhancing the consistency and quality of reconstructed models.}


\section{Implementation Details}
We train all our models with PyTorch. Adam optimizer is used with a learning rate of $1e-4$. For NeRF-related experiments, we follow the baselines~\citep{chen2022transformers,guo2023versatile} to train the model for 1000 epochs. All experiments are conducted on four NVIDIA A5000 GPUs. For the hyper-parameters $\alpha$ and $\beta$, we simply set them as $0.001$.  


\textcolor{blue}{\paragraph{Model Complexity} The comparison of the number of parameters is presented in Table.~\ref{tab:params_psnr}. Our method, GeomNP, utilizes fewer parameters than the baseline, VNP, while achieving better performance on the ShapeNet Car dataset in terms of PSNR.}

\begin{table}[h!]
\centering
\caption{Comparison of the number of parameters and PSNR on the ShapeNet Car dataset.}
\begin{tabular}{lcc}
\toprule
Method & {\# Parameters} & {PSNR} \\ 
\midrule
VNP     & 34.3M   & 24.21 \\ 
GeomNP  & \textbf{24.0M}   & \textbf{25.13} \\ 
\bottomrule
\end{tabular}
\label{tab:params_psnr}
\end{table}

\paragraph{Integration with PixelNeRF} 
\textcolor{blue}{To integrate our method into PixelNeRF, we utilize the same feature extractor and NeRF architecture. Specifically, we employ a pre-trained ResNet to extract features from the observed images. From the latent space of the feature encoder, we predict geometric bases, which are used to re-represent each 3D point in a higher-dimensional space. These re-represented point features are aggregated into latent variables, which are then used to modulate the first two input MLP layers of PixelNeRF's NeRF network. During training, we align the latent variables derived from the context images with those from the target views to ensure consistency.}

% \newpage

\section{More Experimental Results}
\label{supp:more-results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image Regression}

% \begin{figure*}[htbp]
%     \centering
%     \begin{minipage}[b]{0.45\textwidth} 
%         \includegraphics[width=\textwidth]{Figures/image-regression0.pdf} % Adjust filename as needed
%         \caption{CelebA}
%         \label{fig:celeba}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.45\textwidth} 
%         \includegraphics[width=\textwidth]{Figures/image-regression1.pdf} % Adjust filename as needed
%         \caption{Imagenette}
%         \label{fig:imagenette}
%     \end{minipage}
%     \caption{\textbf{Visualizations} of image regression results on CelebA (left) and Imagenette (right).}
%     \label{fig:visualization-image-regression}
% \end{figure*}

\subsection{Image Completion}
 We also conduct experiments of \method{} on image completion (also called image inpainting), which is a more challenging variant of image regression. Essentially, only part of the pixels are given as context, while the INR functions are required to complete the full image. Visualizations in Fig.~\ref{fig:completion} demonstrate the generalization ability of our method to recover realistic images with fine details based on very limited context ($10 \% - 20\%$ pixels).


\begin{figure}[t!]
  \centering
  \includegraphics[width=0.99\textwidth]{Figures/image-completion.pdf} % Adjust the size and filename as needed
  \vspace{-3mm}
  \caption{\textbf{Image completion visualization} on CelebA using $10\%$ (left) and $20\%$ (right) context.}
  \label{fig:completion}
  \vspace{-5mm}
\end{figure}

\subsection{Comparison with GNT.}
\label{sec:compare_gnt}
Specfiically, we use GNT's image encoder and predict the geometric bases, and GNT's NeRF' network for prediction. 



\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{ICML25/Figures/nerf-syn-1view.pdf} % Adjust the size and filename as needed
  \vspace{-3mm}
  \caption{\textbf{Qualitative comparison with GNT on 1-view setting.}}
  \label{fig:1-view-compare}
  \vspace{-5mm}
\end{figure}

\subsubsection{Cross-Category Example.}
\label{sec:cross-category}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{ICML25/Figures/cross-category.pdf} % Adjust the size and filename as needed
  \vspace{-3mm}
  \caption{}
  \label{fig:cross-category}
  \vspace{-5mm}
\end{figure}



% \subsection{Comparison with PixelNeRF}

% \begin{wraptable}{r}{0.42\textwidth}
% \vspace{-4mm}
% \caption{\textbf{Comparison on the DTU MVS dataset.} Training with 1-view context and testing with both 1-view and 3-view context images. Integrating \method{} into the pixelNeRF framework leads to improvement in terms of both PSNR and SSIM.}
% \centering
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{llcc}
% \toprule
%  & {Method} & {PSNR} & {SSIM} \\
% \midrule
% \multirow{2}{*}{1-view} 
% & pixelNeRF & 15.51 & 0.51 \\
% \multirow{-1}{*}{} & \cellcolor{lightblue}\textbf{\method{}} (Ours) & \cellcolor{lightblue}\textbf{15.89} & \cellcolor{lightblue}\textbf{0.58} \\
% \midrule
% \multirow{2}{*}{3-view} 
% & pixelNeRF & 15.80 & 0.56 \\
% \multirow{-1}{*}{} & \cellcolor{lightblue}\textbf{\method{}} (Ours) & \cellcolor{lightblue}\textbf{16.99} & \cellcolor{lightblue}\textbf{0.61} \\
% \bottomrule
% \vspace{-3mm}
% \end{tabular}
% }
% \label{tab:dtu-compare}
% \end{wraptable}
% \noindent {\textbf{Comparison on DTU.}}
% %Our method can be flexibly integrated with other approaches. 
% To ensure a fair comparison with pixelNeRF~\citep{yu2021pixelnerf} using the same encoder and NeRF network architecture, we incorporate our probabilistic framework into pixelNeRF. We conducted experiments on real-world scenes from the DTU MVS dataset~\citep{aanaes2016large}. To explore the capability of dealing with extremely limited context information, we 
% train both models with 1-view context and test the 1-view and 3-view results in terms of PSNR and SSIM~\citep{wang2004image} metrics. Both qualitative results in Table~\ref{tab:dtu-compare} and qualitative results in Fig.~\ref{fig:dtu-visualization} demonstrate our probabilistic modeling can improve the existing methods. Notably, even when trained with a 1-view context image and tested with 3-view context images, our method significantly outperforms pixelNeRF, demonstrating that our probabilistic framework effectively utilizes limited observations.


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=1\textwidth]{ICLR2025/Figures/dtu-results.pdf} % Adjust the size and filename as needed
%   \vspace{-6mm}  
%   \caption{\textbf{Novel view synthesis results with 1-view context on the DTU dataset.} \method{} has a more realistic rendering quality than pixelNeRF~\citep{yu2021pixelnerf} for novel views with extremely limited context views (1-view).} % Caption for the figure
%   \label{fig:dtu-visualization}
%   \vspace{-5mm}
% \end{figure}




\subsection{More results on ShapeNet}
In this section, we demonstrate more experimental results on the novel view synthesis task on ShapeNet in Fig~\ref{fig:nerf-supp-shapenet}, comparison with VNP~\cite{guo2023versatile} in Fig.~\ref{fig:nerf-supp-compare}, and image regression on the Imagenette dataset in Fig.~\ref{fig:image-supp-image}. The proposed method is able to generate realistic novel view synthesis and 2D images.


\begin{figure}[t]
  \centering
\includegraphics[width=1\textwidth]{Figures/nerf-results-more-supp.pdf} % Adjust the size and filename as needed
  \caption{\textbf{More NeRF results on novel view synthesis task on ShapeNet objects.}} % Caption for the figure
  \label{fig:nerf-supp-shapenet}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[width=1\textwidth]{Figures/comparsion_vnp.pdf} % Adjust the size and filename as needed
  \caption{\textbf{Comparison between the proposed method and VNP} on novel view synthesis task for ShapeNet objects. Our method has a better rendering quality than VNP for novel views.} % Caption for the figure
  \label{fig:nerf-supp-compare}
\end{figure}

\begin{figure}[htbp]
  \centering
\includegraphics[width=0.8\textwidth]{Figures/imagenette-more.png} % Adjust the size and filename as needed
  \caption{\textbf{More image regression results on the Imagenette dataset.} Left: ground truth; Right: prediction.} % Caption for the figure
  \label{fig:image-supp-image}
  % \vspace{-3mm}
\end{figure}

\subsection{Training Time Comparison}

\textcolor{blue}{As illustrated in Fig.\ref{fig:train-time}, with the same training time, our method (GeomNP) demonstrates faster convergence and higher final PSNR compared to the baseline (VNP). }

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{ICLR2025/Figures/train_time_psnr.png} % Adjust the size and filename as needed
  \caption{\textbf{Training time vs. PSNR on the ShapeNet Car dataset.} Our method (GeomNP) demonstrates faster convergence and higher final PSNR compared to the baseline (VNP).} % Caption for the figure
  \label{fig:train-time}
  % \vspace{-3mm}
\end{figure}






\subsection{Qualitative ablation of the hierarchical latent variables}
\label{sec:abl-bases}
\textcolor{blue}{In this section, we perform a qualitative ablation study on the hierarchical latent variables. As illustrated in Fig.~\ref{fig:hier-abl}, the absence of the global variable prevents the model from accurately predicting the object's outline, whereas the local variable captures fine-grained details. When both global and local variables are incorporated, GeomNP successfully estimates the novel view with high accuracy.}



\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{./Figures/hierarchical-ablation-new.pdf} % Adjust the size and filename as needed
  \caption{\textbf{Qualitative ablation of the hierarchical latent variables (global and local variables)}. }  % Caption for the figure
  \label{fig:hier-abl}
  % \vspace{-3mm}
\end{figure}



\subsection{More multi-view reconstruction results}
\textcolor{blue}{We integrate our method into GNT~\citep{wang2022attention} framework and perform experiments on the Drums class of the NeRF synthetic dataset. Qualitative comparisons of multi-view results are presented in Fig.~\ref{fig:qua-nerf-syn}. }

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{ICLR2025/Figures/nerf-syn.pdf} % Adjust the size and filename as needed
  \caption{\textbf{Qualitative comparisons of Multi-view results on the Drums class of the NeRF synthetic dataset. } }  % Caption for the figure
  \label{fig:qua-nerf-syn}
  % \vspace{-3mm}
\end{figure}

% \section{NP with Gaussian Splatting}
% \begin{table}[htbp]
%     \centering
%     \caption{Comparison of methods}
%     \begin{tabular}{lccc}
%         \toprule
%         \textbf{Method} & \textbf{PSNR $\uparrow$} & \textbf{SSIM $\uparrow$} & \textbf{LPIPS $\downarrow$} \\
%         \midrule
%         PixelNeRF & 21.76 & 0.78 & 0.203 \\
%         {Splatter Image} & 21.80 & {0.80} & {0.150} \\
%         \bottomrule
%     \end{tabular}
% \end{table}


% Recently, 3D Gaussian Splatting~\citep{kerbl20233d} has gained significant attention for its efficiency and strong performance in reconstructing 3D scenes. Like NeRF, Gaussian Splatting requires overfitting on a specific scene to optimize the 3D Gaussian parameters. To improve generalization, given a single-view context image, Splatter Image~\citep{szymanowicz2024splatter} employs a UNet to predict Gaussian parameters for a new scene. However, Splatter Image remains a deterministic method and does not account for scene uncertainty. Therefore, in this section, we demonstrate that integrating neural processes can enhance Splatter Image's performance.

% Specifically, we employ the UNet encoder to generate a latent variable, and then sample a scene-specific latent vector to estimate Gaussian parameters through the decoder. For multiple-view ($N$) images, we first aggregate multi-view latent features and then infer the latent variables (mean and variance). We sample $N$ times to probabilistically estimate the Gaussian parameters. The prior and posterior distributions are derived from context and target images, respectively. In addition to the original reconstruction loss, we introduce a KL divergence constraint between the prior and posterior distributions, guiding the model to achieve richer representation with limited observations.

% Experiments are conducted on the CO3D dataset. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Image Regression}
% \label{sec:image-regression}



% \begin{figure}[t]
%     \centering
%     \begin{subtable}[b]{0.42\textwidth}
%     % \vspace{-6mm}
%     \begin{tabular}{lcc}
%     \toprule
%                  & CelebA & Imagenette \\ \midrule
%     Learned Init \citep{tancik2021learned} & 30.37  & 27.07       \\
%     TransINR~\citep{chen2022transformers}         & 31.96  & 29.01       \\ 
%     % \hline
%     \rowcolor{lightblue}
%     \method{} (Ours)         & \textbf{33.41}  & \textbf{29.82}      \\ 
%     \bottomrule
%     \end{tabular}
%     \caption{Quantitative results. \method{} outperforms baseline methods consistently on both datasets.}
%     \label{tab:image-regression}
%     \end{subtable} \hfill
%     \begin{subtable}[b]{0.54\textwidth}
%     \includegraphics[width=\textwidth]{Figures/image-regression0.pdf} % Adjust the size and filename as needed
%     \caption{Visualizations on CelebA (left) and Imagenette (right), respectively.} % Caption for the figure
%     \label{fig:visualization-image-regression}
% \end{subtable}
% % \vspace{-2mm}
% \caption{\textbf{Quantitative results and visualizations} of image regression on CelebA and Imagenette.}
% \vspace{-3mm}
% \end{figure}

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.99\textwidth]{Figures/image-completion.pdf} % Adjust the size and filename as needed
%   \vspace{-3mm}
%   \caption{\textbf{Image completion visualization} on CelebA using $10\%$ (left) and $20\%$ (right) context.}
%   \label{fig:completion}
%   \vspace{-3mm}
% \end{figure}


% \noindent{\textbf{Setup.}} Image regression is a common task used for evaluating INRs' capacity of representing a signal~\citep{tancik2021learned,sitzmann2020implicit}. 
% We employ two real-world image datasets as used in previous works~\citep{chen2022transformers,tancik2021learned,gu2023generalizable}. The CelebA dataset~\citep{liu2015deep} encompasses approximately 202,000 images of celebrities, partitioned into training (162,000 images), validation (20,000 images), and test (20,000 images) sets. The Imagenette dataset~\citep{imagenette}, a curated subset comprising 10 classes from the 1,000 classes in ImageNet~\citep{deng2009imagenet}, consists of roughly 9,000 training images and 4,000 testing images. In order to compare with previous methods, we conduct image regression experiments. The context set is an image and the task is to learn an implicit function that regresses the image pixels well in terms of PSNR.
% %\str{What is the point of image regresssion when the image itself is used as context? Why not just copy the context??}
% %\str{The following is a bit strante, is this still image regression? Why not have a separate section?}
% %\str{Is Image Regression the standard name for this task?}


% \noindent{\textbf{Implementation Details.}} 
% Following TransINR~\citep{chen2022transformers}, we resize each image into $178\times 178$, and use patch size 9 for the tokenizer. The self-attention module remains the same as the one in the NeRF experiments (Sec. \ref{sec:nerf-results}). For the Gaussian bases, we predict the 2D Gaussians instead of the 3D. 
% The hierarchical latent variables are inferred in image-level and pixel-level. 





% %The self-attention and global variable remain the same as the one in the NeRF experiments. %We do not use the pixel variable modulation for the computation concern. However, our method still has competitive performance.   

% % \begin{table}[t]
% % \centering
% % \vspace{-6mm}
% % \caption{Quantitative results of image regression.}
% % \label{tab:image-regression}
% % \begin{tabular}{lcc}
% % \toprule
% %              & CelebA & Imagenette \\ \midrule
% % Learned Init \citep{tancik2021learned} & 30.37  & 27.07       \\
% % TransINR~\citep{chen2022transformers}         & 31.96  & 29.01       \\ 
% % \hline
% % \method{} (Ours)         & \textbf{33.41}  & \textbf{29.82}      \\ 
% % \bottomrule
% % \end{tabular}
% % \end{table}



% \noindent{\textbf{Results.}} The quantitative comparison of \method{} for representing the 2D image signals is presented in Table~\ref{tab:image-regression}. \method{} outperforms the baseline methods on both CelebA and Imagenette datasets significantly, showing better generalization ability and representation capacity than baselines. 
% %Note that the Imagenette is a more diverse dataset than the CelebA. The better performance shows that. 
% Fig.~\ref{fig:visualization-image-regression} shows the ability of \method{} to recover the high-frequency details for image regression.
% %regress the image closely to the ground truth, indicating the capability of capturing the detailed texture information. 


% \noindent {\textbf{Image Completion Visualization.}} We also conduct experiments of \method{} on image completion (also called image inpainting), which is a more challenging variant of image regression. Essentially, only part of the pixels are given as context, while the INR functions are required to complete the full image. Visualizations in Fig.~\ref{fig:completion} demonstrate the generalization ability of our method to recover realistic images with fine details based on very limited context ($10 \% - 20\%$ pixels). %, 




% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.99\textwidth]{Figures/image-completion.pdf} % Adjust the size and filename as needed
%   \vspace{-3mm}
%   \caption{\textbf{Image completion visualization} on CelebA using $10\%$ (left) and $20\%$ (right) context.}
%   \label{fig:completion}
%   \vspace{-3mm}
% \end{figure}



% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=1\textwidth]{Figures/image-regression-basis.pdf} % Adjust the size and filename as needed
%   \caption{\textbf{Visualization of geometric bases (Gaussian)} on the context image, which reveals the structure of the object.} % Caption for the figure
%   \label{fig:visualization}
%   \vspace{-5mm}
% \end{figure}


% \noindent{\textbf{Visualization of Geometric Bases.}}
% Moreover, we also visualize the learned Gaussian bases on the image regression task. As shown in Fig. \ref{fig:visualization}, the bases are more concentrated on the objects and complex backgrounds in the image, while sparse on the simple complex. The visualizations indicate that the geometric bases do encode structure information from the context data.




