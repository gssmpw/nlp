\begin{figure*}[t]
    \centering
    \resizebox{0.75\textwidth}{!}{ % Adjust size while maintaining aspect ratio
        \includegraphics{Figures/GNP.pdf}
    }
    \caption{\textbf{Illustration of the proposed \name{}.} }
    %\vspace{-3.5mm}
    \label{fig:graphical_model}
\end{figure*}



Despite their great reconstruction capabilities, Neural Fields are still limited by their lack of generalization. 
While Conditional Neural Fields offer an interesting path forward, they still suffer from the unconstrained nature of stochastic gradient descent and the over-parameterized nature of neural networks~\citep{papa2023train}, thus making representation learning and generalization to few-shot settings hard, whether for 1-D (e.g., for time series), 2-D (e.g., for PINNs), or 3-D (e.g., for occupancy and radiance fields) data.
We alleviate this by imposing geometric and hierarchical structure to the NeF and NeRF functions in 1-, 2-, or 3-D data, such that Neural Fields are constrained to the types of outputs that they predict.
Further, we embed Conditional Neural Fields in a probabilistic learning framework using Neural Processes, so that the learned Neural Fields generalize well even with few-shot context data settings.

\subsection{Probabilistic Neural Process Fields}
%\wy{change example to 2D. Mention extension to nerf in appendix.}
% Conditional Neural Fields, defined in a deterministic setting, bear direct resemblance to Neural Processes and Gaussian Processes and their context and target sets, defined in a probabilistic setting. 
% To make the point clearer, we will use as a running example the modeling of the 3D shape without color texture (that is, $y_C$ and $y_T$ are either 0 or 1, to keep things simple).
Conditional Neural Fields, defined in a deterministic setting, bear direct resemblance to Neural Processes and Gaussian Processes and their context and target sets, defined in a probabilistic setting. To make the point clearer, we will use the 2D image completion task as a running example, where the goal is to reconstruct an entire image from a sparse set of observed pixels (an occluded image). 

In image completion task, the \(
\mathcal{C} 
= \{(x_{C,n},\, y_{C,n})\}_{n=1}^{N}
\) consists of $N$ observed pixel coordinates $x_C$ and their corresponding intensity values $y_C$, while the {target set} $\mathcal{T} = \{x_T\}$ comprises all $M$ pixel coordinates in the image, with $y_T$ denoting the unobserved intensities to be predicted. The objective is thus to infer the full image $y_T$ conditioned on $\mathcal{C}$, effectively regressing pixel intensities across the entire spatial domain using only the sparse context observations. Although our approach is formulated as a general probabilistic framework, we present a novel {3D-specific extension} for {Neural Radiance Fields}, detailed in {Appendix~\ref{sec:appendix-neural-radiance-fields}}.


%Assume that we want to model the 3D shape of an object given a set of 2D views of the object.
%Namely, we want to know for which (target) coordinates $x_T$ in the world, the space is occupied by our object, that is $y_T=1$.
%In this case, our context set $C$ comprises sparse input coordinates (and optionally camera directions), and output surface variables collected from a few observations only.
%In simple words, the objective is to reconstruct the entire object shape using as context only these few observations.

For probabilistic Neural Process Fields, we adopt the Neural Process decomposition from Eq.~\eqref{eq:np} for prior distribution,
\begin{align}
&p(y_T | x_T, x_C, y_C) = \label{eq:prob-nef} \\
=& \int \underbrace{p(y_T | z, x_T, x_C, y_C)}_{\text{Conditional Neural Field}} p(z | x_T, x_C, y_C) d z \nonumber \\
=& \int \prod_{m=1}^M p(y_{T, m} | z, x_{T, m}, x_C, y_C) p(z | x_{T, m}, x_C, y_C) d z, \nonumber
\end{align}
%
where in the last line of Eq.~\eqref{eq:prob-nef} we use the fact that the $M$ target output variables, which comprise the target object, are conditionally independent with respect to the latent variable $z$.
In probabilistic Neural Process Fields, \( z \) encodes object-level information, similar to the object-specific parameters \( \omega_i \) in deterministic Conditional Neural Fields. However, by modeling \( z \) probabilistically, our approach enables generalization across different objects, whereas standard NeFs are limited to fitting a single object at a time.





\subsection{Adding Geometric Priors to Probabilistic Neural Process Fields}

With probabilistic Neural Process Fields, we are able to generalize conditional Neural Fields to account for uncertainty and thus be more robust to smaller training datasets and few-shot learning settings.
Given that (conditional) Neural Fields are typically implemented as standard MLPs, they do not pertain to a specific structure in their output nor are they constrained in the type of values they can predict.
This lack of constraints can have a detrimental impact on the generalization of the learned models, especially when considering Neural Radiance Fields, for which one must also make sure that there is consistency between the 2D observations and the 3D shape of the object.

To address this problem, we propose adding geometric priors to probabilistic Neural Process Fields.
Specifically, we encode the context set $\mathcal{C}$ so that to represent it in terms of structured geometric bases $B_C = \big\{b \big\}_{r=1}^{R}$, rather than using $\mathcal{C}$ directly. Here $R$ is the number of bases. 
These geometric bases must create an information bottleneck through which we embed structure to the context set $\mathcal{C}$, thus $R \ll \|\mathcal{C}\| = N$.
Each geometric basis $b_r= \Big( \mathcal{N}(\mu_r, \Sigma_r), \omega_r\Big)$ contains a Gaussian distribution $\mathcal{N}$ in the 2D spatial plane with covariance $\Sigma_r$, centered around a 2D coordinate $\mu_r$. Note that when extending to the 3D data, $\mathcal{N}$ is a 3D Gaussian. 
Each geometric basis also contains a representation variable $\omega_r$, learned jointly to encode the semantics around the location of $\mu_r$.
The probabilistic Neural Process Field in Eq.~\eqref{eq:prob-nef} becomes
%
\begin{align}
p(y_T | x_T, B_C) = \int \underbrace{p(y_T | z, x_T, B_C)}_{\substack{\text{Geometric Priors on} \\ \text{ Conditional Neural Fields}}} p(z | x_T, B_C) d z
\label{eq:prob-nef-geom}
\end{align}
%

%\textcolor{blue}{Standard MLP-based Conditional Neural Fields impose few constraints on their outputs, limiting their generalization in tasks such as 3D reconstruction or Neural Radiance Fields, where spatial consistency is crucial. We address this by introducing a small set of learned \emph{geometric basis functions}:
% \[
% B_C = \bigl\{ (\mu_r, \Sigma_r, \omega_r)\bigr\}_{r=1}^{R}, 
% \quad R \ll |\mathcal{C}|.
% \]
% Each basis function includes a Gaussian in \(\mathbb{R}^3\) with mean \(\mu_r\) and covariance \(\Sigma_r\), plus a learned embedding \(\omega_r\). This acts as an information bottleneck that encodes spatial structure.
% }




\subsection{Adding Hierarchical Priors to Probabilistic Neural Process Fields}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{./Figures/graphical_model_new.pdf} 
\caption{\textbf{Graphical model for the proposed geometric neural processes.}}
%\vspace{-6mm}
\label{fig: graphical_model}
\end{figure}


The decomposition in Eq.~\eqref{eq:prob-nef} conditioning on the latent $z$ allows generalizing conditional Neural Fields with uncertainty to arbitrary training sets, especially by introducing geometric priors in Eq.~\eqref{eq:prob-nef-geom}.
We note, however, that when learning probabilistic Neural Fields, our training must serve two slightly conflicting objectives.
On one hand, the latent variable encodes the global appearance and geometry of the target object at $x_T, y_T$.
On the other hand, the Neural Fields are inherently local, in that their inferences are coordinate-specific.

To ease the tension, we introduce hierarchical latent variables, having a single global latent variable $z_g$, and $M$ local latent variables $\{z_{l,m}\}_{m=1}^M$ for the $M$ target points $x_T$, to condition the probabilistic Neural Process Fields. A graphical model of our method is provided in Fig.~\ref{fig: graphical_model}.
%
% \begin{align}
% &p(y_T | x_T, B_C) = \nonumber \\
% &= \int p(z_g | x_T, B_C) \int \underbrace{p(y_T | z_g, z_l, x_T, B_C)}_{\substack{\text{Hierarchical Priors on} \\ \text{ Conditional Neural Fields}}} \dots \nonumber \\
% &\dots p(z_l | z_g, x_T, B_C) \;\; d z_l \; d z_g \label{eq:prob-nef-hier-1} \\
% &= \int p(z_g | x_T, B_C) \prod_m \int \underbrace{p(y_{T, m} | z_g, z_{l, m}, x_{T, m}, B_C)}_{\substack{\text{Hierarchical Priors on} \\ \text{ Conditional Neural Fields}}} \dots \nonumber \\
% &\dots p(z_{l, m} | z_g, x_T, B_C) \;\; d z_{l, m} \; d z_g. \label{eq:prob-nef-hier-2}
% \end{align}
\vspace{-1mm}
\begin{align}
&p(y_T | x_T, B_C)  \nonumber \\
&= \int  \int \underbrace{p(y_T | z_g, z_l, x_T, B_C)}_{\substack{\text{Hierarchical Priors on} \\ \text{ Conditional Neural Fields}}}  \nonumber  p(z_l | z_g, x_T, B_C) \;\; d z_l \; \dots \\
&\dots p(z_g | x_T, B_C) \; d z_g \label{eq:prob-nef-hier-1} \\
&= \int \prod_m \int \underbrace{p(y_{T, m} | z_g, z_{l, m}, x_{T, m}, B_C)} \dots \nonumber  \\
&\dots p(z_{l, m} | z_g, x_T, B_C) \;\; d z_{l, m} \; p(z_g | x_T, B_C) d z_g. \label{eq:prob-nef-hier-2}
\end{align}

%
In Eq.~\eqref{eq:prob-nef-hier-1}, we bring $p(z_g | x_T, B_C)$ out of the inside integral, which marginalizes over the local latent variables $z_l$.
In Eq.~\eqref{eq:prob-nef-hier-2}, we further decompose by using the fact that the target variables $y_{T, m}$ and the local latent variables $z_{l, m}$ are conditionally independent. 

\subsection{Implementation}

We next describe the implementation of all individual components, and refer to the Appendix~\ref{sec:implementation-details}  for the full details.

\paragraph{Geometric basis functions.} We implement the geometric basis functions using a transformer encoder, $ \Big(\mu, \Sigma, \omega \Big)_r = \texttt{Encoder} [x_C, y_C]$.
In $p(z_{l, m} | z_g, x_T, B_C)$ of Eq.~\eqref{eq:prob-nef-hier-2}, the prior distribution of each hierarchical latent variable is conditioned on the geometric bases $B_C$ and target inputs $x_T$. 
Since the geometric basis functions rely on Gaussians, we use an MLP with a Gaussian radial basis function to measure their interaction, that is
%
\begin{equation}
\begin{aligned}
    &\langle x_T, B_C \rangle = \\
    &\texttt{MLP}\Big[\sum_{r=1}^R \exp (-\frac{1}{2}(x_T-\mu_r)^T \Sigma_r^{-1}(x_T-\mu_r) ) \cdot \omega_r\Big],
\label{eq:rbf_agg}
\end{aligned} 
\end{equation}
%

\paragraph{Global latent variables.}
%\str{Can you please write this and the next paragraph in a similar style as above? Avoid over-complicated descriptions.}
We model the global latent variable \( z_g \) as a Gaussian distribution:  
\begin{equation}
        \big( \mu_g, \sigma_g \big)
    = \texttt{MLP}\left(\frac{1}{M}\sum_{m=1}^M 
    \langle x_T, B_C \rangle \right),
\label{eq:global-eq}
\end{equation}

where \( p(z_g | x_T, B_C) \) is parameterized by a Gaussian whose mean \( \mu_g \) and variance \( \sigma_g \) are generated via an \texttt{MLP}. Eq~\eqref{eq:global-eq} aggregates representations across all target points to produce a global latent variable \( z_g \), thereby parameterizing the underlying object or scene. This formulation enables our model to capture object-specific uncertainty through the inferred distribution of \( z_g \).

% \paragraph{Local latent variables.}
% \str{Same here.}
% To generate the distribution of the ray-specific latent variable, we first average the location representations ray-wisely. 
% We then obtain the ray-specific latent variable by aggregating the averaged location representation and the object latent variable through a lightweight transformer. We formulate the inference of the ray-specific latent variable as:
% \begin{equation}
%     [\mu_{{r}}, \sigma_{{r}}] = \texttt{Transformer} \Big[\texttt{MLP}[\frac{1}{P}\sum_{\mathbf{r}}
%     <{\bf{x}}_{T}^{n}, {\bf{B}}_C >]; \hat{{\bf{z}}}_o \Big],
% \end{equation}
% where $\hat{{\bf{z}}}_o$ is a sample from the prior distribution $p({\bf{z}}_o | {\bf{X}}_T, {\bf{B}}_C)$. 
% Similar to the object-specific latent variable, we also assume the distribution $p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C)$ is a mean-field Gaussian distribution with the mean $\mu_{{r}}$ and variance $\sigma_{{r}}$. We provide more details of the latent variables in Appendix~\ref{supp:latent-variables}.


\paragraph{Local latent variables.}
To infer the distribution of the local latent variables \( {z}_l \), we first compute the position-aware representation \( \langle \mathbf{x}_{T,m}, {B}_C \rangle \) for each target point \( {x}_{T,m} \) using Eq~\eqref{eq:rbf_agg}. 
The local latent variable \( {z}_{l,m} \) is then derived by combining these representations with the global latent variable \( {z}_g \) via a transformer:
\[
    \big( \mu_{l}, \sigma_{l} \big)  
    = \texttt{Transformer}\left( \texttt{MLP}\left[ \langle {x}_{T,m}, {B}_C \rangle \right]; \hat{{z}}_g \right),
\]
where \( \hat{{z}}_g \) is a sample from the global prior distribution \( p({z}_g \mid {x}_T, {B}_C) \). 
Mirroring the global latent variable \( {z}_g \), we model the local prior distribution \( p({z}_{l,m} \mid {z}_g, {x}_{T,m}, {B}_C) \) as a mean-field Gaussian with parameters \( \mu_{l} \) and \( \sigma_{l} \). This hierarchical structure enables coordinate-specific uncertainty modeling while preserving global geometric consistency. Full architectural details are provided in Appendix~\ref{supp:latent-variables}.

\paragraph{Predictive distribution.}
%\str{Same here, what was with modulation.}
The hierarchical latent variables \( \{{z}_g, {z}_{l,m}\} \) condition the neural network to generate predictions that integrate global and local geometric uncertainty. Specifically, the neural field is conditioned jointly on the global latent variable \( {z}_g \), which encodes object-level structure, and the local latent variables \( {z}_{l,m} \), which capture coordinate-specific variations. The predictive distribution \( p({y}_T \mid {x}_T, {B}_C) \) is obtained by propagating each target coordinate \( {x}_{T,m} \) through the neural network, parameterized by \( {z}_g \) and \( {z}_{l,m} \), to model the distribution of outputs \( {y}_{T,m} \). This process directly leverages the hierarchical prior distributions defined in Eq~\eqref{eq:prob-nef-hier-2}, ensuring consistency across scales. Implementation details of the conditioned network are provided in Appendix~\ref{supp:modulate}.  

\subsection{Training objective}

To optimize the proposed \name{},
we apply variational inference~\citep{garnelo2018neural} to derive the evidence lower bound (ELBO). Specifically, we first introduce the hierarchical variational posterior:
\begin{equation}
\begin{aligned}
& q\bigl(z_g, \{z_{l,m}\}\mid x_T, B_T\bigr)
\,=\, \\
&\prod_{m=1}^M
q\bigl(z_{l,m} \mid z_g, x_{T,m}, B_T\bigr) \, q\bigl(z_g \mid x_T, B_T\bigr),
\end{aligned}
\end{equation}
where $B_T$ are target set-derived bases (available only at training). The variational posteriors are
inferred from the target set $\mathcal{T}$ during training with the same encoder, which introduces more information on the object. The
prior distributions are supervised by the variational posterior using Kullbackâ€“Leibler (KL) divergence,
learning to model more object information with limited context data and generalize to new scenes. The details about the evidence lower bound (ELBO) and derivation are provided in the Appendix~\ref{sec:elbo-general}

Finally, the training objective combines reconstruction, hierarchical latent alignment, and geometric basis regularization:
\begin{equation}
\begin{aligned}
\mathcal{L} = & \, || {y}_T - {y}_T'||^2_2 + \alpha \Big( D_{\text{KL}}\big[p({z}_g | {B}_C) \,\big|\big|\, q({z}_g | {B}_T)\big] \\
& + \sum_{m=1}^M D_{\text{KL}}\big[p({z}_{l,m} | {z}_g, {B}_C) \,\big|\big|\, q({z}_{l,m} | {z}_g, {B}_T)\big] \Big) \\
& + \beta \cdot D_{\text{KL}}\big[{B}_C \,\big|\big|\, {B}_T\big],
\end{aligned}
\end{equation}
where \( y_T' \) denotes predictions, and \( \alpha \), \( \beta \) balance the terms. 
The first term enforces local reconstruction quality, while the second ensures that the prior distributions 
are guided by the variational posterior using the Kullback-Leibler (KL) divergence. 
The third term, the KL divergence, aligns the spatial distributions of \( B_C \) and \( B_T \), 
ensuring that the context bases capture the target geometry.

\subsection{\name{} in 1D, 2D, 3D}
The proposed method generalizes seamlessly to 1D, 2D, and 3D signals by leveraging Gaussian structures of corresponding dimensionality. A single global variable consistently encodes the entire signal (e.g., a 3D object or a 2D image), ensuring unified representation. For local variables, we adopt a dimension-specific formulation: in 1D and 2D signals, local variables are associated with individual spatial locations; while in 3D radiance fields, we developed a mechanism where a unique local variable is assigned to each camera ray, detailed in {Appendix~\ref{sec:appendix-neural-radiance-fields}}. This design preserves both global coherence and local adaptability across signals. 

%\str{Explain in simple words how the model is adapted for each case.}

% In the modeling of {\name{}}, the prior distribution of each hierarchical latent variable is conditioned on the geometric bases and target input. 
% %\textcolor{blue}{To infer each latent variable, we first integrate the geometric bases and each target input, yielding a location representation specific to the target input. The location representation has access to relevant locality information from the geometry bases, \textit{i.e.}, $<{\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C >$. }
% % For generalization, we need to infer latent variables that are specific to the target input. 
% % To this end, 
% We first represent each target location by integrating the geometric bases, \textit{i.e.}, $<{\bf{x}}_{T}^{n}, {\bf{B}}_C >$, which aggregates the relevant locality and semantic information for the given input. 
% Since ${\bf{B}}_{C}$ contains $M$ Gaussians, we employ a Gaussian radial basis function in \cref{eq:rbf_agg} between each target input ${\bf{x}}_{T}^{ n}$ and each geometric basis ${\bf{b}}_i$ to aggregate the structural and semantic information to the 3D location representation. Thus, we obtain the 3D location representation as follows:
% \begin{equation}
% \label{eq:rbf_agg}
%     <{\bf{x}}_{T}^{n}, {\bf{B}}_C > = \texttt{MLP}\Big[\sum_i^{M} \exp (-\frac{1}{2}({\bf{x}}_{T}^{n}-\mu_i)^T\Sigma_i^{-1}({\bf{x}}_{T}^{n}-\mu_i) ) \cdot \omega_i\Big],
% \end{equation} 
% where $\texttt{MLP}[\cdot]$ is a learnable neural network.
% With the location representation $<{\bf{x}}_{T}^{n}, {\bf{B}}_C >$, we next infer each latent variable hierarchically, in object and ray levels. 

% \noindent {\textbf{Object-specific Latent Variable.}} The distribution of the object-specific latent variable ${\bf{z}}_o$ is obtained by aggregating all location representations:
% \begin{equation}
%     [\mu_{{o}}, \sigma_{{o}}] 
%     = \texttt{MLP}\Big[\frac{1}{N \times P}\sum_{n = 1}^{N}\sum_{\mathbf{r}}
%     <{\bf{x}}_{T}^{n}, {\bf{B}}_C >\Big],
% \end{equation} 
% where we assume $p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)$ is a standard Gaussian distribution and generate its mean $\mu_{o}$ and variance $\sigma_{o}$ by a ~\texttt{MLP}. 
% Thus, our model captures objective-specific uncertainty in the NeRF function.


% \noindent {\textbf{Ray-specific Latent Variable.}} 
% % By ray-specific latent variable, the object-specific is expected to capture the local details.
% To generate the distribution of the ray-specific latent variable, we first average the location representations ray-wisely. 
% We then obtain the ray-specific latent variable by aggregating the averaged location representation and the object latent variable through a lightweight transformer. We formulate the inference of the ray-specific latent variable as:
% \begin{equation}
%     [\mu_{{r}}, \sigma_{{r}}] = \texttt{Transformer} \Big[\texttt{MLP}[\frac{1}{P}\sum_{\mathbf{r}}
%     <{\bf{x}}_{T}^{n}, {\bf{B}}_C >]; \hat{{\bf{z}}}_o \Big],
% \end{equation}
% where $\hat{{\bf{z}}}_o$ is a sample from the prior distribution $p({\bf{z}}_o | {\bf{X}}_T, {\bf{B}}_C)$. 
% Similar to the object-specific latent variable, we also assume the distribution $p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C)$ is a mean-field Gaussian distribution with the mean $\mu_{{r}}$ and variance $\sigma_{{r}}$. We provide more details of the latent variables in Appendix~\ref{supp:latent-variables}.

% The detail of the used Transformer is given in the Appendix. In summary, our latent variable is in a hierarchical structure. 
% The location-specific latent variable $r_C$ is to modulate the NeRF function specific to the queried ray (or pixel location in 2D image). We infer a ray-specific variable $r_c$ by first aggregating the feature ray-wisely:
% \begin{equation}
%     r'_c = \text{MLP}(\frac{1}{ P}\sum_j^{P}(\bf{X}'_T)[j])
% \end{equation}


%Then, we perform a hierarchical formulation by incorporating the global variable to obtain the ray-specific variable. This enables us to design the latent variable in a hierarchical structure, which is rational as it learns the scene coarse-to-fine. 

% Then, $r_c$ is obtained by feeding $r'_c$ together with the sampled $\hat{g}_c$ into a Transformer: 
% \begin{equation}
%     r_c = [\mu^c_r, \sigma^c_r] = \text{Transformer}([\hat{g}_c; \hat{r}_C]),
% \end{equation}
% where $\mu^r_g$ and $\sigma^r_g$ are the mean and variance of global variables, respectively. The detail of the used Transformer is given in the Appendix. In summary, our latent variable is in a hierarchical structure. 

% \noindent  \textbf{NeRF Function Modulation.}
% With the hierarchical latent variables $\{{\bf{z}}_o, {\bf{z}}_r^n\}$, we modulate a neural network for a 3D object in both object-specific and ray-specific levels.  Specifically, the modulation of each layer is achieved by scaling its weight matrix with a style vector~\citep{guo2023versatile}. 
% The object-specific latent variable ${\bf{z}}_o$ and ray-specific latent variable ${\bf{z}}_r^n$ are taken as style vectors of the low-level layers and high-level layers, respectively. The prediction distribution $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C})$ are finally obtained by passing each location representation through the modulated neural network for the NeRF function. 
% More details are provided in Appendix~\ref{supp:modulate}. 

% \begin{equation}
% \begin{aligned}
%         p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) &= \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o ) \\
%         &p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o,
% \end{aligned}
% \label{eq:ganp-model}
% \end{equation}


% With the geometric bases, we propose Geometric Neural Processes (\textbf{\name{}}) by inferring the NeRF function distribution $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$ in a probabilistic way.  
% % We can generalize NeRF learning and efficiently adapt the functional distribution to new 3D objects.
% Based on the probabilistic NeRF generalization in~\cref{eq: probabilitic_NeRF_generalization}, we introduce hierarchical latent variables to encode various spatial-specific information into $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$, improving the generalization ability in different spatial levels.
% %\str{Make sure that notation is consistent and not overloaded. Eg, $x^r$ rather than $x^\mathbf{r}$ since it is not that we use the $1:P$ somewhere specific, besides it is not clear that this corresponds to a ray, since the $P$ points could be anywhere.}
% Since all rays are independent of each other, we decompose the predictive distribution in \cref{eq: predictive_w_B} as:
% \begin{equation}
%     p({\bf{Y}}_{T}| {\bf{X}}_{T},  {\bf{B}}_{C})  = \prod_{n=1}^{N} p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{{\mathbf{r}}, n},  {\bf{B}}_{C}),
% \label{eq: predictive_distribution_ray_specific}
% \end{equation}
% where the target input ${\bf{X}}_{T}$ consists of $N \times P$ location points $\{{\bf{x}}_{T}^{{\mathbf{r}}, n}\}_{n=1}^{N}$ for $N$ rays.


% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.9\columnwidth]{ICLR2025/Figures/graphical_model2.pdf} 
% \caption{\textbf{Graphical model for the proposed geometric neural processes.}}
% \label{fig: graphical_model}
% \end{figure}

% Further, we develop a hierarchical Bayes framework for \name{} to accommodate the data structure of the target input ${\bf{X}}_{T}$ in \cref{eq: predictive_distribution_ray_specific}.
% We introduce an object-specific latent variable $\mathbf{z}_o$ and $N$ individual ray-specific latent variables $\{\mathbf{z}_r^{n}\}_{n=1}^{N}$ to represent the randomness of $f_\text{NeRF}$.
% % the probabilistic NeRF function. 


% \str{The formatting here looks weird. Is this the right template?}
% Within the hierarchical Bayes framework, $\mathbf{z}_o$ encodes the entire object information from all target inputs and the geometric bases $\{\mathbf{X}_T, \mathbf{B}_C\}$ in the global level; while every $\mathbf{z}_r^{n}$ encodes ray-specific information from $\{ \mathbf{x}_T^{\mathbf{r}, n}, \mathbf{B}_C\}$ in the local level, which is also conditioned on the global latent variable $\mathbf{z}_o$. 
% The hierarchical architecture allows the model to exploit the structure information from the geometric bases $\mathbf{B}_C$ in different levels, improving the model's expressiveness ability.
% By introducing the hierarchical latent variables in \cref{eq: predictive_distribution_ray_specific}, we model \name{} as:
% % \begin{equation}
% % \small
% %         p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) = \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o, ) p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o, 
% % \label{eq:ganp-model}
% % \end{equation}
% {\small
% \begin{equation}
% \begin{aligned}
%         p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) &= \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o ) \\
%         &p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o,
% \end{aligned}
% \label{eq:ganp-model}
% \end{equation}
% }where $p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_o, {\bf{z}}_r^i)$ denotes the ray-specific likelihood term. In this term, we use the hierarchical latent variables $\{{\bf{z}}_o, {\bf{z}}_r^i\}$ to modulate a ray-specific NeRF function $f_{\text{NeRF}}$ for prediction, as shown in Fig.~\ref{fig: framework}.
% % In general, we first use the object-specific latent variable $\mathbf{z}_o$ to make $f_{NeRF}$ object-specific. Then, the ray-specific latent variable $\mathbf{z}_r$ to enable $f_{\text{NeRF}}$ to capture the local texture information.
% Hence, $f_{\text{NeRF}}$ can explore global information of the entire object and local information of each specific ray, leading to better generalization ability on new scenes and new views.
% A graphical model of our method is provided in Fig.~\ref{fig: graphical_model}. 


% and a semantic representation, \textit{i.e.,} ${\bf{b}}_i = \{ \mathcal{N}(\mu_i, \Sigma_i); \omega_i\}$, 
% %\str{What is $\omega_i$ in the equation? The weight of the Gaussian?We have mixtures of Gaussians? Please clarify.} 
% where $\mu_i$ and $\Sigma_i$ are the mean and covariance matrix of $i$-th Gaussian in 3D space, and $\omega_i$ is its corresponding latent representation. 


% \paragraph{Misalignment between 2D context and 3D structures} It is worth mentioning that the predictive distribution in 3D space is conditioned on 2D context pixels with their ray $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ and 3D target points ${\bf{X}}_{T}$, which is challenging due to potential information misalignment. Thus, we need strong inductive biases with 3D structure information to ensure that 2D and 3D conditional information is fused reliably.




% To mitigate the information misalignment between 2D context views and 3D target points, we introduce geometric bases ${\bf{{B}}}_{C}=\{{\bf{b}}_i\}_{i=1}^{M}$, which {induces prior structure to the context set} $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ geometrically. $M$ is the number of geometric bases. 

% \begin{figure*}[t]
%   \centering  \includegraphics[width=0.99\textwidth]{ICLR2025/Figures/architecture-0.pdf} % Adjust the size and filename as needed
%   % \vspace{-2mm}
% \caption{\textbf{Illustration of our Geometric Neural Processes.} 
% % We solve the problem of radiance field generalization by Neural Processes. 
% % captures uncertainty induced by few available observations.
% We cast radiance field generalization as a probabilistic modeling problem. Specifically, we first construct geometric bases ${\bf{B}}_C$ in 3D space from the 2D context sets ${\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}$ to model the 3D NeRF function (Section~\ref{sec: geometrybases}). We then infer the NeRF function by modulating a shared MLP through hierarchical latent variables ${\bf{z}}_{o}, {\bf{z}}_{r}$ and make predictions by the modulated MLP (Section~\ref{sec: hierar}). 
%   The posterior distributions of the latent variables are inferred from the target sets ${\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{Y}}}_{T}$, which supervises the priors during training (Section~\ref{sec: object}). 
%   } % Caption for the figure
%   \label{fig: framework}
%   %\vspace{-2mm}
% \end{figure*}

% \str{where is the semantic representation coming from? Self-supervised models? Or is it learned?}
% Each geometric basis consists of a Gaussian distribution in the 3D point space and a semantic representation, \textit{i.e.,} ${\bf{b}}_i = \{ \mathcal{N}(\mu_i, \Sigma_i); \omega_i\}$, 
% %\str{What is $\omega_i$ in the equation? The weight of the Gaussian?We have mixtures of Gaussians? Please clarify.} 
% where $\mu_i$ and $\Sigma_i$ are the mean and covariance matrix of $i$-th Gaussian in 3D space, and $\omega_i$ is its corresponding latent representation. 
% Intuitively, the mixture of all 3D Gaussian distributions implies the structure of the object, while $\omega_i$ stores the corresponding semantic information.
% % from a 2D context set, e.g., color and texture. 
% In practice, we use a transformer-based encoder to learn the Gaussian distributions and representations from the context sets, \textit{i.e.,} $\{(\mu_i, \Sigma_i, \omega_i)\} = \texttt{Encoder} [{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}]$. Detailed architecture of the encoder is provided in Appendix~\ref{supp:gaussian}. 

% \begin{equation}
%     {\bf{{B}}}_{C} = \{{\bf{b}}_i\}_{i=1}^{M}, {\bf{b}}_i=\{\mathcal{N}(\mu_i, \Sigma_i); \omega_i\},
%     \\
%      \mu_i, \Sigma_i, \omega_i = \texttt{Encoder} [{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}],
% \end{equation}
% where $M$ is the number of Gaussian bases. 

% where ${\bf{{B}}}_{C}$ are Gaussian bases inferred from the context views $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ with 3D structure information, \textit{i.e.,} 
% ${\bf{{B}}}_{C}=\texttt{Encoder}\Big({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\Big)$. 


% To address the information loss in the context views, we develop a geometry-aware prior distribution for the NeRF function. The geometry-aware prior integrates a set of geometry bases ${\bf{{B}}}_{C}$ and the target location points ${\bf{X}}_{T}$, which enrich the context sets with the {structure locality information}. 
% By doing so, we reformulate the prior distribution of the NeRF function as:
% \begin{equation}
%     p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{{B}}}_{C}), 
% \label{eq: prior_f}
% \end{equation}
% \str{Can a deterministic variable be part of a probabilistic expression?d}
% where ${\bf{{B}}}_{C}$ is a set of Gaussian bases inferred from the context views $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ with 3D structure information, \textit{i.e.,} ${\bf{{B}}}_{C}=\texttt{Encoder}[{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}]$. 
% Specifically, we construct ${\bf{{B}}}_{C}$ as:
% Geometry basis-agnostic ideas, like 4D scene tensor~\cite{chen2022tensorf}, and RBF kernels~\cite{chen2023neurbf} has been used in deterministic NeRF to store the 3D scene geometry and semantic information. This motivates us to use a set of geometry bases to represent both the geometry structure and semantic information of the scene. 
% We assume the space is spanned by a set of basis, with geometric shapes and high-dimensional representation. 
% The geometry basis ${\bf{B}}_C$ is given by a posterior distribution, $p_{\pi}( {\bf{{B}}}_{C}| {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$. This can be explained as the posterior knowledge (color and spatial location) of the scene when a human sees a view of a scene (context image). Then, we model the function distribution as:
% \begin{align}
%     &{\bf{{B}}}_{C} = \{{\bf{b}}_i\}_{i=1}^{M}, {\bf{b}}_i=\{\mathcal{N}(\mu_i, \Sigma_i); \omega_i\},
%     \label{eq: generation_B_1}
%     \\
%     & \mu_i, \Sigma_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}), \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
%     \label{eq: generation_B_2}
%     \\
%     & \omega_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
%     \label{eq: generation_B_3}
% \end{align}
% where $M$ is the number of the Gaussian bases. $\mu \mathbb \in {R}^3$ is the Gaussian center, $\Sigma \in  \mathbb{R}^{3\times 3}$ is the covariance matrix, and $\omega \in \mathbb{R}^{d_B}$ is the corresponding ${d_B}$-dimension semantic representation. 
%Each Gaussian basis represents a 3D Gaussian kernel and its corresponding semantic information. The shape of a Gaussian kernel can reflect a local object structure.
%For each kernel, \js{details here: we use a visual self-attention to estimate the mean $\mu \mathbb \in {R}^3$ and covariance matrix $\Sigma \in  \mathbb{R}^{3\times 3}$, and a corresponding ${d_B}$-dimension semantic representation $\omega \in \mathbb{R}^{d_B}$. }

% With the geometric bases $\mathbf{B}_C$, we review the predictive distribution from  $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ to $p({\bf{Y}}_{T}| {\bf{X}}_{T},{\bf{{B}}}_{C})$.  By inferring the function distribution $p(f_{\text{NeRF}})$, we reformulate the predictive distribution as: 
% \begin{equation}
%     % p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = 
%     p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C}) = \int p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T}) p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C}) df_{\text{NeRF}},
% \label{eq: predictive_w_B}
% \end{equation}
% where $p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C})$ is the prior distribution of the NeRF function, and $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ is the likelihood term. 
% % We integrate the likelihood term with all possible NeRF functions. 
% %\str{How do we do this? The space to integrate over must be huge, no?}
% %\str{The following sentence is a bit weird, can you check it again.}
% % \wy{We integrate the likelihood term over the latent space of all possible variables for modulating NeRF functions by monte carlo sampling, which can be seen as integrating over a function distribution.}
% Note that the prior distribution of the NeRF function is conditioned on the target points ${\bf{X}}_{T}$ and the geometric bases ${\bf{B}}_{C}$. 
% Thus, the prior distribution is data-dependent on the target inputs, yielding a better generalization on novel target views of new objects. 
% Moreover, since ${\bf{B}}_{C}$ is constructed with continuous Gaussian distributions in the 3D space, the geometric bases can enrich the locality and semantic information of each discrete target point, enhancing the capture of high-frequency details~\citep{chen2023neurbf,chen2022tensorf,muller2022instant}.



% \underbrace{p(y_T | x_T, x_C, y_C)}_{\text{NeRF Generalization}}

% & \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
%     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{NeRF Generalization}}
%     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}}.
% \end{aligned}
% \label{eq: probabilitic_NeRF_generalization}
% \end{equation}

% \begin{equation}
% \begin{aligned}
%         p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) &= \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o ) \\
%         &p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o,
% \end{aligned}
% \label{eq:ganp-model}
% \end{equation}


% %
% \begin{equation}
% \begin{aligned}
%     &p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto \\
% &    \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
%     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{NeRF Generalization}}
%     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}}.
% \end{aligned}
% \label{eq: probabilitic_NeRF_generalization}
% \end{equation}
% %



% We want to learn generalized Neural Fields such that, for a new object, whose appearance is $y'_*$ observed at coordinates $x'_*$, we want to generate the object appearance $y'$ at all other coordinates $x'$.
% In this case, the observed $(x'_*, y'_*)$ is the context data for the new object, complementing the context data $(x_*, y_*)$ of our training dataset.
% We visualize the setup in figure \str{TODO} in 1D for simplicity, and in 3D so that to connect our descriptions with the popular NeRF framework.
% The unseen outputs $y'$ depend not only on the new coordinates in $x'$, but also on all the context data. 
% In the case of Navier-Stokes 2D fluid dynamics with PINNs, for instance, the velocity in a new location depends on the observed fluid velocities on all other locations.
% In the case of 3D Neural Radiance Field, the shape and texture from a new camera location and direction also clearly depends on the already observed views of the object.
% We model the appearance $y'$ in new locations $x'$ with the following probabilistic model:
% %
% \begin{align}
% p(y' | x', x, y) \varpropto
% \underbrace{p(y' | x, y)}_{\text{Context conditioning}} \cdot
% \underbrace{p(y | x', x, y)}_{\text{Neural Field}} \cdot
% \underbrace{p(x | x')}_{\text{Sampling}}.
% \label{eq: probabilitic_nef}
% \end{align}
% %
% To conceptualize the generative model of the probabilistic Neural Field from equation~\eqref{eq: probabilitic_nef}, we describe it in the context of graphics rendering of a 3D object.
% We want to generate a novel 2D view $y'$ of the 3D object from a new camera direction $\mathbf{d}'$ in input $\mathbf{x}'$, which corresponds to an angle that we have never seen the object before.
% To generate a novel view, we first sample 3D coordinates, $x \sim x | x'$, which will be the context for our generation (for 3D graphics, we do not need to sample camera directions).
% We then map this context coordinates to color and opacity outputs $y | x$ in the sampled 3D context, using the Neural Radiance Field implemented as a neural network, $y = f_{\omega}(x)$.
% Since we are interested in the novel 2D rendering of the 3D object, we finally sample the 2D pixels $y'$, whose rendered values correspond to integrating (analytically or probabilistically) colors and densities along the new ray. 
% \str{Either change y to correspond to a 3D ouput, or simplify this,}
% \str{Check this again. This must become general.}


% The generation process of this probabilistic formulation is as follows.
% We first start from (or sample) a set of rays $\widetilde{\mathbf{X}}$.
% Conditioning on these rays, we sample 3D points in space $\mathbf{X} \big|\widetilde{\mathbf{X}}$.
% Then, we map these 3D points into their colors and density values with the NeRF function, ${\bf{Y}} = f_{\text{NeRF}}({\bf{{X}}})$.
% Last, we sample the 2D pixels of the viewing image that corresponds to the 3D ray ${\widetilde{\bf{Y}}}| {\bf{{Y}}}, {\bf{X}}$ with a probabilistic process. This corresponds to integrating colors and densities ${\bf{{Y}}}$ along the ray on locations ${\bf{X}}$.


% \subsection{Hierarchical Priors in Probabilistic Neural Fields}





% As we are not just interested in fitting a single and specific 3D object but want to learn how to infer the Neural Radiance Field of any 3D object,  we focus on probabilistic Neural Radiance Fields with the following factorization:
%
% \begin{equation}
%     p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}, f_{NeRF}) \propto
%     \underbrace{p({\bf{\widetilde{y}}}| {\bf{{y}}}^{1:P}, {\bf{{x}}}^{1:P})}_{\text{Integration}}
%     \underbrace{p({\bf{{y}}}^{1:P}|{\bf{{x}}}^{1:P}, f_{\text{NeRF}})}_{\text{NeRF model}}
%     \underbrace{p({\bf{{x}}}^{1:P}|{\bf{\widetilde{x}}})}_{\text{Sampling}}.
% \label{eq: rendering}
% \end{equation}
% \begin{equation}
%     p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}, f_{NeRF}) \propto
%     \underbrace{p({\bf{\widetilde{y}}}| {\bf{{y}}}^{\mathbf{r}}, {\bf{{x}}}^{\mathbf{r}})}_{\text{Integration}}
%     \underbrace{p({\bf{{y}}}^{\mathbf{r}}|{\bf{{x}}}^{\mathbf{r}}, f_{\text{NeRF}})}_{\text{NeRF model}}
%     \underbrace{p({\bf{{x}}}^{\mathbf{r}}|{\bf{\widetilde{x}}})}_{\text{Sampling}}.
% \label{eq: rendering}
% \end{equation}
% \begin{equation}
%     p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}) \varpropto
%     \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
%     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T})}_{\text{NeRF Model}}
%     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}},
% \label{eq: definiation}
% \end{equation}

%
% \str{Is $f_\text{NeRF}$ now a random variable? Normally it is not.}
% \str{This can also be writtena  more fluently}
% The generation process of this probabilistic formulation is as follows.
% We first start from (or sample) a set of rays $\widetilde{\mathbf{X}}$.
% Conditioning on these rays, we sample 3D points in space $\mathbf{X} \big|\widetilde{\mathbf{X}}$.
% Then, we map these 3D points into their colors and density values with the NeRF function, ${\bf{Y}} = f_{\text{NeRF}}({\bf{{X}}})$.
% Last, we sample the 2D pixels of the viewing image that corresponds to the 3D ray ${\widetilde{\bf{Y}}}| {\bf{{Y}}}, {\bf{X}}$ with a probabilistic process. This corresponds to integrating colors and densities ${\bf{{Y}}}$ along the ray on locations ${\bf{X}}$.

% % In the following sections, we will define the various probabilistic terms.
% % \str{Here it would be good to be more explicit and say how are the various probabistic terms are defined. Or we can say that we will specify later, also in the context of Geometric NP. Either way, the current text below looks like deterministic relations, so I think we can not write them down here.}

% \str{I suggest we go directly on conditional neural fields. The way we have it now, we only create extra confusion, unless we are the first to propose this decomposition (but there have been other probabilistic NeRFs before, no?). Or perhaps have better structure in the writing, otherwise it is confusing.. What is context, what target?}
% The probabilistic model in \cref{eq: probabilitic_NeRF} is for a single 3D object, thus requiring optimizing a function $f_{\text{NeRF}}$ afresh for every new object, which is time-consuming. For NeRF generalization, we accelerate learning and improve generalization by amortizing the probabilistic model over multiple objects, obtaining per-object reconstructions by conditioning on context sets ${{\widetilde {\bf{X}}}_C, {\widetilde {\bf{Y}}}_C}$.
% % \str{What about $\widetilde {\bf{X}}_T$? What is that? Also, why (1) has small letters, and here we have capitals?}
% % These context variables are few observations from any new object, that is, the rays and the corresponding observed colors.
% For clarity, we use ${(\cdot)}_{C}$ to indicate context sets with {a few new observations for a new object}, while ${(\cdot)}_{T}$ indicates target sets containing 3D points or camera rays from novel views of the same object.
% Thus, we formulate a probabilistic NeRF for generalization as:
% % \str{update this according to the above equation}
% %
% \begin{equation}
% \begin{aligned}
%     &p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto \\
% &    \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
%     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{NeRF Generalization}}
%     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}}.
% \end{aligned}
% \label{eq: probabilitic_NeRF_generalization}
% \end{equation}
% %
% %\str{Not sure if this sentence is good enough, please check later again.}
% As this paper focuses on generalization with new 3D objects, we keep the same sampling and integrating processes as in ~\cref{eq: probabilitic_NeRF}. We turn our attention to the modeling of the predictive distribution $p({\bf{{Y}}}_{T}| {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ in the generalization step, which implies inferring the NeRF function.

% \paragraph{Misalignment between 2D context and 3D structures} It is worth mentioning that the predictive distribution in 3D space is conditioned on 2D context pixels with their ray $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ and 3D target points ${\bf{X}}_{T}$, which is challenging due to potential information misalignment. Thus, we need strong inductive biases with 3D structure information to ensure that 2D and 3D conditional information is fused reliably.