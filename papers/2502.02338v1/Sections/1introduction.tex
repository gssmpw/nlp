
%1. what's NeRF
Neural Fields (NeFs)~\citep{sitzmann2020implicit,tancik2020fourier} have emerged as a powerful framework for learning continuous, compact representations of signals across domains, including 1D signal~\citep{yin2022continuous}, 2D images~\citep{sitzmann2020implicit}, and 3D scenes~\citep{park2019deepsdf,mescheder2019occupancy}. A notable advancement in 3D scene modeling is Neural Radiance Fields (NeRFs)~\citep{mildenhall2021nerf,barron2021mip}, which extend NeFs to map 3D coordinates and viewing directions to volumetric density and view-dependent radiance. By differentiable volume rendering along camera rays, NeRFs achieve photorealistic novel view synthesis.
Although NeRFs achieve good reconstruction performance, they must be overfitted to each 3D object or scene, resulting in poor generalization to new 3D scenes with few context images.

% Thus, fast adaptation of NeRF representations for new 3D scences is still unexplored 
% Thus, one of the main challenges of NeRFs is to improve their generalization for novel 3D scenes.

% popular for representing data as neural functions, mapping continuous coordinates to signals.
% For instance, 3D scene representations~\cite{park2019deepsdf,mildenhall2021nerf,mescheder2019occupancy,chen2022tensorf} favor INRs due to their continuous, compact, and efficient data representation properties.
% However, characterizing a signal by neural network parameters often requires retraining the network on individual data samples, which is computationally expensive.
% Therefore, models must quickly adapt to new signals from partial observations without extensive fine-tuning.

% \zx{This paragraph is a bit long.}

%Despite the need of fast generalization to the new scene for real-time rendering, 
%\str{I am not sure I follow the logic of this paragraph. We are mixing metalearning, probabilistic learning, 3D and 2D partial obersbation, global and local. What exactly is our point?}
In this paper, we focus on neural field generalization (also referred to as conditional neural fields) and the rapid adaptation of NeFs to new signals. Previous works on NeF generalization have addressed this challenge using gradient-based meta-learning~\citep{tancik2021learned}, enabling adaptation to new scenes with only a few optimization steps~\citep{tancik2021learned,papa2023train}. Other approaches include modulating shared MLPs through HyperNets~\citep{chen2022transformers,mehta2021modulated,dupont2022data,kim2023generalizable} or directly predicting the parameters of scene-specific MLPs~\citep{dupont2021generative,erkocc2023hyperdiffusion}. However, the deterministic nature of these methods cannot capture uncertainty in NeFs, when used with scenes with only limited observations are available. This is important as such sparse data may be interpreted in multiple valid ways.

To address uncertainty arising from having few context images, probabilistic NeFs~\citep{gu2023generalizable, guo2023versatile, kosiorek2021nerf} have recently been investigated. For example, VNP~\citep{guo2023versatile} and PONP~\citep{gu2023generalizable} infer the NeFs using Neural Processes (NPs)~\citep{bruinsma2023autoregressive, garnelo2018neural, wang2020doubly}, a probabilistic meta-learning method that models functional distributions conditioned on partial signal observations. These probabilistic methods, however, do not exploit potential structural information, such as the geometric characteristics of signals (e.g., object shape) or hierarchical organization in the latent space (from global to local). Incorporating such inductive biases can facilitate more effective adaptation to new signals from partial observations.

To jointly capture uncertainty and leverage inherent structural information for efficient adaptation to new signals with few observations, we propose a probabilistic neural fields generalization framework called Geometric Neural Processes Fields (\name{}). Our contributions can be summarized as follows:
\textit{1) Probabilistic NeF generalization framework.}  We formulate NeF generalization as a probabilistic modeling problem, allowing us to amortize a learned model over multiple signals and improve NeF learning and generalization.
\textit{2) Geometric bases.} 
To encode structural inductive biases, we design geometric bases that incorporate prior knowledge (e.g., Gaussian structures), enabling the aggregation of local information and the integration of geometric cues.
%To eliminate the potential information misalignment, we design geometric bases by encoding observations in 2D space with 3D prior structures. Thus, the geometric bases can aggregate locality information to each 3D point, improving the exploration of high-frequency details.
\textit{3) Geometric neural processes with hierarchical latent variables.}
Building on these geometric bases, we develop geometric neural processes to capture uncertainty in the latent NeF function space. Specifically, we introduce hierarchical latent variables at multiple spatial scales, offering improved generalization for novel scenes and views.
% incorporates prior structure information with  into the inference of the INR function distribution. 
% The hierarchical latent variables enable modulating the INR function at multiple spatial levels, thus improving the generalization of INR functions.
Experiments on 1D and 2D signals demonstrate the effectiveness of the proposed method for NeF generalization. Furthermore, we adapt our approach to the formulation of Neural Radiance Fields (NeRFs) with differentiable volume rendering on ShapeNet objects and NeRF Synthetic scenes to validate the versatility of our approach. 
%Furthermore, the approach can be seamlessly applied to NeF generalization in both 2D and 1D signals.
%\yc{shall we have G-NPF for NeRF volume rendering formulation as a separate contribution? though this part was moved to appendix} \wy{yes, this is good idea to state this additional contribution.}

%\str{I think this is not a very strong way of putting it, sounds too niche. I would rather say that NeRF is basically a mapping from a collection of 2D input domain to 3D output domain. These methods focus on the 3D only, that is the output, without account for the fact that this uncertainty should also be directly grounded in the 2D input domain. To make it sound more fundamental (the way it already is).}

% \str{Unclear (in the first sentence) why the solution is specific to the problem we highlighted in the previous paragraph (the discrepancy between 2D and 3D)? It becomes sort of clear in the 2) ..., but it has to be more explicit and direct. What exactly is the relation between 2D and 3D geometry, and how do we make it possible to model this? This has to be in the first sentence, and also in the explanation later on. 'Observations in 2D space with 3D prior structures' sounds a bit vague, not as explicit as it can be. Also, 1, 2, 3, feel a bit disconnected. Is 3 just a random add on that is orthogonal to 1 and 2? Until now, we never talked about the multi-scale nature of geometric structures in the world. I would say that this a key point, if others don't really account for this. Overall, this paragraph needs to be rewritten.}




%only approximate the INR functions in 3D space, neglecting the interaction between 3D functions and 2D observations. 
% Radiance fields model 3D relationships, but the context is just 2D posed images. The inherent information misalignment between the radiance field function and context images could hinder these models' generalization.
%Since the radiance fields model relationships in 3D space, while the only available context observations are 2D images, there is an information misalignment between contexts and functions in radiance field generalization.


%\str{What is probabilize? There is no such word.}
% However, these methods only probabilize the INR functions in the 3D space, neglecting the interaction between 3D functions and 2D partial observations. 
% In contrast, some other methods model in probabilistic perceptives. 
% NeRF-VAE~\cite{kosiorek2021nerf} learns a distribution over radiance fields conditioning on a latent scene representation. 
%Some other works~\cite{} utilize GAN~\cite{goodfellow2020generative} to implicitly learn the distribution of functions. 

% Therefore, despite being probabilistic, the function inferred from 2D observation partials is suboptimal due to the information loss from 2D to 3D spaces.
% Neural processes (NP) is also a meta-learning method, but in a probabilistic manner~\cite{bruinsma2023autoregressive}, which aims to model the distribution of functions given partial observations. In this work, we aim to model the distribution of implicit functions, which aligns with the formulation of NPs. 

%\str{Can we make clear what are the contributions, in short?}


% can accelerate the learning and generalize by amortizing the probabilistic model over multiple objects
% For NeRF generalization, we accelerate the learning and generalize by amortizing the probabilistic model over multiple objects and then obtaining per-object reconstructions by conditioning on context sets $\{{\widetilde {\bf{X}}}_C, {\widetilde {\bf{Y}}}_C\}$.
% The probabilistic model contains an INR function in 3D space and the transition between 2D and 3D variables (ray sampling and integration). This enables to design stochastic processes to model the INR function in 3D by considering the probabilistic interaction between 2D and 3D variables.
 %settings.

% Specifically, we train an attention-based neural network to construct a set of geometric bases from the 2D context observations. 
% Consisting of 3D Gaussians, the geometric bases provide structure information and representations of the 3D scenes, which are utilized to infer the function distribution in 3D. 
% Furthermore, with the geometric bases, we introduce hierarchical latent variables in both object and ray levels to infer the predictive function. 
% By fully utilizing the 3D information provided by the geometric bases, the latent variables specialize information of different spatial levels, achieving both object-specific and ray-specific INR functions for generalization on limited contexts.


% ++++++++++++++++++ editing

% Despite the previous effort on modeling probabilistic INR, they ignore the structure information in the space and only focus on the global object-level latent variable or modulation. 
% %However, such works ignore the information loss from the 2D context observation to the 3D space \wy{sampling issues, maybe refer learn to sample paper} and lack structure information. 
% %Moreover, the hierarchical neural process in VNP is sequential, not exploring the structure information. 
% % However, such methods ignore the local information, resulting in missing the high-frequency details of objects. 
% The structure information is a proxy between the ``generality'' and “specialization”~\cite{cong2023enhancing,bauer2023spatial}. To achieve generality, the model should be able to capture the shared context, which is the global object level. On the other hand, to have high-fidelity rendering results, the model should be able to infer the sample-specific or region-specific information, e.g. specialized self-similar appearance patterns, which is at the local level. 
% In this work, we propose to use hierarchical neural processes (HNP)~\cite{wang2022learning,wang2020doubly,shen2024episodic} to model the distribution of functions, which allows us to explicitly introduce a global and local variable to incorporate the structure information globally and locally. 
% We found that our local variable suits the neural radiance field (NeRF) framework well. Hence,
% to better demonstrate our local variable, we illustrate our method on neural radiance field generalization.
% However, our method is also applicable to other INRs, such as image regression and image completion, as shown in our experiments. 


% In an NPs setting, the context set is given to infer a function for the target set, which models a conditional distribution. For neural radiance fields (NeRF), the context set is the few-view images with the corresponding camera rays. 
% However, the NeRF function we aim to approximate is inherited in 3D space. There exists a gap between the context information and the NeRF function
% %, which is commonly ignored in previous INR generalization literature 
% due to the sampling and integration\cite{martin2021nerf,arandjelovic2021nerf,fang2021neusample,lindell2021autoint,neff2021donerf} from the camera ray to the projected 2D image pixels. \wy{discussion: these papers all about how to improve sampling to speed up the Monte Carlo for efficient training.} This may cause information loss between 2D context and 3D function.
% %inconsistency in formulation. 
% %For instance, 3D points are sampled along the rays~\cite{martin2021nerf}, which may affect the quality of integrating information along camera rays to render the corresponding pixels~\cite{arandjelovic2021nerf}. 
% Instead of directly building the conditional distribution on observed camera rays and projected 2D images, we derive our function distribution fully on the 3D space based on a set of geometric Gaussian basis spanned in space, 
% %Specifically, we choose to use the Gaussian basis due to its continuous property
% which is favored by previous 3D reconstruction methods~\cite{genova2020local,chen2023neurbf,kerbl20233d}. The geometric Gaussian basis is able to reflect the shape of objects (global structure) and aggregate the locality information~\cite{chen2023neurbf,chen2022tensorf} into arbitrary 3D locations in a continuous way.  
% Finally, we provide evidence of a lower bound to train the model. Our approach enables neural radiance field generalization in a single feedforward pass, without fine-tuning. Our contributions can be summarized as follows:
% \begin{enumerate}
%     \item We propose a new probabilistic formulation for NeRF generalization based on hierarchical neural processes, which is also applicable to general INR generalization.
%     \item We propose to infer a set of posterior geometric bases to embed the structure information globally and locally. 
% \end{enumerate}










%RBF kernels have been explored in previous NeRF research
%, as well as other 3D scene reconstruction techniques, like Gaussian Splatting~\cite{kerbl20233d} which assumes the space is composed of a set of Gaussians. 

%To address the above-mentioned issues, we introduce a set of geometry basis spanned in space to provide structure information. 
%In 3D NeRF, this can also eliminate the information loss by directly inferring the continuous 3D space basis from the 2D context information. Then, we contribute to a neural processes formulation of NeRF, based on which we perform hierarchical NP. The hierarchical NP consists of a global variable that models the scene information and a local variable that models the ray-wise or location-wise information. Finally, we provide evidence of a lower bound to train the model. Our approach enables neural radiance field generalization in a single feedforward pass, without fine-tuning. 


%


%-------------------------------------------------

% comments of why using basis:
% 1. For modeling the local neural features (check how this local neural features help?). because of the difficulty
% in representing high-frequency details due to the inductive
% bias [5, 70] of MLPs. To tackle this problem, local neural fields have been proposed and widely adopted. 

% 2. 3D Gaussians as a flexible and expressive scene representation

% 2. flexible kernel position and shape, which have higher spatial adaptivity and can more closely fit target signals

% 3. challenges of current Radiance Field Generalization methods(do not consider uncertainty, fine-tuning, time-consuming, balabala),
% why we use NP

% 4. What we do in our method (our contributions) (model the problem in a probabilistic framework to infer the function for Radiance Field Generalization in a single feedforward pass, without fine-tuning; propose geometric basis to solve the information gap between 2D context and 3D target function; hierarchical neural processes/modulation for XXX)


% The idea of using Geometry bases origins from the previous NeRF works~\cite{chen2022tensorf,muller2022instant} which use either gird regular point latent representation or RBF kernels in the space, to store the 3D scene geometry and semantic information, as well as the Gaussian Splatting~\cite{kerbl20233d} which assumes the space is composed of a set of Gaussians. 


%GENERALIZATION IN DIFFUSION MODELS ARISES FROM GEOMETRY-ADAPTIVE HARMONIC REPRESENTATIONS shows the geometry prior basis connect to our motivation.

% Keypoint: 

% 1. We want to model the distribution of functions to achieve implicit neural field generalization, which is suitable for Neural Processes. 

% 2. The context information has an information gap with the 3D Radiance Field. Previous generalization work, including VNP~\cite{guo2023versatile} ignores this. This motivates us to introduce a set of Gaussian basis to encode the geometry information in the scene. Also, by doing so, we are able to connect NP and NeRF generalization more tightly. 

% 3. The Gaussian basis with radial basis function provides a continuous way of representing each spatial location. 