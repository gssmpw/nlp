
% We evaluate the proposed \method{} on different tasks.
% In Sec.~\ref{sec:nerf-results}, we demonstrate the performance on novel view synthesis for 3D scenes. 
% Further, we show the method can also perform competitively on the 2D image regression task, as well as image completion in Sec.~\ref{sec:image-regression}, although it is originally designed for 3D view synthesis.
% Last, we perform ablation studies in Sec.~\ref{sec:abl-study}. 

\noindent{\textbf{Baselines.}}
%\str{Explain why meta-learning is the type of baseline we want here.}
We compare \method{} with three recent probabilistic INR generalization methods: NeRF-VAE~\citep{kosiorek2021nerf}, PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} on ShapeNet novel view synthesis and image regression tasks.
PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} also rely on Neural Processes, however, they neglect structure information and the probabilistic interaction between 3D functions and 2D partial observations. Additionally, we choose two previous well-known deterministic INR generalization approaches, LearnInit~\citep{tancik2021learned} and  TransINR~\citep{chen2022transformers} as our baselines. Moreover, to demonstrate the flexibility of our method and its ability to handle complex scenes, we integrate \method{} with GNT~\citep{wang2022attention} and conduct experiments on the NeRF Synthetic dataset~\citep{mildenhall2021nerf}. \textcolor{blue}{In addition, we demonstrate our method is also effective for 1-D signals in the Appendix.}


% We choose two previous well-known deterministic INR generalization approaches, LearnInit~\citep{tancik2021learned} and  TransINR~\citep{chen2022transformers} as our baselines. We use the self-attention module similar to TransINR to extract features from the context set.
% Additionally, we compare with three recent probabilistic INR generalization methods: NeRF-VAE~\citep{kosiorek2021nerf}, PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile}.
% PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} also rely on Neural Processes, however, they neglect structure information and the probabilstic interaction between 3D functions and 2D partial observations.


\subsection{Novel View Synthesis}
\label{sec:nerf-results}

% \noindent{\textbf{Setup.}} We perform the 3D novel view synthesis task on ShapeNet~\citep{chang2015shapenet} objects. Following previous works' setup, the dataset consists of objects from three ShapeNet categories: chairs, cars, and lamps. For each 3D object, 25 views of size $128 \times 128$ images are generated from viewpoints randomly selected on a sphere. The objects in each category are divided into training and testing sets, with each training object consisting of 25 views with known camera poses. During testing, a random input view is sampled to evaluate the performance of the novel view synthesis. Following the setting from previous methods, we focus on the single-view (1 shot) and 2-views (2 shot) versions of the task, where single-view or two-view images with their corresponding camera rays are provided as the context set.
\noindent{\textbf{ShapeNet.}} We perform the 3D novel view synthesis task on ShapeNet~\citep{chang2015shapenet} objects. Following previous works' setup~\citep{tancik2021learned}, the dataset consists of objects from three ShapeNet categories: chairs, cars, and lamps. For each 3D object, 25 views of size \(128 \times 128\) images are generated from viewpoints randomly selected on a sphere. The objects in each category are divided into training and testing sets, with each training object consisting of 25 views with known camera poses. At test time, a random input view is sampled to evaluate the performance of the novel view synthesis. Following the setting of previous methods~\citep{chen2022transformers}, we focus on the single-view (1-shot) and 2-view (2-shot) versions of the task, where one or two images with their corresponding camera rays are provided as the context.


% \noindent{\textbf{Implementation Details.}} 
% %We use the visual tokenizer and self-attention modules as TransINR~\citep{chen2022transformers} and VNP~\citep{guo2023versatile}. 
% Our context input is the concatenation of a set of camera rays and the corresponding image pixels from a few views (one or two),
% %the context image and the corresponding ray maps %\str{What is a ray map?}, 
% which are then split into different visual tokens. We use the same patch size $8\times8$ as TransINR~\citep{chen2022transformers} and VNP~\citep{guo2023versatile}, which gives us 256 tokens. 
% Then, a linear layer and a self-attention module are used to project each token into a 512-dimension vector. Based on the 256 tokens, we predict 256 Gaussian bases by two MLP modules. Specifically, one MLP with 2 linear layers is used to map the tokens into a 10-dimension vector, which includes 3-dimension Gaussian centers a 3-dimension vector for constructing the scaling matrix, and a 4-dimension vector for quaternions parameters of the rotation matrix. Both the scaling matrix and rotation matrix are used to build the $3\times 3$ covariance matrix. This procedure is similar to Gaussian construction in the 3D Gassian Splatting~\citep{kerbl20233d}. Details are provided in the Appendix. Another MLP is used to estimate the latent representation of each Gaussian basis. Here, we simply use 32-dimension for each Gaussian basis. 
% Note that using more views as the context set will increase the number of Gaussian basis accordingly, which enriches the ability of describing the geometry of the space. 


\begin{table*}[t]
    \centering
    \caption{\textbf{Qualitative comparison (PSNR) on novel view synthesis of ShapeNet objects.} \method{} consistently outperforms baselines across all categories with both 1-view and 2-view context.}
    \begin{tabular}{lccccc}
        \toprule
        Method & Views & Car & Lamps & Chairs & Average \\
        \midrule
        Learn Init~\citep{tancik2021learned}  & 25 & 22.80 & 22.35 & 18.85 & 21.33 \\
        \midrule
        Tran-INR~\citep{chen2022transformers}  & 1 & 23.78 & 22.76 & 19.66 & 22.07 \\
        NeRF-VAE~\citep{kosiorek2021nerf}  & 1 & 21.79 & 21.58 & 17.15 & 20.17 \\
        PONP~\citep{gu2023generalizable}  & 1 & 24.17 & 22.78 & 19.48 & 22.14 \\
        VNP~\citep{guo2023versatile}  & 1 & 24.21 & 24.10 & 19.54 & 22.62 \\
        \rowcolor{lightblue}
        \textbf{\method{}} (Ours) & 1 & \textbf{25.13} & \textbf{24.59} & \textbf{20.74} & \textbf{23.49} \\
        \midrule
        Tran-INR~\citep{chen2022transformers}  & 2 & 25.45 & 23.11 & 21.13 & 23.27 \\
        PONP~\citep{gu2023generalizable}  & 2 & 25.98 & 23.28 & 19.48 & 22.91 \\
        \rowcolor{lightblue}
        \textbf{\method{}} (Ours) & 2 & \textbf{26.39} & \textbf{25.32} & \textbf{22.68} & \textbf{24.80} \\
        \bottomrule
    \end{tabular}
    \label{tab:nerf-psnr}
    \vspace{-2mm}
\end{table*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{Figures/nerf-results-new.pdf} % Adjust the size and filename as needed
  \vspace{-6mm}  \caption{\textbf{Qualitative results of the proposed \method{} on novel view synthesis of ShapeNet objects.} Both 1-view (top) and 2-view (bottom) context results are presented.} % Caption for the figure
  \label{fig:nerf-visualization}
  \vspace{-3mm}
\end{figure*}

\noindent{\textbf{Implementation Details.}} Our context input is the concatenation of a set of camera rays and the corresponding image pixels from one or two views, which are then split into different visual tokens. We use the same patch size \(8 \times 8\) as TransINR~\citep{chen2022transformers} and VNP~\citep{guo2023versatile}, resulting in 256 tokens. A linear layer and a self-attention module project each token into a 512-dimensional vector. Based on the 256 tokens, we predict 256 geometric bases using two MLP modules: one for 3D Gaussian distribution parameters and the other for the latent representation (32 dimensions).
% \wy{Can be removed to appendix: Specifically, one MLP with 2 linear layers maps the tokens into a 10-dimensional vector, which includes 3-dimensional Gaussian centers, a 3-dimensional vector for constructing the scaling matrix, and a 4-dimensional vector for quaternion parameters of the rotation matrix. Both the scaling matrix and rotation matrix are used to build the \(3 \times 3\) covariance matrix. This procedure is similar to Gaussian construction in the 3D Gaussian Splatting~\citep{kerbl20233d}.} 
%We use one MLP to predict Gaussian basis parameters, which is detailed in the Appendix. Another MLP estimates the latent representation of each Gaussian basis, using a 32-dimensional vector for each Gaussian basis. 
More details are given in Appendix~\ref{supp:gaussian}. 
%Using more views as the context set increases the number of geometric base accordingly, enriching the ability to describe the geometry of the space.
% To have a fair comparison with baseline methods, we report the results of using $8\times 8$ patch size. \str{What do you mean here? Other baselines also use 8x8 patches? If yes, say that.}
We obtain the object-specific and ray-specific modulating vectors (both are 512 dimensions) based on the geometric base. Our NeRF function consists of four layers, including two modulated layers and two shared layers. %\str{This can also be better written. The terminology is not consisten, what is the global and local-raywise modulate vectors? We not really use that words before.}

\noindent{\textbf{Quantitative Results.}} The quantitative comparison in terms of Peak Signal-to-Noise Ratio (PSNR) is presented in Table~\ref{tab:nerf-psnr}. The proposed \method{} consistently outperforms all other baselines across all three categories by a significant margin. On average, \method{} exceeds the previous NP-based method, VNP~\citep{guo2023versatile}, by 0.87 PSNR, %(23.49 vs. 22.62), 
indicating that the proposed geometric bases and probabilistic hierarchical modulation result in better generalization ability. Moreover, with two views of context information, \method{}'s performance improves significantly by around $1$ PSNR. This improvement is expected, as the richer geometric bases information allows for a better representation of the 3D space, leading to improved object-specific and ray-specific latent variables. % to modulate the MLP.




{\noindent {\textbf{Qualitative Results.}}
In Fig.~\ref{fig:nerf-visualization}, we visualize the results of \method{} on novel view synthesis of ShapeNet objects.
\method{} can infer object-specific radiance fields and render high-quality 2D images of the objects from novel camera views, even with only 1 or 2 views as context. More results and comparisons with other VNP are provided in Appendix~\ref{supp:more-results}.}


\textcolor{blue}{\noindent \textbf{Comparison with More SOTAs on NeRF Synthetic.} To illustrate the flexibility and advantages of our proposed method, we evaluate it on the NeRF Synthetic dataset~\citep{mildenhall2021nerf} against recent state-of-the-art methods, including GNT~\citep{wang2022attention}, MatchNeRF~\citep{chen2023explicit}, and GeFu~\citep{liu2024geometry}. For a fair comparison, we use the same encoder and NeRF network architecture while integrating our probabilistic framework into GNT. Following GeFu, we assess performance in 2-view and 3-view settings. As shown in Table~\ref{tab:comparison-gnt}, our method surpasses GeFu by approximately 1 PSNR in the 3-view setting, validating the effectiveness of our probabilistic framework and geometric bases.}

\textcolor{blue}{We also consider a challenging 1-view setting to examine the model’s robustness under extremely limited context. Both Table~\ref{tab:comparison-gnt} and Figure~\ref{fig:1-view-compare} indicate that our method can still reconstruct novel views effectively, whereas GNT fails to do so. Furthermore, we test cross-category generalization for our model and GNT without retraining, training on the \texttt{drums} category and evaluating on \texttt{lego}. As shown in Figure~\ref{fig:cross-category}, our method leverages the available context information more effectively, producing higher-quality generations with superior color fidelity compared to GNT. Additional details are provided in Appendix~\ref{sec:compare_gnt}.}


\begin{table}[htbp]
\centering
\resizebox{\columnwidth}{!}{ % Resize the table to fit within the column width
\begin{tabular}{lcccc}
\toprule
\textbf{Models} & \textbf{\# Views} & \textbf{PSNR ($\uparrow$)} & \textbf{SSIM ($\uparrow$)} &  \textbf{LPIPS ($\downarrow$)}\\ 
\midrule
GNT  & 1 & 10.25 & 0.583 & 0.496\\ 
GeomNP & 1 & \textbf{20.07} & \textbf{0.815} & \textbf{0.208} \\ 
\midrule
GNT & 2 & 23.47 &0.877 &0.151 \\ 
MatchNeRF & 2 &20.57 & 0.864 &0.200  \\ 
GeFu & 2 & 25.30 &0.939 &0.082  \\ 
GeomNP & 2 &  &  &  \\ 
\midrule
GNT & 3 &  25.80 & 0.905 & 0.104 \\ 
MatchNeRF & 3 & 23.20 & 0.897 & 0.164  \\ 
GeFu & 3 &  26.99 & \textbf{0.952} & 0.070  \\ 
GeomNP & 3 & \textbf{27.85} & 0.948 & \textbf{0.068} \\ 
\bottomrule
\end{tabular}
}
\caption{Performance comparison of different models with varying numbers of context views.}
\label{tab:comparison-gnt}
\end{table}





\begin{table}[htbp]
    \centering
    \begin{tabular}{lcc}
        \toprule
                 & CelebA & Imagenette \\ \midrule
        Learned Init \citep{tancik2021learned} & 30.37  & 27.07       \\
        TransINR~\citep{chen2022transformers}  & 31.96  & 29.01       \\ 
        \rowcolor{lightblue}
        \textbf{\method{} (Ours)}         & \textbf{33.41}  & \textbf{29.82}      \\ 
        \bottomrule
    \end{tabular}
    \caption{Quantitative results. \method{} outperforms baseline methods consistently on both datasets.}
    \label{tab:image-regression}
\end{table}


\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.49\textwidth} 
        \includegraphics[width=\textwidth]{Figures/image-regression0.pdf} % Adjust filename as needed
        \caption{\textbf{Visualizations of image regression results} on CelebA (left) and Imagenette (right).}
        \label{fig:visualization-image-regression}
    \end{minipage}
\end{figure}


% \begin{figure*}[htbp]
%     \centering
%     \begin{subtable}[b]{151.2pt} 
%     % \vspace{-6mm}
%     \begin{tabular}{lcc}
%     \toprule
%                  & CelebA & Imagenette \\ \midrule
%     Learned Init \citep{tancik2021learned} & 30.37  & 27.07       \\
%     TransINR~\citep{chen2022transformers}         & 31.96  & 29.01       \\ 
%     % \hline
%     \rowcolor{lightblue}
%     \textbf{\method{}} (Ours)         & \textbf{33.41}  & \textbf{29.82}      \\ 
%     \bottomrule
%     \end{tabular}
%     \caption{Quantitative results. \method{} outperforms baseline methods consistently on both datasets.}
%     \label{tab:image-regression}
%     \end{subtable} \hfill
%     \begin{subtable}[b]{151.2pt} 
%     \includegraphics[width=\textwidth]{Figures/image-regression0.pdf} % Adjust the size and filename as needed
%     \caption{Visualizations on CelebA (left) and Imagenette (right), respectively.} % Caption for the figure
%     \label{fig:visualization-image-regression}
% \end{subtable}
% % \vspace{-2mm}
% \caption{\textbf{Quantitative results and visualizations} of image regression on CelebA and Imagenette.}
% \vspace{-3mm}
% \end{figure*}





% \begin{figure}[t]
%     \centering
%     \begin{subtable}[b]{0.42\textwidth}
%     % \vspace{-6mm}
%     \begin{tabular}{lcc}
%     \toprule
%                  & CelebA & Imagenette \\ \midrule
%     Learned Init \citep{tancik2021learned} & 30.37  & 27.07       \\
%     TransINR~\citep{chen2022transformers}         & 31.96  & 29.01       \\ 
%     % \hline
%     \rowcolor{lightblue}
%     \method{} (Ours)         & \textbf{33.41}  & \textbf{29.82}      \\ 
%     \bottomrule
%     \end{tabular}
%     \caption{Quantitative results. \method{} outperforms baseline methods consistently on both datasets.}
%     \label{tab:image-regression}
%     \end{subtable} \hfill
%     \begin{subtable}[b]{0.54\textwidth}
%     \includegraphics[width=\textwidth]{Figures/image-regression0.pdf} % Adjust the size and filename as needed
%     \caption{Visualizations on CelebA (left) and Imagenette (right), respectively.} % Caption for the figure
%     \label{fig:visualization-image-regression}
% \end{subtable}
% % \vspace{-2mm}
% \caption{\textbf{Quantitative results and visualizations} of image regression on CelebA and Imagenette.}
% \vspace{-3mm}
% \end{figure}



\subsection{Image Regression}
\label{sec:image-regression}


\noindent{\textbf{Setup.}} Our method is flexible to different signals and can also be seamlessly applied to 2D signals. Here, we evaluate our method on the image regression task, a common task for evaluating INRs' capacity of representing a signal~\citep{tancik2021learned,sitzmann2020implicit}. 
We employ two real-world image datasets as used in previous works~\citep{chen2022transformers,tancik2021learned,gu2023generalizable}. The CelebA dataset~\citep{liu2015deep} encompasses approximately 202,000 images of celebrities, partitioned into training (162,000 images), validation (20,000 images), and test (20,000 images) sets. The Imagenette dataset~\citep{imagenette}, a curated subset comprising 10 classes from the 1,000 classes in ImageNet~\citep{deng2009imagenet}, consists of roughly 9,000 training images and 4,000 testing images. In order to compare with previous methods, we conduct image regression experiments. The context set is an image and the task is to learn an implicit function that regresses the image pixels well. % in terms of PSNR.
%\str{What is the point of image regresssion when the image itself is used as context? Why not just copy the context??}
%\str{The following is a bit strante, is this still image regression? Why not have a separate section?}
%\str{Is Image Regression the standard name for this task?}


\noindent{\textbf{Implementation Details.}} 
Following TransINR~\citep{chen2022transformers}, we resize each image into $178\times 178$, and use patch size 9 for the tokenizer. The self-attention module remains the same as the one in the NeRF experiments (Sec. \ref{sec:nerf-results}). For the Gaussian bases, we predict the 2D Gaussians instead of the 3D. 
The hierarchical latent variables are inferred in image-level and pixel-level. 
%The self-attention and global variable remain the same as the one in the NeRF experiments. %We do not use the pixel variable modulation for the computation concern. However, our method still has competitive performance.   

% \begin{table}[t]
% \centering
% \vspace{-6mm}
% \caption{Quantitative results of image regression.}
% \label{tab:image-regression}
% \begin{tabular}{lcc}
% \toprule
%              & CelebA & Imagenette \\ \midrule
% Learned Init \citep{tancik2021learned} & 30.37  & 27.07       \\
% TransINR~\citep{chen2022transformers}         & 31.96  & 29.01       \\ 
% \hline
% \method{} (Ours)         & \textbf{33.41}  & \textbf{29.82}      \\ 
% \bottomrule
% \end{tabular}
% \end{table}



\noindent{\textbf{Results.}} The quantitative comparison of \method{} for representing the 2D image signals is presented in Table~\ref{tab:image-regression}. \method{} outperforms the baseline methods on both CelebA and Imagenette datasets significantly, showing better generalization ability and representation capacity than baselines. 
%Note that the Imagenette is a more diverse dataset than the CelebA. The better performance shows that. 
Fig.~\ref{fig:visualization-image-regression} shows the ability of \method{} to recover the high-frequency details for image regression.
%regress the image closely to the ground truth, indicating the capability of capturing the detailed texture information. 





%, implying good capability of .



% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\textwidth]{Figures/image-regression-vis.pdf} % Adjust the size and filename as needed
%   \caption{Image regression visualization on CelebA and Imagenette, respectively. Left: Ground truth; Right: Prediction.} % Caption for the figure
%   \label{fig:visualization-image-regression}
% \end{figure}



%\noindent {\textbf{Ablation study}}
\subsection{Ablations}
\label{sec:abl-study}

%\noindent

\begin{table}[htbp]
% \centering
\vspace{-4mm}
\caption{\textbf{Sensitivity to the number of geometric bases} on NeRF and image regression.}
\label{tab: ablate_basis}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{Image Regression}} & \multicolumn{2}{c}{\textbf{NeRF}} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-6}
\# Bases & 49 & 169 & 484 & 100 & 250 \\ \midrule
PSNR~($\uparrow$)    & 28.59  & 33.74   &  44.24  &  24.31 &  24.59  \\ \bottomrule
\label{table:num-gaussian}
\end{tabular}
}
\vspace{-6mm}
\end{table}
% \vspace{-2mm}
\textbf{Sensitivity to Number of Geometric Bases}.
We further analyze the sensitivity to the number of geometric bases in the CelebA image regression and Lamps NeRF tasks. 
We further analyze the sensitivity to the number of geometric bases in the CelebA image regression and Lamps NeRF tasks. 
In image regression, we resize the images to \(64 \times 64\) and use different patch sizes to construct 49, 169, and 484 bases. In the NeRF task, we keep the same setup as in Sec. \ref{sec:nerf-results} and construct 100, 250 bases. The results are provided in Table~\ref{table:num-gaussian}. With more bases, \method{} achieves better performance consistently, indicating that large numbers of geometric Gaussian bases further enrich the structure information and lead to stronger predictive functions. 
%However, beyond a certain point, specifically 169 in this experiment, the performance gains diminish. 
We choose the number of bases by balancing the performance and computational costs.





\noindent{\textbf{Importance of Hierarchical Latent Variables.}}
To demonstrate the effectiveness of the hierarchical nature of \method{} with object-specific and ray-specific latent variables for modulation, we performed an ablation study on a subset of the Lamps dataset for fast evaluation. As shown in the last four rows in Table~\ref{table:abl-np}, either object-specific or ray-specific latent variable improves the performance of neural processes, indicating the effectiveness of the specific function modulation. With both ${\bf{z}}_o$ and ${\bf{z}}_r$, the method performs best, demonstrating the importance of the hierarchical modulation by latent variables. 
In addition, the hierarchical modulation also performs well without the geometric bases.
% proposed NPs effectively provides both object-specific and ray-specific functions to capture detailed textures.
%Additionally, without all hierarchical latent variables, \method{} still remains a relatively good performance, which implies the outstanding capacity of the Gaussian basis. 

%drops by $0.5$ PSNR.
%The reason is that the same MLP is used for different scenes and novel views, while with the hierarchical Neural Process \str{TODO}.


% \begin{table}[htbp]
% \centering
% \caption{Ablation Study on a subset of the Lamps scene synthesis (PSNR). }
% \label{tab:my_label}
% \begin{tabular}{lc}
% \toprule
% %Basis & $\mathbf{z}_o$ (Object) & $\mathbf{z}_r$ (Ray) & PSNR \\
% Basis & PSNR \\ 
% %\midrule
% \midrule
% %\cmark & \xmark & \xmark &  25.98\\ 
% \xmark  & 23.06 \\ 
% %\cmark & \xmark & \cmark & 26.29\\ 
% %\cmark & \cmark & \xmark & 26.24\\ 
% \cmark  & 26.48\\ 
% \bottomrule
% \label{table:abl-basis}
% \end{tabular}
% \end{table}



\begin{table}[htbp]
\centering
\vspace{-4mm}
\caption{\textbf{Importance of geometric bases and hierarchical latent variables} on a subset of the Lamps scene synthesis (PSNR). $\mathbf{z}_o$ and $\mathbf{z}_r$ are object-specific variable and ray-specific variable, respectively.  \ymark~ and \xmark~denote whether the component joins the pipeline or not.}
%\vspace{-2mm}
%\label{tab:my_label}
\resizebox{0.25\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
 $\mathbf{B}_C$ & $\mathbf{z}_o$& $\mathbf{z}_r$ & PSNR ($\uparrow$) \\ 
\midrule
\xmark & \cmark & \cmark & 23.06 \\
\cmark & \xmark & \xmark & 25.98 \\
\cmark & \cmark & \xmark & 26.24\\ 
\cmark & \xmark & \cmark & 26.29\\ 
\cmark & \cmark & \cmark & \textbf{26.48}\\ 
% \cmark & \cmark & \xmark & \\ 
% \cmark & \cmark & \cmark & 24.59\\ 
\bottomrule
\label{table:abl-np}
\end{tabular}
}
\vspace{-8mm}
\end{table}

% \noindent 
\noindent{\textbf{Importance of Geometric Bases.}}
%\str{The table seems a bit redundant, similar numbers are in the previous table. Maybe keep the new numbers only? Or merge somehow the tables?}
We also explore the effectiveness of the proposed geometric bases. As shown in Table~\ref{table:abl-np} (rows 1 and 5), with the geometric bases, \method{} performs clearly better. This indicates the importance of the 3D structure information modeled in the geometric bases, which provide specific inferences of the INR function in different spatial levels. Moreover, the bases perform well without hierarchical latent variables, demonstrating their ability to construct 3D information and reduce misalignment between 2D and 3D spaces.


% provide  effectively encodes modulates inputs, making them context-specific. Consequently, even with a fully shared MLP, the model can produce reliable estimations.

% \wy{Need to figure out how to structure two tabless}
%The reason is that the input is re-represented by the Gaussian basis, which indicates the effectiveness of our Gaussian basis. Ho
%\str{What do you mean here by re-represented?}
%For the experiment without the Gaussian basis, we simply use the coordinate embedding with Fourier features and an MLP to obtain the representation of each location. \str{What are coordinate embeddings? I think you must again make sure the wording, terminology and styling is consistent.}
%Then, the global and location variables are obtained by aggregating this representation. The experiment shows that the performance significantly drops without the help of the Gaussian basis. 

%\noindent {\textbf{Ablation study on NP hierarchical maybe? }}

% \noindent 


% 
% the performance is close to the best one. 

% \begin{table}[h]
% \centering
% \caption{Ablation study of the number of Gaussians basis on image regression.}
% \label{your-label}
% \begin{tabular}{lccc}
% \toprule
% \# Basis & 49 & 169 & 484 \\ \midrule
% -     & 28.59  & 33.74   &  44.24  \\ \bottomrule
% \label{table:num-gaussian}
% \end{tabular}
% \end{table}



% \begin{table}[h]
% \centering
% \caption{Ablation study of the number of Gaussians basis on NeRF.}
% \label{your-label}
% \begin{tabular}{lccc}
% \toprule
% \# Basis & 100 & 250 & 441 \\ \midrule
% -     &   & 24.59   &    \\ \bottomrule
% \label{table:num-gaussian-3d}
% \end{tabular}
% \end{table}

% \noindent{\textbf{Uncertainty Visualization}.} 
% As a probabilistic framework, our method can provide uncertainty estimation. To obtain the uncertainty map, we sample ten times from the predicted prior distribution to generate corresponding images and then use the variance map to represent the uncertainty. As shown in Fig.~\ref{fig:uncertainty-visual}, high uncertainty is concentrated around the edges, which is expected, as capturing detailed, sharp changes at the edges is more challenging for the model.

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.99\textwidth]{ICLR2025/Figures/uncertainty.pdf} % Adjust the size and filename as needed
%   \vspace{-3mm}
%   \caption{\textbf{Uncertainty Map of the predictions.} Edges of objects have higher uncertainty since it is more challenging for the model to capture the detailed, sharp changes at the edges.}
%   \label{fig:uncertainty-visual}
%   \vspace{-3mm}
% \end{figure}



% \begin{figure}[t]
%     \centering
    % \begin{minipage}[b]{0.45\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/image_with_heatmap_2.png}
    %     %\caption{Figure 1}
    %     \label{fig:figure1}
    % \end{minipage}
    % \hfill
    % \begin{minipage}[b]{0.45\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/image_with_heatmap_9.png}
    %     %\caption{Figure 2}
    %     \label{fig:figure2}
    % \end{minipage}
    % \vskip\baselineskip
%     \begin{minipage}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/image_with_heatmap_14.png}
%         %\caption{Figure 3}
%         \label{fig:figure3}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/image_with_heatmap_16}
%         %\caption{Figure 4}
%         \label{fig:figure4}
%     \end{minipage}
%     \caption{\textbf{Uncertainty Map of the predictions.} Edges of objects have higher uncertainty since it is more challenging for the model to capture the detailed, sharp changes at the edges.}
%     \label{fig:uncertainty-visual}
%     \vspace{-4mm}
% \end{figure}

