
% \js{Double check connection sentences between subsections}

\noindent \textbf{Notations.} We denote 3D world coordinates by \(\mathbf{p} = (x, y, z)\) and a camera viewing direction by \(\mathbf{d} = (\theta, \phi)\). Each point in 3D space have its color \(\mathbf{c}(\mathbf{p}, \mathbf{d})\), which depends on the location \(\mathbf{p}\) and viewing direction \(\mathbf{d}\). Points also have a density value \(\sigma(\mathbf{p})\) that encodes opacity. We represent coordinates and view direction together as $\mathbf{x} = \{\mathbf{p},\mathbf{d} \}$, color and density together as \(\mathbf{y}(\mathbf{p}, \mathbf{d}) = \{\mathbf{c}(\mathbf{p}, \mathbf{d}), \sigma(\mathbf{p})\}\).
When observing a 3D object from multiple locations, we denote all 3D points as \(\mathbf{X} = \{\mathbf{x}_n \}_{n=1}^N\) and their colors and densities as \(\mathbf{Y} = \{\mathbf{y}_n\}_{n=1}^N\).
Assuming a ray \(\mathbf{r} = (\mathbf{o}, \mathbf{d})\) starting from the camera origin \(\mathbf{o}\) and along direction \(\mathbf{d}\), we sample $P$ points along the ray, with \(\mathbf{x}^{\mathbf{r}} = \{{x}_i^\mathbf{r}\}_{i=1}^P\) and corresponding colors and densities \(\mathbf{y}^{\mathbf{r}} = \{{y}_i^{\mathbf{r}}\}_{i=1}^P\). Further, we denote the observations \(\widetilde{\mathbf{X}}\) and \(\widetilde{\mathbf{Y}}\) as: the set of camera rays \(\widetilde{\mathbf{X}} = \{\widetilde{\mathbf{x}}_n = \mathbf{r}_n\}_{n=1}^N\) and the projected 2D pixels from the rays \(\widetilde{\mathbf{Y}} = \{\widetilde{\mathbf{y}}_n\}_{n=1}^N\).



% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.49\textwidth]{Figures/problemstate.pdf} % Adjust the size and filename as needed
%   \caption{Framework.} % Caption for the figure
%   \label{fig:problem}
% \end{figure}



% \str{Can you clarify what exactly each notation style corresponds to? What is the $\sim$ for instance? The sampled new pixel views? And no tilde are the actual observations?}

\begin{figure}[htbp]
%\vspace{-5mm}
\centering
\centerline{
\includegraphics[width=0.4\columnwidth]{Figures/problemstate.pdf} 
} 
%\vspace{-2mm}
\caption{\textbf{Complete rendering from 3D points to a 2D pixel.}
}
%\vspace{-4mm}
\label{fig:problem}
\end{figure}

\textbf{Background on Neural Radiance Fields.}
We formally describe Neural Radiance Field (NeRF)~\citep{mildenhall2021nerf, arandjelovic2021nerf} as a continuous function \( f_{\text{NeRF}}: \mathbf{x} \mapsto \mathbf{y} \), which maps 3D world coordinates \(\mathbf{p}\) and viewing directions \(\mathbf{d}\) to color and density values \(\mathbf{y}\). 
That is, a NeRF function, \( f_{\text{NeRF}} \), is a neural network-based function that represents the whole 3D object (e.g., a car in Fig.~\ref{fig:problem}) as coordinates to color and density mappings. Learning a NeRF function of a 3D object is an inverse problem where we only have indirect observations of arbitrary 2D views of the 3D object, and we want to infer the entire 3D object's geometry and appearance.
With the NeRF function, given any camera pose, we can render a view on the corresponding 2D image plane by marching rays and using the corresponding colors and densities at the 3D points along the rays. Specifically, given a set of rays \(\mathbf{r}\) with view directions \(\mathbf{d}\), we obtain a corresponding 2D image. The integration along each ray corresponds to a specific pixel on the 2D image using the volume rendering technique described in~\cite{kajiya1984ray}, which is also illustrated in Fig.~\ref{fig:problem}. Details about the integration are given in Appendix~\ref{supp:nerf-render}. 



%\str{Write down the integral.}
%


% Neural Fields are normally considered as an optimization routine in a deterministic setting, whereby95
% the function fNeRF is fit perfectly to the available observations (akin to “overfitting” training data).


\subsection{Probabilistic NeRF Generalization}
% \js{probabilistic NeRF is not new}

\paragraph{Deterministic Neural Radiance Fields} Neural Radiance Fields are normally considered as an optimization routine in a deterministic setting~\citep{mildenhall2021nerf,barron2021mip}, whereby the function $f_{\text{NeRF}}$ fits specifically to the available observations (akin to ``overfitting'' training data).

\paragraph{Probabilistic Neural Radiance Fields} As we are not just interested in fitting a single and specific 3D object but want to learn how to infer the Neural Radiance Field of any 3D object,  we focus on probabilistic Neural Radiance Fields with the following factorization:
\begin{equation}
    p({\bf{\widetilde{Y}}} | {\bf{\widetilde{X}}}) \varpropto
    \underbrace{p({\bf{\widetilde{Y}}} | {\bf{{Y}}}, {\bf{{X}}})}_{\text{Integration}}
    \underbrace{p({\bf{{Y}}} | {\bf{{X}}})}_{\text{NeRF Model}}
    \underbrace{p({\bf{{X}}} | {\bf{\widetilde{X}}})}_{\text{Sampling}}.
\label{eq: probabilitic_NeRF}
\end{equation}
%
% \str{Is $f_\text{NeRF}$ now a random variable? Normally it is not.}
%\str{This can also be writtena  more fluently}
The generation process of this probabilistic formulation is as follows.
We first start from (or sample) a set of rays $\widetilde{\mathbf{X}}$.
Conditioning on these rays, we sample 3D points in space $\mathbf{X} \big|\widetilde{\mathbf{X}}$.
Then, we map these 3D points into their colors and density values with the NeRF function, ${\bf{Y}} = f_{\text{NeRF}}({\bf{{X}}})$.
Last, we sample the 2D pixels of the viewing image that corresponds to the 3D ray ${\widetilde{\bf{Y}}}| {\bf{{Y}}}, {\bf{X}}$ with a probabilistic process. This corresponds to integrating colors and densities ${\bf{{Y}}}$ along the ray on locations ${\bf{X}}$.

% In the following sections, we will define the various probabilistic terms.
% \str{Here it would be good to be more explicit and say how are the various probabistic terms are defined. Or we can say that we will specify later, also in the context of Geometric NP. Either way, the current text below looks like deterministic relations, so I think we can not write them down here.}

%\str{I suggest we go directly on conditional neural fields. The way we have it now, we only create extra confusion, unless we are the first to propose this decomposition (but there have been other probabilistic NeRFs before, no?). Or perhaps have better structure in the writing, otherwise it is confusing.. What is context, what target?}
The probabilistic model in \cref{eq: probabilitic_NeRF} is for a single 3D object, thus requiring optimizing a function $f_{\text{NeRF}}$ afresh for every new object, which is time-consuming. For NeRF generalization, we accelerate learning and improve generalization by amortizing the probabilistic model over multiple objects, obtaining per-object reconstructions by conditioning on context sets ${{\widetilde {\bf{X}}}_C, {\widetilde {\bf{Y}}}_C}$.
% \str{What about $\widetilde {\bf{X}}_T$? What is that? Also, why (1) has small letters, and here we have capitals?}
% These context variables are few observations from any new object, that is, the rays and the corresponding observed colors.
For clarity, we use ${(\cdot)}_{C}$ to indicate context sets with {a few new observations for a new object}, while ${(\cdot)}_{T}$ indicates target sets containing 3D points or camera rays from novel views of the same object.
Thus, we formulate a probabilistic NeRF for generalization as:
% \str{update this according to the above equation}
%
\begin{equation}
\begin{aligned}
    &p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto \\
&    \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
    \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{NeRF Generalization}}
    \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}}.
\end{aligned}
\label{eq: probabilitic_NeRF_generalization}
\end{equation}
%
%\str{Not sure if this sentence is good enough, please check later again.}
As this paper focuses on generalization with new 3D objects, we keep the same sampling and integrating processes as in ~\cref{eq: probabilitic_NeRF}. We turn our attention to the modeling of the predictive distribution $p({\bf{{Y}}}_{T}| {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ in the generalization step, which implies inferring the NeRF function.

\paragraph{Misalignment between 2D context and 3D structures} It is worth mentioning that the predictive distribution in 3D space is conditioned on 2D context pixels with their ray $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ and 3D target points ${\bf{X}}_{T}$, which is challenging due to potential information misalignment. Thus, we need strong inductive biases with 3D structure information to ensure that 2D and 3D conditional information is fused reliably.




% \subsection{Geometric Neural Processes for NeRF} 
\subsection{Geometric Bases} 
\label{sec: geometrybases}
% In NeRF generalization, given that the context set is expected to correspond to too few views with few 3D information, 
% % In NeRF generalization, given the context set corresponding to too few 2D views providing few 3D information, 
% fitting a model for $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ that generalizes well is challenging. 
To mitigate the information misalignment between 2D context views and 3D target points, we introduce geometric bases ${\bf{{B}}}_{C}=\{{\bf{b}}_i\}_{i=1}^{M}$, which {induces prior structure to the context set} $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ geometrically. $M$ is the number of geometric bases. 

\begin{figure*}[t]
  \centering  \includegraphics[width=0.99\textwidth]{./Figures/architecture-0.pdf} % Adjust the size and filename as needed
  % \vspace{-2mm}
\caption{\textbf{Illustration of our Geometric Neural Processes.} 
% We solve the problem of radiance field generalization by Neural Processes. 
% captures uncertainty induced by few available observations.
We cast radiance field generalization as a probabilistic modeling problem. Specifically, we first construct geometric bases ${\bf{B}}_C$ in 3D space from the 2D context sets ${\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}$ to model the 3D NeRF function (Section~\ref{sec: geometrybases}). We then infer the NeRF function by modulating a shared MLP through hierarchical latent variables ${\bf{z}}_{o}, {\bf{z}}_{r}$ and make predictions by the modulated MLP (Section~\ref{sec: hierar}). 
  The posterior distributions of the latent variables are inferred from the target sets ${\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{Y}}}_{T}$, which supervises the priors during training (Section~\ref{sec: object}). 
  } % Caption for the figure
  \label{fig: framework}
  %\vspace{-2mm}
\end{figure*}

%\str{where is the semantic representation coming from? Self-supervised models? Or is it learned?}
Each geometric basis consists of a Gaussian distribution in the 3D point space and a semantic representation, \textit{i.e.,} ${\bf{b}}_i = \{ \mathcal{N}(\mu_i, \Sigma_i); \omega_i\}$, 
%\str{What is $\omega_i$ in the equation? The weight of the Gaussian?We have mixtures of Gaussians? Please clarify.} 
where $\mu_i$ and $\Sigma_i$ are the mean and covariance matrix of $i$-th Gaussian in 3D space, and $\omega_i$ is its corresponding latent representation. 
Intuitively, the mixture of all 3D Gaussian distributions implies the structure of the object, while $\omega_i$ stores the corresponding semantic information.
% from a 2D context set, e.g., color and texture. 
In practice, we use a transformer-based encoder to learn the Gaussian distributions and representations from the context sets, \textit{i.e.,} $\{(\mu_i, \Sigma_i, \omega_i)\} = \texttt{Encoder} [{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}]$. Detailed architecture of the encoder is provided in Appendix~\ref{supp:gaussian}. 



With the geometric bases $\mathbf{B}_C$, we review the predictive distribution from  $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ to $p({\bf{Y}}_{T}| {\bf{X}}_{T},{\bf{{B}}}_{C})$.  By inferring the function distribution $p(f_{\text{NeRF}})$, we reformulate the predictive distribution as: 
\begin{equation}
    % p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = 
    p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C}) = \int p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T}) p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C}) df_{\text{NeRF}},
\label{eq: predictive_w_B}
\end{equation}
where $p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C})$ is the prior distribution of the NeRF function, and $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ is the likelihood term. 
% We integrate the likelihood term with all possible NeRF functions. 
%\str{How do we do this? The space to integrate over must be huge, no?}
%\str{The following sentence is a bit weird, can you check it again.}
% \wy{We integrate the likelihood term over the latent space of all possible variables for modulating NeRF functions by monte carlo sampling, which can be seen as integrating over a function distribution.}
Note that the prior distribution of the NeRF function is conditioned on the target points ${\bf{X}}_{T}$ and the geometric bases ${\bf{B}}_{C}$. 
Thus, the prior distribution is data-dependent on the target inputs, yielding a better generalization on novel target views of new objects. 
Moreover, since ${\bf{B}}_{C}$ is constructed with continuous Gaussian distributions in the 3D space, the geometric bases can enrich the locality and semantic information of each discrete target point, enhancing the capture of high-frequency details~\citep{chen2023neurbf,chen2022tensorf,muller2022instant}.



\subsection{Geometric Neural Processes with Hierarchical Latent Variables}
\label{sec: hierar}

% To achieve the 
With the geometric bases, we propose Geometric Neural Processes (\textbf{\method{}}) by inferring the NeRF function distribution $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$ in a probabilistic way.  
% We can generalize NeRF learning and efficiently adapt the functional distribution to new 3D objects.
Based on the probabilistic NeRF generalization in~\cref{eq: probabilitic_NeRF_generalization}, we introduce hierarchical latent variables to encode various spatial-specific information into $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$, improving the generalization ability in different spatial levels.
%\str{Make sure that notation is consistent and not overloaded. Eg, $x^r$ rather than $x^\mathbf{r}$ since it is not that we use the $1:P$ somewhere specific, besides it is not clear that this corresponds to a ray, since the $P$ points could be anywhere.}
Since all rays are independent of each other, we decompose the predictive distribution in \cref{eq: predictive_w_B} as:
\begin{equation}
    p({\bf{Y}}_{T}| {\bf{X}}_{T},  {\bf{B}}_{C})  = \prod_{n=1}^{N} p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{{\mathbf{r}}, n},  {\bf{B}}_{C}),
\label{eq: predictive_distribution_ray_specific}
\end{equation}
where the target input ${\bf{X}}_{T}$ consists of $N \times P$ location points $\{{\bf{x}}_{T}^{{\mathbf{r}}, n}\}_{n=1}^{N}$ for $N$ rays.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\columnwidth]{./Figures/graphical_model2.pdf} 
\caption{\textbf{Graphical model for the proposed geometric neural processes.}}
\label{fig-supp: graphical_model}
\end{figure}

Further, we develop a hierarchical Bayes framework for \method{} to accommodate the data structure of the target input ${\bf{X}}_{T}$ in \cref{eq: predictive_distribution_ray_specific}.
We introduce an object-specific latent variable $\mathbf{z}_o$ and $N$ individual ray-specific latent variables $\{\mathbf{z}_r^{n}\}_{n=1}^{N}$ to represent the randomness of $f_\text{NeRF}$.
% the probabilistic NeRF function. 


%\str{The formatting here looks weird. Is this the right template?}
Within the hierarchical Bayes framework, $\mathbf{z}_o$ encodes the entire object information from all target inputs and the geometric bases $\{\mathbf{X}_T, \mathbf{B}_C\}$ in the global level; while every $\mathbf{z}_r^{n}$ encodes ray-specific information from $\{ \mathbf{x}_T^{\mathbf{r}, n}, \mathbf{B}_C\}$ in the local level, which is also conditioned on the global latent variable $\mathbf{z}_o$. 
The hierarchical architecture allows the model to exploit the structure information from the geometric bases $\mathbf{B}_C$ in different levels, improving the model's expressiveness ability.
By introducing the hierarchical latent variables in \cref{eq: predictive_distribution_ray_specific}, we model \method{} as:
{\small
\begin{equation}
\begin{aligned}
        p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) &= \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o ) \\
        &p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o,
\end{aligned}
\label{eq:ganp-model}
\end{equation}
}where $p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_o, {\bf{z}}_r^i)$ denotes the ray-specific likelihood term. In this term, we use the hierarchical latent variables $\{{\bf{z}}_o, {\bf{z}}_r^i\}$ to modulate a ray-specific NeRF function $f_{\text{NeRF}}$ for prediction, as shown in Fig.~\ref{fig: framework}.
% In general, we first use the object-specific latent variable $\mathbf{z}_o$ to make $f_{NeRF}$ object-specific. Then, the ray-specific latent variable $\mathbf{z}_r$ to enable $f_{\text{NeRF}}$ to capture the local texture information.
Hence, $f_{\text{NeRF}}$ can explore global information of the entire object and local information of each specific ray, leading to better generalization ability on new scenes and new views.
A graphical model of our method is provided in Fig.~\ref{fig-supp: graphical_model}. 


In the modeling of {\method{}}, the prior distribution of each hierarchical latent variable is conditioned on the geometric bases and target input. 
%\textcolor{blue}{To infer each latent variable, we first integrate the geometric bases and each target input, yielding a location representation specific to the target input. The location representation has access to relevant locality information from the geometry bases, \textit{i.e.}, $<{\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C >$. }
% For generalization, we need to infer latent variables that are specific to the target input. 
% To this end, 
We first represent each target location by integrating the geometric bases, \textit{i.e.}, $<{\bf{x}}_{T}^{n}, {\bf{B}}_C >$, which aggregates the relevant locality and semantic information for the given input. 
Since ${\bf{B}}_{C}$ contains $M$ Gaussians, we employ a Gaussian radial basis function in \cref{suppeq:rbf_agg} between each target input ${\bf{x}}_{T}^{ n}$ and each geometric basis ${\bf{b}}_i$ to aggregate the structural and semantic information to the 3D location representation. Thus, we obtain the 3D location representation as follows:
\begin{equation}
\label{suppeq:rbf_agg}
    <{\bf{x}}_{T}^{n}, {\bf{B}}_C > = \texttt{MLP}\Big[\sum_i^{M} \exp (-\frac{1}{2}({\bf{x}}_{T}^{n}-\mu_i)^T\Sigma_i^{-1}({\bf{x}}_{T}^{n}-\mu_i) ) \cdot \omega_i\Big],
\end{equation} 
where $\texttt{MLP}[\cdot]$ is a learnable neural network.
With the location representation $<{\bf{x}}_{T}^{n}, {\bf{B}}_C >$, we next infer each latent variable hierarchically, in object and ray levels. 

\noindent {\textbf{Object-specific Latent Variable.}} The distribution of the object-specific latent variable ${\bf{z}}_o$ is obtained by aggregating all location representations:
\begin{equation}
    [\mu_{{o}}, \sigma_{{o}}] 
    = \texttt{MLP}\Big[\frac{1}{N \times P}\sum_{n = 1}^{N}\sum_{\mathbf{r}}
    <{\bf{x}}_{T}^{n}, {\bf{B}}_C >\Big],
\end{equation} 
where we assume $p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)$ is a standard Gaussian distribution and generate its mean $\mu_{o}$ and variance $\sigma_{o}$ by a ~\texttt{MLP}. 
Thus, our model captures objective-specific uncertainty in the NeRF function.


\noindent {\textbf{Ray-specific Latent Variable.}} 
% By ray-specific latent variable, the object-specific is expected to capture the local details.
To generate the distribution of the ray-specific latent variable, we first average the location representations ray-wisely. 
We then obtain the ray-specific latent variable by aggregating the averaged location representation and the object latent variable through a lightweight transformer. We formulate the inference of the ray-specific latent variable as:
\begin{equation}
    [\mu_{{r}}, \sigma_{{r}}] = \texttt{Transformer} \Big[\texttt{MLP}[\frac{1}{P}\sum_{\mathbf{r}}
    <{\bf{x}}_{T}^{n}, {\bf{B}}_C >]; \hat{{\bf{z}}}_o \Big],
\end{equation}
where $\hat{{\bf{z}}}_o$ is a sample from the prior distribution $p({\bf{z}}_o | {\bf{X}}_T, {\bf{B}}_C)$. 
Similar to the object-specific latent variable, we also assume the distribution $p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C)$ is a mean-field Gaussian distribution with the mean $\mu_{{r}}$ and variance $\sigma_{{r}}$. We provide more details of the latent variables in Appendix~\ref{supp:latent-variables}.



\noindent  \textbf{NeRF Function Modulation.}
With the hierarchical latent variables $\{{\bf{z}}_o, {\bf{z}}_r^n\}$, we modulate a neural network for a 3D object in both object-specific and ray-specific levels.  Specifically, the modulation of each layer is achieved by scaling its weight matrix with a style vector~\citep{guo2023versatile}. 
The object-specific latent variable ${\bf{z}}_o$ and ray-specific latent variable ${\bf{z}}_r^n$ are taken as style vectors of the low-level layers and high-level layers, respectively. The prediction distribution $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C})$ are finally obtained by passing each location representation through the modulated neural network for the NeRF function. 
More details are provided in Appendix~\ref{supp:modulate}. 
% The modulated MLP layer used in our paper is similar to the style \textit{modulation} in ~\cite{guo2023versatile}. Essentially, we predict a style vector $s\in \mathbb{R}^{d_{in}}$ to multiply or scale the weight matrix of an MLP, $W \in \mathbb{R}^{d_{in} \times d_{out}}$.


% \subsection{Inference of Geometry-aware Neural Processes}
% \label{sec: elbo}
% ELBO

% \noindent{\textbf{Variational Posteriors with the Geometry Bases.}} Solving \textbf{GANPs} with Eq.~\ref{eq:ganp-model} involves estimating the true posterior, $p({\bf{g}},  \{{\bf{r}}_i\}_{r=1}^{N_{ray}} | {\widetilde{\bf{X}}}_T, {\widetilde{\bf{Y}}}_T)$ which is intractable. Hence, we introduce a variational posterior distribution, which can be factorized as follows:
% \begin{equation}
% p({\bf{g}},  \{{\bf{r}}_i\}_{r=1}^{N_{ray}} | {\widetilde{\bf{X}}}_T, {\widetilde{\bf{Y}}}_T) \approx q_{\theta, \phi}({\bf{g}},  \{{\bf{r}}_i\}_{r=1}^{N_{ray}} | {\bf{X}}_T, {\bf{B}}_T),
% \end{equation}

\subsection{Empirical Objective}
\label{sec: object}

\noindent{\textbf{Evidence Lower Bound.}} 
To optimize the proposed \method{},
we apply variational inference~\citep{garnelo2018neural} and derive the evidence lower bound (ELBO) as:
\begin{equation}
\begin{aligned}
% \mathcal{L}_{\text{ELBO}}
& \log   p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C})
\geq \\
&\mathbb{E}_{q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)}  \Big\{  \sum_{n=1}^{N}  \mathbb{E}_{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})} \log p({\bf{y}}_{T}^{{\mathbf{r}}, n}| {\bf{x}}_{T}^{{\mathbf{r}}, n}, {\bf{z}}_o, {\bf{z}}_r^n) \\
&- D_{\text{KL}}[q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {{\bf{B}}_T}) || p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {{\bf{B}}_C}) ] \Big\} \\
& - D_{\text{KL}}[q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T) || p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)], \\
\end{aligned}
\end{equation}
where $q_{\theta, \phi}({\bf{z}}_o,  \{{\bf{z}}_r^i\}_{i=1}^{N} | {\bf{X}}_T, {\bf{B}}_T) = \Pi_{i=1}^{N}q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {{\bf{B}}_T}) q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)$ is the involved variational posterior for the hierarchical latent variables.  ${\bf{B}}_T$ is the geometric bases constructed from the target sets $\{{\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{Y}}}_{T}\}$, which are only accessible during training. 
The variational posteriors are inferred from the target sets during training, which introduces more information on the object. 
The prior distributions are supervised by the variational posterior using Kullback–Leibler (KL) divergence, learning to model more object information with limited context data and generalize to new scenes. Detailed derivations are provided in Appendix~\ref{supp:elbo}.

% \noindent{\textbf{Empirical Objective.}} 
For the geometric bases $\mathbf{B}_C$, we regularize the spatial shape of the context geometric bases to be closer to that of the target one $\mathbf{B}_T$ by introducing a KL divergence. 
Therefore, given the above ELBO, our objective function consists of three parts: a reconstruction loss (MSE loss), KL divergences for hierarchical latent variables, and a KL divergence for the geometric bases. 
%constraint for matching the two sets of Gaussian basis to ensure the basis obtained from context is as close to the one from the target. 
The empirical objective for the proposed \method{} is formulated as:
\begin{equation}
\begin{aligned}
& \mathcal{L}_{\text{\method{}}}  =  ||y - y'||^2_2 + \alpha \cdot \big(  D_{\text{KL}} [p(\mathbf{z}_o|{\bf{B}}_C)|q(\mathbf{z}_o|{\bf{B}}_T)] \\
    & + D_{\text{KL}}[p(\mathbf{z}_r|\mathbf{z}_o,{\bf{B}}_C)|q(\mathbf{z}_r|\mathbf{z}_o,{\bf{B}}_T)] \big) + \beta \cdot D_{\text{KL}}[{\bf{B}}_C, {\bf{B}}_T],
\end{aligned}
\end{equation}
where $y'$ is the prediction. $\alpha$ and $\beta$ are hyperparameters to balance the three parts of the objective. The KL divergence on ${\bf{B}}_C, {\bf{B}}_T$ is to align the spatial location and the shape of two sets of bases. 

% +++++++++++++

% \noindent 
% \textbf{Neural Fields.}
% % \zx{First introduce NeRF (with problem definition and notations) and its disadvantages of inefficient inference, then say what we will do to avoid this problem by NP?} 


% In general, training a NeRF requires overfitting each scene, which is time-consuming. Hence, how to leverage the observation for generalization on the new scene remains a problem. As each INR is a function of a data sample, the problem can be viewed as estimating the distribution of functions. This motivates us to use the Neural Process (NP) in this problem. 
 

% \noindent {\textbf{Neural Process.}} Neural Process~\cite{garnelo2018neural} parameterizes the distribution of functions. Given the context set comprising of the observation $X$ and the corresponding labels $Y$, $D_C = (X_C, Y_C) := (\mathbf{x}_i, \mathbf{y}_{i})_{i\in C} $. The aim of NP is to learn a mapping function from the target points $X_T$ to the target labels $Y_T$,  $D_T = (X_T, Y_T) := (\mathbf{x}_i, \mathbf{y}_{i})_{i\in T} $. The conditional distribution of target points is:
% \begin{equation}
%     p_{\phi}(Y_T|X_T,D_C) = \prod_{\mathbf{x}, \mathbf{y}\in D_{T}} \mathcal{N}(\mathbf{y};\mu_{\mathbf{y}}(\mathbf{x},D_c), \sigma^2_{\mathbf{y}}(\mathbf{x},D_c)).
% \end{equation}

% \begin{equation}
%     p_{\phi}(Y_T|X_T, \mathbf{z}) = \prod_{\mathbf{x}, \mathbf{y}\in D_{T}} \mathcal{N}(\mathbf{y};\mu_{\mathbf{y}}(\mathbf{z}, X_T,D_c), \sigma^2_{\mathbf{y}}(\mathbf{z}, X_T,D_c)),
% \end{equation}
% where $\mathbf{z} \sim p_{\theta}(\mathbf{z|X_T,D_C})$. Using NP can efficiently leverage the limited context/observation to infer the function of INR for the target from a probabilistic perspective. It also can incorporate uncertainty estimation for the unseen view. This is reasonable as the value in the unseen target location should not be deterministic. 

%\zx{advantages of NP? incorporating uncertainty for limited context information, which is suitable for reconstruction tasks?}







% \begin{align}
%     p(\mathbf{y}|\mathbf{x}, I) & = \int_{g} \int_{r}  p(\mathbf{y}, g, r |\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \\
%     &=  \int_{g} \int_{r}  p(\mathbf{y}| g, r) p(g, r | \mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \\
%     % &= \int_{g} \int_{r} \int_{B} p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \mathrm{d}B 
%     &= \int_{g} \int_{r}  p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r 
% \end{align}


%\subsection{Gaussian Basis for INR}
%As shown in Fig.~\ref{fig:framework}, 


%\subsection{Hierarchical Neural Process for INR}







