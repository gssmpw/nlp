
\subsection{Preliminary}

\textcolor{red}{start with 3D, make it clear on 3D. function. }

\noindent 
\textbf{Neural Fields.}
% \zx{First introduce NeRF (with problem definition and notations) and its disadvantages of inefficient inference, then say what we will do to avoid this problem by NP?} 
The implicit neural field defines a continuous function $f_\theta : \mathbf{x} \mapsto \mathbf{y}$ from spatial coordinates to arbitrary signals, where $\theta$ is the function weights, $X\in \mathbb{R}^c$ is the coordinates, and $Y\in \mathbb{R}^d$ is the target signal. In neural radiance field (NeRF)~\cite{mildenhall2021nerf}, the 3D locations with view direction $v$ are mapped to 4D tuples of the densities and RBG values. However, training a NeRF requires overfitting each scene, which is time-consuming. Hence, how to leverage the observation for generalization on the new scene remains a problem. As each INR is a function of a data sample, the problem can be viewed as estimating the distribution of functions. This motivates us to use the Neural Process (NP) in this problem. 
 



\noindent {\textbf{Neural Process.}} Neural Process~\cite{garnelo2018neural} parameterizes the distribution of functions. Given the context set comprising of the observation $X$ and the corresponding labels $Y$, $D_C = (X_C, Y_C) := (\mathbf{x}_i, \mathbf{y}_{i})_{i\in C} $. The aim of NP is to learn a mapping function from the target points $X_T$ to the target labels $Y_T$,  $D_T = (X_T, Y_T) := (\mathbf{x}_i, \mathbf{y}_{i})_{i\in T} $. The conditional distribution of target points is:
\begin{equation}
    p_{\phi}(Y_T|X_T,D_C) = \prod_{\mathbf{x}, \mathbf{y}\in D_{T}} \mathcal{N}(\mathbf{y};\mu_{\mathbf{y}}(\mathbf{x},D_c), \sigma^2_{\mathbf{y}}(\mathbf{x},D_c)).
\end{equation}

\begin{equation}
    p_{\phi}(Y_T|X_T, \mathbf{z}) = \prod_{\mathbf{x}, \mathbf{y}\in D_{T}} \mathcal{N}(\mathbf{y};\mu_{\mathbf{y}}(\mathbf{z}, X_T,D_c), \sigma^2_{\mathbf{y}}(\mathbf{z}, X_T,D_c)),
\end{equation}
where $\mathbf{z} \sim p_{\theta}(\mathbf{z|X_T,D_C})$. Using NP can efficiently leverage the limited context/observation to infer the function of INR for the target from a probabilistic perspective. It also can incorporate uncertainty estimation for the unseen view. This is reasonable as the value in the unseen target location should not be deterministic. 

%\zx{advantages of NP? incorporating uncertainty for limited context information, which is suitable for reconstruction tasks?}


\subsection{$N^2\text{P}$}
\textcolor{red}{Name: use $N^2\text{P}$ temporarily.}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{Figures/framework_draft.pdf} % Adjust the size and filename as needed
  \caption{Framework.} % Caption for the figure
  \label{fig:framework}
\end{figure}

To improve the generalization ability and achieve efficient inference on new scenes of Neural Field methods, we propose to learn to infer specific model parameters for each scene by Neural Processes.
\zx{Advantages}
In the following, we use NeRF generalization to illustrate the proposed method.  In NeRF generalization, a few views (one or two) of images $I_c$ and its corresponding camera pose $\mathbf{p}$ serve as the context information. 
%We represent each location in this scene by aggregating these Gaussians basis based on the radial basis function (RBF). 
Then, based on the context information, the task is to estimate an implicit function that can be used to infer the target for unseen views.

The proposed GP-INR framework assumes a space composed of a set of Gaussian bases that encodes the spatial location and semantic information of a scene. Initially, GP-INR predicts a Gaussian basis $B_C$ from the observed context. Similar to the recent advances in implicit neural fields (e.g., Neurbf~\cite{chen2023neurbf}), at arbitrary new locations $X_T \in \mathbb{R}^{N\times P\times 3}$, we aggregate the Gaussians to form a new representation of this location $X'_C$ via the radial basis function (RBF). By averaging the representations both globally and locally along rays, we obtain a global latent $g_C$ and a ray-specific latent $r_C$ to modulate an MLP for predicting the implicit neural field. During training, with the target view and image $I_T$ also provided, we perform the same procedures for $I_T$ and align latent variables from both the context and target. For simplicity, in the following, we will use $I_C$ as an example to illustrate. 

NerF: $F: (x,y,z,\theta,\phi) \mapsto (c,\sigma)$. 

Context set (camera pose and image rbg level): 
\begin{align}
\tilde{X}_c:&= \{ \tilde{x}_i \}_{i=1}^{N_c}, \tilde{x}_i \in \mathbb{R}^{6} (\text{ray-o}, \text{ray-d})\\
\tilde{Y}_c:&= \{ \tilde{y}_i \}_{i=1}^{N_c}, \tilde{y}_i \in \mathbb{R}^{3} (\text{RGB})
\end{align}

Target set (3d coordinates and 3d rbg with density level):

$x_i$ is a set of 3D points corresponding to a specific ray direction. 
\begin{align}
X_t:&= \{ x_i \}_{i=1}^{N_t}, x_i \in \mathbb{R}^{P\times3} (\text{sampled 3D points})\\
Y_t:&= \{ y_i \}_{i=1}^{N_t}, y_i \in \mathbb{R}^{P\times4} (c,\sigma)
\end{align}

\begin{align}
    p(Y_t|X_t, \tilde{X}_c, \tilde{Y}_c) & = p(Y_t|X_t,B_c)p(B_c|\tilde{X}_c, \tilde{Y}_c) \\
    &= \Pi_{i=1}^{N_t} p(y_i|x_i, g, r_i, B_c) p(r_i|g, x_i) p(g|B_c, X_t) p(B_c|\tilde{X}_c, \tilde{Y}_c)
\end{align}

\begin{align}
    p(Y_t|X_t, \tilde{X}_c, \tilde{Y}_c) & \approx p(Y_t|X_t,B_c) \\
    &= \Pi_{i=1}^{N_t} p(y_i|x_i, g, r_i, B_c) p(r_i|g, x_i) p(g|B_c, X_t)
\end{align}

Question: 
\textcolor{red}{Does the gap between $(\tilde{X},\tilde{Y})$ and $(X,Y)$ support our motivation of using basis for NP? Motivation: as there is an information gap between $(\tilde{X},\tilde{Y})$ as $(X)$ is sampled from $(\tilde{X}$, Y can be seen as sampled from $\tilde{Y}$ ($Y$ is discrete, $\tilde{Y}$ is the integration result over all the ray), we introduce a Gaussian basis to encode the geometry information in the space. The Gaussian basis with radial basis function provides a continuous way of representing spatial location to reduce the information loss. 
}
\begin{align}
    p(\mathbf{y}|\mathbf{x}, I) & = \int_{g} \int_{r}  p(\mathbf{y}, g, r |\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \\
    &=  \int_{g} \int_{r}  p(\mathbf{y}| g, r) p(g, r | \mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \\
    % &= \int_{g} \int_{r} \int_{B} p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \mathrm{d}B 
    &= \int_{g} \int_{r}  p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r 
\end{align}


\subsection{Gaussian Basis for INR}
%As shown in Fig.~\ref{fig:framework}, 
\noindent {\textbf{Gaussian Basis.}} When a human sees a view of a scene (context image), the posterior knowledge about the color and spatial location of the scene is revealed. We encode this posterior knowledge as a form of Gaussian basis set, which is spanned in the space. 
Given an image $I_C \in \mathbb{R}^{H\times W \times 3}$, a visual self-attention module first produces a $M\times D$ tokens with $M$ as the number of visual tokens, and $D$ as the hidden dimension. The number of Gaussians we use equals to the number of tokens $M$. That being said, we use one MLP to predict the spatial shape of Gaussians, including the mean $\mu$, as well as the rotation and scaling parameters for producing the covariance matrix $\Sigma$, and one MLP to produce the corresponding latent representations $\omega \in \mathbb{R}^{M \times D} $. The covariance matrix is obtained by:
\begin{equation}
    \Sigma = RSS^TR^T,
    \label{eq:cov-matrix}
\end{equation}
where $R\in \mathbb{R}^{3\times3}$ is the rotation matrix, and $S \in \mathbb{R}^3$ is the scaling matrix. 

By using the Gaussian basis set, we are able to 1). aggregate the locality information for the input which is similar to NeuRBF~\cite{chen2023neurbf}; 2). generate a global latent variable of the scene and location-specific latent variables for modulating the implicit neural field for generalization purposes.

\noindent {\textbf{Aggregation.}} We represent each spatial location in the scene by considering the scene information. For each spatial location, we aggregate the locality information to the current location. Note that, in the classical INR, the locality latents are learned by overfitting, while ours is based on the predicted Gaussian basis. For arbitrary target locations $X_t$, we employ a Gaussian radial basis function to aggregate the information from the predicted Gaussian basis and obtain a new representation of this location $X'_C$:
\begin{equation}
    X'_C = \text{MLP}(\sum_i^{M} \varphi(x, \mu_i, \Sigma_i)\omega_i)
\end{equation}

\begin{equation}
    \varphi(x, \mu_i, \Sigma_i) = \exp(-\frac{1}{2} (x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i))
\end{equation}

\subsection{Hierarchical Neural Process for INR}


\noindent{{\textbf{Hierarchical Neural Process.}}} Based on the re-represented spatial representation, we perform a hierarchical neural process. Our latent variables consist of two parts, the global variable $g_C$ and the ray or location-specific variables $r_C$. The formulation of NP is given by:
% \begin{equation}
%     p(\mathbf{y}|\mathbf{x}, I_C) = \int p(\mathbf{y}|\mathbf{x}, g_C, r_C) p(g_C, r_C|\mathbf{x}, I_C). 
% \end{equation}

\begin{align}
    p(\mathbf{y}|\mathbf{x}, I) = \int_{g} \int_{r}  p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r 
\end{align}

\textcolor{red}{To be discussed how to write}

\noindent{\textbf{ELBO.}} $p_{\theta_3}$ is a deterministic function, and $p_{\theta_1}$ is the decoder, which is the modulated implicit function in our case. Hence, we are interested in the posterior of $p_{\theta_2}(\mathbf{g}, \mathbf{r} | B)$. We want to align the posteriors from the context and target during training. 

\begin{align}
    \log p(\mathbf{y_t}|\mathbf{x_t}, I_c) = \mathbb{E}_{q(g_c,r_c|\mathbf{x_t}, I_t)} 
\end{align}


We parameterize $p(g_C, r_C|\mathbf{x}, I_C)$ as hierarchical latent variables. We first aggregate globally to obtain $p(g_C)$. The local variable $p(r_C)$ is obtained by firstly aggregating ray-wisely and then updating it through a transformer together with the global vector sampled from $p(g_C)$. 



\noindent {\textbf{Global Variable.}} We use a global latent variable $g$ to describe the scene. In NeRF reconstruction, points are sampled from rays, leading to the target points $X_T\in \mathbb{R}^{N\times P\times 3}$, where $N$ is the number of rays, and $P$ is the number of points sampled from each ray. To obtain the global variable, we average over all points and pass through an MLP. 
\begin{equation}
    g_C = [\mu^c_g, \sigma^c_g] = \text{MLP}(\frac{1}{N\times P}\sum_j^{N\times P}(X'_C)[j]),
\end{equation}
where $\mu^c_g$ and $\sigma^c_g$ are the mean and variance of global variables, respectively. We sample global latent vector by $\hat{g}_c \sim \mathcal{N}(\mu^c_g, \sigma^c_g)$. 


\noindent {\textbf{Location-specific Variable.}} Moreover, modulating the implicit function through the global variable lacks the local detail information. Hence, we propose a spatial location-specific modulation (ray-specific in NerF). We infer a ray-specific variable $r_c$ by first aggregating the feature ray-wisely:
\begin{equation}
    r'_c = \text{MLP}(\frac{1}{ P}\sum_j^{P}(X'_C)[j])
\end{equation}


Then, we perform a hierarchical formulation by incorporating the global variable to obtain the ray-specific variable. This enables us to design the latent variable in a hierarchical structure, which is rational as it learns the scene coarse-to-fine. $r_c$ is obtained by feeding $r'_c$ together with the sampled $\hat{g}_c$ into a Transformer: 
\begin{equation}
    r_c = [\mu^c_r, \sigma^c_r] = \text{Transformer}([\hat{g}_c; \hat{r}_C]),
\end{equation}
where $\mu^r_g$ and $\sigma^r_g$ are the mean and variance of global variables, respectively. Details of the Transformer architecture are given in the Appendix. 


\noindent {\textbf{Modulation.}} The latent variable to modulate the MLP is $[g_c;r_c]$
The modulated MLP layer used in our paper is similar to the style \textit{modulation} in ~\cite{guo2023versatile}. Essentially, we predict a style vector $s\in \mathbb{R}^{d_{in}}$ to multiply or scale the weight matrix of an MLP, $W \in \mathbb{R}^{d_{in} \times d_{out}}$.




\noindent{\textbf{Loss.}} Our loss consists of three parts, the regression loss $L_{\text{MSE}}$, the KL divergence between latent variables from context and target, and the constraint for matching the two sets of Gaussian basis to ensure the basis obtained from context is as close to the one from the target. 

\begin{equation}
    L_{\text{MSE}} = ||y - y'||^2_2 + \alpha D_{\text{KL}}(p(g_C|B_C)|q(g_T|B_T)) + \beta \text{KL}(B_C, B_T)
\end{equation}
where $y'$ is the prediction, $D_{\text{KL}}$ is the KL divergence, $\alpha$ and $\beta$ are two weighting hyperparameters. 

