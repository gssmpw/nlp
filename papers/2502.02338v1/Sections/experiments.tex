
% We validate \name{} extensively in 1D, 2D, and 3D in reverse order.

To show the generality of \name{}, we validate extensively on five datasets, comparing in 2D, 3D, and 1D settings with the recent state-of-the-art.

\subsection{\name{} in 2D image regression}

We start with experiments in 2D image regression, a canonical task~\citep{tancik2021learned,sitzmann2020implicit} to evaluate how well Neural Fields can fit and represent a 2D signal.
In this setting, the context set is an image and the task is to learn an implicit function that regresses the image pixels accurately. % in terms of PSNR.
Following TransINR~\citep{chen2022transformers}, we resize each image into $178\times 178$, and use patch size 9 for the tokenizer. The self-attention module remains the same as 
%the one in the NeRF experiments (Sec.~\ref{sec:3d-nerf}).
our baseline, VNP~\citep{guo2023versatile}. 
For the Gaussian bases, we predict 2D Gaussians.
The hierarchical latent variables are inferred in image-level and pixel-level. 
We evaluate the method on two real-world image datasets as used in previous works~\citep{chen2022transformers,tancik2021learned,gu2023generalizable}.

\paragraph{CelebA~\citep{liu2015deep}.}
CelebA encompasses approximately 202,000 images of celebrities, partitioned into training (162,000 images), validation (20,000 images), and test (20,000 images) sets.

\paragraph{Imagenette dataset~\citep{imagenette}.}
Imagenette is a curated subset comprising 10 classes from the 1,000 classes in ImageNet~\citep{deng2009imagenet}, consists of roughly 9,000 training images and 4,000 testing images.

%\str{What is the point of image regresssion when the image itself is used as context? Why not just copy the context??}
%\str{The following is a bit strante, is this still image regression? Why not have a separate section?}
%\str{Is Image Regression the standard name for this task?}

\input{./Tables/image-results}
\emph{Quantitative results.}
We give quantitative comparisons in Table~\ref{tab:image-regression}.
\name{} outperforms baselines on both CelebA and Imagenette datasets significantly, generalizing better. 
%Note that the Imagenette is a more diverse dataset than the CelebA. The better performance shows that. 

\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.49\textwidth} 
        \includegraphics[width=\textwidth]{Figures/image-regression0.pdf} % Adjust filename as needed
        \caption{\textbf{Visualizations of image regression results} on CelebA (left) and Imagenette (right).}
        \label{fig:visualization-image-regression}
    \end{minipage}
\end{figure}


\emph{Qualitative results.} Fig.~\ref{fig:visualization-image-regression} showcases \name{}'s ability to recover high-frequency details in image regression, producing reconstructions that closely match the ground truth with high fidelity. This highlights the effectiveness of our approach. Additional qualitative results, including image completion, are provided in Appendix~\ref{supp:image-regression}. Specifically, Fig.~\ref{fig:completion} in the Appendix demonstrates that \name{} can reconstruct full signals from minimal observations, further validating its capability.

\subsection{\name{} in 3D novel view synthesis}
\label{sec:3d-nerf}
We continue with experiments in 3D novel view synthesis, a canonical task to evaluate 3D Neural Radiance Fields.
%
%\str{Do we follow an existing setup. If yes, better say it,  like 'We follow the implementation of [REF]. Briefly, we ...'}
%
%\wy{yes, we follow an existing setup.}
% \textcolor{blue}{
We follow the implementation of \cite{guo2023versatile,chen2022transformers}. Briefly, our input context set comprises camera rays and their corresponding image pixels from one or two views. These are split into 256 tokens, each projected into a 512D vector via a linear layer and self-attention. Two MLPs predict 256 geometric bases: one generates 3D Gaussian parameters, and the other outputs 32D latent representations. From these, we derive object- and ray-specific modulating vectors (both 512D). Our NeRF function includes four layers—two modulated and two shared—with further details in Appendix~\ref{supp:gaussian}.
% }
%Our input context set  is the  set of camera rays and the corresponding image pixels from \emph{one or two views} only.
%These are then split into different visual tokens.
% We use the same patch size \(8 \times 8\) as TransINR~\citep{chen2022transformers} and VNP~\citep{guo2023versatile}, resulting in 256 tokens. A linear layer and a self-attention module project each token into a 512-dimensional vector. Based on the 256 tokens, we predict 256 geometric bases using two MLP modules: one for 3D Gaussian distribution parameters and the other for the latent representation (32 dimensions).
% We obtain the object-specific and ray-specific modulating vectors (both are 512 dimensions) based on the geometric base.
% Our NeRF function consists of four layers, including two modulated layers and two shared layers. 
% More details are given in Appendix~\ref{supp:gaussian}.

\input{./Tables/shapenet_results}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{Figures/nerf-results-new.pdf} % Adjust the size and filename as needed
  \vspace{-6mm}  \caption{\textbf{Qualitative results of the proposed \name{} on novel view synthesis of ShapeNet objects.} Both 1-view (top) and 2-view (bottom) context results are presented.} % Caption for the figure
  \label{fig:nerf-visualization}
  \vspace{-3mm}
\end{figure*}


\paragraph{ShapeNet~\citep{chang2015shapenet}.} 
We follow the data setup of~\citep{tancik2021learned}, with objects from three ShapeNet categories: chairs, cars, and lamps. For each 3D object, 25 views of size \(128 \times 128\) images are generated from viewpoints randomly selected on a sphere.
The objects in each category are divided into training and testing sets, with each training object consisting of 25 views with known camera poses.
At test time, a random input view is sampled to evaluate the performance of the novel view synthesis. Following the setting of previous methods~\citep{chen2022transformers}, we focus on the single-view (1-shot) and 2-view (2-shot) versions of the task, where one or two images with their corresponding camera rays are provided as the context.

We first compare with probabilistic Neural Field baselines, including NeRF-VAE~\citep{kosiorek2021nerf}, PONP~\citep{gu2023generalizable}, and VNP~\citep{guo2023versatile}.
Like \name{}, PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} also use Neural Processes, without, however, considering either geometric or hierarchical priors.
Secondly, we also compare with well-established deterministic Neural Fields, including LearnInit~\citep{tancik2021learned} and  TransINR~\citep{chen2022transformers}.
We note that recent works~\citep{liu2023zero,shi2023zero123plus} have shown that training on massive 3D datasets~\citep{deitke2023objaverse} is highly beneficial for Neural Radiance Fields.
We leave massive-scale settings and comparisons to future work.
Thirdly, to demonstrate the flexibility of \name{} to handle complex scenes, we integrate with GNT~\citep{wang2022attention} and conduct experiments on the NeRF Synthetic dataset~\citep{mildenhall2021nerf}.



\emph{Quantitative results.} We show Peak Signal-to-Noise Ratio (PSNR) results in Table~\ref{tab:nerf-psnr}.
\name{} consistently outperforms all other baselines across all categories by a significant margin.
On average, \name{} outperforms the probabilistic Neural Field baselines such as VNP~\citep{guo2023versatile}, by 0.87 PSNR, %(23.49 vs. 22.62), 
indicating that adding structure in the form of geometric and hierarchical priors leads to better generalization. 
With two views for context, \name{} improves significantly by about $1$ PSNR.
% This improvement is expected, as the richer geometric bases information allows for a better representation of the 3D space, leading to improved object-specific and ray-specific latent variables. % to modulate the MLP.

\emph{Qualitative results.}
In Fig.~\ref{fig:nerf-visualization}, we visualize the results on novel view synthesis of ShapeNet objects.
\name{} can infer object-specific radiance fields and render high-quality 2D images of the objects from novel camera views, even with only 1 or 2 views as context. More results and comparisons with other VNP are provided in Appendix~\ref{supp:more-results}.

\paragraph{NeRF Synthetic~\citep{mildenhall2021nerf}.} We further evaluate on the NeRF Synthetic dataset against recent state-of-the-art, including GNT~\citep{wang2022attention}, MatchNeRF~\citep{chen2023explicit}, and GeFu~\citep{liu2024geometry}.
For a fair comparison, we use the same encoder and NeRF network architecture while integrating our probabilistic framework into GNT.
Following GeFu, we assess performance in 2-view and 3-view settings.


\input{./Tables/compare_gnt}
\emph{Quantitative results.} We present results in Table~\ref{tab:comparison-gnt}.
We observe that \name{} surpasses GeFu by approximately 1 PSNR in the 3-view setting, validating the effectiveness of our probabilistic framework and geometric bases.
Moreover, we consider a challenging 1-view setting to examine the model’s robustness under extremely limited context.
Both Table~\ref{tab:comparison-gnt} and Figure~\ref{fig:1-view-compare} indicate that \name{} reconstructs novel views effectively also with only a single view for context, in contrast to GNT that fails in this setting.
We furthermore test cross-category generalization for our model and GNT without retraining, training on the \texttt{drums} category and evaluating on \texttt{lego}.
As shown in Figure~\ref{fig:cross-category}, \name{} leverages the available context information more effectively, producing higher-quality generations with better color fidelity compared to GNT.
We give additional details in Appendix~\ref{sec:compare_gnt}.


%regress the image closely to the ground truth, indicating the capability of capturing the detailed texture information. 



\subsection{\name{} in 1D signal regression}

Following the previous works' implementation~\citep{guo2023versatile,kim2019attentive}, we conduct 1D signal regression experiments using synthetic functions drawn from Gaussian processes (GPs) with RBF and Matern kernels. This kernel selection, as advocated by~\citet{kim2022neural}, ensures diverse function characteristics spanning smoothness, periodicity, and local variability. To evaluate performance, we adopt two key metrics: (1) context reconstruction error, quantifying the log-likelihood of observed data points (context set), and (2) target prediction error, measuring the log-likelihood of extrapolated predictions (target set). We compare with three baselines, VNP~\citep{guo2023versatile}, CNP~\citep{garnelo2018conditional}, and ANP~\citep{kim2019attentive}. 


\emph{Quantitative results.}
We present a quantitative comparison with baselines in Table~\ref{tab:1d-results}. \name{} consistently outperforms the baselines across two types of synthetic data, demonstrating its effectiveness and flexibility in different signals. %handling different types of signals.



%For likelihood estimation, we adopt importance-weighted sampling \citep{Burda2016}, which provides a robust approximation of the true posterior and mitigates underfitting in sparse data regimes.

% \begin{table*}[htbp]
%     \centering
%     \caption{\textbf{Performance comparison on 1D signal regression.} Log-likelihoods ($\uparrow$) of the context set and target set are reported. }
%     \begin{tabular}{lcccccc}
%         \toprule
%         & \multicolumn{2}{c}{RBF kernel GP} & \multicolumn{2}{c}{Matern kernel GP} & \multirow{2}{*}{Parameters} \\
%         \cmidrule(r){2-3} \cmidrule(r){4-5}
%         Method & Context & Target & Context & Target &  \\
%         \midrule
%         CNP & $1.023 \pm 0.033$ & $0.019 \pm 0.015$ & $0.935 \pm 0.036$ & $-0.124 \pm 0.010$ & 0.99 M \\
%         Stacked ANP & $1.381 \pm 0.001$ & $0.400 \pm 0.004$ & $1.381 \pm 0.001$ & $0.183 \pm 0.012$ & 1.52 M \\
%         VNP & $1.377 \pm 0.004$ & ${0.651 \pm 0.001}$ & $1.376 \pm 0.004$ & ${0.439 \pm 0.007}$ & 2.29 M \\
%         \name{} & $1.397 \pm 0.006$ & $\mathbf{0.741 \pm 0.001}$ & $1.376 \pm 0.004$ & $\mathbf{0.545 \pm 0.009}$ & 2.13 M \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:1d-results}
% \end{table*}
%\str{Do not report them in the appendix, do it briefly here, and maybe more details in the appendix.}

\subsection{Ablations}

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Importance of geometric bases and hierarchical latent variables} on a subset of the Lamps scene synthesis (PSNR). $z_g$ and $z_l$ are object-specific variable and ray-specific variable, respectively.  \ymark~ and \xmark~denote whether the component joins the pipeline or not.}
% \label{table:abl-np}
% \resizebox{0.25\textwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
%  $\mathbf{B}_C$ & $\mathbf{z}_o$& $\mathbf{z}_r$ & PSNR ($\uparrow$) \\ 
% \midrule
% \xmark & \cmark & \cmark & 23.06 \\
% \cmark & \xmark & \xmark & 25.98 \\
% \cmark & \cmark & \xmark & 26.24\\ 
% \cmark & \xmark & \cmark & 26.29\\ 
% \cmark & \cmark & \cmark & \textbf{26.48}\\ 
% \bottomrule
% \end{tabular}
% }
% \end{table}



\begin{table*}[htbp]
    \centering
    \begin{minipage}{0.68\textwidth} % Adjust width for first table
        \caption{\textbf{Performance comparison on 1D signal regression.} Log-likelihoods ($\uparrow$) of the context set and target set are reported.}
        \resizebox{\textwidth}{!}{ % Resize to fit within column
        \begin{tabular}{lcccc}
            \toprule
            & \multicolumn{2}{c}{RBF kernel GP} & \multicolumn{2}{c}{Matern kernel GP} \\
            \cmidrule(r){2-3} \cmidrule(r){4-5}
            Method & Context & Target & Context & Target \\
            \midrule
            CNP & $1.023 \pm 0.033$ & $0.019 \pm 0.015$ & $0.935 \pm 0.036$ & $-0.124 \pm 0.010$ \\
            Stacked ANP & $1.381 \pm 0.001$ & $0.400 \pm 0.004$ & $1.381 \pm 0.001$ & $0.183 \pm 0.012$ \\
            VNP & $1.377 \pm 0.004$ & ${0.651 \pm 0.001}$ & $1.376 \pm 0.004$ & ${0.439 \pm 0.007}$ \\
            \name{} & $1.397 \pm 0.006$ & $\mathbf{0.741 \pm 0.001}$ & $1.376 \pm 0.004$ & $\mathbf{0.545 \pm 0.009}$ \\
            \bottomrule
        \end{tabular}
        }
        \label{tab:1d-results}
    \end{minipage}
    \hfill
    \begin{minipage}{0.31\textwidth} % Adjust width for second table
        \centering
        \caption{\textbf{Ablation of geometric bases and hierarchical latent variables}} %on a subset of the Lamps scene synthesis (PSNR $\uparrow$).} %$z_g$ and $z_l$ are object-specific variable and ray-specific variable, respectively.  \ymark~ and \xmark~denote whether the component joins the pipeline or not.}
        \label{table:abl-np}
        \resizebox{0.8\textwidth}{!}{
        \begin{tabular}{lccc}
        \toprule
        $B_C$ & $z_o$ & $z_r$ & PSNR ($\uparrow$) \\ 
        \midrule
        \xmark & \cmark & \cmark & 23.06 \\
        \cmark & \xmark & \xmark & 25.98 \\
        \cmark & \cmark & \xmark & 26.24\\ 
        \cmark & \xmark & \cmark & 26.29\\ 
        \cmark & \cmark & \cmark & \textbf{26.48}\\ 
        \bottomrule
        \end{tabular}
        }
    \end{minipage}
\end{table*}


\begin{table}[htbp]
% \centering
%\vspace{-4mm}
\caption{\textbf{Sensitivity to the number of geometric bases} on NeRF and image regression.}
\label{table:num-gaussian}
%\label{tab: ablate_basis}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{Image Regression}} & \multicolumn{2}{c}{\textbf{NeRF}} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-6}
\# Bases & 49 & 169 & 484 & 100 & 250 \\ \midrule
PSNR~($\uparrow$)    & 28.59  & 33.74   &  44.24  &  24.31 &  24.59  \\ \bottomrule
\end{tabular}
}
%\vspace{-6mm}
\end{table}

\paragraph{Geometric bases.} We first ablate 
the effectiveness of the proposed geometric
bases on a subset of the
3D Lamps scene synthesis task. As shown in Table~\ref{table:abl-np} (rows 1 and 5),
with the geometric bases, GeomNP performs
clearly better. This indicates the importance of
the structure information modeled in the geometric
bases. Moreover, the bases perform well without hierarchical
latent variables, demonstrating their
ability to construct 3D signals from limited 2D context. %information.
% Also either global or local latent variable improves the performance of neural processes, indicating the effectiveness of hierarchical latent variables.

% We first ablate the importance of geometric bases.
% Results in Table~\ref{table:abl-np} (rows 1 and 5) show that geometric bases and the 3D structure bring clear benefits in different spatial levels. Also either global or local latent variable improves the performance of
% neural processes, indicating the effectiveness of hierarchical latent variables.

We further analyze the sensitivity to the number of geometric bases in CelebA image regression and Lamps NeRF tasks.  
Results in Table~\ref{table:num-gaussian} show that more bases lead to better accuracies and better generalization.
We choose the number of bases by balancing the performance and computation.

% consistently, indicating that large numbers of geometric Gaussian bases further enrich the structure information and lead to stronger predictive functions. 
%However, beyond a certain point, specifically 169 in this experiment, the performance gains diminish. 

\paragraph{Hierarchical latent variables.}
We ablate the importance of the hierarchical nature of \name{} on a subset of the Lamps dataset.
%\str{Why not a separate table?}
%\wy{for save place, and if two tables, then each of them is too small}
The last four rows of Table~\ref{table:abl-np} show that both global and local latent variables contribute to improved accuracy, with their combination yielding the best performance. Furthermore, the qualitative ablation study on hierarchical latent variables in Fig.~\ref{fig:hier-abl} in the Appendix~\ref{sec:abl-bases-qua} confirms that they effectively capture global and local structures, respectively.
% With both ${\bf{z}}_o$ and ${\bf{z}}_r$, the method performs best, demonstrating the importance of the hierarchical modulation by latent variables. 
% In addition, the hierarchical modulation also performs well without the geometric bases.

% \noindent 


% We include the most recent probabilistic Neural Field baselines, including NeRF-VAE~\citep{kosiorek2021nerf}, PONP~\citep{gu2023generalizable}, and VNP~\citep{guo2023versatile}, in their respective experimental settings.
% Note that like \name{}, PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} also use Neural Processes, without, however, considering neither geometric nor hierarchical priors.
% Next to these probabilistic baselines, we include comparisons with well-established deterministic Neural Field methods, including LearnInit~\citep{tancik2021learned} and  TransINR~\citep{chen2022transformers}.

% We compare \name{} with three recent probabilistic INR generalization methods: NeRF-VAE~\citep{kosiorek2021nerf}, PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} on ShapeNet novel view synthesis and image regression tasks.
% PONP~\citep{gu2023generalizable} and VNP~\citep{guo2023versatile} also rely on Neural Processes, however, they neglect structure information and the probabilistic interaction between 3D functions and 2D partial observations. Additionally, we choose two previous well-known deterministic INR generalization approaches, LearnInit~\citep{tancik2021learned} and  TransINR~\citep{chen2022transformers} as our baselines. Moreover, to demonstrate the flexibility of our method and its ability to handle complex scenes, we integrate \name{} with GNT~\citep{wang2022attention} and conduct experiments on the NeRF Synthetic dataset~\citep{mildenhall2021nerf}. \textcolor{blue}{In addition, we demonstrate our method is also effective for 1-D signals in the Appendix.}