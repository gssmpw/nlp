\subsection{Neural (Radiance) Fields}

\textbf{Neural Fields (NeFs)} \cite{sitzmann2020implicit} are continuous functions $f_\omega \colon x \mapsto y$, parameterized by a neural network whose parameters $\omega$ we optimize to reconstruct the continuous signal $y$ on coordinates $x$.
As with regular neural networks, fitting Neural Field parameters $\omega$ relies on gradient descent minimization.
Unlike regular networks, however, conventional Neural Fields are explicitly designed to overfit the signal $y$ during reconstruction deterministically, without considering generalization~\citep{mildenhall2021nerf,barron2021mip}.
The reason is that Neural Fields have been primarily considered in transductive learning settings in 3D graphics, whereby the optimization objective is to optimally reconstruct the photorealism of single 3D objects at a time.
In this case, there is no need for generalization across objects.
A single trained Neural Field network is optimized to ``fill in'' the specific shape of a specific 3D object under all possible view points, given input point cloud (coordinates $x$).
For each separate 3D object, we optimize a separate Neural Field afresh.
Beyond 3D graphics, Neural Fields have found applicability in a broad array of 1D~\citep{yin2022continuous} and 2D~\citep{chen2023neurbf} applications, for scientific~\citep{raissi2019physics} and medical data~\citep{de2023spatio}, especially when considering continuous spatiotemporal settings.

\textbf{Neural Radiance Fields (NeRF)}~\citep{mildenhall2021nerf, arandjelovic2021nerf} are Neural Fields specialized for 3D graphics, reconstructing the 3D shape and texture of a single objects.
Specifically, each point \(\mathbf{p} = (p_x, p_y, p_z)\) in the 3D space centered around the object has a color \(\mathbf{c}(\mathbf{p},
 \mathbf{d})\), where \(\mathbf{d} = (\theta, \phi)\) is the direction of the camera looking at the point $p$.
Since objects might be opaque or translucent, points also have opacity \(\sigma(\mathbf{p})\).
In Neural Field terms, therefore, our input comprises point coordinates and the camera direction, that is $x=(\mathbf{p}, \mathbf{d})$, and our output comprises colors and opacities, that is \(y = (\mathbf{c}, \sigma)\).

% \begin{figure}[htbp]
% %\vspace{-5mm}
% \centering
% \centerline{
% \includegraphics[width=0.95\columnwidth]{Figures/problemstate.pdf} 
% } 
% %\vspace{-2mm}
% \caption{\textbf{Inverse learning of a NeRF. \str{Update legend notation}}
% }
% %\vspace{-4mm}
% \label{fig:problem}
% \end{figure}

Optimizing a NeRF is an inverse problem: we do not have direct access to ground-truth 3D colors and points of the object. Instead, we optimize solely based on 2D images from known camera positions \( \mathbf{o} \) and viewing directions \( \mathbf{d} \). 
Specifically, we optimize the parameters \( \omega \) of the NeRF function, which encodes the 3D shape and color of the object, allowing us to render novel 2D views from arbitrary camera positions and directions using ray tracing along \( \mathbf{r} = (\mathbf{o}, \mathbf{d}) \). This ray-tracing process integrates colors and opacities along the ray, accumulating contributions from points until they reach the camera.
The objective is to ensure that NeRF-generated 2D views match the training images. Since these images provide an object-specific context for inferring its 3D shape and texture, we refer to them as \emph{context data}. In contrast, all other unknown shape and texture information is \emph{target data}. 
For a detailed description of the ray-tracing integration process, see Appendix~\ref{supp:nerf-render}.


\textbf{Conditional Neural Fields} \cite{papa2023train} have recently gained popularity to avoid optimizing from scratch a new Neural Field for every new object. Conditional Neural Fields split parameters $\omega$ to a shared part $\omega_{D}$ that is common between objects in the dataset $D$, and an object-specific part $\omega_{i}$ that is optimized specifically for the $i$-th object.
However, the optimization of $\omega_{i}$ is still done independently per object using stochastic gradient descent.

\subsection{Neural Processes}
% Neural Processes (NPs)~\citep{garnelo2018neural,kim2019attentive} are probabilistic models that generalize the concept of Gaussian Processes (GPs)~\citep{rasmussen2003gaussian} with neural networks.
% NPs approximate distributions over functions (e.g., $f_\omega$) by introducing a latent variable $z$ that captures uncertainty at the function level.
% Similar to GPs, NPs have a \emph{context set} $C = \big(x_C, y_C\big)_{n}$ comprising $n$ input $x_i$ and output pairs. 
% NPs (and GPs) use the context variables to model outputs $y_T$ given new inputs $x_T$, that is $p\bigl(y_T \mid x_T, C\bigr)$.
% To contrast with the context set, the $m$ pairs $T = \big(x_T, y_T \big)_{m}$, are referred to as the \emph{target set}.
% NPs introduce a latent variable $z$ to factorize $p\bigl(y_T \mid x_T, C\bigr)$ and make it easier to model large-scale variability, that is
% % To account for variability in the underlying function, NPs factorize this distribution as
% \begin{align}
% p\bigl(y_T \mid x_T, z\bigr)\,p\bigl(z \mid C\bigr).
% \label{eq:np}
% \end{align}
% Here, $p(z \mid C)$ represents a prior distribution for the latent variable $z$, which is derived from (and conditioned on) the context set only.
% During training, the model refines its understanding of $z$ by learning ---usually by variational inference~\citep{garnelo2018neural} --- an approximate posterior $q(z \mid C, T)$ that aligns with the prior $p(z \mid C)$.
% NPs are a principled probabilistic framework to model both predictive uncertainty and function-level variability, while remaining adaptable to partial observations.



% \paragraph{Neural Processes (NPs).}
Neural Processes (NPs)~\citep{garnelo2018neural,kim2019attentive} extend the notion of Gaussian Processes (GPs)~\citep{rasmussen2003gaussian} by leveraging neural networks for flexible function approximation.  
Given a \emph{context set} 
\(
\mathcal{C} 
= \{(x_{C,n},\, y_{C,n})\}_{n=1}^{N}
\)
of $N$ input--output pairs, NPs infer a latent variable $z$ that captures function-level uncertainty.  
When presented with new inputs
\(
x_T 
= \{x_{T,m}\}_{m=1}^M
\),
the goal is to predict
\(
y_T 
= \{y_{T,m}\}_{m=1}^M
\).
Formally, NPs define the predictive distribution:
\begin{equation}
\label{eq:np}
p\bigl(y_T \mid x_T, \mathcal{C}\bigr)
\;=\;
\int
p\bigl(y_T \mid x_T, z\bigr)
\,p\bigl(z \mid \mathcal{C}\bigr)
\;\mathrm{d}z.
\end{equation}
Here, \(p(z \mid \mathcal{C})\) is a prior over \(z\) derived solely from the context set.  
During training, an approximate posterior \(q(z \mid \mathcal{C}, \mathcal{T})\) (where \(\mathcal{T}\) is the \emph{target set} consisting of \((x_{T}, y_{T})\) pairs) is learned via variational inference~\citep{garnelo2018neural}.  
Through this latent-variable formulation, NPs capture both predictive uncertainty and function-level variability, enabling robust performance under partial observations.

%In their simplest form, NPs typically use multilayer perceptrons (MLPs) for both encoder and decoder. Extensions such as Attentive Neural Processes (ANP)\citep{kim2019attentive} incorporate attention mechanisms to better aggregate context information, whereas Transformer Neural Processes (TNP)\citep{nguyen2022transformer} treat each context point as a token, leveraging the transformer architecture to improve function approximation.


% We denote 3D world coordinates by \(\mathbf{p} = (x, y, z)\) and a camera viewing direction by \(\mathbf{d} = (\theta, \phi)\).
% Each point in 3D space have its color \(\mathbf{c}(\mathbf{p}, \mathbf{d})\), which depends on the location \(\mathbf{p}\) and viewing direction \(\mathbf{d}\).

% Points also have a density value \(\sigma(\mathbf{p})\) that encodes opacity. We represent coordinates and view direction together as $\mathbf{x} = \{\mathbf{p},\mathbf{d} \}$, color and density together as \(\mathbf{y}(\mathbf{p}, \mathbf{d}) = \{\mathbf{c}(\mathbf{p}, \mathbf{d}), \sigma(\mathbf{p})\}\).

% \str{This sentence sounds a bit strange, we can denote all 3D points like that anyways, no need to observe them from 'multiple locations', no? In general, I think the paragraph can be written more cleanly.}

% When observing a 3D object from multiple locations, we denote all 3D points as \(\mathbf{X} = \{\mathbf{x}_n \}_{n=1}^N\) and their colors and densities as \(\mathbf{Y} = \{\mathbf{y}_n\}_{n=1}^N\).
% Assuming a ray \(\mathbf{r} = (\mathbf{o}, \mathbf{d})\) starting from the camera origin \(\mathbf{o}\) and along direction \(\mathbf{d}\), we sample $P$ points along the ray, with \(\mathbf{x}^{\mathbf{r}} = \{{x}_i^\mathbf{r}\}_{i=1}^P\) and corresponding colors and densities \(\mathbf{y}^{\mathbf{r}} = \{{y}_i^{\mathbf{r}}\}_{i=1}^P\).
% Further, we denote the observations \(\widetilde{\mathbf{X}}\) and \(\widetilde{\mathbf{Y}}\) as: the set of camera rays \(\widetilde{\mathbf{X}} = \{\widetilde{\mathbf{x}}_n = \mathbf{r}_n\}_{n=1}^N\) and the projected 2D pixels from the rays \(\widetilde{\mathbf{Y}} = \{\widetilde{\mathbf{y}}_n\}_{n=1}^N\).



% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.49\textwidth]{Figures/problemstate.pdf} % Adjust the size and filename as needed
%   \caption{Framework.} % Caption for the figure
%   \label{fig:problem}
% \end{figure}



% \str{Can you clarify what exactly each notation style corresponds to? What is the $\sim$ for instance? The sampled new pixel views? And no tilde are the actual observations?}

% \textbf{Background on Neural Radiance Fields.}
% We formally describe Neural Radiance Field (NeRF)~\citep{mildenhall2021nerf, arandjelovic2021nerf} as a continuous function \( f_{\text{NeRF}}: \mathbf{x} \mapsto \mathbf{y} \), which maps 3D world coordinates \(\mathbf{p}\) and viewing directions \(\mathbf{d}\) to color and density values \(\mathbf{y}\). 
% That is, a NeRF function, \( f_{\text{NeRF}} \), is a neural network-based function that represents the whole 3D object (e.g., a car in Fig.~\ref{fig:problem}) as coordinates to color and density mappings. Learning a NeRF function of a 3D object is an inverse problem where we only have indirect observations of arbitrary 2D views of the 3D object, and we want to infer the entire 3D object's geometry and appearance.
% With the NeRF function, given any camera pose, we can render a view on the corresponding 2D image plane by marching rays and using the corresponding colors and densities at the 3D points along the rays. Specifically, given a set of rays \(\mathbf{r}\) with view directions \(\mathbf{d}\), we obtain a corresponding 2D image. The integration along each ray corresponds to a specific pixel on the 2D image using the volume rendering technique described in~\cite{kajiya1984ray}, which is also illustrated in Fig.~\ref{fig:problem}. Details about the integration are given in Appendix~\ref{supp:nerf-render}. 

