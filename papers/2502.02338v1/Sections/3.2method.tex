
% \js{Double check connection sentences between subsections}

\noindent \textbf{Notations.}
%We work on the 3D space.
% We denote 3D world coordinates with ${\bf{p}}=(x,y,z)$, and the camera viewing direction with $v=(\theta,\phi)$. 
% %from any 3D point in space with the angles $v=(\theta,\phi)$. %denoting pitch and yaw.
% The points in the 3D space have color $c^{{\bf{p}}, v}$ that depends on the 3D location $p$ and the viewpoint of the camera $\bf{v}$.
% The points also have a density value $\sigma^p$ that encodes how opaque that point is.
% We couple them together in $\bf{y}^p=\{c^{{\mathbf{p}}, v}, \sigma^p\}$.
% When examining a whole 3D object from multiple 3D locations in space, $i=1,...,N$, we denote the set of all 3D locations with $\mathbf{X}=\{\mathbf{x}^n\}_{n=1}^N$, and $\mathbf{Y}=\{\mathbf{y}^n\}_{n=1}^N$.
% Assuming a ray $r=(p, v)$ starting from location $p$ and from viewpoint $v$, with $x^r=\{x^r_i\}_{i=1}^P$ and $y^r=\{y^r_i\}_{i=1}^P$ we denote all the 3D locations, and colors on $P$ points sampled from the ray.
% Further, with ${\tilde{\bf{X}}}$ and ${\tilde{\bf{Y}}}$ we denote the observations we have, the set of camera rays ${\tilde{\bf{X}}} = \{  
% \tilde{\bf{x}}_n = r_n \}_n^{N}$, and the projected 2D pixels from the rays ${\tilde{\bf{Y}}} = \{  
% \tilde{\bf{y}}_n^p \}_n^{N}$. 
%
We denote 3D world coordinates by \(\mathbf{p} = (x, y, z)\) and a camera viewing direction by \(\mathbf{d} = (\theta, \phi)\). Each point in 3D space have its color \(\mathbf{c}(\mathbf{p}, \mathbf{d})\), which depends on the location \(\mathbf{p}\) and viewing direction \(\mathbf{d}\). Points also have a density value \(\sigma(\mathbf{p})\) that encodes opacity. We represent coordinates and view direction together as $\mathbf{x} = \{\mathbf{p},\mathbf{d} \}$, color and density together as \(\mathbf{y}(\mathbf{p}, \mathbf{d}) = \{\mathbf{c}(\mathbf{p}, \mathbf{d}), \sigma(\mathbf{p})\}\).
\str{This sentence sounds a bit strange, we can denote all 3D points like that anyways, no need to observe them from 'multiple locations', no? In general, I think the paragraph can be written more cleanly.}
When observing a 3D object from multiple locations, we denote all 3D points as \(\mathbf{X} = \{\mathbf{x}_n \}_{n=1}^N\) and their colors and densities as \(\mathbf{Y} = \{\mathbf{y}_n\}_{n=1}^N\).
Assuming a ray \(\mathbf{r} = (\mathbf{o}, \mathbf{d})\) starting from the camera origin \(\mathbf{o}\) and along direction \(\mathbf{d}\), we sample $P$ points along the ray, with \(\mathbf{x}^{\mathbf{r}} = \{{x}_i^\mathbf{r}\}_{i=1}^P\) and corresponding colors and densities \(\mathbf{y}^{\mathbf{r}} = \{{y}_i^{\mathbf{r}}\}_{i=1}^P\). Further, we denote the observations \(\widetilde{\mathbf{X}}\) and \(\widetilde{\mathbf{Y}}\) as: the set of camera rays \(\widetilde{\mathbf{X}} = \{\widetilde{\mathbf{x}}_n = \mathbf{r}_n\}_{n=1}^N\) and the projected 2D pixels from the rays \(\widetilde{\mathbf{Y}} = \{\widetilde{\mathbf{y}}_n\}_{n=1}^N\).



% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.49\textwidth]{Figures/problemstate.pdf} % Adjust the size and filename as needed
%   \caption{Framework.} % Caption for the figure
%   \label{fig:problem}
% \end{figure}



% \str{Can you clarify what exactly each notation style corresponds to? What is the $\sim$ for instance? The sampled new pixel views? And no tilde are the actual observations?}

\begin{figure}[htbp]
%\vspace{-5mm}
\centering
\centerline{
\includegraphics[width=0.95\columnwidth]{Figures/problemstate.pdf} 
} 
%\vspace{-2mm}
\caption{\textbf{Complete rendering from 3D points to a 2D pixel.}
}
%\vspace{-4mm}
\label{fig:problem}
\end{figure}

\textbf{Background on Neural Radiance Fields.}
We formally describe Neural Radiance Field (NeRF)~\citep{mildenhall2021nerf, arandjelovic2021nerf} as a continuous function \( f_{\text{NeRF}}: \mathbf{x} \mapsto \mathbf{y} \), which maps 3D world coordinates \(\mathbf{p}\) and viewing directions \(\mathbf{d}\) to color and density values \(\mathbf{y}\). 
That is, a NeRF function, \( f_{\text{NeRF}} \), is a neural network-based function that represents the whole 3D object (e.g., a car in Fig.~\ref{fig:problem}) as coordinates to color and density mappings. Learning a NeRF function of a 3D object is an inverse problem where we only have indirect observations of arbitrary 2D views of the 3D object, and we want to infer the entire 3D object's geometry and appearance.
With the NeRF function, given any camera pose, we can render a view on the corresponding 2D image plane by marching rays and using the corresponding colors and densities at the 3D points along the rays. Specifically, given a set of rays \(\mathbf{r}\) with view directions \(\mathbf{d}\), we obtain a corresponding 2D image. The integration along each ray corresponds to a specific pixel on the 2D image using the volume rendering technique described in~\cite{kajiya1984ray}, which is also illustrated in Fig.~\ref{fig:problem}. Details about the integration are given in Appendix~\ref{supp:nerf-render}. 



%\str{Write down the integral.}
%


% Neural Fields are normally considered as an optimization routine in a deterministic setting, whereby95
% the function fNeRF is fit perfectly to the available observations (akin to “overfitting” training data).


\subsection{Probabilistic NeRF Generalization}
% \js{probabilistic NeRF is not new}

\paragraph{Deterministic Neural Radiance Fields} Neural Radiance Fields are normally considered as an optimization routine in a deterministic setting~\citep{mildenhall2021nerf,barron2021mip}, whereby the function $f_{\text{NeRF}}$ fits specifically to the available observations (akin to ``overfitting'' training data).

\paragraph{Probabilistic Neural Radiance Fields} As we are not just interested in fitting a single and specific 3D object but want to learn how to infer the Neural Radiance Field of any 3D object,  we focus on probabilistic Neural Radiance Fields with the following factorization:
%
% \begin{equation}
%     p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}, f_{NeRF}) \propto
%     \underbrace{p({\bf{\widetilde{y}}}| {\bf{{y}}}^{1:P}, {\bf{{x}}}^{1:P})}_{\text{Integration}}
%     \underbrace{p({\bf{{y}}}^{1:P}|{\bf{{x}}}^{1:P}, f_{\text{NeRF}})}_{\text{NeRF model}}
%     \underbrace{p({\bf{{x}}}^{1:P}|{\bf{\widetilde{x}}})}_{\text{Sampling}}.
% \label{eq: rendering}
% \end{equation}
% \begin{equation}
%     p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}, f_{NeRF}) \propto
%     \underbrace{p({\bf{\widetilde{y}}}| {\bf{{y}}}^{\mathbf{r}}, {\bf{{x}}}^{\mathbf{r}})}_{\text{Integration}}
%     \underbrace{p({\bf{{y}}}^{\mathbf{r}}|{\bf{{x}}}^{\mathbf{r}}, f_{\text{NeRF}})}_{\text{NeRF model}}
%     \underbrace{p({\bf{{x}}}^{\mathbf{r}}|{\bf{\widetilde{x}}})}_{\text{Sampling}}.
% \label{eq: rendering}
% \end{equation}
% \begin{equation}
%     p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}) \varpropto
%     \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
%     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T})}_{\text{NeRF Model}}
%     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}},
% \label{eq: definiation}
% \end{equation}
\begin{equation}
    p({\bf{\widetilde{Y}}} | {\bf{\widetilde{X}}}) \varpropto
    \underbrace{p({\bf{\widetilde{Y}}} | {\bf{{Y}}}, {\bf{{X}}})}_{\text{Integration}}
    \underbrace{p({\bf{{Y}}} | {\bf{{X}}})}_{\text{NeRF Model}}
    \underbrace{p({\bf{{X}}} | {\bf{\widetilde{X}}})}_{\text{Sampling}}.
\label{eq: probabilitic_NeRF}
\end{equation}
%
% \str{Is $f_\text{NeRF}$ now a random variable? Normally it is not.}
\str{This can also be writtena  more fluently}
The generation process of this probabilistic formulation is as follows.
We first start from (or sample) a set of rays $\widetilde{\mathbf{X}}$.
Conditioning on these rays, we sample 3D points in space $\mathbf{X} \big|\widetilde{\mathbf{X}}$.
Then, we map these 3D points into their colors and density values with the NeRF function, ${\bf{Y}} = f_{\text{NeRF}}({\bf{{X}}})$.
Last, we sample the 2D pixels of the viewing image that corresponds to the 3D ray ${\widetilde{\bf{Y}}}| {\bf{{Y}}}, {\bf{X}}$ with a probabilistic process. This corresponds to integrating colors and densities ${\bf{{Y}}}$ along the ray on locations ${\bf{X}}$.

% In the following sections, we will define the various probabilistic terms.
% \str{Here it would be good to be more explicit and say how are the various probabistic terms are defined. Or we can say that we will specify later, also in the context of Geometric NP. Either way, the current text below looks like deterministic relations, so I think we can not write them down here.}

\str{I suggest we go directly on conditional neural fields. The way we have it now, we only create extra confusion, unless we are the first to propose this decomposition (but there have been other probabilistic NeRFs before, no?). Or perhaps have better structure in the writing, otherwise it is confusing.. What is context, what target?}
The probabilistic model in \cref{eq: probabilitic_NeRF} is for a single 3D object, thus requiring optimizing a function $f_{\text{NeRF}}$ afresh for every new object, which is time-consuming. For NeRF generalization, we accelerate learning and improve generalization by amortizing the probabilistic model over multiple objects, obtaining per-object reconstructions by conditioning on context sets ${{\widetilde {\bf{X}}}_C, {\widetilde {\bf{Y}}}_C}$.
% \str{What about $\widetilde {\bf{X}}_T$? What is that? Also, why (1) has small letters, and here we have capitals?}
% These context variables are few observations from any new object, that is, the rays and the corresponding observed colors.
For clarity, we use ${(\cdot)}_{C}$ to indicate context sets with {a few new observations for a new object}, while ${(\cdot)}_{T}$ indicates target sets containing 3D points or camera rays from novel views of the same object.
Thus, we formulate a probabilistic NeRF for generalization as:
% \str{update this according to the above equation}
%
\begin{equation}
\begin{aligned}
    &p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto \\
&    \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
    \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{NeRF Generalization}}
    \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}}.
\end{aligned}
\label{eq: probabilitic_NeRF_generalization}
\end{equation}
%
%\str{Not sure if this sentence is good enough, please check later again.}
As this paper focuses on generalization with new 3D objects, we keep the same sampling and integrating processes as in ~\cref{eq: probabilitic_NeRF}. We turn our attention to the modeling of the predictive distribution $p({\bf{{Y}}}_{T}| {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ in the generalization step, which implies inferring the NeRF function.

\paragraph{Misalignment between 2D context and 3D structures} It is worth mentioning that the predictive distribution in 3D space is conditioned on 2D context pixels with their ray $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ and 3D target points ${\bf{X}}_{T}$, which is challenging due to potential information misalignment. Thus, we need strong inductive biases with 3D structure information to ensure that 2D and 3D conditional information is fused reliably.


% \str{Why do you call this the query step?}

% #############################
% By inferring the function distribution $p(f_{\text{NeRF}})$ from the context sets, we can obtain the predictive distribution as: 
% %
% \begin{equation}
%       p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})  
%       = \int p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T}) p(f_{\text{NeRF}}| {{\bf{{X}}}_{T}, \bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) df_{\text{NeRF}} 
% \label{eq: gp_w/o_B}
% \end{equation}
% %
% where $p(f_{\text{NeRF}}| {\bf{X}}_{T} {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ is the prior distribution of the NeRF function, and $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ is the likelihood term. We integrate the likelihood term over the latent space of all possible NeRF functions.
% %\str{The following two sentences may beed to be rewritten.}
% %It is important to note that the context variables by nature contain fewer views and thus less information per a new object.
% It is worth mentioning that inferring the NeRF function needs to incorporate 2D context pixels with their ray $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ and 3D target points ${\bf{X}}_{T}$, which is challenging due to potential information misalignment.  Thus, we need strong inductive biases with 3D structure information to ensure that 2D and 3D conditional information is fused reliably.
% #############################

% \subsection{Geometric Neural Processes for NeRF} 
\subsection{Geometric Bases} 
\label{sec: geometrybases}
% In NeRF generalization, given that the context set is expected to correspond to too few views with few 3D information, 
% % In NeRF generalization, given the context set corresponding to too few 2D views providing few 3D information, 
% fitting a model for $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ that generalizes well is challenging. 
To mitigate the information misalignment between 2D context views and 3D target points, we introduce geometric bases ${\bf{{B}}}_{C}=\{{\bf{b}}_i\}_{i=1}^{M}$, which {induces prior structure to the context set} $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ geometrically. $M$ is the number of geometric bases. 

\begin{figure*}[t]
  \centering  \includegraphics[width=0.99\textwidth]{ICLR2025/Figures/architecture-0.pdf} % Adjust the size and filename as needed
  % \vspace{-2mm}
\caption{\textbf{Illustration of our Geometric Neural Processes.} 
% We solve the problem of radiance field generalization by Neural Processes. 
% captures uncertainty induced by few available observations.
We cast radiance field generalization as a probabilistic modeling problem. Specifically, we first construct geometric bases ${\bf{B}}_C$ in 3D space from the 2D context sets ${\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}$ to model the 3D NeRF function (Section~\ref{sec: geometrybases}). We then infer the NeRF function by modulating a shared MLP through hierarchical latent variables ${\bf{z}}_{o}, {\bf{z}}_{r}$ and make predictions by the modulated MLP (Section~\ref{sec: hierar}). 
  The posterior distributions of the latent variables are inferred from the target sets ${\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{Y}}}_{T}$, which supervises the priors during training (Section~\ref{sec: object}). 
  } % Caption for the figure
  \label{fig: framework}
  %\vspace{-2mm}
\end{figure*}

\str{where is the semantic representation coming from? Self-supervised models? Or is it learned?}
Each geometric basis consists of a Gaussian distribution in the 3D point space and a semantic representation, \textit{i.e.,} ${\bf{b}}_i = \{ \mathcal{N}(\mu_i, \Sigma_i); \omega_i\}$, 
%\str{What is $\omega_i$ in the equation? The weight of the Gaussian?We have mixtures of Gaussians? Please clarify.} 
where $\mu_i$ and $\Sigma_i$ are the mean and covariance matrix of $i$-th Gaussian in 3D space, and $\omega_i$ is its corresponding latent representation. 
Intuitively, the mixture of all 3D Gaussian distributions implies the structure of the object, while $\omega_i$ stores the corresponding semantic information.
% from a 2D context set, e.g., color and texture. 
In practice, we use a transformer-based encoder to learn the Gaussian distributions and representations from the context sets, \textit{i.e.,} $\{(\mu_i, \Sigma_i, \omega_i)\} = \texttt{Encoder} [{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}]$. Detailed architecture of the encoder is provided in Appendix~\ref{supp:gaussian}. 

% \begin{equation}
%     {\bf{{B}}}_{C} = \{{\bf{b}}_i\}_{i=1}^{M}, {\bf{b}}_i=\{\mathcal{N}(\mu_i, \Sigma_i); \omega_i\},
%     \\
%      \mu_i, \Sigma_i, \omega_i = \texttt{Encoder} [{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}],
% \end{equation}
% where $M$ is the number of Gaussian bases. 

% where ${\bf{{B}}}_{C}$ are Gaussian bases inferred from the context views $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ with 3D structure information, \textit{i.e.,} 
% ${\bf{{B}}}_{C}=\texttt{Encoder}\Big({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\Big)$. 


% To address the information loss in the context views, we develop a geometry-aware prior distribution for the NeRF function. The geometry-aware prior integrates a set of geometry bases ${\bf{{B}}}_{C}$ and the target location points ${\bf{X}}_{T}$, which enrich the context sets with the {structure locality information}. 
% By doing so, we reformulate the prior distribution of the NeRF function as:
% \begin{equation}
%     p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{{B}}}_{C}), 
% \label{eq: prior_f}
% \end{equation}
% \str{Can a deterministic variable be part of a probabilistic expression?d}
% where ${\bf{{B}}}_{C}$ is a set of Gaussian bases inferred from the context views $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ with 3D structure information, \textit{i.e.,} ${\bf{{B}}}_{C}=\texttt{Encoder}[{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}]$. 
% Specifically, we construct ${\bf{{B}}}_{C}$ as:
% Geometry basis-agnostic ideas, like 4D scene tensor~\cite{chen2022tensorf}, and RBF kernels~\cite{chen2023neurbf} has been used in deterministic NeRF to store the 3D scene geometry and semantic information. This motivates us to use a set of geometry bases to represent both the geometry structure and semantic information of the scene. 
% We assume the space is spanned by a set of basis, with geometric shapes and high-dimensional representation. 
% The geometry basis ${\bf{B}}_C$ is given by a posterior distribution, $p_{\pi}( {\bf{{B}}}_{C}| {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$. This can be explained as the posterior knowledge (color and spatial location) of the scene when a human sees a view of a scene (context image). Then, we model the function distribution as:
% \begin{align}
%     &{\bf{{B}}}_{C} = \{{\bf{b}}_i\}_{i=1}^{M}, {\bf{b}}_i=\{\mathcal{N}(\mu_i, \Sigma_i); \omega_i\},
%     \label{eq: generation_B_1}
%     \\
%     & \mu_i, \Sigma_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}), \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
%     \label{eq: generation_B_2}
%     \\
%     & \omega_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
%     \label{eq: generation_B_3}
% \end{align}
% where $M$ is the number of the Gaussian bases. $\mu \mathbb \in {R}^3$ is the Gaussian center, $\Sigma \in  \mathbb{R}^{3\times 3}$ is the covariance matrix, and $\omega \in \mathbb{R}^{d_B}$ is the corresponding ${d_B}$-dimension semantic representation. 
%Each Gaussian basis represents a 3D Gaussian kernel and its corresponding semantic information. The shape of a Gaussian kernel can reflect a local object structure.
%For each kernel, \js{details here: we use a visual self-attention to estimate the mean $\mu \mathbb \in {R}^3$ and covariance matrix $\Sigma \in  \mathbb{R}^{3\times 3}$, and a corresponding ${d_B}$-dimension semantic representation $\omega \in \mathbb{R}^{d_B}$. }

With the geometric bases $\mathbf{B}_C$, we review the predictive distribution from  $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ to $p({\bf{Y}}_{T}| {\bf{X}}_{T},{\bf{{B}}}_{C})$.  By inferring the function distribution $p(f_{\text{NeRF}})$, we reformulate the predictive distribution as: 
\begin{equation}
    % p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = 
    p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C}) = \int p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T}) p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C}) df_{\text{NeRF}},
\label{eq: predictive_w_B}
\end{equation}
where $p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C})$ is the prior distribution of the NeRF function, and $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ is the likelihood term. 
% We integrate the likelihood term with all possible NeRF functions. 
%\str{How do we do this? The space to integrate over must be huge, no?}
%\str{The following sentence is a bit weird, can you check it again.}
% \wy{We integrate the likelihood term over the latent space of all possible variables for modulating NeRF functions by monte carlo sampling, which can be seen as integrating over a function distribution.}
Note that the prior distribution of the NeRF function is conditioned on the target points ${\bf{X}}_{T}$ and the geometric bases ${\bf{B}}_{C}$. 
Thus, the prior distribution is data-dependent on the target inputs, yielding a better generalization on novel target views of new objects. 
Moreover, since ${\bf{B}}_{C}$ is constructed with continuous Gaussian distributions in the 3D space, the geometric bases can enrich the locality and semantic information of each discrete target point, enhancing the capture of high-frequency details~\citep{chen2023neurbf,chen2022tensorf,muller2022instant}.

% Since ${\bf{B}}_{C}$ is constructed in the 3D space, 3D target points ${\bf{X}}_{T})$ is able to effectively interact with ${\bf{B}}_{C}$, alleviating the information misalignment.

%\zx{We need some advantages of B in this paragraph}
%\zx{maybe also refer some splatting/RBF kernel methods to introduce what B is, how does B contain 3D structure information}
% \wy{The inferred posterior about the Gaussian bases shapes is able to reflect the object shape, while the semantic posterior (e.g. color and texture) is embedded in the representation of the bases. Moreover, the Gaussian basis spanned in the space is able to help aggregate the locality information for each queried point, which facilitates learning representing high-frequency details~\cite{chen2023neurbf,chen2022tensorf,muller2022instant}.}

% By integrating the prior distribution in equation~\cref{eq: prior_f} into the predictive distribution in equation~\cref{eq: gp_w/o_B}, we replace  $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ with $p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C})$.
% The stochastic processes for NeRF with geometry bases are then derived as:
% % With the encoded geometry base ${\bf{B}}_{C}$, we reformulate the querying step as $p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C})$. By integrating the approximated prior with the bases Eq.~\ref{eq: prior_f} into the generative process in Eq.~\ref{eq: gp_w/o_B}, we derive the stochastic processes for NeRF with Geometry Bases as:
% \str{In 3, $p(f_{Nerf})$ is not conditioned on $X_T$. Is there a difference?}
% \begin{equation}
%     % p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = 
%     p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C}) = \int p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T}) p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{B}}_{C}) df_{\text{NeRF}}.
% \label{eq: gp_w_B}
% \end{equation}
%
% Inferred from the context-generated 3D information in ${\bf{{B}}}_{C}$, the NeRF function prior $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$ reduces the impact of information loss in context views (\wy{due to sampling and integration}) and becomes more suitable for the \zx{query step} $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ in 3D space.

\subsection{Geometric Neural Processes with Hierarchical Latent Variables}
\label{sec: hierar}

% To achieve the 
With the geometric bases, we propose Geometric Neural Processes (\textbf{\method{}}) by inferring the NeRF function distribution $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$ in a probabilistic way.  
% We can generalize NeRF learning and efficiently adapt the functional distribution to new 3D objects.
Based on the probabilistic NeRF generalization in~\cref{eq: probabilitic_NeRF_generalization}, we introduce hierarchical latent variables to encode various spatial-specific information into $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$, improving the generalization ability in different spatial levels.
%\str{Make sure that notation is consistent and not overloaded. Eg, $x^r$ rather than $x^\mathbf{r}$ since it is not that we use the $1:P$ somewhere specific, besides it is not clear that this corresponds to a ray, since the $P$ points could be anywhere.}
Since all rays are independent of each other, we decompose the predictive distribution in \cref{eq: predictive_w_B} as:
\begin{equation}
    p({\bf{Y}}_{T}| {\bf{X}}_{T},  {\bf{B}}_{C})  = \prod_{n=1}^{N} p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{{\mathbf{r}}, n},  {\bf{B}}_{C}),
\label{eq: predictive_distribution_ray_specific}
\end{equation}
where the target input ${\bf{X}}_{T}$ consists of $N \times P$ location points $\{{\bf{x}}_{T}^{{\mathbf{r}}, n}\}_{n=1}^{N}$ for $N$ rays.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{ICLR2025/Figures/graphical_model2.pdf} 
\caption{\textbf{Graphical model for the proposed geometric neural processes.}}
\label{fig: graphical_model}
\end{figure}

Further, we develop a hierarchical Bayes framework for \method{} to accommodate the data structure of the target input ${\bf{X}}_{T}$ in \cref{eq: predictive_distribution_ray_specific}.
We introduce an object-specific latent variable $\mathbf{z}_o$ and $N$ individual ray-specific latent variables $\{\mathbf{z}_r^{n}\}_{n=1}^{N}$ to represent the randomness of $f_\text{NeRF}$.
% the probabilistic NeRF function. 


\str{The formatting here looks weird. Is this the right template?}
Within the hierarchical Bayes framework, $\mathbf{z}_o$ encodes the entire object information from all target inputs and the geometric bases $\{\mathbf{X}_T, \mathbf{B}_C\}$ in the global level; while every $\mathbf{z}_r^{n}$ encodes ray-specific information from $\{ \mathbf{x}_T^{\mathbf{r}, n}, \mathbf{B}_C\}$ in the local level, which is also conditioned on the global latent variable $\mathbf{z}_o$. 
The hierarchical architecture allows the model to exploit the structure information from the geometric bases $\mathbf{B}_C$ in different levels, improving the model's expressiveness ability.
By introducing the hierarchical latent variables in \cref{eq: predictive_distribution_ray_specific}, we model \method{} as:
% \begin{equation}
% \small
%         p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) = \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o, ) p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o, 
% \label{eq:ganp-model}
% \end{equation}
{\small
\begin{equation}
\begin{aligned}
        p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C}) &= \int \prod_{n=1}^{N} \Big\{ \int p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_r^n,{\bf{z}}_o ) \\
        &p({\bf{z}}_{r}^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C) d {\bf{z}}_r^n \Big\} p({\bf{z}}_o |{\bf{X}}_T, {\bf{B}}_C) d {\bf{z}}_o,
\end{aligned}
\label{eq:ganp-model}
\end{equation}
}where $p({\bf{y}}_{T}^{\mathbf{r}, n}| {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_{C}, {\bf{z}}_o, {\bf{z}}_r^i)$ denotes the ray-specific likelihood term. In this term, we use the hierarchical latent variables $\{{\bf{z}}_o, {\bf{z}}_r^i\}$ to modulate a ray-specific NeRF function $f_{\text{NeRF}}$ for prediction, as shown in Fig.~\ref{fig: framework}.
% In general, we first use the object-specific latent variable $\mathbf{z}_o$ to make $f_{NeRF}$ object-specific. Then, the ray-specific latent variable $\mathbf{z}_r$ to enable $f_{\text{NeRF}}$ to capture the local texture information.
Hence, $f_{\text{NeRF}}$ can explore global information of the entire object and local information of each specific ray, leading to better generalization ability on new scenes and new views.
A graphical model of our method is provided in Fig.~\ref{fig: graphical_model}. 

% As mentioned in Eq.~\ref{eq: definiation}, the target inputs can be obtained by randomly sampling ${\bf{x}}_{T}^{1:P}$ from each ray ${\widetilde{\bf{x}}}_{T}$. Thus, target input ${\bf{X}}_{T}$ consists of $N \times P$ location points $\{{\bf{x}}_{T}^{1:P, n}\}_{n=1}^{N}$ given $N$ rays. 
% Since the rendering of each ray is independently \cite{martin2021nerf}, we reformulate the predictive distribution for NeRF in a ray-specific manner:
% \begin{equation}
%     p({\bf{Y}}_{T}| {\bf{X}}_{T},  {\bf{B}}_{C})  = \prod_{n=1}^{N} p({\bf{y}}_{T}^{1:P, n}| {\bf{x}}_{T}^{1:P, n},  {\bf{B}}_{C}).
% \label{eq: predictive_distribution_ray_specific}
% \end{equation}
% Based on the hierarchical data structure of the target inputs, we design a hierarchical latent variable model. In the proposed model, object-specific latent variables ${\bf{g}}$ encode the entire 3D object information; each ${\bf{g}}$ corresponds $N$ individual ray-specific latent variables $\{{\bf{r}}^{n}\}_{n=1}^{N}$.

%\str{Until here. I will check below tomorrow, but maybe you can use the same style like above, more concise and to the point.}
%\str{In general, we have to explain the hierarchical framework better. What is $g$ for instance. }



% that corresponds to the ray's viewpoint $v$. 

% Each viewing image is rendered from a camera pose by marching rays into the 2D image plane and computing colors.

% To optimize a NeRF function for a 3D object, we only have access to several viewing images and their ray information, \textit{e.g., $p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}, f_{NeRF})$}.
% \zx{When learning the NeRF function of a 3D object, we only have access to arbitrary 2D views of the 3D object.
% Each viewing image is rendered from a camera pose by marching rays into the 2D image plane and computing colors.
% can render a viewing image of the object from any camera pose by marching rays into the 2D image plane and computing colors. 
% \str{The notation below looks inconsistent with that of the previou svariable, where we have $c_r$, ...}
% In general, each ray corresponds to a pixel in the 2D image from this view. 
% We use ${\bf{\widetilde{x}}} = [\mathbf{r}_o;\mathbf{r}_d] \in \mathbb{R}^{6}$ and ${\bf{\widetilde{y}}} = [r, g, b] \in \mathbb{R}^{3}$ to denote one specific ray and its corresponding pixel color, respectively. 
% Here $\mathbf{r}_o \in \mathbb{R}^3$ and $\mathbf{r}_d \in \mathbb{R}^3$ denote the origin and the direction of the ray, respectively. Each ray color ${\bf{\widetilde{y}}}$ is the RGB value for the corresponding pixel in the 2D image~\cite{arandjelovic2021nerf}.

% To render the ray color of each pixel, as illustrated in Fig.~\ref{fig:problem}, the volume rendering technique~\cite{kajiya1984ray} is used.
% $P$ discrete location points ${\bf{x}}^{1:P} =\{{\bf{x}}^p\}_{p=1}^P$ are first sampled along the ray, and their densities and colors ${\bf{y}}^{1:P} = \{{\bf{y}}^p\}_{p=1}^P$ are queried by the function $f_{\text{NeRF}}$.
% The information from all discrete locations is then integrated via a rendering equation. 
% % An illustration of the rendering process is shown in Fig.~\ref{fig:problem}.
% We factorize the complete rendering process $p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}})$ as follows:  
% \begin{equation}
%     p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}, f_{NeRF}) \propto
%     \underbrace{p({\bf{\widetilde{y}}}| {\bf{{y}}}^{1:P}, {\bf{{x}}}^{1:P})}_{\text{Integration}}
%     \underbrace{p({\bf{{y}}}^{1:P}|{\bf{{x}}}^{1:P}, f_{NeRF})}_{\text{NeRF model}}
%     \underbrace{p({\bf{{x}}}^{1:P}|{\bf{\widetilde{x}}})}_{\text{Sampling}},
% \label{eq: rendering}
% \end{equation}
% \begin{equation}
%     p({\bf{\widetilde{y}}}|{\bf{\widetilde{x}}}) \varpropto p({\bf{\widetilde{y}}}| {\bf{{y}}}_{1:P}, {\bf{{x}}}_{1:P}) p({\bf{{y}}}_{1:P}|{\bf{{x}}}_{1:P})p({\bf{{x}}}_{1:P}|{\bf{\widetilde{x}}}),
% \label{eq: rendering}
% \end{equation}
% where $p({\bf{\widetilde{y}}}| {\bf{{y}}}^{1:P}, {\bf{{x}}}^{1:P})$ represents the integrating step
% % , where the pre-defined render equation computes the ray color 
% by weighted mixing all location colors using the pre-defined render equation ${\bf{\widetilde{y}}} = f_{\text{render}}({\bf{y}}^{1:P}, {\bf{x}}^{1:P})$  \cite{arandjelovic2021nerf,mildenhall2021nerf}.
% % etails of the equation can be found in~\cite{arandjelovic2021nerf,mildenhall2021nerf}.
% $p({\bf{y}}^{1:P}|{\bf{x}}^{1:P})$ denotes the NeRF modeling step for each queried points, \textit{e.g.}, ${\bf{y}}^{1:P}:= f_{\text{NeRF}}({\bf{x}}^{1}), f_{\text{NeRF}}({\bf{x}}^{2}), ..., f_{\text{NeRF}}({\bf{x}}^{P})$.
% $p({\bf{{x}}}^{1:P}|{\bf{\widetilde{x}}})$ denotes the randomly sampling step along the ray $\bf{\widetilde{x}}$. 
% \zx{Given a set of rays ${\bf{\widetilde{X}}}=\{ {\bf{\widetilde{x}}} \}^{N}$ from the same camera pose, we can obtain its corresponding 2D image ${\bf{\widetilde{Y}}}= \{ {\bf{\widetilde{y}}} \}^{N}$ consisting of all pixel colors by ray-specific rendering. Each specific ray corresponds to a pixel on the image.}
% Moreover, we use ${\bf{\widetilde{X}}}$ and ${\bf{\widetilde{Y}}}$ to represent sets of rays and their corresponding colors in 2D image plane.


% \subsection{Probabilistic Radiance Field Generalization} 

% Conventional methods~\cite{martin2021nerf,chen2022tensorf} usually train a deterministic neural network to optimize the NeRF function $f_{\text{NeRF}}$ in Eq. \ref{eq: rendering}. However, this requires the model to overfit large amounts of viewing images for each 3D object, which is data intensive and time-consuming, with poor generalization ability.

%However, it is difficult to collect location points and their location densities and colors in 3D space (\textcolor{red}{citation}). 
% By contrast, this paper focuses on radiance field generalization on arbitrary new 3D objects with few viewing images. 
% However, the method is applicable to arbitrary implicit neural representation (e.g. 2D images). 
% For a new 3D object
% Therefore, $f_{\text{NeRF}}$ needs to be quickly generalized to a new scene based on \textit{few-shot} context views $\{ {\bf{\widetilde{X}}}_C, {\bf{\widetilde{Y}}}_C\}$. 
% %images and their ray information, which are represented as the context set including ray and pixel color $\{ {\bf{\widetilde{X}}}_C, {\bf{\widetilde{Y}}}_C\}$. 
% The generalized NeRF function is then utilized to render any target set of ray ${\bf{\widetilde{X}}}_{T}$ to get the corresponding pixel color set ${\bf{\widetilde{Y}}}_{T}$ in 2D image plane.

% To achieve fast generalization with limited views, we formulate the problem of radiance field generalization in a probabilistic manner.
% The probabilistic formulation enables us to infer the NeRF function while considering uncertainty, improving the generalization ability on limited context data.
% Formally, we formulate the goal of radiance field generalization as $p({\bf{\widetilde{Y}}}_{T}| {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C} )$. 
% % The target distribution of radiance field generalization is $p({\bf{\widetilde{Y}}}_{T}| {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C} )$. 
% Considering the rendering with a NeRF function in Eq.~\ref{eq: rendering}, we review the target distribution as:
% % \begin{equation}
% %     p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto
% %     \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
% %     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{Generalization}}
% %     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}},
% % \label{eq: definiation}
% % \end{equation}
% \begin{equation}
%     p({\bf{\widetilde{Y}}}_{T} | {\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto
%     \underbrace{p({\bf{\widetilde{Y}}}_{T} | {\bf{{Y}}}_{T}, {\bf{{X}}}_{T})}_{\text{Integration}}
%     \underbrace{p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})}_{\text{Generalization}}
%     \underbrace{p({\bf{{X}}}_{T} | {\bf{\widetilde{X}}}_{T})}_{\text{Sampling}},
% \label{eq: definiation}
% \end{equation}
% Without loss of generality, we share the fixed sampling and integrating processes as in Eq.~\ref{eq: rendering}. Thus, we focus on the modeling of query step $p({\bf{{Y}}}_{T}| {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$.  

%\wy{The ray casting used in volume rendering (sampling process described above) enables sampling in the 3D space continuously, which }
% The probabilistic formulation enables us to obtain the NeRF function by neural processes, which capture uncertainty and directly infer the function in a single feed-forward pass. 
% This leads to both effective and efficient generalization of the NeRF function on new scenes.


% Conventional methods \zx{references\cite{}} usually learn a deterministic neural network $\theta_{NeRF}$ to model a NeRF function. This requires the neural network $\theta_{NeRF}$ to overfit on amounts of viewing images for a 3D object, which is time-consuming. 
% \begin{equation}
% p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \approx  p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, \theta^*_{NeRF}), 
% \end{equation}
% where the best neural network is estimated by maximum a posterior (MAP) on the support sets of ray and ray colors, \textit{e.g.}, $\theta^*_{NeRF} =\mathop{\arg\min}_{\theta} p(\theta_{NeRF}|{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$. For each new 3D object, $\theta_{NeRF}$ will be re-trained for overfitting, yielding worse generalizations. Hence, leveraging limited viewing images for radiance field generalization remains a problem. 

% \textbf{Neural Processes for Radiance Field Generalization.}
% \js{Motivation of generative processes in a probabilistic way}
% In radiance field generalization, each new 3D object only accesses a few viewing images. In this case, a deterministic NeRF function could not capture uncertainty caused by the limited viewing images, leading to worse generalization ability on new 3D objects. 


% \subsection{Stochastic Processes for Radiance Field Generalization with Geometry Basis} 
% \label{sec: geometrybases}

% \noindent \textbf{Stochastic Processes for NeRF.}
% To capture uncertainty and improve generalization ability on limited context views, we cast radiance field generalization as stochastic processes over the NeRF function in 3D space. 
% {The processes directly infer the NeRF function in a feedforward pass, leading to both effective and efficient generalization on new scenes.}
% Specifically, we formulate the predictive distribution of radiance field generalization as follows:
% \begin{equation}
%       p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})  
%       = \int p({\bf{Y}}_{T}|f_{NeRF}, {\bf{X}}_{T}) p(f_{NeRF}| {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) df_{NeRF} 
% \label{eq: gp_w/o_B}
% \end{equation}
% where $p(f_{\text{NeRF}}| {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ is a prior distribution of the NeRF function.
% Since $f_{\text{NeRF}}$ is constructed on 3D space, it should ideally be inferred from the 3D location context sets $\{{\bf{{X}}}_{C}, {\bf{{Y}}}_{C}\}$. 
% \str{I am not sure the following sentences really make a (important) point.}
% However, the 3D location information is not available during practice training.
% The only accessible information of the 3D object is the limited viewing images $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$.
% Moreover, the discrete sampling and integration for rendering results in information loss of the context sets from 3D location space to 2D views (Fig. \ref{fig:problem}).
% Therefore, inferring $f_{\text{NeRF}}$ by the context views is less optimal.
% However, NeRF mapping location points to their location colors and densities should be ideally inferred from the location sets $\{{\bf{{X}}}_{C}, {\bf{{Y}}}_{C}\}$, rather than the ray sets $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$.
% In practice, the former is not available during training. 
% We can not directly transfer the context sets in the ray or camera space $\{{\bf{{X}}}_{C}, {\bf{{Y}}}_{C}\}$ to the 3D location space $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$. This is intractable since the information loss of the context sets from the location space (3D) to the ray space (2D) due to the discrete sampling and integration. 

% To better capture the uncertainty with the context views, we cast radiance field generation as a stochastic process over a NeRF function in the 3D space. 
% The prior distribution of the function should be formulated as $p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{{X}}}_{C}, {\bf{{Y}}}_{C})$. 
% However, in practice, ${\bf{{X}}}_{C}, {\bf{{Y}}}_{C}$ (the context radiance field) is not available during training. We can not directly transfer the context sets in the ray or camera space $\{{\bf{{X}}}_{C}, {\bf{{Y}}}_{C}\}$ to the 3D location space $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$. This is intractable since the information loss of the context sets from the location space (3D) to the ray space (2D) due to the discrete sampling and integration. We introduce a set of geometry basis to address this issue, which can also enrich the query points with the structure locality information.

% Thus, we can factorize the query process in Eq.~\ref{eq: definiation} as:
% \begin{equation}
%     p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) \varpropto p({\bf{Y}}_{T}, f_{NeRF}| {\bf{X}}_{T}, {\bf{{X}}}_{C}, {\bf{{Y}}}_{C}) p( {\bf{{X}}}_{C}, {\bf{{Y}}}_{C} | {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
% \label{eq: query process1}
% \end{equation}
% where $p({\bf{{X}}}_{C}, {\bf{{Y}}}_{C} | {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ denotes generating process from ray to location spaces, which is reverse from the complete rendering from location to ray spaces in Eq.~\ref{eq: rendering}. Therefore, this generating process is intractable since the information loss of the support sets from the location space (3D) to the ray space (2D). 

% \noindent \textbf{Geometry Basis.}
% To address the information loss in the context views, we develop a geometry-aware prior distribution for the NeRF function. The geometry-aware prior integrates a set of geometry bases ${\bf{{B}}}_{C}$ and the target location points ${\bf{X}}_{T}$, which enrich the context sets with the {structure locality information}. 
% By doing so, we reformulate the prior distribution of the NeRF function as:
% \begin{equation}
%     p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = p(f_{\text{NeRF}}| {\bf{X}}_{T}, {\bf{{B}}}_{C}), 
% \label{eq: prior_f}
% \end{equation}
% \str{Can a deterministic variable be part of a probabilistic expression?d}
% where ${\bf{{B}}}_{C}$ is a set of Gaussian bases inferred from the context views $\{{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}\}$ with 3D structure information, \textit{i.e.,} ${\bf{{B}}}_{C}=\texttt{Encoder}[{\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}]$. Specifically, we construct ${\bf{{B}}}_{C}$ as:
% % Geometry basis-agnostic ideas, like 4D scene tensor~\cite{chen2022tensorf}, and RBF kernels~\cite{chen2023neurbf} has been used in deterministic NeRF to store the 3D scene geometry and semantic information. This motivates us to use a set of geometry bases to represent both the geometry structure and semantic information of the scene. 
% % We assume the space is spanned by a set of basis, with geometric shapes and high-dimensional representation. 
% % The geometry basis ${\bf{B}}_C$ is given by a posterior distribution, $p_{\pi}( {\bf{{B}}}_{C}| {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$. This can be explained as the posterior knowledge (color and spatial location) of the scene when a human sees a view of a scene (context image). Then, we model the function distribution as:
% \begin{align}
%     &{\bf{{B}}}_{C} = \{{\bf{b}}_i\}_{i=1}^{M}, {\bf{b}}_i=\{\mathcal{N}(\mu_i, \Sigma_i); \omega_i\},
%     \label{eq: generation_B_1}
%     \\
%     & \mu_i, \Sigma_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}), \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
%     \label{eq: generation_B_2}
%     \\
%     & \omega_i = \texttt{Att}({\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}),
%     \label{eq: generation_B_3}
% \end{align}
% where $M$ is the number of the Gaussian bases. $\mu \mathbb \in {R}^3$ is the Gaussian center, $\Sigma \in  \mathbb{R}^{3\times 3}$ is the covariance matrix, and $\omega \in \mathbb{R}^{d_B}$ is the corresponding ${d_B}$-dimension semantic representation. 
% %Each Gaussian basis represents a 3D Gaussian kernel and its corresponding semantic information. The shape of a Gaussian kernel can reflect a local object structure.
% %For each kernel, \js{details here: we use a visual self-attention to estimate the mean $\mu \mathbb \in {R}^3$ and covariance matrix $\Sigma \in  \mathbb{R}^{3\times 3}$, and a corresponding ${d_B}$-dimension semantic representation $\omega \in \mathbb{R}^{d_B}$. }
% Since the Gaussian bases are continuous in the location space, we can refine arbitrary location points with the Gaussian bases.
% \zx{We need some advantages of B in this paragraph}
% \zx{maybe also refer some splatting/RBF kernel methods to introduce what B is, how does B contain 3D structure information}

% \js{too details, use some \texttt{Atten} to obtain each parameters.}
% Given the context set $[\widetilde{X};\widetilde{Y}] \in \mathbb{R}^{H\times W \times (3+3+3)}$, a visual self-attention module first produces a $M\times D$ tokens with $M$ is the number of visual tokens and $D$ is the hidden dimension. The number of Gaussians we use equals to the number of tokens $M$. Then, we use one MLP to predict centers $\mu$, as well as the rotation $R$ and scaling $S$ matrices parameters for producing covariance matrix $\Sigma$, and one MLP to produce the latent representations $\omega$. The covariance matrix is obtained by $\Sigma^C = RSS^TR^T$.
% \begin{equation}
%     \Sigma = RSS^TR^T.
%     \label{eq:cov-matrix}
% \end{equation}
%where $R\in \mathbb{R}^{3\times3}$ is the rotation matrix, and $S \in \mathbb{R}^3$ is the scaling matrix. 



% The generative process of the NeRF function can be formulated in a probabilistic way:

% Different from previous works, we cast radiance field generalization as a 

% we the distribution over a
% single function $p( )$




% \begin{equation}
%     p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) =  \int p({\bf{Y}}_{T}| f_{NeRF}, {\bf{X}}_{T}) p(f_{NeRF} | {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) d f_{NeRF}
% \end{equation}


%To improve the generalization ability and achieve efficient inference on new scenes of Neural Field methods, we propose to learn to infer specific model parameters for each scene by Neural Processes.
%\zx{Advantages}
%In the following, we use NeRF generalization to illustrate the proposed method.  In NeRF generalization, a few views (one or two) of images $I_c$ and its corresponding camera pose $\mathbf{p}$ serve as the context information. 

%We represent each location in this scene by aggregating this Gaussian basis based on the radial basis function (RBF). 
%Then, based on the context information, the task is to estimate an implicit function that can be used to infer the target for unseen views.

% The proposed GP-INR framework assumes a space composed of a set of Gaussian basis that encode the spatial location and semantic information of a scene. Initially, GP-INR predicts a Gaussian basis $B_C$ from the observed context. Similar to the recent advances in implicit neural fields (e.g., Neurbf~\cite{chen2023neurbf}), at arbitrary new locations $X_T \in \mathbb{R}^{N\times P\times 3}$, we aggregate the Gaussians to form a new representation of this location $X'_C$ via the radial basis function (RBF). By averaging the representations both globally and locally along rays, we obtain a global latent $g_C$ and a ray-specific latent $r_C$ to modulate an MLP for predicting the implicit neural field. During training, with the target view and image $I_T$ also provided, we perform the same procedures for $I_T$ and align latent variables from both the context and target. For simplicity, in the following, we will use $I_C$ as an example to illustrate. 

% Introducing B
% ; 2). enrich the query points representation by aggregating the locality scene information

% \noindent \textbf{Stochastic Processes with Geometry Bases.} 
% By integrating the prior distribution with geometry bases ${\bf{B}}_{C}$ (Eq. \ref{eq: prior_f}) into the predictive distribution in Eq. \ref{eq: gp_w/o_B}, we reformulate the predictive distribution $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C})$ as $p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C})$.
% The stochastic processes for NeRF with geometry bases are then derived as:
% % With the encoded geometry base ${\bf{B}}_{C}$, we reformulate the querying step as $p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C})$. By integrating the approximated prior with the bases Eq.~\ref{eq: prior_f} into the generative process in Eq.~\ref{eq: gp_w/o_B}, we derive the stochastic processes for NeRF with Geometry Bases as:
% \begin{equation}
%     % p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{\widetilde{X}}}_{C}, {\bf{\widetilde{Y}}}_{C}) = 
%     p({\bf{{Y}}}_{T} | {\bf{{X}}}_{T}, {\bf{{B}}}_{C}) = \int p({\bf{Y}}_{T}|f_{NeRF}, {\bf{X}}_{T}) p(f_{NeRF}| {\bf{X}}_{T}, {\bf{B}}_{C}) df_{NeRF}.
% \label{eq: gp_w_B}
% \end{equation}
% Inferred from the context-generated 3D information in ${\bf{{B}}}_{C}$, the NeRF function prior $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$ reduces the impact of information loss in context views and becomes more suitable for the \zx{query step} $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ in 3D space.
% Moreover, since we consider the target location information ${\bf{X}}_{T}$ as well in $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$, the NeRF function $f_{\text{NeRF}}$ is more generalizable to new views of new objects.

% This formulation provides a proxy between 2D context information and the 3D space by representing the 3D space with the continuous Gaussian basis. $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$ is the conditional function distribution where we sample the modulation variable to modulate a shared MLP as the NeRF function, and $p({\bf{Y}}_{T}|f_{\text{NeRF}}, {\bf{X}}_{T})$ is NeRF \zx{rendering process}. We provide a new perspective to model the function distribution fully in 3D space. By doing so, we eliminate the information loss from 2D context to 3D location space.

% \begin{equation}
%     X'_T = \text{MLP}(\sum_i^{M} \varphi(x, \mu_i, \Sigma_i)\omega_i)
% \end{equation}s

% \begin{equation}
%     \varphi(x, \mu_i, \Sigma_i) = \exp (-(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) /2)
% \end{equation}

%Hence, instead of directly using the 2D context set, we update the Gaussian basis (both shape and representation) in 3D space. Then, it is natural to use a continuous aggregation function (e.g. Gaussian radial basis function) to enrich the information at each location. By doing so, we eliminate the information loss from 2D context to 3D location space. 




% \subsection{Modeling of Geometry-aware Neural Processes}
% \subsection{Geometry-aware Neural Processes with Hierarchical Modulation}
% \label{sec: hierar}

% % To achieve the 
% To implement the inference of the NeRF function, we propose the Geometric Neural Process (\textbf{GeomNP}) for radiance field generalization, which efficiently adapt the functional distribution to new 3D objects. Specifically, \textbf{GeomNP} are constructed on a hierarchical Bayes framework. For 3D objects, the framework encodes object-specific and ray-specific latent variables in different levels, which helps the developed model querying the structure information from the Geometry bases $\mathbf{B}_C$. 

% \zx{why and how neural processes?}
% In this section, we present how to perform the \zx{rendering process} $p({\bf{Y}}_{T}| {\bf{X}}_{T},  {\bf{B}}_{C})$, including sampling a function from $p(f_{\text{NeRF}}|{\bf{X}}_{T}, {\bf{{B}}}_{C})$. 

% \noindent{{\textbf{Hierarchical Latent Variables.}}}
% The motivation of hierarchical latent variables in our model is the data structure of target inputs ${\bf{X}}_{T}$.
% As mentioned in Eq.~\ref{eq: definiation}, the target inputs can be obtained by randomly sampling ${\bf{x}}_{T}^{1:P}$ from each ray ${\widetilde{\bf{x}}}_{T}$. Thus, target input ${\bf{X}}_{T}$ consists of $N \times P$ location points $\{{\bf{x}}_{T}^{1:P, n}\}_{n=1}^{N}$ given $N$ rays. 
% Since the rendering of each ray is independently \cite{martin2021nerf}, we reformulate the predictive distribution for NeRF in a ray-specific manner:
% \begin{equation}
%     p({\bf{Y}}_{T}| {\bf{X}}_{T},  {\bf{B}}_{C})  = \prod_{n=1}^{N} p({\bf{y}}_{T}^{1:P, n}| {\bf{x}}_{T}^{1:P, n},  {\bf{B}}_{C}).
% \label{eq: predictive_distribution_ray_specific}
% \end{equation}
% Based on the hierarchical data structure of the target inputs, we design a hierarchical latent variable model. In the proposed model, object-specific latent variables ${\bf{g}}$ encode the entire 3D object information; each ${\bf{g}}$ corresponds $N$ individual ray-specific latent variables $\{{\bf{r}}^{n}\}_{n=1}^{N}$.

% \begin{wrapfigure}{r}{0.5\textwidth}
% \vspace{-5mm}
% \centering
% \centerline{
% \includegraphics[width=0.45\columnwidth]{Figures/graphical_model.png} 
% } 
% \vspace{-2mm}
% \caption{\textbf{Graphical model for the proposed geometric neural processes.}
% }
% \vspace{-2mm}
% \label{fig: graphical_model}
% \end{wrapfigure}
%${\mathbf{z}_o}$

% \subsection{Inferring with Geometric Bases and Modulation}

% \str{The following paragraph is not very clear.}

In the modeling of {\method{}}, the prior distribution of each hierarchical latent variable is conditioned on the geometric bases and target input. 
%\textcolor{blue}{To infer each latent variable, we first integrate the geometric bases and each target input, yielding a location representation specific to the target input. The location representation has access to relevant locality information from the geometry bases, \textit{i.e.}, $<{\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C >$. }
% For generalization, we need to infer latent variables that are specific to the target input. 
% To this end, 
We first represent each target location by integrating the geometric bases, \textit{i.e.}, $<{\bf{x}}_{T}^{n}, {\bf{B}}_C >$, which aggregates the relevant locality and semantic information for the given input. 
Since ${\bf{B}}_{C}$ contains $M$ Gaussians, we employ a Gaussian radial basis function in \cref{eq:rbf_agg} between each target input ${\bf{x}}_{T}^{ n}$ and each geometric basis ${\bf{b}}_i$ to aggregate the structural and semantic information to the 3D location representation. Thus, we obtain the 3D location representation as follows:
\begin{equation}
\label{eq:rbf_agg}
    <{\bf{x}}_{T}^{n}, {\bf{B}}_C > = \texttt{MLP}\Big[\sum_i^{M} \exp (-\frac{1}{2}({\bf{x}}_{T}^{n}-\mu_i)^T\Sigma_i^{-1}({\bf{x}}_{T}^{n}-\mu_i) ) \cdot \omega_i\Big],
\end{equation} 
where $\texttt{MLP}[\cdot]$ is a learnable neural network.
With the location representation $<{\bf{x}}_{T}^{n}, {\bf{B}}_C >$, we next infer each latent variable hierarchically, in object and ray levels. 

\noindent {\textbf{Object-specific Latent Variable.}} The distribution of the object-specific latent variable ${\bf{z}}_o$ is obtained by aggregating all location representations:
\begin{equation}
    [\mu_{{o}}, \sigma_{{o}}] 
    = \texttt{MLP}\Big[\frac{1}{N \times P}\sum_{n = 1}^{N}\sum_{\mathbf{r}}
    <{\bf{x}}_{T}^{n}, {\bf{B}}_C >\Big],
\end{equation} 
where we assume $p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)$ is a standard Gaussian distribution and generate its mean $\mu_{o}$ and variance $\sigma_{o}$ by a ~\texttt{MLP}. 
Thus, our model captures objective-specific uncertainty in the NeRF function.


\noindent {\textbf{Ray-specific Latent Variable.}} 
% By ray-specific latent variable, the object-specific is expected to capture the local details.
To generate the distribution of the ray-specific latent variable, we first average the location representations ray-wisely. 
We then obtain the ray-specific latent variable by aggregating the averaged location representation and the object latent variable through a lightweight transformer. We formulate the inference of the ray-specific latent variable as:
\begin{equation}
    [\mu_{{r}}, \sigma_{{r}}] = \texttt{Transformer} \Big[\texttt{MLP}[\frac{1}{P}\sum_{\mathbf{r}}
    <{\bf{x}}_{T}^{n}, {\bf{B}}_C >]; \hat{{\bf{z}}}_o \Big],
\end{equation}
where $\hat{{\bf{z}}}_o$ is a sample from the prior distribution $p({\bf{z}}_o | {\bf{X}}_T, {\bf{B}}_C)$. 
Similar to the object-specific latent variable, we also assume the distribution $p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}}_C)$ is a mean-field Gaussian distribution with the mean $\mu_{{r}}$ and variance $\sigma_{{r}}$. We provide more details of the latent variables in Appendix~\ref{supp:latent-variables}.

% The detail of the used Transformer is given in the Appendix. In summary, our latent variable is in a hierarchical structure. 
% The location-specific latent variable $r_C$ is to modulate the NeRF function specific to the queried ray (or pixel location in 2D image). We infer a ray-specific variable $r_c$ by first aggregating the feature ray-wisely:
% \begin{equation}
%     r'_c = \text{MLP}(\frac{1}{ P}\sum_j^{P}(\bf{X}'_T)[j])
% \end{equation}


%Then, we perform a hierarchical formulation by incorporating the global variable to obtain the ray-specific variable. This enables us to design the latent variable in a hierarchical structure, which is rational as it learns the scene coarse-to-fine. 

% Then, $r_c$ is obtained by feeding $r'_c$ together with the sampled $\hat{g}_c$ into a Transformer: 
% \begin{equation}
%     r_c = [\mu^c_r, \sigma^c_r] = \text{Transformer}([\hat{g}_c; \hat{r}_C]),
% \end{equation}
% where $\mu^r_g$ and $\sigma^r_g$ are the mean and variance of global variables, respectively. The detail of the used Transformer is given in the Appendix. In summary, our latent variable is in a hierarchical structure. 

\noindent  \textbf{NeRF Function Modulation.}
With the hierarchical latent variables $\{{\bf{z}}_o, {\bf{z}}_r^n\}$, we modulate a neural network for a 3D object in both object-specific and ray-specific levels.  Specifically, the modulation of each layer is achieved by scaling its weight matrix with a style vector~\citep{guo2023versatile}. 
The object-specific latent variable ${\bf{z}}_o$ and ray-specific latent variable ${\bf{z}}_r^n$ are taken as style vectors of the low-level layers and high-level layers, respectively. The prediction distribution $p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C})$ are finally obtained by passing each location representation through the modulated neural network for the NeRF function. 
More details are provided in Appendix~\ref{supp:modulate}. 
% The modulated MLP layer used in our paper is similar to the style \textit{modulation} in ~\cite{guo2023versatile}. Essentially, we predict a style vector $s\in \mathbb{R}^{d_{in}}$ to multiply or scale the weight matrix of an MLP, $W \in \mathbb{R}^{d_{in} \times d_{out}}$.


% \subsection{Inference of Geometry-aware Neural Processes}
% \label{sec: elbo}
% ELBO

% \noindent{\textbf{Variational Posteriors with the Geometry Bases.}} Solving \textbf{GANPs} with Eq.~\ref{eq:ganp-model} involves estimating the true posterior, $p({\bf{g}},  \{{\bf{r}}_i\}_{r=1}^{N_{ray}} | {\widetilde{\bf{X}}}_T, {\widetilde{\bf{Y}}}_T)$ which is intractable. Hence, we introduce a variational posterior distribution, which can be factorized as follows:
% \begin{equation}
% p({\bf{g}},  \{{\bf{r}}_i\}_{r=1}^{N_{ray}} | {\widetilde{\bf{X}}}_T, {\widetilde{\bf{Y}}}_T) \approx q_{\theta, \phi}({\bf{g}},  \{{\bf{r}}_i\}_{r=1}^{N_{ray}} | {\bf{X}}_T, {\bf{B}}_T),
% \end{equation}

\subsection{Empirical Objective}
\label{sec: object}

\noindent{\textbf{Evidence Lower Bound.}} 
To optimize the proposed \method{},
we apply variational inference~\citep{garnelo2018neural} and derive the evidence lower bound (ELBO) as:
\begin{equation}
\begin{aligned}
% \mathcal{L}_{\text{ELBO}}
& \log   p({\bf{Y}}_{T}| {\bf{X}}_{T}, {\bf{B}}_{C})
\geq \\
&\mathbb{E}_{q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)}  \Big\{  \sum_{n=1}^{N}  \mathbb{E}_{q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{\mathbf{r}, n}, {\bf{B}_T})} \log p({\bf{y}}_{T}^{{\mathbf{r}}, n}| {\bf{x}}_{T}^{{\mathbf{r}}, n}, {\bf{z}}_o, {\bf{z}}_r^n) \\
&- D_{\text{KL}}[q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {{\bf{B}}_T}) || p({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {{\bf{B}}_C}) ] \Big\} \\
& - D_{\text{KL}}[q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T) || p({\bf{z}}_o | {\bf{B}}_C,  {\bf{X}}_T)], \\
\end{aligned}
\end{equation}
where $q_{\theta, \phi}({\bf{z}}_o,  \{{\bf{z}}_r^i\}_{i=1}^{N} | {\bf{X}}_T, {\bf{B}}_T) = \Pi_{i=1}^{N}q({\bf{z}}_r^n| {\bf{z}}_o,  {\bf{x}}_{T}^{{\mathbf{r}}, n}, {{\bf{B}}_T}) q({\bf{z}}_o | {\bf{B}}_T,  {\bf{X}}_T)$ is the involved variational posterior for the hierarchical latent variables.  ${\bf{B}}_T$ is the geometric bases constructed from the target sets $\{{\bf{\widetilde{X}}}_{T}, {\bf{\widetilde{Y}}}_{T}\}$, which are only accessible during training. 
The variational posteriors are inferred from the target sets during training, which introduces more information on the object. 
The prior distributions are supervised by the variational posterior using Kullback–Leibler (KL) divergence, learning to model more object information with limited context data and generalize to new scenes. Detailed derivations are provided in Appendix~\ref{supp:elbo}.

% \noindent{\textbf{Empirical Objective.}} 
For the geometric bases $\mathbf{B}_C$, we regularize the spatial shape of the context geometric bases to be closer to that of the target one $\mathbf{B}_T$ by introducing a KL divergence. 
Therefore, given the above ELBO, our objective function consists of three parts: a reconstruction loss (MSE loss), KL divergences for hierarchical latent variables, and a KL divergence for the geometric bases. 
%constraint for matching the two sets of Gaussian basis to ensure the basis obtained from context is as close to the one from the target. 
The empirical objective for the proposed \method{} is formulated as:
\begin{equation}
\begin{aligned}
& \mathcal{L}_{\text{\method{}}}  =  ||y - y'||^2_2 + \alpha \cdot \big(  D_{\text{KL}} [p(\mathbf{z}_o|{\bf{B}}_C)|q(\mathbf{z}_o|{\bf{B}}_T)] \\
    & + D_{\text{KL}}[p(\mathbf{z}_r|\mathbf{z}_o,{\bf{B}}_C)|q(\mathbf{z}_r|\mathbf{z}_o,{\bf{B}}_T)] \big) + \beta \cdot D_{\text{KL}}[{\bf{B}}_C, {\bf{B}}_T],
\end{aligned}
\end{equation}
where $y'$ is the prediction. $\alpha$ and $\beta$ are hyperparameters to balance the three parts of the objective. The KL divergence on ${\bf{B}}_C, {\bf{B}}_T$ is to align the spatial location and the shape of two sets of bases. 

% +++++++++++++

% \noindent 
% \textbf{Neural Fields.}
% % \zx{First introduce NeRF (with problem definition and notations) and its disadvantages of inefficient inference, then say what we will do to avoid this problem by NP?} 


% In general, training a NeRF requires overfitting each scene, which is time-consuming. Hence, how to leverage the observation for generalization on the new scene remains a problem. As each INR is a function of a data sample, the problem can be viewed as estimating the distribution of functions. This motivates us to use the Neural Process (NP) in this problem. 
 

% \noindent {\textbf{Neural Process.}} Neural Process~\cite{garnelo2018neural} parameterizes the distribution of functions. Given the context set comprising of the observation $X$ and the corresponding labels $Y$, $D_C = (X_C, Y_C) := (\mathbf{x}_i, \mathbf{y}_{i})_{i\in C} $. The aim of NP is to learn a mapping function from the target points $X_T$ to the target labels $Y_T$,  $D_T = (X_T, Y_T) := (\mathbf{x}_i, \mathbf{y}_{i})_{i\in T} $. The conditional distribution of target points is:
% \begin{equation}
%     p_{\phi}(Y_T|X_T,D_C) = \prod_{\mathbf{x}, \mathbf{y}\in D_{T}} \mathcal{N}(\mathbf{y};\mu_{\mathbf{y}}(\mathbf{x},D_c), \sigma^2_{\mathbf{y}}(\mathbf{x},D_c)).
% \end{equation}

% \begin{equation}
%     p_{\phi}(Y_T|X_T, \mathbf{z}) = \prod_{\mathbf{x}, \mathbf{y}\in D_{T}} \mathcal{N}(\mathbf{y};\mu_{\mathbf{y}}(\mathbf{z}, X_T,D_c), \sigma^2_{\mathbf{y}}(\mathbf{z}, X_T,D_c)),
% \end{equation}
% where $\mathbf{z} \sim p_{\theta}(\mathbf{z|X_T,D_C})$. Using NP can efficiently leverage the limited context/observation to infer the function of INR for the target from a probabilistic perspective. It also can incorporate uncertainty estimation for the unseen view. This is reasonable as the value in the unseen target location should not be deterministic. 

%\zx{advantages of NP? incorporating uncertainty for limited context information, which is suitable for reconstruction tasks?}







% \begin{align}
%     p(\mathbf{y}|\mathbf{x}, I) & = \int_{g} \int_{r}  p(\mathbf{y}, g, r |\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \\
%     &=  \int_{g} \int_{r}  p(\mathbf{y}| g, r) p(g, r | \mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \\
%     % &= \int_{g} \int_{r} \int_{B} p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r \mathrm{d}B 
%     &= \int_{g} \int_{r}  p_{\theta_1}(\mathbf{y}| g, r) p_{\theta_2}(g, r | B) p_{\theta_3}(B|\mathbf{x}, I) \mathrm{d}g  \mathrm{d}r 
% \end{align}


%\subsection{Gaussian Basis for INR}
%As shown in Fig.~\ref{fig:framework}, 


%\subsection{Hierarchical Neural Process for INR}







