
%1. what's NeRF

Neural Fields (NeFs) and Implicit Neural Representations (INRs)~\citep{sitzmann2020implicit,tancik2020fourier} have recently gained popularity for their ability to learn continuous, compact, and efficient representations of continuous signals, especially for 3D settings~\citep{park2019deepsdf,mildenhall2021nerf,mescheder2019occupancy,chen2022tensorf}. Building on NeFs, neural radiance fields (NeRFs)~\citep{mildenhall2021nerf,barron2021mip} model 3D scene representation as a mapping from 3D coordinates and view directions to color and density values. By integrating these values along camera rays, NeRFs can render photorealistic images of scenes from novel viewpoints. 
Although NeRFs achieve good reconstruction performance, they must be overfitted to each 3D object or scene, resulting in poor generalization to new 3D scenes with few context images.

% Thus, fast adaptation of NeRF representations for new 3D scences is still unexplored 
% Thus, one of the main challenges of NeRFs is to improve their generalization for novel 3D scenes.

% popular for representing data as neural functions, mapping continuous coordinates to signals.
% For instance, 3D scene representations~\cite{park2019deepsdf,mildenhall2021nerf,mescheder2019occupancy,chen2022tensorf} favor INRs due to their continuous, compact, and efficient data representation properties.
% However, characterizing a signal by neural network parameters often requires retraining the network on individual data samples, which is computationally expensive.
% Therefore, models must quickly adapt to new signals from partial observations without extensive fine-tuning.

% \zx{This paragraph is a bit long.}

%Despite the need of fast generalization to the new scene for real-time rendering, 
%\str{I am not sure I follow the logic of this paragraph. We are mixing metalearning, probabilistic learning, 3D and 2D partial obersbation, global and local. What exactly is our point?}
In this paper, we focus on radiance field generalization and fast adaptation of the INR function for novel 3D scenes using only a few context image views. Previous works on INR generalization have approached the problem by gradient-based meta-learning~\citep{tancik2021learned} to adapt to new scenes with a few optimization steps~\citep{tancik2021learned,papa2023train}, 
%\str{Add the CVPR reference for Samuele}
modulating shared MLPs through HyperNets~\citep{chen2022transformers,mehta2021modulated,dupont2022data,kim2023generalizable}, or directly predicting the parameters of scene-specific MLPs~\citep{dupont2021generative,erkocc2023hyperdiffusion}. However, the deterministic nature of these methods cannot account for the uncertainty of scenes or INR functions when only few partial observations are available. This is unrealistic since there can be different interpretations of limited observations. 

To account for uncertainty induced by few available context images, probabilistic INR functions for NeRF \citep{gu2023generalizable, guo2023versatile, kosiorek2021nerf} have also been recently explored. VNP~\citep{guo2023versatile} and PONP~\citep{gu2023generalizable} infer the INR function using Neural Processes (NPs)~\citep{bruinsma2023autoregressive, garnelo2018neural, wang2020doubly, shen2024episodic}, a probabilistic meta-learning method that models functional distributions conditioned on partial signal observations.
\str{I think this is not a very strong way of putting it, sounds too niche. I would rather say that NeRF is basically a mapping from a collection of 2D input domain to 3D output domain. These methods focus on the 3D only, that is the output, without account for the fact that this uncertainty should also be directly grounded in the 2D input domain. To make it sound more fundamental (the way it already is).}
These probabilistic methods, however, only approximate the INR functions in 3D space, neglecting the interaction between 3D functions and 2D observations. 
% Radiance fields model 3D relationships, but the context is just 2D posed images. The inherent information misalignment between the radiance field function and context images could hinder these models' generalization.
Since the radiance fields model relationships in 3D space, while the only available context observations are 2D images, there is an information misalignment between contexts and functions in radiance field generalization.


%\str{What is probabilize? There is no such word.}
% However, these methods only probabilize the INR functions in the 3D space, neglecting the interaction between 3D functions and 2D partial observations. 
% In contrast, some other methods model in probabilistic perceptives. 
% NeRF-VAE~\cite{kosiorek2021nerf} learns a distribution over radiance fields conditioning on a latent scene representation. 
%Some other works~\cite{} utilize GAN~\cite{goodfellow2020generative} to implicitly learn the distribution of functions. 

% Therefore, despite being probabilistic, the function inferred from 2D observation partials is suboptimal due to the information loss from 2D to 3D spaces.
% Neural processes (NP) is also a meta-learning method, but in a probabilistic manner~\cite{bruinsma2023autoregressive}, which aims to model the distribution of functions given partial observations. In this work, we aim to model the distribution of implicit functions, which aligns with the formulation of NPs. 

%\str{Can we make clear what are the contributions, in short?}
\str{Unclear (in the first sentence) why the solution is specific to the problem we highlighted in the previous paragraph (the discrepancy between 2D and 3D)? It becomes sort of clear in the 2) ..., but it has to be more explicit and direct. What exactly is the relation between 2D and 3D geometry, and how do we make it possible to model this? This has to be in the first sentence, and also in the explanation later on. 'Observations in 2D space with 3D prior structures' sounds a bit vague, not as explicit as it can be. Also, 1, 2, 3, feel a bit disconnected. Is 3 just a random add on that is orthogonal to 1 and 2? Until now, we never talked about the multi-scale nature of geometric structures in the world. I would say that this a key point, if others don't really account for this. Overall, this paragraph needs to be rewritten.}
To efficiently adapt to new signals with few observations, 
we propose probabilistic radiance field generalization with Geometric Neural Processes (\name{}). 
Our contributions can be summarized as follows: 
\textit{1) Probabilistic NeRF generalization framework.}  We cast radiance field generalization as a probabilistic modeling problem. By doing so, we can amortize the probabilistic model over multiple objects with few views, facilitating the learning and generalization of NeRF functions.
% can accelerate the learning and generalize by amortizing the probabilistic model over multiple objects
% For NeRF generalization, we accelerate the learning and generalize by amortizing the probabilistic model over multiple objects and then obtaining per-object reconstructions by conditioning on context sets $\{{\widetilde {\bf{X}}}_C, {\widetilde {\bf{Y}}}_C\}$.
% The probabilistic model contains an INR function in 3D space and the transition between 2D and 3D variables (ray sampling and integration). This enables to design stochastic processes to model the INR function in 3D by considering the probabilistic interaction between 2D and 3D variables.
\textit{2) Geometric bases.} To eliminate the 
potential information misalignment, we design geometric bases by encoding observations in 2D space with 3D prior structures. Thus, the geometric bases can aggregate locality information to each 3D point, improving the exploration of high-frequency details.
\textit{3) Geometric neural processes with hierarchical latent variables.}
Based on the geometric bases, we develop geometric neural processes to capture the uncertainty in the latent NeRF function space.
Specifically, we introduce hierarchical latent variables to modulate the INR function at multiple spatial levels, yielding better generalization on new scenes and new views.
% incorporates prior structure information with  into the inference of the INR function distribution. 
% The hierarchical latent variables enable modulating the INR function at multiple spatial levels, thus improving the generalization of INR functions.
Experiments on novel view synthesis of ShapeNet objects and real-world DTU scenes demonstrate the effectiveness of the proposed method on 3D radiance field generalization. Nevertheless, the proposed method can seamlessly apply to INR generalization in 2D signals (images). %settings.

% Specifically, we train an attention-based neural network to construct a set of geometric bases from the 2D context observations. 
% Consisting of 3D Gaussians, the geometric bases provide structure information and representations of the 3D scenes, which are utilized to infer the function distribution in 3D. 
% Furthermore, with the geometric bases, we introduce hierarchical latent variables in both object and ray levels to infer the predictive function. 
% By fully utilizing the 3D information provided by the geometric bases, the latent variables specialize information of different spatial levels, achieving both object-specific and ray-specific INR functions for generalization on limited contexts.


% ++++++++++++++++++ editing

% Despite the previous effort on modeling probabilistic INR, they ignore the structure information in the space and only focus on the global object-level latent variable or modulation. 
% %However, such works ignore the information loss from the 2D context observation to the 3D space \wy{sampling issues, maybe refer learn to sample paper} and lack structure information. 
% %Moreover, the hierarchical neural process in VNP is sequential, not exploring the structure information. 
% % However, such methods ignore the local information, resulting in missing the high-frequency details of objects. 
% The structure information is a proxy between the ``generality'' and “specialization”~\cite{cong2023enhancing,bauer2023spatial}. To achieve generality, the model should be able to capture the shared context, which is the global object level. On the other hand, to have high-fidelity rendering results, the model should be able to infer the sample-specific or region-specific information, e.g. specialized self-similar appearance patterns, which is at the local level. 
% In this work, we propose to use hierarchical neural processes (HNP)~\cite{wang2022learning,wang2020doubly,shen2024episodic} to model the distribution of functions, which allows us to explicitly introduce a global and local variable to incorporate the structure information globally and locally. 
% We found that our local variable suits the neural radiance field (NeRF) framework well. Hence,
% to better demonstrate our local variable, we illustrate our method on neural radiance field generalization.
% However, our method is also applicable to other INRs, such as image regression and image completion, as shown in our experiments. 


% In an NPs setting, the context set is given to infer a function for the target set, which models a conditional distribution. For neural radiance fields (NeRF), the context set is the few-view images with the corresponding camera rays. 
% However, the NeRF function we aim to approximate is inherited in 3D space. There exists a gap between the context information and the NeRF function
% %, which is commonly ignored in previous INR generalization literature 
% due to the sampling and integration\cite{martin2021nerf,arandjelovic2021nerf,fang2021neusample,lindell2021autoint,neff2021donerf} from the camera ray to the projected 2D image pixels. \wy{discussion: these papers all about how to improve sampling to speed up the Monte Carlo for efficient training.} This may cause information loss between 2D context and 3D function.
% %inconsistency in formulation. 
% %For instance, 3D points are sampled along the rays~\cite{martin2021nerf}, which may affect the quality of integrating information along camera rays to render the corresponding pixels~\cite{arandjelovic2021nerf}. 
% Instead of directly building the conditional distribution on observed camera rays and projected 2D images, we derive our function distribution fully on the 3D space based on a set of geometric Gaussian basis spanned in space, 
% %Specifically, we choose to use the Gaussian basis due to its continuous property
% which is favored by previous 3D reconstruction methods~\cite{genova2020local,chen2023neurbf,kerbl20233d}. The geometric Gaussian basis is able to reflect the shape of objects (global structure) and aggregate the locality information~\cite{chen2023neurbf,chen2022tensorf} into arbitrary 3D locations in a continuous way.  
% Finally, we provide evidence of a lower bound to train the model. Our approach enables neural radiance field generalization in a single feedforward pass, without fine-tuning. Our contributions can be summarized as follows:
% \begin{enumerate}
%     \item We propose a new probabilistic formulation for NeRF generalization based on hierarchical neural processes, which is also applicable to general INR generalization.
%     \item We propose to infer a set of posterior geometric bases to embed the structure information globally and locally. 
% \end{enumerate}










%RBF kernels have been explored in previous NeRF research
%, as well as other 3D scene reconstruction techniques, like Gaussian Splatting~\cite{kerbl20233d} which assumes the space is composed of a set of Gaussians. 

%To address the above-mentioned issues, we introduce a set of geometry basis spanned in space to provide structure information. 
%In 3D NeRF, this can also eliminate the information loss by directly inferring the continuous 3D space basis from the 2D context information. Then, we contribute to a neural processes formulation of NeRF, based on which we perform hierarchical NP. The hierarchical NP consists of a global variable that models the scene information and a local variable that models the ray-wise or location-wise information. Finally, we provide evidence of a lower bound to train the model. Our approach enables neural radiance field generalization in a single feedforward pass, without fine-tuning. 


%


%-------------------------------------------------

% comments of why using basis:
% 1. For modeling the local neural features (check how this local neural features help?). because of the difficulty
% in representing high-frequency details due to the inductive
% bias [5, 70] of MLPs. To tackle this problem, local neural fields have been proposed and widely adopted. 

% 2. 3D Gaussians as a flexible and expressive scene representation

% 2. flexible kernel position and shape, which have higher spatial adaptivity and can more closely fit target signals

% 3. challenges of current Radiance Field Generalization methods(do not consider uncertainty, fine-tuning, time-consuming, balabala),
% why we use NP

% 4. What we do in our method (our contributions) (model the problem in a probabilistic framework to infer the function for Radiance Field Generalization in a single feedforward pass, without fine-tuning; propose geometric basis to solve the information gap between 2D context and 3D target function; hierarchical neural processes/modulation for XXX)


% The idea of using Geometry bases origins from the previous NeRF works~\cite{chen2022tensorf,muller2022instant} which use either gird regular point latent representation or RBF kernels in the space, to store the 3D scene geometry and semantic information, as well as the Gaussian Splatting~\cite{kerbl20233d} which assumes the space is composed of a set of Gaussians. 


%GENERALIZATION IN DIFFUSION MODELS ARISES FROM GEOMETRY-ADAPTIVE HARMONIC REPRESENTATIONS shows the geometry prior basis connect to our motivation.

% Keypoint: 

% 1. We want to model the distribution of functions to achieve implicit neural field generalization, which is suitable for Neural Processes. 

% 2. The context information has an information gap with the 3D Radiance Field. Previous generalization work, including VNP~\cite{guo2023versatile} ignores this. This motivates us to introduce a set of Gaussian basis to encode the geometry information in the scene. Also, by doing so, we are able to connect NP and NeRF generalization more tightly. 

% 3. The Gaussian basis with radial basis function provides a continuous way of representing each spatial location. 