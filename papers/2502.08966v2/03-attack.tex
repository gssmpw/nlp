\newcommand{\redacted}{\lozenge}
\section{Tool-based Agent Systems}
\begin{figure}[ht]
    \hspace{-0.1in}\includegraphics[width=1.05\linewidth]{figures/Setup.pdf}
    \caption{Illustration of Tool-based Agent Systems and their security risks. }
    \label{fig:tbas}
\end{figure}


Figure \ref{fig:tbas} illustrates a high-level overview of TBAS. This section provides a concrete description of the TBAS model relevant to our techniques. We assume a user interacts with the agent through a chat interface, similar to ChatGPT or Gemini. The user submits a request, and the agent attempts to fulfill it by leveraging its internal knowledge, tool calls, and prior interactions within the same session.

\begin{algorithm*}
\caption{Tool-Based Agent System (TBAS)}
\label{alg:TBAS}
\begin{algorithmic}[1]
\Require Initial System Message $m_s$, Environment $E$
\State $M \gets\cdot,m_s $ \Comment{Initialize with System Message}
\While{$\textsc{user\_continue}()$} \Comment{If user continues interaction}
    \State $M \gets \textsc{user\_request}() \mathbin{::} M$ \Comment{Append user message to $M$}
    \State $\textbf{ToolCalls} \gets \textsc{get\_next\_toolcalls}(M)$ \Comment{Generate new tool calls based on $M$}
    \ForAll{$t^i \in \textbf{ToolCalls}$}
        \State $E, m \gets \textsc{call\_API}(t^i, E)$ \Comment{Run tool $t^i$ with environment $E$; update $E$ and return $m$}
        \State $M \gets M,m$ \Comment{Append tool response to $M$}
    \EndFor
    \State $M \gets M,\textsc{lm\_response}(M)$ \Comment{Append response from the LM to the user response to $M$}
\EndWhile
\end{algorithmic}
\end{algorithm*}

To illustrate how TBAS work more concretely, we first present some relevant terminologies: 

\noindent
Symbols and Terminologies:
{
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation}
\begin{array}{l@{\hspace{2cm}}l}
\textbf{Message} & m \\
 \textbf{Messages} & M \\
\textbf{Tool Call with inputs i} & t^{i}  \\
\textbf{External Environment}& E  
\end{array}
\label{list:tbas_syms}
\end{equation}}
\noindent Definitions:
{
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation}
\begin{array}{l@{\;}rl}
\textbf{ToolCalls} \enspace & =& \cdot \mid t^{i} \mathbin{::}\textbf{ToolCalls}   \\
 M & = & \cdot \mid M, m
\end{array}
\label{list:tbas_defs}
\end{equation}
}


\noindent
Metafunctions:
{
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation}
\fontsize{9.5}{12}\selectfont %
\hspace*{-5mm}
\begin{array}{l@{\;}rl}
\textbf{GET\_NEXT\_TOOLCALLs} & : & M  \mapsto \textbf{ToolCalls} \\
\textbf{CALL\_API} & : & t^i\times E \mapsto m\times E \\
\textbf{LM\_RESPONSE} & : & M  \mapsto m \\
\textbf{USER\_REQUEST} & : & M  \mapsto m \\
\textbf{USER\_CONTINUE} & : & M  \mapsto \textbf{TRUE}\mid \textbf{FALSE} 
\end{array}
\label{list:tbas_funcs}
\end{equation}
}

The TBAS agent is initialized with a system message ($m_s$) provided by the agent developer, which defines the agent’s role and tone. The developer also supply a list of tools, each defined by its name, signature (describing the legal invocation format), and descriptions. These tools correspond to APIs callable by the runtime.

The agent’s application state, or history, consists of all messages exist in the system: the initial system message, user-issued requests, tool outputs, previous LM responses from the assistant. These elements are concatenated into a text-based input state, which the LM processes to decide its next action—whether to respond to the user or invoke a tool.

At the start of a session, only the initial system message ($m_s$) is present. When the user sends a request based on their initial request or responding to previous interactions, a message is appended to the current history(\textbf{USER\_REQUEST}). 

Based on this input, the LLM generates zero or more Tool Calls(\textbf{GET\_NEXT\_TOOLCALLs}).

The runtime inspects the Tool Call sections and executes the corresponding APIs specified by the developer(\textbf{CALL\_API}). The results from the tool calls are collected and are also appended to the history. 
The LM then processes the entire updated history and generates another message to provide the user with a response (\textbf{LM\_RESPONSE}).

If the user wishes to continue with this conversation, the process is started afresh(\textbf{USER\_CONTINUE}). 

We illustrate this process in Algorithm \ref{alg:TBAS}.

\section{Attack Model for Prompt Injection} 

\textbf{Attacker’s Goal.} The attacker seeks to manipulate the interactions between the user and the Tool-Based Agent System (TBAS) by influencing the tool calls the TBAS might make. These manipulated tool calls can result in the leakage of confidential information or introduce harmful side effects. All malicious goals must rely on the side effects of these tool calls: e.g. using message sending tools to transmit credit card information or exploiting money-transfer tool to steal the user's money.

\textbf{Attacker’s Capabilities.} 
We assume the attacker has detailed knowledge of the TBAS setup, including the system instructions, the tools available to the TBAS and the specific instructions for each tool. However, the attacker does not have knowledge of timing mechanisms, nor do they have access to the internal state or behavior of the agent itself. The attacker is also unaware of user inputs and cannot directly observe the arguments or responses of the tools.

The attacker can influence the output of any tool that depends on external inputs, provided that this influence does not require compromising the tool itself. They cannot modify the implementation of a tool or intercept or alter the communication between a tool’s API and the TBAS. The attacker cannot hack the underlying data source of a tool beyond what is feasible for an untrusted third party interacting with the tool’s underlying application in a legitimate manner. However, the attacker can interact with the application as a normal user and modify data that the tool subsequently reads. See examples in \ref{subsec:prompt_inj_integrity}.





\section{Robust TBAS Objectives and Assumptions}

\subsection{Objectives} Under prompt injection attacks and other sources of confidential data leaks, our primary goals of \textit{robustness} is to:
\begin{itemize}[noitemsep]
    \item \textbf{Prevent private data leakage} -- Ensure that user's private data is not passed to external environments without explicit user confirmation.
    \item \textbf{Defend against prompt injection} -- Ensure that attacker instructions do not lead to unwanted side-effects that compromises the integrity of the user's system.
\end{itemize}
Our secondary goals are:
\begin{itemize}[noitemsep]
\item \textbf{Maintain Utility under attack} -- Minimize disruptions to user tasks, even under possible prompt injection attacks.
\item \textbf{Minimize overhead} -- Minimize unnecessary compute or user confirmations to achieve the above goals.
\end{itemize}

\subsection{Assumptions} 
\label{subsec:robust_tbas_assumptions}
As is standard in Information Flow research, we assume that these labels on both the User and Tool messages are provided to our system. We acknowledge this is a open problem in IFC and  will likely be a burden upon the agent developers to provide a lattice of security labels and an information flow policy on these labels.

More formally, we assume that the developer provides ($L$, $\sqsubseteq$, $\sqcup$) where 
\begin{itemize}[noitemsep]
    \item $L$ is a finite set of security labels, where each label consists of a pair of integrity label and confidentiality label. 
    \item $\sqsubseteq$ is a partial order representing the “flows-to” relation, which determines whether information can flow from one label to another. 
    \item $\sqcup$ is a join operation that computes the least upper bound of two labels within the lattice.
\end{itemize}
For example, in a simple four point lattice, where confidentiality levels are divided to \textbf{Secret} and \textbf{Public} and integrity levels are divided to \textbf{Trusted} and \textbf{Untrusted}, L is defined as:
{
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation}
  \begin{aligned}
    L =  \{&(\textbf{Trusted}, \textbf{Public}),(\textbf{Untrusted}, \textbf{Public}), \\
      & (\textbf{Trusted}, 
\textbf{Private}), (\textbf{Untrusted}, \textbf{Private})\}
  \end{aligned}
\end{equation}
}

Information can only flow to a category that is at least as restrictive as its source, ensuring integrity and confidentiality are preserved; the operator is reflexive since information remains in the same category, and the figure below illustrates the flow-to ($\sqsubseteq$) relation in a 4-point lattice with trust and sensitivity levels.

\vspace*{-1.5em}
\begin{figure}[H]
\centering
\includegraphics[width=0.3\columnwidth]{figures/four-point-lattice.pdf}
\end{figure}
\vspace*{-1.5em}

Our technique can generalize to a more complex and fine-grained lattice. Like many information flow problems, there are cases where a more precise lattice can lead to precise results. For example, drawing inspiration from \cite{ML00}, confidentiality can be represented as the set of channels that are allowed to access information, while integrity reflects the set of sources that have influenced it. 

Furthermore, we assume the developer and the user jointly specify the information flow policy $P$ that denotes security restrictions on a potential tool-call. 
{
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\begin{equation}
P : t^i \mapsto L
\label{eq:security_policy}
\end{equation}
}
A tool call $t^i$ is only allowed to proceed if it is called from an environment with label $(l_i,l_c)\in L$ such that $l \sqsubseteq P(t^i)$. 


We assume that both the tool's response and user messages are also sources of labels, containing regions that are labeled with either low-integrity data from external untrusted sources or high-confidential data that would be inappropriate to share unchecked. 

Internally, tools must also comply with the defined information flow policy. For example, consider a scenario with two tools: \texttt{send\_message}, which can handle private data, and \texttt{read\_sent\_messages}, which returns sent messages as public data. In this setup, a message sent to the user themselves could effectively “launder” private information. Nonadherence to the information flow policy at the tool level creates a vulnerability where a tool capable of processing private (or low-integrity) information could influence the public (or high-integrity) output of another tool, thereby violating confidentiality and integrity.






































