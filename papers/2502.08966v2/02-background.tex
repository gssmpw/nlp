\section{Background and Related Work}

\textbf{Agentic AI Systems and Tool-Based AI Agents.}
The integration of external environments with LMs is often described as \textit{Composite AI Systems} \cite{compositeAI} or simply \textit{agents}\cite{llm_powered_agents,guide_to_llm_abst}. These systems leverage the LM’s capabilities to comprehend natural language \cite{ouyang2022instruction}, perform reasoning \cite{wei2023chainofthoughtpromptingelicitsreasoning,zelikman2024quietstarlanguagemodelsteach, renze2024selfreflectionllmagentseffects} and planning \cite{huang2022languagemodelszeroshotplanners, wang2023planandsolvepromptingimprovingzeroshot, masterman2024landscapeemergingaiagent}. Tool-Based Agent Systems (TBAS) \cite{ReAct}, a subclass of LM agentic systems, operate in a single context and interact with external environments via tool calls. Widely adopted in applications like Bing’s ChatGPT integration \cite{microsoft_AI_bing_chatgpt}, TBAS also power platforms like OpenAI’s GPTs \cite{openai_gpts} and CustomGPT.ai \cite{customgpt_ai}, enabling developers to customize agents with specific instructions and tools.

\textbf{Prompt Injection For LMs.}
A prompt injection attack\cite{liu2024formalizing, liu2024promptinjectionattackllmintegrated} occurs when malicious inputs, or prompts, are introduced into the agent’s history (context) to alter its behavior. Prompt injections, often as natural language instruction pretending to be the user, but at times it could be nonsensical text making its detection even more subtle\cite{zou2023universalattack}. While users can initiate such attacks to bypass application-defined guidelines \cite{liu2024autodangeneratingstealthyjailbreak} or extract system prompts \cite{yang2024prsapromptstealingattacks}, our focus is on prompt injections originating from tools that retrieve data from untrusted sources such as other websites, public reviews, comments, etc. \cite{debenedetti2024agentdojo,zhan2024injecagentbenchmarkingindirectprompt}. These injections can maliciously manipulate the TBAS, causing it to perform unintended or harmful tasks. 

\textbf{Defenses for Prompt Injection and Privacy Leakage.} Defenses can be categorized into two strategies:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textit{Injected Prompt Detection:} Possible prompt injections can be identified using perplexity measures or another LM trained to flag anomalies \cite{protectAIdetector, rahman2024finetunedlargelanguagemodels,hung2024attentiontrackerdetectingprompt}.
    \item \textit{Prompt Impact Mitigation:} These limit injection effects using (i) data sanitization approaches such as parapharsing\cite{jain2023baselinedefensesadversarialattacks}, retokenization\cite{jain2023baselinedefensesadversarialattacks}, delimiters, (ii) fine-tuning on non-instruction-tuned models \cite{piet2024jatmopromptinjectiondefense}, (iii)
        restricting tools based on user requests \cite{debenedetti2024agentdojo}, or (iv)
        pretraining LMs to enforce hierarchies or improve instruction/data separation\cite{wallace2024instructionhierarchytrainingllms, chen2024struqdefendingpromptinjection}.
\end{itemize}

Most of these techniques are heuristic based and not conservative, nor do they allow an application developer to provide a security policy specifying allowed actions given a current integrity and confidentiality environment. Compared to RTBAS, data sanitization methods are heuristics driven and are subject to adversarial jailbreaking  \cite{liu2024autodangeneratingstealthyjailbreak}; tool restrictions allow attacks using unrestricted tools; pre-training techniques are difficult to apply to commercial models, and still rely on the LM to ignore malicious prompts, albeit with greater difficulties. 

Much research\cite{carlini2021extracting, kim2024propile} has been completed on training time privacy concerns. Inference-time techniques have been focused on detecting outputs with possible PIIs\cite{jiang2023migatingvulerabilities} or desensitizing them before sending to the LM\cite{firewallm,papillon}. However, they typically are not focused on tool-based environments.

\textbf{Information Flow for LLMs} 
\cite{siddiqui2024permissiveinformationflowanalysislarge} explores the similar selective propagation approach. However,  their mechanism requires enumerating all possible subsets of relevant prior input regions (documents in RAG) to identify the minimal subset that leads to similar outputs. As noted in their paper, the naive implementation of this mechanism incurs a worst-case complexity that is exponential in the number of prior input regions, potentially reaching thousands in the RAG scenarios. Even their optimized version still requires exponential enumeration with respect to the number of levels in a lattice, resulting in 16-64 additional LM calls with a typical lattice with 4-6 levels. In contrast, as we will discuss later, our mechanism employs a dependency analyzer that efficiently detects relevant input regions in parallel by a single call, reducing the computational overhead from exponential to constant. This fundamental improvement highlights the practicality of our approach. Lastly, unlike our technique, \cite{siddiqui2024permissiveinformationflowanalysislarge} does not specify the propagation of labels beyond labeling the response, which makes it inapplicable to interactive settings such as tool-calling. There is also no mechanism to verify the computed label against allowed policy or solicit user confirmations. 


\textbf{Attention Score as a Measure of Saliency.}
\textit{Attention scores}~\cite{vaswani2017attention,wiegreffe2019attention}, which measure a transformer-based model's ``focus'' on past tokens, is a widely-used technique in the machine learning community to explain a neural network's internal processing\cite{jain2019attention,wang2023label}, prune irrelevant input texts\cite{zhang2023h2o}, etc.  In this work, we leverage attention scores as an input to the \dependencydetector, as they capture the degree to which output tokens are influenced by specific input regions.

\section{Motivation}

\subsection{Prompt Injection as an Integrity Concern} \label{subsec:prompt_inj_integrity}
Integrity in the context of TBAS ensures that the agent’s actions and outputs align faithfully with user requests and the system’s intended purpose. TBAS assists users by calling provided tools to perform actions or retrieve helpful information. Tool responses, however, can contain untrusted, or low-integrity content containing injected prompts. To maintain integrity, the system must safeguard against unauthorized modifications, especially when using integrity-sensitive tools. For instance, tools capable of spending money, sending messages on behalf of users, or performing actions with significant side effects must not execute commands originating from untrusted or compromised inputs.

Consider the following scenarios:
\begin{itemize}[noitemsep]
    \item \textbf{Website Content}: \texttt{fetch\_website}, fetches content from a website. An attacker can plant malicious text on the website, which is then returned by the tool. The tool itself remains uncompromised—it faithfully fetches the content as designed, but the attacker controls the underlying data source.
    \item \textbf{Venmo Description}: \texttt{get\_recent\_transaction} retrieves the user’s recent transactions, including their descriptions. An attacker can plant a malicious prompt in the description of a transaction (see Fig.~\ref{fig:prompt_inj_intro}), which went unnoticed at the time (e.g., here the user may not have paid attention to such a small incoming transfer nor noticed that the description extended to a second paragraph). 
\end{itemize}

However, there are genuine scenarios where low-integrity input is necessary to affect integrity-sensitive tools. 
{
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/receipe.pdf}
\end{figure}
}
For such interactions, our goal is to ensure the user is made aware of such possible security violation but to seek their confirmation as the final arbiter of whether to allow a suspicious call to achieve \textbf{task utility}.

\subsection{Tracking Confidentiality Leakage} 
\label{subsection:unintended_confidentiality}

In a TBAS, confidential data propagates in diverse fashion, making it challenging to track and analyze. Private information may be explicitly required by user instructions or implicitly utilized (e.g., using credit card details to complete a purchase). It can also be employed during intermediate reasoning steps (e.g., using a user’s preferences to recommend new products). The flow of such data can be subtly influenced by tool descriptions or system instructions (e.g., a “Book Flight” tool specifying, “Include frequent flier number when booking”), potentially in ways the user does not anticipate, even when the behavior is not inherently malicious.

Despite the variety of ways confidential data can be used, our technique ensures that its flow is always tracked conservatively. This approach guarantees that for every potential leakage to an external environment, the user is either explicitly informed and provides active confirmation, or implicitly approves the disclosure by agreeing to an established information flow policy.










\subsection{Attention Score}\label{sec:motivation-attn}

This section motivates the attention-based approach to capture the selective propagation of information in the LM. Following common practice, we use the Taylor expansion of the loss function~\cite{NEURIPS2019_2c601ad9} to calculate the \textit{attention score} (a.k.a, \textit{importance score}) for every input token, output word token pair, which is defined as
\begin{align}
    A_{i, o} &:= \mathcal{L}_{LM}(Output_{o}, Input) - \mathcal{L}_{LM}(Output_{o}, Input_{|i}) \\
    &\approx \sum_{h, l} \left| A_{h, l, i, o} \cdot \frac{\partial \mathcal{L}_{LM}(Output_o,\ Input)}{\partial A_{h, l, i, o}} \right|.
\end{align}

Here, $A_{h,l,i,o}$  is the value of the attention matrix of the $o$-th output token on the $i$-th input token of the $h$-th attention head and $l$'s network layer, $Input_{|i}$ is the input tokens with the $i$-th token masked, and $\mathcal{L}_{LM}(Output, Input)$ is the loss of the $o$-th output token on the input. The importance score captures the difference in the loss function before and after the $i$-th token is masked, and is averaged across attention heads and layers. Intuitively, attention scores measure how much "surprise" the LM receives when masking out certain tokens, where a higher attention score indicates a stronger dependency between the output and the input.

Now, we conduct case studies to demonstrate the potential of importance scores in identifying  key dependency relationships in TBAS.


\textbf{Setup.}  We obtain realistic TBAS traces in the AgentDojo Benchmark (see Tab.~\ref{tab:agentdojo} for more details), which is backed by commercial LMs. When calculating the attention scores, we format the tool calls made by the LMs into natural language and collect attention data by running open-sourced models locally on the input-output pairs. The attention score of an input region is calculated by the ratio between the maximum attention score in that region and the maximum attention score across all input tokens. 
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/attn/gpt-125m.pdf}
         \caption{TBAS backed by GPT-4o.}
         \label{fig:gpt-4o}
     \end{subfigure}
     \hfill
    \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/attn/claude-125m.pdf}
         \caption{TBAS backed by Claude.}
         \label{fig:Claude}
     \end{subfigure}
    \caption{Attention score distribution for (non-)dependent data for two models. The attention scores are obtained by the open-source OPT-125m model. The results indicate attention scores' effectiveness in capturing dependency for the LM.}
    \label{fig:attn-args}
\end{figure}

\textbf{Case Study 1: Dependency between the tool call and its arguments.} 
We investigate the dependency between tool calls made by the LM and their input arguments distributed across input tokens. We collected 3,424 input argument–tool call pairs with either a positive or negative dependency relationship labeled via pattern matching and/or semantic dependency; e.g., \texttt{book\_flight} call depends on the output of \texttt{lookup\_flight}. Figure~\ref{fig:attn-args} illustrates the distribution of attention scores obtained by the OPT-125m model~\cite{zhang2022opt} for TBAS, supported by GPT-4o and Claude~\cite{anthropic2023claude} models.

For non-dependent data, 74\% to 86\% of the attention mass is concentrated below 0.2 across both GPT-4o and Claude models. In contrast, for dependent data, only 14\% and 44\% of the attention mass falls below this threshold for GPT-4o and Claude, respectively.


\textit{TakeAway 1:} Attention score effectively capture the dependency between tool's argument and the toolcall.  

\textit{TakeAway 2:} Attention scores of small, open-sourced LM is effective in identifying the dependency of natural languages in TBAS supported by high-end, closed-source LMs.  

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/attn/UserFollowing.pdf}
         \caption{User Instr. Following}
         \label{fig:user-follow}
     \end{subfigure}
     \hfill
    \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/attn/ToolFollowing.pdf}
         \caption{Injected Instr. Following.}
         \label{fig:inj-follow}
     \end{subfigure}
    \caption{Attention score distribution of User Prompt and Tool Response for instruction following. The injected instructions are embedded in the tools' response. When prompt injection happens, the attention density shifts to the tool's response.}
    \label{fig:enter-label}
\end{figure}

\textbf{Case Study 2: Instructions following.} Next, we look at how attention scores can capture the dependency between an instruction and the LM's response to the instruction. We setup the experiment to compare the attention scores the LM pays to the user's prompt and the potentially injected tool's response across 2416 labeled data. Fig.~\ref{fig:user-follow} shows the attention distribution when there is no prompt injection and the LM follows the user's instruction. In this scenario, the LM pays combined attention to the user's prompt as well as the prior tool's response to generate the next output. 
Interestingly, when prompt injection happens and the next output of the LM follows the injected prompt, as displayed in Fig.~\ref{fig:inj-follow}, LM's attention shifts to the Tool's response by a large margin. 

\textit{TakeAway 3:} Attention scores can effectively capture the dependency between the instruction and LLM's output followed by it. 

The above case studies motivate our design of the attention-based dependency screener (\S\ref{sec:attn}). 



