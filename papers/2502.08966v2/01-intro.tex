



\section{Introduction}




Language Models (LMs) excel at complex tasks, using reasoning and planning when prompted with natural language instructions.
However, they are highly susceptible to misleading inputs, particularly {\em prompt injection} attacks, which embed malicious commands to  subvert safeguards and alter user- and vendor-expected LM behavior\cite{zou2023universalattack, liu2024formalizing}.


Meanwhile, recent advancements have led to the development of {\em Agents}--advanced applications of LMs where LMs can interact with external environments by making API calls. These systems, known as Tool-Based Agent Systems (TBAS), include products like OpenAI’s GPTs\cite{openai_gpts}. These systems allow LMs to utilize external tools to perform tasks beyond their standalone capabilities, such as summarizing emails, searching and summarizing websites, booking flights, or initiating financial transactions. 


The risks of prompt injection attacks are far greater in the context of TBAS than in LMs alone. While a LM poorly summarizing a magazine article is low stakes, a maliciously injected prompt into an agent system could trigger high-impact actions, such as unauthorized funds transfers\cite{conversationAIBANKING} or modified flight itineraries\cite{teneoAirlinesTeneos}, drastically expanding the blast radius of potential harm. 
An example illustration of a prompt injection attack is shown in Fig.~\ref{fig:prompt_inj_intro}. 

\begin{figure}
\includegraphics[width=1.15\columnwidth]{figures/prompt_injection_intro_example.pdf}
         \caption{An example prompt injection in TBAS. Prior to this interaction, Mallory embeds a malicious prompt (shown in red) in her Venmo transaction description. The LM calls the get\_recent\_transaction tool to respond to user's request, which returns Mallory's prompt as part of the tool response. The LM reacts to the prompt and sends Mallory \$100.}
\label{fig:prompt_inj_intro}
\vspace{-0.1in}
\end{figure}


Risks to user confidentiality are significant in the context of TBAS. Because LMs have access to the user’s entire interaction history, data from earlier interactions can inadvertently influence future responses. Ambiguous, underspecified, or misinterpreted commands can cause the model to reveal sensitive information, such as personally identifiable information (PII) or financial data, even when explicitly instructed to maintain secrecy. Attackers can exploit this vulnerability to deliberately leak confidential data. 


The risk of attacks on TBAS is so pronounced that the Open Worldwide Application Security Project has recognized Prompt Injection and Sensitive Information Disclosure as the top two security threats in its TOP-10 list for LM-integrated applications\cite{owasp2025}.


Existing approaches to protecting integrity and confidentiality in TBAS face significant limitations and can inadvertently undermine their own effectiveness. For example, OpenAI requires users to confirm every tool call in their commercial TBAS GPTs. While this provides a safeguard, the constant prompts throughout the execution of complex tasks with multiple tool calls can lead to user fatigue. This fatigue increases the likelihood of users mindlessly approving problematic requests or abandoning the system entirely, underscoring the need for more efficient and practical solutions.


Our goal is to develop a flexible system that automatically detects and executes all tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. In such cases, users can weigh the task utility against potential risks. %






         

To achieve this goal, we adapt traditional information flow control (IFC)\cite{IFCDenning76} to the unique challenges presented by TBAS. 
Dynamic taint tracking\cite{Newsome2005DynamicTA} offers a fine-grained method for IFC that associates security metadata with variables, updates labels based on data and control flow dependencies, and enforces security policies. However, this approach is designed for traditional software with structured source code, where dependencies can be explicitly instrumented. 

Controlling information flow in TBAS, in contrast, is uniquely challenging. Unlike traditional software, where source code provides a well-understood representation of how data flows through a program, TBAS operate in dynamic and opaque environments. These environments are dynamic because interactions occur in real-time, driven by unpredictable natural language inputs and responses to tool calls. They are opaque because the relationships between input data, the LM's internal processing, and its resulting tool calls are implicit, complex, and not directly observable or codified--making dependency tracking far from straightforward and fundamentally different from traditional source code-based techniques. Every piece of data in the LM's history could theoretically influence its next tool call, exacerbating the label creep problem common in traditional IFC\cite{languagebasedinfoflow}, where any tainted (e.g., low-integrity or confidential) data propagates unnecessarily through the entire history. This unrestricted propagation disrupts task execution by flagging benign tool calls unnecessarily and overburdening users with constant confirmations.











To address these challenges, \textbf{we introduce \textit{\sysnamelong (\sysname)}, an information flow-based framework that \textit{selectively} propagates security metadata using \textit{dependency screening}. We present two novel screeners for identifying which regions are relevant for generating the next response or tool call. Irrelevant regions are masked--redacted from the history--preventing unnecessary taint propagation without degrading the LM's functionality.}

This approach leverages two key observations about LMs:

\begin{enumerate}
    \item \textbf{Selective History Dependency:} While LMs process their entire history, responses are typically influenced by only a subset of the history. Masking irrelevant regions helps prevent unnecessary data from creeping into the LM's decisions.
    \item \textbf{Missing Data Resilience:} LMs are robust to missing/in\-complete data, allowing irrelevant regions to be masked without significantly impacting task performance.
\end{enumerate}

We propose two complementary approaches for dependency screening:

\begin{itemize}
    \item \textbf{LM-Judge Screening:} This method uses a secondary LM, called a \textit{LM-Judge}, to evaluate the history and identify which regions are critical for the current tool call or response. By explicitly prompting the LM-judge to reason about dependencies, this approach offers flexibility and task-specific adaptability.
    \item \textbf{Attention-Based Screening:} This approach involves training a neural network to quantify how different regions of the context influence tool calls or responses. Higher \textit{attention scores} indicate stronger dependencies, providing a data-driven method to identify relevant regions. 
\end{itemize}

Both approaches allow the system to propagate security metadata selectively, ensuring that low-integrity or confidential data is appropriately handled while minimizing unnecessary tainting. 

We make the following key contributions:

\begin{itemize}
    \item \textbf{\sysname:} A novel framework for defending against prompt injection and sensitive information disclosure in TBAS, based on adapting IFC to the unique challenges of TBAS using dependency screening and selective region masking.
    \item \textbf{Two Novel Screening Approaches:} We propose both LM-Judge and Attention-Based screeners, offering complementary strategies for analyzing dependencies in TBAS.
    \item \textbf{Comprehensive Evaluation:} We evaluate \sysname and its screening approaches on the AgentDojo benchmark\cite{debenedetti2024agentdojo}, which simulates prompt injection attacks on real-world TBAS tasks across domains like banking, travel, and messaging. Our system prevents 100\% of attacks violating security policies with minimal impact on task utility (<2\% degradation), outperforming state-of-the-art (SOTA) defenses. We also create an Accidental Leakage benchmark for evaluating confidentiality protection in TBAS tasks across three domains. 
    In evaluation, \sysname outperforms SOTA defenses by (i) detecting and executing without user confirmation the same set of tool calls as the oracle for all but one task, while matching the oracle's confidentiality protection, and (ii) maximizing overall task utility relative to SOTA defenses, even those requiring 100\% user confirmation.
\end{itemize}
































