
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

\usepackage{tikz}
\usepackage{amsmath}

\usepackage{filecontents}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx} %

\usepackage{amssymb}%
\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage{xspace}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{authblk}
\newcommand{\Siyuan}[1]{\textcolor{cyan}{Siyuan: #1}}
\newcommand{\Peter}[1]{\textcolor{green}{Peter: #1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\renewcommand{\todo}[1]{}

\newcommand{\dependencydetector}{dependency screener\xspace}
\newcommand{\Dependencydetector}{Dependency screener\xspace}
\newcommand{\sysnamelong}{Robust TBAS\xspace}
\newcommand{\sysname}{RTBAS\xspace}

\usepackage{enumitem}
\setlist{nolistsep}
\setenumerate{itemsep=3pt,topsep=3pt,leftmargin=*,labelindent=0pt}
\setitemize{itemsep=3pt,topsep=3pt,leftmargin=*,labelindent=0pt}

\usepackage{titlesec}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}


\begin{document}



\title{\Large \bf \sysname: Defending 
LLM Agents Against Prompt Injection and Privacy Leakage}


\author[1]{Peter Yong Zhong\textsuperscript{*}}
\author[1]{Siyuan Chen\textsuperscript{*}}
\author[1]{Ruiqi Wang}
\author[1]{McKenna McCall}
\author[1]{Ben L. Titzer}
\author[1, 2]{Heather Miller}
\author[1]{Phillip B. Gibbons}
\affil[1]{Carnegie Mellon University}
\affil[2]{Two Sigma Investments}



\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Co-first author.}

\begin{abstract}


Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. 

Existing defenses (OpenAI GPTs) require user confirmation before \textit{every} tool call, placing onerous burdens on users.
We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured.
RTBAS adapts Information Flow Control to the unique challenges presented by TBAS.
We present two novel \textit{dependency screeners}--using LM-as-a-judge and attention-based saliency--to overcome these challenges. 
Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2\% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.

\end{abstract}




\input{01-intro}
\input{02-background}
\input{03-attack}
\input{04-Approach}
\input{05-Evaluation}
\input{05-b-Discussion}
\input{06-Conclusion}
\input{07-Ethics}



\section*{Acknowledgments}

This work supported in part by the Parallel Data Lab, the WebAssembly Research Center and Cylab at Carnegie Mellon University, and the National Science Foundation under grant 2211882. Thanks to Harrison Grodin for important discussions on the presentation of our mechanism. We also thank Noah Singer and Christos Laspias for their valuable feedback. 


\bibliographystyle{plain}
\input{usenix2019_v3.1.bbl}

\appendix
\input{appendix}
\end{document}
