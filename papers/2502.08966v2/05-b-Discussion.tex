\section{Discussion} \label{sec:discussion}

\textbf{Labeling.} One limitation of our technique, common in IFC research, is the need for labeled tool and user messages, along with an information-flow policy understandable to users. However, many applications naturally support region-based labeling, particularly when addressing prompt injection and confidential data leakage. For example, in a \texttt{get\_email} response, fields like \texttt{Subject} and \texttt{Content} could be labeled low-integrity due to susceptibility to natural language injections, while \texttt{Sender} might be high-integrity due to strict schema requirements. Similarly, tools may handle sensitive information; for instance, in a finance application, a \texttt{get\_account\_balance} response could be marked high-confidentiality to prevent accidental or malicious leakage. Recent works on using LLMs for formal safeguards \cite{barth2024automated_reasoning} and privacy policy interpretation \cite{chen2024clearcontextualllmempoweredprivacy, tang2023policygptautomatedanalysisprivacy} offer promising directions to bridge understanding gaps.

\textbf{Cost.} Operating both of our \dependencydetector methods is currently resource-intensive. The attention-based screener requires the agent to generate a preliminary message to analyze attention scores between regions, followed by a second message based on different input during the selective masking process. A potential optimization involves using a smaller model to generate the preliminary message, as it is not part of the agentâ€™s final output.  Smaller models could also benefit the LM-Judge Screener. While early preliminary experiments suggest that small, local models struggle as general-purpose screeners, fine-tuning or prompt-tuning \cite{opsahlong2024optimizinginstructionsdemonstrationsmultistage} on task-specific datasets may enhance their performance. This approach could improve efficiency without compromising effectiveness.
