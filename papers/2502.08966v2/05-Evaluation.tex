

\section{Evaluation}
In this section, we benchmark our techniques in addressing the security threats for TBAS, that is, prompt injection and privacy leakage. We aim to answer the following questions:  

\begin{itemize}[noitemsep]
    \item[\textbf{Q1}:] Under scenarios with prompt injections, how well does our system maintain integrity and utility compared to state-of-the-art defenses? 
    \item[ \textbf{Q2}:] Under scenarios with privacy leakage, how much excessive user confirmations do we burden the user and whether utility is degraded compared to baselines?
    \item[\textbf{Q3}:] How accurate is our detector in determining the information flow within the LM and what is its runtime overhead? 
\end{itemize}


\subsection{End to End Evaluation: Prompt Injection}

\subsubsection{Setup} 

\noindent\textbf{Test Suites.} We benchmark our system on AgentDojo~\cite{debenedetti2024agentdojo}, a state-of-the-art benchmark on agent adversarial robustness against prompt injection attacks. Shown in Tab.~\ref{tab:agentdojo}, the dataset consists of 79 realistic user tasks in four suites: banking, travel, workspace, and slack. Every test suite represents a TBAS application where LLM serves user's request using a given set of tools, e.g. \texttt{send\_money} for the banking suite and \texttt{reserve\_restaurant} for the travel suite. Every test case in a suite requires the LLM to solve a task with multi-round interaction with external tools such as booking a restaurant after filtering through reviews and datary restrictions.

\noindent\textbf{Data Labeling.} To integrate the information flow mechanism, we enhance the task suites by assigning integrity labels based on the application's requirements while remaining agnostic to specific test cases (examples are shown in Tab.~\ref{tab:agentdojo}). The labeling process follows these key principles to satisfy the assumptions we denote on the tool environment in \ref{subsec:robust_tbas_assumptions}:
\begin{itemize}[noitemsep]
    \item Regions in a tool responses that incorporates textual data from external sources is labeled as low-integrity.
    \item Tools with significant side-effects (e.g., sending money) or those can introduce high-integrity data to the external environments (e.g. sending messages) are labeled as high-integrity.
\end{itemize} 


\noindent\textbf{Prompt Injection Attacks.} To emulate prompt injection attacks, each test suite includes a set of injection tasks. These tasks aim to induce the agent to misuse tools and produce harmful side effects, such as making unintended reservations on behalf of the user or leaking user's private data through public channels like emails. When evaluating the benchmark under Prompt Injection attacks, each user task is tested against every injection task in the corresponding test suite, resulting in a total of 629 security test cases.

\noindent\textbf{Baselines.} We evaluate the effectiveness of our mechanism against state-of-the-art prompt injection defenses, as well as two baseline approaches: 
\begin{itemize}[noitemsep]
    \item \textbf{Tool Filter} by AgentDojo: Use the LM as a Judge to filter the set of legal tools that an LM is allowed to use based on the user task.
    \item \textbf{Näive Tainting}: A baseline tainting approach where we assume every region in history affects the next message and needed to be tainted accordingly.
    \item \textbf{Redact All}: A baseline approach where we redact every single region that is not of high-integrity and public and therefore no labels are propagated.
\end{itemize}
\textbf{PI Detector} by \cite{protectAIdetector}, \textbf{Delimiting} by \cite{hinesdefend} and \textbf{Prompt Sandwiching} by \cite{learning_prompt_sandwich_url} were evaluated by AgentDojo \cite{debenedetti2024agentdojo}. \textbf{PI Detector} and \textbf{Delimitting} performed strictly worse than \textbf{Tool Filter}.  \textbf{Prompt Sandwiching} performed better without attack in utility, but suffered a 27\% attack success rate. We do not include these results since we consider \textbf{Tool Filter} the existing SOTA.

\noindent\textbf{Evaluation Metrics.} We follow AgentDojo to use utility and integrity (a.k.a. security in AgentDojo) as two evaluation metrics to compare different defenses, where
\begin{itemize}[noitemsep]
    \item \textbf{Utility} determines whether the agent has solved the task correctly, by inspecting the model output and the mutations in the environment state.
    \item \textbf{Integrity} determines whether the attacker succeeds in their attacks against the system. 
\end{itemize}









\begin{table*}[]
\centering
\caption{Overview of the Prompt Injection Benchmark}
\label{tab:agentdojo}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lllllll@{}}
\toprule
Task Suite & \# User Task & \#Test Case & Number Tools & Number Messages Per Test Case & Example Labelled Low-Integrity Data        & Example High Integrity Tool Calls                                                   \\ \midrule
Banking    & 16           & 144         & 11           & 8.9 +- 3.0                    & External Bills, External Transaction Notes & \texttt{update\_transactions}, \texttt{send\_money} \\
Travel     & 20           & 140         & 28           & 13.6 +- 3.8                   & Hotel Reviews, Restaurant reviews.         & \texttt{send\_email}, \texttt{book\_hotel}        \\
Slack      & 21           & 105         & 11           & 15.6 +- 4.4                   & External Channel messages, Web Contents.   & \texttt{add\_new\_user}                                            \\
Workspace  & 40           & 240         & 24           & 8.7 +- 3.4                    & External Documents in a Cloud Drive        & \texttt{update\_calendar}                                            \\ \bottomrule
\end{tabular}
}
\end{table*}

We evaluate this benchmark suite using GPT-4o, consistent with results from AgentDojo. The Prompt Engineering detector is also implemented using this model. For the Attention-Based detector, which requires access to a LM’s internal weights to compute cross-token attention scores, we use the Phi-3-Mini-128K\cite{abdin2024phi3} instruction-tuned model. However, inference steps are still performed using GPT-4o. 


In this benchmark, we do not model user confirmations. Instead, any apparent unauthorized calls contrary to the information-flow policy are skipped and unperformed.













\subsubsection{Results and Analysis}

We present the results of the AgentDojo dataset both with (Figure \ref{fig:three graphs}) and without (Figure \ref{fig:agentdojoNoAttack}) prompt injection attacks. A cost comparison of running our techniques as a measure of overhead (Table \ref{tab:runtime}) is also provided. 

Importantly, the lack of user confirmations and the subsequent rejection of all apparent suspicious tool calls means that if we are able to seek user confirmations for calls that inheritently depend on low integrity data or in the case of over-tainting, then we are likely to achieve even better performance.

\noindent\textbf{Without Attacks.}
We present the results without attack in Fig \ref{fig:agentdojoNoAttack}. The impact on utility is best illustrated by the difference between the baseline case (no defense) and our techniques. Specifically, we observe a 10\% and 7.4\% degradation in utility for the LM-Judge and Attention-based detectors, respectively. Interestingly, the Tool Filter technique slightly increases utility in the absence of attacks. We speculate that this improvement arises from an implicit planning step, where irrelevant tools are excluded from LLM consideration.  

Our approach performs particularly well in the travel and workspace suites. As illustrated in the results in these suites, our approaches consistently achieve the highest utility among other methods, even exceeding the baseline(no defense) case by $5\%$ on average. These task suites naturally align with a more fine-grained integrity lattice and precise security policy, reducing scenarios that require manual user intervention.  

The Slack dataset, however, emerges as an outlier for our techniques. While our approaches still outperform naive tainting and redact-everything methods, the utility drops to $33\%$ and $22\%$ for the Attention-based and LM-Judge detectors respectively, which is more than halved compared to the mean utility. This performance drop can be attributed to the nature of Slack tasks, which often involve variations in reading content from untrusted websites and performing actions based on that content. We consider such tasks inherently unsafe, necessitating user confirmation. 

\begin{figure}[H]
\includegraphics[width=\columnwidth]{figures/dora_example.pdf}
\label{fig:dora}
\end{figure}
Additionally, some tasks require the agent to send a summary of an untrusted website to a high-integrity source (e.g., posting the summary to a Slack channel). If left unchecked, such actions could compromise the high-integrity source by spreading prompt injection attacks like a virus or conveying unintended statements. 



\noindent\textbf{Under Attack.}
We present the result when under prompt injection attack in Fig \ref{fig:three graphs}. RQ1: Our techniques still retain a high utility compared to the baseline without defense,  only losing less than 1\% utility for the LM-judge screener and 3\% for the Attention-based screener. 

We note that we \textbf{do} prevent 100\% of attacks that violate our security policy. However, in the workspace benchmark, there was one test case where text written by the user, labeled as high-integrity, contained possible prompt injection and is thus not tracked. This illustrates a major limitation for our mechanism, as with any other IFC techniques, that the security guarantees provided are only as good as the labels provided and the policies enforced.

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pi_w_attack/banking.pdf}
         \caption{Banking.}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pi_w_attack/slack.pdf}
         \caption{Slack.}
         \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pi_w_attack/workspace.pdf}
         \caption{Workspace.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pi_w_attack/travel.pdf}
         \caption{Travel.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pi_w_attack/mean-attack.pdf}
         \caption{Weighted Average.}
     \end{subfigure}
        \caption{End-to-end evaluation on Security-Utility trade-off for Prompt Injection. The Top Right Corner indicates that high success rate of the user's task and high integrity of the defense against prompt injection across test cases.}
        \label{fig:three graphs}
\end{figure*}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/no_attack.pdf}
    \caption{The Utility Rate comparison for the Prompt Injection Benchmark without attack. }
    \label{fig:agentdojoNoAttack}
\end{figure}

\begin{figure}[ht!]
     \centering
     \includegraphics[width=\linewidth]{figures/privacy.pdf}
     \caption{The Utility Comparison for the privacy leakage benchmark. The solid bars represent the utility achieved when users block tool calls upon receiving confirmation requests from the defenses. The faint bars indicate the additional utility users can gain by allowing these tool calls. The results demonstrate that our approaches provide a near-optimal balance, offering users flexibility to achieve varying utility levels based on their confirmation choices, unlike GPTs, which require confirmation every time.}
     \label{fig:utilityLeakage}
 \end{figure}



\subsection{End-to-End Evaluation: Privacy Leakage}

This experiment evaluates different defenses against the privacy leakage threat, e.g. accidental reference to chat history, silently booking a restaurant without user's confirmation. For every tool call the LLM makes, the defense mechanisms decide whether to flag the user for confirmation or proceed silently by masking out the private data. An ideal defense should effectively balance the \textbf{transparency} by asking the user for confirmation whenever privacy leakage occurs, and provide a smooth \textbf{user experience} by avoiding unnecessary confirmations when the tool call is independent of private data. 


\begin{table}
\caption{Overall false positive rates and false negative rates, for the Accidental Leakage benchmark.} 
\label{tab:accLeakage}
\vspace{-0.15in}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrr}
\toprule
 & FPR & FNR \\
\midrule
Confirm Never - redact-all & 0 & 0.513514 \\
Confirm Every Time (GPTs) & 0.297297 & 0 \\
RTBAS (LM-judge) & 0.081081 & 0.108108 \\
RTBAS (Attention) & 0.162162 & 0.108108 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\begin{table}[h!]
\centering

\caption{Benchmark for Privacy Leakage}
\resizebox{\linewidth}{!}{
\begin{tabular}{p{2.5cm}|p{4.5cm}|p{4.5cm}}

\textbf{Task Suite} & \textbf{Description} & \textbf{Sensitive Data} \\ \hline
Venmo \newline(12 tasks) & Managing transactions, friend interactions, and account updates. & Transaction details, user info (balance, password), friend lists/info \\ \hline
Flight Booking \newline (12 tasks) & Searching, booking, and updating flights. & Credit card, passport number, user address, booked itinerary \\ \hline
Amazon \newline (13 tasks) & Buying, returning, recommendation of products. Promotions & Credit card, address, past orders, preferences, gender  \\ 
\end{tabular}
}
\label{tab:privacy_benchmark_descrpt}
\end{table}



\noindent\textbf{Synthesized Benchmark.} We are not aware of existing comprehensive benchmark for privacy leakage for TBAS. We manually created 37 test cases across three TBASs in different domains: shopping, finance, and flight booking. We provide a short description of the task suite in table \ref{tab:privacy_benchmark_descrpt}. Each task suite simulates a specific TBAS setup, featuring the same tools, descriptions, and system prompt, to represent a user-facing application. Tools capable of contributing private information to the context are annotated with regions identifying where private data appear in their outputs, along with labels specifying the nature of the private information. 

Each task begins with prior interactions between the user and the agent (i.e., the context window), which may already contain marked private data. This is followed by a user message that outlines the task to be completed. To achieve the task, the LLM may call tools to retrieve information, perform actions with external side effects, and report back to the user with the results.

Tasks vary in complexity. Some require a single reasoning step, such as directly calling a tool or answering a query based on the context. Others involve more intricate reasoning, requiring sequential calls to multiple tools to complete the task. Analyzing private information propagation in complex, multistep tasks is particularly valuable, as these scenarios provide more opportunities to observe indirect propagation of private information. Each tool call should propagate only the relevant information from the context, enabling a detailed and fine-grained evaluation of our approach.

As illustrated in \S\ref{subsection:unintended_confidentiality}, propagation of privacy information can occur in subtle ways. We include the diverse propagation patterns explored in \S\ref{subsection:unintended_confidentiality} as part of this benchmark to evaluate the effectiveness of our \dependencydetector.

We keep in mind the following principles when creating the dataset: %
\begin{itemize}
    \item Every test case has a ground truth tool calling to obtain the utility.
    \item Every test case whose utility does not depend on the private data will see private data in the ground truth tool calling chain.
\end{itemize}






\noindent\textbf{Evaluation Metrics.} Upon evaluation, each tool call made by an agent is manually labeled either as requiring confirmation (leaking private data) or not based on the natural understanding of the tool calling.  
Based on the oracle labels, we consider the following metrics for the benchmark: 
\begin{itemize}
    \item \textit{False Positive Rate} (FPR) measures the proportion of test cases in which the defense mechanism fails to detect a call to a tool that involves privacy leakage,
    \item \textit{False Negative Rate} (FNR) measures the proportion of test cases in which the defense mechanism incorrectly identifies a tool call as leaking private data,
    \item \textit{Utility} that measures the proportion of test cases that the user's task succeeds. Degradation to utility can result from erroneous masking.
\end{itemize}


\noindent\textbf{Approaches Compared.} We compare the following approaches: 
\begin{itemize}
    \item \textbf{Confirm Never - Redact All} redacts all private data upon information propagation. No confirmation necessary since no private information will ever be seen by the agent.
    \item \textbf{Confirm Every Time (GPTs)} assumes every tool call may leak private information and thus always requires confirmation.
    \item \textbf{Selective Propagation} selectively propagates information with the dependency screener. We include two instantiations (LM-Judge based and Attention based) for comparison. 
    \item \textbf{Oracle} represents a human expert that acts as the LM to perform tool calls and decide whether to confirm with the user. 
\end{itemize}



\noindent\textbf{Result and Analysis.}
Table~\ref{tab:accLeakage} shows the trade-off between the false negative rate (FNR) and the false positive rate (FPR) across the synthesized test suites. 

For the baselines, the Confirm Never redacts every private region, hence it will proceed silently by masking out the private data even when it is valid for a tool call to leak private information, e.g. booking a flight with credit card number, resulting in 51\% $FNR$ and severe utility loss. On the other hand, the Confirm Every Time (GPTs) defense taints the tool call as long as there is any private data in the context, resulting in 30\% $FPR$ and redundant user confirmations.

\textbf{Compared to the baselines, our selective propagation defenses effectively tames the trade-off between transparency and user experience}. Compared to the Confirm Never, the LM-Judge-based selective propagation delivers higher transparency to the user by reducing the $FNR$ from 30.7\% to 7.6\% for the Amazon Test Suite, from 58.3\% to 8.3\% for the Flight Booking test case, and from 66.7\% to 16.6\%. 

In contrast, compared to GPTs that require user confirmation for every tool call, our information flow-based defenses significantly reduce unnecessary confirmations. Specifically, the LM-Judge approach and the attention-based approach achieve an FPR of 8.1\% and 16.2\% across all test suites, respectively, whereas GPTs exhibit FPRs of 29\%. In practical terms, a smaller FPR translates into a significantly improved user experience, requiring minimal interaction from the user. This reduction in unnecessary confirmations is particularly crucial for maintaining a seamless and efficient workflow.

Next, we explore the utility results achieved by different approaches. Shown in Fig.~\ref{fig:utilityLeakage}, the solid bars show the success rate of the user tasks when the user blocks every tool call upon confirmation.
The faint bars show the additional utility the user can gain by allowing tool calls.
Confirm Never and GPTs baseline represents two extremes. On one side, Confirm Never does not provide the user with any autonomy in deciding whether a tool call should proceed, resulting in overall 35\% of utility. On the other side, the Confirm Every Time (GPTs) defense prompts the user for confirmation upon every tool call, with zero utility in the worst case and 91\% utility in the best case.



\textbf{Across the two extremes, our selective propagation approaches are able to balance the utility and number of times we seek user confirmation. }Compared to GPTs confirming every time, our approaches obtain the baseline utility of 40\% and 43\% for the LM-Judge and attention-based approach, respectively. That is, our approaches saves the user from from the need to confirm for test cases in which no private data is required for the task to succeed. For example, a large portion of the amazon test suite is confirmation-free services like product recommendation, product searching, etc., our approaches passes 53\% test cases without confirmations. In fact, compared to the oracle, we are losing utility only in 1 out of 15 test cases, because of the overtainting booking history for the current flight lookup. 




Compared to Confirm Never approach, our approach offers users the flexibility to proceed with the task by allowing potentially risky tool calls with the user's permission. This is especially critical in applications like Venmo, where sensitive data and financial activities are always involved. In our evaluation, we are able to achieve 83\% and 75\% of utility when the user allows every tool call, which is the same as GPT (83\%). 






\subsection{Analysis}


\subsubsection{Taint Tracking Accuracy}

We augmented the Privacy Leakage benchmark with precise labels that represents the sets of private information category involved. We evaluate, for every tool calls, how often these labels matches exactly the ground truth label we annotated(Q3).

The user, through this label, can gather more information about the category of data that the tool call purports to leak. A user comfortable with leaking their credit card number to book a flight may be hesitant to share her social security number. 

A mislabeled tool call with more private data categories than actually propagated could be erroneously rejected either interactively or by reference to the policy that the user agrees to prior. Oppositely, a label claiming less private data categories can distort the task, with actually relevant data masked. 
\begin{table}[h]
\centering
\label{tab:percentages}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccc}
\toprule
 Confirm Never (Redact All) & Confirmation Always (GPTs) & RTBAS (LLM Judge) & RTBAS (LLM Judge) \\
\midrule
 22.3\% & 56.7\% & 57.3\% & 70.0\% \\
\bottomrule
\end{tabular}%
}
\end{table}




We show that the selective propagation approach, when instantiated by either the prompting or the attention approach arrives at the exact ground truth label more than 70\% and 57\% of the time, respectively. This is superior to our baseline techniques for redacting all sensitive regions and thus propagate nothing or the always confirm method where we assume a tool call always leak every secret. 

\subsubsection{Dependency Screener Comparisons}

For the Prompt Injection and Privacy Leakage benchmarks, we find that the LM judge and the Attention-based \dependencydetector perform similarly across the benchmarks, with LM Judge performing slightly better overall under attack for Prompt Injection and much better in terms of its detection accuracies for privacy leakage(Tab ~\ref{tab:accLeakage}). We conject the LM judge's ability to explicitly reason about the dependencies and output its chain-of-thought \cite{wei2023chainofthoughtpromptingelicitsreasoning} could help generalize the mechanism across unseen task, and for more subtle propagation cases. However, across end-to-end benchmarks, both methods perform similarly with respect to the overall task utilities. This suggests that the Attention-based approach can detect important dependencies crucial to task success, but can possibly miss more subtle dependencies that may still influence task outcomes.




\subsubsection{Runtime Overhead}
Q3: Our techniques incur higher costs compared to existing methods, primarily due to the overhead introduced by the detectors. The Attention-based detector requires the LLM to run twice: the first run generates a preliminary message, which is used for the attention mechanism to compute dependency results. The second run generates the final output after masking. The LM Judge screener also incur computer overhead by running the judge LLM before the agent generates each new message. In contrast, the tool detector only runs one additional inference for each user message but not between tool calls, and the Prompt Sandwiching approach only marginally increases the number of tokens by repating the user requests. We discuss opportunities for optimization in Sec. \ref{sec:discussion}.
\begin{table}
\caption{Runtime comparison of executing the user tasks on the banking suite of AgentDojo. The metrics are averaged across test cases. The price is calculated against OpenAI's pricing. }
\label{tab:runtime}
\resizebox{\linewidth}{!}{
\begin{tabular}{llrrr}
\toprule
 baseline & price (\$) & time (s) & \#Tokens \\
\midrule
 Vanilla & 0.014712 & 4.369265 & 2709.937500 \\
 Tool Filter (AgentDojo) & 0.008653 & 4.880799 & 1504.625000 \\
 RTBAS (Attn) & 0.027531 & 8.728362 & 5048.687500 \\
 RTBAS (LLM Judge) & 0.031672 & 9.707550 & 5851.562500 \\
\bottomrule
\end{tabular}
}
\end{table}




























 







