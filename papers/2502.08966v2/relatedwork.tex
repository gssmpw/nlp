\section{Background and Related Work}
\textbf{Agentic AI Systems and Tool-Based AI Agents.}
The integration of external environments with LMs is often described as \textit{Composite AI Systems} \cite{compositeAI} or simply \textit{agents}\cite{llm_powered_agents,guide_to_llm_abst}. These systems leverage the LM’s capabilities to comprehend natural language \cite{ouyang2022instruction}, perform reasoning \cite{wei2023chainofthoughtpromptingelicitsreasoning,zelikman2024quietstarlanguagemodelsteach, renze2024selfreflectionllmagentseffects} and planning \cite{huang2022languagemodelszeroshotplanners, wang2023planandsolvepromptingimprovingzeroshot, masterman2024landscapeemergingaiagent}. Tool-Based Agent Systems (TBAS) \cite{ReAct}, a subclass of LM agentic systems, operate in a single context and interact with external environments via tool calls. Widely adopted in applications like Bing’s ChatGPT integration \cite{microsoft_AI_bing_chatgpt}, TBAS also power platforms like OpenAI’s GPTs \cite{openai_gpts} and CustomGPT.ai \cite{customgpt_ai}, enabling developers to customize agents with specific instructions and tools.

\textbf{Prompt Injection For LMs.}
A prompt injection attack\cite{liu2024formalizing, liu2024promptinjectionattackllmintegrated} occurs when malicious inputs, or prompts, are introduced into the agent’s history (context) to alter its behavior. Prompt injections, often as natural language instruction pretending to be the user, but at times it could be nonsensical text making its detection even more subtle\cite{zou2023universalattack}. While users can initiate such attacks to bypass application-defined guidelines \cite{liu2024autodangeneratingstealthyjailbreak} or extract system prompts \cite{yang2024prsapromptstealingattacks}, our focus is on prompt injections originating from tools that retrieve data from untrusted sources such as other websites, public reviews, comments, etc. \cite{debenedetti2024agentdojo,zhan2024injecagentbenchmarkingindirectprompt}. These injections can maliciously manipulate the TBAS, causing it to perform unintended or harmful tasks. 

\textbf{Defenses for Prompt Injection and Privacy Leakage.} Defenses can be categorized into two strategies:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textit{Injected Prompt Detection:} Possible prompt injections can be identified using perplexity measures or another LM trained to flag anomalies \cite{protectAIdetector, rahman2024finetunedlargelanguagemodels,hung2024attentiontrackerdetectingprompt}.
    \item \textit{Prompt Impact Mitigation:} These limit injection effects using (i) data sanitization approaches such as parapharsing\cite{jain2023baselinedefensesadversarialattacks}, retokenization\cite{jain2023baselinedefensesadversarialattacks}, delimiters, (ii) fine-tuning on non-instruction-tuned models \cite{piet2024jatmopromptinjectiondefense}, (iii)
        restricting tools based on user requests \cite{debenedetti2024agentdojo}, or (iv)
        pretraining LMs to enforce hierarchies or improve instruction/data separation\cite{wallace2024instructionhierarchytrainingllms, chen2024struqdefendingpromptinjection}.
\end{itemize}

Most of these techniques are heuristic based and not conservative, nor do they allow an application developer to provide a security policy specifying allowed actions given a current integrity and confidentiality environment. Compared to RTBAS, data sanitization methods are heuristics driven and are subject to adversarial jailbreaking  \cite{liu2024autodangeneratingstealthyjailbreak}; tool restrictions allow attacks using unrestricted tools; pre-training techniques are difficult to apply to commercial models, and still rely on the LM to ignore malicious prompts, albeit with greater difficulties. 

Much research\cite{carlini2021extracting, kim2024propile} has been completed on training time privacy concerns. Inference-time techniques have been focused on detecting outputs with possible PIIs\cite{jiang2023migatingvulerabilities} or desensitizing them before sending to the LM\cite{firewallm,papillon}. However, they typically are not focused on tool-based environments.

\textbf{Information Flow for LLMs} 
\cite{siddiqui2024permissiveinformationflowanalysislarge} explores the similar selective propagation approach. However,  their mechanism requires enumerating all possible subsets of relevant prior input regions (documents in RAG) to identify the minimal subset that leads to similar outputs. As noted in their paper, the naive implementation of this mechanism incurs a worst-case complexity that is exponential in the number of prior input regions, potentially reaching thousands in the RAG scenarios. Even their optimized version still requires exponential enumeration with respect to the number of levels in a lattice, resulting in 16-64 additional LM calls with a typical lattice with 4-6 levels. In contrast, as we will discuss later, our mechanism employs a dependency analyzer that efficiently detects relevant input regions in parallel by a single call, reducing the computational overhead from exponential to constant. This fundamental improvement highlights the practicality of our approach. Lastly, unlike our technique, \cite{siddiqui2024permissiveinformationflowanalysislarge} does not specify the propagation of labels beyond labeling the response, which makes it inapplicable to interactive settings such as tool-calling. There is also no mechanism to verify the computed label against allowed policy or solicit user confirmations. 


\textbf{Attention Score as a Measure of Saliency.}
\textit{Attention scores}~\cite{vaswani2017attention,wiegreffe2019attention}, which measure a transformer-based model's ``focus'' on past tokens, is a widely-used technique in the machine learning community to explain a neural network's internal processing\cite{jain2019attention,wang2023label}, prune irrelevant input texts\cite{zhang2023h2o}, etc.  In this work, we leverage attention scores as an input to the \dependencydetector, as they capture the degree to which output tokens are influenced by specific input regions.