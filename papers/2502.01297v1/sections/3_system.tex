\section{System Description}
\subsection{System Overview}
    Our system follows a standard visual inertial odometry framework akin to MSCKF\cite{wu2015square}. It takes inputs of images and IMU data and outputs 6DoF poses. As illustrated in \cref{fig:teaser}, the system comprises two pivotal modules: initialization and feature matching, both fundamental components in most VIO systems.
    We will provide detailed introductions to these modules. Part of initialization process is elaborated in \cref{sec:Initialization}, and part of feature matching is discussed in \cref{sec:Hybrid-Feature-Tracking}.
    
%% copy from RNIN-VIO as baseline %%
\subsection{Notation}
Here, we will define some notations and definitions that will be used consistently throughout the paper.
The world frame is denoted by $(\cdot)^w$, the camera frame by $(\cdot)^c$, and the body or IMU frame by $(\cdot)^I$. Rotation is represented using both rotation matrices, denoted by $\textbf{R}$, and Hamilton quaternions, denoted by $\textbf{q}$, with
$\otimes$ symbolizing quaternion multiplication. The gravity vector in the world frame is represented as
${\textbf{g}}^w = [0, 0, g]^T$, aligning with the z-axis of the world frame.
Lastly, $\hat{(\cdot)}$ signifies the noisy measurement or estimate of a certain quantity.

\subsection{State Definition}
  Similar to typical Extended Kalman Filter (EKF)-based VIO systems, at frame $k$, we define the system's state vector as follows:
  \begin{equation}
    {\textbf{S}_k} = [{\textbf{S}}_{I_{k-(n-1)}}, ... , {\textbf{S}}_{I_k}, {\textbf{S}}_{E_k}, {\textbf{S}}_{t}],
  \end{equation}
  for $i = k-(n-1), ...,k$, where ${S}_{I_i}$ denotes the state vector of $n$ cloned IMU poses at frame $i$. Each cloned IMU pose includes an orientation ${\textbf{q}}^w_{I_i}$ and a position   ${\textbf{p}}^w_{I_i}$. The term $\textbf{S}_{E_k}$ denotes the extra part of state on frame $k$, which consists of biases of gyroscope $\textbf{b}_{g_k}$ and accelerometer ${\textbf{b}}_{a_k}$, and also the velocity ${\textbf{v}}^w_{I_k}$ in the world frame. The term of ${\textbf{S}}_{t}$ represents time-related calibration parameters, including the IMU-camera time offset, and the rolling shutter time of the camera.

\subsection{IMU measurements} 
  IMU which including a gyroscope and an accelerometer, can measure the mobile deviceâ€™s linear acceleration and angular velocity at a high frequency. However, measurements from consumer-level devices are susceptible to white gaussian noise(zero-mean gaussian noise ${\textbf{n}}_{a_k}$, ${\textbf{n}}_{g_k}$) and time-varying biases (${\textbf{b}}_{g_k}, {\textbf{b}}_{a_k}$). The gyroscope and accelerometer measurements at frame $k$, denotes as 
   ${\hat{\textbf{w}}}_{k}$ and ${\hat{\textbf{a}}}_{k}$ respectively, are given by:
  \begin{equation} 
    \begin{aligned}
      {\hat{\textbf{w}}}_{k} &= {\textbf{w}}_k + {\textbf{b}}_{g_k} + {\textbf{n}}_{g_k} \\
      {\hat{\textbf{a}}}_{k} &= {\textbf{a}}_k + {\textbf{b}}_{a_k} + {\textbf{R}}^{I_k}_{w}\,{\textbf{g}}^w + {\textbf{n}}_{a_k},
    \end{aligned}
  \end{equation}
  where ${\textbf{w}}_k$ and ${\textbf{a}}_k$ represent the ground truth states at frame $k$.
  
   % propagation
   Pre-integration is performed to integrate these measurements between consecutive frames of images. The variables are expressed in the local coordinate of frame $k$, consistent with the implementation in  ~\cite{forster2017manifold-preintergration,qin-tro-2018_VINS-Mono}: 
    \begin{equation}
      \label{eq:preintergration}
      \begin{aligned}
        \bm{\hat{\alpha}}^{I_k}_{I_{i+1}} &= {\bm{\hat{\alpha}}^{I_k}_{I_i}} + {\bm{\hat{\beta}}^{I_k}_{I_i}}\Delta t 
        + \frac{1}{2}\textbf{R}({\bm{\hat{\gamma}}^{I_k}_{I_i}})(\hat{\textbf{a}}_i - \textbf{b}_{a_k})\Delta t^2\\
        \bm{\hat{\beta}}^{I_k}_{I_{i+1}} &= {\bm{\hat{\beta}}^{I_k}_{Ii}} 
        + \textbf{R}({\bm{\hat{\gamma}}^{I_k}_{I_i}})(\hat{\textbf{a}}_i - \textbf{b}_{a_k})\Delta t \\
        \bm{\hat{\gamma}}^{I_k}_{I_{i+1}} &= {\bm{\hat{\gamma}}^{I_k}_{Ii}}Q((\hat{\textbf{w}}_i
        - \textbf{b}_{g_k})\Delta t),
      \end{aligned}
    \end{equation}
    Where $Q(\cdot)$ represents the transformation which will convert the representation of $Euler\ Angles$ to $Quaternion$. Then, the residuals of IMU measurements based on \cref{eq:preintergration} are defined as:
    \begin{equation}
    \label{IMU_residual}
      \begin{split}
      \begin{aligned}
      {\textbf{r}_I}(k) &= 
       \left[
        \begin{matrix}
          \textbf{R}^{I_k}_w({\textbf{p}_{I_{k+1}}^w} - {\textbf{p}^w_{I_k}} + \frac{1}{2}\textbf{g}^w\Delta t^2_k 
            -{\textbf{v}^w_{I_k}}\Delta t_k) -\hat{\bm{\alpha}}^{I_k}_{I_{k+1}} \\
          \textbf{R}^{I_k}_w({\textbf{v}_{I_{k+1}}^w} + \textbf{g}^w\Delta t_k - \textbf{v}^w_{I_k}) - \hat{\bm{\beta}}^{I_k}_{I_{k+1}} \\
          2[{(\textbf{q}^w_{I_k})}^{-1} \otimes \textbf{q}^w_{I_{k+1}} \otimes {(\hat{\bm{\gamma}}^{I_k}_{I_{k+1}})}^{-1}]_{xyz} \\
          \textbf{b}_{a_{k+1}} - \textbf{b}_{a_k} \\
          \textbf{b}_{g_{k+1}} - \textbf{b}_{g_k} \\
        \end{matrix}
       \right]. \\
       % &= ||{H}_{b_k}\tilde{{S}}_{b_k} + {H}_{b_{k+1}}\tilde{{S}}_{b_{k+1}} + 
       % {H}_{E_{k+1}}\tilde{{S}}_{E_{k+1}} - {r}_{u_{k+1}}||
      \end{aligned}
      \end{split}
    \end{equation}
    We also define some cost terms for optimization:
    \begin{equation}
    \label{IMU_cost}
    {C_I} = \sum_{k \in I} ||\textbf{r}_I||^2_{P^{I_k}_{I_{k+1}}},
    \end{equation}
    \begin{equation}
    \label{eq:gyro_cost}
    {C_I}(\bm{\gamma}) = \sum_{k \in I} ||\textbf{r}_I(\bm{\gamma}_k)||^2_{\textbf{P}^{I_k}_{I_{k+1}}}, 
    \end{equation}
    where $\textbf{P}^{I_k}_{I_{k+1}}$ is the covariance matrix, which is same as in ~\cite{qin-tro-2018_VINS-Mono}.  $I$ is the set where IMU measurements should be integrated. $C_I$ is the full pre-integration cost term, and $C_I(\bm{\gamma})$ is the gyroscope-related cost term aimed at constraining rotation.
\subsection{Visual measurements}
Our camera model follows the simple pinhole model, where visual measurements are the reprojection errors on image planes. 3D points are represented in the form of inverse depth. Feature $l$, which is first observed in the $i^{th}$ image, has its visual residual defined when observed again in the $j^{th}$ image as:
 \begin{equation} 
    \begin{aligned}
      \textbf{r}_V(l, j)&=\pi(\textbf{R}_w^{c_j}\textbf{X}_l+\textbf{p}_w^{c_j}) - [u^{c_j}_l,v^{c_j}_l]^T\\
      \textbf{X}_l &= \textbf{R}^w_{c_i}(d_l\cdot{\pi}^{-1}[u^{c_i}_l,v^{c_i}_l]^T) + \textbf{p}^w_{c_i},
    \end{aligned}
  \end{equation}
 where $[u^{c_i}_l,v^{c_i}_l]^T$ is the first observation of the $\textbf{X}_l$ feature that occurred in image $i$, and $[u^{c_j}_l,v^{c_j}_l]^T$ is another observation of $\textbf{X}_l$ in image $j$.  $\pi_c$ is the projection function that converts the normal coordinate to pixel coordinate with camera intrinsic parameters, and $\pi_c^{-1}$ is the back projection. $[\textbf{R}^w_{c_i},\textbf{p}^w_{c_i}]$ represents frame $i$'s camera pose in world coordinate, and $[\textbf{R}_w^{c_j}, \textbf{p}_w^{c_j}]$ represents frame $j$'s pose in camera coordinate. $\textbf{X}_l$ is the 3d coordinate of feature $l$.
 We also define the visual cost term for optimization:
 \begin{equation}
 \label{eq:cost_camera}
     C_V = \sum_{(l, j)\in V} ||{\textbf{r}_V}_{(l, j)}||^2_{\textbf{P}^{c_j}_l},
 \end{equation}
    where $V$ is the set of features that have been observed at least twice in the sliding window. $(l,j)$ denotes feature $l$ has been observed in frame $j$. ${\textbf{P}^{c_j}_l}$ is the covariance matrix for ${\textbf{r}_V}_{(l, j)}$.

\subsection{Sliding Window Filter} 
    Our multi-sensor fusion system  integrates vision and IMU measurements, aiming to minimize the cost function $C_{k}$ at each time step $k$:
  \begin{equation}
    C_{k}=C_{k-1}+C_I+C_V,
  \end{equation}
  where $C_{k-1}$ denote the cost terms related to the prior information obtained from previous time step, $C_I$, and $C_V$ denote the cost terms associated with IMU data, and visual data, respectively. These terms are consistent with those used in traditional MSCKF-based VIO systems. 
  We utilize square root inverse filter\cite{maybeck1982stochastic-square-root, wu2015square} to solve the cost function $C_{k}$ due to its computational efficiency.
 The main difference from \cite{wu2015square} is that ours does not incorporate SLAM features into the state vector, which are typically continuously updated over time. Based on our practical experimentation, we have observed that SLAM features do not significantly enhance accuracy in our system.
