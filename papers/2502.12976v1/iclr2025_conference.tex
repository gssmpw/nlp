
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{pifont}
\usepackage{arydshln}

\newcommand{\jz}[1]{{\textcolor{orange}{[zj: #1]}}}
\newcommand{\yp}[1]{{\textcolor{red}{#1}}}
\newcommand{\cxmark}{\ding{51}\hspace{-2.3mm}\ding{55}}

\title{Does Training with Synthetic Data Truly Protect Privacy?}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yunpeng Zhao\\
National University of Singapore\\
\texttt{yunpeng.zhao@u.nus.edu}
\And
Jie Zhang\\
ETH Zurich\\
\texttt{jie.zhang@inf.ethz.ch}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
As synthetic data becomes increasingly popular in machine learning tasks, numerous methods---without formal differential privacy guarantees---use synthetic data for training. These methods often claim, either explicitly or implicitly, to protect the privacy of the original training data.
In this work, we explore four different training paradigms: coreset selection, dataset distillation, data-free knowledge distillation, and synthetic data generated from diffusion models. While all these methods utilize synthetic data for training, they lead to vastly different conclusions regarding privacy preservation. We caution that empirical approaches to preserving data privacy require careful and rigorous evaluation; otherwise, they risk providing a false sense of privacy.

\end{abstract}

\section{Introduction}
Synthetic data is increasingly utilized for training machine learning (ML) models, especially in situations where real-world data is scarce, sensitive, costly to obtain, or subject to regulations such as GDPR~\citep{gdpr_info}. 
Synthetic data is particularly beneficial in scenarios where data distributions are atypical, such as in federated learning with non-IID data~\citep{fgl}, long-tailed learning~\citep{shin2023fill}, and continual learning~\citep{diffclass}. It enables the creation of diverse datasets that include edge cases or rare events that may be underrepresented in real-world data.
Consequently, training models with synthetic data has proven beneficial for enhancing model robustness and adaptability across a wide range of real-world scenarios. 

Many empirical methods---\textit{without} formal differential privacy guarantees---rely on synthetic data for training, such as coreset selection~\citep{feldman2020introduction}, dataset distillation~\citep{wang2018dataset}, data-free knowledge distillation~\citep{yin2020dreaming}, and synthetic data generated from diffusion models~\citep{realfake}. These approaches involve training ML models using proxy data\footnote{For simplicity, for the rest of the paper, we will always use the term ``synthetic data'' (also for coreset).} instead of the original private training data.
This proxy data can be directly sampled from private sources~\citep{guo2022deepcore,mirzasoleiman2020coresets} or out-of-distribution sources~\citep{wang2023sampling}, iteratively optimized~\citep{Zhang_2023_CVPR,zhao2020dataset}, or generated using GANs~\citep{karras2019style} and diffusion models~\citep{rombach2022high}. Since the model may never encounter any private training data and the synthetic images are often visually distinct from the original private data, these methods often claim to \textit{preserve privacy} while still maintaining satisfactory performance.

In this work, we aim to address the following question: 
\begin{center}
\emph{Does training with synthetic data truly protect privacy?}\footnote{Existing works on generating synthetic data with differential privacy are outside the scope of this work.}
\end{center}

To rigorously measure the privacy leakage of empirical methods trained on synthetic data, we use membership inference attacks~\citep{shokri2017membership} as a privacy auditing tool. We provide a systematic privacy evaluation on these four training paradigms. For each training paradigm, we interact only with the final model trained on synthetic data, and then determine whether a particular data point was part of the \textit{private} training dataset.

We also provide a fair comparison with theoretical defenses with differential privacy, such as DPSGD~\citep{abadi2016deep}, and always report the privacy leakage of these methods in the \textit{worst case}. An ideal defense should strike a good balance between privacy and model utility, while also being efficient.


As shown in~\Cref{fig:intro}, we employed the most rigorous evaluation framework described in~\cite{aerni2024evaluations}, training multiple shadow models to accurately evaluate membership inference success, specifically focusing on the true positive rate (TPR) at very low false positive rates (FPR) on the CIFAR-10 dataset. We ensure that each method achieves at least approximately 73\% accuracy on the test set, as models with low accuracy may lead to misleading conclusions (e.g., overfitting) and are not meaningful.

Similar to the findings in~\cite{aerni2024evaluations}, almost all empirical training paradigms do not provide stronger privacy protection than the basic differential privacy baseline---DPSGD. 
Another interesting finding is that for some dataset distillation methods, the success rate of membership inference attacks is very low, yet the synthetic data \textit{visually} resembles the private data almost entirely (see~\Cref{fig:mtt_vis}). We argue that this does not constitute meaningful privacy protection. Based on these findings, we caution that empirical approaches to preserving data privacy require careful and rigorous evaluation; otherwise, they risk providing misleading conclusions. 


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figure/intro.pdf}
    \caption{A rigorous evaluation of privacy leakage in models trained with synthetic data. We compare the privacy-utility tradeoff and efficiency of four training paradigms---coreset selection, dataset distillation (DD), data-free knowledge distillation (DFKD), and synthetic data generated from diffusion models---against DPSGD.}
    \label{fig:intro}
\end{figure}


\section{Related Work}
\paragraph{Membership Inference Attack}
Membership inference attack~\citep{shokri2017membership} is a canonical approach to estimating privacy leakage, which aims to determine whether a given example was part of the training set. State-of-the-art attack methods frame membership inference as a hypothesis testing problem and evaluate privacy leakage by reporting the true positive rate at very low false positive rates~\citep{lira}. Recent work~\citep{aerni2024evaluations} suggests that many empirical defenses fail to evaluate privacy leakage in worst-case scenarios; instead, they often report average-case privacy leakage, which can significantly underestimate actual privacy risks. Since privacy is not an average metric, we adopt this approach to provide a rigorous and careful evaluation of privacy. 


\paragraph{Training on Synthetic Data} 
Many studies have highlighted that training models on synthetic data can be remarkably effective in scenarios where data collection or distribution is challenging. 
For instance, in long-tailed learning and federated learning, generative models can be used to augment data, resulting in a more balanced distribution~\citep{shin2023fill,fgl}. Similarly, in continual learning, synthetic data is adopted to help mitigate the problem of catastrophic forgetting~\citep{diffclass,zhang2023target,shin2017continual}.
In scenarios where only the target model is available without access to its private training data, generative models can be used to create a synthetic dataset to train a substitute model, which can achieve similar performance to the target model~\citep{zhang2023ideal,lopes2017data}. Furthermore,~\citet{realfake} demonstrated that a model trained solely on synthetic data can perform well on the ImageNet test set. \citet{cazenavette2022dataset,guo2024lossless} showed that even when the original training set is condensed to just 1\% of its size as synthetic data, it is still possible to train a well-performing model on CIFAR10 test set. 
These examples illustrate the effectiveness of synthetic data, but in this work, we are more curious about whether these models trained with synthetic data actually protect privacy.

A related study~\cite{aerni2024evaluations} rigorously evaluates several empirical membership inference defenses, including techniques such as label smoothing~\cite{chen2024hamp}, training loss calibration~\cite{chen2022relaxloss}, and self-supervised learning. While our experiments build on their evaluation framework, we extend this work by specifically investigating empirical methods that train ML models using \textit{synthetic data}.




\section{Misleading Privacy Evaluations on Synthetic Data}

\subsection{Empirical defenses: training models with synthetic data}
In this section, we provide a brief introduction to the four training paradigms for generating synthetic data that we studied. A comparison of these training paradigms is presented in~\Cref{tab:tabel1}, and we show our evaluation setup in~\Cref{fig:pipeline}. 


\begin{table}[h]
\centering
\caption{Comparison of methods with respect to privacy considerations for the final model. ``Private Data'' indicates that the final model is trained directly on some private data, and ``partial'' means that some method requires it. }
\label{tab:tabel1}
\begin{tabular}{ccccc}
\toprule
Training Paradigm & Private Data & Public Data  & Generative Model & Teacher Model  \\
\midrule
Coreset Selection\textsuperscript{1}    & \checkmark & \text{\sffamily X} & \text{\sffamily X}  & \text{\sffamily X} \\

Dataset Distillation\textsuperscript{2} & \text{\sffamily X} & partial & \text{\sffamily X}  &  partial \\

DFKD\textsuperscript{3}                 & \text{\sffamily X} & \text{\sffamily X} & \checkmark & \checkmark  \\

Diffusion\textsuperscript{4}             & \text{\sffamily X} & \checkmark & \checkmark  & \text{\sffamily X}\\
\bottomrule
\end{tabular}

\footnotesize{
\textsuperscript{1}\citep{toneva2018an} \ 
\textsuperscript{2}\citep{guo2024lossless,cazenavette2022dataset,dsa,dm} \ 
\\
\textsuperscript{3}\citep{fang2022up} \ 
\textsuperscript{4}\citep{realfake} \ }
\end{table}


\paragraph{Coreset Selection}
Coreset selection~\citep{toneva2018an, herding} aims to extract a compact and informative subset \(D_{\text{core}}\) that captures the essential characteristics of the original private training set \(D_{\text{train}}\). The selection algorithm begins by evaluating the significance of each data instance in \(D_{\text{train}}\), scoring them based on criteria such as the frequency of forgetting events during training~\citep{toneva2018an} or their distance from the cluster center in feature space~\citep{herding}. After scoring, \(D_{\text{core}}\) is formed by selecting the top \(k\) samples with the highest scores, which can be expressed as:
\[
D_{\text{core}} = \{x_i \in D_{\text{train}} \mid \text{score}(x_i) \in \mathtt{Top}(\text{score}(D_{\text{train}}), k)\},
\]
where \(\text{score}(\cdot)\) is the scoring metric and \(\mathtt{Top}(\cdot, k)\) selects the top \(k\) highest scores. The selected coreset \(D_{\text{core}}\) then replaces the original private dataset for model training.  It is obvious that the samples in the subset still face the potential risk of privacy leakage.


\paragraph{Dataset Distillation (DD)}
DD~\citep{wang2018dataset, dm, dsa, cazenavette2022dataset, guo2024lossless} aims to \textit{learn} a small set of informative synthetic images \(D_{\text{syn}}\) from a large training dataset \(D_{\text{train}}\), such that a neural network trained on \(D_{\text{syn}}\) achieves similar or comparable generalization performance to a network trained on the original dataset. The process begins by initializing images in \(D_{\text{syn}}\) from real images or random noise. After initialization, the goal of DD is formulated as an optimization problem, seeking to minimize the discrepancy between the synthetic and original datasets by aligning their effects on the neural network \(\phi_\theta\) parameterized by \(\theta\). The optimization objective \(\mathcal{L}_{\text{dd}}\) may involve differences between gradients of two sets of network parameters~\citep{dsa}, disparities in feature distributions across multiple sampled embedding spaces~\citep{dm}, or variations in model training trajectories~\citep{cazenavette2022dataset, guo2024lossless}:
\[
\min_{D_{\text{syn}}} \mathcal{L}_{\text{dd}}(\phi_\theta(D_{\text{syn}}), \phi_\theta(D_{\text{train}})).
\]
After convergence, the synthesized dataset \(D_{\text{syn}}\) can be used to train models. 
However, potential privacy risks may arise during the initialization stage if real images from the private dataset \(D_{\text{train}}\) are used to initialize \(D_{\text{syn}}\). A critical concern is whether the subsequent DD optimization process adequately safeguards the privacy of these initial samples. 

\paragraph{Data-Free Knowledge Distillation (DFKD)}
DFKD~\citep{yin2020dreaming,fang2022up} transfers knowledge from a teacher model \( f_{\text{teacher}} \) to a student model \( f_{\text{student}} \) using synthetic data that approximates the private training data. First, \( f_{\text{teacher}} \) is trained on \( D_{\text{train}} \). Then, a generative model \( G \) creates synthetic data \( \{x_i\}_{i=1}^{N} \) to match the teacher model's statistical properties, minimizing an inversion loss:
\[
\min_{G} \mathcal{L}_{\text{gen}}(G, f_{\text{teacher}}, D_{\text{train}}).
\]
The student model is trained on this synthetic data and the teacher's predictions, minimizing the distillation loss:
\[
\min_{f_{\text{student}}} \mathcal{L}_{\text{distill}}(f_{\text{student}}, \{x_i\}_{i=1}^{N}, f_{\text{teacher}}).
\]
This process iterates until convergence. Privacy risks may arise if the teacher model memorizes specific instances from \( D_{\text{train}} \).


\paragraph{Synthetic Data from Fine-Tuned Diffusion Models}
This approach~\citep{realfake} involves fine-tuning a stable diffusion model on a private dataset to learn its distribution. Leveraging the model's powerful generative capabilities, synthetic data is then generated and used to train models. Potential privacy leakage may occur if the fine-tuned diffusion model retains or reveals sensitive information from the private dataset, especially if the generated synthetic data closely resembles the original data. 




\subsection{Privacy Leakage on most vulnerable samples} 
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/eval_privacy.pdf}
    \caption{We evaluate the privacy leakage of private training data in the worst-case scenario for each training paradigm, only interacting with the final model trained on synthetic data.}
    \label{fig:pipeline}
\end{figure}
Our motivation is to properly evaluate privacy leakage in ML models trained on synthetic data. First, we will highlight some misleading evaluations that give a false sense of privacy. Then, we will demonstrate that models tend to strongly memorize the most vulnerable samples, which is why training with synthetic data can lead to privacy leakage.
\paragraph{Misleading Privacy Evaluations on Synthetic data}
Since privacy is not a metric that can be averaged~\citep{DPorg-average-case-dp}, \citet{aerni2024evaluations} advocates for rigorous evaluation by reporting privacy leakage on the most vulnerable samples in a dataset, rather than the average case. To enhance computational efficiency, they suggest using \textit{mislabeled data}, which approximates the privacy leakage of the most vulnerable samples. However, none of the existing work has systematically evaluated privacy leakage in the worst-case scenario for synthetic data.


Previous work~\citep{pmlr-v162-dong22c} claims that dataset distillation can significantly improve data privacy, but the level of privacy protection is not evaluated in a correct way~\citep{carlini2022no}. Some studies~\citep{hao2021data,pmlr-v162-dong22c} claim that training on synthetic data protects privacy because synthetic data is visually dissimilar to the private data. However, all these methods give a false sense of privacy, which we further demonstrate in~\Cref{sec:experiments}.

We begin with a toy experiment on coreset selection and dataset distillation\footnote{See~\Cref{asec:eval_setup} for detailed experimental setup.\label{fn:exp_setup}}. In~\Cref{fig:avg_and_vulnerable}, we demonstrate that for coreset selection and dataset distillation, failing to report privacy leakage on the most vulnerable samples can severely underestimate the true privacy risk. Thus, in this work, 
% \begin{center}
    {we always report privacy leakage in the worst case\footnote{By default, we use \textbf{mislabeled data} as strong canaries to simulate the most vulnerable samples, but even stronger canaries may exist, offering a better capture of privacy leakage.} to avoid underestimating the privacy risk}.
% \end{center}


\begin{figure}[t]
\begin{minipage}[t]{.48\linewidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figure/avg_vs_vulnerable.pdf}
  \caption{Failing to report privacy leakage on the most vulnerable data provides a \textit{false} sense of privacy. We investigate three different defenses: one based on coreset selection and two based on dataset distillation.}
  \label{fig:avg_and_vulnerable}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\linewidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figure/distribution.pdf}
  \caption{ML models tend to strongly memorize the most vulnerable data. We demonstrate this by presenting the loss distribution for both members and non-members, comparing average-case data with worst-case data.}
  \label{fig:loss_distribution}
\end{minipage}
\end{figure}

\paragraph{Strong Memorization on Vulnerable Data}

ML models can memorize sensitive information from their training data, particularly for vulnerable samples such as mislabeled data or outliers~\citep{feldman2020neural}. 
In~\Cref{fig:loss_distribution}, we show that for a mislabeled data point, models can strongly memorize it, leading to a significant difference in the loss distribution for this sample when it is in the training set versus when it is not\footref{fn:exp_setup}. This discrepancy makes MIA much easier. Therefore, later we will show that no matter how carefully synthetic data training is designed, the strong memorization capabilities of ML models still make it easy to perform MIA on the most vulnerable samples.

% \section{Does Training with Synthetic Data Truly Protect Privacy?}
\section{Case Studies: Privacy Leakage in Training with Synthetic Data}
\label{sec:experiments}
In this section, we conduct a systematic and rigorous evaluation of privacy leakage across four training paradigms based on synthetic data. We start by introducing the experimental setups. Subsequently, we will discuss the potential privacy leakage inherent to each method. Finally, we will compare all these methods with a differential privacy baseline (i.e., DPSGD) with respect to model utility, privacy preservation, and computational efficiency.

\subsection{Evaluation Setup}
\label{ssec:eval_setup}
\paragraph{Evaluation Metric} 
Empirical methods designed to preserve data privacy require careful scrutiny. To ensure precise measurement of privacy leakage in models trained with synthetic data, we employ the state-of-the-art membership inference attack LiRA~\citep{lira}) and follow the setup from~\citep{aerni2024evaluations}. In this setup, we report the privacy leakage by evaluating the true positive rate at a low false positive rate on the most vulnerable samples (e.g., mislabeled data).  

\paragraph{Experimental Setup} We conduct all experiments on CIFAR-10~\citep{cifar}, as all training methods are scalable to CIFAR-10 and achieve good test accuracy. We designate 500 random data points as ``audit samples'' on which we evaluate membership inference, and we use mislabeled data as strong canaries to simulate worst case data; the remaining 49,500 samples are always included in every model's training data. For each method, we train 32 shadow models, ensuring that each audit sample is included in the training data of 16 models. For all defenses, we consistently adopt ResNet-18~\citep{resnet} as the network architecture of shadow models. We report the performance of these methods across three dimensions: privacy leakage (TPR@0.1\% FPR), model utility (test accuracy), and efficiency (training time). A strong defense should achieve a balanced tradeoff among privacy, utility, and efficiency.
{The implementation details of all discussed defenses are provided in~\Cref{asec:training_details}.}

\begin{figure}[t]
\begin{minipage}[h]{.49\linewidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figure/coreset_selection.pdf}
  \caption{Coreset selection does not guarantee privacy protection, both random selection and forgetting result in significant privacy leakage. The TPR at 0.1\% FPR for forgetting is 72.94\% while it is 38.70\% for random selection.}
  \label{fig:coreset_roc}
\end{minipage}
\hfill
\begin{minipage}[h]{.48\linewidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figure/privacy_onion.pdf}
  \caption{MI success rate for both the full dataset and the coreset. ``Newly successful'' denotes samples that are typically very safe when trained on the full dataset but experience a significant increase in privacy leakage once included in the coreset.
  }
  \label{fig:privacy_onion}
\end{minipage}
\end{figure}

\subsection{Privacy Leakage in Coreset Selection}

We begin by evaluating coreset selection methods to explore whether privacy protection can be achieved by selecting a representative subset to replace the entire private dataset for model training. 
We compare two representative methods: random selection and forgetting~\citep{toneva2018an}, against the undefended baseline. The coreset size is consistently set to 20,000. The average test accuracy across 32 shadow models is 94.78\% for the undefended baseline, 92.57\% for forgetting, and 90.56\% for random selection.


In \Cref{fig:coreset_roc}, we find that neither random selection nor forgetting is able to protect the membership privacy of auditing samples as the private subset is directly used for training. 

One interesting finding is that the forgetting defense achieves better utility than random selection by selecting informative samples that are easily forgotten during training. However, its privacy leakage is significantly higher. This is likely due to the forgetting method's tendency to select mislabeled samples. While random selection includes roughly 40\% of mislabeled data, the forgetting method increases this to 74\%, suggesting that these vulnerable data points also contribute to improved generalization~\citep{feldman2020neural}, at the cost of reduced privacy.


Another interesting finding is that even in average case evaluations, some samples that would typically be very safe (when trained on the entire dataset) experience a significant increase in privacy leakage once selected for the coreset. This suggests that protecting the privacy of non-coreset samples can degrade the privacy of other samples. In worst-case evaluation scenarios, this becomes even more concerning, as no sample would be willing to sacrifice its privacy for the benefit of others. We illustrate this in~\Cref{fig:privacy_onion}, where users whose privacy appears least at risk may be the most likely not to request their data be included in a coreset.


\subsection{Privacy Leakage in Dataset Distillation}

\begin{table}[t]
\centering
\caption{
A rigorous evaluation of privacy leakage on four dataset distillation methods.
`—' indicates that the model's utility is significantly lower and thus less practical to use.
}
\label{tab:dd_privacy}
\begin{tabular}{cccc}
\toprule
\multicolumn{1}{c}{{Methods}} &
\multicolumn{1}{c}{{Initialization}} &
\multicolumn{1}{c}{{Test Accuracy}} &
\multicolumn{1}{c}{{TPR@0.1\% FPR}} \\
\midrule
\multirow{3}{*}{DM\textsuperscript{1}}   
    & Private  & 84.33\% & \textbf{18.02}\% \\
    & Noise & 72.52\% & 0.24\%  \\
    & OOD   & 75.00\% & 0.18\%  \\ 
\midrule
\multirow{3}{*}{DSA\textsuperscript{2}}  
    & Private  & 84.43\% & \textbf{17.74}\% \\
    & Noise & —       & —       \\
    & OOD   & —       & —       \\ 
\midrule
\multirow{3}{*}{MTT\textsuperscript{3}}  
    & Private  & 81.81\% & 1.51\%  \\
    & Noise & —       & —       \\
    & OOD   & 80.42\% & 0.18\%  \\ 
\midrule
\multirow{3}{*}{DATM\textsuperscript{3}} 
    & Private  & 84.54\% & 1.29\%  \\
    & Noise & —       & —       \\
    & OOD   & —       & —       \\ 
\bottomrule
\end{tabular}
\footnotesize{ \\
\textsuperscript{1} Distribution Matching \ 
\textsuperscript{2} Gradient Matching \ 
\textsuperscript{3} Trajectory Matching \ }
% \vspace{-5mm}
\end{table}

Current work evaluating the privacy leakage of dataset distillation often contains some flaws. These methods face issues in empirical evaluation, such as using improper averaging metrics (e.g., AUC instead of TPR)~\citep{chen2023comprehensive}, evaluating models with low performance (around 60\% test accuracy), or applying incorrect theoretical analyses~\citep{pmlr-v162-dong22c}. Furthermore, none of these methods are evaluated under worst-case scenarios. Such non-rigorous evaluations can result in misleading conclusions~\citep{carlini2022no}.


In this work, we conduct experiments on four representative DD methods to rigorously evaluate their privacy leakage, including DM~\citep{dm}, DSA~\citep{dsa}, MTT~\citep{cazenavette2022dataset}, and DATM~\citep{guo2024lossless}. We tune the model performance to around 80\% test accuracy and evaluate these methods under worst-case scenarios to capture the full extent of potential privacy leakage. 

Popular methods in dataset distillation aim to match the behavior of models trained on synthetic data with those trained on private data. This matching can be based on distribution, gradients, or training trajectories. Additionally, synthetic data can be initialized using real private data, out-of-distribution~(OOD) data, or random noise. These factors can lead to significantly different levels of privacy leakage. We present the overall results in~\Cref{tab:dd_privacy} and provide a detailed analysis of how each factor impacts privacy leakage in the following discussion.


\paragraph{The initialization of synthetic data affects privacy}
\setlength\intextsep{0pt}
\begin{wraptable}{r}{0.40\textwidth}
\centering
\caption{The MI success rate on initialization and non-initialization canaries.}
% \small{
\begin{tabular}{ccc}
\toprule
\multirow{2}{*}{{Method}} & \multicolumn{2}{c}{{Success rate (\%)}} \\ \cmidrule{2-3} 
                                 & {Init}      & {Non-init}       \\ \midrule
DM                               & \textbf{88.89}       & 0.16             \\
DSA                              & \textbf{88.70}       & 0.09             \\
MTT                              & 7.02        & 0.11             \\
DATM                             & 36.05       & 0.24             \\ 
\bottomrule
\end{tabular}
% }
\label{tab:dd_init}
\end{wraptable} 

As shown in~\Cref{tab:dd_privacy}, initializing synthetic data with private data leads to higher privacy leakage in DM compared to initialization with OOD data or random noise. Consequently, while initializing with OOD data or random noise offers better privacy protection, it comes at the cost of substantial reductions in model performance.

% \setlength\intextsep{0pt}

To further illustrate why initialization with private data results in greater privacy leakage, we analyze the privacy risks associated with canaries used during the synthetic data initialization process. As shown in~\Cref{tab:dd_init}, the membership inference success rate for canaries involved in initialization is considerably higher than for those not used during initialization. For canaries present in the training set but excluded from initialization, the success rate is nearly indistinguishable from random guessing. This finding empirically demonstrates that privacy risks are primarily introduced during the initialization phase, as the model tends to strongly memorize these initialization canaries.

\iffalse
\begin{figure}[t]
\begin{minipage}[b]{0.48\textwidth}
\centering
\caption{The utility and privacy leakage of different DD defenses, which are reported using the average test accuracy of all shadow models and TPR@0.1\%FPR, respectively. — indicates that the defense's utility is too low.}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\multicolumn{1}{c}{\textbf{Methods}} &
\multicolumn{1}{c}{\textbf{Initialization}} &
\multicolumn{1}{c}{\textbf{Test Accuracy}} &
\multicolumn{1}{c}{\textbf{TPR@0.1\%FPR}} \\
\midrule
\multirow{3}{*}{DM\textsuperscript{1}}   
    & Real  & 84.33\% & 18.02\% \\
    & Noise & 72.52\% & 0.24\%  \\
    & OOD   & 75.00\% & 0.18\%  \\ 
\midrule
\multirow{3}{*}{DSA\textsuperscript{2}}  
    & Real  & 84.43\% & 17.74\% \\
    & Noise & —       & —       \\
    & OOD   & —       & —       \\ 
\midrule
\multirow{3}{*}{MTT\textsuperscript{3}}  
    & Real  & 81.81\% & 1.51\%  \\
    & Noise & —       & —       \\
    & OOD   & 80.42\% & 0.18\%  \\ 
\midrule
\multirow{3}{*}{DATM\textsuperscript{3}} 
    & Real  & 84.54\% & 1.29\%  \\
    & Noise & —       & —       \\
    & OOD   & —       & —       \\ 
\bottomrule
\end{tabular}}
\tiny{ \\
\textsuperscript{1} Distribution Matching \ 
\textsuperscript{2} Gradient Matching \ 
\textsuperscript{3} Trajectory Matching \ }
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figure/mtt_vis.pdf}
\caption{We choose several mislabeled canaries that were selected for initializing the synthetic dataset of MTT. We then compare their images before and after MTT, as well as their logits output by the shadow model trained on synthetic data.}
\label{fig:mtt_vis}
\end{minipage}
\end{figure}
\fi

\begin{figure}[t]
\begin{minipage}[h]{.46\linewidth}
  \centering
\includegraphics[width=1.0\textwidth]{figure/mtt_vis.pdf}
\caption{
 For DD, the synthetic data leaks the privacy of the private data visually, but this leakage is not captured by MIA.
}
\label{fig:mtt_vis}
\end{minipage}
\hfill
\begin{minipage}[h]{.50\linewidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figure/dfkd_roc.pdf}
  \caption{For DFKD, although the student model is never trained on private data and is only trained on synthetic data, due to knowledge distillation, it still leaks a significant amount of privacy.}
  \label{fig:dfkd_roc}
\end{minipage}
\end{figure}



\paragraph{Visual privacy leakage in trajectory matching with private initialization}
As demonstrated in~\Cref{tab:dd_privacy}, trajectory-based methods seem to provide superior privacy protection compared to DM and DSA, even when private initialization is utilized.
This can be attributed to the fact that trajectory-based approaches, such as MTT and DATM, initially train a teacher model on the original private data and save the training trajectories. During each optimization epoch, a network is selected from the teacher trajectory and trained for several steps using synthetic data, with the objective of matching the trained model's parameters to the teacher trajectory. Notably, MTT and DATM focus on aligning only the early to mid-training trajectories—stages where the model has not yet memorized canaries. 

However, \textbf{a low MI success rate could also provide a false sense of privacy.} In~\Cref{fig:mtt_vis}, we visualize both the synthetic data and the private data, which clearly demonstrates visual privacy leakage that is not captured by MIA.
We present the logits output by the model trained on synthetic data, for both the private data (canaries) used for initialization and the synthetic data itself. This indicates that while the model can strongly memorize the synthetic data, it generalizes well on the canaries, which results in a low MI success rate. A potential explanation for this phenomenon is that trajectory matching effectively introduces a ``weird'' data augmentation on the private data, which is hard to estimate. This aspect is not captured during MIA, which typically employs common augmentation techniques like random flip and random crop.


In summary, despite the low MI success rate, MTT entirely compromises the privacy of private data through visual leakage. A potential defense is to use MTT initialized with OOD data, which achieves a low MI success rate (close to random guessing) while offering stronger visual privacy protection (see~\Cref{fig:mtt_cinic_sscd}). However, this approach results in reduced model performance, with test accuracy dropping to 80.42\%.



\subsection{Privacy Leakage in Data-Free Knowledge Distillation}
Similar to previous work~\citep{aerni2024evaluations}, in~\Cref{fig:dfkd_roc}, we also find that since the teacher model can strongly memorize canaries, during the distillation stage, the student model can also somewhat memorize canaries, leading to significant privacy leakage.

We further visualize the synthetic data and private data in Figure~\ref{fig:dfkd_analysis}, revealing an interesting finding: even when a synthetic image appears completely different from the original private image, it can still somehow trigger the teacher model's memorization of the canary, thereby leaking privacy. The SSIM~\citep{ssim} score is only 0.1661, yet the logits distribution shows an extremely high similarity, with a correlation of nearly 0.999.

In detail, the generator in DFKD is capable of producing synthetic images that, while visually distinct, bear a strong resemblance in logits to the mislabeled data. These synthetic images activate the memories of corresponding canaries encoded in the teacher model's parameters. The distillation process attempts to align the student model’s logits distribution with that of the teacher for the same images. Consequently, the student model may learn some private knowledge from the teacher, including some aspects of the teacher’s memorization of mislabeled data.

In summary, the visual dissimilarity between synthetic data and private data does not necessarily guarantee privacy protection.


\begin{figure}[t]
\begin{minipage}[t]{.48\linewidth}
  \centering

  \includegraphics[width=1.0\linewidth]{figure/dfkd_similar_canary_syn_pairs.pdf}
  \caption{For DFKD, synthetic data can activate the teacher model's memorization of private data, even if the synthetic data appears visually distinct, leaking privacy to student.}
  \label{fig:dfkd_analysis}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\linewidth}
  \centering

      \includegraphics[width=0.99\textwidth]{figure/diffusion.pdf}
    \caption{
    For synthetic data from a fine-tuned diffusion model, image-to-image generation still leaks privacy, while text-to-image results in predictions close to random guessing.}
    %Models trained on synthetic data generated from a fine-tuned diffusion model also leak privacy, with a true positive rate of 12.65\% at a 0.1\% false positive rate.}
    \label{fig:diffusion}
    
\end{minipage}
\end{figure}

\subsection{Privacy Leakage in Synthetic Data from Diffusion Models}
A representative method for this training paradigm is presented by~\citep{realfake}, which involves three stages: (1) first, fine-tuning a stable diffusion model on a private dataset; (2) then using the fine-tuned model for image-to-image generation by passing a text prompt and an initial image from the private data to condition the generation of new images; (3) finally, replacing all private data with the generated synthetic images (while keeping the original labels unchanged) and training models solely on this synthetic data.

The potential privacy risks in this approach include: (1) the final model could still memorize mislabeled data, as the labels are retained, and (2) the fine-tuned diffusion model, having been trained on private data, may leak sensitive information.

The first privacy risk could be mitigated by using text-to-image generation, which does not require labels and is thus less sensitive to mislabeled canaries. In~\Cref{fig:diffusion}, while image-to-image generation still leaks privacy, text-to-image generation results in predictions close to random guessing.

Regarding the second privacy risk, since stable diffusion is pretrained on large public datasets and has strong generative capabilities, it is largely insensitive to mislabeled data and does not exhibit strong memorization. As seen in~\Cref{fig:diffusion}, when using text-to-image generation, the only privacy leakage stems from the fine-tuning process. However, based on membership inference success, this fine-tuning does not result in significant privacy leakage. 

A more interesting direction for future research would be to investigate the privacy-utility tradeoff in diffusion models that are trained from scratch. This approach would enable a fairer comparison with other methods that do not rely on large-scale pretraining datasets.





\subsection{Fair Comparison with DPSGD}
Since differential privacy is the most standard defense, we provide a fair comparison of all four training paradigms against DPSGD~\citep{abadi2016deep}. We tune each method to achieve comparable test accuracy and then evaluate both privacy leakage and training efficiency. Specifically, we consider two DPSGD baselines: one with a test accuracy of 83.26\%, and a high-utility baseline achieving 90.36\% in accuracy.


\begin{table}[t]
\renewcommand\arraystretch{1.18}
\centering
\caption{Fair comparison with DPSGD. We compare the privacy-utility-efficiency tradeoff of the heuristic defenses we study to two DPSGD baselines tuned for different test accuracy.}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Method}   & \textbf{Test Accuracy} & \textbf{TPR@0.1\%FPR} & \textbf{Efficiency} \\ \midrule
Undefended        & 94.78\%                & 100.00\%              & 1.0$\times$         \\
\hdashline
Coreset Selection & 92.57\%                & 72.94\%               & 2.9$\times$         \\
% DATM (Real)       & 84.54\%                & 1.29\%                & 46.7$\times$        \\
MTT (OOD)         & 80.42\%                & 0.18\%                & 39.7$\times$        \\
DFKD              & 92.09\%                & 43.38\%               & 2.9$\times$         \\
Diffusion         & 82.64\%                & 12.65\%               & 385.4$\times$       \\ 
\hdashline
DPSGD (medium)    & 83.26\%                & 0.40\%                & 0.4$\times$         \\
DPSGD (high)      & 90.36\%                & 9.31\%                & 1.9$\times$         \\ \bottomrule
\end{tabular}
\label{tab:cmp_dpsgd}
\end{table}

We present all results in~\Cref{tab:cmp_dpsgd}, where it is clear that DPSGD remains the best defense, achieving a superior trade-off between privacy, utility, and efficiency. It is worth noting that methods like DD and fine-tuning diffusion models are significantly more time-consuming, requiring substantially more training hours compared to DPSGD.




\section{Conclusion}
Empirical methods that claim to preserve data privacy, whether implicitly or explicitly, but lack theoretical guarantees (e.g., differential privacy), require careful scrutiny. In this work, we systematically and rigorously evaluate privacy leakage across all four training paradigms that rely on synthetic data. To avoid providing a false sense of security, we consistently report privacy leakage in the worst-case scenario and conduct fair comparisons with a differential privacy baseline. Our results show that none of these empirical methods achieve a better trade-off than DPSGD.

We hope this work can help researchers gain a deeper understanding of privacy, and that any new empirical defenses should also undergo rigorous evaluation.

\section*{Reproducibility Statement}
To ensure the reproducibility of our results, we provide a detailed description of our attack protocols, as well as the hyperparameters and training details for all defenses in~\Cref{ssec:eval_setup} and~\Cref{asec:training_details}. The source code is available at \url{https://github.com/yunpeng-zhao/syndata-privacy}.

\section*{Acknowledgments}
J.Z. is funded by the Swiss National Science Foundation (SNSF) project grant 214838. We sincerely thank Michael Aerni and Bo Zhao for their insightful discussions during the early stages of our experiments on dataset distillation, as well as the anonymous reviewers for their valuable feedback and constructive suggestions.

\bibliographystyle{iclr2025_conference}
\bibliography{iclr2025_conference}

\newpage
\appendix
% \section{Defense-Specific Hyperparameters and Implementation Details}
\section{Training Details}
\label{asec:training_details}
\subsection{Undefended}
\label{assec:undefended}
For the undefended baseline, we employ the same training procedure as described in~\citep{aerni2024evaluations}. Concretely, ResNet-18 models are trained using the SGD optimizer with a momentum of 0.9 and a weight decay of 0.0005. We use a batch size of 256 and typical data augmentation techniques, including random horizontal flips and random shifts of up to 4 pixels. The models are optimized over 200 epochs with a base learning rate of 0.1. We employ a linear warm-up of the learning rate during the first epoch, followed by a decay of the learning rate by a factor of 0.2 at epochs 60, 120, and 160.

\subsection{Coreset Selection}
We adjust the coreset size to balance the privacy-utility-efficiency tradeoff, which is set to 20,000 to maintain high utility while using as few samples as possible. Reducing the coreset size further results in a significant drop in model test accuracy. The training process and other hyper-parameters for coreset selection remains consistent with the original paper~\citep{toneva2018an}. The resulting coresets are used to train the corresponding shadow models in the same way as the undefended baseline in~\Cref{assec:undefended}.

\subsection{Dataset Distillation}
Given that we consistently use ResNet-18 as the network architecture for our shadow models across various defenses, we initially considered employing it for DD as well. However, in the high image-per-class (ipc) setting, which is essential for improved utility, we faced challenges with the DD process. The optimization had difficulties converging, ultimately failing to deliver satisfactory results. Consequently, we opted for the ConvNet architecture specified in~\citep{dm}, which includes three repeated convolutional blocks. Each block comprises a 128-kernel convolution layer, an instance normalization layer, a ReLU activation function, and an average pooling layer. 

Studies in~\citep{guo2024lossless} have shown that the synthetic dataset generated through DD exhibits satisfactory cross-architecture performance, especially with a large ipc. This implies that using ConvNet for DD does not result in significant performance discrepancies when training shadow models compared to using ResNet-18. To maximize utility, we set the ipc for all DD methods to 1,000. Upon generating the synthetic data, we follow the method described in~\Cref{assec:undefended} to train our shadow models with the ResNet-18 architecture. 

For each DD defense, we explore three different methods for initializing the synthetic dataset: Private, Noise, and OOD initialization. Private initialization involves randomly sampling images class-by-class from the original private training set, and Noise initialization employs Gaussian noise directly. Private initialization is commonly used in practice, while Noise initialization is less discussed due to its challenging optimization process and suboptimal performance. In this work, we attempt to use OOD data for initialization. For example, we use CINIC-10~\citep{cinic10}, an extension of CIFAR-10 incorporating downsampled ImageNet images, for initialization. For each class, we select 1,000 images from CINIC-10 that corresponded to the classes in CIFAR-10 but are not included in it. Next we will introduce the specific implementation details for each DD defense. 
% Note that we have made extensive efforts to tune these hyperparameters to achieve utility that is good enough under the high ipc setting.

\setlength\intextsep{0pt}
\begin{wraptable}{r}{0.5\textwidth}
\centering
\caption{Hyperparameters used for DM and DSA. Private, Noise, and OOD are initialization methods. LR indicates the learning rate. Outer and Inner refer to the number of outer and inner loops.}
\small
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Methods} & \textbf{LR} & \textbf{Iteration} & \textbf{Outer} & \textbf{Inner} \\ \midrule
DM (Private)     & 10          & 30,000             & -              & -              \\
DM (Noise)       & 50          & 100,000            & -              & -              \\
DM (OOD)         & 30          & 100,000            & -              & -              \\
DSA (Private)    & 0.1         & 300                & 100            & 1              \\ \bottomrule
\end{tabular}
\label{tab:hyperparams_dm_dsa}
\end{wraptable}
\paragraph{DM \& DSA} For DM and DSA defenses, it is crucial to adjust the learning rate and training iterations based on different synthetic dataset initialization methods. DSA, involving a dual-layer optimization process, additionally requires specification of the number of both the outer and inner loops. \Cref{tab:hyperparams_dm_dsa} provides the hyperparameter settings for DM and DSA. All other training settings align with those outlined in their original publications~\citep{dm, dsa}. 

\paragraph{MTT \& DATM} For MTT and DATM, training of the teacher trajectory precedes the optimization of synthetic data. We extend the trajectory length for MTT to 100 epochs while keeping other hyperparameters and settings consistent with those described in the original papers~\citep{cazenavette2022dataset, guo2024lossless}. \Cref{tab:hyperparams_mtt_datm} shows the hyperparameters during the synthetic dataset optimization process. All other training parameters remain in alignment with those specified in the respective original methods. ZCA whitening is used in all experiments for MTT and DATM by default. Considering the heavy reliance of MTT and DATM on GPU memory, we adopted the TESLA~\citep{tesla} implementation of DATM and also re-implemented MTT as a TESLA version, following recommendations in~\citep{guo2024lossless}.


\begin{table}[t]
\centering
\caption{Hyperparameters used for the synthetic dataset optimization process in MTT and DATM.}
\renewcommand\arraystretch{1.2}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\textbf{Methods} &
  \textbf{Iteration} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Synthetic\\ Steps\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Expert\\ Epochs\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Min Start\\ Epoch\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Max Start\\ Epoch\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Synthetic\\ Batch Size\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}LR \\ (Pixels)\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}LR\\ (Labels)\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}LR\\ (Step Size)\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Starting\\ Step Size\end{tabular}} \\ \midrule
MTT (Private)  & 1,500  & 100 & 2 & -  & 40 & 200   & 100 & -  & 1e-6 & 0.01 \\
MTT (OOD)      & 20,000 & 100 & 2 & -  & 40 & 200   & 100 & -  & 1e-6 & 0.01 \\
DATM (Private) & 5,000  & 100 & 2 & 40 & 60 & 1,000 & 50  & 10 & 1e-6 & 0.01 \\ \bottomrule
\end{tabular}}
\label{tab:hyperparams_mtt_datm}
\end{table}


\subsection{Data-free Knowledge Distillation}
In line with the protocol described in~\citep{fang2022up}, using only the ``BN'' loss is able to achieve high accuracy. This loss function matches the batch-normalization statistics from the teacher model, which is widely used in most DFKD works. Given its effectiveness, we use this method exclusively to generate synthetic data. This simplifies our approach and allows us to generalize our evaluation to other DFKD methods.

The training protocol in~\Cref{assec:undefended} is adopted to train the teacher model. After that, we perform the distillation process involving 240 iterations. In each iteration, we generate 256 new images, collect predictions from the teacher model, and store these in a memory bank. We then train the student model for 5 epochs using the data in the memory bank.

\subsection{Diffusion Model}
Due to the substantial computational costs associated with Real-Fake~\citep{realfake}, we have made slight adjustments to our experimental setup. In all experiments involving diffusion, we reduced the number of sampling steps during the inference stage to 10. Furthermore, we decreased the number of shadow models from 32 to 16. These modifications allow us to reduce computational expenses while still obtaining meaningful conclusions. We only generate synthetic dataset equal in size to the original private dataset. The training process of shadow models is the same as in~\Cref{assec:undefended}.

\subsection{DP-SGD}
The two heuristic DPSGD baselines without provable privacy guarantees in~\Cref{tab:cmp_dpsgd} are developed following~\citep{aerni2024evaluations}, building upon the state-of-the-art DPSGD training techniques~\citep{de2022unlocking, sander2023tan}. Here, we replace the batch normalization in ResNet-18 with group normalization~\citep{wu2018groupnorm}, swap the order of normalization and ReLU. We carefully tune the hyperparameters according to the scaling law in~\citep{sander2023tan} to achieve medium utility and high utility~(see~\Cref{tab:hyperparams_dpsgd}). {Finally, the privacy budgets are $\epsilon \approx 1.8\times 10^8$ for high-utility DPSGD and $\epsilon \approx 4.4\times 10^7$ for medium-utility DPSGD.}

\begin{table}[b]
\centering
\caption{Hyperparameters of heuristic DPSGD baselines with medium utility and high utility.}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lllllll@{}}
\toprule
\textbf{Method} & \textbf{Noise multiplier} & \textbf{Clipping norm} & \textbf{Batch size} & \textbf{Epochs} & \textbf{LR} & \textbf{Augment} \\ \midrule
DPSGD (medium)  & 0.00625                   & 1                      & 64                  & 4               & 4           & 8                \\
DPSGD (high)    & 0.00625                   & 1                      & 64                  & 16              & 4           & 8                \\ \bottomrule
\end{tabular}}
\label{tab:hyperparams_dpsgd}
\end{table}


\section{Additional Experiments and Analysis}

\subsection{Privacy Leakage in DD with Private Initialization}
When using Private initialization, a portion of mislabeled canaries is inadvertently selected as the starting images for synthesizing categories, e.g.,~a cat mistakenly labeled as a dog could be used for initializing dog category images. 
% We visualize the changes before and after the distillation process for all DD methods with private initialization {\color{blue} in the Appendix.} 
As discussed in~\citep{guo2024lossless}, under high ipc settings, the images optimized through DD remain visually similar to the original images. It is important to emphasize that visual similarity should not be deemed the determinant of privacy leakage; however, such similarity does indicate potential privacy risks. 

We define True Positive samples at an FPR of 0.1\% as those successfully attacked, and divide the training set’s canaries into init canaries and non-init canaries based on their selection for initialization. \Cref{tab:dd_init} shows the attack success rates for these two categories under different DD defenses. It is evident that the success rate on init canaries is significantly higher than on non-init canaries. For canaries in the training set but not used for initialization, the success rate is nearly equivalent to random guessing. This empirically supports our assertion that privacy risks are predominantly introduced during the initialization phase rather than the distillation process.


\subsection{Privacy Leakage During the Distillation Process in DD}
We then perform a case-by-case analysis to explain why the distillation processes of these DD defenses do not leak the membership privacy of mislabeled canaries. 

DM matches the means of neural network-encoded representations of real and synthetic data class by class. During the optimization of synthetic data, the neural network is randomly sampled from the parameter space and undergoes only forward propagation without updating its parameters, thus avoiding any memorization effects related to mislabeled canaries. Given that mislabeled samples constitute a minor fraction of the training data (about 1\%), their influence on the average representation of the original real data is negligible. Therefore, the update process of synthetic data is virtually unaffected by the canaries. 

However, defenses such as DSA, MTT, and DATM initially require training models on original private data to obtain teacher gradients or parameters. Subsequently, models are trained from the same starting point using synthetic data, aiming to elicit similar parameter updates from both real and synthetic data. The privacy risk here stems from the potential retention of canary-related memories in the gradient or parameter trajectories during the teacher model's training process. The optimization goal previously mentioned could lead to models trained on the final synthetic dataset inheriting strong memories of the canaries.

\setlength\intextsep{0pt}
\begin{wrapfigure}{r}{0.45\textwidth}
\begin{center}
\includegraphics[width=\linewidth]{figure/mem_effect_ConvNet.pdf}
\end{center}
\caption{The TPR@0.1\%FPR of 500 mislabeled canaries during the training process of teacher trajectories.}
\label{fig:mem_effect}
\end{wrapfigure}
Fortunately, we find that model memories of mislabeled canaries do not form rapidly (see~\Cref{fig:mem_effect}). In early training epochs, the model have not formed strong memorization to those mislabeled canaries. This finding may provide a reasonable explanation for why there is no privacy leakage during the synthetic data optimization processes in DSA, MTT, and DATM.

DSA randomly samples a neural network at each optimization epoch and trains it incrementally with both synthetic and real data, minimizing the difference between their gradients to update the synthetic images. Since the number of training steps is generally small, the teacher gradients do not yet contain effective memories of the canaries. 

Trajectory-based methods like MTT and DATM first train a teacher model on original private data and save the training trajectory. In each optimization epoch, a network is selected from the teacher trajectory and trained for several steps with synthetic data to match the trained model parameters to the teacher trajectory. However, both MTT and DATM match only the early to mid-training trajectories, during which neither the model parameters in the teacher trajectory have formed strong memories of the canaries. Hence, these gradient and trajectory-matching optimization processes prevent models trained on synthetic data from retaining memories of the canaries, thereby safeguarding member privacy. This also explains why using noise and OOD images for initialization can achieve decent privacy protection. 


\subsection{Visual Dissimilarity Between Private Data and Data Synthesized by DD with OOD Initialization}
Following the previous works~\citep{somepalli2023diffusion, guo2024lossless}, we employ the Self-Supervised Content Duplication (SSCD)~\citep{sscd} method for content plagiarism detection. We use the ResNet50 model trained on the DISC dataset~\citep{disc}. For a given query synthetic image and all reference images from the original private dataset, we infer through the detection model to obtain a 512-dimensional feature vector for each image. The similarity between them is calculated by computing the inner product between the query feature and each reference feature. For each synthetic image, we present the corresponding private images with top 3 similarities.

We select some synthetic images from DM and MTT, both with OOD initialization, and attempt to retrieve similar images from the original private dataset. The results are shown in~\Cref{fig:dm_cinic_sscd} and~\Cref{fig:mtt_cinic_sscd}, respectively. It is obvious that for DD methods with OOD initialization, the synthetic data is visually dissimilar to those private data.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/DM_cinic_sscd.pdf}
    \caption{Visualization on synthetic data generated by DM (OOD) and retrieved private data with SSCD. The synthetic data are with orange border. The synthesized data does not exhibit evident visual similarity to any of the private data.}
    \label{fig:dm_cinic_sscd}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/mtt_cinic_sscd.pdf}
    \caption{Visualization on synthetic data generated by MTT (OOD) and retrieved private data with SSCD. The synthetic data are with orange border. The synthesized data does not exhibit evident visual similarity to any of the private data.}
    \label{fig:mtt_cinic_sscd}
\end{figure}

\subsection{Detailed Analysis of The Privacy Leakage in DFKD}
In this section, we investigate how membership information of mislabeled canaries is transferred from the teacher to the student model through the use of dissimilar synthetic data. As demonstrated in~\Cref{fig:dfkd_analysis}, the teacher model can strongly memorize the incorrect labels associated with mislabeled samples, resulting in a logits distribution where the value of wrongly-labeled class significantly exceed that of the correct labels. The generator in DFKD is capable of producing synthetic images that, while visually distinct, bear a strong resemblance in logits to the mislabeled data. These synthetic images activate the memories of corresponding canaries encoded in the teacher model's parameters. The distillation process attempts to align the student model’s logits distribution with that of the teacher for the same images. Consequently, the student model may inherit similar parameters to the teacher, including some aspects of the teacher’s memorization of mislabeled data. 


\subsection{Detailed Analysis of Figure 9}
\Cref{fig:dfkd_analysis} reveals that, despite having no direct exposure to the original mislabeled data, the student model still inherits a portion of the teacher model’s memory, exhibiting high logit value for the mislabel. Notably, the student model's memorization of mislabels is comparatively weaker than teacher. Therefore, influenced by its generalization capabilities, the student model tends to classify canaries under their correct labels rather than the mislabels. 

We assessed the accuracy with which both teacher and student models classify the training set's canaries into their respective incorrect labels. The results were 100\% for the teacher and 5.28\% for the student, indicating that using accuracy as the metric to evaluate whether a model remembers mislabels is misleading. Even if the logits for a mislabeled class are not the highest, their magnitude may still suffice for effective LiRA. 

\section{Evaluation Setup of Figure 3 and Figure 4}
\label{asec:eval_setup}

{For~\Cref{fig:avg_and_vulnerable}, the experiments were conducted on CIFAR-10, where we used 500 mislabeled samples to simulate the most vulnerable data. The attack settings follow those of LiRA. For each defense, we trained 16 models, ensuring that each sample appeared in the training set of only half of the models. We used leave-one-out cross-validation—each time using one model as the victim and the remaining 15 models as the attacker's shadow models. The shadow models were implemented using a ConvNet architecture. We evaluated privacy protection levels for both the average case and the worst case.}

{\Cref{fig:loss_distribution} follows the same experimental setting as the undefended baseline without any defense, described in~\Cref{assec:undefended}, except that we set the number of shadow models to 256 instead of 32. For the ``average-case'' subfigure, we selected a normal sample and plotted its loss distributions when it was a member and when it was not. For the ``most vulnerable'' subfigure, we selected a mislabeled canary and did the same.}

\end{document}
