\section{Related Work}
\paragraph{Membership Inference Attack}
Membership inference attack~\citep{shokri2017membership} is a canonical approach to estimating privacy leakage, which aims to determine whether a given example was part of the training set. State-of-the-art attack methods frame membership inference as a hypothesis testing problem and evaluate privacy leakage by reporting the true positive rate at very low false positive rates~\citep{lira}. Recent work~\citep{aerni2024evaluations} suggests that many empirical defenses fail to evaluate privacy leakage in worst-case scenarios; instead, they often report average-case privacy leakage, which can significantly underestimate actual privacy risks. Since privacy is not an average metric, we adopt this approach to provide a rigorous and careful evaluation of privacy. 


\paragraph{Training on Synthetic Data} 
Many studies have highlighted that training models on synthetic data can be remarkably effective in scenarios where data collection or distribution is challenging. 
For instance, in long-tailed learning and federated learning, generative models can be used to augment data, resulting in a more balanced distribution~\citep{shin2023fill,fgl}. Similarly, in continual learning, synthetic data is adopted to help mitigate the problem of catastrophic forgetting~\citep{diffclass,zhang2023target,shin2017continual}.
In scenarios where only the target model is available without access to its private training data, generative models can be used to create a synthetic dataset to train a substitute model, which can achieve similar performance to the target model~\citep{zhang2023ideal,lopes2017data}. Furthermore,~\citet{realfake} demonstrated that a model trained solely on synthetic data can perform well on the ImageNet test set. \citet{cazenavette2022dataset,guo2024lossless} showed that even when the original training set is condensed to just 1\% of its size as synthetic data, it is still possible to train a well-performing model on CIFAR10 test set. 
These examples illustrate the effectiveness of synthetic data, but in this work, we are more curious about whether these models trained with synthetic data actually protect privacy.

A related study~\cite{aerni2024evaluations} rigorously evaluates several empirical membership inference defenses, including techniques such as label smoothing~\cite{chen2024hamp}, training loss calibration~\cite{chen2022relaxloss}, and self-supervised learning. While our experiments build on their evaluation framework, we extend this work by specifically investigating empirical methods that train ML models using \textit{synthetic data}.