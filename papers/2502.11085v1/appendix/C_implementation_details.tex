\section{Implementation Details}
% QMOF we used a LR 5e-5 since the pretrained model on transition was giving NaN when using 8e-5. 
For both pretraining and fine-tuning experiments, we primarily follow the JMP hyperparameters. However, due to resource constraints requiring smaller batch sizes compared to JMP, we adjusted the learning rate to ensure training stability, as detailed below.

\textbf{For pretraining,} we use a batch size of 20 and a learning rate (LR) of 1e-4 for the small backbone (GemNet-OC-S). For the large backbone (GemNet-OC-L), the batch size is reduced to 12 to fit GPU memory. Additionally, when training with the OC22 dataset on the large backbone, a LR of 1e-4 caused gradient instability, thus we used a LR of 1e-5 for that particular run. Unless otherwise specified, each experiment is run for five epochs on the specified number of samples for each section of the paper. The best checkpoint is selected based on the performance in the validation set. To handle the large size of the upstream validation sets, validation is performed on a smaller subset of 2,000 samples.


\textbf{For finetuning,} we use the batch size specified in the JMP codebase and a default learning rate (LR) of 8e-5, except for cases where adjustments were needed to stabilize training. Specifically, we use 5e-5 for QMOF, 8e-4 for MatBench when pretrained on Transition1x. 
% 4e-5 for MD22 when using the large backbone with ANI-1x pretraining.

\clearpage

