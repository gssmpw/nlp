\section{More Epochs or More Data?}

In this section, we explore the trade-off between increasing the number of training epochs and expanding the dataset size under a fixed computational budget. Specifically, we aim to answer the following question:

\highlighttext{Given a fixed computational budget, is it more effective to train on a smaller dataset for more epochs or to train on a larger dataset for fewer epochs?}

\textbf{Setup.} To investigate this question, we compare two scenarios under the same computational budget of \(10M\) samples: (1) training on \(2M\) samples for 5 epochs, and (2) training on \(1M\) samples for 10 epochs. We evaluate the performance of models pretrained on ANI-1x, Transition-1x, OC20, and OC22, and fine-tune them on the downstream datasets: rMD17, MD22, SPICE, and QM9. For comparison, we also include the results of JMP-L and JMP-S, which use \(120M\) samples for 2 epochs.

\textbf{Results.} Table \ref{tab:rehearsal} presents the downstream performance for the two scenarios. Across all datasets, ANI-1x consistently achieves the best performance, regardless of whether the model is trained on \(2M\) samples for 5 epochs or \(1M\) samples for 10 epochs. For example, on rMD17, ANI-1x achieves a test error of 5.4 in both scenarios, outperforming JMP-S (6.7) and JMP-L (5.1). Similarly, on SPICE, ANI-1x achieves a test error of 5.08 (2M samples, 5 epochs) and 5.04 (1M samples, 10 epochs), compared to 5.71 for JMP-S and 4.75 for JMP-L.

Interestingly, increasing the number of epochs from 5 to 10 while reducing the dataset size from \(2M\) to \(1M\) does not significantly degrade performance for ANI-1x. This suggests that for highly relevant datasets like ANI-1x, training on fewer samples for more epochs can be as effective as training on more samples for fewer epochs. However, for less relevant datasets like OC20 and OC22, increasing the number of epochs does not compensate for the reduced dataset size, as their performance degrades significantly.

\textbf{Conclusion.} Our findings indicate that the choice between more epochs or more data depends on the relevance of the dataset to the downstream task. For highly relevant datasets like ANI-1x, training on fewer samples for more epochs can yield comparable or even superior performance, while for less relevant datasets, increasing the dataset size is more beneficial. This further underscores the importance of dataset quality and relevance, as quantified by CSI, in determining the optimal pretraining strategy.


\input{tables/rehearsal}


\clearpage

