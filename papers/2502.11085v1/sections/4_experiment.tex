\input{tables/main_results_ID}
\section{Experiments}
\label{4_experiment}
We evaluate the impact of pretraining on different upstream datasets for downstream performance and investigate how well the CSI values in Figure \ref{fig:main_CSI_score} reflect the relevance of these datasets. We begin by defining the datasets, baselines, and evaluation setup.



\noindent\textbf{Upstream Datasets:} Following JMP~\cite{shoghimolecules}, we perform pretraining on upstream datasets of small molecules, including ANI-1x~\cite{smith2020ani} and Transition-1x~\cite{schreiner2022transition1x}, as well as large-scale catalysis datasets, OC20~\cite{chanussot2021open} and OC22~\cite{tran2023open}. These datasets vary in domain focus and graph size, enabling us to examine how these factors impact the generalization of pretraining across downstream tasks. The ground-truth labels, energy and forces, are computed using Density Functional Theory (DFT).

\noindent\textbf{Downstream Datasets:} For downstream evaluation, we focus on in-distribution (ID) tasks involving energy or force prediction, following the definition in JMP~\cite{shoghimolecules}. We discuss out-of-distribution (OOD) tasks in Section \ref{5_OOD}. Given the large number of pairs for ID evaluation, we focus on the first molecule for force tasks and the energy target for the multi-property dataset QM9~\cite{wu2018moleculenet}. The selected targets and their corresponding datasets are: Aspirin in rMD17~\cite{christensen2020role}, Ac-Ala3-NHMe in MD22~\cite{chmiela2023accurate}, solvated amino acids in SPICE~\cite{eastman2023spice}, and $U_0$ 
in QM9~\cite{wu2018moleculenet}. 
% For OOD evaluations, we use the Band Gap property from the QMOF dataset~\cite{rosen2021machine} and the first OOD label, Phonons, from MatBench~\cite{dunn2020benchmarking}.

\noindent\textbf{Baselines:} 
We report the original performance of JMP, where "JMP-S" and "JMP-L" correspond to the small and large backbones, respectively. Additionally, we present our reproduced fine-tuning results using the official JMP checkpoints, denoted as "JMP-S\textsuperscript{*}" and "JMP-L\textsuperscript{*}".

For our budgeted evaluation, we present results in two categories: pretraining on a single upstream dataset and pretraining on a mixture of all datasets. For single-dataset experiments, we randomly sample \(\mathcal{N}\) instances from the original upstream data. For mixed pretraining, we construct the training set using two different strategies. (1) Balanced Sampling, where an equal number of samples is drawn from each of the four upstream datasets, totaling \(\mathcal{N}\) samples; and (2) Temperature-Based Sampling, which preserves the dataset proportions used in the full 120M sample set of JMP~\cite{shoghimolecules}. 


% \noindent\textbf{Evaluation Setup:} We set a computational budget of 10M total training samples to ensure that the experiments are accessible and reproducible for researchers. Each pretraining run completes within 1–2 days on an A100 GPU. To minimize computational costs, we use the lightweight GemNet-OC-S backbone~\cite{gasteiger2022gemnet}. From the pretraining stage, we obtain a checkpoint for each upstream dataset. These pretrained models serve as starting points for fine-tuning on the downstream tasks, allowing us to evaluate the relevance of each upstream dataset to the downstream tasks. For fine-tuning, we use the same number of epochs as the baseline JMP-S to match the original performance reported in JMP~\cite{shoghimolecules} for each downstream task considered.

\noindent\textbf{Evaluation Setup:} We pretrain the GemNet-OC-S model~\cite{gasteiger2022gemnet} on each individual upstream dataset, as well as on mixed variants defined in the baselines. For our main experiments, we set a fixed computational budget of \( \mathcal{C} = 10 \times 10^6 \), achieved by training on \( \mathcal{N} = 2 \times 10^6 \) samples for \( \mathcal{E} = 5 \) epochs. This budget ensures accessibility and reproducibility, with each pretraining run completing within 1–2 days on an A100 GPU. This represents a \( 24\times \) reduction in computational cost compared to the pretraining budget used in JMP~\cite{shoghimolecules}. Additional budget configurations are explored in later sections and the appendix. Each pretrained model is then fine-tuned separately on each downstream task.

% \textcolor{blue}{Two findings: (1) There exists smaller subsets surprisingly it's always Ani that outperofrms using mixed data or other upstreams and can outperform even large scale inefficient pretraining 120M x2 epochs .. Mixed Sampling proposed in JMP might not be optimal (2) CSI is a good correlation measure between performance and fid ssore.}

\subsection{Does CSI Correlate with Better Performance?}

In Section \ref{3_setup}, we introduced the Chemical Similarity Index (CSI) and presented CSI values. The results indicate that ANI-1x exhibits the highest similarity to all downstream datasets. This finding raises a critical question:

\highlighttext{Can CSI reliably guide the selection of pretraining datasets to achieve optimal performance on specific downstream tasks?}

\input{tables/budget}
\input{tables/large_backbone}

Table \ref{tab:main_results_id} summarizes the downstream performance of models pretrained on different datasets in an in-distribution setting. Notably, the ANI-1x model consistently outperforms all other individual datasets and even mixed variants. For instance, on both rMD17, SPICE and QM9 datasets, ANI-1x achieves an MAE of 5.4, 5.13 and 2.9, compared to 6.7, 5.71 and 3.3 for JMP-S. Remarkably, this performance is achieved with \( 24\times \) less computational budget than JMP-S. Additionally, pretraining on alternative upstream tasks results in weaker performance compared to ANI-1x, correlating with the trends observed in our CSI values.

Furthermore, temperature-based mixing, which follows the JMP formulation and prioritizes high-CSI datasets (OC20 and OC22), performs worse. In contrast, balanced mixing improves slightly by incorporating more samples from the more relevant ANI-1x and Transition-1x but remains inferior to individual pretraining on ANI-1x. These results indicate that, under a limited budget, mixing upstream datasets with varying CSI values is suboptimal and demands significantly more computational resources for convergence.


\textbf{Takeaway.} Our experiments highlight three key insights for in-distribution downstream tasks: (1) High-quality, task-relevant upstream datasets like ANI-1x outperform larger, mixed datasets, when included within the mix. (2) Mixing upstream datasets can match the benefits of highly relevant pretraining but requires significantly more compute and training time.  (3) CSI effectively predicts downstream performance, as lower CSI values (e.g., ANI-1x) consistently correlate with better results.


% supporting our hypothesis that a carefully selected subset from the mixture of upstream tasks can deliver strong performance while significantly reducing computational cost.

\subsection{What is the Effect of Computational Budget?}

Building on our earlier findings, we now investigate how varying the computational budget impacts downstream performance. Specifically, we ask:

\highlighttext{Do our findings about dataset relevance in terms of CSI hold across different budget levels?}

% \textbf{Setup.} We evaluate additional pretraining budgets of \(1M\), and \(3M\) samples (5 epochs each) compared to the \(2M\).
% on ANI-1x, Transition-1x, OC20, and OC22, comparing them to JMP-L and JMP-S, which use \(120M\) samples (2 epochs). All models are fine-tuned on rMD17, MD22, SPICE, and QM9.

Table \ref{tab:budget} presents the downstream performance across additional pretraining budgets of \(1M\), and \(3M\) samples (5 epochs each) compared to \(2M\). ANI-1x remains superior to other datasets across all budgets, aligning with its low CSI. Increasing the budget from \(1M\) to \(2M\) samples generally improves performance; however, further scaling to \(3M\) samples results in diminishing returns on rMD17, SPICE, and QM9, with only a slight improvement on MD22. This occurs despite the pretraining loss for \(3M\) being better than \(2M\), highlighting that improved pretraining performance does not always translate to better downstream results.

\textbf{Takeaway.} Our findings are consistent across budget levels: the upstream dataset with the lowest CSI yields the best downstream performance.
% ANI-1x remains the superior dataset, while large-scale OC20 and OC22 underperform. This highlights the importance of dataset quality and relevance, as captured by CSI, over sheer scale.

\subsection{What is the Effect of Changing the Architecture Size?}
In the previous sections, we used the small variant, GemNet-OC-S, as our backbone. Here, we address the question:

\highlighttext{Does the correlation between CSI and downstream performance hold across different architecture sizes?}

% \textbf{Results.} 
Table \ref{tab:large} reports the downstream performance when using the large variant, GemNet-OC-L, as the backbone. Consistent with the results on the small backbone, models pretrained on ANI-1x achieve the best performance, correlating well with the CSI values. Notably, we achieve state-of-the-art results with an MAE of 4.8 on Aspirin in rMD17 and 2.6 on $U_0$ in QM9, even though our reproduced JMP-L baseline results fall short of those reported in the original JMP paper. 

\textbf{Takeaway.} Our findings hold across backbone sizes, showing the potential of relevance-based upstream dataset selection over large-scale mixing for improved performance.

\input{figures/high_and_low_CSI}
\input{tables/ood}
\input{figures/CSI_score_OOD}

\subsection{Is More Data Always Better?}
The common pretraining paradigm assumes that larger and more diverse datasets lead to better generalization. Here, we challenge this assumption from another perspective:

\highlighttext{Does adding pretraining data from less relevant sources improve or degrade downstream performance?}

To evaluate this, we pretrain on a mixture of \(2M\) samples from the best CSI dataset (ANI-1x) and \(1M\) from OC22, a non-aligned upstream dataset. We then compare this pretraining checkpoint with the \(2M\)-sample baseline in Figure \ref{fig:high_low_CSI}. Surprisingly, adding OC22 degraded downstream performance despite the increased pretraining budget. 

\textbf{Takeaway.} Including additional pretraining data from less relevant sources can harm downstream performance. Our findings highlight the CSI metric as a practical tool for designing effective pretraining datasets.





%


