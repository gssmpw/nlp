\section{Formulation and Setup}
\label{3_setup}

In this section, we present our problem setup, notion of a computational budget, and the a formulation of dataset similarity. We then detail how we adapt the Fréchet Inception Distance (FID) to the molecular domain, yielding the \emph{Chemical Similarity Index (CSI)}. Our setup is illustrated in Figure \ref{fig:main_figure}. Throughout this work, we use the term 'molecular' broadly to encompass both molecular and materials domains, as well as their respective datasets.
% Throughout this work, we use the term \emph{chemistry datasets} to refer both to \emph{molecular datasets} (e.g., QM9~\cite{wu2018moleculenet}, rMD17~\cite{christensen2020role}) and to \emph{catalysis and material datasets} (e.g., OC20~\cite{chanussot2021open}, OC22~\cite{tran2023open}, QMOF~\cite{rosen2021machine}). Molecular datasets typically focus on atomic interactions and properties at the molecular scale, while material datasets frequently involve periodic structures, surfaces, or catalytic materials.

\subsection{Formal Setting}

\paragraph{Upstream and Downstream Datasets.} Let \(\{\mathcal{D}_{u}^{(1)}, \mathcal{D}_{u}^{(2)}, \ldots, \mathcal{D}_{u}^{(K)}\}\) denote a collection of \(K\) \emph{upstream} (pretraining) datasets, each containing molecular structures paired with relevant atomic properties (e.g., energies and forces). In the typical paradigm, the user aggregates all upstream datasets into one dataset for pretraining:
\[
\mathcal{D}_u = \bigcup_{i=1}^{K} \mathcal{D}_u^{(i)}.
\]
We further define \(\mathcal{D}_{d}\) as the \emph{downstream} dataset, which focuses on a specific prediction task (e.g., predicting an atomic property).



\paragraph{Multi-task Pretraining.}  
We consider a neural network \(\Phi(\cdot; \theta)\), where \(\theta\) encompasses the shared backbone parameters \(\theta_b\) and task-specific head parameters \(\theta_e\) (for energy prediction) and \(\theta_f\) (for force prediction). During pretraining, the network is trained to simultaneously predict energies and forces. Formally, the multi-task pretraining objective over an upstream dataset \(\mathcal{D}_u^{(i)}\) is given by:
\begin{equation}
\theta^{(i)*} \;=\; \arg\min_{\theta} \;\mathcal{L}_{\text{pretrain}}(\theta; \mathcal{D}_{u}^{(i)}),
\label{eq:pretrain-argmin}
\end{equation}
where \(\theta = \{\theta_b, \theta_e, \theta_f\}\) and
\begin{equation}
\mathcal{L}_{\text{pretrain}}(\theta; \mathcal{D}_{u}^{(i)}) \;=\; 
\alpha \,\mathcal{L}_{\text{energy}}(\theta_b, \theta_e; \mathcal{D}_{u}^{(i)}) 
\;+\; 
\beta \,\mathcal{L}_{\text{forces}}(\theta_b, \theta_f; \mathcal{D}_{u}^{(i)}).
\label{eq:pretrain-loss}
\end{equation}
Often, \(\mathcal{L}_{\text{energy}}\) is computed using the Mean Absolute Error (MAE), while \(\mathcal{L}_{\text{forces}}\) uses the mean per-atom Euclidean (L2) distance. Coefficients \(\alpha\) and \(\beta\) weight the importance of energy and force tasks, respectively, with \(\beta > \alpha\) often chosen to prioritize accurate force predictions in atomistic modeling. Usually, pretraining is performed on the joint upstream dataset $\mathcal{D}_u$ or a subsampled version.

\paragraph{Fine-Tuning.}  
After the multi-task pretraining phase, the task-specific heads \(\theta_e\) and \(\theta_f\) are discarded, and a new task-specific head \(\theta_h\) is attached to the pretrained backbone \(\theta_b\). The downstream objective then becomes:
\begin{equation}
\theta_{d}^{*} \;=\; \arg\min_{\theta_b, \theta_h} \; \mathcal{L}_{\text{finetune}}(\theta_b, \theta_h; \theta_b^{(i)*}, \mathcal{D}_{d}),
\label{eq:finetune-argmin}
\end{equation}
where \(\theta_b^{(i)*}\) denotes the pretrained backbone parameters from Eq.~(\ref{eq:pretrain-argmin}).
% \begin{equation}
% \mathcal{L}_{\text{finetune}}(\theta_b, \theta_h; \theta_b^{(i)*}, \mathcal{D}_{d}) 
% %\;=\; 
% %\mathrm{MAE}\bigl(\Phi(\cdot; \theta_b, \theta_h), \,\mathcal{D}_{d}\bigr).
% \label{eq:finetune-loss}
% \end{equation}
Intuitively, the downstream training refines the shared backbone parameters \(\theta_b\) and learns the task-specific head \(\theta_h\) to capture the target property in \(\mathcal{D}_d\).


% \paragraph{Multi-task Pretraining} We consider a multi-head neural network \(\Phi(\cdot; \theta)\), where \(\theta\) encompasses a set of shared base parameters. Different output heads are attached to handle multiple tasks (e.g., energy and force prediction) during pretraining. Formally, the multi-task pretraining objective over an upstream dataset \(\mathcal{D}_u^{(i)}\) is given by:
% \begin{equation}
% \theta_{u}^{(i)*} \;=\; \arg\min_{\theta} \;\mathcal{L}_{\text{pretrain}}(\theta; \mathcal{D}_{u}^{(i)}),
% \label{eq:pretrain-argmin}
% \end{equation}
% where
% \begin{equation}
% \mathcal{L}_{\text{pretrain}}(\theta; \mathcal{D}_{u}^{(i)}) \;=\; 
% \alpha \,\mathcal{L}_{\text{energy}}(\theta; \mathcal{D}_{u}^{(i)}) 
% \;+\; 
% \beta \,\mathcal{L}_{\text{forces}}(\theta; \mathcal{D}_{u}^{(i)}).
% \label{eq:pretrain-loss}
% \end{equation}
% Here, \(\mathcal{L}_{\text{energy}}\) and \(\mathcal{L}_{\text{forces}}\) typically use the Mean Absolute Error (MAE). Coefficients \(\alpha\) and \(\beta\) weight the importance of energy and force tasks, respectively, with \(\beta > \alpha\) often chosen to prioritize accurate force predictions in atomistic modeling. Usually, pretraining is performed on the joint upstream dataset $\mathcal{D}_u$ or a subsampled version.


% \paragraph{Fine-Tuning} After the multi-task pretraining phase, the specialized (energy/force) output heads are discarded, and a new task-specific head is attached to the shared parameters \(\theta\). The downstream objective then becomes:
% \begin{equation}
% \theta_{u,d}^{(i)*} \;=\; \arg\min_{\theta} \; \mathcal{L}_{\text{finetune}}(\theta; \theta_{u}^{(i)*}, \mathcal{D}_{d}),
% \label{eq:finetune-argmin}
% \end{equation}
% where \(\theta_{u}^{(i)*}\) denotes the pretrained parameters from Eq.~(\ref{eq:pretrain-argmin}) and
% \begin{equation}
% \mathcal{L}_{\text{finetune}}(\theta; \theta_{u}^{(i)*}, \mathcal{D}_{d}) 
% \;=\; 
% \mathrm{MAE}\bigl(\Phi(\cdot; \theta), \,\mathcal{D}_{d}\bigr).
% \label{eq:finetune-loss}
% \end{equation}
% Intuitively, the downstream training refines the shared parameters to capture the target property in \(\mathcal{D}_d\).

\textit{In this paper, we consider two additional needed definitions for this setting: (1) computational budget and (2) similarity metrics.}

\paragraph{Computational Budget.} Following \citet{hammoud2024pretraining}, we define the \emph{computational budget} \(\mathcal{C}\) to be the product of the number of epochs \(\mathcal{E}\) and the number of unique samples \(\mathcal{N}\) in the pretraining dataset:
\begin{equation}
\mathcal{C} \;=\; \mathcal{E} \;\times\; \mathcal{N}.
\end{equation}
Hence, the computational budget \(\mathcal{C}\) represents the total number of samples processed over training. It naturally splits into two factors: the dataset size (\(\mathcal{N}\)) and the number of passes through it (\(\mathcal{E}\)). The choice of \(\mathcal{C}\) depends on the available computing resources. In our experiments, we explore multiple pretraining budget settings to examine how trading off \(\mathcal{N}\) against \(\mathcal{E}\) under a fixed \(\mathcal{C}\) affects both pretraining and downstream performance.

\paragraph{Dataset Similarity.} 
A key objective of this work is to estimate how well an upstream dataset \(\mathcal{D}_u\) aligns with a downstream dataset \(\mathcal{D}_d\). We therefore seek a distance metric
\[
\delta(\mathcal{D}_u, \mathcal{D}_d)
\]
that quantifies their relevance or ``similarity.'' In principle, a smaller value \(\delta(\mathcal{D}_u, \mathcal{D}_d)\) indicates a greater overlap or relevance. Thus, among multiple candidate upstream datasets \(\{\mathcal{D}_{u}^{(1)}, \ldots, \mathcal{D}_{u}^{(K)}\}\), the one that minimizes 
% \[
% \arg \min_{1 \le k \le K} \,\delta\!\bigl(\mathcal{D}_{u}^{(k)}, \mathcal{D}_d\bigr)
% \]
\[
\underset{1 \leq i \leq K}{\operatorname{argmin}} \, \delta\bigl(\mathcal{D}_{u}^{(i)}, \mathcal{D}_d\bigr)
\]
should provide the most effective pretraining for \(\mathcal{D}_d\). In this paper, we empirically test this assumption, examining whether lower \(\delta\)-values indeed correlate with improved downstream performance. This motivates using \(\delta\) as a principled metric to select or combine upstream datasets under a fixed computational budget.



% \paragraph{Dataset Similarity} The primary objective of this work is to estimate how well an upstream dataset \(\mathcal{D}_u\) aligns with a downstream dataset \(\mathcal{D}_d\), facilitate effective \emph{upstream dataset selection}. We seek a distance metric, 
% \[
% \delta(\mathcal{D}_u, \mathcal{D}_d)
% \]
% that quantifies the relevance or ``similarity'' between \(\mathcal{D}_u\) and \(\mathcal{D}_d\).
% \yasir{Up to this point, everything reads well, then now we need to explicitly formulate this as a selection problem }

% In principle, a lower distance \(\delta\) indicates higher overlap or relevance; thus, if
% \[
% \delta\bigl(\mathcal{D}_{u_1}, \mathcal{D}_{d_1}\bigr) 
% \;<\; 
% \delta\bigl(\mathcal{D}_{u_2}, \mathcal{D}_{d_2}\bigr),
% \]
% we expect downstream performance on \(\bigl(\mathcal{D}_{u_1}, \mathcal{D}_{d_1}\bigr)\) to exceed that of \(\bigl(\mathcal{D}_{u_2}, \mathcal{D}_{d_2}\bigr)\). Verifying this empirically supports \(\delta(\cdot,\cdot)\) as a principled guide to selecting or combining upstream datasets under a fixed computational budget.

\subsection{The Chemical Similarity Index (CSI)}


\paragraph{Recap of FID.} 
Our proposed \emph{Chemical Similarity Index (CSI)} draws its inspiration from the well-known Fréchet Inception Distance (\textsc{FID}) ~\cite{heusel2017gans}. Recall that FID is commonly used in computer vision to compare two sets of images via their feature distributions. Specifically, if one extracts features (e.g., from an Inception network) for datasets \(X\) and \(Y\) and denotes their empirical means and covariances by \(\mu_X, \Sigma_X\) and \(\mu_Y, \Sigma_Y\), then
\begin{equation}
\mathrm{FID}(X, Y)
\;=\;
\|\mu_X - \mu_Y\|^2
\;+\;
\mathrm{Tr}\Bigl(\Sigma_X + \Sigma_Y - 2(\Sigma_X \Sigma_Y)^{1/2}\Bigr).
\end{equation}
The central idea is to represent each sample in a feature space where distances encode semantic similarity and then compare the distributions of these representations for the two datasets.


\input{figures/CSI_score_main}

\paragraph{Design Considerations for CSI.} 
Adapting FID to the molecular domain involves several key design choices regarding how to best modify the metric:

\hspace{1em}\textbf{1. Feature Type:} The original FID metric for images uses features from the last layer before the head, relying on a single feature type. In contrast, molecular graph-based models often use two types of features: node and edge features. Models like GemNet-OC~\cite{gasteiger2022gemnet} retain distinct node and edge features before the head, while transformer-based models like EquiformerV2~\cite{liaoequiformerv2} integrate edge information into unified node embeddings. To ensure compatibility across backbones, we focus on node embeddings at the last backbone layer, as edge information is either explicitly maintained (e.g., GemNet-OC) or indirectly encoded into node representations (e.g., EquiformerV2).

\hspace{1em}\textbf{2. Aggregation Strategy:} FID in computer vision operates on fixed-size feature vectors, while molecular graphs have variable-sized embeddings. To adapt FID, we consider two strategies: flattening, which concatenates all node features into a single vector, and mean pooling, which averages node features for a graph-level representation. While mean pooling aligns with the original FID approach, it may oversmooth molecular features. In models like EquiformerV2~\cite{liaoequiformerv2} and GemNet-OC~\cite{gasteiger2022gemnet}, backbone node features are designed for further processing in the model's head before pooling. Since we extract features directly from the backbone, mean pooling at this stage risks losing feature diversity. Thus, we choose flattening to preserve the full feature distribution. CSI values using mean pooling are shown in the appendix.


\hspace{1em}\textbf{3. Sampling in Long-Tail Distributions:} To make feature extraction computationally feasible, we subsample a representative subset from each upstream and downstream dataset. However, molecular datasets often exhibit long-tail distributions, where common structures are overrepresented and rare ones underrepresented. These imbalances can skew mean and covariance estimates for our metric. For example, specific molecules may appear much more frequently than others, and in catalytic datasets, certain slab–adsorbate configurations can dominate~\cite{chanussot2021open}. To address this, we use a class-balancing sampling scheme to ensure a balanced representation of structures. Plots for long-tail distribution are in the appendix.

\textbf{Overall}, our CSI metric leverages node embeddings with flattening for aggregation and uses class-balanced sampling to address distribution imbalances. We subsample 10k instances from each upstream and downstream dataset for feature extraction to maintain computational feasibility. To ensure our metric remains independent of the baselines in this study, we extract features for our metric using EquiformerV2~\cite{liaoequiformerv2} pretrained on OC20~\cite{chanussot2021open}. For completeness, CSI values computed using the baseline JMP~\cite{shoghimolecules} are included in the appendix.



% ANI-1x~\cite{smith2020ani} and Transition-1x~\cite{schreiner2022transition1x}, as well as large-scale catalysis datasets, OC20~\cite{chanussot2021open} and OC22~\cite{tran2023open}.



\paragraph{CSI Between Upstream and Downstream Results.} In Figure \ref{fig:main_CSI_score}, we present the CSI values for pairs of upstream and downstream tasks related to energy and force predictions, with additional details about the datasets and targets provided in Section \ref{4_experiment}. ANI-1x~\cite{smith2020ani} consistently achieves the closest alignment across all downstream tasks, reflecting its design goal of maximizing chemical diversity. Transition-1x~\cite{schreiner2022transition1x}, which focuses on transition states, shows as the second most aligned dataset, suggesting that its emphasis on high-energy transition states leads to partial overlap with downstream distributions. In contrast, the catalysis datasets, OC20~\cite{chanussot2021open} and OC22~\cite{tran2023open}, exhibit the weakest alignment. While OC20 and OC22 are often favored for pretraining~\cite{shoghimolecules, kolluru2022transfer} due to their scale and chemical diversity, our metric suggests they may not align well with the considered downstream tasks. 
Next, we examine whether these alignment values correlate with downstream performance.
