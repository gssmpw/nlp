\section{Beyond In-Distribution}
\label{5_OOD}

Recall that our pretraining process is conducted on upstream tasks involving molecules and catalysts, with energy and force as targets. For downstream tasks with different labels (e.g., band gap in QMOF) or from distinct chemical domains such as materials (e.g., MatBench and QMOF), we classify these as out-of-distribution (OOD). While our main results focused on ID evaluation, here we explore our metric's applicability to OOD tasks. Specifically, we examine three cases: the Band Gap property from QMOF~\cite{rosen2021machine}, Phonons (the first non-energy target in JMP tables) from MatBench~\cite{dunn2020benchmarking}, and $\Delta_\epsilon$ from QM9, explicitly categorized as OOD in the JMP paper.

In Figure \ref{fig:CSI_OOD}, we present the CSI values for OOD domains, where the OOD label ($\Delta_\epsilon$) for QM9 follows the same values as in Figure \ref{fig:main_CSI_score}. We observe that QMOF exhibits a pattern similar to other ID domains shown in Figure \ref{fig:main_CSI_score}. However, MatBench displays a distinct pattern, showing strong correlation with OC20 and OC22, followed by ANI-1x and Transition-1x. Next, we analyze the correlation between CSI and downstream performance under OOD evaluation.

Table \ref{tab:ood} shows that $\Delta_\epsilon$ in QM9 aligns with the CSI pattern, similar to ID evaluation, suggesting that CSI is effective for OOD in the label space. In QMOF, the different upstream sources achieve similar performance which lags behind the full pretraining by JMP. For MatBench (evaluated over 5 folds), OC22 achieves the best mean performance while OC20 lags behind, despite our metric predicting both to be equally suitable. Additionally, for both QMOF and MatBench, mixed pretraining variants generalize better than individual sources. This suggests that when the downstream domain differs from all upstream sources, mixing diverse upstream domains provides the best performance.

While our CSI metric holds well for ID evaluation, further research is needed to improve OOD generalization across different chemical domains. One direction is to explore additional upstream sources to better capture domain variations. Another potential extension is leveraging foundation models trained beyond energy and forces, which could enhance feature extraction and improve similarity assessments.


% - Reiterate what is OOD.
% - Mention that we study X,Y,Z datasets as OOD and why it's considered OOD.
% - Mention that we can still select the best dataset among the upstream.
% - Why we need better metrics or possibly foundation models for the CSI metric for this problem ?
% - While it works for qmof it doesnt for matbench.
% - mention mixed data as a good solution here.
% - Hopefully fid based sampling works.


% \textcolor{blue}{Limitations we discuss OOD}

% \textit{OOD experiments}
% \textit{we say metric is naive but it showed what we wanted to show that superior subsets exist.}
% \textit{Here we only use node features however ufture better metrics might invovle edge features .. }

