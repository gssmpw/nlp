\section{Related Work} 



\textbf{Pretraining for Atomic Property Prediction.} Inspired by the success of pretraining in computer vision and natural language processing, pretraining for atomic property prediction has gained significant attention in recent years. Most approaches in molecular machine learning focus on self-supervised learning~\cite{liu2021pre, jiao2023energy, chen2021algebraic, kolluru2022transfer, zhou2022uni, ji2024exploring}, as generating labels for molecular datasets is computationally expensive. In contrast, fewer studies explore the effectiveness of supervised transfer learning~\cite{smith2019approaching, smith2018outsmarting, kolluru2022transfer}. However, in both self-supervised and supervised settings, the focus has primarily been on improving feature representation, often overlooking the impact of pretraining dataset relevance on downstream performance. Recently, JMP~\cite{shoghimolecules} introduced multi-domain pretraining, enabling joint pretraining on various upstream sources concurrently. While effective, JMP pretraining requires enormous computational resources to reproduce and does not reveal how each upstream source impacts downstream performance. Our work addresses this gap by systematically studying the relationship between upstream pretraining datasets and downstream performance, enabling researchers to develop effective pretraining models even with limited computational resources.

% While effective, this approach is computationally expensive and treats all downstream tasks equally, without considering their alignment with upstream datasets. This gap in systematically exploring the relationship between upstream and downstream tasks motivates our work, where we focus on dataset selection based on task alignment, rather than solely on data size.
% Our work addresses this gap by systematically studying the relationship between upstream pretraining datasets and downstream task performance. We aim to highlight the importance of proper data selection for pretraining, drawing attention to its significant implications for both computational efficiency and downstream performance.


\input{figures/main_figure}

\textbf{Computational Budgeting.} Recent research highlights the importance of studying model performance under computationally budgeted setups. In continual learning (CL), works by \citet{prabhu2023computationally} and \citet{ghunaim2023real} show that simple baselines often outperform state-of-the-art methods in compute-constrained settings. TiC-CLIP~\cite{tic-clip-v2} further demonstrates efficient rehearsal-based training for time-continuous data. For Vision Transformers, \citet{pan2022budgeted} propose a framework to dynamically control model complexity during training, achieving competitive performance under varying budgets. \citet{li2019budgeted} formalize budgeted training, showing that budget-aware learning rate schedules, such as linear decay, are critical for robust performance across tasks like image classification and object detection. In multi-domain learning, \citet{berriel2019budget} introduce Budget-Aware Adapters, which reduce computational complexity while maintaining accuracy by selecting relevant feature channels. These findings across domains emphasize the critical need for more efficient approaches that can achieve competitive performance while minimizing computational costs.

\textbf{Data Selection.} Efficient training through data selection has been explored via two primary approaches: subset selection and dataset distillation. Subset selection aims to identify a representative subset of the training data that matches or even outperforms training on the full dataset. Several methods have been proposed for vision and NLP tasks~\cite{attendu2023nlu, killamsetty2021grad, killamsetty2021glister, kaushal2019learning, bairi2015summarization, lapedriza2013all}. Dataset distillation, introduced by Wang et al.~\cite{wang2018dataset}, focuses on generating a smaller, synthetic subset of the dataset that preserves performance while reducing training time and storage requirements. Subsequent work has explored techniques such as meta-learning~\cite{zhou2022dataset, nguyendataset, nguyen2021dataset}, gradient matching~\cite{zhao2021dataset}, and distribution matching~\cite{zhao2023dataset}. While most research in distillation has focused on vision tasks, a few studies have extended it to graph data~\cite{jingraph, liu2022graph, jin2022condensing}, though primarily targeting knowledge and social graphs rather than molecular graphs.

Two recent vision studies are particularly relevant to our work. First, \citet{hammoud2024pretraining} shows that increasing pretraining data diversity enhances performance only when distribution shifts between upstream and downstream tasks are minimized. Second, \citet{li2023internet} introduces a method to dynamically leverage the open web, reducing the distribution gap between upstream and downstream tasks through targeted representation learning. Findings from other domains suggest that aligning upstream datasets may be crucial for effective pretraining.
% Inspired by these findings from the vision domain, we explore upstream data selection for atomic property prediction.


\textbf{Comparison to Our Work.} To the best of our knowledge, no prior work has specifically explored upstream dataset selection for molecular graphs, which present unique challenges due to their structural and chemical complexity. In this work, we take the first step in addressing this gap by focusing on aligning upstream and downstream distributions at the dataset level rather than subselecting at a sample-wise level or creating a synthetic distilled version of the dataset.

