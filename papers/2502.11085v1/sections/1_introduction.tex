\section{Introduction}
 
Machine learning is transforming molecular modeling, driving advancements in accurate predictions and simulations of molecular behavior~\cite{chanussot2021open, tran2023open, liaoequiformerv2}. These breakthroughs directly impact the acceleration of progress in crucial fields such as drug discovery~\cite{huang2021therapeutics} and global climate change mitigation~\cite{sriram2024open}. The improvements in this field have been primarily attributed to innovations in model architectures~\cite{liaoequiformerv2, gasteiger2021gemnet, passaro2023reducing} and the growing availability of large-scale molecular datasets. In recent years, the sizes of molecular datasets have increased dramatically - from tens of thousands of examples \cite{christensen2020role, chmiela2023accurate, wu2018moleculenet} to hundreds of millions \cite{chanussot2021open, tran2023open}. This rapid growth in scale has also caused a surge in the computational resources required for pretraining, increasing from a few days on a single GPU to over a thousand GPU-days \cite{shoghimolecules, liaoequiformerv2}. This trend begs the question: 

\textit{\textcolor{darkyellow}{\faLightbulb} {\textbf{Is scaling data and resources the only path forward in atomic property prediction, or can intelligent data selection achieve similar performance more efficiently?}}}

\input{figures/pull_figure}

While data selection strategies for pretraining have been explored in fields like natural language processing~\cite{penedo2024the} and computer vision~\cite{hammoud2024pretraining,li2023internet}, this area remains largely underexplored in atomic property prediction, where unique challenges arise. In his paper, we challenge the prevailing assumption that "bigger is better" by exploring whether a smaller, strategically selected dataset can lead to comparable or even superior performance while substantially reducing computational demands. We introduce a pretraining paradigm that shifts the focus from data and compute scaling to selecting the most relevant upstream dataset for improved downstream performance.\newline
% We introduce a novel pretraining paradigm that departs from the current emphasis on data and compute scale.  Our approach shifts the focus from mere data scaling to analyzing the impact of upstream datasets relveance to their downstream perforaance. We introduce a pretraining paradigm that shifts the focus from data and compute scaling to extracting the best featutre pretraining representation for downstream performance.


Through a simple baseline, our experiments reveal two key insights:

% \begin{enumerate}%[leftmargin=4pt]
% \setlength{\itemsep}{0pt}  % Reduce space between items
% \setlength{\parskip}{0pt} % Reduce space between paragraphs
\textbf{(1) Competitive Performance Can Be Achieved with 24$\mathbf{\times}$ Fewer Resources:} Selecting upstream datasets based on their relevance to the downstream task achieves performance on par with or exceeding that of large-scale pretrained models like JMP \cite{shoghimolecules} while utilizing only \textbf{1/24th} of the computational resources, as shown in Figure \ref{fig:pull_figure}.\newline 
\textbf{(2) Quality Outperforms Quantity:} Expanding the pretraining dataset by incorporating additional data from less relevant sources can negatively impact downstream performance rather than enhance it.
% High-quality, task-relevant samples outperform larger datasets of mixed quality. For example, selecting two million samples from a closely aligned dataset yields better results than incorporating additional pretraining data from less relevant sources.



% \end{enumerate}


explore the potential of dataset selection for pretraining in atomic property prediction, we introduce the Chemical Similarity Index (CSI), a simple metric inspired by the Fr√©chet Inception Distance (FID) from computer vision. CSI measures the alignment between an upstream dataset and a downstream task, enabling the selection of chemically relevant pretraining data. By focusing on these highly relevant datasets, we significantly reduce computational costs while maintaining competitive performance and, in many cases, achieving improvements. While large-scale datasets like OC20 \cite{chanussot2021open, tran2023open} and mixed datasets like JMP \cite{shoghimolecules} are popular choices for pretraining in molecular domains~\cite{kolluru2022transfer, shoghimolecules}, our findings challenge their universal utility. %\yasir{Let's think of a stronger closure that doesn't repeat what we already said.} 
Surprisingly, pretraining on a single, carefully selected dataset guided by CSI often outperforms models trained on mixtures, even when those include the most relevant dataset. 


\textbf{The contributions of this paper are threefold:} (1) We introduce a novel framework for computationally efficient pretraining of molecular machine learning models, demonstrating that strategic data selection can match or outperform models trained on much larger datasets. (2) We propose the Chemical Similarity Index (CSI), a metric for assessing the similarity between upstream and downstream molecular datasets, enabling effective dataset selection. (3) We provide an extensive empirical evaluation demonstrating the effectiveness of our approach, offering a practical and efficient alternative to the current trend of ever-increasing data and computational costs in molecular machine learning.
