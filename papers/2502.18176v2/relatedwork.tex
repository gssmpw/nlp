\section{Related Work}
\textbf{Zero-Shot Image Classification.}
Unlike traditional models that are limited to predefined categories, vision-language models (VLMs) are trained on open-vocabulary data and align the embeddings of images and their captions into a common semantic space. This enables them to perform as zero-shot classifiers by matching the semantics of images to textual categories, offering superior generality and flexibility. CLIP \citep{radford2021learning}, trained on extensive internet image-text pairs, achieves advanced results in zero-shot classification tasks. Additionally, other VLMs including Stable Diffusion \citep{rombach2022high}, Imagen \citep{saharia2022palette}, and DaLLE-2 \citep{ramesh2022hierarchical} also possess zero-shot classification capabilities \citep{li2023your, clark2024text}.

\textbf{Adversarial Purification in Pixel Space.}
A prevalent paradigm of adversarial purification aims to maximize the log-likelihood of samples to remove perturbations in pixel space. Since purification has no assumption of the attack type, enabling it to defend against unseen attacks using pre-trained generative models such as PixelCNN \citep{song2017pixeldefend}, GANs \citep{samangouei2018defense}, VAEs \citep{li2020defense}, Energy-based models \citep{hill2020stochastic, yoon2021adversarial}, and Diffusion Models \citep{nie2022diffusion, chen2023robust, zhang2025causaldiff}. Owing to the capability of diffusion models, diffusion-based adversarial purification achieves state-of-the-art robustness among these techniques.



\textbf{CLIP-based Defense.}
While CLIP achieves impressive accuracy in zero-shot classification, it remains vulnerable to imperceptible perturbations \citep{Fort2021CLIPadversarial, mao2022understanding}. Adversarially training the CLIP model on ImageNet / Tiny-ImageNet \citep{schlarmann2024robust, mao2022understanding, wang2024pre} enhances its robustness but undermines its zero-shot capabilities and struggles against unseen attacks. \citet{choi2025adversarial} suggests smoothing techniques for certification. \citet{li2024one} advocates using robust prompts for image classification, but the defensive effectiveness is limited. Additionally, other research focuses on the out-of-distribution (OOD) robustness of the CLIP model \citep{tu2024closer, galindounderstanding}, which is orthogonal to our adversarial defense objectives.

%