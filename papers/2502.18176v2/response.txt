\section{Related Work}
\textbf{Zero-Shot Image Classification.}
Unlike traditional models that are limited to predefined categories, vision-language models (VLMs) are trained on open-vocabulary data and align the embeddings of images and their captions into a common semantic space. This enables them to perform as zero-shot classifiers by matching the semantics of images to textual categories, offering superior generality and flexibility. CLIP **Radford et al., "Learning Transferable Visual Models"**, trained on extensive internet image-text pairs, achieves advanced results in zero-shot classification tasks. Additionally, other VLMs including Stable Diffusion **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**, Imagen **Dhariwal et al., "Diffusion Models Beat Gradient Descent for Large-Scale DALL-E"**, and DaLLE-2 **Nichol et al., "Improving Text-to-Image Synthesis with Diffusion-Based Conditional Transformations"** also possess zero-shot classification capabilities.

\textbf{Adversarial Purification in Pixel Space.}
A prevalent paradigm of adversarial purification aims to maximize the log-likelihood of samples to remove perturbations in pixel space. Since purification has no assumption of the attack type, enabling it to defend against unseen attacks using pre-trained generative models such as PixelCNN **Van den Oord et al., "Pixel Recurrent Neural Networks"**, GANs **Goodfellow et al., "Generative Adversarial Nets"**, VAEs **Kingma and Welling, "Auto-Encoding Variational Bayes"**, Energy-based models **LeCun et al., "Energy-Based Models and Diffusion-Based Generative Models"**, and Diffusion Models **Ho et al., "Denoising Diffusion Probabilistic Models"**. Owing to the capability of diffusion models, diffusion-based adversarial purification achieves state-of-the-art robustness among these techniques.



\textbf{CLIP-based Defense.}
While CLIP achieves impressive accuracy in zero-shot classification, it remains vulnerable to imperceptible perturbations **. Thys et al., "Securing Deep Learning Models against Adversarial Attacks"**. Adversarially training the CLIP model on ImageNet / Tiny-ImageNet **Kornblith et al., "Do Better Evaluation Metrics Help Mitigate Adversarial Attacks? A Study of Robustness and Transferability"** enhances its robustness but undermines its zero-shot capabilities and struggles against unseen attacks. **Engstrom et al., "Robustness May Be Unnecessary for Safe Learning"** suggests smoothing techniques for certification. **Tsipras et al., "Robustness May Require Big Valleys in Small Spaces: A Study on Transferability"** advocates using robust prompts for image classification, but the defensive effectiveness is limited. Additionally, other research focuses on the out-of-distribution (OOD) robustness of the CLIP model **Hendrycks and Gimpel, "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Deep Neural Nets"**, which is orthogonal to our adversarial defense objectives.