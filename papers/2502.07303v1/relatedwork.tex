\section{Related Work}
\subsection{Collaborative Filtering}

\noindent \textbf{Non-Generative Collaborative Filtering.}
Non-generative collaborative filtering learns user and item representations in a discriminative manner. Most non-generative CF models rely on Bayesian Personalized Ranking loss~\cite{rendle2012bpr} along with the application of deep learning techniques~\cite{he2017neural}, such as graph neural networks~\cite{wang2019neural, he2020lightgcn}, contrastive learning~\cite{wujc2021self, lin2022improving, yu2022graph, liu2024twincl}, and diffusion models~\cite{zhao2024denoising, yi2024directional} to enhance more robust and informative representation learning.

\noindent \textbf{Generative Collaborative Filtering.}
The generative collaborative filtering methods take a different approach from learning user and item representations. These methods aim to directly reconstruct user-item interaction matrices by generating probability distributions. The denoising autoencoders (DAEs)~\cite{vincent2008extracting, bengio2013generalized} and variational autoencoders (VAEs)~\cite{kingma2013auto} have been prevalent in generative CF. The DAE-based CF models~\cite{wu2016collaborative, liang2018variational} encode corrupted user-item interactions into latent representations and decode them to reconstruct the original input, learning to denoise and capture underlying patterns. The VAE-based recommenders~\cite{liang2018variational, ma2019learning, shenbin2020recvae} encode user-item interactions into latent probability distributions and decode these distributions to predict user preferences and interactions.
Recently, diffusion models~\cite{lin2024survey} have also shown promise in generative CF. DiffRec~\cite{wang2023diffusion} is the pioneer in using diffusion models in collaborative filtering by modeling the distributions of user-item interaction probabilities. GiffCF~\cite{zhu2024graph} and CF-Diff~\cite{hou2024collaborative} further advance and enhance diffusion-based CF models by incorporating graph signal processing and attention mechanisms, respectively.

\vspace{-3pt}
\subsection{Generative Models}

\noindent \textbf{Diffusion Models}  
~\cite{sohl2015deep, song2019generative, ho2020denoising, song2020score} generate data by iteratively refining samples from Gaussian noise to meaningful outputs. These models have achieved strong results across domains, including images~\cite{ho2020denoising, rombach2022high}, molecules~\cite{xu2022geodiff, jing2022torsional}, and recommendation systems~\cite{lin2024survey, li2023diffurec, yang2024generate}. 

\noindent \textbf{Flow Matching}
~\cite{lipman2022flow, liu2022flow, albergo2022building} is a flow-based method using continuous normalizing flows~\cite{chen2018neural} as alternatives to diffusion models. Conditional Flow Matching~\cite{lipman2022flow, tong2023improving} directly learns a vector field that defines the probability flow from a source distribution to the target distribution, conditioned on individual data points. Flow matching offers faster inference and improved efficiency and has been applied to various generative tasks such as images~\cite{esser2403scaling}, videos~\cite{polyak2024movie}, and proteins~\cite{bose2023se}. However, its potential in recommender systems remains unexplored, presenting opportunities for future research.  

\vspace{-2pt}