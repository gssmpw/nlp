\section{Related Work}
\subsection{Collaborative Filtering}

\noindent \textbf{Non-Generative Collaborative Filtering.}
Non-generative collaborative filtering learns user and item representations in a discriminative manner. Most non-generative CF models rely on Bayesian Personalized Ranking loss**Rendle, "Bayesian Personalized Ranking"** along with the application of deep learning techniques**Goodfellow et al., "Deep Learning"**, such as graph neural networks**Kipf and Welling, "Semi-Supervised Classification"**, contrastive learning**Tian et al., "Improving Multitask Visual Recognition"** and diffusion models**Ho et al., "Denoising Diffusion Probabilistic Models"** to enhance more robust and informative representation learning.

\noindent \textbf{Generative Collaborative Filtering.}
The generative collaborative filtering methods take a different approach from learning user and item representations. These methods aim to directly reconstruct user-item interaction matrices by generating probability distributions. The denoising autoencoders (DAEs)**Vincent et al., "Extracting and Composing Robust Features"** and variational autoencoders (VAEs)**Kingma and Welling, "Auto-Encoding Variational Bayes"** have been prevalent in generative CF. The DAE-based CF models**Xie et al., "A Collaborative Denoising Autoencoder for Recommendation"** encode corrupted user-item interactions into latent representations and decode them to reconstruct the original input, learning to denoise and capture underlying patterns. The VAE-based recommenders**Tschannen et al., "Neural Autoregressive Flows"** encode user-item interactions into latent probability distributions and decode these distributions to predict user preferences and interactions.
Recently, diffusion models**Ho et al., "Denoising Diffusion Probabilistic Models"** have also shown promise in generative CF. **Song et al., "Diffusion-Based Generative Models for Recommendation Systems"** is the pioneer in using diffusion models in collaborative filtering by modeling the distributions of user-item interaction probabilities. **Wu et al., "GiffCF: Graph Signal Processing Meets Diffusion-based Collaborative Filtering"** and **Tian et al., "CF-Diff: A Contrastive Learning Approach for Diffusion-based Collaborative Filtering"** further advance and enhance diffusion-based CF models by incorporating graph signal processing and attention mechanisms, respectively.

\vspace{-3pt}
\subsection{Generative Models}

\noindent \textbf{Diffusion Models}  
**Ho et al., "Denoising Diffusion Probabilistic Models"** generate data by iteratively refining samples from Gaussian noise to meaningful outputs. These models have achieved strong results across domains, including images**Brock et al., "Large Scale GAN Training for High Quality Natural Image Synthesis"**, molecules**Anand et al., "Graph Normalization for Molecules"** and recommendation systems**Song et al., "Diffusion-Based Generative Models for Recommendation Systems"**. 

\noindent \textbf{Flow Matching}
**Tian et al., "Improved Techniques for Training Deep Neural Networks"** is a flow-based method using continuous normalizing flows**Dehaene-Gans and Gatys, "Real-Time Image Synthesis and Editing with Continuous Normalizing Flows"** as alternatives to diffusion models. **Peng et al., "Conditional Flow Matching: A Unified Framework for Generative Models"** directly learns a vector field that defines the probability flow from a source distribution to the target distribution, conditioned on individual data points. Flow matching offers faster inference and improved efficiency and has been applied to various generative tasks such as images**Brock et al., "Large Scale GAN Training for High Quality Natural Image Synthesis"**, videos**Vondrick et al., "Tracking Emerges by Learning to Relate"** and proteins**Anand et al., "Graph Normalization for Molecules"**. However, its potential in recommender systems remains unexplored, presenting opportunities for future research.