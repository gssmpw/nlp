% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{comment}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\definecolor{massive_benchmarks}{RGB}{99,172,91}
\definecolor{interactive_games}{RGB}{114,75,164}

%RB
\newcommand{\cut}[1]{}
\newcounter{tbsnr}
\newenvironment{tbs}
{\addtocounter{tbsnr}{1}\par\bigskip\noindent\fbox{\thetbsnr}
\hspace*{\fill}\begin{minipage}{7cm}\tt}
{\end{minipage}\hspace*{\fill}\bigskip}
\newcommand{\tb}[1]{\begin{tbs}{#1}\end{tbs}}

\newcommand{\rb}[1]{\textcolor{blue}{\textbf{RB: #1}}}
\newcommand{\af}[1]{\textcolor{magenta}{\textbf{AF: #1}}}
\newcommand{\fm}[1]{\textcolor{red}{\textbf{FM: #1}}}
\newcommand{\ds}[1]{\textcolor{orange}{\textbf{DS: #1}}}
\newcommand{\rf}[1]{\textcolor{cyan}{RF: #1}}
\newcommand{\as}[1]{\textcolor{green}{AS: #1}}

\usepackage[color=lightgray]{todonotes}
\usepackage{enumitem}
\usepackage{booktabs}

% <- MG ----------------------------
\definecolor{Green}{RGB}{98,115,19} 
\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} % default note settings, used by macros below.
\newcommand{\mario}[2][]{\note[#1]{\textbf{Mario}}{Green!40}{#2}}
\newcommand{\Mario}[2][]{\mario[inline,#1]{#2}}
\newcommand{\response}[1]
{\vspace{3pt}\hrule\vspace{3pt}\textbf{#1: }}
% ------------------------------->

%Other commands to signal uncertainty
\usepackage{soul}
\sethlcolor{lightgray}

\usepackage{amsmath}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Triangulating LLM Progress\\ through Benchmarks, Games, and Cognitive Tests}
%\title{More Than Just Answers:\\ Evaluating LLMs through Interactive Games and Targeted Cognitive Tests}
%\title{Core cognitive abilities of Social Language Learners}
% More Than Just Answers: Evaluating LLMs Through Interactive Games and Targeted Cognitive Tests


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\author{
  \textbf{Filippo Momentè\textsuperscript{1}\thanks{Corresponding author. \\Email: \href{mailto:filippo.momente@studenti.unitn.it}{filippo.momente@studenti.unitn.it}}
},
  \textbf{Alessandro Suglia\textsuperscript{2}},
  \textbf{Mario Giulianelli\textsuperscript{3}},
  \textbf{Ambra Ferrari\textsuperscript{1}},
\\
  \textbf{Alexander Koller\textsuperscript{4}},
  \textbf{Oliver Lemon\textsuperscript{2}},
  \textbf{David Schlangen\textsuperscript{5}},
  \textbf{Raquel Fernández\textsuperscript{6}},
  \textbf{Raffaella Bernardi\textsuperscript{1}}
\\
\\
  \textsuperscript{1}University of Trento,
  \textsuperscript{2}Heriot-Watt University,
  \textsuperscript{3}ETH Zürich,\\
  \textsuperscript{4}Saarland University,
  \textsuperscript{5}University of Potsdam,
  \textsuperscript{6}University of Amsterdam
\\
}
\begin{document}
\maketitle
%--------------------------------------------
\begin{abstract}
% We examine three evaluation paradigms for LLMs and argue that they provide complementary perspectives. 
\begin{comment} % previous abstract (14 Feb 9pm CET)
We investigate which of two evaluation paradigms---large static question-answering benchmarks (e.g., MMLU and BBH) or interactive games (e.g., Signalling Games or Taboo)---is most effective at discriminating %differentiating %performance across 
LLMs of varying quality.
Then, inspired by human cognitive assessments, we compile a suite of targeted tests %focus on evaluation datasets 
that measure cognitive abilities deemed essential for effective language use and investigate their correlation with model performance in benchmarks and games. 
Our analyses reveal that interactive games are superior to static benchmarks in discriminating models.
Peformance in games correlates with common-sense reasoning, working memory, and theory of mind, while performance on static benchmarks correlates more strongly with reasoning, planning, and pragmatic skills. %\todo{\rb{This sentence about correlation is misleading. MML and BBH have higher correlation with causal/logical reasoning and even planning than the games}}.
We advocate the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.\looseness-1  
% We argue for future work in developing interactive benchmarks and carefully designed tasks inspired by assessing human cognitive abilities.  
\end{comment}
% \begin{comment}  % New attempt
We examine three evaluation paradigms: large question-answering benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind).
First, we investigate which of the former two---benchmarks or games---is most effective at discriminating LLMs of varying quality.
Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. 
Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models.
Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games.
%in particular working memory and emotional intelligence are only significantly correlated with performance in games.
%Performance in games correlates with common-sense reasoning, working memory, and theory of mind, while performance on static benchmarks correlates more strongly with reasoning, planning, and pragmatic skills.
We advocate the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.\looseness-1
% \end{comment}
\end{abstract}
%--------------------------------------------

%====================================
\section{Introduction}
\label{sec:intro}
%====================================

%\todo[inline]{
%Evaluating LLMs is critical to track progress, identify blind spots, and ultimately advance towards the kind of language-based AI systems we want as a society
%}

%\todo[inline]{
%Currently, the most widespread way to evaluate LLMs is by means of large multi-task holistic benchmarks, such as MMLU and BBH.\\
%- \textbf{Advantages}: standardisation, ... \\
%- \textbf{Problems}: overclaims of emerging capabilities, sensitivity to prompt format, ...
%}


Evaluating LLMs is critical to track progress, identify blind spots, and ultimately advance towards the kind of language-based AI systems we want as a society~\cite{wooldridge1995intelligent}. Currently, 
the most widespread way to evaluate LLMs is by means of \textbf{large benchmarks} made up of miscellaneous question-answering (QA) tasks. 
Pre-LLM benchmarks such as GLUE and SuperGLUE~\cite{glue,NEURIPS2019_4496bf24} have been replaced by even larger evaluation suites such as MMLU~\cite[Measuring Massive Multitask Language Understanding;][]{hendryckstest2021}, GSM8K~\cite[Graduate School Math;][]{cobbe2021gsm8k}, or BBH~\cite[BIG-Bench Hard;][]{suzgun-etal-2023-challenging,srivastava2023beyond}. Models with high performance on these benchmarks are taken to possess extensive \textbf{world knowledge along with complex problem-solving abilities.} 
%and 
%reasoning abilities. 
%have been replaced by MMLU~\cite[Measuring Massive Multitask Language Understanding;][]{hendryckstest2021} which consists of multiple-choice questions spanning 57 academic subjects, GSM8K~\cite[Graduate School Math;][]{cobbe2021gsm8k} containing high quality linguistically diverse grade school math word problems, or BBH~\cite[BIG-Bench Hard;][]{suzgun-etal-2023-challenging} including diverse problems that may require multi-step reasoning. 

%While the earlier benchmarks aimed to measure model language understanding, MMLU aims to check whether LMs can apply knowledge from a specific domain: it consists of multiple-choice questions spanning 57 academic subjects; Big-Bench assembles a diversity of tasks drawing problems from linguistics, childhood development, maths, common-sense reasoning etc. These efforts brought the community to move towards "Holistic Evaluation of Language Models" (HELM)~\cite{liang2023holistic} to 
%compare a variety of models on the same core scenarios and metrics under standardized conditions.

%\begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.35   \textwidth]{images/place_holder_v3.png} 
 %   \caption{Three complementary evaluation methods.\rf{by now, I think this figure is not that helpful after all}}
 %   \label{fig:overview}
  %  \vspace{-1em}
%\end{figure}

This trend has promoted standardisation in LLM evaluation protocols, with online leaderboards constantly updated as new models are released. 
Despite this undeniable benefit, large QA benchmarks like those mentioned above are not without problems. 
Evaluation results may be inflated by data contamination (see, e.g.,~\citealt{gema:arewe25} for MMLU and~\citealt{mirzadeh2025gsmsymbolic} for GSMK8) and distorted by model sensitivity to prompt format~\cite{zhuo2024prosa}. 
% An additional weakness of such benchmarks is that, by design, they
%arguably, 
Moreover, by design, such benchmarks
overlook actual language use in favour of knowledge-intensive tasks where success is measured against gold-standard reference answers provided in a single conversational turn. This contrasts with the view, put forward by philosophers and psycholinguists alike \cite{wittgenstein1953investigations,austin1962things,searle1969speech,clark1996using}, that the quintessence of language resides in \textit{situated language use}, i.e., using language for a purpose in social and task-based multi-turn interactions \cite{bisk-etal-2020-experience}.\looseness-1

The situated and interactive view underpins a parallel evaluation trend where LLMs are evaluated as \textbf{goal-directed language users} by means of \textbf{interactive games}~\cite{schla:what23,suglia2024visually}.\footnote{Online leaderboards have started to appear for the interactive games evaluation paradigm; see, e.g., \url{https://textarena.ai/}, \url{https://clembench.github.io}.} 
This interactive evaluation paradigm goes beyond single-turn text generation, which is critical for deploying LLMs as agents. %, and is less prone to data contamination.  
Additionally, it is less susceptible to data contamination
%---both because LLM training data primarily comes from web sources and 
because the vast space of possible multi-turn interactions is unlikely to be fully represented in the training data. As a result, interactive games provide a more robust framework for evaluating the true generalisation capacity of LLMs~\cite{hupkes2023taxonomy}.
Yet, despite these advantages, it is not easy to pinpoint which specific abilities underpin models' performance on interactive language games---a difficulty that to some extent also applies to static question-answering benchmarks such as MMLU.\looseness-1


%A third evaluation trend is also gaining momentum, where LLMs are evaluated using smaller targeted datasets carefully designed to test specific cognitive abilities, such as planning \citep{zheng2024naturalplanbenchmarkingllms}, cognitive flexibility \citep{kennedy2024cognitive}, theory of mind \citep{gu:simpletom25}, or emotional intelligence \citep{paech2023eqbench}, drawing inspiration from human cognitive assessments. %inspired by human cognition. 
%While each such dataset alone does not get us very far in the quest for robust LLM evaluation, we contend that this type of evaluation paradigm has the potential to enhance our understanding of what fundamental abilities LLMs must develop to be able to function effectively as language agents, where multiple skills may be required and possibly interact.\looseness-1

%In this paper, we examine these three evaluation paradigms --- non-interactive question-answering benchmarks, interactive games, and targeted cognitive tests---and argue that they can provide complementary perspectives. 
In this paper, we examine these two evaluation paradigms---large QA benchmarks and interactive games---and argue that they can provide complementary perspectives. 
First, we investigate whether QA benchmarks or games are more effective in gauging qualitative differences between models, e.g., % between models, 
across model families and sizes. 
We evaluate a selection of current LLMs from four %different 
model families and
%We 
find that %interactive 
games highlight differences between %current
LLMs more strongly than %static
QA benchmarks: While scaling model size leads to systematic improvements on %static 
benchmarks, it doesn't guarantee performance boosts in interactive language use. 
% Then, to 
To shed light on the abilities underlying models' performance on these two evaluation frameworks, we resort to \textbf{targeted cognitive tests}. We propose a taxonomy of cognitive skills motivated by neurocognitive science and compile a list of existing evaluation datasets designed to assess each skill in isolation. 
%We evaluate a selection of LLMs from four different model families 
%on the three evaluation paradigms (Figure~\ref{fig:overview}) 
We then investigate to what extent increased performance on specific cognitive abilities correlates with performance gain in large QA benchmarks vs.\ interactive games.  Our analysis shows that while 
%higher-order 
causal and logical reasoning %abilities
correlate with both static and interactive tests, differences emerge regarding core executive functions and social/emotional skills; in particular, working memory and emotional intelligence %abilities 
are only significantly correlated with performance in %interactive 
games.
%seem to better correlate with the latter tests.
%We see that performance both on static and interactive evaluation correlates with performance on test measuring the higher-order reasoning ability, while planning is more dominant in the static problem-solving tasks, working memory seems to be beneficial for interactive games. Of the social skills, pragmatics seems to be relevant for both static and interactive tests, while emotional intelligence and ToM seem to correlate better with the latter.\todo{please check}
%\rf{todo: what's the upshot of it all?}

% We show that:
% \begin{itemize}[nosep]
% \item interactive games are more effective at highlighting relevant differences among models;
% \item well-curated lab-like "cognitive abilities" tests could help investigate such differences 
% \end{itemize}

%These results suggest that single-turn, school test-based benchmarks evaluate model capacity to store knowledge, hence they promote bigger models. Instead, interactive games go beyond such metrics and evaluate models at work. As such they are able to distinguish the performance of models with similar ``storage capacity''. However, it is not clear yet which cognitive abilities such models employ when succeeding in the games. Collaboration between cognitive neuroscientists and AI researchers is essential for developing appropriate evaluation methods that are large-scale but also precise in the abilities they measure.


%\todo[inline]{
%To test for specific cognitive abilities is becoming more prominent, with multiple evaluation datasets being created that aim to target concrete cognitive skills. By themselves, each of these datasets does not get us very far. Yet we argue this type of benchmark is essential to arrive at a better understanding of what abilities LLMs should master to be able to perform useful tasks as language agents, where multiple cog abilities will typically be put to use and interact.
%}


%The advantage of this trend is that models' progress has been monitored and leaderboards are available online and constantly updated when new models are developed; hence a standardized evaluation approach has been achieved.
%However, it has been shown that this static reference-based evaluation method is subject to data contamination (see for instance~\citet{gema:arewe25} for MMLU and~\citet{mirzadeh2025gsmsymbolic} for GSMK8) and sensitive to prompt format~\cite{zhuo2024prosa}. 
%Hence, they bring to overestimate the capabilities that are claimed to emerge in LLMs with the increase of their size. 
%Moreover, they are not a proper tool to systematically evaluate LLMs language use and to promote communication-driven learning paradigms~\cite{schlangen2019language}.

%https://arxiv.org/abs/2406.04127  Are we done with MMLU?

%\todo[inline]{
%A different strand of work has focused on evaluating LLMs as goal-directed language users by means of interactive games. \\
%- \textbf{Advantages}: less prone to data contamination, beyond sinle-turn text generation, ...\\
%- \textbf{Problems}: yet, it is unclear which cog abilities contribute to increased performance (i.e., %presumably games are a way to test underlying abilities at play in language use)  ...
%}

 
%Leveraging on the neurocognitive science literature, \citet{maho:diss24} advocates the need to distinguish formal vs.\ functional linguistics competencies and note that whereas formal linguistic competence in LLMs improves with the amount of training data that is not the case for the functional abilities. To test for specific cognitive abilities is becoming more prominent, with multiple evaluation datasets being created that aim to target concrete cognitive skills. By themselves, each of these datasets does not get us very far. Yet we argue this type of benchmark is essential to arrive at a better understanding of what abilities LLMs should master to be able to perform useful tasks as language agents, where multiple cognitive abilities will typically be put to use and interact. Formal linguistic competencies have been recently reviewed by~\citet{waldis:holmes24}, and we focus on the functional linguistic one. 


%A notable effort is by~\cite{warstadt-etal-2023-findings} who organize the BabyLM challenge to optimize pretraining given data limitations inspired by human development, yet their attention is on the data size rather than the expected skills a social language learner should master. In this direction, leveraging on the neurocognitive science literature, ~\citet{maho:diss24} advocates the need to distinguish formal vs. functional linguistics competencies and note that whereas formal linguistic competence in LLms improves with the amount of training data that is not the case for the functional abilities.

%\todo[inline]{
%We investigate what type of evaluation instrument -- multi-task static benchmarks vs interactive games -- helps to better highlight differences across models (re.~model size and family), showing that interactive games are more effective at highlighting relevant differences. Single turn text generation evaluates model capacity to store knowledge, hence they promote bigger models; games go beyond such metric and evaluate models at work: hence model of similar size perform differently.
%}

%\todo[inline]{
%Motivated by neuro-cognitive science, we consider a set of key cognitive skills that underlie human functional linguistic abilities, and compile a list of datasets that target each of these skills in isolation. We investigate to what extent these abilities correlate with performance gain in multi-task static benchmarks vs interactive games. 
%}


%====================================
\section{Models}
\label{sec:models}
%====================================

We apply our evaluation framework to a selection of open-weight LLMs ranging from 7B to 72B models. Considering that instruction following capabilities are essential for our analysis, we selected models that have an average performance on the IFEval benchmark~\cite{zhou2023instruction} higher than 70\% (see Figure~\ref{fig:accuracy-ifevl-mmlu-taboo} for details). We evaluate the following models: Olmo-2-1124 with 7 and 13 billion parameters (\texttt{OLMo-2-1124-*-Instruct})~\cite{olmo20242olmo2furious};  Qwen2.5 with 7B, 32B, and 72B parameters (\texttt{Qwen2.5-*-Instruct})~\cite{qwen2, qwen2.5}; LLama-3 with 8B (\texttt{Llama3.1-8B-Instruct}) and 70B parameters (\texttt{Llama3.3-70B-Instruct})~\cite{dubey2024llama3herdmodels}, and \texttt{Falcon3-10B-Instruct}~\cite{falcon3}. See Appendix~\ref{sec:appendix-models} for further model details.


%\begin{figure*}[h!]
 %   \caption{We selected models with IFEvel performance higher than 70\%.}
  %  \centering
   % \includegraphics[width=0.5\textwidth]{images/accuracy_plotts_8_models/instr_8_models.png} 
    %\label{fig:IFEval}
%\end{figure*}


%====================================
\section{Static vs.\ Interactive Assessments}
\label{sec:static-interactive}
%====================================

\subsection{Benchmarks}

\paragraph{Large QA benchmarks}  We take MMLU \cite{hendryckstest2021} and BBH~\cite{suzgun-etal-2023-challenging} as representative of large QA benchmarks. MMLU evaluates whether LLMs can apply knowledge from specific domains: it consists of multiple-choice questions spanning 57 academic subjects. BBH assembles diverse tasks drawing problems from linguistics, child development, maths, and common-sense reasoning, among others.


%We take the games in the clembench  platform as sample of \emph{language in use} interactive tests in which various abilities are put at work~\cite{chalamalasetti-etal-2023-clembench}. 

%\paragraph{Interactive (or dialogue) games} 
%As described in \citet{schlangen2019language}, a \textit{dialogue game} is a ``constructed activity with a clear beginning and end, in which players attempt to reach a predetermined goal state primarily by means of producing and understanding linguistic material". For this reason, interactive games are a way to assess situated language use directly. For this purpose, 
\paragraph{Interactive games}
We take  \texttt{clembench}~\cite{chalamalasetti-etal-2023-clembench} as a characteristic benchmark to assess LLMs' gameplay ability in dialogue games.  We consider the games 1)~\textit{Taboo}, 2)~standard \textit{Wordle} and the two variants \textit{Wordle (Clue)} and \textit{Wordle (Critic)}, 3)~\textit{Reference Game}, 4)~\textit{Image Game}, and 5)~\textit{Private/Shared}. See Appendix~\ref{appendix:clembench}.




\begin{figure}[h!]
    \centering
    \begin{tabular}{l}
    %\includegraphics[width=0.4\textwidth]{images/accuracy_plotts_8_models/instr_8_models.png}\\
    %\includegraphics[width=0.4\textwidth]{images/accuracy_plotts_8_models/mmlu_8-models.png}\\
    %\includegraphics[width=0.4\textwidth]{images/accuracy_plotts_8_models/taboo_8_models.png}
    \includegraphics[width=0.45\textwidth]{images/geometric-fig/radar_massive-interactive.png}
    \end{tabular}
    \caption{Accuracy across the different model sizes: IFEval, \textcolor{massive_benchmarks}{Static}, and \textcolor{interactive_games}{Interactive} assessments.}\label{fig:accuracy-ifevl-mmlu-taboo}
    \vspace{-0.4em}
\end{figure}
%====================================
%\subsection{Discriminating across Models}
%\subsection{Discriminating Storing vs.\ Using Knowledge}
%\subsection{Discriminating Storage vs.\ Usage}
\subsection{How to Identify Blind Spots in LLMs}
%====================================
%Evaluating LLMs is critical to track progress, identify blind spots, and ultimately advance towards the kind of language-based AI systems we want as a society~\cite{wooldridge1995intelligent}.



\begin{figure*}[h!]
    \centering
\begin{tabular}{cc}
 \includegraphics[width=0.35\textwidth]{images/lama_qwen_8-70/lama_8_70_dataset_disc.png } \hspace{1cm}
 \includegraphics[width=0.35\textwidth]{images/lama_qwen_70-72/models_70-72_dis.png}
 \end{tabular}
   \caption{Comparing datasets in their power to discriminate between models of different size but same family (left) and of different families but similarly large (right). The number next to the benchmark's name indicates the ratio of performance between the two models. The asterisk '*' next to \textit{Wordle} indicates that the ratio is undefined.}\label{fig:dataset-discriminative}
\vspace{-0.8em}
\end{figure*}

LLM evaluation instruments have most practical use when they allow us to track progress by identifying blind spots in models. Here we compare the two evaluation paradigms under study on the extent to which they highlight differences between current models, helping us form hypotheses about possible problem sources and successful mitigation strategies. Figure~\ref{fig:accuracy-ifevl-mmlu-taboo} 
shows models' performance on IFEval, the large QA benchmarks, and interactive games. As mentioned in Section~\ref{sec:models}, all models %we test 
are reasonably able to follow instructions as measured by IFEval. While the OLMo-2 models are more inconsistent across different model sizes, all the other models exhibit the expected pattern of showcasing better performance on both large QA benchmarks and interactive games when parameter count increases. At the same time, we observe that most of the interactive games highlight the benefits of large model sizes much more strongly. This can more easily be appreciated in Figure~\ref{fig:dataset-discriminative} (left) for Llama-3.1-8B vs.\ Llama-3.3-70B. In this visualisation, the further away a benchmark is from the diagonal, the more affected performance is by model size. While playing \textit{Wordle} is extremely challenging for any model, scaling up the number of parameters appears to be fundamental to succeed at \textit{Private/Shared}, \textit{Image Game}, and \textit{Reference Game}---much more so than for MMLU and BBH.\looseness-1 
    
Is size however all we need? 
Figure~\ref{fig:dataset-discriminative} (right) shows that QA benchmarks do not substantially distinguish between large models of comparable size (Llama-3.3-70B-Instruct vs.\ Qwen2.5-72B-Instruct): scaling on the number of parameters results in performance boosts across model families. Hence, arguably large QA benchmark test for abilities than can be expressed within parametric knowledge. Given that such benchmarks currently are the standard LLM evaluation paradigm, it is not surprising that scaling is high on the agenda of model developers.
In contrast, interactive games seem to provide a different picture: models with comparable parametric capacity perform very differently on \textit{Image Game}, \textit{Private/Shared}, and \textit{Wordle (Clue/Critic)}. A similar trend can be observed among the other models we evaluated (see details in Appendix~\ref{sec:sup-plots}). This result supports the hypothesis that size is not all there is behind the potential of LLMs to learn inferential strategies for effective language use in interaction. 

%In the next section, we aim to shed some light on the cognitive abilities involved in interactive games and large QA benchmarks. 




%We focus our attention on whether current large question-answering benchmarks are able to discriminate performance across models and whether interactive games can offer a more robust way to showcase the ability \emph{to use language}.  
%Figure~\ref{fig:accuracy-ifevl-mmlu-taboo} reports models' performance on IFEval, the large question-answering and interactive benchmarks.  While Olmo 2 models are more inconsistent across different sizes, the Llama 3.1-3.3 and Qwen 2.5 ones exhibit the expected pattern of showcasing better performance when parameter count increases for both multitask benchmarks and interactive games. This is not surprising for the former since that is the standard testbed that bigger models are chasing. In the following, 
%we investigate what is behind this standard picture and 
%we ask which approach (multitask benchmarks or interactive  games) better highlights differences among model size,  and between models of the same size but of different families. 
% \todo[inline]{
%What type of evaluation instrument -- multi-task static benchmarks vs interactive games -- is more effective at highlightigh  differences across models?
%}
%\todo[inline]{
%Result (1) Model size: Figure~\ref{fig:accuracy-modelsize} While Olmo is more erratic and shows somewhat surprinsing patters, Lama and Qwen exhibit the expected pattern of better performance with parameter increase for both multitask benchmarks and interactive games. This is not surprising for the former since that's the standard barometer the bigger models were likely chasing. As for the interactive games, we observe that they highlight the benefits of large model sizes much more strongly Figure~\ref{fig:accuracy-modelsize-llama} visualizes the comparison between LLama 7B and 70B.
%}



%\begin{figure*}[h!]
 %   \caption{Accuracy across the different model size for the massive benchmarks and two interactive games}
  %  \centering
  %  \begin{tabular}{ll}
  %  \includegraphics[width=0.35\textwidth]{images/accuracy_plotts_8_models/mmlu_8-models.png}
  %  &
   % \includegraphics[width=0.355\textwidth]{images/accuracy_plotts_8_models/bbh_8_models.png}\\
    %\includegraphics[width=0.355\textwidth]{images/accuracy_plotts_8_models/taboo_8_models.png}
    %&
    %\includegraphics[width=0.35\textwidth]{images/accuracy_plotts_8_models/reference_game_8_models.png}
    %\end{tabular}
    %\label{fig:accuracy-modelsize}
    
%\end{figure*}


%Figure~\ref{fig:accuracy-modelsize} (left) visualizes the comparison between Llama-3.1-8B-Instruct and Llama-3.3-70B-Instruct. As we can see, interactive games highlight the benefits of large model sizes much more strongly than the static large-scale school-like tests~\ref{fig:accuracy-modelsize}. Interestingly, Figure~\ref{fig:accuracy-modelsize} (right) shows that multitask benchmarks do not distinguish between large models of comparable size (Llama-3.3-70B-Instruct vs. Qwen2.5-72B-Instruct), while interactive games do. We conjecture that multitask benchmarks focus on knowledge stored in the model parameters, hence they are not able to highlight differences in their underlying capabilities considering that models of similar size can store  comparable  amounts of knowledge.
%On the other hand, interactive games evaluate models' abilities to \emph{use}  knowledge; indeed with the interactive tasks we observe more marked differences between similarly large models. A similar trend can be observed among the other models we evaluated (see details in the Appendix \todo{let's add a precise reference to the Section or image in the appendix}).

%\begin{figure*}[h!]
 %   \centering
  %  \begin{tabular}{ll}
%%\includegraphics[width=0.45\textwidth]{images/lama_qwen_8-70/lama_8-70_massive.png}
 %%   &
  %  \includegraphics[width=0.45\textwidth]{images/lama_qwen_8-70/lama_8-70_game_massive.png} &
% %   \includegraphics[width=0.45\textwidth]{images/lama_qwen_70-72/massive_70-72B.png} &
 %\includegraphics[width=0.45\textwidth]{images/lama_qwen_70-72/games_massive_70-72B.png}
% \end{tabular}
 %    \caption{Comparison between models of different sizes (Llama-3.1-8B-Instruct vs.\ 70B) (\textbf{Left}) and similar size (Llama-3.3-70B-Instruct vs. Qwen2.5-72B-Instruct) (\textbf{Right}).\rf{I would remove these %plots}}\label{fig:accuracy-modelsize}
  %  \end{figure*}
    

%We hypothesize that datasets possess different levels of capability of discriminating across models' performances. For this reason, in Figure~\ref{fig:dataset-discriminative} we visualize the distance in performance between two models either belonging to the same family or sharing similar parameter sizes, and compute the ratio between the results in a given benchmark for both in order to understand the magnitude of such distance. 
% To better highlight the different capacities of datasets to discriminate models' performance we visualize the accuracy of the compared models reporting the distance of each \rb{filippo}  Figure~\ref{fig:dataset-disc}. 
%When comparing models with similar instruction-following ability and similar storage capacity (viz.\ same size), only the interactive games are able to discriminate between models. This result speaks in favour of this new emerging evaluation method which has been advocated in the NLP community (e.g.~\citet{schlangen2019language, schlangen2023dialogue}). However, pinpointing what abilities each interactive game evaluates and where they differ from those of the large static question-answering benchmarks is not trivial. For this reason, we extend our evaluation to core cognitive abilities.


    
%\begin{figure*}[h!]
 %\caption{Model same size}
  %  \centering
   % \begin{tabular}{cc}
 %\includegraphics[width=0.5\textwidth]{images/lama_qwen_70-72/massive_70-72B.png} &
 %\includegraphics[width=0.5\textwidth]{images/lama_qwen_70-72/interactive_games_70-72B.png}
 %\end{tabular}
%\end{figure*}   \label{fig:accuracy-model-same-size}

    

%====================================
\section{Cognitive Abilities Assessment}
\label{sec:eval_ca}
%====================================

%\todo[inline]{The benchmarks we use}
% To these two evaluation approaches, we add a battery of datasets aimed at benchmarking LLMs on \emph{cognitive ability in isolation}. We focus on the cognitive abilities involved in human functional linguistic competence, i.e., 

We now turn to cognitive tests---a complementary evaluation method that focuses on specific cognitive abilities deemed essential for effective language use in real-world situations.
We explore the use of targeted cognitive tests to complement evaluation based on large QA benchmarks and interactive games.
% We introduce a third evaluation method that focuses on specific cognitive abilities essential for effective language use in real-world situations. Since these abilities are considered prerequisites for strong performance in the other two evaluation approaches, monitoring them is crucial when comparing models.\looseness-1
%to achieve goals and communicate with others. We take a taxonomy of such abilities, and select one dataset for each of it. 



\subsection{Taxonomy and Datasets}

We present a taxonomy of cognitive abilities involved in human \textit{functional linguistic competence} \cite{maho:diss24}. 
%The taxonomy we present 
It is guided by neurocognitive research~\cite{ward:guide19}, and it separates capabilities into two distinct macro-categories known to recruit different brain networks: executive functions and socio-emotional skills. \textbf{Executive functions} are broadly defined as the complex processes by which we control and optimise our thoughts and behaviour \cite{baddeley:WM86} and are divided into \emph{core} and \emph{higher-order} abilities. 
%, where the latter subsume the former. 
%for self-monitoring and error corrections we only found datasets based on human evaluation~\cite{tyen-etal-2024-llms,lin-etal-2024-criticbench}}.
%To evaluate higher-order abilities, we consider the following datasets:  \textsc{Natural Plan}~\cite{zheng2024naturalplanbenchmarkingllms}, to evaluate ``planning'', \textsc{CLadder}~\cite{jin2023cladder} for causal reasoning, LogiQA 2.0 ~\cite{logiqa} for logical reasoning, WinoGrande ~\cite{winogrande} which evaluates ``commonsense reasoning". To evaluate core abilities, we include the WCST and the LNT test ~\citep{kennedy2024cognitive} for ``cognitive flexibility'' and a \textit{n-back} task ~\citep{workingmemory} for ``working memory''.
\textbf{Socio-emotional skills} represent the abilities necessary to interact adaptively with other individuals \cite{higgins:ARP87}, including the ability to recognize their emotional and cognitive states. 

For each cognitive ability, we select an existing evaluation dataset designed to test it in isolation drawing inspiration from human cognitive assessments. We discard datasets that require manual evaluation from the analysis. 
Table~\ref{tab:exe-func} and Table~\ref{tab:soc} list the abilities in the taxonomy and the datasets we use to evaluate them.\footnote{We found no dataset to evaluate inhibitory control. The datasets we found for Emotion-regulation, Self-awareness~\cite{liu-etal-2024-interintent}, Empathy~\cite{chen-etal-2024-emotionqueen} and Social Problem-solving~\cite{du:llmfdecision24} require human evaluation.
%Together with Reasoning and Planning, a third higher-order ability is Problem-solving, which is tightly connected with reasoning. We evaluate models on three different reasoning types.
} 
Socio-emotional skills have only recently entered the evaluation landscape in NLP, and they have done so with a forceful presence: remarkably, small benchmarks already exist for almost all 
of the abilities in this category. 
%listed we found at least one benchmark, as reported in Table~\ref{tab:soc}.\footnote{The datasets we found for Emotion-regulation, Self-awareness~\cite{liu-etal-2024-interintent}, Empathy~\cite{chen-etal-2024-emotionqueen} and Social Problem-solving~\cite{du:llmfdecision24} require human evaluation.}
 %Interestingly, we also found a dataset proposing a battery of tests to carry out a broad "Cognitive Assessments for Language Models", CALM~\cite{mcduff:calm2024}. 
 %We include in our evaluation the following datasets: \textsc{Social IQa} \cite{sap-etal-2019-social} for social commonsense, EQ-Bench~\cite{paech2023eqbench} for emotional intelligence, LM-Pragmatics~\cite{hu-etal-2023-fine} for pragmatic abilities, and SimpleToM~\cite{gu:simpletom25} for explicit theory of mind and attribution and judgment, viz. applied theory of mind.\footnote{The datasets we found to evaluate Self-regulation~\cite{liu-etal-2024-interintent}, Empathy~\cite{chen-etal-2024-emotionqueen} and Social Problem-solving~\cite{du:llm24} are based on human evaluation.}
 %and InterIntent~\cite{liu-etal-2024-interintent} and CALM~\cite{mcduff:calm2024}.


%They have been the focus of attention both in the NLP and AI at-large communities well before the LLM era.
%Details about the taxonomy are reported in Appendix~\ref{subsec:appendix-taxonomy}.  

%\paragraph{Executive functional abilities.} Executive functions are broadly defined as the complex processes by which we control and optimize our thoughts and behaviour \cite{baddeley:WM86}.
%These include problem solving, logical and commonsense reasoning, and planning abilities.
%They have been the focus of attention both in the NLP and AI at-large communities well before the LLM era. %, and researchers have long-standing experience in benchmarking models on their problem-solving, logical and commonsense reasoning, planning abilities; 
%Behind these ``higher-order'' abilities, cognitive neuroscience identifies core functional abilities---working memory,  cognitive flexibility, inhibitory control, and self-monitoring and error corrections. 

%For these four %three 
%core abilities, we found a suitable dataset only for the first two.\footnote{We found no dataset to evaluate inhibitory control and for self-monitoring and error corrections; we only found datasets based on human evaluation~\cite{tyen-etal-2024-llms,lin-etal-2024-criticbench}.} 




\begin{table}[t] 
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lll@{}} 
\toprule
& \textbf{Cognitive Ability} & \textbf{Benchmark} \\ 
\midrule
\multirow{3}{*}{\rotatebox{90}{Core}} &
Working Memory  & \citet{workingmemory} \\
& Cognitive Flexibility  & \citet{kennedy2024cognitive} \\
& Inhibitory Control  & -- \\\hline 
\multirow{4}{*}{\rotatebox{90}{HO}} &
Logical Reasoning  & \citet{logiqa} \\
%Numerical Reasoning & -- \\
& Causal Reasoning & \citet{jin2023cladder} \\
& Commonsense Reasoning & \citet{winogrande} \\
& Planning  & \citet{zheng2024naturalplanbenchmarkingllms} \\
%Self-Monitoring \& Correction & -- \\
\bottomrule
\end{tabular}
}
\caption{Core and Higher-Order Executive Functions.}
\label{tab:exe-func}
\vspace{-0.2em}
\end{table}


\begin{table}[t] \small \centering
\begin{tabular}{@{}ll@{}}\toprule
 \bf Cognitive Ability & \bf Benchmark \\ \midrule
Pragmatics & \citet{hu-etal-2023-fine} \\
Theory of Mind & \citet{gu:simpletom25} \\
Attribution and Judgement & \citet{gu:simpletom25} \\
Social Commonsense Reasoning & \citet{sap-etal-2019-social}\\
Emotional Intelligence & \citet{paech2023eqbench}\\
Emotion Regulation & -- \\
Self-Awareness & -- \\ 
Empathy & -- \\ 
Social Problem-Solving & -- \\
\bottomrule
\end{tabular}
\caption{Social and Emotional Skills.}
\label{tab:soc}
\vspace{-1.3em}
\end{table}

% taboo	A game where one player tries to get the other player to guess a word without using certain 'taboo' words in their clues.	incremental learning/processing	language model/world model
% wordle	A game where one player thinks of a word and the other player tries to guess it by suggesting words that are similar or related.	incremental learning/processing	language model/world model
% wordle_withclue	A variant of Wordle where the guesser is given a clue to help them guess the target word.	incremental learning/processing	language model/world model
% wordle_withcritic	A variant of Wordle where the guesser's suggestions are evaluated by a third player, who provides feedback on their accuracy.	incremental learning/processing	language model/world model
% imagegame	A game where one player draws a picture and the other player tries to guess what it represents.	multimodal grounding	situation model

% with a 
%\rf{should the Cognitive Abilities (taxonomy and corresponding datasets) be in a dedicated section or here as a subsection?}
%\as{I think this is a really interesting outcome of this work so probably worth promoting as a standalone section. However, we might want to make it a subsection if Ambra thinks it would be a strong claim.}
%\af{I think that the taxonomy is well-motivated and grounded in cognitive theory. However, there are no clear-cut top-down predictions in terms of how the different abilities should correlate with one another. I propose that we describe what emerges in an exploratory fashion.}

%%====================================
%\section{Experiment Design}
%%====================================
%\rf{we now need two experimental sections with our results: one with the correlation analysis and one focusing %on model size/family comparisons. Not sure what order makes more sense.}



%====================================
\subsection{Cognitive Ability Analysis}
%====================================
Equipped with our taxonomy and associated cognitive tests, we aim to shed some light on the cognitive abilities involved in interactive games and large QA benchmarks. 
%Besides direct model comparisons, this analysis allows us to explore whether there is any correlation between models' cognitive abilities and their performance on static or interactive benchmarks. 
Figure~\ref{fig:cogabilities} reports Kendall's $\tau$ correlation coefficients, with asterisks indicating statistical significance ($p\!<\!0.05$); see Appendix~\ref{sec:sup-plots} for a detailed correlation matrix between single datasets.
The analysis reveals that performance both on static and interactive evaluation correlates with performance on tests measuring higher-order reasoning abilities; while planning is more dominant in static problem-solving tasks, working memory seems to be beneficial for games. Among the social skills, pragmatics appears to be relevant for both static and interactive tests, while emotional intelligence and ToM correlate better with the latter.
%Figure~\ref{fig:cogabilities} reports Kendall's Tau correlations, %revealing that among core abilities, only working memory shows a %meaningful correlation with current evaluation methods, in particular %with interactive games (Private/Shared, Reference Game). Among higher-%order abilities, reasoning and planning seem to be behind good %performance on MMLU and BBH, and interestingly, they play a stronger %role in Reference Game than in Wordle or Taboo, where common-sense %reasoning shows a higher correlation.  
%Finally, among socio-emotional skills, we find that emotional %intelligence aligns moderately to strongly with success in interactive %games, while theory of mind is particularly linked to Taboo and %Wordle. Somewhat unexpectedly, static benchmarks show the strongest %association with pragmatic skills.
While these results suggest that interactive tests correlate more strongly with socio-emotional skills than static tests, this analysis remains speculative, as we still lack carefully curated cognitive abilities tests specifically designed for LLMs.\looseness-1

%how models' performance on MMLU, BBH, and interactive games correlates with the spectrum of their cognitive abilities. 
%The models' performance on clembench Reference game seems to correlate with their working memory ability (higher the former, higher the latter) --Kendall's Tau correlation (0.69); instead, for instance, social intelligence does not look to have any role in Clembench Taboo -- as one would expect given the nature of this game. Our analysis relies on the set of cognitive ability tests the community has produced so far; clearly work needs to be done on them for such analyses to become more reliable.


%\begin{figure}[h!]
    %\centering
%\begin{tabular}{cc}
% \includegraphics[width=0.45\textwidth]{images/geometric-fig/cognitive_abilities_all.png} \end{tabular}
% \caption{Cognitive Abilities Spectrum} \label{fig:cognitive-abilities}
%\end{figure}

\begin{figure}[h!]
    \centering
% \includegraphics[width=0.45\textwidth]{images/correlation_final-kendall/correlation_with_ca.png}
 \includegraphics[width=\columnwidth]{images/correlation_final-kendall/correlation_compressed.png}
 \caption{Correlation of cognitive abilities with Static and Interactive assessments (* indicates $p< 0.05$).}  
 \label{fig:cogabilities}
 \vspace{-0.9em}
\end{figure}
  

%\begin{figure*}[h!]
 %   \centering
%\begin{tabular}{ccc}
 % \includegraphics[width=0.35\textwidth]{images/geometric-fig/cognitive_abilities_all.png} &
 %\includegraphics[width=0.45\textwidth]{images/correlation_final-kendall/scatter-plotts/referencegame_wm.png} & \includegraphics[width=0.4\textwidth]{images/correlation_final-kendall/scatter-plotts/tamboo_social.png}
 % \end{tabular}
  % \caption{Cognitive Abilities Spectrum (left) and Correlation with Cognitive Abilities: high (middle) vs.\ low (right) correlation}  \label{fig:cogabilities}
 %\end{figure*}


%\todo[inline]{
%Which cognitive abilities correlate with performance gain in multi-task static benchmarks vs interactive games. Figure~\ref{fig:correlation} and Figure~\ref{fig:scatterplots}
%}

%\begin{figure*}[h!]
 %\caption{Correlation with Cognitive Abilities}
  %  \centering
 %\includegraphics[width=1\textwidth]{images/correlation_final-kendall/correlation_with_ca.png}
%\end{figure*}
 %   \label{fig:correlation}




%====================================
\section{Related Work}
\label{sec:related}
%====================================

 \citet{waldis:holmes24} proposes {\tt Holmes} as a framework to assess the English linguistic competence of language models. They evaluate models' competence (morphology, syntax, and semantics) by comparing them across architectures and sizes by probing their internal representations. %through the use of probing classifiers from the models' internal representations. 
 %They show that overall models excel at  morphology and syntax, that such competence scales with model size and is further boosted by instruction tuning. Whereas the architecture has an impact on the models' lexical semantic competence with the encoder models encoding linguistic phenomena better. Moreover, they compare models on their logical deduction reasoning ability, paying particular attention to negation, and to discourse focusing on rhetorical structure showing, again through probing classifiers, that LMs encode less information about these functional phenomena. 
%Finally, 
Moreover, by measuring the correlation between {\tt Holmes} and downstream tasks results, they observe that morphology highly correlates with reasoning. Rather than on formal linguistic competence, we focus on functional linguistic competences and compare them not just with large QA benchmarks but also with interactive games.
%\citet{lu-etal-2024-emergent} instead evaluate models on functional linguistic competence, present in BigBench~\cite{srivastava2023beyond}, aiming to verify whether they are "emergent abilities", as claimed in the literature, and show that they are the result of a combination of in-context learning, model memory, and linguistic knowledge.
%Social reasoning reviewed in~\citet{ma-etal-2023-towards-holistic}. 
\citet{ma-etal-2023-towards-holistic} carry out a holistic evaluation of LLMs' Theory of Mind by inspecting the literature through the competences a model with a ToM should have based on a known taxonomy. 
%This taxonomized review of benchmarks highlights which abilities are already evaluated and which are not.  
%\citet{hu2024survey} call the attention on games as an instrument to study the emergent abilities of LLMs, considering their core functional components: perception, memory, thinking, role-playing, action, and learning. 
Similarly, we take a top-down approach but consider the whole spectrum of cognitive abilities and highlight the importance of connecting them with the complementary benchmarks largely used by the community to monitor LLMs' progress.



%and take a picture of the current evaluation landscape both for executive functional and social and emotional abilities. 


%====================================
\section{Conclusion}
%====================================
Our results show the different discriminating power of interactive games over one-turn static large QA benchmarks.
 Crucially, we argue that in order to claim that LLMs have emerging abilities, measuring performance on large QA benchmarks or interactive games is not sufficient per se, but should rather be %accompanied by correlation analysis 
 triangulated with controlled tests designed to evaluate such abilities. Furthermore, we highlight the potential value of carefully designed controlled benchmarks inspired by human cognitive ability assessment as a good means for such correlation analyses. While each cognitive assessment test alone does not get us very far in the quest for robust LLM evaluation, we contend that this type of evaluation paradigm has the potential to enhance our understanding of what fundamental abilities LLMs must develop to be able to function effectively as language agents, where multiple skills may be required and possibly  interact.   
 Nevertheless, we agree with \citet{millire2024anthropocentric} that caution should be exerted before drawing conclusions about LLMs' abilities from these tests meant for humans. New carefully designed behavioural experiments for LLMs should be proposed, and supplemented with mechanistic studies.

 
 %Hence, we call for future work for the creation  of new interactive benchmarks which can discriminate well between models and which correlate strongly with underlying cognitive abilities. 

%\tb{The longstanding link between AI and cognitive science has recently been strongly reinforced in several papers which describe it as a new emerging field, Machine Psychology\cite{hagen:machinepsy} or AI Psychology\cite{ivan:howt25}, the importance of profiting of the evaluation toolkit from developmental psychology has been advocated by~\citet{FRANK2023990}.}

\section*{Limitations}
Our evaluation prompts models to provide direct answers without employing chain-of-thought (CoT) reasoning or similar capability elicitation techniques. 
While different elicitation strategies may enhance question-answering, interactive, and cognitive abilities in different ways \citep{shunyu-etal-2023-tree,hao-etal-2023-reasoning,li-etal-2024-emotional}, we opted for an approach that remains agnostic to specific evaluation methods and datasets. 
This ensures a consistent basis for comparison across models, though future work could explore how alternative prompting strategies influence performance across the three evaluation paradigms.
Moreover, for the cognitive abilities assessments, we used currently available datasets; such resources have started to be compiled only very recently, hence the tests we used may not guarantee to evaluate the intended abilities in LLMs. Nevertheless, they help in establishing our message and call for more analysis in such direction.\looseness-1 

%\rf{add point on the fact that the cog abilities datasets may not be perfect to evaluate the intended abilities}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{biblio}

\appendix


\section*{Appendix}


\section{Models}
\label{sec:appendix-models}
The Olmo-2-1124 series~\cite{olmo20242olmo2furious} includes models with 7 billion and 13 billion parameters (\texttt{OLMo-2-1124-*-Instruct}). Both models are designed for a variety of tasks, including chat, mathematics, and reasoning. They have undergone supervised fine-tuning on the Tülu 3 dataset and further training using DPO techniques.

The Qwen2.5 series~\cite{qwen2, qwen2.5} includes models with 7B, 32B, and 72B parameters (\texttt{Qwen2.5-*-Instruct}). They are multilingual, supporting over 29 languages, and excel in coding, mathematics, and instruction following.

The Llama-3 series~\cite{dubey2024llama3herdmodels} includes models with 8B (\texttt{Llama3.1-8B-Instruct}) and 70B parameters (\texttt{Llama3.3-70B-Instruct}). These models are optimized for multilingual dialogue and support various languages. They use an optimized transformer architecture and are fine-tuned for instruction following.

The Falcon3 series~\cite{falcon3} includes a model with 10 billion parameters. It achieves state-of-the-art results in reasoning, language understanding, instruction following, code, and mathematics tasks. It supports four languages (English, French, Spanish, Portuguese) and a context length of up to 32K.

\section{Interactive Games}\label{appendix:clembench}

We leverage \textit{clembench}~\cite{chalamalasetti-etal-2023-clembench}, a benchmark that assesses models' gameplay ability in well-defined dialogue games such as:

1) \textit{Taboo}: a game where one player tries to get the other player to guess a word without using certain `taboo' words in their clues; 

2) \textit{Wordle}: a game where one player thinks of a word and the other player tries to guess it by suggesting words that are similar or related;

3) \textit{Wordle (Clue)}: a variant of Wordle where the guesser is given a clue to help them guess the target word; 

4) \textit{Wordle (Critic)}: A variant of Wordle where the guesser's suggestions are evaluated by a third player, who provides feedback on their accuracy; 

5) \textit{Reference Game}: a game where one player is presented with three grids and is tasked to make the other---who is also presented with the same three grids, but potentially in a different order---identify a pre-specified one;

6) \textit{Private/Shared}: a game where a customer agent goes through a form with a service agent and, after each turn, a third agent,\footnote{
    In clembench, all interactions are mediated by a ``Game Master''. This agent plays a particularly active role in \textit{Private/Shared}.
} probes the customer on what they believe that the service agent already knows;
% this game makes use of the fact that in clembench, all interactions are mediated by a ``Game Master''. The main interaction is between a customer and a service agent, where the customer knows a set of slot values that the service agent needs to ask for. After each turn, the Game Master separately queries customer and agent which information is already known and which is not; 
hence, which information is ``private'' and which is ``shared''.

% \subsubsection*{Taxonomy of Cognitive Abilities}
\section{Taxonomy of Cognitive Abilities}
\label{subsec:appendix-taxonomy}






\begin{itemize}
\item Executive Functions:
\begin{itemize}
\item Core abilities
\begin{description}
\item[Working Memory:] Hold and manipulate information in mind over short periods;
\item[Inhibitory Control] Suppress automatic, inappropriate, or impulsive responses and resist distractions;
\item[Cognitive Flexibility:] Adapt to new situations, switch between tasks, and think about multiple concepts simultaneously;
%\item[Self-monitoring and Error correction:] Assess one's performance and behavior to ensure they align with desired outcomes, or correct otherwise.
\end{description}

\item Higher-order abilities
\begin{description}

\item[Planning:] Set goals, develop steps to achieve them, and anticipate potential obstacles;
\item[Causal Reasoning:] Understand cause-and-effect relationships;
\item[Logical Reasoning]: Deductive and inductive reasoning;
%\item[Numerical Reasoning]: Quantitative reasoning and arithmetic problem-solving.
\item[Commonsense Reasoning:] Apply general common knowledge to everyday scenarios, including understanding basic physical properties, such as gravity, solidity, and object interaction;
%\item[Self-monitoring and Error correction:] Assess one's performance and behavior to ensure they align with desired outcomes, or correct otherwise. 
\end{description}
\end{itemize}

\item Socio-emotional skills:
\begin{description}
\item[Social Commonsense Reasoning:] Understand social norms and expectations;
\item[Social Problem-Solving:] Analyze social situations, generate solutions, and make decisions that foster positive interactions;
\item[Emotional Intelligence:]  Recognize, interpret, and 
manage one’s own and others’ emotions.
\item[Emotion Regulation:] Manage and modify one's emotional responses in appropriate ways;
\item[Self-Awareness:] Recognize and understand one's own emotions, thoughts, and behaviors;%subgroup of EMI?
\item[Empathy:] Share and understand the feelings of others, both emotionally and cognitively;
\item[Theory of Mind:] Understand that others have thoughts, beliefs, desires, and intentions different from one's own;
\item[Attribution and Judgment:] Interpret the causes of others' behavior, distinguishing between intentional and unintentional actions.
\item[Pragmatics:] Aspects of communication that go beyond formal language competence: considering communicative intentions, the communicative context of the utterance, shared knowledge between speakers, manners, social and cultural norms.
 \end{description}

\end{itemize}

% \subsubsection*{Benchmarks used to evaluate the Cognitive Abilities}
\section{Benchmarks for Cognitive Abilities}

% \begin{itemize}
%     \item[WorkingMemory:] 
%     \item[LLM-Cognitive-Flexibility:]
%     \item[LogiQA 2.0:]
%     \item[CLadder:]
%     \item[WinoGrande:]
%     \item[NATURAL PLAN:]
%     \item[EQBench:]
%     \item[LM-Pragmatics:]
%     \item[SocialIQA:]
%     \item[SimpleToM:]
% \end{itemize}

\begin{description}
    \item[Working Memory] ~\cite{workingmemory} (referred as \textit{WM} in this work)  is a set of verbal and spatial n-back tasks presented with three levels of difficulties from $n=1$ to $n=3$. The model has to identify whether the current stimulus (a letter in a string or a spatial location in a grid) is the same as the $n$ back stimulus or not.

    \item[Cognitive Flexibility] ~\cite{kennedy2024cognitive} (referred as \textit{LLM-Cognitive-Flexibility} in this work) aims to test to what degree LLMs can rapidly switch tasks within a single context window. To this end, it employs two neuropsychological tests, the Wisconsin Card Sorting Test (WCST)~\cite{grant:wcst48} and the Letter-Number Test (LNT)~\cite{roger:costs93} commonly used to measure cognitive flexibility in humans.

    \item[Logical Reasoning] \textit{LogiQA 2.0}~\cite{logiqa} This dataset evaluates logical reasoning using the same data both in NLI and Machine Reading Comprehension format (text, question, multiple-choice) for each of the following (deductive) reasoning types: categorical, sufficient condition, necessary condition, disjunctive, conjunctive reasoning.

    \item[Causal Reasoning] \textsc{CLadder}~\cite{jin2023cladder} focus es on formal causal reasoning (causal inference), as opposed to commonsense causal reasoning. The dataset is constructed from formal logic-based templates that are then verbalised into natural language as binary questions.

    \item[Commonsense Reasoning] \textit{WinoGrande}~\cite{winogrande} A large-scale dataset of 44k commonsense reasoning problems consisting of pairs of nearly identical questions with two answer choices (as in the original Winograd Schema Challenge~\cite{morgenstern:wsc15} but without its bias). 

    \item[Planning] \textsc{Natural Plan}~\cite{zheng2024naturalplanbenchmarkingllms} is a realistic planning benchmark consisting of three tasks expressed in natural language: Trip Planning, Meeting Planning and Calendar Scheduling. Models are given a situation and a problem to solve (e.g. find a trip plan that satisfies some constraints given the situation described.) Each task contains problems of different levels of complexity based on the number of cities, people or days involved. The problems are often based on numerical reasoning too. We evaluate models on the Trip Planning and the Calendar Scheduling tasks.

    \item[Emotional Intelligence] \textit{EQ-Bench}~\cite{paech2023eqbench} the model is given an emotionally charged short dialogue (generated by GPT-4) and has to score the four possible emotions felt by a given character. Scores are compared against a reference score. 

    \item[Pragmatics] \cite{hu-etal-2023-fine} (referred as \textit{LM-Pragmatics} in this work) is a benchmark evaluating LLMs' understanding of seven pragmatics phenomena: deceit, indirect speech, irony, maxims, metaphor, humour, and coherence. Scenarios are grounded into social situations, requiring LLMs to interpret utterances. The task is designed as a multi-choice questionnaire with 2-5 questions based on the subtask.

    \item[Social Commonsense] \textsc{Social IQa} \cite{sap-etal-2019-social} a dataset for evaluating social commonsense reasoning and emotional intelligence. Each sample includes a short scenario and three multiple-choice questions across six categories: intentions, reactions, descriptions, motivations, needs, and consequences. Transfer learning on this dataset has shown strong performance on other commonsense reasoning benchmarks.

    \item[Attribution and Judgment/Theory of Mind] \textit{SimpleToM}\cite{gu:simpletom25} contains concise, diverse stories each with questions that ask models to predict behavior ("Will Mary pay for the chips or report the mold?"), judgment ("Mary paid for the chips. Was that reasonable?") or mental states ("Is Mary likely to be aware that 'The can of Pringles has moldy chips in it.'? Yes or No?") The first two subtasks have been taken as a reference for the Attribution and Judgment cognitive ability, while the last as a reference for Theory of Mind.
\end{description}


% \subsubsection*{Benchmark Implementations}
\section{Benchmark Implementations}
For the majority of the static benchmarks evaluated in this work we relied on the popular framework for the evaluation of LLMs \textit{lm-eval}\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}} (ver. 0.4.7), which already made available many of the selected benchmarks, and enabled a common interface for the implementation of most of the remaining ones.  

The benchmarks which were already present within the framework are: \textsc{Social IQa}~\cite{sap-etal-2019-social}, WinoGrande \cite{winogrande}, EQ-Bench \cite{paech2023eqbench}, LogiQA 2.0 \cite{logiqa}, MMLU \cite{hendryckstest2021}. 
The benchmarks which have been implemented in the framework over the course of the study are: \textsc{CLadder} \cite{jin2023cladder}, LM-Pragmatics \cite{hu-etal-2023-fine}, SimpleToM \cite{gu:simpletom25}, \textsc{Natural Plan} \cite{zheng2024naturalplanbenchmarkingllms}.

As for BBH and IFEval, they were also available in the framework and potentially usable, however we decided to rely on the scores made available by Huggingface in their Open Leaderboard 2 \footnote{\url{https://huggingface.co/spaces/open-llm-leaderboard/blog}} as they also rely on the \textit{lm-eval} framework for evaluation. 
This has been possible for all of the models except one (\textit{OLMo-2-1124-13B-Instruct}), for which the scores were not available in the leaderboard at the time of this study. In this case, we have used the code made available by Huggingface and followed their instruction for reproducing the results (in particular, given the instruction-tuned nature of the model, the settings \textit{apply$\_$chat$\_$template} and \textit{fewshot$\_$as$\_$multiturn} were applied.

As for the interactive games, we have used the implementation provided by version 1.5 of the \textit{clembench} \cite{chalamalasetti-etal-2023-clembench}. The remaining benchmarks (WM, LLM-Cognitive-Flexibility) have been implemented outside of the framework, as \textit{lm-eval} did not provide support for the multi-turn nature of the tasks. 

\subsection{Zero-shot and Few-shot Tasks}   
The majority of the tasks have been evaluated in a zero-shot setting with the exception of MMLU (5-shot), BBH (3-shot) (following common practices in model evaluation, e.g. in the Open Leaderboard 2 fo BBH) and \textsc{Natural Plan} (5-shot). In the case of \textsc{Natural Plan}, our models performed really poorly when evaluated in a zero-shot fashion---resulting in scores close to 0. Given that the task relies on the models producing answers in a strict format for parsing, we opted for using the 5-shot version provided by the benchmark's authors.

% Ale stopped reviewing here; to be continued.

\subsection{Metrics}
\subsubsection{Evaluation}
In the evaluation of models, we followed the original works' implementations as well as associated metrics. However, it may be the case that for a certain benchmark more metrics were defined, or that the original work did not aggregate results across subtasks. For this reason, we report here the metrics we used for evaluating models.

In the case of \textit{Clembench games}, we computed performance by computing the ratio between the quality score (a number from 0 to 100) and the percentage of played games (a number between 0 and 1) divided by 100. 

In the case of \textit{IFEval}, following what was done in the Open Leaderboard 2, we averaged the results obtained on prompt-level and instruction-level strict accuracy.

As for \textit{EQ-Bench}, we computed the task-specific score as it was implemented in the \textit{lm-eval}.

Regarding \textit{WM}, we only considered the subtask \textit{Verbal N-3}, and we computed the accuracy for the results obtained across the 50 trials defined in the original work.

In the case of \textit{LLM-Cognitive-Flexibility}, we ran each subtask 8 times with 25 trials each, and computed the average of the accuracy obtained in each run. In this case, the accuracy was computed only on the trials for which response parsing was successful. We then averaged the accuracy obtained on both subtasks to compute the final score.

In the case of \textsc{CLadder}, we followed the original work which treated the task as generative and probed for the presence of the substrings "yes"/"no" at the beginning of the model's answer.

In \textsc{Natural Plan}, the original work defined a rule-based procedure to parse specific data from the generated plan (e.g., dates). We reuse their parsing procedure and verify whether the expected elements are all present in the parsed plan.

For the remaining tasks (LogiQA 2.0, WinoGrande, LM-Pragmatics, \textsc{Social IQa}, MMLU, BBH, SimpleToM), we treated them as a multiple-choice question answering task that is evaluated based on the likelihood of the correct answer for the task.

In the case of BBH, the Open Leaderboard 2's evaluation code excludes three of the original tasks from the overall score's computing: \textit{dyck languages}, \textit{navigate} and \textit{word sorting}. The performances on these subtasks are therefore also ignored in the performance reported in this study.

In the case multiple subtasks were present (LM-Pragmatics, MMLU, BBH, \textsc{Natural Plan}, LLM-Cognitive-Flexibility), we computed the micro-average over the results achieved on each subtask. In the specific case of SimpleToM, since the subtasks were associated with two different Cognitive Abilities, we've aggregated the score of the subtasks \textit{behaviour} and \textit{judgment} into a single score (under Attribution and Judgment), and considered the \textit{mental state} subtask separately (under Theory of Mind).

\subsubsection{Correlation}
For measuring the pair-wise correlation between benchmarks, we've computed the Kendall rank correlation coefficient (or Kendall's Tau) (Tau-b version). It measures rank correlation according to this formula: 

\[
\tau_b = \frac{P - Q}{\sqrt{(P + Q + T_x)(P + Q + T_y)}}
\]
where:
\begin{align*}
P &= \text{number of concordant pairs}, \\
Q &= \text{number of discordant pairs}, \\
T_x &= \text{tie correction for variable } X, \\
T_y &= \text{tie correction for variable } Y.
\end{align*}

This method was preferable over the others given its robustness in case of few data points, as it was in our case.
We have also experimented with the Pearson correlation coefficient and observed that in the majority of the cases, the correlation patterns were similar, however with larger positive as well as negative correlations compared to Kendall.
We've relied on the implementation provided by the \textit{pingouin} Python package \cite{pingouin}.
\subsection{Generation Settings}
The tasks which required the models to generate text are: EQ-Bench, WM, MMLU, IFEval, the clembench games, LLM-Cognitive-Flexibility, \textsc{Natural Plan}, \textsc{CLadder}.
With the exception of Working Memory and LLM-Cognitive-Flexibility, all tasks have been evaluated by applying a temperature of 0.
Following the original implementation, we have applied a temperature of 1 to WM and 0.7 to LLM-Cognitive-Flexibility. In these cases, however, the increased randomness caused by the higher temperature was mitigated by averaging the results obtained over multiple trials.

As for the other generation settings, we also have followed what was prescribed in the original works regarding the tokens for the termination of the generation, the maximum or minimum number of tokens. 
In the case of \textsc{Natural Plan}, the original work did not provide specific information regarding the settings they have adopted for the evaluation. Given the highly challenging nature of the task, we have set the minimum and maximum number of tokens to 90 and 350 respectively. This was derived based on the minimum and maximum number of tokens in the gold plans. 

\section{Limitations in the Evaluations}
In certain cases, results have not been computed on all the subtasks available for that benchmark. In addition to BBH, we also made special arrangements for Working Memory and \textsc{Natural Plan}.
In the case of \textsc{Natural Plan}, we have not considered results coming from the \textit{meeting} subtask, while for WM we have only considered those coming from the \textit{Verbal (Base) N-3} subtask. In the first case, the high amount of resources required for evaluating the task, especially for the larger models (60\% of the prompts contained above 14k space-separated words (up to 38.3k) vs 100\% below 15.1k for the \textit{travel} subtask and below 7.09k for the \textit{calendar} subtask prevented us from doing so. As for the second, we've only considered the base version of the verbal subtask and excluded its variations as they would not provide meaningful information for this study. 

\section{Model Implementations}
All the models used in this study have been made available by Huggingface, and have been accessed through their \textit{transformers} \cite{wolf-etal-2020-transformers} library.
For text generation, we have been applying the default chat template specified within the same library.

\section{Computational Resources}
As a reference, the time required for running through all the benchmarks for the Llama-3.1-8B-Instruct model on 1 A100 GPU with batch size set to 'auto' in the \textit{lm-eval} (i.e. it automatically fits into the memory the maximum batch size possible for each task). For the Clembench games, LLM-Cognitive-Flexibility and WM, the batch size is 1. The time also includes time required for procedures performed by the \textit{lm-eval} prior to the actual evaluation (only for those datasets evaluated through the framework) and loading the model into the memory (all tasks).
\begin{description}
    \item[LLM-Cognitive-Flexibility:] \char`~1:50 min
    \item[LogiQA 2.0:] \char`~5 min 
    \item[\textsc{CLadder}:] \char`~19:30 min
    \item[WinoGrande:] \char`~1 min
    \item[\textsc{Natural Plan}:] \char`~4:50 hours
    \item[WM] \char`~2:40 min
    \item[EQ-Bench:] \char`~3 min 
    \item[LM-Pragmatics:] \char`~6:30 min 
    \item[\textsc{Social IQa}:] \char`~1:30 min 
    \item[SimpleToM:] \char`~2:40 min 
    \item[MMLU:] \char`~14 min 
    \item[Taboo:] \char`~3:30 min
    \item[Reference Game:] \char`~3:00 min
    \item[Image Game:] \char`~2.40 min
    \item[Wordle:] \char`~7:50 min
    \item[Wordle (Critic):] \char`~2:50 min
    \item[Wordle (Clue):] \char`~2:15 min
    \item[Private/Shared:] \char`~17:30 min
\end{description}

%\section{Example Appendix}
%\label{sec:appendix}

%This is an appendix.

\section{Supplementary Plots}
\label{sec:sup-plots}

%\subsection{Static vs.}

Figure~\ref{fig:additional_disc} and Figure~\ref{fig:additional_disc_same_size} show the supplementary plots for the results in Section~\ref{sec:static-interactive} (comparing models of different size but same family and models of similar size respectively). Moreover, we report the supplementary plots for the results in Section~\ref{sec:eval_ca}. Figure~\ref{fig:cogabilities_all} presents a direct comparison of models based on our selected cognitive tests.   Figure~\ref{fig:cogabilities_complete} reports an extended version of Figure~\ref{fig:cogabilities}. Finally, Figure~\ref{fig:scatterplots} reports by means of example two scatter plots representing respectively situations of high and low correlation between two benchmarks (a game and a cognitive ability).



\begin{figure*}[h!]
    \centering
\begin{tabular}{cc}
    \centering
  \includegraphics[width=0.45\textwidth]{images/additional_disc/olmo_disc.png}  &
  \includegraphics[width=0.45\textwidth]{images/additional_disc/qwen_7_32_disc.png} \\   
  \includegraphics[width=0.45\textwidth]{images/additional_disc/qwen_7_72_disc.png} &  
  \includegraphics[width=0.45\textwidth]{images/additional_disc/qwen_32_72_disc.png}    
\end{tabular}\caption{Comparing datasets in their power to discriminate models across size.}\label{fig:additional_disc}
  \end{figure*}

\begin{figure*}[h!]
    \centering
\begin{tabular}{cc}
    \centering
  \includegraphics[width=0.45\textwidth]{images/additional_disc/qwen_7_llama8_disc.png}  &
  \includegraphics[width=0.45\textwidth]{images/additional_disc/llama_8_olmo_7_disc.png} \\   
  \includegraphics[width=0.45\textwidth]{images/additional_disc/olmo_2_qwen_7_disc.png} &  
  \includegraphics[width=0.45\textwidth]{images/additional_disc/olmo_2_falcon_10.png}    
\end{tabular}\caption{Comparing datasets in their power to discriminate models with similar size across families.}\label{fig:additional_disc_same_size}
  \end{figure*}


%Among higher-order executive functions, all models heavily struggle with planning and causal reasoning, though larger models perform better than smaller ones.
%While larger models exhibit some degree of working memory, they lack cognitive flexibility to the same extent as smaller models. In the domain of social and emotional abilities, the weakest area for all models is attribute/judgement (or applied theory of mind). 

\begin{figure}[h!]
    \centering
  \includegraphics[width=0.45\textwidth]{images/geometric-fig/cognitive_abilities_all.png}    \caption{Cognitive Abilities Spectrum of LLMs}  \label{fig:cogabilities_all}
  \end{figure}


\begin{figure}[h!]
    \centering
 \includegraphics[width=0.45\textwidth]{images/correlation_final-kendall/correlation_with_ca.png}
   \caption{Correlation of cognitive abilities with Large QA benchmarks and Interactive Games. The correlation matrix does not include results on Wordle and Image Game as model performances' were too low.}  \label{fig:cogabilities_complete}
\end{figure}



\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
 \includegraphics[width=0.45\textwidth]{images/correlation_final-kendall/scatter-plotts/referencegame_wm.png} &
 \includegraphics[width=0.45\textwidth]{images/correlation_final-kendall/scatter-plotts/taboo_simpletom.png}
\end{tabular}
   \caption{High (left) and low (right) correlation }  \label{fig:scatterplots}
\end{figure}
\end{document}
