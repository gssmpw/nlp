% without SA 1
% without IT 2
% without UB 3
% without contrastive learning  1.5
\begin{table}[h]
\centering
{
\scalebox{0.85}{
\begin{tabular}{@{}ccccccc@{}}
\toprule
\textit{LLM} & \multicolumn{2}{c}{\textit{Qwen2-7b}} &\multicolumn{2}{c}{\textit{Llama3-8b}} & \multicolumn{2}{c}{\textit{GPT-3.5}} \\
\midrule
\textit{metric} & acc & f1  & acc & f1 & acc & f1  \\
\midrule
\textit{\Ours} & \textbf{56.0} & \textbf{52.73} & \textbf{43.8} & \textbf{43.5} & \textbf{46.7} & \textbf{45.95} \\
\midrule
\multicolumn{7}{c}{\textit{Multi-granular Content Filter }} \\
\midrule
\textit{w/o PF} & 52.2 & 49.87 & 41.5 & 40.51 & 43.5& 42.37\\
\textit{w/o SF} & 55.1 & 50.67 & 42.8 & 42.17 & 45.2 & 43.83\\
\textit{w/o ALL} & 51.6 & 48.39 & 40.9 & 40.12  &  42.4 & 41.65\\
\midrule
% \textit{w/o UB} & 0.0183 & 0.0220 & 0.0287 & 0.0324  \\ 
% \textit{w/o ALL} & 0.0180 & 0.0219 & 0.0285 & 0.0313  \\
\multicolumn{7}{c}{\textit{Agent-Based Memory}} \\
\midrule
\textit{w/o AM} & 53.5 & 50.97 & 42.7 & 41.63  & 44.3 & 42.85\\
\bottomrule
\end{tabular}
}
}
\caption{\textbf{Ablation study on the 2wikiMQA dataset.}}
% The table evaluates the impact of different content filter components (Paragraph-Level Filter (PF), Sentence-Level Filter (SF)) and the Agent-Based Memory (AM) module. Results demonstrate that using both content filter components achieves optimal performance, while removing any single component degrades performance. The Agent-based Memory module is shown to be critical for maintaining high QA answer accuracy.
\label{tab:4_ablation_qqt}
\vspace{-0.2in}
\end{table}

