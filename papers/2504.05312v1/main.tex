% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{multirow}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{tcolorbox}
\usepackage{subcaption}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]
{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{colortbl}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{booktabs}


% \newcommand{\Ours}[0]{AdamRAG\xspace}
% \newcommand{\Ours}[0]{ARMAR\xspace}
\newcommand{\Ours}[0]{Amber\xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{AURA: Adaptive Understanding with Multi-Granular Filtering and Agent-Based Memory Augmentation}
% \title{ARMAR: Adaptive Retrieval with Multi-Granular Filtering and Agent-Based Memory Augmentation}
% \title{Towards Adaptive Memory-Updating for Enhanced Retrieval-Augmented Generation }
\title{Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation}

\author{%
   Qitao Qin, Yucong Luo,Yihang Lu, Zhibo Chu, Xianwei Meng
   \\
   University of Science and Technology of China \\
   Hefei, Anhui, China \\
  %, $^\spadesuit$Tencent \\
  \texttt{\{qqt,prime666,lyhsa22,zb.chu\}@mail.ustc.edu.cn} \\ \texttt{mengxw@iim.ac.cn}\\
  }
% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\input{sections/0_abstract}

\input{sections/1_introduction}

\input{sections/2_related_work}

\input{sections/3_preliminaries}

\input{sections/4_method}

\input{sections/5_experiment_setup}

\input{sections/6_experiments}
\input{sections/7_conclusion}

\bigskip
% \bibliography{ref}
\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}.

\bibitem[{Asai et~al.(2023)Asai, Wu, Wang, Sil, and Hajishirzi}]{asai2023self}
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.
\newblock Self-rag: Learning to retrieve, generate, and critique through self-reflection.
\newblock \emph{arXiv preprint arXiv:2310.11511}.

\bibitem[{Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, Van Den~Driessche, Lespiau, Damoc, Clark et~al.}]{borgeaud2022improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George~Bm Van Den~Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et~al. 2022.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pages 2206--2240. PMLR.

\bibitem[{Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning}]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning. 2015.
\newblock A large annotated corpus for learning natural language inference.
\newblock \emph{arXiv preprint arXiv:1508.05326}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:1877--1901.

\bibitem[{Chen(2017)}]{chen2017reading}
D~Chen. 2017.
\newblock Reading wikipedia to answer open-domain questions.
\newblock \emph{arXiv preprint arXiv:1704.00051}.

\bibitem[{Chen et~al.(2023)Chen, Zaharia, and Zou}]{chen2023chatgpt}
Lingjiao Chen, Matei Zaharia, and James Zou. 2023.
\newblock How is chatgpt's behavior changing over time?
\newblock \emph{arXiv preprint arXiv:2307.09009}.

\bibitem[{Gao et~al.(2023)Gao, Yen, Yu, and Chen}]{gao2023enabling}
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023.
\newblock Enabling large language models to generate text with citations.
\newblock \emph{arXiv preprint arXiv:2305.14627}.

\bibitem[{Ho et~al.(2020)Ho, Nguyen, Sugawara, and Aizawa}]{ho2020constructing}
Xanh Ho, Anh-Khoa~Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
\newblock Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.
\newblock \emph{arXiv preprint arXiv:2011.01060}.

\bibitem[{Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave}]{izacard2023atlas}
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{Journal of Machine Learning Research}, 24(251):1--43.

\bibitem[{Jeong et~al.(2024)Jeong, Baek, Cho, Hwang, and Park}]{jeong2024adaptive}
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung~Ju Hwang, and Jong~C Park. 2024.
\newblock Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity.
\newblock \emph{arXiv preprint arXiv:2403.14403}.

\bibitem[{Jiang et~al.(2023)Jiang, Xu, Gao, Sun, Liu, Dwivedi-Yu, Yang, Callan, and Neubig}]{jiang2023active}
Zhengbao Jiang, Frank~F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.
\newblock Active retrieval augmented generation.
\newblock \emph{arXiv preprint arXiv:2305.06983}.

\bibitem[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer. 2017.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock \emph{arXiv preprint arXiv:1705.03551}.

\bibitem[{Karpukhin et~al.(2020)Karpukhin, O{\u{g}}uz, Min, Lewis, Wu, Edunov, Chen, and Yih}]{karpukhin2020dense}
Vladimir Karpukhin, Barlas O{\u{g}}uz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.
\newblock Dense passage retrieval for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2004.04906}.

\bibitem[{Khattab et~al.(2022)Khattab, Santhanam, Li, Hall, Liang, Potts, and Zaharia}]{khattab2022demonstrate}
Omar Khattab, Keshav Santhanam, Xiang~Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.
\newblock Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp.
\newblock \emph{arXiv preprint arXiv:2212.14024}.

\bibitem[{Khot et~al.(2022)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and Sabharwal}]{khot2022decomposed}
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022.
\newblock Decomposed prompting: A modular approach for solving complex tasks.
\newblock \emph{arXiv preprint arXiv:2210.02406}.

\bibitem[{Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee et~al.}]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et~al. 2019.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:453--466.

\bibitem[{Lazaridou et~al.(2022)Lazaridou, Gribovskaya, Stokowiec, and Grigorev}]{lazaridou2022internet}
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022.
\newblock Internet-augmented language models through few-shot prompting for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2203.05115}.

\bibitem[{Lyu et~al.(2024)Lyu, Li, Niu, Xiong, Tang, Wang, Wu, Liu, Xu, and Chen}]{lyu2024crud}
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo~Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2024.
\newblock Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models.
\newblock \emph{ACM Transactions on Information Systems}.

\bibitem[{Ni et~al.(2021)Ni, Qu, Lu, Dai, {\'A}brego, Ma, Zhao, Luan, Hall, Chang et~al.}]{ni2021large}
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo~Hern{\'a}ndez {\'A}brego, Ji~Ma, Vincent~Y Zhao, Yi~Luan, Keith~B Hall, Ming-Wei Chang, et~al. 2021.
\newblock Large dual encoders are generalizable retrievers.
\newblock \emph{arXiv preprint arXiv:2112.07899}.

\bibitem[{Rajpurkar(2016)}]{rajpurkar2016squad}
P~Rajpurkar. 2016.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}.

\bibitem[{Ram et~al.(2023)Ram, Levine, Dalmedigos, Muhlgay, Shashua, Leyton-Brown, and Shoham}]{ram2023context}
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.
\newblock In-context retrieval-augmented language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 11:1316--1331.

\bibitem[{Robertson et~al.(1995)Robertson, Walker, Jones, Hancock-Beaulieu, Gatford et~al.}]{robertson1995okapi}
Stephen~E Robertson, Steve Walker, Susan Jones, Micheline~M Hancock-Beaulieu, Mike Gatford, et~al. 1995.
\newblock Okapi at trec-3.
\newblock \emph{Nist Special Publication Sp}, 109:109.

\bibitem[{Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih}]{shi2023replug}
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.
\newblock Replug: Retrieval-augmented black-box language models.
\newblock \emph{arXiv preprint arXiv:2301.12652}.

\bibitem[{Stelmakh et~al.(2022)Stelmakh, Luan, Dhingra, and Chang}]{stelmakh2022asqa}
Ivan Stelmakh, Yi~Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022.
\newblock Asqa: Factoid questions meet long-form answers.
\newblock \emph{arXiv preprint arXiv:2204.06092}.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample}]{Touvron2023LLaMAOA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur{\'e}lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023{\natexlab{a}}.
\newblock \href {https://api.semanticscholar.org/CorpusID:257219404} {Llama: Open and efficient foundation language models}.
\newblock \emph{ArXiv}, abs/2302.13971.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{b}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Trivedi et~al.(2022)Trivedi, Balasubramanian, Khot, and Sabharwal}]{trivedi2022interleaving}
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.
\newblock Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.
\newblock \emph{arXiv preprint arXiv:2212.10509}.

\bibitem[{Voorhees et~al.(1999)}]{voorhees1999trec}
Ellen~M Voorhees et~al. 1999.
\newblock The trec-8 question answering track report.
\newblock In \emph{Trec}, volume~99, pages 77--82.

\bibitem[{Wang et~al.(2024{\natexlab{a}})Wang, Xue, Zhou, Zhang, Wang, Chen, Wang, and Wong}]{wang2024self}
Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Guanhua Chen, Huimin Wang, and Kam-fai Wong. 2024{\natexlab{a}}.
\newblock Self-dc: When to retrieve and when to generate? self divide-and-conquer for compositional unknown questions.
\newblock \emph{arXiv preprint arXiv:2402.13514}.

\bibitem[{Wang et~al.(2024{\natexlab{b}})Wang, Zha, Yu, Zhao, Chen, Wang, Wang, Yan, Liu, Han et~al.}]{wang2024retriever}
Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu~Han, et~al. 2024{\natexlab{b}}.
\newblock Retriever-and-memory: Towards adaptive note-enhanced retrieval-augmented generation.
\newblock \emph{arXiv preprint arXiv:2410.08821}.

\bibitem[{Wang et~al.(2023)Wang, Araki, Jiang, Parvez, and Neubig}]{wang2023learning}
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md~Rizwan Parvez, and Graham Neubig. 2023.
\newblock Learning to filter context for retrieval-augmented generation.
\newblock \emph{arXiv preprint arXiv:2311.08377}.

\bibitem[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang, Dong, Wei, Lin, Tang, Wang, Yang, Tu, Zhang, Ma, Xu, Zhou, Bai, He, Lin, Dang, Lu, Chen, Yang, Li, Xue, Ni, Zhang, Wang, Peng, Men, Gao, Lin, Wang, Bai, Tan, Zhu, Li, Liu, Ge, Deng, Zhou, Ren, Zhang, Wei, Ren, Fan, Yao, Zhang, Wan, Chu, Cui, Zhang, and Fan}]{Yang2024Qwen2TR}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Min Xue, Na~Ni, Pei Zhang, Peng Wang, Ru~Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, and Zhi-Wei Fan. 2024.
\newblock \href {https://api.semanticscholar.org/CorpusID:271212307} {Qwen2 technical report}.
\newblock \emph{ArXiv}, abs/2407.10671.

\bibitem[{Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning}]{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W Cohen, Ruslan Salakhutdinov, and Christopher~D Manning. 2018.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
\newblock \emph{arXiv preprint arXiv:1809.09600}.

\bibitem[{Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao}]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}.

\bibitem[{Yoran et~al.(2023)Yoran, Wolfson, Ram, and Berant}]{yoran2023making}
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.
\newblock Making retrieval-augmented language models robust to irrelevant context.
\newblock \emph{arXiv preprint arXiv:2310.01558}.

\bibitem[{Yu et~al.(2023)Yu, Zhang, Pan, Ma, Wang, and Yu}]{yu2023chain}
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.
\newblock Chain-of-note: Enhancing robustness in retrieval-augmented language models.
\newblock \emph{arXiv preprint arXiv:2311.09210}.

\bibitem[{Zhou et~al.(2023)Zhou, Jurafsky, and Hashimoto}]{Zhou2023NavigatingTG}
Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023.
\newblock \href {https://api.semanticscholar.org/CorpusID:265150666} {Navigating the grey area: How expressions of uncertainty and overconfidence affect language models}.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}.

\end{thebibliography}

\input{sections/8_appendix}

\end{document}
