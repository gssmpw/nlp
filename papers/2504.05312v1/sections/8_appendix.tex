\cleardoublepage

\appendix
\section{Multi-granular Content Filter}\label{appendix:filter}
In \Ours, for the Multi-granular content filter, we finetune the LLaMA 3-8B and Qwen 2-7B models using the LLamaFactory framework. Specifically, for the query-reference classification task, we finetune the model into two categories: useful and useless, retaining only the useful ones. For the context filter, we use only the extracted content from the original passages and feed it into the LLMs. Both fine-tuning processes are conducted over 2 epochs, with the per-device training batch size set to 4.

\paragraph{Chunk-Level Filter}
The accuracy  of responses generated by LLMs can be significantly compromised by noisy retrieved contexts~\cite{yoran2023making}. To mitigate this, we introduce the Chunk-Level Filter module to enhance response accuracy and robustness. This module utilizes LLMs to filter out irrelevant knowledge. Rather than directly querying an LLM to identify noise, we incoporate a Natural Language Inference (NLI) framework~\cite{bowman2015large} for this purpose. Specially, for a query  \(q\) and retrieved reference \(r\), the NLI task evaluates whether the knowledge contains reliable answers, or usefule information aiding the response to the question. This results in a judgment \(j\) categorized as useful or useless. The operation of the Chunk-Level Filter can be mathematically represented as :
\begin{equation}
    F_{\theta}(q, r) \rightarrow j \in \{\text{useful}, \text{useless}\}
\end{equation}
Knowledge is retained if the NLI result is classified as useful, and the reference is discarded when the NLI result is classified as useless. The NLI training dataset is constructed semi-automatically. We provide task instruction, query \(q\), along with retrieved reference \(r\) as prompt to GPT-4, which then generated a brief explanation \(e\) and a classification result \(j\).
The prompt template is as follows:

\begin{tcolorbox}
    \textbf{[Instruction]:} Your task is to solve the NLI problem: given the
premise in [Knowledge] and the hypothesis that "The [Knowledge]
contains reliable answers aiding the response to [Question]". You
should classify the response as useful and useless.

    \textbf{[Question]:}\{Question\}
    
    \textbf{[Knowledge]:}\{Knowledge\}
    
    \textbf{[Format]:}\{Explanation\}\{NLI result\}
\end{tcolorbox}

\paragraph{Sentence-Level Filter}
Followed by previous work~\cite{wang2023learning}, we use the STRINC measure for single-hop QA datasets and CXMI for multi-hop datasets. We train the sentence-Level Filter models \(M_{slf}\), using context filtered with above two measures. To create training data for \(M_{slf}\), for each training sample with query \(q\), we concatenate the retrieved passages \(P\) and query \(q\), then, we apply the filter method \(f\) to obtain filtered context \(t_{output}\) as output. The \(s\) is the sentence of the retrieved passages, and \(t_{output}\) can be represented as:
\begin{equation}
    t_{\text{output}} = \left[ s_{text} \mid s_{strinc} == 1 \right] 
\end{equation}
\begin{equation}
    t_{\text{output}} = \left[ s_{text} \mid s_{cxmi} >= threshold \right] 
\end{equation}

We train \(M_{slf}\) by feeding in query and retrieved passages \(P\), and ask it to generate filtered context.
% \section{Classifier Performance}
% \label{classifier performance}
% \input{images/confusion_matrix}
% To understand the performance of the proposed classifier, we analyze its effectiveness across two LLM models. As shown in figure \ref{fig:2_confusion}, whether in Llama3-8b or Qwen2-7b, our \Ours classifier, achieves over 90\% accuracy in classifying useful retrieved passages. Furthermore, it successfully excludes more than 40\% of negative retrieved knowledge, significantly improving the quality of the knowledge and eliminating irrelevant information.
\section{Retriever \& Corpus}\label{appendix:retriever}
To ensure a fair comparison of all baselines, we align the retriever and corpus across all methods for each dataset. For both single-hop and multi-hop datasets, we employ BM25~\cite{robertson1995okapi}, implemented in the search tool Elasticsearch, as the foundational retriever. For the external document corpus, we use the Wikipedia corpus preprocessed by ~\cite{karpukhin2020dense} for single-hop datasets, and the preprocessed corpus by ~\cite{trivedi2022interleaving} for multiple-hop datasets. For long-form ASQA dataset, we employ dense retriever GTR-XXL~\cite{ni2021large} and use the corpus provided by ALCE, consisting of the 2018-12-20 Wikipedia snapshot, segmented into 100-word passages.
\section{Implementation Details}
For computing resources, we utilize NVIDIA 4090 GPUs with 24GB of memory. Additionally, due to the frequent access to the LLM, we employ VLLM as the inference framework. The software stack includes Python 3.10.15, VLLM 0.6.3.post1, PyTorch 2.5.0, and CUDA 12.1.
\section{Detailed prompt}
% \label{rec_loss}
% To enhance the model's accuracy in predicting the next item, we employ a binary cross-entropy (BCE) loss function. In our training process, each positive sample is paired with a negative sample using a 1:1 negative sampling strategy. The target item embedding calculated from MIRM consists of a positive item (\(\text{pos}\)) and a negative item (\(\text{neg}\)). For each pair, we define label vector \(y = [1, 0]\) and generate predicted logits \(x = [x_{\text{pos}}, x_{\text{neg}}]\). The BCE loss is calculated as:

% \vspace{-0.2in}
% \begin{equation}
% \mathcal{L}_{\text{bce}} = - \left( y \cdot \log(x) + (1 - y) \cdot \log(1 - x) \right)
% \end{equation}

% Minimizing this loss encourages the model to assign higher probabilities to positive samples and lower probabilities to negative samples, thereby improving its ability to distinguish relevant items for accurate next-item predictions.

We present all the prompts used in our method in Tables A3 and A4. In Table A3, we detail the prompt for the Multi-granular Content Filter. Specifically, at the Memory Initialization stage, {query} represents the original query 
q, and {refs} refers to the retrieved 
k passages obtained by feeding the original query q into the retriever. At the Iterative Information Collection stage, {query} still represents the original query q, and {note} refers to the content of the optimal memory $M_{opt}$. Additionally, as mentioned in the main text, LLMs tend to ask similar questions if previous ones were not well resolved. To address this, we introduce the already-asked questions list {query log} to avoid repetition. At the Note-Updating stage, {query} still refers to q, while {refs} represents new retrieved k passages based on the updated queries, and {note} refers to $M_{opt}$. In the Memory Updating phase, {query} represents the original query q, while {best note} and {new note} represent 
$M_{opt}$ and $M_{cur}$, respectively. 
\\
In the Multi-granular Content Filter stage, for the Chunk-Level Filter, {External\_knowledge} refers to the retrieved k passages, from which we filter out useless passages, retaining only the useful ones. Next, for the Sentence-Level Content Filter, {context} refers to each useful passage. After passing through this filter, we extract important sentences from the passages to generate the answer.
\\
In the Agent-based Memory Update, we assume three roles in the memory generation process: reviewer, challenger, and refiner. The reviewer evaluates the strengths and weaknesses of the note memory based on the query and {refs}. The challenger, using the reviewer’s feedback, provides suggestions to revise and enhance the memory. Finally, the refiner uses both the reviewer’s insights and the challenger’s suggestions to refine and generate the new memory. In the final memory updating phase, we compare the new memory with the initialized memory to select the best memory.
\begin{table*}[h]
  \centering
  \begin{tabular}{|p{16cm}|}
    \hline
    \multicolumn{1}{|>{\columncolor{gray!10}}c|}
    {\textbf{Prompt of the Memory Initialization Stage}} 
    \\
    \hline
    \textbf{Instruction:} \\
    Based on the provided document content, write a note. The note should integrate all relevant information from the original text that can help answer the specified question and form a coherent paragraph. Please ensure that the note includes all original text information useful for answering the question. \\
    \\
    Question to be answered: \{query\} \\
    Document content: \{refs\} \\
    \\
    Please provide the note you wrote: \\
    \hline
    \multicolumn{1}{|>{\columncolor{gray!10}}c|}{\textbf{Prompt of the Iterative query rewritten Stage}} \\
    \hline
    \textbf{Instruction:} \\
    Task: Based on the notes, propose a new question. The new question will be used to retrieve documents to supplement the notes and help answer the original question. The new question should be concise and include keywords that facilitate retrieval. The new question should avoid duplication with the existing question list. \\
    \\
    Original question: \{query\} \\
    Notes: \{note\} \\
    Existing question list: \{query\_log\} \\
    \\
    Provide your new question,you MUST reply with the new question on the last line, starting with "\#\#\# New Question". \\
    \hline
    \multicolumn{1}{|>{\columncolor{gray!10}}c|}{\textbf{Prompt of the Chunk-Level Filter}} \\
    \hline
    \textbf{Instruction:} \\
  You are an advanced AI model specialized in understanding the Natural Language Inference (NLI) tasks. Your task is to do the NLI problem. The premise is [External Knowledge]. The hypothesis is "There exist clear and unambiguous answer in the [External Knowledge] that can convincingly and soundly answer the Question." Your response should be in one of (useful,useless). \\
    \\
    External Knowledge: \{External\_Knowledge\} \\
    Question: \{Question\} \\
    \\
    Now give me the NLI result, which 1. should be one of (useful,useless). 2.Please strictly following this json format and fill xxx with your answer. 3. Please notice the Escape Character and keep correct format. 4. Please just give me the concise Json response and no ther redundant words. 5. The output should not appear Here is the NLI result, Just strictly follow the format below: \\
    \{"NLI result":"xxx"\}
        \\
    \hline
    \multicolumn{1}{|>{\columncolor{gray!10}}c|}{\textbf{Prompt of the Sentence-Level Filter}} \\
    \hline
    \textbf{Instruction:} \\
   You are an AI model specialized in extracting helpful sentences from a given context. Your task is to extract helpful sentences while removing irrelevant or unhelpful ones based on the provided question and context.
   
    \\
    Question: \{query\} \\
    context: \{context\} \\
    \\
    Now provide the extracted helpful sentences, which should include only valid and relevant sentences from the context. \\
    \hline
  \end{tabular}
  \caption{All prompts of Memory initialize, query rewritten and Multi-granular Content Filter.}
  \label{tab:All prompts of IIC and AMR}
\end{table*}

\begin{table*}[h]
  \centering
  \begin{tabular}{|p{16cm}|}
    \hline
    \multicolumn{1}{|>{\columncolor{gray!10}}c|}
    {\textbf{Prompt of the reviewer in Agent-based memory}} 
    \\
    \hline
    \textbf{Instruction:} \\
    Task: Analyze the relationship between the query, retrieved documents, and notes. Identify the strengths and weaknesses of how well the notes align with the query and incorporate the information from the retrieved documents. Highlight areas where the notes effectively cover the query and the references, as well as areas where they could be improved to better address the query or utilize the information from the references.\\
    Question: \{query\} \\
    retrieved documents: \{refs\} \\
    note: \{note\}
    \\
    Provide an analysis of the notes with a focus on the strengths and weakness: \\
    \hline
    \multicolumn{1}{|>{\columncolor{gray!10}}c|}{\textbf{Prompt of the challenger in Agent-based memory}} \\
    \hline
    \textbf{Instruction:} \\
    Based on the provided reviewer information, provide specific and actionable suggestions to improve the notes. The goal is to ensure the notes comprehensively and accurately address the query while fully utilizing relevant information from the retrieved documents. \\
    Question: \{query\} \\
    retrieved documents: \{refs\}\\
    Notes: \{note\} \\
    reviewer information: \{review\_info\} \\
    
    Provide detailed suggestions to revise and enhance the notes: \\
    \hline
     \multicolumn{1}{|>{\columncolor{gray!10}}c|}{\textbf{Prompt of the refiner in Agent-based memory}} \\
    \hline
    \textbf{Instruction:} \\
    Refine the provided notes based on the reviewer information and suggestions. The goal is to ensure the notes are improved to better address the query and fully utilize the relevant information from the retrieved documents. \\
    Question: \{query\} \\
    retrieved documents: \{refs\}\\
    Notes: \{note\} \\
    reviewer information: \{review\_info\} \\
    suggestions: {suggestions}
    \\
    Provide the refined notes that incorporate the feedback from the reviewer information and suggestions: \\
    \hline
    \multicolumn{1}{|>{\cellcolor{gray!10}}c|}{\textbf{Prompt of the memory updating}} \\
    \\
    \textbf{Instruction:}
    \newline{}
    Task: Please help me determine which note is better based on the following evaluation criteria:
    \newline{}
    1. Contains key information directly related to the question.
    \newline{}
    2. Completeness of Information: Does it cover all relevant aspects and details?
    \newline{}
    3. Level of Detail: Does it provide enough detail to understand the issue in depth?
    \newline{}
    4. Practicality: Does the note offer practical help and solutions?
    \newline{}
    Please make your judgment adhering strictly to the following rules:
    % \newline{}
    % - If Note 2 does not add new meaningful content on top of Note 1, or only adds redundant information, return 
    % \{``status": ``False"\}
    % % {{"status":"False"}}
    % directly.
    \newline{}
    - If Note 2 has significant improvements over Note 1 based on the above criteria, return 
    \{``status": ``True"\}
    % {{"status":"True"}}
    directly; otherwise, return
    \{``status": ``False"\}
    % {{"status":"False"}}
    .
    \newline{}
    Question: \{query\}
    \newline{}
    Provided Note 1: \{best\_note\}
    \newline{}
    Provided Note 2: \{new\_note\}
    \newline{}
    Based on the above information, make your judgment without explanation and return the result directly.
    \\
    \hline
  \end{tabular}
  \caption{All prompts of Agent-based Memory Update}
  \label{tab:All prompts of Agent-based Memory Update}
\end{table*}