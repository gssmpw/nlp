\section{Results and Analysis}
In this section, we evaluate our proposed framework, \Ours, on six real-world datasets and compare it against several baselines, including No retrieval, single-time and adaptive RAG methods.
\subsection{Main Results.}
We implemented the \Ours on six datasets. The comparison with baseline models is summarized in Table \ref{tab:2_main_results}. Key observations are as follows:

\paragraph{\Ours vs. Single-time RAG.}
Results show that our method surpassed all STRAG for all six QA datasets. Meanwhile, it is noteworthy that our method outperformed Vanilla by over 30\% on the Natural Questions, 2WikiMQA, and HotpotQA datasets. Even on the remaining three datasets, it still achieved an approximate 10\% improvement. These achievements highlight its superiority and effectiveness. An intuitive explanation is that STRAG heavily depends on the quality of one-time retrieval, whereas our method can adaptively explore more knowledge in the corpus and filter useless chunks and irrelevant sentence in chunk. Therefore, it is able to demonstrate that our method preserves more and more effective knowledge.

\paragraph{\Ours vs. Adaptive RAG.} In Table \ref{tab:2_main_results}, we conduct an in-depth comparison of our approach with several existing ARAG models,including FLARE, Self-RAG, ReAct, Adaptive-RAG and Adaptive-Note. Our method consistently outperforms baselines in single-hop, multi-hop, and long-form QA tasks, particularly in accuracy. Even compared to the state-of-the-art ARAG method, it improves by over 10\%, demonstrating its superiority, effectiveness, and robustness. We provide an in-depth analysis of the baseline limitations and the factors contributing to our success.
% Results indicate that our method almost consistently achieves the highest performance in single-hop QA, multi-hop QA and Long-form QA tasks, especially with respect to the acc. Even for the state-of-the-art ARAG method, we can achieve an improvement of more than 10\%, which highlights its superiority, effectiveness and robustness. To explain these trends, we conduct an in-depth analysis of the limitations of these baselines and the resons behiend our success. 
% \textbf{ReAct} leverages the LLM's internal knowledge to determine whether further retrieval actions are necessary. However, LLMs may be overly confident~\cite{Zhou2023NavigatingTG} in their own responses, which can interface with retrieval decisions and lead to ignoring partial knowledge. 
\textbf{First}, ReAct and Flare relies on LLMs' internal knowledge to guide retrieval decisions, but its inherent overconfidence~\cite{Zhou2023NavigatingTG} may hinder retrieval efficacy by neglecting existing knowledge.
In contrast, our method employs a greedy strategy to first gather information extensively, followed by a careful assessment of whether to incorporate new, useful knowledge into the existing framework. This process optimizes knowledge extraction and significantly enhances response accuracy.
% In contrast, our method fully aggregates useful information through Multi-granular Content Filter before deciding whether to processed with the next round of integrating useful information into the exising knowledge. This allows our method to maxmize the extraction of useful knowledge from the corpus, thereby significantly improve the accuracy of the responses.
\textbf{Second}, Self-RAG faces challenges in training effective models for complex tasks due to numerous classifications
such as labeled inputs, retrieved paragraphs, and output categorizations.
Unlike this approach, our Multi-granular Content filter training strategy is relatively simple, yet it maximizes the utilization of valuable information through multiple iterations and agent-based memory.
% often resulting in poor performance, especially in multi-hop QA.
% For \textbf{Self-RAG}, due to the numerous classifications involved in Self-RAG, such as labeled inputs, retrieved paragraphs, and output categorizations, it becomes challenging to train a model effectively on complex problems. This often results in poor performance, particularly in multi-hop question answering scenarios.
% Our method focuses on extracting valuable content directly from useful paragraphs rather than evaluating them on a scale or determining their level of support. This streamlined approach avoids the complexities associated with scoring systems, thereby enhancing efficiency and effectiveness in handling intricate queries. Additionally, the agent-based memory component enables more effective self-reflection, which is crucial for addressing multi-step reasoning problems and improving overall task performance.
% \textbf{Flare} uses logit probabilities as confidence scores to trigger the next retrieval phase, but these scores may not accurately reflect the true need for retrieval, affecting its ability to determine the optimal timing for proactive information fetching.
% \textbf{Flare} utilizes logit probailities as confidence scores to decide whether to initiate the next retrieval phase. However, these scores may not accurately gauge the actual need for retrieval, impacting Flare's ability to determine the optimal timing for proactive information fetching. 
% In contrasting to focusing on the precise moment for retrieval, \Ours emphasizes assessing whether the amount of valuable  information in memory has increased, as this can significantly enhance the quality of the responses generated. 
\textbf{Third}, The Adaptive-RAG method adapts retrieval strategies based on query complexity, and Adaptive-Note generates new memory in each iteration until memory growth stabilizes. However, both of them neglect passage quality, which affects answer accuracy. Instead, our method focuses on the importance of retrieving relevant paragraphs, aiming to minimize the impact of irrelevant information on the LLM's decision-making when answering questions.

% Misclassifying complex queries can lead to suboptimal retrieval, and its effectiveness with diverse question types, like long-form questions, remains unclear.
% \textbf{Adaptive-RAG} method focuses on tailoring retrieval strategies according to the perceived complexity of the query. However, this approach does not account for the quality of the retrieved passages, which can be crucial for generating accurate and contextually relevant responses. The misclassification of a complex query as a simple one can lead to suboptimal retrieval strategies being applied, thereby compromising the final answer's accuracy. Furthermore, the method’s effectiveness in handling diverse question types, such as long-form questions, remains uncertain. It may necessitate training new classifiers or adapting existing ones to ensure optimal performance across all question categories. 
% \textbf{Adaptive-Note} generates new memory in each iteration until memory growth stabilizes, but it overlooks the quality of retrieved knowledge and underestimates the agent's role in handling complex problems.
% \textbf{Adaptive-Note} primarily generates new memory during each iteration until it determines that the knowledge within the memory no longer grows. However, this method overlooks the quality of the retrieved knowledge and underestimates the importance of the agent in handling complex problems.
% \input{images/confusion_matrix}
% \subsection{Performance on different LLMs}
% To prove the generality and effectiveness of our method across different LLMs, we use released open-source LLMs - Qwen2-7b and LLama3-8b. we compare the overall performance of our method with that of Vanila RAG and different adaptive RAG methods. The result reveal that our method is significantly superior in diffferent LLM and six datasets.
%\input{images/zhexian}
\subsection{Classifier Performance}
\begin{figure}[ht]
    \centering
    \hspace{-2cm}
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/qwen2-7b_confusion.pdf}
        \caption{Qwen2-7b}
        \label{fig:4_confusion1}
    \end{subfigure}
    \hspace{-0.8cm}
    %\hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/Llama3-8b_consuion.pdf}
        \caption{Llama3-8b}
        \label{fig:4_confusion2}
    \end{subfigure}
    \hspace{-2cm}
    \vspace{-0.2cm}
    \caption{\textbf{Confusion matrix for fine-tuned LLMs. Our Fine-Tuned LLMs serve as excellent classifiers.} }
    \vspace{-0.3cm}
    \label{fig:confusion}
\end{figure}

To understand the performance of the proposed classifier, we analyze its effectiveness across two LLM models. As shown in figure \ref{fig:confusion}, whether in Llama3-8b or Qwen2-7b, our \Ours classifier, achieves over 90\% accuracy in classifying useful retrieved passages. Furthermore, it successfully excludes more than 40\% of negative retrieved knowledge, significantly improving the quality of the knowledge and eliminating irrelevant information.
\begin{figure}[ht]
    \centering
    \hspace{-1.2cm}
    \begin{subfigure}[t]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/asqa_bold.pdf}
        \caption{ASQA}
        \label{fig:1_fairtopk}
    \end{subfigure}
    \hspace{0.2cm}
    %\hfill
    \begin{subfigure}[t]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/sqaud_bold.pdf}
        \caption{SQuAD}
        \label{fig:2_fairtopk}
    \end{subfigure}
    \hspace{-1.2cm}
    
    \begin{subfigure}[t]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/nq_bold.pdf}
        \caption{Natural Questions}
        \label{fig:3_fairtopk}
    \end{subfigure}
    \hspace{0.15cm}
    \begin{subfigure}
    [t]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/trivia_bold.pdf}
        \caption{TriviaQA}
        \label{fig:4_fairtopk}
    \end{subfigure}
    \hspace{-1.2cm}
    \begin{subfigure}[t]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/2wiki_bold.pdf}
        \caption{2WikiMQA}
        \label{fig:5_fairtopk}
    \end{subfigure}
    \hspace{0.15cm}
    \begin{subfigure}[t]{0.23\textwidth}
        \includegraphics[width=\textwidth]{images/hotpot_bold.pdf}
        \caption{HotpotQA}
        \label{fig:6_fairtopk}
    \end{subfigure}
    \caption{\textbf{Results of the in-depth comparison under a fair \textbf{top-$\boldsymbol{k}$}.}}
    \vspace{-0.7cm}
    \label{fig:Fair_top-k_1}
\end{figure}

\subsection{In-depth comparison under a fair top-k}
Under the same raw top-k setting, ARAG methods generally retrieve more passages compared to single-step methods. Unfortunately, while we can specify the top-k value for each step, the inherent retrieval uncertainly in ARAG prevents us from controlling the total number of retrieved passages. To ensure a fairer performance comparsion, we address the discrepancy by calculating the average number of unique passages retrieved per sample across all adaptive steps, which we term the fair top-k. Figure \ref{fig:Fair_top-k_1} illustrates the overall performance of Vanilla RAG under this fair-top-k setting. It is evident that as the number of retrieved passages increases, the Vanilla method shows little to no significant improvement. These findings further emphasize the superiority of our method.

% \input{table/4_ablation_study}
\input{table/ablation_study_2}


\subsection{Ablation Study}

To analyze the contributions of components in the proposed \Ours method, particularly the fine-tuning of the Multi-granular Content Filter and the Agent-based Memory Updater in Adaptive information Collector, we conducted an ablation study on the 2WikiMQA dataset. The above components comprise three key components:
\begin{itemize}[left=0em, itemsep=-5pt, topsep=5pt]
    \item \textbf{Chunk-Level Filter (CF):} Input: Query and retrieved chunk. Output: useful / useless.
    \item \textbf{Sentence-Level Filter (SF):} Input: query and filtered chunk. Output: filtered sentences.
    \item \textbf{Agent-Based Memory Updater (AMU): } whether to use agent-based method for memory.
\end{itemize}

\paragraph{Effect of the Multi-granular Content Filter.}  
% To assess the impact of each Multi-granular Content Filter component, we systematically removed one type of content. The results in Table \ref{tab:4_ablation_qqt} show that the model performs best when both filters are used. Removing any single filter causes a significant performance drop, emphasizing the importance of each component. Notably, excluding the Paragraph-Level Filter  results in a greater performance decline than excluding the Sentence-Level Filter, highlighting the Paragraph-Level Filter's greater role in filtering irrelevant information.
To evaluate the contribution of each Multi-granular Content Filter component, we systematically removed one type of content. Additionally, removing both two types (\textit{w/o ALL}) effectively disables the content filter stage. The results, presented in Table \ref{tab:ablation}, show that the model achieves its best performance when both two filter strategy are used together. Conversely, removing any single type of data leads to a noticeable decline in performance, highlighting the importance of each component in enhancing the framework’s overall effectiveness. Interestingly, the performance when Sentence-Level Filter (\textit{w/o SF}) are excluded remains higher than when Chunk-Level Filter (\textit{w/o CF}) are omitted. This indicates that, The Chunk-Level Filter plays a more significant role in our approach, effectively filtering out irrelevant chunk information to a large extent.
\paragraph{Impact of Agent-based Memory  Updater.}  
To examine the role of Agent-based Memory Updater(\textit{w/o AMU}) in \Ours, we conducted an ablation experiment by removing the AMU module. As shown in Table \ref{tab:ablation}, removing this module significantly decreases performance. This highlights the agent’s critical role in memory generation. The agent ensures the creation of more efficient memory, thereby enabling the LLM to provide more accurate responses.

The ablation study highlights the importance of each content filter component and the agent-based memory module. Multi-granular Content Filter and Agent-based Memory Updater significantly enhance the performance of the \Ours framework.

\input{table/top_k_analysis}
\input{table/interative_para}

\subsection{Parameter Analysis}
\paragraph{Impact of top-k.} In Table \ref{tab:Different Top-k}, we present a comparison between our method and Vanilla RAG across different top-k settings. The results demonstrate that our approach consistently outperforms Vanilla RAG under the same top-k conditions, highlighting its robustness in achieving reliable improvements regardless of the number of retrieved passages. Additionally, in most cases, the performance of our system improves as the number of retrieved passages (top-k) per step increases.

\paragraph{Impacts of max iterations} 
As shown In Table \ref{tab:Adaptive parameter}, performance on these complex QA datasets improves with increasing iterations, peaking at 3. We therefore recommend setting the max iterations to 3 for optimal performance.

