\section{Introduction}

In recent years, Large Language Models (LLMs) ~\cite{brown2020language,achiam2023gpt,touvron2023llama,anil2023palm} have demonstrated exceptional performance across various tasks, including question answering (QA)~\cite{yang2018hotpotqa,kwiatkowski2019natural}, owing to their ability to capture diverse knowledge through billions of parameters. However, even the most advanced LLMs often suffer from hallucinations~\cite{chen2023chatgpt} and factual inaccuracies due to their reliance on parametric memory. Additionally, it is impractical for these models to memorize all of the ever-evolving knowledge.
To address these challenges, retrieval-augmented generation (RAG)~\cite{borgeaud2022improving,izacard2023atlas,shi2023replug} have garnered increasing attention. These models retrieve passages relevant to the query from external corpora and incorporate them as context to the LLMs, enabling the generation of more reliable answers. By integrating retrieved information, retrieval-augmented LLMs maintain both the accuracy and timeliness of their knowledge.
Early studies on RAG primarily focused on single-hop queries~\cite{lazaridou2022internet,ram2023context}, where answers can typically be found within a single document. However, these methods often fall short when handling complex QA tasks, such as long-form QA and multi-hop QA, which require aggregating information from multiple sources. Unlike single-hop QA, these queries necessitate connecting and synthesizing information across multiple documents and cannot be solved by a single retrieval-and-response step. For instance, the query ``\textit{Is Microsoft Office 2019 available in a greater number of languages than Microsoft Office 2013?}'' requires three reasoning steps: first, retrieving information about the languages supported by ``\textit{Office 2019}''; second, retrieving similar information for ``\textit{Office 2013}''; and finally, comparing the two sets of data to produce an answer.

To address this issue, Adaptive RAG has been proposed. It adaptively selects appropriate retrieval questions and timing based on the difficulty of the user query to flexibly capture more valuable knowledge for answering open-domain QA tasks, achieving a balance between effectiveness and efficiency. However, these methods still have several problems. 
\textbf{\textit{First}}, each retrieval operates independently and lacks a summarizing memory of previous retrieval fragments, which may cause the outputs to reflect only limited knowledge from specific retrieval steps while neglecting the integration and interaction of retrieved information from different steps.
\textbf{\textit{Second}}, when the LLM uses these retrieved fragments for reasoning, it does not actively evaluate the validity of the information. Consequently, without the ability to determine when to proactively stop retrieval based on known information or update the queries that need to be retrieved, it may lead to inefficiencies or the retrieval of irrelevant information.
\textbf{\textit{Third}}, the effective parts within the retrieved text segments are very few, and excessive redundant information introduces noise, which can obscure important details and negatively impact the model's performance.

To this end, we propose \textbf{A}daptive \textbf{m}emory-\textbf{b}ased optimization for \textbf{e}nhanced \textbf{R}AG (\textbf{\Ours}). \Ours comprises three core components: Agent-based Memory Updater (AMU), Adaptive Information Collector (AIC), and Multi-granular Content Filter (MCF). These components work in unison to automatically integrate and update retrieved information as the LLM's memory, dynamically adjust the queries based on known information, and employ multi-granular content filtering during retrieval to retain useful information and reduce noise, thereby achieving outstanding performance. 
\textbf{\textit{Firstly}}, to address the issue in which each retrieval operates independently and lacks a summarizing memory of previous retrieval fragments, AMU employs a multi-agent collaborative approach. By coordinating various agents, AMU optimizes the LLM's current memory. This process ensures that the knowledge structure is continuously refined and enriched, effectively integrating all valuable information from previous retrieval steps.
\textbf{\textit{Secondly}}, AIC utilizes the real-time memory generated by AMU to update the queries that need to be retrieved and decides when to stop retrieval. By automatically adjusting the retrieval process based on the accumulated knowledge, AIC ensures that subsequent retrievals are more targeted and efficient, effectively addressing the challenge of insufficient knowledge accumulation and avoiding unnecessary retrievals.
\textbf{\textit{Lastly}}, we fine-tune the LLM to function as MCF to reduce noise during retrieval. MCF includes two levels of filtering capabilities. Firstly, it assesses the validity of the entire retrieved text segment and the query, determining whether the information is relevant and useful. Secondly, from the valid retrieved segments, it filters out irrelevant content and retains essential information. This approach effectively reduces redundant information and highlights crucial details, thereby enhancing the overall performance of the model.

In summary, our contributions are as follows.
\begin{itemize}[left=0.em, itemsep=-5pt, topsep=5pt]
\item We propose the Agent-based Memory Updater, which uses a multi-agent approach to integrate information and form memory from previous retrievals, optimizing the LLM's memory.

\item We develop the Adaptive Information Collector, which updates retrieval queries and decides when to stop retrieval, making the process more targeted and efficient.

\item We introduce the Multi-granular Content Filter to reduce noise by filtering irrelevant content at multiple levels, enhancing model performance.

\item Extensive experiments validate the effectiveness of \Ours, showing significant improvements over existing methods in open-domain QA.
\end{itemize}
