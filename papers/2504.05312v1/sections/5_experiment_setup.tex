% \section{Experiments}
% In this section, we evaluate our proposed framework, \Ours, on six real-world datasets and compare it against several baselines, including No retrieval, single-time and adaptive RAG methods.
% To assess the effectiveness of \Ours, we conduct a comprehensive analysis addressing four research questions. Additionally, we investigate the impact of different DUEGs and the various input data modalities on the results.Furthermore, we perform ablation studies to investigate the impact of fine-tuning strategies and post-alignment, as well as evaluate the performance differences between our LLM-based user modeling approach and alternative user representation methods. The following research questions are explored:

% \begin{itemize}[left=0.em, itemsep=-5pt, topsep=5pt]
%     \item \textbf{RQ1}: How does \Ours perform compared with traditional sequential recommender models and LLM-based methods?  
%     \item \textbf{RQ2}: What are the differences in performance between the LLM DUEG and alternative DUEGs for user representation?  
%     \item \textbf{RQ3}: How do different data modalities impact the performance of \Ours?
%     \item \textbf{RQ4}: How does the post-alignment model affect the performance of \Ours?
%     \item \textbf{RQ5}: How does fine-tuning the MIRM combined with post-alignment training influence the overall performance of \Ours?  
% \end{itemize}

\section{Experimental Setup}
In this section, we present the datasets, models, metrics, and implementation details.
More experiment setup can see appendix \ref{appendix:filter} and \ref{appendix:retriever}.
\subsection{Datasets and Evaluation Metrics}

% \begin{itemize}[left=0.em, itemsep=-5pt, topsep=5pt]
%     \item \textbf{Amazon} \footnote{\url{https://jmcauley.ucsd.edu/data/amazon/}}: Collected from the Amazon cloth online shopping platform.
%     \item \textbf{PixelRec} \footnote{\url{https://github.com/westlake-repl/PixelRec}}: An image dataset for recommender systems with raw pixels and text.
%     \item \textbf{MovieLens} \footnote{\url{https://grouplens.org/datasets/movielens/}}: A commonly-used movie recommendation dataset that contains user ratings.
% \end{itemize}
To simulate a realistic scenario, where different queries have varying complexities, we use both the single-hop, multi-hop and long-form QA datasets simultaneously, in the unified experimental setting.
\paragraph{Single-hop QA}
\input{table/qqt_main_result}
For simpler queries, we use three benchmark single-hop QA datasets, which consist of queries and their associated documents containing answers, namely \textbf{1) SQuAD v1.1}~\cite{rajpurkar2016squad}, \textbf{2) Natural Questions}~\cite{kwiatkowski2019natural} and \textbf{3) TriviaQA}~\cite{joshi2017triviaqa}.

\paragraph{Multi-hop QA} 
To consider more complex query scenarios, we use two benchmark multi-hop QA datasets, which require sequential reasoning over multiple documents, namely
\textbf{1) 2WikiMultiHopQA (2WikiMQA)}~\cite{ho2020constructing} and \textbf{2) HotpotQA}~\cite{yang2018hotpotqa}. For both single-hop QA and multi-hop QA, we report the accuracy (\textbf{acc}) and F1-score (\textbf{f1}) as evaluation metrics, where acc measures if the predicted answer contains the ground-truth, and f1 measures the number of overlapping words between the predicted answer and the ground-truth.

\paragraph{Long-form QA}
We select an English dataset \textbf{ASQA}~\cite{stelmakh2022asqa}. Specially, we use the ASQA dataset with 948 queries recompiled by ALCE~\cite{gao2023enabling} and apply ALCE's official evaluation metrics, involving String Exact Match (\textbf{str-em}) and String Hit Rate (\textbf{str-hit}). 

% For all three datasets, we arrange the interaction sequences in sequential order. We utilize a leave-one-out approach to split the data into training, validation, and testing sets. Detailed statistics of the datasets are provided in Table \ref{tab:1_data_statistics}.  The evaluation metrics are Normalized Discounted Cumulative Gain 
%  (NDCG@K), Recall (Recall@K), which are evaluated on the full amount of data. The abbreviations N, and R are respectively used to denote NDCG, and Recall.

\subsection{Baseline\&LLMs}
% We employ Qwen2vl-2b\footnote{\url{https://github.com/QwenLM/Qwen2-VL}} as the backbone model for both MIRM and DUEG (experiments with other MLLM backbones are presented in the appendix \ref{Impact_of_Different_MLLM_Backbone}). For each dataset, we create three types of data mixtures, each consisting of 10,000 data points, to fine-tune the MIRM.  Additionally, we employ SASRec as the ID-based recommendation model for contrastive learning, with an embedding dimension same as the MIRM.

% For all methods involving LLMs, each experiment is trained for a maximum of 5 epochs with a batch size of 128. A learning rate warm-up strategy is employed, initializing the learning rate at 1/100 of its maximum value 1e-4, and dynamically adjusting it over training steps using a cosine scheduler.
We extensively compare three types of baselines: 1) No Retrieval (\textbf{NoR}), which directly feeds queries into LLMs to output answers without any retrieval process; 2) Single-time RAG (\textbf{STRAG}), which retrieves knowledge in a one-time setting to answer the original queries; 3) Adaptive RAG (\textbf{ARAG}), which leverages an adaptive forward exploration strategy to retrieve knowledge to enhance answer quality. For STRAG, we select Vanilla RAG, Chain-of-note~\cite{yu2023chain}, Self-Refine, and Self-Rerank are simplified from Self-RAG~\cite{asai2023self}. For ARAG, we include five recent famous methods for comparison - FLARE~\cite{jiang2023active}, Self-RAG, ReAct~\cite{yao2022react}, Adaptive-RAG~\cite{jeong2024adaptive} and Adaptive-Note~\cite{wang2024retriever}. Additionally, we conduct experiments on multiple LLMs, including Qwen2-7b~\cite{Yang2024Qwen2TR}, Llama3-8b~\cite{Touvron2023LLaMAOA} and GPT-3.5 (OpenAI gpt-3.5-turbo-instruct).   We default to using Llama3-8b as the Multi-granular Content Filter LLM, detail experiment setting about multi-filter content see appendix \ref{appendix:filter}. Unless otherwise specified, Llama3-8b was employed as the default model.
% \subsection{Retriever \& Corpus}
% To ensure a fair comparison of all baselines, we align the retriever and corpus across all methods for each dataset. For both single-hop and multi-hop datasets, we employ BM25~\cite{robertson1995okapi}, implemented in the search tool Elasticsearch, as the foundational retriever. For the external document corpus, we use the Wikipedia corpus preprocessed by ~\cite{karpukhin2020dense} for single-hop datasets, and the preprocessed corpus by ~\cite{trivedi2022interleaving} for multiple-hop datasets. For long-form ASQA dataset, we employ dense retriever GTR-XXL~\cite{ni2021large} and use the corpus provided by ALCE, consisting of the 2018-12-20 Wikipedia snapshot, segmented into 100-word passages.

% % \subsection{Results and Analysis }
% % In this section, we compare \Ours against NoR, STRAG, and ARAG baselines, %taking into metrics of both acc and f1 on squad, nq, trivia, 2WikiMQA, HotpotQA and ASQA datasets, 
% % to showcase the effectiveness and robustness of \Ours.

% % \paragraph{Baselines.} %~{}

% % FPMC \cite{rendle2010factorizing}, GRU4Rec \cite{tan2016improved}, and SASRec \cite{kang2018self} are traditional sequential recommendation models based on Markov Chains, RNN, and attention mechanisms, respectively. DuoRec \cite{qiu2022contrastive} employs contrastive learning to extract discriminative information for sequential recommendation. SASRec-Content is a variant of SASRec that directly utilizes content feature representations as sequence inputs. It includes three versions: text-only, image-only, and a combination of text and image. CoLLM \cite{zhang2023collm} and HLLM \cite{chen2024hllm} are sequential recommendation models based on large language models (LLMs), both achieving state-of-the-art performance.


