\section{Methods}
In this section, we define the task and present an overview of our proposed method, \Ours (illustrated in Figure 1). Following this, we provide a detailed explanation of each individual component.

\subsection{Problem Formulation}
RAG aims to enhance the generation quality of LLMs by integrating relevant information from an external document corpus \(D = \{d_1, d_2, \dots, d_n\}\). Given a user input \(x\) or a query \(q\), the core of RAG involves using a retriever \(R\) to identify and select a subset of pertinent documents from \(D\). The LLM then leverages both the original input and these retrieved documents to produce an improved output. Generally, this process seeks to achieve an output \(y\) based on the input and retrieved context.

\subsection{\Ours Overview}
Figure \ref{fig:1_framework} presents the architecture of \Ours, an adaptive memory updating iterative RAG method. It consists of three main components: an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter.

Given a query \(q\), we initialize the LLM's memory \(M_0\) as an empty set. Acting as the primary scheduler, the \textbf{Adaptive Information Collector} initiates the iterative loop. At each iteration \(t\), we retrieve the top \(k\) text chunks \(C_t = \{ c_1^t, c_2^t, \dots, c_k^t \}\) from the corpus \(D\) based on \(q\). The \textbf{Multi-granular Content Filter} then assesses the relevance of each chunk \(c_i^t\) to \(q\) and filters out irrelevant content, retaining the most pertinent sub-paragraphs to form the refined document set \(P_t\).
Subsequently, the \textbf{Agent-based Memory Updater} employs three agents—the Reviewer, Challenger, and Refiner—to collaboratively integrate the new references \(P_t\) with the previous memory \(M_{t-1}\), producing the updated memory \(M_t\). These agents ensure that the memory is optimized for answering \(q\).
The adaptive information collector then evaluates whether the LLM can adequately respond to \(q\) using the current memory \(M_t\). If not, it generates a new retrieval query \(q_{t+1}\) based on \(M_t\) and \(q\) and proceeds to the next iteration. If the LLM can answer \(q\) satisfactorily, the adaptive information collector terminates the loop.
After the iterative process concludes, we use the final memory \(M_T\) as context to generate the answer \(\alpha \in A\) through the LLM's zero-shot in-context learning (ICL). 

\subsection{Agent-based Memory Updater (AMR)}

In real-world scenarios, user queries vary significantly in complexity, necessitating the formulation of tailored strategies for each query. Enhanced LLM based on memory provide an effective solution to this challenge. Memory, which represents the information known to the LLM during retrieval, enables the model to determine retrieval strategies effectively. Among these, memory updating is a critical component. It requires the LLM to leverage historical and newly retrieved information to regenerate the latest memory aligned with the query.

Inspired by the concept of multi-agent collaboration, we propose an \textbf{Agent-based Memory Updater} framework, which employs a cooperative, multi-agent approach to memory updating. Specifically, AMR consists of three independent agents: the Reviewer, the Challenger, and the Refiner. Through iterative dialogue, these agents reflect upon and optimize the memory. The inputs to AMR include the current memory \( m_t \), the retrieved passages \( p_t \) for the current query \( q_t \), and the original user query \( q \). Based on these inputs, the LLM initially generates an updated memory \( m_{t+1} \).

\textbf{Reviewer.} As the primary evaluator in the AMR framework, the Reviewer examines the proposed memory update \( m_{t+1} \) using the current memory \( m_t \), retrieved passages \( p_t \), and user query \( q \). The Reviewer assesses the correctness and relevance of \( m_{t+1} \) to the user's intent, identifying strengths and weaknesses. By sharing evaluations with the Challenger and Refiner, the Reviewer facilitates collaborative refinement and coordinates strategies to ensure alignment with collective goals. This evaluation process ensures memory updates are rigorously reviewed, improving the LLM's retained information.

\textbf{Challenger.} Acting as the critical analyst, the Challenger builds upon the Reviewer's assessment by examining \( m_{t+1} \), identifying potential flaws and overlooked constraints. Through interaction with the Reviewer and Refiner, the Challenger scrutinizes the validity of the memory update, raising probing questions about conflicts with existing knowledge or unmet user requirements. These interactions enable collective strategy adaptation, ensuring \( m_{t+1} \) is robust and well-aligned with both the user query and knowledge base.

\textbf{Refiner.} As the agent responsible for implementing improvements, the Refiner synthesizes feedback from the Reviewer and Challenger to refine \( m_{t+1} \). It translates critiques into concrete modifications, focusing on enhancing accuracy, clarity, and adherence to user query. The Refiner resolves issues identified by other agents, producing a revised \( m_{t+1} \) that better satisfies objectives. Through collaboration with the Reviewer and Challenger, the Refiner streamlines the feedback loop and maintains modification records, contributing to an effective refinement cycle.

Through the complementary collaboration of the Reviewer, Challenger, and Refiner within AMR, the proposed method effectively leverages the strengths of each agent. This triadic interaction ensures that memory updates undergo rigorous evaluation, critical examination, and precise refinement. As a result, the updated memory \( m_{t+1} \) becomes increasingly accurate, relevant, and aligned with the user’s query across multiple iterative cycles.

\subsection{Adaptive Information Collector (AIC)}
We propose the \textbf{Adaptive Information Collector} as the primary scheduler to control the entire RAG workflow. The role of AIC is to evaluate whether the information currently available in the memory, generated by the AMU, is sufficient to answer the user query \( q \).

Specifically, each iteration of AIC follows three key steps. The process initializes with the user query \( q_0 = q \) and an empty memory \( m_0 = \emptyset \). Firstly, AIC begins by retrieving the top \( k \) text chunks \( C_t = \{ c_1^t, c_2^t, \dots, c_k^t \} \) from the corpus \( D \) based on the current query \( q_t \) using a retrieval mechanism. Next, the query \( q \), the retrieved text chunks \( C_t \), and the current memory \( m_t \) are input into the Agent-based Memory Updater (AMU), which generates an updated memory \(m_{t+1} = \text{AMU}(q_t, C_t, m_t)\). Lastly, AIC then evaluates whether the updated memory \( m_{t+1} \) contains sufficient information to fully answer the query \( q \). If the memory is deemed sufficient, the iterative process terminates, and the latest memory \( m_T \), along with the query \( q \), are inputted into the LLM using in-context learning to produce the final answer \( a \). However, if the updated memory \( m_{t+1} \) is insufficient, AIC generates a refined query \(q_{t+1} = \text{AIC}(q, q_t, m_{t+1}).\) to target the missing information and proceeds to the next iteration. This iterative approach ensures that the final memory \( m_T \) is comprehensive and well-aligned with the user’s informational needs.

This iterative design allows AIC to dynamically refine queries and memory updates, ensuring that the final memory \( m_T \) contains the necessary information to answer the user’s query comprehensively.

\subsection{Multi-granular Content Filter (MCF)}

The Adaptive Information Collector, despite leveraging the filtering capabilities of the Agent-based Memory Updater and employing adaptive retrieval to refine the query \( q_t \), often retrieves the top \( k \) text chunks \( C_t = \{ c_1^t, c_2^t, \dots, c_k^t \} \) that still include irrelevant information. These irrelevant parts can be categorized into two levels: chunk-level irrelevance, where an entire chunk \( c_i \) may be unrelated to the query \( q \), and sentence-level irrelevance, where even within a relevant chunk \( c_i \), only a subset of the sentences may be pertinent to the query \( q \), while the remainder constitutes noise.

Based on these insights, we used STRINC, CXMI metrics, and GPT-4 (detail see in appendix \ref{appendix:filter}) to generate a multi-granular content filter dataset and subsequently fine-tuned a LLM using multi-task learning to create the \textbf{Multi-granular Content Filter}, denoted as \( F_c \). This content filter operates hierarchically, applying two levels of filtering to each chunk \( c_i \).

At the first level, a chunk-level filtering prompt, formulated as \( f_c(prompt_{chunk}, q, p_i) \), determines whether a chunk is relevant to \( q \). If \( f_c \) returns False, the chunk is directly discarded; otherwise, it progresses to the second level. The chunk-level filter is defined as:

\begin{small}
\begin{equation}
f_c({\small prompt_{chunk}}, q, c_i) = 
\begin{cases} 
\text{\small True}, & \text{\small if } c_i \text{\small \  relevant to } q \\
\text{\small False}, & \text{\small if } c_i \text{\small \ not relevant to } q
\end{cases}
\end{equation}
\end{small}

\noindent At the second level, a sentence-level evaluation is performed for chunks that pass the initial filter, where \( p_i = f_c(prompt_{sentence}, q, c_i) \) assesses each sentence within the chunk to retain the relevant sentences. The output of this stage is a refined set of relevant sentences \( P_t = \{ p_1, p_2, \dots, p_m \} \), where \( p_i \) are the relevant sentences.

This hierarchical filtering approach significantly reduces noise in the retrieved information by isolating only the relevant content at both chunk and sentence levels. The MCF, ensures that AIC operates with higher precision, improving the quality and relevance of the memory \( m_t \) in each iteration and, consequently, the overall performance.