\section{Related Works}
Animated Pedagogical Agents (APAs) are virtual characters designed to facilitate learning by providing guidance, feedback, and motivation to users. Initially conceptualized as tools to make learning more engaging, APAs have evolved significantly over the years. Their design is grounded in established theoretical frameworks, such as the persona effect, social agency theory, and embodied cognition theory, which emphasizes the importance of human-like interactions in fostering emotional connections, enhancing cognitive engagement, and improving knowledge retention **Mori, "The Uncanny Valley"**.

APAs have demonstrated their value across diverse educational domains, including electronics education **Wouters, "A Survey on Pedagogical Agents"** and language learning **Bui, "Pedagogical Agents in Language Learning"**. Studies have shown that the use of APAs increases learner engagement and facilitates knowledge transfer by integrating multimodal cues, such as gestures, facial expressions, and speech, into the learning experience **Kim, "Multimodal Interaction in Educational Systems"**. By acting as social partners, APAs create interactive environments that encourage active learning and reduce cognitive load **Korbak, "The Role of Social Partnerships in Learning"**.

Despite their potential, existing APAs face critical limitations that hinder their widespread adoption and effectiveness. One major challenge is the lack of natural and emotionally resonant speech. Many APAs rely on rudimentary voice synthesizers, resulting in robotic and monotone outputs that fail to establish meaningful emotional connections with learners **Vandello, "Natural Language Processing for Emotional Expression"**. Furthermore, lip synchronization technologies often fall short in aligning speech with mouth movements, leading to a jarring user experience. For example, Heeyo **Heeyo Team, "Heeyo Platform Overview"**, an educational startup valued at \$20 million USD, employs APAs with only repetitive jaw-opening motions that do not correspond accurately to the spoken phonemes, reducing the sense of realism and immersion.

Another significant limitation lies in the adaptability of APA interactions. Many systems operate on script-based frameworks, predefining interactions and feedback that cannot dynamically adjust to the learner’s context or needs. On platforms like Cramify **Cramify Team, "Cramify Platform Features"**, APAs are limited to pre-configured video-based interactions, offering no real-time customization of dialogue or feedback. This restricts the ability of APAs to provide personalized and responsive learning experiences, which are essential for addressing the unique challenges and preferences of individual learners.

In addition, a highly successful example of APAs with synchronized facial expressions and mouth movements is Duolingo’s video call feature **Duolingo Team, "Duolingo Video Call Feature"**, introduced in September 2024. This feature allows users to practice English through video calls with an APA named Lilly, sparking widespread discussion online. However, Duolingo didn't open-source the underlying technologies, and the complexity of building APAs brings significant technical challenges for developers and researchers who wish to implement their own APAs for experiments and applications.

We developed VTutor to address the aforementioned challenges. Unlike conventional script-based APAs, VTutor can use LLMs to generate personalized, context-specific guidance and feedback in real-time with matched facial expressions, creating a more engaging, human-like interacting experience. By leveraging these innovations, VTutor aims to redefine the role of APAs in education, making them more effective and accessible across diverse interaction environments.