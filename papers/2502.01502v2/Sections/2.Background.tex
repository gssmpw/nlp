% 1-ReRAM Based accelerator and ARAS
% 2-ReRAM writing methods
% 3-current technique to improve lifespan AND ENDURANCE DISTRIBUTION
% 4-transformer

\section{Background and Related work}\label{s:Background}
In this section, we provide some background on ReRAM writing schemes and techniques used to prolong the lifespan of ReRAM cells. Additionally, we describe the in-situ matrix transposition operation. Finally, we discuss the architecture of popular Transformer-based neural networks.

\subsection{ReRAM Cell and Crossbar Architecture}\label{subs:Writing}
ReRAM cells are two-terminal devices (see Figure~\ref{fig:CrossBars}(b)) capable of storing values through their multilevel conductance states. In DNN accelerators, ReRAM cells store network weights. The process of writing these weights (i.e., weight updates) involves transitions between different conductance states, typically triggered by electrical inputs. The specific mechanism driving these transitions varies depending on the type of ReRAM insulator used~\cite{insulator, insulator2, Park_2013, Woo_2016}. Generally, the conductance of ReRAM cells is increased or decreased through positive and negative programming voltage pulses, corresponding to weight increases and decreases, respectively. Ideally, ReRAM cells would exhibit a linear weight update response to identical programming voltage pulses. However, real-world devices, as documented in the literature~\cite{trade-offs_MLC,Gao_2015}, deviate from this ideal behavior and exhibit "non-ideal" properties. In addition, process variations impact the ReRAM cells specifications, both within the same device and across different devices.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\columnwidth]{images/Subarray.pdf}
    % \vskip -0.10in
    \caption{Crossbar architecture and analog operation with ReRAM cells (a), ReRAM cell device (b).}
    % \vskip -0.25in
    \label{fig:CrossBars}
\end{figure}

To handle the impact of non-ideality on multi-level cell (MLC) and enhance tolerance to process variations, the Program and Verify (P\&V) method has been proposed~\cite{Accurate_Program_Verify,ISPVA}. P\&V involves applying narrow voltage pulses with progressively increasing magnitude, combined with read pulses to verify whether the cell has reached the desired state, as shown in Figure~\ref{fig:P&V}. This approach not only accounts for process variation but also helps identify worn-out cells, as those that are stuck at a certain value will not respond to further programming pulses. As illustrated in Figure~\ref{fig:P&V}, the programming pulses have a higher amplitude and longer duration than the read pulses, meaning the additional read pulses introduce minimal overhead. According to \cite{Assist_Techniques}, the P\&V method increases energy consumption and write latency by only 5\% and 6\%, respectively, compared to an "ideal" cell.

The most compact and straightforward structure for storing a DNN weight matrix in ReRAM cells is the crossbar array configuration, where each 1T1R cell is positioned at the intersection points. Crossbars provide the benefit of high integration density~\cite{NeuroSim_writing} and serve a dual function: storing weights and performing dot-product operations between inputs and weights. As illustrated in Figure~\ref{fig:CrossBars}(a), the input vector is encoded into read voltage signals, allowing for the parallel execution of matrix-vector multiplication using the weights stored in the crossbar. The resulting weighted sums are captured at the end of each column as analog currents, which are then post-processed by additional peripherals to convert them into digital values.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.70\columnwidth]{images/ProgramAndVerify.pdf}
    % \vskip -0.10in
    \caption{Program and verify (P\&V).}
    \label{fig:P&V}
    % \vskip -0.10in
\end{figure}

To update weights in ReRAM crossbars, NeuroSim~\cite{NeuroSim_writing} introduce a row-by-row writing which is typically employed, as shown in Figure~\ref{fig:WeightUpdating}. The weight increase and decrease operations require distinct programming voltage polarities, so the weight update process is divided into two steps, each with its specific voltage polarity for every row. In each step, the SLs (Source Lines) provide voltage pulses or constant voltages (if no update) to modify each selected cell, while the BL (Bit Line) provides the required polarity for that particular step. It is important to note that the update of a each cell's value follows the P\&V scheme, which is achieved by applying specific pulses through the associated SL and BL, and the number and amplitude of pulses required are determined by the current cell value and the desired target value. Consequently, in a crossbar array, each SL is equipped with its independent driver to deliver distinct pulses.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\columnwidth]{images/WeightUpdating.pdf}
    \vskip -0.10in
    \caption{Example of a Row-by-Row writing scheme. The gray cells keep their values in each steps, while the red and green cell values are modified.}
    % ...during the Weight Increase and Weight Decrease procedures respectively.
    \label{fig:WeightUpdating}
    \vskip -0.15in
\end{figure}

\subsection{ReRAM Lifespan Improvement Techniques}\label{subs:Lifespan improvements}
NVMs, particularly ReRAMs, suffer from limited write endurance, causing cells to become stuck at a fixed value (permanent failure) when the number of write cycles exceeds the cell's endurance. For ReRAM cells, this limit typically occurs after $10^6$ to $10^9$ write cycles on average~\cite{Weebit, crossbar, Realizing}. To extend memory lifespan, two primary solutions are wear-leveling (WL) and fault tolerance techniques~\cite{FLOWER_and_FaME, WoLFRaM}.

\textbf{Wear-leveling techniques.} These techniques aim to distribute writes, which are often concentrated on a small subset of cells, more evenly across the entire memory~\cite{WoLFRaM,Realizing,ARS}. As a result, uniform wear-leveling (UWL) represents the upper bound of wear-leveling methods. Wear-leveling techniques are applied at various levels of the memory hierarchy, whether using ReRAM for storage or in DNN accelerators. For instance, row-level wear leveling extends the lifespan of each memory page (ReRAM cells for storage) or processing element (ReRAM cells for computation) by balancing writes across rows~\cite{ARS, SAWL,Bloom_filter}. Similarly, column-level wear leveling balances write operations across cells in the same row by rotating the order of writes across columns at different granularities~\cite{WELCOMF,ILF}. In \cite{ReNEW,On_Endurance}, row-level and column-level wear leveling techniques are adapted for ReRAM-based accelerators. However, these approaches still suffer from a lack of \textit{adaptability}, as they assume the accelerator has sufficient capacity to store all the required data.

%Similarly, column-level wear leveling balances cell flips by rotating or flipping rows, with proposals suggesting column-level techniques at different granularities~\cite{WELCOMF,ILF}.

% ReNEW~\cite{ReNEW} balances device endurance and programming conditions (such as pulse length and amplitude) to extend the accelerator's lifespan. This approach harnesses the neural network's inherent error tolerance to occurred faults in worn-out cells.

\textbf{Fault tolerance techniques.} Extensive research has been conducted to address stuck-at faults caused by limited endurance in NVMs~\cite{FLOWER_and_FaME,FREE-p,Data_Block,ECP}. For instance, RETROFIT~\cite{Realizing} employs Error Correction Codes (ECC) to correct faults in worn-out cells and uses ECC to identify "weak" rows, which are then leveraged by row-level wear leveling techniques. However, to the best of our knowledge, no research has explored ReRAM-based accelerators for DNN inference that leverage the inherent fault tolerance of DNNs to mitigate the negative impact of worn-out cells.

% About On-endurance paper.
% \subsection{Endurance and fault model}\label{subs:Endurance Model} we can mention normal distribution assumption in the methodology...

\subsection{In-Situ Matrix Transposition}\label{subs:Transposition}
Matrix transposition is a fundamental operation in many DNNs, including architectures like Transformers and GPT models. Performing matrix transposition with standard methods can be resource-intensive, as these approaches often require extra memory to hold both the original and transposed matrices. This additional memory requirement can be costly, particularly in large-scale DNNs where efficient memory management is critical to sustaining performance and minimizing latency. 

In-place matrix transposition has been widely studied, with early work dating back to 1967 and 1970~\cite{Algorithm_380,Algorithm_302}. However, in 1977, Cate and Twigg introduced Algorithm 513~\cite{Algorithm_513}, which proposed an efficient approach for matrix transposition directly within memory, avoiding the need for additional memory allocation. This algorithm focuses on optimizing the transposition process by rearranging matrix elements stored in a one-dimensional arrayâ€”often referred to as a flattened matrix. The primary advantage of this method is that it handles transposition in-situ (without extra storage), which is particularly beneficial for memory-constrained environments or when working with large matrices. For example, consider $2\times4$ matrix and its transposed:

\begin{equation}
  M = 
  \begin{bmatrix}
    101 & 102 & 103 & 104 \\
    201 & 202 & 203 & 204 \\
  \end{bmatrix}
  %
  , M^{T} =
  \begin{bmatrix}
    101 & 201 \\
    102 & 202 \\
    103 & 203\\
    104 & 204\\
  \end{bmatrix} 
\end{equation}

Figure~\ref{fig:Transposition} shows the flattened representation of matrix $M$. The transposed matrix can be obtained by applying specific permutations to the indices. In this example, the permutation is divided into four distinct cycles: $\{0\}$, $\{1,2,4\}$, $\{3,6,5\}$, and $\{7\}$. Each cycle represents a set of indices that need to be swapped. For instance, the cycle $\{1,2,4\}$ indicates that the first and second indices should be swapped, followed by swapping the first and fourth indices, ensuring that the correct transposition is achieved. Cycles that contain only one index, like \{0\} and \{7\}, indicate fixed points, where the index maps to the same position in both the original and transposed matrix, as described in~\cite{Algorithm_513}.

Equation~\ref{eq:Transposition} defines a straightforward formula that identifies the position of an element $\alpha$ in a flattened matrix of size $N \times M$ and determines where it maps in the transposed matrix. This mapping is used when performing an in-place matrix transposition, ensuring that the element at a given index in the original flattened matrix is correctly relocated to its corresponding index in the transposed version.

\begin{equation}
    P(\alpha) =\left\{
      \begin{array}{@{}ll@{}}
        MN-1, & \text{if}\ \alpha=MN-1 \\
        N\alpha \ mod \ (MN-1), & \text{otherwise}
      \end{array}\right. 
      \label{eq:Transposition}
\end{equation}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.70\columnwidth]{images/Transposition.pdf}
    \vskip -0.10in
    \caption{Flattened representation of matrix $M$ and its transposed form. Each colored set represents a cycle in Algorithm 513, indicating which elements need to be swapped in order to generate the transposed matrix.}
    \label{fig:Transposition}
    \vskip -0.10in
\end{figure}

% The main challenge addressed by Algorithm 513 is avoiding data overwrites while efficiently swapping matrix elements to their new positions.

\subsection{Transformer DNNs}\label{subs:Transformers}
Transformers are built from several stacked encoders and decoders that process input sequences through self-attention and feed-forward networks (FFNs) like Fig~\ref{fig:Transformer}. In the encoder, each token in the input sequence is transformed into query, key and value vectors using fully-connected (FC) layers. Subsequently, by combining these vectors across all tokens, the query ($Q$), key ($K$), and value ($V$) matrices are produced for the entire input sequence. To compute the self-attention scores, the query matrix $Q$ is multiplied by the transposed key matrix $K$ and then scaled, to reveal relationships between tokens. After softmax normalization and dropout, these scores form the self-attention matrix, which is multiplied by $V$ to generate each head output. Multiple attention heads run in parallel to capture diverse patterns, and their outputs are combined and processed through an FC layer. Afterwards, a residual connection and normalization layer stabilize the output values. The processed attention output then passes through an FFN, which generates the encoder's final output.

The output of each encoder or decoder block in a Transformer model can either be passed as input to subsequent layers or directed to a task-specific output layer, such as a classification layer. Decoder blocks follow a structure and flow similar to encoder blocks but with some key differences. Unlike encoder blocks, which handle sequences of tokens, decoder blocks, used in generation tasks, produce tokens one-by-one, feeding each generated token back as input for the next token's generation.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.80\columnwidth]{images/Transformer.pdf}
    \vskip -0.10in
    \caption{Diagram of the Transformer architecture, featuring $N$ sequential encoder blocks followed by $N$ sequential decoder blocks.}
    \label{fig:Transformer}
    \vskip -0.15in
\end{figure}
