\section{Experimental Results}\label{s:Results}
This section evaluates the performance degradation of the Hamun accelerator over time, its lifespan improvement, and the scheduler reconfiguration overhead. First, we present the progression of retired crossbar columns over time and assess how this impacts overall performance degradation. Second, we analyze the overall lifespan improvements achieved by Hamun, breaking down the contributions of individual optimizations to total accelerator longevitylly, we analyze the offline scheduler overhead, highlighting the cost introduced by the re-scheduling task in response to new faults. This analysis also examines each optimization’s contribution to reducing reconfiguration overhead.

\subsection{Hamun Performance Evaluation}\label{subs:trend}
Figure~\ref{fig:Performance} illustrates the Hamun gradual performance decline over time as ReRAM cells undergo wear and experience retirement. As noted in Section~\ref{s:Methodology}, Hamun is designed to operate until performance degradation reaches a 40\% throughput drop, at this point it halts operations. However, this threshold is adaptable based on user requirements. For instance, in applications where real-time throughput is acceptable, the performance threshold could be set to a lower level to ensure consistent real-time processing and prolong the lifespan of the accelerator.

As shown in the figure, the accelerator maintains peak performance for a significant portion of its operational lifespan. This demonstrates the effectiveness of Hamun's optimizations, such as fault-handling and wear-leveling, in sustaining throughput and minimizing performance drops. The lifespan data, expressed in days, is based on a utilization rate assumption of 25\%. Across all benchmarks, the lifespan of ReRAM-based accelerators remains limited to about a year due to ReRAM cell endurance constraints, even with the significant lifespan improvements achieved by Hamun. This restricted longevity highlights the necessity of advancements in ReRAM technology to establish ReRAM-based accelerators as competitive options against other computation platforms.

Hamun does not change the number of ReRAM cells that require writing in the accelerator compared to the baseline, which is determined by the DNN model size. Consequently, the energy consumption for weight writing, the largest contributor to energy usage in each inference (as reported in \cite{ARAS}), remains unchanged from the baseline. Since the accelerator’s throughput diminishes over time, the static energy per inference, which contributes slightly in total energy, increases gradually and leads to a small increase in total energy consumption.

% \begin{figure*}
%     \centering
%     \begin{subfigure}{0.32\textwidth}
%         \includegraphics[width=\textwidth]{images/BERT_Trend.pdf}
%         \label{fig:first}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.32\textwidth}
%         \includegraphics[width=\textwidth]{images/GPT_Trend.pdf}
%         \label{fig:second}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.32\textwidth}
%         \includegraphics[width=\textwidth]{images/ViT_Trend.pdf}
%         \label{fig:third}
%     \end{subfigure}
%     \vskip -0.20in
%     \caption{Performance degradation and the number of retired columns over time.}
%     \vskip -0.10in
%     \label{fig:Performance}
% \end{figure*}

\begin{figure}
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{images/BERT_Trend.pdf}
        \label{fig:first}
    \end{subfigure}
    \vskip +0.10in
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{images/GPT_Trend.pdf}
        \label{fig:second}
    \end{subfigure}
    \vskip +0.10in
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{images/ViT_Trend.pdf}
        \label{fig:third}
    \end{subfigure}
    \caption{Performance degradation and number of retired columns over time.}
    \vskip -0.10in
    \label{fig:Performance}
\end{figure}

\subsection{Hamun Lifespan Improvement}\label{subs:Lifespan}
Figure~\ref{fig:lifespan_Improvement} illustrates the lifespan improvements achieved by Hamun over the baseline, with or without the fault-handling mechanism. Additionally, the figure breaks down the contributions of each optimization to the total lifespan enhancement. As explained in Section~\ref{s:Methodology}, the baseline ceases operation once any ReRAM cell fails. In contrast, Hamun employs a fault-handling mechanism that retires faulty cells and dynamically generates new scheduling and binding configurations, allowing the accelerator to continue functioning until the performance degradation reaches the user-defined threshold.

% Figure~\ref{fig:lifespan_Improvement} demonstrates Hamun’s lifespan improvement over the baseline. As detailed in Section~\ref{s:Methodology}, baseline stops operation upon the failure of any ReRAM cell. In contrast, Hamun incorporates a fault handling mechanism that retires failed cells, dynamically generating new scheduling and binding configurations to quarantine these cells and accelerator continues until user-acceptable performance degradation. Hamun also integrates various optimizations to further extend the accelerator’s lifespan. Among these, the batch execution technique and fault-handling mechanism have the a significant impact. Overall, Hamun achieves over 13$\times$ improvement in the lifespan of ReRAM-based accelerators compared to ARAS. Furthermore, the improvements of $59\%$ and $37\%$ are attributed to the fault handling and batching schemes, respectively.

Among the optimizations, the batch execution technique and fault-handling mechanism have the largest impacts on extending lifespan. Overall, Hamun delivers a substantial improvement, extending ReRAM-based accelerator lifespan by $13.2\times$ over the baseline. Specifically, $4.6\times$ improvement is due to fault handling, while batching adds another $2.6\times$. Both wear-leveling and approximate computing provide an additional $1.1\times$ improvement, which although not negligible their contributions are overshadowed by the huge impact of fault handling. Notably, in configurations where fault handling is not included, both wear leveling and approximation significantly enhance lifespan by $2.5\times$, underscoring their effectiveness in isolation. Moreover, wear-leveling and approximate computing significantly reduce the overhead of reconfigurations, a topic discussed in more detail in the following section.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{images/Lifespan_Improvement.pdf}
    \vskip -0.05in
    \caption{Breakdown of each optimization’s contribution to lifespan enhancement over the baseline (ARAS).}
    \label{fig:lifespan_Improvement}
    \vskip -0.20in
\end{figure}

\subsection{Hamun Reconfiguration Overhead}\label{subs:Scheduler_Utilizatio}
Figure~\ref{fig:Reconfiguration} presents the average number of inference executions that can be processed between two consecutive reconfiguration operations for different Hamun configurations. Hamun optimizations reduce the frequency of reconfigurations, which require to suspend inference operations on the accelerator and consume energy in the host system to generate new scheduling and binding configurations. By increasing the number of inference executions per reconfiguration, Hamun reduces reconfiguration overhead, thereby optimizing both performance and energy efficiency.

As Figure~\ref{fig:Reconfiguration} shows, the fault handling optimization alone, when executing the ViT model, requires a reconfiguration after approximately $2.2 \times 10^{5}$ inferences. When wear-leveling (WL) is applied in addition to fault handling, the number of inference executions per configuration increases to around $3.1 \times 10^{5}$. Unlike the lifespan improvement, the approximation scheme has the most significant impact on extending configuration reuse and thus, reducing reconfiguration overhead, allowing ViT to reach $4.18 \times 10^{6}$ inference executions before requiring reconfiguration. This demonstrates the effectiveness of Hamun in reducing the frequency of re-scheduling, thus minimizing performance overhead associated with reconfiguration. With the configuration defined for the accelerator size described in Section~\ref{s:Methodology}, Hamun supports approximately 2.9 million inferences on average across the benchmark set without requiring reconfiguration interruptions.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{images/SchedulerFrequency.pdf}
    \vskip -0.05in
    \caption{Average number of inferences per reconfiguration.}
    \label{fig:Reconfiguration}
    \vskip -0.20in
\end{figure}
