\section{Hamun Accelerator}\label{s:Accelerator}
In this section, we describe the hardware architecture of our proposed ReRAM-based accelerator, which is organized into multiple hierarchical levels. We then explain the execution flow, detailing the weight-writing and computation procedures.

\subsection{Architecture}\label{subs:Architecture}
Most prior works (e.g.,~\cite{RAELLA, ISAAC}) design accelerators under the assumption that all DNN weights are pre-stored in ReRAM crossbars. This approach demands substantial resources, including large numbers of ReRAM crossbars and on-chip SRAM buffers, making it inefficient for large networks. In contrast, proposals like ARAS~\cite{ARAS} operate with limited resources by adopting a layer-by-layer execution, where the computation of one layer overlaps with the writing of weights for subsequent layer(s). However, the frequent updates to ReRAM cells for executing DNN layers reduce the lifespan of the accelerator due to ReRAM's limited endurance cycles. This is one of the key challenges that Hamun addresses by proposing an approximate computing scheme along with some wear-leveling optimizations.

Figure~\ref{fig:Top_view} presents a top-down view of the Hamun accelerator architecture. Figure~\ref{fig:Top_view}(a) shows a high-level schematic of the chip architecture. A single chip comprises several key components: an External IO Interface (Ext-IO), multiple Processing Elements (PEs), a Special Function Unit (SFU), Accumulation Units (ACC), and a Global Buffer (Gbuffer). The Ext-IO facilitates communication with Main Memory (MM), loading network weights and inputs, and storing the final outputs. The ACC units aggregate partial results from neural operations for a given layer, which may be generated across multiple PEs when a layer spans several PEs due to its size. The SFU handles transitional operations such as pooling, non-linear activations (e.g., sigmoid or ReLU), and normalization, ensuring support for the full range of computations required for state-of-the-art DNNs. The Global Buffer (Gbuffer) acts as a storage unit for intermediate activations produced during the execution of each layer.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\columnwidth]{images/Top_view.pdf}
    \vskip -0.10in
    \caption{Architecture of the Hamun accelerator including the organization of: (a) Chip, (b) Processing Element (PE), (c) Analog Processing Unit (APU), and (d) Transposing Banks.}
    \vskip -0.15in
    \label{fig:Top_view}
\end{figure}

Figure~\ref{fig:Top_view}(b) illustrates the structure of a Processing Element (PE), which consists of $m \times n$ Analog Processing Units (APUs), $m$ buffers to store either input activations or weights, an output buffer for storing partial sums, $n$ accumulation modules to sum the partial outputs from the APUs in each column, shift registers to serialize activations, and a multiplexer that switches between the weight-writing and computation phases. Figure~\ref{fig:Top_view}(c) displays the main components of an Analog Processing Unit (APU), which includes a ReRAM-based crossbar array of 1T1R cells used to store synaptic weights. Each APU also includes an input register, which is used to store activations, a write register to store weights, a Mask register to disable an entire crossbar column in the writing/computing procedure, a WL/BL (Wordline/Bitline) driver to control the wordlines and bitlines, and an SL (Sourceline) driver to generate required voltage pulses in the sourcelines. Moreover, each APU has an analog multiplexer, a shared pool of Analog-to-Digital Converters (ADCs), and functional units for accumulating and shifting the partial results from different bitlines across iterations. Further details on the analog dot-product operation procedure using ReRAM crossbars, as well as the ReRAM cell writing process, can be found in Section~\ref{subs:Execution Dataflow}.

The Gbuffer is divided into different bank groups. While one bank group operates in a standard mode, the other bank group follows a distinct writing procedure that results in a transposed matrix format when reading data. As discussed in Section~\ref{subs:Transformers}, in the attention block, calculating the score matrix requires performing matrix multiplication between the query matrix and the transposed key matrix. This scheme facilitates the transposition of the key matrix in-situ, without introducing any additional latency overhead.

Figure~\ref{fig:Example_In_Transposition} provides a simplified example illustrating our proposed in-situ transposition scheme. In (a) we present an original matrix that needs transposing. The matrix elements are generated sequentially in a row-by-row manner and transferred via the NoC to storage. For this example, the NoC transmits only two elements per transaction. These are then stored in two memory banks for each transfer. Figure~\ref{fig:Example_In_Transposition}(b) depicts the resulting memory layout post-storage. During writing, consecutive elements in a row are distributed across two banks, while consecutive elements in a column are written in the same entry in adjacent banks, enabling the transposed matrix to be retrieved by reading the banks entry-by-entry. In some NoC transactions, element pairs need to be swapped to ensure correct storage order; for instance, elements $(a_{20}, a_{21})$ are swapped before writing, so $a_{20}$ is written into bank 1 and $a_{21}$ into bank 0. Similar swaps occur during reading to ensure accurate transposition during transfer, e.g., when reading $(a_{21}, a_{11})$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/Example_In_Transposition.pdf}
    \vskip -0.10in
    \caption{(a) Original matrix and (b) Post-storage matrix layout.}
    \vskip -0.25in
    \label{fig:Example_In_Transposition}
\end{figure}

Figure~\ref{fig:Top_view}(d) illustrates our hardware proposal for the transposing bank group, which consists of 16 banks, each with a data width of one byte, and a 16-byte swapping register capable of holding 16 elements of the key matrix. As previously discussed, each attention block in Transformer models includes a FC layer to generate the key matrix. In the Hamun architecture, FC layers operate token-by-token, resulting in a row-by-row generation of the key matrix. Each row is divided into multiple NoC transactions, with each transaction consisting of 16 bytes. These transactions are transferred through the chip's mesh network. Each transaction is further split into 16 distinct elements, and these elements are distributed across the 16 banks in the transposing bank group. The swapping register reorders the elements based on the desired transposed matrix configuration and aligns each elements with its corresponding banks. Equation~\ref{eq:Transposition} (in section~\ref{subs:Transposition}) maps how each element in the original flattened matrix is assigned to its proper position in the transposed matrix. Therefore, to store elements in transposed order, first, the reordering of the elements is executed according to the corresponding bank for each element, as outlined by Equation~\ref{eq:id} below. Next, all elements are simultaneously stored in their respective banks at the entries determined by Equation~\ref{eq:entry}.

% Therefore, here we define Equation~\ref{eq:id} and Equation~\ref{eq:entry} to show how each element is assigned to a specific bank and how it is placed within an entry in that bank.

\vskip -0.10in
\begin{equation}
    id = P(\alpha) \, mod \, (\#banks)
    \label{eq:id}
\end{equation}

\vskip -0.10in
\begin{equation}
    entry = P(\alpha) \///(\#banks)
    \label{eq:entry}
\end{equation}

The function $P(\alpha)$ defines the correct index in the flattened transposed matrix for an element with index $\alpha$ in the original matrix, as detailed in Equation~\ref{eq:Transposition}. Also, the number of banks, represented by $\#banks$, is set to 16 in the transposing bank group. This in-situ approach eliminates the overhead for transposing the key matrix in Transformer DNNs, as the scheduler statically pre-generates the required swap instructions and bank entry mappings based on the key matrix size and number of banks in the transposing bank group.

\subsection{Execution Dataflow}\label{subs:Execution Dataflow}
n this section, we explain the dataflow of the Hamun accelerator, which involves two distinct procedures: writing weights into the ReRAM crossbars and performing dot-product computations. As previously detailed in Section~\ref{subs:Writing}, the dot products in each crossbar are computed between the input vector and all weights stored in each column of the crossbar. Consequently, when a ReRAM cell becomes worn out, the resulting dot product in the column containing the faulty cell will be incorrect. The most effective solution is to retire the faulty cell, which requires disabling the entire column that includes the damaged cell. In Hamun, this process is handled by masking the faulty column when the MUX in each crossbar connects the column to the ADC, avoiding using the faulty column.

% Unlike traditional crossbars where the word-line (WL) enables all cells in a row, in this architecture, the word-line connects all cells in a column (see Figure~\ref{fig:New_WeightUpdate}).

% A significant distinguishing feature of Hamun compared to previous proposals, is its adoption of a column-by-column scheme for weight updates, as opposed to the traditional row-by-row approach.

In this paper, a PE row refers to a group of APUs organized in a single row within a PE and share a common buffer for input activations and weights. The PE row serves as the smallest granularity for mapping a DNN layer. Each layer of the DNN is mapped to at least one PE row, where different output neurons (or kernels in convolutional layers) are assigned across distinct APUs within the row. The computations for the same input activation set, across various output neurons, are carried out simultaneously within the PE row. If the number of output neurons exceeds the capacity of a PE row, or if the size of each output neuron surpasses the number of ReRAM cells in an APU column, the layer mapping extends to additional PE rows. This hierarchical organization ensures efficient utilization of resources while accommodating the diverse computational demands of DNN layers.

\emph{Weight Writing Procedure:} As indicated in Figure~\ref{fig:WeightUpdating}, the ReRAM weight writing procedure follows the row-by-row approach within each crossbar. The weight-writing process for a ReRAM crossbar row begins by fetching the weights from main memory. These weights are transferred through the chip NoC to the corresponding PE, where they are stored directly in the destination buffer, bypassing the shift registers. The target APU then reads these weights from the buffer and loads them into its Writing Registers. Simultaneously, mask signals — one bit per column — are fetched from the host and stored in the Mask Register. Note that mask signals are only fetched during the writing of the first row and reused for subsequent rows. The APU drivers are configured to adjust the weights by applying either increasing or decreasing pulses to the ReRAM cells in two phases (Figure~\ref{fig:WeightUpdating}). The SL drivers generate these pulses with varying amplitudes based on the new weights stored in the Writing Registers and the current cell values in the ReRAM according to P\&V scheme. If the mask signal indicates that the column contains a faulty cell, the SL driver is disabled for that column since that column will be masked in computation stage. Meanwhile, the BL driver controls the polarity of the programming signals, as the weight increase and decrease phases require opposite polarities for proper adjustment of the ReRAM cells. The process is repeated for all rows in the crossbar, while simultaneously other crossbars follow the same procedure. Since all cells in a crossbar row are written concurrently, the row's writing latency is governed by the slowest cell in each phase. In other words, the longest latency cell in a row determines the overall write time for that row. Additionally, Hamun takes into account the main memory bandwidth as a limiting factor for the number of APUs can perform weight updates concurrently.
 % which restricts APUs writing across multiple APUs.
 
In the P\&V scheme, after each programming pulse, the value of each ReRAM cell is read to check whether it has reached the desired state. If a cell fails to change after successive pulses and becomes stuck at a specific value, it is identified as faulty. Whenever a new fault is detected, the accelerator sends a notification to the host, which initiates a routine to isolate the faulty cell that is discussed in more detail in Section~\ref{subs:Scheduler Fault handling}.

% A key distinguishing feature of Hamun, compared to prior proposals, is its use of a column-by-column scheme for weight updates, rather than the conventional row-by-row approach. The weight-writing process for a ReRAM crossbar column begins by fetching the weights from main memory. These weights are transferred through the NoC to the corresponding PE, where they are stored directly in the destination buffer, bypassing the shift registers. The target APU then reads these weights from the buffer and loads them into its Writing Registers. Simultaneously, mask signals—one bit per column—are fetched from the host and stored in the Mask Register. Importantly, mask signals are only fetched during the writing of the first column and reused for subsequent columns. The APU drivers are configured to adjust the weights by applying either increasing or decreasing pulses to the ReRAM cells in two phases (Figure~\ref{fig:New_WeightUpdate}). The BL drivers generate these pulses with varying amplitudes based on the new weights stored in the Writing Registers and the current cell values in the ReRAM. The WL driver activates or deactivates all cells in a column based on the mask signal corresponding to that column. If the mask signal indicates the column contains a faulty cell, the WL driver disables the entire column to prevent further operations. Meanwhile, the SL driver controls the polarity of the programming signals, as the weight increase and decrease phases require opposite polarities for proper adjustment of the ReRAM cells. The process is repeated for all columns in the crossbar, and if a column’s mask signal is disabled, the weight update is skipped to avoid generating erroneous dot products due to faulty cells.

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.8\columnwidth]{images/New_WeightUpdate.pdf}
%     \vskip -0.10in
%     \caption{Hamun adopts a column-by-column weight update mechanism, which enables the retiring of amortized ReRAM cells by simply disabling the corresponding WL, effectively prolonging the operational lifespan of the accelerator.}
%     \vskip -0.15in
%     \label{fig:New_WeightUpdate}
% \end{figure}

\emph{Dot-Product Computations Procedure:} The dot-product computation in Hamun begins by fetching the input activations from on-chip memory (or the main memory in the case of the first layer) and distributing them across the PEs according to the placement of the stored weights. All the weights within a PE row correspond to a single layer and share the same input activation. The activations are then serialized within the PEs, where the PE controller activates the corresponding buffer, and the activations are streamed bit-serially into the buffer. The shift register set, which is responsible to serialize activations, is shared for different PE rows to reduce area overhead. Like other architectures~\cite{ISAAC, PRIME, ReDy}, Hamun iterates over the bits of the activations to perform the dot-product operations (as illustrated in Figure~\ref{fig:CrossBars}(a)). The crossbars compute the partial sums for each activation bit in every iteration, which are then shifted and accumulated across iterations to form the final result. For columns that contain faulty cells, the masking signal is used to disable these columns, preventing incorrect dot-product calculations. The controller, responsible for providing the select signal to the MUX, will skip any column with an active mask signal, ensuring that faulty columns are excluded from the computation process.

In the final iteration, each APU returns its portion of the neural computation, and the complete result for each kernel or filter is computed by aggregating all the APUs' partial sums. Depending on the kernel size and the mapping of the neural network layer, this accumulation can be performed either within the PEs or in the global chip-level accumulator. Once the dot-product calculations are complete, the results are passed through activation, normalization, and pooling functions, after which they are stored in the Global Buffer. The accelerator is then ready to compute the next convolution window in a convolutional layer or process the next token in a Fully Connected layer for tasks like Large Language Models (LLMs).
