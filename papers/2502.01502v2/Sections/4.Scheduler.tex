% 1- scheduler compilation flow and execution scheme
% 2- aligning and retiring a col and mask signal still the other APU canused the masked one.
% 3- overhead in counter and wearout level in scheduler

\section{Hamun Scheduler}\label{s:Scheduler}
Figure~\ref{fig:Compilation} shows the Hamun scheduler, which is divided into multiple main components. The offline scheduler manages both computation and weight-writing tasks by efficiently assigning necessary resources to each task in the Binding procedure, and also orchestrates tasks in the Scheduling procedure. The offline scheduler focuses on two key optimizations aimed at prolonging the acceleratorâ€™s lifespan. In contrast, the online component continuously monitors the appearance of worn-out cells and notifies the host of their location. The offline scheduler generates a set of instructions that control both the weight writing procedure and the dot-product computations procedure during each inference. These instructions are then executed by the accelerator for each inference task.

To maintain the \textit{adaptability} of the accelerator as the size of the network grows, Hamun follows a similar execution scheme to those used in previous works~\cite{ARAS, MNEMOSENE}, which emphasize efficient resource utilization. However, Hamun focuses on addressing the primary limitation of these designs, the restricted lifespan of the accelerator due to frequent ReRAM cell updating. Hamun scheduler adopts a systematic layer-by-layer computation strategy and overlaps the computation of the current layer with the writing of weights for subsequent layers. As soon as the computation for a layer is done, the scheduler reallocates the resources used for this computation for writing the weights of subsequent layer(s). Depending on the number of available crossbars, the accelerator can either update the weights for part of a layer, an entire layer, or multiple layers simultaneously.

% Our scheduler can efficiently accommodate different layers of weights within a single PE, yet each row of APUs in a PE belongs to one layer, since the same activations are broadcast to all APUs in the row.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{images/NewCompilationFlow.pdf}
    \vskip -0.10in
    \caption{Hamun Compilation Flow.}
    \label{fig:Compilation}
    \vskip -0.25in
\end{figure}

Figure~\ref{fig:Execution_Flow} shows an example of the Hamun execution flow for an encoder block commonly used in LLMs. Initially, based on the available accelerator ReRAM crossbar resources, the weights for multiple layers are written into the ReRAM crossbars. In this example, the first three FC layers responsible for generating the Query, Key, and Value matrices, along with the final FC layer in the attention block, are written simultaneously. Hamun performs matrix multiplication using ReRAM crossbars, where one matrix play the role of weights and the other play the role of activations. Once the Query, Key, and Value matrices are produced, the resources used for their respective FC layers are released, and the Key and Value matrices are written into ReRAM memory to perform the matrix multiplication required to get the score matrix. It is important to note that before writing, the Key matrix must be transposed. Hamun utilizes in-situ transposition for this purpose, as described in section~\ref{subs:Architecture}. Once the matrix multiplications are complete, the assigned ReRAM crossbars are released, allowing the weights for the final two feed-forward FC layers to be written. As illustrated, Hamun follows a layer-by-layer computation approach, respecting the data dependencies between layers to ensure correct execution. Moreover, Hamun overlaps the weight writing process with the computation of dot products to effectively hide the latency associated with costly ReRAM writes, optimizing both time and resource utilization.

The key insight in Hamun's offline scheduling and binding process lies in its dual strategy of overlapping layer computations with the writing of weights for the subsequent layers, alongside efficient resource management to maximize parallelism in the weight update process. By executing the computation of one layer while concurrently writing the weights for the next layer(s), Hamun reduces latency and ensures that weight writing doesn't become a bottleneck. Moreover, efficient resource allocation allows multiple weights to be updated simultaneously, which not only increases performance but also ensures faster transitions between layers.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{images/Scheduler_Execution_Flow.pdf}
    \vskip -0.10in
    \caption{An example of the execution flow in Hamun for an encoder block typically found in LLMs, showcasing its layer-by-layer computation scheme, which efficiently overlaps weight updates with ongoing computations.}
    \label{fig:Execution_Flow}
    \vskip -0.25in
\end{figure}

\subsection{Hamun Fault Handling}\label{subs:Scheduler Fault handling}
In this section, we describe the wear-out fault-handling mechanism employed by Hamun. As mentioned earlier, the Program \& Verify (P\&V) scheme used for updating ReRAM cells can detect cells that become stuck at a specific value (permanent failure) and do not respond to programming pulses. If a ReRAM cell fails due to reaching its endurance limit while being updated, the accelerator immediately alerts the host system via the online monitoring component (Figure~\ref{fig:Compilation}). Hamun's fault-handling routine estimates the impact of this faulty cell in the final accuracy, and if it is above the user-specified threshold it retires the column where the faulty cell resides and re-does the scheduling and binding process to ensure accurate DNN inference despite the fault.

Figure~\ref{fig:Compilation} illustrates the Hamun compilation process when a new cell wears out during inference execution. The process starts with analyzing the PyTorch model to extract the network structure and data dependencies. Following this, the scheduling and binding tasks are performed offline, based on the network structure and its dependencies. Two key optimizations are applied during this phase. The first is network batch execution, where weights are reused across different inferences within a batch, reducing the overhead of multiple memory writing operations for each inference. The second optimization focuses on resource management during the binding process, where resources are prioritized according to their wear level. The wear level is tracked using counters that monitor the frequency of resource usage, this data being stored in the host side to avoid adding extra area overhead to the accelerator. To ensure balanced writing distribution across the ReRAM crossbars, those crossbars that have been utilized less frequently are given higher priority in the binding process to assign new computations to them. This approach optimizes resource usage and prolongs the lifespan of the ReRAM cells by evenly distributing the wear.

After the offline scheduler generates the instructions for executing network inferences, the accelerator proceeds with performing inferences based on this configuration. When a new ReRAM cell wears out, the accelerator notifies the host of the faulty cell's location and the current inference execution stops. At this point, during the "Request" stage, the wear levels of resources are updated based on the number of inferences executed with the current scheduling and binding configuration. A new request is then sent to the scheduler, prompting it to redo the scheduling and binding processes to generate a new configuration that may exclude the worn-out cell while maintaining full network accuracy.

% It's important to note that the affected crossbar may be utilized for multiple layers of computation.

Hamun introduces an approximation optimization that, instead of retiring a faulty cell immediately after its detection, it determines its impact on accuracy together with previously detected and not retired cells. If the collective impact do not significantly degrade accuracy, the current scheduling and binding configuration is maintained, allowing continued operation. However, if the expected accuracy loss exceeds a user-defined acceptable threshold, all faulty cells are retired simultaneously, and a new configuration is computed. This strategy ensures minimal interruptions and maximizes the accelerator's operational lifespan by only retiring cells when absolutely necessary.

When a ReRAM cell becomes worn out, the entire column containing the faulty cell in the crossbar is deactivated by disabling the corresponding mask signal to prevent incorrect dot-product computations. Consequently, during the re-binding process, no weights from any layer are assigned to that specific column. This ensures that the faulty column is not used in future computations. Multiple PE rows are utilized for a layer when the size of the input exceeds the number of cells available in a crossbar column. In such cases, since the final result of the dot-product is obtained by accumulating partial results from all APUs within each PE column, it is crucial that the number of crossbar columns to which the layer's weights are mapped remains consistent across all APUs in a PE column. Therefore, the number of crossbar columns used in each APU is aligned with the minimum number of available columns across all crossbars within a PE column, providing balanced resource utilization and accurate accumulation of results.

At first glance, aligning the number of APUs' columns within each PE column may appear to underutilize the accelerator's resources. However, if the number of columns used in a crossbar is lower than the total number of available columns on that crossbar, this alignment introduces an advantage in the event of a cell in the used columns becoming faulty. In such scenarios, the scheduler can reassign one of the unused columns within the crossbar to replace the faulty one.

This process repeats until the accelerator executes the specified number of inferences set by the user. Throughout the operation, the system continually monitors the health of the ReRAM cells and dynamically updates the scheduler whenever a new worn-out cell is detected.

\subsection{Wear Leveling Techniques}\label{subs:WL}
As illustrated in Figure~\ref{fig:Top_view}, the Hamun system architecture is organized into multiple levels, enabling the implementation of Wear Leveling (WL) techniques in different levels of abstraction. Each PE row includes a counter that tracks the frequency of its use, referred to as the "wear level" of that row. These counters are stored on the host side, and during the binding process, the offline scheduler uses these wear levels to efficiently assign resources for each layer. These counters are updated at "Request" stage according to number of inferences that are executed with the current binding and scheduling configuration.

Hamun employs a counter per PE row rather than per ReRAM crossbar row or per cell for two main reasons. First, having a counter for each individual crossbar cell or row would significantly increase the complexity of the scheduling and binding process, thereby slowing down the offline scheduler's execution time. Managing counters at such a fine-grained level would introduce substantial overhead in updating each cell's usage, especially as faults emerge. Second, Hamun includes a crossbar-level WL technique that eliminates the need to monitor the frequency of cell or row utilization within each crossbar. This approach ensures balanced wear across cells without adding per-cell tracking, streamlining both system efficiency and durability.

The wear level of each PE is determined by the maximum wear level among all of its rows. During the binding process, the scheduler first sorts the available PEs based on their wear level index. Then, for each layer, the scheduler selects the PEs with the lowest wear levels to evenly distribute wear across the system. Similarly, when assigning rows within a PE, Hamun also applies WL techniques, prioritizing rows with lower wear level counters.

As mentioned before, Hamun also employs WL techniques at the crossbar level to enhance the longevity of ReRAM cells. During the weight-writing process in the ReRAM crossbar, cells storing the least significant bits (LSBs) experience a higher frequency of updates compared to those storing the most significant bits (MSBs). To address this imbalance, Hamun utilizes a byte-granularity WL scheme. For each inference, this scheme remaps bit positions to different ReRAM cells in a round robin manner. Figure~\ref{fig:wl}(a) provides an example, in which each cell stores two bits, meaning that four cells are required to store an 8-bit weight. In this example, the two LSB bits of an 8-bit weight are initially stored in \textit{cell 0} of a given set of cells. Then, the WL technique reassigns these bits to \textit{cell 3} in the subsequent inference, so update frequency is distributed evenly across all cells. In addition, this remapping is seamlessly integrated with the dot-product computation by aligning the ADC column sampling order with the remapped bit positions.

% Marc: Try to reorganize this sentence, right now it is a bit confusing.
% When a layerâ€™s weights cannot fully utilize all rows within a subarray, assigning the same physical address sets repeatedly for each inference results in uneven wear across rows.
% subarray
Figure~\ref{fig:wl}(b) illustrates another wear leveling (WL) technique employed by Hamun, designed to evenly distribute wear across ReRAM crossbar rows. When a layerâ€™s weights do not fully occupy all available rows in a crossbar, utilizing the same set of physical address for each inference leads to uneven wear in specific rows. This repeated ReRAM update causes certain rows to undergo more frequent write cycles, leading to premature degradation of those crossbar rows while underutilizing others. To mitigate this, Hamun shifts the starting point of the weight-writing procedure to a different physical address for each inference. As shown in Figure~\ref{fig:wl}(b), for each inference, the starting row of the weight-writing process is incremented by one. This dynamic shifting ensures that all rows wears out at the same pace, extending the life of the ReRAM crossbar. To ensure accuracy in the dot-product computation, the order of activations fetched into the APUâ€™s input register is adapted so that each activation is correctly multiplied by its corresponding weight.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{images/WL.pdf}
    \vskip -0.1in
    \caption{Wear leveling techniques at the crossbar abstraction level. (a): First scheme remaps bit positions to different ReRAM cells for each inference. (b): Second scheme shifts the starting row of the weight-writing procedure in each crossbar for each inference.}
    \label{fig:wl}
    \vskip -0.20in
\end{figure}

The proposed WL schemes in Hamun aim to evenly distribute write operations across all cells in a ReRAM crossbar. Another related approach, proposed by ARAS~\cite{ARAS}, takes a different route by increasing the similarity across the weights of different layers during inference, which reduces the number of weight updates required. While this method successfully decreases the total weight update frequency, it leaves the "hotspot" issue unresolved, especially in cells storing the LSBs, which still experience most of the updates. However, when Hamun's WL techniques are applied on top of ARAS's scheme, the reduced number of total weight updates results in even greater improvements in the accelerator's lifespan. This is because Hamun's WL scheme ensures that the reduced number of weight updates is more evenly distributed across all cells, addressing the imbalance between LSB and MSB cells, and mitigating the wear hotspot issue. Consequently, the combination of both approaches maximizes the lifespan of ReRAM cells by reducing and evenly distributing the frequency of updates.

\subsection{Batch Execution}\label{subs:Batch}
% Batch execution model
% On-chip memory constraint
The weight updating procedure in ReRAM-based accelerators is a costly process in terms of energy consumption, latency, and its impact on the lifespan of memory cells. To mitigate this cost, Hamun employs a batch execution technique. This method allows the accelerator to execute multiple inferences without reallocating resources or rewriting weights between each inference. Essentially, the weights for a given layer are written into the ReRAM crossbars once, and the same set of weights is reused for consecutive inferences. The scheduler in Hamun still follows the same execution flow as described above, but instead of immediately reallocating resources to the next layer after completing one, it processes multiple inferences using the current layer's weights. While Hamunâ€™s batch execution technique helps reduce the cost of frequent weight updates, it does generate more partial results during the computation process, which increases pressure on the on-chip SRAM memory.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\columnwidth]{images/Batching.pdf}
    \vskip -0.10in
    \caption{Batch Execution with size two. Blue blocks represent computation steps for a new inference, while patterned blue and gray blocks indicate additional writes required by the second inference.}
    \label{fig:Batching}
    \vskip -0.25in
\end{figure}

Figure~\ref{fig:Batching} illustrates the batch execution of the previous example (Figure~\ref{fig:Execution_Flow}). For layers with static weights, such as Fully Connected and Convolutional layers, batch execution allows the reuse of written weights across multiple inferences, with the corresponding partial results stored separately in on-chip memory. This means that once a set of weights is written, they are reused for all computations associated with multiple inferences before being overwritten by new weights. Even in cases where the accelerator only has enough ReRAM storage to accommodate a portion of a layer, all computations for that portion across different inferences are completed before reallocating the ReRAM for the next portion.

In cases where a layer does not have static, pre-known weights such as matrix multiplication in an attention block, batch execution becomes more complex. Specifically, executing that layer for multiple inferences requires a separate weight update for each inference, as one of the operands in the matrix multiplication must be written to ReRAM for every inference (shown in patterned blue and gray color in the figure). This weight updating procedure begins as soon as the operand is computed by the preceding layer. Unlike layers with static weights, where Hamun achieves significant lifespan improvements by reusing the same weights across multiple inferences, non-static layers do not benefit from this optimization.

The number of inferences that can be batched together in Hamun depends on the available on-chip SRAM memory. A larger batch size leads to more inferences being processed simultaneously, which generates more partial results, requiring additional on-chip memory for storage. To optimize this, Hamun employs an offline iterative procedure to determine the maximum number of DNN inferences that can be included in a batch. In each iteration, the procedure increments the batch size and calculates the required on-chip memory. The process continues until the memory requirement exceeds the accelerator's on-chip memory capacity. Once the optimal batch size is identified, Hamun sets this value for its offline scheduler, which then generates the appropriate instructions for executing the batch. This approach maximizes the number of inferences processed in parallel, increasing the reuse of written weights in the ReRAM crossbar. As a result, it significantly improves the lifespan of the accelerator by allowing it to execute a greater number of DNN inferences.

\subsection{Approximate Computing}\label{subs:Approximation}
The inherent fault tolerance of deep neural networks (DNNs) allows them to accommodate some inaccuracies in their computations without significantly affecting performance. This resilience, stemming from the redundant and distributed nature of DNN learning processes, enables DNNs to tolerate occasional weight errors. Such fault tolerance is particularly advantageous in ReRAM-based accelerators, where cell wear-related inaccuracies can often be tolerated without requiring immediate retirement of faulty cells. This tolerance can extend the acceleratorâ€™s lifespan by reducing the need for frequent reconfiguration.

% Because each networkâ€™s fault tolerance varies according to its unique learning procedures, Hamun leverages this property to delay the retirement of faulty cells until their effects surpass the networkâ€™s tolerance threshold. To determine each networkâ€™s specific tolerance to faults, Hamun employs an offline procedure to estimate the acceptable level of inaccuracy based on user-defined requirements. This process identifies the maximum number of faults that can affect layer weights before impacting overall performance.

As detailed in Section~\ref{subs:Scheduler Fault handling}, when a ReRAM cell wears out, the scheduler notifies the host via the online monitoring component. Depending on the DNN layer size and binding configuration, a worn-out cell can impact one or multiple layers or even introduce multiple faults within a single layer. To extend the acceleratorâ€™s lifespan, Hamun introduces a fault-tolerant strategy that optimizes fault handling. Rather than retiring each faulty cell immediately, Hamun assesses the faultâ€™s impact on inference accuracy. If the accuracy loss is greater than a user-defined threshold, the faulty cell is retired, and a new scheduling and binding configuration is computed. Otherwise, the retirement is deferred, allowing the accelerator to continue with the existing configuration.

As additional cells wear out, Hamun repeats this process by evaluating the cumulative effect of these multiple faults. This iterative assessment continues until the accumulated faults degrade accuracy beyond the acceptable threshold, at which point all faulty cells are retired, and the system is reconfigured to maintain accuracy. This approach minimizes interruptions by retiring cells only when absolutely necessary, thereby maximizing the operational lifespan of the accelerator.

To gauge the impact of faulty cells on accuracy, Hamun employs an offline estimation method to ensure that accuracy loss remains within the acceptable threshold. During this process, Hamun randomly introduces an equal number of faults across all layers and evaluates the inference accuracy using the entire validation or test dataset. These faults are applied to random weights and random bit positions within those weights, ensuring a general assessment of network robustness. Moreover, to evaluate diverse fault scenarios, for each inference in the validation or test dataset, a different set of random faults is imposed.

Through a iterative process, Hamun progressively increases the fault count uniformly across all layers until accuracy degradation exceeds the user-defined threshold. This provides an \textbf{optimal fault tolerance threshold}, representing the networkâ€™s resilience to inaccuracies. This threshold is used at runtime to guide decisions on whether to postpone retirement of faulty cells without compromising network accuracy.

At runtime, when a fault occurs, Hamunâ€™s host system identifies the affected layer(s) based on the faultâ€™s location. To evaluate potential accuracy loss from multiple faults, Hamun tracks the number of weights impacted within each layer. If this number surpasses the \textbf{fault tolerance threshold} for any layer, Hamun flags the accuracy impact as unacceptable. In such cases, all faulty cells are retired, and the system redoes the binding and scheduling to maintain inference accuracy. This method ensures that accuracy loss remains within the user-specified limits, optimizing both performance and lifespan.
