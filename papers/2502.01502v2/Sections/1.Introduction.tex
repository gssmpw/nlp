% 1- General ReRAM-based accelerator explanations
% 2- Limitation of ReRAM-based accelerators
% 3- ARAS solutions
% 4- ARAS limitation
% 5- Our goal (extend lifespan)
% 6- Our technique (approximate computing and scheduler)
% 7- Optimizations
% 8- Contributions

\section{Introduction}\label{s:intro}
Processing Using Memory (PUM) offers a promising solution to reduce the high computational and energy demands of Deep Neural Network (DNN) inference. Several Non-Volatile Memory (NVM) technologies, such as Phase-Change Memory (PCM)~\cite{PCM}, Spin-Transfer Torque Magnetic RAM (STT-MRAM)~\cite{STT}, and Resistive RAM (ReRAM)~\cite{ResiRCA}, can effectively implement the critical dot-product operations for DNNs in the memory arrays. Among these, ReRAM stands out due to its lower read latency and higher density, making it an ideal choice for DNN accelerators. As a result, there is increasing interest in ReRAM-based PUM designs for their dense and efficient computation capabilities~\cite{PRIME, sparse_ReRAM, CASCADE}.

While ReRAM offers many advantages, its high write latency and energy consumption pose significant challenges in designing efficient DNN accelerators. To address these limitations, prior research has proposed executing multiple DNN inferences in a pipelined fashion to reduce the write overheads of ReRAM~\cite{RAPIDNN, FORMS, ISAAC, TIMELY, AtomLayer, NVMExplorer, RAELLA, Neurosim_mapping, PUMA}. These accelerators are typically designed with the assumption that there are sufficient ReRAM cells to store all network weights, meaning that DNN weights are written only once at the start of the first inference. Although this method minimizes ReRAM's write inefficiencies, it introduces a new challenge that could hinder the future viability of ReRAM-based accelerators as a leading computational platform.

As DNNs continue to grow in complexity, scaling an accelerator's resources to accommodate the largest models is neither area- nor power-efficient. Additionally, this approach lacks \textit{adaptability}, as the accelerators used at present may be unable to handle future, larger networks. To address this problem, some recent proposals~\cite{ARAS, MNEMOSENE, PipeLayer} introduced adaptive, ReRAM-based accelerators for DNN inference that efficiently write DNN weights into a limited-size accelerator at runtime. Unlike previous accelerators, which assume all DNN weights are preloaded, these solutions employ an scheduler to manage tasks such as data transfers, weight updates, and dot-product computations.

% The ARAS scheduler, along with architectural optimizations, significantly reduces the costly ReRAM write process, demonstrating that ReRAM-based accelerators can outperform commercial platforms like Google TPU.

As an example, ARAS~\cite{ARAS} introduces an offline scheduler and a novel execution scheme that allocates available ReRAM cells and perform weight updates of upcoming layers concurrently with the computations in the current layer. Computations are performed sequentially, layer by layer, due to the data dependencies across layers. This approach allows the accelerator to networks of unlimited size with limited resources, and maximizes resource utilization. While ARAS, and other previous proposals, addresses the \textit{adaptability} challenge and introduces new optimizations to reduce the energy and latency costs of ReRAM writes, frequent ReRAM cell updates to allocate resources for upcoming layers remain an important problem due to the limited write endurance of ReRAM cells~\cite{ReRAM_Challenges, ReRAM_characterizing, Aging}.

Current commercial ReRAM storage chips~\cite{Weebit, crossbar} exhibit much lower endurance compared to other memory technologies. This reveals a significant gap in the development of ReRAM cells optimized for PUM applications. Consequently, the lifespan of ReRAM-based accelerators is compromised due to the frequent cell updates required for DNN inference. In this paper, we propose \textit{Hamun\footnotemark}, an approximate computation method to prolong the lifespan of ReRAM-based accelerators. The key innovation is a scheduler that manages weight writings across ReRAM cells, and continues to work even when some cells completely wear out by leveraging the intrinsic fault tolerance of DNN inference. The scheduler includes a mechanism that retires faulty cells once their impact on accuracy exceeds the error tolerance of the DNN.

\footnotetext{A lake that requires special attention to ensure their long-term sustainability.}

Hamun scheduler, similar to ARAS, performs static binding and scheduling tasks for a given DNN, orchestrating the instructions that the accelerator will execute during each inference. When a cell wears out during the weights update for a particular layer, the accelerator sends the location of the faulty cell to the host. The host identifies which network layers are affected by the faulty cell for the current binding configuration and estimates the potential accuracy loss caused by the new fault. If the accuracy impact remains within the DNN’s fault tolerance limits, the accelerator continues with the existing binding and scheduling configuration. Otherwise, all faulty cells are retired, and the scheduler generates a new binding and scheduling configuration that excludes them.

As previously mentioned, this paper introduces a procedure to estimate the accuracy impact of faulty cells. This procedure computes a strict threshold for the number of tolerable faults per each network layer. If the number of faults in a layer exceeds this threshold, the scheduler sets a new binding configuration that excludes the faulty cells. This method ensures that the accuracy loss does not exceed a user-specified limit. The threshold for each layer is determined through a static process that incrementally impose random faults into the network model until the accuracy loss surpasses the defined limit.

% The second procedure, called \textit{Urmia\footnotemark[\value{footnote}]}, offers slightly lower confidence compared to \textit{Hamun}. In this method, a Multi-Layer Perceptron (MLP) network is pre-trained to predict whether the current number of faults per layer will cause an accuracy loss greater than the specified limit. The confidence of this prediction corresponds to the MLP’s accuracy, which is not 100\%, as there can always be corner cases where the MLP makes incorrect predictions.

Hamun scheduler incorporates various optimizations to reduce the frequency of ReRAM cell updates during each inference and reuse the weights for multiple inferences. In particular, it evenly distributes writes across all cells, preventing premature wear-out of specific cells. Another optimization is batch execution, where multiple inferences are processed simultaneously. In this way, weights of a layer are reused across all inferences within a batch before updating the weights for the next layers. Partial results are stored in an on-chip SRAM buffer, and the buffer size determines the maximum number of inferences that can be processed in a batch.

% WEIGHT SHIFTING

In summary, this work aims to extend the lifespan of ReRAM accelerators by using an approximation technique that delays the retirement of wear-out cells if their impact on accuracy is negligible. The main contributions are:

\begin{itemize}

\item We introduce a scheduler that performs binding and scheduling tasks to maximize the lifespan of ReRAM-based accelerators. The scheduler incorporates various optimizations to reuse weights both within and across inferences, and it ensures even utilization of all ReRAM resources to prevent premature wear-out of specific cells. In addition, the scheduler includes a mechanism that retires faulty cells once their impact on accuracy exceeds the DNN’s error tolerance.

% \item The scheduler utilizes two distinct approximation schemes to estimate the accuracy impact of faulty cells. If the estimated impact is below the specified limit, the scheduler delays the retirement of the faulty cell to extend its lifespan. One scheme adopts a conservative approach to assess the potential accuracy impact, while the other employs an MLP to estimate accuracy loss limits. Although the MLP approach offers greater potential for extending the accelerator's lifespan, it has lower confidence due to the inherent limitations of the MLP's performance.

\item The scheduler estimates how faulty cells might affect overall accuracy. When the estimated impact is within an acceptable range, as defined by the user, it delays the retirement of these cells to help extend their useful lifespan.

\item We evaluate the proposed scheme, called \textit{Hamun}, on three representative DNNs. On average, \textit{Hamun}, with all its optimizations, extends the lifespan of the accelerator by $13.2\times$. Notably, the improvements of $4.6\times$ and $2.6\times$ are directly attributed to the fault-handling and batching schemes, respectively.

\end{itemize}

% A paragraph for paper organization in sections.
