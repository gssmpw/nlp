% Baseline
% Modeling
% Endurance
% Benchmarks

\section{Methodology}\label{s:Methodology}
We developed an event-driven simulator to accurately model the lifespan of Hamun, and compare it against ARAS~\cite{ARAS}, which serves as our baseline. Lifespan is defined as the number of inferences a ReRAM-based accelerator can perform while maintaining an acceptable level of throughput. In ARAS, the simulation ends as soon as any ReRAM cell reaches a wear-out point, as it lacks a fault-handling mechanism to deal with faulty cells. In contrast, Hamun is designed with a fault-tolerant approach that allows it to continue functioning even when individual cells wear out. It maintains operation until throughput degradation reaches a user-defined threshold, allowing for greater longevity and resilience in performance. For this simulation, we set the acceptable throughput drop to $40\%$ of the maximum accelerator throughput.

The evaluation of area, latency, and energy consumption for the proposed ReRAM-based accelerator leverages a multi-tool methodology for detailed component modeling. ReRAM crossbars are simulated using NeuroSim~\cite{Neurosim_github}. For on-chip buffers, CACTI-P~\cite{cacti-p} is used, and logic components, such as control and computation units, are implemented in Verilog and synthesized using Synopsys Design Compiler~\cite{Design_compiler} with a 28/32nm technology library. Main memory is assumed to be an LPDDR4 module with 8 GB capacity and 19.2 GB/s bandwidth (single-channel) and is simulated using DRAMSim3~\cite{DRAMsim3}.

To evaluate the accelerator's lifespan, each ReRAM cell is initialized with a specific endurance (number of writes before wear-out). The initial endurance of the cells follow a normal distribution, as established in \cite{Realizing}, with a Coefficient of Variation (CoV) of $0.2$ and a mean value of $2.5\times10^9$.

For a fair comparison with the ARAS baseline, we configured the accelerator with similar parameters, as outlined in Table~\ref{tab:Param}. Specifically, we set the accelerator to include 64 PEs, each composed of 6x4 APUs. Each APU features a crossbar array of $128\times128$ ReRAM cells, and each ReRAM cell has a 2-bit storage resolution. Consequently, representing an 8-bit weight requires four consecutive ReRAM cells. Furthermore, the batch size, or the number of inferences processed concurrently, depends on the available on-chip SRAM memory as explained above. To consider adequate on-chip memory capacity and enable fair comparison, we configure this parameter at 8 MB, in line with the Google Edge TPUâ€™s on-chip SRAM size.

\begin{table}[t!]
\caption{Hamun accelerator configuration parameters.}
\label{tab:Param}
\centering
\resizebox{0.6\columnwidth}{!}{%
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \cellcolor[gray]{0.9} Technology & 32 nm \\
    \cellcolor[gray]{0.9} Frequency & 1 GHz \\ 
    \cellcolor[gray]{0.9} Number of ADCs per APUs & 16 \\
    \cellcolor[gray]{0.9} ADCs Sampling Precision & 6-bits \\
    \cellcolor[gray]{0.9} PE Buffers Size & 1.5 KB \\
    \cellcolor[gray]{0.9} Crossbar Computation Latency & 96 Cycles \\
    \cellcolor[gray]{0.9} Crossbar Row Writing Latency & 6000 Cycles \\
    \hline
    \end{tabular}%
}
\vskip -0.20in
\end{table}

We evaluate our scheme on three prominent DNNs with distinct architectures and applications: Vision Transformer (ViT)~\cite{ViT} for image classification, BERT~\cite{BERT} for question-answering, and GPT-2~\cite{GPT2} for text classification. The Vision Transformer is assessed on the ImageNet~\cite{ImageNet} dataset, which is widely used in image classification tasks. BERT is evaluated on the SQuAD v1.1~\cite{SQuAD} dataset, a benchmark for question-answering tasks that requires the model to identify answers within a context passage. GPT-2, a decoder-based language model, is tested on the IMDB~\cite{imdb} dataset for text classification, specifically for sentiment analysis. Both BERT and ViT contain 12 encoder blocks, while GPT-2 is composed of 12 decoder blocks. The INT8 accuracy (F1) for GPT-2 and BERT are $91.37\%$ and $79.73\%$ respectively, while the accuracy (Top-1) for ViT is $80.96\%$. In the Hamun approximation scheme, the maximum acceptable accuracy loss is set at $1\%$, although this threshold can be adjusted according to user requirements.
