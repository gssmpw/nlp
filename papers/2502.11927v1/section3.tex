
Having examined several illustrative examples that highlight the limitations of naive applications of current continual learning approaches, we now turn to a systematic analysis of three key conceptual challenges that must be addressed to move the field forward.
We structure our discussion around three fundamental aspects: the nature of continuity in learning problems, 
the choice of appropriate spaces and metrics for measuring similarity, and the role of local objectives in learning.
For each aspect, we first present key considerations that emerge from our analysis, 
followed by specific recommendations for future research directions.


\subsection{On Continuity} 
\begin{tcolorbox}[colback=orange!10,colframe=orange!50,boxsep=-1pt]
\textbf{Considerations: Continuity.}
When designing CL systems, one must examine what continuity means and how it manifests. Two fundamental forms of continuity shape the space of possible approaches: temporal continuity in how tasks evolve, and continuity in the underlying task space itself. These distinct types of continuity create different constraints on learning algorithms and require different treatment.
\end{tcolorbox}

\textbf{Cons \#1: Temporal continuity.}
We will refer to a change over time of the joint distribution of data points and prediction targets as ``drift''.
This is classically handled by assigning a potentially different distribution $\mathcal{D}_i$ to every data point $x_i$ \cite{gama2014survey},
with drift occuring when $\mathcal{D}_i \neq \mathcal{D}_j$.
We advocate here for the approach of \citet{hinder2020towards}, who propose a 
Distribution Process to capture this drift
by associating each datapoint $x_i$ with a time $t_i$, such that two datapoints sharing a time also share the same distribution.
The distributions $D_t$ are defined as Markov kernels in the time domain,
and it is now possible to postulate limiting statements similar to the batch setup or discuss concepts such as the mean distribution over a period of time.

\textbf{Cons \#2: Task continuity.}
While the comparatively simple case of continuously varying mixing coefficients of a discrete task set has been considered under the name ``task-free continual learning'' \cite{Lee2020A, jin2021gradient, shanahan2021encoders}, the possibility of a truly continuous task set has been raised \cite{van2022three}, but we are not aware of a systematic exploration of this setting.
For example, in the task-free setting one might first infer task identity and then use task-specific components \cite{heald2021contextual},
but even if task identity inference is solved,
the lack of a discrete task set in the harder case makes the use of task-specific components no longer trivial.

\begin{tcolorbox}[colback=blue!10,colframe=blue!50,boxsep=-1pt]
\textbf{Recommendations: Continuity.}
Based on our analysis of CL continuity challenges, we propose three key directions for future research:
1) formalizing temporal dynamics through drift processes rather than point-wise distributions, 2) understanding
and managing the impact of data presentation schedules, and 3) developing principled approaches for handling
continuous rather than discrete task spaces.
These recommendations aim to help researchers and practitioners better handle continuous aspects of learning while maintaining theoretical rigor and practical applicability.
\end{tcolorbox}

\textbf{Rec \#1: Use Distribution Processes to capture drift.}
We recommend working with Distribution Processes over datapoint-indexed distributions when modelling drift, as this makes the temporal structure explicit.
In particular, the extent to which temporally close distributions are expected to be similar must then be assumed explicitly rather than implicitly.
It is crucial that these assumptions are understood in continual learning, as they are the core idea underlying the notion of tasks,
and further are the reason we expect forward and backward transfer to be possible.
We believe the improvement in clarity of thinking associated with this new formalization will enhance future work.

\textbf{Rec \#2: Consider schedule dependence.}
Let us formalize a data stream as
an underlying dataset and an order in which this is presented, or \textit{schedule}.
While known in stream learning \cite{gama2014survey},
the effects of such a schedule are considered explicitly only by relatively few CL works \cite{Yoon2020Scalable, wang2022schedule},
and \citet{wang2022schedule} showed that most existing continual learning algorithms suffer drastic fluctuations in performance under different schedules.
After considering the expected temporal correlations of the data stream via drift processes, it is likely that significant permutation symmetries (\eg, discrete task orderings) will remain.
After establishing which permutations of the stream do not constitute meaningful information from which the model should learn,
future work should strive to maximize invariance of CL algorithms to such permutations.

\textbf{Rec \#3: Towards continuous task identity.}
Finally we note that, while discreteness of the underlying task set has been an important and productive underlying assumption in continual learning research, principled methods of handling task identity in the truly continuous case (\eg, section \ref{sec:boxpush} and maybe even \ref{sec:starcraft}) should be developed.
Task-specific components, for example, should still be possible where task identity is not discrete.
When representing a task as, \eg, some embedding in a continuous latent space, however, they are no longer trivial and are indeed interestingly non-trivial.
Such principled approaches should strive to account for the now much richer geometry of the task space.


\subsection{On Spaces} 
\begin{tcolorbox}[colback=orange!10,colframe=orange!50,boxsep=-1pt]
\textbf{Considerations: Spaces.}
When examining CL systems, we encounter three distinct types of continuous spaces: 
parameter space, data space, and function space. Each of these spaces requires careful consideration 
of how to measure ``similarity'' or ``distance'' - a choice that is sometimes forced by the problem 
structure. Even after selecting a space, the choice of metric remains critical, as different metrics 
can capture different aspects of the learning problem. Some scenarios may even require inherently 
asymmetric measures of similarity.
\end{tcolorbox}

\textbf{Cons \#1: The three common spaces.}
Most obviously, we have the continuous space of parameters.
Often we also have a continuous space of possible data items, \eg, arrays of floating point pixel values.
Finally, we have the continuous space of functions representable by our neural network.
If we identify a ``task'' with ``the mapping from inputs to outputs which solves the task'' then it can be seen as a special case of a function space.

When one needs to measure ``similarity'' or ``distance'' in continual learning, one will in general do so in one of these spaces.
Sometimes this is a choice, sometimes it is forced.
For example, when considering a mixture of experts solution to a variety of tasks where the architectures of the neural network models corresponding to the experts differ, it is impossible to measure distance in parameter space.
In this case we must instead consider function space.

\textbf{Cons \#2: Metrics.}
Even once the choice of space is made, ``distances'' are not determined until we choose a metric on that space.
Sometimes there will be a natural choice
(\eg, the Fisher metric in function space for classification tasks, or more generally for tasks where the output is a probability distribution).
In an application such as weight space regularization, there is a simple choice of the Euclidean metric,
but this choice is inherently incapable of identifying more or less important parameters for a given task, and may even violate safety constraints in a case like that of section \ref{sec:constraints}.
The more expressive choice of the Fisher metric as used in Natural Gradient Descent would allow such parameters to be identified.
This may allow a new task to make use of those subspaces of parameter space left unspecified by the preceding tasks.

\textbf{Cons \#3: Divergences.}
Finally, it is often the case that a notion of ``distance'' in a continual learning problem can be identified with a KL divergence, and is thus inherently asymmetrical.
For example, suppose we wish to identify new tasks by measuring the ``distance'' between a memory buffer and a sequence of new datapoints.
If the memory buffer contains datapoints from tasks A and B, but the sequence of new datapoints comes only from task B, is it a new task? Clearly not.
But if this was reversed, and the new datapoints came from A and B, while the buffer came only from B, then the memory buffer would be insufficient to determine correct behaviour on the new points from task A and the answer to the question ``is there a new task'' must be yes.
Consider the case of two 2D Gaussian distributions centered at (0,0) and (1, 0) with isotropic standard deviations 2 and 0.5, respectively.
The KL divergence in one direction is 2.8 bits, but in the other it is 20.5 bits.
Intuitively, this is because samples from the small Gaussian are in-distribution for the large Gaussian, but not vice-versa.
More concretely, in the previously considered application of the Fisher metric to parameter space regularization,
one direction corresponds to measuring distances relative to the Fisher metric measured on the new datapoints, whereas as the other corresponds to using the Fisher metric measured on the buffer.

\begin{tcolorbox}[colback=blue!10,colframe=blue!50,boxsep=-1pt]
\textbf{Recommendations: Spaces.}
Based on our analysis of the different spaces and metrics in continual learning, we propose 
several practical guidelines for developing more effective methods. These recommendations 
focus on making explicit choices about spaces and metrics, recognizing potential asymmetries 
in similarity measures, and considering alternative spaces when standard approaches fail.
\end{tcolorbox}

\textbf{Rec \#1: Choose the correct metric and space.}
Firstly, one must choose the space in which to measure this similarity or distance.
The straightforward option might be to consider raw data such as pixel values, but perhaps semantic differences would be easier to detect in some function space, such as a latent space of a neural network.
Then, having identified the correct space, one must choose a metric on that space.
Even when making ``no choice'' and using the Euclidean metric, one should be mindful of what this means.
For example, when doing weight space regularization, using a quadratic penalty in the Euclidean metric corresponds to the assumption that the appropriate posterior on weights is an isotropic Gaussian.
Making the implications of this ``non-choice'' concrete will allow the implicit assumptions to be sanity-checked.

\textbf{Rec \#2: Remember that the correct notion of similarity may not be symmetric.}
One should also pay attention to any asymmetries in the application of a notion of distance.
Often the ``distance'' measure required in an algorithm will correspond to a KL divergence.
Whether you would like your distance measure to behave like forward KL divergence or reverse KL divergence depends on the purpose of the measure: ``how informative is task A about task B'' will often have a different answer to ``how informative is task B about task A''.
Choosing the wrong direction here will likely result in severe algorithm underperformance, even though both directions agree when the tasks being compared are relatively similar.
Since asymmetries here become most salient when similarity is low, toy examples with large distances should be considered and sanity-checked by comparing both possible directions.

\textbf{Rec \#3: Consider patching broken methods by switching spaces or metrics.}
If a continual learning method fails in some particular application, it may be salvageable by altering the space in which distances are measured.
Suppose, for example that one uses functional regularization in a task where the output of the network is target robot arm pose parameterized by joint angles.
This may fail if task success is dependent on end effector pose, and the sensitivity of end effector pose to joint angle is itself highly dependent on robot pose, due to nonlinear kinematics.
In this case, re-expressing the output in terms of end effector pose via a kinematics model may resolve these difficulties.

\subsection{On Objectives}
\begin{tcolorbox}[colback=orange!10,colframe=orange!50,boxsep=-1pt]
\textbf{Considerations: Objectives.}
Current perspectives on continual learning tend to focus narrowly on accumulating knowledge through 
classification tasks. However, this view may be inherently limiting, as it emphasizes conditional 
knowledge (''which class, given these classes?") over unconditional understanding. The relationship 
between classification, density estimation, and generative modeling suggests broader ways to think 
about knowledge retention in continual learning systems.
\end{tcolorbox}

\textbf{Cons \#1: Accumulating unconditional knowledge.} \\
The knowledge involved in successful classification is inherently of a very conditional nature,
\ie, we answer the question ``given that this datapoint is drawn from the distribution of one of these $N$ classes, which class is it''.
We argue that focusing on classification objectives over density estimation or generative objectives makes continual or lifelong learning unnecessarily overcomplicated.
For example, out of distribution detection is clearly more closely related to density estimation,
and there are whole classes of replay based continual learning algorithms which are closely related to generation.
We believe that building continual learning algorithms on top of narrow classification tasks neglects the potential synergies of introducing generative or density based objectives,
as we shall now discuss.

\begin{tcolorbox}[colback=blue!10,colframe=blue!50,boxsep=-1pt]
\textbf{Recommendations: Objectives.}
Drawing from our analysis of the role of different learning objectives, we propose several 
directions for expanding beyond pure classification in continual learning. These recommendations 
emphasize the potential benefits of incorporating generative and density-based approaches, 
both for avoiding catastrophic forgetting and for more robust task identification.
\end{tcolorbox}

\textbf{Rec \#1: Consider generation for avoiding forgetting.} \\
Where the base task incorporates a generative objective, many challenges related to regularizing on or reviewing data examples from previous tasks are greatly simplified by direct exploitation of this generative function to create synthetic datapoints \cite{Robins95pseudorehearsal}.

\textbf{Rec \#2: Consider densities for task identification.} \\
In the presence of density estimation capabilities available from the base task, it is much easier to assign future datapoints to tasks and to consider questions of task boundaries, be they discrete or continuous.

\textbf{Rec \#3: Consider the energy-based model connection.} \\
Even in the case of primarily classification objectives there seems to be great potential for density estimation via connections to energy-based models \cite{grathwohl2020secretlyenergybased, li2022energybasedforcontinual}.
This could be of great use in the primary evaluation settings common within continual learning.

