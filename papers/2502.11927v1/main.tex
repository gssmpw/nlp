
\documentclass{article}
\usepackage{changes}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}

\usepackage{tcolorbox}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{lipsum}
\setlipsum{%
  par-before = \begingroup\color{gray!20},
  par-after = \endgroup
}


\definecolor{cerisepink}{rgb}{0.93, 0.23, 0.51}

% author colors
\definechangesauthor[name=MetaCommentary, color=blue]{META}
\definechangesauthor[name=Rupert, color=orange]{RM}
\definechangesauthor[name=Roshni, color=cyan]{RK}
\definechangesauthor[name=Samin, color=brown]{SH}
\definechangesauthor[name=Nikhil, color=teal]{NC}
\definechangesauthor[name=Fabian, color=purple]{FH}
\definechangesauthor[name=Carlo, color=olive]{CD}
\definechangesauthor[name=Robin, color=magenta]{RH}
\definechangesauthor[name=Dustin, color=cyan]{DC}
\definechangesauthor[name=Raffaello, color=violet]{RC}
\definechangesauthor[name=Gido, color=green]{GvdV}
\definechangesauthor[name=AF, color=teal]{AF}
\definechangesauthor[name=AC, color=brown]{AC}
\definechangesauthor[name=TT, color=red]{TT}
\definechangesauthor[name=Marco, color=cerisepink]{MC}
\definechangesauthor[name=Ex, color=blue]{Existing}
\definechangesauthor[name=Ne, color=red]{Needed}
\definechangesauthor[name=In, color=orange]{Internal}

\newcommand{\RK}[1]{\textcolor{purple}{RK: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% abbreviations
\usepackage{xspace}
\newcommand{\eg}{\emph{e.\thinspace{}g.}\@\xspace}
\newcommand{\ie}{\emph{i.\thinspace{}e.}\@\xspace}
\newcommand{\Ie}{\emph{I.\thinspace{}e.}\@\xspace}
\newcommand{\etal}{\emph{et al.}\@\xspace}
\newcommand{\iid}{\emph{i.\thinspace{}i.\thinspace{}d.}\@\xspace}
\newcommand{\wrt}{\emph{w.\thinspace{}r.\thinspace{}t.}\@\xspace}
\newcommand{\cf}{\emph{cf.}\@\xspace}
\newcommand{\etc}{\emph{etc}}

\icmltitlerunning{Continual Learning Should Move Beyond Incremental Classification}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\twocolumn[
\icmltitle{\hfill Continual Learning Should Move Beyond Incremental Classification \hfill}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.15 in

\normalsize\bf{Rupert Mitchell} \hfill\small\it{TU Darmstadt, Hessian Center for AI, Germany}\\
\normalsize\bf{Antonio Alliegro} \hfill\small\it{Polytechnic University of Turin, Italy}\\
\normalsize\bf{Raffaello Camoriano} \hfill\small\it{Polytechnic University of Turin, Italian Institute of Technology, Italy}\\
\normalsize\bf{Dustin Carrión-Ojeda} \hfill\small\it{TU Darmstadt, Hessian Center for AI, Germany}\\
\normalsize\bf{Antonio Carta} \hfill\small\it{University of Pisa, Italy}\\
\normalsize\bf{Georgia Chalvatzaki} \hfill\small\it{TU Darmstadt, Hessian Center for AI, Germany}\\
\normalsize\bf{Nikhil Churamani} \hfill\small\it{University of Cambridge, United Kigdom}\\
\normalsize\bf{Carlo D'Eramo} \hfill\small\it{University of Würzburg, Germany}\\
\normalsize\bf{Samin Hamidi} \hfill\small\it{Independent Researcher}\\
\normalsize\bf{Robin Hesse} \hfill\small\it{TU Darmstadt, Germany}\\
\normalsize\bf{Fabian Hinder} \hfill\small\it{Bielefeld University, Germany}\\
\normalsize\bf{Roshni Ramanna Kamath} \hfill\small\it{TU Darmstadt, Hessian Center for AI, Germany}\\
\normalsize\bf{Vincenzo Lomonaco} \hfill\small\it{University of Pisa, Italy}\\
\normalsize\bf{Subarnaduti Paul} \hfill\small\it{University of Bremen, Germany}\\
\normalsize\bf{Francesca Pistilli} \hfill\small\it{Polytechnic University of Turin, Italy}\\
\normalsize\bf{Tinne Tuytelaars} \hfill\small\it{KU Leuven, Belgium}\\
\normalsize\bf{Gido M van de Ven} \hfill\small\it{KU Leuven, Belgium}\\
\normalsize\bf{Kristian Kersting} \hfill\small\it{TU Darmstadt, Hessian Center for AI, German Research Center for AI, Germany}\\
\normalsize\bf{Simone Schaub-Meyer} \hfill\small\it{TU Darmstadt, Hessian Center for AI, Germany}\\
\normalsize\bf{Martin Mundt} \hfill \small\it{University of Bremen, Germany}

\vskip 0.35in
]

\begin{abstract}

Continual learning (CL) is the sub-field of machine learning concerned with accumulating knowledge in dynamic environments. So far, 
CL research has mainly focused on incremental classification tasks, where models learn to classify new categories while retaining knowledge of previously learned ones. Here, we argue that maintaining such a focus limits both theoretical development and practical applicability of CL methods. Through a detailed analysis of concrete examples --- including multi-target classification, robotics with constrained output spaces, learning in continuous task domains, and higher-level concept memorization --- we demonstrate how current CL approaches often fail when applied beyond standard classification. We identify three fundamental challenges: (C1) the nature of continuity in learning problems, (C2) the choice of appropriate spaces and metrics for measuring similarity, and (C3) the role of learning objectives beyond classification. For each challenge, we provide specific recommendations to help move the field forward, including formalizing temporal dynamics through distribution processes, developing principled approaches for continuous task spaces, and incorporating density estimation and generative objectives. In so doing, this position paper aims to broaden the scope of CL research while strengthening its theoretical foundations, making it more applicable to real-world problems.
\end{abstract}

\section{Introduction}
\label{submission}

While textbook machine learning methods assume data distributions are stationary and all training data is collected upfront, in many practical applications, new data becomes available, and new requirements (tasks) emerge over time. Learning then becomes a continual process, updating model parameters all the time to keep track of the changing conditions. The non-stationarity of this `incremental classification' setting ---be it due to the new tasks resulting in new loss terms or due to shifts in the data distribution (aka domain shifts)--- makes standard methods fail, resulting in ‘catastrophic forgetting’ of previously learned knowledge. In contrast, the goal of continual learning methods is to accumulate knowledge without such catastrophic forgetting.


Continual learning (CL) is a broad framework primarily explored in research papers through the lens of classification. The dominant setup consists of a sequence of classification tasks, usually obtained by taking a classification benchmark dataset and splitting it into smaller parts, referred to as ‘tasks’, each containing data exclusively from a disjoint subset of classes. When learning a task, it is assumed that only the data of the current task is accessible. This setup is chosen for its high reproducibility, transparency, and simplicity. Many CL methods are evaluated and compared only in this setup, encouraging overfitting to or even designing specifically for this particular setup.
It is implicitly assumed that conclusions derived from this setup and algorithms designed for it generalize to more practical use cases and other tasks beyond classification. But is that really the case?

\newpage
In this position paper, we argue that moving beyond the incremental classification paradigm is crucial for developing CL methods that are theoretically grounded and broadly applicable to real-world problems.
While we indeed acknowledge the utility of addressing incremental classification, we argue that such solutions may not generalize as well as often implicitly assumed.
In particular, many works claim ``state of the art'' results in CL while only considering incremental classification.
To this end, we highlight the limits of methodology developed solely in the context of supervised classification
by examining concrete examples involving multi-target classification, 
optimization with constrained output spaces,
CL in the absence of a natural discretization of tasks,
and higher-level concept memorization.
We combine these with conceptual analysis of prototypical continual learning methods like iCaRL \cite{Rebuffi_2017_icarl} and regularization-based ones like EWC \cite{kirkpatrick_ewc_et_al_2017} or moment matching \cite{Lee2017_imm}. By illustrating challenging scenarios where CL is particularly relevant, we highlight potential pitfalls when applying na\"ive implementations to our selected examples. This approach provides valuable insights for the CL research community, guiding future research directions. 

We proceed as follows. We start by examining concrete examples that illustrate key limitations of current CL approaches. We then analyze fundamental conceptual challenges these examples reveal. Finally, we conclude with recommendations for future research.



\section{Core Examples} 
In the following subsections, we consider important example problems, each illustrating an extension of classic supervised CL.
In each case, to illustrate the importance of considering extension, we consider the difficulties in applying the popular pillars of CL methodology: functional approaches, regularization strategies, and data retention, respectively (\ie, iCaRL and Knowledge Distillation, Elastic Weight Consolidation, Coresets).
We close each subsection with suggestions for future directions of CL research to address these difficulties.

\subsection{How well addressed is supervised CL for classification?}
\label{sec:faces}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{images/facial.pdf}
\caption{Map of diverse facial expressions on Arousal-Valence axes. This representation captures the inherently continuous variation of expressions as opposed to, e.g., ``angry'' and ``sad''.}
\label{fig:facial}
\end{center}
\end{figure}
To examine generalizability in familiar territory, we start with an example close to standard class-incremental supervised learning.
Specifically, we consider the problem of continual facial expression detection and classification from image data using neural networks.

A common representation for facial expressions uses 12 Action Units --- discrete facial muscle regions that can be active or inactive. While more detailed representations indeed exist, we consider this simple case only.
The core challenge 
here is that  
it is actually a multi-target prediction problem.
While it is a classification problem, the archetypical CL problem has a single set of discrete clusters, which we identify as classes.
It is not clear how to adapt CL methods that rely heavily on such clustering to this problem.
Further, requiring the presence of explicit classes requires an explicit discretization of the problem into clusters.
We argue that for data such as facial expressions, such a discretization is at best difficult to correctly construct, and at worst incoherent.
If we were to formulate our problem in terms of regression, such as in the 2D arousal-valence representation of expressions seen in Fig.~\ref{fig:facial}, such discretization issues would disappear. This, however, would explicitly restrict us to CL methods that function correctly in the absence of class 
labels.

Even if we were to na\"ively apply a popular method that uses class labels to this problem, \eg, iCarl \cite{Rebuffi_2017_icarl}, we would encounter similar issues. 
Specifically, iCarl populates a memory buffer such that it is balanced across classes and 
the mean of the examples $p_j$ for any given class is close to the mean $\mu$ for the cluster $X$ of datapoints $x$ corresponding to that class:
\begin{equation}
p_k \leftarrow \argmin\nolimits_{x \in X} \left\| \mu - \frac{1}{k}\left[\phi(x)\!+\!\sum\nolimits_{j=1}^{k-1} \phi(p_j)\right] \right\|,
\end{equation}
where $\phi$ is some reasonable feature representation of datapoints.
If one treats the entire dataset as a single cluster, then we do not expect the center of that cluster to be meaningful, as the distribution is highly multimodal.
Alternatively, one could use the multi-target classes and consider a single cluster for the purposes of iCarl to correspond to a choice of class for every target (every Action Unit).
Unfortunately, even in this case, with only two classes per target, the total number of 
clusters grows exponentially in the number $N$ of targets (Action Units) as $2^N$.
While this may still be possible for the case of 12 Action Units and, in turn, $4096$ total clusters, it will quickly explode combinatorially as $N$ increases; 
32 Action Units would give already $4.3 \times 10^9$ clusters, likely exceeding the total dataset size by orders of magnitude and making the idea of taking the average of the datapoints within a cluster impossible.

The traditional classification-based settings of CL encourage methods to explicitly rely on class labels,
and this implicitly requires the data to be discretized into a single sensibly-sized clustering.
We have seen that alternatives such as multi-target classification bring their own problems, and that more continuous regression formulations of the prediction task may remove these difficulties.
We further note that the cross-entropy loss itself introduces difficulties for class-incremental learning, due to the necessity to add new output nodes,
and, more generally, due to the non-constant curvature of the cross-entropy loss interacting poorly with the implicit gaussian posteriors of EWC-like parameter regularization methods.
We expect that there are many applications for CL where the assumption of a single-target classification objective artificially complicates CL, and results in CL methods not generalizing as well as they should.


\subsection{How can CL accommodate constraints?}
\label{sec:constraints}
\begin{figure}[t]
\centering
    \begin{center}
    \includegraphics[width=1.0\linewidth]{images/sphere_crop1.png}
    \caption{Depiction of a trajectory learned from demonstrations (shown in red) on the surface of a sphere. This example illustrates the challenge of constrained structured prediction in robotics, where valid outputs must lie on a two-dimensional manifold (the sphere's surface) within a three-dimensional space. Traditional continual learning approaches using Euclidean distance metrics may fail to maintain such geometric constraints during learning.}
    \label{fig:sphere}
    \end{center}
\end{figure}

Robotics presents another domain where naive applications of common CL methods can be unreliable.
In robotics, problems often involve predictions lying in nonlinear output spaces
due to physical constraints imposed by a robot's embodiment and environment.
Directly minimizing a loss measured in a Euclidean output space may fail to capture the structure of the output, compromising the optimality of control outputs and potentially violating safety constraints.
While substantial progress has been made on structured prediction for robotics in, standard, non-continual settings, extending these approaches to continual learning remains 
largely unexplored.

Consider the task of generating robot arm trajectories constrained to lie on the surface of a sphere, for example, to ensure safety by avoiding collisions (Fig.~\ref{fig:sphere}).
The full space of end effector locations is three dimensional, but the space of valid outputs is the two dimensional spherical surface.
Methods have been proposed to constrain model outputs to such structured target spaces, such as encoding the output space into a linear surrogate space for training, and then decoding predictions back into the original structured space \cite{bakir2007predicting}. This can also be done implicitly with surrogate losses that enforce desired output properties \cite{ciliberto2020general}, an approach used in imitation learning \cite{zeestraten2017approach, duan2024structuredpredrobotimit} and reinforcement learning \cite{liu2022robot}. However, the feasibility of extending this approach to continual robot learning remains understudied. 
A pioneering work in this direction is \cite{daab2024incremental}, introducing a method for incrementally learning motion primitives on Riemannian manifolds.


If one were to naively apply a parameter-space regularization method, such as EWC \cite{kirkpatrick_ewc_et_al_2017}, to a task with manifold constraints on the outputs, the approach would minimize the squared distance in parameter space between old and new parameters, weighted by their importance.
Specifically, one is assuming that the increase in loss for task $t$ as the parameters $\theta$ drift from their optimum $\theta_t^*$ in future learning is approximately proportional to
\begin{equation}
    \mathcal{L}_t (\theta) - \mathcal{L}_t (\theta^*_t) \propto \sum\nolimits_{i=1}^{|\theta|} F_{tii} (\theta_i - \theta^*_{ti})^2 ,
\end{equation}
where $F_{tii}$ is the diagonal of the Fisher matrix measuring the relevance of particular parameters $i$ to task $t$.
Unfortunately, it is likely that, even if the predictions at $\theta^*_t$ obey the manifold constraints, the predictions of some arbitrary $\theta$ which is merely close to $\theta_t^*$ according to the Fisher matrix will not.
If the manifold constraints represent, for example, a safety constraint, this is clearly %constitutes 
unacceptable behavior for a CL algorithm.
Not only should each task optimum satisfy the constraints for that task, but the CL algorithm must maintain their satisfaction throughout further training next to minimization the loss. 


In summary, CL methods tend to focus on ensuring that future outputs remain ``close'' to past outputs,
and assume that sufficiently close outputs will remain valid.
In the presence of manifold constraints, it is clear that a naive distance measure on the full output space will not be sufficient to hold future outputs within the valid range.
We expect that exploitation of the surrogate loss approach may allow parameter regularization, functional regularization and simple memory buffer-based CL methods to potentially generalize to the structured prediction setting.
But this generalization must be demonstrated, and, in cases where this surrogate loss is not provided, the relevant problems compensated for in some other way.



\subsection{What is a task? CL in continuous domains.}
\label{sec:boxpush}


\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{images/pushing_box.png}
\caption{A robot arm pushing a box onto a target marker (green). The arm makes contact at a single point and must adjust for the weight distribution in the box. Image from \cite{tiboni2024domain}.}
\end{center}
\end{figure}


In the classic case of CL we have either a single task with a growing number of classes or a discrete set of tasks; here, 
the term ``task'' typically refers to a context in which an input-output pair can be assigned a loss.
For example, one might consider classifying whether an MNIST digit is prime as one task, and classifying whether it is divisible by 3 as another.
Alternatively, one could progressively introduce new classes within the same task, \ie, the classic class-incremental setting.
These standard CL settings are tautologically discrete,
but are discrete changes the only ones we should be concerned with in CL?

For the sake of illustration, consider the problem of pushing a box along the ground using a single manipulator, \ie., force can be applied at a single location on the box.
Now, allow the box to contain different arrangements of items such that its internal weight distribution varies.
Imagine solving this problem as a human. You will instantly understand that you would need to apply force along a line passing through the box's center of mass; otherwise, the box would rotate instead of sliding forward.
Further, if the content of the box is not visible, a human can infer the location of the center of mass from the way the box reacts to being pushed and adjust their strategy to compensate.
Clearly, the correct action varies depending on the weight distribution of the box, but how can this be formulated within the incremental classification framework?
Not only is the space of ``tasks'', \ie., the center of mass locations continuous, but there is no task label, and the task should be inferred from context.
One could argue that, since this inference from context is possible, there is only one task with instead multiple classes,
but then the problem again recurs when trying to discretize into classes.
The robot may, however, encounter novel regions of weight distribution space, 
and it is desirable to transfer knowledge about such regions across time,
indicating the presence of some task-like or class-like continual element.
The aggregation of policies across this continuous ``task'' space is thus a natural thing to attempt, and is a problem to which CL should ideally offer solutions.


One can naively imagine applying knowledge distillation \cite{hinton2015distilling} to this box pushing problem. For instance, 
the loss $\mathcal{L}$ due to \citet{hoiem2016learningwithoutforgetting} 
generalizes in the presence of a memory buffer to the following:
\begin{equation}
    \mathcal{L}(\mathbf{\theta}) = 
    \sum\nolimits_n D_\text{KL} ( T_n || P_n(\mathbf{\theta}))
    + \lambda
    \sum\nolimits_b D_\text{KL} ( T_b || P_b(\mathbf{\theta}))
\end{equation}
consisting of two KL-divergence terms between target distributions $T$ and predicted distributions $P(\theta)$.
For new data, the targets $T_n$ are perfectly confident ground truth probabilities, whereas targets $T_b$ for data in the memory buffer are set to the original predicted distribution when this data was memorized.
$\lambda$ is a hyperparameter which allows prioritization between new and old data, 
and we have omitted a temperature parameter
(\ie., implicitly set it to one) from the buffer term
for simplicity.
Suppose that over time the distribution of weight distributions encountered by our robot shifts.
The immediate difficulty is that it is non-trivial to distinguish new tasks from old tasks, as the true boundaries are fuzzy.
If we regularize the learned function weakly, the model will forget weight configurations encountered only in the old data,
but if we regularize strongly then it will be unable to improve its performance on weight configurations more common in the new data.
Rather than simply holding the function stable on old data and allowing it to drift on novel points, it is necessary that the function be regularized to different degrees in different regions even if they have been encountered before.
In particular, it is no longer true that lower drift on all old datapoints is always better --- some amount of ``forgetting'' is desirable in order to improve behavior in scenarios where the model fit is imperfect due 
to 
sparse but extant data.


The problem of continual learning in a real world setting where novel classes or corrupted sensor data may be present and must be handled correctly is more broadly referred to as Open World learning~\cite{mundt2020holisticreview}.
An existing angle of attack on the problem of unmarked task or class boundaries is
Out-of-Distribution (OOD) detection~\cite{hendrycks17baseline,ODIN,gramICML2020,react,energy,gradnorm,cappio2022relationalreasoning}, which focuses on identifying samples which deviate from the previously seen distribution due to the presence of a discrete distribution shift.
Unfortunately, our problem here is deeper --- the discrete clusters or distribution shifts which OOD detects are not merely unlabelled, but nonexistent.
Looking forward, we argue that the notion of ``task'' in the classic incremental setting must be generalized, not only to cases where the task labels are implicit rather than explicit, but to cases where no discrete task label can coherently be assigned due to the continuous nature of the task space.


\subsection{What is memorable?}
\label{sec:starcraft}

The classic continual learning paradigm focuses on retaining input-output pairs,
a natural approach for avoiding catastrophic forgetting.
However, humans also retain more abstract forms of knowledge, suggesting that this input-output paradigm may be insufficient \cite{ilievski2024humansmachines}.
We explore this issue in the context of reinforcement learning (RL), where the expense of gathering data makes
memory especially valuable.

RL memory buffers typically store concrete state-action-result tuples. However, humans also remember more abstract information, such as the availability of strategies.
Consider the so-called ``zergling rush'' in Starcraft II (Fig. \ref{fig:zergling}): if zerglings infiltrate the opponent's base early on,
they can quickly win by destroying the opponent's economy.
To prevent this, players position their buildings to act as walls in order to block their entrances.
The mere possibility of a zergling rush, even if rarely executed, deeply shapes the game.
Humans remember this strategic principle ---how can we capture this sort of abstract knowledge in CL systems?


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{zergling_rush_inkscape_v2.pdf}
\caption{Zergling rush in \textit{Starcraft II}: the blue player (with the tan/blue buildings) has failed to completely block the entrance at the lower right, allowing zerglings (small and red) into their base.
}
\label{fig:zergling}
\end{center}
\end{figure}


Coreset methods \cite{bachem2015_coresets} illustrate the limitations of focusing solely on concrete examples.
A coreset is a weighted subset of the whole dataset which achieves some particular metric of performance, and is usually optimized to be as small as possible.
For example, \citet{Mirzasoleiman2020coreset} consider the smallest set $S$ which, given weights $\gamma_j$, results in total loss gradients $\nabla \mathcal{L}(\theta)$ within $\epsilon$ of the total gradient for the whole dataset  $D$ for all parameter values $\theta$ of a given model:
\begin{multline}
    S^* = \text{arg min}_{S \subseteq D, \gamma_j \geq 0} |S|,\ \text{s.t.} \\
    \underset{\theta \in \Theta}{\text{max}}
    ||
    \sum\nolimits_{i \in D} \nabla \mathcal{L}_i (\theta)
    - \sum\nolimits_{j \in S} \gamma_j \nabla \mathcal{L}_j (\theta) ||
    \leq \epsilon
\end{multline}
The problem here is that the underlying method,
say, a model-free reinforcement learning algorithm such as Soft Actor-Critic \cite{Haarnoja2018softactorcritic}, does not natively know how to reason about strategic counterfactuals.
Concrete examples of the zergling rush being used against an opponent who has not walled off will be very sparse during optimal self-play, so the contribution to total gradients in such data may be low.
Further,
if the underlying RL algorithm requires many examples to reliably learn the universal availability of the strategy, individual examples would likely not improve the gradient approximation for other examples very much.
Thus, even if there is a noticeable total gradient contribution corresponding to rare actual zergling rushes,
the size of a coreset which included the relevant examples might be impractically large.
This becomes even clearer when we look at techniques inspired by explainability methods~\cite{gilpin2018explainingexplanations,burkart2021surveyexplainability}, such as Prototype Networks \cite{Chen:2019:TLL}.
Adapted for CL \cite{Rymarczyk:2023:ICI}, the heuristic for buffer population would be ``store those examples most relevant to decisions."
Clearly,
if the underlying method is unable to sufficiently generalize to correct decisions from individual concrete examples, or there is no actual concrete example available, then this whole class of methods cannot solve our problem.
The problem here is the fundamental difficulty of compressing high level concepts like this 
availability of a strategy (\ie, systematic counterfactual use as opposed to occasional actual use) into a memory containing only real concrete examples.


The high level problem of remembering something more abstract than raw data, is, of course, not a new one.
Indeed the Never Ending Learners of \citet{Chen_neil, mitchell_never_ending_learning} integrate varied information sources 
into a database of abstract relational beliefs.
Further, humans constitute an existence proof of the feasibility of such a heterogeneous memory architecture in biological neural networks \cite{marr1971archicortex,mcclelland1995complementarylearningsystems}.
Even when constrained to considering only long-term memory in particular, multiple components can be distinguished, such as episodic, semantic, and procedural memory \cite{tulving1972episodicandsemanticmemory,graf1985implicitexplicitmemory}.
Nor is it the case that richer notions of memory are unknown to contemporary work on artificial neural networks \cite{thorne2020neural_db}.
We argue that this problem of remembering higher level information should be revisited in the contemporary CL context.



\section{Conceptual Framing: Where from here?}
\input{section3}



\section{Concluding Remarks}
We have argued that expanding the scope of continual learning (CL) research beyond supervised classification with discrete tasks is crucial for the development of theoretically grounded and widely applicable CL systems.
Through the use of illustrative examples, we have analysed the limitations of na\"ively applying current approaches, and have noted the potential of the notions of ``task'', ``similarity'' and ``memorization'' for generalization.

Key recommendations include selecting appropriate spaces in which to measure similarity,
taking care when choosing distance measures on those spaces, and accounting for any relevant asymmetries.
We further suggest integrating generative objectives for the mitigation of catastrophic forgetting
and the potential of density modeling to identify task transitions and out-of-distribution data.
By pursuing these research directions and examining the CL problem from the first principles when encountering atypical applications, we believe that the field can make significant strides towards flexible and adaptive learning systems that bring the recent progress of the field to new areas.

Although significant challenges remain in broadening CL beyond supervised classification, we believe the concrete recommendations in this paper --- from careful selection of similarity metrics to integration of generative objectives --- provide practical steps forward. By examining how current methods fail on non-standard problems and analyzing their underlying assumptions, we hope that this more nuanced view of CL's scope and challenges will help researchers develop methods that gracefully handle the diversity of tasks found in practice.

\section*{Acknowledgements}
Rupert Mitchell was supported in this work by the Hessian research priority programme LOEWE within the project ``Whitebox''.
This paper is a result of the ``Symposium on Continual Learning Beyond Classification'', generously funded by the Hessian Center for AI via the Connectom Networking and Innovation Fund.



\bibliography{syclec_position}
\bibliographystyle{icml2024}




\end{document}
