\section{Method}
\label{sec:Method}


\subsection{A Framework for Explanation Quality}

Consider a classification problem where we have a model, denoted as a function $f$ that maps an input $\mathbf{x}\in\mathbb{R}^D$ to an output $\hat{y}\in \{1, \ldots, C\}$. This function estimates the probability distribution across classes i.e., $f(\mathbf{x})_i=p(y_i|\mathbf{x})~\forall i \in \{1, \ldots, C\}$, so $\hat{y} = \text{argmax}(f(\mathbf{x}))$. To identify the features utilized by the model to predict $\hat{y}$, we employ an explanation function $\Phi$ as follows:
\begin{equation}
    \Phi(\mathbf{x}, f, \hat{y}) = \mathbf{e}
    % \Phi_{f}(\mathbf{x}\mid f, y_i) = \mathbf{e}^i  
\end{equation}
$\Phi$ outputs a real-valued vector $\mathbf{e}$ with $D$ components that assign attribution to each feature $x_i$ in $\mathbf{x}$, indicating its relative importance in $f$'s prediction of class $\hat{y}$.

Various methods exist to assess the suitability or fitness of $\mathbf{e}$ based on different attributes. Generally, we can define a quality measure $\Psi$ as a function that evaluates the quality of a given explanation $\mathbf{e}$, relative to the model $f$, the input $\mathbf{x}$, and the predicted label $\hat{y}$. For brevity, we denote this scalar value as $q_e$, and we use simply $q$ to refer to the function with a fixed $f$, $\mathbf{x}$, and $\hat{y}$ that evaluates a given explanation:

\begin{equation}
    q_e := q(\mathbf{e}) := \Psi(\mathbf{e}, \mathbf{x}, f, \hat{y})
    % \Psi(\mathbf{e}^i, \mathbf{x}, f) = q^i
    \label{eq:q_e}
\end{equation}

%This quality evaluation function $\Psi$ quantifies how well the explanation $\mathbf{e}$ aligns with predetermined criteria, providing a measurable way to assess the accuracy and relevance of the explanation in reflecting the modelâ€™s decision-making process.

\subsection{The Need for Relative Information}
\label{sec:Method_relative}
The value $q_e$ provides practitioners with a measure of how well an explanation $\mathbf{e}$ adheres to predefined quality criteria. It enables comparisons between different explanations by contrasting $q_e$ with $q_{e'}$. However, a crucial question remains for practitioners: How good is $\mathbf{e}$ compared to the whole set of alternative explanations? (see Figure~\ref{fig:general_explanation}).

To answer this question, we could check the position of $q_e$ in the unknown distribution of $q$ across all possible explanations $\mathbf{e'}$. Yet, this approach is impractical as it requires calculating the distribution of $q$ for all possible explanations, which is computationally infeasible.

A strategy to tackle this problem consists of estimating the latent distribution of the quality measure by sampling $q$ values. For instance, Pixel-Flipping \cite{bach2015pixel} compares $q_{e}$ against $q_{e^r}$ which represents the quality score for a random explanation (denoted $\mathbf{e}^r$). This method effectively performs a single-sample estimation of the quality distribution. Pixel-Flipping transforms the original quality measure $q$ into a new transformed measure:

\begin{equation}
    qt(\mathbf{e}) = \Psi(\mathbf{e}, \mathbf{x}, f, \hat{y}) - \Psi(\mathbf{e^r}, \mathbf{x}, f, \hat{y}) = q_e - q_{e^r}
    \label{eq:qt_pixel_flipping}
\end{equation}

However, relying on just one random sample ($q_{e^r}$) can compromise the accuracy of the estimation. A more robust transformation would involve sampling multiple random explanations, computing their quality, and calculating an average. Yet, this method incurs higher computational costs.

To overcome these challenges, our goal is to develop a transformed quality measure $qt$ that satisfies the following criteria:
\begin{itemize}
    \setlength\itemindent{1em} % Adjusts the indent of the label to the right
    \item[\textbf{(R1)}] Provides a value that clearly indicates the relative standing of $q_{e}$ within the latent distribution of all possible $q_{e'}$ values. By evaluating $qt(\mathbf{e})$, a user should be able to determine whether $\mathbf{e}$'s quality is above-average, average, or below-average.
    \item[\textbf{(R2)}] Preserves the comparative information inherent in $q$, especially its capacity to rank explanations. Given a explanation $\mathbf{e^i}$, any explanation $\mathbf{e^j}$ with higher quality, should also have a higher $qt$. More formally, $qt$ should be constructed so that, given any pair of explanations $(e^i, e^j)$, if $q_{e^i}<q_{e^j}$ holds, then $qt(\mathbf{e}^i)<qt(\mathbf{e}^j)$.
    \item[\textbf{(R3)}] Is computationally efficient.
\end{itemize}

\subsection{Proposed Method}\label{sec:Proposed_method}
We propose using the $\mathbf{e}^{inv}$ explanation, which ranks features in inverse order to $\mathbf{e}$, as an alternative to the commonly used random explanation $\mathbf{e}^r$. This approach aims to improve the quality of estimations while maintaining low computational cost. Given $o = \text{argsort}(\mathbf{e})$, we define
\begin{equation}
\label{eq:e_inv}
    e^{inv}_{o_i} := e_{o_{D-i+1}}~~\forall{i} \in \left[1..D\right]
\end{equation}
where $D$ is the number of variables in $\mathbf{e}$.
Therefore, $\mathbf{e}^{inv}$ is a permutation of the values of $\mathbf{e}$ constructed so that the most attributed variable in $\mathbf{e}$ gets the smallest attribution in $\mathbf{e}^{inv}$, the second most attributed variable in $\mathbf{e}$ gets the second smallest attribution, and so on. $\mathbf{e}^{inv}$ is, then, the opposite interpretation of the original explanation $\mathbf{e}$. An example of this procedure is shown below.%in Figure~\ref{fig:einv_example}.

% \begin{figure}[h]
%   \centering
  \begin{minipage}{\columnwidth}
    \begin{align*}
    \mathbf{e} &= [0.1, -0.1, 9.0, 4.0]  & o=\text{argsort}(\mathbf{e}) &= [1, 0, 3, 2] \\
    \mathbf{e}^{inv} &= [4.0, 9.0, -0.1, 0.1]  & \text{argsort}(\mathbf{e}^{inv}) &= [2, 3, 0, 1]
    \end{align*}
  \end{minipage}
%   \caption{Example of constructing $\mathbf{e}^{inv}$. By design, the ranking by attribution of the features in $\mathbf{e}^{inv}$ is the same as for $\mathbf{e}$, but in reverse order (i.e. $\text{argsort}(\mathbf{e})$ = $\text{reversed}(\text{argsort}(\mathbf{e}^{inv}))$).}
%   \label{fig:einv_example}
% \end{figure}
By design, the ranking by attribution of the features in $\mathbf{e}^{inv}$ is the same as for $\mathbf{e}$, but in reverse order (i.e. $\text{argsort}(\mathbf{e})$ = $\text{reversed}(\text{argsort}(\mathbf{e}^{inv}))$).

Once we have $\mathbf{e^{inv}}$, we define the proposed transformation, which we will call its Quality Gap Estimation (\texttt{QGE}), as the difference between the quality value of the original explanation $\mathbf{e}$ and the quality value of $\mathbf{e^{inv}}$:

\begin{equation}
\label{eq:qge}
    \texttt{QGE} = \Psi(\mathbf{e}, \mathbf{x}, f, \hat{y}) - \Psi(\mathbf{e^{inv}}, \mathbf{x}, f, \hat{y})
\end{equation}

The rationale behind this method is intuitive: \texttt{QGE} increases not only when $\Psi(\mathbf{e}, \mathbf{x}, f, \hat{y})$ is high, indicating the high quality of $\mathbf{e}$, but also when $\Psi(\mathbf{e}^{inv}, \mathbf{x}, f, \hat{y})$ is low, reflecting the poor quality of the inversely ranked explanation. A substantial gap between these values suggests that many random explanations would have quality values falling between these two, thereby indicating that $\mathbf{e}$ is of much higher quality than an average-quality explanation. Conversely, a small \texttt{QGE} implies that the quality difference between $\mathbf{e}$ and $\mathbf{e}^{inv}$ is minimal, suggesting that the order in which $\mathbf{e}$ ranks features is approximately as effective as any alternative, regardless of the absolute value of $q_e$. This pattern suggests that \texttt{QGE} satisfies requirement \textbf{R1}, with values approximately zero for average-quality explanations, negative for below-average, and positive for above-average explanations. Moreover, if \textbf{R2} is met, the more above-average $\mathbf{e}$'s quality is, the higher \texttt{QGE} will be, and similarly, the more below-average $q_e$'s quality is, the lower \texttt{QGE} will be. The level of compliance with \textbf{R2} is assessed in Section \ref{sec:suitability_qge}.

Regarding \textbf{R3}, this requirement is met because \texttt{QGE} can be computed quickly. Generally, the cost of computing any transformation $qt$ is dominated by the cost of computing $\Psi$, which is generally a costly function. The fewer times a transformation $qt$ needs to compute $\Psi$, the faster it will be. Determining the \texttt{QGE} requires computing $\Psi$ only twice (the original $\Psi(\mathbf{e}, \mathbf{x}, f, \hat{y})$, and the additional $\Psi(\mathbf{e^{inv}}, \mathbf{x}, f, \hat{y})$). This approach avoids the computational expense of needing multiple samples of $\Psi$ for different explanations to estimate a distribution, as alternative methods do. Other than computing $\Psi$, \texttt{QGE} requires a single subtraction and computing $\mathbf{e}^{inv}$ as indicated in Eq.~\ref{eq:e_inv}. While the time needed to compute $\mathbf{e}^{inv}$ is generally negligible compared to the cost of computing $\Psi$, this step can also be sped up in most cases. Although Eq.~\ref{eq:e_inv} preserves the original attribution values and reallocates them in reverse order, an alternate but straightforward computation could set $e_i^{inv} := -e_i \forall i \in [0..D]$ to achieve the goal of inversely ranking features compared to $\mathbf{e}$ ($\text{argsort}(e)$ = $\text{reversed}(\text{argsort}(e^{inv}))$). However, the magnitudes of the attributions are not maintained after this transformation and, in instances where the quality metric $\Psi$ requires bounded attribution values, users would need to adhere to the original formulation in Equation \ref{eq:e_inv} if a shift and scale of $e_i^{inv} := -e_i$ is unsuitable.

\ifdefined\doubleblind
  An implementation of \texttt{QGE} is available in [GitHub repository removed for double-blind review]
\else
  An implementation of \texttt{QGE} for a wide variety of quality measures is available in the Quantus toolkit: \url{https://github.com/understandable-machine-intelligence-lab/Quantus}
\fi
