\section{Introduction}
\label{sec:Introduction}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/qge.png}
    % {images/method_summary.png}
    \caption{(Step 1). The usual XAI pipeline allows the user to obtain an explanation $e$ for a prediction $\hat{y}$ using any explanation method. This is demonstrated for two distinct inputs ($\mathbf{x_0}$ and $\mathbf{x_1}$), producing predictions $\hat{y}_0$ and $\hat{y}1$, and explanations $\mathbf{e_0}$ and $\mathbf{e_1}$, respectively. (Step 2). To assess the quality of explanation $\mathbf{e}$ for prediction $\hat{y}$, the user computes a quality measure $q$. In this example, we use the area under the Pixel-Flipping curve, though the method can work with any attribution-based quality measure. (Problem) Despite both input/explanation pairs registering identical $q$ values, it remains unknown to the user that $\mathbf{e_0}$ has higher quality than most explanations for the first prediction, while $\mathbf{e_1}$ has average quality compared to other explanations for the second prediction, as shown by their histograms. (Solution) To allow the user to effectively gauge the relative quality of explanation $\mathbf{e}$ against alternative explanations $\mathbf{e'}$, we introduce $\texttt{QGE}$, which measures the difference between the quality of $\mathbf{e}$ and the quality of $\mathbf{e}_{inv}$ (a rearrangement of $\mathbf{e}$ ranking features in reverse order). This comparative quality metric does not require costly sampling of the $q$ distribution. Although both explanations have equivalent $q$ values, using \texttt{QGE}, the user can discern that $\mathbf{e_0}$ is a high-quality explanation for the first prediction, while $\mathbf{e_1}$ is merely average for the second one. The user may then seek a better explanation for the second prediction.}
    \label{fig:general_explanation}
\end{figure*}
% Image source
%https://www.figma.com/file/xaMffbyGDdePRrLbUELSzZ/xai-measure?type=design&node-id=0%3A1&mode=design&t=pCAlxJSLKHzeU6gE-1

Model output explanations play a crucial role in AI alignment by enhancing transparency, understandability, and trustworthiness in AI systems. In most contexts related to explainability, ground-truth explanations are unavailable \cite{dasgupta2022, hedstrom2023metaquantus}. This absence inherently complicates the task of evaluating explanations. Consequently, efforts to evaluate explanations vary widely, ranging from assessing the robustness of explanations to noise, their complexity, and their localization, to evaluating how faithfully an explanation represents the underlying model.

Although it is not possible to develop a metric based on verified ground truth, a crucial insight is that the adequacy of an explanation can still be assessed by comparing it relative to other explanations with the use of quality measures. Commonly, XAI evaluation methods yield numerical fitness measures that indicate the degree to which an explanation adheres to a certain criterion. However, these numerical values do not provide insight into an explanation's standing relative to the spectrum of possible explanations. In this study, we introduce a method that quantifies the quality of attribution-based explanation methods by efficiently estimating how an explanation's quality compares to the rest of the potential explanations. Despite its apparent simplicity, our evaluations using various sanity checks demonstrate that this strategy enhances the reliability of quality metrics.

While other studies have employed different notions of randomness to compare or evaluate explanation methods \cite{findingrightbommer, samek2015}, these approaches are often more computationally intensive and yield less favorable results. The proposed method is independent of the specific dataset, model, task, and, importantly, of the quality metric used.

The main contributions of this paper include:
\begin{itemize}
    \item The introduction of the Quality Gap Estimate (\texttt{QGE}), a novel evaluation strategy that renders existing quality metrics more informative by aiding in the determination of an explanationâ€™s quality relative to alternatives with a limited increase of computational requirements.
    \item A redefinition of the problem of assessing the superiority of an explanation over its alternatives as a sampling problem, which generalizes the conventional method of comparison with a random explanation.
    \item An assessment of the applicability of \texttt{QGE} across a wide array of established quality metrics, evaluating its impact on various dimensions including faithfulness, localization, and robustness.
    \item The presentation of experimental findings that demonstrate an enhancement in the statistical reliability of existing quality metrics through the application of \texttt{QGE}, providing XAI practitioners with more reliable interpretation tools.
\end{itemize}