\section{Additional Results}
This Appendix lists results that complement those mentioned in Section~\ref{sec:experimental-results}.

\subsection{Distribution of QGE}\label{sec:distribution-qge}
The exhaustive exploration of all possible explanations for the 5 input samples used for each of the \texttt{Avila} and \texttt{Glass} datasets confirms that the distribution of \texttt{QGE} is centered around zero, as desired for \textbf{R1} and shown in Fig.~\ref{fig:qge_distribution}.

% The plots for this figure is generated with https://github.com/annahedstroem/eval-project/blob/main/nbs/plot-qge-distribution.ipynb
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{images/qge-histograms-avila-glass_large.png}
    \caption{Density histogram of \texttt{QGE} for every possible explanation of 5 different input samples for datasets \texttt{Avila} and \texttt{Glass}.}
    \label{fig:qge_distribution}
\end{figure}

\subsection{Spearman Correlation Results}\label{sec:exhaustive-spearman}
\normalsize
To complete the analysis in Section~\ref{sec:large_dataset_exploration}.a we also measured the Spearman correlation ($\rho_{q,qt}$) across the same 5 input samples. Fig.~\ref{fig:exhaustive_spearman} shows that \texttt{QGE} outperforms $\texttt{QRAND}_{\text{K}}$ with up to $k=4$ samples for \texttt{Avila}, and $k=5$ for \texttt{Glass}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\linewidth]{images/avila_spearman_large.png}
    \includegraphics[width=0.45\linewidth]{images/glass_spearman_large.png}
    \caption{Spearman correlation of $qt$ with the original $q$ for the \texttt{Avila} and \texttt{Glass} datasets. The blue line indicates the average correlation $\rho_{q,\texttt{QRAND}_{\text{K}}}$ for each value of $K$ over 5 different inputs. The shaded area shows the average $\pm\sigma$. The orange line records the average correlation $\rho_{q,\texttt{QGE}}$ with dashed lines representing the average $\pm\sigma$.}
    \label{fig:exhaustive_spearman}
\end{figure}

\subsection{Pixel-Level Explanations}\label{sec:pixel-level-explanations}
Section~\ref{sec:large_dataset_exploration}.b reports experiments performed on superpixel-level explanations. For those explanations, instead of perturbing input variables separately, the perturbations are applied to blocks of contiguous pixels, denominated superpixels. The superpixel size used for \texttt{CIFAR} was 4x4, while for \texttt{ImageNet} we used 32x32. For completeness, Table~\ref{tab:effects_per_model_and_dataset} reports the results for perturbations applied to individual pixels, which is analogous to the perturbation mode used in all other datasets. These results show that despite having less of an impact than for super-pixel-level explanations, using \texttt{QGE} is advantageous over $\texttt{QRAND}_1$. The enhancement of the effect for superpixel-level explanations (which are higher-quality explanations), confirms the result listed in Section~\ref{sec:exhaustive_exploration} that indicates that the advantage of \texttt{QGE} is larger the higher the quality of the explanations.

% The plots for tables 3 through 6 are generated with https://github.com/annahedstroem/eval-project/blob/main/nbs/3-get_deltas.ipynb after having generated the corresponding files with https://github.com/annahedstroem/eval-project/blob/main/src/extract_measures.py and processed them with https://github.com/annahedstroem/eval-project/blob/main/src/2-measure-performance.py
\begin{table}[!t]
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|l|c|c}
        %\hline
        Dataset & Model & $\Delta \tau$ & $\Delta\rho$\\
        \toprule
        \texttt{cifar} & \texttt{resnet50} & 0.054$\pm$0.03 & 0.058$\pm$0.03\\
        \texttt{imagenet} & \texttt{resnet18} & 0.018$\pm$0.01 & 0.020$\pm$0.01\\
        \texttt{imagenet} & \texttt{resnet50} & 0.019$\pm$0.01 & 0.021$\pm$0.01\\
        \texttt{imagenet} & \texttt{vgg16} & 0.016$\pm$0.01 & 0.018$\pm$0.01\\
        \texttt{imagenet} & \texttt{vit\_b\_32} & 0.040$\pm$0.02 & 0.043$\pm$0.02\\
        \texttt{imagenet} & \texttt{maxvit\_t} & 0.017$\pm$0.02 & 0.018$\pm$0.02\\
        \bottomrule
    \end{tabular}
    \caption{Magnitude of the increase in Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$ for explanations at the pixel level.}
    \label{tab:effects_per_model_and_dataset}
    \end{sc}
    \end{small}
    \end{center}
\end{table}

\subsection{Variation of the Effect With the Distribution's Standard Deviation}\label{sec:exploring-std}

A deeper analysis of the results in Section~\ref{sec:experiments_datasets_models}.c shows considerable variation in the distribution of $q$ depending on the model used, as illustrated in Figure \ref{fig:exhaustive_qmeans_hists}. Table \ref{tab:effects_per_model} also presents the average standard deviation of the $q$ distribution for each model. The fully-trained \textit{mlp} model exhibits a wide range of $q$ values for each input (the average $\sigma$ is 0.19), indicating a substantial difference in quality between the best and worst explanations. In contrast, the \textit{undertrained} model displays significantly less variability in $q$ across both datasets. The most pronounced case is the \textit{untrained} model, which shows highly concentrated $q$ distributions, i.e., minimal numerical differences between the $q$ values of the best and worst explanations. Despite these variations, the impact of utilizing \texttt{QGE} over $\texttt{QRAND}_1$ is consistently positive, confirming its robustness and effectiveness across various conditions.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.33\columnwidth]{images/avila_qmean_dists_large.png}
    \includegraphics[width=0.30\columnwidth]{images/avila_ood-mean_qmean_dists_large.png}
    \includegraphics[width=0.30\columnwidth]{images/avila_untrained_qmean_dists_large.png}
    \caption{Histograms for the distributions of $q$ for 5 different inputs using the \textit{trained}, \textit{ood-mean} and \textit{untrained} models.}
    \label{fig:exhaustive_qmeans_hists}
\end{figure}

\subsection{Effect on Existing Quality Measures}\label{sec:effect-on-existing-appendix}

In Figure \ref{fig:meta_eval_area_graph_fmnist}, we show the different area graphs which each contain the results from the meta-evaluation analysis (set as coordinates on a 2D plane) for the \texttt{fMNIST} \cite{fashionmnist2015} and \texttt{ImageNet} \cite{ILSVRC15} datasets, respectively. The titles hold the summarising $\mathbf{MC}$ score and each edge contains the meta-evaluation vector scores. By inspecting the colored areas of the respective estimators in terms of their size and shape, we can deduce the overall performance of both failure modes. Larger colored areas imply better performance on the different scoring criteria and the grey area indicates the area of an optimally performing quality estimator. The Quantus \cite{hedstrom2022quantus} and MetaQuantus \cite{hedstrom2023metaquantus} libraries were used for the experiments.
% To reproduce experiments, please see the code available at \ifdefined\doubleblind
%   [GitHub repository removed for double-blind review]
% \else
%     \url{https://github.com/understandable-machine-intelligence-lab/Quantus}
% \fi

\begin{figure}[h!]
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{images/full_area_graph_fMNIST_large.png}
        \caption{\texttt{fMNIST - LeNet}}
        \label{fig:fmnist}
    \end{subfigure}
    
    \vspace{1em} % Space between the images
    
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{images/full_area_graph_ImageNet_large.png}
        \caption{\texttt{ImageNet - ResNet18}}
        \label{fig:imagenet}
    \end{subfigure}
    \caption{
 A graphical representation of the benchmarking results aggregated over 3 iterations with $K=5$. We use $\{\textit{Saliency, Integrated Gradients, Input X Gradient}\}$ as explanation methods.
 Each column corresponds to a quality estimator, from left to right: \texttt{Pixel-Flipping} (PF) (Faithfulness), \texttt{Relevance Rank Accuracy} (RRA) and \texttt{Relevance Mass Accuracy} (RMA)
 (Localization). The bottom row shows results with \texttt{QGE}. Solid shapes correspond to input perturbations, and striped shapes to model perturbations. The grey area indicates the area of an optimally performing estimator, i.e., $\mathbf{m}^{*} = \mathbb{1}^4$. The MC score (indicated in brackets) is averaged over Model- and Input perturbation tests. Higher values and larger colored areas indicate higher performance.} 
\label{fig:meta_eval_area_graph_fmnist}
\end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{images/full_area_graph_ImageNet.png}
%     \caption{
%  A graphical representation of the \texttt{ImageNet} benchmarking results aggregated over 3 iterations with $K=5$. Here, we use $\{\textit{Gradient, GradCAM}\}$ as explanation methods in the meta-evaluation.
%  Each column corresponds to a quality estimator, from left to right: \textit{Pixel-Flipping} (PF) (Faithfulness), \textit{Relevance Rank Accuracy} (RRA) and \textit{Relevance Mass Accuracy} (RMA)
%  (Localization). The bottom row shows results with \texttt{QGE}.
%  The grey area indicates the area of an optimally performing estimator, i.e., $\mathbf{m}^{*} = \mathbb{1}^4$. The MC score (indicated in brackets) is averaged over Model- and Input perturbation tests. Higher values and larger colored areas indicate higher performance.} 
% \label{fig:meta_eval_area_graph_imagenet}
% \end{figure}

\section{Code and Reproducibility}
%Please find the attached file ``Code and reproducibility.zip" which contains the implementation used in all the experiments and detailed instructions on how to reproduce them.
An implementation of \texttt{QGE} for a wide variety of quality measures is available in the Quantus toolkit: \url{https://github.com/understandable-machine-intelligence-lab/Quantus}

The code used for all experiments is available at \url{https://github.com/annahedstroem/eval-project}