\section{Experimental results}\label{sec:experimental-results}
To verify the quality of our method we performed experiments focused on two main points:

\begin{itemize}
    \item Assesing the level of compliance of \texttt{QGE} with the requirements listed in Section~\ref{sec:Method_relative}:
    \begin{enumerate}
        \item[\textbf{(R1)}] is met by the design of \texttt{QGE} (see intuitive explanation after Eq.~{\ref{eq:qge}}). We report a complete exploration of the \texttt{QGE} distribution that confirms this.
        \item[\textbf{(R2)}] Evaluating \texttt{QGE} in its ability to preserve the information inherent in the original quality metric $q$ (\textbf{R2}). The details of this evaluation are discussed in Section~\ref{sec:suitability_qge}.
        \item[\textbf{(R3)}] The cost of computing a transformation $qt$ is dominated by how many times it needs to compute the original quality function $\Psi$. Our experiments confirm this, and the results presented allow the comparison of \texttt{QGE} with an alternative of similar cost. 
    \end{enumerate}

    \item Assessing the statistical reliability of \texttt{QGE} compared to competitive baseline methods. The experiments conducted for this are reported in Section \ref{sec:effect_on_metrics}. %\textcolor{red}
% {\Large We find that .......[sumarise findings]....... Reference Appendix.}
%     %Investigating the impact of using $qge$ on the robustness and adversarial reactivity of established quality metrics. The findings from these experiments are detailed in Section \ref{sec:effect_on_metrics}.
\end{itemize}

In our experiments, we took a fixed model $f$, input sample $\mathbf{x}$, and label $y$. Although these parameters remained constant for each individual experiment, we varied them across different experiments to test the robustness and general applicability of our method.

\ifdefined\doubleblind
  The code used for all experiments is available at [GitHub repository removed for double-blind review]
\else
  The code used for all experiments is available at \url{https://github.com/annahedstroem/eval-project}
\fi

\subsection{Suitability of \texttt{QGE}}\label{sec:suitability_qge}
For our initial experiments, we employed the \texttt{Pixel-Flipping} quality measure \cite{bach2015pixel}. Let $f_y(\mathbf{x})$ represent the output of model $f$ for class $y$, and $\mathbb{P}(\mathbf{x},\mathbf{e}, M)$ denote a perturbation function that modifies all features of $\mathbf{x}$ except for the $M$ most attributed features according to explanation $\mathbf{e}$ (with $M \in [0,D]$). The quality of explanation $\mathbf{e}$ is then measured as the average value\footnote{Some implementations measure the area under the curve instead of the average activation level. Both alternatives are equivalent since they are proportional.} of $f_y(\mathbf{x})$ over all possible levels of feature selection $M$:

\begin{equation}
\label{eq:q_value}
    q_e:=\Psi(\mathbf{e},\mathbf{x},f,y) = \frac{1}{D}\sum_{m=0}^{D}{f_y(\mathbb{P}(\mathbf{x},\mathbf{e},m))}
\end{equation}

In this experimental setup, the perturbation function $\mathbb{P}(\mathbf{x}, \mathbf{e}, M)$ results in $\mathbf{x'}$, where the $D-M$ least relevant features (as determined by $\mathbf{e}$) are replaced with zeros. This introduces the well-known problem that $\mathbf{x'}$ is outside the distribution for which the model $f$ was trained~\cite{hase2021}, an issue that is addressed below (see ``Advantage of using \texttt{QGE} vs. $\texttt{QRAND}_1$ across datasets and models").

We compared two different transformations: our evaluation measure, \texttt{QGE}, was compared against $\texttt{QRAND}_{\text{K}}$, a baseline measure that estimates the quality distribution across all explanations by sampling $k$ random explanations. It calculates the difference between $q_e$ and the average quality of those $k$ samples, generalizing the usual comparison with a single random explanation.
\begin{equation}
    \label{eq:qrand}
    \texttt{QRAND}_{\text{K}}
    = q_{e} - \frac{\sum_{i=0}^{K}{q_{r_k}}}{K}
\end{equation} where ${q_{r_k}}$ represents the quality of a random explanation.

To assess adherence to \textbf{R2} for both quality metric transformations $qt$ (either \texttt{QGE} or $\texttt{QRAND}_{\text{K}}$ for a range of $k$ values), we used the following evaluation metrics (both implemented in SciPy~\cite{2020SciPy-NMeth}):
\begin{itemize}
    \item \textbf{Kendall rank correlation ($\tau$):} As a direct translation of \textbf{R2}, we computed Kendall's rank correlation $\tau$, i.e. the level of agreement of the order after the transformation with the order after the transformation (i.e. $q_i<q_j \Longrightarrow qt_i<qt_j$ and $q_i>q_j \Longrightarrow qt_i>qt_j$).
    \item \textbf{Spearman correlation ($\rho$)} between the transformed quality metric $qt$ (either \texttt{QGE} or $\texttt{QRAND}_{\text{K}}$) and the original $q$.
\end{itemize}

Four scenarios are reported in Section \ref{sec:exhaustive_exploration}: (a) the examination of the complete set of all possible explanations for two small datasets; (b) an exploration in larger datasets; (c) an analysis of the effect of the model used; and (d) the use of measures other than \texttt{Pixel-Flipping}.

\subsubsection{a. Exhaustive exploration of the explanation space}\label{sec:exhaustive_exploration}
We first tested our method using two small datasets in which the complete list of all possible explanations can be enumerated in a reasonable time. It's crucial to note that differences in $q_e$ and $q_{e'}$ arise solely when $\mathbf{e}$ and $\mathbf{e'}$ differ in their feature ranking orders. This constraint significantly simplifies the space of possible explanations and facilitates the enumeration process.

The datasets selected for this experiment were the \texttt{Avila} dataset~\cite{avila_dataset} and the \texttt{Glass Identification} dataset~\cite{misc_glass_identification_42}, both small tabular datasets. Given the limited number of variables in these datasets, the complete set of distinct explanations (with respect to $q$; i.e. every attribution vector that yields a different order when argsorted) corresponds to the set of all possible permutations of the variables. This set can be exhaustively explored. The examples in \texttt{Avila} consist of 10 variables, which yields a total of $10!=3,628,800$ different explanations while \texttt{Glass} has 9 variables, amounting to $9!=362,880$ explanations.

Our analysis confirms that the distribution of \texttt{QGE} is centered on zero (see Appendix~\ref{sec:distribution-qge}). Consequently, if \textbf{R2} is met, (i.e. high-quality explanations obtain higher \texttt{QGE} values), then \textbf{R1} is also met since the user can clearly distinguish between explanations with an above-average quality (which have positive \texttt{QGE}) and explanations with below-average quality (which have negative \texttt{QGE}). The following experiments aim to assess the degree to which \textbf{R2} is met.

For each dataset, we trained a Multilayer Perceptron (\texttt{MLP}). The models achieved test accuracies of 0.99 on the \texttt{Avila} dataset and 0.77 on the \texttt{Glass} dataset, respectively. For each explanation $\mathbf{e}$ in the set of all possible explanations, we calculated $q_e$ (Eq.\ref{eq:q_value}), \texttt{QGE} (Eq. \ref{eq:e_inv}), and $\texttt{QRAND}_{\text{K}}$ (Eq. \ref{eq:qrand}) for values of $k$ ranging from 1 to 10.

% \begin{table}[h]
%     \centering
%     \begin{tabular}{l|r|r|r|r|c}
%         \hline
%         Dataset & Classes & N & D & \# of explanations & Acc.\\
%         \hline
%         \texttt{Avila} & 12 & 20,867 & 10 & $10! = 3,628,800$ & 0.99\\
%         \texttt{Glass} & 6 & 214 & 9 & $9!=362,880$ & 0.77\\
%         \hline
%     \end{tabular}
%     \caption{Description of the datasets used. \textit{N} refers to the number of instances and \textit{D} is the number of variables of each instance. \textit{Acc.} indicates the test accuracy of the model to be explained.}
%     \label{tab:small_dataset_summary}
% \end{table}

\paragraph{Order preservation: Kendall's $\tau$}

To measure the extent to which a transformation $qt$ (either $\texttt{QRAND}_{\text{K}}$ or \texttt{QGE}) preserves the order of the original quality measure $q$, we compute Kendall's $\tau_{q,qt}$. 

In Fig.~\ref{fig:exhaustive_correct_pairs} we report the results for 5 different $\mathbf{x}$ inputs, for each of which all possible explanations were computed. These show that for \texttt{Avila}, on average, using \texttt{QGE} as a transformation maintains the correct ordering for 85\% of pairs, which results in a $\tau_{q,\texttt{QGE}}$ of 0.7 ($\pm$ 0.12). To obtain a comparable result with the conventional $\texttt{QRAND}_{\text{K}}$, more than $k=6$ random samples are needed. Similarly, for the \texttt{Glass} dataset, \texttt{QGE} obtains a $\tau_{q,\texttt{QGE}}$ value of 0.74 ($\pm$0.12), equivalent to using more than 6 random samples. Additional results (reported in the Appendix) confirm that a similar advantage is also found when measuring Spearman's rank correlation ($\rho$) instead of Kendall's $\tau$.

% The plots for figures 4 through 8 are generated with https://github.com/annahedstroem/eval-project/blob/main/nbs/3-plot.ipynb after having generated the corresponding files with https://github.com/annahedstroem/eval-project/blob/main/src/extract_measures.py and processed them with https://github.com/annahedstroem/eval-project/blob/main/src/2-measure-performance.py
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\linewidth]{images/avila_mlp_full_tau.png}
    \includegraphics[width=0.45\linewidth]{images/glass_mlp_full_tau.png}
    \caption{Kendall's $\tau$ for the \texttt{Avila} and \texttt{Glass} datasets. The blue line indicates the average $\tau_{q,\texttt{QRAND}_{\text{K}}}$ for each value of $K$ over 5 different inputs, with the shaded area showing the average 
 $\pm\sigma$. The orange line records the average $\tau_{q,\texttt{QGE}}$, with dashed lines representing the average $\pm\sigma$.)}
    \label{fig:exhaustive_correct_pairs}
\end{figure}

% \paragraph{Ability to detect exceptionally good rankings}

% The results above show that there are some ordering errors in $qt$. However, it is important to determine if errors are equally distributed with respect to $q$ (i.e. do these errors affect the ability to identify exceptionally good rankings?).

% To evaluate this, we first computed the z-score $z = \frac{q_\mathbf{e} - \mu_{\mathbf{q}}}{\sigma_\mathbf{q}}$ for all possible explanations, where $\mu_{\mathbf{q}}$ and $\sigma_\mathbf{q}$ represent the mean and standard deviation of the explanation quality distribution, respectively. We then filtered out explanations that exhibited a z-score above a set threshold to isolate those considered to be exceptionally good.

% We assessed the capability of $qt$ to distinguish these superior explanations from others using the area under the precision-recall curve (AUC) for various z-score thresholds. This measure helps determine how effectively $qt$ can identify top-quality rankings.

% The results, illustrated in Fig.~\ref{fig:exhaustive_auc_exceptional}, show that as the z-score of the explanations increases, the performance of \texttt{QGE} significantly surpasses that of 
% $\texttt{QRAND}_{\text{K}}$. For example, when explanations have a z-score of 2 or higher, 
% \texttt{QGE} correctly identifies 99\% of them, which is more effective than $\texttt{QRAND}_{\text{K}}$ with 5 samples. This ability is particularly valuable as practitioners often prioritize exceptionally high-quality explanations over those of average quality, highlighting the importance of robust ranking capabilities in practical settings.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{images/avila_auc_exceptional.png}
%     \includegraphics[width=\columnwidth]{images/glass_auc_exceptional.png}
%     \caption{AUC for the detection of rankings at varying levels of exceptionality. The blue line represents the average AUC for $qt=\texttt{QRAND}_{\text{K}}$ at each $K$ value over 10 different inputs, with the shaded area showing the average $\pm\sigma$. The orange line displays the average AUC when $qt=\texttt{QGE}$, with dashed lines indicating the average $\pm\sigma$}.
% \label{fig:exhaustive_auc_exceptional}
% \end{figure}

\paragraph{Ability to rank exceptionally high-quality explanations}

%To assess the efficacy of $qt$ in preserving the order of exceptionally high-quality explanations,
We analyzed Kendall's $\tau$ between $q$ and $qt$ for different subsets of explanations, stratified by their quality levels. The results, depicted in Fig.~\ref{fig:exhaustive_tau_exceptional}, illustrate that the capability of $qt$ to accurately rank explanations improves with the increasing quality of the explanations. Notably, this improvement is significantly more pronounced for $qt=\texttt{QGE}$ compared to $qt=\texttt{QRAND}_{\text{K}}$, indicating a superior performance in distinguishing high-quality explanations, which are usually the focus of practitioners.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{images/avila_mlp_full_tau_exceptional.png}
    \includegraphics[width=\columnwidth]{images/glass_mlp_full_tau_exceptional.png}
    \caption{Kendall's $\tau$ for explanations of a given level of exceptionality. The blue line represents the average correlation $\tau_{q,\texttt{QRAND}_{\text{K}}}$ for each $K$ value across 10 different inputs, with the shaded area indicating the average$\pm\sigma$. The orange line shows the average correlation $\tau_{q,\texttt{QGE}}$,  with dashed lines marking the average $\pm\sigma$.}
    \label{fig:exhaustive_tau_exceptional}
\end{figure}

These experiments show that \texttt{QGE} meets requirement \textbf{R2} to a higher extent than the existing alternative, $\texttt{QRAND}_{\text{K}}$ except for large values of $K$. Moreover, the computational cost of $\texttt{QRAND}_{\text{K}}$ is proportional to $K$, so it needs to far exceed the computational cost of \texttt{QGE}, which is on par with the computational cost of $\texttt{QRAND}_1$ (two calculations of $\Psi$). This demonstrates that \texttt{QGE} also complies with \textbf{R3} to a much higher extent than $\texttt{QRAND}_{\text{K}}$ to obtain comparable performance.

\subsubsection{b. Performance on larger datasets}\label{sec:large_dataset_exploration}
To confirm the applicability of our findings across different types of data and larger datasets, we expanded our experiments to include both image datasets (\texttt{MNIST}, \texttt{CIFAR}, \texttt{ImageNet}) and a textual dataset (\texttt{20newsgroups}).% Details of these datasets are summarized in Table \ref{tab:large_dataset_summary}.

% \begin{table}[h]
%     \centering
%     \begin{tabular}{|l|r|r|r|}
%         \hline
%         Dataset & N & D & Explanations\\
%         \hline
%         MNIST & - & - & -\\
%         CIFAR & - & - & -\\
%         Imagenet & - & - & -\\
%         20newsgroups & - & - & -\\
%         \hline
%     \end{tabular}
%     \caption{Description of the datasets used. \textit{N} refers to the number of instances and \textit{D} is the number of variables of each instance.}
%     \label{tab:large_dataset_summary}
% \end{table}

For \texttt{20newsgroups}, we trained an \texttt{MLP} model that achieved an accuracy of 0.78. For \texttt{MNIST}, we utilized a small convolutional network with 0.93 accuracy, and for \texttt{CIFAR}, a \texttt{ResNet50} network obtained an accuracy of 0.77. With \texttt{ImageNet}, we tested five different pre-trained models from the TorchVision library: three convolutional architectures (\texttt{ResNet18}, \texttt{ResNet50}, and \texttt{VGG16}) and two attention-based architectures (\texttt{ViT\_b\_32} and \texttt{MaxViT\_t}).

Due to the impracticality of processing all possible explanations for these larger datasets, we sampled 10,000 random explanations for each tested input\footnote{Although random explanations frequently have lower quality than explanations obtained with existing explanation methods, the latter are costly to obtain, and too few to yield statistically significant results. Moreover, Section~\ref{sec:exhaustive_exploration} shows that the effect observed for average-quality explanations is maintained or enhanced for high-quality explanations, indicating that exploring a sizable set of random explanations is more informative for this experiment than exploring a handful of very good explanations.}. For each dataset-model pair, five different inputs were tested, and we report the average increase in Kendall's $\tau$ when using \texttt{QGE} as opposed to $\texttt{QRAND}_1$ ($\Delta \tau=\tau_{q,\texttt{QGE}}-\tau_{q,\texttt{QRAND}_1}$) and the average 
 increase in Spearman's $\rho$ ($\Delta\rho=\rho_{q,\texttt{QGE}}-\rho_{q,\texttt{QRAND}_1}$).

% Additionally, to test on exceptionally good rankings, which are unlikely to be obtained at random, we used a few well-known explanation methods: (for explanation methods we used the implementation in the Captum library).

The results\footnote{Importantly, a well-known issue when explaining predictions involving images is that pixel-level perturbations interact suboptimally with convolutional networks. A prevalent solution involves using explanations at the superpixel level~\cite{blücher2024decoupling}). Therefore, we used superpixel level explanations for \texttt{CIFAR} and \texttt{ImageNet}. For completeness, the results for explanations at the pixel level are reported in the Appendix.}, detailed in Table \ref{tab:effects_per_model_and_dataset_chunky}, consistently show a substantial positive impact on both Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$ across a variety of datasets and model architectures.

\begin{table}[!t]%CHUNKY
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|l|c|c}
        %\hline
        Dataset & Model & $\Delta \tau$ & $\Delta\rho$\\
        \toprule
        \texttt{20newsgr.} & \texttt{mlp} & 0.119$\pm$0.03 & 0.124$\pm$0.06\\
        \texttt{mnist} & \texttt{mlp} & 0.152$\pm$0.06 & 0.150$\pm$0.05\\
        \texttt{cifar} & \texttt{resnet50} & 0.060$\pm$0.01 & 0.066$\pm$0.01\\
        \texttt{imagenet} & \texttt{resnet18} & 0.142$\pm$0.07 & 0.138$\pm$0.06\\
        \texttt{imagenet} & \texttt{resnet50} & 0.174$\pm$0.04 & 0.170$\pm$0.03\\
        \texttt{imagenet} & \texttt{vgg16} & 0.102$\pm$0.04 & 0.106$\pm$0.03\\
        \texttt{imagenet} & \texttt{vit\_b\_32} & 0.165$\pm$0.02 & 0.163$\pm$0.02\\
        \texttt{imagenet} & \texttt{maxvit\_t} & 0.118$\pm$0.05 & 0.120$\pm$0.05\\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \caption{Magnitude of the increase in Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$ on large datasets.}\label{tab:effects_per_model_and_dataset_chunky}
\end{table}

\subsubsection{c. Advantage of using \texttt{QGE} vs. $\texttt{QRAND}_1$ across datasets and models}
\label{sec:experiments_datasets_models}
As discussed in Section \ref{sec:suitability_qge}, using the \texttt{Pixel-Flipping} average activation as a quality measure necessitates a perturbation function that transforms inputs, potentially pushing the model to operate on data points outside of its training distribution~\cite{hase2021}. To mitigate any effects from this interaction, we repeated the experiments from Section \ref{sec:exhaustive_exploration} using models that were exposed to masked inputs during training, as outlined in~\cite{hase2021}. Furthermore, we experimented with masking using the average value for each input attribute, rather than zeros. We also tested models with reduced accuracies to diversify the conditions. Table \ref{tab:effects_per_model} summarizes the average results from these experiments for five different inputs.

% Standard deviations obtained with https://github.com/annahedstroem/eval-project/blob/main/src/3-get_qstds.py
\begin{table}[!t]
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|c|c|c|c}
        %\hline
        \multicolumn{5}{c}{{Avila}} \\
        \hline
        Model & Acc. & $\sigma(q)$ &$\Delta \tau$ & $\Delta\rho$\\
        \toprule
        \texttt{MLP} & 0.99 & 0.191 & 0.198$\pm$0.12 & 0.173$\pm$0.10\\
        \texttt{ood-mean} & 0.80 & 0.080 & 0.288$\pm$0.05 & 0.245$\pm$0.03\\
        \texttt{ood-zeros} & 0.80 & 0.085 & 0.247$\pm$0.09 & 0.213$\pm$0.06\\
        \texttt{undertr.} & 0.75 & 0.105 & 0.315$\pm$0.06 & 0.257$\pm$0.03\\
        \texttt{untrained} & 0.05 & 0.001 & 0.434$\pm$0.01 & 0.298$\pm$0.00\\
        \hline
        \multicolumn{5}{c}{{Glass}}\\
        \hline
        Model & Acc. & $\sigma(q)$ &$\Delta \tau$ & $\Delta\rho$\\
        \toprule
        \texttt{MLP} & 0.77 & 0.198 & 0.241$\pm$0.11 & 0.204$\pm$0.09\\
        \texttt{ood-mean} & 0.63 & 0.070 & 0.314$\pm$0.01 & 0.262$\pm$0.01\\
        \texttt{ood-zeros} & 0.63 & 0.052 & 0.267$\pm$0.06 & 0.230$\pm$0.04\\
        \texttt{undertr}. & 0.60 & 0.168 & 0.191$\pm$0.02 & 0.184$\pm$0.02\\
        \texttt{untrained} & 0.23 & 0.007 & 0.173$\pm$0.10 & 0.163$\pm$0.08\\
        \hline
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \caption{Magnitude of the increase in Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$ for the \texttt{Avila} and \texttt{Glass} datasets and different models all using an \texttt{MLP} architecture: \texttt{MLP} refers to the fully trained model exposed to no masked input; \texttt{ood-mean} and \texttt{ood-zeros} were exposed during training to inputs masked with zeros and the average value of each attribute, respectively; \texttt{undertr.} was trained only until achieving 70\% the accuracy of the fully trained model; and \texttt{untrained} was not exposed to any data. $\sigma(q)$ indicates the standard deviation of the distribution of $q$ across all possible explanations.}\label{tab:effects_per_model}
\end{table}

These results show that using \texttt{QGE} consistently yields better outcomes than $\texttt{QRAND}_1$, irrespective of the dataset or model type. However, the nature of the model significantly influences the extent of the advantage offered by \texttt{QGE}. A deeper analysis of this effect is included in Appendix~\ref{sec:exploring-std}.

\subsubsection{d. Suitability for other quality metrics}\label{sec:suitability_for_metrics}
All experiments reported above use \texttt{Pixel-Flipping} \cite{bach2015pixel} as a quality metric. This metric is a popular choice, which is why we have explored it extensively. However, to determine whether the observed effects are consistent across different quality measures, we tested our method using a variety of measures spanning different quality dimensions. For all measures, we used the implementations in the Quantus~\cite{hedstrom2022quantus} library.

\paragraph{Faithfulness metrics}
%The quality measure used in the previous experiments measures the faithfulness of the attributions, i.e. how well attributions represent the contribution of individual features to the model's prediction. We additionally tested
In addition to \texttt{Pixel-Flipping}, we tested \texttt{Faithfulness Correlation} \cite{bhatt2020}, \texttt{Faithfulness Estimate} \cite{AlvarezCorr18}, and \texttt{Monotonicity Correlation} \cite{arya2019explanation} for 1,000 random explanations. The results, summarized in Table \ref{tab:effects_faithfulness}, show that \texttt{QGE} performs superiorly to $\texttt{QRAND}_1$ for all measures, obtaining substantial increases for three of the four measures. However, it offers little advantage for \texttt{Faithfulness Correlation}. This metric is known to be unstable, often yielding highly variable results for the same input across different executions \cite{tomsett2020, hedstrom2023metaquantus, hedstrom2022quantus}. This variability undermines the informative advantage of $\Psi(\mathbf{e^{inv}}, \mathbf{x}, f, \hat{y})$ (Eq. \ref{eq:qge}) over $\Psi(\mathbf{e^{r}}, \mathbf{x}, f, \hat{y})$ using a random explanation $\mathbf{e^{r}}$, explaining the lack of advantage observed.

\begin{table}[!t]
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|c|c}
        %\hline
        \multicolumn{3}{c}{\texttt{Pixel-Flipping}}\\
        \hline
        Model & $\Delta \tau$ & $\Delta \rho$\\
        \toprule
        \texttt{ResNet18} & 0.142$\pm$0.07 & 0.138$\pm$0.06\\
        \texttt{VGG16} & 0.102$\pm$0.04 & 0.106$\pm$0.03\\
        \hline
        \multicolumn{3}{c}{\texttt{FaithfulnessCorrelation}}\\
        \hline
        Model & $\Delta PA$ & $\Delta \rho$\\
        \toprule
        \texttt{ResNet18} & 0.007$\pm$0.01 & 0.007$\pm$0.02\\
        \texttt{VGG16} & 0.009$\pm$0.01 & 0.012$\pm$0.01\\
        \hline
        \multicolumn{3}{c}{\texttt{FaithfulnessEstimate}}\\
        \hline
        Model & $\Delta PA$ & $\Delta \rho$\\
        \toprule
        \texttt{ResNet18} & 0.368$\pm$0.02 & 0.279$\pm$0.02\\
        \texttt{VGG16} & 0.397$\pm$0.02 & 0.290$\pm$0.02\\
        \hline
        \multicolumn{3}{c}{\texttt{MonotonicityCorrelation}}\\
        \hline
        Model & $\Delta PA$ & $\Delta \rho$\\
        \toprule
        \texttt{ResNet18} & 0.353$\pm$0.05 & 0.277$\pm$0.02\\
        \texttt{VGG1} & 0.374$\pm$0.03 & 0.288$\pm$0.01\\
        \hline
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \caption{Magnitude of the increase in Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$ for faithfulness metrics on predictions of a \texttt{ResNet18} and \texttt{VGG16} models on the \texttt{Imagenet} dataset.}
    \label{tab:effects_faithfulness}
\end{table}

\paragraph{Localization metrics}
We evaluated the effectiveness of \texttt{QGE} using various localization measures, including \texttt{AttributionLocalisation} \cite{kohlbrenner2020towards}, \texttt{TopKIntersection} \cite{theiner2021}, \texttt{RelevanceRankAccuracy}, \texttt{RelevanceMassAccuracy} \cite{arras2021ground}, and \texttt{AUC}~\cite{Fawcett}. For these tests, we utilized the \texttt{CMNIST} dataset \cite{bykov2021noisegrad}, training a \texttt{ResNet18} model to perform the evaluations. We then assessed the localization of 10,000 random attributions using these measures, applying both the $\texttt{QRAND}_1$ and \texttt{QGE} transformations. The results are summarized in Table \ref{tab:effects_localization}, which reports increases in Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$. \texttt{QGE} consistently outperforms $\texttt{QRAND}_1$ across all tested localization metrics, enhancing both the Kendall and the Spearman correlation of the transformed metrics with the original ones. The magnitude of the advantage that \texttt{QGE} provides over $\texttt{QRAND}_1$ varies depending on the nature of the quality metric used, with the most significant improvements observed using \texttt{AttributionLocalisation}, \texttt{RelevanceMassAccuracy} and \texttt{AUC}.

\begin{table}[!t]
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|c|c}
        %\hline
        \multicolumn{3}{c}{\texttt{CMNIST} - \texttt{ResNet18}}\\
        \toprule
        Measure &$\Delta \tau$ & $\Delta\rho$\\
        \hline
        \texttt{Attr.Loc.} & 0.500$\pm$0.00 & 0.309$\pm$0.00\\
        \texttt{TopKInt.} & 0.015$\pm$0.01 & 0.022$\pm$0.01\\
        \texttt{Rel.RankAcc.} & 0.089$\pm$0.02 & 0.093$\pm$0.02\\
        \texttt{Rel.MassAcc.} & 0.501$\pm$0.01 & 0.310$\pm$0.01\\
        \texttt{AUC} & 0.496$\pm$0.00 & 0.304$\pm$0.00\\
        \hline
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \caption{Magnitude of the increase in Kendall and Spearman correlation when using \texttt{QGE} instead of $\texttt{QRAND}_1$ for different localization metrics %(\texttt{AttributionLocalization}, \texttt{TopKIntersection}, \texttt{RelativeRankAccuracy}, \texttt{RelativeRankMass}),  
    on predictions of a \texttt{ResNet18} model on the \texttt{CMNIST} dataset.}
    \label{tab:effects_localization}
\end{table}

\paragraph{Robustness and randomization metrics}
We considered incorporating robustness and randomization metrics into our evaluations. These metrics typically assess the explanation method itself rather than the explanations obtained. While they can be quantified using the \texttt{QGE} transformation, they are not amenable to comparison using $\texttt{QRAND}_1$, as the latter does not rely on the explanation method. Therefore, no direct comparison between \texttt{QGE} and $\texttt{QRAND}_1$ was feasible for these metrics.

\subsection{Effect on existing quality metrics}\label{sec:effect_on_metrics}
%Check how this technique affects the robustness and adversary reactivity of existing metrics.

In this experiment, we investigate the statistical reliability of our proposed \texttt{QGE} transformation compared to the original quality metric. 
We follow the meta-evaluation methodology outlined in \cite{hedstrom2023metaquantus} where metric reliability is assessed in two main steps: performing a minor- (noise resilience, $NR$) or disruptive- (reactivity to adversaries, $AR$) perturbation and then, measuring how the metric scores and method rankings changed, post-perturbation. 
For each perturbation scenario, intra-consistency ($\text{IAC}$; the similarity of score distributions under perturbation), and inter-consistency ($\text{IEC}$; the ranking similarity among different explanation methods) are computed, resulting in a meta-consistency vector $\mathbf{m} \in \mathbb{R}^{4}$ and a summarising score $\mathbf{MC} \in [0, 1]$:
%$[\text{IAC}_{NR}, \text{IAC}_{AR}, \text{IEC}_{NR}, \text{IEC}_{AR}]$. 
\begin{equation}
    \mathbf{MC} = \left(\frac{1}{|\mathbf{m^{*}}|}\right){\mathbf{m^{*}}}^T\mathbf{m} \quad \text{where} \quad \mathbf{m} = \begin{bmatrix} 
    {IAC}_{NR}\\
    {IAC}_{AR}\\
    {IEC}_{NR}\\ 
    {IEC}_{AR}
    \end{bmatrix},
    \label{eq:meta-eval-vector}
\end{equation}
where $\mathbf{m}^{*} = \mathbb{1}^4$
represents an optimally performing quality estimator. %as defined by the all-one indicator vector. 
A higher $\mathbf{MC}$ score, approaching 1, signifies greater reliability on the tested criteria. Perturbations are applied to either the model parameters or input.
%This framework helps in understanding how metric scores and rankings behave under controlled conditions.
%Details on the meta-evaluation framework used %including a mathematical discussion of the individual scoring criteria,
%can be found in \cite{hedstrom2023metaquantus}. % Our experiments are discussed in Appendix. 

%In summary, meta-evaluation tests the statistical reliability of a metric in two steps: (1) perturbation of a verified strength is performed and (2) then, evaluation outcomes are compared against strength-based expectations. Here, perturbations are applied to $model-$ (parameters) or $input$ spaces, separately, assessing the metrics' resilience to noise ($NR$) with minor perturbation or its reactivity to adversary ($AR$) with disruptive perturbation (e.g., that minor perturbation of input should in similar evaluation score distribution outcomes). The tested criteria include the intra- ($IAC$) and inter-consistency ($IEC$), which measure the similarity in score distributions and ranking of different explanation methods, respectively, post-perturbation. More details on the meta-evaluation framework including a mathematical discussion of the individual scoring criteria, can be found in the original publication. 

We used the \texttt{fMNIST} \cite{fashionmnist2015} and \texttt{ImageNet} \cite{ILSVRC15} datasets. For the first toy dataset we use the LeNet architecture \cite{lecun2010mnist} and for \texttt{ImageNet} we use a pre-trained \texttt{ResNet-18} \cite{he2015deep} from \cite{Pytorch2019}. The results shown in Table~\ref{tab:effects_existing_measures} demonstrate that the \texttt{QGE} method yields reliability improvements across tested metrics. This performance enhancement is most notable for \texttt{Pixel-Flipping}, where \texttt{QGE} significantly enhances the inter-consistency (IEC) under adversarial reactivity (\textit{AR}), indicating a marked improvement in the metric's ability to differentiate between meaningful and random inputs and models, as detailed in Appendix~\ref{sec:effect-on-existing-appendix}. Also, \texttt{QGE}'s effect on localization is competitive, though not statistically significant.

\begin{table}[!t]
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|c|c|c}
        %\hline
        \multicolumn{4}{c}{\texttt{fMNIST} - \texttt{LeNet}}\\
        \toprule
        Measure & PF & RRA & RMA\\
        \hline
        $\texttt{QRAND}_1$ & 0.579 & 0.599 & 0.585\\
        \texttt{QGE} & 0.801 & 0.598 & 0.587\\
        \hline
        \multicolumn{4}{c}{\texttt{ImageNet} - \texttt{ResNet18}}\\
        \toprule
        Measure & PF & RRA & RMA\\
        \hline
        $\texttt{QRAND}_1$ & 0.634 & 0.594 & 0.596\\
        \texttt{QGE} & 0.849 & 0.568 & 0.619\\
        \hline
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \caption{MC score when using \texttt{QGE} and $\texttt{QRAND}_1$ for \texttt{Pixel-Flipping} (PF) (Faithfulness), \texttt{Relevance Rank Accuracy} (RRA) and \texttt{Relevance Mass Accuracy} (RMA) (Localization) on predictions of a \texttt{ResNet18} model on the \texttt{fMNIST} dataset.}
    \label{tab:effects_existing_measures}
\end{table}