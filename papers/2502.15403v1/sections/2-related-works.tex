\section{Related Works}

Evaluating the quality of methods without ground truth explanation labels is a significant challenge for reserachers~\cite{brunke, brocki2022, rong2022, hedstrom2023metaquantus}. Many studies have shown that faithfulness metrics are highly sensitive to their parameterisation during evaluation; altering patch sizes or pixel occlusion tactics can significantly affect the outcomes~\cite{tomsett2020, brunke, brocki2022, rong2022, hedstrom2023metaquantus}. %Other studies have examined the impact of model performance variables such as optimizers and activation functions~\citep{karimi2023on}, as well as explanation method hyperparameters like random seed~\cite{bansal2020} or choice of baseline~\citep{sturmfels2020visualizing, integratedgradients}. 
These findings are concerning; if small changes in parameters cause large variations in evaluation outcomes, it may be hard to trust the results. 
 
For more reliable estimations of explanation quality, individual explanation methods have been evaluated \textit{relative} to a random baseline~\cite{samek2015, nguyen2020, ancona2019}. The concept of using the random explainer as a worst-case reference point has also been used for calculating explanation skill scores~\citep{findingrightbommer} or as part of a paired t-test to compare with existing explanation methods~\citep{irof2020}. Most similar to our work~\cite{blücher2024decoupling} is incorporating information about the pixel-flipping inverse curve. Our contribution is different in both aim and applicability. Their approach aims at enhancing the occlusion process, specifically the masking strategy for pixel-flipping~\cite{samek2015}. We provide a general-purpose evaluative solution applicable across various explanation metrics such as localization, faithfulness, and robustness.

%Recent advances have heightened awareness regarding the many challenges of XAI evaluation \cite{brunke, brocki2022, rong2022, hedstrom2023metaquantus}. Notably, even minor changes to evaluation parameters like patch sizes or pixel occlusion tactics \citep{samek2015, bach2015pixel, arya2019explanation, bhatt2020, rong2022} can drastically alter evaluation outcomes.
%Furthermore, multiple independent works have detected empirical ``confounders" \citep{sundararajan2018, kokhlikyan2021investigating, yona2021, bindershort2022, hedstroem2023sanity} that impact the widely accepted randomization sanity test \citep{adebayo2018}, underscoring the challenge of evaluating explanations without ground truth.

%Several works have addressed the reliability of evaluation metrics by comparing them visually or numerically with a random explanation \cite{samek2015, nguyen2020, ancona2019}; others have integrated randomness estimates into the evaluation itself \cite{yeh2019, irof2020, findingrightbommer}. %Further approaches explore the explanation space \cite{smilkov2017smoothgrad, bykov2021noisegrad}, assess the out-of-distribution (OOD) effects of perturbations \cite{oodxai2021, localbaselines2021}, and introduce more realistic perturbations into evaluations \cite{iclrvaexai2019}. Our contributions differ from \cite{blücher2024decoupling} which leverages the inverse pixel-flipping curve in the evaluation; as their approach concentrates on enhancing the ``occlusion process'', i.e., the masking strategy for pixel-flipping \cite{samek2015}. We aim for a universal evaluative solution that improves the reliability of quality estimators across various dimensions of explanation quality such as localization, faithfulness, and robustness.
