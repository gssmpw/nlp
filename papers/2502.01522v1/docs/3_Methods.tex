\section{Method}

\subsection{Preliminaries}

SD is a type of latent diffusion model~\cite{rombach2022high} that performs a sequence of gradual denoising operations within the latent space and subsequently remaps the denoised latent code into the pixel space to generate the final output image. During the training process, SD initially encodes an input image $x$ into a latent code $z$ using a Variational Auto-Encoder (VAE)~\cite{kingma2013auto}. In the subsequent stages, the noisy latent code $z_t$ at timestep $t$ serves as the input for the denoising U-Net $\epsilon_\theta$, which interacts with text condition $c$ via cross-attention. The training objective for this process is defined as follows:
\begin{equation}
\label{eq:diffusion loss}
\mathcal{L} = \mathbb{E}_{z,c,\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta (z_t, t, c) \|_2^2 \right],
\end{equation}
where $\epsilon$ represents random noise sampled from a standard Gaussian distribution.



\subsection{Problem Formulation}

Following~\cite{pham2024blur2blur}, we formulate a blurry image $y$ as a function of the corresponding sharp image $x$ with underlying structural features $f_{s}$ through a blur operator $\mathcal{F}_C(\cdot, k)$, which is associated with a device-dependent blur domain $C$ and a blur kernel $k$:
\begin{equation}
    y = \mathcal{F}_C(x, f_{s}, k) + \eta,
\end{equation}
where $\eta$ is a noise term. Our objective is to develop a function $\mathcal{G}_C$ that can extract structural representations from a blurry image $y \in C$. The extracted features will serve as conditions for a pre-trained generative diffusion model $SD$ with an adapter $\mathcal{A}$ to recover the sharp image, i.e.,
\begin{equation}
\begin{aligned}
    x &= SD\left[y, \mathcal{A}(f_{s}), z\right], \\
    f_{s} &= \mathcal{G}_C(y),
\end{aligned}
\end{equation}
where $z$ is random Gaussian noise.


\subsection{BD-Diff} 

Q-Former often acts as a representation extractor in conditional image generation~\cite{li2023blip, li2024blip}. Inspired by this, BD-Diff employs two Q-Formers to separately extract structural features and blur patterns from blurry images as illustrated in Figure~\ref{fig:train}. These extracted features serve as generation conditions for three specifically designed training tasks, thereby achieving decoupling: 1) The deblurring task (Section~\ref{sec:task1}) equips a Q-Former $Q_s$ with the ability to extract structural representation from synthetic data pairs. These features are then sent to an adapter $\mathcal{A}$ to provide restoration condition for SD; 2) The blur-transfer task (Section~\ref{sec:task2}) enables another Q-Former $Q_b$ to learn blur patterns from unpaired blurry images within a specific domain; 3) The reconstruction task (Section~\ref{sec:task3}) activates both $Q_s$ and $Q_b$ to ensure that the extracted structural and blur features are complementary, thus enabling $Q_s$ to effectively handle images from specific blur domains that lack paired data. For inference (Section~\ref{sec:inference}), the $Q_s$ and $\mathcal{A}$ are retained to restore sharp images, as illustrated in Figure~\ref{fig:infer}.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\textwidth]{images/train.pdf}
	\caption{Training pipeline of BD-Diff. BD-Diff achieves blur pattern-structure representation decoupling by joint training on three specific tasks: 1) The deblurring task, denoted as $T_1$, enables the structure extractor to learn structural information from synthetic data; 2) The blur-transfer task, denoted as $T_2$, facilitates the blur extractor in learning blur patterns from unpaired blurry images; 3) The reconstruction task, denoted as $T_3$, ensures that the structural features and blur patterns are complementary.}
	\label{fig:train}
\end{figure}

\subsection{Deblurring}
\label{sec:task1}

In SD, additional generation conditions related to image structure, such as sketches and depth maps~\cite{cheng2024theatergen,peng2024controlnext}, are often incorporated by adding additional residuals through an adapter~\cite{zhang2023adding,mou2024adapter}. However, the entanglement of the blur pattern and structural information in the degraded image $y$ makes directly using it as the control signal for $\mathcal{A}$ problematic, as it often leads to instability and produces artifacts~\cite{lin2025diffbir}. To mitigate these issues, we employ a Q-Former $Q_s$ as a feature extractor to capture the underlying structural features from $y$. 

Denoted as $T_1$ in Figure~\ref{fig:train}, we feed the degraded image $x$ into the CLIP image encoder~\cite{radford2021learning}, whose output interacts with the learnable query tokens of $Q_s$ through cross-attention. In this process, we set the word ``structure'' as the input text for $Q_s$ in anticipation that it can extract features $f_s$ that capture the low-level structure of the image. We follow previous works~\cite{liu2024diff,lin2025diffbir} to employ an adapter $\mathcal{A}$ for BD-Diff. As illustrated in Figure~\ref{fig:adapter}, $f_s$ which contains the structural features of $x$, interacts with the down-sample blocks of $\mathcal{A}$ via cross-attention and is incorporated into the SD U-Net as additional residuals to serve as a structure condition.

The deblurring task is trained non-reconstructive, thus requiring paired blurry-sharp image data. We utilize the method from~\cite{hendrycks2019benchmarking} to synthesize blur to sharp images, thereby creating the necessary paired data for training. The loss function of the deblurring task can be depicted as:
\begin{equation}
\label{eq:loss1}
\mathcal{L}_1 = 
\mathbb{E}_{z,c_s,f_s,\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta (z_t, c_s, f_s, t) \|_2^2 \right], 
\end{equation}
where we set the text condition $c_s$ as ``sharp and clean image'' for the original text cross-attention of SD.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{images/adapter.pdf}
	\caption{Schematic illustration of the adapter of BD-Diff.}
	\label{fig:adapter}
\end{figure}

\subsection{Blur-tansfer}
\label{sec:task2}

When the deblurring task is trained solely on synthetic data, it becomes challenging for $Q_s$ to handle images with unseen blur domains $C$. To address this issue, we employ the blur-transfer task that leverages unpaired blurred images from $C$ to help the model better extract structural features. 

In SD, additional generation conditions related to style are typically considered through additional cross attention mechanisms~\cite{ye2023ip,chen2024artadapter}. Inspired by this, we use blur patterns as a special style form. Denoted as $T_2$ in Figure~\ref{fig:train}, we employ another Q-Former $Q_b$ to extract $f_b$ that captures the blur pattern of the image $y_1 \in C$. We set the word ``blur'' as the input text of $Q_b$ to extract representations related to the blur pattern. These representations will be fed into SD through additional cross-attention operations. Specifically, given the query features $Z$ and the text features $c$, the output of cross-attention $Z'$ from BD-Diff can be defined by the following equation:
\begin{equation}
Z' = \text{Attention}(Q, K, V) + \text{Attention}(Q, K', V'),
\end{equation}
where $Q = ZW_q$, $K = cW_k$, and $V = cW_v$ are the query, key, and value matrices of the original text attention operation, respectively. Here, $W_q$, $W_k$, and $W_v$ are the frozen projection matrices. Additionally, $K' = c_{b}W'_k$ and $V' = c_{b}W'_v$ are the newly added key and value matrices for the blur-pattern attention operation, respectively, with $W'_k$ and $W'_v$ being the new trainable projection matrices.

The optimization goal of the blur transfer task is to utilize $f_b$ as a blur condition to generate another image $y_2 \in C$, with a different structure but in the same blur domain. The loss function can be depicted as follows:
\begin{equation}
\label{eq:loss2}
\mathcal{L}_2 = 
\mathbb{E}_{z,c_b,f_b,\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta (z_t, c_b, f_b, t) \|_2^2 \right], 
\end{equation}
where the text condition $c_b$ is set as ``blurry image'', $f_b$ is the extracted blur features of $y_1$, and $z_t$ is obtained by adding noise to the latent code of $y_2$. In this way, we anticipate that $Q_b$ can learn to extract unknown blur patterns in an unsupervised paradigm.

\subsection{Reconstruction}
\label{sec:task3}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\textwidth]{images/infer.pdf}
	\caption{Inference process of BD-Diff. We preserve the $Q_s$ and $\mathcal{A}$ to generate structure guidance, while the newly added cross-attention layer for blur-transfer is omitted.}
	\label{fig:infer}
\end{figure}

In the aforementioned two tasks, we use text prompts and a decoupled conditioning mechanism, which allows structure and blur patterns to independently serve as conditions for the diffusion model, to guide the two Q-Formers to focus on their specific tasks. However, there may still be optimization biases. Specifically, $Q_s$ might learn the inverse function of synthetic blur, causing it to still perform poorly in unknown domains. To circumvent this, we employ the reconstruction task, which is illustrated as $T_3$ in Figure~\ref{fig:train}. In this task, $Q_s$ and $Q_b$ are used to extract structure and blur patterns from the same image $y \in C$. These features are simultaneously considered as generation conditions to reconstruct $y$, with the loss function depicted as:
\begin{equation}
\label{eq:loss3}
\mathcal{L}_3 = 
\mathbb{E}_{z,c_b,f_b,f_c,\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta (z_t, c_b, f_b,f_c, t) \|_2^2 \right].
\end{equation}
This process enables the structure and blur patterns to complement each other, enhancing the generalization capability of $Q_s$ when facing realistic blur patterns.

\subsection{Training and Inference}
\label{sec:inference}

For fast convergence, we first train the deblurring and reblur tasks separately, then incorporate the reconstruction task for joint training. The loss function of the joint training process can be depicted as follows:
\begin{equation}
\label{eq:our loss}
\mathcal{L} = \alpha  \cdot \mathcal{L}_1 + \beta \cdot \mathcal{L}_2 + \gamma \cdot \mathcal{L}_3.
\end{equation}
Here, $\mathcal{L}_1$, $\mathcal{L}_2$, and $\mathcal{L}_3$ represent the loss functions for the deblurring task, the blur-transfer task, and the reconstruction task, respectively; $\alpha + \beta + \gamma = 1$ stands for the sample weights for these three tasks, where we set them to 1:1:1. During the training process, only the Q-Formers, the adapter and the newly added projection metrics are optimized. 

The inference process of BD-Diff is illustrated in Figure~\ref{fig:infer}. We retain $Q_s$ and $\mathcal{A}$ to extract structural features, while the newly added cross-attention mechanism for the blur-transfer task is omitted. We note that SD tends to excessively embellish details, resulting in deviations from the original image. To alleviate this phenomenon, we replace the original SD VAE with a refined-VAE\footnote{See Appendix~\ref{appendix: vaerefiner} for more details.}, based on previous works~\cite{chen2025unirestore,zhang2024diff}.


