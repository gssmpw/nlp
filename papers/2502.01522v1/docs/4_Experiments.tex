\section{Experiments}
\label{sec:exp}
\subsection{Datasets and Metrics}

We evaluate our BD-Diff on four widely-used real-world datasets: GoPro~\cite{nah2017deep}, RealBlur-J\&R~\cite{rim2020real}, and REDS~\cite{nah2019ntire}. To emulate real-world deblurring scenarios without paired data, we adhere to existing works~\cite{pham2024blur2blur} to split the training set into distinct blurry subset $\mathcal{B}$ and sharp subset $\mathcal{S}$. We employ the synthetic blur technique proposed by~\cite{rim2020real} to generate synthetic blur data $\mathcal{B'}$ from $\mathcal{S}$, thereby providing blurry-sharp image pairs for supervised training. The statistics of source image sets are reported in Table~\ref{tab:Data description}. To effectively evaluate image quality, we include several no-reference image quality assessment (NR-IQA) metrics: MANIQA~\cite{yang2022maniqa}, LIQE~\cite{zhang2023blind}, MUSIQ~\cite{ke2021musiq}, and CLIP-IQA~\cite{wang2023exploring}, which align with prior works~\cite{lin2025diffbir,zhang2024diff} for fair comparison\footnote{See Appendix~\ref{appendix: additional metrics} for additional results.}.

\subsection{Implementation Details}
\label{sec:details}


We leverage the stable diffusion v1.5\footnote{stabilityai/stable-diffusion-v1.5} as the pre-trained generative diffusion model. For the image encoder, we employ the ViT-L/14 model from CLIP~\cite{radford2021learning} and set the number of learnable query tokens in the Q-Former to 16, consistent with Blip-Diffusion~\cite{li2024blip}. The structural and blur pattern extractors are initialized using the pre-trained weights provided by BLIP-Diffusion for fast convergence. The training process of BD-Diff includes two phases. First, we train the deblurring and blur-transfer tasks for 50,000 steps as a warm-up. Then, we conduct joint training across all tasks with a 1:1:1 sampling ratio for an additional 500,000 steps. All experiments are implemented using PyTorch and executed on NVIDIA A100 GPUs. We utilize the AdamW~\cite{loshchilov2017decoupled} optimizer with a uniform learning rate of $5 \times 10^{-5}$ across all training tasks. For the inference phase, we adapt the DDIM sampler~\cite{Song_Meng_Ermon_2020} with 30 steps to generate outputs. The guidance scale for classifier-free guidance~\cite{ho2022classifier} is set to 7.5.

\begin{table}[!t]
\caption{Statistics of datasets used as unknown domains.}
\label{tab:Data description}
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Dataset}} & \multicolumn{4}{c}{Number of data samples}    \\
\multicolumn{1}{c}{}                         & Sharp ($\mathcal{S})$  & Blur ($\mathcal{B})$ & Synthetic blur ($\mathcal{B'})$ & Test \\
\midrule
GoPro                                        & 1261  & 842   & 4210           & 1111 \\
REDS                                         & 14400 & 9600  & 48000          & 3000 \\
RealBlur-J                                   & 2064  & 1375  & 6875           & 1474 \\
RealBlur-R                                   & 2064  & 1375  & 6875           & 1474 \\
\bottomrule
\end{tabular}}
\end{table}


\begin{figure*}[!t]
  \centering
  \setlength{\tabcolsep}{0.5pt} 
  \begin{tabular}{ccccccccccc}
    &  \small Ground Truth & \small AirNet & \small PromptIR & \small Blur2Blur & \small DA-CLIP & \small DiffBir & \small Diff-Plugin & \small \textbf{Ours}  & \small Blur \\
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{\small GoPro}} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/8.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/1.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/2.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/3.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/4.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/5.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/6.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/7.png} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v1/0.png} \\
   \multicolumn{10}{c}{\vspace{-14.5pt}} \\
    \raisebox{1.55\height}{\rotatebox[origin=c]{90}{\small REDS}} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/8.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/1.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/2.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/3.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/4.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/5.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/6.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/7.png} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v2/0.png} \\
   \multicolumn{10}{c}{\vspace{-14.5pt}} \\
    \raisebox{1\height}{\rotatebox[origin=c]{90}{\small RealBlur-J}} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/8.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/1.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/2.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/3.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/4.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/5.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/6.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/7.png} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v3/0.png} \\
   \multicolumn{10}{c}{\vspace{-14.5pt}} \\
    \raisebox{1\height}{\rotatebox[origin=c]{90}{\small RealBlur-R}} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/8.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/1.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/2.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/3.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/4.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/5.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/6.png} & 
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/7.png} &
    \includegraphics[width=.109\textwidth]{images/paper_visualization400/v4/0.png} \\
  \end{tabular}
  \caption{Visual comparison on deblurring results. Key regions are marked with red boxes and magnified.}
  \label{fig:visualization of deblur}
\end{figure*}

\begin{figure*}[!t]
  \centering
  \setlength{\tabcolsep}{0.5pt} 
  \begin{tabular}{ccccccccc}
    &  \small Ground Truth & \small Diff-Plugin & \small DiffBir & \small DA-CLIP & \small Ours w/o $Q_s$ & \small Ours w/o $T_{2}T_{3}$ & \small  Ours w/o $T_{3}$ & \small \textbf{Ours} \\
    \raisebox{1.8\height}{\rotatebox[origin=c]{90}{\small GoPro}} &
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/0.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/1.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/2.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/3.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/4.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/5.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/6.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s1/7.png} \\
   \multicolumn{9}{c}{\vspace{-14.5pt}} \\
    \raisebox{2\height}{\rotatebox[origin=c]{90}{\small REDS}} &
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/0.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/1.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/2.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/3.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/4.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/5.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/6.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s2/7.png} \\
   \multicolumn{9}{c}{\vspace{-14.5pt}} \\
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{\small RealBlur-J}} &
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/0.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/1.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/2.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/3.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/4.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/5.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/6.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s3/7.png} \\
   \multicolumn{9}{c}{\vspace{-14.5pt}} \\
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{\small RealBlur-R}} &
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/0.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/1.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/2.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/3.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/4.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/5.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/6.png} & 
    \includegraphics[width=.1228\textwidth]{images/paper_visualization400/s4/7.png} \\
  \end{tabular}
\caption{Visual comparison on structural reconstruction. Key regions are marked with red boxes and magnified.}
\label{fig:visualization of structural}
\end{figure*}
\subsection{Comparisons with State-of-the-art Methods}
\label{sec:Comparison}


In this section, we compare BD-Diff with several SOTA methods, including the generative-diffusion-based approaches DiffBir~\cite{lin2025diffbir}, Diff-Plugin~\cite{liu2024diff}, and DA-CLIP~\cite{luo2023controlling}, as well as the non-diffusion-based methods PromptIR~\cite{potlapalli2024promptir}, AirNet~\cite{chee2018airnet}, and Blur2Blur (unpaired training)~\cite{pham2024blur2blur}. 
% We sourced performance metrics directly from the original publications or reproduced the results using the officially released codes for a fair comparison.

\input{tables/exp_compare_sota}

\textbf{Quantitative Comparisons.} The performance comparisons on the test sets of our selected datasets are presented in Table~\ref{tab:Model Performance on No-Reference Metrics}. BD-Diff outperforms all baseline models, including the latest unpaired training approach (Blur2Blur) and supervised training generative-diffusion-based method (Diff-Plugin). The results indicate that paired training models exhibit suboptimal performance when faced with unknown domain blur patterns in these real-world datasets. Non-generative-diffusion-based unpaired training method Blur2Blur demonstrates some effectiveness in these scenarios, but it still falls short compared to generative-diffusion-based approaches in most cases. This can be attributed to the capacity of generative diffusion models to generate intricate textures and details through extensive pre-training and aesthetic alignment. Our BD-Diff harnesses the advantages of generative diffusion models and enhances the generalization capability of structure extraction through decoupled training. This results in favorable performance against existing SOTA methods on unpaired deblurring tasks.

\begin{figure}[!t]
  \centering
  \setlength{\tabcolsep}{1pt} 
  \begin{tabular}{cccc}
    &  \small Blur &  \small Sharp &  \small Blur-Transferred  \\
     \raisebox{2.2\height}{\rotatebox[origin=c]{90}{\small GoPro}} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt2/square_0.png} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt2/square_1.png} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt2/square_2.png} \\
    \multicolumn{4}{c}{\vspace{-14pt}} \\
    \raisebox{2.3\height}{\rotatebox[origin=c]{90}{\small REDS}} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt1/square_0.png} & 
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt1/square_1.png} & 
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt1/square_2.png} \\
    \multicolumn{4}{c}{\vspace{-14pt}} \\
    \raisebox{1.6\height}{\rotatebox[origin=c]{90}{\small RealBlur-J}} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/rb/square_0.png} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/rb/square_1.png} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/rb/square_2.png} \\
    \multicolumn{4}{c}{\vspace{-14pt}} \\
    \raisebox{1.6\height}{\rotatebox[origin=c]{90}{\small RealBlur-R}} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt3/square_0.png} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt3/square_1.png} &
    \includegraphics[width=.15\textwidth]{images/paper_visualization400/bt3/square_2.png} \\
  \end{tabular}
\caption{Visualization of blur transferring. The blur pattern in the images of the first column is transferred to the images in the second column to generate the blurred output in the third column.}
  \label{fig:blur_transfered}
\end{figure}

\textbf{Qualitative Comparisons.}
To assess the visual quality of the results generated by different models, we present their deblurred outputs on the test set of our selected datasets. Figure~\ref{fig:visualization of deblur} illustrates the effectiveness of BD-Diff in deblurring across various scenarios, including nighttime, indoor, and wild real-world scenes. On the other hand, existing supervised training methods struggle to remove real blur patterns. In contrast, the performance of the deblurring module limits unpaired training methods and fails to achieve satisfactory visual results. We observe that using inappropriate features as input for generative-diffusion-based methods may lead to artifacts and distortions. Based on this, we compare the structure reconstruction performance of BD-Diff with existing supervised generative-diffusion-based methods and with three variants\footnote{Detailed in Section~\ref{sec:ablation}.} of our model that only processes supervised training. As demonstrated in Figure~\ref{fig:visualization of structural}, BD-Diff performs favorably in reconstructing detailed structures such as facial features, text, and intricate patterns. In contrast, other supervised diffusion-based methods struggle to effectively decouple structure and blur patterns in real-world scenarios, resulting in distortions and artifacts in the restored images.

\subsection{Ablation Study}
\label{sec:ablation}
In this section, we explore the effectiveness of each key component and training phase of BD-Diff. All ablation studies are conducted under the same settings as described in Section~\ref{sec:details} for fair comparisons.


\textbf{Effectiveness of the Reconstruction Task.} To validate the effectiveness of the reconstruction task, we perform ablation by removing the reconstruction task during the joint training phase, denoted as ``w/o $T_3$". The quantitative results presented in Table~\ref{tab:No-Reference Metrics Results of Ablation Studies} and qualitative results shown in Figure~\ref{fig:visualization of structural} and Figure~\ref{fig:ablation} indicate that omitting the reconstruction task results in decline in both quantitative metrics and visual quality. This underscores the importance of the reconstruction task in enhancing the complementarity between structural features and blur patterns, improving the generalization ability of the structure extractor $Q_s$.


\textbf{Effectiveness of the Blur-transfer Task.} We conduct ablation studies to evaluate the effectiveness of the blur-transfer task by removing the joint-training phase, denoted as ``w/o $T_{2}T_{3}$". As illustrated in Table~\ref{tab:No-Reference Metrics Results of Ablation Studies}, Figure~\ref{fig:visualization of structural} and Figure~\ref{fig:ablation}, training the deblurring task solely on synthetic data is insufficient to handle unknown domain blurs, resulting in decline in both quantitative results and visual quality. This is because the structure extractor overfits synthetic blur patterns and fails to decouple unseen blur patterns in real-world scenarios effectively. We further validate the effectiveness of the blur pattern extractor $Q_b$ by visualizing the blur-transfer results. We randomly select a sharp image $a$ and a blurred image $b$ from the test sets. We encode $b$ into a latent code and perform diffusion sampling conditioned on the extracted blur pattern of $a$. The results in Figure~\ref{fig:blur_transfered} demonstrate that $Q_b$ successfully captures and transfers the blur patterns from $a$ to $b$. Although the blur-transfer result $c$ does not achieve pixel-level correspondence with $b$ due to the randomness of the diffusion process, $c$ remains independent of the structure of $a$. This indicates that $Q_b$ has been effectively trained to extract only the blur pattern. On the other hand, $Q_s$ can focus on extracting the complementary structural features.


\textbf{Effectiveness of the Structure Extractor.} We investigate the impact of the structure extractor $Q_s$ by removing it in the deblurring task, denoted as ``w/o $Q_s$". As demonstrated in Table~\ref{tab:No-Reference Metrics Results of Ablation Studies}, the absence of $Q_s$ results in a performance decline across all datasets. Figure~\ref{fig:visualization of structural} and  Figure~\ref{fig:ablation} reveal the emergence of artifacts and distortions when $Q_s$ is removed, underscoring its importance in eliminating features unrelated to structural information to enhance the quality of the restored output. 


\begin{figure}[!t]
  \centering
  \setlength{\tabcolsep}{4pt} 
  \begin{tabular}{ccc}
   \small  Ground Truth & \small  Ours w/o $Q_s$ &\small  Ours w/o $T_{2}T_{3}$   \\
    \includegraphics[width=.15\textwidth]{images/ablation/1_ok.png} & 
    \includegraphics[width=.15\textwidth]{images/ablation/2_ok.png} & 
    \includegraphics[width=.15\textwidth]{images/ablation/3_ok.png} \\
    \multicolumn{3}{c}{\vspace{-12pt}} \\
   \small   Blur &  \small Ours w/o $T_{3}$ & \small \textbf{Ours}  \\
    \includegraphics[width=.15\textwidth]{images/ablation/4_ok.png} & 
    \includegraphics[width=.15\textwidth]{images/ablation/5_ok.png} & 
    \includegraphics[width=.15\textwidth]{images/ablation/6_ok.png} \\
  \end{tabular}
	\caption{Deblurring results of ablation studies. Key regions are marked with red boxes and magnified.}
	\label{fig:ablation}
\end{figure}

\input{tables/exp_ablation}



