\section{Related work}
\label{sec:related_work}
Today, there is a plethora of outlier detection algorithms available in the literature~\cite{aggarwal2017introduction}. Among classic unsupervised outlier detection methods are proximity-based algorithms~\cite{breunig2000lof}, which are based on local neighborhood information around each data point. The most popular algorithm from this group, called Local Outlier Factor (LOF)~\cite{breunig2000lof}, calculates the distances to nearest neighbors to estimate local densities and deems data points with a substantially lower local density as outliers. Another popular group of algorithms consists of statistical models~\cite{goldstein2012histogram,li2022ecod} that first determine the probability distribution of the data and then detect outliers as points outside of the fitted distribution. Popular methods from this group include Histogram-Based Outlier Score (HBOS)~\cite{goldstein2012histogram}, which uses multiple univariate feature histograms to score outlying objects, and the recently proposed Empirical Cumulative Distribution-based Outlier Detection algorithm (ECOD)~\cite{li2022ecod}, which derives a cumulative distribution function to locate rare events in the tails of a distribution. Finally, Isolation Forest~\cite{liu2008isolation} is an example of an ensemble outlier detection method~\cite{LazarevicBagging} that detects anomalies by measuring the number of tree splits required to isolate a data point from others. However, these popular outlier detection algorithms are tailored for scalar data and cannot handle mixed-type datasets out of the box.

Datasets consisting of data points from different domains are sometimes referred to as \textit{multi-modal}. This term is used, for example, in biology when referring to multiomics patient data, in robotics when referring to data coming from different sensors, and recently in text-image-audio applications of large language models (LLMs). There have been proposals of outlier detection methods for multi-modal data, however, they either assume that the different domains all have the same representation~\cite{wellhausen2020safe} or fuse multiple data-specific models into one predictor~\cite{ide2017multi}. The proposed RSIF algorithm constructs a single model capable of handling datasets with mixtures of features of arbitrary data types.

The method presented in this paper builds upon recent advancements in classification algorithms for complex data. Sathe and Aggarwal~\cite{sathe2017similarity} have proposed a method that is capable of handling complex objects in situations where regular features are absent, but similarities between examples are attainable. The algorithm, called Similarity Forest, uses a distance-based projection to order objects and create decision tree splits. More recently, Piernik et al. have proposed the Random Similarity Forest algorithm~\cite{piernik2022random}, a combination of Similarity Forest and Random Forest~\cite{breiman2001random} capable of handling scalar, complex, and mixed datasets. However, the aforementioned algorithms are supervised learners and require labeled data to perform similarity-based projections. In this paper, we combine ideas from projection-based algorithms with the notion of isolation known from Isolation Forests~\cite{liu2008isolation}. Our approach proposes a new way of selecting reference objects for projections to create an unsupervised outlier detection algorithm suitable for multi-modal problems.