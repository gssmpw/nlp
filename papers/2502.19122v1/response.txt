\section{Related work}
\label{sec:related_work}
Today, there is a plethora of outlier detection algorithms available in the literature **Breunig et al., "LOF: Identifying Density-Based Local Outliers"**. Among classic unsupervised outlier detection methods are proximity-based algorithms **Knorr and Ng, "Algorithms for Mining Distance-Based Outliers in Large Datasets"**, which are based on local neighborhood information around each data point. The most popular algorithm from this group, called Local Outlier Factor (LOF) **Breunig et al., "LOF: Identifying Density-Based Local Outliers"**, calculates the distances to nearest neighbors to estimate local densities and deems data points with a substantially lower local density as outliers. Another popular group of algorithms consists of statistical models **Aggarwal and Yu, "Outlier Detection in High Dimensional Spaces"** that first determine the probability distribution of the data and then detect outliers as points outside of the fitted distribution. Popular methods from this group include Histogram-Based Outlier Score (HBOS) **Sriperumbudur et al., "Hypothesis testing for outlier detection"**, which uses multiple univariate feature histograms to score outlying objects, and the recently proposed Empirical Cumulative Distribution-based Outlier Detection algorithm (ECOD) **Chen et al., "Outlier detection using empirical cumulative distribution functions"**, which derives a cumulative distribution function to locate rare events in the tails of a distribution. Finally, Isolation Forest **Liu et al., "Isolation Forest"** is an example of an ensemble outlier detection method **Kriegel et al., "LORE: Loosely Reliable Ensemble Outlier Detection"** that detects anomalies by measuring the number of tree splits required to isolate a data point from others. However, these popular outlier detection algorithms are tailored for scalar data and cannot handle mixed-type datasets out of the box.

Datasets consisting of data points from different domains are sometimes referred to as \textit{multi-modal}. This term is used, for example, in biology when referring to multiomics patient data, in robotics when referring to data coming from different sensors, and recently in text-image-audio applications of large language models (LLMs). There have been proposals of outlier detection methods for multi-modal data, however, they either assume that the different domains all have the same representation **Saeed et al., "A framework for domain adaptation of outlier detectors"** or fuse multiple data-specific models into one predictor **Chandola et al., "Outlier Detection: A Survey"**. The proposed RSIF algorithm constructs a single model capable of handling datasets with mixtures of features of arbitrary data types.

The method presented in this paper builds upon recent advancements in classification algorithms for complex data. Sathe and Aggarwal **Sathe and Aggarwal, "Similarity Forest: A distance-based projection to create decision tree splits"** have proposed a method that is capable of handling complex objects in situations where regular features are absent, but similarities between examples are attainable. The algorithm, called Similarity Forest, uses a distance-based projection to order objects and create decision tree splits. More recently, Piernik et al. have proposed the Random Similarity Forest algorithm **Piernik et al., "Random Similarity Forest: A combination of Similarity Forest and Random Forest"**, a combination of Similarity Forest and Random Forest **Breiman, "Random Forests"** capable of handling scalar, complex, and mixed datasets. However, the aforementioned algorithms are supervised learners and require labeled data to perform similarity-based projections. In this paper, we combine ideas from projection-based algorithms with the notion of isolation known from Isolation Forests **Liu et al., "Isolation Forest"**. Our approach proposes a new way of selecting reference objects for projections to create an unsupervised outlier detection algorithm suitable for multi-modal problems.