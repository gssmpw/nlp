\section{Related Work}
% Related Work
A growing interest has been in leveraging large language models for optimization and constraint programming tasks. Early efforts like~\cite{nl4opt} focused on linear programming problems using entity recognition and logical forms, achieving promising results with ChatGPT (92.7\% accuracy on NL4OPT). Additionally, Holy Grail 2.0~\cite{tsouros2023holy} proposed a blueprint for building conversation modeling assistants. 
Significant improvements in execution accuracy of LLM generated \textsc{MiniZinc} code were achieved through using in-line annotation of entities in problem descriptions.~\cite{ner4opt2023, ner4opt2024}
 Sophisticated approaches emerged with a multi-agent Chain-of-Experts framework~\cite{complexor}, and LLM-agent, Optimus~\cite{optimus}, which developed a modular system for handling complex problem descriptions. However, these systems remain tied to specific solvers such as Gurobi and Cvxpy. The challenge of data scarcity was addressed by data augmentation, leveraging CodeT5 to achieve higher accuracy than zero-shot LLM approaches, although still limited to linear programming problems using PuLP~\cite{synthesis}.
Significant strides were made in training custom LLMs for optimization modeling through their OR-Instruct framework but remained constrained by single-solver dependency~\cite{orlm}. A notable contribution to evaluation methodologies came from the MAMO benchmark, focusing on LLMs' mathematical modeling processes rather than solution correctness. In CP, LLMs' potential in search space optimization has been demonstrated through generating streamliners using \textsc{MiniZinc}~\cite{streamllm}. Natural language to constraint model translation has also been explored through a simple decomposition-based prompting approach with GPT models~\cite{tsouros2023holy}. Building on this, in-context learning strategies such as Retrieval Augmented Generation (RAG) have been explored to build constraint models in CPMPY~\cite{michailidis_et_al:LIPIcs.CP.2024.20}. Domain-specific applications have also emerged, focusing on supply chain optimization while preserving data privacy~\cite{optiguide} and diagnosing infeasible optimization problems through interactive conversations~\cite{optichat}.

Our work, \textsc{Text2Zinc}, addresses several key limitations in existing research. First, unlike previous datasets focusing on single problem types, we uniquely integrate both satisfaction and optimization problems spanning continuous and discrete domains. Second, we leverage \textsc{MiniZinc}'s solver-agnostic modeling capabilities, moving beyond the solver-specific approaches of previous attempts. Finally, we explore the effectiveness of knowledge graphs as novel, structured, intermediate representations, an aspect unexplored in the existing literature.