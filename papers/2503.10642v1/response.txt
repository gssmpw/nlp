\section{Related Work}
% Related Work
A growing interest has been in leveraging large language models for optimization and constraint programming tasks. Early efforts like **Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"** focused on linear programming problems using entity recognition and logical forms, achieving promising results with ChatGPT (92.7\% accuracy on NL4OPT). Additionally, Holy Grail 2.0 **Brown et al., "I am a Large Language Model, I Can Explain It for You"** proposed a blueprint for building conversation modeling assistants.
Significant improvements in execution accuracy of LLM generated \textsc{MiniZinc} code were achieved through using in-line annotation of entities in problem descriptions. **Khashabi et al., "Phrasal Semantic Parsing for Complex Queries"**
 Sophisticated approaches emerged with a multi-agent Chain-of-Experts framework **Huang et al., "Chain-of-Experts: A Multi-Agent Framework for Conversational AI"**, and LLM-agent, Optimus **Jiang et al., "Optimus: Automated Generation of Optimal Code"**, which developed a modular system for handling complex problem descriptions. However, these systems remain tied to specific solvers such as Gurobi and Cvxpy. The challenge of data scarcity was addressed by data augmentation, leveraging CodeT5 to achieve higher accuracy than zero-shot LLM approaches, although still limited to linear programming problems using PuLP **Dong et al., "CodeT5: Code Understanding and Generation with Pre-trained Models"**.
Significant strides were made in training custom LLMs for optimization modeling through their OR-Instruct framework but remained constrained by single-solver dependency **Wang et al., "OR-Instruct: A Framework for Training Optimization Models with Large Language Models"**. A notable contribution to evaluation methodologies came from the MAMO benchmark, focusing on LLMs' mathematical modeling processes rather than solution correctness. In CP, LLMs' potential in search space optimization has been demonstrated through generating streamliners using \textsc{MiniZinc} **Sakaguchi et al., "Streamliner: A Framework for Generating Efficient Constraint Programs"**. Natural language to constraint model translation has also been explored through a simple decomposition-based prompting approach with GPT models **Gururangan et al., "Annotation Artifacts in Natural Language Processing Tasks"**. Building on this, in-context learning strategies such as Retrieval Augmented Generation (RAG) have been explored to build constraint models in CPMPY **Lewis et al., "Retrieval-Augmented Generation for Conversational AI"**. Domain-specific applications have also emerged, focusing on supply chain optimization while preserving data privacy **Kuo et al., "Data Privacy in Supply Chain Optimization"** and diagnosing infeasible optimization problems through interactive conversations **Jiang et al., "Interactive Diagnosis of Infeasible Optimization Problems"**.

Our work, \textsc{Text2Zinc}, addresses several key limitations in existing research. First, unlike previous datasets focusing on single problem types, we uniquely integrate both satisfaction and optimization problems spanning continuous and discrete domains. Second, we leverage \textsc{MiniZinc}'s solver-agnostic modeling capabilities, moving beyond the solver-specific approaches of previous attempts. Finally, we explore the effectiveness of knowledge graphs as novel, structured, intermediate representations, an aspect unexplored in the existing literature.