\section{Related Work}
\paragraph{Learning from Feedback.}

Other than the heuristic binary feedback studied in this work, prior research has explored feedback from various sources, such as humans~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, other models~\cite{yang-etal-2022-re3}, tools~\cite{DBLP:conf/nips/SchickDDRLHZCS23}, and knowledge bases~\cite{gao-etal-2023-rarr}. This paper focuses on demonstrating the effectiveness of the proposed method and other feedback types are beyond the scope of this paper.

\paragraph{Test-Time Training.}

Test-Time Training (TTT) has shown success in the image modality by addressing distribution shifts and enhancing model capacity through self-supervised fine-tuning on each test case~\cite{DBLP:conf/icml/SunWLMEH20,DBLP:conf/nips/LiuKDBMA21,DBLP:journals/corr/abs-2310-13807}. Recent studies have extended TTT to the text modality~\cite{DBLP:conf/iclr/Hardt024,DBLP:journals/corr/abs-2401-11504}. The most relevant work, by~\citet{aky√ºrek2024surprisingeffectivenesstesttimetraining}, uses TTT to enhance the reasoning ability of LLMs. However, their method relies heavily on human scaffolding for self-supervision and does not generalize beyond ARC-AGI~\cite{chollet2019measureintelligence}. In contrast, FTTT is generally applicable.

\paragraph{Learning to Optimize.}

Learning to Optimize (L2O) trains a network to act as an optimizer for another network~\cite{DBLP:journals/jmlr/ChenCCH0WY22}. Early approaches used reinforcement learning to train such optimizers~\cite{DBLP:conf/iclr/LiM17,DBLP:conf/icml/ChenHCDLBF17}, while recent work focuses on discovering analytical white-box optimizers~\cite{DBLP:conf/icml/BelloZVL17,DBLP:conf/nips/ChenLHRW0DLHLL23}. The most relevant work, MEND~\cite{DBLP:conf/iclr/MitchellLBFM22}, trains a network to predict weight updates from training gradients. \method{} builds on this idea, extending it to learn from test-time feedback with a distinct architecture.