\begin{table*}
\centering
\small
% \footnotesize
% \resizebox{\linewidth}{!}{
% \setlength{\tabcolsep}{5pt}
\begin{tabular}[t]{l|r|ccc|c}
\toprule
 \makecell[c]{\textbf{Method}} & \makecell[c]{\textbf{\#Param.}} & \makecell[c]{\textbf{MATH}} & \makecell[c]{\textbf{GSM8K}} & \makecell[c]{\textbf{MBPP}} & \makecell[c]{\textbf{Avg.}} \\
\midrule
\multicolumn{6}{c}{\texttt{Llama-3.1-8B-Instruct}} \\
\midrule
% Best-of-N~\cite{DBLP:journals/corr/abs-2407-21787} & \makecell[c]{-} & 0.2200$_{0.0142}$ & 0.4419$_{0.0333}$ & 0.2196$_{0.0076}$ & 0.2778$_{0.0302}$ & \\
% Best-of-N~\cite{DBLP:journals/corr/abs-2407-21787} & \makecell[c]{-} & 0.2200 & 0.4419 & 0.2196 & 0.2778 & \\
% BitFit &  & & & & & \\
Adapter~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19} & 134M & 0.5933$_{0.0151}$ & 0.7979$_{0.0056}$ & 0.2632$_{0.0058}$ & 0.5515 \\
% Prefix-Tuning~\cite{li-liang-2021-prefix} & 136M & $_{}$ & $_{}$ & $_{}$ & \\
% ReFT~\cite{DBLP:journals/corr/abs-2404-03592} &  & & & & & \\
(IA)$^3$~\cite{DBLP:conf/nips/LiuTMMHBR22} & 524K & 0.6187$_{0.0105}$ & 0.8929$_{0.0107}$ & 0.5685$_{0.0022}$ & 0.6934 \\
LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22} & 1.7M & 0.6387$_{0.0136}$ & 0.9186$_{0.0037}$ & 0.5639$_{0.0242}$ & 0.7071 \\
LN-Tuning~\cite{DBLP:conf/iclr/ZhaoT0MX24} & 266K & 0.6280$_{0.0113}$ & 0.8899$_{0.0056}$ & 0.5748$_{0.0175}$ & 0.6976 \\
Full Fine-Tuning & 8B & 0.6027$_{0.0136}$ & 0.7722$_{0.0056}$ & 0.4034$_{0.0096}$ & 0.5928 \\
% Adapter~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19} & 134M & 0.1120 & 0.3710 & 0.1262 & 0.0370 & \\
% Prefix-Tuning~\cite{li-liang-2021-prefix} & 136M & 0.0600 & 0.3213 & 0.0981 & 0.0000 & \\
% % ReFT~\cite{DBLP:journals/corr/abs-2404-03592} &  & & & & & \\
% (IA)$^3$~\cite{DBLP:conf/nips/LiuTMMHBR22} & 524K & 0.1360 & 0.4615 & 0.2243 & 0.2407 & \\
% LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22} & 1.7M & 0.1360 & 0.4706 & 0.2243 & 0.1481 & \\
% LN-Tuning~\cite{DBLP:conf/iclr/ZhaoT0MX24} & 266K & 0.1480 & 0.4163 & 0.1776 & 0.1296 & \\
% Full Fine-Tuning & 8B & 0.1208 & 0.4163 & 0.2019 & 0.1667 & \\
\midrule
\method{} & 439K & \textbf{0.7013}$_{0.0050}$ & \textbf{0.9246}$_{0.0056}$ & \textbf{0.6184}$_{0.0159}$ & 0.7481 \\
\toprule
\multicolumn{6}{c}{\texttt{Mistral-7B-Instruct-v0.3}} \\
\midrule
% Best-of-N~\cite{DBLP:journals/corr/abs-2407-21787} & \makecell[c]{-} & & & & & \\
% BitFit &  & & & & & \\
Adapter~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19} & 134M & \textbf{0.5418}$_{0.0111}$ & 0.8264$_{0.0021}$ & 0.2763$_{0.0076}$ & 0.5482 \\
% Prefix-Tuning~\cite{li-liang-2021-prefix} & 136M & $_{}$ & $_{}$ & $_{}$ & \\
(IA)$^3$~\cite{DBLP:conf/nips/LiuTMMHBR22} & 524K & 0.5041$_{0.0056}$ & 0.8686$_{0.0060}$ & 0.4914$_{0.0185}$ & 0.6214 \\
LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22} & 1.7M & 0.5117$_{0.0091}$ & 0.8686$_{0.0016}$ & 0.4968$_{0.0046}$ & 0.6257 \\
LN-Tuning~\cite{DBLP:conf/iclr/ZhaoT0MX24} & 266K & 0.4357$_{0.0115}$ & 0.8259$_{0.0051}$ & 0.4065$_{0.0095}$ & 0.5560 \\
Full Fine-Tuning & 7B & 0.5388$_{0.0157}$ & 0.7355$_{0.0016}$ & 0.2548$_{0.0095}$ & 0.5097 \\
% Adapter~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19} & 134M & 0.0903 & 0.4575 & 0.1516 & 0.0400 & \\
% Prefix-Tuning~\cite{li-liang-2021-prefix} & 136M & & & 0.0839 &  & \\
% % ReFT~\cite{DBLP:journals/corr/abs-2404-03592} &  & & & & & \\
% (IA)$^3$~\cite{DBLP:conf/nips/LiuTMMHBR22} & 524K & 0.0970 & 0.3409 & 0.1710 & 0.1000 & \\
% LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22} & 1.7M & 0.0971 & 0.3447 & 0.1129 & 0.0900 & \\
% LN-Tuning~\cite{DBLP:conf/iclr/ZhaoT0MX24} & 266K & 0.0609 & 0.2636 & 0.1355 & 0.1400 & \\
% Full Fine-Tuning & 7B & 0.1294 & 0.4461 & 0.1129 & 0.0100 & \\
\midrule
\method{} & 439K & 0.4891$_{0.0111}$ & \textbf{0.9003}$_{0.0039}$ & \textbf{0.5194}$_{0.0070}$ & 0.6363 \\
\bottomrule
\end{tabular}
% }
% \vspace{-5pt}
\caption{Fine-tuning results on four datasets with a budget of 32. \#Param. denotes the number of trainable parameters. We report the mean of three runs with different random seeds and standard deviation in the subscript. \textbf{Bold} entries are the best results.}
\label{tab:meta}
% \vspace{-0.5cm}
\end{table*}