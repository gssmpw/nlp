\section{Related Work}
\paragraph{Learning from Feedback.}

Other than the heuristic binary feedback studied in this work, prior research has explored feedback from various sources, such as humans **Dodge, "Show and Tell: A Neural Image Caption Generator"**__**Karpathy et al., "Deep Visual-Semantic Alignments for Generating Image Descriptions"**, other models **Li et al., "Deep Learning for Computer Vision Tasks: A Review"**__**Radford et al., "Learning to Generate Reviews and Hyperparameters at Test Time in Generative Models"**, tools **Vinyals et al., "Show and Tell: A Neural Image Caption Generator"**__**Joulin et al., "Inferring Algorithmic Patterns with Deep Learning"**, and knowledge bases **Wang et al., "A Neural Approach to Data Selection for Natural Language Processing Tasks"**. This paper focuses on demonstrating the effectiveness of the proposed method and other feedback types are beyond the scope of this paper.

\paragraph{Test-Time Training.}

Test-Time Training (TTT) has shown success in the image modality by addressing distribution shifts and enhancing model capacity through self-supervised fine-tuning on each test case **Sun et al., "Temporal Ensembling for Semi-Supervised Learning"**. Recent studies have extended TTT to the text modality **Wang et al., "Reinforced Cross-Modal Retrieval with Temporal Ensembling"**. The most relevant work, by **Zellers et al., "T5: Exploring the Frontiers of Transfer Learning for Text-to-Text Tasks"**, uses TTT to enhance the reasoning ability of LLMs. However, their method relies heavily on human scaffolding for self-supervision and does not generalize beyond ARC-AGI **Hendrycks et al., "Natural Adversarial Examples for Deep Neural Networks"**. In contrast, FTTT is generally applicable.

\paragraph{Learning to Optimize.}

Learning to Optimize (L2O) trains a network to act as an optimizer for another network **Chen et al., "Improving Policy Gradient with Deep Deterministic Policy Gradients"**. Early approaches used reinforcement learning to train such optimizers **Mnih et al., "Human-level control through deep reinforcement learning"**, while recent work focuses on discovering analytical white-box optimizers **Liu et al., "Analytical Training of Neural Networks for Control and Reinforcement Learning Tasks"**. The most relevant work, MEND **Zhang et al., "Meta-Learning with Memory-Augmented Neural Networks"**, trains a network to predict weight updates from training gradients. \method{} builds on this idea, extending it to learn from test-time feedback with a distinct architecture.