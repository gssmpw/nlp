% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{url}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage[most]{tcolorbox}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{ulem}
% \usepackage{minted}

\usepackage{pgfplotstable}
\usepackage{subfigure}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations}
\usetikzlibrary{pgfplots.groupplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{shapes.multipart}
\usetikzlibrary{shapes.geometric}

\definecolor{ugreen}{cmyk}{1,0,1,0.498}
\definecolor{lyyblue}{cmyk}{0.8278,0.3333,0,0.2941}
\definecolor{lyygreen}{cmyk}{0.6813,0,0.725,0.3725}
\definecolor{lyyred}{cmyk}{0,0.8855,0.8767,0.1098}
\definecolor{dblue}{cmyk}{1,0.5487,0,0.5569}
\definecolor{royalblue}{HTML}{4169e1}

\definecolor{myred}{HTML}{E33222}

\newcommand{\cmark}{\ding{52}}%
\newcommand{\xmark}{\ding{56}}%

\definecolor{gr}{RGB}{0, 146, 0}
\newcommand{\gr}[1]{\textcolor{gr}{#1}}
\newcommand{\red}[1]{\textcolor{myred}{#1}}
\newcommand{\purp}[1]{\textcolor{purple}{#1}}
\newcommand{\org}[1]{\textcolor{orange}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\teal}[1]{\textcolor{teal}{#1}}
\newcommand{\gray}[1]{\textcolor{lightgray}{#1}}

\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}

\newcommand{\greencheck}{\textcolor{gr}{\ding{51}}}
\newcommand{\redcross}{\textcolor{myred}{\ding{55}}}

\newcommand{\hlc}[2][yellow]{{%
    \colorlet{foo}{#1}%
    \sethlcolor{foo}\hl{#2}}%
}

\newcommand{\method}{\textsc{OpTune}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Learning to Reason from Feedback at Test-Time}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\author{Yanyang Li, Michael R. Lyu, Liwei Wang\thanks{Corresponding author.}\\
The Chinese University of Hong Kong \\
\texttt{\{yyli21,lyu,lwwang\}@cse.cuhk.edu.hk}
}

\begin{document}
\maketitle
\begin{abstract}
Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, \method{}, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and \method{} achieve superior scalability and performance\footnote{\url{https://github.com/LaVi-Lab/FTTT}}.
\end{abstract}

\section{Introduction}

Leveraging external feedback from interactions with the environment during test time has emerged as a promising approach for large language models (LLMs). This includes applications such as LLM-based agents~\cite{DBLP:conf/iclr/YaoZYDSN023,DBLP:conf/nips/ShinnCGNY23} and, more recently, test-time scaling~\cite{wu2024inferencescalinglawsempirical,DBLP:journals/corr/abs-2408-03314,liu20251bllmsurpass405b}. Such methods further enhance the potential of LLMs to solve challenging tasks, e.g., Olympiad-level math problems~\cite{guan2025rstarmathsmallllmsmaster} and competitive programming~\cite{openai2025competitiveprogramminglargereasoning}.

Significant progress in this area typically falls into two categories~\cite{DBLP:journals/corr/abs-2408-03314}, as illustrated in Figure~\ref{fig:compare}: sequential revision and parallel sampling. Sequential revision methods~\cite{DBLP:conf/nips/ShinnCGNY23,DBLP:conf/nips/MadaanTGHGW0DPY23} incorporate previous attempts into the LLM's context, while parallel sampling methods~\cite{DBLP:journals/corr/abs-2407-21787,DBLP:conf/nips/XieKZZKHX23} generate new attempts independently of prior failures.
However, both approaches have notable limitations. Sequential revision is computationally expensive due to long context lengths and faces challenges~\cite{muennighoff2025s1simpletesttimescaling}, such as position bias~\cite{DBLP:journals/tacl/LiuLHPBPL24} and attention noise~\cite{ye2024differentialtransformer}. In contrast, parallel sampling, while efficient, fails to learn from previous errors~\cite{DBLP:journals/corr/abs-2407-21787}.
Unlike these paradigms, human reasoning follows a different pattern: humans store recent experiences in ``fast weights''~\cite{DBLP:conf/nips/BaHMLI16}, enabling them to neither revisit past errors explicitly nor start each attempt without any prior knowledge. Recent research suggests that the weights of neural networks could serve as a natural memory mechanism during test time~\cite{DBLP:journals/corr/abs-2401-11504}.

% poor length generalization \cite{muennighoff2025s1simpletesttimescaling}

% in-context learning is similar to gradient descent~\cite{DBLP:conf/icml/OswaldNRSMZV23}

% store not only to parameters~\cite{DBLP:journals/corr/abs-2401-11504} but also activations~\cite{DBLP:conf/iclr/DathathriMLHFMY20,DBLP:journals/corr/abs-2404-03592}

Building on these observations, we propose a novel paradigm that leverages Test-Time Training (TTT)~\cite{DBLP:conf/icml/SunWLMEH20,DBLP:journals/corr/abs-2310-13807} to store past experiences in model weights rather than in the context. This approach bridges the gap between sequential revision and parallel sampling by indirectly incorporating knowledge into the LLM without disrupting in-context reasoning. Specifically, we introduce \textbf{F}eedback-based \textbf{T}est-\textbf{T}ime \textbf{T}raining (FTTT), which employs a carefully designed TTT task enriched with feedback through self-reflection. We demonstrate that FTTT improves test-time computation scalability on two mathematical reasoning and two code generation datasets, using \texttt{Llama-3.1-8B-Instruct}~\cite{DBLP:journals/corr/abs-2407-21783} and \texttt{Mistral-7B-Instruct-v0.3}~\cite{DBLP:journals/corr/abs-2310-06825}.

\input{figures/ttt}

Inspired by advancements in learning to optimize~\cite{DBLP:journals/jmlr/ChenCCH0WY22}, we explore training learnable test-time optimizers to yield Pareto-optimal cost-performance tradeoffs. Our proposed learnable optimizer, \method{}, is a lightweight neural network that predicts weight updates from the gradients of the previous attempt. Unlike traditional parameter-efficient fine-tuning (PEFT) methods, \method{} works on the gradient rather than the activation space. Experiments on three reasoning datasets and two different LLMs demonstrate the effectiveness of \method{}, outperforming five widely used PEFT baselines.

% compatible with advanced search by deeply modifying the static policy

\section{Feedback-based Test-Time Training}
\label{sec:ttt}

\subsection{The Test-Time Training Task}

The problem of exploiting test-time feedback is as~\cite{DBLP:conf/nips/ShinnCGNY23}: given a question $Q$, a model $M$ attempts to solve $Q$ within a budget of $N$ attempts. A verifier $V$ evaluates each attempt, such as the $n$-th attempt $A_n$, and provides feedback $V(A_n)$. This work focuses on binary verifiers, which determine whether $A_n$ is correct. These verifiers are well-established, rule-based systems that are both cost-effective and efficient to evaluate.

When the model generates attempts sequentially, our goal is to enable $M$ to learn from previous attempts to improve subsequent ones. To achieve this, we frame learning from previous attempts as a training problem: at each step $n$, we optimize $M$ using $Q$, $A_n$, and $V(A_n)$, aiming for $M$ to generate a better $A_{n+1}$. This way internalizes the past attempts into weights for efficient inference of $A_{n+1}$. As a result, the sequence of attempts can be viewed as an $N$-step optimization process.

A key challenge is designing an effective supervised task using $Q$, $A_n$, and $V(A_n)$ to improve the model's ability to solve $Q$. We build on the intuition that \textit{a model capable of judging the correctness of a solution should also be able to solve the question itself}. Concretely, given $Q$ and $A_n$, we train $M$ to predict verbal feedback $F$ that aligns with $V(A_n)$. This leads to our FTTT loss:
\begin{equation}
\scalebox{0.8}{\ensuremath{
    \mathcal{L}_\mathrm{FTTT}\left(Q, A_n\right)=-\frac{1}{l_0}\log M_{n-1}\left(F\mid Q, A_n\right)
}}
    \label{eqn:ttt-loss}
\end{equation}
where $l_0$ is the length of $F$ and $M_0$ denotes the raw LLM. In this work, $F$ is set to ``Your answer is incorrect.'' when $V\left( A_n\right)$ implies an incorrect $A_n$.

\subsection{Self-Reflected Feedback}

Since we are working with a binary verifier, the learning signal is limited at each interaction. Previous research suggests that LLMs can self-correct errors when provided with external signals~\cite{DBLP:conf/iclr/0009CMZYSZ24}. Inspired by this, we aim to enhance the learning signal by leveraging the model to generate silver-standard training labels.

We first sample a reflection $R_n$ from the model given $Q$, $A_n$, $F$ and the instruction $P$:
\begin{equation}
% \scalebox{0.8}{\ensuremath{
    R_n\sim M_0\left(R\mid Q,A_n,F,P\right)
% }}
\end{equation}
In practice, we use $M_0$ to generate $R_n$ to mitigate the risk of degraded self-reflection ability after training.
% Below is an example for \texttt{Llama-3.1-8B-Instruct}~\cite{DBLP:journals/corr/abs-2407-21783}:
% \patchcmd{\quote}{\rightmargin}{\leftmargin 15pt \rightmargin}{}{}
% \begin{quote}
% \small %\it
% \begin{tcolorbox}[breakable, colback=white, colbacktitle=blue!5!white, colframe=black, boxrule=1pt, title={\textcolor{black}{\textbf{Reflection Generation Example}}}]
% \textbf{User:} Solve the following math problem $\ldots$\\
% \textbf{Assistant:} $\ldots$ the final answer is: $\ldots$\\
% \textbf{User:} Your answer is incorrect. \textcolor{gray}{Please carefully check the solution and summarize all mistakes in short. Do NOT provide the corrected solution. Do NOT say ``my solution''.}\\
% \textbf{Assistant:} \textcolor{royalblue}{Here is the summary of the mistakes in the previous solution $\ldots$}
% \end{tcolorbox}
% \end{quote}
% \patchcmd{\quote}{\rightmargin}{\leftmargin 26pt \rightmargin}{}{}
% The sentence in \textcolor{gray}{gray} is $P$ and the one in \textcolor{royalblue}{blue} is the generated reflection $R_n$.
The auxiliary loss is then defined as:
\begin{equation}
\scalebox{0.8}{\ensuremath{
    \mathcal{L}_\mathrm{aux}\left(Q, A_n, R_n\right)=-\frac{1}{l_n} \log M_{n-1}\left(R_n\mid Q, A_n, F\right)
}}
    \label{eqn:reflect-loss}
\end{equation}
where $l_n$ is the length of $R_n$. Eq.~\ref{eqn:reflect-loss} can be interpreted as a sequence-level distillation loss~\cite{kim-rush-2016-sequence}, where knowledge from the raw model $M_0$ is distilled into the trained model $M_{n-1}$ to prevent overfitting.
Finally, the overall loss is as:
\begin{equation}
% \scalebox{0.8}{\ensuremath{
    \mathcal{L}_\mathrm{final}=\mathcal{L}_\mathrm{FTTT} + \mathcal{L}_\mathrm{aux}
% }}
\end{equation}
Below is a training example with self-reflection, where underlined sentences are the training target:

\patchcmd{\quote}{\rightmargin}{\leftmargin 15pt \rightmargin}{}{}
\begin{quote}
\small %\it
\begin{tcolorbox}[breakable, colback=white, colbacktitle=blue!5!white, colframe=black, boxrule=1pt, title={\textcolor{black}{\textbf{Training Example with Self-Reflection}}}]
\textbf{User:} Solve the following math problem $\ldots$\\
\textbf{Assistant:} $\ldots$ the final answer is: $\ldots$\\
\textbf{User:} \ul{Your answer is incorrect.} \uwave{Here is the summary of the mistakes in the previous solution $\ldots$}
\end{tcolorbox}
\end{quote}
\patchcmd{\quote}{\rightmargin}{\leftmargin 26pt \rightmargin}{}{}
The \ul{underlined} sentence corresponds to $F$ in Eq.~\ref{eqn:ttt-loss} and the \uwave{wave-underlined} sentence represents $R_n$ for Eq.~\ref{eqn:reflect-loss}.
Algorithm~\ref{alg:ttt} summarizes our FTTT.

\input{figures/algo}

\paragraph{Discussion.}

As shown in Table~\ref{tab:compare}, FTTT combines the strengths of sequential revision and parallel sampling. Like sequential revision, it leverages memory (i.e., weights) to store past failed attempts, while avoiding the length generalization issues as in parallel sampling. Figure~\ref{fig:compare} highlights the advantages of FTTT from a probabilistic graphical model perspective, where both sequential revision and parallel sampling are special cases of FTTT with additional independence assumptions. In particular, sequential revision assumes that attempts form a Markov process, whereas parallel sampling treats each attempt as independent.

In terms of efficiency, FTTT is highly optimized, requiring one additional backward propagation computed in parallel for all tokens in one attempt, whose cost is negligible. The dominant overhead of FTTT is generating $R_n$. However, FTTT is still much faster than sequential revision (see Section~\ref{sec:result1}), as $R_n$ is typically short.

Moreover, FTTT closely resembles vanilla TTT~\cite{DBLP:conf/icml/SunWLMEH20}, but with additional inputs beyond $Q$, such as $A_n$, $V\left(A_n\right)$, and $R_n$, as defined in the problem. Consequently, it inherits the same convergence guarantees as TTT.
% Some advent algorithms ~\cite{DBLP:journals/corr/abs-2403-19094,DBLP:conf/nips/YaoYZS00N23,DBLP:journals/corr/abs-2405-03553} may also bridge the gap between sequential revision and parallel sampling to some extent. But they merely manipulate the output logits in the shallow layer and are unable to revise the ranking of unexplored candidates.

\input{tables/compare}

\section{A Learnable Test-Time Optimizer}

\subsection{The Learning to Optimize Problem}

Although FTTT achieves success (see Section~\ref{sec:result1}), it simply accumulates the gradients of the feedback received so far to update the weights. This raises the question: can we design a better test-time optimizer that more effectively exploits feedback?

Motivated by learning to optimize~\cite{DBLP:journals/jmlr/ChenCCH0WY22}, we adopt a neural network as the test-time optimizer. Concretely, this learnable test-time optimizer is formulated as $f_\theta\left(Q, \left\{A_i, V\left(A_i\right)\right\}^n_{i=1}\right)$, which predicts updates for all LLM weights based on the previous $n$ attempts, and $\theta$ is the optimizer parameter. However, this direct formulation leads to prohibitively large networks due to high-dimensional input and output spaces. For a maximum number of $m$ tokens per attempt and an $l$-layer LLM, the input space grows to $n \times m \times l$, even when updating only a scalar (we exclude the token count of $Q$, as it is significantly smaller than $m$). Since updates for all weight matrices across all layers are predicted jointly, the dimensionality of the input and output spaces becomes unmanageable. We therefore simplify $f_\theta$ by introducing the following assumptions:
\begin{enumerate}[noitemsep, nolistsep, label={(\bfseries A\arabic*):},left=\parindent]
    \item \textbf{Markov Property}: The latest attempt captures all relevant information from previous attempts.
    \item \textbf{Independent Update}: The optimizer predicts updates for each parameter independently, similar to conventional optimizers.
\end{enumerate}

\textbf{A1} eliminates the dependency on $n$ and \textbf{A2} enables updates to be predicted independently for each weight, significantly reducing the size of the output space. The learnable test-time optimizer now becomes $\tilde{\nabla}_{W_i}=f_{\theta_{W_i}}\left(Q, A_n, V\left(A_n\right)\right)$, where it predicts the update for the weight $W_i$ in the $i$-th layer based on $Q$ and the latest attempt $A_n$.

To train all $f$, we define the following loss:
\begin{equation}
\scalebox{0.8}{\ensuremath{
    \mathcal{L}_\mathrm{meta}=-\log M\left(\hat{A} \mid Q, \left\{W+\tilde{\nabla}_{W}\mid\forall \ W \in \mathcal{W}\right\}\right)
}}
    \label{eqn:meta}
\end{equation}
where $\hat{A}$ is the ground-truth for $Q$ and $\mathcal{W}$ is the set of LLM weights. Eq.~\ref{eqn:meta} encourages $f$ to predict updates that increase the likelihood of generating the correct answer after applying the updates.

\subsection{A Parameter-Efficient Architecture in The Gradient Space}

Given the limited learning signal at test time, we design the learnable optimizer to be parameter-efficient to alleviate overfitting. However, the input and output spaces of $f_{\theta_{W_i}}\left(Q, A_n, V\left(A_n\right)\right)$ are large due to their lengths, making even a simple linear projection parameter-intensive. Additionally, $V\left(A_n\right)$ may be heterogeneous to $Q$ and $A_n$, e.g., a scalar, posing challenges for modeling.

Inspired by the success of FTTT in Section~\ref{sec:ttt} and recent works~\cite{DBLP:conf/iclr/MitchellLBFM22,DBLP:journals/corr/abs-2401-11504}, we propose a parameter-efficient architecture in the gradient space as the learnable optimizer.

\paragraph{Gradient-based Input Compression.}

Instead of directly inputting $Q$, $A_n$, and $V\left(A_n\right)$, we first project them into the gradient space, since recent work suggests that long context can be effectively compressed by gradients~\cite{DBLP:journals/corr/abs-2401-11504}. This way reduces the token count $m$ in $A_n$ to a constant and unifies the spaces of $Q$, $A_n$, and $V\left(A_n\right)$ to ease the modeling. To compress $Q$ and $A_n$, we use the next token prediction loss, while for $V\left(A_n\right)$, we include $\mathcal{L}_\mathrm{FTTT}$ in Eq.~\ref{eqn:ttt-loss}. The final loss for compressing the optimizer input is:
\begin{equation}
% \scalebox{0.8}{\ensuremath{
    \mathcal{L}_\mathrm{compress}=-\frac{1}{m}\log M\left(A_n\mid Q\right) + \mathcal{L}_\mathrm{FTTT}
% }}
\end{equation}
The input of $f_{\theta_{W_i}}$ to predict the update of $W_i$ now is the gradient $\nabla_{W_i}$ of $\mathcal{L}_\mathrm{compress}$ w.r.t. $W_i$. Consequently, $f_{\theta_{W_i}}$ receives a fixed-size tensor as input rather than a variable-length sequence.

\paragraph{Gradient Decomposition.}

Although $f_{\theta_{W_i}}$ operates on a smaller space after compression, the dimensionality of the gradient space remains large for direct processing. We utilize the observation that $\nabla_{W_i}\in \mathbb{R}^{d\times d}$ (assuming $W_i\in \mathbb{R}^{d\times d}$) can be decomposed into two vectors to further reduce the dimensionalities: the input to a linear projection with weight $W_i$, $u_i\in \mathbb{R}^{d \times 1}$, and the gradient of $\mathcal{L}_\mathrm{compress}$ w.r.t. the output of the projection, $\delta_{i+1}\in \mathbb{R}^{d \times 1}$~\cite{DBLP:conf/iclr/MitchellLBFM22}. In this framework, $f_{\theta_{W_i}}$ takes the decomposed $u_i$ and $\delta_{i+1}$ as its input and predicts $\tilde{u}_i$ and $\tilde{\delta}_{i+1}$. The update is then reconstructed as $\tilde{\nabla}_{W_i}=\tilde{\delta}_{i+1}\tilde{u}_i^T$. This approach reduces the dimension from $d^2$ to $2d$.

\input{figures/arch}

\paragraph{Model Architecture.}

The architecture of $f_{\theta_{W_i}}\left(u_i, \delta_{i+1}\right)$, named \method{}, is shown in Figure~\ref{fig:arch} and defined as follows:
\begin{align}
    \left[\bar{u}_i, \bar{\delta}_{i+1}\right]&=\mathrm{Norm}\left(\left[u_i, \delta_{i+1}\right]\right)\\
    h_i&=\theta_2\mathrm{Dropout}\left(\theta_1\left[\bar{u}_i, \bar{\delta}_{i+1}\right]\right)\\
    \left[\tilde{u}_i, \tilde{\delta}_{i+1}\right]&=h_i + \left[\bar{u}_i, \bar{\delta}_{i+1}\right]
\end{align}
where $\theta_1 \in \mathbb{R}^{r \times 2d}$ and $\theta_2 \in \mathbb{R}^{2d \times r}$ are the optimizer parameters with $r \ll d$. $[\cdot]$ denotes the vector concatenation. $\mathrm{Norm}$ normalizes $u_i$ and $\delta_{i+1}$ to have zero mean and unit variance separately. $\mathrm{Dropout}$ is the dropout regularization~\cite{DBLP:journals/jmlr/SrivastavaHKSS14}. In practice, $\theta_1$ and $\theta_2$ are shared across all weights with the same shape. \method{} is similar to the Bottleneck Adapter~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19}, with the key difference that its input is gradients and its output is the weight update. As such, \method{} can also be regarded as a specialized PEFT technique tailored to reasoning.

% \textcolor{red}{a two-column workflow diagram}

\input{tables/pilot}

\section{Experiments}
\label{sec:exp}

\subsection{Setup}

\paragraph{Datasets.}

We evaluate both baselines and our method on math and coding reasoning tasks:
(a) Mathematical reasoning: MATH~\cite{DBLP:conf/nips/HendrycksBKABTS21} and GSM8K~\cite{DBLP:journals/corr/abs-2110-14168}, using the test split from~\citet{DBLP:conf/iclr/LightmanKBEBLLS24} for MATH.
(b) Code generation: MBPP~\cite{DBLP:journals/corr/abs-2108-07732} and HumanEval~\cite{DBLP:journals/corr/abs-2107-03374}.
For all datasets, we report results on subsets where models fail with greedy decoding. We use Exact Match as the evaluation metric as well as the verifier for math tasks and Pass@1 for code.

\paragraph{Models.}

We conduct experiments with \texttt{Llama-3.1-8B-Instruct}~\cite{DBLP:journals/corr/abs-2407-21783} and \texttt{Mistral-7B-Instruct-v0.3}~\cite{DBLP:journals/corr/abs-2310-06825}.
We evaluate both models with zero-shot prompting and follow the official instructions when evaluating the model on each dataset\footnote{\url{https://huggingface.co/datasets/meta-llama/Llama-3.1-8B-Instruct-evals}}.

\paragraph{Baselines.}

We compare FTTT with the following test-time scaling methods:
\begin{itemize}[noitemsep, nolistsep]
    \item \textbf{Beam Search}~\cite{ow1988filtered} is a popular search algorithm that approximates the most confident model prediction.
    % \item \textbf{Self-Evaluation Guided Beam Search}~\cite{DBLP:conf/nips/XieKZZKHX23}
    % \item \textbf{Stochastic Beam Search}~\cite{DBLP:conf/icml/KoolHW19}
    % \item \textbf{Beam Search Sampling}~\cite{DBLP:journals/corr/abs-1910-03771} is a randomized version of beam search. Instead of keeping the top-$B$ candidates deterministically, it samples them stochastically to avoid similar predictions, where $B$ is the beam size. The best prediction is finally selected based on feedback.
    \item \textbf{Self-Consistency}~\cite{DBLP:conf/iclr/0002WSLCNCZ23} samples multiple predictions and selects the most frequent answer.
    \item \textbf{Best-of-N}~\cite{DBLP:journals/corr/abs-2407-21787} samples $N$ predictions independently and picks the best one based on external feedback.
    \item \textbf{Revision}~\cite{DBLP:journals/corr/abs-2408-03314} iteratively refines answers by conditioning the model on previous attempts.
    \item \textbf{Self-Refine}~\cite{DBLP:conf/nips/MadaanTGHGW0DPY23} alternates between self-critique and refinement. We select the best solution based on feedback.
\end{itemize}

\input{figures/scale}

For sampling-based methods, we use nucleus sampling~\cite{DBLP:conf/iclr/HoltzmanBDFC20} with a temperature of 0.6 and $p=0.95$, following~\citet{DBLP:journals/corr/abs-2407-21787}. All methods are allocated a budget of 32. For FTTT, we fine-tune the model with LoRA~\cite{DBLP:conf/iclr/HuSWALWWC22}, using a rank of 4 and a dropout ratio of 0.05. We use the Adam optimizer~\cite{DBLP:journals/corr/KingmaB14} with a learning rate of 1e-5, except for \texttt{Mistral-7B-Instruct-v0.3} in coding tasks, where we use 2e-5.

As \method{} is a specialized PEFT method, we compare it with the following PEFT approaches: \textbf{Adapter}~\cite{DBLP:conf/icml/HoulsbyGJMLGAG19}, \textbf{(IA)$^3$}~\cite{DBLP:conf/nips/LiuTMMHBR22}, \textbf{LoRA}~\cite{DBLP:conf/iclr/HuSWALWWC22} and \textbf{LN-Tuning}~\cite{DBLP:conf/iclr/ZhaoT0MX24}. We also include \textbf{full fine-tuning} that updates all LLM weights. For \method{}, $r=16$ and the dropout ratio is 0.1. We only apply \method{} to the query and value projections in the last two layers of the LLM. Detailed configurations are in Appendix~\ref{app:setting}.

% BitFit~\cite{ben-zaken-etal-2022-bitfit}

% ReFT~\cite{DBLP:journals/corr/abs-2404-03592}

% \textbf{Prefix-Tuning}~\cite{li-liang-2021-prefix}

\subsection{Training-Free Results}
\label{sec:result1}

Table~\ref{tab:pilot} compares FTTT with various baselines across four reasoning datasets. FTTT, both with and without self-reflected feedback, outperforms conventional test-time scaling methods on average. This success is partially explained by the findings of \citet{ye2024physicslanguagemodels22}, which show that training with error-correction data enhances reasoning capabilities and models do not retry during inference. FTTT is also efficient. For instance, the inference time of \texttt{Llama-3.1-8B-Instruct} on GSM8K with a budget of 32 is 3 GPU hours for the best parallel sampling method (Best-of-N) and 20 GPU hours for the best sequential revision method (Self-Refine). In contrast, FTTT achieves inference times of approximately 3 GPU hours without self-reflected feedback and 4 GPU hours with self-reflected feedback.

% the inference time of \texttt{Llama-3.1-8B-Instruct} with FTTT on GSM8K is 46 minutes on 4 NVIDIA H800, while for Best-of-N and Self-Refine, their inference time is 47 minutes and 5 hours

Notably, self-reflected feedback does not always improve results. Its effectiveness appears to depend on the LLM's self-reflection ability. To test this, we computed the Spearman rank correlation between FTTT and Self-Refine, a self-reflection-based algorithm. The Spearman coefficient ($r = 0.8656$, $p \leq 0.05$) indicates a strong positive correlation, supporting our hypothesis. We also observe that Self-Consistency performs poorly on code tasks because sampled code snippets rarely match exactly, making majority voting akin to random selection.

Figure~\ref{fig:scale} illustrates performance for FTTT and baselines under varying budgets. FTTT consistently outperforms baselines, with greater gains under constrained budgets. In contrast, Revision and Self-Consistency do not scale well. Revision struggles with long-context reasoning due to length generalization issues~\cite{li-etal-2024-making}, while Self-Consistency fails to leverage feedback, often discarding correct answers during majority voting due to long-tailed distributions of correct answers~\cite{DBLP:journals/corr/abs-2407-21787}.

% a line plot that shows the number of additional problems solved given more budgets

% the portion of problems solved in different difficulty levels in MATH

\subsection{Fine-Tuning Results}

\input{tables/meta}

\input{figures/finetune}

We present the results of PEFT baselines and \method{} with a budget of 32 in Table~\ref{tab:meta}. Best-of-N is applied to PEFT baselines to exploit test-time feedback. HumanEval is excluded as it lacks a training set. Table~\ref{tab:meta} highlights the effectiveness of \method{}, outperforming all PEFT baselines by at least 2.58\% on average. \method{} is also parameter-efficient, with 439K trainable parameters that are comparable to the most lightweight PEFT method (LN-Tuning, 266K parameters), while surpassing the best PEFT method (LoRA, 1.7M parameters) with an order of magnitude fewer parameters.
However, \method{} shows suboptimal performance on MATH for \texttt{Mistral-7B-Instruct-v0.3}, which is consistent with other PEFT methods with few trainable parameters (e.g., (IA)$^3$, LoRA, LN-Tuning). This is likely due to \texttt{Mistral-7B-Instruct-v0.3}'s limited mathematical reasoning capabilities, requiring significant parameter updates to improve performance in this domain.

\method{} incurs negligible inference overhead. For example, on GSM8K with \texttt{Llama-3.1-8B-Instruct} and a budget of 32, the best test-time scaling baseline (FTTT) requires 4 GPU hours, whereas \method{} uses only 1.5 GPU hours, benefiting from shorter yet accurate predictions.

% fastest adapter 6.5 GPU hours

\input{tables/example}

Finally, Figure~\ref{fig:tune-scale} examines the scaling behavior of PEFT baselines and \method{}. Initially, \method{} underperforms compared to other PEFT methods and FTTT, but it mostly achieves superior results when the budget exceeds 2. \method{}'s weaker performance with smaller budgets arises from its reliance on an initial attempt sampled from the raw LLM to initiate the process. This initial attempt often fails but is still counted as a valid attempt, making \method{} less competitive in low-budget settings.

\subsection{Analysis}

\paragraph{Ablation Study.}

Table~\ref{tab:ablation} presents an ablation study on the architecture design of \method{}. The results demonstrate that all components are essential, as removing any of them significantly degrades performance. Notably, normalization is the most critical component, as it addresses the varying gradient scales of different weights.

% how to combine predicted updates when scaling

\paragraph{Case Study.}

Table~\ref{tab:case} provides two examples on GSM8K where the leading PEFT method, LoRA, fails, but \method{} succeeds using \texttt{Llama-3.1-8B-Instruct}. These examples highlight \method{}'s superior ability to correctly interpret and reason through questions, unlike LoRA.

\input{tables/ablation}

% generalize to multiple-choice QA like GPQA

\section{Related Work}

\paragraph{Learning from Feedback.}

Other than the heuristic binary feedback studied in this work, prior research has explored feedback from various sources, such as humans~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, other models~\cite{yang-etal-2022-re3}, tools~\cite{DBLP:conf/nips/SchickDDRLHZCS23}, and knowledge bases~\cite{gao-etal-2023-rarr}. This paper focuses on demonstrating the effectiveness of the proposed method and other feedback types are beyond the scope of this paper.

\paragraph{Test-Time Training.}

Test-Time Training (TTT) has shown success in the image modality by addressing distribution shifts and enhancing model capacity through self-supervised fine-tuning on each test case~\cite{DBLP:conf/icml/SunWLMEH20,DBLP:conf/nips/LiuKDBMA21,DBLP:journals/corr/abs-2310-13807}. Recent studies have extended TTT to the text modality~\cite{DBLP:conf/iclr/Hardt024,DBLP:journals/corr/abs-2401-11504}. The most relevant work, by~\citet{akyürek2024surprisingeffectivenesstesttimetraining}, uses TTT to enhance the reasoning ability of LLMs. However, their method relies heavily on human scaffolding for self-supervision and does not generalize beyond ARC-AGI~\cite{chollet2019measureintelligence}. In contrast, FTTT is generally applicable.

\paragraph{Learning to Optimize.}

Learning to Optimize (L2O) trains a network to act as an optimizer for another network~\cite{DBLP:journals/jmlr/ChenCCH0WY22}. Early approaches used reinforcement learning to train such optimizers~\cite{DBLP:conf/iclr/LiM17,DBLP:conf/icml/ChenHCDLBF17}, while recent work focuses on discovering analytical white-box optimizers~\cite{DBLP:conf/icml/BelloZVL17,DBLP:conf/nips/ChenLHRW0DLHLL23}. The most relevant work, MEND~\cite{DBLP:conf/iclr/MitchellLBFM22}, trains a network to predict weight updates from training gradients. \method{} builds on this idea, extending it to learn from test-time feedback with a distinct architecture.

\section{Conclusion}

In this paper, we propose a novel paradigm that leverages optimization to address the challenge of exploiting test-time feedback, resulting in improved scaling performance. We further present a learnable test-time optimizer, \method{}, which surpasses various PEFT baselines. Both FTTT and \method{} are efficient in terms of speed and trainable parameter count.

\section*{Limitations}

The current evaluation setting limits FTTT's potential by providing only binary feedback (i.e., correct or incorrect) for each attempt. However, developing complex reasoning environments with rich feedback is beyond the scope of this work. Additionally, while continuous feedback, such as that from reward models~\cite{DBLP:journals/corr/abs-2409-12122}, has been extensively studied, it is not examined here. Our method can be adapted to continuous feedback with minimal modifications, such as using REINFORCE~\cite{DBLP:journals/ml/Williams92}. For coherence, we leave this exploration to future work.

% \section*{Acknowledgments}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Hyperparameter Settings}
\label{app:setting}

Below is the detailed configurations of different fine-tuning methods:
\begin{itemize}[noitemsep, nolistsep]
    \item \textbf{Adapter} uses a learning rate of 1e-4 and the reduction factor of the bottleneck is 16.
    % \item \textbf{Prefix-Tuning} uses a learning rate of 2e-4 for \texttt{Llama-3.1-8B-Instruct} and 5e-5 \texttt{Mistral-7B-Instruct-v0.3} and the prefix length is 10.
    \item \textbf{(IA)$^3$} uses a learning rate of 5e-5.
    \item \textbf{LoRA} uses a learning rate of 2e-5. We only apply LoRA to the query and value projections in the last 8 layers, with a rank of 16 and a dropout ratio of 0.05.
    \item \textbf{LN-Tuning} uses a learning rate of 4e-4.
    \item \textbf{Full Fine-Tuning} uses a learning rate of 1e-5.
\end{itemize}
The number of training epochs is 100, 10, and 3 for MBPP, GSM8K, and MATH respectively. We use the Adam optimizer with a batch size of 20 for all methods in all datasets, including \method{}.

For \method{}, we sample 10 attempts for each training example together with the raw question to construct the model input. We employ nucleus sampling~\cite{DBLP:conf/iclr/HoltzmanBDFC20} with a temperature of 0.6 and $p=0.95$ to generate attempts. The number of training epochs for MBPP, GSM8K, and MATH is set to 10, 3, and 3 respectively. The learning rate is 1e-5. In inference, we sample an attempt using the same hyperparameters as in data generation before applying \method{} to mitigate the train-test discrepancy. We alternate between sampling attempts from the raw LLM and predicting refined attempts from sampled attempts when scaling \method{} with more budgets.

\input{figures/learning}

\section{Prompts}
\label{sec:prompt}

Below is the reflection generation prompts $P$ for \texttt{Llama-3.1-8B-Instruct} and \texttt{Mistral-7B-Instruct-v0.3}:

\patchcmd{\quote}{\rightmargin}{\leftmargin 15pt \rightmargin}{}{}
\begin{quote}
\small %\it
\begin{tcolorbox}[breakable, colback=white, colbacktitle=blue!5!white, colframe=black, boxrule=1pt, title={\textcolor{black}{\textbf{Llama-3.1-8B-Instruct}}}]
\textbf{User:} Solve the following math problem $\ldots$\\
\textbf{Assistant:} $\ldots$ the final answer is: $\ldots$\\
\textbf{User:} Your answer is incorrect. \textcolor{gray}{Please carefully check the solution and summarize all mistakes in short. Do NOT provide the corrected solution. Do NOT say ``my solution''.}\\
\textbf{Assistant:} \textcolor{royalblue}{Here is the summary of the mistakes in the previous solution $\ldots$}
\end{tcolorbox}
\end{quote}
\patchcmd{\quote}{\rightmargin}{\leftmargin 26pt \rightmargin}{}{}

\patchcmd{\quote}{\rightmargin}{\leftmargin 15pt \rightmargin}{}{}
\begin{quote}
\small %\it
\begin{tcolorbox}[breakable, colback=white, colbacktitle=blue!5!white, colframe=black, boxrule=1pt, title={\textcolor{black}{\textbf{Mistral-7B-Instruct-v0.3}}}]
\textbf{User:} Solve the following math problem $\ldots$\\
\textbf{Assistant:} $\ldots$ the final answer is: $\ldots$\\
\textbf{User:} Your answer is incorrect. \textcolor{gray}{Carefully check the solution step-by-step and list all mistakes in short. MUST NOT provide the correct answer. Your response MUST be in the third person tone..}\\
\textbf{Assistant:} \textcolor{royalblue}{Here is the summary of the mistakes in the previous solution $\ldots$}
\end{tcolorbox}
\end{quote}
\patchcmd{\quote}{\rightmargin}{\leftmargin 26pt \rightmargin}{}{}
Sentences in \textcolor{gray}{gray} are the prompt and the one in \textcolor{royalblue}{blue} is the generated reflection.

\section{Additional Results}

Figure~\ref{fig:learning} is the training curves of various PEFT methods, including \method{}. We observe that Adapter shows a clear signal of overfitting, where it has a training loss close to 0, while its performance on the test set is the worst. All PEFT methods seem to converge smoothly. For \method{}, its training is not as stable as baselines, suggesting the difficulty of learning to optimize problem.

\end{document}
