\textbf{Overview for convex loss}

In Section~\ref{sec:AppendixConvProof}, we are interested in the convergence of Option 1 when the objective functional $F$ is convex in a distributional sense:
\begin{equation}
    F(q) \geq F(q') + \int \dFdq(q')\mathrm{d}(q-q'), \quad \text{for any $q,q'\in\mathcal{P}$}.
\end{equation}
Then, the regularized objective $L(q) = F(q) + \beta \KL(q\|\pref)$ becomes strongly convex with $\beta>0$.
Formally, we assume that
\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]
    \item[(i)] $\dFdq$ is bounded: There exists $B_F > 0$ such that $\|\dFdq(q)\|_\infty \leq B_F$ for any $q \in \mathcal{P}$,
    \item[(ii)] $\dFdq$ is Lipshitz continuous with respect to the TV distance: There exists $L_{\mathrm{TV}}  > 0$ such that $\|\dFdq(q) - \dFdq(q')\|_\infty \leq L_\mathrm{TV} \TV(q,q')$ for any $q,q' \in \mathcal{P}$.
    \item[(iii)] $F$ is convex: $F(q) \geq F(q') + \int \dFdq(q')\mathrm{d}(q-q')$ for any $q,q' \in \mathcal{P}$,
\end{enumerate}
In the update of Option 1, we iteratively use the distributional proximal operator. We minimize
\begin{equation}
\mathrm{E}_{q}\left[\sum_{j=1}^{k}\frac{2j}{k (k+1)} g^{(j)}\right] + \beta \KL(q\|\pref)  
+ \beta' \frac{2}{k}\KL(q\|\pref) \label{eq:appendixA-summary-min}
\end{equation}
by the minimizer $\qstark$, where  $\gk$ denotes $\dFdq(\qk)$ with $\qk \simeq \qstark$ ($\qk$ is constructed by neural networks and samples from $\pref$, while we do not directly compute $\qk$.). Intuitively, with $k \to \infty$, we hope that $q$ is almost converged around the minimizer $\qstar$, i.e. $q \simeq \qstar$. Then, from the differentiability of $F$, each $\E_q[g^{(j)}]$ in the first term 
would be a linear functional that well approximates $F(q^{(j)})$, with suficiently large $j$. In addition, $\beta' \frac{2}{k}\KL(q\|\pref)$ vanishes, so the equation (\ref{eq:appendixA-summary-min}) is roughly written as
\begin{align}
    &\sum_{j=1}^{k}\frac{2j}{k (k+1)} \left[ \mathrm{E}_{q}\left[g^{(j)}\right] + \beta \KL(q\|\pref) \right] 
+ \beta' \frac{2}{k}\KL(q\|\pref) \\
\simeq &\sum_{j=1}^{k}\frac{2j}{k (k+1)} (F(\qj) +\beta \KL(q\|\pref))
+ \beta' \frac{2}{k}\KL(q\|\pref)\\
\simeq &\sum_{j=1}^{k}\frac{2j}{k (k+1)} (F(\qstarj) +\beta \KL(\qstarj\|\pref)).
\end{align}
This is the weighted average of the regularized losses $L(q) = F(q) + \beta \KL(q\|\pref)$.
From this concept, we will show that the convergence represented as
\begin{equation}
    \left[ \text{Weighted average of } L(\qstark) \right] = \left[ \text{Weighted average of } L(\qstar) \right]+ \mathcal{O}\left(\frac{1}{k}\right).
\end{equation}

\textbf{Overview for nonconvex loss}

In Section~\ref{section:da-nonconvex-proof}, we are interested in the case that $F$ is not necessarily convex (mainly) in Option 2. 
Alternatively, we use the assumption that $F$ is smooth in terms of KL-divergence:
\begin{itemize}
\item[(ii)'] (The weaker smoothness derived from (ii)). There exists $S_F \geq 0$ such that 
\begin{equation}
    F(q) \leq F(q') + \int \dFdq(q')\mathrm{d}(q-q') + \frac{S_F}{2}\KL(q\|q') \quad \text{for any}\quad  q,q' \in \gP, \label{eq:appendix-da-nonconvex-overview-smoothness}
\end{equation}
\end{itemize}
This is induced by Lipschitz continuity of $\dFdq$ ((ii) of Assumption~\ref{ass:ConvexF} and ~\ref{ass:NonconvexF}) and Pinsker's inequality. When the inner-loop error is ignored, it is possible to prove convergence using only the smoothness (\ref{eq:appendix-da-nonconvex-overview-smoothness}) instead of Lipschitz continuity of $\dFdq$.
Please note that KL-divergence would be the ``quadratic term" in the ordinary definition of  smoothness.
Following the theoretical analysis of standard nonconvex optimization, our goal is to show that the ``derivative" of the (regularized) objective $L(q)$ goes to zero in the form of functional derivative:
\begin{equation}
    \frac{\delta L}{\delta q}(\qstark,x) \to 0 \quad \text{(up to constant w.r.t. $x$.)}
\end{equation}
Please note that we can ignore $\frac{\delta L}{\delta q}(\qstark,x)$ if it is a constant because of the definition of the functional derivatives. 
% 

At the first step, both in Option 1 and 2, the regularized objective $L(q)$ (roughly) monotonically decrease during the Dual Averaging. Ignoring some terms, we want to show that
\begin{equation}
    L(\qstarnext) - L(\qstark) \lesssim - \KL(\qstark\|\qstarnext) < 0.
\end{equation}
Next, by taking a telescoping sum, we obtain that
\begin{equation}
    \frac{1}{K}\sum_{k=1}^K (\text{weight})_k \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right).
\end{equation}
From the above equation, it is immediately shown that
\begin{equation}
    \min_{k=1,...,K} (\text{weight})_k \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right),
\end{equation}
where $(\text{weight})_k \simeq k$ in the Option 1, while $(\text{weight})_k \simeq 1$ in the Option 2.
Therefore, when we choose the Option 2, it holds that
\begin{equation}
    \min_{k=1,...,K} \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right)
\end{equation}
Finally, we get the conclusion $\frac{\delta L(\qstark)}{\delta q} \to 0\; \text{(up to constant w.r.t. $x$)}$ because $\KL(\qstark\|\qstarnext)$ is the approximate ``moment generation function" of $\frac{\delta L(\qstark)}{\delta q}$; when $k \to \infty$,
\begin{equation}
    \min_{k=1,..,K}\left(\text{``Variance" of }\frac{\delta L(\qstark)}{\delta q} \right) \simeq \min_{k=1,..,K} \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right).
\end{equation}
This result aligns with the result for standard, non-distributinal Dual Averaging~\citep{LIU2023nonconvexDA}, $\min_{k=1,...,K}\|\text{(gradient of the objective)}_k\|^2 = \mathcal{O}(1/K)$ because the variance is the second moment.

\subsection{When $F$ is convex\label{section:da-convex-proof}}\label{sec:AppendixConvProof}
% We provide the convergence analysis done in \textcolor{red}{cite} for completeness. Note that there are minor changes in the results from prior works.

We will show that, when $F$ is convex in the distributional sense, the convergence of the Option 1 can be written as 
\begin{equation}
    \left[ \text{Weighted average of } L(\qstark) \right] - \left[ \text{Weighted average of } L(\qstar) \right] = \mathcal{O}\left(\frac{1}{K}\right).
\end{equation}
We require $\beta$ to be positive to make the regularized objective $F(q) + \beta\KL(q\|\pref)$ strongly convex.
In addition, one of the necessary conditions $\beta' \geq \beta$ implies that the ``learning rate" should be controlled to converge.
\begin{thm*}[restated - Theorem~\ref{thm-da-nishikawa}]
Assume that $F$ is convex, $|\dFdq|\leq B_F$, $\dFdq$ is $\LipTV$-Lipshitz with respect to $q$ in TV distance, $\TV(\qstark,\qk)\leq \epsilon_\TV$, and $\beta' \geq \beta$. Then,
    \begin{align}
    &\frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ F(\qstark) + \beta \KL(\qstark\|\pref) - F(\qstar) - \beta \KL(\qstar\|\pref)\right]\\
    \leq&\frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ 2 \LipTV\epsilon_\TV +\E_{\qstark}[g^{(k)}] - \E_{\qstar}[g^{(k)}] + \beta (\KL(\qstark\|\pref) - \KL(\qstar\|\pref))\right]\\
    \leq&2\LipTV\epsilon_{\mathrm{FD}}+\frac{2}{K(K+1)}\left[B_F + \beta(K+3)\KL(\qstar\|\pref) + \E_{\qstar}[g^{(1)}] + \frac{B_F^2 K}{\beta}\right].
  \end{align}
%    where
%  \begin{equation}
%    \epsilon_{\mathrm{FD}} = \frac{2}{K(K+1)}\sum_{k=1}^{K} k \epsilon_\TV.
%  \end{equation}
\end{thm*}

First, we want to show that the $\KL$ regularization term plays a role of the ``quadratic" regularization, which makes the regularized objective $F(q) + \beta\KL(q\|\pref)$ strongly convex.
We observe that the sum of the linear (convex and concave) objective and the $\KL$ regularization term
\begin{equation}
    \tilde{F}(q) = \mathrm{E}_q[r(x)] + \beta \KL(q\|p)
\end{equation}
is strongly convex in the distributional sense. In particular, if $q$ is the minimizer of $\tilde{F}$, then
\begin{equation}
    \tilde{F}(q) + \frac{\beta}{2}\mathrm{TV}(q,q')^2 \leq \tilde{F}(q') \text{  for all  } q' \in \gP.
\end{equation}
This result is useful to prove Lemma~\ref{lem:da-convex-vkprev-vk}, which controls the regularized and linearized objective $\E_q[\dFdq] + \beta \KL(q\|\pref)$:

\begin{lem}[\cite{NEURIPS2021_a34e1ddb}]
    \label{lem:da-convex}
  $\tilde{F}:\mathcal{P}\to \mathbb{R}$, $\tilde{F}(q) = \mathrm{E}_q[r(x)] + \beta \KL(q\|p)$.
  Assume that 
  $r$ is bounded. We put $r_1:=\frac{dq}{dp}, \; r_2' := \frac{dq'}{dp}$,
  \begin{equation}
    \tilde{F}(q) \leq \tilde{F}(q') + \mathrm{E}_q[(r_1-r_2)(r(x) + \beta \log r_1)] - \frac{\beta}{2}\|r_1-r_2\|^2_{L^1(p)}.
  \end{equation}
  Especially, if $q \propto \exp (-\frac{r}{\beta})p$, 
  \begin{equation}
    \tilde{F}(q) \leq \tilde{F}(q') - \frac{\beta}{2}\|r_1-r_2\|^2_{L^1(p)}.
  \end{equation}
\end{lem}
\begin{proof}
Omitted. See \cite{NEURIPS2021_a34e1ddb}. We use Pinsker's inequality
$\mathrm{TV}(q,q')^2 \lesssim \KL(q\|q')$.
\end{proof}
Given that $F(q) + \beta\KL(q\|\pref)$ is strongly convex, let us prove Theorem~\ref{thm-da-nishikawa}:
\begin{proof}
  The main parts of the proof follow existing papers~\citep{NEURIPS2021_a34e1ddb, nishikawa2022twolayer}. The key difference lies in that we are not using Langevin sampler in the inner loop, which slightly changes how the error of $\qk$ is handled. 

  We begin by linearizing the weighted sum of the losses.
  $\gk$ denotes $\dFdq(\qk)$, and $\qstar$ denotes the minimizer of $F$.
  From the convexity of $F$,
  \begin{align}
    &\frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ F(\qstark) + \beta \KL(\qstark\|\pref) - F(\qstar) - \beta \KL(\qstar\|\pref)\right]\\
    \leq& \frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ \int \frac{\delta F}{\delta q}(\qstark) d(\qstark - \qstar)  + \beta (\KL(\qstark\|\pref) - \KL(\qstar\|\pref))\right]\\
    \leq& \frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ \left| \int \frac{\delta F}{\delta q}(\qstark) - \frac{\delta F}{\delta q}(\qk) d(\qstark - \qstar)\right|+\E_{\qstark}[g^{(k)}] - \E_{\qstar}[g^{(k)}]\right. \\
    &\left.+ \beta (\KL(\qstark\|\pref) - \KL (\qstar\|\pref))\right].
  \end{align}
  Here, we bound the estimation error of $\gk = \dFdq(\qk)$ using the Lipschitz continuity of $\dFdq$ with respect to $q$:
  \begin{align}
    &\left| \int \frac{\delta F}{\delta q}(\qstark) - \gk d(\qstark - \qstar)\right| \\
    \leq& \TV(\qstark, \qstar) \sup_x \left|\frac{\delta F}{\delta q}(\qstark,x) - \gk(x) \right|\\
    \leq& 2L_\mathrm{q}\epsilon_\TV.
  \end{align}
  Then we obtain
  \begin{align}
      &\frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ F(\qstark) + \beta \KL(\qstark\|\pref) - F(\qstar) - \beta \KL(\qstar\|\pref)\right]\\
      \leq& \frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[2L_\mathrm{q}\epsilon_\TV +\E_{\qstark}[g^{(k)}] - \E_{\qstar}[g^{(k)}]\right. \\
      &\left.+ \beta (\KL(\qstark \|\pref) - \KL (\qstar\|\pref))\right].\label{eq:appendix-da-ineq-weighted-sum}
  \end{align}
  This implies that it is sufficient to bound the weighted sum of the (regularized) linearized objectives $\E_{\qstark}[g^{(k)}] + \beta (\KL(\qstark \|\pref), \; k=1,...,K$.
  
  In each update of Option 1, $\qstarnext$ is obtained by maximizing 
  \begin{equation}
    V_{k+1}(q) = - \mathrm{E}_{q}\left[\sum_{j=1}^{k}j g^{(j)}\right] - \frac{\beta k (k+1)}{2}\KL(q\|\pref)  
    - \beta' (k+1)\KL(q\|\pref). 
  \end{equation}
  %by $\qstarnext$ in each update. 
  We also define
  \begin{equation}
    r_{*}^{(k+1)} = \frac{\mathrm{d} \qstarnext}{\mathrm{d}\pref}, \quad V_{k+1}^*= V_{k+1}(\qstarnext).
   \end{equation}

  Then, we can show that $V^*_k$ has the following recursive relation.
  Lemma~\ref{lem:da-convex-vkprev-vk} approximately implies
  \begin{equation}
      k \E_{\qstark}[g^{(k)}] + k \beta \KL(\qstarnext\|\pref)) \lesssim V_{k}^* - V_{k+1}^* + \mathcal{O}(1),
  \end{equation}
  then, summing from $k=1$ to $K$, we roughly obtain,
  \begin{equation}
      \frac{2}{K(K+1)}\left[ \sum_{k=1}^K k(\E_{\qstark}[g^{(k)}] + \beta \KL(\qstark\|\pref)) + V_{K+1}^* \right]\lesssim \mathcal{O}\left(\frac{1}{K}\right).\label{eq:appendix-da-lem2-implication}
  \end{equation}
  So, as long as we prove Lemma~\ref{lem:da-convex-vkprev-vk}, we can connect this inequality (\ref{eq:appendix-da-lem2-implication}) to the inequality (\ref{eq:appendix-da-ineq-weighted-sum}) because $V_{K+1}^* \gtrsim -\sum_{k=1}^K k(\E_{\qstar}[g^{(k)}] + \beta \KL(\qstar\|\pref))$:
   
  \begin{lem}
    \label{lem:da-convex-vkprev-vk}
      For $k\geq 1$,
      \begin{equation}
        V_{k+1}^* \leq V_{k}^* - k\E_{\qstark}[g^{(k)}] - (\beta k+\beta')\KL(\qstarnext\|\pref) + \frac{B_F^2k}{\beta (k-1) + 2 \beta'}.\label{eq:da-convex-lem-2}
      \end{equation}
  \end{lem}
  For the proof of Lemma~\ref{lem:da-convex-vkprev-vk}, please refer to Section~\ref{sec:da-convex-lem}, in which we use Lemma~\ref{lem:da-convex}.
  % \begin{rem*}[Interpretation of Lemma~\ref{lem:da-convex-vkprev-vk}]

%\end{rem*}

  From here, we will rigorously explain the result derived from Lemma~\ref{lem:da-convex-vkprev-vk}.
  When $k=1$, because $\|\gk\|_\infty\leq B_F$,
  \begin{equation}
    V_1^* \leq B_F - \beta \KL(\hat{q}^{(1)}\|\pref).
  \end{equation}
   By taking a telescoping sum of (\ref{eq:da-convex-lem-2}),
  \begin{align}
    V_{K+1}^* \leq&B_F - \sum_{k=1}^{K} k \left(\E_{\qstark}[g^{(k)}] + \beta \KL(\qstark\|\pref)\right) - [\beta (K+1)+ \beta']\KL(\hat{q}^{(k+2)}\|\pref)\\
    &+ \sum_{k=1}^{K}\frac{B_F^2k}{\beta(k-1) + 2\beta'}\\
    \leq&B_F - \sum_{k=1}^{K} k \left(\E_{\qstark}[g^{(k)}] + \beta \KL(\qstark\|\pref)\right) + \frac{B_F^2 K}{\beta},\label{eq:da-convex-vk-upperbound}
  \end{align}
  where we used $\beta' \geq \beta$ in the last inequality. 
  On the other hand, we see that
  \begin{align}
    V_{K+1}^* \geq& - \E_{\qstar}\left[\sum_{k=1}^{K}kg^{(k)}\right] - \frac{\beta K(K+1) + 2\beta' (K+1)}{2}\KL(\qstar\|\pref)\\
    =& - \E_{\qstar}\left[\sum_{k=1}^{K}k (g^{(k)} + \beta \KL(\qstar\|\pref))\right] - \beta'(K+1)\KL(\qstar\|\pref)  \label{eq:da-convex-vk-lowerbound}
  \end{align}
  because $V_{K+2}^*$ is the maximal value. Combining the upper bound (Eq.~\ref{eq:da-convex-vk-upperbound}) and the lower bound (Eq.~\ref{eq:da-convex-vk-lowerbound}),
  \begin{align}
    &\sum_{k=1}^{K}k\left(\E_{\qstark}[g^{(k)}] - \E_{\qstar}[g^{(k)}] + \beta (\KL(\qstark\|\pref)-\KL(\qstar\|\pref))\right)\\
    \leq& B_F + \beta' (K+1)\KL(\qstar\|\pref) + \frac{B_F^2 K}{\beta}.
  \end{align}
  Finally we get the convergence rate:
  \begin{align}
    &\frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ F(\qstark) + \beta \KL(\qstark\|\pref) - F(\qstar) - \beta \KL(\qstar\|\pref)\right]\\
    \leq&\frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ 2 \LipTV\epsilon_\TV +\E_{\qstark}[g^{(k)}] - \E_{\qstar}[g^{(k)}] + \beta (\KL(\qstark\|\pref) - \KL(\qstar\|\pref))\right]\\
    \leq&2\LipTV\epsilon_{\mathrm{FD}}+\frac{2}{K(K+1)}\left[B_F + \beta'(K+1)\KL(\qstar\|\pref)  + \frac{ B_F^2 K}{\beta}\right].
  \end{align}
  %   where
  % \begin{equation}
  %   \epsilon_{\mathrm{FD}} = \frac{2}{K(K+1)}\sum_{k=1}^{K} k \epsilon_\TV. 
  % \end{equation}
  This concludes the assertion. 
\end{proof}

\subsubsection{Proof of Lemma~\ref{lem:da-convex-vkprev-vk}\label{sec:da-convex-lem}}\label{sec:AppendixNonconvexConv}
   \begin{proof}
      We can calculate the relation between $V_{k+1}^* = V_{k+1}(\qstarnext)$ and $V_{k}(\qstarnext)$ as
      \begin{align}
        V_{k+1}^* =& - \E_{\qstarnext}\left[\sum_{j=1}^{k}j g^{(j)}\right] - \frac{(k+1)( \beta k + 2 \beta') }{2}\KL(\qstarnext \|\pref)\\
        =& - \E_{\qstarnext}\left[\sum_{j=1}^{k-1}j g^{(j)}\right] - \frac{\beta k (k-1)}{2}\KL(\qstarnext \|\pref) 
        - \beta' k \KL(\qstarnext\|\pref)
        \\
        &- k\E_{\qstarnext}[g^{(k)}]  - (\beta k + \beta') \KL(\qstarnext \|\pref) \\
        = & V_{k}(\qstarnext) - k \E_{\qstark}[g^{(k)}]+ k(\E_{\qstark}[g^{(k)}]-\E_{\qstarnext}[g^{(k)}]) - (\beta k+\beta')\KL(\qstarnext\|\pref),\label{eq:da-convex-vkp1-and-vk-0}
      \end{align}
where we used the definitions of $V_k$ and $V_{k+1}$.
Next, we upper bound the RHS of \Eqref{eq:da-convex-vkp1-and-vk-0} using Lemma~\ref{lem:da-convex} about the convexity of $\KL$:
      \begin{align}
        &(\text{RHS of \Eqref{eq:da-convex-vkp1-and-vk-0}})\\
        \leq& V_{k}^* - \frac{\beta k (k-1) + 2 \beta' k }{4}\|\rstarknext - \rstark\|_{L^1(\pref)}^2
        %\KL(\qstarnext \| \qstark ) 
        -k \E_{\qstark}[g^{(k)}]+ k(\E_{\qstark}[g^{(k)}]-\E_{\qstarnext}[g^{(k)}])\nonumber \\
        &- ( \beta k+\beta')\KL(\qstarnext\pref)\quad (\because \text{the second equation in Lemma~\ref{lem:da-convex} and the optimality of $V_k^*$})\\
        \leq& V_{k}^* -  \frac{\beta k (k-1) + 2 \beta' k }{4}\|\rstarknext - \rstark\|_{L^1(\pref)}^2  -k \E_{\qstark}[g^{(k)}]+ k\left|\E_{\qstark}[g^{(k)}]-\E_{\qstarnext}[g^{(k)}]\right|\nonumber  \\
        &- \beta(k+1)\KL(\qstarnext\|\pref)\\
        \leq& V_{k}^* - \frac{\beta k (k-1) + 2 \beta' k }{4}\|\rstarknext - \rstark\|_{L^1(\pref)}^2 -k \E_{\qstark}[g^{(k)}]+ B_F k\|\rstarknext - \rstark\|_{L^1(\pref)}\nonumber \\
        &- \beta(k+1)\KL(p_*^{(t+1)}\|q)\; \quad (\because \|g^{(k)}\|_\infty \leq B_F) \\
        \leq& V_{k}^* - k\E_{\qstark}[g^{(k)}] - \beta(k+1)\KL(\qstarnext\|\pref) + \frac{B_F^2k}{\beta (k-1) + 2\beta' }, %(\because\text{completing the square})
      \end{align}
      where we used the arithmetic-geometric mean inequality in the last inequality. 
      This concludes the proof. 
 % Then, we conclude the proof of the lemma.
  \end{proof}

\subsection{Convergence proof for non-convex loss $F$\label{section:da-nonconvex-proof}}

%We partially used the proof techniques in ~\cite{LIU2023nonconvexDA}.

Here, we give the proof of Theorem \ref{thm:NonconvexConv} and Corollary \ref{cor:DualConv}. In this section, we always assume Assumption \ref{ass:NonconvexF} holds. We are mainly interested in the property that $F$ is smooth with respect to KL-divergence instead of convexity:
\begin{enumerate}
\item[(ii)'] (A weaker version of (ii)). There exists $S_F \geq 0$ such that 
\begin{equation}
    F(q) \leq F(q') + \int \dFdq(q')\mathrm{d}(q-q') + \frac{S_F}{2}\KL(q\|q') \quad \text{for any}\quad  q,q' \in \gP. \label{eq:appendix-da-nonconvex-weak-smoothness}
\end{equation}
\end{enumerate}
This property can be derived from (ii) in Assumption~\ref{ass:NonconvexF} that is Lipschitz continuity of $\dFdq$:
\begin{lem}
    Assume that $\dFdq$ is Lipshitz continuous with respect to the TV distance: There exists $L_{\mathrm{TV}}  > 0$ such that $\|\dFdq(q) - \dFdq(q')\|_\infty \leq L_\mathrm{TV} \TV(q,q')$ for any $q,q' \in \mathcal{P}$. 
    Then, 
    \begin{equation}
        F(q) \leq F(q') + \int \dFdq(q')\mathrm{d}(q-q') + \frac{\LipTV}{2}\mathrm{TV}(q,q')^2 \quad \text{for any}\quad  q,q' \in \gP.
    \end{equation}
    From Pinsker's inequality,
    \begin{equation}
        F(q) \leq F(q') + \int \dFdq(q')\mathrm{d}(q-q') + \LipTV \KL(q\|q') \quad \text{for any}\quad  q,q' \in \gP.
    \end{equation}
\end{lem}
\begin{proof}
    The proof is almost identical to the proof of $\LipTV$-smoothness commonly discussed in the context of standard optimization.
    For $q,q'\in\gP$, we define $q_t = q' + t (q-q') \in \gP$. Then,
    \begin{align}
        &F(q) - F(q') - \int \dFdq(q')\mathrm{d}(q-q')\\
        = & \int_{t=0}^{t=1}  \int \dFdq(q_t) \mathrm{d}(q-q') \mathrm{d}t
        - \int \dFdq(q')\mathrm{d}(q-q')\\
        = & \int_{t=0}^{t=1}  \int \left[ \dFdq(q_t) - \dFdq(q') \right] \mathrm{d}(q-q') \mathrm{d}t\\
        \leq &  \int_{t=0}^{t=1} t \LipTV \mathrm{TV}(q,q')^2 \mathrm{d}t \quad (\text{$\LipTV$-Lipshitz continuity of $\dFdq$})\\
        = & \frac{\LipTV}{2}\mathrm{TV}(q,q')^2,
    \end{align}
    while we used the fundamental theorem of calculus in the first equation and the assumption in the inequality.
\end{proof}

We emphasize that when the inner-loop error is ignored, it is possible to prove convergence using only the smoothness (\ref{eq:appendix-da-nonconvex-weak-smoothness}) instead of (ii) in Assumption~\ref{ass:NonconvexF}. 
Therefore, we will use the notation $S_F$ for the parts that can be derived using Assumption (ii)' instead of Assumption (ii).

Under the assumptions, our goal is to show that the functional derivative of $L(q)=F(q)+\beta\KL(q\|\pref)$ goes to a constant:
\begin{equation}
    \frac{\delta L}{\delta q} \to 0 \quad (\text{up to constant w.r.t. $x$})
\end{equation}

We prepare the following Lemma about the convexity of $\KL$:
\begin{lem}
    \label{lem-da-nonconvex-nitanda}
    The following equations hold:
    \begin{itemize}
        \item[\textup{(i)}]
        $\KL(q\|p) = \KL(q'\|p) + \int \frac{\delta}{\delta q}\KL(q'\|p) \mathrm{d}(q-q') + \KL(q\|q')$,
        \item[\textup{(ii)}]
        Let $r: \mathbb{R}^d \to \mathbb{R}$ be a smooth function.
        For $\tilde{F}(q)=\E_q[r(x)] + \KL(q\|p), \; q' = \exp(-r)q$, it holds that
        \begin{equation}
            \tilde{F}(q)=\tilde{F}(q') + \KL(q\|q').
        \end{equation}
        $\tilde{F}(q)$ is uniquely minimized at $q=q'$.
    \end{itemize}
\end{lem}
We omit its proof because it is straight forward. 
Lemma~\ref{lem-da-nonconvex-nitanda} implies that $\KL$ also plays a important role as a ``quadratic" penalty term of the proximal operator whose output is 
\begin{equation}
    \underset{q\in \gP}{\mathrm{argmin}} \left\lbrace H(q) + \KL(q\|\pref) \right\rbrace, \text{  for any functional $H$}.
\end{equation}
This property allows for the use of standard nonconvex convergence analysis of Dual Averaging based on the proximal operator~\citep{LIU2023nonconvexDA}.

Roughly speaking, the intermediate goal is to show that $L(q)$ monotonically decreases in each $k$th iteration:
\begin{equation}
    L(\qstarnext) - L(\qstark) \lesssim 0.
\end{equation}
By the weaker smoothness of $F$ (Eq.(\ref{eq:appendix-da-nonconvex-weak-smoothness})), the left hand side is approximately bounded as
\begin{align}
    L(\qstarnext) - L(\qstark) \lesssim& \int \dFdq(\qstark)\mathrm{d}(\qstarnext - \qstark) + \KL(\qstarnext\|\qstark)\\
    \lesssim& \int \dLdq(\qstark)\mathrm{d}(\qstarnext - \qstark)+ \KL(\qstarnext\|\qstark),\label{eq-da-nonconvex-motivation-lem-1}
\end{align}
ignoring various minor terms and constants. To bound the right hand side in (\ref{eq-da-nonconvex-motivation-lem-1}), we show the following inequality regarding the Option 2 using Lemma~\ref{lem-da-nonconvex-nitanda}. 
\begin{lem}
    \label{lem-da-nonconvex-1}
    Assume that $\TV(\qstark, \qk) \leq \epsilon_\TV/2$ for all $k$. Then, the Option 2 achieves 
    \begin{align}
        &\int \dLdq(\qk)\mathrm{d}(\qstarnext - \qstark)\\
        \leq & B_F\epsilon_\TV + \frac{\beta'}{k}\left(\KL(\qstark\|\pref)-\KL(\qstarnext\|\pref)-k\KL(\qstarnext\|\qstark)
        - (k+1)\KL(\qstark \| \qstarnext) \right)
    \end{align}
\end{lem}
\begin{proof}
    In the Option 2, $\qstarnext$ minimizes
    \begin{equation}
        r_{k+1}(q) \coloneq \sum_{j=1}^k \int  j \dLdq(\qj)\mathrm{d}(q-\qstarj)
        + \beta' (k+1) \KL(q\|\pref).
    \end{equation}
    We interpret that Option 2 computes the proximal operator of the weighted sum of $\dLdq$ and this concept is justified by Lemma~\ref{lem-da-nonconvex-nitanda}.
    By the definition of $r_k$ and $r_{k+1}$, it holds that 
    \begin{equation}
        r_{k+1}(q) = r_k(q) + \int k \dLdq(\qk) \mathrm{d}(q-\qstark)
        +\beta' \KL(q\|\pref).\label{eq-da-noncovex-r_knext}
    \end{equation}
    From Lemma~\ref{lem-da-nonconvex-nitanda}, we also have 
    \begin{equation}\label{eq:rkupdateDiff}
        r_k(q) - r_k(\qstark) = \beta' k \KL(q\|\qstark), \text{  for all $q \in \gP$}
    \end{equation}
    because $r_k(q)$ is just the sum of the linear functional of $q$ and the KL-divergence ignoring the constant. 
    Letting $q = \qstarnext$ and we obtain
    \begin{align}\label{eq:rkinductionFirstSide}
        0 \leq& r_k(\qstarnext) - r_k(\qstark) - \beta' k \KL(\qstarnext\|\qstark)\\
        =& r_{k+1}(\qstarnext) -  \int k \dLdq(\qk) \mathrm{d}(\qstarnext-\qstark)
        -\beta' \KL(\qstarnext\|\pref)\\
        &-r_k(\qstark) - \beta' k \KL(\qstarnext\|\qstark).
    \end{align}
    In the last equality, we decomposed the sum using the equation (\ref{eq-da-noncovex-r_knext}).
    Thus,
    \begin{align}
        &\int k \dLdq(\qk) \mathrm{d}(\qstarnext-\qstark)\\
        \leq& r_{k+1}(\qstarnext) -r_k(\qstark) 
        -\beta' \KL(\qstarnext\|\pref)
        - \beta' k \KL(\qstarnext\|\qstark).
    \end{align}
    By using the same argument as \Eqref{eq:rkupdateDiff} for $k \leftarrow k+1$, we have 
    \begin{align}
        r_{k+1}(\qstarnext)
        + \beta' (k+1)\KL(\qstark\|\qstarnext) 
        =& r_{k+1}(\qstark)\\
        =&r_k(\qstark) + \beta' \KL(\qstark\|\pref).
    \end{align}
    Substituting this relation to \Eqref{eq:rkinductionFirstSide} and noticing $|\dFdq|\leq B_F$, we obtain the assertion. 
    % Combining the above inequalities and $|\dFdq|\leq B_F$, we get the desired result.
\end{proof}

We also show the result for Option 1 that is similar to Lemma~\ref{lem-da-nonconvex-1}:

\begin{lem}%[\ref{lem-da-nonconvex-1}']
    \label{lem-da-nonconvex-1-dash}
    Assume that $\TV(\qstark, \qk) \leq \epsilon_\TV/2$ for all $k$. Then, Algorithm 1 achieves 
    \begin{align}
        &\int \dFdq(\qk)\mathrm{d}(\qstarnext - \qstark)\\
        \leq & B_F\epsilon_\TV + \frac{\beta k + \beta'}{k}\left(\KL(\qstark\|\pref)-\KL(\qstarnext\|\pref) \right) \\
        & 
        -  \left( \frac{\beta (k-1) + 2 \beta' }{2}\right) \KL(\qstarnext \| \qstark) 
        -  \left( \frac{\beta (k+1) + 2 \beta'(1+1/k) }{2}\right) \KL(\qstark \| \qstarnext) 
    \end{align}
\end{lem}

\begin{proof}
    Please refer to Appendix~\ref{sec:da-nonconvex-proof-1-dash}. The proof is almost identical to the proof of Lamma~\ref{lem-da-nonconvex-1}.
\end{proof}

We mentioned that our rough intermediate goal was to show that $L(q)$ monotonically decreases in each $k$th iteration:
\begin{equation}
    L(\qstarnext) - L(\qstark) \lesssim 0.
\end{equation}
Rigorously, we will show that
\begin{equation}
    \tilde{L}_k(q) \coloneq L(q) + \frac{\beta'}{k}\KL(q\|\pref)
\end{equation}
decreases. This is an objective function with additional regularization imposed by the hyperparameter $\beta'$. 
From Lemma~\ref{lem-da-nonconvex-1}, we prove that $\tilde{L}_k(\qstark)$ decreases as in the following lemma. 
\begin{lem}\label{lem:LdiffInduction}
    Assume that $\TV(\qstark, \qk) \leq \epsilon_\TV/2$ for all $k$ and $2\beta + S_F \leq 2 \beta'$. Then, Option 2 satisfies
    %$S_F \leq 2\beta$. Then Algorithm 2 satisfies 
    \begin{align}
        \tilde{L}_{k+1}(\qstarnext) - \tilde{L}_k(\qstark)
         \leq 
         & (\LipTV +B_F)\epsilon_\TV    % +\frac{\beta'}{k}\left(\KL(\qstark\|\pref)-\KL(\qstarnext\|\pref) \right)  \\ &
         - \frac{\beta' (k+1)}{k} \KL(\qstark \|\qstarnext),
        \label{eq-da-nonconvex-monotone}
    \end{align}
    and Option 1 satisfies 
    \begin{align}
        \tilde{L}_{k+1}(\qstarnext) - \tilde{L}_k(\qstark)
         \leq 
         & (\LipTV +B_F)\epsilon_\TV    % +\frac{\beta'}{k}\left(\KL(\qstark\|\pref)-\KL(\qstarnext\|\pref) \right)  \\ &
         - \frac{\beta k(k+1)  +2 \beta' (k+1)}{2k} \KL(\qstark \|\qstarnext),
        \label{eq-da-nonconvex-monotone}
    \end{align}
%    \begin{equation}
%        \tilde{L}_{k}(q_*^{(k+1)}) -  \tilde{L}_{k}(q_*^{(k)})
%        \leq (\LipTV +B_F)\epsilon_\TV -\beta \KL(q_*^{(k+1)}\|q_*^{(k)}).\label{eq-da-nonconvex-monotone}
%    \end{equation}
\end{lem}
By this lemma, it can be shown that the sum of the KL divergences $\KL(\qstark \|\qstarnext)$ converges to 0 at a rate of $\mathcal{O}(1/K)$ through a telescoping sum:
we obtain that
\begin{equation}
    \frac{1}{K}\sum_{k=1}^K (\text{weight})_k \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right).
\end{equation}
From the above equation, it is immediately shown that
\begin{equation}
    \min_{k=1,...,K} (\text{weight})_k \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right),
\end{equation}
where $(\text{weight})_k \simeq k$ in Option 1, while $(\text{weight})_k \simeq 1$ in Option 2. In Option 2, we will be able to show that $\min_k \KL(\qstark\|\qstarnext) \to 0$, which implies that $\qstark$ converges to some point (in fact, this is the stationary point).

Now let us prove Lemma~\ref{lem:LdiffInduction}:
\begin{proof}
    We only prove the inequality for Option 2.
    By the smoothness of $F$ (the weaker smoothness (ii)' in Eq.(\ref{eq:appendix-da-nonconvex-weak-smoothness})), the Lipschitz continuity of $\dFdq$ and the property of the KL-divergence (Lemma \ref{lem-da-nonconvex-nitanda}), we have 
    \begin{align}
        L(\qstarnext) - L(\qstark)
        \leq& \int \dFdq(\qstark)\mathrm{d}(\qstarnext - \qstark) + \frac{S_F}{2}\KL(\qstarnext\|\qstark)\\
        & + \beta \KL(\qstarnext \| \qstark) + \int (- \beta  \gbarkprev) \mathrm{d} (\qstarnext - \qstark) \\
        \leq & \int \left( \dFdq(\qk) - \beta \gbarkprev \right)\mathrm{d}(\qstarnext - \qstark) \\
        & + \frac{S_F}{2}\KL(\qstarnext\|\qstark) + \beta \KL(\qstarnext \| \qstark) + 
        \LipTV \epsilon_\TV. \label{eq-da-nonconvex-monotone0}
    \end{align}
    We have used the induced smoothness of $F$ (Eq.(\ref{eq:appendix-da-nonconvex-weak-smoothness})) in the first inequality, and used the Lipschitz continuity in the second inequality.
    By Lemma~\ref{lem-da-nonconvex-1}, we can bound $\int \left( \dFdq(\qk) - \beta \gbarkprev \right)\mathrm{d}(\qstarnext - \qstark) = \int \dLdq(\qstark) \mathrm{d}(\qstarnext - \qstark)$.
    Thus the RHS of \Eqref{eq-da-nonconvex-monotone0} can be bounded as 
    \begin{align}
        (\text{RHS})
        \leq& \frac{\beta'}{k}\left(\KL(\qstark\|\pref)-\KL(\qstarnext\|\pref)
        -k\KL(\qstarnext\|\qstark)
        - (k+1)\KL(\qstark \| \qstarnext)
        \right)\\
        &+ \frac{2\beta + S_F}{2}\KL(\qstarnext\|\qstark)+ (\LipTV +B_F) \epsilon_\TV \\
        \leq& \frac{\beta'}{k}\left(\KL(\qstark\|\pref)-\KL(\qstarnext\|\pref) - (k+1)\KL(\qstark \| \qstarnext) \right) \\
        &+ \frac{(2\beta + S_F - 2 \beta')}{2}\KL(\qstarnext\|\qstark)+ (\LipTV +B_F) \epsilon_\TV,
    \end{align}
    which gives the assertion in Option 2 by noting  $\frac{\beta'}{k}\KL(\qstarnext\|\pref) \geq \frac{\beta'}{k+1}\KL(\qstarnext\|\pref)$. As for Option 1, we repeat the same argument to show the desired result.
%    From the definition of $\tilde{L}_k$,
 %   \begin{equation}
  %      \tilde{L}_{k}(\qstarnext) -  \tilde{L}_{k}(\qstark)
  %      \leq - \frac{2\beta-S_F}{2}\KL(\qstarnext\|\qstark)
  %      + (\LipTV +B_F) \epsilon_\TV.
  %  \end{equation}
\end{proof}

 
Combining Lemma \ref{lem-da-nonconvex-1} and Lemma \ref{lem:LdiffInduction}, 
we will prove that
\begin{equation}
    \frac{1}{K}\sum_{k=1}^K (\text{weight})_k \KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right)
\end{equation}
where $(\text{weight})_k \simeq k$ in Option 1 and $(\text{weight})_k \simeq 1$ in Option 2. 
When we take Option 2, it holds that $\min_k \KL(\qstark\|\qstarnext) = \mathcal{O}(1/K)$.
% Finally, we get the conclusion $\frac{\delta L(\qstark)}{\delta q} \to \mathrm{(constant.)}$ because 
It is important that $\KL(\qstark\|\qstarnext)$ is also the approximate ``moment generation function" $\psi_q(g) = \log (\E_q[\exp(-g+\E_q[g]])$ of $\frac{\delta \tilde{L}_k(\qstark)}{\delta q}$; when $k \to \infty$,
\begin{equation}
    \min_{k=1,..,K}\left(\text{``Variance" of }\frac{\delta \tilde{L}_k}{\delta q} \right)(\qstark,x) \simeq \min_{k=1,..,K}\KL(\qstark\|\qstarnext) = \mathcal{O}\left(\frac{1}{K}\right).
\end{equation}
This can be interpreted as $\frac{\delta \tilde{L}_k}{\delta q}(\qstark,x) \to 0\; \text{(up to constant w.r.t. $x$)}$.

This result is consistent with the findings for standard, non-distributional Dual Averaging~\citep{LIU2023nonconvexDA}, where $\min_{k=1,...,K}\|\text{(gradient of the objective)}_k\|^2 = \mathcal{O}(1/K)$ as the variance corresponds to the second moment..


We rigorously formulate the above discussion as the following proposition. 
\begin{prop}
Let $$\Psi_K := \frac{1}{K\beta'} (\tilde{L}_1(\hat{q}^{(1)})- L^*) %+ \beta'\KL(\qstar^{(1)}\| \pref))  
+ \frac{(\LipTV +B_F)}{K\beta'} \sum_{k=1}^K \epsilon_\TV. $$
Then, Algorithm 1 satisfies  
    \begin{align}
        & \frac{1}{K} \sum_{k=1}^K \frac{\beta k + 2 \beta' }{2} \KL(\qstark \| \qstarnext)  \leq \Psi_K 
    \end{align}
and Algorithm 2 satisfies  
    \begin{align}
        & \frac{1}{K} \sum_{k=1}^K \KL(\qstark \| \qstarnext)  \leq     
        \Psi_K.
    \end{align}
This also yields that the following bound holds for Algorithm 2:  
    \begin{align}
        & \min_{1 \leq k \leq K} \psi_{\qstark}\left(\frac{k}{\beta' (k+1)}\frac{\delta \tilde{L}_k}{\delta q}(\qstark)  \right)  \leq  
        \E_{k \sim \mathrm{Univ}([K])} \left[ \psi_{\qstark}\left(\frac{k}{\beta' (k+1)}\frac{\delta \tilde{L}_k}{\delta q}(\qstark)  \right)   \right]  \\
        & \leq  \Psi_K,
    \end{align}
    where the expectation in the middle term is taken over a random index $k$ uniformly chosen from $[K] =\{1,\dots,K\}$. 
\end{prop}
\begin{proof}
Summing up \eqref{eq-da-nonconvex-monotone} for $k=1,\dots,K$, we have that 
\begin{align}
\tilde{L}_{K+1}(\hat{q}^{(K+1)}) - \tilde{L}_1(\hat{q}^{(1)}) \leq &   (\LipTV +B_F) \sum_{k=1}^K \epsilon_\TV 
% + \beta' \KL(\qstar^{(1)}\| \pref)   
% - \sum_{k=2}^{K} \frac{\beta'}{k(k-1)} \KL(\qstark \| \pref)  \\&
% -   \frac{\beta'}{K}\KL(\qstar^{(K+1)}\| \pref)  
- \beta'  \sum_{k=1}^{K}\frac{k+1}{k}\KL(\qstark \| \qstarnext). 
\end{align}
This yields that 
\begin{align}
&  
\frac{\beta'}{K} \sum_{k=1}^{K} % \left[ 
\frac{k+1}{k}\KL(\qstark \| \qstarnext)   
%+ \frac{1}{K} \sum_{k=1}^{K} 
%\frac{1}{k}( \KL(\qstarnext \| \pref) - \KL(\qstark \| \pref)) 
%\frac{1}{k(k-1)} \KL(\qstark \| \pref)
%\right] 
% \\ & 
%+ \frac{\beta'}{K} \sum_{k=2}^{K}\frac{1}{k(k-1)} \KL(\qstark \| \pref)  \\ &
+ 
\frac{1}{K}( \tilde{L}_{K+1}(\hat{q}^{(K+1)}) - L^*)\\
 \leq  &
\frac{1}{K} (\tilde{L}_1(\hat{q}^{(1)})- L^*) %+ \beta'\KL(\qstar^{(1)}\| \pref))  
+ \frac{(\LipTV +B_F)}{K} \sum_{k=1}^K \epsilon_\TV. \label{eq-da-nonconvex-bound-kl-sum}
\end{align}
Please note that 
\begin{equation}
\frac{\beta'}{K} \sum_{k=1}^{K} % \left[ 
\KL(\qstark \| \qstarnext)   
\leq
\frac{\beta'}{K} \sum_{k=1}^{K} % \left[ 
\frac{k+1}{k}\KL(\qstark \| \qstarnext)  
+
\frac{1}{K}( \tilde{L}_{K+1}(\hat{q}^{(K+1)}) - L^*)
\end{equation}
by the optimality of $L^*$ and that the RHS in (\ref{eq-da-nonconvex-bound-kl-sum}) is $\mathcal{O}(1/K)$.
Here, we see that 
\begin{align}
&  \KL(\qstark \| \qstarnext)\\
= & 
\E_{\qstark}[ - \gbarkprev + \gbark ] -  \log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\gbark)])  \\
= & 
 \E_{\qstark}[ - \gbarkprev + \gbark ]  +  \log(\E_{\qstark}[\exp(-\gbark + \gbarkprev)]) \\  
%=&  \E_{\qstark}[ - \gbarkprev + \gbark ]  + 
=&  \log\left\{\E_{\qstark}\left[\exp\left(-\gbark + \gbarkprev - \E_{\qstark}\left[ - \gbark + \gbarkprev \right]\right)\right]\right\}, 
%\\ =& \psi_{\qstark}(\gbark - \gbarkprev). 
%= & 
% \E_{\qstark}\left[ \frac{k}{\beta' (k+1)} \dLdq(\qstark) - \frac{1}{k+1}  \gbarkprev \right] \\ 
% & -  \tfrac{k}{k+1}\log(\E_{\pref}[\exp(-\gbarkprev)]) + \tfrac{k}{k+1} \log[(\E_{\pref}[\exp(-\gbark)])^{\tfrac{k+1}{k}}] \\  
%\geq & 
% \frac{k}{k+1} \left\{   \E_{\qstark}\left[ \frac{1}{\beta'} \dLdq(\qstark) \right]  - 
% \log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\tfrac{k+1}{k}\gbark)]) \right\}, 
\end{align}
%where we used the convexity of $(\cdot)^{(k+1)/k}$ in the last inequality. 
Now, the term $\gbark - \gbarkprev$ can be evaluated as 
$$\gbark - \gbarkprev = \frac{k}{\beta' (k+1)} \dLdq(\qstark) - \frac{1}{k+1}  \gbarkprev
 =  \frac{k}{\beta' (k+1)} \frac{\delta \tilde{L}_k}{\delta q}(\qstark),
$$
which yields the assertion because 
\begin{align}
    &\KL(\qstark \| \qstarnext)\\
    =& \log\left\{\E_{\qstark}\left[\exp\left( \frac{k}{\beta' (k+1)} \frac{\delta \tilde{L}_k}{\delta q}(\qstark) -\E_{\qstark}\left[\frac{k}{\beta' (k+1)} \frac{\delta \tilde{L}_k}{\delta q}(\qstark)\right]\right)\right]\right\},
\end{align}
which is the ``moment generating function" $\psi_{\qstark}$ of $\frac{k}{\beta' (k+1)} \frac{\delta \tilde{L}_k}{\delta q}(\qstark)$.
%
%On the other hand, the right hand side can be also evaluated as   
%\begin{align}
%& 
%  \log\left\{\E_{\qstark}\left[\exp\left(- \frac{k}{\beta' (k+1)} \dLdq(\qstark) + \frac{1}{k+1}  \gbarkprev - \E_{\qstark}\left[ - \frac{k}{\beta' (k+1)} \dLdq(\qstark) + \frac{1}{k+1}  \gbarkprev  \right] \right)\right]\right\}
% \\ 
%\geq & 
%  \log\left\{\E_{\qstark}\left[\exp\left(- \frac{k}{\beta' (k+1)} \dLdq(\qstark) + \frac{1}{k+1}  \gbarkprev - \E_{\qstark}\left[ - \frac{k}\beta' (k+1)} \dLdq(\qstark) + \frac{1}{k+1}  \gbarkprev  \right] \right)\right]\right\} \\  
%\geq & 
% \frac{k}{k+1} \left\{   \E_{\qstark}\left[ \frac{1}{\beta'} \dLdq(\qstark) \right]  - 
% \log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\tfrac{k+1}{k}\gbark)]) \right\}, 
%\end{align}
%which yields the first assertion. 
%which yields the assertion. 
%Here, we see that 
%\begin{align}
%&  \KL(\qstark \| \qstarnext)-  
%\frac{1}{k+1}\KL(\qstark \| \pref) \\
%= & 
%\E_{\qstark}[ - \gbarkprev + \gbark ] -  \log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\gbark)]) \\
%& - \tfrac{1}{k+1} \left( \E_{\qstark}[ - \gbarkprev]   -  \log(\E_{\pref}[\exp(-\gbarkprev)]) \right) \\
%= & 
% \E_{\qstark}[ - \tfrac{k}{k+1}\gbarkprev + \gbark ] \\ 
% & -  \tfrac{k}{k+1}\log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\gbark)]) \\ 
%= & 
% \E_{\qstark}[ - \tfrac{k}{k+1}\gbarkprev + \gbark ] \\ 
% & - \tfrac{k}{k+1}\log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\gbark)]) \\  
%= & 
% \E_{\qstark}[ - \tfrac{k}{k+1}\gbarkprev + \gbark ] \\ 
% & -  \tfrac{k}{k+1}\log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\gbark)]) \\  
%= & 
% \E_{\qstark}\left[ \frac{k}{\beta' (k+1)} \dLdq(\qstark) \right] \\ 
% & -  \tfrac{k}{k+1}\log(\E_{\pref}[\exp(-\gbarkprev)]) + \tfrac{k}{k+1} \log[(\E_{\pref}[\exp(-\gbark)])^{\tfrac{k+1}{k}}] \\  
%\geq & 
% \frac{k}{k+1} \left\{   \E_{\qstark}\left[ \frac{1}{\beta'} \dLdq(\qstark) \right]  - 
% \log(\E_{\pref}[\exp(-\gbarkprev)]) + \log(\E_{\pref}[\exp(-\tfrac{k+1}{k}\gbark)]) \right\}, 
%\end{align}
%where we used the convexity of $(\cdot)^{(k+1)/k}$ in the last inequality. 
\end{proof}

\begin{comment}
We characterize $\KL(\qstarnext\|\qstark)$ by this proposition:
\begin{prop}
    For all $k\geq 1$, in the limit $\gbark \rightarrow \gbarkprev$ in KL divergence,
    \begin{align}
        &\left|\KL(\qstarnext\|\qstark) - \frac{1}{2}\mathrm{Var}_{\qstarnext}\left(\frac{k}{\beta(k+1)}\frac{\delta L_k}{\delta q}(q_*^{(k)})\right)\right|\\
        =& \mathcal{O}\left(\| \gbark - \gbarkprev\|_{L^3(\qstarnext)}^3 + \textcolor{red}{\text{(Bound of $\frac{\delta L_k}{\delta q}$$\simeq B_F?$)}}\frac{B_F \epsilon}{\beta}\right),
    \end{align}
    where $\Var_q$ is the variance with respect to $q$.
\end{prop}
\begin{proof}
By an asymptotic expansion of $\KL$,
\begin{align}
    &\KL(\qstarnext\|\qstark) \\
    =& \int -\gbark +\gbarkprev + \log \frac{\int e^{-\gbarkprev}d\pref}{\int e^{-\gbark}d\pref} \mathrm{d}\qstarnext\\
    =& \int \left[ \frac{1}{2}\|-\gbarkprev + \gbark\|^2(x)
    -\frac{1}{2}\left(\int e^{-\gbarkprev+\gbark}-1 \mathrm{d}\qstarnext \right)^2
    \right] \mathrm{d}\qstarnext\\
    &+ \mathcal{O}\left(\| \gbark - \gbarkprev\|_{L^3(\qstarnext)}^3 \right)\\
    =&\frac{1}{2}\mathrm{Var}_{\qstarnext}(-\gbarkprev + \gbark)
    + \mathcal{O}\left(\| \gbark - \gbarkprev\|_{L^3(\qstarnext)}^3 \right).
\end{align}
From the definition of $\gbark$,
\begin{equation}
    \gbark = \frac{k}{k+1}\gbarkprev + \frac{k}{\beta(k+1)}\dFdq(\qk).
\end{equation}
Because of the optimality of $\qstark \propto \exp(-\gbarkprev)\pref$,
\begin{equation}
    \gbarkprev = - \frac{k}{\beta} \left(\frac{\delta \tilde{L}_k}{\delta q}(\qstark) - \dFdq(\qstark) \right).
\end{equation}
Then we see that
\begin{equation}
    \gbark - \gbarkprev = \frac{k}{\beta(k+1)}\frac{\delta L_k}{\delta q}(q_*^{(k)}) + \Theta \left( \frac{B_F}{\beta} \epsilon \right).
\end{equation}
\end{proof}
\end{comment}

\subsubsection{Proof of Lemma~\ref{lem-da-nonconvex-1-dash}}\label{sec:da-nonconvex-proof-1-dash}

\begin{proof}
    The proof is almost identical to Lemma \ref{lem-da-nonconvex-1}. 
    In Algorithm 1, $\qstarnext$ is defined as the minimizer of the following quantity: 
    \begin{equation}
        r_{k+1}(q) \coloneq \sum_{j=1}^k j \left( \int   \dFdq(\qj)\mathrm{d}(q-\qstarj)  + \beta \KL(q \| \pref) \right)
        + \beta' (k+1) \KL(q\|\pref).
    \end{equation}
    Hence, we have 
    \begin{equation}
        r_{k+1}(q) = r_k(q) + \int k \dFdq(\qk) \mathrm{d}(q-\qstark)
        + \left(\beta k  + \beta' \right) \KL(q\|\pref) \label{eq:lem-da-nonconvex-proof_opt1}
    \end{equation}
    by the definition of $r_k$. Moreover, Lemma~\ref{lem-da-nonconvex-nitanda} and the optimality of $\qstark$ gives that  
    \begin{equation}\label{eq:rkupdateDiff2}
        r_k(q) - r_k(\qstark) =  \left( \frac{\beta k(k-1)}{2} + \beta' k \right) \KL(q\|\qstark), \quad \text{for all $q\in\gP$.}
    \end{equation}
    Substituting $q \leftarrow \qstarnext$, we obtain
    \begin{align}
        0 \leq& r_k(\qstarnext) - r_k(\qstark) -  \left( \frac{\beta k(k-1)}{2} + \beta' k \right) \KL(\qstarnext\|\qstark) \\
        =& r_{k+1}(\qstarnext) -  \int k \dFdq(\qk) \mathrm{d}(\qstarnext-\qstark)
        -( \beta k + \beta') \KL(\qstarnext\|\pref) \quad (\text{used (\ref{eq:lem-da-nonconvex-proof_opt1})})\\
        &-r_k(\qstark) -  \left( \frac{\beta k(k-1)}{2} + \beta' k \right) \KL(\qstarnext\|\qstark).\label{eq:rkinductionFirstSide2}
    \end{align}
    This is equivalent to 
    \begin{align}
        &\int k \dFdq(\qk) \mathrm{d}(\qstarnext-\qstark)\\
        \leq& r_{k+1}(\qstarnext) -r_k(\qstark) 
        -(\beta k + \beta') \KL(\qstarnext\|\pref)
        - \left( \frac{\beta k(k-1)}{2} + \beta' k \right) \KL(\qstarnext\|\qstark).
    \end{align}
    By using the same argument as \Eqref{eq:rkupdateDiff2} for $k \leftarrow k+1$, we have 
    \begin{align}
        & r_{k+1}(\qstarnext)
        + \left( \frac{\beta k(k+1)}{2} + \beta' (k+1) \right) \KL(\qstark\|\qstarnext)  \\
        =& r_{k+1}(\qstark)\\
        =&r_k(\qstark) + (\beta k + \beta') \KL(\qstark\|\pref).
    \end{align}
    Substituting this relation to \Eqref{eq:rkinductionFirstSide2} and noticing $|\dFdq|\leq B_F$, we obtain the assertion. 
    % Combining the above inequalities and $|\dFdq|\leq B_F$, we get the desired result.
\end{proof}