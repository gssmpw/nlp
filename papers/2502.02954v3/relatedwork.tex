\section{Related work.}
\vspace{-1mm}
\textbf{Mean-field optimization.}\quad
PDA method \citep{NEURIPS2021_a34e1ddb,nishikawa2022twolayer}, an extension of DA method~\citep{Nesterov2009} to the distribution optimization setting, was the first method that proves the quantitative convergence for minimizing entropy regularized convex functional. Subsequently, P-SDCA method~\citep{oko2022particle}, inspired by the SDCA method~\citep{shalev2013stochastic}, achieved the linear convergence rate. Mean-field Langevin dynamics~\citep{mei2018mf} is the most standard particle-based distribution optimization method, derived as the mean-field limit of the noisy gradient descent, and its convergence rate has been well studied by \cite{mei2018mf,hu2021mean,nitanda22mfld,chizat2022meanfield,suzuki2023mfld,nitanda2024improved}. Additionally, several mean-field optimization methods such mean-field Fisher-Rao gradient flow~\citep{liu2023polyak} and entropic fictitious play~\citep{chen2023entropic,pmlr-v202-nitanda23a} have been proposed with provable convergence guarantees. We note that the convergence rates of these methods were established under isoperimetric conditions such as log-Sobolev and Poincar\'e inequalities, which ensure concentration of the probability mass.
IKLPD method \citep{pmlr-v238-yao24a} shares similarities with our method, as it employs the normalizing flow to solve intermediate subproblems in the distributional optimization procedure, and its convergence does not depend on isoperimetric conditions. However, the applicability of IKLPD to alignment tasks remains uncertain since handling the proximity to the reference distribution is non-trivial.

%However, the algorithms shown above are limited to the Log-Sobolev inequality, which implies the unimodality of the densities.
%Our distributional optimization algorithm via diffusion models is free from such isoperimetric inequalities, which leads to more realistic assumptions of the data distribution.

\vspace{-1mm}
\textbf{Fine-tuning of diffusion models.}\quad
Recently, alignment of diffusion models have been investigated, inspired by LLM fine-tune methods, such as RLHF~\citep{ziegler2020LLMft}, DPO~\citep{rafailov2023DPO}, and KTO~\citep{ethayarajh2024KTO}. Applying them to diffusion models entails additional difficulty since the output density $\pref$ of the diffusion model is not available, and hence several techniques have been developed to circumvent the explicit calculation of $\pref$. 
For instance, \cite{fan2023reward,black2024reward,clark2024reward} invented the maximization algorithm of the reward in each diffusion time step. \cite{uehara2024reward} used Doob's $h$-transform to compute the correction term that can be automatically derived from the density ratio between the generated and the reference distributions. Instead of optimizing original DPO and KTO objectives, \cite{Wallace2024DiffusionDPO} considered the evidence lower bound (ELBO) and \cite{li2024diffusionKTO} defined a new objective function to replicate KTO.
\revisedStart
%\cite{marion2024implicit} also studied fine-tuning of diffusion models as distributional optimization. However, their convergence analysis for diffusion models was limited to RLHF approach and the single Gaussian assumption.\revisedEnd
\cite{marion2024implicit} also studied fine-tuning of diffusion models as distributional optimization within the RLHF framework and conducted convergence analysis for the one-dimensional Gaussian distribution.\revisedEnd
%*********
%They unified Langevin dynamics and the fine-tuning of diffusion models as an optimization problem over the probability space. However, their theoretical analysis was constrained by the logarithmic Sobolev inequality in the context of Langevin dynamics, as well as by the reinforcement learning approach and the single Gaussian assumption in the context of diffusion models.
%*********

%Nevertheless, the difficulty to directly optimize the nonlinear objectives of the output distribution, such as DPO and KTO, via diffusion models have not been solved yet.
% We tackled this problem from the mean-field perspective and Doob's h-transform technique.

%And LoRA?

% unified, theoretical way of direct density optimization
% 拡散モデル: pathの密度しか密度が扱えない

% mean-field: sampling NG 多峰
% proximal gibbs
% mean-field langevin も遅い 画像でうまく行かない


%\paragraph{Application DPO}
%marginal distribution w.r.t. $x_0$
%generalization of DP


%\begin{itemize}
%    \item Introduction: 1.5 pages
%    \item Preliminaries: 1 page
%    \item Core theorems 1 page
%    \item Applications 1 page
%    \item Diffusion Model: 1.5 pages
%    \item Experiments: 2.5 pages
%    \item Conclusion: 0.5 pages
%\end{itemize}

%\color{red}
%KEYWORDS:
%\begin{itemize}
%    \item diffusion (as a schodinge bridge?)
%    \item Doob
%    \item RLHF, PPO (uehara etc.)
%    \item sampling
%    \item mean field (?)
%    \item diffusion-DPO
%    \item diffusion-KTO
%\end{itemize}
%\color{black}

\vspace{-1mm}