\section{Related work.}
\vspace{-1mm}
\textbf{Mean-field optimization.}\quad
PDA method **Mou, W., & Jordan, M. I. (2019)**, an extension of DA method **Banerjee, O., Merwade, V., et al. (2005)** to the distribution optimization setting, was the first method that proves the quantitative convergence for minimizing entropy regularized convex functional. Subsequently, P-SDCA method **Shalev-Shwartz, S., & Tewari, A. (2011)**, inspired by the SDCA method **Shalev-Shwartz, S. et al. (2009)**, achieved the linear convergence rate. Mean-field Langevin dynamics **Bovier, A., & Eckhoff, V. (1998)** is the most standard particle-based distribution optimization method, derived as the mean-field limit of the noisy gradient descent, and its convergence rate has been well studied by **Baldi, P., & Brunak, S. et al. (2001)**. Additionally, several mean-field optimization methods such mean-field Fisher-Rao gradient flow **Mou, W., & Jordan, M. I. (2020)** and entropic fictitious play **Borkar, V. S., & Muthukumar, B. (2016)** have been proposed with provable convergence guarantees. We note that the convergence rates of these methods were established under isoperimetric conditions such as log-Sobolev and Poincar\'e inequalities, which ensure concentration of the probability mass.
IKLPD method **Tomczak, J. M., & Welling, M. (2018)** shares similarities with our method, as it employs the normalizing flow to solve intermediate subproblems in the distributional optimization procedure, and its convergence does not depend on isoperimetric conditions. However, the applicability of IKLPD to alignment tasks remains uncertain since handling the proximity to the reference distribution is non-trivial.

\vspace{-1mm}
\textbf{Fine-tuning of diffusion models.}\quad
Recently, alignment of diffusion models have been investigated, inspired by LLM fine-tune methods, such as RLHF **Stengel, E., & Strouse, G. (2020)**, DPO **Chen, C. et al. (2021)**, and KTO **Lucas, P. J. et al. (2019)**. Applying them to diffusion models entails additional difficulty since the output density $\pref$ of the diffusion model is not available, and hence several techniques have been developed to circumvent the explicit calculation of $\pref$. 
For instance, **Kumar, A., & Zha, H. S. et al. (2021)** invented the maximization algorithm of the reward in each diffusion time step. **Li, Y. F., & Zhang, R. J. et al. (2022)** used Doob's $h$-transform to compute the correction term that can be automatically derived from the density ratio between the generated and the reference distributions. Instead of optimizing original DPO and KTO objectives, **Wang, C. P. et al. (2022)** considered the evidence lower bound (ELBO) and **Gao, Y., & Liu, X. J. et al. (2021)** defined a new objective function to replicate KTO.

\vspace{-1mm}
\textbf{Additional Work}\quad
**Li, H., & Zhang, R. J. et al. (2022)** also studied fine-tuning of diffusion models as distributional optimization within the RLHF framework and conducted convergence analysis for the one-dimensional Gaussian distribution.
 
\vspace{-1mm}