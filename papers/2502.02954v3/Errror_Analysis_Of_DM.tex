

\subsection{Overview}
Here, we analyze the sampling error caused by the diffusion model. Let us organize the settings and notations used in this section.

{\bf Target distribution.} The target distribution is $q_*=q_0$, which is decomposed as $q_*(x) = \rho_*(x) p_*(x)$ with $p_*=p_0$ and $\rho_*$. 
Here $p_*$ and $\rho_*$ represent the distribution of the original model and the density ratio obtained by fine-tuning, respectively. 

{\bf Sampling with score-based diffusion model.} In the score-based diffusion model, we start with the forward process, which is written as a stochastic differential equation (SDE).
Choosing the Ornsteinâ€“Uhlenbeck (OU) process, $\{\bar{X}_t\}_{t\geq 0}$ follows the following SDE:
\begin{align}
    \bar{X}_0 \sim p_*,\ \mathrm{d}\bar{X}_t = - \bar{X}_t\mathrm{d}t + \sqrt{2}\mathrm{d}B_t,
    \label{eq:Appendix-Diffusion-Preliminary-1}
\end{align}
where $\{B_t\}_{t\geq 0}$ is the standard Brownian motion.
At each time $t$, the law of $X_t$ is written as
\begin{align}
\label{eq:Appendix-Diffusion-Preliminary-3}
    p_t(x) = \int p_*(y)\exp\bigg(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\bigg)\mathrm{d}x,
\end{align}
where $m_t = e^{-t}$ and $\sigma_t^2 = 1 - e^{-2t}$. 
In the same way, define $\{\bar{Y}_t\}_{t\geq 0}$ by replacing $p_*$ by $q_*$ in (\ref{eq:Appendix-Diffusion-Preliminary-1}) and let $q_t$ be the law of $\bar{Y}_t$. 

Then, for some $T\geq 0$, we can define the reverse process for $\{\bar{X}_{t}^\leftarrow\}_{0\leq t \leq T}$. (Let $B_t$ below be distinct from the one in (\ref{eq:Appendix-Diffusion-Preliminary-1}).)
\begin{align}
\label{eq:Appendix-Diffusion-Preliminary-2}
    \bar{X}_{0}^\leftarrow\sim p_{T},\ \mathrm{d}\bar{X}_{t}^\leftarrow = \{\bar{X}_{t}^\leftarrow + 2\nabla \log p_{T-t}(\bar{X}_{t}^\leftarrow)\}\mathrm{d}t + \sqrt{2}\mathrm{d}B_t.
\end{align}
Then, the law of $\bar{X}_{t}^\leftarrow$ equals $p_{T-t}$, which is why we call (\ref{eq:Appendix-Diffusion-Preliminary-2}) as the reverse process.
In the same way, we define $\{\bar{Y}_{t}^\leftarrow\}_{0\leq t \leq T}$ as the reverse process of $\{\bar{Y}_{t}\}_{t\geq 0}$. 
% We define $\{\bar{X}^\leftarrow_{t}\}_{0\leq t\leq T}$ by replacing $q$ by $p$ in (\ref{eq:Appendix-Diffusion-Preliminary-2}).

% Describing $\nabla p_{T-t}$ more precisely, 
% Define $p_t$ in the same way as $q_t$. 
% More precisely, 
% $p_{t}$ is obtained by replacing $q_*$ by $p_*$ in (\ref{eq:Appendix-Diffusion-Preliminary-3}), which is log likelihood of the distribution at time $t$ of the forward diffusion process starting from $p_*=p_0$. 

{\bf Doob's h-transform.} 
By applying Doob's h-transform to $\nabla \log q_{T-t}$, it can be decomposed into the original score $\nabla\log p_{T-t}$ and a correction term.
\begin{align}
    \nabla \log q_{T-t}(\bar{Y}_{t}^\leftarrow) = \nabla\log p_{T-t}(\bar{Y}_{t}^\leftarrow) + \nabla_x \log (\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x])|_{x=\bar{Y}_{t}^\leftarrow}.
    \label{eq:Appendix-Diffusion-Preliminary-4}
\end{align}
See Lemma~\ref{lem:H-transform} for derivation.

{\bf Approximation of the score and correction term.} 
Because we do not have access to the exact value of $p_{t}$ and $\rho_*$ and therefore cannot implement (\ref{eq:Appendix-Diffusion-Preliminary-4}) exactly, we consider approximating them by, e.g., neural networks.
We approximate $\nabla \log p_{T-t}(x)$ by $s(x,t)\colon \R^{d+1}\to \R^d$.
Also, we approximate
$u_*(x,t)=\nabla_x \log (\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x])$ by $u(x,t)\colon \R^{d+1}\to \R^d$.

{\bf Discretization.} 
Also, we need to discretize the stochastic differential equation.
We finally obtain the approximation of (\ref{eq:Appendix-Diffusion-Preliminary-2}), denoted by $\{ Y^\leftarrow_t\}_{0\leq t \leq T}$, as
\begin{align}
    Y^\leftarrow_0 \sim \mathcal{N}(0,I_d),\ 
    \mathrm{d} Y^\leftarrow_t = \{ Y^\leftarrow_t + 2s( Y^\leftarrow_{kh},kh)+2u( Y^\leftarrow_{kh},kh)\}\mathrm{d}t + \sqrt{2}\mathrm{d}B_t,\ t\in [kh,(k+1)h].
    \label{eq:Appendix-Diffusion-Preliminary-5}
\end{align}


{\bf Obtaining the correction term (approximately).} 
\label{sec:AppendixCorrectionTermBound}
Given the score network $s(x,t)$ approximating $\nabla \log p_t(x)$ and the function $\bar{h}$ that approximates $h_*$, we can approximate the correction term $u(x,t)$.
Remind that, for fixed $x\in \R^d,\ t=kh, s=k(h+1)\in \R$, the correction term is calculated as
\begin{align}
    \nabla_x \log (\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x]|)
    = 
    \frac{
        \int
        \mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-s}=x']
        \frac{\partial}{\partial x}\mathbb{P}[\bar{X}_{T-s}=x'|\bar{X}_{T-t}=x]\mathrm{d}x'
    }{
        \mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x]
    }.
    \label{eq:Appendix-Diffusion-Preliminary-6}
\end{align}
One way to approximate (\ref{eq:Appendix-Diffusion-Preliminary-6}) starts from approximating $\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x]$. 
If we run the reverse diffusion process
\begin{align}
    \tilde{X}_{t}^\leftarrow=x,\ \mathrm{d}\tilde{X}_{\tau}^\leftarrow = \{\tilde{X}_{\tau}^\leftarrow + 2\nabla \log p_{T-\tau}(\tilde{X}_{\tau}^\leftarrow)\}\mathrm{d}\tau + \sqrt{2}\mathrm{d}B_\tau,
\end{align}
we obtain that the law of $\tilde{X}_{T}^\leftarrow$ is equal to that of $\bar{X}_0|\bar{X}_{T-t}=x$. 
Therefore, by running the approximated reverse process (with a slight abuse of notation)
\begin{align}
    \tilde{X}_{t} = x,\ 
    \mathrm{d}\tilde{X}_{\tau} = \{\tilde{X}_\tau + 2s(\tilde{X}_{lh},lh)\}\mathrm{d}t + \sqrt{2}\mathrm{d}B_\tau,\ \tau\in [lh,(l+1)h],
    \label{eq:Appendix-Diffusion-Preliminary-9}
\end{align}
multiple times, the sample of $\tilde{X}_T$, denoted by $\{\tilde{x}_{T,i}\}_{i=1}^n$, can approximate $\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x]$ as
\begin{align}
    \mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x] \approx
    \frac1n\sum_{i=1}^n \rho'(\tilde{x}_{T,i}),
      \label{eq:Appendix-Diffusion-Preliminary-8}
\end{align}
where $\rho'$ is the approximation of $\rho_*$. 

On the other hand, we approximate $\frac{\partial}{\partial x}\mathbb{P}[\bar{X}_{T-s}=x'|\bar{X}_{T-t}=x]$ by approximating $\mathbb{P}[\bar{X}_{T-s}=x'|\bar{X}_{T-t}=x]$ with a Gaussian distribution.
Specifically, because $\bar{X}_{T-s}=x'|\bar{X}_{T-t}=x$ is obtained by the following reverse diffusion process
\begin{align}
    \tilde{X}_{t}^\leftarrow=x,\ \mathrm{d}\tilde{X}_{\tau}^\leftarrow = \{\tilde{X}_{\tau}^\leftarrow + 2\nabla \log p_{T-\tau}(\tilde{X}_{\tau}^\leftarrow)\}\mathrm{d}\tau + \sqrt{2}\mathrm{d}B_\tau,
\end{align}
we approximate $\nabla \log p_{T-\tau}(\tilde{X}_{\tau}^\leftarrow)$ by $s(x_{kh},kh)$ to obtain
\begin{align}
    \dot{X}_{t}^\leftarrow=x_{kh},\ \mathrm{d}\dot{X}_{\tau}^\leftarrow = \{\dot{X}_{\tau}^\leftarrow + 2s(x_{kh},kh)\}\mathrm{d}\tau + \sqrt{2}\mathrm{d}B_\tau.
\end{align}
The distribution of $\tilde{X}_{s}^\leftarrow$ is denoted by
\begin{align}
    \mathcal{N}\big(e^h x_{kh} + 2(e^h-1)s(x_{kh},kh), e^{2h} - 1\big).
     \label{eq:Appendix-Diffusion-Preliminary-7}
\end{align}
Using this, our approximation is
\begin{align}
   & \frac{\partial}{\partial x}\mathbb{P}[\bar{X}_{T-s}=x'|\bar{X}_{T-t}=x]
   \\ & \approx 
    \frac{\partial}{\partial x}\frac{1}{(2\pi (e^{2h} - 1))^\frac{d}{2}}
    \exp\bigg(-\frac{(x'-(e^h x + 2(e^h-1)s(x_{kh},kh)))^2}{2(e^{2h} - 1)}\bigg)
    \\ & = \frac{e^h(x'-(e^h x + 2(e^h-1)s(x_{kh},kh)))}{(e^{2h} - 1)(2\pi (e^{2h} - 1))^\frac{d}{2}}
    \exp\bigg(-\frac{(x'-(e^h x + 2(e^h-1)s(x_{kh},kh)))^2}{2(e^{2h} - 1)}\bigg).
\end{align}
This implies that, if we sample $\{x'_{j}\}_{j=1}^m$ from (\ref{eq:Appendix-Diffusion-Preliminary-7}),
\begin{align}
  &  \int
        \mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-s}=x']
        \frac{\partial}{\partial x}\mathbb{P}[\bar{X}_{T-s}=x'|\bar{X}_{T-t}=x]\mathrm{d}x'
  \\ &  \approx \frac{e^h}{(e^{2h} - 1)}\frac1m\sum_{j=1}^m \mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-s}=x'_j] (x'_{j}-(e^h x + 2(e^h-1)s(x_{t},t))),
\end{align}
and approximate each $\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-s}=x'_j]$ in the same way as (\ref{eq:Appendix-Diffusion-Preliminary-8}), we can approximate the correction term (\ref{eq:Appendix-Diffusion-Preliminary-6}). 



Now, our goal is to bound the error of the whole pipeline under the following assumptions of $p_*$ and $\rho_*$ and the score approximation error.
\begin{assumption}[Assumption~\ref{assumption:TVBoundMainText}, restated]\label{assumption:TVBoundMainText-2}
\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]
\item 
    $\nabla \log p_t$ is $L_p$-smooth at every time $t$ and it has finite second moment $\mathbb{E}[\|\bar{X}_t\|^2_2] \leq \mathsf{m} < \infty$ for all $t\in \R_+$ and $x\in \R^d$. 
\item  $\nabla \log \rho_*$ is $L_\rho$-smooth and bounded as $C_\rho^{-1}\leq \rho_* \leq C_\rho$ for a constant $C_\rho$.
\item   The score estimation error is bounded by 
%$\|s(\cdot,t)-\nabla \log p_{T-t}\|_{L^\infty}\leq \varepsilon$ 
\revisedStart
$\E_{\bar{X}_{\cdot}^{\leftarrow}}[\|s(\bar{X}_{t}^{\leftarrow},t)-\nabla \log p_{T-t}(\bar{X}_{T-t}^{\leftarrow})\|^2]\leq \varepsilon$ 
\revisedEnd
at each time $t$. 
\item 
$\E_{p_t}[\|u_*(x,lh) - u(x,lh)\|^2] \leq \varepsilon_{\rho,l}^2$~~for any $1 \leq l \leq T/h$.
%$\E_{X_{lh}}[\E_{\bar{D}_{n,l}(X_{lh})}[\|u_*(X_{lh},lh) - \hat{u}(X_{lh},lh)\|^2]] \leq \varepsilon_{\rho,l}^2$~~for any $1 \leq l \leq T/h$
\end{enumerate}
\end{assumption}
% \begin{assumption}\label{assumption:smoothnessP}
%     For the original distribution $p_*$, we assume that its score $\nabla \log p_t$ is $L_p$-smooth at every time $t$ and it has finite second moment $\mathbb{E}[\|\bar{X}_t\|^2_2] \leq \mathsf{m} < \infty$ for all $t\in \R_+$ and $x\in \R^d$. 
% \end{assumption}
% \begin{assumption}\label{assumption:smoothnessH}
%     For the density ratio $\rho_*$, we assume that $\nabla \log \rho_*$ is $L_\rho$-smooth and bounded as $C_\rho^{-1}\leq \rho_* \leq C_\rho$ for a constant $C_\rho$. 
% \end{assumption}
% \begin{assumption}\label{assumption:ApproxError}
%     The score estimation error is bounded by $\|s(\cdot,t)-\nabla \log p_{T-t}\|_{L^\infty}\leq \varepsilon$ at each time $t$. 
% \end{assumption}
% \begin{align}
%     \| p_{0|t}(x|x') - \hat{p}_{0|t}(x|x')\|_1^2 \leq \sum_{l=k}^{N}\frac{T}{h}\mathbb{E}_{p_{0|t}(x|t')}[\|\nabla \log p_{T-lh}(x)-s(x,lh)\|^2]
% \end{align}

% Under this, Theorem~\ref{} is restated as follows.
% \begin{thm}
%     Under Assumptions~\ref{assumption:smoothnessP}, \ref{assumption:smoothnessH}, and \ref{assumption:ApproxError}, take
%     \begin{align}
%         T &\simeq \log \frac{d+\mathsf{m}}{\varepsilon}, 
%         \\
%         \varepsilon &\lesssim ,
%         \\
%         h & \lesssim,
%        \\ n &\lesssim ,
%       \\  m &\lesssim ,
%         .
%     \end{align}
%     Then, the distribution $\hat{q}$ of $Y_T$ satisfies
%     \begin{align}
%         \mathrm{KL}(q_*\|\hat{q}) \leq \varepsilon_*
%     \end{align}
% \end{thm}
\begin{thm}[Theorem \ref{thm:Diffusion-1}, restated]\label{thm:Diffusion-1-appendix}
Suppose that Assumption \ref{assumption:TVBoundMainText-2} is satisfied. Then, we have the following bound on the distribution $\hat{q}$ of $Y^\leftarrow_T$: 
\begin{align}
\TV(q_*,\hat{q})^2
\lesssim  T \varepsilon^2 + \sum_{l=1}^{T/h} h \varepsilon_{\rho,l}^2 + T (L_pC_\rho^2+L_\rho)^2(dh + \mathsf{m}^2 h^2 )+ \exp(-2 T)\KL(\qstar \| N(0,I)).
\end{align}
\end{thm}
\begin{proof}
    Suppose that $\|s(\cdot,t)+u(\cdot,t)-\nabla \log q_{T-t}\|_{L^\infty}\leq \varepsilon'$ and $\nabla \log q_{T-t}$ is $L_q$-smooth at every time $t$. 
    According to \cite{chen2023sampling} and Pinsker's inequality, the distribution $\hat{q}$ of $Y_T$ satisfies
    \begin{align}
        \mathrm{KL}(\hat{q}\|q_*)^2 \lesssim
       T \varepsilon^2 + \sum_{l=1}^{T/h} h \varepsilon_{\rho,l}^2 + T \Lipdp^2(dh + \mathsf{m}^2 h^2 )+ \exp(-2 T)\KL(\qstar \| N(0,I)).
    \end{align}
    According to \Cref{lem:Smoothness}, $L_q$ is bounded by
    \begin{align}
        L_q \lesssim L_pC_\rho^2+L_\rho,
    \end{align}
    which yields the assertion.
\end{proof}


In the bound, we assumed that the term $\varepsilon_{\rho,l}^2$ can be bounded, however this approximation error can be derived as in the following theorem with additional technical conditions. 
\begin{assumption}[Assumption~\ref{ass:BoundingHMainText} restated]
%\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]\label{ass:NonconvexF}
    %\item[(i)] 
    (i) $\nabla_x s(\cdot,\cdot)$ is $H_s$-Lipschitz continuous in a sense that $\|\nabla_x s(x,t) - \nabla_y s(y,t)\|_{\mathrm{op}} \leq H_s \|x- y\|$
for any $x,y \in \sR^d$ and $0 \leq t \leq T$ and $\E[\| s(\tilde{X}_{kh}^\leftarrow,kh)\|^2] \leq Q^2$ for any $k$, 
    %\item[(ii)] 
    (ii) There exists $R > 0$ such that $\sup_{t,x}\{\|\nabla_x^2 \log p_t(x)\|_{\mathrm{op}},\|\nabla_x^2 \log s(x,t)\|_{\mathrm{op}}\} \leq R$.
%\end{enumerate}
\end{assumption}
\begin{thm}[Theorem~\ref{thm:Diffusion-2} restated]
Suppose that Assumptions \ref{assumption:TVBoundMainText} and \ref{ass:BoundingHMainText} hold.  
%the same condition as Theorem \ref{thm:delhDiffXYExp} holds 
Assume that $\|\rho_* - \rho\|_\infty \leq \varepsilon'$, $\|\rho\|_\infty \leq C_\rho$,
and 
$\sup_x\|\nabla \rho_*(x)\|\leq R_\rho$, $\|\nabla \rho_*(x) - \nabla \rho_*(y)\| \leq L_\rho\|x-y\|~(\forall x,y)$.
%(and some additional technical conditions). 
%Let $L_\varphi$ and $R_\varphi$ be as given in Lemma \ref{lemm:phiYboundLip}.
Then, for any choice of $0 \leq h \leq \delta \leq 1/(1 + 2R)$, we have that 
\begin{align}
%& %\E_{\bar{Y}_{\cdot}^{\leftarrow}}[ \|\nabla_x \E[\rho(\tilde{X}_{T}^\leftarrow) |  \tilde{X}_{t}^\leftarrow = x] |_{x = \bar{Y}_{t}^{\leftarrow}} -  \hat{u}(\bar{Y}_{k_t h}^\leftarrow,t) \|^2]  
\varepsilon_{\rho,l}^2 %\\
\lesssim & 
C_\rho^3 \left\{ \Xi_{\delta,\varepsilon}
+  R_\varphi^2 \left(\varepsilon^2+ \Lipdp^2 d(\delta + \mathsf{m} \delta^2) \right) + 
[ L_\varphi^2 (\mathsf{m} + Q^2 + d h)  
+ R_\varphi^2  (1 + 2R)^2] h^2\right\} \\
& +   \min\{T-lh,1/(2+2R)\}^{-1} \varepsilon'^2,
%+  C_\rho (1 + 2R) \sqrt{\frac{\log(T/h)}{n h}}, 
%= & \gO\left(\varepsilon^2 + \delta + \frac{\varepsilon_\TV^2}{\delta}\right), 
\end{align}
where 
%\begin{align}
$
\Xi_{\delta,\varepsilon} := C_\rho^2 (1+2R)^2 \delta  
+  C_\rho^2 \frac{1 + \delta R_\varphi^2}{\delta} [\varepsilon^2+ \Lipdp^2 d(h + \mathsf{m} h^2)], 
%\varepsilon_{\TV}^2
%+  R_\varphi^2  [\varepsilon^2 + \Lipdp^2 d(\delta + \mathsf{m} \delta^2)],
%&  = \gO\left(\delta + \varepsilon^2 + \frac{\varepsilon_\TV^2}{\delta}\right),
$ and 
$R_\varphi$ and $L_\varphi$ are constants introduced in Lemma \ref{lemm:phiYboundLip}. 
\end{thm}


\subsection{Bounding the smoothness}
The proof of Theorem \ref{thm:Diffusion-1} (i.e., Theorem \ref{thm:Diffusion-1-appendix}) utilizes the smoothness of the density $q_t$ corresponding to the aligned model. The following lemma gives its bound. 

\begin{lem}\label{lem:Smoothness}
    Suppose that $\nabla \log p_t$ is $L_p$-Lipschitz for all $t$ and $\nabla \log \rho_*$ is $L_\rho$-Lipschitz, and $C_\rho^{-1}\leq \rho_*\leq C_\rho$. 
    Then, $\nabla\log  q_t$ is $L_q$-Lipschitz for all $t$, where $L_q$ is bounded by
    \begin{align}
        L_q \leq \min \bigg\{\frac{4(L_p+L_\rho)^2}{2(L_p+L_\rho)-1}, (4+C_\rho^2) L_p + 4L_\rho\bigg\}\lesssim L_pC_\rho^2+L_\rho.
    \end{align}
\end{lem}
\begin{proof}
    We divide the proof into two parts, with $\sigma_t^2 = \frac{1}{2(L_p + L_\rho)}$ as the boundary.
    
    First consider the case when $\sigma_t^2 \leq \frac{1}{2(L_p + L_\rho)}$. 
    Remind that
    \begin{align}
        q_t (x) = \int q_*(y)\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\bigg(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\bigg)\mathrm{d}y,
    \end{align}
    with $\sigma_t^2 = 1-e^{-2t}$ and $m_t = e^{-t}$. 
    From this, $\nabla_x q_t(x)$ and $\nabla_x^2 q_t(x)$ are computed as
    \begin{align}
        \nabla_x q_t(x) &= \nabla_x \int q_*(y)\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y 
        % \\ &= \int q_*(y)\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{x-m_ty}{\sigma_t^2}\mathrm{d}y
        \\ &= m_t^{-1}\int(\nabla_y q_*(y))\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y
    \end{align}
    and
    \begin{align}
        \nabla_x^2 q_t(x) 
       & = m_t^{-2}\int(\nabla_y^2 q_*(y))\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y
       .
    \end{align}
    Thus, we can compute $\nabla^2_x \log q_t(x)$ as
    \begin{align}
      &  \nabla^2_x \log q_t (x) 
      \\ &= \frac{\nabla^2_x q_t (x)}{q_t (x)} - \frac{\nabla_x q_t (x)(\nabla_x q_t (x))^\top }{(q_t (x))^2}
    \\ &= \frac{ m_t^{-2} \int (\frac{\nabla_y^2 q_*(y)}{q_*(y)}-\frac{\nabla_y q_*(y)\nabla_y q_*(y)^\top}{q_*(y)^2})q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}
    \label{eq:Appendix-Diffusion-Smoothness-1}
    \\ &\quad + \frac{ m_t^{-2} \int \frac{\nabla_y q_*(y)\nabla_y q_*(y)^\top}{q_*(y)^2} q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}
    \label{eq:Appendix-Diffusion-Smoothness-2}
       \\ &\quad -\frac{m_t^{-2} \int \frac{\nabla_y q_*(y)}{q_*(y)}q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y\int \frac{\nabla_y q_*(y)}{q_*(y)}q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}{(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y)^2}.
       \label{eq:Appendix-Diffusion-Smoothness-3}
        % \\
        % &= \frac{m_t^{-1} \int \nabla_y q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y},
    \end{align}
    Eq.~(\ref{eq:Appendix-Diffusion-Smoothness-1}) is an expectation of $\nabla_y^2 \log q_*(y) = \frac{\nabla_y^2 q_*(y)}{q_*(y)}-\frac{\nabla_y q_*(y)\nabla_y q_*(y)^\top}{q_*(y)^2}$ with respect to a distribution 
    \begin{align}
        A(y|x) \propto q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big).
    \end{align}
    Therefore, (\ref{eq:Appendix-Diffusion-Smoothness-1}) is bounded by $m_t^{-2}(L_p+L_\rho)$ from the assumption.
    On the other hand, the other two terms are regarded as the covariance of $\nabla \log q_*(y)$ with respect to that distribution.
    Because $\sigma_t^2 \leq \frac{1}{2(L_p + L_\rho)}$, $A(y|x)$
    is $(L_p + L_\rho)$-strongly concave, and therefore satisfies the Poincar\'e inequality with a constant $\frac{1}{L_p + L_\rho}$. Therefore, for any $a\in \R^d$, we have
    \begin{align}
  &  a^\top (\text{(\ref{eq:Appendix-Diffusion-Smoothness-2})}+\text{(\ref{eq:Appendix-Diffusion-Smoothness-3})})a
  \\ &  = m_t^{-2}a^\top(\mathbb{E}_{A(y|x)}[(\nabla \log q_*(y))(\nabla \log q_*(y))^\top] - \mathbb{E}_{A(y|x)}[\nabla \log q_*(y)]\mathbb{E}_{A(y|x)}[\nabla \log q_*(y)]^\top)a
      \\ &    \leq \frac{m_t^{-2}}{L_p + L_\rho} \cdot \mathbb{E}[\|a\nabla^2 \log q_*(y)\|^2]\leq \frac{m_t^{-2}}{L_p + L_\rho}(L_p+L_\rho)^2 = m_t^{-2}L_p+L_\rho.
    \end{align}
    This implies that (\ref{eq:Appendix-Diffusion-Smoothness-2})$+$(\ref{eq:Appendix-Diffusion-Smoothness-3}) is $m_t^{-2}(L_p+L_\rho)$-smooth and $\nabla_x^2 \log q_t(x)$ is $2m_t^{-2}(L_p+L_\rho)$-smooth.
    Because $m_t = \sqrt{1-\sigma_t^2}$, we have $m_t \geq \sqrt{1-\frac{1}{2(L_p+L_\rho)}}.$ By applying this, we have $2m_t^{-2}(L_p+L_\rho)\leq \frac{4(L_p+L_\rho)^2}{2(L_p+L_\rho)-1}$.

    Next, let us consider the case when $\sigma_t^2 \geq \frac{2}{(L_p + L_\rho)}$. 
    Note that  $\nabla_x q_t(x)$ and $\nabla_x^2 q_t(x)$ are also written as
    \begin{align}
        \nabla_x q_t(x) &= 
        -\int q_*(y)\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{x-m_ty}{\sigma_t^2}\mathrm{d}y
    \end{align}
    and
    \begin{align}
       & \nabla_x^2 q_t(x) 
     \\  & =-\int q_*(y)\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{I}{\sigma_t^2}\mathrm{d}y   
      \\ & \quad  + \int q_*(y)\frac{1}{(2\pi\sigma_t^2)^{\frac{d}{2}}}\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{(x-m_ty)(x-m_ty)^\top}{\sigma_t^4}\mathrm{d}y.
    \end{align}
    Thus, we can compute $\nabla_x^2 \log q_t(x)$ as
    \begin{align}
      &  \nabla^2_x \log q_t (x) 
     \\  &= \frac{\nabla^2_x q_t (x)}{q_t (x)} - \frac{\nabla_x q_t (x)(\nabla_x q_t (x))^\top }{(q_t (x))^2}
      \\ &=  -\frac{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{I}{\sigma_t^2}\mathrm{d}y }{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}
       \label{eq:Appendix-Diffusion-Smoothness-4}
      \\ & \quad  + \frac{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{(x-m_ty)(x-m_ty)^\top}{\sigma_t^4}\mathrm{d}y}{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}
       \label{eq:Appendix-Diffusion-Smoothness-5}
      \\ & \quad - \frac{\big(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{(x-m_ty)}{\sigma_t^2}\mathrm{d}y\big)\big(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\frac{(x-m_ty)}{\sigma_t^2}\mathrm{d}y\big)^\top}{\big(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y\big)^2}.
       \label{eq:Appendix-Diffusion-Smoothness-6}
    \end{align}
    Eq.~(\ref{eq:Appendix-Diffusion-Smoothness-4}) is bounded by $\sigma_t^{-2}\leq 2(L_p+L_\rho)$. 
    On the other hand, (\ref{eq:Appendix-Diffusion-Smoothness-5})+(\ref{eq:Appendix-Diffusion-Smoothness-6}) are transformed into
    \begin{align}
       & \text{(\ref{eq:Appendix-Diffusion-Smoothness-5})+(\ref{eq:Appendix-Diffusion-Smoothness-6})}
       \\ & = \frac{m_t^2}{\sigma_t^4}\frac{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)yy^\top\mathrm{d}y}{\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y}
      \\ & \quad - \frac{m_t^2}{\sigma_t^4}\frac{\big(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)y\mathrm{d}y\big)\big(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)y\mathrm{d}y\big)^\top}{\big(\int q_*(y)\exp\big(-\frac{1}{2\sigma_t^2}\|m_t y - x\|^2\big)\mathrm{d}y\big)^2}
      % \\ &  = \frac{\int q_*(\frac{z-x}{m_t})\exp\big(-\frac{\|z\|^2}{2\sigma_t^2}\big)\frac{zz^\top}{\sigma_t^4}\mathrm{d}z}{\int q_*(\frac{z-x}{m_t})\exp\big(-\frac{\|z\|^2}{2\sigma_t^2}\big)\mathrm{d}z}
      % \\ & \quad - \frac{\big(\int q_*(\frac{z-x}{m_t})\exp\big(-\frac{\|z\|^2}{2\sigma_t^2}\big)\frac{z}{\sigma_t^2}\mathrm{d}z\big)\big(\int q_*(\frac{z-x}{m_t})\exp\big(-\frac{\|z\|^2}{2\sigma_t^2}\big)\frac{z}{\sigma_t^2}\mathrm{d}z\big)^\top}{\big(\int q_*(\frac{z-x}{m_t})\exp\big(-\frac{\|z\|^2}{2\sigma_t^2}\big)\mathrm{d}z\big)^2}
     \\ & =\frac{m_t^2}{\sigma_t^4}\mathrm{Var}_{q_{0|t}(y|x)}(y),
    \end{align}
    where $\mathrm{Var}_{q_{0|t}(y|x)}(y)$ means the variance of $X_0$ conditioned on $X_t=x$, with respect to $q_t$.

    Thus, bounding $\mathrm{Var}_{q_{0|t}(y|x)}(x)$ yields the conclusion. 
    Lemma~\ref{lem:Bounded-Discrepancy} implies that
    \begin{align}
        \mathrm{Var}_{q_{0|t}(y|x)}(y) \leq C_\rho^2 \mathrm{Var}_{p_{0|t}(y|x)}(y).
        \label{eq:Appendix-Diffusion-Smoothness-7}
    \end{align}
    Similarly to $\nabla_x^2 \log q_t(x)$, $\nabla_x^2 \log p_t(x)$ satisfies
    \begin{align}
        \nabla_x^2 \log p_t(x) = -\sigma_t^{-2}I_d + \frac{m_t^2}{\sigma_t^4}\mathrm{Var}_{p_{0|t}(y|x)}(y).
        \label{eq:Appendix-Diffusion-Smoothness-8}
    \end{align}
    By combining (\ref{eq:Appendix-Diffusion-Smoothness-7}) and (\ref{eq:Appendix-Diffusion-Smoothness-8}), we have
    \begin{align}
        \frac{m_t^2}{\sigma_t^4} \mathrm{Var}_{q_{0|t}(y|x)}(y) \leq C_\rho^2
        \sigma_t^{-2}I_d + C_\rho^2\nabla_x^2 \log p_t(x).
    \end{align}
    Therefore, from the assumption that $\nabla_x \log p_t(x)$ is $L_p$-Lipschitz and $\sigma_t^{-2}\geq 2(L_p+L_\rho)$, we obtain that $ \|\frac{m_t^2}{\sigma_t^4} \mathrm{Var}_{q_{0|t}(y|x)}(y)\|\leq (2+C_\rho^2) L_p + 2L_\rho$. 

    By putting it all together, $\nabla \log q_t$ is $((4+C_\rho^2) L_p + 4L_\rho)$-Lipschitz.
\end{proof}

\begin{lem}\label{lem:Bounded-Discrepancy}
    When $C_\rho^{-1}\leq h_*(x)\leq C_\rho$, we have
    \begin{align}
        \frac{q_{0|t}(x|x')}{p_{0|t}(x|x')}\leq C_\rho^2 
    \end{align}
    for all $x,x'$ and $t$.
\end{lem}
\begin{proof}
    We can write $q_{0|t}(x|x')$ as
    \begin{align}
        q_{0|t}(x|x') = \frac{q_{0,t}(x,x')}{\int q_{0}(x'')q_{t|0}(x|x'')\mathrm{d}x''}
        =  \frac{p_*(x)\rho_*(x)q_{t|0}(x'|x)}{\int p_*(x'')\rho_*(x'')q_{t|0}(x|x'')\mathrm{d}x''}.
    \end{align}
    Because $C_\rho^{-1}\leq \rho_*(x)\leq C_\rho$ and $q_{t|0}(x'|x)=p_{t|0}(x'|x)$, we have
    \begin{align}
        q_{0|t}(x|x') \leq  C_\rho^2\frac{p_*(x)p_{t|0}(x'|x)}{\int p_*(x'')p_{t|0}(x|x'')\mathrm{d}x''}
        = C_\rho^2p_{0|t}(x|x'),
    \end{align}
    which concludes the proof.
\end{proof}

\subsection{Estimation Error of the Correction Term}
\label{sec:proofOfCorrection}
% \begin{lem}
%     Suppose that $\nabla \log p_t$ is $L_p$-smooth.
%     Also assume that the error of the score function $s(x,t)$ is bounded by $\varepsilon$ in $L^\infty$-norm for all $t=kh$. 
%     If we independently sample $\{\tilde{y}_{T,i}\}_{i=1}^n$ using (\ref{eq:Appendix-Diffusion-Preliminary-9}), we have
%     \begin{align}
%      \bigg| \frac1n\sum_{i=1}^n \rho'(\tilde{y}_{T,i})-  \mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x] \bigg|\lesssim  \|\rho'-\rho_*\|_{\infty}+
%     T^\frac12 \varepsilon +  TL_pd^\frac12 h^\frac12+\frac{C_\rho}{\sqrt{n}}\log (\delta^{-1}) 
%     \end{align}
%     with probability $1-\delta$. 
% \end{lem}
% \begin{proof}
%     According to \cite{chen2023improved} (Theorem 1 without the term on the convergence of forward process) and Pinsker's inequality, we have
%     \begin{align}
%         \TV(\tilde{y}_{T,i},\bar{X}_0|\bar{X}_{T-t}=x)
%         \lesssim \mathrm{KL}(\tilde{y}_{T,i},\bar{X}_0|\bar{X}_{T-t}=x)^\frac12 \lesssim T^\frac12 \varepsilon +  TL_pd^\frac12 h^\frac12.
%     \end{align}
%     Because $\rho_*$ is bounded by $C_\rho$, 
%     \begin{align}
%       |\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{T-t}=x]
%         -
%         \mathbb{E}[\rho_*(\tilde{y}_{T,i})]| 
%         &\leq \|h\|_\infty\mathrm{TV}(\tilde{y}_{T,i},\bar{X}_0|\bar{X}_{T-t}=x)
%      \\ &   \lesssim C_\rho(T^\frac12 \varepsilon +  TL_pd^\frac12 h^\frac12).
%         \label{eq:Appendix-Correction-Approximation-1}
%     \end{align}
%     Bounding $\frac1n\sum_{i=1}^n \rho_*(\tilde{y}_{T,i}) -\mathbb{E}[\rho_*(\tilde{y}_{T,i})]$ by Hoeffding's inequality, we have
%     \begin{align}
%         \bigg|\frac1n\sum_{i=1}^n \rho_*(\tilde{y}_{T,i}) -\mathbb{E}[\rho_*(\tilde{y}_{T,i})]\bigg|
%         \leq \frac{C_\rho}{\sqrt{2n}}\log (2\delta^{-1})
%         \label{eq:Appendix-Correction-Approximation-2}
%     \end{align}
%     with probability $1-\delta$. 
%     By combining (\ref{eq:Appendix-Correction-Approximation-1}) and (\ref{eq:Appendix-Correction-Approximation-2}) and replacing $\frac1n\sum_{i=1}^n \rho_*(\tilde{y}_{T,i})$ by $\frac1n\sum_{i=1}^n \rho'(\tilde{y}_{T,i})$, we obtain the assertion. 
% \end{proof}


\input{hTransformErrorAnalysis}

\subsection{Doob's H-transform}
\begin{lem}\label{lem:H-transform}
    % Eq.~\ref{eq:Appendix-Diffusion-Preliminary-4} holds.
    For all $t\in [0,T]$, the following relationship holds.
    \begin{align}
    \nabla_x \log q_{t}(x) = \nabla_x \log p_{t}(x) + \nabla_x \log (\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{t}=x]).
    \label{eq:Appendix-Diffusion-Doob-1}
    \end{align}
\end{lem}
\begin{proof}
    Let us denote the joint distribution of $\bar{X}_0$ and $\bar{X}_t$ as $p_{0,t}(\bar{X}_0,\bar{X}_t)$, the conditional distributions of $\bar{X}_0$ given $\bar{X}_t$ as $p_{0|t}(\bar{X}_0|\bar{X}_t)$, and the conditional distributions of $\bar{X}_t$ given $\bar{X}_0$ as $p_{t|0}(\bar{X}_t|\bar{X}_0)$. Define $q_{t|0}$ in the same way. 
    It is straightforward to see that %verify (\ref{eq:Appendix-Diffusion-Doob-1}): 
    \begin{align}
        \log p_{t}(x) + \log (\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{t}=x])
        &= \log (p_{t}(x)\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{t}=x])
        \\ & = \log p_{t}(x)\int_{x'} \rho_*(x')p_{0|t}(x'|x)\mathrm{d}x'
        \\ & = \log \int_{x'} \rho_*(x')p_{0,t}(x',x)\mathrm{d}x'
        \\ & = \log \int_{x'} \rho_*(x')p_0(x')p_{t|0}(x|x')\mathrm{d}x'
        \\ & = \log \int_{x'} q_0(x')p_{t|0}(x|x')\mathrm{d}x'.
        \label{eq:Appendix-Diffusion-Doob-2}
    \end{align}
    Note that $p_{t|0}(x|x')$ and $q_{t|0}(x|x')$ are the same in (\ref{eq:Appendix-Diffusion-Doob-2}). Therefore,
    \begin{align}
        \log p_{t}(x) + \log (\mathbb{E}[\rho_*(\bar{X}_0)|\bar{X}_{t}=x]|)
       & = \log \int_{x'} q_0(x')q_{t|0}(x|x')\mathrm{d}x'
        \\ & = \log q_t(x), 
    \end{align}
    which concludes the proof.
\end{proof}

