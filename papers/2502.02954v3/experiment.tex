
In this Appendix, we compare our method with existing approaches in a very simple numerical experiment and confirm that our method is also capable of performing image generation alignment.
A more practical and clearer comparison using realistic benchmarks is left for future work.

\subsection{Alignment for Gaussian Mixture Models}

We explain how to align the pre-trained diffusion model for Gaussian Mixture Models and the technical details of our algorithm. Almost the same algorithm was used for the other experiments.

\textbf{A Pre-Trained Score-Based Diffusion Model}
We pre-trained a score-based diffusion model to sample from the 2 dimensional mixture of Gaussian Mixture Models. The target density was $\frac{1}{2}\left(\mathcal{N}(\mu_1,\Sigma) + \mathcal{N}(\mu_2,\Sigma)\right)$, $\mu_1 = [-2.5, 0], \; \mu_2 = [2.5, 0], \; \Sigma = [[1, 0],[0,5]]$. The score model was implemented as simple 4 layer neural networks. The learning rate of pre-train was 0.0005, the batch size was 100, the number of epochs was 1000. The pre-train MSE loss is shown in the left side in Figure~\ref{fig:mog_pretrain}. The minimum losses until the current epoch were plotted. The histogram of 20000 samples from the pre-trained score-based diffusion model is the right figure in Figure~\ref{fig:mog_pretrain}. For sampling, $T$ was set to be 10 and the number of sampling steps was 100.
\begin{figure}[htbp]
\centering
\begin{minipage}[htbp]{0.49\columnwidth}
    \centering
    \includegraphics[height = 5cm]{figures/MoG_pretrain_loss.png}
    \label{fig:mog_pretraiin_hist}
\end{minipage}
\begin{minipage}[htbp]{0.49\columnwidth}
    \centering
    \includegraphics[height=4.65cm]{figures/MoG_ref_hist.png}
    \label{fig:b}
\end{minipage}
\caption{
\textbf{Left.} Pre-train MSE loss of denoising score matching. The minimum losses until the current epoch were plotted.
\textbf{Right.} The histogram of 20000 samples from the pre-trained DDPM.}
\label{fig:mog_pretrain}
\end{figure}

\textbf{The objective}
The target was the mean of the Gaussian in the right side, $\mu_w := \mu_2 = [2.5,0]$.
The preference of point $x_w$ and $x_l$ were determined by the Euclidean distance $d(\cdot, \mu_w) $ from $\mu_w := [2.5,0]$. $x_w \succ x_l$ if and only if $d(x_w,\mu_w) < d(x_l,\mu_w)$.
The DPO objective was used in this setting. 2000 points from $\pref$ were sampled to calculate the expectation of the functional derivative. 
Note that the reference objectives\footnote{the DPO loss of Ref. in Table~\ref{table:comparison}, our algorithm at $k=0$ before the random initialization of the potential, and the initial (true and approx.) objectives in Diffusion-DPO in Figure~\ref{fig:da-summary} and \ref{fig:da-unsmoothed}} can be (and were) analytically calculated because $q/\pref = 1$.
The regularization terms $\beta$ and $\gamma$ were 0.04 and 0.1. The metric loss was calculated with the samples s.t. $d(x,\mu_w)<10$ in random 1000 samples.

As a counter method, Diffusion-DPO~\citep{Wallace2024DiffusionDPO} was implemented with the learning rate = 0.0005 and 0.0001 for w/ reg and w/o reg respectively, and batch size = 5000.
We remark that, in practice, it is hardly realistic to compute the true DPO objective during Diffusion-DPO, but in this case, we forcefully carried out the computation by estimating the density ratio $q(x)/\pref(x)$ with $100 \times 2$ samples for each $x$: The densities $q$ and $\pref$ were estimated by repeatedly computing the denoising path and empirically obtaining their marginal densities, which implies some degree of numerical instability in Figure~\ref{fig:da-summary} and additional computational cost\footnote{``Opt.time" and ``GPU memory "also include the time and space required to compute the ``True Objective"}  in Table~\ref{table:comparison}.

Please note that the reason why the values of ``Upper bound"\footnote{We took $\omega(\lambda_t)$ in (2) in \cite{Wallace2024DiffusionDPO} to be 0.5 (constant, as recommended in the paper). 
%Changing $\omega$ by a constant affects the value of the upper bound, however, it does not significantly affect the convergence of the true DPO loss (w/o reg).
We will leave the investigation of the phenomena that arise when \(\omega\) is turned into a time-dependent function for future work.
}
 and ``True Objective" in the existing method shown in Figure~\ref{fig:da-summary} are reversed is that their ``Upper bound" approximates the true DPO loss by replacing the current reverse process with the ROU process~\citep{Wallace2024DiffusionDPO}.
This implies that the upperbound may be too loose to be used as a surrogate of the true loss.

The phenomenon that the ``True Objective w/o reg." of the existing method diverges upward in Figure~\ref{fig:da-summary} is likely due to the absence of additional entropy regularization, leading to catastrophic forgetting~\cite{uehara2024RLHF,tang2024finetuningdiffusionmodelsstochastic,li2024derivativefreeguidancecontinuousdiscrete,zhao2024scoresactionsframeworkfinetuning}.
On the other hand,  when applying our method, we incorporate entropy regularization.

Additionally, some degree of numerical instability is unavoidable due to the experimental methodology because we empirically estimated the density ratio to compute the true DPO loss in using Diffusion-DPO.

\textbf{Dual Averaging in Option 1}
We implemented Option 1 for the experiment. We used 8 NVIDIA V100 GPUs with 32GB memory.
The plotted losses (smoothed with EMA) in Figure~\ref{fig:da-summary} demonstrated that DA algorithm decreased the losses.
Please refer to Figure~\ref{fig:da-unsmoothed} for losses without smoothing.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/MoG_unsmoothed_0305.png}
    \caption{(For Reference) Losses without smoothing corresponding to Figure~\ref{fig:da-summary}. ``D-DPO": Diffusion-DPO. ``Ours": Proposed DA.}
    \label{fig:da-unsmoothed}
\end{figure}
We estimated the log-density ratio $\gbarkprev = -\log \qstark / \pref + \mathrm{const}.$ with neural networks $f_k \simeq \gbarkprev$. In each loop, the potential was trained by 1000 points in $x_1, x_2 \in [-5,5]$, the learning rate was 0.0005, the number of epochs was 1000.
We kept $\gbarkprev$ as neural networks and generate the dataset for $\gbark$ from the equation ($\beta=\beta'$ for simplicity)
\begin{align}
            \gbark &= \frac{2}{\beta(k+1)(k+2)}
        \left[
            \frac{\beta k (k+1)}{2}\gbarkprev + k\dFdq(\qk)
        \right]\\
        &= \frac{k}{k+2}\gbarkprev + \frac{2k}{\beta k(k+2)}\dFdq(\qk).\label{eq-appendix-experiment-recurrence}
\end{align}
So, we only need one model to store for DA loops (we don't need $\bar{g}^{(k-2)},..., \bar{g}^{(1)}$). 
The psudocode for this phase is described as Algorithm~\ref{alg:DA-train-f}.
    
\begin{algorithm}[htbp]
\label{alg:DA-train-f}
\caption{Dual Averaging (Option 1)}
\begin{algorithmic}
    \REQUIRE{
            $F$: an objective,
            $s$: pre-trained score,
            $\beta$: Regularization scale,
            $K$: number of loops.
        }\\
    \ENSURE{
            $f_{K}$: a trained potential.
    }
    \STATE Define $q^{(0)}=\pref$ as the reference density obtained by the pre-trained score $s$ 
    \STATE Initialize NN $f_1$ randomly
    \STATE Collect samples $(x_i)_i$ (e.g. collect data at the final denoising step from $\pref$ by score function $s$)\\
    %\STATE We define $\qzero \propto \exp(-f_0)\pref$ (no actual computation)\\
    %\STATE Construct dataset $\lbrace (x_i, \frac{1}{3\beta}\dFdq(q^{(0)},x_i)\rbrace_i$ with $(x_i)_i$ and $f_0$ using equation (\ref{eq-main-DPO-derivative})\\
    %\STATE Train $f_1$ to approximate $\bar{g}^{(0)} = \frac{1}{3\beta}\dFdq(q^{(0)})$ \\
    \FOR{$k = 1,...,K-1$}
        \STATE $\qk \propto \exp(-f_k)\pref$ (no actual computation)
        \STATE Construct dataset $\lbrace (x_i, \frac{2k}{\beta k(k+2)}\dFdq(\qk,x_i)\rbrace_i$ with $(x_i)_i$ and $f_k$ using equation (\ref{eq-main-DPO-derivative})\\
        \STATE Train $f_{k+1}$ to approximate $\frac{k}{k+2}f_{k} + \frac{2k}{\beta k(k+2)}\dFdq(\qk)$ by minimizing MSE.
    \ENDFOR
    \STATE \textbf{End}
\end{algorithmic}
\end{algorithm}

The heat map of the trained potential $f_k$ is shown in Figure~\ref{fig:MoG-heatmap}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/MoG_potential_heatmap.png}
    \caption{The heatmap of the potential $f_k$ in the $k$th loop in Dual Averaging for Gaussian Mixture Model. The target point was $[2.5,0]$. Note that $f_k$ is the negated log-density ratio: the aligned density is $\exp(-f_k)\pref$.}
    \label{fig:MoG-heatmap}
\end{figure}

\textbf{Doob's h-transform}
We sampled the aligned images with 50 diffusion steps. The guidance term of Doob's h-transform was calculated in every diffusion steps. The conditional expectation $\E[h_T(\Xref_T) \mid x]$ was calculated by Monte Carlo with 30000 samples. The psudocode for this phase is described as Algorithm~\ref{alg:doob-sampling}.
\revisedStart
In phase 2 (the sampling phase), our simplest solution (i.e., estimating the correction term at each time step using Monte Carlo) has a time complexity of $\mathcal{O}(L^2)$, where $L$ represents the number of time steps in the denoising process. When we set $N$ as the number of particles needed to estimate one correction term, to compute the correction term for each sample simultaneously, $ \mathcal{O}(N)$ memory space is required. The sampling error for each correction term would be $\mathcal{O}(1/\sqrt{N})$. This leads to severer additional computational effort in phase 2 than the time and space consumed as in Table~\ref{table:comparison} for Phase I\footnote{In Table~\ref{table:comparison}, we showed the computational cost for Phase I, not for Phase II because we only require Phase I to compute the true loss. Note that we used Phase II to compute the metric loss, however, the true loss is much more important.}.
However, this Doob's h-transform technique itself has been used in image generation~\citep{uehara2024RLHF,uehara2024reward}, Bayesian samping~\citep{heng2024schrodingerbridge}, and filtering~\citep{chopin2023doob}. 
As a more practical alternative of our phase 2, the idea of approximating the correction term using neural ODE solvers for faster test-time implementation has also been proposed in ~\citep{uehara2024reward,uehara2024RLHF}.
\revisedEnd

\begin{algorithm}[htbp]
\caption{Doob's h-transform (A simplest implementation)}
\begin{algorithmic}\label{alg:doob-sampling}
    \REQUIRE{
            $F$: an objective,
            $f_K$: a trained potential, 
            $s$: pre-trained score,
        }\\
    \ENSURE{
            $x_T$: an output approximately from $\exp{(-f_K)}\pref$.
    }
    \STATE Initialize $x_0$ as white noise
    \STATE Set number of steps $L$ and the time $T$.
    \STATE Set the step size $\stepsize = T/L$
    \FOR{$l = 0,...,L-1$}
        \revisedStart
        \STATE Initialize $N$ samples $(x_{l,i})_{i\leq N}$ as $x_l$.
        \FOR{$l' = l,...,L-1$}
            \STATE (denoising step of $(x_{l,i})_{i\leq N}$)
        \ENDFOR
        \STATE Store $N$ samples of $\Xref_T$ as $(x_{l,i})_{i\leq N}$.
        \STATE Approximate $u(x_{l},lh) = \nabla \log \E[\exp{(-f_K(\Xref_T)} \mid \Xref_{\tprev}=x_{l}]$ by Monte Carlo with $(x_{l,i})_{i\leq N}$. %in (\ref{eq:experiment-doob}) by Monte Carlo 
        \revisedEnd
        \STATE Sample white noise $\xi_\mathrm{noise}$
        \STATE $x_{l+1} \coloneq x_{l} + \delta (x_{l} + 2 s(x_{l},T-\tprev) + 2 u(x_{l},lh)) + \sqrt{2h} \xi_\mathrm{noise}$
    \ENDFOR
    \STATE \textbf{End}
\end{algorithmic}
\end{algorithm}
\begin{comment}
======================= this part may not be needed =======================

In Doob's h-transform, $\nabla \log h_{l\delta}(x)$ is calculated as
\begin{align}
    &\nabla \log h_{l\delta}(x)\\
    =&\nabla \log \E_{\pmREF}[h_T(\Xref_T) \mid \Xref_{l\delta} = x]\\
    =& 
    \frac{
        \int
        \E_{\pmREF}[h_T(\Xref_T)\mid \Xref_{\tnext}=y]\frac{\partial}{\partial x}\mathcal{N}(\Xref_{\tnext}\mid m_{l\delta}(x), \sigma^2_{l\delta})\mathrm{d}y
    }{
        \int
        \E_{\pmREF}[h_T(\Xref_T)\mid \Xref_{\tnext}=y]\mathcal{N}(y \mid m_{l\delta}(x), \sigma^2_{l\delta})\mathrm{d}y
    }\label{eq:experiment-doob},
\end{align}
where $m_{l\delta}(x) = e^\delta x + (-2 s(x, T-\tprev))(1-e^\delta), \; \sigma^2_{l\delta} = e^{2\delta} - 1$, $\mathcal{N}(y\mid \mu,\sigma^2) \coloneq \frac{1}{\sqrt{2\pi\sigma^2}^d}\exp{\left(-\frac{\|y-\mu\|^2}{2\sigma^2}\right)}$.
The partial derivative $\frac{\partial}{\partial x}\mathcal{N}(y \mid m_{l\delta}(x), \sigma^2_{l\delta})$ is
\begin{equation}
    \frac{\left(\frac{\partial}{\partial x}m_{l\delta}(x)^\top \right) (m_{l\delta}(x) - y)}{\sigma_{l\delta}^2}\mathcal{N}(y \mid m_{l\delta}(x), \sigma^2_{l\delta}).
\end{equation}
Note that $\left(\frac{\partial}{\partial x}m_{l\delta}(x)^\top \right)$ is a $d \times d$ matrix and $\left(\frac{\partial}{\partial x}m_{l\delta}(x)^\top \right)_{ij} = \frac{\partial}{\partial \evx_i}m_{l\delta}(x)_j$.

=====================================================================
\end{comment}
\subsection{Image Generation Alignment}\label{sec:Appendix-Experimennt}

We aligned the image generation of the basic pre-trained model in Diffusion Models Course (source: \cite{huggingfacecourse}). The pipeline path we utilized was ``\texttt{johnowhitaker/ddpm-butterflies-32px}". The summarized results are show in Figure~\ref{fig:BT-summary}.

\textbf{The pre-trained Model}
The model samples the images of butterflies of $32 \times 32$ pixels. Number of the sampling step was 1000.

\textbf{The objective}
The target color was $[0.9,0.9,0.9]$ in RGB. The reward is visualized in Figure~\ref{fig:BT-compare}.
6400 samples from $\pref$ to calculate the expectation of $F$ and $\dFdq$. $\beta$ and $\gamma$ were $0.05$ and $1$. The output of the functional derivative was clipped in $\pm20$ to stabilize the training step. In calculation of the DPO objective, the sample that has a higher reward is the ``winning" sample.

\textbf{Dual Averaging}
 In each loop, $f_k$ was trained by 1024 images from pooled 6400 images, the learning rate was 0.0001, the batch size was 64, and the number of epochs was 5. $f_k$s were implemented by Unet2Dmodel in Diffusers library~\cite{huggingface2022Diffusers}. How the potential $f_k$ learned the reward, the distance from the target color, was shown in the right side of Figure~\ref{fig:BT-compare}. The DA algorithm succeeded to extract the target images from the true reward.

 \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/BT_compare_reward.png}
    \caption{\textbf{Left.} Output images of $\pref$ sorted by the distance from the target color ($=-$reward). \textbf{Right.} Images sorted by the learned potential in $k=2$.}
    \label{fig:BT-compare}
\end{figure}

\textbf{Doob's h-transform}
We sampled the aligned images with 1000 diffusion steps. The guidance term of Doob's h-transform was calculated in every 10 diffusion steps for faster sampling.
We calculated the drift term of Doob's h-transform once in 10 diffusion steps for faster sampling. In addition, We defined a decay rate $r_d = 0.95$ and a strongness $s=5$ for $\nabla \log \rho_{t}$ and finally we added $r_d^{m'} \nabla \log \E [\exp(- s f_K(X^\leftarrow_T)\mid x_{10m\delta }]$ as a drift term in $l = 10m + m'$th diffusion step to balance the computational cost and stability. The conditional expectation $\E[\rho_T(\Xref_T) \mid x]$ was calculated by Monte Carlo with 128 samples.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/BT_summary.png}
    \caption{\textbf{Left. }The smoothed loss during DA for image generation alignment from $k=1$. ``Objective'': DPO objective.
        ``Regularized Objective'': ``Objective'' + $\beta \KL(q\|\pref)$, $\beta=0.05$
    \textbf{Right.} Examples of aligned image generation. ``iter=2'': ours with $k=2$ DA iterations, ``iter=2'': ours with $k=1$ DA iteration. ``Reference'': samples from $\pref$.}
    \label{fig:BT-summary}
\end{figure}

\subsection{Tilt correction for Generation of Medical Image Data}

Our goal of this experiment was generating images with no rotation with an unconditional pre-trained model that generates rotated images. The objective was based on DPO. The rotation angle of each image was predicted with CNN.
The summarized results are in Figure~\ref{fig:CT-summary}.

\textbf{Dataset}
 10000 images of Head CT ($64 \times 64$ pixel) in Medical MNIST~\cite{MedicalMNISTClassification} were leveraged. They include some rotated images, the angles are up to $90\tcdegree$. To aggravate the existing situation, we augmented the data by rotating images (up to $45\tcdegree$) from this dataset to be 40000. A predictor of angles were trained with the augmented dataset to define the reward in place of human preference due to preparation difficulties. The number of epochs was 10, 95\% and 5\% of the dataset were used for training and validation, the training MSE loss was 3.79, and the validation MSE loss was 3.25.  Note that there were rotated images in the original dataset, so the labels of the rotation angles made in the augmentation were noisy.

\textbf{Pre-trained Autoencoder}
We pre-trained autoencoder from scratch. It encodes gray-scaled $64\times64$ pixels into latent $32\times 32$ pixels. It only has convolution layers so that geometrical features were preserved for simplicity. The training data was $95\%$ of augmented 40000 images and the validation data was $5\%$ of them. This autoencoder was fed white-noised data to make the model robust with noise. The pretraining MSE loss is shown in the left side of Figure~\ref{fig:CT-pretrain}.
\paragraph{Latent Diffusion Model}
We also pre-trained a (latent) diffusion model based on Unet2DModel in Diffusers~\cite{huggingface2022Diffusers}. Number of sampling steps was 1000. The beta scheduler was set to be \texttt{squaredcos\_cap\_v2}.
In pre-training, we leveraged the augmented dataset up to 20000 images, the number of epochs was 15, the batch size was 8, the learning rate was 0.0001. The pretraining MSE loss is shown in right side of Figure~\ref{fig:CT-pretrain} and the generated images are displayed in Figure~\ref{fig:CT-latentdiffusion}.
\begin{figure}[htbp]
\centering
\begin{minipage}[htbp]{0.49\columnwidth}
    \centering
    \includegraphics[height = 4cm]{figures/CT_autoencoder_loss.png}
    \label{fig:CT-autoencoder-loss}
\end{minipage}
\begin{minipage}[htbp]{0.49\columnwidth}
    \centering
    \includegraphics[height=3.95cm]{figures/CT_pretrain.png}
    \label{fig:CT-diffusion}
\end{minipage}
\caption{
\textbf{Left.} The MSE loss in pretraining the Autoencoder. The solid blue line and the dashed red represent the validation loss and the train loss.
\textbf{Right.} The MSE loss in pretraining the diffusion model.}
\label{fig:CT-pretrain}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/CT_latent_diffusion.png}
    \caption{\textbf{Top.} Outputs of pre-trained latent diffusion. \textbf{Bottom.} Decoded images of the outputs. }
    \label{fig:CT-latentdiffusion}
\end{figure}

\textbf{The objective}
The reward was defined to be $-|\text{predicted angle}|$ of the pre-trained predictor (in place of humans), described in the left side of Figure~\ref{fig:CT-compare}.
6400 samples from $\pref$ to calculate the expectation of $F$ and $\dFdq$. $\beta$ and $\gamma$ were $0.01$ and $0.1$. The output of the functional derivative was clipped in $\pm 5$ to stabilize the training step. In calculation of the DPO objective, the sample that has a higher reward (in the more vertical direction) is the ``winning" sample.


\textbf{Dual Averaging}
In each loop, $f_k$ was trained by 6400 images from $\pref$. All the generated images were reused during DA. The learning rate was 0.0001, the batch size was 64, and the number of epochs was 5. We compared the learned potential with the reference reward in Figure~\ref{fig:CT-compare}. We see that DA iterations worked well to replicate the reference reward.

\begin{figure}[htbp]
   \centering
    \includegraphics[width=0.8\linewidth]{figures/CT_compare_reward.png}
    \caption{\textbf{Left.} Output CT images of the pre-trained model, sorted by the absolute values of estimated angles ($=-$reward). \textbf{Right.} Output CT Images sorted by the trained potential in the $3$rd loop in DA.}
    \label{fig:CT-compare}
\end{figure}

\textbf{Doob's h-transform}
We sampled the aligned images with 1000 diffusion steps. The guidance term of Doob's h-transform was calculated once in 10 diffusion steps. Technical settings were the same with . The conditional expectation was calculated by Monte Carlo with 128 samples. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/CT_summary.png}
    \caption{\textbf{Left. }The smoothed loss during DA for tilt correction from $k=1$. ``Objective'': DPO objective. The target point was $[2.5, 0]$.
        ``Regularized Objective'': ``Objective'' + $\beta \KL(q\|\pref)$, $\beta=0.01$
    \textbf{Right.} Tilt-corrected Head CT image generation. ``iter=3'': ours with $k=3$ DA iterations, ``iter=1'': ours with $k=1$ DA iteration. ``Reference'': samples from $\pref$.}
    \label{fig:CT-summary}
\end{figure}
