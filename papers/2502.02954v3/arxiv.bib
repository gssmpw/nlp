%%%%%%%%%%%%%%%%%%%%%%%%%% EXPERIMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{huggingfacecourse,
  author = {HuggingFace},
  title = {The Hugging Face Diffusion Models Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year = {2022}
}
% note = "[Online; accessed 2024-09-20]"

@misc{huggingface2022Diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}

% this mnist is not the Medical MNIST we used...
% inproceedings{MedicalMNISTClassification,
%    title={MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis},
    author={Yang, Jiancheng and Shi, Rui and Ni, Bingbing},
    booktitle={IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
    pages={191--195},
    year={2021}
}

% the following one is correct:
@misc{MedicalMNISTClassification,
author = {Arturo Polanco Lozano},
title = {Medical MNIST Classification},
year = {2017},
publisher = {GitHub},
booktitle = {GitHub repository},
url = {https://github.com/apolanco3225/Medical-MNIST-Classification}
}

The main parts of the proof follow existing papers. The key difference lies in not using a Langevin sampler in the inner loop, which slightly changes how the error
%%%%%%%%%%%%%%%%%%%%%%%%%% DIFFUSION MODELS %%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{chen2023improved,
  title={Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions},
  author={Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
  booktitle={International Conference on Machine Learning},
  pages={4735--4763},
  year={2023},
  organization={PMLR}
}

@InProceedings{Sohl-Dickstein2015thermodynamics,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html}
}

@inproceedings{song2019generatiive,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{ho2020DDPM,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{
song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@inproceedings{vahdat2021latent,
 author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11287--11302},
 publisher = {Curran Associates, Inc.},
 title = {Score-based Generative Modeling in Latent Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
 volume = {34},
 year = {2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% Reinfocement learning %%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{ziegler2020LLMft,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

@misc{uehara2024RLHF,
      title={Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review}, 
      author={Masatoshi Uehara and Yulai Zhao and Tommaso Biancalani and Sergey Levine},
      year={2024},
      eprint={2407.13734},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.13734}, 
}

@misc{clark2024reward,
      title={Directly Fine-Tuning Diffusion Models on Differentiable Rewards}, 
      author={Kevin Clark and Paul Vicol and Kevin Swersky and David J Fleet},
      year={2024},
      eprint={2309.17400},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.17400}, 
}

@misc{black2024reward,
      title={Training Diffusion Models with Reinforcement Learning}, 
      author={Kevin Black and Michael Janner and Yilun Du and Ilya Kostrikov and Sergey Levine},
      year={2024},
      eprint={2305.13301},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.13301}, 
}

@inproceedings{fan2023reward,
 author = {Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {79858--79885},
 publisher = {Curran Associates, Inc.},
 title = {DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fc65fab891d83433bd3c8d966edde311-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{uehara2024reward,
      title={Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control}, 
      author={Masatoshi Uehara and Yulai Zhao and Kevin Black and Ehsan Hajiramezanali and Gabriele Scalia and Nathaniel Lee Diamant and Alex M Tseng and Tommaso Biancalani and Sergey Levine},
      year={2024},
      eprint={2402.15194},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15194}, 
}

@misc{marion2024implicit,
      title={Implicit Diffusion: Efficient Optimization through Stochastic Sampling}, 
      author={Pierre Marion and Anna Korba and Peter Bartlett and Mathieu Blondel and Valentin De Bortoli and Arnaud Doucet and Felipe Llinares-López and Courtney Paquette and Quentin Berthet},
      year={2024},
      eprint={2402.05468},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05468}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%% DPO, KTO %%%%%%%%%%%%%%%%%%%%%%%%%%


@InProceedings{Wallace2024DiffusionDPO,
    author    = {Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil},
    title     = {Diffusion Model Alignment Using Direct Preference Optimization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {8228-8238}
}

@inproceedings{rafailov2023DPO,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {53728--53741},
 publisher = {Curran Associates, Inc.},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@misc{li2024diffusionKTO,
      title={Aligning Diffusion Models by Optimizing Human Utility}, 
      author={Shufan Li and Konstantinos Kallidromitis and Akash Gokul and Yusuke Kato and Kazuki Kozuka},
      year={2024},
      eprint={2404.04465},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.04465}, 
}

@misc{ethayarajh2024KTO,
      title={KTO: Model Alignment as Prospect Theoretic Optimization}, 
      author={Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela},
      year={2024},
      eprint={2402.01306},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01306}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%% mean field, optimization %%%%%%%%%%%%%%%%%%%%%%%%%%

@article{LIU2023nonconvexDA,
title = {Rate analysis of dual averaging for nonconvex distributed optimization},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {5209--5214},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.117},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323004639},
author = {Changxin Liu and Xuyang Wu and Xinlei Yi and Yang Shi and Karl H. Johansson}
}

@inproceedings{
oko2022particle,
title={Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization},
author={Kazusato Oko and Taiji Suzuki and Atsushi Nitanda and Denny Wu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=PQQp7AJwz3}
}

@misc{nitanda2017spgMF,
      title={Stochastic Particle Gradient Descent for Infinite Ensembles}, 
      author={Atsushi Nitanda and Taiji Suzuki},
      year={2017},
      eprint={1712.05438},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1712.05438}, 
}

@inproceedings{Chizat2018nips,
 author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{chizat2022meanfield,
title={Mean-Field Langevin Dynamics : Exponential Convergence and Annealing},
author={L{\'e}na{\"\i}c Chizat},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=BDqzLH1gEm},
note={}
}

@inproceedings{mei2018mf,
    author = {Mei, Song and Montanari, Andrea and Nguyen, Phan\-Minh},
    title = {A mean field view of the landscape of two-layer neural networks},
    booktitle = {Proceedings of the National Academy of Sciences of the United States of America},
    year = {2018},
    number = {115},
    volume = {33},
    pages = {E7665–E7671}
}


@InProceedings{nitanda22mfld,
  title = 	 { Convex Analysis of the Mean Field Langevin Dynamics },
  author =       {Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {9741--9757},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/nitanda22a/nitanda22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/nitanda22a.html}
}


@inproceedings{
nishikawa2022twolayer,
title={Two-layer neural network on infinite dimensional data:  global optimization guarantee in the mean-field regime},
author={Naoki Nishikawa and Taiji Suzuki and Atsushi Nitanda and Denny Wu},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=Hr8475tQGKE}
}

@inproceedings{NEURIPS2021_a34e1ddb,
 author = {Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {19608--19621},
 publisher = {Curran Associates, Inc.},
 title = {Particle Dual Averaging: Optimization of Mean Field Neural Network with Global Convergence Rate Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a34e1ddbb4d329167f50992ba59fe45a-Paper.pdf},
 volume = {34},
 year = {2021}
}

@Article{Nesterov2009,
author={Nesterov, Yurii},
title={Primal-dual subgradient methods for convex problems},
journal={Mathematical Programming},
year={2009},
month={Aug},
day={01},
volume={120},
number={1},
pages={221--259},
}

@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}

@inproceedings{hu2021mean,
  title={Mean-field Langevin dynamics and energy landscape of neural networks},
  author={Hu, Kaitong and Ren, Zhenjie and {\v{S}}i{\v{s}}ka, David and Szpruch, {\L}ukasz},
  booktitle={Annales de l'Institut Henri Poincare (B) Probabilites et statistiques},
  volume={57},
  number={4},
  pages={2043--2065},
  year={2021},
  organization={Institut Henri Poincar{\'e}}
}

@book{bakry2014analysis,
  title={Analysis and geometry of Markov diffusion operators},
  author={Bakry, Dominique and Gentil, Ivan and Ledoux, Michel and others},
  series={Grundlehren der mathematischen Wissenschaften},
  volume={348},
  year={2014},
  publisher={Springer}
}

@inproceedings{suzuki2023mfld,
 author = {Suzuki, Taiji and Wu, Denny and Nitanda, Atsushi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {15545--15577},
 publisher = {Curran Associates, Inc.},
 title = {Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/32133a6a24d6554263d3584e3ac10faa-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{nitanda2024improved,
      title={Improved particle approximation error for mean field neural networks}, 
      author={Atsushi Nitanda},
      year={2024},
      journal={arXiv preprint 2405.15767},
}

@article{liu2023polyak,
  title={Polyak--{\L}ojasiewicz inequality on the space of measures and convergence of mean-field birth-death processes},
  author={Liu, Linshan and Majka, Mateusz B and Szpruch, {\L}ukasz},
  journal={Applied Mathematics \& Optimization},
  volume={87},
  number={3},
  pages={48},
  year={2023},
  publisher={Springer}
}

@article{chen2023entropic,
  title={Entropic fictitious play for mean field optimization problem},
  author={Chen, Fan and Ren, Zhenjie and Wang, Songbo},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={211},
  pages={1--36},
  year={2023}
}


@InProceedings{pmlr-v202-nitanda23a,
  title = 	 {Primal and Dual Analysis of Entropic Fictitious Play for Finite-sum Problems},
  author =       {Nitanda, Atsushi and Oko, Kazusato and Wu, Denny and Takenouchi, Nobuhito and Suzuki, Taiji},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26266--26282},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/nitanda23a/nitanda23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/nitanda23a.html},
  abstract = 	 {The entropic fictitious play (EFP) is a recently proposed algorithm that minimizes the sum of a convex functional and entropy in the space of measures such an objective naturally arises in the optimization of a two-layer neural network in the mean-field regime. In this work, we provide a concise primal-dual analysis of EFP in the setting where the learning problem exhibits a finite-sum structure. We establish quantitative global convergence guarantees for both the continuous-time and discrete-time dynamics based on properties of a proximal Gibbs measure introduced in Nitanda et al. (2022). Furthermore, our primal-dual framework entails a memory-efficient particle-based implementation of the EFP update, and also suggests a connection to gradient boosting methods. We illustrate the efficiency of our novel implementation in experiments including neural network optimization and image synthesis.}
}

@InProceedings{pmlr-v238-yao24a,
  title = 	 {Minimizing Convex Functionals over Space of Probability Measures via {KL} Divergence Gradient Flow},
  author =       {Yao, Rentian and Huang, Linjun and Yang, Yun},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2530--2538},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/yao24a/yao24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/yao24a.html},
  abstract = 	 {Motivated by the computation of the non-parametric maximum likelihood estimator (NPMLE) and the Bayesian posterior in statistics, this paper explores the problem of convex optimization over the space of all probability distributions. We introduce an implicit scheme, called the implicit KL proximal descent (IKLPD) algorithm, for discretizing a continuous-time gradient flow relative to the Kullbackâ Leibler (KL) divergence for minimizing a convex target functional. We show that IKLPD converges to a global optimum at a polynomial rate from any initialization; moreover, if the objective functional is strongly convex relative to the KL divergence, for example, when the target functional itself is a KL divergence as in the context of Bayesian posterior computation, IKLPD exhibits globally exponential convergence. Computationally, we propose a numerical method based on normalizing flow to realize IKLPD. Conversely, our numerical method can also be viewed as a new approach that sequentially trains a normalizing flow for minimizing a convex functional with a strong theoretical guarantee.}
}

@article{holley1987logarithmic,
  title={Logarithmic Sobolev inequalities and stochastic Ising models},
  author={Holley, Richard and Stroock, Daniel},  
  journal={Journal of statistical physics},
  volume={46},
  number={5-6},
  pages={1159--1194},
  year={1987}
}

%%%%%%%%%%%%%%%%%%%%%%%%%% doob %%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Rogers2000Doob, place={Cambridge}, edition={2}, series={Cambridge Mathematical Library}, title={Diffusions, Markov Processes and Martingales}, publisher={Cambridge University Press}, author={Rogers, L. C. G. and Williams, David}, year={2000}, collection={Cambridge Mathematical Library}}


@InProceedings{chopin2023doob,
  title = 	 {Computational Doob h-transforms for Online Filtering of Discretely Observed Diffusions},
  author =       {Chopin, Nicolas and Fulop, Andras and Heng, Jeremy and Thiery, Alexandre H.},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {5904--5923},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chopin23a/chopin23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chopin23a.html},
  abstract = 	 {This paper is concerned with online filtering of discretely observed nonlinear diffusion processes. Our approach is based on the fully adapted auxiliary particle filter, which involves Doob’s $h$-transforms that are typically intractable. We propose a computational framework to approximate these $h$-transforms by solving the underlying backward Kolmogorov equations using nonlinear Feynman-Kac formulas and neural networks. The methodology allows one to train a locally optimal particle filter prior to the data-assimilation procedure. Numerical experiments illustrate that the proposed approach can be orders of magnitude more efficient than state-of-the-art particle filters in the regime of highly informative observations, when the observations are extreme under the model, and if the state dimension is large.}
}


@article{heng2024schrodingerbridge,
author = {Jeremy Heng and Valentin De Bortoli and Arnaud Doucet},
title = {{Diffusion Schrödinger Bridges for Bayesian Computation}},
volume = {39},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {90 -- 99},
keywords = {Optimal transport, Schrödinger bridge, score matching, Stochastic differential equation, Time reversal},
year = {2024},
doi = {10.1214/23-STS908},
URL = {https://doi.org/10.1214/23-STS908}
}

%%%%%%%%%%%%%%%%%%%%%%%%%% applications %%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{shen2024MRIdiffusion,
      title={Promptable Counterfactual Diffusion Model for Unified Brain Tumor Segmentation and Generation with MRIs}, 
      author={Yiqing Shen and Guannan He and Mathias Unberath},
      year={2024},
      eprint={2407.12678},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2407.12678}, 
}

@ARTICLE{fontanella2024BrainDiffusionGenerationAnomaly,
  author={Fontanella, Alessandro and Mair, Grant and Wardlaw, Joanna and Trucco, Emanuele and Storkey, Amos},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Pathology;Image segmentation;Lesions;Imaging;Diffusion models;Computed tomography;Training;Anomaly maps;Counterfactual examples;Diffusion models;Segmentation masks},
  doi={10.1109/TMI.2024.3460391}}


@InProceedings{krishnamoorthy23aDiffusionBlackBox,
  title = 	 {Diffusion Models for Black-Box Optimization},
  author =       {Krishnamoorthy, Siddarth and Mashkaria, Satvik Mehul and Grover, Aditya},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {17842--17857},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/krishnamoorthy23a/krishnamoorthy23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/krishnamoorthy23a.html}
}

@inproceedings{
chen2023sampling,
title={Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
author={Sitan Chen and Sinho Chewi and Jerry Li and Yuanzhi Li and Adil Salim and Anru Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zyLVMgsZ0U_}
}

@misc{Mischeler:Note:2019,
author = {St{\'e}phane Mischler},
title = {An introduction to evolution {PDEs}, {Chapter} 0: ON THE {Gronwall} LEMMA},
year = {2019},
url = {https://www.ceremade.dauphine.fr/~mischler/Enseignements/M2evol2018/chap0.pdf}
}

@book {MR755001,
    AUTHOR = {Bismut, Jean-Michel},
     TITLE = {Large deviations and the {M}alliavin calculus},
    SERIES = {Progress in Mathematics},
    VOLUME = {45},
 PUBLISHER = {Birkh\"auser Boston, Inc., Boston, MA},
      YEAR = {1984},
}

@article{ELWORTHY1994252,
title = {Formulae for the Derivatives of Heat Semigroups},
journal = {Journal of Functional Analysis},
volume = {125},
number = {1},
pages = {252--286},
year = {1994},
author = {K.D. Elworthy and X.M. Li},
}

@article{ling2024diffusion,
  title={Diffusion model-based probabilistic downscaling for 180-year East Asian climate reconstruction},
  author={Ling, Fenghua and Lu, Zeyu and Luo, Jing-Jia and Bai, Lei and Behera, Swadhin K and Jin, Dachao and Pan, Baoxiang and Jiang, Huidong and Yamagata, Toshio},
  journal={npj Climate and Atmospheric Science},
  volume={7},
  number={1},
  pages={131},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{tsuboi2009direct,
  title={Direct density ratio estimation for large-scale covariate shift adaptation},
  author={Tsuboi, Yuta and Kashima, Hisashi and Hido, Shohei and Bickel, Steffen and Sugiyama, Masashi},
  journal={Journal of Information Processing},
  volume={17},
  pages={138--155},
  year={2009},
  publisher={Information Processing Society of Japan}
}

@article{sugiyama2008direct,
  title={Direct importance estimation for covariate shift adaptation},
  author={Sugiyama, Masashi and Suzuki, Taiji and Nakajima, Shinichi and Kashima, Hisashi and Von B{\"u}nau, Paul and Kawanabe, Motoaki},
  journal={Annals of the Institute of Statistical Mathematics},
  volume={60},
  pages={699--746},
  year={2008},
  publisher={Springer}
}

@inproceedings{ouyang2022training,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

%%%% additional 

@misc{zhao2024scoresactionsframeworkfinetuning,
      title={Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning}, 
      author={Hanyang Zhao and Haoxian Chen and Ji Zhang and David D. Yao and Wenpin Tang},
      year={2024},
      eprint={2409.08400},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.08400}, 
}

@misc{tang2024finetuningdiffusionmodelsstochastic,
      title={Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond}, 
      author={Wenpin Tang},
      year={2024},
      eprint={2403.06279},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2403.06279}, 
}

@misc{li2024derivativefreeguidancecontinuousdiscrete,
      title={Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding}, 
      author={Xiner Li and Yulai Zhao and Chenyu Wang and Gabriele Scalia and Gokcen Eraslan and Surag Nair and Tommaso Biancalani and Shuiwang Ji and Aviv Regev and Sergey Levine and Masatoshi Uehara},
      year={2024},
      eprint={2408.08252},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.08252}, 
}