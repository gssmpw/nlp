
\subsection{Direct Preference Optimization}

We can apply the proposed algorithm to the (populational) objective of Direct Preference Optimization (DPO)~\citep{rafailov2023DPO}. 
DPO is an effective approach for learning from human preference for not only language models but also diffusion models.

\paragraph{Original DPO objective}
Let $x_w, x_l$ be ``winning" and ``losing" outputs independently sampled from the reference model $\pref$. The event $\lbrace x_w \succ x_l \rbrace$ is determined by the human preference. The original DPO objective is formulated as
\begin{equation}
    L_{\mathrm{DPO,original}}(q) = -\E_{x_w,x_l}
    \left[
        \log \sigma \left(\gamma \log \frac{q(x_w)}{\pref(x_w)}- \gamma \log \frac{q(x_l)}{\pref(x_l)}\right)
    \right],
\end{equation}
where $\gamma > 0$ is a hyperparameter.  The expectation is taken by $x_w, x_l$, that are ``winning" and ``losing" samples from $\pref$.
~\cite{Wallace2024DiffusionDPO} derived a new objective that is the upper bound of $ L_{\mathrm{DPO,original}}(q)$, but it is a specialized derivation of the optimization method for DPO.

\paragraph{Reformulating of the DPO Objective}
The goal is to directly minimize $L_{\mathrm{DPO,original}}(q)$, however, in  the above expression, we cannot apply DPO to diffusion models directly because the expectaion with tuple $(x_w,x_l)$ is not formulated well. We start with another expression of the objective of DPO:
\begin{equation}
    L_{\mathrm{DPO}}(q) \coloneq - \E_{x_w\sim p_{\mathrm{ref}}}\E_{x_l\sim p_{\mathrm{ref}}}
    \left[ 
         \log \sigma \left(\gamma \log \frac{q(x_w)}{\pref(x_w)}- \gamma \log \frac{q(x_l)}{\pref(x_l)}\right)\mathbbm{1}_{x_w \succ x_l}(x_w,x_l)
    \right],
\end{equation}
$\mathbbm{1}_{x \succ y}(x,y)$ is 1 if $x \succ y$, 0 otherwise.
This $L_\mathrm{DPO}$ is in the regime of our algorithm and the functional derivative can be derived:
\begin{prop}
    The functional derivative of $L_\mathrm{DPO}(q)$ is calculated as
    \begin{align}
    &\frac{\delta L_\mathrm{DPO}}{\delta q}(q,x) \\
    =&
    -\gamma\E_{x_l\sim p_{\mathrm{ref}}}
    \left[
        \left(
            1- \sigma
            \left(
               - \gamma f(x) + \gamma f(x_l)
            \right)
        \right)
        \frac{\int e^{-f} dp_\mathrm{ref}}{e^{-f(x)}}
        \mathbbm{1}_{x \succ x_l}(x,x_l)
    \right]\\
    &+     \gamma\E_{x_w\sim p_{\mathrm{ref}}}
    \left[
        \left(
            1- \sigma
            \left(
                - \gamma f(x_w)
                + \gamma f(x)
            \right)
        \right)
        \frac{\int e^{-f} dp_\mathrm{ref}}{e^{-f(x)}}
        \mathbbm{1}_{x_w \succ x}(x_w,x)
    \right],
    \end{align}
    where $q = e^{-f}\pref / \int e^{-f}\mathrm{d}\pref$. This functional derivative is tractable in our settings.
\end{prop}

\begin{proof}
In this proof, we use the following notations:
\begin{itemize}
    \item $p_\mathrm{ref}: $ the output distribution of a pre-trained model,
    \item $q \coloneq e^{-f} p_\mathrm{ref} / \int e^{-f} dp_\mathrm{ref}$: the output distribution of an aligned model,
    \item $\mathrm{LSL}(q_1, q_2) \coloneq \log \sigma (\gamma \log q_1/p_\mathrm{ref} - \gamma \log q_2/p_\mathrm{ref})$,
    \item $\partial_1 \mathrm{LSL}(q_1, q_2) = \gamma (1 - \sigma(\gamma \log q_1/p_\mathrm{ref} - \gamma \log q_2/p_\mathrm{ref})) \frac{1}{q_1} $,
    \item $\partial_2 \mathrm{LSL}(q_1, q_2) = -(1 - \sigma(\gamma \log q_1/p_\mathrm{ref} - \gamma \log q_2/p_\mathrm{ref})) \frac{1}{q_2} $,
    \item $\psi(r_1, r_2) \coloneq \gamma (\log r_1 - \log r_2)$,
    \item $\mathrm{Inv}(f,x) = \frac{\int e^{-f} dp_\mathrm{ref}}{e^{-f(x)}}$.
\end{itemize}
The objective of DPO is written as
\begin{equation}
    L_\mathrm{DPO}(q) \coloneq - \mathrm{E}_{x_w\sim p_{\mathrm{ref}}}\mathrm{E}_{x_l\sim p_{\mathrm{ref}}}
    \left[ 
        \mathrm{LSL}(q(x_w),qp(x_l)) \mathbbm{1}_{x_w \succ x_l}(x_w,x_l)
    \right].
\end{equation}
We obtain the first variation of the objective as follows:
\begin{align}
    &L_\mathrm{DPO}(q + \epsilon(\tilde{q}-q))\\
    =&
    L_\mathrm{DPO}(q) - \epsilon
    \mathrm{E}_{x_w\sim p_{\mathrm{ref}}}\mathrm{E}_{x_l\sim p_{\mathrm{ref}}}
    \left[ 
        \left(
            \partial_1 \mathrm{LSL}(q(x_w), q(x_l)) (\tilde{q}-q)(x_w)
            \right.\right.\\
            &\left.\left.\quad\quad\quad+\partial_2 \mathrm{LSL}(q(x_w), q(x_l)) (\tilde{q}-q)(x_l) 
        \right)
        \mathbbm{1}_{x_w \succ x_l}(x_w,x_l)
    \right]+\mathcal{O}(\epsilon^2)\\
    =&
    L_\mathrm{DPO}(p)\\
    &-
    \epsilon
    \left[
        \mathrm{E}_{x_l\sim p_{\mathrm{ref}}}
        \left[ 
            \int \gamma (1 - \sigma(\psi(\frac{q(x_w)}{p_\mathrm{ref}(x_w)}, \frac{q(x_l)}{p_\mathrm{ref}(x_l)}) ))(\tilde{q}-q)(x_w) \mathrm{Inv}(f,x_w)dx_w
            \mathbbm{1}_{x_w \succ x_l}
        \right]\right.\\
    &\left.+
    \mathrm{E}_{x_w\sim p_{\mathrm{ref}}}
        \left[ 
            \int \gamma (1 - \sigma(\psi(\frac{q(x_w)}{p_\mathrm{ref}(x_w)}, \frac{q(x_l)}{p_\mathrm{ref}(x_l)}) ))(\tilde{q}-q)(x_l) \mathrm{Inv}(f,x_l)dx_l
            \mathbbm{1}_{x_w \succ x_l}
        \right]
    \right]\\
    &+\mathcal{O}(\epsilon^2).
\end{align}
Then, the first derivative of $F$ is
\begin{align}
    \frac{\delta L_\mathrm{DPO}}{\delta p}(p,x) =&-
    \E_{x_l\sim p_{\mathrm{ref}}}
    \left[
        \gamma
        \left(
            1- \sigma
            \left(
                \psi(\frac{q(x)}{\pref(x)},\frac{q(x_l)}{\pref(x_l)})
            \right)
        \right)
        \mathbbm{1}_{x \succ x_l}(x,x_l)
    \right]\mathrm{Inv}(f,x)\\
    &+ \E_{x_w\sim p_{\mathrm{ref}}}
    \left[
        \gamma
        \left(
            1- \sigma
            \left(
                \psi(\frac{q(x_w)}{\pref(x_w)},\frac{q(x)}{\pref(x)})
            \right)
        \right)
        \mathbbm{1}_{x_w \succ x}(x_w,x)
    \right]\mathrm{Inv}(f,x).
\end{align}
\end{proof}

\subsection{Kahneman-Tversky optimization}

Assume that the whole data space $\mathbb{R}^d$ is split into a desirable domain $\mathcal{D}_\mathrm{D}$ and an undesirable domain $\mathcal{D}_\mathrm{U}$.
The objective of original KTO~\citep{ethayarajh2024KTO} is formulated as
\begin{align}
    L_\mathrm{KTO}(q)=
    &\E_{x\sim \pref}
    \left[ 
    \gamma_D \left(1 - \sigma \left(\kappa\log \frac{q}{\pref} - \KL(q\|\pref)\right)
    \right)\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{D}\rbrace}
    \right. \\
    &\quad \left.+
    \gamma_U \left(1 - \sigma \left(\KL(q\|\pref)-\kappa\log \frac{q}{\pref} \right)\right)\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{U}\rbrace}
    \right],
\end{align}
where $\gamma_D, \; \gamma_U, \; \kappa$ are hyper parameters, and $\sigma$ is a sigmoid function.

\begin{prop}
    The functional derivative of $L_\mathrm{KTO}$ is calculated as
    \begin{align}
        &\frac{\delta L_\mathrm{KTO}}{\delta q}(q,x)\\
        =& 
        - \kappa \gamma_D \sigma_\mathrm{deriv}(\phi(x))\frac{\int e^{-f}\mathrm{d}\pref}{e^{-f(x)}}\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{D}\rbrace}\\
        &+ (-f(x) - \log \int e^{-f(x)}d\pref)
        %+ \log \frac{q(x)}{\pref(x)}
        \E_{y\sim \pref}
        \left[ 
        \gamma_D \sigma_{\mathrm{deriv}}\left(\phi(y)\right)\mathbbm{1}_{\lbrace y \in \mathcal{D}_\mathrm{D}\rbrace}
        \right]\\
        &+ \kappa \gamma_U \sigma_\mathrm{deriv}(-\phi(x))\frac{\int e^{-f}\mathrm{d}\pref}{e^{-f(x)}}\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{U}\rbrace}\\
        &- (-f(x) - \log \int e^{-f(x)}d\pref)
        %\log \frac{q(x)}{\pref(x)}
        \E_{y\sim \pref}
        \left[ 
        \gamma_U \sigma_{\mathrm{deriv}}\left(-\phi(y)\right)\mathbbm{1}_{\lbrace y \in \mathcal{D}_\mathrm{U}\rbrace}
        \right]
    \end{align}
    where $\sigma_\mathrm{deriv}(\cdot) \coloneq \sigma(\cdot) (1- \sigma(\cdot))$, 
    $\phi(x) \coloneq \kappa\log \frac{q(x)}{\pref(x)} - \KL(q\|\pref)$, $q = \frac{e^{-f}\pref}{\int e^{-f}d\pref}$. 
\end{prop}
The functional derivative of KTO can be calculated if you have $f(x)$ and the samples from  $\pref$. Note that $\log q(x)/\pref(x) = -f(x) - \log \int e^{-f(x)}d\pref$.

\begin{proof}
    The first variation of $L_\mathrm{KTO}$ is
    \begin{align}
        &L_\mathrm{KTO}(q+\epsilon(\tilde{q}-q))-L_\mathrm{KTO}(q)\\
        \simeq&\epsilon
        \E_{x\sim \pref}
        \left[-
            \kappa \gamma_D \sigma_\mathrm{deriv}(\phi(x))
            \left(\frac{\tilde{q}(x)-q(x)}{q(x)}
        + \int \log \frac{q(y)}{\pref(y)} \mathrm{d}(\tilde{q}-q)(y)\right)
        \mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{D}\rbrace}
        \right]\\
    &+ \epsilon       
    \E_{x\sim \pref}
        \left[
            \kappa \gamma_U \sigma_\mathrm{deriv}(-\phi(x))
            \left(\frac{\tilde{q}(x)-q(x)}{q(x)}
        - \int \log \frac{q(y)}{\pref(y)} \mathrm{d}(\tilde{q}-q)(y)\right)
        \mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{U}\rbrace}
        \right]\\
    =&\epsilon
      \int
        \left\lbrace-
            \kappa \gamma_D \sigma_\mathrm{deriv}(\phi(x))
            \mathrm{Inv}(f,x)\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{D}\rbrace}\right.\\
        &\left.+ \log \frac{q(x)}{\pref(x)}
        \E\left[
        \kappa \gamma_D \sigma_\mathrm{deriv}(\phi(y))
        \mathbbm{1}_{\lbrace y \in \mathcal{D}_\mathrm{D}\rbrace}
        \right]
        \right\rbrace
        \mathrm{d}(\tilde{q}-q)(y)
        \\
    +&        
    \epsilon
      \int
        \left\lbrace
            \kappa \gamma_U \sigma_\mathrm{deriv}(-\phi(x))
            \mathrm{Inv}(f,x)\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{D}\rbrace}\right.\\
            &\left.- \log \frac{q(x)}{\pref(x)}
            \E\left[
            \kappa \gamma_D \sigma_\mathrm{deriv}(-\phi(y))
            \mathbbm{1}_{\lbrace y \in \mathcal{D}_\mathrm{U}\rbrace}
            \right]
        \right\rbrace
        \mathrm{d}(\tilde{q}-q)(y),
    \end{align}
    where $\mathrm{Inv}(f,x) = \frac{\int e^{-f} dp_\mathrm{ref}}{e^{-f(x)}}$.
    Now the desired result immediately follows.
\end{proof}
