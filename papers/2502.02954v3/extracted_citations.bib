@inproceedings{NEURIPS2021_a34e1ddb,
 author = {Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {19608--19621},
 publisher = {Curran Associates, Inc.},
 title = {Particle Dual Averaging: Optimization of Mean Field Neural Network with Global Convergence Rate Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a34e1ddbb4d329167f50992ba59fe45a-Paper.pdf},
 volume = {34},
 year = {2021}
}

@Article{Nesterov2009,
author={Nesterov, Yurii},
title={Primal-dual subgradient methods for convex problems},
journal={Mathematical Programming},
year={2009},
month={Aug},
day={01},
volume={120},
number={1},
pages={221--259},
}

@InProceedings{Wallace2024DiffusionDPO,
    author    = {Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil},
    title     = {Diffusion Model Alignment Using Direct Preference Optimization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {8228-8238}
}

@misc{black2024reward,
      title={Training Diffusion Models with Reinforcement Learning}, 
      author={Kevin Black and Michael Janner and Yilun Du and Ilya Kostrikov and Sergey Levine},
      year={2024},
      eprint={2305.13301},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.13301}, 
}

@article{chen2023entropic,
  title={Entropic fictitious play for mean field optimization problem},
  author={Chen, Fan and Ren, Zhenjie and Wang, Songbo},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={211},
  pages={1--36},
  year={2023}
}

@article{chizat2022meanfield,
title={Mean-Field Langevin Dynamics : Exponential Convergence and Annealing},
author={L{\'e}na{\"\i}c Chizat},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=BDqzLH1gEm},
note={}
}

@misc{clark2024reward,
      title={Directly Fine-Tuning Diffusion Models on Differentiable Rewards}, 
      author={Kevin Clark and Paul Vicol and Kevin Swersky and David J Fleet},
      year={2024},
      eprint={2309.17400},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2309.17400}, 
}

@misc{ethayarajh2024KTO,
      title={KTO: Model Alignment as Prospect Theoretic Optimization}, 
      author={Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela},
      year={2024},
      eprint={2402.01306},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01306}, 
}

@inproceedings{fan2023reward,
 author = {Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {79858--79885},
 publisher = {Curran Associates, Inc.},
 title = {DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fc65fab891d83433bd3c8d966edde311-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{hu2021mean,
  title={Mean-field Langevin dynamics and energy landscape of neural networks},
  author={Hu, Kaitong and Ren, Zhenjie and {\v{S}}i{\v{s}}ka, David and Szpruch, {\L}ukasz},
  booktitle={Annales de l'Institut Henri Poincare (B) Probabilites et statistiques},
  volume={57},
  number={4},
  pages={2043--2065},
  year={2021},
  organization={Institut Henri Poincar{\'e}}
}

@misc{li2024diffusionKTO,
      title={Aligning Diffusion Models by Optimizing Human Utility}, 
      author={Shufan Li and Konstantinos Kallidromitis and Akash Gokul and Yusuke Kato and Kazuki Kozuka},
      year={2024},
      eprint={2404.04465},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.04465}, 
}

@article{liu2023polyak,
  title={Polyak--{\L}ojasiewicz inequality on the space of measures and convergence of mean-field birth-death processes},
  author={Liu, Linshan and Majka, Mateusz B and Szpruch, {\L}ukasz},
  journal={Applied Mathematics \& Optimization},
  volume={87},
  number={3},
  pages={48},
  year={2023},
  publisher={Springer}
}

@misc{marion2024implicit,
      title={Implicit Diffusion: Efficient Optimization through Stochastic Sampling}, 
      author={Pierre Marion and Anna Korba and Peter Bartlett and Mathieu Blondel and Valentin De Bortoli and Arnaud Doucet and Felipe Llinares-López and Courtney Paquette and Quentin Berthet},
      year={2024},
      eprint={2402.05468},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05468}, 
}

@inproceedings{mei2018mf,
    author = {Mei, Song and Montanari, Andrea and Nguyen, Phan\-Minh},
    title = {A mean field view of the landscape of two-layer neural networks},
    booktitle = {Proceedings of the National Academy of Sciences of the United States of America},
    year = {2018},
    number = {115},
    volume = {33},
    pages = {E7665–E7671}
}

@article{nitanda2024improved,
      title={Improved particle approximation error for mean field neural networks}, 
      author={Atsushi Nitanda},
      year={2024},
      journal={arXiv preprint 2405.15767},
}

@InProceedings{nitanda22mfld,
  title = 	 { Convex Analysis of the Mean Field Langevin Dynamics },
  author =       {Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {9741--9757},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/nitanda22a/nitanda22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/nitanda22a.html}
}

@InProceedings{pmlr-v202-nitanda23a,
  title = 	 {Primal and Dual Analysis of Entropic Fictitious Play for Finite-sum Problems},
  author =       {Nitanda, Atsushi and Oko, Kazusato and Wu, Denny and Takenouchi, Nobuhito and Suzuki, Taiji},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26266--26282},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/nitanda23a/nitanda23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/nitanda23a.html},
  abstract = 	 {The entropic fictitious play (EFP) is a recently proposed algorithm that minimizes the sum of a convex functional and entropy in the space of measures such an objective naturally arises in the optimization of a two-layer neural network in the mean-field regime. In this work, we provide a concise primal-dual analysis of EFP in the setting where the learning problem exhibits a finite-sum structure. We establish quantitative global convergence guarantees for both the continuous-time and discrete-time dynamics based on properties of a proximal Gibbs measure introduced in Nitanda et al. (2022). Furthermore, our primal-dual framework entails a memory-efficient particle-based implementation of the EFP update, and also suggests a connection to gradient boosting methods. We illustrate the efficiency of our novel implementation in experiments including neural network optimization and image synthesis.}
}

@InProceedings{pmlr-v238-yao24a,
  title = 	 {Minimizing Convex Functionals over Space of Probability Measures via {KL} Divergence Gradient Flow},
  author =       {Yao, Rentian and Huang, Linjun and Yang, Yun},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2530--2538},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/yao24a/yao24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/yao24a.html},
  abstract = 	 {Motivated by the computation of the non-parametric maximum likelihood estimator (NPMLE) and the Bayesian posterior in statistics, this paper explores the problem of convex optimization over the space of all probability distributions. We introduce an implicit scheme, called the implicit KL proximal descent (IKLPD) algorithm, for discretizing a continuous-time gradient flow relative to the Kullbackâ Leibler (KL) divergence for minimizing a convex target functional. We show that IKLPD converges to a global optimum at a polynomial rate from any initialization; moreover, if the objective functional is strongly convex relative to the KL divergence, for example, when the target functional itself is a KL divergence as in the context of Bayesian posterior computation, IKLPD exhibits globally exponential convergence. Computationally, we propose a numerical method based on normalizing flow to realize IKLPD. Conversely, our numerical method can also be viewed as a new approach that sequentially trains a normalizing flow for minimizing a convex functional with a strong theoretical guarantee.}
}

@inproceedings{rafailov2023DPO,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {53728--53741},
 publisher = {Curran Associates, Inc.},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}

@inproceedings{suzuki2023mfld,
 author = {Suzuki, Taiji and Wu, Denny and Nitanda, Atsushi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {15545--15577},
 publisher = {Curran Associates, Inc.},
 title = {Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/32133a6a24d6554263d3584e3ac10faa-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{uehara2024reward,
      title={Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control}, 
      author={Masatoshi Uehara and Yulai Zhao and Kevin Black and Ehsan Hajiramezanali and Gabriele Scalia and Nathaniel Lee Diamant and Alex M Tseng and Tommaso Biancalani and Sergey Levine},
      year={2024},
      eprint={2402.15194},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15194}, 
}

@misc{ziegler2020LLMft,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

