% !TEX program = pdflatex
\pdfoutput=1
\documentclass{article} % For LaTeX2e
\usepackage{arxiv,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%%%
\usepackage{wrapfig}
\usepackage{array}
\usepackage{longtable}
\usepackage[section]{algorithm}
\usepackage{algorithmic}

% RequireとEnsureをInputとOutputにする
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%%%

%% reviesed
\newcommand{\revisedStart}{\color{black}}
\newcommand{\revisedEnd}{\color{black}}

\usepackage{url}
\usepackage{amsmath, amssymb, mathtools, amsthm, mathcomp}


\usepackage{amsfonts,amscd,amssymb,bm,bbm,mathrsfs}

\usepackage{hyperref}

\usepackage{cleveref}
\usepackage{autonum}

\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\theoremstyle{plain}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\newtheorem*{rem*}{Remark}

\usepackage{comment}

\title{Direct Distributional Optimization for Provable Alignment of Diffusion Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Ryotaro Kawata$^{1,2,*}$, Kazusato Oko$^{2,3,\dagger}$, Atsushi Nitanda$^{4,5,\ddagger}$, Taiji Suzuki$^{1,2,\S}$}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\MOD}{\textcolor{red}{(MOD)}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begingroup  % 所属情報を大きめに表示
%\centering
%\large
\vspace{-2em} % 調整
$^1$Department of Mathematical Informatics, University of Tokyo, Japan \\
$^2$Center for Advanced Intelligence Project, RIKEN, Japan \\
$^3$Department of ECCS, UC Berkeley \\
$^4$CFAR and IHPC, Agency for Science, Technology and Research (A$\star$STAR), Singapore \\
$^5$College of Computing and Data Science, Nanyang Technological University, Singapore \\
%\vspace{1em}\\ % 余白を調整
$^*$\texttt{\href{mailto:kawata-ryotaro725@g.ecc.u-tokyo.ac.jp}{kawata-ryotaro725@g.ecc.u-tokyo.ac.jp}} \\
$^\dagger$\texttt{\href{mailto:oko@berkeley.edu}{oko@berkeley.edu}}\\
$^\ddagger$\texttt{\href{mailto:atsushi_nitanda@cfar.a-star.edu.sg}{atsushi\_nitanda@cfar.a-star.edu.sg}} \\
$^\S$\texttt{\href{mailto:taiji@mist.u-tokyo.ac.jp}{taiji@mist.u-tokyo.ac.jp}}\\
\vspace{1em}
\par
\endgroup


\begin{abstract}
We introduce a novel alignment method for diffusion models from distribution optimization perspectives while providing rigorous convergence guarantees.
We first formulate the problem as a generic regularized loss minimization over probability distributions and directly optimize the distribution using the Dual Averaging method.
Next, we enable sampling from the learned distribution by approximating its score function via Doob's $h$-transform technique.
The proposed framework is supported by rigorous convergence guarantees and an end-to-end bound on the sampling error, which imply that when the original distribution's score is known accurately, the complexity of sampling from shifted distributions is independent of isoperimetric conditions.
This framework is broadly applicable to general distribution optimization problems, including alignment tasks in Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO). We empirically validate its performance on synthetic and image datasets using the DPO objective.
% This method effectively aligns distributions, bypassing the need for explicit density calculations, and is applicable to a wide range of optimization tasks, including Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO). Empirical validation on synthetic and image datasets demonstrates the method’s ability to handle complex multimodal distributions, showcasing its utility in model alignment tasks.

% We propose a novel model refinement method for aligning diffusion models, offering rigorous convergence guarantees without requiring isoperimetric conditions. Our approach is based on the dual averaging scheme for distributional optimization, which guides an aligned diffusion process in combination with the density ratio estimation using neural networks with Doob's $h$-transform. This framework is broadly applicable to general distribution optimization problems including alignment tasks in reinforcement learning with human feedback (RLHF), direct preference optimization (DPO), and Kahneman-Tversky optimization (KTO) settings, and we empirically validate its perfomance on synthetic and image datasets with the DPO objective.
\end{abstract}

\section{Introduction}
Diffusion models~\citep{Sohl-Dickstein2015thermodynamics,ho2020DDPM,song2021scorebased} have recently emerged as powerful tools for learning complex distributions and performing efficient sampling. Within the framework of foundation models, a common approach involves pre-training on large-scale datasets, followed by adapting the model to downstream tasks or aligning it with human preferences~\citep{ouyang2022training}. This alignment is typically formalized as nonlinear distribution optimization, with a regularization term that encourages proximity to the pre-trained distribution. Examples of such alignment methods include Reinforcement Learning with Human Feedback (RLHF)~\citep{ziegler2020LLMft}, Direct Preference Optimization (DPO)~\citep{rafailov2023DPO,Wallace2024DiffusionDPO}, and Kahneman-Tversky Optimization (KTO)~\citep{ethayarajh2024KTO,li2024diffusionKTO}.

\revisedStart
Specifically, there methods solve the minimization problems of a regularized functional $F(q)+\beta\KL(q\|\pref)$ over $q$ in the probability space $\mathcal{P}$, where  $\pref$ is the probability density corresponding to the pretrained diffusion model. 
%^of the output samples at the final denoising step, denoted by $X^\leftarrow_T$.
% The alignment objective is the functional $F(q)+\beta\KL(q\|\pref)$ whose input is the density $q$. 
However, this type of distributional optimization problem over the density $q$ is challenging because the output density governed by the reference model cannot be evaluated and neither is the aligned model $q$. 
They are accessible only through samples generated from their corresponding generative models.  
%using the score networks. 
Existing distributional optimization methods such as {\it mean-field Langevin dynamics}~\citep{mei2018mf} and {\it particle dual averaging}~\citep{NEURIPS2021_a34e1ddb} resolved this problem by adapting a Langevin type sampling procedure to calculate a functional derivative of the objective without explicitly evaluating the densities. 
However, the distributions $q$ and $\pref$ are highly complex and multimodal from which it is extremely hard to generate data by a standard MCMC type methods including the Langevin dynamics. 
This difficulty can be mathematically characterized by isoperimetric conditions, such as logarithmic Sobolev inequality (LSI) \citep{bakry2014analysis}, that has usually exponential dependency on the data dimension $d$ for multimodal data yielding the curse of dimensionality. 
Unfortunately, the existing distribution optimization methods mentioned above are sensitive to the LSI constant, so they suffer from severely slow convergence, failing to align diffusion models. 

% On the other hand, the densities $q$ and $\pref$ are highly complex and multimodal from which it is extremely hard to generate data by a standard MCMC type methods such as a Langevin dynamics. 
% This difficulty can be mathematically characterized by isoperimetric conditions, such as logarithmic Sobolev inequality (LSI), that has usually exponential dependency on the data dimension $d$ for multimodal data yielding the curse of dimensionality. 
% Unfortunately, existing standard distribution optimization methods including {\it mean-field Langevin dynamics}~\citep{mei2018mf} are sensitive to LSI, so they suffer from severely slow convergence, failing to align diffusion models. 

% They are formulated as minimization problems of a regularized functional $F(q)+\beta\KL(q\|\pref)$ over $q$ in the probability space $\mathcal{P}$, where $q$ and $\pref$ are the aligned and pretrained densities of the output samples at the final denoising step, denoted by $X^\leftarrow_T$.
% % The alignment objective is the functional $F(q)+\beta\KL(q\|\pref)$ whose input is the density $q$. 

% However, this type of distributional optimization problem over the density $q(X^\leftarrow_T)$ is challenging because the output densities governed by the reference and the aligned models cannot be evaluated. They are accessible only through samples using the score networks.
% In addition, the densities $q$ and $\pref$ are highly complex and multimodal, which implies that isoperimetric conditions, such as logarithmic Sobolev inequality (LSI), become significantly weaker. Specifically, the LSI constant, which determines the strength of LSI, becomes exponentially small as the data dimension $d$ increases.
% Standard distribution optimization methods including mean-field Langevin dynamics~\citep{mei2018mf} are sensitive to LSI, so they suffer from severely slow convergence, failing to align diffusion models. 


That is to say, alignment of diffusion models has two challenges: (i) inaccessibility of the output densities and (ii) muitimodality of the densities. This naturally leads to a fundamental question:
\revisedEnd
\begin{comment}
    In the context of distribution optimization, mean-field particle methods such as mean-field Langevin dynamics~\citep{mei2018mf}, particle dual averaging~\citep{NEURIPS2021_a34e1ddb}, and particle stochastic dual coordinate ascent~\citep{oko2022particle} have been developed to optimize convex functionals defined over distributions with a KL-divergence regularizer from a simple distribution, such as a Gaussian. The convergence rate analyses~\citep{mei2018mf,hu2021mean,nitanda22mfld,chizat2022meanfield,NEURIPS2021_a34e1ddb} for these methods are also well established under some isoperimetric conditions, such as log-Sobolev inequality, which ensures sufficient concentration of probability mass~\citep{bakry2014analysis}, facilitating efficient exploration and faster convergence. These methods can naturally handle distribution optimization, but, their application to align diffusion models remains largely unexplored. This is primarily due to two challenges: (i) particle-based optimization methods are not designed for resampling from the distribution where obtained particles follow, and (ii) pre-trained distribution is typically highly complex multimodal distributions, failing to satisfy isoperimetric conditions. This naturally leads to a fundamental question:
\end{comment}

\begin{center}
% {\it Can we develop a model refinement procedure that produces a diffusion-based model with improved alignment from a pre-trained reference model, while ensuring rigorous convergence guarantees without requiring isoperimetric conditions?}
{\it Can we develop an alignment algorithm for diffusion models from distribution optimization perspectives, while ensuring rigorous convergence guarantees without isoperimetric conditions?}
\end{center}

We address this question by  
developing a diffusion-model based distribution optimization method and providing rigorous convergence and sampling error guarantees, and demonstrate its applicability to several tasks involved with diffusion model alignment.
\revisedStart
Our method represents the aligned model by a diffusion model that can be described by merely adding a correction term to the score function of the original reference model. During optimizing the model, we don't rely on any MCMC sampler but only use samples generated by the original reference model (and the aligned diffusion model). This characteristics is helpful to resolve the issue of isoperimetric condition.  
\revisedEnd

\begin{figure}[htbp]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.6\linewidth]{figures/overview.png}
    \vspace{-3mm}
    \caption{Overview of the proposed method integrating Dual Averaging and Doob’s h-transform.}
    \label{fig:proposed_method}
    \vspace{-2mm}
\end{figure}


    \vspace{-2mm}
\subsection{Our Contributions}
    \vspace{-2mm}
To tackle the two challenges mentioned above: absence of (i) sampling guarantee and (ii) isoperimetry, we propose a general framework that integrates dual averaging (DA) method~\citep{Nesterov2009} and diffusion model~\mbox{\citep{Sohl-Dickstein2015thermodynamics,ho2020DDPM,song2021scorebased}}. 
The DA method is an iterative algorithm that constructs the Gibbs distribution converging to the optimal distribution (i.e., alignment). 
% The DA method operates in a double loop scheme: in the outer loop, Gibbs distributions converging to the optimal distribution (i.e., alignment) are successively constructed, while in the inner loop, these intermediate distributions are approximately obtained using sampling methods such as Langevin Monte-Carlo. 
A key advantage of this DA scheme is its ability to bypass isoperimetric conditions with the help of high sampling efficiency of the reference diffusion model, enabling isoperimetry-free sampling from an aligned distribution. %, as long as we get an accurate sampling from the distribution with diffusion model-based methods. 
% by replacing inner-loop sampling methods with diffusion model-based methods. 
Specifically, an aligned diffusion process that approximately generates the Gibbs distribution obtained by DA method can be constructed through the Doob's $h$-transform~\citep{Rogers2000Doob} and density ratio estimation with respect to a reference distribution using neural networks (see Figure \ref{fig:proposed_method} for illustration).

We summarize our contribution below.
\begin{itemize}[topsep=0mm,leftmargin = 8mm] %itemsep=-1mm,
    \item We establish a model alignment method to align diffusion models, with convergence guarantees for both convex and nonconvex objectives, based on distribution optimization theory and Doob's $h$-transform. 
    Notable distinctions from other mean-field optimization methods are that our method works without isoperimetry conditions such as LSI
    and allows for sampling from multimodal distributions. 
    \item We also analyze the sampling error due to the approximation of the drift estimators with neural networks and the discretization of the process, and evaluate how these errors affect the final sampling accuracy. 
    \item Our general framework encompasses several major alignment problems such as RLHF, DPO, and KTO, establishing a provable alignment method for these scenarios. We demonstrate the applicability of our framework to these settings and empirically validate its performance on both synthetic and image datasets, aiming at data augmentation for a specific mode of distribution, using DPO objective.
\end{itemize}
We also emphasize that our method has the potential to be applied to general distribution optimization problems beyond alignment tasks such as density ratio estimation under the covariate shift setting~\citep{sugiyama2008direct,tsuboi2009direct} and climate change tracking~\citep{ling2024diffusion}.

\subsection{Related work.}

\vspace{-1mm}
\textbf{Mean-field optimization.}\quad
PDA method \citep{NEURIPS2021_a34e1ddb,nishikawa2022twolayer}, an extension of DA method~\citep{Nesterov2009} to the distribution optimization setting, was the first method that proves the quantitative convergence for minimizing entropy regularized convex functional. Subsequently, P-SDCA method~\citep{oko2022particle}, inspired by the SDCA method~\citep{shalev2013stochastic}, achieved the linear convergence rate. Mean-field Langevin dynamics~\citep{mei2018mf} is the most standard particle-based distribution optimization method, derived as the mean-field limit of the noisy gradient descent, and its convergence rate has been well studied by \cite{mei2018mf,hu2021mean,nitanda22mfld,chizat2022meanfield,suzuki2023mfld,nitanda2024improved}. Additionally, several mean-field optimization methods such mean-field Fisher-Rao gradient flow~\citep{liu2023polyak} and entropic fictitious play~\citep{chen2023entropic,pmlr-v202-nitanda23a} have been proposed with provable convergence guarantees. We note that the convergence rates of these methods were established under isoperimetric conditions such as log-Sobolev and Poincar\'e inequalities, which ensure concentration of the probability mass.
IKLPD method \citep{pmlr-v238-yao24a} shares similarities with our method, as it employs the normalizing flow to solve intermediate subproblems in the distributional optimization procedure, and its convergence does not depend on isoperimetric conditions. However, the applicability of IKLPD to alignment tasks remains uncertain since handling the proximity to the reference distribution is non-trivial.

%However, the algorithms shown above are limited to the Log-Sobolev inequality, which implies the unimodality of the densities.
%Our distributional optimization algorithm via diffusion models is free from such isoperimetric inequalities, which leads to more realistic assumptions of the data distribution.

\vspace{-1mm}
\textbf{Fine-tuning of diffusion models.}\quad
Recently, alignment of diffusion models have been investigated, inspired by LLM fine-tune methods, such as RLHF~\citep{ziegler2020LLMft}, DPO~\citep{rafailov2023DPO}, and KTO~\citep{ethayarajh2024KTO}. Applying them to diffusion models entails additional difficulty since the output density $\pref$ of the diffusion model is not available, and hence several techniques have been developed to circumvent the explicit calculation of $\pref$. 
For instance, \cite{fan2023reward,black2024reward,clark2024reward} invented the maximization algorithm of the reward in each diffusion time step. \cite{uehara2024reward} used Doob's $h$-transform to compute the correction term that can be automatically derived from the density ratio between the generated and the reference distributions. Instead of optimizing original DPO and KTO objectives, \cite{Wallace2024DiffusionDPO} considered the evidence lower bound (ELBO) and \cite{li2024diffusionKTO} defined a new objective function to replicate KTO.
\revisedStart
%\cite{marion2024implicit} also studied fine-tuning of diffusion models as distributional optimization. However, their convergence analysis for diffusion models was limited to RLHF approach and the single Gaussian assumption.\revisedEnd
\cite{marion2024implicit} also studied fine-tuning of diffusion models as distributional optimization within the RLHF framework and conducted convergence analysis for the one-dimensional Gaussian distribution.\revisedEnd
%*********
%They unified Langevin dynamics and the fine-tuning of diffusion models as an optimization problem over the probability space. However, their theoretical analysis was constrained by the logarithmic Sobolev inequality in the context of Langevin dynamics, as well as by the reinforcement learning approach and the single Gaussian assumption in the context of diffusion models.
%*********

%Nevertheless, the difficulty to directly optimize the nonlinear objectives of the output distribution, such as DPO and KTO, via diffusion models have not been solved yet.
% We tackled this problem from the mean-field perspective and Doob's h-transform technique.

%And LoRA?

% unified, theoretical way of direct density optimization
% 拡散モデル: pathの密度しか密度が扱えない

% mean-field: sampling NG 多峰
% proximal gibbs
% mean-field langevin も遅い 画像でうまく行かない


%\paragraph{Application DPO}
%marginal distribution w.r.t. $x_0$
%generalization of DP


%\begin{itemize}
%    \item Introduction: 1.5 pages
%    \item Preliminaries: 1 page
%    \item Core theorems 1 page
%    \item Applications 1 page
%    \item Diffusion Model: 1.5 pages
%    \item Experiments: 2.5 pages
%    \item Conclusion: 0.5 pages
%\end{itemize}

%\color{red}
%KEYWORDS:
%\begin{itemize}
%    \item diffusion (as a schodinge bridge?)
%    \item Doob
%    \item RLHF, PPO (uehara etc.)
%    \item sampling
%    \item mean field (?)
%    \item diffusion-DPO
%    \item diffusion-KTO
%\end{itemize}
%\color{black}

\vspace{-1mm}
\section{Problem Setting}
\vspace{-2mm}

\textbf{Distributional Optimization.}\quad
Let $\gP$ be the space of probability density functions with respect to the Lebesgue measure on $(\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$.  
%such that \textcolor{red}{some assumptions}. 
Let $F:\mathcal{P} \to \mathbb{R}$ be a functional and $\pref \in \gP$ be the reference density. 
In this work, we consider the regularized loss minimization problem over $\mathcal{P}$:
\begin{equation}
    \underset{q \in \mathcal{P}}{\min}\left\lbrace L(q) \coloneq F(q) + \beta \KL(q \| \pref) \right\rbrace,\label{eq:setup-minimization}
\end{equation}
where $\KL(q \|\pref) \coloneq \E_q [\log \frac{q}{\pref}]$ is the Kullback-Leibler divergence, and $\beta > 0$ is a regularization coefficient.
We assume that $F$ is differentiable. That is, the functional $F$ has \textit{first order variation} $\dFdq:\mathcal{P}\times \mathbb{R}^d\ni (q,x)\to \dFdq(q,x)\in\mathbb{R}$ such that for all $q,q' \in \mathcal{P}$,
\begin{equation}
    \left.\frac{\mathrm{d}F(q+\epsilon(q'-q))}{\mathrm{d}\epsilon}\right|_{\epsilon=0} = \int \dFdq(q,x)(q'-q)(x)\mathrm{d}x.
\end{equation}
In the following we assume that there exists a unique minimizer $\qstar := \argmin_{q \in \gP} L(q)$. 

\textbf{Diffusion Models.}\quad
$\pref$ is the output density of a pre-trained diffusion model~\citep{Sohl-Dickstein2015thermodynamics,song2019generatiive,ho2020DDPM,song2021scorebased,vahdat2021latent}, while
$p_* \in \mathcal{P}$ is the target distribution of pre-training.
A ``noising'' process $\lbrace \bar{X}_t \rbrace_{t\geq 0}$ denotes the Ornstein-Uhlenbeck (OU) process from $p_*(x)$. 
% The law of $\bar{X}_t$ given $\bar{X}_0 \sim p_*$ is $\mathcal{N}(m_t \bar{X}_0,\sigma_t), \; m_t = e^{-t}, \sigma_t^2 =1 - e^{-2t}$. 
The law of $\bar{X}_t$ can be written as $p_t(x) = \int \mathcal{N}(m_t \bar{X}_0,\sigma_t) \mathrm{d}p_*(\bar{X}_0)$ with $m_t = e^{-t}, \sigma_t^2 =1 - e^{-2t}$.
Then, the reverse process $\{\bar{X}_t^\leftarrow\}_{0\leq t \leq T}$ ($T\geq 0$) can be defined as
\begin{align}
    \bar{X}_t^\leftarrow \sim p_t,\ \mathrm{d}\bar{X}_t^\leftarrow = \{\bar{X}_t^\leftarrow + 2 \nabla \log p_{T-t}(\bar{X}_t^\leftarrow)\}\mathrm{d}t + \sqrt{2} \mathrm{d}B_t.
\end{align}
Then it holds that $\mathrm{Law}(\bar{X}_t^\leftarrow)=\mathrm{Law}(\bar{X}_{T-t})$, which enables us to sample from $p_*$.
In practice, we approximate the score $\nabla \log p_{T-t}(\bar{X}_t^\leftarrow)$ function by a score network 
$s:\mathbb{R}^{d+1}\to \mathbb{R}^d:(x,t) \mapsto s(x,t)$.
In addition, we initialize $\Xref_0 \sim \mathcal{N}(0,I_d) \simeq p_T$ and the process is time-discretized.
The random variable generated by the following dynamics with step size $h$ is denoted by $\{\Xref_t\}_{0\leq t \leq T}$.
\begin{equation}
\Xref_0 \sim \mathcal{N}(0,I_d),\ 
        \mathrm{d}\Xref_t = \lbrace  \Xref_{\tprev}+ 2s(\Xref_{\tprev},\tprev) \rbrace \mathrm{d}t
        + \sqrt{2}\mathrm{d}B_t, \ t \in [\tprev, \tnext], \; l=1,...,L=T/\stepsize.
\end{equation}
In the same way, we define $q_t$ as the density of the diffusion process corresponding to $q_*$, and $\{{\bar{X}}_t\}_{t\geq 0}$ and $\{{\bar{X}^\leftarrow}_t\}_{0\leq t \leq T}$ as the corresponding noising and the backward process.


% The SDE is formulated as
% \begin{equation}
%         \mathrm{d}\Xref_t = \lbrace  \Xref_{\tprev}+ 2s(\Xref_{\tprev},\tprev) \rbrace \mathrm{d}t
%         + \sqrt{2}\mathrm{d}B_t, \quad t \in [\tprev, \tnext], \; l=1,...,L=T/\stepsize,
% \end{equation}
% where $\delta$ is the step size and $L=T/\delta$ is the number of steps, assumed to be integer for simplicity. 
% We define $\pref \simeq p_*$ as the law of the output $\Xref_T$, and $\pmREF$ as the path measure of $\lbrace\Xref_t\rbrace_t$.

Now we have two challenges in this problem (\ref{eq:setup-minimization}):
\begin{itemize}[topsep=0mm,itemsep=-1mm,leftmargin = 12mm] 
%[topsep=0mm,itemsep=-1mm,leftmargin = 5.5mm,labelsep=0.5mm]
\item[(A).]\textbf{Multimodality of $\pref$ and $\qstar$.} %\quad
Our goal is to obtain samples from $\qstar$.
Now we tackle with the case that $\pref$ and $\qstar$ have high \textit{multimodality}: $\pref$ and $\qstar$ have multiple modes or maxima, in other words, the potentials $-\log\pref$ and $-\log\qstar$ are extremely far from concave. Probability distributions of real-world data like images often have such complex structures, which implies that LSI is significantly weak.
\item[(B).]\textbf{Inaccesibility to the density $\pref$.} %\quad
We cannot directly calculate the density $\pref$ because diffusion models only have the score network. We only have information of $\pref$ as samples from $\pref$.
\end{itemize}

\textbf{Inapplicability of Mean-Field Langevin Dynamics.}\quad
Mean-field Langevin dynamics (MFLD)~\citep{mei2018mf} is the most standard particle-based optimization method tailored to solve the special case of the problem~\eqref{eq:setup-minimization} where $F$ is convex functional and $\pref$ is a strongly log-concave distribution such as Gaussian distribution. The convergence rate of MFLD~\citep{nitanda22mfld,chizat2022meanfield} has been established under the condition where the proximal distribution: $p_q \propto \pref \exp{\left(-\beta^{-1}\dFdq(q,\cdot)\right)}$ associated with MFLD iteration $q$ satisfies logarithmic Sobolev inequality (LSI), which says sufficient concentration of $p_q$. Typically, LSI for $p_q$ should rely on the isoperimetry of the reference distribution $\pref$ with Holley-Strook argument~\citep{holley1987logarithmic} since $\dFdq(q,\cdot)$ is not expected to encourage it in general. Therefore, the replacement of $\pref$ to a pre-trained distribution, which is highly complex and has multi-modality, leads to failure or weak satisfaction of LSI. As a result, the convergence of MFLD is significantly slowed down.
Additionally, MFLD is practically implemented so that finite particles approximately follow the ideal dynamics of MFLD, and it is not intended for resampling from the final distribution represented by these particles. In short, the lack of (i) a sampling guarantee and (ii) a isoperimetric condition limits the applicability of MFLD in our problem setting. 

\vspace{-2mm}
To address these challenges, we employ the following strategy, which will be detailed in Section~\ref{section:algorithm-introduction}.
\begin{itemize}[topsep=0mm,itemsep=-1mm,leftmargin = 12mm] 
%[topsep=0mm,itemsep=-1mm,leftmargin = 5.5mm,labelsep=0.5mm]
%[topsep=0mm,itemsep=-1mm,leftmargin = 12mm]
\item[(A).]\textbf{Modifications of diffusion models for sampling from a multimodal distribution.}\quad
First, we obtain samples from $\pref$ using a diffusion model which works for a broad range of probability distributions that have complex structures such as multimodality. 
\revisedStart
Then, we reconstruct the diffusion model to sample from 
$\qstar$ by simply adding a correction term.
\revisedEnd

\item[(B).]\textbf{A distributional optimization algorithm that does not require the density $\pref$.}\quad
Second, following the dual averaging (DA) method in the distribution optimization setting, we constract a sequence of distributions $\qstark$ converging to the optimum, which guides an aligned diffusion process in combination with the density ratio estimation using neural networks with Doob's $h$-transform. 
We only have to calculate the density ratio between $\qstark$ and $\pref$ and the samples from $\pref$ to run our algorithm.
\end{itemize}

\begin{comment}
    \textbf{Denoising Diffusion Probabilistic Model.}\quad
    Let $p_0 \in \mathcal{P}$ be a target distribution of pre-training, $T$ be the total diffusion time.
    A ``noising" process $\lbrace X_t \rbrace_{t\in[0,T]}$ from $p_0(x)$ is called as the Ornstein-Uhlenbeck (OU) process. The Stochastic Differential Equation (SDE) is
    \begin{equation}
        \mathrm{d}X_t = - X_t \mathrm{d}t + \sqrt{2}\mathrm{d}B_t, \quad X_0 \sim p_0,
    \end{equation}
    where $\lbrace B_t \rbrace_{t\in[0,T]}$ is $d$-dimensional Brownian motion. Then, we see $X_t|X_0 \sim \mathcal{N}(m_t X_0,\sigma_t), \; m_t = e^{-t}, \sigma_t^2 =1 - e^{-2t}$. We denote the law of $X_t$ as $p_t \coloneq \mathrm{Law}(X_t)$.
    A time-reversed ``denoising" process $\lbrace \Xrou_t \rbrace_{t\in[0,T]}$ is the reversed OU process
    \begin{equation}
        \mathrm{d}\Xrou_t = \lbrace \Xrou_t + 2 \nabla \log \bar{p}_{T-t}(\Xrou_t) \rbrace + \sqrt{2} \mathrm{d}B_t, \quad \Xrou_0 \sim p_T.
    \end{equation}
    Here, $\nabla \log p_{T-t}$ is called the score. To sample from $p_0$, it is known to be effective to run the modified backward process $\lbrace\Xref_t\rbrace_{t\in[0,T]}$ with the approximated score network $s:\mathbb{R}^{d+1}\to \mathbb{R}^d:(x,t) \mapsto s(x,t) \simeq \nabla \log p_t(x)$. In addition, we initialize $\Xref_0 \sim \mathcal{N}(0,I_d) \simeq p_T$ and the process is time-discretized.
    The SDE is formulated as
    \begin{equation}
        \mathrm{d}\Xref_t = \lbrace  \Xref_{\tprev}+ 2s(\Xref_{\tprev},T-\tprev) \rbrace
        + \sqrt{2}\mathrm{d}B_t, \quad t \in [l\delta, (l+1)\delta], \; l=1,...,L=T/\delta,
    \end{equation}
    where $\delta$ is the step size and $L=T/\delta$ is the number of steps, assumed to be integer for simplicity.  We define $\pref$ as the law of the output $\Xref_T$, and $\pmREF$ as the path measure of $\lbrace\Xref_t\rbrace_t$.
    
    \textbf{Score Matching.}\quad
    The score network is ideally selected to minimize the denoising score matching loss
    \begin{equation}
        \E_t [\E_{x_0}\E_{x_t|x_0}[\|s(x_t,t) - \nabla \log p_t(x_t|x_0)\|_2^2]],
    \end{equation}
    where $t \sim \mathrm{Unif}[0,T]$, $x_0 \sim p_0$, $x_t|x_0 \sim p_t(x_t|x_0)$. In practice, we have finite data $\lbrace x_{0,i}\rbrace_{i=1}^{n}$, $x_{0,i}\overset{\text{i.i.d.}}{\sim}p_0$. So the estimated score network $\hat{s}$ minimizes the empirical loss
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^n \E_t \E_{x_t|x_{0,i}}[\|s(x_t,t) - \nabla \log p_t(x_t|x_{0,i})\|_2^2]].
    \end{equation}
\end{comment}

\vspace{-3mm}
\section{Applications}
\vspace{-3mm}

Our framework of the distributional optimization for the pre-trained diffusion models
includes important fine-tuning methods for diffusion models.
% We are mainly interested in when $F(q)$ is a functional of the form
%\begin{equation}
%    F(q) = G(q, \pref), \quad G: \mathcal{P}\times\mathcal{P}\to \mathbb{R}\text{ is also a functional.}
% \end{equation}
% Then, (\ref{eq:setup-minimization}) can be seen as an objective for fine-tunes of the diffusion model. 
The first term $F(q)$ in (\ref{eq:setup-minimization}) represents \textit{the human preference}, acting as feedback from the outputs of $\pref$. Note that $F(q)$ can be dependent of $\pref$. The second term $\beta \KL(q\|\pref)$ in (\ref{eq:setup-minimization}) provides regularization to prevent $q$ from collapsing.

\begin{example}[Reinforcement Learning]
    Our study includes the case $F$ is limited to be a linear functional:
    \vspace{-2mm}
    \begin{equation}
        \underset{q \in \mathcal{P}}{\min}\left\lbrace \E_q [-r(x)] + \beta \KL(q \| \pref) \right\rbrace,
    \end{equation}
    \vspace{-1mm}
    where $r(x)$ is a reward function. In this case, the optimal distribution is obtained as
    %\begin{equation}
    $\textstyle    q_*(x) \propto \exp \left(\frac{r(x)}{\beta} \right) \pref(x)$.
    %\end{equation}
    This type of the problems has been studied as the Reinforcement Learning~\citep{fan2023reward,black2024reward,clark2024reward,uehara2024reward,marion2024implicit}. 
    %For example, see ~\cite{uehara2024RLHF}.
\end{example}

\vspace{-1mm}
The following two examples have not been directly solved via diffusion models:

\begin{example}[DPO]
    % \textcolor{red}{the functional derivative is bounded if the initialization is bounded, but not smooth (not in TV, nor in KL. $\because |\dFdq(q,x)-\dFdq(q',x)|\simeq |\frac{1}{q(x)}-\frac{1}{q'(x)}|$)}
    Direct Preference Optimization (DPO) \citep{rafailov2023DPO} is an effective approach for learning from human preference for not only language models but also diffusion models.
    Our algorithm can directly minimize the DPO objective,
    while \cite{Wallace2024DiffusionDPO} tried applying DPO to diffusion models via minimization of an upper bound of the original objective.
    In DPO, humans decide which sample is more preferred given two samples from $\pref$. Let $x_w$ and $x_l$ be ``winning" and ``losing" samples from $\pref$. $x_w \succ x_l$ denote the event that $x_w$ is preferred to $x_l$.
    The DPO objective can be written as
    \vspace{-1mm}
    \begin{equation}\textstyle
        L_{\mathrm{DPO}}(q) \coloneq - \E_{x_w\sim p_{\mathrm{ref}}}\E_{x_l\sim p_{\mathrm{ref}}}
        \left[ 
             \log \sigma \left(\gamma \log \frac{q(x_w)}{\pref(x_w)}- \gamma \log \frac{q(x_l)}{\pref(x_l)}\right)\mathbbm{1}_{x_w \succ x_l}(x_w,x_l)
        \right],
    \end{equation}
    where $\E_{x\sim p}$ denotes expectation with respect to $x$ whose probability density is $p \in \mathcal{P}$, $\sigma$ is a sigmoid function, $\mathbbm{1}_{x \succ y}(x,y)$ is one if $x \succ y$ and is zero otherwise.
    % We have the functional derivatives as the form $\E_{\pref}[\psi(f)]$, where $q = e^{-f}{\pref} / \int e^{-f}d{\pref}$, $\psi:\mathbb{R}\to\mathbb{R}$. 
    Precisely, the functional derivative of $L_\mathrm{DPO}(q)$ is calculated as
    \begin{align}\textstyle 
    \frac{\delta L_\mathrm{DPO}}{\delta q}(q,x) 
    =& \textstyle 
    -\gamma\E_{x_l\sim p_{\mathrm{ref}}}
    \left[
        \left(
            1- \sigma
            \left(
                -\gamma f(x) + \gamma f(x_l)
            \right)
        \right)
        \frac{\int e^{-f} dp_\mathrm{ref}}{e^{-f(x)}}
        \mathbbm{1}_{x \succ x_l}(x,x_l)
    \right]\\
    &\textstyle  +     \gamma\E_{x_w\sim p_{\mathrm{ref}}}
    \left[
        \left(
            1- \sigma
            \left(
                - \gamma f(x_w) 
                + \gamma f(x)
            \right)
        \right)
        \frac{\int e^{-f} dp_\mathrm{ref}}{e^{-f(x)}}
        \mathbbm{1}_{x_w \succ x}(x_w,x)
    \right],\label{eq-main-DPO-derivative}
    \end{align}
    where $q = e^{-f}\pref / \int e^{-f}\mathrm{d}\pref$. See Appendix~\ref{section:appendix-functional-derivative} for the derivation.
    Therefore, we only need samples from $\pref$ and the log-density ratio or the potential $f$ to calculate the functional derivatives.
\end{example}
\begin{example}[KTO]
    % \textcolor{red}{Note: the functional derivative is not sufficiently smooth (although it is bounded)}

    Our algorithm can also minimize $L_\mathrm{KTO}$ directly.
    Assume that the whole data space $\mathbb{R}^d$ is split into a desirable domain $\mathcal{D}_\mathrm{D}$ and an undesirable domain $\mathcal{D}_\mathrm{U}$.
    The objective of the original KTO \citep{ethayarajh2024KTO} is formulated as
    \begin{align}
\textstyle        L_\mathrm{KTO}(q)=
        &\textstyle \E_{x\sim \pref}
        \left[ 
        \gamma_D \left(1 - \sigma \left(\kappa\log \frac{q}{\pref} - \KL(q\|\pref)\right)
        \right)\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{D}\rbrace}
        \right. \\
        &\textstyle \quad \left.+
        \gamma_U \left(1 - \sigma \left(\KL(q\|\pref)-\kappa\log \frac{q}{\pref} \right)\right)\mathbbm{1}_{\lbrace x \in \mathcal{D}_\mathrm{U}\rbrace}
        \right],
    \end{align}
    where $\gamma_D, \; \gamma_U, \; \kappa$ are hyper parameters, and $\sigma$ is a sigmoid function. 
    \cite{li2024diffusionKTO} defined objectives compatible with diffusion models based on KTO, but our algorithm can directly minimize $L_\mathrm{KTO}$.
    Like the DPO objective, we only have to calculate samples from $\pref$ and the potential $f$ of the density ratio ($f$ is defined as $q = e^{-f}{\pref} / \int e^{-f}d{\pref}$) to calculate the functional derivatives. Please refer to Appendix~\ref{section:appendix-functional-derivative} for the concrete formulation of $\frac{\delta L_\mathrm{KTO}}{\delta q}$.
\end{example}

\vspace{-2mm}
\section{The Nonlinear Distributional Optimization Algorithm}
\label{section:algorithm-introduction}
\vspace{-1mm}

Now we make a concrete introduction of our proposed approach.
Our goal of the distribution optimization (\ref{eq:setup-minimization}) is to train a neural network that approximates $\fopt = \log \frac{\qstar}{p_{\rm ref}} + (\text{const.})$.
To achieve this, we utilize the \textit{Dual Averaging} (DA) algorithm \citep{Nesterov2009,NEURIPS2021_a34e1ddb,nishikawa2022twolayer}, and we iteratively construct a tentative local potential $f_k$ by approximating the update the DA algorithm.
After we obtain $f_K$, we estimate the diffusion model that generates the desired output  (approximately) following $q_*$, through \emph{Doob's h-transform technique}. 
%function $f_*$, we get the desired output $Y_T^\leftarrow \sim \exp(-f_*)\pref$ by Doob's h-transform.

\textbf{Phase 1: Dual Averaging.}\quad
Let $f_1$ be a randomly initialized potential. First, we initialize $\qone \propto \exp(-f_1)\pref$, where $f_1$ is a randomly initialized neural network.
Then, the distribution $q^{(k)}$ is updated recursively by pulling back the weighted sum of gradients from the dual space to the primal space.
There are two options of DA methods.
For a given hyper-parameter $\beta' > 0$, the update of Option 1 is given as 
\begin{flalign}
\text{\bf (Opt. 1)} &  & \qstarnext &    
\textstyle = \argmin\limits_{q \in \mathcal{P}}   
\Big\lbrace\frac{2}{k(k+1)} \sum\limits_{j=1}^k   j  \left( \E_q \left[ \dFdq(\qj) \right] + \beta \KL(q \| \pref) \right)  +  \frac{2 \beta'}{k} \KL(q \| \pref) \Big\rbrace  &  \\
 &  & &  =: \exp{(-\gbark)}\pref,   &   \label{eq:DAAlg1}
%    \qstarnext \coloneq \exp{(-\gbark)}\pref = \underset{q \in \mathcal{P}}{\min}\left\lbrace \E_q\left[\gbark \right] + \KL(q \| \pref) \right\rbrace,
\end{flalign}
where $\gbark(x) = \sum_{j=1}^k w_j^{(k)} \dFdq(\qj,x),~ w_j^{(k)} = \frac{j}{\beta k(k+1)/2+ \beta'(k+1)}~~(j=1,\dots,k)$. \revisedStart 
By Lemma~\ref{lem-da-nonconvex-nitanda} in Appendix~\ref{section:da-nonconvex-proof}, $\gbark$ can be explicitly determined.
\revisedEnd
% \begin{flalign} \textstyle 
% \text{where}~~
% \gbark(x) = \sum_{j=1}^k w_j^{(k)} \dFdq(\qj,x), ~~ w_j^{(k)} = \frac{j}{\beta k(k+1)/2+ \beta'(k+1)}~~(j=1,\dots,k),  & & 
% %    \gbark(x) = \frac{1}{\beta}\sum_{j=1}^k w_j \dFdq(\qj,x), \quad w_j = \frac{2j}{(k+1)(k+2)}, \text{ for $j=1,...,k$.}
% \end{flalign}
We train a neural network $f_{k+1}$ to approximate $\gbark$\footnote{
For DPO and KTO, it suffices to obtain the neural network $\gbark$ to minimize $\E_{\pref}[(f - \gbark)^2]$ where the expectation with respect to $\pref$ is simulated by generating data from $\pref$, while obtaining $\gbark$ for general settings requires Doob’s h-transform similar to Phase 2. Please also refer to Section~\ref{sec:numerical-experiments}.
% While it is possible to apply Doob’s h-transform to obtain $\gbark$ here for general settings, for DPO and KTO, it suffices to train the neural network by minimizing $\E_{\pref}[(f - \gbark)^2]$ where the expectation with respect to $\pref$ is simulated by generating data from $\pref$. 
% Although it is possible to perform a rigorous error analysis for this approximation, we leave it out in this work for simplicity.
} and define the next step as $q^{(k+1)} \propto \exp{(-f_{k+1})}\pref$. Similarly, the update of Option 2 is given as
\begin{flalign}
\text{\bf (Opt. 2)} &  \!\!\!& \qstarnext &    
\textstyle \!=\! \argmin\limits_{q \in \mathcal{P}}   
\Big\lbrace\frac{2}{k(k+1)} \sum\limits_{j=1}^k   j  \left( \E_q \left[ \dFdq(\qj) 
 - \beta \gbarj \right] \right) \! + \! \frac{2 \beta'}{k} \KL(q \| \pref) \Big\rbrace.   & \label{eq:DAAlg2}
 %\\ & & & \coloneq \exp{(-\gbark)}\pref.   & 
%    \qstarnext \coloneq \exp{(-\gbark)}\pref = \underset{q \in \mathcal{P}}{\min}\left\lbrace \E_q\left[\gbark \right] + \KL(q \| \pref) \right\rbrace,
\end{flalign}
Here, we again express as $\qstarnext(x) \propto \exp(-  \gbark(x)) \pref(x)$ where 
$\gbark(x) = \sum_{j=1}^k w_j^{(k)} ( \dFdq(\qj,x) -\beta \gbarj(x))$ with $w_j^{(k)} = \frac{j}{\beta' (k+1)}$. 
Then, $q^{(k+1)}$ is obtained in the same manner as Option 1. 
This phase of DA update is summarized in Algorithm~\ref{alg:DA-train-f-mainpart}. \revisedStart For the more detailed algorithm in Option 1, please refer to Algorithm~\ref{alg:DA-train-f}.\revisedEnd
\vspace{-2mm}
\begin{algorithm}[htbp]
\label{alg:DA-train-f-mainpart}
\caption{Dual Averaging (DA)}
\begin{algorithmic}
    \REQUIRE{
            $s$: pre-trained score,
            $f_1$: initialized neural networks
        }\\
    \ENSURE{
            $f_{K}$: a trained potential.
    }
    \STATE Set  $q^{(0)}=\pref$ and $q^{(1)} \propto \exp(- f_1)\pref$
    \FOR{$k = 1,...,K-1$}
        \STATE Obtain $\gbark$ via the DA algorithm with Option 1 (Eq.~\eqref{eq:DAAlg1}) or Option 2 (Eq.~\eqref{eq:DAAlg2}) using the recurrence formula (\ref{eq-appendix-experiment-recurrence}), where $\qstarnext \propto \exp(-\gbark)\pref$ is the ideal update. 
        \STATE Train a neural network $f_{k+1}$ to approximate $\gbark$, and set $q^{(k+1)} \propto \exp{(-f_{k+1})}\pref$.  % $\frac{k}{k+2}f_{k} + \frac{2k}{\beta k(k+2)}\dFdq(\qk)$
    \ENDFOR
    %\STATE \textbf{End}
\end{algorithmic}
\end{algorithm}

\vspace{-2mm}
\textbf{Phase 2: Sampling with Doob's h-transform.}\quad
After we obtain the solution $f_K$, we want to sample from $q_K \propto \exp(-f_K)p_{\rm ref}$, which approximates the optimal solution of (\ref{eq:setup-minimization}).
When sampling from $q_K$, it is necessary to obtain the score function related to this distribution. However, constructing the score function of $q_*$ only from the score function of $p_{\rm ref}$ and $f_K$ requires a particular technique.
Specifically, we apply Doob's h-transform~\citep{Rogers2000Doob,chopin2023doob,uehara2024reward,heng2024schrodingerbridge}.
By introducing the correction term $u_*\colon \R^{d+1}\to\R^d$ defined by 
\begin{equation}
u_*(y,t)= \nabla \log \E[\exp(-f_*(\bar{X}_T^\leftarrow))\mid \bar{X}_t^\leftarrow =y], % \Xref_{T-t}=y], 
%\E_{\pmREF}[\exp(-f_*(\Xref_T))\mid \Xref_{T-t}=y],
\end{equation}
the score function of $q_*$ at $(x,t)$ is written as $
 \nabla \log q_{t}(y)  =  \nabla \log p_{t}(X^\leftarrow_t,T-t)+u_*(y,t)$, where $q_{t}$ is the law of the backward process at time $t$ whose output distribution is the optimal solution $q_*$.
 We provide the derivation in Lemma~\ref{lem:H-transform} and refer readers to \cite{Rogers2000Doob,chopin2023doob} for more details and a formal
treatment of Doob’s h-transform.
By approximating $\log p_{t}(X^\leftarrow_t,T-t)$ by the score network $s(x,t)$ and the correction term $u_*(x,t)$ by $u(x,t)$ and discretizing the dynamics, 
we obtain the following update
\begin{equation}
    Y^\leftarrow_0 \sim \mathcal{N}(0,I_d),\
    \mathrm{d}Y^\leftarrow_t = \lbrace Y^\leftarrow_t+ 2(s(Y^\leftarrow_{lh},lh) + u(Y^\leftarrow_{lh}, lh)) \rbrace \mathrm{d}t
    + \sqrt{2}\mathrm{d}B_t,\ t\in [lh,(l+1)h],
    \label{eq:h-transformed-backward}
\end{equation}
% then it is known that $\rho_t$ yields the density ratio $\rho_t = q_{K,t}/p_{\mathrm{ref},t}$ where  $p_{\mathrm{ref},t}$ and $q_{K,t}$ are the densities of $\Xref_t$ and $\Ydoobd_t$ respectively. 
% In particular, we can see that the (unnormalized) density ratio at $T$ is $h_T(x) = \exp(-f_K(x))$ that indicates exactly what we want: $h_T(x) = \exp(-f_K(x)) \propto q_K/\pref$. 
% Indeed, if we let $p_{\mathrm{ref},t}$ and $q_{K,t}$ be the density of $\Xref_t$ and $\Ydoobd_t$ and write $h_t = q_{K,t}/p_{\mathrm{ref},t}$ that is the density ratio at time $t$, then the correction term $u$ is calculated as the ``score" of the density ratio:
%where $u(y,t) \simeq \nabla \log h_t(\Xref_t) = \nabla \log \E_{\pmREF}[\exp(-f_K(\Xref_T))\mid \Xref_t=y]$, 
% which we call Doob's h-transform. 
% Consequently, using the approximated score function $\nabla \log p_{\mathrm{ref},t} \simeq s(\Ydoobd_{t},T-t)$ of the original model, 
% the score function of the fine-tuned diffusion model obtained by the Doob's h-transform
% can be obtained by 
% \begin{equation}
%  \nabla \log q_{K,t}(y)  \simeq  s(y,T-t)+u(y,t),
% \end{equation}
% as an approximate of the score $\nabla \log q_{K,t}(y)$. 
where $u(x,t)$ can be computed as $u(x,t) = \nabla_x \log \E[\exp(-f_K(X_T^{\leftarrow}))|X_t^{\leftarrow} = x]$,
which can be estimated by running the reference diffusion model $(X_t^{\leftarrow})$.   
% To implement the dynamics above we discretize the process and we need to approximate the score function by, e.g., neural networks.
%We further need to approximate $u(x,t)$, because this involves expectation and differentiation regarding some expectation of $f_K(x)$. 
The practical treatment for this is discussed in Appendix~\ref{section:appendix-error-diffusion}. 
\revisedStart For experimental information, please have a look at Section~\ref{sec:numerical-experiments} and Algorithm~\ref{alg:doob-sampling} in Appendix~\ref{sec:Appendix-Experimennt}.\revisedEnd

\vspace{-2mm}
\section{Theoretical Analysis \label{section_theoretical}}
\vspace{-2mm}
In this section, we give theoretical justification of our proposed algorithm.
More concretely, we show the rate of convergence of the (inexact) DA method and give an approximation error bound on the diffusion model based on the h-transform. 

\vspace{-2mm}
\subsection{Convergence rate of the DA method}
\vspace{-1mm}

We give the convergence rate of the DA algorithm in the two settings: when $F$ is (I) 
 convex and (II) non-convex, respectively.  

\noindent \textbf{(I): Convex objective $F$.}
First, we show the rate when $F$ is convex. 
We basically follow the proof technique of \cite{NEURIPS2021_a34e1ddb,nishikawa2022twolayer}.
In the analysis, we put the following assumption on $F$. 
%for Theorem~\ref{thm-da-nishikawa} are as follows:
\begin{assumption}\label{ass:ConvexF}
The loss function $F$ satisfies the following conditions:     
\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]
    \item[(i)] $\dFdq$ is bounded: There exists $B_F > 0$ such that $\|\dFdq(q)\|_\infty \leq B_F$ for any $q \in \mathcal{P}$,
    \item[(ii)] $\dFdq$ is Lipshitz continuous with respect to the TV distance: There exists $L_{\mathrm{TV}}  > 0$ such that $\|\dFdq(q) - \dFdq(q')\|_\infty \leq L_\mathrm{TV} \TV(q,q')$ for any $q,q' \in \mathcal{P}$.
    \item[(iii)] $F$ is convex: $F(q) \geq F(q') + \int \dFdq(q')\mathrm{d}(q-q')$ for any $q,q' \in \mathcal{P}$,
\end{enumerate}
\end{assumption}

Then, we can show that the (inexact) DA algorithm achieves the following convergence rate that yields $\mathcal{O}(K^{-1})$ convergence of the objective.
\begin{thm}[\revisedStart Convergence of the objective in Option 1 \revisedEnd]
    \label{thm-da-nishikawa}
    Suppose that $\beta' \geq \beta$ and we train the potential $f_{k+1}$ so that it is sufficiently close to $\gbark$ as 
    %\begin{equation}
        $\TV(\qstark,\qk) \leq \epsilon_\mathrm{TV}$ for all $k$. 
    %\end{equation}
    Then, under Assumption \ref{ass:ConvexF}, Option 1 satisfies the following convergence guarantee: 
  \begin{equation}
    \frac{2}{K(K+1)}\sum_{k=1}^{K}k \left[ L(\qstark) - L(\qstar) \right]
\leq 
2L_{\mathrm{TV}}\epsilon_{\mathrm{TV}}+\left(\frac{2B_F}{K(K+1)}  + \frac{2 \beta' \KL(q_*\|\pref) + 2B_F^2 \beta^{-1}}{K} \right).
  \end{equation}
%    where
%  \begin{equation}
%    $\epsilon_{\mathrm{FD}} = \frac{2}{K(K+1)}\sum_{k=1}^{K} k \epsilon_{\mathrm{TV}}$. 
%  \end{equation}
    % \begin{align}
    %     &\frac{2}{K(K+1)}\sum_{k=1}^{K}
    %     k \left[
    %         L(\qstark) - L(\qstar)
    %     \right]\\
    %     \leq&
    %     2L_\mathrm{TV}\epsilon_\mathrm{TV}
    %     +\mathcal{O}
    %     \left(
    %         \frac{B_F}{K^2}
    %         +
    %         \frac{
    %         \beta \KL(\qstar\|\pref)
    %             + \beta^{-1}B_F^2
    %         }
    %         {
    %             K
    %         }
    %     \right).
    % \end{align}
\end{thm}
See Appendix~\ref{sec:AppendixConvProof} for the proof.
From this theorem, we see that the DA algorithm with Option 1 achieves $\gO(1/K)$ convergence. The assumption (ii) in Assumption \ref{ass:ConvexF} is required to bound an expectation of $\gbark$ in the bound.  
The assumption (iii) is required to bound the difference between the exact update $\qstark$ and the inexact update $q^{(k)}$. If the update is exact, we don't need this assumption. 

\noindent \textbf{(II): Non-convex objective $F$.}
We also give a convergence for a non-convex loss $F$.  
Here, we put the following assumption. 
\begin{assumption}\label{ass:NonconvexF}
The loss function $F$ satisfies the following conditions:     
\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]%\label{ass:NonconvexF}
    % \item[(i)] $F$ is smooth: There exists $S_F \geq 0$ such that $F(q) \leq F(q') + \int \dFdq(q')\mathrm{d}(q-q') + \frac{S_F}{2}\KL(q\|q')$ for any $q,q' \in \gP$,
    \item[(i)] $\dFdq$ is bounded: There exists $B_F > 0$ such that $\|\dFdq(q)\|_\infty \leq B_F$ for any $q \in \gP$,
    \item[(ii)] $\dFdq$ is Lipshitz continuous with respect to the TV distance: $\|\dFdq(q) - \dFdq(q')\|_\infty \leq L_{\mathrm{TV}} \TV(q,q')$ for any $q,q' \in \gP$, %\textcolor{red}{TV or KL}
    \item[(iii)] $F$ is lower bounded.
\end{enumerate}
\end{assumption}
Assumptions (i) and (ii) are the same as the convex case (Assumption \ref{ass:ConvexF}), and lower boundedness (iii) is weaker than convexity in Assumption \ref{ass:ConvexF}.
% The smoothness condition (i) is a quite standard assumption in the finite dimensional optimization literature. It is a just functional derivative version of its finite dimensional counterpart. 
Assumption (ii) induces the following type of smoothness commonly observed in standard optimization: (ii)' There exists $S_F \geq 0$ such that $F(q) \leq F(q') + \int \dFdq(q')\mathrm{d}(q-q') + \frac{S_F}{2}\KL(q\|q')$.
%\begin{itemize}
    %\item[(ii)'] (A weaker version of (ii)). \\
    %There exists $S_F \geq 0$ such that $F(q) \leq F(q') + \int %\dFdq(q')\mathrm{d}(q-q') + \frac{S_F}{2}\KL(q\|q')$
%\end{itemize}
When the inner-loop error is ignored, it is possible to prove convergence using only the smoothness assumption (ii)' instead of assumption (ii). For details, please refer to Appendix~\ref{sec:AppendixNonconvexConv}.

Under this assumption, we can show the following convergence guarantee with respect to the sequence $(\qstark)_{k=1}^K$ even in a non-convex setting.  
\begin{thm}\label{thm:NonconvexConv}
%%%%%%%%%%%% S_F = 2 * L_TV %%%%%%%%%%%%%%
Suppose that $\beta' > \beta + \LipTV$ and $\TV(\qstark,\qk) \leq \epsilon_\mathrm{TV}$ for all $k$, then under Assumption \ref{ass:NonconvexF}, both Option 1 and 2 yield the following convergence: 
    \begin{itemize}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]
        \item[(i)] $\lim_{k \to \infty} \KL(\qstarnext\|\qstark) = 0$.
        \item[(ii)] For all $K$, it holds that
        \begin{equation}
            \underset{k=1,...,K}{\min}\left\{c_k \KL(\qstarnext \|\qstark \right) \}
            \leq 
            \frac{(\tilde{L}_1(\qstarone - L(\qstar)) + (\LipTV +B_F) K \epsilon_\mathrm{TV} }{K\beta'} =: \Psi_K,
        \end{equation}
        where $c_k = \frac{\beta k + 2 \beta' }{2}$ for Option 1 and $c_k =1$ for Option 2. 
    \end{itemize}
\end{thm}
See Appendix~\ref{sec:AppendixNonconvexConv} for the proof.
The proof utilizes an analogous argument with \cite{LIU2023nonconvexDA} that analyzed convergence of a DA method in a finite dimensional non-convex optimization problem. However, we need to reconstruct a proof because we should work on the probability measure space, which is not a vector space, and carefully make use of the property of the KL-divergence. 
We see that $\Psi_K = \gO(1/K)$ if $\epsilon_\mathrm{TV}$ is sufficiently small as $\epsilon_\mathrm{TV} = \gO(1/K)$, and thus the discrepancy between $\qstarnext$ and $\qstark$ converges. 
Especially, the convergence of $\KL(\qstarnext\|\qstark)$ yields the convergence of the ``dual variable'' for Option 2 as in the following corollary. 
For that purpose, we define 
%\begin{equation}
$\tilde{L}_k(q) \coloneq L(q) + \frac{\beta'}{k}\KL(q\|\pref)$ 
%\end{equation}
(see its similarity to the inner objective of the DA update \eqref{eq:DAAlg1} and \eqref{eq:DAAlg2}), 
and define $\psi_{q}\left( g \right) = \log\left(\E_q[\exp( - g + \E_q[g])] \right)$ which is a ``moment generating function'' of a dual variable $g$. 
\begin{cor}[\revisedStart Convergence in Option 2 \revisedEnd] \label{cor:DualConv}
Under the same condition as Theorem \ref{thm:NonconvexConv}, we have that 
$$
\textstyle \min\limits_{1 \leq k \leq K} \psi_{\qstark}\left(\frac{k}{\beta' (k+1)}\frac{\delta \tilde{L}_k}{\delta q}(\qstark)  \right) 
\leq  \Psi_K.
$$
\end{cor}
\vspace{-0.3cm}
Roughly speaking, this corollary indicates that 
the variance of the gradient $\frac{\delta \tilde{L}_k}{\delta q}(\qstark)$ converges as $\mathrm{Var}_{\qstark}\left(\frac{\delta \tilde{L}_k}{\delta q}(\qstark)\right) = \gO(1/K)$ because we may approximate $\psi_{q}(g) \simeq \frac{1}{2}\mathrm{Var}_q(g)$ when $|g- \E_q[g]|$ is sufficiently small.
Therefore, we have a convergence guarantee of the dual variable (gradient) $\frac{\delta \tilde{L}_k}{\delta q}(\qstark,x) \to 0 \; \text{(up to a constant w.r.t. $x$)}$ even in a non-convex setting, which justifies usage of our method for general objective functions.  

\vspace{-1mm}
\subsection{Discretization error of Doob's h-transform}\label{subsection:doob}
\vspace{-1mm}

We now provide the sampling error analysis after obtaining the approximate solution $f_K$.
\revisedStart
In the analysis of Dual Averaging, $\pref$ was assumed to be known, and the goal was to obtain the optimal solution of (\ref{eq:setup-minimization}) that corresponds to $\qstar = \exp(-\fopt)\pref$. From here, considering that $\pref$ estimates $p_*$, we shift our focus to sample from the optimal alignment $q_* \propto \rho_* p_*$, \revisedEnd while the dynamics we implement involves several approximations:    
(i) time discretization approximation, (ii) approximation of the score $\nabla_x \log p_t$ by $s(x,T-t)$, (iii) approximation of $\rho^*$ by $\rho = \exp(-f_K)$, (iv) approximation of $u_*$ by $u$.  
%, (iv) empirical approximation of $\nabla \log \rho_t(y)$. 

%The first approximation is discretization of the dynamics (\ref{eq:h-transformed-backward}). 

% In addition to this, we need to approximate the correction term $u_*(x,t)=\nabla_x\mathbb{E}_{\mathbb{P}_{\rm ref}}[\exp (- f_*(X_0))|X_t=x]$ by using $u = \exp (- f_K)$, as
% \begin{align}
%     \hat{u}(x,kh) =  
% & 
% \frac{ \frac{1}{n} \sum_{i=1}^n \left[ 
% \rho'({x}_{i,(T - lh)/h}) 
% \frac{(I - 2 \nabla^\top s(x,lh))}{\sqrt{e^{2h}-1}} [{x}_{i,(l+1)h} - (x - 2 s(x,lh))] 
% \right]}{\frac{1}{n} \sum_{i=1}^n \rho'({x}_{(T - lh)/h}) },
% \end{align}
% where 
% $D_{n,l}(x) := (({x}_{i,kh})_{k=l+1}^{(T - lh)/h})_{i=1}^n$ is an i.i.d. sequence generated from our reference model with a initial condition $X_{lh} = x$.  
% By using $\hat{u}(x,kh)$, we obtain the implementable dynamics as
% \begin{equation}
%     \mathrm{d}\Ydoobc_t = [\Ydoobc_t+ 2 (s(\Ydoobc_{\tprev},\tprev) + \hat{u}(\Ydoobc_{\tprev},\tprev))] 
%     + \sqrt{2}\mathrm{d}B_t, \quad t \in [\tprev, \tnext].
% \end{equation}

% \begin{equation}
%     \mathrm{d}\bar{X}^{\leftarrow}_t = (\bar{X}_t^\leftarrow + 2\nabla_x p_{T-t}(\bar{X}^\leftarrow_{\tprev}) ) \rd t 
%     + \sqrt{2}\mathrm{d}B_t, \quad t \in [\tprev, \tnext].
% \end{equation}
% The SDE corresponding to our model: 





%Here, we want to bound the discrepancy between $q_* \propto \rho_* p_*$ and $\hat{q} \propto \rho \pref$ where $\rho = \exp(- f_K)$ obtained by the DA algorithm in Phase 1. 

% In the above SDE, $\nabla \log \rho_{t}(x)$ is calculated as
% \begin{align}
%     &\nabla \log\rho_t(x)\\
%     =&\nabla \log \E_{\pmREF}[\rho_T(\Xref_T) \mid \Xref_t = x]\\
%     =& 
%     \frac{
%         \int
%         \E_{\pmREF}[\rho_T(\Xref_T)\mid \Xref_{\tnext}=y]\frac{\partial}{\partial x}\mathcal{N}(\Xref_{\tnext}\mid m_{\tnext - t}(x), \sigma^2_{\tnext - t})\mathrm{d}y
%     }{
%         \int
%         \E_{\pmREF}[\rho_T(\Xref_T)\mid \Xref_{\tnext}=y]\mathcal{N}(y \mid m_{\tnext - t}(x), \sigma^2_{\tnext - t})\mathrm{d}y
%     },
% \end{align}
% where $m_{u}(x) = e^u x + (-2 s_{T-\tprev}(x))(1-e^u), \; \sigma^2_u = e^{2u} - 1$.
% The partial derivative $\frac{\partial}{\partial x}\mathcal{N}(y \mid m_{\tnext - t}(x), \sigma^2_{\tnext - t})$ is
% \begin{equation}
%     \frac{\left(\frac{\partial}{\partial x}m_{\tnext - t}(x)^\top \right) (m_{\tnext - t}(x) - y)}{\sigma_{\tnext - t}^2}\mathcal{N}(y \mid m_{\tnext - t}(x), \sigma^2_{\tnext - t}).
% \end{equation}
% Note that $\left(\frac{\partial}{\partial x}m_{\tnext - t}(x)^\top \right)$ is a $d \times d$ matrix and $\left(\frac{\partial}{\partial x}m_{\tnext - t}(x)^\top \right)_{ij} = \frac{\partial}{\partial x_i}m_{\tnext - t}(x)_j$.
% Finally, we discretize $\nabla \log \rho_t$ with respect to time $t$:
% \begin{equation}
%     \mathrm{d}\Ydoobd_t = \lbrace  \Ydoobd_t+ 2s(\Ydoobd_{\tprev},\tprev) + 2\nabla \log \rho_{\tprev}(\Ydoobd_{\tprev}) \rbrace
%     + \sqrt{2}\mathrm{d}B_t, \quad t \in [\tprev, \tnext].
% \end{equation}
%Let $u_*(x,t)$ be the optimal correction term as $u^*(x,t) = \nabla_x \log(\E[\rho_*(\bar{X}_T^\leftarrow)|\bar{X}_t^\leftarrow = x])$ where $\bar{X}_t^\leftarrow$ is the reverse process of the true reference distribution $p_t$. 


To evaluate how such approximation affects the final quality of our generative model, 
we will give a bound of the TV-distance between $q_*$ and $\hat{q} = \rho \pref$ by putting all those errors together. 
To do so, we put the following assumption.  
\begin{assumption}\label{assumption:TVBoundMainText}
\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]
\item 
    $\nabla \log p_t$ is $L_p$-smooth at every time $t$ and it has finite second moment $\mathbb{E}[\|\bar{X}_t\|^2_2] \leq \mathsf{m} < \infty$ for all $t\in \R_+$ and $x\in \R^d$. 
\item  $\nabla \log \rho_*$ is $L_\rho$-smooth and bounded as $C_\rho^{-1}\leq \rho_* \leq C_\rho$ for a constant $C_\rho$.
\item   The score estimation error is bounded by 
\revisedStart
$\E_{\bar{X}_{\cdot}^{\leftarrow}}[\|s(\bar{X}_{t}^{\leftarrow},t) \!\! - \!\! \nabla \log p_{T-t}(\bar{X}_{t}^{\leftarrow})\|^2]\!\leq \!\varepsilon$ 
\revisedEnd
at each time $t$. 
\item 
$\E_{\bar{X}_{\cdot}^{\leftarrow}}[\|u_*(\bar{X}_t^{\leftarrow},t) - u(\bar{X}_{lh}^{\leftarrow},lh)\|^2] \leq \varepsilon_{\rho,l}^2$~~for any $1 \leq l \leq T/h$
and $t \in [lh,(l+1)h)$.
%$\E_{X_{lh}}[\E_{\bar{D}_{n,l}(X_{lh})}[\|u_*(X_{lh},lh) - \hat{u}(X_{lh},lh)\|^2]] \leq \varepsilon_{\rho,l}^2$~~for any $1 \leq l \leq T/h$
\end{enumerate}
\end{assumption}
This assumption is rather standard, for example, \cite{chen2023sampling} employed these conditions except the last condition on $u$ and $u_*$.
The fourth assumption in Assumption~\ref{assumption:TVBoundMainText} is imposed to mathematically formulate the situation: $\qstar/\pref\simeq q_*/p_*$.  
%Under this assumption, 
Then, we obtain the following error bound: 
\begin{thm}\label{thm:Diffusion-1}
Suppose that Assumption \ref{assumption:TVBoundMainText} is satisfied. Then, we have the following bound on the distribution $\hat{q}$ of $Y^\leftarrow_T$: 
\begin{equation}
\textstyle
\TV(q_*,\hat{q})^2
\lesssim  T \varepsilon^2 + \sum_{l=1}^{T/h} h \varepsilon_{\rho,l}^2 + T (L_pC_\rho^2+L_\rho)^2(dh + \mathsf{m} h^2 )+ \exp(-2 T)\KL(q_* \| N(0,I)).
\end{equation}
\end{thm}
The proof is given in Appendix \ref{sec:AppendixCorrectionTermBound}. 
It basically follows \cite{chen2023sampling,chen2023improved}, but we have derived the smoothness of $\nabla \log(q_{*,t})$ from that of $\nabla \log(p_{t})$. 
In this bound, we did not give any evaluation on $\varepsilon_{\rho,l}^2$, however, this error term can be bounded as follows with additional conditions. 
\begin{assumption}\label{ass:BoundingHMainText}
%\begin{enumerate}[topsep=0mm,itemsep=-1mm,leftmargin = 6mm]\label{ass:NonconvexF}
    %\item[(i)] 
    (i) $\nabla_x s(\cdot,\cdot)$ is $H_s$-Lipschitz continuous in a sense that $\|\nabla_x s(x,t) - \nabla_y s(y,t)\|_{\mathrm{op}} \leq H_s \|x- y\|$
for any $x,y \in \sR^d$ and $0 \leq t \leq T$ and $\E[\| s(\tilde{X}_{kh}^\leftarrow,kh)\|^2] \leq Q^2$ for any $k$, 
    %\item[(ii)] 
    (ii) There exists $R > 0$ such that $\sup_{t,x}\{\|\nabla_x^2 \log p_t(x)\|_{\mathrm{op}},\|\nabla_x^2 \log s(x,t)\|_{\mathrm{op}}\} \leq R$.
%\end{enumerate}
\end{assumption}
\begin{thm}\label{thm:Diffusion-2}
Suppose that Assumptions \ref{assumption:TVBoundMainText} and \ref{ass:BoundingHMainText} hold 
%the same condition as Theorem \ref{thm:delhDiffXYExp} holds 
and $\|\rho_* - \rho\|_\infty \leq \varepsilon'$ and $\|\rho\|_\infty \leq C_\rho$. 
We also assume $\nabla \rho^*$ is bounded and Lipschitz continuous. 
%Let $L_\varphi$ and $R_\varphi$ be as given in Lemma \ref{lemm:phiYboundLip}.
Then, for any choice of $0 \leq h \leq \delta \leq 1/(1 + 2R)$, we have that 
\begin{align}
%& %\E_{\bar{Y}_{\cdot}^{\leftarrow}}[ \|\nabla_x \E[\rho(\tilde{X}_{T}^\leftarrow) |  \tilde{X}_{t}^\leftarrow = x] |_{x = \bar{Y}_{t}^{\leftarrow}} -  \hat{u}(\bar{Y}_{k_t h}^\leftarrow,t) \|^2]  
\textstyle \varepsilon_{\rho,l}^2 %\\
\lesssim & \textstyle 
C_\rho^3 \left\{ \Xi_{\delta,\varepsilon}
+  R_\varphi^2 \left(\varepsilon^2+ \Lipdp^2 d(\delta + \mathsf{m} \delta^2) \right) + 
[ L_\varphi^2 (\mathsf{m} + Q^2 + d h)  
+ R_\varphi^2  (1 + 2R)^2] h^2\right\} \\
& \textstyle +   \min\{T-lh,1/(2+2R)\}^{-1} \varepsilon'^2,
%+  C_\rho (1 + 2R) \sqrt{\frac{\log(T/h)}{n h}}, 
%= & \gO\left(\varepsilon^2 + \delta + \frac{\varepsilon_\TV^2}{\delta}\right), 
\end{align}
where 
%\begin{align}
$
\Xi_{\delta,\varepsilon} := C_\rho^2 (1+2R)^2 \delta  
+  C_\rho^2 \frac{1 + \delta R_\varphi^2}{\delta} [\varepsilon^2+ \Lipdp^2 d(h + \mathsf{m} h^2)], 
%\varepsilon_{\TV}^2
%+  R_\varphi^2  [\varepsilon^2 + \Lipdp^2 d(\delta + \mathsf{m} \delta^2)],
%&  = \gO\left(\delta + \varepsilon^2 + \frac{\varepsilon_\TV^2}{\delta}\right),
$ and 
$R_\varphi$ and $L_\varphi$ are constants introduced in Lemma \ref{lemm:phiYboundLip}. 
%\end{align} 
%and $c_\eta > 0$ is a universal constant. 
\end{thm}
The proof is given in Appendix~\ref{sec:proofOfCorrection}.
We applied the so-called {\it Bismut-Elworthy-Li} integration-by-parts formula \citep{MR755001,ELWORTHY1994252} to obtain the discretization error. 
By substituting $\delta \leftarrow \sqrt{h}$ to balance the terms related to $h$ and $\delta$, we obtain a simplified upper bound of $\sum_{l=1}^{T/h} h \varepsilon_{\rho,l}^2$ as 
$
\sum_{l=1}^{T/h} h \varepsilon_{\rho,l}^2 \lesssim 
\left( 1 + \frac{1}{\sqrt{h}}\right) \varepsilon^2 T + T \sqrt{h} + ( T + \log(1/h)) \varepsilon'^2. 
$
These results can be seen as $h$-Transform extension of the approximation error analysis given in \cite{chen2023sampling}. 
However, the approximation error corresponding to $u$ has worse dependency on $h$. This is because the computation of $u$ uses the discretized process $\pref$ and is affected by its error while the ordinary diffusion model does not require sampling to obtain the score. 

\vspace{-2mm}
\section{Numerical Experiments}\label{sec:numerical-experiments}
\vspace{-2mm}

We conducted numerical experiments to confirm the effectiveness of minimizing nonlinear objectives. We used Option 1 and $\beta$ was set to be 0.04.
We also compared the DPO objective~\citep{rafailov2023DPO} we minimized with the upper-bound~\citep{Wallace2024DiffusionDPO} using a toy case, Gaussian Mixture Model (GMM).
For detailed experimental setting, please refer to Appendix~\ref{section:appendix-experiments}.

\textbf{Alignment for Gaussian Mixture Model.}\quad
We aligned a score-based diffusion model to sample from a 2-dimensional GMM using the DPO objective. The reference density was defined as $\frac{1}{2}\left(\mathcal{N}(\mu_1,\Sigma) + \mathcal{N}(\mu_2,\Sigma)\right)$, $\mu_1 = [-2.5, 0], \; \mu_2 = [2.5, 0], \; \Sigma = [[1, 0],[0,5]]$.
The target point was $\mu_w := \mu_2$.
The preference of points $x_w$ and $x_l$ was determined by the Euclidean distance $d(\cdot, \mu_w) $ from $\mu_w$. $x_w \succ x_l$ if and only if $d(x_w,\mu_w) < d(x_l,\mu_w)$. \revisedStart We describe the implementation details below:

% \begin{itemize}[topsep=0mm,leftmargin = 8mm]
% \item[(1)]

\textit{Dual Averaging.}\quad
We set the hyperparameter $\beta'$ in [0.04, 0.2], which controls the learning speed illustrated at the middle in Figure~\ref{fig:da-summary}.
In $k$th DA iteration ($k=1,...,K$), using empirical samples $\lbrace x_i = (x_{1,i},x_{2,i})\rbrace_{i=1}^{1000}$ from $\pref$ and the previous potential $f_{k-1}$ ($q^{(k-1)} \propto e^{-f_{k-1}}\pref$) implemented by neural networks, we prepare $\lbrace (x_i, \dFdq(q^{(k-1)},x_i))\rbrace_{i=1}^{1000}$ to construct $f_{k} \simeq \bar{g}^{(k-1)}$ with the recurrence formula (\ref{eq-appendix-experiment-recurrence}) in Appendix~\ref{section:appendix-experiments}.
Please note that, to calculate $\dFdq$ for DPO, we only need the $f_k $ and empirical samples from $\pref$ to calculate the expectation $\E_{x\sim \pref}[\cdot]$ as described in Eq. (\ref{eq-main-DPO-derivative}).
The convergence of the true DPO loss is depicted in Figure~\ref{fig:da-summary}, compared to Diffusion-DPO~\citep{Wallace2024DiffusionDPO}. The true DPO loss, upperbound, and metric loss (mean of the Euclidean distance of the particles from $\mu_w$) attained by Diffusion-DPO and our method, with optimization time and GPU memory consumption (in Phase I for ours), are summarized in Table \ref{table:comparison}.
%In practice, it is hardly realistic to compute the true loss during Diffusion-DPO, but in this case, we forcefully carried out the computation: The densities $q$ and $\pref$ were estimated by repeatedly computing the denoising path and empirically obtaining their marginal densities.
Diffusion-DPO optimizes the approximate upperbound instead of the true loss, and therefore it failed to completely control the true loss.
Compared to Diffusion-DPO, our algorithm allowed us to optimize the loss to a smaller value within an acceptable computational budget. 
% \textit{Comparison with the upper-bound in the previous work.}\quad
% We see that all of them decreased during DA, however, a noticeable gap persisted between the two, as shown in Figure~\ref{fig:da-summary}. 
% This indicates that the upper bound is not tight enough to guarantee the minimization of the original objective.
\revisedEnd

% \item[(2)]
\textit{Doob's h-transform.}\quad
We constructed the aligned model by calculating the correction terms. The conditional expectations were calculated using naive Monte Carlo method with 30000 samples.
The number of the diffusion steps was 50. The histograms of aligned samples are shown at the right of Figure~\ref{fig:da-summary}. 
% The empirical densities of them reflect non-linearity of the objective. 
\revisedStart
In the simplest phase 2 that we present (Algorithm~\ref{alg:doob-sampling}), we used significant computational resources, with $\mathcal{O}(L^2)$ time complexity. To estimate the correction term $u$ for each sample simultaneously by Monte Carlo with $N$ samples, $\mathcal{O}(N)$ memory space is required. However, this Doob's h-transform technique itself has been used in image generation~\citep{uehara2024RLHF,uehara2024reward}, Bayesian samping~\citep{heng2024schrodingerbridge}, and filtering~\citep{chopin2023doob}. In particular, computational resources can be saved by approximating the correction term using a neural ODE~\citep{uehara2024reward}. 
\revisedEnd
% \end{itemize}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/MoG_summary_0305.png}
    %\caption{\revisedStart
    %\textbf{Upper Left and Upper Center.} The loss during optimization for Gaussian Mixture Model in Diffusion-DPO with/without regularization (left) and ours (center).
    %    ``True Objective'': the true DPO loss~\citep{rafailov2023DPO} whose target point was $\mu_w = [2.5, 0]$.
    %    %``Regularized Objective'': ``Objective" + $\beta \KL(q\|\pref)$,
    %    ``Upperbound'': An upperbound of ``Objective'' optimized in Diffusion-DPO~\citep{Wallace2024DiffusionDPO}.
    %    %``Regularized Upperbound'': ``Upperbound'' + $\beta \KL(q\|\pref)$, $\beta=0.04$
    %    \textbf{Upper Right.} Aligned samples by Doob's h-transform. ``iter" represents the iteration of DA. ``reference'' represents the empirical density of $\pref$.
    %    \textbf{Bottom.} The quantative comparison between Diffusion-DPO with/without regularization ($\beta=0.04$ or $0$) and our DDO. In addition to the true DPO loss, ``Metric Loss" is the mean of the Euclidean distance of the particles from $\mu_w$, as one of the benchmarks. The computational efficiency was evaluated during the optimization phase (our phase 1). \revisedEnd}
    \vspace{-2mm}
    \caption{\revisedStart
    \textbf{Left and Middle.} The smoothed loss during optimization for Gaussian Mixture Model in Diffusion-DPO with/without regularization (left) and ours (middle).
        ``True Objective'': the true DPO loss~\citep{rafailov2023DPO} whose target point was $\mu_w = [2.5, 0]$.
        %``Regularized Objective'': ``Objective" + $\beta \KL(q\|\pref)$,
        ``Upperbound'': An approximate upperbound of ``Objective'' optimized by Diffusion-DPO~\citep{Wallace2024DiffusionDPO}.
        %``Regularized Upperbound'': ``Upperbound'' + $\beta \KL(q\|\pref)$, $\beta=0.04$
        \textbf{Right.} Aligned samples by Doob's h-transform. ``Ref.'' represents the empirical density of $\pref$.
        \revisedEnd}
    \vspace{-3mm}
    \label{fig:da-summary}
\end{figure}
\vspace{-2mm}
\begin{table}[ht]
\vspace{-3mm}
    \centering
    \caption{The quantitative comparison between Diffusion-DPO and our method}
\scalebox{0.85}{
    \begin{tabular}{lcccc}
        \toprule
        & Ref. & \begin{tabular}{c}Diffusion-DPO\\(50 iter., w/o reg.)\end{tabular} & \begin{tabular}{c}Diffusion-DPO\\(200 iter., w/ reg.)\end{tabular} & \begin{tabular}{c}\textbf{Ours}\\($\beta'=0.04$)\end{tabular} \\
        \midrule
        True DPO loss             & 0.346 & 0.340 & 0.343 & \textbf{0.328} \\
        (Approx.) Upperbound      & -     & 0.311 & 0.337 & -              \\
        Metric loss               & 3.256 & 3.011 & 3.303 & \textbf{2.022} \\
        Opt. time (s)             & -     & 585   & 2280  & 1320 (Phase I)    \\
        GPU memory (\%)           & -     & 6.54  & 6.54  & 8.75 (Phase I)   \\
        \bottomrule
    \end{tabular}\label{table:comparison}
    }
\end{table}
% 実行時間, gpuの修正 (20250304)
% w/o: 34min * 50/200
% w/ : 38min
%ours: 22min (speed testのため, beta' = 0.04のみ, 念の為やり直した)

%\revisedEnd
\vspace{-1mm}
\textbf{Image Generation Alignment based on a Target Color.}\quad
We aligned the image generation of the basic pre-trained model available Diffusion Models Course (source: \cite{huggingfacecourse}) to simulate specific generations for data augmentation. The pre-trained model generates RGB-colored $32\times 32$ pixel images of butterflies. 
\revisedStart
In this experiment, the model was aligned based on a target color, which is (R,G,B)= (0.9, 0.9, 0.9) while (1,1,1) corresponds to white,
%cabbage butterflies would be more preferred,
using the DPO objective. 
Please refer to Figure~\ref{fig:BT-compare} for an illustration of the preference indicated by DPO.
\revisedEnd
We leveraged 1024 images generated by the pre-trained model to train $f_k$ in each DA iteration. 
We see that the (regularized) objective went down during DA and the generated images reflected the target density more, on the right side of Figure~\ref{fig:BT-and-CT}. Please refer to Figure~\ref{fig:BT-summary} in Appendix for the convergence results.

\textbf{Tilt Correction for Image Generation.}\quad
The goal of this experiment is data augmentation of images facing the same direction, given only a dataset of rotated images. We used 10000 images of Head CT in Medical MNIST~\citep{MedicalMNISTClassification} and augmented it by rotating images up to 40000. Data augmentation of brain images is useful for tumor analysis and anomaly detection~\citep{shen2024MRIdiffusion,fontanella2024BrainDiffusionGenerationAnomaly}. In each DA iteration, 6400 images were generated to train $f_k$.
The results are in Figure~\ref{fig:BT-and-CT}. For the convergence results, please refer to Figure~\ref{fig:CT-summary} in Appendix.
From the results, we see that the (regularized) objective decreased during DA and the generated images became more preferred.
\begin{figure}[h]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.5\linewidth]{figures/BT_and_CT.png}
    \vspace{-2mm}
    \caption{\revisedStart
    \textbf{Left.} Examples of aligned image generation. Our goal was to generate light-colored butterflies. ``iter=2'': ours with $k=2$ DA iterations, ``iter=2'': ours with $k=1$ DA iteration. ``Ref.'': samples from $\pref$. \textbf{Right.} Tilt-corrected Head CT image generation. ``iter=3'': ours with $k=3$ DA iterations, ``iter=1'': ours with $k=1$ DA iteration. ``Ref,'': samples from $\pref$.
    \revisedEnd
    }
    \label{fig:BT-and-CT}
\end{figure}


% \vspace{-7mm}
\vspace{-4mm}
\section{Conclusion}
\vspace{-2mm}

We proved that the distributional optimization can be solved even if $\pref$ and $\qstar$ are mutilmodal and we only have access to the score, not the density.
This setting includes important fine-tuning methodologies for diffusion models: Reinforcement Learning, DPO, and KTO.
Our algorithms are based on the DA algorithm and Doob's h-transform technique and it can solve them more directly than previous works.
They are guaranteed to be useful for general objective functions, the dual variable converges even in a non-convex setting.
While our framework has potential applications in general distributional optimization problems, such as density ratio estimation under covariate shift and climate change tracking, further exploration of these applications is left for future work.
% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\section*{Acknowledgments}
RK was partially supported by JST CREST (JPMJCR2015).
KO is partially supported by JST ACT-X (JP-MJAX23C4).
TS was partially supported by JSPS KAKENHI (24K02905, 20H00576) and JST CREST (JPMJCR2115).
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.
This research is supported by the National Research Foundation, Singapore, Infocomm Media Development Authority under its Trust Tech Funding Initiative, and the Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP-2024-004). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore, Infocomm Media Development Authority, and the Ministry of Digital Development and Information. 

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\clearpage

\section*{Notations}

\input{symb_list.tex}

\newpage
% \clearpage

\revisedStart
\section{Convergence Analysis of Dual Averaging}
\revisedEnd

\input{DA_convergence_proof}

\section{Derivations of functional derivatives\label{section:appendix-functional-derivative}}
\input{derive_func_derivative.tex}

\section{Error Analysis of Diffusion Model}\label{section:appendix-error-diffusion}
\input{Errror_Analysis_Of_DM}


\section{Details of Numerical Experiments}
\label{section:appendix-experiments}
\input{experiment.tex}

\end{document}
