\section{Related work}
{\bf Autoregressive and Diffusion Language Modeling.} LLMs based on autoregressive modeling, like GPT-3 ____, PaLM ____ and their successors, have achieved tremendous successes across a wide range of language tasks. On the other hand, discrete diffusion____ arises as an alternative for language modeling____ recently. A popular version is masked diffusion that iterative transits tokens into a masked state in the forward process. It is closely related to any-order autoregressive models____.

{\bf Variational Bayes Language Modeling.}
In the context of language models, ____ introduced a variational autoencoder for text generation. 
Building on this, ____ proposed the use of von Mises-Fisher distribution in VAEs. ____ presented OPTIMUS, a large-scale pretrained deep latent variable model for natural language. ____ studied VAE for language modeling with learnable prior model. 

{\bf Large Language Models with Explicit Latent Space.} ____ repurpose token-level LLMs to generate latent chains of thought. ____ repurpose the hidden state of Transformers as continuous latent space. They are all post-training methods that demonstrate the advantages of explicit latent learning. Concurrent to our work, ____ train generative models for the latent embedding of a pretrained auto-encoder. 

{\bf Complementary Learning: Fast and Slow.} The dual-rate learning can be connected to the theory of complementary learning systems____, which suggests that the hippocampus supports rapid learning of specific experiences, while the neocortex facilitates slower learning of general knowledge.

{\bf Declarative-Procedural Model in Cognitive Science.} The declarative-procedural model, primarily developed by Ullman ____, offers a cognitive framework for understanding language processing and memory. This model posits two distinct but interacting systems: \textit{Declarative memory:} Responsible for storing and recalling facts, events, and arbitrary associations. In language, it is associated with vocabulary, irregular forms, and idiomatic expressions ____. \textit{Procedural memory:} Involved in learning and executing cognitive and motor skills. In language, it is linked to grammar rules, regular morphology, and syntax ____. In our model, $\rvz$ parallels  declarative or episodic memory, representing explicit facts and events. The decoder generator corresponds to procedural memory, embodying the implicit rules and patterns for language generation and comprehension.

 {\bf Language of Thought (LOT) Hypothesis.}  Proposed by Fodor ____, the LOT hypothesis posits that thinking occurs in a mental language with its own syntax and semantics. This ``mentalese'' is theorized to underlie our ability to learn and use natural languages. Recent work has explored computational implementations of LOT-like structures in cognitive modeling ____ and program induction ____.

{\bf Test-Time Computation.} The field of language modeling has seen growing interest in adaptive computation --- also known as dynamic evaluation --- as a method to enhance test-time performance. ____ pioneered this approach to introduce the Adaptive Computation Time mechanism for recurrent neural networks, enabling dynamic adjustment of per-step computation. The concept evolved with ____, who developed dynamic evaluation to adapt model parameters at test time based on recent context. A recent advancement came from ____, who introduced a non-parametric cache mechanism that efficiently adapts to local context during test time without modifying model parameters.