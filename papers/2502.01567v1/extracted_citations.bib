@inproceedings{Kasai2022DeepSF,
  title={Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation},
  author={Kasai, Jungo and Pappas, Nikolaos and Peng, Hao and Cross, James and Smith, Noah A},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@inproceedings{bowman2016generating,
  title={Generating sentences from a continuous space},
  author={Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  booktitle={Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning},
  pages={10--21},
  year={2016}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@book{fodor1975language,
  title={The Language of Thought},
  author={Fodor, Jerry A},
  year={1975},
  publisher={Harvard University Press}
}

@article{graves2016adaptive,
  title={Adaptive computation time for recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1603.08983},
  year={2016}
}

@article{hao2024training,
  title={Training large language models to reason in a continuous latent space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@article{hoffman2024training,
  title={Training chain-of-thought via latent-variable inference},
  author={Hoffman, Matthew Douglas and Phan, Du and Dohan, David and Douglas, Sholto and Le, Tuan Anh and Parisi, Aaron and Sountsov, Pavel and Sutton, Charles and Vikram, Sharad and A Saurous, Rif},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{hoogeboomautoregressive,
  title={Autoregressive Diffusion Models},
  author={Hoogeboom, Emiel and Gritsenko, Alexey A and Bastings, Jasmijn and Poole, Ben and van den Berg, Rianne and Salimans, Tim},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{hu2023amortizing,
  title={Amortizing intractable inference in large language models},
  author={Hu, Edward J and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{krause2018dynamic,
  title={Dynamic evaluation of neural sequence models},
  author={Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={2766--2775},
  year={2018}
}

@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015}
}

@inproceedings{li2020optimus,
  title={Optimus: Organizing sentences via pre-trained modeling of a latent space},
  author={Li, Chunyuan and Gao, Xiang and Li, Yuan and Li, Xiujun and Peng, Baolin and Zhang, Yizhe and Gao, Jianfeng},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4678--4699},
  year={2020}
}

@inproceedings{lou2024discrete,
  title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  author={Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{mcclelland1995there,
  title={Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory},
  author={McClelland, James L and McNaughton, Bruce L and O'Reilly, Randall C},
  journal={Psychological Review},
  volume={102},
  number={3},
  pages={419},
  year={1995}
}

@inproceedings{pang2021latent,
  title={Latent space energy-based model of symbol-vector coupling for text generation and classification},
  author={Pang, Bo and Wu, Ying Nian},
  booktitle={International Conference on Machine Learning},
  pages={8359--8370},
  year={2021},
  organization={PMLR}
}

@article{piantadosi2011learning,
  title={Bootstrapping in a language of thought: A formal model of numerical concept learning},
  author={Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
  journal={Cognition},
  volume={123},
  number={2},
  pages={199--217},
  year={2011}
}

@article{sahoo2024simple,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin T and Rush, Alexander and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2406.07524},
  year={2024}
}

@article{shi2024simplified,
  title={Simplified and Generalized Masked Diffusion for Discrete Data},
  author={Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K},
  journal={arXiv preprint arXiv:2406.04329},
  year={2024}
}

@article{the2024large,
  title={Large Concept Models: Language Modeling in a Sentence Representation Space},
  author={The, LCM and Barrault, Lo{\"\i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-juss{\`a}, Marta R and others},
  journal={arXiv preprint arXiv:2412.08821},
  year={2024}
}

@article{ullman2001neural,
  title={The neural basis of lexicon and grammar in first and second language: The declarative/procedural model},
  author={Ullman, Michael T},
  journal={Bilingualism: Language and cognition},
  volume={4},
  number={2},
  pages={105--122},
  year={2001},
  publisher={Cambridge University Press}
}

@article{ullman2004contributions,
  title={Contributions of memory circuits to language: The declarative/procedural model},
  author={Ullman, Michael T},
  journal={Cognition},
  volume={92},
  number={1-2},
  pages={231--270},
  year={2004},
  publisher={Elsevier}
}

@inproceedings{uria2014deep,
  title={A deep and tractable density estimator},
  author={Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  booktitle={International Conference on Machine Learning},
  pages={467--475},
  year={2014},
  organization={PMLR}
}

@inproceedings{xu2018spherical,
  title={Spherical latent spaces for stable variational autoencoders},
  author={Xu, Jiacheng and Durrett, Greg},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={4503--4513},
  year={2018}
}

@article{yu2022latent,
  title={Latent diffusion energy-based model for interpretable text modeling},
  author={Yu, Peiyu and Xie, Sirui and Ma, Xiaojian and Jia, Baoxiong and Pang, Bo and Gao, Ruiqi and Zhu, Yixin and Zhu, Song-Chun and Wu, Ying Nian},
  journal={arXiv preprint arXiv:2206.05895},
  year={2022}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

