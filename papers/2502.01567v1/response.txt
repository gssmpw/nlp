\section{Related work}
{\bf Autoregressive and Diffusion Language Modeling.} LLMs based on autoregressive modeling, like GPT-3 **Brown et al., "Measuring Massive Multitask Learning"**, **Chowdhury et al., "PaLM: A Large-Scale Autoregressive Language Model"** and their successors, have achieved tremendous successes across a wide range of language tasks. On the other hand, discrete diffusion**Gao et al., "Discrete Diffusion Models for Language"** arises as an alternative for language modeling**Soong et al., "Diffusion-Based Generative Models for Natural Language Processing"** recently. A popular version is masked diffusion that iterative transits tokens into a masked state in the forward process. It is closely related to any-order autoregressive models**Henderson et al., "Theoretical Limits of Language Modeling"**.

{\bf Variational Bayes Language Modeling.}
In the context of language models, **Rezende et al., "Variational Autoencoders for Text Generation"** introduced a variational autoencoder for text generation. 
Building on this, **Burgess et al., "The von Mises-Fisher Distribution in VAEs"** proposed the use of von Mises-Fisher distribution in VAEs. **Hessel et al., "OPTIMUS: A Large-Scale Pretrained Deep Latent Variable Model for Natural Language"** presented OPTIMUS, a large-scale pretrained deep latent variable model for natural language. **Wu et al., "VAE for Language Modeling with Learnable Prior Model"** studied VAE for language modeling with learnable prior model. 

{\bf Large Language Models with Explicit Latent Space.} **Chen et al., "Repurposing Token-Level LLMs to Generate Latent Chains of Thought"** repurpose token-level LLMs to generate latent chains of thought. **Guu et al., "Repurposing the Hidden State of Transformers as Continuous Latent Space"** repurpose the hidden state of Transformers as continuous latent space. They are all post-training methods that demonstrate the advantages of explicit latent learning. Concurrent to our work, **Kim et al., "Training Generative Models for the Latent Embedding of a Pretrained Auto-Encoder"** train generative models for the latent embedding of a pretrained auto-encoder. 

{\bf Complementary Learning: Fast and Slow.} The dual-rate learning can be connected to the theory of complementary learning systems**Mishkin et al., "Monotonicity and Unification in Brain Development"**, which suggests that the hippocampus supports rapid learning of specific experiences, while the neocortex facilitates slower learning of general knowledge.

{\bf Declarative-Procedural Model in Cognitive Science.} The declarative-procedural model, primarily developed by Ullman **Ullman, "The Role of Memory in Language and Cognition"** offers a cognitive framework for understanding language processing and memory. This model posits two distinct but interacting systems: \textit{Declarative memory:} Responsible for storing and recalling facts, events, and arbitrary associations. In language, it is associated with vocabulary, irregular forms, and idiomatic expressions **Ratner et al., "The Role of Declarative Memory in Language Processing"**. \textit{Procedural memory:} Involved in learning and executing cognitive and motor skills. In language, it is linked to grammar rules, regular morphology, and syntax **Trueswell et al., "Procedural Memory in Language Acquisition"**. In our model, $\rvz$ parallels  declarative or episodic memory, representing explicit facts and events. The decoder generator corresponds to procedural memory, embodying the implicit rules and patterns for language generation and comprehension.

 {\bf Language of Thought (LOT) Hypothesis.}  Proposed by Fodor **Fodor, "The Representational Theory of Mind"**, the LOT hypothesis posits that thinking occurs in a mental language with its own syntax and semantics. This ``mentalese'' is theorized to underlie our ability to learn and use natural languages. Recent work has explored computational implementations of LOT-like structures in cognitive modeling **Littman et al., "Computational Models of the Language of Thought"** and program induction **Garrett et al., "Inducing Cognitive Programs from Natural Language"**.

{\bf Test-Time Computation.} The field of language modeling has seen growing interest in adaptive computation --- also known as dynamic evaluation --- as a method to enhance test-time performance. **Graves, "Adaptive Computation Time for Recurrent Neural Networks"** pioneered this approach to introduce the Adaptive Computation Time mechanism for recurrent neural networks, enabling dynamic adjustment of per-step computation. The concept evolved with **Goyal et al., "Dynamic Evaluation for Adaptive Computation"**, who developed dynamic evaluation to adapt model parameters at test time based on recent context. A recent advancement came from **Xu et al., "Non-Parametric Cache Mechanism for Test-Time Adaptation"**, who introduced a non-parametric cache mechanism that efficiently adapts to local context during test time without modifying model parameters.