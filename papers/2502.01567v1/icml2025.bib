@article{hoffman2013stochastic,
  title={Stochastic variational inference},
  author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal={Journal of Machine Learning Research},
  year={2013}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{chelba2013one,
  title={One billion word benchmark for measuring progress in statistical language modeling},
  author={Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
  journal={arXiv preprint arXiv:1312.3005},
  year={2013}
}

@article{cohan2018discourse,
  title={A discourse-aware attention model for abstractive summarization of long documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  journal={arXiv preprint arXiv:1804.05685},
  year={2018}
}

@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}

@article{freitag2017beam,
  title={Beam search strategies for neural machine translation},
  author={Freitag, Markus and Al-Onaizan, Yaser},
  journal={arXiv preprint arXiv:1702.01806},
  year={2017}
}



@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@inproceedings{pang2021-generative,
    title = "Generative Text Modeling through Short Run Inference",
    author = "Pang, Bo  and
      Nijkamp, Erik  and
      Han, Tian  and
      Wu, Ying Nian",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.98/",
    doi = "10.18653/v1/2021.eacl-main.98",
    pages = "1156--1165",
    abstract = "Latent variable models for text, when trained successfully, accurately model the data distribution and capture global semantic and syntactic features of sentences. The prominent approach to train such models is variational autoencoders (VAE). It is nevertheless challenging to train and often results in a trivial local optimum where the latent variable is ignored and its posterior collapses into the prior, an issue known as posterior collapse. Various techniques have been proposed to mitigate this issue. Most of them focus on improving the inference model to yield latent codes of higher quality. The present work proposes a short run dynamics for inference. It is initialized from the prior distribution of the latent variable and then runs a small number (e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The major advantage of our method is that it does not require a separate inference model or assume simple geometry of the posterior distribution, thus rendering an automatic, natural and flexible inference engine. We show that the models trained with short run dynamics more accurately model the data, compared to strong language model and VAE baselines, and exhibit no sign of posterior collapse. Analyses of the latent space show that interpolation in the latent space is able to generate coherent sentences with smooth transition and demonstrate improved classification over strong baselines with latent features from unsupervised pretraining. These results together expose a well-structured latent space of our generative model."
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{yu2022latent,
  title={Latent diffusion energy-based model for interpretable text modeling},
  author={Yu, Peiyu and Xie, Sirui and Ma, Xiaojian and Jia, Baoxiong and Pang, Bo and Gao, Ruiqi and Zhu, Yixin and Zhu, Song-Chun and Wu, Ying Nian},
  journal={arXiv preprint arXiv:2206.05895},
  year={2022}
}

@article{hu2022lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2022}
}

@article{xu2023diverse,
  title={Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference},
  author={Xu, Yan and Kong, Deqian and Xu, Dehong and Ji, Ziwei and Pang, Bo and Fung, Pascale and Wu, Ying Nian},
  journal={arXiv preprint arXiv:2306.01153},
  note={\url{https://arxiv.org/pdf/2306.01153}},
  year={2023}
}

@inproceedings{pang2021latent,
  title={Latent space energy-based model of symbol-vector coupling for text generation and classification},
  author={Pang, Bo and Wu, Ying Nian},
  booktitle={International Conference on Machine Learning},
  pages={8359--8370},
  year={2021},
  organization={PMLR}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@book{murphy2012machine,
  title={Machine Learning: A Probabilistic Perspective},
  author={Murphy, Kevin P.},
  year={2012},
  publisher={MIT Press},
  address={Cambridge, MA},
  series={Adaptive Computation and Machine Learning series}
}

@article{jordan1999introduction,
  title={An introduction to variational methods for graphical models},
  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}


@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and Brain Sciences},
  volume={40},
  pages={e253},
  year={2017}
}

@article{mcclelland1995there,
  title={Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory},
  author={McClelland, James L and McNaughton, Bruce L and O'Reilly, Randall C},
  journal={Psychological Review},
  volume={102},
  number={3},
  pages={419},
  year={1995}
}



@article{ullman2004contributions,
  title={Contributions of memory circuits to language: The declarative/procedural model},
  author={Ullman, Michael T},
  journal={Cognition},
  volume={92},
  number={1-2},
  pages={231--270},
  year={2004},
  publisher={Elsevier}
}

@article{ullman2001neural,
  title={The neural basis of lexicon and grammar in first and second language: The declarative/procedural model},
  author={Ullman, Michael T},
  journal={Bilingualism: Language and cognition},
  volume={4},
  number={2},
  pages={105--122},
  year={2001},
  publisher={Cambridge University Press}
}

@article{ullman2005cognitive,
  title={A cognitive neuroscience perspective on second language acquisition: The declarative/procedural model},
  author={Ullman, Michael T},
  journal={Mind and context in adult second language acquisition},
  volume={141178},
  year={2005},
  publisher={Georgetown University Press Washington, DC}
}

@article{paradis2009declarative,
  title={Declarative and procedural determinants of second languages},
  author={Paradis, Michel},
  volume={40},
  year={2009},
  publisher={John Benjamins Publishing}
}

@article{ullman2020role,
  title={The role of memory systems in disorders of language},
  author={Ullman, Michael T and Pullman, Mariel Y},
  journal={Nature Reviews Neurology},
  volume={16},
  number={3},
  pages={167--181},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{morgan2014declarative,
  title={The declarative/procedural model of learning and memory as a framework for understanding recovery in aphasia},
  author={Morgan-Short, Kara and Ullman, Michael T},
  journal={Aphasiology},
  volume={28},
  number={8-9},
  pages={919--947},
  year={2014},
  publisher={Taylor \& Francis}
}



@article{blei2017variational,
  title={Variational inference: A review for statisticians},
  author={Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={112},
  number={518},
  pages={859--877},
  year={2017}
}

@article{kumaran2016learning,
  title={What learning systems do intelligent agents need? Complementary learning systems theory updated},
  author={Kumaran, Dharshan and Hassabis, Demis and McClelland, James L},
  journal={Trends in Cognitive Sciences},
  volume={20},
  number={7},
  pages={512--534},
  year={2016}
}

@misc{gokaslan2019openwebtext,
  title={OpenWebText corpus},
  author={Gokaslan, Aaron and Cohen, Vanya},
  year={2019},
  howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}
}

@inproceedings{hoogeboom2021argmax,
  title={Argmax flows and multinomial diffusion: Learning categorical distributions},
  author={Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14},
  year={2021}
}

@inproceedings{bowman2016generating,
  title={Generating sentences from a continuous space},
  author={Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  booktitle={Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning},
  pages={10--21},
  year={2016}
}

@inproceedings{xu2018spherical,
  title={Spherical latent spaces for stable variational autoencoders},
  author={Xu, Jiacheng and Durrett, Greg},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={4503--4513},
  year={2018}
}

@inproceedings{li2020optimus,
  title={Optimus: Organizing sentences via pre-trained modeling of a latent space},
  author={Li, Chunyuan and Gao, Xiang and Li, Yuan and Li, Xiujun and Peng, Baolin and Zhang, Yizhe and Gao, Jianfeng},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4678--4699},
  year={2020}
}

@inproceedings{ba2016using,
  title={Using fast weights to attend to the recent past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  booktitle={Advances in Neural Information Processing Systems},
  volume={29},
  pages={4331--4339},
  year={2016}
}

@inproceedings{munkhdalai2017meta,
  title={Meta networks},
  author={Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  volume={70},
  pages={2554--2563},
  year={2017}
}

@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2016}
}

@article{graves2016adaptive,
  title={Adaptive computation time for recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1603.08983},
  year={2016}
}

@inproceedings{krause2018dynamic,
  title={Dynamic evaluation of neural sequence models},
  author={Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  booktitle={Proceedings of the 35th International Conference on Machine Learning},
  pages={2766--2775},
  year={2018}
}

@inproceedings{Kasai2022DeepSF,
  title={Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation},
  author={Kasai, Jungo and Pappas, Nikolaos and Peng, Hao and Cross, James and Smith, Noah A},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{lucas2019don,
  title={Don't blame the elbo! a linear vae perspective on posterior collapse},
  author={Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@book{fodor1975language,
  title={The Language of Thought},
  author={Fodor, Jerry A},
  year={1975},
  publisher={Harvard University Press}
}

@book{fodor2008lot,
  title={LOT 2: The Language of Thought Revisited},
  author={Fodor, Jerry A},
  year={2008},
  publisher={Oxford University Press}
}

@article{piantadosi2011learning,
  title={Bootstrapping in a language of thought: A formal model of numerical concept learning},
  author={Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
  journal={Cognition},
  volume={123},
  number={2},
  pages={199--217},
  year={2011}
}

@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015}
}

@inproceedings{uria2014deep,
  title={A deep and tractable density estimator},
  author={Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  booktitle={International Conference on Machine Learning},
  pages={467--475},
  year={2014},
  organization={PMLR}
}

@book{marcus2001algebraic,
  title={The Algebraic Mind: Integrating Connectionism and Cognitive Science},
  author={Marcus, Gary F},
  year={2001},
  publisher={MIT Press}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@article{sahoo2024simple,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin T and Rush, Alexander and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2406.07524},
  year={2024}
}

@article{shi2024simplified,
  title={Simplified and Generalized Masked Diffusion for Discrete Data},
  author={Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K},
  journal={arXiv preprint arXiv:2406.04329},
  year={2024}
}

@inproceedings{lou2024discrete,
  title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  author={Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{campbell2022continuous,
  title={A continuous time framework for discrete denoising models},
  author={Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28266--28279},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{the2024large,
  title={Large Concept Models: Language Modeling in a Sentence Representation Space},
  author={The, LCM and Barrault, Lo{\"\i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-juss{\`a}, Marta R and others},
  journal={arXiv preprint arXiv:2412.08821},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{hu2023amortizing,
  title={Amortizing intractable inference in large language models},
  author={Hu, Edward J and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{hoffman2024training,
  title={Training chain-of-thought via latent-variable inference},
  author={Hoffman, Matthew Douglas and Phan, Du and Dohan, David and Douglas, Sholto and Le, Tuan Anh and Parisi, Aaron and Sountsov, Pavel and Sutton, Charles and Vikram, Sharad and A Saurous, Rif},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{yoon2018bayesian,
  title={Bayesian model-agnostic meta-learning},
  author={Yoon, Jaesik and Kim, Taesup and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{dieleman2022continuous,
  title={Continuous diffusion for categorical data},
  author={Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and others},
  journal={arXiv preprint arXiv:2211.15089},
  year={2022}
}

@article{zheng2024masked,
  title={Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling},
  author={Zheng, Kaiwen and Chen, Yongxin and Mao, Hanzi and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2409.02908},
  year={2024}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{li2022composing,
  title={Composing ensembles of pre-trained models via iterative consensus},
  author={Li, Shuang and Du, Yilun and Tenenbaum, Joshua B and Torralba, Antonio and Mordatch, Igor},
  journal={arXiv preprint arXiv:2210.11522},
  year={2022}
}

@article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}

@article{han2022ssd,
  title={Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control},
  author={Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2210.17432},
  year={2022}
}

@article{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{hao2024training,
  title={Training large language models to reason in a continuous latent space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{hsu2024liger,
  title={Liger Kernel: Efficient Triton Kernels for LLM Training},
  author={Hsu, Pin-Lun and Dai, Yun and Kothapalli, Vignesh and Song, Qingquan and Tang, Shao and Zhu, Siyu and Shimizu, Steven and Sahni, Shivam and Ning, Haowen and Chen, Yanning},
  journal={arXiv preprint arXiv:2410.10989},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{hoogeboomautoregressive,
  title={Autoregressive Diffusion Models},
  author={Hoogeboom, Emiel and Gritsenko, Alexey A and Bastings, Jasmijn and Poole, Ben and van den Berg, Rianne and Salimans, Tim},
  booktitle={International Conference on Learning Representations},
  year={2022}
}