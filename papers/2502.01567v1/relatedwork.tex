\section{Related work}
{\bf Autoregressive and Diffusion Language Modeling.} LLMs based on autoregressive modeling, like GPT-3 \citep{brown2020language}, PaLM \citep{chowdhery2022palm} and their successors, have achieved tremendous successes across a wide range of language tasks. On the other hand, discrete diffusion~\citep{austin2021structured} arises as an alternative for language modeling~\citep{lou2024discrete,shi2024simplified,sahoo2024simple} recently. A popular version is masked diffusion that iterative transits tokens into a masked state in the forward process. It is closely related to any-order autoregressive models~\citep{uria2014deep, hoogeboomautoregressive}.

{\bf Variational Bayes Language Modeling.}
In the context of language models, \citet{bowman2016generating} introduced a variational autoencoder for text generation. 
Building on this, \citet{xu2018spherical} proposed the use of von Mises-Fisher distribution in VAEs. \citet{li2020optimus} presented OPTIMUS, a large-scale pretrained deep latent variable model for natural language. \citet{pang2021latent,yu2022latent} studied VAE for language modeling with learnable prior model. 

{\bf Large Language Models with Explicit Latent Space.} \citet{zelikman2022star,hu2023amortizing,hoffman2024training} repurpose token-level LLMs to generate latent chains of thought. \citet{hao2024training} repurpose the hidden state of Transformers as continuous latent space. They are all post-training methods that demonstrate the advantages of explicit latent learning. Concurrent to our work, \citet{the2024large} train generative models for the latent embedding of a pretrained auto-encoder. 

{\bf Complementary Learning: Fast and Slow.} The dual-rate learning can be connected to the theory of complementary learning systems~\citep{mcclelland1995there}, which suggests that the hippocampus supports rapid learning of specific experiences, while the neocortex facilitates slower learning of general knowledge.

{\bf Declarative-Procedural Model in Cognitive Science.} The declarative-procedural model, primarily developed by Ullman \cite{ullman2004contributions}, offers a cognitive framework for understanding language processing and memory. This model posits two distinct but interacting systems: \textit{Declarative memory:} Responsible for storing and recalling facts, events, and arbitrary associations. In language, it is associated with vocabulary, irregular forms, and idiomatic expressions \cite{ullman2001neural}. \textit{Procedural memory:} Involved in learning and executing cognitive and motor skills. In language, it is linked to grammar rules, regular morphology, and syntax \cite{ullman2004contributions}. In our model, $\rvz$ parallels  declarative or episodic memory, representing explicit facts and events. The decoder generator corresponds to procedural memory, embodying the implicit rules and patterns for language generation and comprehension.

 {\bf Language of Thought (LOT) Hypothesis.}  Proposed by Fodor \cite{fodor1975language}, the LOT hypothesis posits that thinking occurs in a mental language with its own syntax and semantics. This ``mentalese'' is theorized to underlie our ability to learn and use natural languages. Recent work has explored computational implementations of LOT-like structures in cognitive modeling \cite{piantadosi2011learning} and program induction \cite{lake2015human}.

{\bf Test-Time Computation.} The field of language modeling has seen growing interest in adaptive computation --- also known as dynamic evaluation --- as a method to enhance test-time performance. \citet{graves2016adaptive} pioneered this approach to introduce the Adaptive Computation Time mechanism for recurrent neural networks, enabling dynamic adjustment of per-step computation. The concept evolved with \citet{krause2018dynamic}, who developed dynamic evaluation to adapt model parameters at test time based on recent context. A recent advancement came from \citet{Kasai2022DeepSF}, who introduced a non-parametric cache mechanism that efficiently adapts to local context during test time without modifying model parameters.