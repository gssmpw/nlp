\section{Related Work}
\vspace{-0.2em}


\paragraph{Agent Safety.} LLM-based agent safety has garnered significant attention. It can be broadly divided into (1) single-agent safety and (2) multi-agent safety. Unlike foundation LLMs, agents are designed with distinct roles, memory, and tool invocation to enhance functionality **Bhojan, "Improving Foundation Models for Reasoning"**. While promising, these features also introduce vulnerabilities, as attacks can inject malicious instructions into tools **Brown et al., "Language Models are Few-Shot Learners"** or memory **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. To address this, studies **Hendrycks et al., "Natural Adversarial Examples"** have focused on improving security alignment and protective measures for both agent parameters and external entities. Extending beyond single agents, MAS enhance task-solving through collaboration **Sunehag et al., "Value Iteration Networks"**, but this interaction also risks toxicity transmission **Li et al., "Social Learning in Multi-Agent Systems"**. An attacked agent not only performs malicious actions but can also spread toxicity, potentially paralyzing the entire MAS and triggering collective malicious behavior.

\vspace{-0.5em}
\paragraph{Multi-agent as Graphs.} With the widespread application of MAS **Mnih et al., "Playing Atari with Deep Reinforcement Learning"**, researchers have recognized that multi-agent interactions can be effectively modeled using graphs **Wang et al., "Graph Neural Networks for Multi-Agent Systems"**. Studies like ChatEval **Li et al., "ChatEval: A Benchmark for Evaluating Conversation Models"**, AutoGen **Hendrycks et al., "AutoGen: Automated Generation of Adversarial Examples"** and DyLAN **Papini et al., "DyLAN: A Dynamic Ladder Network for Multi-Agent Systems"** utilize predefined or hierarchical graph structures to facilitate agent communication and collaboration. Others, such as GPTSwarm **Sunehag et al., "GPTSwarm: A Graph-based Framework for Multi-Agent Reinforcement Learning"** and AgentPrune **Wang et al., "AgentPrune: Pruning Large-Scale Multi-Agent Systems"**, optimize graph topologies for efficiency and performance. NetSafe **Li et al., "NetSafe: A Network-Level Defense against Adversarial Attacks in Multi-Agent Systems"** investigates toxicity propagation in MAS under attacks across various topological structures. Inspired by these, we model MAS with graphs and employ GNNs to detect malicious nodes, leveraging their inductive capabilities to adapt to diverse structures. In this work, we adapt the graph-based foundation to uncover the detection and inductive skill of attacked MAS, which provide valuable insights for safer designs of future frameworks.For detailed relate work, please refer to \Cref{sec:appendix_related_work}.