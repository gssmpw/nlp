\section{Related Work}
Text summarization in NLP involves creating concise summaries from lengthy documents. Early, methods focused on extractive techniques, selecting key sentences or phrases \cite{el2021automatic}. 
%\textcolor{blue}{TR: However, abstractive techniques first read and comprehend the entire input text, then generate a summary in a manner similar to how a human would.}
%Text summarization is a crucial task in NLP, aimed at creating concise and coherent summaries of lengthy documents. Early efforts primarily focused on extractive methods, selecting key sentences or phrases, as demonstrated by Luhn et al.\cite{Luhn} and Edmundson et al.\cite{Edmundson}\cite{el2021automatic}. %Subsequent advancements included statistical and graph-based algorithms like LexRank \cite{Lexrank} and TextRank \cite{Textrank}. With the rise of machine learning, supervised methods using models like SVM and MaxEnt further improved extractive summarization. 

Initially, text summarization research focused on single-document summarization, where key information is extracted from a single document to generate a concise summary. Several recent surveys, such as \cite{koh2022empirical,luo2024comprehensive,zhang2024systematic} provide a comprehensive overview of summarization datasets and techniques, spanning from statistical methods to deep learning models. 
%Romain P. et al. \cite{Romain} proposed a neural network-based model with intra-attention, using reinforcement learning and supervised algorithms to generate readable abstractive summaries on the CNN/DM and New York Times datasets.
%%Yang L. et al. \cite{Yang} developed an iterative extractive summarization model using a tree structure, which was also evaluated on the CNN/DM \cite{nallapati2016abstractive} and New York Times datasets. Sanchit et al. \cite{Sanchit} proposed an extractive model that uses sentence embeddings and K-means clustering to select the most informative sentence from each cluster for summary generation. Naveen S. et al. \cite{Naveen} introduced a binary optimization-based model using MOBDE, which evaluates summary sentences based on statistical features, achieving competitive results on the DUC2001 and DUC2002 datasets. %These approaches highlight various methodologies and \textcolor{red}{dataset limitations} in single-document summarization.

%Multi-document summarization produces a unified summary by analyzing multiple documents on a specific topic. This process either selects sentences directly (extractive) or rephrases them (abstractive) before combining them. Jian-Ping M. et al. \cite{Jian} introduced an extractive framework using `Exemplar' features for relevance and coverage and `Position' features for ordering, showing modest ROUGE score improvements on DUC datasets.

%Rasim M. A. et al. \cite{Rasim} proposed an evolutionary algorithm for summarization with low redundancy but high computational overhead, tested on DUC2002 and DUC2004 datasets. Giang T. et al. \cite{Giang} introduced a timeline-based summarization technique for events occurring over different times, which was effective on timeline data but less suitable for diverse datasets like WikiHow due to the complexity and variability of topics.

While the early methods struggled with rephrasing and merging content, the introduction of sequence-to-sequence models like the pointer-generator model, augmented with coverage and attention mechanisms \cite{See2017GetTT} marked a significant advancement, although challenges like repeated content and factual inaccuracies or hallucinations persisted. 
%The pointer-generator model, augmented with a coverage mechanism \cite{See2017GetTT}, addresses the issue of out-of-vocabulary (OOV) words and helps minimize the repetition of phrases, 
Our earlier works \cite{rehman2021automatic,rehman-etal-2022-named,rehman2023research,10172215,rehman2022analysis, rehman2022abstractive,rehman2024analysis} explored the performance of various pre-trained models in open-domain and scholarly domains. Pre-trained models such as T5 \cite{JMLR:v21:20-074}, BART \cite{lewis-etal-2020-bart}, PEGASUS \cite{zhang2020pegasus} and large language models (LLMs) like the GPT-family of models \cite{brown2020language} established new state-of-the-art scores in summarization. This paper chooses models with parameters between a few hundred million to less than 10 billion and assess their performance on a diversity of datasets.
%\cite{goyal2022news, pu2023summarization}. Studies like those by Liu et al. \cite{liu2023learning} suggest that although smaller models perform well in automated evaluations, they tend to lag behind in human assessments, highlighting the complexities of model evaluation. %Zhang et al. \cite{zhang2023summit} introduced SummIt, a novel method for summary improvement, which leverages LLMs to iteratively refine summaries using self-evaluation and feedback, enhancing coherence and informativeness.