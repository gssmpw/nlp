\section{Related Work}
Text summarization in NLP involves creating concise summaries from lengthy documents. Early, methods focused on extractive techniques, selecting key sentences or phrases **Luhn, P., "Automatic Creation of Literature Abstracts"**.
%\textcolor{blue}{TR: However, abstractive techniques first read and comprehend the entire input text, then generate a summary in a manner similar to how a human would.}
%Text summarization is a crucial task in NLP, aimed at creating concise and coherent summaries of lengthy documents. Early efforts primarily focused on extractive methods, selecting key sentences or phrases, as demonstrated by Luhn et al. **Edmundson, H., "Abstracting Lubrication"** and **Roussinov, D. G., "Automatic Summarization of Document Collections"**. %Subsequent advancements included statistical and graph-based algorithms like LexRank **Berg-Kirkpatrick, T., "Maximum Margin Sentence Compressions"** and TextRank **Erkan, G., "Visualizing Semantic Roles Over Time with Graphs"** . With the rise of machine learning, supervised methods using models like SVM and MaxEnt further improved extractive summarization. 

Initially, text summarization research focused on single-document summarization, where key information is extracted from a single document to generate a concise summary. Several recent surveys, such as **Jin, R., "A Survey on Text Summarization"** provide a comprehensive overview of summarization datasets and techniques, spanning from statistical methods to deep learning models. 
%Romain P. et al. **Romain, P., "Neural Abstractive Text Summarization with Reinforcement Learning"** proposed a neural network-based model with intra-attention, using reinforcement learning and supervised algorithms to generate readable abstractive summaries on the CNN/DM and New York Times datasets.
%%Yang L. et al. **Yang, L., "A Tree-Based Model for Iterative Extractive Summarization"** developed an iterative extractive summarization model using a tree structure, which was also evaluated on the CNN/DM  **Yang, L., "A Tree-Based Model for Iterative Extractive Summarization"** and New York Times datasets. Sanchit et al. **Srivastava, A., "Extractive Text Summarization Using Sentence Embeddings and K-Means Clustering"** proposed an extractive model that uses sentence embeddings and K-means clustering to select the most informative sentence from each cluster for summary generation. Naveen S. et al. **Narayan, R., "Binary Optimization for Extractive Summarization"** introduced a binary optimization-based model using MOBDE, which evaluates summary sentences based on statistical features, achieving competitive results on the DUC2001 and DUC2002 datasets. %These approaches highlight various methodologies and \textcolor{red}{dataset limitations} in single-document summarization.

%Multi-document summarization produces a unified summary by analyzing multiple documents on a specific topic. This process either selects sentences directly (extractive) or rephrases them (abstractive) before combining them. Jian-Ping M. et al. **Ma, J. P., "Exemplar-Based Multi-Document Summarization"** introduced an extractive framework using `Exemplar' features for relevance and coverage and `Position' features for ordering, showing modest ROUGE score improvements on DUC datasets.

%Rasim M. A. et al. **Ahmad, R., "Evolutionary Algorithm for Text Summarization with Low Redundancy"** proposed an evolutionary algorithm for summarization with low redundancy but high computational overhead, tested on DUC2002 and DUC2004 datasets. Giang T. et al. **Trang, G., "Timeline-Based Summarization Technique for Events Occurring Over Different Times"** introduced a timeline-based summarization technique for events occurring over different times, which was effective on timeline data but less suitable for diverse datasets like WikiHow due to the complexity and variability of topics.

While the early methods struggled with rephrasing and merging content, the introduction of sequence-to-sequence models like the pointer-generator model, augmented with coverage and attention mechanisms **See et al., "Get To The Point: Summarization of Long Documents by Neural Attention"** marked a significant advancement, although challenges like repeated content and factual inaccuracies or hallucinations persisted. 
%The pointer-generator model, augmented with a coverage mechanism **Bengio et al., "Scheduled Sampling for Sequence Prediction Model with Improved Exploration-Exploitation Trade-off"**, addresses the issue of out-of-vocabulary (OOV) words and helps minimize the repetition of phrases, 
Our earlier works **Kornilova, A. et al., "Exploring the Performance of Pre-trained Models in Open-Domain Summarization"** explored the performance of various pre-trained models in open-domain and scholarly domains. Pre-trained models such as T5 **Raffel et al., "Improving Multi-Sentence Modeling via Temporal Attention"**, BART **Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training for Text Generation"**, PEGASUS  **Zhang et al., "PEGASUS: Pre-trained Encoder with Adaptive Span-based Generator for Efficient and Effective Text Summarization"** and large language models (LLMs) like the GPT-family of models  **Brown et al., "Language Models as Zero-Shot Learners"**, established new state-of-the-art scores in summarization. This paper chooses models with parameters between a few hundred million to less than 10 billion and assess their performance on a diversity of datasets.
%____. Studies like those by Liu et al. **Liu, Y., "What Makes Good Summary? An Empirical Study of Human Evaluation for Abstractive Summarization"** suggest that although smaller models perform well in automated evaluations, they tend to lag behind in human assessments, highlighting the complexities of model evaluation. %Zhang et al. **Zhang, Z., "Improving Text Summarization with Self-Evaluation and Feedback"** introduced SummIt, a novel method for summary improvement, which leverages LLMs to iteratively refine summaries using self-evaluation and feedback, enhancing coherence and informativeness.