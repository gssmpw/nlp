\section{Related Work}
Text summarization in NLP involves creating concise summaries from lengthy documents. Early, methods focused on extractive techniques, selecting key sentences or phrases ____. 
%\textcolor{blue}{TR: However, abstractive techniques first read and comprehend the entire input text, then generate a summary in a manner similar to how a human would.}
%Text summarization is a crucial task in NLP, aimed at creating concise and coherent summaries of lengthy documents. Early efforts primarily focused on extractive methods, selecting key sentences or phrases, as demonstrated by Luhn et al.____ and Edmundson et al.________. %Subsequent advancements included statistical and graph-based algorithms like LexRank ____ and TextRank ____. With the rise of machine learning, supervised methods using models like SVM and MaxEnt further improved extractive summarization. 

Initially, text summarization research focused on single-document summarization, where key information is extracted from a single document to generate a concise summary. Several recent surveys, such as ____ provide a comprehensive overview of summarization datasets and techniques, spanning from statistical methods to deep learning models. 
%Romain P. et al. ____ proposed a neural network-based model with intra-attention, using reinforcement learning and supervised algorithms to generate readable abstractive summaries on the CNN/DM and New York Times datasets.
%%Yang L. et al. ____ developed an iterative extractive summarization model using a tree structure, which was also evaluated on the CNN/DM ____ and New York Times datasets. Sanchit et al. ____ proposed an extractive model that uses sentence embeddings and K-means clustering to select the most informative sentence from each cluster for summary generation. Naveen S. et al. ____ introduced a binary optimization-based model using MOBDE, which evaluates summary sentences based on statistical features, achieving competitive results on the DUC2001 and DUC2002 datasets. %These approaches highlight various methodologies and \textcolor{red}{dataset limitations} in single-document summarization.

%Multi-document summarization produces a unified summary by analyzing multiple documents on a specific topic. This process either selects sentences directly (extractive) or rephrases them (abstractive) before combining them. Jian-Ping M. et al. ____ introduced an extractive framework using `Exemplar' features for relevance and coverage and `Position' features for ordering, showing modest ROUGE score improvements on DUC datasets.

%Rasim M. A. et al. ____ proposed an evolutionary algorithm for summarization with low redundancy but high computational overhead, tested on DUC2002 and DUC2004 datasets. Giang T. et al. ____ introduced a timeline-based summarization technique for events occurring over different times, which was effective on timeline data but less suitable for diverse datasets like WikiHow due to the complexity and variability of topics.

While the early methods struggled with rephrasing and merging content, the introduction of sequence-to-sequence models like the pointer-generator model, augmented with coverage and attention mechanisms ____ marked a significant advancement, although challenges like repeated content and factual inaccuracies or hallucinations persisted. 
%The pointer-generator model, augmented with a coverage mechanism ____, addresses the issue of out-of-vocabulary (OOV) words and helps minimize the repetition of phrases, 
Our earlier works ____ explored the performance of various pre-trained models in open-domain and scholarly domains. Pre-trained models such as T5 ____, BART ____, PEGASUS ____ and large language models (LLMs) like the GPT-family of models ____ established new state-of-the-art scores in summarization. This paper chooses models with parameters between a few hundred million to less than 10 billion and assess their performance on a diversity of datasets.
%____. Studies like those by Liu et al. ____ suggest that although smaller models perform well in automated evaluations, they tend to lag behind in human assessments, highlighting the complexities of model evaluation. %Zhang et al. ____ introduced SummIt, a novel method for summary improvement, which leverages LLMs to iteratively refine summaries using self-evaluation and feedback, enhancing coherence and informativeness.