\section{Related Work}
This section provides an overview of the primary methods for assessing question difficulty in reading comprehension and examines the application of LLMs in this domain. 

\subsection{Psychometric Assessment with Item Response Theory}

Prior research on question difficulty classification in the domain of reading comprehension relies on traditional psychometric approaches, particularly Item Response Theory (IRT) analysis. IRT provides a mathematical framework for modeling how the latent ability of a test taker and the characteristics of an item (e.g. difficulty, discrimination) interact to produce a response ____. By establishing a statistical relationship between question difficulty and learner ability, IRT facilitates the selection of items that are appropriately challenging for each learner ____. Large-scale assessments in reading comprehension, such as the National Assessment of Educational Progress (NAEP), the Programme for International Student Assessment (PISA), and the Progress in International Reading Literacy Study (PIRLS), depend heavily on IRT for calibrating item difficulty and reporting student performance. For instance, the U.S. NAEP employs 2-PL and 3-PL IRT models to estimate both discrimination and difficulty parameters for its reading comprehension items ____. The PISA reading literacy assessment similarly utilizes IRT scaling to summarize student ability and item difficulty across countries ____. The sample-invariance of IRT-derived item difficulties allows test developers to assemble equivalent test forms and maintain a calibrated item bank. Overall, IRT has provided a robust framework for calibrating reading comprehension items, though its dependence on large-scale pilot testing may limit its scalability in dynamic testing environments.

\subsection{Large Language Models for Question Difficulty Estimation}

Recent studies demonstrate that LLMs hold significant promise for predicting difficulty in reading comprehension questions. When employed effectively, these models can detect subtle pitfalls, such as misleading phrasing, or required inference that contributes to the question difficulty. For example, SÃ¤uberli et al. used GPT-4 to automatically answer and evaluate multiple-choice items, finding that its assessments of difficulty and quality closely aligned with human evaluators ____. Such research highlights the potential for modern LLMs to serve not only as text generators but also as effective evaluators of question difficulty. Building on these findings, recent work has explored innovative approaches to harness LLMs for difficulty assessment. Park et al. proposed using an LLM as a cohort of ``students'' with varying ability levels to estimate question difficulty ____. Dutulescu et al. investigated LLM-based measures that were designed to capture the intricate cognitive demands of reading comprehension questions that simpler methods may overlook ____. Notably, prompt design and calibration play crucial roles in these methods; researchers have experimented with strategies such as asking the model to vocalize its reasoning or simulating a student's perspective to yield more reliable difficulty estimates. Xu et al. have introduced adaptive prompting, wherein the style of prompting is adjusted based on the predicted difficulty level (easy versus hard) to elicit optimal reasoning from the LLM ____. In conclusion, LLM-based approaches offer a scalable and adaptive alternative for estimating question difficulty, complementing traditional methods and paving the way for more dynamic assessment systems. To move towards developing such dynamic and scalable assessments, in this study, we compare the difficulty estimates given by LLMs to the ones retrieved from IRT analysis on questions from the SARA dataset that test reading comprehension proficiency across various skills. With those findings, we gather insights into how these models can complement the traditional psychometric approaches to inform the design of automated difficulty assessment tools.