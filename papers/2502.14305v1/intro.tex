\section{Introduction}

Large language models (LLMs)~\cite{dubey2024llama, jiang2023mistral, team2023gemini, liu2024deepseek} have ushered in a new era in artificial intelligence and machine learning, driving significant improvements in deployed systems across various industries.

LLMs suitable for real-world applications come in diverse forms, differing in size (ranging from hundreds of millions to hundreds of billions of parameters), architectural design (e.g., encoder-based models like BERT~\cite{devlin2018bert} versus decoder-based models like GPT-3~\cite{brown2020language}), and training paradigms (such as pre-training, instruction tuning, or test-time computation~\cite{dubey2024llama,team2023gemini,jiang2023mistral,mueller2023meta,liu2024deepseek,guo2025deepseek}).

In industry, LLMs are deployed to address a multitude of applications:
\begin{itemize}
    \item \textbf{Search:} Enabling embedding generation~\cite{wang2022text, wang2023improving} and semantic ranking and matching~\cite{qin2023large}.
    \item \textbf{Retrieval and Ranking:} Supporting retrieval~\cite{zhao2024dense} and ranking of items~\cite{li2023agent4ranking,firooz2025360brew,li2023text}.
    \item \textbf{Generative Use Cases:} Powering applications such as chatbots, assistants, and image generators~\cite{achiam2023gpt, dam2024complete, ramesh2022hierarchical}.
\end{itemize}

Furthermore, scaling laws for LLMs have established a strong correlation between model size, validation loss, and downstream task performance~\cite{kaplan2020scaling, hoffmann2022training, raffel2020exploring, wei2022chain}. As a result, increasing the model size is often one of the most effective strategies for enhancing performance. Modern LLMs, particularly autoregressive decoder-only models, have expanded to hundreds of billions of parameters.

While large LLMs exhibit high performance, deploying such large models incurs substantial infrastructure costs, especially for latency- or throughput-sensitive tasks. However, both academia and industry have developed strategies for creating and deploying efficient small language models (SLMs). Here, we primarily focus on methods which leverage an existing internally trained large LLM to create an efficient SLM that largely maintains the original model's accuracy. Approaches to achieve this include white-box or black-box distillation~\cite{hinton2015distilling, gu2024minillm, jin2021modality, agarwal2024policy, tunstall2023zephyr}, compression techniques such as quantization~\cite{frantar2022gptq, behdin2023quantease} or sparsification~\cite{frantar2023sparsegpt, meng2024osscar, sun2023simple, meng2024alps}.


In this work, we present a comprehensive set of lessons learned from developing and deploying various SLMs into production at a large-scale social networking company. We address a wide array of predictive and generative use cases, with inference performance and latency constraints in serving as key considerations. Specifically, our contributions are as follows:
\begin{itemize}
    \item We discuss several large-scale use cases for which language models are useful.
    \item For these use cases, we explore techniques for developing tailored SLMs, with a focus on knowledge distillation and model compression methods such as quantization and structured pruning.
    \item We discuss inference, latency, and other serving considerations, offering insights into the infrastructure required to reliably deploy SLMs in high-throughput or low-latency production environments, and share practical lessons from our real-world deployments.
\end{itemize}


We apply these methods to the following real-world use cases, with experimental results and discussion provided in Section \ref{section:exp}:
\begin{itemize}
    \item \textbf{SLM for predictive tasks obtained through distillation and  pruning} - We leverage a foundation model (FM) for ranking and recommendations~\cite{firooz2025360brew, li2023text} and leverage distillation and pruning to create an SLM that is efficient for serving latency-sensitive use cases. The final SLM we create is more than $20\times$ smaller without any appreciable loss in quality.
    \item \textbf{SLM for reasoning task obtained through distillation} - We leverage various flavors of KD to compress a latency-sensitive reasoning model by more than $5\times$ without any appreciable loss in quality. 
\end{itemize}


The remainder of the paper is organized as follows. In Section~\ref{section:rel_work}, we review related work. Section~\ref{section:methods} details the efficiency techniques employed for creating, modifying, and deploying SLMs. Section~\ref{section:exp} presents experimental results. We then address practical deployment considerations in Section~\ref{section:deployment} and conclude with a discussion of our findings and suggestions for future research.




