\section{Methods}
\label{section:methods}


In this section, we detail various techniques that allow SLMs to retain  strong generalization or task-specific performance, while allowing efficient serving from a latency or throughput standpoint. Specifically, we discuss training via knowledge distillation and post-training model compression. We also intersperse serving and training efficiency concerns throughout the write-up. 




\subsection{Knowledge Distillation}

Modern LLMs work with tokens as the currency of input and output. Let $\mathbf{x}=[x_1, x_2, x_3, \dots]$ represent an input prompt consisting of a sequence of tokens. Given this prompt, a large language model (LLM) generates a response $\mathbf{y}=[y_1, y_2, y_3, \dots, y_T]$, producing tokens sequentially in an autoregressive manner. An LLM models the probability distribution $q_\theta(\mathbf{y}|\mathbf{x})$, parametrized by $\theta$.

Knowledge distillation (KD)~\cite{hinton2015distilling} transfers knowledge from a larger and expressive ``teacher'' model to a smaller ``student'' model, allowing the latter to approximate teacher performance with reduced computational resources. KD can be broadly performed in two different ways (1) by leveraging the output of a teacher model to train the student~\cite{tunstall2023zephyr,guo2025deepseek}(also known as \textbf{black-box} distillation) or (2) by leveraging intermediate outputs~\cite{muralidharan2024minitron,hinton2015distilling} (also known as \textbf{white-box} distillation). White-box distillation using the soft probabilistic outputs of the teacher is a powerful technique and helps provide richer information than hard labels used in supervised fine-tuning (SFT), helping the student generalize better, especially in tasks where smaller models struggle to discover patterns in noisy data.


We consider white-box KD using a training objective with the following general structure. Formally, given a fixed teacher model distribution $p(\mathbf{y}|\mathbf{x})$, the student model $q_\theta$ under the same vocabulary is trained by minimizing the following objective:
\begin{align}
\mathcal{L}[p_{\mathbf{y}}, \mathcal{D}(p \| q_\theta)] 
&= \\
\mathbb{E}_{\mathbf{x} \sim p_{\mathbf{x}}} \mathbb{E}_{\mathbf{y} \sim p_{\mathbf{y}}(\cdot|\mathbf{x})} \Bigg[ \sum_{t=1}^{T} \mathcal{D}\big(p(\cdot|\mathbf{y}_{<t}, \mathbf{x}) \notag & \| q_\theta(\cdot|\mathbf{y}_{<t}, \mathbf{x})\big) \Bigg]
\end{align}
where $p_{\mathbf{y}}$ denotes the distribution from which the response $\mathbf{y}$ is sampled, $\mathcal{D}$ is a divergence measure between two next-token distributions, and $T$ is the maximum response length. This objective emphasizes two aspects: 
\begin{enumerate}
  \item Responses are drawn from $p_{\mathbf{y}}$, which may correspond to ground truth data, the teacher model (sequence-level KD) ~\cite{kim2016sequence}, or the student model itself (on-policy KD) ~\cite{agarwal2024policy,gu2024minillm,zhou2023distillspec}. Recent advancements~\cite{xu2024speculative,ko2024distillm} explore a balance between on-policy and off-policy sampling to mitigate the mismatch between student-generated responses and the teacher’s distribution while addressing inefficiencies in online student autoregressive training.
  \item The student model is optimized to minimize the discrepancy between its next-token distribution $q_\theta$ and the teacher’s predictions $p$, ensuring knowledge transfer across the response sequence.
\end{enumerate}

In this work, we explore different KD strategies based on the task requirements. We also experiment with various student initialization techniques and divergence measures.

Let $\mathcal{V}$ denote the vocabulary. The commonly used divergences are:
\begin{itemize}
    \item Forward Kullback-Leibler (KL) Divergence (FKL):
\begin{equation*}
\mathcal{D}_{\text{FKL}} \left[ p(y_t | \mathbf{y}_{<t}, \mathbf{x}) \| q_\theta (y_t | \mathbf{y}_{<t}, \mathbf{x}) \right] = \sum_{i \in \mathcal{V}} p(i | \cdot) \log \left( \frac{p(i | \cdot)}{q_\theta (i | \cdot)} \right),
\end{equation*}
    \item Reverse Kullback-Leibler Divergence (RKL):
\begin{equation*}
\mathcal{D}_{\text{RKL}} \left[ p(y_t | \mathbf{y}_{<t}, \mathbf{x}) \| q_\theta (y_t | \mathbf{y}_{<t}, \mathbf{x}) \right] = \sum_{i \in \mathcal{V}} q_\theta (i | \cdot) \log \left( \frac{q_\theta (i | \cdot)}{p(i | \cdot)} \right),
\end{equation*}
    \item Jensen-Shannon Divergence (JSD):
\begin{align*}
\mathcal{D}_{JS(\beta)} \left[ p(y_t | \mathbf{y}_{<t}, \mathbf{x}) \| q_\theta(y_t | \mathbf{y}_{<t}, \mathbf{x}) \right] = \; &   \beta \mathcal{D}_{\text{FKL}} \left[ p \| m \right] \\ & + (1 - \beta) \mathcal{D}_{\text{FKL}} \left[ q_\theta \| m \right],
\end{align*}
where \( m = \beta p + (1 - \beta) q_\theta \) is the mixture distribution.
\end{itemize}



\subsection{Post-training compression}

Model compression is a widely-studied area of machine learning efficiency. We specifically focus on post-training compression (PTC) techniques to improve the inference efficiency of SLMs and LLMs. 


Our focus is on post-training compression, where compression is applied to the model after training. Recent compression methods follow a layerwise compression approach (Equation~\ref{eq:layerwise}) since modern models have ballooned in size and it can be scaled to large models without a major compromise in accuracy. 

We now present a mathematical framework for layerwise PTC using calibration data. Let
$\mathbf{X} \in \mathbb{R}^{n \times d}$
denote the calibration data that serve as inputs to a linear layer of the model (e.g., an MLP or an attention projection). Here, \( n \) is the number of tokens in the calibration dataset, and \( d \) is the input dimension of the layer. For instance, in the case of an MLP down projection layer of a Transformer block, \( d \) corresponds to the intermediate size of the model.

Furthermore, let $\mathbf{W} \in \mathbb{R}^{d \times p}$
denote the weight matrix of the layer, where \( p \) is the output dimension. In the MLP down projection example, \( p \) represents the hidden size of the model. We denote by
$\hat{\mathbf{W}} \in \mathbb{R}^{d \times p}$
the weight matrix after compression. The layerwise reconstruction error is defined as
$\|\mathbf{X}\mathbf{W} - \mathbf{X}\hat{\mathbf{W}}\|_F^2.$
Thus, for each layer that undergoes compression, we consider an optimization problem of the form
\begin{equation}\label{eq:layerwise}
\min_{\hat{\mathbf{W}}} \|\mathbf{X}\mathbf{W} - \mathbf{X}\hat{\mathbf{W}}\|_F^2 \quad \text{subject to} \quad \hat{\mathbf{W}} \in \mathcal{Q},
\end{equation}
where \( \mathcal{Q} \subseteq \mathbb{R}^{d \times p} \) denotes the set of feasible solutions that conform to a particular compression scheme. In practice, the set \( \mathcal{Q} \) often exhibits a discrete structure, which renders the optimization problem in \eqref{eq:layerwise} challenging to solve.

Here, we consider two compression techniques: \\
\noindent\textbf{Quantization} In quantization, the model weights are represented in lower precision, using a fewer number of bits. Quantization has proved to be successful in the LLM domain, to obtain compressed models with small accuracy loss~\cite{frantar2022gptq,smoothquant}. In this work, we consider weight-only quantization, where only model weights are quantized, as well as weight and activation quantization. We study methods such as GPTQ~\cite{frantar2022gptq} and QuantEase~\cite{behdin2023quantease} for 4-bit weight-only quantization (aka W4A16), SmoothQuant~\cite{smoothquant} for 8-bit weight-and-activation quantization (aka W8A8), and 8-bit floating-point (FP8) quantization. Since quantization is dependent on hardware, we discuss the details of quantization-related experiments in Section~\ref{section:deployment} (deployment).

\noindent\textbf{Structured Pruning} Neural network pruning~\cite{obd,obs} is a post-training compression technique, where some model weights are set to zero, resulting in sparse model weight matrices. Recent years have seen a surge of papers in pruning techniques for LLMs---examples include~\cite{meng2024alps,frantar2023sparsegpt,sun2023simple}. Without any structure on the model sparsity, however, it can be difficult to realize any inference acceleration from pruning. Therefore, in this work, we pursue a structured pruning approach.
In structured pruning, the goal is to obtain smaller models via removing some neurons from the model weights. In particular, we study MLP pruning, where the goal is to reduce the intermediate size of the model via removing some hidden neurons in feed-forward layers. We also study attention pruning, where we remove a certain number of attention heads from the model~\cite{meng2024osscar,structured1,structured2}.
In this paper, we use OSSCAR~\cite{meng2024osscar} which uses a discrete optimization approach for structured pruning. OSSCAR can be scaled to large scale problems, which we consider. We leverage OSSCAR because it represents the state-of-the-art when it comes to post-training structured pruning of LLMs and leads to the least drop in accuracy when compared to other methods. 

\begin{figure*}[hbpt]
    \centering
    \includegraphics[width=0.9\textwidth]{mental_model.png} 
    \caption{Overview of the process of creating SLMs via distillation and compression.}
    \label{fig:mental_model}
\end{figure*}

\subsection{Training and serving efficiency} 
\label{subsec:Training and serving efficiency}

Despite significant algorithmic advances, the challenges of training and serving LLMs persist. Efficient training and serving remain critical for practical deployment, requiring ongoing improvements in kernel optimization, distributed training, and inference acceleration.

\noindent
\textbf{Training Efficiency} LLM training presents a formidable challenge due to the sheer scale of these models and the quadratic complexity of transformer architectures. Model FLOPs utilization (MFU)~\cite{chowdhery2023palm} is commonly used to measure GPU efficiency, making it necessary to optimize kernel operations and distributed training strategies. We have implemented Liger Kernel~\cite{hsu2024liger} in Triton~\cite{tillet2019triton}, incorporating several key optimizations. First, we employ kernel fusion to reduce repetitive memory transfers between SRAM and DRAM. Next, we adopt in-place tensor modifications to avoid creating additional tensors whenever possible, thus lowering the memory footprint. We also apply chunking, which prevents the full materialization of large tensors and provides tuning flexibility while maintaining comparable performance. In combination, these optimizations reduce training time by 20\% and memory usage by 60\%. Additional performance-memory tradeoffs such as gradient checkpointing and CPU offloading can lead to as much as a threefold speedup.

For distributed training, we use ZeRO~\cite{rajbhandari2020zero} to shard model parameters and data across multiple GPUs, overlapping computation with communication to sustain high MFU. Together with the DeepSpeed team, we have optimized the ZeRO algorithm, and for network-constrained clusters, we have developed ZeRO++~\cite{wang2023zero++,dai2024enhancing} to mitigate non-deterministic synchronization issues that can hinder convergence. ZeRO++ provides a 2.4$\times$ speedup over vanilla ZeRO.



\noindent
\textbf{Serving Efficiency} Serving LLMs efficiently poses unique challenges due to both high computational demands and strict latency requirements. In production environments, the choice of a serving framework is pivotal for maximizing throughput and minimizing response times. Several solutions, including vLLM, SGLag, TRT-LLM, and MLC-LLM~\cite{kwon2023vllm,zheng2024sglang,TRT-LLM,mlc-llm}, have been proposed to address these needs. In our use cases, we evaluated vLLM and SGLang extensively. Our benchmarks revealed that SGLang is better suited to our workloads because its radix tree-based caching mechanism aligns well with our usage patterns and it integrates tightly with FlashInfer~\cite{ye2025flashinfer}, whose efficient attention kernels accelerate the sequence lengths and batch sizes we typically handle.

To further improve serving performance, we deploy our models on NVIDIA H100 GPUs at FP8 precision, striking a practical balance between computational efficiency and model quality. Additional details regarding the serving engine configurations for our experiments can be found in Section~\ref{section:deployment}.




