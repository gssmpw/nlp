\section{Conclusion}

In this paper, we highlight the importance of training and deploying small language models (SLMs) for real-world industry use cases. We specifically drill into model distillation and compression techniques, carefully dissecting the value added by each technique for model generalization. After creating high-quality SLMs, we also detail deployment lessons learned in terms of latency and hardware efficiency. Future directions could include on-policy data-based training for more robust distillation and extending pruning to the attention parts of the transformer blocks.