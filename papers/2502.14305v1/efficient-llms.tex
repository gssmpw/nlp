
\documentclass[sigconf,screen,nonacm,review=false,timestamp=false]{acmart}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{placeins}
\usetikzlibrary{patterns} 



\usepackage{xcolor}
\definecolor{myblue}{RGB}{31,119,180}  
\definecolor{myred}{RGB}{214,39,40}       
\definecolor{mygreen}{RGB}{44,160,44}    
\definecolor{myorange}{RGB}{255,140,0} 



\pgfdeclarepatternformonly[pattern color=myblue]{sparse north east lines}
  {\pgfpoint{0pt}{0pt}}{\pgfpoint{6pt}{6pt}}{\pgfpoint{6pt}{6pt}}%
  {
    \pgfsetlinewidth{0.6pt} 
    \pgfpathmoveto{\pgfpoint{0pt}{0pt}}
    \pgfpathlineto{\pgfpoint{6pt}{6pt}}
    \pgfusepath{stroke}
  }



\pgfdeclarepatternformonly[pattern color=mygreen]{sparse horizontal lines}
  {\pgfpoint{0pt}{0pt}}{\pgfpoint{6pt}{6pt}}{\pgfpoint{6pt}{6pt}}%
  {
    \pgfsetlinewidth{0.4pt}
    \pgfpathmoveto{\pgfpoint{0pt}{3pt}}
    \pgfpathlineto{\pgfpoint{6pt}{3pt}}
    \pgfusepath{stroke}
  }


\pgfdeclarepatternformonly[pattern color=myred]{sparse south west lines}
  {\pgfpoint{0pt}{0pt}}
  {\pgfpoint{6pt}{6pt}}
  {\pgfpoint{6pt}{6pt}}
  {
    \pgfsetlinewidth{0.6pt}
    \pgfpathmoveto{\pgfpoint{0pt}{6pt}}
    \pgfpathlineto{\pgfpoint{6pt}{0pt}}
    \pgfusepath{stroke}
  }



\pgfdeclarepatternformonly[pattern color=myred]{sparse vertical lines}
  {\pgfpoint{0pt}{0pt}}{\pgfpoint{6pt}{6pt}}{\pgfpoint{6pt}{6pt}}%
  {
    \pgfsetlinewidth{0.4pt}
    \pgfpathmoveto{\pgfpoint{3pt}{0pt}}
    \pgfpathlineto{\pgfpoint{3pt}{6pt}}
    \pgfusepath{stroke}
  }




\begin{document}


\title{Efficient AI in Practice: Training and Deployment of Efficient LLMs for Industry Applications}


\author{Kayhan Behdin}
\authornote{Authors contributed equally to this research, sorted by last name}
\email{kbehdin@linkedin.com}


\author{Yun Dai}
\authornotemark[1]
\email{yudai@linkedin.com}
\author{Ata Fatahibaarzi}
\authornotemark[1]
\email{afatahibaarzi@linkedin.com}
\author{Aman Gupta}
\authornotemark[1]
\email{amagupta@linkedin.com}
\author{Qingquan Song}
\authornotemark[1]
\email{qsong@linkedin.com}

\affiliation{%
  \institution{LinkedIn}
    \city{Sunnyvale}
  \state{California}
  \country{USA}
}



\author{Shao Tang}
\email{shatang@linkedin.com}
\author{Hejian Sang}
\email{hsang@linkedin.com}
\author{Gregory Dexter}
\email{gdexter@linkedin.com}
\author{Sirou Zhu}
\email{sirzhu@linkedin.com}
\author{Siyu Zhu}
\email{jzhu@linkedin.com}
\author{Tejas Dharamsi}
\email{tdharamsi@linkedin.com}
\author{Maziar Sanjabi}
\email{maz@linkedin.com}
\author{Vignesh Kothapalli}
\email{vkothapalli@linkedin.com}
\author{Hamed Firooz}
\email{hfirooz@linkedin.com}
\affiliation{%
  \institution{LinkedIn}
    \city{Sunnyvale}
  \state{California}
  \country{USA}
}

\author{Zhoutong Fu}
\email{zfu@linkedin.com}
\author{Yihan Cao}
\email{yihacao@linkedin.com}
\author{Pin-Lun Hsu}
\email{byhsu@linkedin.com}
\author{Fedor Borisyuk}
\email{fborisyu@linkedin.com}
\author{Zhipeng Wang}
\email{zhipwang@linkedin.com}
\author{Rahul Mazumder}
\email{rmazumder@linkedin.com}
\author{Natesh Pillai}
\email{npillai@linkedin.com}
\author{Luke Simon}
\email{lsimon@linkedin.com}
\affiliation{%
  \institution{LinkedIn}
    \city{Sunnyvale}
  \state{California}
  \country{USA}
}



\renewcommand{\shortauthors}{Behdin, Dai, Fatahi, Gupta, Song, Tang et al.}
\newcommand{\st}[1]{{\color{orange} #1}}
\begin{abstract}
Large language models (LLMs) have demonstrated remarkable performance across a wide range of industrial applications, from search and recommendations to generative tasks. Although scaling laws indicate that larger models generally yield better generalization and performance, their substantial computational requirements often render them impractical for many real-world scenarios at scale. In this paper, we present methods and insights for training small language models (SLMs) that deliver high performance and efficiency in deployment. We focus on two key techniques: (1) knowledge distillation and (2) model compression via quantization and pruning. These approaches enable SLMs to retain much of the quality of their larger counterparts while significantly reducing training, serving costs, and latency. We detail the impact of these techniques on a variety of use cases at a large professional social network platform and share deployment lessonsâ€”including hardware optimization strategies that enhance speed and throughput for both predictive and reasoning-based applications.
\end{abstract}

\keywords{large language models, distillation, pruning, compression, quantization}

\maketitle

\input{intro}
\input{rel_work}
\input{methods}
\input{experiments}
\input{deployment}
\input{conclusion}


\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}


\appendix
\input{appendix}





\end{document}
\endinput

