\section{Deployment}
\label{section:deployment}

\subsection{RecSys use case}\label{section:recsys-deploy}
SLM variants of the RecSys use case have been deployed to an A/B test.

\noindent \textbf{Serving Infrastructure} For all use cases, we benchmark and serve traffic using nodes with 256 CPU cores, 2TB of host memory and 8 NVIDIA H100 GPUs. As mentioned earlier, we deploy SGLang (version 0.4.1) as the serving engine. We use tensor parallelism to concurrently use more than 1 GPU for inference. To maximize performance, we employ FP8 quantization for both weights and activations and use FlashInfer~\cite{ye2025flashinfer} as the primary attention backend. Moreover, SGLang incorporates RadixAttention, which enables prefix caching for prompts sharing common prefixes.

\noindent \textbf{Workloads} For the RecSys workload, we follow the prompt structure outlined in Section~\ref{subsection:recsys_task} and utilize context lengths of 16k and 32k. Given the predictive nature of the task, the output length (i.e., the number of generated tokens) is set to 1, rendering the workload heavily reliant on the prefill phase. Consequently, optimizing the prefill stage is crucial for performance. For instance, prefix caching substantially improves prefill (and decode) times when a prompt’s key (K) and value (V) tensors for its shared prefix have already been processed. In cases where there are $k$ candidate items to be ranked for a member $m$, $k$ prompts are served. These prompts share a long prefix containing the user information and historical item interactions. Once one prompt is served, its KV tensors are cached, allowing subsequent prompts for the same member to reuse the cached data—a process we refer to as a hot prefill.

\noindent \textbf{Metrics} We use two key metrics - time to first token (TTFT) and time per output token (TPOT). For prediction tasks, which are prefill-intensive, TTFT is the primary metric as it reflects the duration of the prefill phase. For generative tasks, both TTFT and TPOT are important. Additionally, we report the total serving throughput for various context lengths.

\noindent \textbf{Results} In terms of quality, the models under consideration performn similarly since their AUCs are similar. For performance, we report TTFT and throughput measurements for both 16k and 32k context lengths. P99 TTFT results for a workload with 1 QPS (i.e., $m=1$) and 1 or more prompt per member (i.e., $k=1$ or more) can be found in Figure~\ref{figure:p99ttft} (detailed p50, p99 and throughput numbers can be found in Tables~\ref{table:m1k1_16k}, ~\ref{table:m1k1_32k} and ~\ref{table:m1k4_32k} in Appendix~\ref{subsec:Extra tables}). From the figure and the tables, we can conclude that latency drops drastically as model size becomes smaller. Serving traffic with 32k context is significantly slower than serving traffic with 16k context. Setting $k$ more than 1 doesn't hurt latency much, because of KV caching. 

To better understand the effect of model pruning on inference latency, we present the break down of forward pass for a single layer in Figure~\ref{figure:profiling}. As it can be seen, the attention step is the main latency bottleneck. Our structured pruning of the attention heads improves the attention latency by about $40\%$ which in turn results in more than $28\%$ speed up in prefill latency.



\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={LLMs},
      ylabel={P99 TTFT (ms)},
      xlabel style={font=\large},
      ylabel style={font=\large},
      xtick=data,                    
      symbolic x coords={FM,8B,6.4B,3B,2.4B,2.1B},
      x tick label style={rotate=45, anchor=east},
      width=0.9\columnwidth,        
      height=0.7\columnwidth,       
      legend style={at={(0.95,0.95)}, anchor=north east} 
    ]
      \addplot[
        mark=*,
        color=myblue,
        thick,
        nodes near coords={\pgfkeysvalueof{/pgfplots/y}},
        every node near coord/.append style={font=\footnotesize, anchor=south}
      ] coordinates {
        (8B,282)
        (6.4B,262)
        (3B,209)
        (2.4B,197)
        (2.1B,184)
      };
      \addlegendentry{m=1, k=1, 16k};

      \addplot[
        mark=square*,
        color=myorange,
        thick,
        nodes near coords={\pgfkeysvalueof{/pgfplots/y}},
        every node near coord/.append style={font=\footnotesize, anchor=south}
      ] coordinates {
        (8B,643)
        (6.4B,613)
        (3B,472)
        (2.4B,456)
        (2.1B,391)
      };
      \addlegendentry{m=1, k=1, 32k};

      \addplot[
        mark=triangle*,
        color=mygreen,
        thick,
        nodes near coords={\pgfkeysvalueof{/pgfplots/y}},
        every node near coord/.append style={font=\footnotesize, anchor=south}
      ] coordinates {
        (8B,671)
        (6.4B,655)
        (3B,500)
        (2.4B,488)
        (2.1B,403)
      };
      \addlegendentry{m=1, k=4, 32k};

    \end{axis}
  \end{tikzpicture}
  \caption{P99 TTFT (ms) for various LLMs}
  \label{figure:p99ttft}
\end{figure}




\subsection{Generative use case}

\begin{table}[t]

\centering
\begin{tabular}{lc}
\hline\hline
\textbf{Model} & \textbf{IQM (\%)} \\
\hline\hline
KD Model   &+20.29\%  \\
\hline\hline
\end{tabular}
\caption{Online A/B test results for reasoning task.}
\label{table:gen_use_case_online}
\label{tab:gen_use_case_online}
\end{table}%

The reasoning task outlined in Section 4.3 was launched online for a 1\% A/B test. Results are outlined in Table~\ref{tab:gen_use_case_online}. KD, along with data changes, helped the model improve by 20.29\% on an internal quality metric (IQM). We also discuss deployment lessons from a generative use case to study the effect of different quantization schemes on model inference speed and accuracy. 

\noindent \textbf{Serving Infrastructure} Our setup is mostly similar to Section~\ref{section:recsys-deploy}. However, in addition to using NVIDIA H100 GPUs, we also study the effect of using older NVIDIA A100 GPUs. To this end, 
we use the vLLM backend (version 0.6.1) for serving. We use 1 GPU for serving (TP=1).

\noindent \textbf{Workloads} The workload here consists of prompts with varying lengths, averaging to 3.8k tokens per request, with 1 request per second. The output generation is capped to 2k tokens. As we focus on a generative task here, we report both TTFT and TPOT. We study a Llama3-based model with the 8B size, and consider several serving scenarios with or without quantization using different hardware.

\noindent \textbf{Performance Results} The inference speed results are reported in Table~\ref{table:quantization}. Using the state-of-the-art H100 GPUs results in faster inference (both in terms of TTFT and TPOT) compared to A100 GPUs. In particular, we observe that FP8 serving with H100s leads to the smallest TTFT and TPOT. However, limiting our comparisons to A100 GPUs, we see that using INT8 (W8A8) quantization results in significant reduction of both TTFT and TPOT. On the other hand, INT4 (w4A16) quantization actually increases TTFT, while decreasing TPOT. For generative task with long output sequences, we observe that W4A16 has smaller latency. In summary, we have observed that using higher-end hardware such as H100 leads to the most speed-ups. When such hardware is not available, for generative tasks with long output sequences, W4A16 has the most speed-up, while W8A8 can be more appropriate for prefill-heavy tasks. For the sake of completeness, we present a brief comparison of quantization methods in terms of accuracy in Appendix~\ref{appendix:quantization}.






\begin{table}[]

\begin{tabular}{cccc}
\hline \hline
Model & P50 TTFT (ms) & P50 TPOT (ms) & GPU  \\ \hline \hline
FP16    &   136 &  10.3         & H100          \\
FP8 & 122 &  9.4 & H100 \\
FP16    &       332     & 18.3 & A100          \\
W8A8 (INT) &     227       & 12.9 & A100   \\
W4A16 (INT) &       389     &  11.2 & A100   \\
\hline \hline
\end{tabular}
\caption{Comparison of different quantization methods for the Llama-3 8B model.}
\label{table:quantization}
\end{table}





\begin{figure}[htb]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar stacked,
            symbolic x coords={3B (16k), 2.1B (16k), 3B (32k), 2.1B (32k)},
            xtick=data,
            x tick label style={rotate=45, anchor=east},
            ylabel={Latency (ms)},
            ymin=0,
            legend pos=north west,
            bar width=15pt,
            x tick style={draw=none}, 
            width=\columnwidth,
            height=6cm,
            legend image code/.code={
              \draw[] (0,-0.075cm) rectangle (0.03cm,0.075cm);
            }
        ]

        

        \addplot+[ybar, 
                    fill=myblue!50!white, 
                  draw=myblue!55!white,
          ] plot coordinates {
            (3B (16k), 3)
            (2.1B (16k), 2.5)
            (3B (32k), 7)
            (2.1B (32k), 5)
        };
        
        \addplot+[ybar, 
                  fill=myorange!50!white, 
                 draw=myorange!60!white,
          ] plot coordinates {
            (3B (16k), 1)
            (2.1B (16k), 1)
            (3B (32k), 2)
            (2.1B (32k), 1.8)
        };
        
        \addplot+[ybar, 
fill=mygreen!50!white, 
                  draw=mygreen!55!white,
        ] plot coordinates {
            (3B (16k), 1)
            (2.1B (16k), 1)
            (3B (32k), 2.5)
            (2.1B (32k), 2)
        };

        \legend{Attention, AllReduce, MLP}
        \end{axis}
    \end{tikzpicture}
    \caption{Latency breakdown of a single Transformer block for pruned and unpruned models. As context size increases, attention becomes a major bottleneck.}
    \label{figure:profiling}
\end{figure}

