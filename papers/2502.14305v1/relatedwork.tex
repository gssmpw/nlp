\section{Related work}
\label{section:rel_work}

Research on efficient LLMs has extensively explored pruning, knowledge distillation, and quantization. Muralidharan et al. \cite{muralidharan2024compact} investigate pruning and distillation to maintain performance in smaller models. The Minitron paper \cite{muralidharan2024minitron} focuses on advanced pruning for deployment, while Apple's study \cite{gunter2024apple} integrates multiple techniques, including quantization, for efficiency. Insights on hyper-parameter tuning \cite{ashkboos2024computational, pareja2024unveiling} offer valuable training guidance but lack focus on compression or deployment, and surveys like \cite{zhu2023survey} provide overviews without empirical validation. In contrast, our work emphasizes lessons on efficiency through distillation and compression, coupled with practical deployment considerations such as inference optimization and GPU capacity utilization. With hundreds of millions of users worldwide, our use cases require models that efficiently handle large-scale data and deliver personalized, relevant content tailored to professional networking functionalities. We focus on fine-tuning language models specifically for industry use cases, ensuring alignment with our unique requirements and optimizing models for high-demand, real-world environments. This application-driven approach not only bridges the gap between theoretical compression methods and practical deployment needs but also provides actionable insights on optimizing GPU usage and deployment strategies. By addressing both efficiency and deployment challenges, our research distinguishes itself from more generalized studies, offering tangible solutions for large-scale social networks.