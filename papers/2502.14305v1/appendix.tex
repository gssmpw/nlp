\section{Appendix}
\subsection{Comparison of Quantization Methods}\label{appendix:quantization}


We present a comparison of different quantization methods. We use the Meta Llama 3.1 8B Instruct model, and quantize the model using 1024 samples from the open source C4~\cite{raffel2020exploring} dataset as the calibration set. We report the zero-shot accuracy of the model on three open-source tasks PIQA~\cite{piqa} and ARC easy/challenge~\cite{arc}. We compare W8A8 quantization with SmoothQuant~\cite{smoothquant}, FP8 quantization on H100 GPUs, and W4A16 quantization with GPTQ~\cite{frantar2022gptq} and QuantEase~\cite{behdin2023quantease}.
The results are shown in Table~\ref{table:quantization-accuracy}. We see that 8-bit quantization generally has a small loss of accuracy. On the other hand, GPTQ with W4A16 shows some model quality degradation. However, using QuantEase for better optimization helps to reduce the model quality gap. In our internal experiments, we have observed similar trends when comparing different methods.


\begin{table}[]
\begin{tabular}{cccc}
\hline \hline
Model & ARC-c & ARC-e & PIQA \\ \hline \hline
FP16    & 0.5299   &     0.8136      &    0.7982   \\
FP8    &  0.5179  &     0.8056      &    0.7922      \\
W8A8-INT    &  0.5171  &     0.8123      &     0.7954      \\
W4A16-INT-GPTQ    &   0.436 &      0.7306     &      0.7437    \\
W4A16-INT-QuantEase    & 0.5077   &         0.8068  &      0.7954     \\
\hline \hline
\end{tabular}
\caption{Comparison of different quantization schemes with the Llama 3.1 8B Instruct model.}
\label{table:quantization-accuracy}
\end{table}


\subsection{Extra tables for Section~\ref{section:deployment}}
\FloatBarrier
\label{subsec:Extra tables}
\begin{table}[htbp]
\begin{tabular}{llll}
\hline\hline
Model & P50 TTFT (ms) & P99 TTFT (ms) & Throughput \\ \hline\hline
FM    & 1032           & 1039           & 14127       \\
8b    & 271           & 282           & 14121      \\
6.4B  & 256           & 269           & 14121      \\
3B    & 195           & 209           & 14121      \\
2.4B  & 189           & 197           & 14122      \\
2.1B  & 171           & 184           & 14110      \\  \hline\hline
\end{tabular}
\caption{Results for $m=1$, $k=1$ for $16k$ context length using 4 GPUs (tp=4).}
\label{table:m1k1_16k}
\end{table}

\begin{table}[htbp]
\begin{tabular}{llll}
\hline\hline
Model & P50 TTFT (ms) & P99 TTFT (ms) & Throughput  \\ \hline\hline
FM    & 407661           & 45791           & 15740   \\
8b    & 626           & 643           & 28427       \\
6.4B  & 600           & 613           & 28427       \\
3B    & 452           & 472           & 28423       \\
2.4B  & 437           & 456           & 28422      \\ 
2.1B  & 367           & 391           & 28420       \\ \hline\hline

\end{tabular}
\caption{Results for $m=1$, $k=1$ for $32k$ context length using 4 GPUs (tp = 4).}
\label{table:m1k1_32k}
\end{table}

\newpage

\begin{table}[htbp]
\begin{tabular}{llll}
\hline \hline
Model & P50 TTFT (ms) & P99 TTFT (ms) & Throughput  \\ \hline \hline
FM    & 179483           & 370376           & 45081       \\ 
8b    & 646           & 671           & 115568      \\ 
6.4B  & 626           & 655           & 115560      \\ 
3B    & 477           & 500           & 115546      \\ 
2.4B  & 465           & 488           & 115544      \\ 
2.1B  & 378           & 403           & 115520      \\ \hline \hline
\end{tabular}
\caption{Results for $m=1$, and $k=4$ for $32k$ context length using 4 GPUs (tp=4).}
\label{table:m1k4_32k}
\end{table}