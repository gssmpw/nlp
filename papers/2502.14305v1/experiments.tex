\section{Experiments}
\label{section:exp}

\subsection{Setup}

\textbf{Training details} We consider models ranging in size from a billion to $\sim 100\text{B}$ parameters. For all use cases, we appropriately tune the learning rate, learning rate warmup schedule and decay, as well as weight decay. Context length varies from a few hundred tokens to up to 32k, depending on the use case. We provide use-case specific details at the appropriate place.  


\noindent \textbf{Prompt Structures} For predictive tasks, we are mainly interested in ranking use cases. Hence, the use of decoder-based LLMs is prefill-dominant. For generative and reasoning based-tasks, we are also interested in decoding latencies. We relegate the details of the prompt structures to subsections discussing the specific use cases. 


\noindent \textbf{Quality metrics} We use different accuracy measures across tasks. For predictive tasks, we use area under the curve (AUC). For generative tasks, we rely on validation loss and task-specific metrics.






\subsection{Foundational model for RecSys}\label{subsection:recsys_task}

\subsubsection{Problem setup}

We base our experiments on an internal foundation model (FM) trained using text, primarily for the purpose of ranking and recommendations
~\cite{firooz2025360brew}. The FM consists of 100B+ parameters and is trained to approximate the following distribution for a large variety of recommendation tasks where users interact with items:  
\begin{equation}
    P (m, (e_1, t_1),...,(e_T, t_T)),
\end{equation}
where $m$ represents a user, and each pair $(e_t, i_t)$ for $t = 1, ..., T$ represents the user's interaction $i_t$ (like or click or equivalent) with an entity $e_t$ (such as post on a social media platform) . As mentioned before, the featurization is performed purely via text, allowing the FM to effectively generalize across heterogeneous tasks. The model is then used to estimate the following probabilities for future interactions with entities:

\begin{equation}
    P(i_t, i_{t+1},... | \text{Task instruction}, m, (e_1, i_1),..., (e_{t-1}, i_{t-1}), e_t, e_{t+1},...)
\end{equation}


Text-based featurization makes decoder-only LLMs an attractive option for training this model jointly on a large and varied of recommendation tasks. The FM uses single-token generation for pointwise ranking and probability estimation. The reader is encouraged to peruse ~\cite{firooz2025360brew} for more details about the FM.

Due to the large size of the FM---which contains over 100 billion parameters---serving it online for latency-sensitive applications is challenging. In this work, we present experiments demonstrating how we achieved a more than 20$\times$ reduction in model size, enabling the online serving of a distilled version of the FM with only a modest loss in accuracy.


Our approach, illustrated in Figure~\ref{fig:mental_model}, proceeds in three stages: 1) \textbf{Distillation} on the full model. 2) \textbf{One-shot pruning}\footnote{``One-shot'' indicates pruning without any re-training.} to significantly reduce the model size. 3) \textbf{Re-distillation} of the pruned model to recover generalization capabilities.
All distillation is performed in a post-training setting using data from various recommendation tasks. We optionally also quantize the model (details in Section~\ref{section:deployment}). We measure quality by reporting the AUC on the test sets of in-domain tasks, computing AUC per task and then averaging across tasks.

\subsubsection{Distillation Results}

To evaluate how knowledge distillation (KD) affects model generalization, we compare it against standard supervised fine-tuning (SFT). We focus on the task performance retention of the distilled (or fine-tuned) student models relative to the original foundation model (FM). Specifically, we use Llama-3.1-8B-Instruct and Llama-3.2-3B-Instruct~\cite{dubey2024llama} as our student models. These models offer strong performance while remaining sufficiently compact for throughput- and latency-sensitive environments. In both KD and SFT, responses are generated from the ground-truth action-prediction labels.

For KD, the per-token loss is a weighted combination of: 90\% forward KL divergence between teacher and student logits, and 10\% cross-entropy loss with the ground-truth labels.

Additionally, we include an extra 5\% loss contribution computed over the entire sequence (including the prompt), normalized by its token count. Thus, 95\% of the loss is computed solely on action prediction tokens, while the remaining 5\% is computed over the prompt tokens.

Each KD and SFT configuration undergoes hyperparameter tuning (e.g., peak learning rate, warmup schedule, decay schedule, and weight decay). Figure~\ref{fig:distillation_sft_compare} summarizes the results by reporting the AUC delta relative to the original FM (which achieves the best performance). The main observations are as follows:
\begin{itemize}
    \item \textbf{SFT}: The 3B and 8B student models fine-tuned only with SFT underperform compared to the FM, which is expected given their smaller size and post-training. The performance drop for the 3B model (-1.21\%) is larger than that of the 8B model (-0.62\%).
    \item \textbf{KD}: Using logit supervision from the FM consistently preserves task performance better than SFT. The 8B-KD model shows a minor AUC drop of -0.06\% (compared to -0.62\% for 8B-SFT), while the 3B-KD model (-0.15\%) substantially mitigates the loss relative to 3B-SFT (-1.21\%). These results demonstrate the effectiveness of KD for transferring knowledge, allowing smaller models to remain competitive.
\end{itemize}

Overall, KD performs well on transferring the knowledge from the teacher model and improving smaller model's generalization performance, enabling us to compress a 100B+ FM into smaller models with 3B and 8B parameters. 









\begin{figure}[t]
    \centering
\begin{tikzpicture}
    \begin{axis}[
        ybar,
        bar width=0.6cm,
        width=6cm,
        height=6cm,
        ylabel={AUC Delta (\%)},
        ylabel style={font=\small},
        xtick={8B-KD, 8B-SFT, 3B-KD, 3B-SFT},
        symbolic x coords={8B-KD, 8B-SFT, 3B-KD, 3B-SFT},
        scaled y ticks=false,
        enlarge x limits=0.2,
        ymin=-1.4,
        ymax=0.1,
        nodes near coords={\pgfmathprintnumber[fixed, precision=2]{\pgfplotspointmeta}},
        every node near coord/.append style={font=\footnotesize},
        x tick label style={font=\small, rotate=45, anchor=east},
        title style={font=\small},
    ]
    \draw [thin, dashed] (rel axis cs:0,0.934) -- (rel axis cs:1,0.934);
    
    \addplot[fill=myblue!50!white, bar shift=0pt,draw=black!70] coordinates {(8B-KD, -0.06)};
    \addplot[fill=myorange!50!white, bar shift=0pt,draw=black!70] coordinates {(8B-SFT, -0.62)};
    \addplot[fill=myblue!50!white, bar shift=0pt,draw=black!70] coordinates {(3B-KD, -0.15)};
    \addplot[fill=myorange!50!white, bar shift=0pt,draw=black!70] coordinates {(3B-SFT, -1.21)};
    \end{axis}
\end{tikzpicture}
    \caption{Comparison of Distillation and SFT on the Foundation Model. Knowledge distillation consistently outperforms SFT by effectively leveraging teacher supervision to preserve and enhance performance.}
    \label{fig:distillation_sft_compare}
\end{figure}


\subsubsection{Post-training Compression Results}

After obtaining the distilled student models, we apply structured pruning and on-the-fly FP8 quantization to further compress the model to meet our serving latency requirements (steps outlined in Figure \ref{fig:mental_model}). Since the distilled models (DMs) are decoder-only transformers, our approach focuses on applying structured pruning to remove redundant MLP up/down projection neurons, as well as attention heads in each transformer layer while preserving its capabilities on in-domain ranking tasks. Since one-shot pruning results in loss in quality, we apply targeted fine-tuning after each pruning step to ensure that the model adapts to its reduced size without significant performance loss. To this end, we again leverage knowledge distillation to bridge the gap between the original and pruned models, transferring key insights and ensuring the pruned version closely aligns with the outputs of the original foundational model. 

For structured pruning, we leverage the OSSCAR algorithm~\cite{meng2024osscar}. We now discuss the details of various ablations that we conducted towards structured pruning.




\noindent \textbf{Effect of SFT vs. distillation}
As illustrated in Figure~\ref{fig:mental_model}, we employ OSSCAR to prune the distilled model in a layerwise fashion. After pruning, we use either SFT or KD to restore any lost generalization, with the unpruned distilled model serving as the teacher. To demonstrate the effectiveness of each approach, we examine an 8B distilled model and its 6.4B pruned counterpart (i.e., 20\% pruning of the MLP layers). Table~\ref{table:prune_sft_vs_distil} presents the results. Consistent with the findings for pure distillation, the pruned model benefits significantly more from KD than from SFT. Results for the 3B model (pruned to 2.4B) mirror this trend and are omitted for brevity.

While SFT offers a more straightforward optimization path, distillation provides additional flexibility by leveraging teacher--student training to refine model weights more effectively. In practice, the choice of distillation algorithms and associated losses (e.g., forward KL combined with SFT loss) may vary depending on data availability, computational constraints, and the chosen pruning ratio. Nevertheless, in most cases, including a forward KL term proves highly beneficial in counteracting the performance drop associated with pruning.

Pruning the 8B and 3B distilled models down to 6.4B and 2.4B, respectively, can yield further improvements in serving efficiency. Additional details on deployment are provided in Section~\ref{section:deployment}.

\noindent \textbf{Effect of degree and schedule of pruning}
We next investigate how varying the pruning ratio impacts downstream task accuracy. Table~\ref{tab:amount_pruning} reports one-shot pruning results (i.e., no SFT or KD is applied post-pruning) on an 8B model that has undergone an SFT stage (based on Llama-3.1-
8B-Instruct) comparable to the foundation model. As expected, more aggressive pruning reduces model size but also significantly degrades task accuracy. Notably, small pruning ratios (e.g., from 8B to 6.8B) have minimal effect on performance, while heavier pruning leads to larger accuracy drops. However, as shown in Table~\ref{tab:sft_kd_for_prune}, such performance losses can be mitigated by applying distillation.

We also explore \emph{gradual pruning}~\cite{benbaki2023fast} in which the model is pruned in multiple steps, with knowledge distillation after each pruning. Table~\ref{tab:gradual_pruning} summarizes results for pruning a distilled 3B model to 2.4B (a 37.5\% MLP sparsity) either in a single step or in two smaller steps (removing 1536 hidden neurons in each step, for a total of 3072). The gradual approach recovers model AUC better than the single-step approach, achieving near-lossless compression from 3B to 2.4B.

Finally, to study attention-head pruning, we take the 2.4B model obtained by gradual MLP pruning and prune half of its attention heads via OSSCAR in a one-shot manner, followed by KD. Table~\ref{tab:mlp_vs_attn_pruning} shows that one-shot attention pruning incurs a modest quality loss, but the subsequent KD phase restores the model to AUC parity with the 2.4B baseline.













\begin{table}[t]
\centering
\begin{tabular}{lc}
\hline\hline
\textbf{Model} & \textbf{AUC Delta (\%)} \\
\hline\hline
8B Distilled Model  & -\\ %[0.5em]
6.4B Pruned Model (20\%) + SFT   &-0.47\%  \\
6.4B Pruned Model (20\%) + Distillation   &-0.06\%  \\
\hline\hline
\end{tabular}
\caption{Distillation vs. SFT for post-pruning retraining. The pruned model has its MLP layers pruned by $20\%$.}
\label{table:prune_sft_vs_distil}
\label{tab:sft_kd_for_prune}
\end{table}




\begin{table}[t]
\centering
\begin{tabular}{lcc}
\hline\hline
\textbf{Model}  & \textbf{AUC Delta (\%)} \\
\hline\hline
8B Model    &  - \\
6.8B Pruned Model    & 0.0\% \\
6.4B Pruned Model   &  -1.33\% \\
6.0B Pruned Model   &  -1.72\% \\

\hline\hline
\end{tabular}
\caption{Evaluation of the 8B-parameter model post-SFT and its pruned variants, focusing on MLP pruning performed in a one-shot manner (i.e., no retraining after pruning).}
\label{tab:amount_pruning}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{lcc}
\hline\hline
\textbf{Model} & \textbf{\#Params} & \textbf{AUC Delta (\%)} \\
\hline\hline
3B (Distilled from FM)         & 3B &  -\\
MLP Prune + Distill    & 2.4B &  -0.12\%\\
MLP Prune + Distill (Gradual)   & 2.4B & 0.03\% \\

\hline\hline
\end{tabular}
\caption{Comparison of one-step vs gradual pruning for a 3B model distilled from the FM.}
\label{tab:gradual_pruning}
\end{table}


\begin{table}[t]
\centering
\begin{tabular}{lcc}
\hline\hline
\textbf{Model} & \textbf{\#Params} & \textbf{AUC Delta (\%)} \\
\hline\hline
MLP Pruning       & 2.4B &  -\\
$\wedge$ + Attention Pruning    & 2.1B &  -1.07\%\\
$\wedge$ + Distillation  & 2.1B & 0.02\% \\

\hline\hline
\end{tabular}
\caption{Results for attention pruning. We consider the 2.4B gradual pruning model from Table~\ref{tab:gradual_pruning} as the base. The second row shows the result for one-shot attention pruning, while the last row shows the results after performing distillation.}
\label{tab:mlp_vs_attn_pruning}
\end{table}


\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=0.5cm,
            width=7cm,
            height=6cm,
            ylabel={AUC Delta (\%)},
            ylabel style={font=\small},
            symbolic x coords={Full Precision,C4 1024,C4 4096,Task-Specific 350,Task-Specific 700},
            xtick=data,
            enlarge x limits=0.2,
            ymin=-1.2,  % adjusted for delta values
            ymax=0.1,
            ytick={-1.0, -0.8, -0.6, -0.4, -0.2, 0.0},
            nodes near coords,
            every node near coord/.append style={font=\footnotesize},
            x tick label style={font=\small, rotate=45, anchor=east},
            title style={font=\small},
        ]
        \addplot[fill=myblue!40!white, draw=black!70] coordinates {
            (Full Precision, 0.00)
            (C4 1024,      -1.06)
            (C4 4096,      -0.64)
            (Task-Specific 350, -0.12)
            (Task-Specific 700, -0.10)
        };
        \end{axis}
    \end{tikzpicture}
    \caption{Comparison of one-shot pruning methods. The bars indicate the drop (in percentage points) relative to the full precision baseline. The pruned model is a 6.4B model (20\% MLP pruning).}
    \label{fig:calibration}
\end{figure}

\noindent \textbf{Effect of calibration data}
Figure~\ref{fig:calibration} captures the effect of the calibration dataset $\boldsymbol{X}$ (discussed in Section~\ref{section:methods} and Equation~\eqref{eq:layerwise}) on the accuracy of the pruned versions of the 8B student model (results for the 3B model are similar and are hence omitted for clarity). The \emph{Full Precision} bar illustrates the baseline accuracy of the non-pruned model. We consider two different datasets for calibration - C4~\cite{raffel2020exploring}, an open source dataset, and an in-domain dataset. When we prune using a randomly sampled portion of the C4 dataset (1,024 or 4,096 examples), accuracy drops, although more samples mitigate this drop to an extent. These results indicate that increasing the number of calibration examples from 1,024 to 4,096 can partially recover lost accuracy due to pruning. However, leveraging fewer but more domain-specific samples (350 or 700 examples from the target task) yields better accuracy values, which closely match the full precision baseline. This highlights the importance of using task-relevant data for calibration, even if it involves fewer examples, as it can produce more accurately pruned models than generic calibration sets.


\subsection{Reasoning Task}

We detail experiments for a use case that requires generating reasoning for various input prompts.

\subsubsection{Distillation Results}

We investigate several distillation methods to produce a 1.5B model initialized from Qwen-2.5-1.5B-Instruct \cite{yang2024qwen2}. The teacher models are obtained by training various sizes of Qwen-2.5 Instruct models using SFT with identical hyperparameters. Performance is primarily measured by validation loss, with lower values indicating higher accuracy (see Table~\ref{tab:distillation_results}). 

In a single-stage, word-level training setup, the Forward KL (FKL, $\beta=0$) loss achieves the lowest validation loss, outperforming the Jensen–Shannon Divergence (JSD, $\beta=0.5$), SFT, and Reverse KL (RKL, $\beta=1.0$) approaches. Notably, a larger teacher model does not necessarily lead to superior outcomes; for example, a 3B$\rightarrow$1.5B distillation can outperform a 7B$\rightarrow$1.5B configuration.

In contrast, a two-stage approach---an initial word-level distillation phase followed by on-policy training with the FKL loss (oFKL)---consistently yields better performance than single-stage training alone. Our experiments show that initializing the second stage with the best checkpoint from the first stage (e.g., an FKL-distilled model) is more effective than starting from either an SFT model or the original, non-SFT model. Additionally, an on-policy sampling fraction of 1.0 maximizes accuracy, although a fraction of 0.5 (fr=0.5) may be preferred when faster training is desired. We also observe that generating approximately 300 tokens in this task (denoted as tk in Table~\ref{tab:distillation_results}) during on-policy updates strikes an optimal balance between performance and efficiency, outperforming both 200-token and 400-token alternatives, while a generation temperature in the range of 0.8--0.9 is generally most effective.

Interestingly, the student model can sometimes surpass the teacher’s performance. In the two-stage training paradigm, employing larger teacher models (e.g., 14B rather than 7B or 3B) appears to provide additional benefits for the 1.5B student, although this trend is less pronounced in the single-stage FKL-only training.

Overall, these findings underscore the effectiveness of a two-stage training strategy: an initial supervised fine-tuning phase establishes a strong generative foundation, and subsequent on-policy distillation refines the model’s capabilities, leading to improved generalization and performance.



\begin{table}[htbp]
\centering
\begin{tabular}{c c c}
\hline\hline
\textbf{Ablation} & \textbf{Training Method} & \textbf{Val Loss} \\
\hline\hline
\multirow{4}{*}{Baseline} & SFT & 0.2236 \\
& 3B-SFT & 0.2081 \\
& 7B-SFT & 0.1941 \\
& 14B-SFT & 0.1771\\
\hline
\multirow{4}{*}{Single Stage}
& FKL & 0.2045 \\
& FKL (3B) & \textbf{0.2015} \\
& JSD ($\beta=0.5$) & 0.2143 \\
& RKL & 0.2333 \\
& oFKL & 0.2107 \\
\hline
\multirow{7}{*}{Two Stages} & SFT-SFT & 0.2295 \\
& SFT-oFKL & 0.1982 \\
& FKL-oFKL & 0.1939 \\
& FKL-oFKL (tk=200) & 0.1910 \\
& FKL-oFKL (tk=300) & \textbf{0.1894} \\
& FKL-oFKL (tk=400) & 0.1910 \\
& FKL-oFKL (tk=300, fr=0.5) & 0.1917 \\
\hline
\multirow{4}{*}{Different Teachers} & FKL (3B)-oFKL (3B) & 0.1954 \\
& FKL (3B)-oFKL (7B) & 0.1918 \\
& FKL (7B)-oFKL (7B) & 0.1894 \\
& FKL (14B)-oFKL (14B) & \textbf{0.1863} \\
\hline\hline
\end{tabular}
\caption{Validation losses for various training methods and ablations. Unless explicitly specified (e.g., FKL (3B) indicates distillation using the 3B-SFT model as the teacher), the default teacher is the 7B-SFT model.}
\label{tab:distillation_results}
\end{table}




