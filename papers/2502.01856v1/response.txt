\section{Related Work}
\label{sec:Related}

\subsection {Single-Modality 3D Object Detection}
Single-modality 3D object detection methods are based solely on either LiDAR or camera data, each providing specific advantages. LiDAR-based techniques leverage precise spatial data to generate accurate 3D representations. PointNet **Qi et al., "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"** pioneered direct processing of point clouds, which was further developed by VoxelNet **Chang et al., "VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition"**, introducing voxelized features for improved efficiency and spatial detail. Subsequent methods such as SECOND **Yan et al., "SECOND: Deep Learning for 3D Object Detection in Point Clouds"** and PV-RCNN **Deng et al., "PV-RCNN: Point-Voxel Feature Learning for 3D Object Detection"** expanded on these by enhancing spatial representation and optimizing feature extraction, producing more reliable bounding box predictions. On the other hand, camera-only approaches, though limited in depth accuracy, provide semantic richness essential for object classification. DETR3D **Xu et al., "DETR3D: Real-Time 3D Object Detection from Single LiDAR Scan"** uses transformers to lift 2D image features into 3D space, and BEVDepth **Liu et al., "BEVDepth: A Novel Method for Depth Estimation in the Bird's Eye View Space"** enhances depth estimation, achieving better 3D localization through refined view transformations. However, single-modality approaches inherently lack the complementary insights that multimodal fusion can offer.

\subsection{ Multimodal Sensor Fusion}
Multimodal fusion methods integrate spatially rich LiDAR data with semantically informative camera data, forming a comprehensive perception model. BEV fusion has become a common framework for such integration, with models like BEVFusion **Liu et al., "BEVFusion: A Novel Framework for Multimodal Sensor Fusion"** and related models **Wu et al., "Multimodal Fusion for 3D Object Detection in Autonomous Driving"** adopting lift-splat-shoot (LSS) transformations to align image data in BEV space, allowing it to be fused with LiDAR features effectively. This approach enables the model to capture both spatial geometry and semantic richness, creating a unified feature space that enhances detection performance. CMT **Wang et al., "CMT: Camera-LiDAR Multi-Task Learning for 3D Object Detection"** and MSMD-Fusion **Li et al., "MSMD-Fusion: Multiscale Sensor Fusion for 3D Object Detection"** utilize attention mechanisms and hierarchical fusion strategies to align and integrate LiDAR and camera features. While CMT models interactions through transformers and MSMD-Fusion employs multiscale fusion, these methods do not account for sensor reliability, limiting performance under sensor degradation.



\subsection{Temporal Fusion}
Temporal fusion techniques aggregate features across multiple frames, improving detection performance by capturing motion and continuity. BEVDet4D **Li et al., "BEVDet4D: Real-Time 3D Object Detection from Multi-Frames LiDAR Scan"** and BEVFormer **Xu et al., "BEVFormer: A Novel Method for Temporal Fusion in 3D Object Detection"** leverage temporal BEV representations to consolidate information over time, enhancing robustness against transient occlusions. For example, BEVDet4D coordinates frame-by-frame BEV features, while BEVFormer applies spatio-temporal transformers to integrate cross-frame data, achieving stable detection over time. Similarly, 3D-VID **Zhang et al., "3D-VID: 3D Video Understanding for Autonomous Driving"** utilizes attention mechanisms across point cloud frames to capture object transformations, offering improved detection in dynamic driving scenarios. Although temporal fusion captures scene continuity, it does not fully address issues of degraded data within individual frames.



\subsection{Robustness of LiDAR and Camera Fusion}
Ensuring robustness in LiDAR-camera fusion has become increasingly important, especially when dealing with noisy or partially corrupted data from either modality. TransFusion **Wang et al., "TransFusion: Transformer-Based Adaptive Weighting for Robust Multimodal Fusion"** uses transformer-based adaptive weighting to prioritize reliable sensor inputs, showing potential in managing modality-specific reliability. GAFusion **Liu et al., "GAFusion: Guided Adaptive Fusion for 3D Object Detection"** refines this further, using LiDAR-derived depth information to guide adaptive fusion, selectively refining camera features to enhance cross-modal interaction under adverse conditions. SparseFusion **Zhang et al., "SparseFusion: Robust Multimodal Fusion with Sparse Representations"** further enhances robustness by employing sparse representations from both modalities, increasing efficiency while managing data quality in challenging scenarios. Although these approaches improve the robustness of the fusion, they often lack explicit mechanisms for dynamically adjusting fusion weights based on real-time sensor reliability, leaving the fusion process vulnerable to reliability issues under adverse sensor conditions.

\begin{figure*}[tb]
    \centering
  \includegraphics[width=1\textwidth]{Images/network_final.png}
  \caption{The overal architecture of ReliFusion.}
  
  \label{fig:Network_Reli}
\end{figure*}


ReliFusion addresses the limitations of existing methods by introducing a Reliability module that dynamically adjusts the contribution of LiDAR and camera features based on real-time confidence scores. These scores, derived through CMCL **Wang et al., "CMCL: Confidence Map Learning for Reliable Multimodal Fusion"**, guide the CW-MCA module for robust multimodal fusion. Additionally, a STFA module enhances detection stability by leveraging cross-frame dependencies, ensuring accurate performance even under sensor degradation.

%\subsection{Transformer-based Object Detection}
%The advent and success of the Transformer architecture, initially introduced for natural language processing tasks by Vaswani \textit{et al.}____, paved the way for its application across a spectrum of computer vision tasks. One of the groundbreaking applications was DETR (DEtection TRansformer)____ by Carion \textit{et al.}, demonstrating the utility of transformers for end-to-end object detection in images. Following the success of DETR, several works have further advanced transformer-based object detection____. For instance, methods like Deformable DETR ____ introduced deformable attention modules, enabling fine-grained object localization and achieving competitive results on standard object detection benchmarks.

%Conversely, integrating Transformer architectures into 3D object detection has introduced specific challenges, largely attributed to the inherent volumetric characteristics of 3D datasets. Highlighting the versatility of the Transformer, studies by Misra \textit{et al.} ____ and Liu \textit{et al.} ____ treated point clouds as sequential data, thereby circumventing traditional hierarchical modules. Miao \textit{et al.} ____ further integrated the self-attention paradigm within a sparse convolutional framework. Expanding on this trajectory, Sheng \textit{et al.} ____ developed a Transformer-based model layered over a two-stage detector, directing attention towards point assemblies delineated by RoIs. 



%Contrary to previous methodologies that apply the Transformer, mainly to generate proposals and leverage it for cross-attention, our approach leverages the Transformer's self-attention to merge camera and LiDAR inputs, improving the synthesis of global context from both data streams.