\section{Related Work}
\label{sec:Related}

\subsection {Single-Modality 3D Object Detection}
Single-modality 3D object detection methods are based solely on either LiDAR or camera data, each providing specific advantages. LiDAR-based techniques leverage precise spatial data to generate accurate 3D representations. PointNet \cite{qi2017pointnet++} pioneered direct processing of point clouds, which was further developed by VoxelNet \cite{zhou2018voxelnet}, introducing voxelized features for improved efficiency and spatial detail. Subsequent methods such as SECOND \cite{yan2018second} and PV-RCNN \cite{shi2020pv} expanded on these by enhancing spatial representation and optimizing feature extraction, producing more reliable bounding box predictions. On the other hand, camera-only approaches, though limited in depth accuracy, provide semantic richness essential for object classification. DETR3D \cite{wang2022detr3d} uses transformers to lift 2D image features into 3D space, and BEVDepth \cite{li2023bevdepth} enhances depth estimation, achieving better 3D localization through refined view transformations. However, single-modality approaches inherently lack the complementary insights that multimodal fusion can offer.

\subsection{ Multimodal Sensor Fusion}
Multimodal fusion methods integrate spatially rich LiDAR data with semantically informative camera data, forming a comprehensive perception model. BEV fusion has become a common framework for such integration, with models like BEVFusion \cite{liang2022bevfusion,liu2023bevfusion} and related models \cite{jiao2023msmdfusion,li2024gafusion} adopting lift-splat-shoot (LSS) transformations \cite{philion2020lift} to align image data in BEV space, allowing it to be fused with LiDAR features effectively. This approach enables the model to capture both spatial geometry and semantic richness, creating a unified feature space that enhances detection performance. CMT \cite{yan2023cross} and MSMD-Fusion \cite{jiao2023msmdfusion} utilize attention mechanisms and hierarchical fusion strategies to align and integrate LiDAR and camera features. While CMT models interactions through transformers and MSMD-Fusion employs multiscale fusion, these methods do not account for sensor reliability, limiting performance under sensor degradation.



\subsection{Temporal Fusion}
Temporal fusion techniques aggregate features across multiple frames, improving detection performance by capturing motion and continuity. BEVDet4D \cite{huang2022bevdet4d} and BEVFormer \cite{li2022bevformer} leverage temporal BEV representations to consolidate information over time, enhancing robustness against transient occlusions. For example, BEVDet4D coordinates frame-by-frame BEV features, while BEVFormer applies spatio-temporal transformers to integrate cross-frame data, achieving stable detection over time. Similarly, 3D-VID \cite{zhai2022vid} utilizes attention mechanisms across point cloud frames to capture object transformations, offering improved detection in dynamic driving scenarios. Although temporal fusion captures scene continuity, it does not fully address issues of degraded data within individual frames.



\subsection{Robustness of LiDAR and Camera Fusion}
Ensuring robustness in LiDAR-camera fusion has become increasingly important, especially when dealing with noisy or partially corrupted data from either modality. TransFusion \cite{bai2022transfusion} uses transformer-based adaptive weighting to prioritize reliable sensor inputs, showing potential in managing modality-specific reliability. GAFusion \cite{li2024gafusion} refines this further, using LiDAR-derived depth information to guide adaptive fusion, selectively refining camera features to enhance cross-modal interaction under adverse conditions. SparseFusion \cite{xie2023sparsefusion} further enhances robustness by employing sparse representations from both modalities, increasing efficiency while managing data quality in challenging scenarios. Although these approaches improve the robustness of the fusion, they often lack explicit mechanisms for dynamically adjusting fusion weights based on real-time sensor reliability, leaving the fusion process vulnerable to reliability issues under adverse sensor conditions.

\begin{figure*}[tb]
    \centering
  \includegraphics[width=1\textwidth]{Images/network_final.png}
  \caption{The overal architecture of ReliFusion.}
  
  \label{fig:Network_Reli}
\end{figure*}


ReliFusion addresses the limitations of existing methods by introducing a Reliability module that dynamically adjusts the contribution of LiDAR and camera features based on real-time confidence scores. These scores, derived through CMCL, guide the CW-MCA module for robust multimodal fusion. Additionally, a STFA module enhances detection stability by leveraging cross-frame dependencies, ensuring accurate performance even under sensor degradation.

%\subsection{Transformer-based Object Detection}
%The advent and success of the Transformer architecture, initially introduced for natural language processing tasks by Vaswani \textit{et al.}\cite{vaswani2017attention}, paved the way for its application across a spectrum of computer vision tasks. One of the groundbreaking applications was DETR (DEtection TRansformer)\cite{carion2020end} by Carion \textit{et al.}, demonstrating the utility of transformers for end-to-end object detection in images. Following the success of DETR, several works have further advanced transformer-based object detection\cite{gao2021fast,sun2021sparse,zhu2020deformable}. For instance, methods like Deformable DETR \cite{zhu2020deformable} introduced deformable attention modules, enabling fine-grained object localization and achieving competitive results on standard object detection benchmarks.

%Conversely, integrating Transformer architectures into 3D object detection has introduced specific challenges, largely attributed to the inherent volumetric characteristics of 3D datasets. Highlighting the versatility of the Transformer, studies by Misra \textit{et al.} \cite{misra2021end} and Liu \textit{et al.} \cite{liu2021group} treated point clouds as sequential data, thereby circumventing traditional hierarchical modules. Miao \textit{et al.} \cite{mao2021voxel} further integrated the self-attention paradigm within a sparse convolutional framework. Expanding on this trajectory, Sheng \textit{et al.} \cite{sheng2021improving} developed a Transformer-based model layered over a two-stage detector, directing attention towards point assemblies delineated by RoIs. 



%Contrary to previous methodologies that apply the Transformer, mainly to generate proposals and leverage it for cross-attention, our approach leverages the Transformer's self-attention to merge camera and LiDAR inputs, improving the synthesis of global context from both data streams.