%-------------------------------------------------------------------------------
\section{Watermark Presence Attack}
\label{sec:watermark_detection_attack}
%-------------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure/pipeline-v11.pdf}
    \vspace{-0.6cm}
    \caption{The overview of watermark presence attack.}
    \label{fig:pipeline}
    \vspace{-0.6cm}
\end{figure}

We introduce a promising watermark presence attack to 
detect the presence of latent-based watermarks by analyzing a set of images generated by the target LDM.

\subsection{Threat Model}
\label{sec:threat_model}
The watermark presence attack targets a scenario with two parties: the model owner providing the image generation service and the watermark presence attacker.

\noindent
\textbf{Model Owner.}
The owner of the LDM deploys it on a platform (\eg \cite{huggingface, civitai, tensorart}) and provides image generation services through API access. To protect image copyrights and ensure traceability in cases of misuse, the owner embeds imperceptible watermarks in each generated image without degrading image quality. For any given image, the owner can verify whether it contains their watermark and identify the associated user, a process known as watermark verification, which must remain highly accurate even after image perturbations.
The model owner controls the entire LDM, image generation, and verification process. 

\noindent
\textbf{Watermark Presence Attacker.}
The attacker aims to detect the presence of watermarks in images generated by target LDM \(D_{tar}\). The attacker generates images using the API and controls only the prompts, without access to the model's internals, the initial noise, or any knowledge of the model, watermark method, or watermark detector. 
Also, the attacker can utilize open-source models \(D_{cle}\) to generate watermark-free images with the same prompts. 

\subsection{Overview}
\label{sec:wpa_overview}
\Cref{fig:pipeline} illustrates our watermark presence attack, consisting of three modules: Image Generation, Feature Extraction, and Feature Analysis.

\textbf{Image Generation.} In this module, we generate two image sets: the target image set \(I_{tar}\) from the target LDM \(D_{tar}\) and the clean image set \(I_{cle}\) from the clean LDM \(D_{cle}\). Both sets share the same prompt set \(P\) to ensure any differences are primarily due to the watermark. These image sets are then used to train the watermark feature extractor in the next module.

\textbf{Feature Extraction.} The goal of this module is to train a Watermark Feature Extractor $WFE$, which tries to extract the constant watermark from the generated images. To achieve this, we design three loss functions for training $WFE$ based on the extracted features: the first loss \(\mathcal{L}_{at}\) encourages $WFE$ to aggregate target image features to find the constant watermark signal; the second loss \(\mathcal{L}_{dtc}\) motivates the extractor 
to distinguish between target image and clean image features; and the third loss \(\mathcal{L}_{gc}\) let the clean image features follow a random distribution to prevent extractor from only detecting the signals caused by model difference instead of watermark. These loss components are explained in the \cref{sec:feature_extraction}. Thus, the total loss function is:
\begin{equation}
\mathcal{L}_{{total}} = \mathcal{L}_{at} + \alpha\mathcal{L}_{dtc} + \beta\mathcal{L}_{gc},
\end{equation}
where \(\alpha\) and \(\beta\) are hyperparameters to control the contribution of each loss. 

\textbf{Feature Analysis.} This module determines whether the target diffusion model \( D_{tar} \) contains a watermark. We use the trained  $WFE$ from previous method to extract  \( F_{cle} \) and \( F_{tar} \) from  \( I_{cle} \) and  \( I_{tar} \). Then, we measure the distribution difference between these two features using Maximum Mean Discrepancy (MMD) \cite{MMD} metric. If the feature distributions differ significantly, our method predicts that the target model is watermarked and vice versa. 



\subsection{Image Generation}
We begin by collecting a prompt set \(P\) to create the training dataset, comprising \(I_{\text{cle}}\) and \(I_{\text{tar}}\). Ideally, a watermark-free version of the target model would be used as the clean model to generate a corresponding watermark-free image. The only distinction between the two sets is the presence of the watermark. By analyzing distributional differences, we can infer the watermark's presenceâ€”if no difference is observed, the image is watermark-free; if differences exist, a watermark is likely present.
In practice, attackers often have access to only an approximate watermark-free model. Since most LDMs are fine-tuned from open-source models \cite{10377881,10.1145/3658170}, there is a similarity between the output distributions of the target and clean LDMs. This similarity amplifies the differences caused by the watermark, facilitating effective detection of its presence.

\subsection{Feature Extraction}
\label{sec:feature_extraction}
  
In this module we attempt to detect watermark signal in the generated images by training a Watermark Feature Extractor $WFE$.  The $WFE$ should have the following behaviors for successful watermark presence attack: when the watermark exists in the target images, the extractor should identify the watermark signal, causing \(F_{tar}\) to converge; besides, the extractor should also identify the distribution difference between the \(F_{cle}\) and \(F_{tar}\) caused by the watermark: furthermore, the extractor should only detect the constant signal contributed by the watermark instead of the inherent difference between \(D_{cle}\) and \(D_{tar}\). To achieve these behaviors, We design three types of losses to achieve these properties. To encourage  \(F_{tar}\) to converge during training, we introduce aggregating loss for targe feature \(\mathcal{L}_{at}\), which calculates the variance of the target features as shown in \cref{equ:Lat}:
\begin{equation}
\mathcal{L}_{at}=\frac{1}{N(N-1)} \sum_{i=1}^{N} \sum_{j=1, j \neq i}^{N} \left\|f_{{tar}}^i-f_{{tar}}^j\right\|^2
\label{equ:Lat}
\end{equation}

Besides, we introduce difference \(\mathcal{L}_{dtc}\) loss to distinguish the difference between the \(F_{cle}\) and \(F_{tar}\).   \(\mathcal{L}_{dtc}\)  calculates the reciprocal of the difference between the matched \(f_{tar}\) and \(f_{cle}\) as shown in \cref{equ:Ldtc}. 
\begin{equation}
\mathcal{L}_{dtc} =\frac{1}{\frac{1}{N^2} \sum_{i=1}^{N} \sum_{j=1}^{N} \left\|f_{{tar}}^i-f_{{cle}}^i\right\|^2} 
\label{equ:Ldtc}
\end{equation}

Even if the target images are watermark-free,  \(\mathcal{L}_{at}\) and \(\mathcal{L}_{dtc}\) may converge due to the model difference between \(D_{cle}\) and \(D_{tar}\). $WFE$ can falsely treat the model difference as the watermark difference, which leads to false positive detection result. To alleviate this , we propose the third loss \(\mathcal{L}_{gc}\) to prevent the \(WFE\) model from learning the model-difference features. \(\mathcal{L}_{gc}\) is motivated by one property of watermarking: when the input is a watermark-free image the watermark extractor should produce a random output. Therefore, \(\mathcal{L}_{gc}\) encourages the extracted features from the clean images to follow a random distribution. Hence, \(\mathcal{L}_{gc}\) calculates the KL divergence \cite{KLDivergence} between the \(F_{cle}\)'s distribution and a Gaussian distribution, as shown in \cref{equ:Lmc} .
\begin{equation}
\mathcal{L}_{gc} =\frac{1}{N} \sum_{i=1}^{N} \text{KL}\left(f_{{cle}}^i \parallel \frac{1}{M} \right), \label{equ:Lmc}
\end{equation}
where \( N \) is the batch size, \( M \) is the feature dimension size.  

\begin{figure*}[t]
  \centering
  \includegraphics[width=.8\linewidth]{Figure/SWA-LDM.pdf}
  % \includegraphics[width=\linewidth]{Figure/SWA-LDM.pdf}
  \vspace{-0.3cm}
  \caption{The framework of \tool. 
  We extract the key $\mathbf{k}$ from randomly initialized latent variables and use it to shuffle the remaining latent variables where the watermark is inserted. This ensures the watermark information is randomized in each generated image. 
  }
  \label{fig:swa-ldm}
  \vspace{-0.5cm}
\end{figure*} 


%-------------------------------------------------------------------------------
\section{\tool{}}
%-------------------------------------------------------------------------------

We introduce \tool, a plug-and-play component for existing latent watermarking methods that generates image-dependent watermarks to counter watermark presence attack.

\subsection{Overview}
The framework of \tool is shown in \cref{fig:swa-ldm}. 
During watermark embedding, \tool initializes latent noise \(\mathbf{z}_T\) sampled from a standard Gaussian distribution, which it then splits into key, noise, and watermark channels. The key and noise channels retain random noise, while the watermark channel is reinitialized with watermark-embedded noise based on the chosen latent-based watermarking method.

To randomize the watermark,
\tool leverages the inherent randomness of latent noise by extracting a random seed (key) from the noise in the key channel. 
To ensure reliable key recovery to counter diffusion inversion errors and image transmission noises, we design a robust key construction mechanism and enhance the key channel for stronger key information. 
The key then seeds the random number generator to shuffle the watermark and noise channels, and the key channel is merged to produce the watermarked latent noise \(\hat{\mathbf{z}}_T\). The subsequent denoising and image generation process follows the standard procedure of LDMs.

During watermark verification, \tool restores the image to latent space, obtaining \(\hat{\mathbf{z}}'_0\), and uses diffusion inversion method to approximate the original latent noise \(\hat{\mathbf{z}}'_T\). \tool partitions \(\hat{\mathbf{z}}'_T\) to extract the key from the key channel, which is used to reshuffle the remaining channels. This process recovers the latent noise from the watermark channel, from which the watermark is extracted and verified.

The closest work, Gaussian Shading~\cite{yang2024gaussian}, using stream ciphers to encrypt latent noise, introducing randomness to the watermarked latent distribution. 
However, stream ciphers require a unique nonce per latent noise to achieve randomness, meaning each generated image must be paired with a specific nonce. This nonce must be managed and matched with the corresponding image during watermark verification, as it is essential for decrypting the latent noise to verify the watermark. 
This nonce management complicates practical implementation in LDM applications that generate high volumes of images. 
In contrast, \tool operates without any additional information management.

\subsection{Watermark Embedding}
\label{sec:watermark_embedding}
Each step of watermarking embedding in \tool{} is illustrated below.

\noindent
\textbf{Channel Splitting.}
\tool initializes the latent noise \( \mathbf{z}_T \in \mathbb{R}^{c \times h \times w} \) and divide it into key channels \( \mathbf{z}_T^k \in \mathbb{R}^{c_k \times h \times w} \), noise channels \( \mathbf{z}_T^n \in \mathbb{R}^{c_n \times h \times w} \), and watermark channels \( \mathbf{z}_T^w \in \mathbb{R}^{c_w \times h \times w} \). The key and noise channels are filled with randomly sampled noise from a standard Gaussian distribution \( \mathcal{N}(0, \mathbf{I}) \). Meanwhile, the watermark channel is initialized with a chosen latent-based watermarking method (\eg, \cite{yang2024gaussian})

\noindent
\textbf{Key Construction.}
\tool uses a pseudorandom number generator (PRNG) and shuffle algorithm to randomize the latent noise in the watermark and noise channel. The PRNG seed must meet three criteria: (1) each seed is randomly generated and unique per image, (2) it can be reliably reconstructed during watermark verification, and (3) it does not require additional management.

To achieve this, \tool derives the key \( \mathbf{k} \) directly from the latent noise. Given that LDMs transform latent noise \( \mathbf{z}_T \), sampled from a Gaussian distribution \(\mathcal{N}(0, \mathbf{I})\), into an image \( \mathbf{x}_0 \), this approach retains the necessary randomness and ensures compatibility with diffusion inversion, fulfilling the requirements for \( \mathbf{k} \).
However, during diffusion inversion, the reconstructed \( \mathbf{z}_T' \) may not perfectly match the original \( \mathbf{z}_T \), especially when \( \mathbf{x}_0 \) experiences perturbations. Therefore, \tool must reliably construct \( \mathbf{k} \) even in the presence of these variances. 
For robustness, \tool abstracts specific elements from the latent noise to construct each bit of \( \mathbf{k} \). 
First, we define a mapping function to consistently sample fixed locations within the latent noise for each bit in \( \mathbf{k} \). 
Specifically, a mapping function \( \mathcal{M}: \{1, 2, \dots, N\} \to \{ (i, j, q) \mid i \in [1, c_k], j \in [1, h], q \in [1, w] \} \), with \( N = c_k \times h \times w \), allows \tool to consistently access the same positions in \( \mathbf{z}_T^k \) for \( \mathbf{k} \)-bit construction. For simplicity, \( \mathcal{M} \) is implemented as a sequential mapping, unfolding \(\mathbf{z}_T^k\) linearly to assign each bit of \( \mathbf{k} \).

Next, each bit of \( \mathbf{k} \) is sampled based on the sign of specific latent variables \( z^k_{T, i, j, q} \) within \( \mathbf{z}_T^k \). Letting \( M \) denote the bit-length of \( \mathbf{k} \), each bit is determined as follows: 
\begin{equation}
\mathbf{k} = \left[ k_1, \dots, k_{M} \right]\quad k_m = \begin{cases}
1, \text{if } z^k_{T, i_m, j_m, q_m} > 0 \\
0, \text{if } z^k_{T, i_m, j_m, q_m} \leq 0
\end{cases}
\end{equation}
where \((i_m, j_m, q_m) = \mathcal{M}(m)\) indicates the index of \(k_m\) in the latent noise \(\mathbf{z}_T^k\).

\input{sec/alg_redundant}

\noindent
\textbf{Key Channel Enhancement.}
While the key construction accounts for noise variations, it may still fail to reliably recover \( \mathbf{k} \) under perturbations. To address this, we propose a method to construct redundant key information within \(\mathbf{z}_T^k\), ensuring robust key extraction with minimal modification to \(\mathbf{z}_T^k\).
Let \(R\) represent the number of redundant key, and define the \(r\)-th redundant key as \(\mathbf{k}^r\), where \(r \in [1, R]\) and \(k^r_{m} = k_m\) for \(m \in [1, M]\).
For each redundant key \(\mathbf{k}^r\), we map it to a set of latent noise using the mapping function \(\mathcal{M}\). The latent variable \(z^k_{T, i_n, j_n, q_n}\) corresponds to \(k^r_{m}\) with \((i_n, j_n, q_n) = \mathcal{M}(r \times M + m)\). If the relationship between \(k^r_{m}\) (either 0 or 1) and \(z^k_{T, i_n, j_n, q_n}\) (either \(\leq 0\) or \(> 0\)) does not match, we search for a latent noise element that satisfies the condition and swap the corresponding values.
The key channel enhancement process is detailed in \cref{alg:generate_key}. This algorithm takes the latent noise \(\mathbf{z}_T^k\), the key \(\mathbf{k}\), and the number of redundant key \(R\) as input, and outputs the enhanced latent noise \(\overline{\mathbf{z}}_T^{k}\), which includes the redundant key. 

\noindent
\textbf{Latent Noise Shuffling.}
As previously discussed, \tool embeds the watermark into the latent noise of the watermark channel, resulting in \(\hat{\mathbf{z}}_T^w\). To randomize this embedded watermark, \tool uses \( \mathbf{k} \) as a seed for the pseudorandom number generator (PCG64)~\cite{oneill:pcg2014}. The Fisher-Yates shuffle algorithm~\cite{Fisher_Yates-AFP} is then applied to permute \( \operatorname{concat}(\hat{\mathbf{z}}_T^w, \mathbf{z}_T^n) \), dispersing watermark information across the latent space. Finally, we concatenate the enhanced latent noise \(\overline{\mathbf{z}}_T^{k}\) with the shuffled watermark channel to form the final watermarked latent noise \(\hat{\mathbf{z}}_T\).

\noindent
\textbf{Image Generation.}
After constructing the watermarked latent noise \(\hat{\mathbf{z}}_T\), the image generation process follows the standard procedure of the LDMs. Specifically, we utilize DDIM~\cite{song2020denoising} for denoising of \(\hat{\mathbf{z}}_T\). Once the denoised latent \(\hat{\mathbf{z}}_0\) is obtained, the watermarked image \(\hat{\mathbf{x}}_0\) is generated by applying the LDM decoder \(\mathcal{D}\): \(\hat{\mathbf{x}}_0 = \mathcal{D}(\hat{\mathbf{z}}_0)\).

\subsection{Watermark Verification}
\noindent
\textbf{Diffusion Inversion.}
For watermark verification, we use the LDM encoder \(\mathcal{E}\) to map the watermarked image \(\hat{\mathbf{x}}_0\) back to the latent space, obtaining \(\hat{\mathbf{z}}'_0 = \mathcal{E}(\hat{\mathbf{x}}_0)\). We then apply diffusion inversion over \(T\) timesteps, estimating the additive noise to recover 
\(\hat{\mathbf{z}}'_T \approx \hat{\mathbf{z}}_T\). Here, DDIM inversion~\cite{dimm_inversion} is used to approximate the original latent noise.

\noindent
\textbf{Robust Key Extraction.}  
With \(\hat{\mathbf{z}}'_T\) obtained, we partition it to isolate the key channel \(\overline{\mathbf{z}}_T^{k'}\) containing the redundant key information and the shuffled channel. Using a fixed mapping function \(\mathcal{M}\), we extract the redundant key information from predetermined positions in  \(\overline{\mathbf{z}}_T^{k'}\) to obtain both the key \(\mathbf{k}'\) and its redundant bits \(\{\mathbf{k}^{r'} \mid r \in [1, R]\}\). 
Each bit \(k'_m\) of \(\mathbf{k}'\) is determined by a majority voting mechanism, wherein if more bits are zero than one among \(k'_m\) and \(\{ k^{r'}_m | r \in [1, R] \}\), \(k'_m\) is set to zero; otherwise, it is set to one.

\noindent
\textbf{Reshuffling Watermark Information and Verification.}  
After recovering the key \(\mathbf{k}'\), we use it as the seed for a pseudorandom number generator (PCG64) and reapply the Fisher-Yates shuffle algorithm to re-shuffle the latent noise, excluding the key channel. 
This reshuffled latent noise is then split to isolate \(\hat{\mathbf{z}}_T^{w'}\) and \(\mathbf{z}_T^{n'}\). 
Finally, based on the latent-based watermarking method employed, we extract and verify the watermark from \(\hat{\mathbf{z}}_T^{w'}\).