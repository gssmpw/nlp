% \clearpage
% \setcounter{page}{1}
% \maketitlesupplementary

\section{Experimental Details}
All experiments are implemented using PyTorch 2.0.1 and the Diffusers 0.24.0 library, running on a single NVIDIA A800 GPU.

\subsection{Baseline Methods}
We detail the specific configurations of different baseline watermarking methods used in our experiments.
\begin{itemize}
\item{For Tree-Ring \cite{wen2023tree}, }
it embeds a carefully constructed watermark pattern in the Fourier space of the initial latent noise. Following the original paper, we set the watermark pattern to multiple concentric rings, where each ring maintains a constant value drawn from a Gaussian distribution. This design ensures rotation invariance and resilience against various image transformations while minimally deviating from an isotropic Gaussian distribution. The radius of the watermark pattern is set to 16 to balance generation quality and verification performance. We embed the watermark into one latent channel and vary the constant values along the rings to generate distinct watermarks.

\item{For Gaussian Shading \cite{yang2024gaussian},} 
we adopt the parameters recommended in the original paper to balance watermark capacity and robustness. 
Specifically, the watermark size is $1/8$ of the latent height, $1/8$ of the latent width, and one channel. For generated images with resolution $3 \times 512 \times 512$, the corresponding latent noise dimensions are $4 \times 64 \times 64$, resulting in watermark dimensions of $1 \times 8 \times 8$. During embedding, the watermark is redundantly replicated and inserted into three latent noise channels to enhance robustness.


\item{For DiffuseTrace \cite{lei2024diffusetrace}, }
We use the publicly available code to obtain the DiffuseTrace Encoder-Decoder architecture, and pre-train the Encoder-Decoder to generate 3-channel latent noise (dimensions $3 \times 64 \times 64$) containing the DiffuseTrace watermark. To meet the input requirement of $4 \times 64 \times 64$ latent noise for Stable Diffusion (SD) models, we concatenate the 3-channel watermarked latent noise with a $1 \times 64 \times 64$ lantent noise sampled from a Gaussian distribution. To ensure compatibility with different SD models, we fine-tune the Encoder-Decoder for each specific SD model to initialize latent noise tailored to the model.  Following the original implementation, the bit length of the watermark is set to 48 during both training and testing.
\end{itemize}


\subsection{The Architecture of WFE}
In \cref{sec:feature_extraction}, we have introduced the Watermark Feature Extractor (WFE). Here, we provide details of its architecture, as shown in \cref{tab:wfe_architecture}. The WFE processes input images with a resolution of $256 \times 256$ through a series of $3 \times 3$ convolutional layers with stride 2, progressively reducing the spatial dimensions to an $8 \times 8$ feature map. Each convolutional layer is followed by a ReLU activation to introduce non-linearity. The resulting feature map is flattened and passed through two dense layers: the first projects it to a hidden dimension of 512, stabilized by LayerNorm, and the second produces a 100-dimensional watermark feature vector. A sigmoid activation function is applied to the output, ensuring that the values are in the range [0, 1], suitable for representing watermark features.


\input{Table/WFE_arch}

\subsection{Evaluated Models}
We utilized three widely-used Stable Diffusion models as base models: Stable Diffusion v1-5 (SD v1-5), Stable Diffusion v2-1 (SD v2-1), and SDXL 1.0-base (SDXL 1.0). Additionally, we downloaded 60 checkpoints from Hugging Face \cite{huggingface}, which include models fine-tuned from these base models or equipped with adapters. These customized models, comprising either fine-tuned versions or base models enhanced with adapters, were used in our experiments. A list of the adapters and fine-tuned models can be found in \cref{tab:name_model}.

\input{Table/app_evaluated_models}
\begin{figure*}[t]
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=0.4\linewidth]{Figure/ablation_studies/legend_image.png} 
    \end{minipage}
    % \vspace{-0.1cm} 
    \resizebox{0.75\linewidth}{!}{%
    \subfloat[\tool{}(G-S).]{\label{fig:keybit_G-S}\includegraphics[width=0.30\linewidth]{Figure/ablation_studies/key_bit_G-S.png}}\hspace{0.05\linewidth}
     \subfloat[\tool{}(T-R).]{\label{fig:keybit_T-R}\includegraphics[width=0.30\linewidth]{Figure/ablation_studies/key_bit_T-R.png}}\hspace{0.05\linewidth}
     \subfloat[\tool{}(D-T).]{\label{fig:keybit_D-T}\includegraphics[width=0.30\linewidth]{Figure/ablation_studies/key_bit_D-T.png}}
     }
     % \vspace{-0.3cm}
    \caption{Performance of \tool with varying bit number of key. The effectiveness is demonstrated through AUC and stealthiness metrics, where (a) compares \tool(G-S) with Gaussian Shading and (b) compares \tool(T-R) with Tree-Ring. (c) compares \tool(D-T) with DiffuseTrace.}
    
    \label{fig:keybit}
\end{figure*}

\subsection{Image Perturbation Settings}
In \cref{sec:robustness_exp}, we evaluate the robustness of SWA-LDM against seven common image perturbations, which simulate potential attacks. The types of perturbations and their respective parameter ranges are detailed as follows:
\begin{itemize}
\item {JPEG Compression}, where the image is compressed using quality factors (QF) set to \{100, 90, 80, 70, 60, 50, 40, 30, 20, 10\}; 
\item {Random Crop}, which retains a randomly selected region covering \{80\%, 90\%\} of the original image area, discarding the rest; 
\item {Random Drop}, where randomly selected regions covering \{10\%, 20\%, 30\%, 40\%, 50\%\} of the image area are replaced with black pixels; 
\item {Resize and Restore} (Resize), where the image is resized to \{20\%, 30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%\} of its original dimensions and then restored to the original size; 
\item {Gaussian Blur} (GauBlur), applied with blur radii \( r \) set to \{1, 2, 3, 4\}; 
\item {Median Filter} (MedFilter), using kernel sizes \( k \) of \{1, 3, 5, 7, 9, 11\}; 
\item {Brightness Adjustment}, which modifies the image brightness using brightness factors \{0, 2, 4, 6\}.
\end{itemize}

\section{Additional Ablation Studies}
\vspace{3pt}
% \noindent
\textbf{Impact of bit number of key on stealthiness and verification performance.} 
In \cref{sec:watermark_embedding}, we have introduced how \tool{} employs a pseudorandom number generator (PRNG) and a shuffle algorithm to randomize the watermarked latent noise. The key is derived from the latent noise and serves as the seed for the PRNG. 
To analyze the impact of key bit number on watermark stealthiness and verification performance, we have further evaluated \tool across a range of key lengths from 4 to 32 bits, based on the setup described in \cref{sec:experiment_setup}. 
Results in \cref{fig:keybit}, indicate that both watermark stealthiness and verification AUC remain consistent regardless of the key's bit number within this range. These findings suggest that the choice of key length does not compromise the effectiveness or concealment of the watermark, providing flexibility in the design of the key construction process.


\begin{table}[t]
    \centering
    \caption{Impact of key construction on watermark verification AUC. The table compares results with (\Checkmark) and without (\XSolidBrush) the proposed key construction mechanism. Left to right are LDMs fine-tuned from SD v1-5/SD v2-1/SDXL 1.0.}

    \label{tab:key_construction}
    % \vspace{-0.3cm}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}cccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Key \\ Construction\end{tabular}}} & \multicolumn{3}{c}{\textbf{Watermark Methods}} \\ \cmidrule(l){2-4} 
     & \tool (T-R) & \tool (D-T) & \tool (G-S) \\ \midrule
    \XSolidBrush & 0.517/0.532/0.521 & 0.482/0.495/0.500 & 0.491/0.502/0.496 \\
    \Checkmark & 0.999/0.997/0.996 & 0.999/0.978/0.810 & 0.999/0.997/0.998 \\ \bottomrule
    \end{tabular}
    }
  % \vspace{-0.3cm}
\end{table}

% \vspace{3pt}
\noindent
\textbf{Impact of key construction.}
As described in \cref{sec:watermark_embedding}, the key is constructed by sampling each bit from the sign of specific latent variables. To assess its importance, we have replaced this mechanism with fixed latent variables as the key.  
Following the experimental setup in \cref{sec:experiment_setup}, the results in \cref{tab:key_construction} show that removing the key construction significantly degrades performance. 
As analyzed in \cref{sec:watermark_embedding}, during diffusion inversion, the reconstructed latent noise may not perfectly match the original latent noise, especially when the image experiences perturbations. These mismatches prevent accurate key reconstruction, making watermark verification infeasible. This underscores the critical role of key construction in maintaining robust watermarking.

\input{Table/as_sample_method}

% \vspace{3pt}
\noindent
\textbf{Impact of sampling methods.}
We tested five commonly used sampling methods. As shown in \cref{tab:sample_method}, our method demonstrates stable watermark verification AUC across different sampling methods.

\input{Table/as_step}
% \vspace{3pt}
\noindent
\textbf{Impact of inversion step.}
In practice, the specific denoising step used in generation is often unknown, which can result in a mismatch with the inversion step. However, as shown in \cref{tab:step}, this step mismatch does not affect the performance of our watermarking approach.
