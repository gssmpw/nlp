\section{Function-Level Experiments}
We present our experiments on the function-level synthetic data mixtures in this section.
\subsection{Setup}
\paragraph{Dataset}
Our dataset primarily comprises synthetic F* data, along with selected data from datasets of other programming languages (CodeAlpaca, Evol-Instruct, Deepseek-Prover-V1, RunbugRun, stack-exchange-preferences). CodeAlpaca collects synthetic coding instructions from ChatGPT following self-instruct, including diverse programming problems and languages~\cite{codealpaca}. Evol-Instruct also contains high-quality synthetic natural language to code data~\cite{wei2024magicoder}. The Deepseek-Prover-V1~\cite{xin2024deepseek} dataset includes extensive Lean 4 proof data to enhance theorem-proving capabilities in LLMs. RunbugRun provides an executable dataset for automated program repair on commonly used programming languages~\cite{prenner2023runbugrun}. 
We conduct experiments to test the impact of the composition of the data and different mixing ratios on  F* code.
\subsection {Synthetic Data Generation Setup}
We select code LLMs demonstrating relatively superior capability in generating F* code: Qwen2.5-Coder-32B-Instruct, Qwen2.5-Coder-14B-Instruct~\cite{hui2024qwen2}, CodeLlama-13b-Instruct-hf~\cite{roziere2023code}, 
DeepSeek-Coder-V2-Lite-Instruct~\cite{liu2024deepseek}, 
DeepSeek-R1-Distill-Qwen-32B ~\cite{guo2025deepseek}. Different prompt templates are adopted in generating different tasks of data. All promptings are done in zero shot. We use a temperature of 0.7 to generate data.  

% \subsection {Training}
% We use the fine-tuning setup in \ref{sec:Function-level Data Experiment Setting Experiment settings} to train a Qwen2.5-Coder-7B model.

\paragraph{Evaluation Dataset and Metrics} 
\label{synth simple}
We sample 2,000 instructions from synthesized F* dataset as hold-out test set, equally covering all 3 problem types described above. All evaluations are done on the same test set.
During evaluation, the model first generates an initial response. If incorrect, we prompt the same model again with the erroneous code and the error message for repair.  We record both the initial code generation accuracy and the overall accuracy after a single repair attempt. The response codes are generated with T=0.1. 

\subsection{Results}\label{sec:simplesynthreault}


% \begin{table}[ht]
% \centering
% \setlength{\tabcolsep}{2.5pt}
% \small 
% \renewcommand{\arraystretch}{1}

% \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% Model& Initial Accuracy & Accuracy (at most 1 repair)\\
% \midrule
%  Qwen-2.5-coder-7B-instruct & 0.25 & 0.30  \\
%  Qwen-2.5-coder-14B-instruct & 0.50 & 0.55 \\
%  Qwen-2.5-coder-32B-instruct  & 0.48 & 0.58\\
% Qwen-2-72B-instruct & 0.34  & 0.43  \\
% DeepSeek-Coder-V2-Lite-Instruct & 0.43  & 0.53  \\
% LLama-3.1-70B & 0.21 & 0.27\\
% LLama-3.1-405B & -- & --\\
% GPT-4o & -- & --\\
% Claude-3.5-Sonnet & -- & --\\
% Our model & -- & --\\
% \bottomrule
% \end{tabular}
% }
% \vspace{5pt}
% \caption{Comparison among different models' performance on the validation set}
% \label{resultcomparison}
% \end{table}

% \begin{table}[ht]
% \centering
% \setlength{\tabcolsep}{2.5pt}
% \small 
% \renewcommand{\arraystretch}{1}

% \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% Data & Initial Accuracy & Accuracy (at most 1 repair)\\
% \midrule
% 54000 synthetic data  & 0.42 & 0.47 \\
% 54000 synthetic data + 80000 magicoder data   & 0.52 & 0.56 \\
% 93000 synthetic data +  80000 magicoder +20000 (deepseekprover+ runbugrun+codealpaca)+ 30000 stack exchange & 0.55 & 0.58\\
% 93000 synthetic data +  80000 magicoder +20000 (deepseekprover+ runbugrun+codealpaca )& -- & --\\
% 93000 synthetic data +  80000 magicoder & -- & --\\
% 93000 synthetic data& -- & --\\

% \bottomrule
% \end{tabular}
% }
% \vspace{5pt}
% \caption{Comparison among different data mixtures}
% \label{different data mixures}
% \end{table}
\begin{table}[]
\centering
\footnotesize
\setlength{\tabcolsep}{1.2pt} 
\setlength{\extrarowheight}{1.2pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lcc}
\toprule
\textbf{Model  }                         & \textbf{Pass@1}& \textbf{+Repair} \\ \midrule
\textsc{Qwen-2.5-coder-7B-instruct}      & 0.25             & 0.30                       \\
\textsc{Qwen-2.5-coder-14B-instruct}     & 0.50              & 0.55                        \\
\textsc{Qwen-2.5-coder-32B-instruct}     & 0.48             & 0.58                       \\ 
\textsc{Qwen-2-72B-instruct}            & 0.34             & 0.43                     \\
\textsc{DeepSeek-Coder-33B-Instruct} & 0.29             & 0.38                      \\
\textsc{DeepSeek-Coder-V2-Lite-Instruct} & 0.43             & 0.53                       \\
\textsc{DeepSeek-V3} & 0.66             & 0.78                     \\
\textsc{LLama-3.1-70B }                  & 0.21             & 0.27                         \\
\textsc{Llama-3.3-70B-Instruct}                  & 0.17             & 0.26                     \\
\textsc{GPT-4o}                       &  0.60               &  0.70                       \\        \midrule
\multicolumn{3}{l}{\textbf{Fine-tune Data Mixture}} \\ \midrule
54K F* Only & 0.42 & 0.47 \\
+ Evol & 0.52 &0.56\\
93K F* Only & 0.48 & 0.52\\
+ DSP-V1  & 0.52 & 0.54\\
+ DSP-V1 + Evol + CodeAlpaca + RBR  & 0.58 & 0.62\\
+ DSP-V1 + Evol + CodeAlpaca + RBR (14B)$^{\dagger}$ & \textbf{0.74} & \textbf{0.77}\\
\textit{\space - F* NL2Code} & 0.48 (\textbf{-}) & 0.52(\textbf{-})\\

\bottomrule
\end{tabular}
}
\caption{Performance comparison across different models and fine-tuning data mixtures. \textbf{F* only}: synthetic F* data, Evol: 80K (54K F*) / 50K (93K F*) Magicoder-Evol-Instruct data, \textbf{DSP-V1}: 20K Deepseek-Prover-V1 data, \textbf{CodeAlpaca}: 15K CodeAlpaca data, \textbf{RBR}: 15K RunBugRun data.\\
$^{\dagger}$: Adopting Qwen2.5-Coder-14B as base model.}
\label{tab:syntheticresults}
\end{table}

\paragraph{Comparing Against Powerful LLMs} Compared with 6 popular open-source LLMs: Qwen-2.5-coder-7B-instruct, Qwen-2.5-coder-14B-instruct, Qwen-2.5-coder-32B-instruct ~\cite{hui2024qwen25codertechnicalreport}, Qwen-2-72B-instruct ~\cite{hui2024qwen2}, DeepSeek-Coder-V2-Lite-Instruct~\cite{liu2024deepseek}, LLaMa-3.1-70B, our model achieves the best performance in both generation and repair within the F* framework. The results in Table \ref{tab:syntheticresults} demonstrate that in the initial generation, our model significantly outperforms non-code models such as LLaMA-3.1-70B and Qwen-2-72B in terms of accuracy. At the same time, the accuracy of the generation also surpasses that of code models with larger parameter sizes such as Qwen-2.5-coder-32B-instruct, indicating that our instruction-tuning dataset is highly effective in enhancing the model’s ability to generate F* data. Our initial generation accuracy is also comparable to GPT-4o, which is generally challenging given the size of its base model parameters.
% \paragraph{Detailed comparison of the models’ capability generating and repairing F* code } 
% \sun{I don't think we will need to talk a lot about this. Maybe move to appendix}
% Generally, code LLMs outperform non-code LLMs in both generation and repair, and for code models, larger parameter sizes typically correlate with better performance. Notably, the Qwen-2.5-coder series models show that smaller models are less capable of repairing erroneous code. Especially for Qwen-2.5-coder-14B-instruct, it surpasses Qwen-2.5-coder-32B-instruct in initial generation, but falls short in repair tasks, which may due to smaller models' limited ability to understand error information, making it more difficult for them to modify the code based on that information. Additionally, the error information in F*’s execution feedback is not detailed, which could further confuse models with fewer parameters. 
\paragraph{Benefits of data diversity and the effect of different data mixtures }
As shown in Table \ref{tab:syntheticresults}, data diversity has a positive impact on the model's performance. When more diverse language data (e.g. data from Evol-Instruct \& Deepseek-Prover-V1) is added to the F* synthetic dataset, the model’s accuracy on the F* validation set significantly improves, regardless of the amount of original F* synthetic dat (2) Within a certain range, more diverse data leads to better model performance.  (3)  More high-quality synthetic data indeed leads to better model performance, which suggests that for languages like F*, where the model’s knowledge is still limited, increasing the amount of high-quality language-specific fine-tuning data is beneficial for improving the model’s performance. 
% (4) The mixing ratio of F* data with other diverse data also impacts the model’s capability. 



% \begin{table}[ht]
% \centering
% \setlength{\tabcolsep}{2.5pt}
% \small 
% \renewcommand{\arraystretch}{1}

% \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% Data & Initial Accuracy & Accuracy (at most 1 repair)\\
% \midrule
% final data  & -- & --\\
% final data without repair tasks & 0.50 & 0.55\\
% final data without nl2code tasks & -- & --\\
% \bottomrule
% \end{tabular}
% }
% \vspace{5pt}
% \caption{Ablation}
% \label{different data mixures}
% \end{table}