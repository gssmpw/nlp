@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}


@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{xia2022less,
  title={Less training, more repairing please: revisiting automated program repair via zero-shot learning},
  author={Xia, Chunqiu Steven and Zhang, Lingming},
  booktitle={Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={959--971},
  year={2022}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@inproceedings{xia2023automated,
  title={Automated program repair in the era of large pre-trained language models},
  author={Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={1482--1494},
  year={2023},
  organization={IEEE}
}

@inproceedings{jin2023inferfix,
  title={Inferfix: End-to-end program repair with llms},
  author={Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1646--1656},
  year={2023}
}

@inproceedings{deng2023large,
  title={Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models},
  author={Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming},
  booktitle={Proceedings of the 32nd ACM SIGSOFT international symposium on software testing and analysis},
  pages={423--435},
  year={2023}
}

@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{muennighoff2023octopack,
  title={Octopack: Instruction tuning code large language models},
  author={Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne},
  journal={arXiv preprint arXiv:2308.07124},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{swamy2011secure,
  title={Secure distributed programming with value-dependent types},
  author={Swamy, Nikhil and Chen, Juan and Fournet, C{\'e}dric and Strub, Pierre-Yves and Bhargavan, Karthikeyan and Yang, Jean},
  journal={ACM SIGPLAN Notices},
  volume={46},
  number={9},
  pages={266--278},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}

@article{wei2024selfcodealign,
  title={Selfcodealign: Self-alignment for code generation},
  author={Wei, Yuxiang and Cassano, Federico and Liu, Jiawei and Ding, Yifeng and Jain, Naman and Mueller, Zachary and de Vries, Harm and Von Werra, Leandro and Guha, Arjun and Zhang, Lingming},
  journal={arXiv preprint arXiv:2410.24198},
  year={2024}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{wei2024magicoder,
  title={Magicoder: Empowering code generation with oss-instruct},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}


@article{xu2024magpie,
  title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2406.08464},
  year={2024}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chen2024diversity,
  title={On the Diversity of Synthetic Data and its Impact on Training Large Language Models},
  author={Chen, Hao and Waheed, Abdul and Li, Xiang and Wang, Yidong and Wang, Jindong and Raj, Bhiksha and Abdin, Marah I},
  journal={arXiv preprint arXiv:2410.15226},
  year={2024}
}

@misc{fstar2024,
  title     = {F*: A Proof-Oriented Programming Language},
  author = {Microsoft},
  year      = {2024},
  howpublished = {\url{https://fstar-lang.org}},
}


@misc{ocamlgithub,
   author ={Allen Geng},
  title     = {ocamlgithub},
  year      = {2022},
  howpublished = {\url{https://huggingface.co/datasets/AllenGeng/ocamlgithub}},
}
@misc{rocq,
   author ={Inria Rennes-Bretagne-Atlantique},
  title     = {The Rocq Prover},
  year      = {2024},
  howpublished = {\url{https://rocq-prover.org}},
}


@article{prenner2023runbugrun,
  title={RunBugRun--An Executable Dataset for Automated Program Repair},
  author={Prenner, Julian Aron and Robbes, Romain},
  journal={arXiv preprint arXiv:2304.01102},
  year={2023}
}



@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}


@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}


@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}



@article{yu2024large,
  title={Large language model as attributed training data generator: A tale of diversity and bias},
  author={Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{instructgpt2022,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}
@article{kopf2024openassistant,
  title={Openassistant conversations-democratizing large language model alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{zheng2024lmsyschat1mlargescalerealworldllm,
      title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
      year={2024},
      eprint={2309.11998},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.11998}, 
}

@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}



@article{wang2024codeclm,
  title={CodecLM: Aligning Language Models with Tailored Synthetic Data},
  author={Wang, Zifeng and Li, Chun-Liang and Perot, Vincent and Le, Long T and Miao, Jin and Zhang, Zizhao and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2404.05875},
  year={2024}
}

@article{cassano2024knowledge,
  title={Knowledge transfer from high-resource to low-resource programming languages for code llms},
  author={Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA2},
  pages={677--708},
  year={2024},
  publisher={ACM New York, NY, USA}
}


@article{polu2022formal,
  title={Formal mathematics statement curriculum learning},
  author={Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2202.01344},
  year={2022}
}
@misc{first2023baldurwholeproofgenerationrepair,
      title={Baldur: Whole-Proof Generation and Repair with Large Language Models}, 
      author={Emily First and Markus N. Rabe and Talia Ringer and Yuriy Brun},
      year={2023},
      eprint={2303.04910},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.04910}, 
}

@article{wang2023lego,
  title={Lego-prover: Neural theorem proving with growing libraries},
  author={Wang, Haiming and Xin, Huajian and Zheng, Chuanyang and Li, Lin and Liu, Zhengying and Cao, Qingxing and Huang, Yinya and Xiong, Jing and Shi, Han and Xie, Enze and others},
  journal={arXiv preprint arXiv:2310.00656},
  year={2023}
}

@inproceedings{wang2023dt,
  title={Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function},
  author={Wang, Haiming and Yuan, Ye and Liu, Zhengying and Shen, Jianhao and Yin, Yichun and Xiong, Jing and Xie, Enze and Shi, Han and Li, Yujun and Li, Lin and others},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12632--12646},
  year={2023}
}
@article{wang2020learning,
  title={Learning to prove theorems by learning to generate theorems},
  author={Wang, Mingzhe and Deng, Jia},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18146--18157},
  year={2020}
}

@article{jiang2022thor,
  title={Thor: Wielding hammers to integrate language models and automated theorem provers},
  author={Jiang, Albert Qiaochu and Li, Wenda and Tworkowski, Szymon and Czechowski, Konrad and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Mi{\l}o{\'s}, Piotr and Wu, Yuhuai and Jamnik, Mateja},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8360--8373},
  year={2022}
}
@misc{polu2020generativelanguagemodelingautomated,
      title={Generative Language Modeling for Automated Theorem Proving}, 
      author={Stanislas Polu and Ilya Sutskever},
      year={2020},
      eprint={2009.03393},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.03393}, 
}
@article{han2021proof,
  title={Proof artifact co-training for theorem proving with language models},
  author={Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W and Polu, Stanislas},
  journal={arXiv preprint arXiv:2102.06203},
  year={2021}
}
@article{zhao2023decomposing,
  title={Decomposing the enigma: Subgoal-based demonstration learning for formal theorem proving},
  author={Zhao, Xueliang and Li, Wenda and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2305.16366},
  year={2023}
}
@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{claude,
  title     = {Claude 3.5 Sonnet},
  author = {Anthropic},
  year      = {2024},
  howpublished = {\url{https://www.anthropic.com/claude/sonnet}},
}

@misc{dafny,
  title     = {The Dafny Programming and Verification Language},
  author = {Microsoft},
  year      = {2024},
  howpublished = {\url{https://dafny.org}},
}

@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Hu, Jian and Wu, Xibin and Wang, Weixun and Zhang, Dehao and Cao, Yu and others},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{zhang2024textbf,
  title={Only-IF: Revealing the Decisive Effect of Instruction Diversity on Generalization},
  author={Zhang, Dylan and Wang, Justin and Charton, Francois},
  journal={arXiv preprint arXiv:2410.04717},
  year={2024}
}

@misc{jimenez2024swebenchlanguagemodelsresolve,
      title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?}, 
      author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
      year={2024},
      eprint={2310.06770},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06770}, 
}

@article{loughridge2024dafnybench,
  title={DafnyBench: A Benchmark for Formal Software Verification},
  author={Loughridge, Chloe and Sun, Qinyi and Ahrenbach, Seth and Cassano, Federico and Sun, Chuyue and Sheng, Ying and Mudide, Anish and Misu, Md Rakib Hossain and Amin, Nada and Tegmark, Max},
  journal={arXiv preprint arXiv:2406.08467},
  year={2024}
}
@article{dong2023abilities,
  title={How abilities in large language models are affected by supervised fine-tuning data composition},
  author={Dong, Guanting and Yuan, Hongyi and Lu, Keming and Li, Chengpeng and Xue, Mingfeng and Liu, Dayiheng and Wang, Wei and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2310.05492},
  year={2023}
}
@misc{lin2025prover,
      title={Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving}, 
      author={Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia Li and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin},
      year={2025},
      eprint={2502.07640},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.07640}, 
}
@article{ahn2024large,
  title={Large language models for mathematical reasoning: Progresses and challenges},
  author={Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2402.00157},
  year={2024}
}
@online{h4stackexchange,
  author = {Lambert, Nathan and Tunstall, Lewis and Rajani, Nazneen and Thrush, Tristan},
  title = {HuggingFace H4 Stack Exchange Preference Dataset},
  year = 2023,
  url = {https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences},
}

@article{chakraborty2024towards,
  title={Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming},
  author={Chakraborty, Saikat and Ebner, Gabriel and Bhat, Siddharth and Fakhoury, Sarah and Fatima, Sakina and Lahiri, Shuvendu and Swamy, Nikhil},
  journal={arXiv preprint arXiv:2405.01787},
  year={2024}
}
@article{xin2024deepseek,
  title={DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
  author={Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan},
  journal={arXiv preprint arXiv:2405.14333},
  year={2024}
}

@article{yang2023leandojo,
  title={Leandojo: Theorem proving with retrieval-augmented language models},
  author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={21573--21612},
  year={2023}
}
@book{digitalsystem,
author = {Grout, Ian},
title = {Digital Systems Design with FPGAs and CPLDs},
year = {2008},
isbn = {075068397X},
publisher = {Newnes},
address = {USA},
abstract = {This textbook explains how to design and develop digital electronic systems using programmable logic devices (PLDs). Totally practical in nature, the book features numerous (quantify when known) case study designs using a variety of Field Programmable Gate Array (FPGA) and Complex Programmable Logic Devices (CPLD), for a range of applications from control and instrumentation to semiconductor automatic test equipment.Key features include:* Case studies that provide a walk through of the design process, highlighting the trade-offs involved.* Discussion of real world issues such as choice of device, pin-out, power supply, power supply decoupling, signal integrity- for embedding FPGAs within a PCB based design.With this book engineers will be able to:* Use PLD technology to develop digital and mixed signal electronic systems* Develop PLD based designs using both schematic capture and VHDL synthesis techniques* Interface a PLD to digital and mixed-signal systems* Undertake complete design exercises from design concept through to the build and test of PLD based electronic hardwareThis book will be ideal for electronic and computer engineering students taking a practical or Lab based course on digital systems development using PLDs and for engineers in industry looking for concrete advice on developing a digital system using a FPGA or CPLD as its core. *Case studies that provide a walk through of the design process, highlighting the trade-offs involved.*Discussion of real world issues such as choice of device, pin-out, power supply, power supply decoupling, signal integrity- for embedding FPGAs within a PCB based design.}
}

@misc{dai2025processsupervisionguidedpolicyoptimization,
      title={Process Supervision-Guided Policy Optimization for Code Generation}, 
      author={Ning Dai and Zheng Wu and Renjie Zheng and Ziyun Wei and Wenlei Shi and Xing Jin and Guanlin Liu and Chen Dun and Liang Huang and Lin Yan},
      year={2025},
      eprint={2410.17621},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.17621}, 
}

@misc{hui2024qwen25codertechnicalreport,
      title={Qwen2.5-Coder Technical Report}, 
      author={Binyuan Hui and Jian Yang and Zeyu Cui and Jiaxi Yang and Dayiheng Liu and Lei Zhang and Tianyu Liu and Jiajun Zhang and Bowen Yu and Keming Lu and Kai Dang and Yang Fan and Yichang Zhang and An Yang and Rui Men and Fei Huang and Bo Zheng and Yibo Miao and Shanghaoran Quan and Yunlong Feng and Xingzhang Ren and Xuancheng Ren and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12186}, 
}

@inproceedings{kozyrev2024coqpilot,
  title={CoqPilot, a plugin for LLM-based generation of proofs},
  author={Kozyrev, Andrei and Solovev, Gleb and Khramov, Nikita and Podkopaev, Anton},
  booktitle={Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
  pages={2382--2385},
  year={2024}
}

@article{huang2025leanprogress,
  title={LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction},
  author={Huang, Suozhi and Song, Peiyang and George, Robert Joseph and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2502.17925},
  year={2025}
}

@article{lin2025goedel,
  title={Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving},
  author={Lin, Yong and Tang, Shange and Lyu, Bohan and Wu, Jiayun and Lin, Hongzhou and Yang, Kaiyu and Li, Jia and Xia, Mengzhou and Chen, Danqi and Arora, Sanjeev and others},
  journal={arXiv preprint arXiv:2502.07640},
  year={2025}
}

@article{song2024towards,
  title={Towards large language models as copilots for theorem proving in lean},
  author={Song, Peiyang and Yang, Kaiyu and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2404.12534},
  year={2024}
}

@article{wu2024internlm2,
  title={Internlm2. 5-stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems},
  author={Wu, Zijian and Huang, Suozhi and Zhou, Zhejian and Ying, Huaiyuan and Wang, Jiayu and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2410.15700},
  year={2024}
}
