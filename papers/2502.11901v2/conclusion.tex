\section{Conclusion}
In this work, we propose a synthetic data recipe for instruction-tuning code language models to become proficient proof-oriented programmers in F* under extreme data scarcity. By synthesizing function-level F*, diversifying with other programming languages and tasks and generating new verification tasks on a repository level, we build a powerful \name that outperforms powerful language models, even GPT-4o with only 14B parameter. We further show that \name can work together with existing code LMs to improve their proof-oriented programming capabilities by large margins.
More broadly, we present a variable path for low-resource programming languages and verification tools, lowering the barrier to adopting formal verification in real-world software development.

