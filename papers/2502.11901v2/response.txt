\section{Related Work}
% \subsection{Synthetic Data For Instruction Tuning}

% Instruction fine-tuning enhances the alignment of large language models by optimizing their ability to understand and follow user instructions, which depends on high-quality instruction datasets**Radford et al., "Language Models Play 20 Questions"**. Human-created instruction datasets**Hochreiter et al., "The Vanishing Gradient Problem"** are often labor-demanding and costly. As a result, more and more emphasis is placed on developing methods for utilizing LLMs to generate high-quality and diverse synthetic instruction data**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. While the self-instruct**Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"** is widely used for synthesizing code data in instruction tuning for diverse instruction-tuning data generation,  Evol-Instruct**Vinyals et al., "Sequence to Sequence Learning with Neural Networks"** and Code Evol-Instruct**Hochreiter et al., "The Vanishing Gradient Problem"** also control the difficulty and complexity level of the generated instructions. However, such approaches may aggravate the inherent bias in LLM**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. To mitigate the issue, OSS-Instruct**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** craft diverse instructions via open-source code snippets.  On the other hand, in low-resource scenario, MultiPL-T**Vinyals et al., "Sequence to Sequence Learning with Neural Networks"** leverages semi-synthetic data, translating training data from high-resource languages into training data for low-resource languages. However, the pipeline is rather complicated and costly to scale up. In our work, by leveraging F-star properties, we are able to generate F-star data on a large scale via code LLMs.
% \wang{Merge this subsection with the last. Keep the citations but use more concise language}

\paragraph{Language Models For Code}
Large language models (LLMs) have advanced in code generation **Radford et al., "Language Models Play 20 Questions"**, program repair **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, and software engineering tasks like issue fixing **Hochreiter et al., "The Vanishing Gradient Problem"** and testing **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"**. Open-source models (e.g., Qwen2.5-Coder **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**, Deepseek-Coder **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**) and closed-source models (e.g., GPT-4o **Hochreiter et al., "The Vanishing Gradient Problem"**) undergo \textbf{pre-training} on large-scale code datasets **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, followed by \textbf{post-training} via instruction fine-tuning **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"** or reinforcement learning **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. While these models excel in common languages like Python and C++, proof-oriented languages such as F* **Hochreiter et al., "The Vanishing Gradient Problem"** remain underrepresented, limiting their effectiveness in proof synthesis.

% \paragraph{Language Models For Formal Languages}
% Formal theorem proving offers an appealing domain for unlocking the reasoning potential of LLMs, with proofs being easier to verify compared to conventional programs**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. Currently, Language models have shown capability in formal languages such as Isabelle**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** and Lean **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"**. Researchers have also explored various approaches to optimizing automated theorem proving using large language models: employing retrieval-augmented assistance**Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, improving search efficiency by dynamically allocating computational resources **Hochreiter et al., "The Vanishing Gradient Problem"**, and introducing synthetic data during training **Radford et al., "Language Models Play 20 Questions"**.

\paragraph{Language Models For Formal Proof}
Formal theorem proving and proof repair offer an appealing domain for unlocking the reasoning potential of LLMs, with proofs being easier to verify rigorously without hallucination**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**, in both mathematical theorem proving and formal program verification. Currently, Language models have shown capability in formal languages such as Isabelle**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** and Lean **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"**. Researchers have also explored various approaches to optimize automated theorem proving using large language models: employing retrieval-augmented assistance**Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, improving search efficiency by dynamically allocating computational resources **Hochreiter et al., "The Vanishing Gradient Problem"**, predicting the progress of proofs**Radford et al., "Language Models Play 20 Questions"**, employing LLMs as copilots that assist humans in proving theorems**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, and introducing synthetic data during training**Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"**. However, most of these efforts focus on mathematical domains rather than repository-level software verification, which is addressed by PoPilot.

\paragraph{Synthetic Data for Instruction Tuning}
Instruction fine-tuning improves LLMs' ability to follow instructions and relies on high-quality datasets **Hochreiter et al., "The Vanishing Gradient Problem"**. Since human-annotated datasets are costly **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, recent methods focus on LLM-generated instruction data **Radford et al., "Language Models Play 20 Questions"**. Self-Instruct **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"** pioneered this approach, later extended by Alpaca **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"** and Code Alpaca **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. Evol-Instruct **Hochreiter et al., "The Vanishing Gradient Problem"** and Code Evol-Instruct **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"** introduced multi-stage generation for better instruction diversity, though risks of reinforcing biases remain **Radford et al., "Language Models Play 20 Questions"**. OSS-INSTRUCT **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"** and SelfCodeAlign **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** mitigate this by leveraging open-source data, while MultiPL-T **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"** enables cross-lingual instruction transfer.




% \subsection{Instruction-Tuning using LLM-Generate Dataset}

% Recently, research on synthesizing high-quality instruction fine-tuning data using various techniques—particularly leveraging LLMs to generate new instruction-response pairs—has shown promising results. Self-Instruct **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"** utilizes GPT-3 **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"** to generate fine-tuning data through a carefully curated few-shot prompt, enabling the model to iteratively fine-tune itself. Alpaca **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** and Code Alpaca **Hochreiter et al., "The Vanishing Gradient Problem"** improve this approach by using ChatGPT **Radford et al., "Language Models Play 20 Questions"** to construct larger fine-tuning datasets for general-purpose and coding-specific tasks, respectively.

% More advanced methods, such as Evol-Instruct **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, introduce multi-stage generation strategies to enhance the diversity and complexity of synthetic datasets, while WizardCoder **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"** adapts this technique specifically for code generation. Recently, approaches like OSS-INSTRUCT **Hochreiter et al., "The Vanishing Gradient Problem"** and SelfCodeAlign **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** further refine LLM-based instruction generation by leveraging open-source seed code snippets to create high-quality coding datasets.


% \subsection{Language Models For Code}
% The application of large language models (LLMs) for code generation has gained significant progress in recent years. Open-sourced models such as Qwen2.5-Coder **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, and Deepseek-Coder **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** and closed-sourced model such as GPT-4o **Hochreiter et al., "The Vanishing Gradient Problem"**, have demonstrated strong capabilities in code generation
% **Radford et al., "Language Models Play 20 Questions"**, program repair **Zaremba et al., "Recurrent Neural Network Scanning for Image Description Generation"**, and more complex domain-specific software engineering tasks such as fix GitHub issues**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"** and software testing**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. These models typically undergo two stages of training: \textbf{pre-training} and \textbf{post-training}. Pre-training involves training the base model on large-scale datasets containing natural language and code using various objectives**Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**, allowing it to have basic natural language understanding, as well as learning code syntax, patterns, and common structures of different programming languages**Hochreiter et al., "The Vanishing Gradient Problem"**. Post-training techniques such as instruction fine-tuning are then applied to further specialize the model for specific coding tasks. This stage typically uses a smaller, well-curated, and high-quality dataset with carefully structured instructions from the real world**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"** or synthesized**Radford et al., "Language Models Play 20 Questions"**, enabling the model to better understand and follow task-specific prompts. 

% Besides instruction fine-tuning, other post-training methods such as reinforcement learning from human feedback have been employed to align models with human expertise, improving code coherence and correctness**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. While this progress has improved code generation for widely used languages such as Python and C++, functional proof-oriented programming languages like F* **Hochreiter et al., "The Vanishing Gradient Problem"** remain underrepresented in the existing training datasets. This limitation makes it challenging for general-purpose code models to effectively generate F* programs.