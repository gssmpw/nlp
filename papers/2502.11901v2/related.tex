\section{Related Work}

% \subsection{Synthetic Data For Instruction Tuning}

% Instruction fine-tuning enhances the alignment of large language models by optimizing their ability to understand and follow user instructions, which depends on high-quality instruction datasets\cite{zhou2023limaalignment,selfinstruct}. Human-created instruction datasets\cite{instructgpt2022,köpf2023openassistantconversationsdemocratizing,zheng2024lmsyschat1mlargescalerealworldllm} are often labor-demanding and costly. As a result, more and more emphasis is placed on developing methods for utilizing LLMs to generate high-quality and diverse synthetic instruction data\cite{selfinstruct, gunasekar2023textbooksneed,wang2024codeclmaligninglanguagemodels,xu2024magpiealignmentdatasynthesis}. While the self-instruct\cite{selfinstruct,codealpaca} is widely used for synthesizing code data in instruction tuning for diverse instruction-tuning data generation,  Evol-Instruct\cite{xu2023wizardlmempoweringlargelanguage} and Code Evol-Instruct\cite{wizardcoder} also control the difficulty and complexity level of the generated instructions. However, such approaches may aggravate the inherent bias in LLM\cite{yu2023largelanguagemodelattributed}. To mitigate the issue, OSS-Instruct\cite{wei2024magicoder} craft diverse instructions via open-source code snippets.  On the other hand, in low-resource scenario, MultiPL-T\cite{cassano2024knowledgetransferhighresourcelowresource} leverages semi-synthetic data, translating training data from high-resource languages into training data for low-resource languages. However, the pipeline is rather complicated and costly to scale up. In our work, by leveraging F-star properties, we are able to generate F-star data on a large scale via code LLMs.
% \wang{Merge this subsection with the last. Keep the citations but use more concise language}

\paragraph{Language Models For Code}
Large language models (LLMs) have advanced in code generation \cite{chen2021evaluating,austin2021program}, program repair \cite{xia2022less,xia2023automated,jin2023inferfix}, and software engineering tasks like issue fixing \cite{jimenez2023swe} and testing \cite{deng2023large}. Open-source models (e.g., Qwen2.5-Coder \cite{hui2024qwen2}, Deepseek-Coder \cite{guo2024deepseek}) and closed-source models (e.g., GPT-4o \cite{hurst2024gpt}) undergo \textbf{pre-training} on large-scale code datasets \cite{radford2018improving,nijkamp2022codegen}, followed by \textbf{post-training} via instruction fine-tuning \cite{muennighoff2023octopack,roziere2023code,luo2023wizardcoder,codealpaca} or reinforcement learning \cite{ouyang2022training,bai2022training}. While these models excel in common languages like Python and C++, proof-oriented languages such as F* \cite{swamy2011secure} remain underrepresented, limiting their effectiveness in proof synthesis.

% \paragraph{Language Models For Formal Languages}
% Formal theorem proving offers an appealing domain for unlocking the reasoning potential of LLMs, with proofs being easier to verify compared to conventional programs\cite{yang2023leandojo}. Currently, Language models have shown capability in formal languages such as Isabelle\cite{jiang2022thor,wang2023lego,zhao2023decomposing} and Lean \cite{polu2022formal,han2021proof,yang2023leandojo}. Researchers have also explored various approaches to optimizing automated theorem proving using large language models: employing retrieval-augmented assistance\cite{yang2023leandojo}, improving search efficiency by dynamically allocating computational resources \cite{wang2023dt}, and introducing synthetic data during training \cite{wang2020learning}.

\paragraph{Language Models For Formal Proof}
Formal theorem proving and proof repair offer an appealing domain for unlocking the reasoning potential of LLMs, with proofs being easier to verify rigorously without hallucination\cite{yang2023leandojo}, in both mathematical theorem proving and formal program verification. Currently, Language models have shown capability in formal languages such as Isabelle\cite{jiang2022thor,wang2023lego,zhao2023decomposing} and Lean \cite{polu2022formal,han2021proof,yang2023leandojo}. Researchers have also explored various approaches to optimize automated theorem proving using large language models: employing retrieval-augmented assistance\cite{yang2023leandojo}, improving search efficiency by dynamically allocating computational resources \cite{wang2023dt}, predicting the progress of proofs\cite{huang2025leanprogress}, employing LLMs as copilots that assist humans in proving theorems\cite{song2024towards, kozyrev2024coqpilot}, and introducing synthetic data during training\cite{wang2020learning, xin2024deepseek, lin2025goedel, wu2024internlm2}. However, most of these efforts focus on mathematical domains rather than repository-level software verification, which is addressed by PoPilot.

\paragraph{Synthetic Data for Instruction Tuning}
Instruction fine-tuning improves LLMs' ability to follow instructions and relies on high-quality datasets \cite{zhou2024lima,wang2022self}. Since human-annotated datasets are costly \cite{instructgpt2022,kopf2024openassistant,zheng2024lmsyschat1mlargescalerealworldllm}, recent methods focus on LLM-generated instruction data \cite{wang2022self,gunasekar2023textbooks,wang2024codeclm,xu2024magpie}. Self-Instruct \cite{wang2022self} pioneered this approach, later extended by Alpaca \cite{alpaca} and Code Alpaca \cite{codealpaca}. Evol-Instruct \cite{xu2023wizardlm,ahn2024large} and Code Evol-Instruct \cite{luo2023wizardcoder} introduced multi-stage generation for better instruction diversity, though risks of reinforcing biases remain \cite{yu2024large}. OSS-INSTRUCT \cite{wei2024magicoder} and SelfCodeAlign \cite{wei2024selfcodealign} mitigate this by leveraging open-source data, while MultiPL-T \cite{cassano2024knowledge} enables cross-lingual instruction transfer.




% \subsection{Instruction-Tuning using LLM-Generate Dataset}

% Recently, research on synthesizing high-quality instruction fine-tuning data using various techniques—particularly leveraging LLMs to generate new instruction-response pairs—has shown promising results. Self-Instruct \cite{wang2022self} utilizes GPT-3 \cite{brown2020language} to generate fine-tuning data through a carefully curated few-shot prompt, enabling the model to iteratively fine-tune itself. Alpaca \cite{alpaca} and Code Alpaca \cite{codealpaca} improve this approach by using ChatGPT to construct larger fine-tuning datasets for general-purpose and coding-specific tasks, respectively.

% More advanced methods, such as Evol-Instruct \cite{xu2023wizardlm}, introduce multi-stage generation strategies to enhance the diversity and complexity of synthetic datasets, while WizardCoder \cite{luo2023wizardcoder} adapts this technique specifically for code generation. Recently, approaches like OSS-INSTRUCT \cite{wei2024magicoder} and SelfCodeAlign \cite{wei2024selfcodealign} further refine LLM-based instruction generation by leveraging open-source seed code snippets to create high-quality coding datasets.


% \subsection{Language Models For Code}
% The application of large language models (LLMs) for code generation has gained significant progress in recent years. Open-sourced models such as Qwen2.5-Coder\cite{hui2024qwen2}, and Deepseek-Coder\cite{guo2024deepseek} and closed-sourced model such as GPT-4o\cite{hurst2024gpt}, have demonstrated strong capabilities in code generation
% \cite{chen2021evaluating,austin2021program}, program repair\cite{xia2022less,xia2023automated,jin2023inferfix}, and more complex domain-specific software engineering tasks such as fix GitHub issues\cite{jimenez2023swe} and software testing\cite{deng2023large}. These models typically undergo two stages of training: \textbf{pre-training} and \textbf{post-training}. Pre-training involves training the base model on large-scale datasets containing natural language and code using various objectives\cite{radford2018improving}, allowing it to have basic natural language understanding, as well as learning code syntax, patterns, and common structures of different programming languages\cite{nijkamp2022codegen}. Post-training techniques such as instruction fine-tuning are then applied to further specialize the model for specific coding tasks. This stage typically uses a smaller, well-curated, and high-quality dataset with carefully structured instructions from the real world\cite{muennighoff2023octopack} or synthesized\cite{roziere2023code,luo2023wizardcoder,codealpaca}, enabling the model to better understand and follow task-specific prompts. 

% Besides instruction fine-tuning, other post-training methods such as reinforcement learning from human feedback have been employed to align models with human expertise, improving code coherence and correctness\cite{ouyang2022training,bai2022training}. While this progress has improved code generation for widely used languages such as Python and C++, functional proof-oriented programming languages like F* \cite{swamy2011secure} remain underrepresented in the existing training datasets. This limitation makes it challenging for general-purpose code models to effectively generate F* programs.