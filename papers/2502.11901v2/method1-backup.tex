\section{Background}


\section{Function-Level Dataset Collection}

As an SMT-guided language, F* enables convenient verification through execution by F* compiler. However, F*, like many other formal-verification related languages (verified Rust, Rocq(Coq), Lean etc.), are comparatively low resource.  In the-stack-v2~\cite{lozhkov2024starcoder2stackv2} containing over 3B files in 600+ programming and markup languages,  F* has only 29.6k entries (less than 0.002\%), much fewer than common programming languages like Python (80.6M entries, 2.95\%), Java (223M entries, 8.17\%).
%Due to the lack of available data, we generate lots of new data using LLMs,\sun{Never uss such type of language like lots of etc.} only integrating the verified ones in our dataset exploiting F*’s convenience in verification and increase the diversity of the dataset by incorporating relevant data from other languages to supplement it.
This scarcity implies both the lack of knowledge of the off-the-shelf pre-trained checkpoints and the insufficiency of existing resources to further train the model. We therefore synthesize new problems by prompting language models, controlling the quality by running F* code against the checker to retain only the correct code and increase the diversity of the dataset by incorporating relevant data from other languages to supplement it.

Additionally, F* is a dependently-typed language, where type definitions depend on values. This allows for more precise specification of program properties and invariants but also introduces complexity due to the need for intricate computations to determine type equality and detailed type reasoning. Programmers also often need to go back and forth while writing F* code. Therefore, enhancing the model's ability to iteratively refine code is crucial, which we achieve by including code repair tasks in our dataset. 

Moreover, since F* is primarily used for verifying program properties with SMT solvers, we integrate proof-oriented code completion tasks to align with this use case. By focusing on proof generation, we aim to strengthen the model’s capability to assist in formal reasoning and writing of complex proofs.

To summarize, we considered the following key challenges in our dataset collection.

\begin{itemize}
\item  \textbf{Data scarcity}: Addressing the low-resource scenario of F* by incorporating diverse instruction-tuning data from other languages to enhance model performance and by utilizing F*’s amenability in verification to generate large amounts of data.
\item \textbf{Programming complexity}: Tackling the potential need for iterative refinement due to the difficulties of F* programming, we introduce code repair tasks.
\item \textbf{Primary use case in formal verification}: Integrating proof-oriented code completion tasks to align with its role in SMT solver-based program verification. 
\end{itemize}



\subsection{Data Synthesis}
In this section, we describe our method for synthesizing high-quality F* training data. To generate diverse programming tasks, we construct instruction data from existing open-source code snippets inspired by OSS-Instruct~\cite{wei2024magicoder}. We select code models that outperform non-code models in terms of accuracy and quality for F* code generation and prompt them to create high-quality tasks from a variety of code examples.



\subsubsection{Task generation}
We included three types of tasks in our dataset: natural language based tasks, proof-oriented code completion tasks and code repair tasks. Below, we describe each task in detail.

\paragraph{Natural Langugage To Code Tasks}

We use code examples from the F* source code repository and GitHub OCaml ~\cite {ocamlgithub} as our seed corpus. \sun{add the source of the code seeds} The code snippets extracted from these source codes are diverse, correct and practical, inspiring LLM to generate high-quality and varied instructions and responses. The response code generated by the model is also required to be self-contained to facilitate the verification of code correctness and the generation of other types of tasks. 

\paragraph{Proof-oriented code completion tasks}
In proof-oriented code completion problems, we choose verified and self-contained code snippets from the responses models generated in all tasks. We then ask the model to generate code completion problems that require proving a specific property or function related to the given snippet in order to obtain problems and corresponding responses related to proofs. This part of data, with greater complexity, enhances the model’s ability to write proofs in F* and perform reasoning. Verified response codes can also be reused as snippets for future code completion tasks.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{latex/fig/code2codeillu.pdf}
    \caption{Function-level Proof-oriented programming example.}
    \label{fig:code2codeillu}
\end{figure}
\paragraph{Code repair tasks.}
It is difficult to generate correct code on the first try for many programming tasks, even for human. Instead of discarding the flawed code, they typically analyze the results and modify it to fix any errors~\cite{chen2023teachinglargelanguagemodels}. Therefore, It is crucial for LLMs to have the capability to repair code as well.  Given the complexity of writing F* programs, which often requires iterative refinement, models trained on synthetic datasets should be capable of repairing erroneous code effectively. Code repair pairs in our dataset are generated by prompting the model to fix the given incorrect code response, supplying both the erroneous code and the execution log obtained after verification. 


The verified instruction-response pairs are added to our dataset.

\subsubsection{Execution-based data evaluation and dataset filtering}\label{Execution-based data evaluation and dataset filtering}
As an SMT-guided language, F* can directly and accurately receive feedback from execution, enabling the identification of discrepancies between code behavior and formal specifications. Therefore, no extra effort such as test cases and LLM judges is required for verifying F* data. Specifically, to verify the response codes, we put the code snippet within the F* environment and execute them, sparing little effort. Data that successfully compile and run are retained, while incorrect ones and their associated error messages are stored for inclusion in the repair dataset. This verifiability is a favorable property of F* that we could leverage to obtain data quality signals for free, whereas in general domains, the instruction tuning data is largely unverifiable or relies on heuristics~\cite{wei2024selfcodealign}. 

\subsection{Diversification} \label{Diversification}
Diverse instructions can better enhance an LLM’s ability to generalize to new tasks ~\cite{wei2022finetunedlanguagemodelszeroshot}, as well as improve its comprehension and adherence to instructions ~\cite{chen2024diversitysyntheticdataimpact,zhang2024textbfonlyifrevealingdecisiveeffectinstruction,dong2024abilitieslargelanguagemodels}. This is particularly crucial in our case since the F* community is smaller than common programming languages with scarcer source codes and more sparse documentation. Therefore, we integrate diverse instruction-tuning data pairs in other languages besides synthetic F* data to supplement our dataset and enhance model's capability. This general approach is applicable beyond F*, as it can be extended to other programming languages with limited available data, helping to improve model performance in low-resource scenarios.

\subsection{Dataset Analysis}
