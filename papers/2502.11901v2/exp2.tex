\section{Project-Level Proof Synthesis}
In this section, we present the experiments on project-level proof synthesis tasks.
\subsection{Training}

\paragraph{Training Dataset}     
Both the definition generation and repair datasets are formatted using predefined prompt templates, which are listed in Appendix \ref{sec:appendix}. The prompt settings for definition generation and repair are listed in \ref{sec:prompt construction setting}.

Next we integrated the formatted existing and repository-level definition generation data, model-generated and synthetic proof repair data, and the mixed synthetic Function-Level data, and prepared different data mixtures to dine the model, allowing us to explore the influence of each dataset and develop a best training corpus. 

We detail the training set-up for \name in \ref{sec:Model-generate Data Experiment Setting Experiment settings}. 

\subsection{Validation}

After expanding the problem-solution pairs for both definition proof and repair tasks, we apply execution-based validation (detail in \ref{Execution-based data evaluation and dataset filtering}) to filter out incorrect solutions and store error messages for the repair dataset and enhance the diversity of the data. Since our data generation process involves multiple LLMs and stages, and diversity is limited by the fixed number of seed contexts and reference examples, we apply an additional de-duplication step. Specifically, we limit each unique type declaration to at most two instances in the repair dataset, ensuring diversity while minimizing redundancy.


\subsection{\name Auto-pilots and Co-pilots}
% \wang{Please make the experiment settings very clear.Refer to https://arxiv.org/pdf/2405.01787 for experiment set-up. }

In this section, we evaluate the supervised fine-tuned models using different training data mixtures as well as baseline models on the task of proof generation and proof repair. Our evaluation data is a random-sampled, held-out test set with 1K repository -level definition proof problems. The validation process consists of the following three stages:
\begin{table*}[]
% \footnotesize

\setlength{\tabcolsep}{3pt} % Reduce column spacing
\centering
\small
\begin{tabular}{@{}l|c c c c@{}}
\toprule
\textbf{Baseline Models}                                     & \textbf{Generate@5} & \textbf{Repair@5} & \textbf{Gen+Rep (Total 10)}& \textbf{Generate@10} \\ \midrule
\textsc{Qwen2.5-Coder-7B-Instruct}                     & 23.4  & 0.2   & 23.6    & 26.3   \\
\textsc{Qwen2.5-Coder-14B-Instruct}                     & 24  & 0.4   & 24.4    & 26.9   \\
\textsc{Qwen2.5-Coder-32B-Instruct}                     & 24.2  & 2.5   & 26.7    & 27.1   \\
\textsc{Deepseek-Coder-V2-LITE-Instruct}                    & 24.4  & 0.7   & 25.1    & 25.1   \\
\textsc{Deepseek-V3}                    & 18.7  & 3.6   & 22.3    & 28.6   \\
\textsc{Deepseek-Coder-33B-Instruct}                    & 22.3  & 4.6   & 26.9    & 28.8   \\
\textsc{GPT-4o}                                       & 22.2  & 1.7   & 23.9    &    23.8    \\
\textsc{Qwen2.5-72B-Instruct}                        & 23.4  & 3.0   & 26.4    & 25.8   \\
\textsc{Llama-3.3-70B-Instruct-Turbo}                  &  19.6     &   3.9    &    23.5     &      21.6 \\
\textsc{Llama-3.1-70B}                  &  19.3     &   2.3    &    21.6     &      22.4\\ \midrule
\multicolumn{5}{l}{\textbf{Data Mixture}} \\ \midrule
\textit{Existing Repos}                                  & 30.7  & 1.0   & 31.7    & 35.3   \\
\hspace{3mm}+ Syn. Project Proof                                   & 32.2  & 2.2   & 34.4    & 36.2   \\
\hspace{3mm} + Func + Syn. Project Proof                          & 32.8  & 2.7   & 35.5    & 37.8   \\
\hspace{3mm}+ Syn. Project Proof  + Syn. Repair                          & 32.7  & 0.7   & 33.4    & 37.5   \\
\hspace{3mm}+ Syn. Project Proof  + Model Repair                          & 33.1  & 4.2   & 37.3    & 37.2   \\
\hspace{3mm}+ Syn. Project Proof  + All Repair                            & \textbf{34.0}  & 4.7   & 38.7    & 38.0   \\ 
\hspace{3mm} \textsc{\name}          & \cellcolor{cyan!10}33.0  & \cellcolor{cyan!10}\textbf{6.4}   & \cellcolor{cyan!10}\textbf{39.4}    & \cellcolor{cyan!10}\textbf{38.5}   \\ 
\bottomrule
\end{tabular}
\caption{Performance comparison of baseline models and fine-tuning data configurations. \textbf{Existing Repos}: 30K existing repository level definition + proofs from the seed dataset; \textbf{Syn. Project Proof} : 30K model generated new definitions + proofs as described in \ref{Generating New Problems}; \textbf{Func}: synthetic simple questions mixed with other datasets in \ref{synth simple}; \textbf{Syn. Repair}: 30K synthetic repair data in \ref{Synthetic Repair Data}, \textbf{Model Repair}: 30K model-generarted repair data in \ref{Repair Data From The Model}; \textbf{All Repair}: Syn. Repair + Model Repair; \textbf{\name}: Existing Repos + Syn. Project Proof + All Repair + 180K mixed function-level coding data used to finetune the best performance in Table \ref{tab:syntheticresults}}
\label{tab:results}
\end{table*}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/gen_repair_comparison_updated.pdf}
    \caption{\name repairing failed outputs for state-of-the-art models.}
    \label{fig:repair_result}
\end{figure}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{@{}c|c c|c|c|c@{}}
% \toprule
% \textbf{Model} & \textbf{Gen@5} & \textbf{Rep@5} & \textbf{Gen@5 + Rep@5} & \textbf{Ours Rep@5} & \textbf{Gen@5 + Ours Rep@5} \\ \midrule
% \textsc{Qwen2.5-Coder-32B-Ins}  & 23.5  & +0.8   &  24.3  & \cellcolor{cyan!10} 7.8  &  \cellcolor{cyan!10} 31.3 \\
% \textsc{DS-Coder-33B-Ins}       & 22.3  & +4.6   &  26.9  & \cellcolor{cyan!10} 9.8  &  \cellcolor{cyan!10} 32.1 \\
% \textsc{GPT-4o}                 & 22.2  & +1.7   &  23.9  & \cellcolor{cyan!10} 14.7 &  \cellcolor{cyan!10} \textbf{36.9} \\
% \textsc{Qwen2.5-72B-Ins}        & 23.4  & +3.0   &  26.4  & \cellcolor{cyan!10} 11.6 &  \cellcolor{cyan!10} 35.0 \\
% \textsc{Llama-3.3-70B-Ins}      & 19.6  & +3.9   &  23.5  & \cellcolor{cyan!10} 8.7  &  \cellcolor{cyan!10} 28.3 \\ 
% \bottomrule
% \end{tabular}
% \caption{Repair}
% \label{tab:gen_repair_comparison}
% \end{table*}



\paragraph{Definition Generation and Self-Repair:}\label{Definition Generation and Self-Repair:} The model generates F* proofs given a type declaration and context (see example prompt in \ref{sec:appendix}), sampling either 5 times followed by 5 repair attempts (Sample-5 + Repair-5) or 10 times directly (Sample-10). If at least one proof compiles, the problem is solved. Otherwise, for the Sample-5 group, failed attempts and error messages are stored for repair, where the model randomly selects an incorrect response and generates 5 additional repair attempts to fix the incorrect proofs. This setup allows us to compare whether self-repairing incorrect generations improves success rates over simply increasing the sampling budget.

\paragraph{Repair Using Outputs from Other Models:} To further assess model performance, we use incorrect proofs generated by a baseline model as input to our fine-tuned model. This allows us to evaluate whether fine-tuning enhances the modelâ€™s ability to generalize and improve proof repair across different model outputs.


\subsection{Result}

We evaluate fine-tuning performance using \textbf{Generate@5} and \textbf{Repair@5} metrics. Generate@5 represents the correctness rate when sampling the model five times on the proof generation task, while Repair@5 measures the number of additional questions correctly repaired after sampling five repair attempts on incorrect solutions from the previous stage. As shown in Table \ref{tab:results}, fine-tuning \textsc{Qwen2.5-Coder-14B} on our generated dataset outperforms five larger state-of-the-art coding LLMs in both repository-level proof generation and self-repair tasks.

\paragraph{Proof Repairing Ability:} We use the model fine-tuned with the data mixture Existing Repository-Level Definitions + Synthetic Project Proofs + All Repair dataset (see Table \ref{tab:results}) to repair incorrect outputs from the baseline models. The results in Figure \ref{fig:repair_result} demonstrate its effectiveness in repairing out-of-distribution incorrect proofs. Notably, our fine-tuned model surpasses all baseline large models in correcting their own incorrect answers, bringing a significant improvement in the total number of correct solutions. This suggests a possible application where a fine-tuned, smaller model serves as a debugging assistant for larger models, allowing for potentially efficient co-serving of these powerful models to efficiently adapt to the domain of proof oriented programming.

\paragraph{Data Mixture Analysis:} Table \ref{tab:results} shows how different fine-tuning data mixtures impact proof generation and repair. (1) Augmenting the existing repository-level set with model-generated definitions significantly improves Generate@5 and Repair@5 accuracy, demonstrating the effectiveness of our generated definitions. (2) Rule-based synthetic repair data alone does not enhance repair performance, suggesting that rule-based errors may not accurately reflect the error situations the model would have encountered. (3) Model-generated repair data improves both repair accuracy (from 32.2 to 33.1) and generation accuracy (from 2.7 to 4.2), indicating that language-model-produced errors and repairs can better capture real-world failure patterns than the synthetic ones, that are largely synthetic, generated by mutating ASTs of the program. (4) \name trained with \textbf{Existing Repos + Syn. Project Proof + All Repair + Mixed Function-Level Data} -- our most diverse mixture of data points -- gives the highest Gen+Rep (39.4) and Generate@10 (38.5) across the board, confirming that diverse data improves both proof generation and repair. The function-level synthetic data for F* and mixture from other languages well complement the repository-level F* verification and repair data, and boosts the model's performance.  



