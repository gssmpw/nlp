\section{Project-Level Dataset Synthesis}





In this section, we describe how we synthesize more project-level proof generation problems from existing repositories. Project-level verification involves generating or repairing proofs for definitions embedded within complete verification repositories, where correctness depends not only on local function logic but also on broader module-level contextâ€”including previously established lemmas, type invariants, module imports, and shared assumptions. Our goal is to generate new problem-solution pairs based on existing programming contexts where the \textbf{problem} consists of a type declaration, and the \textbf{solution} is the correct F* definition (a proof) that satisfies the given type declaration (See examples in \ref{sec:Problem-Solution Pair Examples}).

\subsection{Generating New Problems} \label{Generating New Problems}

We start the problem-solution pairs generation from a seed dataset, in which each instance has the following structure:


    \paragraph{Definition}An initial problem-solution pair of a definition in the context. 
    \paragraph{Context} Required context information from existing repositories, such as opened premises and pre-defined definitions, and selected premises which are likely to be used in the body of the definitions\cite{yang2023leandojo}.
    \paragraph{Examples} A set of semantically similar problem-solution pairs retrieved from the same context using the similarity between types in their embedding space\cite{chakraborty2024towards}.


We prompt the language model to create new definitions or prove new properties based on the context in the seed dataset. The detail is listed as follows:
% \wang{Refer to the microsoft paper.be concrete on what we did. e.g. when we say we "obtain", we "retrieve" with "xxx" approach}
% \begin{itemize}

%     \item \textbf{Generation Prompt Curation:} The prompt template provides all relevant premises and pre-defined definitions as building blocks, enabling the model to construct new definitions and their corresponding proofs within the given context. To ensure that the model follows the format and structural conventions of F* definitions and proofs, we include multiple example definitions with varying structures retrieved from the same context as references. Additionally, the prompt is designed to encourage diversity in generated definitions while discouraging direct copying from the provided examples.
    
%     \item \textbf{Definition Generation:} For each unique context in the seed dataset, we format a prompt using the predefined template and sample two powerful LLMs multiple times to generate a set of candidate problem-solution pairs specific to that context. 

%     \item \textbf{Data Filtering:} To ensure data quality and prevent redundancy, we apply a de-duplication strategy after generating multiple problem-solution pairs for each context. (1) We compute the sequence similarity between generated definitions within the same context set and filter out those exceeding a predefined threshold to maintain diversity, while also filtering out pairs that resemble the reference examples. (2) Additionally, we remove any generated definitions that overlap with the test set to prevent data leakage and ensure a fair evaluation. 
% \end{itemize}

% Through these three steps, we expand the size of diverse problem-solution pairs for a fixed size of seed contexts. This ensures that the generated data remains both representative of real-world F* usage and easy to validate. 

\paragraph{Generation Prompt Curation:} We structure the prompt with relevant premises and pre-defined definitions, providing essential context for generating new definitions and proofs. (Prompt see \ref{sec:New Definition Prompt Example}). To guide the model in following F* conventions, we retrieve multiple example definitions with varied structures from the same context while ensuring diversity and discouraging direct copying.

\paragraph{Definition Generation:} For each unique context in the seed dataset, we apply the predefined prompt template and sample multiple candidate problem-solution pairs using two LLMs.

\paragraph{Data Filtering:} To maintain quality and prevent redundancy, we apply de-duplication by (1) computing sequence similarity and filtering out overly similar definitions, including those resembling reference examples, and (2) remove any generated definitions that overlap with the test set to prevent data leakage and ensure a fair evaluation. 

These steps expand the dataset with diverse problem-solution pairs while maintaining real-world F* relevance and validation feasibility. Figure \ref{fig:length} shows that the problem-solution pair augmentation is effective for simpler definitions but struggles with longer and more complex ones, reflecting the long-tailed distribution observed in both real-world and synthesized datasets. This alignment suggests that our synthetic data captures the difficulty distribution in real F* development.


\begin{figure}
    \centering
    \includegraphics[width=.75\linewidth]{fig/ouput.pdf}
    \caption{Length Comparison between Generated Definitions vs Existing Definitions.}
    \label{fig:length}
\end{figure}

\subsection{Creating Repair Data}

In this section, we generate new problem-solution pairs for the proof repair task. The \textbf{problem} consists of a type declaration, an incorrect proof, and the corresponding error message from the F* compiler, while the \textbf{solution} is the corrected proof. To construct this dataset, we combine rule-based data synthesis with LLM-generated repair data.

\subsubsection{Synthetic Repair Data} \label{Synthetic Repair Data}
We generate a synthetic mutation dataset from the F* dataset by randomly modifying ground-truth solutions at the abstract syntax tree level. If a mutation causes type checking to fail, we treat it as a synthetic error and use it to train a model to recover the original solution. Mutations include omitting parts, replacing arguments with underscores, and modifying control structures (e.g., removing branches or let-definitions). These errors mimic those commonly made by human F* programmers, but we avoid mutating identifiers.
\subsubsection{Repair Data From The Model}\label{Repair Data From The Model}


%  Since in section \ref{Generating New Problems} we expand the set of problems, we now prompt the model to attempt solutions for these problems and their corresponding context, obtaining incorrect proofs and their associated error messages. To collect correct answers for the repair task, we either use the original correct proofs or prompt the LLMs to generate new valid proofs.

% \begin{itemize}
%     \item \textbf{Repair Problems Generation:} We merge the initial problems from the seed dataset with the generated problems and prompt the LLM to solve them. The solutions are then validated (as detailed in \ref{Diversification}), and the incorrect solutions with their corresponding error messages are collected as new proof repair problems.

%     \item \textbf{Obtaining Correct Repairs:} We apply two approaches to obtain the correct repairs for the generated proof repair problems. (1) We prompt LLMs to directly solve the proof repair problems generated in the previous step. However, since proof repair is a particularly challenging task for LLMs without fine-tuning, we can only get limited correct answers by solving it in a zero-shot setting. (2) To address this, we reuse the original solutions from the definition generation task as the answers for the corresponding repair problems. 

%     \item \textbf{Data Filtering:} (1) We remove duplicate repair problems by filtering out identical incorrect proofs. (2) For each original definition generation problem, we keep at most three repair problems to prevent excessive repetition. (3) Finally, we remove any correct answers that appear in the test set to ensure fair evaluation.

% \end{itemize}

Since Section \ref{Generating New Problems} expands the problem set, we now prompt the model to solve these problems within their given contexts, collecting incorrect proofs and their corresponding error messages. Correct answers for the repair task are retrieved either from the original correct proofs or by prompting LLMs to generate new valid proofs.


\paragraph{Repair Problems Generation:} We combine the seed dataset problems with the generated ones and prompt the LLM to solve them. Solutions are validated (\ref{Diversification}), and incorrect proofs with error messages are collected from the compiler as repair problems.

\paragraph{Obtaining Correct Repairs:} (1) We prompt LLMs to solve the repair problems directly, though zero-shot performance is often limited. (2) Alternatively, we reuse the original correct proofs from the definition generation task as repair solutions.

\paragraph{Data Filtering:} (1) Duplicate repair problems are removed by filtering identical incorrect proofs. (2) Each definition generation problem contributes at most three repair problems to prevent redundancy. (3) Correct answers appearing in the test set are removed to ensure fair evaluation.

We can see that the dominating model-generated errors is \textit{identifier not found} and \textit{syntax error}. While syntax errors reflect model's limited understanding of F* grammar, identifier not found errors indicate deeper semantic and type-related challenges that are characteristic of F* language.

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=1.06\linewidth]{fig/error_types.pdf}
    \caption{Distribution of Top 10 Error Types of Model-Generated Repair Data\kexin{if the actual number of the distribution is helpful, maybe use a table - smaller pies here are not very clear which one has higher \%}}
    \label{fig:error-distribution}
\end{figure}
\fi
