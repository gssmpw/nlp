\section{Background}
\begin{figure*}[h]
    \centering
    \includegraphics[width=.8\linewidth]{fig/figure.pdf}
    \caption{Illustration of repository-level data generation pipeline.}
    \label{fig:pipline}
\end{figure*}
Formal verification is a process of examining the correctness of the operation of software programs by mathematical proof~\cite{digitalsystem}. F* is an SMT-solver based proof-oriented language that enables convenient verification through execution by F* compiler. However, F*, like many other languages used for formal proofs (verified Rust, Rocq(Coq), Lean etc.), are comparatively low resource.  The popular open coding data corpus Stack-v2~\cite{lozhkov2024starcoder} containing over 3B files in 600+ programming and markup languages,  F* has only 29.6k entries (less than 0.002\%), much fewer than common programming languages like Python (80.6M entries, 2.95\%), Java (223M entries, 8.17\%). This scarcity implies both the lack of knowledge of the off-the-shelf pre-trained checkpoints and the insufficiency of existing resources to further train the model. 

Additionally, F* is a dependently-typed language, where type definitions depend on values. This allows for more precise specification of program properties and invariants but also introduces complexity due to the need for intricate computations to determine type equality and detailed type reasoning \cite{chakraborty2024towards}. Programmers also often need to go back and forth while writing F* code. Therefore, enhancing the model's ability to repair code is crucial for both iterative improvement of automatic proof synthesis and better assistance to human programmers.

% In reality, proof-oriented programming is often adopted on a project level to verify properties that has complex cross-function and even cross-file dependencies. This, together with the type-dependent nature of the programming language, makes the task extremely difficult to learn. \wang{background from Microsoft paper}

In reality, the challenge of proof-oriented programming is further exacerbated by the cross-repository dependencies of the code. Proof-oriented programming often spans multiple repositories, especially in the large-scale formal verification of software systems, where the project contains different components relying on verifying properties from other repositories. This introduces huge challenges in dependency and environment management, as the proofs account for resolving inter-repository specification and module openings beyond the single repository \cite{chakraborty2024towards}. This property, together with the type-dependent nature of the programming language, makes the task difficult to learn for model and even human expertise. 
\section{Function-Level Dataset Collection}
In this section, we describe how we synthesize diverse function-level programming tasks from existing open-source code snippets. We focus on three types of tasks: natural language to code tasks, proof-oriented code completion tasks, and code repair tasks. To evaluate the generated data, we rely on F*’s feature to automatically verify code correctness via execution. In addition, the dataset is diversified by incorporating instruction-tuning data from other languages.


% Moreover, since F* is primarily used for verifying program properties with SMT solvers, we integrate proof-oriented code completion tasks to align with this use case. By focusing on proof generation, we aim to strengthen the model’s capability to assist in formal reasoning and writing of complex proofs.

% To summarize, we considered the following key challenges in our dataset collection.

% \begin{itemize}
% \item  \textbf{Data scarcity}: Addressing the low-resource scenario of F* by incorporating diverse instruction-tuning data from other languages to enhance model performance and by utilizing F*’s amenability in verification to generate large amounts of data.
% \item \textbf{Programming complexity}: Tackling the potential need for iterative refinement due to the difficulties of F* programming, we introduce code repair tasks.
% \item \textbf{Primary use case in formal verification}: Integrating proof-oriented code completion tasks to align with its role in SMT solver-based program verification. 
% \end{itemize}



\subsection{Data Synthesis}
In this section, we describe our method for synthesizing high-quality F* training data. To generate diverse programming tasks, we construct instruction data from existing open-source code snippets inspired by OSS-Instruct~\cite{wei2024magicoder}. We select code models that outperform non-code models in terms of accuracy and quality for F* code generation and prompt them to create high-quality tasks from a variety of code examples.



\subsubsection{Task generation}
We included three types of tasks in our dataset: natural language based tasks, proof-oriented code completion tasks and code repair tasks. Below, we describe each task in detail.

\paragraph{Natural Langugage To Code Tasks}

We use code examples from the F* source code repository and GitHub OCaml ~\cite{ocamlgithub} as our seed corpus.The code snippets extracted from these source codes are diverse, correct and practical, inspiring LLM to generate high-quality and varied instructions and responses. The response code generated by the model is also required to be self-contained to facilitate the verification of code correctness and the generation of other types of tasks. 

\paragraph{Proof-oriented code completion tasks}
In proof-oriented code completion problems, we choose verified and self-contained code snippets from the responses models generated in all tasks. We then ask the model to generate code completion problems that require proving a specific property or function related to the given snippet in order to obtain problems and corresponding responses related to proofs. This part of data, with greater complexity, enhances the model’s ability to write proofs in F* and perform reasoning. Verified response codes can also be reused as snippets for future code completion tasks.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{fig/code2codeillu.pdf}
    \caption{Function-level Proof-oriented programming example.}
    \label{fig:code2codeillu}
\end{figure}
\paragraph{Code repair tasks.}
It is difficult to generate correct code on the first try for many programming tasks, even for human. Instead of discarding the flawed code, they typically analyze the results and modify it to fix any errors~\cite{chen2023teaching}. Therefore, It is crucial for LLMs to have the capability to repair code as well.  Given the complexity of writing F* programs, which often requires iterative refinement, models trained on synthetic datasets should be capable of repairing erroneous code effectively. Code repair pairs in our dataset are generated by prompting the model to fix the given incorrect code response, supplying both the erroneous code and the execution log obtained after verification. 


The verified instruction-response pairs are added to our dataset.

\subsubsection{Execution-based data evaluation and dataset filtering}\label{Execution-based data evaluation and dataset filtering}
As an SMT-guided language, F* can directly and accurately receive correctness feedback from execution, enabling the identification of discrepancies between code behavior and formal specifications. Therefore, no extra effort such as test cases and LLM judges is required for verifying F* data. Specifically, to verify the response codes, we put the code snippet within the F* environment and execute them, sparing little effort. Data that successfully compile and run are retained, while incorrect ones and their associated error messages are stored for inclusion in the repair dataset. This verifiability is a favorable property of F* that we could leverage to obtain data quality signals for free, whereas in general domains, the instruction tuning data is largely unverifiable or relies on heuristics~\cite{wei2024selfcodealign}. 

\subsection{Diversification} \label{Diversification}
Diverse instructions can better enhance an LLM’s ability to generalize to new tasks \cite{wei2021finetuned}, as well as improve its comprehension and adherence to instructions ~\cite{chen2024diversity,zhang2024textbf,dong2023abilities}. This is particularly crucial in our case since the F* community is smaller than common programming languages with scarcer source codes and more sparse documentation. Therefore, we integrate diverse instruction-tuning data pairs in other languages besides synthetic F* data to supplement our dataset and enhance model's capability. This general approach is applicable beyond F*, as it can be extended to other programming languages with limited available data, helping to improve model performance in low-resource scenarios.

% \subsection{Dataset Analysis}
