[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhou2023limaalignment",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "selfinstruct",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "instructgpt2022",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "k\u00f6pf2023openassistantconversationsdemocratizing",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "zheng2024lmsyschat1mlargescalerealworldllm",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang",
        "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "selfinstruct",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "gunasekar2023textbooksneed",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "wang2024codeclmaligninglanguagemodels",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "xu2024magpiealignmentdatasynthesis",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "selfinstruct",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "codealpaca",
        "author": "Sahil Chaudhary",
        "title": "Code Alpaca: An Instruction-following LLaMA model for code generation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xu2023wizardlmempoweringlargelanguage",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wizardcoder",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yu2023largelanguagemodelattributed",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wei2024magicoder",
        "author": "Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming",
        "title": "Magicoder: Empowering code generation with oss-instruct"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cassano2024knowledgetransferhighresourcelowresource",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chen2021evaluating",
        "author": "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others",
        "title": "Evaluating large language models trained on code"
      },
      {
        "key": "austin2021program",
        "author": "Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others",
        "title": "Program synthesis with large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xia2022less",
        "author": "Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Less training, more repairing please: revisiting automated program repair via zero-shot learning"
      },
      {
        "key": "xia2023automated",
        "author": "Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming",
        "title": "Automated program repair in the era of large pre-trained language models"
      },
      {
        "key": "jin2023inferfix",
        "author": "Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",
        "title": "Inferfix: End-to-end program repair with llms"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jimenez2023swe",
        "author": "Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik",
        "title": "Swe-bench: Can language models resolve real-world github issues?"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "deng2023large",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "hui2024qwen2",
        "author": "Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others",
        "title": "Qwen2. 5-coder technical report"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "guo2024deepseek",
        "author": "Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "hurst2024gpt",
        "author": "Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others",
        "title": "Gpt-4o system card"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "radford2018improving",
        "author": "Radford, Alec",
        "title": "Improving language understanding by generative pre-training"
      },
      {
        "key": "nijkamp2022codegen",
        "author": "Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming",
        "title": "Codegen: An open large language model for code with multi-turn program synthesis"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "muennighoff2023octopack",
        "author": "Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne",
        "title": "Octopack: Instruction tuning code large language models"
      },
      {
        "key": "roziere2023code",
        "author": "Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others",
        "title": "Code llama: Open foundation models for code"
      },
      {
        "key": "luo2023wizardcoder",
        "author": "Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin",
        "title": "Wizardcoder: Empowering code large language models with evol-instruct"
      },
      {
        "key": "codealpaca",
        "author": "Sahil Chaudhary",
        "title": "Code Alpaca: An Instruction-following LLaMA model for code generation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "swamy2011secure",
        "author": "Swamy, Nikhil and Chen, Juan and Fournet, C{\\'e}dric and Strub, Pierre-Yves and Bhargavan, Karthikeyan and Yang, Jean",
        "title": "Secure distributed programming with value-dependent types"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "yang2023leandojo",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "jiang2022thor",
        "author": "Jiang, Albert Qiaochu and Li, Wenda and Tworkowski, Szymon and Czechowski, Konrad and Odrzyg{\\'o}{\\'z}d{\\'z}, Tomasz and Mi{\\l}o{\\'s}, Piotr and Wu, Yuhuai and Jamnik, Mateja",
        "title": "Thor: Wielding hammers to integrate language models and automated theorem provers"
      },
      {
        "key": "wang2023lego",
        "author": "Wang, Haiming and Xin, Huajian and Zheng, Chuanyang and Li, Lin and Liu, Zhengying and Cao, Qingxing and Huang, Yinya and Xiong, Jing and Shi, Han and Xie, Enze and others",
        "title": "Lego-prover: Neural theorem proving with growing libraries"
      },
      {
        "key": "zhao2023decomposing",
        "author": "Zhao, Xueliang and Li, Wenda and Kong, Lingpeng",
        "title": "Decomposing the enigma: Subgoal-based demonstration learning for formal theorem proving"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "polu2022formal",
        "author": "Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya",
        "title": "Formal mathematics statement curriculum learning"
      },
      {
        "key": "han2021proof",
        "author": "Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W and Polu, Stanislas",
        "title": "Proof artifact co-training for theorem proving with language models"
      },
      {
        "key": "yang2023leandojo",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "yang2023leandojo",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "wang2023dt",
        "author": "Wang, Haiming and Yuan, Ye and Liu, Zhengying and Shen, Jianhao and Yin, Yichun and Xiong, Jing and Xie, Enze and Shi, Han and Li, Yujun and Li, Lin and others",
        "title": "Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "wang2020learning",
        "author": "Wang, Mingzhe and Deng, Jia",
        "title": "Learning to prove theorems by learning to generate theorems"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "yang2023leandojo",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "jiang2022thor",
        "author": "Jiang, Albert Qiaochu and Li, Wenda and Tworkowski, Szymon and Czechowski, Konrad and Odrzyg{\\'o}{\\'z}d{\\'z}, Tomasz and Mi{\\l}o{\\'s}, Piotr and Wu, Yuhuai and Jamnik, Mateja",
        "title": "Thor: Wielding hammers to integrate language models and automated theorem provers"
      },
      {
        "key": "wang2023lego",
        "author": "Wang, Haiming and Xin, Huajian and Zheng, Chuanyang and Li, Lin and Liu, Zhengying and Cao, Qingxing and Huang, Yinya and Xiong, Jing and Shi, Han and Xie, Enze and others",
        "title": "Lego-prover: Neural theorem proving with growing libraries"
      },
      {
        "key": "zhao2023decomposing",
        "author": "Zhao, Xueliang and Li, Wenda and Kong, Lingpeng",
        "title": "Decomposing the enigma: Subgoal-based demonstration learning for formal theorem proving"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "polu2022formal",
        "author": "Polu, Stanislas and Han, Jesse Michael and Zheng, Kunhao and Baksys, Mantas and Babuschkin, Igor and Sutskever, Ilya",
        "title": "Formal mathematics statement curriculum learning"
      },
      {
        "key": "han2021proof",
        "author": "Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W and Polu, Stanislas",
        "title": "Proof artifact co-training for theorem proving with language models"
      },
      {
        "key": "yang2023leandojo",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "yang2023leandojo",
        "author": "Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree",
        "title": "Leandojo: Theorem proving with retrieval-augmented language models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "wang2023dt",
        "author": "Wang, Haiming and Yuan, Ye and Liu, Zhengying and Shen, Jianhao and Yin, Yichun and Xiong, Jing and Xie, Enze and Shi, Han and Li, Yujun and Li, Lin and others",
        "title": "Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "huang2025leanprogress",
        "author": "Huang, Suozhi and Song, Peiyang and George, Robert Joseph and Anandkumar, Anima",
        "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "song2024towards",
        "author": "Song, Peiyang and Yang, Kaiyu and Anandkumar, Anima",
        "title": "Towards large language models as copilots for theorem proving in lean"
      },
      {
        "key": "kozyrev2024coqpilot",
        "author": "Kozyrev, Andrei and Solovev, Gleb and Khramov, Nikita and Podkopaev, Anton",
        "title": "CoqPilot, a plugin for LLM-based generation of proofs"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "wang2020learning",
        "author": "Wang, Mingzhe and Deng, Jia",
        "title": "Learning to prove theorems by learning to generate theorems"
      },
      {
        "key": "xin2024deepseek",
        "author": "Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan",
        "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data"
      },
      {
        "key": "lin2025goedel",
        "author": "Lin, Yong and Tang, Shange and Lyu, Bohan and Wu, Jiayun and Lin, Hongzhou and Yang, Kaiyu and Li, Jia and Xia, Mengzhou and Chen, Danqi and Arora, Sanjeev and others",
        "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving"
      },
      {
        "key": "wu2024internlm2",
        "author": "Wu, Zijian and Huang, Suozhi and Zhou, Zhejian and Ying, Huaiyuan and Wang, Jiayu and Lin, Dahua and Chen, Kai",
        "title": "Internlm2. 5-stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "zhou2024lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      },
      {
        "key": "wang2022self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "instructgpt2022",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "kopf2024openassistant",
        "author": "K{\\\"o}pf, Andreas and Kilcher, Yannic and von R{\\\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\\'a}rd and others",
        "title": "Openassistant conversations-democratizing large language model alignment"
      },
      {
        "key": "zheng2024lmsyschat1mlargescalerealworldllm",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang",
        "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      },
      {
        "key": "gunasekar2023textbooks",
        "author": "Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others",
        "title": "Textbooks are all you need"
      },
      {
        "key": "wang2024codeclm",
        "author": "Wang, Zifeng and Li, Chun-Liang and Perot, Vincent and Le, Long T and Miao, Jin and Zhang, Zizhao and Lee, Chen-Yu and Pfister, Tomas",
        "title": "CodecLM: Aligning Language Models with Tailored Synthetic Data"
      },
      {
        "key": "xu2024magpie",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen",
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "alpaca",
        "author": "Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto ",
        "title": "Stanford Alpaca: An Instruction-following LLaMA model"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "codealpaca",
        "author": "Sahil Chaudhary",
        "title": "Code Alpaca: An Instruction-following LLaMA model for code generation"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "xu2023wizardlm",
        "author": "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin",
        "title": "Wizardlm: Empowering large language models to follow complex instructions"
      },
      {
        "key": "ahn2024large",
        "author": "Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng",
        "title": "Large language models for mathematical reasoning: Progresses and challenges"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "luo2023wizardcoder",
        "author": "Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin",
        "title": "Wizardcoder: Empowering code large language models with evol-instruct"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "yu2024large",
        "author": "Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao",
        "title": "Large language model as attributed training data generator: A tale of diversity and bias"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "wei2024magicoder",
        "author": "Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming",
        "title": "Magicoder: Empowering code generation with oss-instruct"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "wei2024selfcodealign",
        "author": "Wei, Yuxiang and Cassano, Federico and Liu, Jiawei and Ding, Yifeng and Jain, Naman and Mueller, Zachary and de Vries, Harm and Von Werra, Leandro and Guha, Arjun and Zhang, Lingming",
        "title": "Selfcodealign: Self-alignment for code generation"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "cassano2024knowledge",
        "author": "Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun",
        "title": "Knowledge transfer from high-resource to low-resource programming languages for code llms"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "alpaca",
        "author": "Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto ",
        "title": "Stanford Alpaca: An Instruction-following LLaMA model"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "codealpaca",
        "author": "Sahil Chaudhary",
        "title": "Code Alpaca: An Instruction-following LLaMA model for code generation"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "xu2023wizardlm",
        "author": "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin",
        "title": "Wizardlm: Empowering large language models to follow complex instructions"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "luo2023wizardcoder",
        "author": "Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin",
        "title": "Wizardcoder: Empowering code large language models with evol-instruct"
      }
    ]
  },
  {
    "index": 52,
    "papers": [
      {
        "key": "wei2024magicoder",
        "author": "Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming",
        "title": "Magicoder: Empowering code generation with oss-instruct"
      }
    ]
  },
  {
    "index": 53,
    "papers": [
      {
        "key": "wei2024selfcodealign",
        "author": "Wei, Yuxiang and Cassano, Federico and Liu, Jiawei and Ding, Yifeng and Jain, Naman and Mueller, Zachary and de Vries, Harm and Von Werra, Leandro and Guha, Arjun and Zhang, Lingming",
        "title": "Selfcodealign: Self-alignment for code generation"
      }
    ]
  },
  {
    "index": 54,
    "papers": [
      {
        "key": "hui2024qwen2",
        "author": "Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others",
        "title": "Qwen2. 5-coder technical report"
      }
    ]
  },
  {
    "index": 55,
    "papers": [
      {
        "key": "guo2024deepseek",
        "author": "Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence"
      }
    ]
  },
  {
    "index": 56,
    "papers": [
      {
        "key": "hurst2024gpt",
        "author": "Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others",
        "title": "Gpt-4o system card"
      }
    ]
  },
  {
    "index": 57,
    "papers": [
      {
        "key": "chen2021evaluating",
        "author": "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others",
        "title": "Evaluating large language models trained on code"
      },
      {
        "key": "austin2021program",
        "author": "Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others",
        "title": "Program synthesis with large language models"
      }
    ]
  },
  {
    "index": 58,
    "papers": [
      {
        "key": "xia2022less",
        "author": "Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Less training, more repairing please: revisiting automated program repair via zero-shot learning"
      },
      {
        "key": "xia2023automated",
        "author": "Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming",
        "title": "Automated program repair in the era of large pre-trained language models"
      },
      {
        "key": "jin2023inferfix",
        "author": "Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",
        "title": "Inferfix: End-to-end program repair with llms"
      }
    ]
  },
  {
    "index": 59,
    "papers": [
      {
        "key": "jimenez2023swe",
        "author": "Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik",
        "title": "Swe-bench: Can language models resolve real-world github issues?"
      }
    ]
  },
  {
    "index": 60,
    "papers": [
      {
        "key": "deng2023large",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models"
      }
    ]
  },
  {
    "index": 61,
    "papers": [
      {
        "key": "radford2018improving",
        "author": "Radford, Alec",
        "title": "Improving language understanding by generative pre-training"
      }
    ]
  },
  {
    "index": 62,
    "papers": [
      {
        "key": "nijkamp2022codegen",
        "author": "Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming",
        "title": "Codegen: An open large language model for code with multi-turn program synthesis"
      }
    ]
  },
  {
    "index": 63,
    "papers": [
      {
        "key": "muennighoff2023octopack",
        "author": "Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne",
        "title": "Octopack: Instruction tuning code large language models"
      }
    ]
  },
  {
    "index": 64,
    "papers": [
      {
        "key": "roziere2023code",
        "author": "Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others",
        "title": "Code llama: Open foundation models for code"
      },
      {
        "key": "luo2023wizardcoder",
        "author": "Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin",
        "title": "Wizardcoder: Empowering code large language models with evol-instruct"
      },
      {
        "key": "codealpaca",
        "author": "Sahil Chaudhary",
        "title": "Code Alpaca: An Instruction-following LLaMA model for code generation"
      }
    ]
  },
  {
    "index": 65,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 66,
    "papers": [
      {
        "key": "swamy2011secure",
        "author": "Swamy, Nikhil and Chen, Juan and Fournet, C{\\'e}dric and Strub, Pierre-Yves and Bhargavan, Karthikeyan and Yang, Jean",
        "title": "Secure distributed programming with value-dependent types"
      }
    ]
  }
]