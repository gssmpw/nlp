%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{lcsys}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage[noend]{algpseudocode}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{tabularx}
% \usepackage[numbers]{natbib}
% \usepackage{colortbl} 
% \usepackage{xcolor}  
% \usepackage[table]{xcolor}
\usepackage[hidelinks]{hyperref}
\hypersetup{
  colorlinks,
  citecolor= blue,
  linkcolor=black,
  urlcolor=black}
\usepackage[style=ieee, url=false, doi=false, natbib=true, mincitenames=1, maxcitenames=1, maxbibnames=99]{biblatex}
\addbibresource{main.bib}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}


\title{\LARGE \bf
Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on Temporal Difference Learning
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Zahra Shahrooei and Ali Baheri% <-this % stops a space
% \thanks{This work was not supported by any organization}% <-this % stops a space
\thanks{Zahra Shahrooei and Ali Baheri are with the Department of Mechanical Engineering, Rochester Institute of Technology, Rochester, NY 14623 USA. (e-mail: zs9580@rit.edu; akbeme@rit.edu).}%
% \thanks{P. Misra is with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small pmisra@cs.wright.edu}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

The primary goal of reinforcement learning is to develop decision-making policies that prioritize optimal performance, frequently without considering risk or safety. In contrast, safe reinforcement learning seeks to reduce or avoid unsafe states. This letter introduces a risk-averse temporal difference algorithm that uses optimal transport theory to direct the agent toward predictable behavior. By incorporating a risk indicator, the agent learns to favor actions with predictable consequences. We evaluate the proposed algorithm in several case studies and show its effectiveness in the presence of uncertainty. The results demonstrate that our method reduces the frequency of visits to risky states while preserving performance. A Python implementation of the algorithm is available at {\color{red}\url{https://github.com/SAILRIT/Risk-averse-TD-Learning}}.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Reinforcement learning (RL) algorithms focus on maximizing performance, primarily through long-term reward optimization. However, this objective alone does not always prevent negative or high-risk outcomes. Ensuring safety is crucial for RL applications in robotics, autonomous systems, and safety-critical tasks \cite{gu2024review}. To address this concern, researchers have explored various approaches to integrate safety into RL. A comprehensive review of safe RL methods can be found in \cite{garcia2015comprehensive}.

Several early safe RL studies incorporate safety into the optimization criterion \cite{heger1994consideration,gaskett2003reinforcement,sato2001td,geibel2005risk,achiam2017constrained,chow2019lyapunov,baheri2020deep}. For example, in the worst-case criterion, a policy is considered optimal if it maximizes the worst-case return, reducing variability due to inherent or parametric uncertainty that may lead to undesirable outcomes \cite{heger1994consideration,gaskett2003reinforcement}. The optimization criterion can also be adjusted to balance return and risk using a subjective measure, such as a linear combination of return and risk which can be defined as the variance of return \cite{sato2001td} or as the probability of entering an error state \cite{geibel2005risk}. The other way is to optimize the return subject to constraints resulting in the constrained optimization criterion \cite{achiam2017constrained,chow2019lyapunov}. Other approaches aim to avoid heuristic exploration strategies which are blind to the risk of actions. Instead, they propose modifications the exploration process to guide the agent toward safer regions. Safe exploration techniques include prior knowledge of the task for search initialization \cite{okawa2022safe}, learn from human demonstrations \cite{ramirez2023safe}, and incorporate a risk metric to the algorithm \cite{gehring2013smart,law2005risk}.

\begin{figure}[!t]
    \centering
     \includegraphics[scale=0.45]{OT_css.pdf}
     \caption{Conceptual illustration of optimal transport theory. Here, $\mu(x)$ is a probability distribution over the source space $\mathcal{X}$, and $\nu(y)$ is a probability distribution over the target space $\mathcal{Y}$. The arrows represent the optimal transport plan $T$, which reallocates mass from $\mu$ to $\nu$ to minimize the total transport cost.}
     \label{Fig 1.}
\end{figure}

Building on these strategies, we explore the use of optimal transport (OT) theory to enhance safety in RL by guiding the agent to prioritize visiting safer states during training. OT is highly valued for its ability to measure and optimize the alignment between probability distributions by minimizing the cost of transforming one distribution into another as shown in Fig. \ref{Fig 1.}. It takes into account the geometry of the distributions and provides a more interpretable comparison, particularly when the distributions have non-overlapping supports or complex structures \cite{santambrogio2015optimal}. There are a few applications of OT in safe RL \cite{baheri2023risk,queeney2023optimal,metelli2019propagating,shahrooei2024optimal,baheri2024synergy,baheri2023understanding}. For example, \citet{queeney2023optimal} apply OT theory to develop a safe RL framework that incorporates robustness through an OT cost uncertainty set. This approach constructs worst-case virtual state transitions using OT perturbations which improves safety in continuous control tasks compared to standard safe RL methods. \citet{metelli2019propagating} propose a novel approach called Wasserstein Q-learning (WQL), which uses Bayesian posterior distributions and Wasserstein barycenters to model and propagate uncertainty in RL. Their method demonstrates improved exploration and faster learning in tabular domains compared to classic RL algorithms. They also show preliminary success in adapting WQL to deep architectures for Atari games. \citet{shahrooei2024optimal} use OT for reward shaping in Q-learning. Through minimization of the Wasserstein distance between the policy’s stationary distribution and a predefined risk distribution, the agent is encouraged the agent to visit safe states more frequently.

In this letter, we incorporate OT theory into temporal difference (TD) learning to enhance agent safety during learning. We propose a risk-averse TD framework that considers both the reward and the uncertainty associated with actions without relying on expert knowledge or predefined safety constraints. We use Wasserstein distance to quantify the total risk level at each state and prioritize the actions that contribute less to this risk. This encourages the agent to take safer actions more frequently and avoid higher-risk ones. In other words, the agent tries to take actions with more predictable outcomes and avoid those with highly variable or uncertain consequences.


% In this letter, we present a risk-sensitive temporal difference (TD) algorithm.
% We show our algorithm outperforms standard TD algorithms in case studies 

The contributions of this letter are: (i) introduction of a risk-averse TD learning algorithm to enhance agent safety, (ii) safety bounds of the algorithm which demonstrates less visitation to risky states, and (iii) applications on case studies with different forms of uncertainty in reward function, transition function, and states to show our algorithm reduces visiting unsafe states while preserving performance.
% We structure this letter as follows: Section \ref{sec:prelim} provides a mathematical overview of temporal difference learning and OT theory. Section \ref{sec:SafeSarsa} elaborates on incorporating OT-based risk indicator into temporal difference learning. Section \ref{sec:Evaluation} presents experiments across various case studies and discusses the results. Section \ref{sec: Conclusion} concludes and suggests future directions.



\section{Preliminaries}
\label{sec:prelim}

This section reviews Markov decision processes, partially observable Markov decision processes, temporal difference learning algorithms, and the basic principles of OT theory.


\subsection{Markov Decision Processes and Temporal Difference Learning}

\noindent{\textbf{Markov Decision Processes (MDPs). }}MDPs represent a fully observable reinforcement learning environment. A finite MDP is defined by the tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{T},\mathcal{R}, \gamma)$, where $\mathcal{S}$ is a set of states, $\mathcal{A}$ is a set of actions, $\mathcal{T}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ 
is the transition probability function, where $\mathcal{T}(s'|s, a)$ represents the probability of transitioning to state $s'$ when action $a$ is taken in state $s$, $\mathcal{R}: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is the reward function, and \(\gamma \in [0, 1)\) is the discount factor. The agent follows a policy $\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ that maps states to action probabilities, with the objective to maximize the expected discounted return, $G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$.

\noindent{\textbf{Temporal Difference Learning. }}The Q-value of action $a$ in state $s$ under policy $\pi$ is given by $Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | s_t = s, a_t = a]$, which can be incrementally learned. In one-step TD, the Q-value update rule is $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \delta_t$ where $\delta_t$ is the TD error at time $t$ and $\alpha$ is the learning rate. In the Q-learning algorithm \cite{watkins1992q}, the TD error is defined as:
\begin{equation}
\delta_t = r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)
\label{eq:4} 
\end{equation}
Similarly, in the SARSA algorithm \cite{rummery1994line}, the TD error is given by: 
\begin{equation}
\delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
\label{eq:3} 
\end{equation}
SARSA($\lambda$) extends one-step SARSA using eligibility traces to incorporate multi-step updates and improve learning efficiency. An eligibility trace tracks the degree to which each state-action pair has been recently visited, which enables updates to consider a weighted history of past experiences. The Q-value update rule in SARSA($\lambda$) is:
\begin{equation}
Q_{t+1}(s, a) = Q_t(s, a) + \alpha \delta_t e_t(s, a), \quad \text{for all } s, a
\end{equation}
where $e_t(s, a)$ is the eligibility trace. The eligibility trace $e_t(s, a)$ decays over time and is updated as follows:
\begin{equation}
e_t(s, a) = 
\begin{cases} 
\gamma \lambda e_{t-1}(s, a) + 1 & \text{if } s = s_t \text{ and } a = a_t \\
\gamma \lambda e_{t-1}(s, a) & \text{otherwise}
\end{cases}
\end{equation}
where $\lambda \in [0, 1]$ controls the trace decay rate. Higher $\lambda$ values give greater weight to longer-term past states. TD algorithms often use an $\epsilon$-greedy strategy for action generation. The parameter $\epsilon$ can either be fixed or decay over time to balance exploration and exploitation. The behavioral policy is expressed as:

\begin{equation}
\pi(s_t, a_t) =
\begin{cases} 
1 - \epsilon & \text{if } a_t \in \arg\max_a Q(s_t, a) \\
\frac{\epsilon}{|\mathcal{A}|} & \text{otherwise}
\end{cases}
\end{equation}


\noindent{\textbf{Partially Observable MDPs (POMDPs). }}POMDPs generalize MDPs to account for environments where the agent cannot directly observe the underlying state. A POMDP is defined by the tuple $\mathcal{PM} = (\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{R}, \mathcal{Z}, \gamma)$, where $\mathcal{S}$, $\mathcal{A}$, $\mathcal{T}$, $\mathcal{R}$, and $\gamma$ maintain their definitions from MDPs, and $\mathcal{O}$ is a set of observations the agent can perceive. The observation function $\mathcal{Z}: \mathcal{S} \times \mathcal{A} \to \mathcal{O}$ maps states and actions to observation probabilities.
 


\subsection{Optimal transport theory}

The OT theory aims to find minimal-cost transport plans to move one probability distribution to another within a metric space. This involves a cost function $c(x, y)$ and two probability distributions, $\mu(x)$ and $\nu(y)$. The goal is to find a transport plan that minimizes the cost of moving $\mu$ to $\nu$ under $c(x, y)$, often using the Euclidean distance for explicit solutions \cite{villani2009optimal}.

We focus on discrete OT theory, assuming $\mu$ and $\nu$ as source and target distributions, respectively, both belonging to $\mathcal{P}p(\mathbb{R}^n)$ with finite supports $\{x_i\}_{i=1}^{m_1}$ and $\{y_j\}_{j=1}^{m_2}$, and corresponding probability masses $\{a_i\}_{i=1}^{m_1}$ and $\{b_j\}_{j=1}^{m_2}$. The cost between support points is represented by an $m_1 \times m_2$ matrix $C$, where $C_{ij} = |x_i - y_j|_p^p$ denotes the transport cost from $x_i$ to $y_j$. The OT problem seeks the transport plan $P^\ast$ that minimizes the cost while ensuring that the marginals of $P^\ast$ match $\mu$ and $\nu$:

\begin{equation}
\min_{P \in \mathbb{R}^{m_1 \times m_2}} \sum_{i=1}^{m_1} \sum_{j=1}^{m_2} P_{ij} C_{ij}  
\label{eq:3}
\end{equation}
% \begin{equation}
% \begin{aligned}
% \min_{P \in \mathbb{R}^{m_1 \times m_2}} \quad & \sum_{i=1}^{m_1} \sum_{j=1}^{m_2} P_{ij} C_{ij} 
% \end{aligned} \label{eq:3} 
% \end{equation}
Here, the coupling matrix \( P_{ij} \) indicates the probability mass transported from \( x_i \) to \( y_j \) and \( \sum_{j=1}^{m_2} P_{ij} = a_i \)
for all \( i \), and \( \sum_{i=1}^{m_1} P_{ij} = b_j\) for all \( j \). Additionally, \( P_{ij} \geq 0 \) for all \( i, j \).
% \text{subject to} \quad & \sum_{j=1}^{m_2} P_{ij} = a_i, \quad \forall i \in \{1, \ldots, m_1\} \\
% & \sum_{i=1}^{m_1} P_{ij} = b_j, \quad \forall j \in \{1, \ldots, m_2\} \\
% & P_{ij} \geq 0, \quad \forall i, j
% \begin{definition}[Wasserstein Distance \cite{villani2009optimal}]
% \begin{definition}[\cite{villani2009optimal}]


\textit{Definition 1 \cite{villani2009optimal}:} 
The Wasserstein distance between $\mu$ and $\nu$ is computed using the OT plan $P^\ast$ obtained from solving the above linear programming problem:
\begin{equation}
W_p(\mu, \nu) = \left( \langle P^\ast, C \rangle \right)^{\frac{1}{p}}
\label{eq:4}
\end{equation}
where $ p\geq 1$ and $\langle \cdot, \cdot \rangle$ denotes the inner product.
% \end{definition}

To enhance numerical stability and computational efficiency, an entropy regularization term can be added to the objective, leading to the regularized Wasserstein distance, which can be solved iteratively using the Sinkhorn iterations \cite{cuturi2013sinkhorn}.

\textit{Definition 2 \cite{cuturi2013sinkhorn}:} The Entropy-regularized OT problem is formulated as:
\begin{equation}
\min_{P \in \mathbb{R}^{m_1 \times m_2}} \quad \sum_{i=1}^{m_1} \sum_{j=1}^{m_2} P_{ij} C_{ij} + \varepsilon \sum_{i=1}^{m_1} \sum_{j=1}^{m_2} P_{ij} (\log P_{ij} - 1)
\label{eq:5}
\end{equation}
where $\varepsilon > 0$ is a regularization parameter that balances transport cost and the entropy of the transport plan $P$.
% \end{definition}


\section{Methodology}
\label{sec:SafeSarsa}

Consider an RL agent interacting with an environment defined by an MDP. For each state $s$, we define the \emph{Q-distribution} $Q_s$ as the normalized distribution over the agent’s estimated Q-values for the available actions $a_{i}$, $i = 1, \ldots, N$. Formally, we have $Q_s = \sum_{i=1}^{N} q_i^s \delta_{\mathbf{a}_i}$, where $q_i^s$ is the probability assigned to action $a_i$ based on the current Q-value estimations, and $\delta_{\mathbf{a}_i^s}$ is the Dirac measure centered at $a_i$. Intuitively, $Q_s$ captures how the agent’s Q-values are distributed across actions at state $s$. We also introduce a corresponding \emph{T-distribution} $T_t$, which represents a normalized distribution over the target values for the same set of actions. Specifically, we have $\quad T_t = \sum_{i=1}^{N} p_i^t \delta_{\mathbf{a}_i}$, where $p_i^t$ is the probability associated with action $a_i$ based on target values. For each action $a$ in state $s$, we define a risk indicator $U(s, a)$. The goal is to quantify how much an action contributes to the overall risk of the agent’s policy in that state. First, we compute the OT map $P^\ast$ between the \emph{Q-distribution} $Q_s$ and the \emph{T-distribution} $T_t$ by solving the entropy-regularized Wasserstein distance formulation. The total risk in state $s$ is measured by the Wasserstein distance $W(Q_s, T_t)$. For an action $a_i$, the flow $\Delta(s, a_i)$ is defined as the absolute difference between the outgoing flow (transport from $a_i$ to other actions $b \neq a_i$) and the incoming flow (transport from other actions $b \neq a_i$ to $a_i$):
\begin{equation}
\Delta(s, a_i) = \left| \sum_{b \neq a_i} P^\ast_{a_i, b} - \sum_{b \neq a_i} P^\ast_{b, a_i} \right|
\label{eq:12}
\end{equation}
Here, the first summation $\sum_{b \neq a_i} P^\ast_{a_i, b}$ represents how much probability mass is transported away from $a_i$ to other actions, while the second summation $\sum_{b \neq a_i} P^\ast_{b, a_i}$ denotes how much mass flows into $a_i$ from other actions. The absolute difference between these two flows, $\Delta(s, a_i)$, captures how much the probability of an action in \emph{Q-distribution} must be \enquote{redistributed} to match \emph{T-distribution}. We subsequently normalize this value by the total Wasserstein distance:
\begin{equation}
U(s, a_i) = \frac{\Delta(s,a_i)}{W(Q_s, T_t)} \label{eq:13}
\end{equation}
\begin{figure}[!b]
    \centering
     \includegraphics[scale=0.3]{Example.pdf}
     \caption{Example: For a fixed state and four available actions, we compute the total uncertainty of the state $W(Q_s, T_t)$ and the contribution of each action to this uncertainty.}
     \label{Fig 2. }
\end{figure}
\begin{table}[!b]
    \centering
    \caption{Example: Incorporating uncertainty to the behavioral policy considering $\beta=0.5$. Standard SARSA chooses $a_{4}$, while our algorithm prefers $a_{2}$. The safest available action is $a_{1}$.}
    \label{tab:example}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Action} & \boldmath{$Q(s,a)$} & \boldmath{$U(s,a)$} & \boldmath{$Q(s,a)-\beta U(s,a)$} \\
        \midrule
        $a_{1}$ & $0.2$ & \colorbox{yellow}{\boldmath{$0.21$}}  & $0.09$ \\
        $a_{2}$ & $0.3$ & $0.37$ & \colorbox{yellow}{\boldmath{$0.112$}} \\
        $a_{3}$ & $0.1$ & $0.78$ & $-0.29$ \\
        $a_{4}$ & \colorbox{yellow}{\boldmath{$0.4$}} & $0.62$ & $0.087$ \\
        \bottomrule
    \end{tabular}
\end{table}
%
The risk indicator $U(s, a_i)$ thus reflects how much action $a_i$ is responsible for the mismatch between $Q_s$ and $T_t$. Equivalently, it reveals to what extent the action $a_i$ needs to be adjusted (positively or negatively) for $Q_s$ to align with $T_t$ in a cost-efficient manner. Higher values of $\Delta(s, a_i)$ indicate larger corrections to the probability of $a_i$ in $Q_s$, which suggest more uncertainty or risk. While standard SARSA does not account for safety, we incorporate the above risk indicator into the behavioral policy. Let $\beta$ be a risk sensitivity coefficient that specifies how strongly the agent prioritizes safety relative to reward. We then modify the behavioral policy as follows:
\begin{equation} 
Q(s, a) - \beta U(s, a) \label{eq:14}
\end{equation}
Importantly, the agent’s action-selection process is biased to favor actions with both high Q-values and low-risk indicators. Indeed, we encourage the agent to choose the action for which it has the highest confidence in the outcome among the actions experienced in that state. This approach enables a directed exploration strategy that prioritizes safer actions with higher rewards. Notably, the uncertainty we address here is aleatoric, which is inherent to the environment and irreducible. Example $1$ further illustrates the influence of this risk indicator on both Q-values and policy decisions. In particular, it demonstrates the trade-off between exploiting high-return actions and mitigating high-risk actions to ensure safer exploration and more stable learning.

In the context of POMDPs, we use the SARSA($\lambda$) variant. Specifically, the Q and target distributions associated with the agent's current observation $o$ and the available actions $a_{1},\cdots ,a_{N}$ are integrated into the safety indicator term $U(o, a_i)$. Moreover, the flexibility of our approach enables its extension to scenarios that involve using multiple consecutive observations $o_{t-n},\cdots ,o_{t}$, which can help the algorithm better capture non-Markovian properties.

\textit{Example: }Consider a fixed state $s$ with four available actions. For this state, we have access to Q- and T-distributions over actions as depicted in Fig. \ref{Fig 2. }. Table. \ref{tab:example} shows that how the risk indicator term affects the Q-values and action selection. As observed, $a_1$ is the safest action and $a_{4}$ offers the highest reward. In decision-making, standard SARSA prefers action $a_{4}$, whereas risk-averse SARSA chooses $a_{2}$ to balance the reward and safety.


\begin{theorem}\label{thm:safety_bounds}
% Reduced Risk of Visiting Hazardous States
% \begin{theorem}[\parbox{\columnwidth}{Safety Bound: Reduced Risk of Visiting Hazardous States}]
Let $\mathcal{S}_{\text {risk }} \subset \mathcal{S}$ be a set of hazardous or \enquote{risky} states to avoid. For each $t$, define policy $\pi_t$ to be the $\epsilon$-greedy policy derived from the risk-augmented Q -values $Q_t(s, a)-\beta U_t(s, a)$. Assume that every state-action pair is explored infinitely often in the limit (i.e., persistent exploration) and the step-size $\left\{\alpha_t\right\}$ satisfies usual stochastic-approximation conditions. Then for sufficiently large $\beta$, there exists a constant $0<c<1$ such that
\begin{equation} 
\lim _{t \rightarrow \infty} \operatorname{Pr}_{\pi_t}\left[s \in \mathcal{S}_{\text {risk }}\right] \leq c \cdot \operatorname{Pr}_{\pi^0}\left[s \in \mathcal{S}_{\text {risk }}\right],
\end{equation} 
%
where $\pi^0$ is a baseline policy (e.g., standard SARSA's $\epsilon$-greedy policy w.r.t. $Q$ alone).
\end{theorem}

\begin{proof}
Let $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma)$ be a finite MDP with state space $\mathcal{S}$ and action space $\mathcal{A}$. Any stationary policy $\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ (where $\mathcal{P}(\mathcal{A})$ is the space of distributions over actions) induces a transition probability
\begin{equation} 
P_\pi\left(s^{\prime} \mid s\right)=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{T}\left(s^{\prime} \mid s, a\right)
\end{equation} 
Because the state and action sets are finite, each $\pi$ yields a finite-state Markov chain with transition matrix $P_\pi$. If $P_\pi$ is irreducible and aperiodic, there is a unique stationary distribution $\mu_\pi$. By the Ergodic Theorem for Markov chains, for any initial state $s_0$, the fraction of time the chain spends in state $s$ converges to $\mu_\pi(s)$ almost surely. Suppose $\pi^0$ is derived from (say) standard SARSA or Q-learning with $\epsilon$-greedy action selection based purely on $Q(s, a)$. Let $\mu^0$ be the unique stationary distribution of the Markov chain induced by $\pi^0$. Then
\begin{equation} 
\underset{\pi^0}{\operatorname{Pr}}\left\{s \in \mathcal{S}_{\text {risk }}\right\}=\sum_{s \in \mathcal{S}_{\text {rikk }}} \mu^0(s)
\end{equation} 
In contrast, the \enquote{safe} policy $\pi_t$ at episode (or time) $t$ follows an $\epsilon$-greedy strategy with respect to the risk-adjusted Q-values, $Q_t(s, a)-$ $\beta U_t(s, a)$
\begin{equation}
\pi_t(a \mid s)= 
\begin{cases} 
1-\epsilon, & \text{if } a \in \arg \max\limits_{a^{\prime}} 
\big( Q_t(s, a^{\prime}) - \beta U_t(s, a^{\prime}) \big) \\[6pt]
\frac{\epsilon}{|\mathcal{A}|}, & \text{otherwise.}
\end{cases}
\end{equation}
%
For large $\beta$, actions with large $U_t(s, a)$ become less likely to be chosen. Over time $t \rightarrow \infty$, if the algorithm converges, then $Q_t \rightarrow Q_\beta^*$ and $U_t \rightarrow U_\beta^*$ in some stable sense, and hence $\pi_t \rightarrow \pi_\beta$. Let $\pi_\beta$ denote the limiting \enquote{risk-averse} policy and $\mu_\beta$ its stationary distribution. Intuitively, if an action $a$ leads frequently to or transitions inside $\mathcal{S}_{\text {risk }}$, it will accumulate a larger risk indicator $U(s, a)$, because repeatedly visiting or transitioning into hazardous states forces significant \enquote{corrections} in the Q-distribution (cf. the definition of $U$ via OT). Under the update $Q(s, a)-\beta U(s, a)$, if $U(s, a)$ is large, then $Q(s, a)-\beta U(s, a)$ might be substantially less than competing actions. Thus, for large $\beta$, the probability $\pi_\beta(a \mid s)$ of choosing such a risky action decreases, unless its Q-value is significantly higher than the alternatives. We now compare $\mu_\beta\left(\mathcal{S}_{\text {risk }}\right)$ (the stationary measure of risky states under $\left.\pi_\beta\right)$ with $\mu^0\left(\mathcal{S}_{\text {risk }}\right)$ (the stationary measure under $\pi^0$). Let $\mathcal{A}_{\text {risk }}(s) \subseteq \mathcal{A}$ denote the actions in state $s$ whose transitions have high probability of landing in $\mathcal{S}_{\text {risk }}$ or staying there. Formally, for a given threshold $\delta>0$, define
\begin{equation} 
\mathcal{A}_{\text {risk }}(s)=\left\{a \in \mathcal{A}: \operatorname{Pr}\left[s_{t+1} \in \mathcal{S}_{\text {risk }} \mid s_t=s, a_t=a\right] \geq \delta\right\}
\end{equation} 
Because $\beta U(s, a)$ increases if an action is repeatedly leading to these hazardous states, for $\beta$ sufficiently large, actions in $\mathcal{A}_{\text {risk }}(s)$ are given low preference in $\pi_\beta$. One classical method is to compare two Markov chains $P_{\pi^0}$ and $P_{\pi_\beta}$ via a coupling argument. Intuitively, whenever $\pi^0$ chooses a \enquote{risky} action in state $s, \pi_\beta$ might choose a safer action in the same state with strictly higher probability if $\beta$ is large. Over many transitions, the chain under $\pi_\beta$ accumulates strictly fewer visits to $\mathcal{S}_{\text {risk }}$. In finite-state Markov chains, the stationary distribution $\mu$ is the normalized left-eigenvector of the transition matrix $P$. As we tune $\beta \rightarrow \infty$, the transition probabilities in $P_{\pi_\beta}\left(s^{\prime} \mid s\right)$ that lead to $\mathcal{S}_{\text {risk }}$ shrink, while transitions within the safe region $\mathcal{S} \backslash \mathcal{S}_{\text {risk }}$ become more likely. Consequently, the fraction of time spent in $\mathcal{S}_{\text {risk }}$ must decrease compared to the baseline. Formally,
\begin{equation} 
\mu_\beta(s)=\frac{1}{Z_\beta} \exp (\Phi(s, \beta)) \mu^0(s)
\end{equation} 
for some function $\Phi$ that accounts for changes in transition probabilities. Because transitions to $\mathcal{S}_{\text {risk }}$ are heavily penalized for large $\beta, \Phi$ is negative for $s \in \mathcal{S}_{\text {risk }}$; thus $\mu_\beta\left(\mathcal{S}_{\text {risk }}\right)$ must shrink relative to $\mu^0\left(\mathcal{S}_{\text {risk }}\right)$. Since the long-run fraction of time spent in $\mathcal{S}_{\text {risk }}$ converges to the respective stationary measure for each chain, we obtain
\begin{equation}
\begin{split}
\lim _{t \rightarrow \infty} \operatorname{Pr}\left\{s \in \mathcal{S}_{\text {risk }}\right\} 
&= \mu_\beta\left(\mathcal{S}_{\text {risk }}\right) \\
&\leq c \mu^0\left(\mathcal{S}_{\text {risk }}\right)= c \underset{\pi^0}{\operatorname{Pr}}\left\{s \in \mathcal{S}_{\text {risk }}\right\}.
\end{split}
\end{equation}
\end{proof}



\section{Experimental Results}
\label{sec:Evaluation}
\begin{figure}[!b]
    \centering
    \begin{minipage}{0.15\textwidth}
        \centering
        \includegraphics[scale=0.17]{CS_letter_mdp1.pdf}
    \end{minipage}\hfill
    \begin{minipage}{0.15\textwidth}
        \centering
        \includegraphics[scale=0.17]{CS_letter_mdp2.pdf}
    \end{minipage}\hfill
    \begin{minipage}{0.15\textwidth}
        \centering
        \includegraphics[scale=0.17]{CS_letter_pomdp.pdf}
        % \caption{Second Figure Caption}
        % \label{fig:figure2}
    \end{minipage}
    \caption{Case studies with low uncertainty level: (Left) grid-world with slippery states, (Center) cliff walking with traps, where the blue cells represent the trap region. (Right) rover navigation task with partial observability of obstacle locations.}
    \label{Fig 3. }
\end{figure}


\subsection{Case Studies}

We evaluate risk-averse SARSA in three case studies with uncertainties in rewards, transitions, and states (Fig. \ref{Fig 3. }). For each case, we conduct experiments under low and high uncertainty levels and further examine the effect of increasing the environment size on performance.

\noindent{\textbf{Case study 1: Grid-world with Reward Uncertainty. }}We consider a $10 \times 10$ grid-world environment with normal, goal, and slippery states \cite{gehring2013smart}. The agent can move up, down, left, and right. For any movement to a normal state, the agent receives the reward of $-1$, while transitions to slippery states result in a random reward in the range $[-12,10]$. Collisions with walls incur a reward of $-10$. The episode terminates when the agent either reaches the goal state in the top-right corner or completes a maximum of $100$ steps.


\noindent{\textbf{Case study 2: Cliff Walking with Transition Uncertainty. }}The cliff walking environment \cite{xuan2022sem} consists of three zones: the cliff region, the trap region, and the feasible region. The agent starts at the bottom-left corner to reach the goal at the bottom-right corner while avoiding the cliff zone, which represents unsafe states. Entering the cliff region results in task failure. The agent can move freely within the feasible region in four directions: up, down, left, and right. Entering the trap region forces the agent to move downward, regardless of its chosen action, eventually ending up in the cliff region. Each movement yields a reward of $-1$. If the agent collides with the environment borders, its position remains unchanged, but it still earns the movement reward. Reaching the target earns the agent a reward of $101$, while entering the cliff region results in a $-49$ penalty.
\begin{table}[!t]
    \centering
    \tiny
    % \scriptsize
    \caption{Average  R $\pm$ Std over different algorithms on grid-world environment.}
    \label{tab:grid}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Scenario} & \textbf{SARSA} & \textbf{Q-learning} & \textbf{Ours} \\
        \midrule
         $10 \times 10$ LU & $-15.70 \pm 17.32$ & $-13.38 \pm 14.53$ & \colorbox{yellow}{\boldmath{$-12.04 \pm 12.83$}} \\
        $10 \times 10$ HU & $-16.26 \pm 20.38$ & $-14.14 \pm 18.52$ & \colorbox{yellow}{\boldmath{$-12.92 \pm 16.79$}} \\
        $30 \times 30$ HU & $-119.16 \pm 126.65$ & $-108.21 \pm 118.76$ & \colorbox{yellow}{\boldmath{$-98.77 \pm 112.15$}} \\ 
        \bottomrule
    \end{tabular}
\end{table}
% Upon reaching either the target or the cliff region, the training episode restarts, and the agent is reset to its starting position.

\noindent{\textbf{Case study 3: Rover Navigation with Partial Observability. }}In this case study, a rover must navigate a two-dimensional terrain map represented as a $10 \times 10$ grid, where $3$ of the grid cells are obstacles. Each grid cell represents a state, and the rover can move in eight geographic directions. However, the environment is stochastic; for example, as shown in Fig. \ref{Fig 3. }, when the rover takes the action east, it moves to the intended grid cell with a probability of $0.9$ but may move to one of the adjacent cells with a probability of $0.05$. Partial observability exists because the rover cannot directly detect the locations of obstacle cells through its measurements. When the rover moves to a cell adjacent to an obstacle, it can identify the exact location of the obstacle (marked in magenta) with a probability of $0.6$ and observe a probability distribution over nearby cells (marked in pink). Colliding with an obstacle results in an immediate penalty of $10$, while reaching the goal region provides no immediate reward. All other grid cells impose a penalty of $2$. We consider $\gamma = 0.99$ and $\lambda =0.9$.


\begin{figure}[!b]
  \centering
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figs/grid_return_low_uncertainty.pdf}
    \caption{ }
    \label{fig:subfig-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figs/cliff_return_L.pdf}
    \caption{}
    \label{fig:subfig-b}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{p3obs.pdf}
    \caption{}
    \label{fig:subfig-b}
  \end{subfigure}
  \caption{ Comparison between average cumulative reward over $50$ random seeds for (a) grid-world and (b) cliff walking (c) rover navigation case studies with low uncertainty level. In all case studies, risk-averse SARSA outperforms other baselines.}
  \label{Fig 4. }
\end{figure}

\begin{figure}[ht!]
    \centering
     \includegraphics[scale=0.11]{Figs/grid_heat_map_low.pdf}
     \caption{ Comparison between state visitation  density for different algorithms on grid-world with low uncertainty.}
     \label{Fig 5. }
\end{figure}

\begin{figure}[ht!]
    \centering
     \includegraphics[scale=0.11]{Figs/grid_heat_map_high.pdf}
     \caption{ Comparison between state visitation density for different algorithms on grid-world with high uncertainty.}
     \label{Fig 6. }
\end{figure}

\begin{figure}[ht!]
    \centering
     \includegraphics[scale=0.14]{Figs/heatmap_cliff_LU.pdf}
     \caption{Comparison between the density of state visitation for different algorithms in case of cliff walking low uncertainty.}
     \label{Fig 7. }
\end{figure}

\begin{figure}[ht!]
    \centering
     \includegraphics[scale=0.14]{Figs/heatmap_cliff_HU.pdf}
     \caption{Comparison between the density of state visitation for different algorithms in case of cliff walking high uncertainty.}
     \label{Fig 8. }
\end{figure}


\subsection{Discussion}
Fig. \ref{Fig 4. } shows the average return of different algorithms over $50$ random seeds for different case studies with a low level of uncertainty. The results demonstrate that the risk-averse SARSA algorithm converges to a higher return value and exhibits higher stability throughout the learning process, mainly because of the risk indicator term, which guides exploration toward safer and more consistent actions. For the cliff walking case study, risk-averse SARSA not only achieves rapid convergence but also demonstrates a higher confidence in its return estimates. In contrast, both Q-learning and SARSA display greater variability, which reflects higher uncertainty in their returns.

\begin{table}[!b]
    \centering
    \tiny % or \scriptsize for even smaller text
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Scenario} & \textbf{R/F} & \textbf{SARSA} & \textbf{Q-Learning} & \textbf{Ours} \\
        \midrule
        \multirow{2}{*}{$10 \times 7$ LU} 
        & R $\pm$ Std & $72.48 \pm 36.1$ & $74.59 \pm 28.52$ & \colorbox{yellow}{\boldmath{$87.99 \pm 10.65$}}  \\
        & F & $72.74$ & $61.46$ & \colorbox{yellow}{\boldmath{$21.74$}} \\
        \midrule
        \multirow{2}{*}{$10 \times 7$ HU} 
        & R $\pm$ Std & $69.82 \pm 38.69$ & $84.27 \pm 23.99$ & \colorbox{yellow}{\boldmath{$89.53 \pm 8.65$}}  \\
        & F & $196.94$ & $77.56$ & \colorbox{yellow}{\boldmath{$30.14$}}  \\
        \midrule
        \multirow{2}{*}{$30 \times 21$ HU}  
        & R $\pm$ Std & $-177.07 \pm 54.61$ & $42.11 \pm 37.44$ & \colorbox{yellow}{\boldmath{$55.59 \pm 22.36$}}  \\
        & F & $523.64$ & $362.66$ & \colorbox{yellow}{\boldmath{$196.48$}}  \\
        \bottomrule
    \end{tabular}
    \caption{Average performance metrics for different scenarios of cliff walking environment.}
    \label{tab:cliff}
\end{table}

Table. \ref{tab:grid}, \ref{tab:cliff}, and \ref{tab:pomdp} present a quantitative comparison of risk-averse SARSA performance to other baselines, providing the average return (R) and standard deviation (std) under low uncertainty (LU), high uncertainty (HU), and increasing environment size scenarios across last $20$ episodes for all case studies. The results in Table. \ref{tab:grid} confirm that risk-averse SARSA achieves the highest return and the lowest std in different scenarios. We present the state visitation map for LU and HU shown in Fig. \ref{Fig 5. } and Fig. \ref{Fig 6. }. As expected, the SARSA algorithm, which lacks any safety considerations, demonstrates a high frequency of visits to slippery regions (darker red). In contrast, Q-learning performs better by exploring more efficient paths, but it still exhibits notable visits to unsafe states in comparison to risk-averse SARSA. For the cliff walking case study, the observations in Table. \ref{tab:cliff} demonstrate that risk-averse SARSA outperforms SARSA and Q-learning algorithms by converging to higher return values with lower std values. For this case study, the state visitation graph in Fig. \ref{Fig 7. } and Fig. \ref{Fig 8. } highlights the limitations of SARSA, where the agent struggles to identify the optimal path to the goal state in both LU and HU scenarios. Consequently, most episodes end without successfully reaching the goal. Q-learning performance is closer to our algorithm, however the rate of reaching the goal and escaping from the cliff at the beginning of episodes is lower than risk-averse SARSA. Moreover, in this case study number of failures (F) for risk-averse agent is significantly lower than both SARSA and Q-learning. For instance, in the LU scenario, risk-averse SARSA reduces failures by $35\%$ compared to SARSA and $30\%$ compared to Q-learning. This reduction is even greater in the other two scenarios. Overall, in both MDP case studies, our algorithm obtained a higher cumulative reward than Q-learning and SARSA, while improving the stability and the safety of the agent by avoiding unpredictable actions. Furthermore, for both MDP case studies, although increasing the size of the environment causes lower return values, risk-averse SARSA still maintains the best performance.
\begin{table}[!t]
    \centering
    \tiny % or \scriptsize for even smaller text
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Scenario} & \textbf{R/F} & \textbf{SARSA($\lambda$)}  & \textbf{Ours} \\
        \midrule
        \multirow{2}{*}{$10 \times 10$ LU} 
        & R $\pm$ Std & $-155.04 \pm 109.09$ & \colorbox{yellow}{\boldmath{$-149.96 \pm 106.41$}}  \\
        & F & $2805.73$ & \textbf{\colorbox{yellow}{\boldmath{$2663.48$}}}  \\
        \midrule
        \multirow{2}{*}{$10 \times 10$ HU} 
        & R $\pm$ Std & \colorbox{yellow}{\boldmath{$-247.70 \pm 102.58$}} & $-264.00 \pm 89.72$ \\
        & F & $9039.29$ & \textbf{\colorbox{yellow}{\boldmath{$8948.18$}}} \\
        \midrule
        \multirow{2}{*}{$30 \times 30$ HU}  
        & R $\pm$ Std & \colorbox{yellow}{\boldmath{$-1020.70 \pm 39.26$}} & $-1024.74 \pm 57.09$ \\
        & F & $30026.71$ & \textbf{\colorbox{yellow}{\boldmath{$30026.71$}}} \\
        \bottomrule
    \end{tabular}
    \caption{Average number of obstacle collisions for SARSA($\lambda$), and risk-averse SARSA($\lambda$) algorithms for POMDP case study.}
    \label{tab:pomdp}
\end{table}


For the POMDP case study, as can be seen in Table. \ref{tab:pomdp}, risk-averse SARSA($\lambda$) achieves superior performance compared to standard SARSA($\lambda$) in the case of low-partial observability. By increasing the partial observability degree (HU) and the size of the environment, the agent performance falls back. This shows that the performance of our algorithm can be influenced by the degree of partial observability in the environment. Specifically, when the agent receives indistinguishable or highly similar observations for different underlying states, the accuracy of the estimated Q-value and target distributions and, consequently, the reliability of the risk indicator becomes questionable. The number of failures for risk-averse SARSA($\lambda$) across the three scenarios is slightly lower than SARSA($\lambda$).

% \noindent{\textbf{Limitations. }}While our risk-averse SARSA algorithm can outperform standard SARSA and Q-learning algorithms in terms of reward and safety of the agent, it suffers from high computational cost. While using the entropy-regularized Wasserstein distance helps reduce this cost, determining the OT map for the set of candidate actions at each state remains computationally expensive.


\section{Conclusions}
\label{sec: Conclusion}

We presented a risk-averse temporal difference algorithm based on optimal transport theory. We demonstrated the effectiveness of this approach in encouraging agents to prioritize less uncertain actions, leading to a reduction in visits to risky states and an improvement in cumulative rewards. Compared to standard temporal difference algorithms, our algorithm demonstrated robust performance in environments with reward, transition, and state uncertainties. Although our algorithm outperforms standard TD learning methods, it has its limitations. Determining the optimal transport map for candidate actions at each state is computationally expensive. While using the entropy-regularized extension of OT reduces this computational cost, further improvements in computational efficiency will be a focus of future work. 








\addtolength{\textheight}{-3cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\renewcommand*{\bibfont}{\footnotesize}
% \bibliographystyle{plainnat}
\printbibliography

% \begin{thebibliography}{99}

% \bibitem{c1}
% J.G.F. Francis, The QR Transformation I, {\it Comput. J.}, vol. 4, 1961, pp 265-271.

% \bibitem{c2}
% H. Kwakernaak and R. Sivan, {\it Modern Signals and Systems}, Prentice Hall, Englewood Cliffs, NJ; 1991.

% \bibitem{c3}
% D. Boley and R. Maier, "A Parallel QR Algorithm for the Non-Symmetric Eigenvalue Algorithm", {\it in Third SIAM Conference on Applied Linear Algebra}, Madison, WI, 1988, pp. A20.

% \end{thebibliography}

\end{document}
