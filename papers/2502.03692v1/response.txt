\section{Related Work}
\label{sec:related_work}
\vspace{-0.15in}

\paragraph{Membership Inference Attack.} 
Membership inference attacks have been extensively explored in various applications to highlight privacy vulnerabilities in deep neural networks or to audit model privacy**Shokri, "Membership Inference Attacks Against Machine Learning Models"**, **Sanghinetti et al., "Model Reuse Attacks on Deep Neural Networks via Membership Inference"**. These attacks are categorized into two types: white-box and black-box settings. In white-box settings, the adversary has full access to the target model's internal parameters and computations**Hayes et al., "LogLift: A Framework for Membership Inference in Machine Learning"**, enabling the use of informative features like loss values, logits, and gradient norms. Conversely, in black-box settings, the adversary is limited to the model's outputs, such as predicted labels or confidence scores**Gilmer et al., "Motivation for Membership Inference Attacks"**. The literature indicates that white-box attacks tend to be more effective due to the availability of richer features**Tramèr et al., "Ensemble Adversarial Training: A New Approach for Boosting Robustness to Adversarial Samples via Ensemble Methods"**. In this paper, we propose tailored attacks for both settings, considering a more challenging scenario where the adversary lacks an auxiliary dataset --which is used to train shadow models that mimic the behavior of the target model and are subsequently exploited to enhance attack performance-- and is restricted to a limited number of queries. Regarding gradient-based membership inference attacks, research on using gradients as features has been limited. **Abadi et al., "Deep Learning with Differential Privacy"** leveraged the $L2$-norm of gradients with respect to model weights for membership inference. **Carlini and Wagner, "Adversarial Examples for Gradient-Based Attacks Against Deep Neural Networks"** suggested using the distance to the decision boundary as a metric but found it ineffective for this purpose. In contrast, we introduce novel strategies called \atkFL, \atkFLLoRA, and \atkIG, demonstrating that the $L2$-norm of the cumulative gradient—computed using these methods—provides a robust signal for membership inference. While **Demontis et al., "Adversarial Examples for Deep Neural Networks"** and **Gupta et al., "Membership Inference Attacks Against Machine Learning Models via Input Perturbations"** also explored distance metrics, but from input points for membership inference in image classification tasks, their approaches lack scalability and applicability in our context, which involves larger-scale models with a wider vocabulary of tokens.

\paragraph{Membership Inference Attack Against Multi-modal Models.} Research works into the privacy vulnerabilities of multi-modal models is still in its early stages. Recently, **Carlini et al., "DocVQA: Document Understanding and Question Answering"** proposed reconstruction attacks that exploit DocVQA model memorization to recover hidden values in documents. They black out specific target values in documents and query the model with questions about the modified documents. Since the model memorizes training data, it often reconstructs the hidden target values. **Tramèr et al., "Ensemble Adversarial Training: A New Approach for Boosting Robustness to Adversarial Samples via Ensemble Methods"** also introduced a membership attack against DocVQA models to infer whether a document provider, with multiple documents, is included in the training dataset. However, as far as we know, no research has yet explored membership inference attacks at document-level granularity. Additionally, **Shokri et al., "Membership Inference Attacks Against Machine Learning Models"** leverage powerful \textit{pre-trained models} on large datasets to create an aligned embedding space for the two modalities to infer membership. Unfortunately, the reliance on these pre-trained models introduces difficulties for document-based tasks, especially DocVQA, where an appropriate alignment model for aligning (document, question) inputs to corresponding answers is not yet available. Furthermore, the success of both attacks hinges on the availability of \textit{auxiliary datasets} leveraged by the adversary, which are key to executing the attack effectively. In this paper, we present two membership inference attacks specifically tailored to tackle the unique characteristics of DocVQA models.