\begin{table}[t]
\begin{center}
\begin{small}
\small
\begin{tabular}{lccc}
\toprule
Layer & VT5(PFL) & Donut(DocVQA) & Pix2Struct-B(DocVQA)\\
\midrule 
Embedding Projection Layer & 67.0 & 71.33 & 68.66\\
Embedding Layer Norm & 65.33 & 76.0 & 64.67\\
\cmidrule{1-4}
Last Decoder Block FC1 & \textbf{68.33} & \textbf{78.0} & 68\\
Last Decoder Block FC2 & 68.17 & 77.33 & \textbf{68.83}\\
Last Decoder Block Layer Norm & 61.83 & 76.83 & 67.5\\
\cmidrule{1-4}
Random Decoder Block FC1 & 61.33 & 72.0 & 67.5\\
Random Decoder Block FC2 & 64.0 & 73.0 & 65.17\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\caption{\textbf{Effect of selected layer to tune} from each target model. Attack performances are reported in terms of Accuracy.}
\label{tab:ablate_layer}
\vspace{-0.2in}
\end{table}

