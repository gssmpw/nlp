\section{Related Work}
\label{sec:related_work}
\vspace{-0.15in}

\paragraph{Membership Inference Attack.} 
Membership inference attacks have been extensively explored in various applications to highlight privacy vulnerabilities in deep neural networks or to audit model privacy~\citep{shokri2017membership}. These attacks are categorized into two types: white-box and black-box settings. In white-box settings, the adversary has full access to the target model's internal parameters and computations~\citep{carlini2022membership, yeom2018privacy, nasr2019comprehensive, rezaei2021difficulty, sablayrolles19a, li2021membership}, enabling the use of informative features like loss values, logits, and gradient norms. Conversely, in black-box settings, the adversary is limited to the model's outputs, such as predicted labels or confidence scores~\citep{choquette2021label, shokri2017membership, salem2018ml, sablayrolles19a, song2021systematic, hui2021practical}. The literature indicates that white-box attacks tend to be more effective due to the availability of richer features~\citep{song2019privacy,nasr2019comprehensive}. In this paper, we propose tailored attacks for both settings, considering a more challenging scenario where the adversary lacks an auxiliary dataset --which is used to train shadow models that mimic the behavior of the target model and are subsequently exploited to enhance attack performance-- and is restricted to a limited number of queries. Regarding gradient-based membership inference attacks, research on using gradients as features has been limited. \citet{nasr2019comprehensive} leveraged the $L2$-norm of gradients with respect to model weights for membership inference. \citet{rezaei2021difficulty} suggested using the distance to the decision boundary as a metric but found it ineffective for this purpose. In contrast, we introduce novel strategies called \atkFL, \atkFLLoRA, and \atkIG, demonstrating that the $L2$-norm of the cumulative gradient—computed using these methods—provides a robust signal for membership inference. While \citet{maini2021dataset} and \citet{li2021membership} also explored distance metrics, but from input points for membership inference in image classification tasks, their approaches lack scalability and applicability in our context, which involves larger-scale models with a wider vocabulary of tokens.

\paragraph{Membership Inference Attack Against Multi-modal Models.} Research works into the privacy vulnerabilities of multi-modal models is still in its early stages. Recently, \citet{tito2024privacy,pinto24a} proposed reconstruction attacks that exploit DocVQA model memorization to recover hidden values in documents. They black out specific target values in documents and query the model with questions about the modified documents. Since the model memorizes training data, it often reconstructs the hidden target values. \citet{tito2024privacy} also introduced a membership attack against DocVQA models to infer whether a document provider, with multiple documents, is included in the training dataset. However, as far as we know, no research has yet explored membership inference attacks at document-level granularity. Additionally, \citet{ko2023practical,hu2022m} leverage powerful \textit{pre-trained models} on large datasets to create an aligned embedding space for the two modalities to infer membership. Unfortunately, the reliance on these pre-trained models introduces difficulties for document-based tasks, especially DocVQA, where an appropriate alignment model for aligning (document, question) inputs to corresponding answers is not yet available. Furthermore, the success of both attacks hinges on the availability of \textit{auxiliary datasets} leveraged by the adversary, which are key to executing the attack effectively. In this paper, we present two membership inference attacks specifically tailored to tackle the unique characteristics of DocVQA models.