To investigate whether LLMs naturally capture student misconception patterns, we analyze their behavior on multiple-choice questions and compare it against actual student response data. Our investigation focuses on two key aspects: (1). whether LLMs assign higher probabilities to incorrect answers that commonly mislead students, and (2). whether LLMs tend to select the same wrong answers that students frequently choose. To enable this analysis, we develop a systematic framework as follows:


\subsection{Preliminaries}

Let $\mathcal{Q} = \{q_1, q_2, \ldots, q_n\}$ denote our collection of $n$ multiple-choice questions. For each question $q_i$, we define:

\begin{itemize}
    \item A set of $m$ answer options $\mathcal{A}_i = \{a_{i1}, a_{i2}, \cdots, a_{im}\}$
    \item The correct answer index $c_i \in \{1, 2, \cdots, m\}$
    \item The empirical student response distribution $\mathcal{S}_i = \{s_{i1}, s_{i2}, \cdots , s_{im}\}$, where $s_{ij}$ represents the proportion of students selecting option $j$ for question $i$ such that $\sum_{j=1}^m s_{ij} = 1$
\end{itemize}

This formalization enables our analysis of the alignment between LLM behavior and student misconception patterns through both probabilistic correlation measures and direct incorrect answer comparisons.

\subsection{Likelihood Calculation of LLM's Answer Choice}

We quantify an LLM's preference for each answer choice using two formatting approaches implemented through the EleutherAI Harness evaluation framework~\cite{eval-harness}.


\subsubsection{Index-based Approach:}
In the index-based approach, we present the model with both the question and all answer choices in a structured format, where each answer choice is assigned a corresponding letter index (A, B, C, ...). The model then predicts a single letter choice representing the answer choice. For choice index $j \in \{A, B, C, \cdots\}$ of question $q_i$, we compute the log-likelihood as:

$$\mathcal{L}_{a_{ij}}^{\text{index}} = \log P(j \mid q_i, \mathcal{A}_i; \theta)$$

where $\theta$ represents the model parameters.

\subsubsection{Text-based Approach:}
In the text-based approach, we evaluate each answer choice independently by computing its likelihood when paired with only the question as input. For each choice $a_{ij}$ from question $q_i$, we compute the log-likelihood by summing over all tokens in the answer text and take an average to avoid giving long answers an unfair advantage:

$$\mathcal{L}_{a_{ij}}^{\text{text}} = \frac{\sum_{t=1}^{T_{ij}} \log P(x_t^{(ij)} \mid q_i, x_{<t}^{(ij)}; \theta)}{T_{ij}}$$

where $x_t^{(ij)}$ is the $t$-th token in choice $a_{ij}$; $T_{ij}$ is the number of tokens in choice $a_ij$ and $x_{<t}^{(ij)}$ represents all preceding tokens in the answer. The summation and average is necessary here because we measure the likelihood of the entire answer text sequence, token by token. 

The two approaches differ in their computation of answer probabilities. Index-based approach computes the conditional probability of each index given the complete context of all options, while text-based approach evaluates the likelihood of each answer text independently through token-wise probability estimation.

\subsubsection{Probability Normalization:}
For both approaches, we convert log-likelihoods to probabilities using the softmax function:

$$P_{ij}(a_{ij} \mid q_i) = \frac{\exp(\mathcal{L}_{a_{ij}})}{\sum_{j=1}^{n} \exp(\mathcal{L}_{a_{ij}})},$$

where $\mathcal{L}_{a_{ij}}$ represents either $\mathcal{L}_{a_{ij}}^{\text{index}}$ or $\mathcal{L}_{a_{ij}}^{\text{text}}$ depending on the approach used.

\subsection{Analysis Framework}

Our investigation employs two approaches to understand how LLMs align with student misconceptions and answer the proposed research questions.

\subsubsection{RQ1: Correlation Between LLM Generation Probabilities and Student Selection Patterns for Distractors:}

For each question-option pair, we compute the LLM's predicted likelihood distribution $P_{ij}$ and obtain student selection probability $s_{ij}$ for each option $a_{ij}$ over all answers choices $\mathcal{A}_i$ with $c_i$ being the correct answer. Then, we adopt statistical methods to compute the correlation coefficient $\rho_i$ between the language model's generation likelihood and student selection frequencies:

\begin{equation}
    \rho_i = \text{corr}(\{P_{ij, j \neq c_i}\}, \{s_{ij, j \neq c_i}\})
\end{equation}

By comparing the LLM's likelihood patterns with actual student selection rates, we can assess whether the model's confidence in distractors aligns with the cognitive misconceptions that commonly mislead students. This alignment, if present, would suggest that LLMs may naturally capture aspects of human cognitive biases and misconceptions that lead to systematic errors in problem-solving.

Our analysis specifically excludes correct answers $(j \neq c_i)$ to isolate these misconception patterns. By focusing only on incorrect options, we can better examine the distribution of plausible but incorrect reasoning, rather than having our analysis confounded by the model's ability to identify correct answers. Higher likelihood scores indicate higher model confidence in an answer choice, allowing us to quantify which incorrect options the model finds most plausible and compare this directly with student error patterns.


\subsubsection{RQ2: Alignment of LLM Mistakes and Student Misconception Patterns:}
Our second analysis examines whether LLM errors align with the most common student misconceptions. Specifically, we investigate if an LLM, when selecting an incorrect answer, chooses the same incorrect option that most students select. This behavioral analysis complements our perplexity-based approach by examining the model's actual answer selections rather than its probability distributions. The analysis proceeds as follows:

\begin{enumerate}
    \item For each question $q_i$, we record the LLM's selected choice option $o_i$ and define an error indicator:
    \begin{equation}
        e_i = \begin{cases}
            1 & \text{if } o_i \neq c_i \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation}

    \item For cases where $e_i = 1$, we compute an alignment score:
    \begin{equation}
        \alpha_i = \frac{s_{i,o_i}}{\max\limits_{j \neq c_i} s_{ij}}
    \end{equation}
\end{enumerate}

The alignment score $\alpha_i$ quantifies how well the LLM's incorrect answers correspond to the most common student misconceptions. A score of $\alpha_i = 1$ indicates perfect alignment, meaning the LLM selected the same incorrect option that students most frequently chose when making mistakes.
To illustrate this measure, consider a multiple-choice question where students' selection rates for incorrect options are distributed as follows: Option B (30\%), Option C (20\%), and Option D (15\%). In this case, Option B represents the most common student misconception. If an LLM makes an error by selecting Option C, the alignment score would be calculated as $\alpha = \frac{20\%}{30\%} = 0.67$. This score reflects that while the LLM did select a relatively common incorrect answer, it did not align with the predominant student misconception (Option B). Lower alignment scores indicate greater divergence between the LLM's error patterns and the predominant student misconceptions. This metric provides valuable insights into whether LLMs naturally capture the cognitive biases that lead students toward specific incorrect answers, helping us understand the extent to which these models might reflect human-like patterns of misunderstanding. The mean alignment score across all questions is defined as:

$$\bar{\alpha} = \frac{\sum_{i=1}^N \alpha_i e_i}{\sum_{i=1}^N e_i}$$
