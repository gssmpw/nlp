The emergence of Large Language Models (LLMs) has  demonstrated remarkable capabilities across diverse educational applications: from generating curriculum materials \cite{moein2024beyond} to providing personalized tutoring \cite{alsafari2024towards,sonkar2023class,schmucker2024ruffle} and real-time feedback \cite{nicolicioiu2024panza,bewersdorff2023assessing}, to modeling human cognitive processes \cite{mcintosh2024inadequacy,sonkar2024llm}. Within this broad spectrum of LLM's capabilities, a particularly intriguing direction is their potential to understand and predict patterns in student thinking, especially how students interact with multiple-choice questions and select incorrect choices (distractors).

Multiple-choice questions (MCQs) serve as powerful diagnostic tools in educational assessment \cite{eedi-mining-misconceptions-in-mathematics}. Instead of just being random wrong answers, high-quality distractors of MCQs should reflect common misconceptions or reasoning errors that students encounter. The patterns in how students select these distractors often reveal systematic misconceptions shared across learners \cite{smith1994misconceptions}, providing valuable insights into their learning processes and making MCQs particularly effective for identifying knowledge gaps in student understanding.

A fundamental question emerges: do LLMs inherently capture the patterns in how students select distractors? Understanding whether these models naturally align with student misconception patterns have significant implications for both educational practice and our understanding of LLMs' reasoning capabilities. This insight could improve how we develop educational technologies, from enhancing assessment tools to creating tutoring systems that better anticipate and address common student misunderstandings. Further, this investigation could reveal whether LLMs develop internal representations that parallel human cognitive patterns.

To investigate this question systematically, we propose an analysis framework focusing on two specific research questions:

\textbf{RQ1:} Do the distractors that students more frequently select correspond to ones that LLMs assign higher generation likelihood to?

\textbf{RQ2:}  When an LLM selects an incorrect answer, does it choose the same distractor that most commonly misleads students?

We conduct our analysis using a comprehensive dataset of $3,202$ multiple-choice questions with real-world student response distributions and two families of large language models - LLaMA \cite{touvron2023llama} and Qwen \cite{bai2023qwen}, with parameters ranging from 0.5B to 72B. To quantify the relationship between LLM generation likelihood and student response patterns in distractor selections, we introduce a novel alignment score. This study presents the first empirical investigation of whether LLMs' generation preferences and error choices align with common student misconceptions.


Our analysis reveals two key findings. First, when examining LLMs' generation probabilities for distractors (RQ1), we observe moderate correlations with student selection patterns (Pearson $r=0.28-0.37$), with larger models demonstrating slightly stronger alignment. Second, our analysis of LLMs' incorrect selections (RQ2) shows a striking tendency across both small and large models to choose the same wrong answers that students most commonly select. Even the smallest model (0.5B parameters) selects students' most common incorrect answer about $51\%$ of the time, while larger models reach up to $59\%$, indicating that this alignment with student error patterns may be an inherent property of LLMs rather than purely a function of model scale.


These findings have important implications for educational applications, particularly in MCQs design. The moderate alignment between LLM prediction and student misconception patterns suggests that while LLMs exhibit certain correlation with student thinking process, they likely approach problems differently than students do. Rather than viewing this as a limitation, it presents an opportunity: LLM-generated distractors could potentially probe different aspects of student understanding, complementing rather than replacing human expertise in the MCQs design. Notably, our finding that smaller models show similar patterns when making mistakes as larger models suggests a cost-efficient approach to generating plausible distractors. This finding points toward a promising hybrid approach where LLMs could help expand the range of assessment options by generating novel distractors while human experts ensure quality and coverage of the known misconceptions in the distractors. More broadly, our work contributes to a better understanding of how to effectively integrate LLM capabilities into educational assessment design, highlighting both their potential and limitations in modeling student cognitive process.

