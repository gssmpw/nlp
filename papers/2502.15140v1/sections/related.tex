\subsection{Student Misconception Analysis in Education}

Understanding and addressing student misconceptions is fundamental to improving educational outcomes, as these misconceptions arise when learners develop incorrect mental models that persist despite formal instruction \cite{chi1994conceptual}. Traditional methods for identifying misconceptions have relied on concept inventories like mal-rules \cite{malrules,sleeman}, the Force Concept Inventory in physics \cite{hestenes1992force}, while both distractors and corresponding feedback messages in math MCQs have been leveraged to help students overcome common errors through targeted remediation \cite{li2024automated,mcnichols2023automated}. Psychometric models like Item Response Theory (IRT) have been employed to measure how misconceptions influence student performance \cite{embretson2000item}, while data-driven approaches have enabled large-scale analysis of student errors, leveraging NLP and machine learning to uncover latent misconceptions in open-ended responses \cite{pardos2,pardos1}. Deep knowledge tracing models further enhance our understanding by tracking student learning trajectories and predicting future misconceptions \cite{pardos2013adapting,piech2015deep}.

Researchers have developed probabilistic models to analyze students' textual responses, aiming to uncover prevalent misconceptions without requiring instructors to predefine them \cite{michalenko2017data}, while in intelligent tutoring systems \cite{baker_test}, methods have been proposed to identify knowledge components in dialogue turns and diagnose the correctness of student responses \cite{scarlatos2023exploring}. Recent research has focused on verifying student solutions in mathematical problem-solving contexts, with emphasis on detecting errors in student reasoning and providing customized feedback \cite{daheim2024stepwise,liu2023novice}. The development of dialogue tutoring datasets with rich pedagogical properties has been explored to better understand and address student misconceptions, aiming to simulate realistic tutor-student interactions \cite{macina2023mathdial}. These various approaches and studies collectively underscore the significance of leveraging advanced computational techniques to detect and address student misconceptions, thereby informing the development of more effective educational tools and strategies.

\subsection{Automated Distractor Generation}

Researchers have investigated LLMs for automated distractor generation in MCQs \cite{baral2024automated,eedi-mining-misconceptions-in-mathematics,bitew2023distractor}, with studies revealing that while LLMs can produce mathematically valid distractors, they often fall short in anticipating common student errors or misconceptions \cite{feng2024exploring}. This limitation has prompted the development of enhanced approaches, including model training pipelines incorporating pairwise rankers to assess distractor plausibility based on potential student misconceptions. Using synthetic student choice datasets, researchers have also demonstrated improved generation of distractors that align more closely with common student misunderstandings \cite{lee2025generating}. Similarly, an overgenerate-and-rank approach has been proposed that trains ranking models to predict how likely distractors are to be selected by real students \cite{scarlatos2024improving}.

In-context learning techniques have emerged as another promising direction for generating both distractors and corresponding feedback messages in math MCQs. By providing LLMs with examples of questions and corresponding distractors, these techniques guide the generation process toward more relevant outputs \cite{mcnichols2023automated}. These collective advancements in automated distractor generation aim to reduce the manual effort required in crafting assessments while improving their diagnostic capabilities through closer alignment between distractors and actual student misconceptions. 