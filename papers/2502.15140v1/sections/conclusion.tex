Our study provides the first quantitative framework for evaluating whether LLMs naturally capture patterns in how students select distractors in multiple-choice questions. Through our dual analysis framework, we found moderate correlations between LLM-assigned generation probabilities and student distractor selection distributions. These findings suggest that while LLMs show some alignment with student response patterns, they may not fully capture the cognitive processes underlying student choices. Interestingly, we found that smaller models, despite making more mistakes overall, show similar alignment with student error patterns as larger models. This suggests that computationally efficient smaller models could be valuable tools for distractor generation. Beyond this practical insight, the partial alignment between LLMs and student responses suggests an interesting opportunity: LLMs might generate distractors that probe different aspects of student understanding than traditional human-designed ones targeting known misconceptions. Future work should explore hybrid approaches that combine LLM capabilities with human expertise in educational assessment design, while remaining mindful that LLMs' underlying approach to problem-solving may fundamentally differ from human cognitive processes.