\input{sections/tables/dataset}

In this section, we present a comprehensive empirical analysis examining how well LLMs capture student misconception patterns in multiple-choice questions. Through two complementary analyses - one focusing on LLMs' generation probabilities and another on their actual answer selections - we investigate whether these models naturally encode patterns in how students select incorrect answers.

\subsection{Student Performance Dataset}

Our analysis uses a dataset of $3,202$ multiple-choice questions drawn from six core academic domains: mathematics, biology, physics, social science, reading comprehension, and humanities. 
These questions were collected from three established educational assessment platforms, with detailed subject distribution shown in Table~\ref{tab:dataset}. To ensure reliable analysis of student performance patterns, we applied two filtering criteria: (1) each question must have responses from at least 50 students, and (2) the error rate must exceed 5\% to enable meaningful analysis of misconceptions. For consistency in our analysis, we included only questions with exactly four answer choices. The aggregated student performance data shows an average correct response rate of 60.5\% across all subjects. 

\input{sections/tables/rq1}

\begin{figure}[t!]
    \centering
        \includegraphics[width=0.8\textwidth]{sections/images/pearson.pdf} 
    \caption{
    % Pearson correlation between LLM generation probabilities and student selection frequencies across model sizes, showing consistently stronger alignment with student reasoning for index-based approach (left) compared to text-based approach (right). Results shown for base and instruction-tuned variants of LLaMA and Qwen model families.
    Pearson correlation between LLM generation probabilities and student selection frequencies for incorrect answer choices (distractors) across model sizes. The index-based approach (left) measures correlation for A/B/C/D label selection probabilities, while the text-based approach (right) measures correlation for full distractor text generation probabilities. Results shown for base and instruction-tuned variants of LLaMA and Qwen model families demonstrate relatively stronger alignment between LLMs and student distractor selection patterns as model size increases, especially for text-based approach.
    }
    \label{img:rq1_alignment_size}
\end{figure}

\subsection{RQ1: Correlation Between LLM Generation Probabilities and Student Selection for Distractors}

Our first research question examines whether LLMs assign higher generation probabilities to the same incorrect answers that students commonly select. To investigate this systematically, we analyzed statistical correlations between LLM generation probabilities and student selection frequencies with Pearson's Correlation, Spearman's Rank Correlation and Kendall's tau Correlation (Details can be found in Appendix~\ref{app:stats-corr}). We tested this using two distinct probability measurement approaches: index-based, where models assign probabilities to answer labels (A/B/C/D), and text-based, where models compute generation probabilities for complete answer texts across different model sizes (0.5B to 72B parameters) and architectures (Qwen and LLaMA families\footnote{We use Qwen-2.5 for all parameter sizes; LLaMA-3.1 for 8b and 70b and LLaMA-3.2 for 1b and 3b parameter sizes.}). As shown in Table \ref{tab:rq1} and Figure \ref{img:rq1_alignment_size}, our analysis reveals several key patterns.

\subsubsection{Difference on Likelihood Approach:} The index-based approach shows better alignment between LLM likelihood and student selection distribution on distractors, with correlations ranging from 0.28 to 0.37 (Pearson). Most notably, qwen-14b achieves a correlation of 0.365 (Pearson) and 0.348 (Spearman) with student selection patterns - a moderate correlation given that these models were never explicitly trained to capture student misconceptions. This moderate correlation suggests that LLMs inherently encode some meaningful patterns about how students reason through multiple-choice questions, but are still not able to fully understand the cognitive processes underlying student misconceptions. Such natural alignment between LLM probabilities and student distractor preferences opens exciting possibilities for using these models to understand and predict student misconceptions.

The index-based approach consistently shows stronger correlations compared to text-based across all models and correlation metrics. This substantial difference (e.g., Qwen-72b: $0.352$ vs $0.183$ for Pearson correlation) is particularly illuminating as it mirrors how students actually approach multiple-choice questions. When students select answers, they typically compare options (A/B/C/D) simultaneously rather than evaluating each option's text in isolation. The index-based approach better captures this comparative decision-making process, while the text-based approach artificially forces sequential, independent evaluation of each option. This suggests that the stronger correlation in the index-based approach may stem from its better alignment with natural multiple-choice problem-solving strategies.


\input{sections/tables/rq2}

\subsubsection{Model Size Impact:} Figure \ref{img:rq1_alignment_size} reveals interesting scaling patterns across model sizes. For index-based approach, we observe a general upward trend in correlation as model size increases, with Qwen models showing particularly strong improvement from 0.5B to 14B parameters. The largest models maintain these strong correlations, demonstrating the benefits of model scale in capturing student reasoning patterns. The text-based approach shows more consistent improvement with scale across both model families, though with lower overall correlation values.

The figure also reveals that instruction-tuned variants (-instruct) of both model families generally show stronger correlations than their base counterparts, particularly noticeable in larger models. This suggests that instruction tuning may further enhance models' ability to capture student-like reasoning patterns. These findings provide quantitative evidence that LLMs' generation probabilities partially reflect student distractor preferences, particularly when the task format mirrors typical multiple-choice selection processes.



\subsection{RQ2: Alignment of LLM Mistakes and Student Misconception Patterns}

Our second research question examines whether LLMs, when answering incorrectly, tend to select the same distractors that commonly mislead students. To investigate this systematically, we analyzed incorrect answer selections from LLMs ranging from 0.5B to 72B parameters. For each LLM mistake, we tracked whether the model selected the distractor that was most commonly chosen by students ($1^{st}$ Dist), the second most common ($2^{nd}$ Dist), or the least common ($3^{rd}$ Dist). We tested this across two different prompting formats: index-based (where models select from A/B/C/D) and text-based (where models select the full answer text). The results in Table \ref{tab:rq2} reveal three key patterns:

\subsubsection{Similar Error Patterns Across Model Sizes:} Both small and large models show remarkably consistent patterns in selecting the first and second most common student distractors. For index-based prompting, the smallest model (Qwen-0.5B) selects the most common student distractor 51.6\% of the time and the second most common 32.6\% of the time, while the largest model (Qwen-72B) shows similar proportions at 59.3\% and 25.1\% respectively. This consistency suggests that the ability to capture student misconception patterns is not solely dependent on model size.

\subsubsection{A Cost-Effective Approach to Distractor Generation:} Our findings suggest a surprisingly cost-effective approach to automated distractor generation: using smaller language models. The key insight is that while smaller models make more mistakes, they make ``student-like'' mistakes. Specifically, when Qwen-0.5B ($n=1781$ incorrect answers) and Qwen-72B ($n=435$ incorrect answers) make mistakes, they show similar alignment with student error patterns, despite the dramatic difference in model size and overall accuracy. This opens up an efficient pathway for generating plausible distractors by:
\begin{enumerate}
    \item Leveraging smaller models' higher error rate to generate more potential distractors
    \item Using their natural alignment with student misconception patterns to ensure these distractors are pedagogically relevant
    \item Taking advantage of their computational efficiency and lower resource requirements
\end{enumerate}

\subsubsection{Impact of Likelihood Calculation Approach:} The method of presenting answer choices to LLMs significantly affects their alignment with student error patterns. Index-based prompting (using A/B/C/D) consistently shows higher alignment scores compared to text-based prompting across all model sizes. For example, Qwen-72B achieves a 59.3\% first-distractor selection rate with index-based prompting versus 42.6\% with text-based prompting, suggesting that simpler answer formats might better capture natural misconception patterns.
