\appendix




\section{Glossary}
\begin{table}[!htp]
\centering
\adjustbox{max width=\linewidth}{%
\begin{tabular}{lrr}\toprule
\textbf{Term} &\textbf{Definition} \\\midrule
Attention &\cellcolor[HTML]{fffdfa}search exposure, or likelihood of being viewed on a search result \\
Relevance &\cellcolor[HTML]{fffdfa}score per individual per query, where a higher score indicates better rankability \\
Dataset &\cellcolor[HTML]{fffdfa}set of individuals, with relevance scores available per individual per query \\
Cumulative attention &attention over a sequence of queries \\
\bottomrule
\end{tabular}}
\caption{Glossary of terms utilized in the paper.}\label{tab:glossary}

\end{table}

\section{Information Loss with Expectation-only Approaches} \label{sec:dist_example}
We observe that critical information about distributions of attention may be missing from prior formulations e.g., as shown in Figure~\ref{fig:distribution_reliability}, where the mean attention may match the mean relevance scores for an individual, but the distributions may be very dissimilar. Consider two distributions $A$ and $R$ defined as follows

\[
A = \Ncal(0, \sigma^2) \quad \quad \text{ and } \quad \quad R = 0.5\Ncal(-\mu_R, \sigma^2) + 0.5\Ncal(\mu_R, \sigma^2),
\]

Also, define $$\Tilde{R} = \Ncal(0, \sigma^2)$$

Clearly $\mu_{A} - \mu_{R} = \mu_{A} - \mu_{\Tilde{R}} = 0$. However, the distribution between attention and relevance is clearly not the same. Then, for $\sigma_{\tilde{R}} = \sigma_A < \sigma_R$, attention is spread out less broadly across individuals as relevance. Thus, attention is much more concentrated for some individuals, while relevance is not concentrated within the same individuals. Importantly, fairness metrics that only consider means would consider this setting fair.


Then 
\begin{align}
D_{\text{KL}}(A \| R) = \frac{1}{2} \left[\frac{\sigma_A^2}{\sigma_R^2} + \frac{(\mu_R - \mu_A)^2}{\sigma_R^2} - 1 + \log \frac{\sigma_R^2}{\sigma_A^2}\right].
\end{align}

Clearly, $D_{\text{KL}}(A \| \Tilde{R}) = 0 < D_{\text{KL}}(A \| R)$, better measuring the discrepancy between cumulative attention and relevance, unlike mean distance measures in previous work.

\begin{figure}[htb!]
\centering
\begin{minipage}[htb!]
{0.4\textwidth}
\includegraphics[width=\textwidth]{images/full_distribution_amortized_fairness.pdf}
\caption{Critical information about the distributions of relevance and attention (e.g., the variance) may be missing in such formulations.
}\label{fig:distribution_reliability}
\end{minipage}
\end{figure}

\section{Example of Fairwashing}

As shown in Figure~\ref{fig:fairwashing_amortized_ranking}, past formulations of amortized fair ranking may be prone to fairwashing.

\begin{figure}[htb!]
\centering
\begin{minipage}[htb!]
{0.5\textwidth}
\includegraphics[width=\textwidth,trim={1.5cm 11cm 0 5cm},clip]{images/Figure2.pdf}
\caption{\textbf{Past work in amortized fair ranking has ignored the impact of query polarity, and only considered expected cumulative attention}. 
Here, if all individuals are equally relevant, and expected attention scores for ranks 1,2,3 are $\{0.5,0.5,0\}$ respectively, the sequence of queries appear fair because an individual's expected attention accumulated over the four queries is proportional to their relevance. However, we the female doctor is allocated attention only for the queries with negative polarity (``rude",``bad dentist"). This leads to \emph{fairwashing}.
}\label{fig:fairwashing_amortized_ranking}
\end{minipage}
\end{figure}






\section{Datasets}
\label{sec:datasets}
We utilize four datasets in our experiments. In each dataset, relevance scores are normalized to form a distribution within a ranking. \texttt{fairtrec2021} is licensed under the CC BY-SA 3.0 license. \texttt{rateMDs} is released as a part of open-sourced research publication~\cite{thawani2019online}.

\subsection{Synthetic Hiring Datasets}
Two synthetic datasets are generated, to mimic the example shown in Figure~\ref{fig:fairwashing_amortized_ranking}. The sensitive attribute in each dataset is the sex of the individual being ranked. We generate two versions, one with two unique relevance scores, and one where the relevance is continuous. In the binary relevance dataset (\texttt{synth-binary}) the relevance for individuals is either 1.01 or 0.99, with male individuals having a relevance score of 1.01 for queries with positive polarity, and 0.99 for queries with negative polarity. This represents a nearly uniform relevance distribution. In the continuous dataset (\texttt{synth-cont}), the relevance in each group is sampled from a normal distribution, with mean and standard deviation $1$ and $0.2$ respectively for one group, and $1$ and $0.1$ for the other group per query (i.e., disparate uncertainty for groups)~\cite{rastogi2024fairness}. Each query in the dataset has a polarity value $\in \{-1,1\}$. Note that relevance scores are normalized within each query set before all re-ranking interventions.

\subsection{RateMDs}
We also utilize a healthcare dataset~\cite{thawani2019online} for ranking doctors corresponding to a text query. The sensitive attribute is sex. The original dataset contains reviews corresponding to 6197 profiles of doctors on \href{https://www.ratemds.com/}{RateMDs}. We consider each of these profiles to be a unique individual. We verified that the `name' metadata associated with each of these profiles is unique (including after lower-casing and removing punctuations).  Each ranking then corresponds to a text query such as ``short wait time". The ranking is produced by using a pre-trained LLM model\footnote{\url{https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L-2}} to match the text to reviews of doctors, and order the doctors in decreasing order of ranking score. The text-match score is averaged across all reviews corresponding to a given doctor for each query. Additionally, each query is associated with a polarity $\in \{-1,1\}$, which is annotated based on the sentiment polarity of the query (positive sentiment: 1, negative sentiment: -1). Note that some queries were specific to a speciality: e.g., ``best dentist", and relevance scores were produced for all doctors in the dataset (i.e., including doctors who have a different specialty). Thus, our results produced are highly influenced by the LLM-driven scores. Ideally, the scores allocated to doctors from different specialties will be low. In practice, while we observed some variance in top-selected items for different but similar queries, we do observe that some profiles occupy top positions in several rankings. We leave experiments with varying how the relevance score is produced to future work.\looseness=-1 

\subsection{FairTREC2021}
In this dataset, the items correspond to Wikipedia articles, and the query is the corresponding article domain~\cite{trec-fair-ranking-2021}. We utilize the standard evaluation split\footnote{\url{https://ir-datasets.com/trec-fair-2021.html\#trec-fair-2021/eval}} for experiments, which is a benchmark dataset used in multi-query fair ranking evaluation. The sensitive attribute is geographic location(s) referred to in the data, all of which we categorize into one of three groups (Europe/North America, No continent, or Others) to avoid non-overlapping groups. The query polarity score is a continuous score $\in [-1,1]$, and is produced by a pre-trained sentiment classification model~\cite{barbieri-etal-2020-tweeteval}. Specifically, the polarity score is the sum of the sentiment polarity of the query -- where -1 denotes negative, 0 denotes neutral, and 1 denotes positive -- weighted by predicted probabilities of each polarity class. We use the evaluation split of this dataset for all experiments. Note that the corpus corresponding to the FairTREC2021 dataset contains over 6.3M documents. However, the majority of these documents are not relevant to even a single query in the evaluation set. Thus, we filter the ranking corpus to documents marked as being relevant to at least one query for re-ranking before all experiments. Note that this dataset selection step is a normative choice, made with the assumption that relevance scores are unbiased: i.e., documents not relevant to any query need not be re-ranked for any higher attention. When making similar decisions at scale, it might be important to assess if there is a high degree of under-ranking, e.g., by expanding the query and corpus.

\section{Individual vs Group Fairness}







\subsection{Tail Probability Bounds for Cumulative Attention and Relevance}
\begin{theorem}
Let $X_i^t \sim \text{Bernoulli}(p_i^t)$ and
\[
    X_i = \sum_{t\in \Tcal} X_i^t.
\]
The expected value of $X_i$ is given by:
\[
    \EE[X_i] = \sum_{t\in \Tcal} p_i^t.
\]
\vspace{1em}
Then, for any $\delta > 0$, we have the following:
\[
    \mathbb{P}\left(|X_i - \EE[X_i]| \geq \delta \EE[X_i]\right) \leq 2\exp\left(-\frac{\delta^2 \mathbb{E}[X_i]}{2 + \delta}\right).
\]
\end{theorem}
\begin{proof}
    Assume that $X_i^t$'s are independent for different $t$ and observe that the domain of random variable $X_i^t$ is $\{0, 1\}$, i.e., bounded and non-negative. Using Chernoff bounds for the sum of independent Bernoulli random variables we have upper and tail bounds, respectively:

    \[
        P(X_i \geq (1 + \delta)\EE[X_i]) \leq \exp\left(-\frac{\delta^2 \EE[X_i] }{2 + \delta}\right),
    \]
    \[
        P(X_i \leq (1 - \delta)\EE[X_i]) \leq \exp\left(-\frac{\delta^2 \EE[X_i]}{2}\right).
    \]
    Applying a union bound for both the upper and lower tails, we have:
    \[
        P\left(|X_i - \EE[X_i] | \geq \delta \EE[X_i]\right) \leq 2 \exp\left(-\frac{\delta^2 \EE[X_i]}{2 + \delta}\right).
\]
\end{proof}
\subsection{Proof of Lemmas}
\begin{lemma}
Define the following:
    \begin{align*}
        D_{L_1}(P \| Q) &= |\mu_P - \mu_Q|
    \end{align*}

    $D_{L_1}$ satisfies definition \ref{def:divergence} for $P$ and $Q$ when $\mu_P$ and $\mu_Q$ are sufficient statistics for their respective distributions. 
\end{lemma}
\begin{proof}
We prove that $D_{L_1}(P \| Q) = |\mu_P - \mu_Q|$ satisfy the following properties:

\paragraph{1. Non-negativity:} \\
For $D_{L_1}(P \| Q)$, the expressions involve absolute values, which are non-negative by definition. Thus,
\begin{align}
&D_{L_1}(P \| Q) = |\mu_P - \mu_Q| \geq 0.
\end{align}

\paragraph{2. Positivity:} \\
For $D_{L_1}(P \| Q) = |\mu_P - \mu_Q|$, we have $D_{L_1}(P \| Q) = 0$ if and only if $\mu_P = \mu_Q$. Since $\mu_P$ and $\mu_Q$ are sufficient statistics, $\mu_P = \mu_Q$ implies $P = Q$, and conversely, if $P = Q$, then $\mu_P = \mu_Q$.


\paragraph{3. Subadditivity:}
For \( D_{L_1}(P \| Q) \), we verify subadditivity under convolution in terms of the mean-difference divergence:
\[
D_{L_1}(P_1 \circ P_2 \| R_1 \circ R_2) \leq D_{L_1}(P_1 \| R_1 ) + D_{L_1}(P_2 \| R_2).
\]
Since expectation is linear under convolution:
\[
\mu_{P_1 \circ P_2} = \mu_{P_1} + \mu_{P_2},  \quad \mu_{R_1 \circ R_2} = \mu_{R_1} + \mu_{R_2},
\]
the mean-difference divergence simplifies to:
\[
|\mu_{P_1 \circ P_2} - \mu_{R_1 \circ R_2}|
= |(\mu_{P_1} + \mu_{P_2}) - (\mu_{R_1} + \mu_{R_2})|.
\]
Rewriting,
\[
|\mu_{P_1} - \mu_{R_1} + \mu_{P_2} - \mu_{R_2}|
\]
Applying the triangle inequality,
\[
|\mu_{P_1} - \mu_{R_1} + \mu_{P_2} - \mu_{R_2}| \leq |\mu_{P_1} - \mu_{R_1}| + |\mu_{P_2} - \mu_{R_2}|.
\]
Thus, subadditivity holds:
\[
D_{L_1}(P_1 \circ P_2 \| R_1 \circ R_2) \leq D_{L_1}(P_1  \| R_2) + D_{L_1}(P_2 \| R_2).
\]


\paragraph{4. Scaling over averages:}
For $D_{L_1}(P \| Q)$, scaling over averages refers to how the divergence behaves when comparing averages (means) of distributions.

\[
\left| \frac{\mu_{P_1} + \mu_{P_2}}{2} - \frac{\mu_{Q_1} + \mu_{Q_2}}{2} \right| \leq \frac{|\mu_{P_1} - \mu_{Q_1}| + |\mu_{P_2} - \mu_{Q_2}|}{2}
\]
Again, this holds due to the triangle inequality for absolute values.

\paragraph{5. Positive homogeneity with degree one:}
For $D_{L_1}(P \| Q)$, $\alpha > 0$:
\[
D_{L_1}(\alpha P \| \alpha Q) = |\alpha \mu_P - \alpha \mu_Q| = \alpha |\mu_P - \mu_Q| = \alpha D_{L_1}(P \| Q),
\] where here scaling denotes scaling the random variables corresponding to the distribution.
Thus, positive homogeneity holds.

\end{proof}




\subsection{Individual Fairness Upper-Bounds Group Fairness}
\label{ref:sec_proof}
\begin{theorem}
    For any jointly convex DistFaiR divergence that is subadditive under the convolution operation, positively homogeneous with degree $s$, and scales under averages, amortized group fairness is upper-bounded by amortized individual fairness. Specifically, we have the following inequality:
    \begin{align}
        \max_{g_k \in \Gcal} D(A_{g_k} \| R_{g_k}) \leq   
 \max_{i \in \Dcal} D(A_i \| R_i) \hspace{0.5cm},
    \end{align}
where $A$ and $R$ are distributions that denote attention and relevance, respectively, individuals $i\in \{1, \ldots, n\}$, and $g_k$ denotes the set of individuals $i$ that belong to group $k$.
\end{theorem}

\begin{proof}
\newcommand{\Xgk}{X_{g_{k}}}
\newcommand{\Ygk}{Y_{g_{k}}}
\newcommand{\sumXgk}{\frac{1}{|g_k|}\sum_{i\in g_k} X_{i}}
\newcommand{\sumYgk}{\frac{1}{|g_k|}\sum_{i\in g_k} Y_{i}}
Let $A_i, R_i$ denote the distributions of random variables $X_i, Y_i$, respectively. 

{\bf Assume $D$ is subadditive, positively homogeneous, and scales under averages for transformations to the random variable corresponding to the distributions in each case.}

Denote 
\begin{equation}
    X_{g_k} = \frac{1}{|g_k|}\sum_{i\in g_k} X_i \quad \text{ and } \quad Y_{g_k} = \frac{1}{|g_k|}\sum_{i\in g_k} Y_i,
\end{equation}

such that, by scaling property of $D$,

\begin{equation}
    X_{g_k} \sim A_{g_k} \quad \text{ and } \quad Y_{g_k} \sim R_{g_k}.
\end{equation}

Denote $$X'_i = \frac{1}{|g_k|}X_i \quad \text{ and } \quad Y'_i = \frac{1}{|g_k|}Y_i$$ s.t. $$X'_i \sim A'_i \quad \text{ and } \quad Y'_i \sim R'_i .$$

$A_{g_k} = A'_1 \circ A'_2 \circ \ldots \circ A'_{|g_k|}$ and $R'_{g_k} = R'_1 \circ R'_2 \circ \ldots \circ R'_{|g_k|}$, where $\circ$ denotes convolution. Recall that $X_i$'s and $Y_i$'s are independent.

\begin{align}
    D(A_{g_k}  \| R_{g_k}) &\le \sum_{i\in g_k} D(A'_i \| R'_i) \label{eq:subadditivity} \\
    &= \frac{1}{|g_k|^s}\sum_{i\in g_k} D(A_i \| R_i) \label{eq:homogeneity} \\
    &\le \frac{1}{|g_k|^{s-1}} \max_{i\in g_k} D(A_i \| R_i) \\
    &\le \max_{i\in \Dcal} D(A_i \| R_i) \label{eq:assumption_groups},
\end{align}

where Equation \ref{eq:subadditivity} is a result of subadditivity and Equation \ref{eq:homogeneity} is a result of positive homogeneity~\cite{kanamori2014scale} with degree $s$. Specifically, subadditivity of a divergence under convolution refers to divergences $D$ satisfying the property:
\[
D(P_1 \circ P_2 \| Q_1 \circ Q_2) \leq D(P_1 \| Q_1 ) + D(P_2 \| Q_2).
\] for some distributions $P_1, P_2, Q_1, Q_2$.

Taking the max over all groups,
\begin{equation}
    max_{g_k \in \Gcal} D\big(A_{g_k} \| R_{g_k}\big) \le \max_{i \in \Dcal} D(A_i \| R_i),
\end{equation}
completes the proof. Note that Equation \ref{eq:assumption_groups} holds when $|g_k| \geq 1$, which is an underlying assumption in the fairness measurement. We experimentally validate this theorem in Figure~\ref{fig:indiv_group}.

Note that this bound becomes especially pertinent when there are a large number of groups, and the ranges of group and individual fairness are similar.  



















\end{proof}








































\subsection{Tail Probability Bounds for Polarity-Aware Cumulative Attention and Relevance}
\begin{theorem}
Let $X_i^t \sim \text{Bernoulli}(p_i^t)$ and $\eta(q_t) \in [a_t, b_t]$; $a_t, b_t \in \RR$. With a slight abuse of notation, let $\Tilde{X}_i^t = X_i^t\cdot \eta(q_t) \in [a_t, b_t]$ and
\[
    \Tilde{X}_i = \sum_{t\in \Tcal} \Tilde{X}_i^t,
\]
The expected value of $\Tilde{X}_i$ is given by:
\[
    \mathbb{E}[\Tilde{X}_i] = \sum_{t\in \Tcal} \eta(q_t) \cdot p_i^t.
\]
Then, for any $\delta > 0$, we have the following:
\[
    P\left(|\Tilde{X}_i - \EE[\Tilde{X}_i]| \geq \delta\right) \leq 2\exp\left(-\frac{2\delta^2}{\sum_{t \in \Tcal} (b_t - a_t)^2}\right).
\]
\end{theorem}

\begin{proof}
Assume that $\Tilde{X}_i^t$'s are independent for different $t$ and observe that each $\Tilde{X}_i^t \in [a_t, b_t]$, i.e., bounded. Using Hoeffding's inequality for the sum of independent bounded random variables, we have:

\[
    P\left( \Tilde{X}_i \geq \mathbb{E}[\Tilde{X}_i] + \delta \right) \leq \exp\left(-\frac{2\delta^2}{\sum_{t \in \mathcal{T}} (b_t - a_t)^2}\right),
\]
\[
    P\left( \Tilde{X}_i \leq \mathbb{E}[\Tilde{X}_i] - \delta \right) \leq \exp\left(-\frac{2\delta^2}{\sum_{t \in \mathcal{T}} (b_t - a_t)^2}\right).
\]

By applying a union bound for the upper and lower tails, we get:

\[
    P\left( |\Tilde{X}_i - \mathbb{E}[\Tilde{X}_i]| \geq \delta \right) \leq 2 \exp\left( -\frac{2\delta^2}{\sum_{t \in \mathcal{T}} (b_t - a_t)^2} \right).
\]
\end{proof}


 

 























\input{sections/Table_figr}

\section{Xing Dataset}
\label{app:xing_dataset}
The Xing dataset~\cite{zehlike2017fa} contains top-ranked candidates in response to 57 queries submitted on \href{http://xing.com}{Xing}, all in the context of online hiring. Candidate information is anonymized. All candidates are categorized into two groups (male and female) based on their sex. Note that unique IDs are not available for each individual, as all candidate names, pictures etc. were either ``removed or obfuscated by hashing"~\cite{zehlike2017fa}. Further, relevance scores are not available for any candidate or query.  

To use this dataset in our study, we make some assumptions. First, we assume that the top-40 individuals in each ranking are distinct. This leads to a dataset of 57 queries and 2236 individuals. To illustrate the impact of polarity, we also assume that the four specific queries related to software engineering -- `Software Engineer',`Back end Developer',`Front End Developer',`Application Developer' -- have a polarity of one, and the rest have a polarity of zero. This simulates a setup where e.g., the recruiter is searching specifically to hire a software engineer. Since there are no relevance scores in this dataset, we assume that male and female candidates are equally relevant (i.e., equally worthy of attention).


Since an individual, under this setting, can only appear once (i.e., corresponding to a single query), amortized individual fairness is less meaningful. Thus, we only measure group fairness. We measure different group fairness measures including DistFaiR on the full dataset. In Table~\ref{tab:xing_dataset}, we measure proportion of change in (un)fairness when polarity is considered with respect to the polarity agnostic fairness measurement, randomly sampling queries with replacement several times.   Similar to results in the main text, a positive value indicates that unfairness is higher when polarity is considered, or that the rankings were actually more unfair than they appeared. Thus, we find that most group fairness metrics are sensitive to query polarity, and vary across distance measures on the Xing dataset as well.  

\begin{table}[!htp]\centering
\begin{tabular}{c|rr}
\toprule
\textbf{Metric} &\textbf{Group Fairwashing} \\\midrule
$L_1$ &0.16 \\
$L_2^{\text{var}}$ &0.40 \\
$W_1$ &-0.90 \\
DP &0.16 \\
EUR &15.20 \\
IAA &0.16 \\
\bottomrule
\end{tabular}
\caption{Impact of query polarity on fairness measurement for the Xing dataset. Results are averaged over 10 sets, each a random sample of queries with replacement.}\label{tab:xing_dataset}

\end{table}





\section{Integer Linear Programming Formulation}
\label{sec:ilp_appendix}
We rely on the Gurobi software~\cite{achterberg2019s} for each optimization. Within the solver, the optimality gap~\cite{bixby2007progress} is measured during each optimization search iteration. The Gurobi solver also uses routines of branch-and-bound~\cite{wolsey1999integer}, presolve~\cite{achterberg2020presolve}, cutting planes~\cite{marchand2002cutting}, etc. We emphasize that our framework - DistFaiR â€“ can also be applied to continuously relaxed, more computationally efficient formulations. We highlight this as an important direction of future work. In implementation, we introduce an auxilary variable upper-bounding unfairness across all individuals (so worst-case unfairness), which is then minimized. 

Note that we ignore ties in system relevance scores in all cases. We verified that variations in DCG@K on accounting vs not accounting for tied relevance scores are very small (less than 0.0001). In implementation, we also pre-filter and re-rank only top-k individuals in each ranking. Thus,  our results are sensitive to this pre-filtering choice. As also mentioned in prior work~\cite{biega2018equity}, different pre-filtering choices may lead to different degrees of fairness gains. We leave the exploration of different pre-filtering choices to future work.


\section{Individual Fairness Bounds Group Fairness under DistFaiR.}
\label{sec:fairness_bounds}
Theorem~\ref{theorem:indiv_group} shows that group (un)fairness is upper-bounded by individual (un)fairness for some classes of distance functions. In Figure~\ref{fig:indiv_group}, we experimentally validate this by computing group and individual unfairness for the $W_1$ divergence measure.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{images/individual_vs_group_fairness.pdf}
    \caption{Individual Fairness Bounds Group Fairness under DistFaiR (here, $DistFaiR(W_1)$)}
    \label{fig:indiv_group}
\end{figure}


\section{Note on Query Polarity Aware Ranking}
Note that we can also utilize normalized versions of query polarity, e.g., with softmax-based normalization.  However, normalization in this manner ensures may convert queries with negative polarity to positive -- thus we use unnormalized scores in our experiments. 


\section{Additional Baseline: FIGR}

We show performance of FIGR in Table~\ref{tab:figr}. Due to the difference in optimization objective, and the fact that FIGR optimizes fairness per ranking (i.e., proportion-based group and individual unfairness per ranking), we find that this baseline underperforms DistFaiR as well as other exposure-based fairness interventions. We also rely on the official open-sourced implementation of the algorithm for experiments, and only consider binary groups as in the paper (results with more groups showed similar trends). We consider an audit interval length of $k=10$, and observe similar results for different $k$.\looseness=-1

\section{Fairness Over Time}
We observe that variance in online fairness is lower for divergence measures that use higher order moments in Figure~\ref{fig:online_fairness}, on the \texttt{rateMDs} dataset. Verifying this across more divergence measures is thus an interesting direction of future work.
\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.6\linewidth]{images/time_varying.pdf}
    \caption{Online fair ranking fairness on \texttt{rateMDs}.}
\label{fig:online_fairness}
\end{figure}

\section{Additional Results: Impact of Re-ranking on Group Fairness}


Re-ranking interventions optimized to improve individual fairness tend to improve or retain EUR as seen in Table~\ref{tab:group-re-ranking} for atleast one divergence measure on three of four datasets. Importantly, DistFaiR underperforms IAA on the IAA individual fairness measurement which makes sense because DistFaiR focuses on worst-case distance between individuals, while IAA focuses on average across individuals. Thus, there are tradeoffs between average and worst-case performance as seen in other fairness contexts~\cite{yang2023change}. Note that we set degree of permissible performance loss (i.e., least possible nDCG) to 80\%, which all methods exceed. 




\begin{table}[htb!]
\centering
\caption{\textbf{DistFaiR also improve IAA and EUR in a majority of cases (here, positive, higher is better). However, there are some distance and dataset-dependent variations.}  We show change in fairness when compared to the unconstrained ranking. IAA baseline outperforms DistFaiR on the IAA fairness measurement}\label{tab:group-re-ranking}
\adjustbox{max width=0.7\linewidth}{%

\begin{tabular}{lccccc}
\toprule
Dataset & Baseline & \multicolumn{2}{c}{Fairness} & nDCG@10 \\
\cmidrule(r){3-4}\\
 & & IAA & EUR &  \\
\midrule

\multirow{4}{*}{\texttt{synth-binary}} 
& IAA & \textbf{68.88}\%  & 8.84\% & 100\%\\
 & FoE &  13.31\% & 20.81\% & 100\% \\
 & DistFaiR($L_1$) &  58.32\% & 47.05\%& 100\% \\

 & DistFaiR($L_2^{var}$) &  38.99\% & 	59.05\% & 100\%\\
 & DistFaiR($W_1$) & 43.39\% & \textbf{76.10}\% & 100\%  \\
 
\midrule

\multirow{4}{*}{\texttt{synth-cont}} 
& IAA & \textbf{45.37}\% & \textbf{38.52\%} & 92\%\\
  & FoE &  1.26\% & -139.41\% & 98\% \\
  & DistFaiR($L_1$) &  27.96\% & -36.10\% & 88\% \\

 & DistFaiR($L_2^{var}$) &  34.39\% & 	-39.66\% & 87\%\\
 & DistFaiR($W_1$) & 0.47\% & -125.17\% & 86\%  \\


\midrule

\multirow{5}{*}{\texttt{FairTREC2021}} 
& IAA & \textbf{7.55}\% & 41.50\%  & 100\%\\
& FoE & 4.40\% & 41.07\%  & 100\%\\


  & DistFaiR($L_1$) & 6.12\%  & 45.54\%  & 100\%\\

 & DistFaiR($L_2^{var}$) &  4.45\% & 	43.40\%  & 100\%\\
 & DistFaiR($W_1$) & 6.07\%  & \textbf{46.57\%}  & 100\%\\

\midrule
\multirow{4}{*}{\texttt{rateMDs}} 
& IAA & \textbf{5.67}\% & -7.94\% &  89\% \\
 & FoE & -0.32\% & 26.96\% & 97\% \\

 & DistFaiR($L_1$) &  -0.41\% & \textbf{62.19\%} &  85\%  \\
 & DistFaiR($L_2^{var}$) &  -0.60\% & 39.99\%&  84\% \\
 & DistFaiR($W_1$) &  -1.68\% & 57.41\%&  84\% \\

\bottomrule
\end{tabular}
}
\end{table}





\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/online_offline_unfairness.pdf}
    \caption{Online fair ranking -- where queries arrive one after the other for ranking -- underperforms or performs similarly as offline fair ranking where the whole set of queries is known apriori. However, the degree of difference is small. The $L_1$ distance function is used for this figure.}
    \label{fig:online_offline_optimization}

\end{figure}


\section{Performance Trade-offs with Query Polarity}
Similar to Figure~\ref{fig:varying_theta}, we observe both performance-fairness trade-offs, and the impact of polarity on fairness optimization on the \texttt{rateMDs} (see Figure~\ref{fig:tradeoffs_ratemd}) dataset. 
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/tradeoffs_ratemd.pdf}
    \caption{Fairness over different choices of $\theta$ for \texttt{rateMDs}.}
\label{fig:tradeoffs_ratemd}

\end{figure}

\section{Online vs Offline Optimization}
We observe that online optimization underperforms or performs similarly fully offline fairness optimization on three of the four datasets (see Table~\ref{fig:online_offline_optimization}).


\section{Multiple properties per query}
\label{sec:multiple_properties}

\begin{table}
\caption{Impact of fair ranking with a vector of query properties}\label{tab:vector}
\adjustbox{max width=\linewidth}{
\begin{tabular}{llrr}
\toprule
 &  & Pre-intervention & Post-intervention \\
Dataset & Measure &  &  \\
\midrule
\multirow[t]{4}{*}{\texttt{synth-binary}} & IAA & 27.75 & 13.78\\
 & DistFaiR($L_1$) & 2.21 & 0.60 \\
\cline{1-4}
\multirow[t]{4}{*}{\texttt{synth-cont}} & IAA & 17.82 & 13.26 \\
 & DistFaiR($L_1$) & 0.92 & 0.59 \\
\bottomrule
\end{tabular}}
\end{table}
We empirically conduct experiments where each query in the synthetic dataset contains three total properties. We define fairness as the sum of fairness metrics with each component considered separately. 


We observe that online optimization reduces unfairness from across metrics in the \texttt{synth-binary} and \texttt{synth-cont} datasets in Table~\ref{tab:vector}.

