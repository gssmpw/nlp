\begin{figure*}[ht!]
\centering
\begin{subfigure}
    \centering
    \includegraphics[width=0.31\textwidth,trim={0 0.25cm 0 0},clip]{images/multi_dataset_individual_fairwashing.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[width=0.31\textwidth,trim={0 0.25cm 0 0},clip]{images/multi_dataset_group_fairwashing.pdf}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[width=0.31\textwidth,trim={0 0.25cm 0 0},clip]{images/tradeoffs.pdf}
\end{subfigure}
\caption{(a) and (b) show the difference (as relative change) between fairness metrics measured with and without query polarity. Query polarity impacts all amortized fairness metrics, as they differ from zero as seen in the plots. (rightmost) We plot the re-ranking performance of polarity agnostic and aware re-rankings under different permissible performance loss changes for the \texttt{synth-cont} dataset (DistFaiR($L_1$)), where we can see polarity agnostic re-ranking underperforms polarity aware re-ranking. %
}\label{fig:varying_theta}


\end{figure*}
\vspace{-0.5em}
\section{Results}

We measure the percentage change in unfairness pre- and post- re-ranking. A positive change -- decrease in unfairness -- is desired.

\paragraph{DistFaiR Improves Worst-Case  Fairness}
Table ~\ref{tab:full_results_main} shows that our re-rankings reduce individual unfairness, when unfairness is measured as the worst-case divergence measure between the attention and relevance distributions across individuals. We find that DistFaiR outperforms or performs on par with IAA. FIGR (Table~\ref{tab:figr})---which solves a different notion of ``underranking"---does not improve performance as measured by our metrics. Further, as expected, optimizing the divergence measure itself often leads to highest decrease in unfairness (for example, DistFaiR($W_1$) has highest improvement in fairness for the $\Delta$DistFaiR($W_1$) individual fairness measurement). Note that not all differences were statistically significant.

Additionally, as seen in Appendix Table~\ref{tab:group-re-ranking},  DistFaiR underperforms IAA-based re-ranking on the IAA metric. This makes sense because DistFaiR focuses on reducing worst-case divergence, while IAA focuses on the average across individuals. Thus, there appear to be tradeoffs between average and worst-case performance. Such observations have also been made in other fairness contexts~\cite{yang2023change}. %

\paragraph{Divergence Metric is an Important Design Choice.}
Our results show that the divergence metric is an important design choice. We find that the performance of $L_1$ and $L_2^{\text{var}}$ are close  (e.g., on \texttt{FairTREC2021}). We hypothesize that the  optimization with $W_1$ is more difficult, due to which performance improvements are smaller. Note that $L_2^{\text{var}}$ is the $W_2$ solution under assumptions of gaussianity. It is possible that using the $L_2^{\text{var}}$ measure could be an easier objective, but we can remove the distribution assumption for the general $W_2$.\looseness=-1 


\paragraph{Individual Fairness Not Always at Odds with Group Fairness.}
Reducing individual unfairness under DistFaiR also reduces group unfairness in most cases (Table~\ref{tab:full_results_main}), as averaged across test splits, even without imposing group-level constraints.  While group unfairness does increase in some cases, the degree of change cannot exceed a specific limit (upto individual unfairness) as per our theoretical findings.  
We also see similar trends on a standard group fairness metric,  EUR~\cite{morik2020controlling} (see Table~\ref{tab:group-re-ranking} in Appendix). Interestingly, the IAA baseline almost always improves group fairness, though  DistFaiR reduces unfairness to a higher degree on two datasets. We also observe higher variance for group unfairness, potentially due to multiple solutions with same individual but different group unfairness.\looseness=-1 







\paragraph{Online vs Offline Optimization.}
We observe that fully offline optimization reduces unfairness equally or more effectively than fully online optimization (Appendix Figure~\ref{fig:online_offline_optimization}). Thus, even if the full set of queries is not known apriori, partial offline optimization could be useful when a subset of queries is available. Experimentally, variance in online fairness is lower when optimizing for divergences beyond mean-based differences (\texttt{rateMDs} dataset; Figure~\ref{fig:online_fairness}) over time.\looseness=-1 


\paragraph{Fairness Metrics are Sensitive to Query Polarity.}
\label{sec:query_polarity}
In Figure~\ref{fig:varying_theta} (a) and (b), we compute the relative change between fairness metrics measured with and without query polarity, averaged across tuning splits. We observe that all fairness metrics, for both individual and group fairness, are sensitive to query polarity. When the relative change is positive, this indicates fairwashing: rankings seem more fair than they actually are.
We observe that fairwashing occurs, especially for group fairness metrics. Thus, if one relies on the query polarity agnostic metrics, conclusions regarding the (un)fairness of the rankings would be incorrect. It may also be important to consider divergence measures beyond difference in means to avoid systematic under-ranking of a specific group across queries.\looseness=-1 


\paragraph{Ranking Quality and Fairness Tradeoff.}
We study the variation in fairness across thresholds of allowable ranking quality loss ($\theta$) in the ILP optimization. Lower unfairness is observed at lower $\theta$ for the polarity-aware re-ranking  (Figure~\ref{fig:varying_theta} (c)), indicating a ranking quality and fairness tradeoff. Additionally, polarity agnostic re-ranking performance leads to higher (worse) unfairness than when query polarity is used. This matches our discussion that fairness metrics are sensitive to query polarity. Experimentally, higher standard deviation is observed in polarity-aware ranking. We also observe similar trends on the \texttt{rateMDs} dataset (Appendix Figure~\ref{fig:tradeoffs_ratemd}).

Importantly, in many real-world applications, different queries may have multiple differing real-world properties beyond polarity.
Accordingly, we can generalize our distribution-aware fairness definition to allow multiple query properties as a vector, where multiple properties form a multi-dimensional distribution. Initial results with this setup for the synthetic datasets are in the Appendix~\ref{sec:multiple_properties}.





