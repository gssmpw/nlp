\input{sections/Table_indiv}

\section{Experiments: Online Fair Ranking}  

Our experiments are focused on an \emph{online fair ranking setup}, similar to ~\cite{biega2018equity}. We assume a realistic setup where a new query arrives at each time $t$, and we re-rank the system-produced ranking at time $t$ to improve fairness. We assume knowledge of attention allocated to individuals in rankings till time $t$ to produce this new fair ranking (i.e., a running memory of cumulative attention per individual)\footnote{Code: \url{https://github.com/MLforHealth/DistFaiR}}.  


\subsection{Experimental Setup}
\paragraph{Datasets} We utilize two synthetic datasets which represent the setting described in the example shown in Figure~\ref{fig:fairwashing_amortized_ranking} where female individuals are allocated attention in four out of eight rankings (all with negative polarity) and two real-world fair ranking datasets~\cite{thawani2019online,trec-fair-ranking-2021}. A dataset summary is in Table~\ref{tab:ds_summary} and further details are provided in Appendix~\ref{sec:datasets}. We also benchmark the impact of query polarity on the Xing dataset~\cite{zehlike2017fa} in the Appendix (see Appendix~\ref{app:xing_dataset}).  Our empirical study focuses on post-processing fairness interventions, where individual relevance -- or ``groundtruth" -- scores are known~\cite{gorantla2023sampling}.


\paragraph{Query Properties} We experiment with polarity as the query property. The polarity score is synthetically generated for \texttt{synth-binary} and \texttt{synth-cont} and manually annotated for \texttt{rateMDs}. For the \texttt{FairTREC 2021} dataset, a pre-trained sentiment classification model is used to generate polarity~\cite{barbieri-etal-2020-tweeteval} (see Appendix~\ref{sec:datasets}). 

\paragraph{Distance Functions}
\label{sec:distance_metrics}
We consider three (pseudo) divergences metrics for measuring unfairness under DistFaiR:
\begin{itemize}[leftmargin=0.2in]
    \setlength\itemsep{0em}
    \item $\mathbf{L_1}$ distance is defined as the difference between the mean of two distributions: $D_{L_1}(A \| R) = |\mathbb{E}_{X \sim A}[X] - \mathbb{E}_{Y \sim R}[Y]|$. 
    \begin{itemize}
        \item This distance function has been studied in ~\cite{biega2018equity}, where fairness is computed as the sum of distance values across individuals and is referred to as the inequity of amortized attention (IAA).  We note that this function is generally not a proper divergence. However, for distributions $A$ and $R$ whose first moments are sufficient statistics, $D_{L_1}$ satisfies definition \ref{def:divergence}.
    \end{itemize}
    \item $\mathbf{L_2^{\text{var}}}$ distance is defined as the difference in mean and variance of two distributions\footnote{We use squared differences as we expect a square root of this to perform similarly.}: 
    \begin{align*} D_{L_2^{\text{var}}}(A \| R) &= (\mathbb{E}_{X \sim A}[X] - \mathbb{E}_{Y \sim R}[Y])^2 \\&+ (\sigma{}_{X \sim A}[X] - \sigma_{Y \sim R}[Y])^2.\end{align*}
    We note that $D_{L_2^{\text{var}}}$ benefits from $W_2$, a proper divergence, for two Gaussians, which has the properties for Theorem \ref{theorem:indiv_group}. %
    \item $\mathbf{W_{1}}$ distance is defined as the Wasserstein distance between the distribution of expected attention ($\{a_i^t\}_{t=1}^{\mathcal{T}}$) and distribution of expected relevance ($\{r_i^t\}_{t=1}^{\mathcal{T}}$) for an individual. $D_{W_1}(A \| R)=\frac{1}{T}\sum_{k=1}^{T} |a_i^{(k)} - r_i^{(k)}|$, where $(k)$ denotes the $k$th order statistic of empirical measures $\hat{A}_i$ and $\hat{R}_i$ from which $a_i^t$ and $r_i^t$ is sampled. 
    
\end{itemize}
\subsection{Evaluation}
\label{sec:metrics}
We utilized the following fairness criteria.



\paragraph{Individual Unfairness:} We use three different distance measures defined in Section~\ref{sec:distance_metrics} to measure the unfairness as:  DistFaiR($L_1$), DistFaiR($L_2^{\text{var}}$), and DistFaiR($W_1$).
The amortized fairness defined by DistFaiR($L_1$) is similar to the IAA fairness measure studied by \cite{biega2018equity}. However, we consider the \emph{worst-case} distance between attention and relevance distributions, while \cite{biega2018equity} consider the sum of difference across all individuals, which may hide heightened unfairness in some individuals. Our work also generalizes amortized fairness to include appropriate measurements of discrepancies between distributions that require higher-order moments to be specified, i.e., with $L_2^{\text{var}}$ and $W_1$ distances.



\paragraph{Group Unfairness:}
In addition to the group unfairness metrics directly induced by the three distance metrics using Definition~\ref{def:group-unfairness}, we consider a standard exposure-based group unfairness definitions: Exposed Utility Ratio (EUR). \cite{singh2018fairness,morik2020controlling} define the EUR difference as the absolute difference in the ratios of average exposure and average relevance between groups. We also measure an attention parity metric: Demographic Parity\cite{morik2020controlling} (DP). 


\paragraph{Performance}
\label{sec:perf}
We measure the ranking quality via the DCG@K score, which is the sum of the relevance of the top-K individuals, with a logarithmic discount based on their position: 
$$\sum_{k=1}^{K}\frac{r_{\text{rank}(k)}^t}{log_2(k+1)},$$
where ${\text{rank}(k)}$ returns the index of the individual at rank $k$. After re-ranking, the DCG@K is normalized by the DCG@K of the previous (ideal) ranking to produce a normalized DCG@K between 0 and 1. 


\subsection{Baselines: Fair Re-ranking }

\textbf{IAA}: A method to reduce inequity of amortized attention (IAA) introduced by Biega \emph{et al.}\cite{biega2018equity}. An ILP is solved to reduce the absolute difference in the mean of the cumulative attention and cumulative relevance distributions, summed across all individuals. In contrast, our method focuses on \emph{worst-case} minimization.

\textbf{FoE}: A linear program for ranking assignments with Birkhoff Von Neumann decomposition~\cite{lewandowski1986algorithmic} is solved to ensure fairness of exposure (FoE)~\cite{singh2018fairness}. The quality of rankings is maximized, with the constraint that the cumulative attention to relevance ratio is the same for all individuals. We re-rank only top-k individuals in each ranking. The original ranking is returned if solution is infeasible.\looseness=-1


\textbf{FIGR}~\cite{gorantla2021problem}: This method jointly aims to reduce ``underranking" (which is closely related to individual fairness) in rankings that are post-processed with group fairness constraints. Unlike the other baselines, this is a proportion-based re-ranker for each ranking, and does not explicitly consider attention distributions. Thus, we present results for this baseline in the Appendix. 

\vspace{-0.5em}
\subsection{Hyperparameter Tuning}
We stratified all datasets into two subsets: 50\% tuning and 50\% test sets, so no individuals or queries are present in both splits. All parameters (e.g., $\theta$; when tuned) are tuned using the tuning split. For \texttt{FairTREC 2021}, we use the full evaluation split, and do not perform any additional tuning -- we sample queries with replacement thrice to obtain variance. We run all optimization algorithms on a 3.2 GHz CPU with 16 GB RAM for $\leq$ 60 minutes, with a feasibility tolerance of $1e-9$. We set K=10 while measuring ranking quality and assume logarithmic discounts in attention till K=10 and zero otherwise.

We also pre-filter~\cite{biega2018equity}, and only re-rank the top-k individuals in each ranking. For moment-based divergences, $L_1$ and $L_2^{var}$,  we minimize maximum divergence only among the top-k at each step, as we found that this performs better. This means that even when the maximum divergence measure across all individuals cannot be reduced, we still re-rank to reduce the next possible highest divergence value. For $W_1$, we minimize divergence across all individuals. For the FoE baseline, constraints are set only for individuals in the top-k positions to make re-ranking feasible. Note that our results are sensitive to these pre-filtering choices. Post-tuning, we find that $k=50$ works well across datasets.

Our experimental flow is as follows: first, we implement our fair ranking definitions (DistFaiR) and compare to baselines. Second, we test if fairness metrics are affected by query polarity. Third, we perform several ablations for, e.g., the fairwashing effect.

