\section{Background and Related Work}
\vspace{-0.9em}
\paragraph{Fair Ranking Metrics and Interventions.} Rankings with high ranking quality may be unfair at a group or individual level~\cite{biega2018equity, diaz2020evaluating, singh2018fairness, morik2020controlling, bower2020individually,zehlike2021fairness,mehrotra2022fair}. Previous works have proposed various techniques to quantify and mitigate unfairness by allocating exposure or visual attention proportionally to relevance at a group or individual level~\cite{yang2023vertical,biega2018equity, diaz2020evaluating, singh2018fairness, morik2020controlling, bower2020individually,zehlike2021fairness,heuss2022fairness}. Some of these are in-processing (i.e., during ranking generation )~\cite{singh2019policy,bower2020individually,xu2024fairsync}, while some are post-processing~\cite{biega2018equity,sarvi2022understanding} interventions.  In some cases, relevance scores used to produce the ranking are jointly estimated along with fairness optimization~\cite{morik2020controlling,singh2019policy,bower2020individually}. Extensive work has also focused on proportion-based ranking, instead of exposure-based ranking~\cite{gorantla2023sampling,gorantla2021problem,geyik2019fairness}. We direct the interested reader to ~\cite{raj2022measuring} for a detailed review of fairness metrics. While prior papers have proposed some distribution-based measures~\cite{garcia2021maxmin,diaz2020evaluating}, these have been for fairness under stochastic ranking policies for a single query~\cite{gorantla2023sampling,singh2018fairness}. In contrast, we focus on the multi-query amortized setup and consider distributions driven by attention over a sequence of queries, rather than (only) due to the stochastic nature of rankings. 


\looseness=-1
\paragraph{Trade-offs between Group and Individual Fairness.}
Prior work has proposed algorithms to optimize individual fairness without violating group ranking fairness or other constraints such as item diversity~\cite{garcia2021maxmin,saito2022fair,gorantla2021problem,gorantla2023sampling,flanigan2021fair}. Bower \emph{et al.} \citep{bower2020individually} showed empirically that improving individual fairness is beneficial to improving group fairness in in-processing fair ranking. To the best of our knowledge, no work has theoretically analyzed the relation between group and individual ranking fairness in the amortized setting (e.g., if one bounds the other). Note that similar analyses exist in the classification space~\cite{dwork2012fairness}. In this work, we concretely show that under the proposed definitions of fairness, group unfairness is upper-bounded by individual unfairness.



\paragraph{Impact of Queries in Ranking.}
To the best of our knowledge, no prior fairness metrics or interventions utilize information about the query itself in measuring fairness. Closest to our finding is recent work by Patro \emph{et al.}~\cite{patro2022fair}, where the authors observe that ``user attention may not directly translate to provider utility due to missing context-specific factors"~\cite{patro2022fair}. We expand on this observation and empirically show that attention-based metrics may fail specifically in a \emph{cross-query amortization} setup. Our finding is also a generalization of a recent finding~\cite{de2023unfair}, where it was shown that search results can be manipulated in an amortized setting. Our findings also broadly highlight the risk of fairwashing -- e.g., due to search engine manipulation -- when not considering query polarity. Lastly, while notions of multisided exposure fairness, group over-exposure, and under-exposure~\cite{wu2022joint,burke2017multisided,wang2021user} are also related to our problem (where the impact of queries or users are considered in fairness formulation), they still assume that all attention is positive. In our work, we propose a method to integrate real-valued query properties such as sentiment polarity into the fairness definition without making these assumptions. 









