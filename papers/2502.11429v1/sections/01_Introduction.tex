\begin{figure}[htb!]
\centering
\begin{minipage}[htb!]
{\linewidth}
\centering
\includegraphics[width=\linewidth,trim={0.1cm 5.5cm 0.1cm 5.5cm},clip]{images/Figure1.pdf}

\caption{\textbf{Past work in amortized fair ranking minimizes the differences between an individual's expected cumulative attention and relevance over queries, where queries are considered exchangeable.}  However, such formulations lack critical information about the distributions of attention and the properties of queries (e.g., polarity).  Our approach, DistFaiR, aims to overcome this.  The example here considers two search queries with opposite polarities. Both individuals are equally relevant, and have equal expected attention, but have different attention distributions in (a) and (b).}
\label{fig:need_for_distributions}
\end{minipage}
\end{figure}

\section{Introduction}
\label{submission}
Automated ranking systems are widely employed in several high-impact settings, such as ordering job candidates, guiding health-related decisions, and influencing purchasing decisions for safety-critical products~\cite{clarke2020overview,zeide2022silicon, zehlike2020reducing,hajian2016algorithmic}.
These systems directly influence access to critical resources, such as employment opportunities, healthcare, and safe consumer products, all of which significantly affect health and economic outcomes~\cite{saito2022fair,garcia2021maxmin,fabbri2020effect}, among others. However, prior work has shown that some automated rankings may be biased against some minoritized groups of individuals~\cite{biega2018equity,geyik2019fairness}: for example, women are less likely to occupy higher positions in rankings corresponding to searches made in some online hiring contexts~\cite{chen2018investigating}. The increased adoption of large language models (LLMs) as efficient text rankers~\cite{sanner2023large,zhuang2023open,hou2023large,gao2023chat} has the potential to increase the prevalence of automated rankings. However, this expanded usage risks amplifying existing biases in the distribution of user attention and economic opportunity. Mitigating such risks is a critical step towards building a responsible web-based system such as search engines whose performative power~\cite{hardt2022performative} to amplify bias has been demonstrated~\cite{mendler2024engine}.  \looseness=-1

One domain where automated ranking systems are ubiquitous is {\em search}~\cite{altman2005ranking}. Previous works have proposed several interventions and metrics~\cite{beutel2019fairness,naghiaei2022cpfair} to ensure that user attention in search is fairly distributed. In these frameworks, ranking algorithms are considered to be mediators of \emph{exposure} to searchers~\cite{joachims2021recommendations,schnabel2016recommendations}, where exposure is defined as the likelihood of \emph{visual attention} from searchers. A common intervention to achieve fair ranking is distributing rankings, and thereby attention, as a function of \emph{relevance}~\cite{singh2018fairness, biega2018equity}. 

Notably, fair exposure can be impossible in a single ranking where attention decays quicker than relevance, for instance, when all individuals have equal relevance and rankings have position bias w.r.t. attention (see Section~\ref{ref:section_ranking_def}). As a result, many proposed fairness interventions primarily focus on achieving fair exposure on the aggregate, i.e., over a sequence of queries (e.g., {``good dentist", ``good optometrists", ...})~\cite{biega2018equity,singh2018fairness}. That is, fairness is \emph{amortized} over a sequence, e.g., query $\#1$ and $\#2$ in Figure~\ref{fig:need_for_distributions}.



We identify two key limitations in current definitions of fair attention-based amortized ranking: (1) existing methods primarily focus on differences between the mean of attention and relevance distributions across queries, which fails to account for discrepancies in higher-order moments, such as variance or skewness, that may impact fairness (see Figure~\ref{fig:need_for_distributions} and Figure~\ref{fig:distribution_reliability} in Appendix for intuition), and (2) these methods assume that all attention is inherently positive, overlooking cases where unfairness arises due to disproportionate attention for negative or harmful queries compared to equally relevant counterparts, e.g., Figure~\ref{fig:fairwashing_amortized_ranking} in Appendix. 

Our approach, {\em distribution-aware fairness in ranking} (DistFaiR), overcomes these limitations. Our contributions are as follows:
\vspace{-1.05em}

\begin{itemize}
    \item We formalize a definition of amortized ranking fairness that accounts for differences (beyond means) in the distributions of cumulative relevance and cumulative exposure for individuals over a sequence of queries.
    \item We identify a set of measures that enable attention and relevance distribution-aware fairness in ranking (DistFaiR). We also consider a worst-case definition of fairness. %
    Specifically, we demonstrate theoretically and empirically that, for these measures, individual fairness upper bounds group fairness for the identified set of DistFaiR measures. Also, we show empirically that individual and group fairness are not always at odds, i.e., improving individual fairness often improves group fairness. 
    \item We demonstrate {\em fairwashing}, a phenomenon where a ranking appears to be more fair than it is when the polarities of queries are not accounted for. We propose polarity-dependent modifications to our newly proposed and existing fairness metrics that address this issue of fairwashing.
    
\end{itemize}




\vspace{-0.2em}
