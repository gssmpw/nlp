\section{Introduction}\label{sec: intro}

\begin{quote}
    \textit{``Physics is the foundation of all the natural sciences.''} \\
    \vspace{-3em} % Adjust the spacing as needed
    \begin{flushright}
        --- Max Planck
    \end{flushright}
\end{quote}
\vspace{-1em}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/example.pdf}
    \caption{An overall illustration of {\benchmark}. The top part represents the hierarchical physics domains and subjects. The bottom part showcases one concrete example.
    % \shizhe{The text in the image is a bit small and difficult to read. Could we enlarge it to the same font size as the main text?}
    }
    \label{fig:example}
\end{figure}
Physics forms the foundation for natural sciences \citep{planck1949autobiography, hawking1988time, giancoli2000physics}, and physics problem solving constitutes a significant aspect of reasoning for artificial intelligence (AI) \citep{phyre2019bakhtin, PhysQA2023Ding, physicsreasoner2024pang, MoRA2024jaiswal}.
Efforts to solve physics problems with machines date back to the mid-to-late 20th century \citep{larkin1980expert, mendelson1984physics, klahr1986ai}.
After large language models (LLMs) have revolutionized the natural language processing community, significant attention has been paid to solving complex mathematical reasoning problems, which span several areas, such as creating challenging benchmarks \citep{CollegeMath2024Tang, omnimath2024gao, ugmathbench2025xu}, exploring advanced prompting techniques \citep{CoT2022Wei, CoT-SC2022Wang}, applying supervised fine-tuning (SFT) \citep{dartmath2024tong, E-GSM2024Xu}, and leveraging continued pretraining strategies \citep{minerva202lewkowycz, llemma2023azerbayev}.
In contrast, despite its comparable, or even greater challenges for AI reasoning \citep{OlympiadBench2024He, physicsreasoner2024pang}, physics has not yet garnered the same level of attention as mathematics.




\begin{table*}[t]
\centering
\footnotesize
\caption{Comparison of various benchmarks. For ``Level``, 1: Middle School, 2: High School, 3: College Entrance Examination, 4: Competition, 5: Undergraduate or above. ``\#Test" shows the number of textual test examples in physics, while ``\#UG" refers to the number of textual physics test examples of at least the undergraduate level. ``\#Subjects" specifies the number of physics subjects, and ``-" means that the dataset does not divide undergraduate-level physics into more fine-grained subjects. ``\# Ans. Types" is the number of answer types in physics. ``Language”: ``EN'' for English and “ZH” for Chinese. ``Eval.'' describes the evaluation methods and ``Leak. Det.'' states whether data leakage detection is performed, which can alleviate potential test set contamination.}
\begin{tabular}{llccccccc}
\toprule
 \textbf{Dataset} & \textbf{Level}  & \textbf{\# Test}& \textbf{\# UG} & \textbf{\# Subjects} & \textbf{\# Ans. Types} & \textbf{Language} & \textbf{Eval.} & \textbf{Leak. Det.}  \\
\midrule
MMLU & 2, \textbf{5} & 548 & 118 & 3 & 2 & EN & Rule & \usym{2717} \\
AGIEval & 3 & 200 & 0 & - & 3 & ZH & Rule & \usym{2717} \\
C-Eval & 1, 2, \textbf{5} & 601 & 200 & - & 1 & ZH & Rule & \usym{2717} \\
GAOKAO & 3 & 111 & 0 & - & 2 & ZH & \textbf{Rule \& Model} & \usym{2717} \\
JEEBench & 3 & 123 & 0 & - & 2 & EN & Rule & \usym{2717} \\
CMMLU & 2, \textbf{5} & 423 & 147 & 3 & 2 & ZH & Rule & \usym{2717} \\
SciEval & - & 1,657 & - & 3 & 3 & EN & Rule & \usym{2717} \\
PhysQA & 1 & 1,770 & 0 & 5 & - & EN & Rule & \usym{2717} \\
GPQA & \textbf{5} & 227 & 227 & 8 & 1 & EN & Rule & \usym{2717} \\
OlympiadBench & 4 & 376 & 0 & 5 & 4 & \textbf{EN \& ZH} & Rule & \textbf{\usym{1F5F8}} \\
OlympicArena & 4 & 796 & 0 & 6 & \textbf{7} & \textbf{EN \& ZH} & \textbf{Rule \& Model} & \textbf{\usym{1F5F8}} \\
PhysicsQA & 2 & 370 & 0 & 6 & - & - & Rule & \usym{2717} \\
\midrule
\textbf{{\benchmark}} & \textbf{5} & \textbf{11,040} & \textbf{11,040} & \textbf{13} & \textbf{7} & \textbf{EN \& ZH} & \textbf{Rule} \& \textbf{Model} & \textbf{\usym{1F5F8}} \\
\bottomrule
\end{tabular}
\label{tab:compare_with_existing_benchmark}
\end{table*}



%\todo{explain the differences between math and physics, link the error analysis part}
Early investigations into solving physics problems were often studied alongside other scientific domains \citep{minerva202lewkowycz, scienceqa2022Lu, SciBench2023Wang}. 
However, physics, an ancient and well-established discipline, has its unique characteristics and deserves separate treatment for AI. %, similar to mathematics.
Unlike mathematics, which predominantly relies on logical reasoning, physics problems often require additional domain-specific knowledge (e.g., laws and principles) for resolution \citep{phyre2019bakhtin, physicsreasoner2024pang}. 
Moreover, physics problem-solving typically involves multiple applications of physical laws or formulas, making physics reasoning even more demanding than math reasoning.
Thus, evaluating the capabilities of LLMs in physics reasoning is of significant importance.
%Recently, many works have started to focus exclusively on physics reasoning \citep{physicsreasoner2024pang, MoRA2024jaiswal}, signaling a shift toward recognizing its importance:
%With the rapid advancement of LLMs, evaluating their capabilities in physics reasoning has gained increasing attention \citep{PhysQA2023Ding, OlympiadBench2024He, OlympicArena2024huang}. 
%Existing physics benchmarks \citep{agieval2023zhong, GaokaoBench2023zhang, JEEBench2023Arora, sciagent2024ma, PhysQA2023Ding, PhysicsQA2024jaiswal} often consist of exam or textbook problems, primarily targeting middle or high school physics and typically being multiple-choice questions, which are relatively easy for current LLMs to handle. 
Existing physics benchmarks \citep{agieval2023zhong, GaokaoBench2023zhang, JEEBench2023Arora, sciagent2024ma, PhysQA2023Ding, PhysicsQA2024jaiswal} primarily target middle or high school physics and typically are multiple-choice questions, which are relatively easy for current LLMs to handle. 
Although some recent benchmarks have begun to explore competition-level \citep{OlympiadBench2024He, OlympicArena2024huang} or college-level \citep{MMLU2020hendrycks, c-eval2024huang, cmmlu2023li} physics, they remain limited in either scope or size of undergraduate-level physics (see Table~\ref{tab:compare_with_existing_benchmark}), which encompasses a broad range of topics and is widely used in educational assessments for humans \citep{phyedu11999McDermott, phyedu21992Heller, phyedu32003redish}. 
These underscore the demand for a comprehensive benchmark specifically designed to evaluate the physics reasoning abilities of LLMs at the undergraduate level.


%Nevertheless, as summarized in Table~\ref{tab:compare_with_existing_benchmark}, existing physics benchmarks remain limited in scope, difficulty, or size. Furthermore, no current benchmarks comprehensively cover undergraduate-level physics, which is both broad in scope and widely used in educational assessments for human beings\citep{phyedu11999McDermott, phyedu21992Heller, phyedu32003redish}.



In this paper, we introduce \textbf{{\benchmark}}, a large-scale and comprehensive benchmark tailored for evaluating the physics problem-solving abilities of LLMs across multiple \textbf{U}nder\textbf{G}raduate-level \textbf{Physics} (\textbf{{\benchmark}}) disciplines, as shown in Figure~\ref{fig:example}.
We carefully collect, format, split, and filter undergraduate-level physics problems (see Section~\ref{sec: creation}), creating a benchmark comprising 5,520 distinct problems in three main domains, 13 core subjects, and 59 key topics, classified into six atomic answer types and one compound answer type.
We further translate these problems into English to enable bilingual evaluation, resulting in 11,040 problems in total.
To better delineate the skills needed to solve various physics problems, we categorize the problems into four distinct physics reasoning skills correlated with the requisite solution capacities (see Appendix~\ref{app: skills}).
We also conduct rigorous data leakage detection on some mainstream LLMs to validate the quality of our {\benchmark}.
These attributes are outlined more clearly in Figure~\ref{fig:example} and Table~\ref{tab:compare_with_existing_benchmark}.
To address the challenges of answer assessment brought by unique features of physics problems (e.g., physical constants, equivalent quantities defined in problem descriptions, examples given in Table~\ref{tab:physcis_judge_example}), we develop \textbf{M}odel-\textbf{A}ssistant \textbf{R}ule-based \textbf{J}udgment (\textbf{\judge}) (see Section~\ref{sec: evaluation}) that combines the high calculation precision of rule-based judgment methods with the flexibility of model-based approaches.
Human evaluation has shown {\judge}'s reliability on answer judgment of physics problems (see Section~\ref{sec: human_eval}).



% conduct experiments -> how many LLMs -> summarize key findings.

%We perform a comprehensive evaluation of 31 advanced LLMs, incorporating proprietary models such as OpenAI-o1-mini \citep{o1}, general-purpose open-source models like Qwen-2.5-Instruct \citep{qwen252024Yang}, specialized math LLMs (e.g. NuminaMath \citep{numinamath7b}), and even o1-like LLMs like DeepSeek-R1-Distilled \citep{deepseekr12025deepseekai}.

We perform a comprehensive evaluation of 31 advanced LLMs, incorporating proprietary models, general-purpose open-source models (e.g. Qwen-2.5-Instruct \citep{qwen252024Yang}), specialized math LLMs (e.g. NuminaMath \citep{numinamath7b}), and even o1-like LLMs (e.g. DeepSeek-R1-Llama-70B \citep{deepseekr12025deepseekai}).
The inclusion of math LLMs aims to assess the extent to which training on specialized math corpus contributes to physics reasoning.
Despite LLMs' strong math reasoning abilities, the best overall accuracy achieved in {\benchmark} is 49.8\% by OpenAI-o1-mini \citep{o1}.
These results highlight the challenges that {\benchmark} pose to current LLMs in terms of physics problem-solving, underscoring the significance for future research with an emphasis on physics as well.
To summarize our key findings:

1. {\benchmark} is a challenging dataset for LLMs in physics problem-solving, with OpenAI-o1-mini achieving the highest overall accuracy of 49.8\%.

2. Unlike math problem-solving, math-specialized LLMs yield only minor improvements over their general-purpose counterparts in {\benchmark}, suggesting the compulsion for more high-quality physics corpora.

3. O1-like LLMs suggest a promising direction for future advancements in physics reasoning. Among them, DeepSeek-R1-Distill-Llama-70B achieves the second-highest overall accuracy on UGPhysics, though there remains a performance gap compared to top-tier closed-source LLMs.

4. Unlike abstract math reasoning, math derivation in the context of physics requires additional knowledge and involves practical meanings, where LLMs currently fall short.

%4. LLMs usually perform well on Knowledge Recall problems but struggle with math derivation in the context of physics, which requires extra knowledge and involves prectical meanings.

5. Error analysis reveals that, unlike math reasoning, the primary types of errors made by OpenAI-o1-mini are flawed reasoning, knowledge deficiency, and wrong application.
