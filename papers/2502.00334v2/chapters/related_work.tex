\section{Related Work}\label{sec: related_work}


%\textbf{Physics Benchmarks}.
%Physics is an ancient yet vibrant discipline, and automatic physics problem-solving has drawn interest since the early days of AI research \citep{larkin1980expert, mendelson1984physics, klahr1986ai}.
%With the advent of highly capable LLMs, researchers have started to evaluate their physics reasoning abilities through a variety of benchmarks.
%High school-level benchmarks \citep{sciQ2017welbl, scienceqa2022Lu, E-eval2024hou}, while valuable, are relatively simple and lack problems that involve complex reasoning or computational skills \citep{OlympiadBench2024He}. 
%Benchmarks based on college entrance exams, such as \citep{agieval2023zhong, GaokaoBench2023zhang, JEEBench2023Arora}, offer slightly more advanced problems. However, these problems are neither categorized into fine-grained physics subjects nor sufficiently challenging. 
%Benchmarks like MMLU-STEM \citep{MMLU2020hendrycks}, C-Eval-STEM \citep{c-eval2024huang}, and CMMLU-STEM \citep{cmmlu2023li} include a selection of college-level physics questions. 
%Nevertheless, these are predominantly presented in a multiple-choice format, limiting the diversity of answer types and the scope of evaluation.
%Similarly, more advanced benchmarks such as GPQA \citep{gpqa2023rein}, OlympiadBench \citep{OlympiadBench2024He}, and OlympicArena \citep{OlympicArena2024huang}, while containing challenging physics problems, are limited in both their size and the breadth of topics within the field of physics.
%Additionally, these benchmarks do not focus solely on physics but also include questions from other scientific domains or mathematics. While PhysQA \citep{PhysQA2023Ding} and PhysicsQA \citep{PhysicsQA2024jaiswal} are specialized physics benchmarks, they remain relatively simple.
%The success of LLMs emphasizes the evaluation of foundational knowledge and human-like or superhuman abilities across various domains and fields \citep{OlympicArena2024huang}.
%This leads to the creation of many benchmarks to test and push the limits of LLMs in mathematics \citep{gsm8k2021cobbe, MATH2021hendrycks, CollegeMath2024Tang, MathBench2024Liu, omnimath2024gao, ugmathbench2025xu},
%visual reasoning \citep{geoqa2021chen, geoqa+2022Cao, OlympiadBench2024He, OlympicArena2024huang, MathVista2023lu}, embodied AI (physical reasoning) \citep{piqa2019bisk, phyre2019bakhtin,newton2023wang}, and many others \citep{theoremqa2023chen, pds2024xu, rewardbench2024lambert, feabench2024mudur2024, processbench2024zheng}.
%There is growing interest in dynamic benchmarks to mitigate test set contamination \citep{MATH_FUNC2024srivastava, GSM1K2024zhang, VarBench2024qian, LiveBench2024white}.
%In addition to creating new benchmarks, researchers have also explored the impact of modifying existing ones \citep{GSM-IC2023shi, GSM-Plus2024Li, R-GSM2024chen, E-GSM2024Xu}.
%In contrast, our proposed {\benchmark} encompasses a broader range of undergraduate-level physics subjects, offering diverse answer types, and providing a significantly larger number of test examples in physics.



\textbf{Physics Benchmarks}.
The growing interest in LLM evaluation has led to the creation of benchmarks across various domains, such as mathematics \citep{gsm8k2021cobbe, MATH2021hendrycks, CollegeMath2024Tang, MathBench2024Liu, omnimath2024gao, ugmathbench2025xu}, visual reasoning \citep{geoqa2021chen, geoqa+2022Cao, OlympiadBench2024He, OlympicArena2024huang, MathVista2023lu}, embodied AI (physical reasoning) \citep{piqa2019bisk, phyre2019bakhtin,newton2023wang}, dynamic benchmarks to mitigate test set contamination \citep{MATH_FUNC2024srivastava, GSM1K2024zhang, VarBench2024qian, LiveBench2024white}, and many others \citep{theoremqa2023chen, pds2024xu, rewardbench2024lambert, feabench2024mudur2024,E-GSM2024Xu, processbench2024zheng}.
Physics is an ancient yet dynamic discipline and researchers have increasingly turned to benchmarks to assess LLMs in physics reasoning.
While high school-level benchmarks \citep{sciQ2017welbl, scienceqa2022Lu, E-eval2024hou} are valuable, they lack complex reasoning or computational challenges \citep{OlympiadBench2024He}. 
College entrance exam-based benchmarks \citep{agieval2023zhong, GaokaoBench2023zhang, JEEBench2023Arora} present more advanced problems, but they often lack fine-grained subject categorization. 
Benchmarks like MMLU-STEM \citep{MMLU2020hendrycks}, C-Eval-STEM \citep{c-eval2024huang}, and CMMLU-STEM \citep{cmmlu2023li} include some college-level physics questions, yet they are predominantly multiple-choice questions. 
Advanced benchmarks such as GPQA \citep{gpqa2023rein}, OlympiadBench \citep{OlympiadBench2024He}, and OlympicArena \citep{OlympicArena2024huang} provide challenging physics problems but are limited in size and breadth, often incorporating other scientific domains. 
Specialized physics benchmarks like PhysQA \citep{PhysQA2023Ding} and PhysicsQA \citep{PhysicsQA2024jaiswal} remain relatively simple.
In contrast, our proposed {\benchmark} encompasses a broader range of undergraduate-level physics subjects, offering diverse answer types, and providing a significantly larger number of test examples.
%\todo{highlight MARJ evaluation}
%\textbf{Other Benchmarks}.
%The success of LLMs emphasizes the evaluation of foundational knowledge and human-like or superhuman abilities across various domains and fields \citep{OlympicArena2024huang}.
%This leads to the creation of many benchmarks to test and push the limits of LLMs in mathematics \citep{gsm8k2021cobbe, MATH2021hendrycks, CollegeMath2024Tang, MathBench2024Liu, omnimath2024gao, ugmathbench2025xu},
%visual reasoning \citep{geoqa2021chen, geoqa+2022Cao, OlympiadBench2024He, OlympicArena2024huang, MathVista2023lu}, embodied AI (physical reasoning) \citep{piqa2019bisk, phyre2019bakhtin,newton2023wang}, and many others \citep{theoremqa2023chen, pds2024xu, rewardbench2024lambert, feabench2024mudur2024, processbench2024zheng}.
%There is growing interest in dynamic benchmarks to mitigate test set contamination \citep{MATH_FUNC2024srivastava, GSM1K2024zhang, VarBench2024qian, LiveBench2024white}.
%In addition to creating new benchmarks, researchers have also explored the impact of modifying existing ones \citep{GSM-IC2023shi, GSM-Plus2024Li, R-GSM2024chen, E-GSM2024Xu}.
%We believe that our {\benchmark} could serve as a foundation for advancing research on the physics reasoning abilities of LLMs.

\textbf{Answer Judgment}.
Evaluating model-generated answers to complex mathematical problems has long been a challenging task. 
Researchers have primarily relied on two approaches: rule-based methods, often combined with elaborate answer-cleaning codes \citep{MATH2021hendrycks, OlympiadBench2024He, PhysicsQA2024jaiswal}, and model-based methods that employ LLMs as evaluators \citep{omnimath2024gao}. 
While rule-based methods are efficient, they struggle with handling complex answers \citep{omnimath2024gao}. 
On the other hand, model-based methods offer more flexibility but often fall short in accurately assessing numerical values, possibly due to the current limitations of LLMs in performing precise calculations \citep{PoT2022Chen, ugmathbench2025xu}. 
This issue is particularly pronounced in physics, where customized relative error requirements for different problems are required.
As evidenced by a 12\% judgment error rate for physics problems in OlympiadBench \citep{OlympiadBench2024He}, evaluating model-generated answers for physics problems presents an even greater challenge due to frequent appearance of physical constants and equivalent quantities (see Table~\ref{tab:physcis_judge_example}).  
To address the challenge of answer assessment of physics problems, we propose {\judge}, a two-stage evaluation framework that integrates both the precise calculation of rule-based judgment with the flexibility of model-based assessment (details in Section~\ref{sec: evaluation}).
Human evaluation has confirmed the reliability of our {\judge} scoring framework .%(see Section~\ref{sec: human_eval}).


\textbf{LLMs for Reasoning}.
Significant efforts have been devoted to leveraging LLMs for solving reasoning problems, particularly in mathematics.
Beyond evaluation \citep{MathBench2024Liu, CollegeMath2024Tang, omnimath2024gao}, researchers have explored various approaches, including advanced prompting techniques \citep{CoT2022Wei, CoT-SC2022Wang}, supervised fine-tuning (SFT) \citep{dartmath2024tong, E-GSM2024Xu}, and continued pretraining strategies \citep{minerva202lewkowycz, llemma2023azerbayev}.
To assess the impact of math-related training on physics reasoning of LLMs, we also evaluate several math-specific LLMs in {\benchmark}.
More recently, there has been much work dedicated specifically to physics reasoning \citep{physicsreasoner2024pang, MoRA2024jaiswal},
yet there is still a lack of a specialized physics corpus for LLMs to continually pretrain or further SFT.
Our findings underscore the necessity for further research in this area.
%Our work falls within this emerging area by introducing a comprehensive benchmark designed to evaluate the physics reasoning capabilities of LLMs.
%We believe our {\benchmark} will serve as a foundation for advancing future research at machine learning for physics reasoning.

%\todo{explain why include math LLMs for evaluation}

