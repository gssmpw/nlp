\section{The {\benchmark} Benchmark}\label{sec: benchmark}

\subsection{{\benchmark} Overview}\label{sec: overview}

We introduce {\benchmark}, a large and comprehensive undergraduate-level physics benchmark specifically designed to thoroughly evaluate the physics problem-solving ability of LLMs.
{\benchmark} is large in size, including 5,520 physics problems presented bilingually for better evaluation.
It covers three domains: Mechanics \& Thermodynamics, Electromagnetism, and, Modern Physics, encompassing 13 core subjects and 59 different topics in undergraduate-level physics (details are in Appendix~\ref{app: distribution}).
Similar to \citet{OlympiadBench2024He, OlympicArena2024huang, ugmathbench2025xu}, each problem is structured with 7 answer types to facilitate answer judgment, including six atomic answer types and one compound type that is a list of atomic ones.
To provide a more granular analysis of LLMs' physics reasoning ability, we categorize each test example into four distinct physics reasoning capabilities, which could possibly show which skill sets certain families of models succeed or fail on.
Detailed statistics of {\benchmark} are shown in Table~\ref{tab:benchmark_statistics}.
Additionally, data leakage detection on several LLMs is conducted to identify potential data contamination (see Section~\ref{sec: data_leakage}).

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Benchmark Statistics}
    \begin{tabular}{l r}
    \toprule
    \textbf{Statistic} & \textbf{Number} \\
    \midrule
    Total Problems & 5520 \\
    Number of Language & $\times 2$ \\
    Total Domains & 3 \\
    Total Subjects & 13 \\
    Total Topics & 59 \\
    Total Answer Types & 7 \\
    Total Difficulty Level & 4 \\
    \midrule
    Average Problem Tokens & 82.4 \\
    Average Solution Tokens & 318.5 \\
    Average Number of Answers & 1.34 \\
    \bottomrule
    \end{tabular}
    \label{tab:benchmark_statistics}
\end{table}




\subsection{{\benchmark} Creation}\label{sec: creation}

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Examples of different answer types.}
    \begin{tabular}{lcc}
\toprule
\textbf{Type} & \textbf{Abbrev.}    & \textbf{Example}                         \\ \hline
Numerical Value & NV & $2.51 \times 10^{-4}$                             \\
Expression & EX & $\sqrt{2/\mu\sigma\omega}$                \\
Equation  & EQ & $\nabla\cdot\boldsymbol{J}_{\omega}-i\omega\rho_{\omega}=0$   \\
Interval & IN & $(-\infty,E/cB]$             \\ 
%Tuple & TUP & (2.1, 3.4) \\
True/False & TF & Yes \\
Multiple Choice & MC & A \\
Compound & - & $\omega/c,\, \boldsymbol{k}\cdot\boldsymbol{A}_{0}=0$\\
\bottomrule
\end{tabular}
\label{tab:examples of answer types}
\end{table}

Our {\benchmark} creation process can mainly be divided into three distinct phases: data collection \& cleaning, data processing \& filtering, and problem annotation.

% collect, format, filter, dedup
\textbf{Data Collection \& Cleaning}.
The {\benchmark} is sourced from several undergraduate-level physics exercise books. 
The corresponding PDF files are converted to LaTeX format using the Mathpix tool for optical character recognition. 
Both the original PDFs and the converted LaTeX files are manually reviewed and corrected by our team. 
The LaTeX files are then structured into a ``Problem—Solution—Answer'' format using various markups. 
Deduplication is carried out based on model embeddings to eliminate potential repeated or similar problems. 
Currently, problems containing images are excluded to focus on text-only reasoning of {\benchmark}.

% split, translate, delete estimation / proof / subject / explanation
\textbf{Problem Processing \& Filtering}.
In physics, some problems are progressive, where subsequent questions may depend on the answers or information from previous ones. 
Unlike \citet{OlympiadBench2024He, OlympicArena2024huang}, we split these progressive problems into independent new problems, incorporating all relevant information in each new problem. 
Additionally, we exclude problems that lack definitive answers for assessing correctness, such as estimation, proof, and explanation problems. 
Several examples are provided in Appendix~\ref{app: filter}. 
All problems are initially in Chinese and then translated into English to facilitate bilingual evaluation.



% Type & Difficulty Levels
\textbf{Problem Annotation}.
\citet{OlympiadBench2024He, OlympicArena2024huang, ugmathbench2025xu} suggest that classifying answer types can facilitate the evaluation pipeline. 
In our {\benchmark}, we categorize answers into seven types: six atomic answer types and one compound type, which consists of a list of atomic answers separated by commas. 
One concrete example for each atomic answer type is presented in Table~\ref{tab:examples of answer types}. 
To emphasize the focus on physics reasoning, we label each test example with one of four distinct physics reasoning skills: Knowledge Recall, Laws Application, Math Derivation, and Practical Application (``Others'' for the remaining). 
We use \texttt{GPT-4o} as the annotator for the categorization of skill sets. Further details are provided in Appendix~\ref{app: skills}.











\subsection{{\judge} Evaluation Framework}\label{sec: evaluation}


% 1. deal with unit/numerical -> scientific notation, round to the gt (relative error)
% 2. deal with physics constans -> transform to 1
% 3. Equivalent quantity substitution -> resort to model judgement
% add the whole algorithm



% judge answer correctness is more challenging for physics.
%Evaluating model-generated answers to complex mathematical problems has long been a challenging task. 
%Researchers have traditionally relied on two primary approaches: rule-based methods, often combined with elaborate answer-cleaning codes \citep{MATH2021hendrycks, OlympiadBench2024He, PhysicsQA2024jaiswal}, and model-based methods that employ LLMs as evaluators \citep{omnimath2024gao}. 
%Rule-based methods are efficient but struggle to handle complex answers \citep{omnimath2024gao}. 
%On the other hand, model-based methods offer more flexibility but often fail to accurately judge numerical values, primarily due to the current limitations of LLMs in performing precise calculations \citep{PoT2022Chen, ugmathbench2025xu}. 
%This issue is particularly prominent in physics, where customized relative error requirements for different problems are involved.

Evaluating model-generated answers for physics problems presents a great challenge, as evidenced by a 12\% judgment error rate for physics problems in OlympiadBench \citep{OlympiadBench2024He}. 
Illustrative examples are provided in Table~\ref{tab:physcis_judge_example}.
Several reasons are given as follows:

\begin{table}
    \centering
    \footnotesize
    \caption{Examples of Challenges for Answer Judgment, where``GT`` stands for ground-truth answers and ``Model Ans.`` is the model-generated answers.}
    \begin{tabular}{ccc}
    \toprule
         GT & Model Ans. & Comments \\
    \midrule
        0.055 s & 55 ms & unit conversion    \\
        3000 & $3.02 \times 10^6$ & intermediate precision \& unit \\
          $\chi_0 \frac{h\nu}{kT}$ & $\chi_0 \frac{E_2 - E_1}{kT}$ & $h\nu = E_2 - E_1$\\
          $\hbar$ & 1  & inclusion of physical constants \\
    \bottomrule
    \end{tabular}
    \label{tab:physcis_judge_example}
\end{table}


1. \textbf{Precision Issues}: The occurrence of physical constants poses challenges to the calculation precision, which could be exacerbated in multi-step reasoning, where intermediate values accumulate errors. Additionally, unit conversions or providing final answers in different units can further complicate the evaluation.


2. \textbf{Equivalent Quantities}: Physics problems often define equivalent quantities in problem descriptions, leading to multiple correct ways to express the final answer. These expressions may not always be mathematically equivalent, making it difficult to apply rule-based evaluation.
Additionally, it is conventional to omit certain physical constants in the final answer, further complicating the answer judgment.


These highlight the unreliability of relying solely on traditional rule-based or model-based methods for evaluating answers to physics problems. 
To address these evaluation challenges, we propose a Model-Assistant Rule-based Judgment (\judge) pipeline, which combines the efficiency of customized rule-based methods for simple answers like numerical values with the flexibility of model-based methods to handle more complex cases (see \cref{alg:judge}).





The {\judge} involves a two-stage evaluation process. 
In the first stage, a rule-based judgment system is employed, followed by a second stage where \texttt{GPT-4o} is used to assess cases flagged as ``False'' by the rule-based method.
The entire pipeline is described in detail in Appendix~\ref{app: constants}. 
In the rule-based matching stage, multiple answers to a given problem are evaluated individually. If any one answer deviates from the ground-truth, the result is marked as ``False''. 
For the model-based judgment stage, all answers are assessed collectively in a single evaluation prompt.
For the rule-based matching stage, different types of answers are handled separately.
TF and MC are judged after transforming the model-generated answers to the same format of ground-truth.
For NV, answers are converted into scientific notation and only the base of the scientific notation is considered, allowing for a relative error of up to $1e^{-2}$ to account for unit differences or rounding.
EX and EQ are normalized by removing all physical constants.
IN are judged by comparing the two endpoints, treating them as either NV or EX.
For the model-based judgment stage, we require the evaluator model to pay attention to physical constants as well as equivalent quantities in the problem description.
The few-shot judging prompt is long and will be released in our code repository.
From the analysis in Section~\ref{sec: human_eval},
the {\judge} pipeline offers a reliable answer assessment.
%for physics problems.


% model: not good at precision; rule: not enough to judge equivalent quantities.






 

 

 
