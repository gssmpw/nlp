\section{Experiments}\label{sec: experiments}

\subsection{Experimental Setup}

\textbf{Evaluated LLMs}.
Our evaluation covers 31 leading LLMs, including closed-source commercial LLMs, open-source general-purpose LLMs, o1-like LLMs, and specialized math LLMs.
Based on our {\benchmark}, we provide a thorough evaluation of the physics reasoning capabilities of current LLMs. 
The evaluated LLMs are listed below:

For proprietary LLMs, we select OpenAI-o1-mini \citep{o1}, GPT4o \citep{gpt4o2024openai}, and GPT4o-mini \citep{gpt4o2024openai}.

For open-source general-purpose LLMs, we evaluated the LLaMA-3.1-Instruct series (8B, 70B) \citep{llama312024dubey}, LLaMA-3.3-Instruct-70B, Qwen2.5-Instruct (7B, 72B)\citep{qwen252024Yang}, Yi-1.5-Chat (6B, 9B, 34B) \citep{ai2024yi}, Ministral-8B-Instruct-2410 \citep{ministral8b}, Mistral-Nemo-Instruct-2407 \citep{mistral_nemo_blog}, Mistral-Small-Instruct-2409 \citep{mistral_small_blog}, Mistral-Large-Instruct-2407 \citep{mistral_large_blog}, DeepSeek-MOE-16B-Chat \citep{dai2024deepseekmoe}, and DeepSeek-V2-Lite-Chat \citep{deepseekv2}.

We also incorporate specialized math LLMs to assess the extent to which continued training and SFT on math-related content can enhance physics reasoning: DeepSeekMath-7B (-RL, -Instruct) \citep{deepseekmath2024shao}, Qwen2.5-Math (7B, 72B)\citep{Qwen25Math2024Yang}, Mathstral-7B \citep{mathstral2023}, NuminaMath-7B-CoT \citep{numinamath7b}, and OpenMath2-Llama-3.1 (8B, 70B) \citep{openmathinstruct2024toshniwal}.

For o1-like LLMs, we cover QwQ-32B-Preview \citep{qwq32b_blog}, Skywork-o1-Open-Llama-3.1-8B \citep{skyworkmodelcard}, and DeepSeek-R1 \citep{deepseekr12025deepseekai} distilled series (DeepSeek-R1-Distilled-Llama-8B, -Llama-70B; -Qwen-7B, -Qwen-32B). 

We provide the details of these LLMs in Appendix~\ref{app:models}.


\textbf{Evaluation Setting}.
Following \citet{OlympiadBench2024He, OlympicArena2024huang}, all our experiments use zero-shot prompts, tailored to different answer types for better answer extraction and rule-based matching.
Detailed prompts are given in Appendix~\ref{app: prompts}.
We use vLLM\footnote{\href{https://github.com/vllm-project/vllm}{https://github.com/vllm-project/vllm}} to speed up the evaluation process.
To maintain consistency in evaluations and facilitate reproduction, we set the maximum output length to 4,096 tokens and employ a greedy decoding strategy with the temperature 0.
For LLMs with a maximum output length of less than 4,096 tokens during SFT, such as NuminaMath-CoT-7B, we adjust the maximum output length to align with their specific SFT configurations.
More Details of the evaluation setting are given in Appendix~\ref{app: parameters}.







\begin{table*}[!thb]
\centering
\footnotesize
\caption{\textbf{Main Results on {\benchmark}} (all figures are in \%). Models are classified into four different categories according to their purpose and origin. The best results within each column are \textbf{bolded} and the best results of LLMs within a similar group are \underline{underlined}.
``Mec. and Ther.`` stands for Mechanics \& Thermodynamics, and ``Elec.`` represents Electromagnetism.}
\scalebox{1.0}{
\begin{tabular}{lccccccccccc}
\hline
\multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Mec. and Ther.}} & \multicolumn{2}{c}{\textbf{Elec.}} & \multicolumn{2}{c}{\textbf{Modern Physics}} & \multicolumn{2}{c}{\textbf{Overall}} & \textbf{Average}\\
 \cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9}
 & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} &  \\
\hline
\multicolumn{10}{c}{{\textit{Closed-source LLMs}}} \\
\cdashline{1-10}
OpenAI-o1-mini-2024-09-12 & \textbf{48.47} & \textbf{49.08} & \textbf{43.58} & \textbf{43.15} & \textbf{54.06} & \textbf{52.75} & \textbf{49.96} & \textbf{49.60} & \textbf{49.78} \\
GPT-4o-2024-08-06 & 36.97 & 36.84 & 36.40 & 34.58 & 42.80 & 40.63 & 39.29 & 38.01 & 38.66 \\
GPT-4o-mini-2024-07-18 & 27.81 & 26.07 & 24.84 & 22.38 & 30.72 & 29.28 & 28.51 & 26.78 & 27.64 \\
\hline
\multicolumn{10}{c}{{\textit{Open-source Chat LLMs}}} \\
\cdashline{1-10}
Yi-1.5-6B-Chat & 10.99 & 7.28 & 11.99 & 7.82 & 13.66 & 9.29 & 12.26 & 8.21 & 10.24 \\
Qwen2.5-7B-Instruct & \underline{23.89} & \underline{20.23} & \underline{24.09} & \underline{17.99} & \underline{25.57} & \underline{21.77} & \underline{24.62} & \underline{20.49} & \underline{22.55} \\
LLaMA3.1-8B-Instruct & 12.64 & 7.67 & 14.35 & 9.85 & 16.80 & 13.00 & 14.66 & 10.25 & 12.45 \\
Ministral-8B-Instruct-2410 & 13.95 & 10.81 & 15.52 & 8.99 & 19.20 & 12.17 & 16.39 & 11.07 & 13.73 \\
Yi-1.5-9B-Chat & 16.00 & 11.73 & 15.85 & 13.28 & 19.94 & 15.40 & 17.61 & 13.51 & 15.56 \\
\hline
Mistral-Nemo-Instruct-2407 & 14.08 & 11.64 & 14.78 & 11.99 & 18.41 & 16.27 & 16.00 & 13.62 & 14.81 \\
DeepSeek-MOE-16B-Chat & 3.75 & 3.18 & 4.93 & 4.82 & 7.16 & 4.49 & 5.36 & 4.00 & 4.68 \\
DeepSeek-V2-Lite-Chat & 6.50 & 4.93 & 6.75 & 5.78 & 9.47 & 7.59 & 7.77 & 6.18 & 6.97 \\
Mistral-Small-Instruct-2409 & \underline{22.71} & \underline{21.27} & \underline{22.70} & \underline{18.42} & \underline{29.97} & \underline{23.21} & \underline{25.72} & \underline{21.59} & \underline{23.66} \\
Yi-1.5-34B-Chat & 18.79 & 13.38 & 18.63 & 12.74 & 23.17 & 17.84 & 20.58 & 15.13 & 17.85 \\
\hline
LLaMA3.1-70B-Instruct & 27.90 & 24.89 & 26.98 & 22.81 & 32.98 & 27.23 & 29.86 & 25.51 & 27.68 \\
LLaMA3.3-70B-Instruct & 33.61 & 25.81 & 33.30 & 24.84 & 39.18 & 26.83 & 35.87 & 26.07 & 30.97 \\
Qwen2.5-72B-Instruct & 35.96 & 35.70 & 33.19 & 34.05 & 37.13 & 38.22 & 35.98 & 36.47 & 36.22 \\
Mistral-Large-Instruct-2407 & \underline{38.06} & \underline{36.70} & \underline{38.33} & \underline{34.90} & \underline{44.20} & \underline{40.36} & \underline{40.65} & \underline{37.92} & \underline{39.28} \\
\hline
\multicolumn{10}{c}{{\textit{Specialized Mathematical LLMs}}} \\
\cdashline{1-10}
DeepSeek-Math-7B-Instruct & 13.25 & 12.34 & 16.49 & 13.17 & 19.07 & 15.62 & 16.21 & 13.84 & 15.03 \\
DeepSeek-Math-7B-RL & 15.17 & 11.68 & 15.20 & 12.74 & 18.54 & 15.40 & 16.58 & 13.41 & 14.99 \\
NuminaMath-7B-CoT & 13.60 & 15.52 & 14.45 & 15.52 & 18.06 & 18.50 & 15.60 & 16.76 & 16.18 \\
Mathstral-7B-v0.1 & 14.82 & 12.47 & 17.77 & 14.99 & 19.94 & 17.06 & 17.45 & 14.80 & 16.12 \\
OpenMath2-Llama-3.1-8B & 8.63 & 6.28 & 10.17 & 7.39 & 11.95 & 9.55 & 10.27 & 7.83 & 9.05 \\
Qwen2.5-Math-7B-Instruct & 23.84 & 21.05 & 22.38 & 18.09 & 26.53 & 21.34 & 24.71 & 20.67 & 22.69 \\
OpenMath2-Llama-3.1-70B & 20.31 & 18.70 & 22.16 & 18.63 & 25.44 & 21.51 & 22.75 & 19.86 & 21.30 \\
Qwen2.5-Math-72B-Instruct & \underline{39.54} & \underline{39.84} & \underline{35.87} & \underline{38.44} & \underline{41.19} & \underline{39.44} & \underline{39.60} & \underline{39.44} & \underline{39.52} \\

\hline
\multicolumn{10}{c}{{\textit{o1-like LLMs}}} \\
\cdashline{1-10}
DeepSeek-R1-Distill-Qwen-7B & 29.25 & 20.62 & 27.41 & 17.45 & 29.80 & 20.68 & 29.17 & 20.11 & 24.64 \\
Skywork-o1-Open-Llama-3.1-8B & 13.47 & 9.55 & 14.45 & 8.67 & 15.36 & 10.12 & 14.42 & 9.64 & 12.03 \\
DeepSeek-R1-Distill-Llama-8B & 16.35 & 7.15 & 17.13 & 6.53 & 20.90 & 9.08 & 18.37 & 7.84 & 13.11 \\
QwQ-32B-Preview & 36.84 & \underline{38.01} & 35.65 & \underline{31.58} & 38.61 & \underline{38.92} & 37.37 & \underline{37.30} & 37.34 \\
DeepSeek-R1-Distill-Qwen-32B & 35.22 & 28.51 & 33.40 & 23.34 & 38.09 & 28.80 & 36.11 & 27.75 & 31.93 \\
DeepSeek-R1-Distill-Llama-70B & \underline{43.24} & 34.26 & \underline{42.93} & 28.80 & \underline{49.91} & 36.78 & \underline{45.96} & 34.38 & \underline{40.17} \\
\bottomrule
\end{tabular}
}
\label{tab: main_results}
\end{table*}


\begin{figure*}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth} % Adjust width if needed
        \centering
        \includegraphics[width=\textwidth]{figures/subject.pdf}
        \caption{Accuracy Across Subjects}
        \label{fig:acc_subject}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth} % Adjust width if needed
        \centering
        \raisebox{1.5em}{ % Adjust this value to move the second figure up
            \includegraphics[width=\textwidth]{figures/skills.pdf}
        }
        \caption{Accuracy Across Reasoning Skills}
        \label{fig:acc_level}
    \end{subfigure}
    \caption{The distribution of overall accuracy across subjects, and physics reasoning skills. (a) The overall accuracy of different subjects averaged across 8 strong LLMs listed in Figure (b). Each bar consists of several segments with colors indicating their corresponding reasoning skills. 
    (b) The overall accuracy of reasoning skills, averaged across all subjects. Only 8 strong LLMs are included for brevity. ``KR'': Knowledge Recall; ``LA'': Laws Application; ``MD'': Math Derivation; ``PA'': Practical Application; ``OT'': Others.}
    \label{fig:acc_subject_level}
\end{figure*}
\subsection{Main Results}
% challenging -> acc low
% close vs open
% model size
% math specialized high but not enough
% how current o1-like LLMs perform

The main results are shown in Table~\ref{tab: main_results} and more detailed results are given in Appendix~\ref{app: results}.
From Table~\ref{tab: main_results}, we have the following observations.

\textbf{Our {\benchmark} presents a significant challenge for current LLMs.}
The highest overall accuracy, 49.78\%, is achieved by OpenAI-o1-mini, followed by DeepSeek-R1-Distill-Llama-70B with 40.17\%. 
Notably, 15 out of 31 evaluated LLMs score below 20\%, and only two models surpass the 40\% overall accuracy. 
In contrast, OpenAI-o1-mini achieves over 90\% accuracy on MATH \citep{MATH2021hendrycks}, over 60\% on olympic and undergraduate math problems \citep{omnimath2024gao, ugmathbench2025xu}.
Although current LLMs have powerful math reasoning abilities, they still struggle with complex physics reasoning.
%Despite their impressive performance in mathematical problem-solving, current LLMs still struggle with complex physics reasoning.

\textbf{Open-source LLMs are catching up with closed-source LLMs, but a performance disparity remains.}
Five open-source LLMs achieve overall accuracy comparable to or even exceeding GPT-4o.
However, the best performing open-source LLM, DeepSeek-R1-Distill-Llama-70B, lags by around 10\%  behind OpenAI-o1-mini.
Furthermore, GPT-4o-mini still outperforms many open-source LLMs, even surpassing some o1-like LLMs.


\textbf{Model performance improves with increasing parameter size within the same model family.}
As model size grows from 7B to 72B, Qwen2.5-Math-Instruct exhibits an approximate 17\% increase in overall accuracy, with consistent improvements across different domains and languages. 
A similar trend is observed in o1-like LLMs, where DeepSeek-R1-Distill-Llama sees an even more pronounced performance gain by 27\% when scaling from 8B to 70B.


\textbf{Math-specialized LLMs outperform their general-purpose counterparts, but the improvement is less pronounced than in mathematics.}
Qwen2.5-Math-7B-Instruct achieves only a 0.14\% higher average accuracy than Qwen2.5-7B-Instruct, while Qwen2.5-Math-72B-Instruct outperforms Qwen2.5-72B-Instruct by 3.3\%. 
In contrast, math-specific LLMs usually outperform their general-purpose counterparts in solving mathematical problems by a large margin (around or over 10\%) \citep{MathBench2024Liu, ugmathbench2025xu}.
This suggests that continued pre-training and further supervised fine-tuning on mathematical corpora yield only marginal gains in physics problem-solving, highlighting the need for future efforts to incorporate physics-specific content during training.

\textbf{O1-like LLMs yield surprisingly strong results.}
DS-R1-Llama-70B achieves the second-highest overall accuracy on {\benchmark}. 
Notably, QwQ-32B attains a competitive accuracy of 37.3\%, closely approaching Qwen2.5-Math-72B-Instruct (39.5\%) despite its significantly smaller model size. 
Furthermore, QwQ-32B outperforms DeepSeek-R1-Distill-Llama-70B on problems in ZH.%, demonstrating its strength in handling Chinese physics problems.
