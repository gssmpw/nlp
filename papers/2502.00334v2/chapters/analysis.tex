

\section{Analysis}\label{sec:analysis}
\subsection{Fine-grained Analysis}\label{sec: more_results}
% Across subjects and Difficulty Level
% about zh & en (some model zh > en, some en > zh)




%In this section, we conduct an in-depth analysis of LLMs' performance across different subjects, physics reasoning skills, and language preferences.
%We select 8 strong LLMs for analysis here and delay the complete results in Table~\ref{tabapp: results_sub1}, \ref{tabapp: results_sub2}, \ref{tabapp: results_sub3}, and \ref{tabapp: results_skill} in Appendix~\ref{app: results}.
In this section, we conduct an in-depth analysis using 8 strong LLMs and delay the complete results to Appendix~\ref{app: results}.


\textbf{LLMs show varying performance across different subjects, although the disparity is relatively small.} 
As shown in Figure~\ref{fig:acc_subject}, the average overall accuracy of eight strong LLMs reveals that they perform particularly well in Semiconductor Physics (27.0\%) and Atomic Physics (22.9\%). 
In contrast, their performance is slightly lower in Theoretical Mechanics (13.8\%). 
Additionally, LLMs show minor performance variation across six out of 13 subjects, with accuracies hovering around 19\%.
In comparison, LLMs' performance can vary from 10\% to 70\% across different math topics \citep{MathBench2024Liu}.


\textbf{LLMs exhibit varying levels of physics reasoning skills.}
As shown in Figure~\ref{fig:acc_level}, the selected 8 LLMs display similar performance trends across different physics reasoning skills.
They perform well on Knowledge Recall tasks but struggle with Math Derivation problems.
This suggests that recalling physics concepts is relatively simple for LLMs, whereas performing complex math derivations in a physics context (usually require physics knowledge and practical meanings) is more challenging.
Notably, OpenAI-o1-mini outperforms the other models across all four distinct physics reasoning skills (as well as ``Others'').


\textbf{LLMs exhibit varying performance across different languages when solving physics problems.}
From Table~\ref{tab: main_results}, some LLMs demonstrate only minor discrepancies in performance between English (EN) and Chinese (ZH), such as OpenAI-o1-mini, Qwen2.5-Math-Instruct, and QwQ-32B. 
However, other LLMs exhibit a significant performance gap between ZH and EN, such as Yi-1.5-Chat and LLaMA-3.1.
For further illustration, Figure~\ref{fig:language} presents the performance of a subset of LLMs in both languages, with the models sorted by the difference in accuracy between EN and ZH. 
It is evident that LLaMA-3.3-70B-Instruct and DeepSeek-R1-Llama-70B show a substantial discrepancy between ZH and EN, while Qwen-2.5-72B-Instruct and QwQ-32B-Preview exhibit negligible differences.
This discrepancy is reasonable, as LLaMA models have limited Chinese corpus for pretraining and fine-tuning \citep{llama312024dubey}, whereas Qwen LLMs are trained on a much larger Chinese corpus \citep{qwen252024Yang, Qwen25Math2024Yang}.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/language.pdf}
    \caption{Performance in different languages, sorted by the difference of EN - ZH.}
    \label{fig:language}
\end{figure}





\subsection{Reliability of Evaluation}\label{sec: human_eval}

Despite several studies utilizing LLMs to evaluate correctness across all test examples \citep{omnimath2024gao} or specific subsets \citep{GaokaoBench2023zhang}, the capability of our {\judge} to reliably evaluate physics problems remains inconclusive.
To substantiate our {\judge} evaluation method, we conduct a human evaluation to determine its alignment with human judgment on a randomly selected subset of 100 test examples. 
Specifically, we initially annotate whether each solution adheres to the ground-truth answer for its corresponding problem, establishing these annotations as the gold standard. 
Subsequently, we compare the evaluations generated by our rule-model combination with the gold standard.
We find that our {\judge} evaluation achieves an accuracy of 98\% when compared to human annotations, underscoring the reliability of our evaluation methods and outcomes. 
Furthermore, our evaluation approach is efficient in assessing correctness for examples whose answers can be easily verified by Sympy, while also demonstrating resilience in handling complex answers that are not suitable for rule-based judgments.


\subsection{Error Analysis}\label{sec: error}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/error.pdf}
    \caption{Distribution of Error Types of OpenAI-o1-mini}
    \label{fig:error}
\end{figure}

% instruction following, exceding max ouput, misunderstanding, flawed reasoning, computation error, knowledge defeciency, wrong application
To gain deeper insights into the performance of LLMs, we select 100 incorrect answers generated by OpenAI-o1-mini and have these errors annotated by human evaluators to determine failure reasons.
As illustrated in Figure~\ref{fig:error}, the primary error types are flawed reasoning, knowledge deficiency, and incorrect application, which contrast with those in mathematics, where calculation is one of the major sources of errors \citep{PoT2022Chen,ugmathbench2025xu}.
This suggests that reasoning and math derivation in physics, which require additional knowledge and involve real-world meanings, are more challenging than the abstract reasoning in mathematics.
%, which aligns well with \citet{physicsreasoner2024pang, MoRA2024jaiswal}.
%This indicates that our {\benchmark} can reveal the current limitations of LLMs in solving physics problems, despite their strong capabilities in mathematical reasoning.
Several cases are provided in Appendix~\ref{app: error}.
% introduce the definition of each error.
% analyze main error types.





\subsection{About Data Leakage}\label{sec: data_leakage}


\begin{table}[t]
\centering
\footnotesize
\caption{The proportion (in \%) of data leakage detection for: (a) The proportion of contaminated examples. (b) The proportion of contaminated and correct examples. ``Prop.'' stands for proportion.}
\begin{tabular}{lcc}
\toprule
\textbf{Model}& \textbf{(a) Prop.} & \textbf{(b) Prop.} \\
\midrule
DeepSeek-Math-7B-RL & 0.0\% & 0.0\% \\
LLaMA3.1-8B-Instruct & 0.53\% & 0.06\% \\
LLaMA3.3-70B-Instruct & 0.65\% & 0.29\% \\
Qwen2.5-Math-7B-Instruct & 0.65\% & 0.36\% \\
Qwen2.5-Math-72B-Instruct & 0.75\% & 0.68\% \\
QwQ-32B-Preview & 0.71\% & 0.65\% \\
DeepSeek-R1-Distill-Qwen-7B & 0.0\% & 0.0\% \\
DeepSeek-R1-Distill-Qwen-32B & 0.0\% & 0.0\% 
\\
\bottomrule
\end{tabular}
\label{tab:contamination}
\end{table}

%To alleviate data contamination from skewing the results of the aforementioned experiments, it is critical to perform data leakage detection. 
We perform data leakage detection to alleviate the potential data contamination in {\benchmark}.
Following \citet{benbench2024xu}, we utilize n-gram accuracy to detect any data leakage within different LLMs. 
Concretely, we combined each problem with its solution in the dataset and randomly chose K positions for extracting 5-grams.
A sample is considered contaminated if the 5-grams predicted by the model match the actual 5-grams from the dataset. 
The results for a subset of LLMs are presented in Table~\ref{tab:contamination}. 
It is evident that most models exhibit some degree of data leakage.
Among them, Qwen2.5-MATH-72B-instruct shows the highest level of leakage, accurately predicting 5 grams in 78 samples. 
Additionally, we report on the contaminated samples that are subsequently answered correctly by the tested models. 
The numbers of both ``Contaminated'' and ``Contaminated \& Correct'' samples are extremely low, suggesting that data leakage has minimal impact on {\benchmark}. 
%In conclusion, {\benchmark} presents substantial challenges for current LLMs.

% ackowlege there is some contamination and claim the influence is minor.
