\section{More Results}\label{app: results}

Results across different subjects are given in Table~\ref{tabapp: results_sub1}, \ref{tabapp: results_sub2} and \ref{tabapp: results_sub3}.
Results across different physics reasoning skills are provided in Table~\ref{tabapp: results_skill}.


\begin{table*}[!thb]
\centering
\footnotesize
\caption{\textbf{Results across subjects of Mechanics \& Thermodynamics on {\benchmark}} (all figures are in \%). Models are classified into four different categories according to their purpose and origin. The best results within each column are \textbf{bolded} and the best results of LLMs within a similar group are \underline{underlined}.}
\scalebox{1.0}{
\begin{tabular}{lcccccccccccc}
\hline
\multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Class. Mech.}} & \multicolumn{2}{c}{\textbf{Theor. Mech.}} & \multicolumn{2}{c}{\textbf{Relativity}} & \multicolumn{2}{c}{\textbf{Ther.dyn.}} & \multicolumn{2}{c}{\textbf{Stat. Mech.}}\\
 \cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
 & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} \\
\hline
\multicolumn{11}{c}{{\textit{Closed-source LLMs}}} \\
\cdashline{1-11}
OpenAI-o1-mini-2024-09-12 & \textbf{47.01} & \textbf{47.61} & \textbf{40.13} & \textbf{35.74} & \textbf{61.35} & \textbf{56.04} & \textbf{54.30} & \textbf{56.45} & \textbf{46.79} & \textbf{51.43} \\
GPT-4o-2024-08-06 & 37.68 & 36.84 & 31.03 & 24.45 & 36.71 & 42.03 & 38.71 & 39.78 & 38.21 & 40.00 \\
GPT-4o-mini-2024-07-18 & 29.19 & 27.27 & 20.69 & 20.06 & 26.09 & 22.22 & 29.84 & 28.49 & 29.11 & 27.50 \\
\hline
\multicolumn{11}{c}{{\textit{Open-source Chat LLMs}}} \\
\cdashline{1-11}
Yi-1.5-6B-Chat & 12.08 & 9.09 & 6.27 & 4.08 & 10.63 & 5.80 & 14.78 & 9.41 & 9.64 & 5.54 \\
Qwen2.5-7B-Instruct & \underline{26.67} & \underline{23.68} & \underline{20.06} & \underline{15.36} & \underline{20.77} & \underline{14.01} & \underline{28.76} & \underline{25.27} & \underline{19.82} & \underline{16.79} \\
LLaMA3.1-8B-Instruct & 12.68 & 8.37 & 11.29 & 4.39 & 9.66 & 5.31 & 16.13 & 8.87 & 12.14 & 8.57 \\
Ministral-8B-Instruct-2410 & 14.00 & 12.56 & 11.91 & 8.46 & 9.66 & 8.70 & 17.47 & 12.63 & 14.29 & 9.11 \\
Yi-1.5-9B-Chat & 17.58 & 12.80 & 10.66 & 8.46 & 14.01 & 10.14 & 20.16 & 15.32 & 14.64 & 10.18 \\
\hline
Mistral-Nemo-Instruct-2407 & 13.88 & 10.29 & 12.23 & 11.29 & 12.56 & 8.70 & 15.86 & 13.98 & 14.82 & 13.39 \\
DeepSeek-MOE-16B-Chat & 3.71 & 2.87 & 3.45 & 2.19 & 2.90 & 3.86 & 4.30 & 5.11 & 3.93 & 2.68 \\
DeepSeek-V2-Lite-Chat & 6.46 & 5.38 & 5.64 & 3.13 & 4.35 & 3.38 & 8.60 & 7.53 & 6.43 & 4.11 \\
Mistral-Small-Instruct-2409 & \underline{22.61} & \underline{19.74} & \underline{20.69} & \underline{15.67} & \underline{15.94} & \underline{16.91} & \underline{22.85} & \underline{23.12} & \underline{26.43} & \underline{27.14} \\
Yi-1.5-34B-Chat & 20.33 & 14.83 & 13.48 & 7.52 & 22.71 & 11.59 & 21.77 & 17.47 & 16.07 & 12.50 \\
\hline
LLaMA3.1-70B-Instruct & 28.23 & 25.48 & 24.45 & 19.12 & 27.05 & 21.26 & 31.72 & 28.23 & 27.14 & 26.43 \\
LLaMA3.3-70B-Instruct & 32.66 & 26.56 & 28.21 & 16.61 & 32.37 & 22.71 & 41.67 & 29.84 & 33.21 & 28.39 \\
Qwen2.5-72B-Instruct & \underline{38.52} & \underline{34.33} & \underline{36.05} & 25.71 & 38.16 & 33.82 & 36.29 & \underline{41.13} & 31.07 & 40.54 \\
Mistral-Large-Instruct-2407 & 36.72 & \underline{34.33} & 35.74 & \underline{28.53} & \underline{38.65} & \underline{37.68} & \underline{42.74} & \underline{41.13} & \underline{38.04} & \underline{41.61} \\
\hline
\multicolumn{11}{c}{{\textit{Specialized Mathematical LLMs}}} \\
\cdashline{1-11}
DeepSeek-Math-7B-Instruct & 14.47 & 12.92 & 11.60 & 9.72 & 11.11 & 9.18 & 12.90 & 13.71 & 13.39 & 13.21 \\
DeepSeek-Math-7B-RL & 15.55 & 12.68 & 13.79 & 8.46 & 10.63 & 9.18 & 18.55 & 12.37 & 14.82 & 12.50 \\
NuminaMath-7B-CoT & 14.23 & 18.42 & 13.17 & 11.91 & 8.21 & 10.14 & 14.78 & 16.13 & 14.11 & 14.82 \\
Mathstral-7B-v0.1 & 15.79 & 13.04 & 12.23 & 10.34 & 13.04 & 7.73 & 14.52 & 12.63 & 15.71 & 14.46 \\
OpenMath2-Llama-3.1-8B & 10.05 & 6.94 & 5.64 & 4.08 & 4.35 & 4.83 & 11.02 & 7.53 & 8.21 & 6.25 \\
Qwen2.5-Math-7B-Instruct & 26.08 & 25.72 & 21.00 & 19.12 & 23.19 & 16.43 & 26.61 & 20.70 & 20.54 & 17.14 \\
OpenMath2-Llama-3.1-70B & 21.05 & 20.10 & 15.99 & 16.61 & 16.43 & 14.98 & 23.39 & 20.70 & 21.07 & 17.86  \\
Qwen2.5-Math-72B-Instruct & \underline{42.22} & \underline{39.83} & \underline{36.36} & \underline{35.42} & \underline{43.96} & \underline{42.51} & \underline{37.10} & \underline{38.98} & \underline{37.32} & \underline{41.96}  \\

\hline
\multicolumn{11}{c}{{\textit{o1-like LLMs}}} \\
\cdashline{1-11}
DeepSeek-R1-Distill-Qwen-7B & 33.13 & 23.56 & 25.08 & 17.24 & 29.95 & 28.02 & 30.38 & 18.01 & 24.82 & 17.14  \\
Skywork-o1-Open-Llama-3.1-8B & 17.46 & 13.88 & 10.03 & 5.96 & 7.25 & 5.31 & 16.40 & 9.14 & 9.82 & 6.96  \\
DeepSeek-R1-Distill-Llama-8B & 17.70 & 7.30 & 9.40 & 4.39 & 21.26 & 9.66 & 17.20 & 8.33 & 15.89 & 6.79  \\
QwQ-32B-Preview & 37.44 & \underline{39.95} & 33.54 & 29.78 & 43.96 & \underline{43.96} & 40.05 & \underline{42.47} & 33.04 & \underline{34.64}  \\
DeepSeek-R1-Distill-Qwen-32B & 37.44 & 29.07 & 28.53 & 22.57 & 42.03 & 40.10 & 35.48 & 31.18 & 33.04 & 25.00  \\
DeepSeek-R1-Distill-Llama-70B & \underline{44.26} & 33.97 & \underline{38.24} & \underline{30.09} & \underline{50.24} & 37.68 & \underline{45.70} & 40.86 & \underline{40.36} & 31.43 \\

\bottomrule
\end{tabular}
}
\label{tabapp: results_sub1}
\end{table*}

\begin{table*}[!thb]
\centering
\footnotesize
\caption{\textbf{Results across subjects of Electromagnatism on {\benchmark}} (all figures are in \%). Models are classified into four different categories according to their purpose and origin. The best results within each column are \textbf{bolded} and the best results of LLMs within a similar group are \underline{underlined}.}
\scalebox{1.0}{
\begin{tabular}{lcccccccccc}
\hline
\multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Class. Elec.}} & \multicolumn{2}{c}{\textbf{Elec.Dy.}} & \multicolumn{2}{c}{\textbf{Geo. Optics}} & \multicolumn{2}{c}{\textbf{Wave Optics}} \\
 \cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9}
 & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH}  \\
\hline
\multicolumn{9}{c}{{\textit{Closed-source LLMs}}} \\
\cdashline{1-9}
OpenAI-o1-mini-2024-09-12 & \underline{45.38} & \underline{\textbf{48.21}} & \underline{\textbf{41.85}} & \underline{\textbf{40.76}} & \underline{27.59} & \underline{\textbf{31.03}} & \underline{45.36} & \underline{\textbf{40.40}} \\
GPT-4o-2024-08-06 & 38.46 & 40.77 & 32.07 & 35.87 & 20.69 & 25.86 & 39.40 & 27.48 \\
GPT-4o-mini-2024-07-18 & 25.13 & 26.41 & 21.20 & 17.93 & 18.97 & 20.69 & 27.81 & 20.20 \\
\hline
\multicolumn{9}{c}{{\textit{Open-source Chat LLMs}}} \\
\cdashline{1-9}
Yi-1.5-6B-Chat & 12.56 & 8.21 & 8.70 & 8.70 & 8.62 & 12.07 & 13.91 & 5.96 \\
Qwen2.5-7B-Instruct & \underline{26.15} & \underline{20.00} & \underline{17.39} & \underline{18.48} & \underline{20.69} & \underline{15.52} & \underline{26.16} & \underline{15.56} \\
LLaMA3.1-8B-Instruct & 12.05 & 9.74 & 14.67 & 10.87 & 15.52 & 8.62 & 16.89 & 9.60 \\
Ministral-8B-Instruct-2410 & 14.87 & 6.67 & 11.96 & 11.96 & 8.62 & 12.07 & 19.87 & 9.60 \\
Yi-1.5-9B-Chat & 17.18 & 15.38 & 16.30 & 11.41 & 17.24 & \underline{15.52} & 13.58 & 11.26 \\
\hline
Mistral-Nemo-Instruct-2407 & 15.38 & 12.82 & 16.85 & 11.41 & 13.79 & 10.34 & 12.91 & 11.59 \\
DeepSeek-MOE-16B-Chat & 3.85 & 3.85 & 4.89 & 4.35 & 5.17 & 1.72 & 6.29 & 6.95 \\
DeepSeek-V2-Lite-Chat & 7.69 & 4.87 & 6.52 & 7.07 & 6.90 & 3.45 & 5.63 & 6.62 \\
Mistral-Small-Instruct-2409 & \underline{25.13} & \underline{19.74} & \underline{17.93} & \underline{20.65} & \underline{17.24} & \underline{15.52} & \underline{23.51} & \underline{15.89} \\
Yi-1.5-34B-Chat & 20.00 & 12.05 & 14.13 & 14.13 & 15.52 & \underline{15.52} & 20.20 & 12.25 \\
\hline
LLaMA3.1-70B-Instruct & 28.46 & 24.62 & 26.09 & 23.37 & 10.34 & 17.24 & 28.81 & 21.19 \\
LLaMA3.3-70B-Instruct & 33.85 & 29.49 & 30.98 & 23.91 & 20.69 & 17.24 & 36.42 & 20.86 \\
Qwen2.5-72B-Instruct & 35.64 & 37.18 & 31.52 & 33.70 & 18.97 & 27.59 & 33.77 & \underline{31.46} \\
Mistral-Large-Instruct-2407 & \underline{43.33} & \underline{40.00} & \underline{33.15} & \underline{34.78} & \underline{24.14} & \underline{29.31} & \underline{37.75} & 29.47 \\
\hline
\multicolumn{9}{c}{{\textit{Specialized Mathematical LLMs}}} \\
\cdashline{1-9}
DeepSeek-Math-7B-Instruct & 17.69 & 14.87 & 13.04 & 11.41 & 17.24 & 10.34 & 16.89 & 12.58 \\
DeepSeek-Math-7B-RL & 17.44 & 14.62 & 17.93 & 10.87 & 13.79 & 12.07 & 10.93 & 11.59 \\
NuminaMath-7B-CoT & 15.90 & 15.13 & 15.22 & 15.22 & 10.34 & 22.41 & 12.91 & 14.90 \\
Mathstral-7B-v0.1 & 16.41 & 13.08 & 18.48 & 19.02 & 15.52 & 15.52 & 19.54 & 14.90 \\
OpenMath2-Llama-3.1-8B & 10.00 & 6.15 & 8.70 & 7.61 & 13.79 & 8.62 & 10.60 & 8.61 \\
Qwen2.5-Math-7B-Instruct & 24.62 & 22.56 & 19.57 & 13.59 & 15.52 & 13.79 & 22.52 & 15.89 \\
OpenMath2-Llama-3.1-70B & 21.54 & 21.28 & 23.91 & 15.76 & 17.24 & \underline{25.86} & 22.85 & 15.56 \\
Qwen2.5-Math-72B-Instruct & \underline{40.26} & \underline{40.77} & \underline{28.26} & \underline{38.59} & \underline{\textbf{29.31}} & 22.41 & \underline{36.09} & \underline{38.41} \\

\hline
\multicolumn{9}{c}{{\textit{o1-like LLMs}}} \\
\cdashline{1-9}
DeepSeek-R1-Distill-Qwen-7B & 32.05 & 20.51 & 18.48 & 13.59 & 15.52 & 10.34 & 29.14 & 17.22 \\
Skywork-o1-Open-Llama-3.1-8B & 11.54 & 8.97 & 14.13 & 9.78 & 20.69 & 6.90 & 17.22 & 7.95 \\
DeepSeek-R1-Distill-Llama-8B & 18.46 & 6.92 & 12.50 & 7.07 & 13.79 & 6.90 & 18.87 & 5.63 \\
QwQ-32B-Preview & 35.38 & \underline{35.13} & 31.52 & \underline{36.96} & 20.69 & 22.41 & 41.39 & \underline{25.50} \\
DeepSeek-R1-Distill-Qwen-32B & 37.44 & 27.44 & 22.28 & 21.74 & 15.52 & 17.24 & 38.41 & 20.20 \\
DeepSeek-R1-Distill-Llama-70B & \underline{\textbf{46.41}} & 31.54 & \underline{34.78} & 28.80 & \underline{25.86} & \underline{29.31} & \underline{\textbf{46.69}} & 25.17 \\
\bottomrule
\end{tabular}
}
\label{tabapp: results_sub2}
\end{table*}


\begin{table*}[!thb]
\centering
\footnotesize
\caption{\textbf{Results across subjects of Modern Physics on {\benchmark}} (all figures are in \%). Models are classified into four different categories according to their purpose and origin. The best results within each column are \textbf{bolded} and the best results of LLMs within a similar group are \underline{underlined}.}
\scalebox{1.0}{
\begin{tabular}{lcccccccccc}
\hline
\multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Quan. Mech.}} & \multicolumn{2}{c}{\textbf{Atomic Phy.}} & \multicolumn{2}{c}{\textbf{S.-S. Phy.}} & \multicolumn{2}{c}{\textbf{Semi. Phy.}} \\
 \cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9}
 & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} \\
\hline
\multicolumn{9}{c}{{\textit{Closed-source LLMs}}} \\
\cdashline{1-9}
OpenAI-o1-mini-2024-09-12 & \textbf{49.95} & \textbf{50.54} & \textbf{58.58} & \textbf{54.86} & \textbf{43.60} & \textbf{45.93} & \textbf{63.98} & \textbf{60.75} \\
GPT-4o-2024-08-06 & 41.71 & 39.06 & 44.48 & 39.23 & 36.05 & 41.28 & 46.77 & 58.94 \\
GPT-4o-mini-2024-07-18 & 27.28 & 28.66 & 33.99 & 28.63 & 27.91 & 27.33 & 36.02 & 37.63 \\
\hline
\multicolumn{9}{c}{{\textit{Open-source Chat LLMs}}} \\
\cdashline{1-9}
Yi-1.5-6B-Chat & 11.48 & 7.56 & 15.19 & 9.84 & 9.30 & 8.72 & 22.04 & 16.67 \\
Qwen2.5-7B-Instruct & \underline{23.06} & \underline{20.22} & \underline{27.10} & \underline{21.75} & \underline{22.09} & \underline{21.51} & \underline{34.95} & \underline{30.65} \\
LLaMA3.1-8B-Instruct & 13.05 & 10.79 & 19.89 & 14.75 & 10.47 & 11.05 & 27.96 & 18.28 \\
Ministral-8B-Instruct-2410 & 15.90 & 10.21 & 21.86 & 12.68 & 15.70 & 11.63 & 27.42 & 20.97 \\
Yi-1.5-9B-Chat & 16.58 & 14.82 & 22.84 & 14.43 & 14.53 & 14.53 & 29.03 & 24.19 \\
\hline
Mistral-Nemo-Instruct-2407 & 16.68 & 14.33 & 20.44 & 17.38 & 15.12 & 13.37 & 20.97 & 24.19 \\
DeepSeek-MOE-16B-Chat & 5.00 & 4.22 & 9.73 & 5.03 & 6.40 & 1.74 & 6.99 & 5.91 \\
DeepSeek-V2-Lite-Chat & 6.67 & 6.48 & 13.01 & 9.07 & 2.91 & 4.07 & 13.44 & 9.68 \\
Mistral-Small-Instruct-2409 & 27.67 & 23.06 & \underline{31.91} & \underline{22.40} & \underline{25.58} & \underline{21.51} & \underline{37.10} & \underline{29.57} \\
Yi-1.5-34B-Chat & \underline{18.84} & \underline{18.06} & 26.56 & 17.27 & 22.09 & 11.05 & 31.18 & 25.81 \\
\hline
LLaMA3.1-70B-Instruct & 31.11 & 27.38 & 33.22 & 26.01 & 29.65 & 24.42 & 45.16 & 34.95 \\
LLaMA3.3-70B-Instruct & 34.54 & 26.30 & 43.83 & 25.36 & \underline{36.63} & 27.33 & 44.09 & 36.56 \\
Qwen2.5-72B-Instruct & 34.25 & 37.98 & 40.00 & 37.81 & 33.14 & 34.30 & 42.47 & 45.16 \\
Mistral-Large-Instruct-2407 & \underline{42.39} & \underline{41.71} & \underline{47.10} & \underline{38.69} & 34.30 & \underline{36.05} & \underline{48.92} & \underline{45.16} \\
\hline
\multicolumn{9}{c}{{\textit{Specialized Mathematical LLMs}}} \\
\cdashline{1-9}
DeepSeek-Math-7B-Instruct & 15.51 & 13.15 & 23.17 & 16.39 & 15.70 & 17.44 & 21.51 & 23.66 \\
DeepSeek-Math-7B-RL & 16.98 & 12.76 & 20.44 & 17.81 & 15.12 & 12.21 & 20.97 & 20.97 \\
NuminaMath-7B-CoT & 17.27 & 16.19 & 18.91 & 20.22 & 12.21 & 14.53 & 23.66 & 26.34 \\
Mathstral-7B-v0.1 & 16.88 & 16.00 & 20.77 & 17.81 & 23.26 & 12.21 & 29.57 & 23.66 \\
OpenMath2-Llama-3.1-8B & 9.91 & 8.34 & 13.33 & 10.16 & 11.05 & 8.14 & 17.20 & 14.52 \\
Qwen2.5-Math-7B-Instruct & 24.44 & 24.14 & 28.74 & 18.58 & 19.77 & 16.28 & 33.33 & 24.19 \\
OpenMath2-Llama-3.1-70B & 21.10 & 18.25 & 28.52 & 22.62 & 28.49 & 21.51 & 31.18 & 33.87 \\
Qwen2.5-Math-72B-Instruct & \underline{39.35} & \underline{41.22} & \underline{43.61} & \underline{37.60} & \underline{31.98} & \underline{34.30} & \underline{47.85} & \underline{43.55} \\

\hline
\multicolumn{9}{c}{{\textit{o1-like LLMs}}} \\
\cdashline{1-9}
DeepSeek-R1-Distill-Qwen-7B & 27.67 & 21.00 & 32.35 & 20.33 & 19.77 & 12.21 & 38.17 & 28.49 \\
Skywork-o1-Open-Llama-3.1-8B & 12.95 & 9.52 & 17.70 & 10.38 & 11.63 & 7.56 & 20.43 & 14.52 \\
DeepSeek-R1-Distill-Llama-8B & 16.39 & 9.13 & 25.57 & 9.07 & 11.05 & 6.98 & 31.72 & 10.75 \\
QwQ-32B-Preview & 33.56 & \underline{37.88} & 45.36 & \underline{40.87} & 27.33 & 28.49 & 43.55 & 44.62 \\
DeepSeek-R1-Distill-Qwen-32B & 32.78 & 26.10 & 44.70 & 31.48 & 26.16 & 21.51 & 45.70 & 37.10 \\
DeepSeek-R1-Distill-Llama-70B & \underline{46.71} & 35.82 & \underline{53.66} & 35.63 & \underline{40.70} & \underline{36.05} & \underline{57.53} & \underline{48.39} \\
\bottomrule
\end{tabular}
}
\label{tabapp: results_sub3}
\end{table*}



\begin{table*}[!thb]
\centering
\footnotesize
\caption{\textbf{Results across different skill sets on {\benchmark}} (all figures are in \%). Models are classified into four different categories according to their purpose and origin. The best results within each column are \textbf{bolded} and the best results of LLMs within a similar group are \underline{underlined}.}
\scalebox{1.0}{
\begin{tabular}{lcccccccccccc}
\hline
\multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Know. Recall}} & \multicolumn{2}{c}{\textbf{Laws App.}} & \multicolumn{2}{c}{\textbf{Math Deri.}} & \multicolumn{2}{c}{\textbf{Prac. App.}} & \multicolumn{2}{c}{\textbf{Others}}\\
 \cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
 & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} & \textbf{EN} & \textbf{ZH} \\
\hline
\multicolumn{11}{c}{{\textit{Closed-source LLMs}}} \\
\cdashline{1-11}
OpenAI-o1-mini-2024-09-12 & \textbf{69.18} & \textbf{63.87} & \textbf{53.60} & \textbf{51.46} & \textbf{42.16} & \textbf{45.05} & \textbf{47.92} & \textbf{45.75} & \textbf{50.49} & \textbf{50.97} \\
GPT-4o-2024-08-06 & 63.36 & 55.25 & 42.46 & 38.94 & 31.40 & 33.71 & 34.54 & 32.72 & 38.83 & 40.69 \\
GPT-4o-mini-2024-07-18 & 53.77 & 48.29 & 30.81 & 28.00 & 20.72 & 22.48 & 24.59 & 17.00 & 29.61 & 26.70 \\
\hline
\multicolumn{11}{c}{{\textit{Open-source Chat LLMs}}} \\
\cdashline{1-11}
Yi-1.5-6B-Chat & 26.37 & 21.23 & 14.46 & 7.97 & 6.71 & 5.81 & 10.67 & 4.34 & 15.53 & 9.71 \\
Qwen2.5-7B-Instruct & \underline{48.29} & \underline{40.24} & \underline{26.78} & \underline{21.72} & \underline{17.48} & \underline{15.68} & \underline{19.71} & \underline{13.92} & \underline{27.18} & \underline{22.33} \\
LLaMA3.1-8B-Instruct & 34.93 & 25.51 & 15.99 & 10.83 & 8.02 & 6.44 & 14.10 & 6.51 & 17.48 & 12.62 \\
Ministral-8B-Instruct-2410 & 37.67 & 26.37 & 16.91 & 10.42 & 10.41 & 8.65 & 14.83 & 5.61 & 19.90 & 14.56 \\
Yi-1.5-9B-Chat & 36.13 & 29.62 & 20.49 & 13.64 & 10.95 & 9.95 & 14.29 & 9.40 & 18.45 & 16.02 \\
\hline
Mistral-Nemo-Instruct-2407 & 35.62 & 32.71 & 15.94 & 12.62 & 11.58 & 10.50 & 12.84 & 8.50 & 16.99 & 16.50 \\
DeepSeek-MOE-16B-Chat & 13.18 & 9.42 & 5.52 & 4.45 & 3.06 & 2.43 & 4.88 & 2.71 & 7.77 & 4.85 \\
DeepSeek-V2-Lite-Chat & 22.26 & 17.98 & 8.94 & 6.34 & 3.78 & 3.74 & 4.16 & 3.25 & 8.25 & 5.34 \\
Mistral-Small-Instruct-2409 & \underline{50.17} & \underline{41.78} & \underline{25.45} & \underline{20.75} & \underline{20.54} & \underline{18.74} & \underline{19.53} & \underline{12.48} & \underline{31.55} & \underline{27.67} \\
Yi-1.5-34B-Chat & 42.81 & 33.56 & 23.91 & 15.59 & 12.43 & 10.81 & 17.36 & 11.93 & 22.33 & 13.59 \\
\hline
LLaMA3.1-70B-Instruct & 52.23 & 42.47 & 30.76 & 24.83 & 23.87 & 23.06 & 27.12 & 18.81 & 29.61 & 28.16 \\
LLaMA3.3-70B-Instruct & 59.25 & 40.92 & 37.71 & 24.48 & 28.47 & 24.37 & 34.54 & 20.25 & 35.44 & 33.01 \\
Qwen2.5-72B-Instruct & 56.68 & 56.34 & 38.02 & 37.61 & 29.01 & 32.39 & 33.27 & 26.76 & \underline{40.29} & \underline{39.32} \\
Mistral-Large-Instruct-2407 & \underline{64.73} & \underline{58.73} & \underline{42.51} & \underline{38.17} & \underline{33.56} & \underline{34.86} & \underline{37.43} & \underline{27.12} & \underline{39.81} & \underline{38.35} \\
\hline
\multicolumn{11}{c}{{\textit{Specialized Mathematical LLMs}}} \\
\cdashline{1-11}
DeepSeek-Math-7B-Instruct & 37.33 & 32.36 & 16.96 & 14.05 & 10.50 & 10.27 & 13.02 & 6.69 & 19.42 & 16.99 \\
DeepSeek-Math-7B-RL & 38.36 & 30.14 & 16.35 & 12.62 & 12.39 & 10.27 & 9.58 & 10.67 & 20.87 & 14.56 \\
NuminaMath-7B-CoT & 32.36 & 33.90 & 14.51 & 18.14 & 12.57 & 11.85 & 12.12 & 11.03 & 20.39 & 23.30 \\
Mathstral-7B-v0.1 & 41.10 & 28.94 & 16.66 & 14.41 & 12.70 & 12.43 & 13.56 & 9.76 & 19.42 & 17.48 \\
OpenMath2-Llama-3.1-8B & 21.92 & 18.66 & 11.24 & 8.07 & 6.71 & 4.95 & 7.78 & 7.23 & 13.11 & 7.28 \\
Qwen2.5-Math-7B-Instruct & 45.21 & 35.10 & 26.26 & 20.34 & 18.87 & 18.78 & 20.61 & 11.39 & 25.73 & 28.16 \\
OpenMath2-Llama-3.1-70B & 47.60 & 37.67 & 22.79 & 21.67 & 16.67 & 14.10 & 19.89 & 17.54 & 25.24 & 20.39 \\
Qwen2.5-Math-72B-Instruct & \underline{61.30} & \underline{56.51} & \underline{41.44} & \underline{38.83} & \underline{33.69} & \underline{38.15} & \underline{33.82} & \underline{28.57} & \underline{39.81} & \underline{39.81} \\
\hline
\multicolumn{11}{c}{{\textit{o1-like LLMs}}} \\
\cdashline{1-11}
DeepSeek-R1-Distill-Qwen-7B & 47.26 & 34.59 & 32.40 & 20.95 & 22.03 & 16.53 & 26.40 & 15.73 & 31.55 & 21.36 \\
Skywork-o1-Open-Llama-3.1-8B & 31.16 & 21.58 & 16.04 & 9.04 & 9.01 & 7.61 & 12.12 & 6.51 & 16.02 & 11.65 \\
DeepSeek-R1-Distill-Llama-8B & 39.55 & 19.86 & 21.15 & 8.28 & 10.68 & 4.91 & 16.46 & 4.52 & 19.90 & 10.19 \\
QwQ-32B-Preview & 58.05 & \underline{57.19} & 41.29 & \underline{39.04} & 28.15 & \underline{31.85} & 36.53 & \underline{31.46} & 43.20 & \underline{38.83} \\
DeepSeek-R1-Distill-Qwen-32B & 56.85 & 45.89 & 39.29 & 30.10 & 27.61 & 22.07 & 37.25 & 22.24 & 35.44 & 30.10 \\
DeepSeek-R1-Distill-Llama-70B & \underline{64.38} & 46.06 & \underline{49.41} & 36.02 & \underline{38.56} & 31.35 & \underline{43.04} & 28.39 & \underline{48.54} & 34.47 \\

\bottomrule
\end{tabular}
}
\label{tabapp: results_skill}
\end{table*}
