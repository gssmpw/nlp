\section{Detailed Experimental Setup}\label{app: exp_setup}
\subsection{Evaluated LLMs}\label{app:models}


Our evaluation encompasses a range of LLMs, including both proprietary commercial models and publicly accessible models. For open-source LLMs, we cover general-purpose LLMs, o1-like LLMs, and specialized math LLMs.

Closed-source LLMs are listed as follows:
\begin{itemize}
    \item \textbf{o1-preview} \citep{o1}: An early preview of OpenAI's o1 model, designed to reason about hard problems using broad general knowledge about the world. We used \texttt{o1-preview-2024-09-12} for our evaluation.
    \item \textbf{GPT-4o} \citep{gpt4o2024openai}: GPT-4o is a multimodal LLM, and has the same high intelligence as GPT-4 Turbo but is much more efficient. For evaluation, we use this specific version: \texttt{GPT-4o-2024-08-06}.
    \item \textbf{GPT-4o-mini}: GPT-4o-mini is even more efficient and cheaper than GPT-4o with the cost of minor performance drop. We use \texttt{GPT-4o-mini-2024-07-18} for our experiments.
    %\item \textbf{Claude-3-Opus} \citep{claude3}: Anthropic's most intelligent model, claimed to outperform its peers on most of the common evaluation benchmarks for AI systems.
%    \item \textbf{Gemini-1.5-Pro} \citep{gemini2023team}: Google's best performing multimodal model optimized for a wide-range of reasoning tasks.
\end{itemize}

The following open-source general-purpose LLMs are evaluated on our {\benchmark}:
\begin{itemize}
    %\item \textbf{Llama-3.1-Instruct} \citep{llama312024dubey}:  \href{https://www.llama.com/llama3/license/}{LLaMA 3 Community License}.
    \item \textbf{LLaMA-3.1-Instruct} \citep{llama312024dubey}: LLaMA-3.1 models are the most capable of the LLaMA families as of writing this paper. We used instruction finetuned 8B and 70B versions of the model. These models are licensed under \href{https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE}{Llama 3.1 Community License}.
    \item \textbf{Ministral-8B-Instruct-2410} \citep{ministral8b}: \href{https://mistral.ai/licenses/MRL-0.1.md}{mrl License}
    \item \textbf{Mistral-Nemo-Instruct-2407} \citep{mistral_nemo_blog}: \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0}
    \item \textbf{Mistral-Small-Instruct-2409} \citep{mistral_small_blog}: \href{https://mistral.ai/licenses/MRL-0.1.md}{MRL License}.
    \item \textbf{Mistral-Large-Instruct-2407} \citep{mistral_large_blog}: \href{https://mistral.ai/licenses/MRL-0.1.md}{MRL License}.
    \item \textbf{Qwen2.5-Instruct} \citep{qwen252024Yang}: Qwen2.5 series are developed with dedication to math and coding. We used 7B and 72B models. 7B models are licensed under \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0}, while 72B models are under \href{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE}{Qwen  License}.
    \item \textbf{Yi-1.5-Chat} \citep{ai2024yi}: Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability compared to its predecessor. We used 6B, 9B, 34B variants. Yi-1.5 series are licensed under \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0}.
    \item \textbf{DeepSeek-V2-Lite-Chat} \citep{deepseekv2}: model under \href{https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-MODEL}{Model License} code under \href{https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-CODE}{MIT License}.
    \item \textbf{deepseek-moe-16b-chat} \citep{dai2024deepseekmoe}: Model under \href{https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/LICENSE-MODEL}{Model License}, code under \href{https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/LICENSE-CODE}{MIT License}.
\end{itemize}


For o1-like LLMs, we use the following:

\begin{itemize}
    \item \textbf{QwQ-32B-Preview} \citep{qwq32b_blog}: QwQ-32B-Preview is developed by the Qwen Team, focused on advancing AI reasoning capabilities. It is under \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0} License.
    \item \textbf{Skywork-o1-Open-Llama-3.1-8B} \citep{skyworkmodelcard}: Skywork-o1-Open-Llama-3.1-8B is an LLM that incorporates o1-like slow thinking and reasoning capabilities. It is developed by the Skywork team.
    \item \textbf{DeepSeek-R1 Distilled serires} \citep{deepseekr12025deepseekai}:
    These LLMs are distilled from DeepSeek's first-generation reasoning models DeepSeek-R1. Model under \href{https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL}{Model License}, code under \href{https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE}{MIT License}. In our evaluation, we use DeepSeek-R1-Distilled-Llama-8B, -Llama-70B; -Qwen-7B, -Qwen-32B.
\end{itemize}



We also experiment with the following specialized math LLMs in our study:
\begin{itemize}
    \item \textbf{DeepSeekMath-7B} \citep{deepseekmath2024shao}: DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and continues pre-training on math-related tokens. We tested both DeepSeekMath-7B-RL and DeepSeekMath-7B-Instruct variants. Models are under \href{https://github.com/deepseek-ai/DeepSeek-Math/blob/main/LICENSE-MODEL}{Model License} while code is under \href{https://github.com/deepseek-ai/DeepSeek-Math/blob/main/LICENSE-CODE}{MIT License}.
    \item \textbf{Qwen2.5-Math} \citep{Qwen25Math2024Yang}: Qwen2.5-Math is a series of specialized math language models built upon the Qwen2.5 LLMs. We evaluated 7B and 72B variants. They are under the same license as the Qwen2.5-Instruct series.
    \item \textbf{Mathstral-7B} \citep{mathstral2023}: Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. This model is published under \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0}.
    \item \textbf{Numinamath-7B-CoT} \citep{numinamath7b}: This model is finetuned from DeepSeekMath-7B-base with two stages of supervised fine-tuning to solve math problems using chain of thought (CoT). It is licensed under \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache 2.0}.
    \item \textbf{OpenMath2-Llama-3.1} \citep{openmathinstruct2024toshniwal}:
    These are specialized math LLMs that have undergone SFT on 2.3M augmented GSM-8K and MATH training examples. These models are trained by Nvidia and are licensed under \href{https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE}{Llama 3.1 Community License}.
\end{itemize}





\subsection{Evaluation Prompts}\label{app: prompts}

The prompts employed in our experiments are presented in Table~\ref{tabapp:prompts}, with detailed explanations of response types available in Table~\ref{tabapp:answer type instructions}. 
For simplicity, we here only show prompts used by English problems. 
The prompts for Chinese problems are quite similar and will be released in our code repository. 
Based on \citet{OlympiadBench2024He, OlympicArena2024huang}, these prompts are tailored for diverse subjects and response types to improve evaluation efficiency.
It should be noted that for chat models, we will adhere to their official \href{https://huggingface.co/docs/transformers/main/en/chat_templating}{chat template}.


\begin{table}[!t]
\centering
\footnotesize
\caption{Evaluation prompts for English problems with single answers or multiple answers. \{problem\} is the specific problem to evaluate. \{subject\} denotes the subject this problem belongs to and all subjects are given in Figure~\ref{fig:example}. \{answer\_type\_description\} are specified in Table~\ref{tabapp:answer type instructions}.}
\begin{tabular}{p{12cm}}
\toprule
Evaluation Prompt for Single Answer \\
\midrule
The following is an open-ended problem from \{subject\} of undergraduate-level Physics. \\
The answer of The problem should be \{answer\_type\_description\}. \\
Please calculate the answer according to the given requirements and the information provided. Please use LaTeX format to represent the variables and formulas used in the solution process and results. \\
Please end your solution with "So the final answer is \boxed{\text{answer}}(unit)." and give the result explicitly, note that the unit of the answers should not be included in \boxed{}. \\
\\
\{problem\}
\\
\midrule
Evaluation Prompt for Multiple Answers \\
\midrule
The following is an open-ended problem from \{subject\} of undergraduate-level Physics. \\
The question has multiple answers, with the answers in order being \{answer\_type\_description\}. \\
Please calculate the answer according to the given requirements and the information provided. Please use LaTeX format to represent the variables and formulas used in the solution process and results. \\
Please end your solution with "So the final answer is \boxed{\text{multiple answers connected with commas}}." and give the result explicitly, note that the unit of the answers should not be included in \boxed{}. \\
\\
\{problem\}
\\
\bottomrule
\end{tabular}%


\label{tabapp:prompts}
\end{table}








\subsection{Evaluation Parameters}\label{app: parameters}

To maintain consistency in evaluations and facilitate reproduction, we set the maximum output length to 4,096 tokens and employ a greedy decoding strategy with the temperature 0.
For LLMs with a maximum output length of less than 4,096 tokens during SFT, such as NuminaMath-CoT-7B, we adjust the maximum output length to align with their specific SFT configurations.
For \texttt{OpenAI-o1-mini}, we set it to 8,192 tokens, as this model often requires a higher token count for reasoning tasks. 
Exceeding the maximum output length can result in no output being returned.
Similarly, we also set the maximum output length o1-like LLMs to be 8,192 tokens.
The temperature for the \texttt{OpenAI-o1-mini} model is restricted to a value of 1 due to new regulations from OpenAI. 
Setting the temperature to other values would result in an error from the API.




\subsection{{\judge} Details}\label{app: constants}

The whole pipeline of {\judge} is given in \cref{alg:judge}.
For the first stage, different types of answers (TF, MC, NV, EX, EQ, IN) are handled as follows:
\begin{itemize}
    \item For TF and MC, answers are judged after transforming model-generated answers to the same format as the golden answers.
    \item For NV, answers are converted into scientific notation. 
    Only the bases of the scientific notation are considered, allowing for a relative error of up to to account for unit differences or rounding.
    \item For EX and EQ, answers are normalized by removing all physical constants.
    The physical constants are listed in Table~\ref{tabapp: physics_constant}.
    \item For IN, answers are judged by comparing the two endpoints, treating them as either NV or EX based on the context.
\end{itemize}
In the second stage, \texttt{GPT-4o} is employed to evaluate answers that were flagged as "False" by the rule-based system. 
We manually design judging prompts based on the prompt provided by \citet{omnimath2024gao}.
As this few-shot prompt is long, we will release it in our code repository.

\begin{algorithm}[t] 
   \caption{{\judge} Pipeline} 
   \label{alg:judge} 
\begin{algorithmic} 
   \STATE {\bfseries Input:} Problem $P$, Solution $S$, Golden Answer List $GT$, Model Solution $s$, Model Answer List $A$.
   \STATE Initialize $Correctness = False$
   \IF{$len(A)$ not equals $len(GT)$}
   \STATE return $False$
   \ENDIF

    \FOR{$gt, a$ in $GT, A$}
    \STATE $flag = False$
   \IF{$gt$ equals $a$}
   \STATE $flag = True$; continue
   \ENDIF % direct equal
   \IF{$P$ is a T/F or MC question}
   \STATE Transform $gt$, $a$ to standard forms: $gt'$, $a'$
   \IF{$gt'$ equals $a'$}
   \STATE $flag = True$; continue
   \ENDIF
   \ENDIF % TF or MC equal
   \IF{$gt$ is expression or equation}
   \STATE Normalizing $a$, $gt$ to $a'$, $gt'$ by removing physical constants
   \IF{$gt$ equals $a$ as equation or $gt'$ equals $a'$ as equation}
   \STATE $flag = True$; continue
    \ELSIF{$gt$ equals $a$ as expression or $gt'$ equals $a'$ as expression}
    \STATE $flag = True$; continue
   \ENDIF
   \ENDIF % equation or expression equal
   \IF{$gt$ is Numeric Value}
   \STATE Transform $a$, $gt$ into scientific notation: $a = a_{base} \times 10^{a_{exp}}$, $gt = gt_{base} \times 10^{gt_{exp}}$
   \IF{$|a_{base} - gt_{base}|/|gt_{base}| < \epsilon$}
   \STATE $flag = True$; continue
   \ENDIF
   \ENDIF % numeric value
   \IF{$gt$ is interval}
   \STATE let $c, d$ be endpoints of $gt$; $e, f$ be endpoints of $a$
   \IF{$c$ equals $e$ as NV or EX and $d$ equals $f$ as NV or EX}
   \STATE $flag = True$; continue
   \ENDIF
   \ENDIF
   \ENDFOR
   \IF{$flag$ equals $True$}
   \STATE return $True$
   \ELSE
   \STATE return $ModelJudge(P, S, GT, s, A)$
   \ENDIF
\end{algorithmic} 
\end{algorithm} 



\begin{table}[ht]
\centering
\caption{Physical Constants in {\judge}.}
\label{tabapp: physics_constant}
\begin{tabular}{|l|c|}
\hline
\textbf{Physical Quantity} & \textbf{Symbol} \\
\hline
Speed of light in vacuum & $ c $ \\
Newtonian constant of gravitation & $ G $ \\
Avogadro constant & $ N_A $ \\
Universal gas constant & $ R $ \\
Boltzmann constant (macroscopic) & $ R/N_A $ \\
Molar volume of ideal gas & $ V_m $ \\
Elementary charge (proton charge) & $ e $ \\
Electron mass & $ m_e $ \\
Electron charge-to-mass ratio & $ -e / m_e $ \\
Proton mass & $ m_p $ \\
Neutron mass & $ m_n $ \\
Vacuum permittivity (electric constant) & $ \varepsilon_0 $ \\
Vacuum permeability (magnetic constant) & $ \mu_0 $ \\
Electron magnetic moment & $ \mu_e $ \\
Proton magnetic moment & $ \mu_p $ \\
Bohr radius & $ a_0 $ \\
Bohr magneton & $ \mu_B $ \\
Nuclear magneton & $ \mu_N $ \\
Planck constant & $ \hbar $ \\
Planck constant & $ h $ \\
Fine-structure constant & $ \alpha $ \\
Rydberg constant & $ R_\infty $ \\
Compton wavelength & $ \frac{\hbar}{mc} $ \\
Proton-electron mass ratio & $ \frac{m_p}{m_e} $ \\
Boltzmann constant & $ k $ \\
\hline
\end{tabular}
\end{table}