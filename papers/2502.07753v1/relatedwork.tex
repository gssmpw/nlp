\section{Related Work}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/all-images-for-das_2.pdf}
    \caption{Diverse generations from Direct Ascent Synthesis across a range of concepts and styles. Results were obtained by optimizing against an ensemble of three CLIP models, with prompt augmentation to control image aesthetics: discouraging text generation (-0.3 × "Optical Character Recognition"), enhancing rendering quality (0.3 × "octane render, unreal engine, ray tracing, volumetric lighting"), and preventing image stacking (-0.3 × "multiple exposure").}
    \label{fig:all-images}
\end{figure*}
%
\subsection{The Evolution of Image Synthesis}
Image synthesis has traditionally followed two parallel tracks. The generative track progressed from VAEs \cite{kingma2022autoencodingvariationalbayes} and GANs \cite{goodfellow2014generativeadversarialnetworks} to diffusion models \cite{ho2020denoisingdiffusionprobabilisticmodels,rombach2022highresolutionimagesynthesislatent}, achieving remarkable quality through increasingly complex training. The discriminative track revealed rich internal representations through feature visualization \cite{yosinski2015understandingneuralnetworksdeep, nguyen2016synthesizingpreferredinputsneurons, olah2017feature} and adversarial examples \cite{goodfellow2015explainingharnessingadversarialexamples}, while models like CLIP \cite{radford2021learningtransferablevisualmodels} demonstrated that discriminative training can capture general visual concepts.

\subsection{Bridging Discrimination and Generation}
Several works have hinted at deeper connections between these approaches. Feature inversion methods \cite{mahendran2014understanding} showed that discriminative representations contain generative information, though with limited quality. Analysis of GAN discriminators \cite{bau2019seeinggangenerate} revealed latent spaces similar to generators, suggesting common representational principles. The success of optimization-based synthesis through techniques like deep image prior \cite{Ulyanov_2018_CVPR} and neural style transfer \cite{Gatys_2016_CVPR} demonstrated that careful optimization can sometimes replace explicit generative training.

More recently, the release of OpenAI's CLIP models \cite{radford2021learningtransferablevisualmodels} sparked a series of experiments in the open-source community that used CLIP similarity to a target text prompt to guide optimization in the latent space of various GAN generators. In particular, VQGAN-CLIP \cite{crowson2022vqgan} was used extensively for creative image generation and editing, and early practitioners quickly discovered the value of augmentations for improving and stabilizing the optimization process.

\subsection{Adversarial attacks on large models}
Despite early hopes to the contrary (especially due to scaling, e.g. \citet{dehghani2023scalingvisiontransformers22}), large models still suffer from adversarial examples. \citet{Fort2021CLIPadversarial, Fort2021CLIPadversarialstickers} shows that OpenAI CLIP models \citep{radford2021learningtransferablevisualmodels} can be fooled by small, easy-to-find, targeted, pixel-level modifications to the input image. Even very robust out-of-distribution detectors based on large scale pretrained models \citep{fort2021exploringlimitsoutofdistributiondetection} suffer from an equivalent brittleness under targeted attacks \citep{fort2022adversarialvulnerabilitypowerfulnear}. Transferable adversarial image attacks on proprietary models such as GPT-4, Claude and Gemini were first constructed in \citet{fort2024ensembleeverywheremultiscaleaggregation}. While there have been dedicated approaches improving adversarial robustness on small datasets (e.g. \citet{madry2019deeplearningmodelsresistant}), no solution has yet emerged at scale.

\subsection{The Role of Multi-Scale Processing}
The importance of multi-scale representations spans both classical and modern approaches \citep{Lindeberg01011994}, from Gaussian pyramids \cite{1095851} to recent architectures with explicit multi-scale processing \cite{fort2024ensembleeverywheremultiscaleaggregation}. This aligns with cognitive science findings that human visual processing operates across multiple spatial frequencies \cite{JEANTET2018123}. Our work builds directly on these insights by showing that multi-resolution optimization can bridge the gap between discriminative and generative processes.

The most directly related work is \citet{whitaker2024imstack}, which independently explored similar ideas of optimization-based image synthesis in their open source project.