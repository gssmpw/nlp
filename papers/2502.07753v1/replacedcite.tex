\section{Related Work}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/all-images-for-das_2.pdf}
    \caption{Diverse generations from Direct Ascent Synthesis across a range of concepts and styles. Results were obtained by optimizing against an ensemble of three CLIP models, with prompt augmentation to control image aesthetics: discouraging text generation (-0.3 × "Optical Character Recognition"), enhancing rendering quality (0.3 × "octane render, unreal engine, ray tracing, volumetric lighting"), and preventing image stacking (-0.3 × "multiple exposure").}
    \label{fig:all-images}
\end{figure*}
%
\subsection{The Evolution of Image Synthesis}
Image synthesis has traditionally followed two parallel tracks. The generative track progressed from VAEs ____ and GANs ____ to diffusion models ____, achieving remarkable quality through increasingly complex training. The discriminative track revealed rich internal representations through feature visualization ____ and adversarial examples ____, while models like CLIP ____ demonstrated that discriminative training can capture general visual concepts.

\subsection{Bridging Discrimination and Generation}
Several works have hinted at deeper connections between these approaches. Feature inversion methods ____ showed that discriminative representations contain generative information, though with limited quality. Analysis of GAN discriminators ____ revealed latent spaces similar to generators, suggesting common representational principles. The success of optimization-based synthesis through techniques like deep image prior ____ and neural style transfer ____ demonstrated that careful optimization can sometimes replace explicit generative training.

More recently, the release of OpenAI's CLIP models ____ sparked a series of experiments in the open-source community that used CLIP similarity to a target text prompt to guide optimization in the latent space of various GAN generators. In particular, VQGAN-CLIP ____ was used extensively for creative image generation and editing, and early practitioners quickly discovered the value of augmentations for improving and stabilizing the optimization process.

\subsection{Adversarial attacks on large models}
Despite early hopes to the contrary (especially due to scaling, e.g. ____), large models still suffer from adversarial examples. ____ shows that OpenAI CLIP models ____ can be fooled by small, easy-to-find, targeted, pixel-level modifications to the input image. Even very robust out-of-distribution detectors based on large scale pretrained models ____ suffer from an equivalent brittleness under targeted attacks ____. Transferable adversarial image attacks on proprietary models such as GPT-4, Claude and Gemini were first constructed in ____. While there have been dedicated approaches improving adversarial robustness on small datasets (e.g. ____), no solution has yet emerged at scale.

\subsection{The Role of Multi-Scale Processing}
The importance of multi-scale representations spans both classical and modern approaches ____, from Gaussian pyramids ____ to recent architectures with explicit multi-scale processing ____. This aligns with cognitive science findings that human visual processing operates across multiple spatial frequencies ____. Our work builds directly on these insights by showing that multi-resolution optimization can bridge the gap between discriminative and generative processes.

The most directly related work is ____, which independently explored similar ideas of optimization-based image synthesis in their open source project.