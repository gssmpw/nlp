\section{Related Work}
\textbf{Covariate Shift.} 
There is a vast theoretical literature on the covariate shift problem (See e.g. ____ and the review in ____). 
In the following, we primarily review the literature related to the optimality analysis of the covariate shift problem.
Pioneering work ____ studies weighted maximum likelihood estimation in the covariate shift problem under the misspecified setting. Their optimality result is only under the asymptotic setting. 
More recently, a thread of research considers the optimality of covariate shift problem in the kernel regression setup ____. Their results analyze the truncated-reweighted kernel ridge regression. However, their corresponding minimax rate result relies on the likelihood ratio bound and ____ analyze the maximum likelihood optimality under the well-specified and low-dimensional setting. In their setup, vanilla maximum likelihood attains the optimal rate.
Another celebrated work by ____ delves into the nonparametric classification problem and propose a novel characterization of transfer exponent, a local regularity measurement, and show a nearest neighbor based method attain the minimax rate. One advantage of our result established is that we can provide an instance-based optimality in a refined geometric characterization.
When confining the scope to the high-dimensional linear regression, several studies have investigated the minimax rates under general distribution shifts; however, the entanglement of different types of distribution shift leads to suboptimal or inapplicable results when specifically applied to the covariate shift problem ____. Another attempt of minimax optimality from ____ derives the essentially same estimator as ours, but their optimality guarantees only compromise to the linear class, which is relatively obvious. General information-theoretic optimality has remained a notable gap and resolving this constitutes one of the major technical advancements central to our contribution.











\noindent\textbf{Stochastic Gradient Methods in Linear Regression.}
Recent advances in stochastic gradient descent (SGD) for linear regression have yielded extensive theoretical analyses that bridge gaps between empirical practice and formal optimization frameworks. These studies validate practical techniques—such as the exponential decay step size schedule, widely adopted in implementations—by establishing their minimax-optimal risk guarantees ____, insights previously unattainable within conventional black-box optimization paradigms. Another notable example involves the accelerated gradient method which retains the efficacy even in regimes with substantial gradient noise given certain noise scheme, as demonstrated by ____ and subsequent studies ____.
In the context of high-dimensional linear regression, a line of research, starting from the concept of benign overfitting under overparametrized model ____, studies the provable generalization power of stochastic gradient based optimization ____. 
When extending this analysis to scenarios involving covariate shift, recent work by ____ examines both the capabilities and limitations of stochastic gradient methods under such distributional mismatches.
Our results provide the upper bound for SGD with a geometrically decaying stepsize schedule and Nesterov momentum acceleration, which requires a more complex analysis of the iterative process. 
Another related research line concerns the nonparametric regression problem. ____ study the stochastic gradient method in the setup of reproducing kernel hilbert space, and ____ consider the functional linear regression.
A parallel line centers on SGD on nonparametric regression, exploring the optimality conditions of SGD in the context of reproducing kernel ____.