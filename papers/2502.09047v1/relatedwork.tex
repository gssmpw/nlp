\section{Related Work}
\textbf{Covariate Shift.} 
There is a vast theoretical literature on the covariate shift problem (See e.g. \citet{ben2010theory, germain2013pac, cortes2010learning, cortes2019adaptation} and the review in \citet{sugiyama2012machine,kouw2019review}). 
In the following, we primarily review the literature related to the optimality analysis of the covariate shift problem.
Pioneering work \citep{shimodaira2000improving} studies weighted maximum likelihood estimation in the covariate shift problem under the misspecified setting. Their optimality result is only under the asymptotic setting. 
More recently, a thread of research considers the optimality of covariate shift problem in the kernel regression setup \citep{ma2023optimally, pathak2022new}. Their results analyze the truncated-reweighted kernel ridge regression. However, their corresponding minimax rate result relies on the likelihood ratio bound and \citet{ge2023maximum} analyze the maximum likelihood optimality under the well-specified and low-dimensional setting. In their setup, vanilla maximum likelihood attains the optimal rate.
Another celebrated work by \citet{kpotufe2021marginal} delves into the nonparametric classification problem and propose a novel characterization of transfer exponent, a local regularity measurement, and show a nearest neighbor based method attain the minimax rate. One advantage of our result established is that we can provide an instance-based optimality in a refined geometric characterization.
When confining the scope to the high-dimensional linear regression, several studies have investigated the minimax rates under general distribution shifts; however, the entanglement of different types of distribution shift leads to suboptimal or inapplicable results when specifically applied to the covariate shift problem \citep{zhang2022class, mousavi2020minimax}. Another attempt of minimax optimality from \citet{lei2021near} derives the essentially same estimator as ours, but their optimality guarantees only compromise to the linear class, which is relatively obvious. General information-theoretic optimality has remained a notable gap and resolving this constitutes one of the major technical advancements central to our contribution.











\noindent\textbf{Stochastic Gradient Methods in Linear Regression.}
Recent advances in stochastic gradient descent (SGD) for linear regression have yielded extensive theoretical analyses that bridge gaps between empirical practice and formal optimization frameworks. These studies validate practical techniques—such as the exponential decay step size schedule, widely adopted in implementations—by establishing their minimax-optimal risk guarantees \citep{ge2019step, pan2021eigencurve}, insights previously unattainable within conventional black-box optimization paradigms. Another notable example involves the accelerated gradient method which retains the efficacy even in regimes with substantial gradient noise given certain noise scheme, as demonstrated by \citet{jain2018accelerating} and subsequent studies \citep{varre2022accelerated}.
In the context of high-dimensional linear regression, a line of research, starting from the concept of benign overfitting under overparametrized model \citet{bartlett2020benign}, studies the provable generalization power of stochastic gradient based optimization \citep{zou2021benign, wu2022last, lirisk,Zhang2024OptimalityAcceleratedSGD}. 
When extending this analysis to scenarios involving covariate shift, recent work by \citet{wu2022power} examines both the capabilities and limitations of stochastic gradient methods under such distributional mismatches.
Our results provide the upper bound for SGD with a geometrically decaying stepsize schedule and Nesterov momentum acceleration, which requires a more complex analysis of the iterative process. 
Another related research line concerns the nonparametric regression problem. \citet{Dieuleveut2016NonparametricStochasticApproximation} study the stochastic gradient method in the setup of reproducing kernel hilbert space, and \citet{cai2006prediction} consider the functional linear regression.
A parallel line centers on SGD on nonparametric regression, exploring the optimality conditions of SGD in the context of reproducing kernel \citep{Dieuleveut2016NonparametricStochasticApproximation}.