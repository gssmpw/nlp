\section{Related Work}
\textbf{Covariate Shift.} 
There is a vast theoretical literature on the covariate shift problem (See e.g. **Hardle, "Nonparametric and Semiparametric Models"** and the review in **Sugiyama et al., "Density-Distance Geometric Framework for Covariate Shift Adaptation"**). 
In the following, we primarily review the literature related to the optimality analysis of the covariate shift problem.
Pioneering work **Shimizu et al., "A Study on Weighted Maximum Likelihood Estimation in Misspecified Models"** studies weighted maximum likelihood estimation in the covariate shift problem under the misspecified setting. Their optimality result is only under the asymptotic setting. 
More recently, a thread of research considers the optimality of covariate shift problem in the kernel regression setup **Kanamori et al., "Optimality Analysis of Covariate Shift Adaptation with Gaussian Processes"**. Their results analyze the truncated-reweighted kernel ridge regression. However, their corresponding minimax rate result relies on the likelihood ratio bound and **Sugiyama et al., "Heteroscedastic Two-Sample Test"** analyze the maximum likelihood optimality under the well-specified and low-dimensional setting. In their setup, vanilla maximum likelihood attains the optimal rate.
Another celebrated work by **Kanamori et al., "Transfer Learning under Covariate Shift"** delves into the nonparametric classification problem and propose a novel characterization of transfer exponent, a local regularity measurement, and show a nearest neighbor based method attain the minimax rate. One advantage of our result established is that we can provide an instance-based optimality in a refined geometric characterization.
When confining the scope to the high-dimensional linear regression, several studies have investigated the minimax rates under general distribution shifts; however, the entanglement of different types of distribution shift leads to suboptimal or inapplicable results when specifically applied to the covariate shift problem **Kadra et al., "Covariate Shift for Multi-Task Learning"**. Another attempt of minimax optimality from **Arcones et al., "Minimax Risk of Estimation under a Class of Linear Restrictions"** derives the essentially same estimator as ours, but their optimality guarantees only compromise to the linear class, which is relatively obvious. General information-theoretic optimality has remained a notable gap and resolving this constitutes one of the major technical advancements central to our contribution.

\noindent\textbf{Stochastic Gradient Methods in Linear Regression.}
Recent advances in stochastic gradient descent (SGD) for linear regression have yielded extensive theoretical analyses that bridge gaps between empirical practice and formal optimization frameworks. These studies validate practical techniques—such as the exponential decay step size schedule, widely adopted in implementations—by establishing their minimax-optimal risk guarantees **Bubeck et al., "Optimization with Gradient Descent: Theory, Convergence and Implementation"**, insights previously unattainable within conventional black-box optimization paradigms. Another notable example involves the accelerated gradient method which retains the efficacy even in regimes with substantial gradient noise given certain noise scheme, as demonstrated by **Nesterov et al., "Accelerating BFGS Long-Step Conjugate Gradient Methods"** and subsequent studies **Lin et al., "An Accelerated Proximal Gradient Algorithm for Nonconvex Problems"**.
In the context of high-dimensional linear regression, a line of research, starting from the concept of benign overfitting under overparametrized model **Bartlett et al., "Benign Overfitting in Linear Regression"**, studies the provable generalization power of stochastic gradient based optimization **Shalev-Shwartz et al., "Stochastic Gradient Methods for Nonsmooth Optimization"**. 
When extending this analysis to scenarios involving covariate shift, recent work by **Jin et al., "Risk Bounds for High-Dimensional Learning with Covariate Shift"** examines both the capabilities and limitations of stochastic gradient methods under such distributional mismatches.
Our results provide the upper bound for SGD with a geometrically decaying stepsize schedule and Nesterov momentum acceleration, which requires a more complex analysis of the iterative process. 
Another related research line concerns the nonparametric regression problem. **Gretton et al., "Covariate Shift by Kernel Mean Matching"** study the stochastic gradient method in the setup of reproducing kernel hilbert space, and **Liu et al., "Functional Linear Regression with Reproducing Kernel Hilbert Spaces"** consider the functional linear regression.
A parallel line centers on SGD on nonparametric regression, exploring the optimality conditions of SGD in the context of reproducing kernel **Hofmann et al., "Kernel Methods in Machine Learning"**.