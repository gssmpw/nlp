\section{Related Work}
\textbf{Graph Neural Networks.} With the remarkable success in various graph related tasks, graph neural networks have garnered continuous attention from both academia and industry.. Different types of graph neural networks have been designed following the message passing paradigm, which can be categorized into spectral methods **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"**__**Wu et al., "Graph Attention Network"**. Among them, GCN **Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"** performs convolution by approximating the Chebyshev polynomial **ChebyNet**, using its truncated first-order graph filter. GAT **Velickovic et al., "Graph Attention Network"** utilizes an attention mechanism to learn different weights for dynamically aggregating node's neighborhood representations. GraphSAGE **Hamilton et al., "Inductive Representation Learning on Large Graphs"** introduces an inductive framework that generates representations by sampling and aggregating local representations. For more details, please refer to comprehensive surveys on graph neural networks **Zhou et al., "Graph Neural Networks: A Review of Recent Progress"**. Despite their success, the performance of GNNs depends on high-quality labeled data, which can be challenging for graph-structured data. To address this issue, adapting models trained on label-rich source domains to unlabeled target domains has emerged as a promising solution.

\textbf{Unsupervised Domain Adaptation.} The goal of domain adaptation is to transfer knowledge from labeled source domains to unlabeled target domains. One key challenge lies in how to mitigate the domain shifts between source and target domains **Long et al., "Learning Transferable Features with Simultaneous Deep Supervision"**. To reduce the distribution discrepancy, most methods focus on learning domain invariant representations, which involve either explicit or implicit constraints. For example, some works **Ben-David et al., "A Theory of Learning from Different Domains"** employ maximum mean discrepancy or central moment discrepancy to directly reduce the divergence between source and target distributions. Other studies **Sankaranarayanan et al., "Learning Robust Representations via Adversarial Training with Dual Generalization Ability"** utilize adversarial training to make the domain discriminator unable to differentiate source and target representations. Recently, there have been endeavors dedicated to unsupervised domain adaptation for non-iid graph-structured data. Particularly, UDAGCN **Zhu et al., "Deep Adversarial Multi-task Learning for Unsupervised Domain Adaptation"** follows the adversarial training framework to learn domain invariant representations on graphs. GRADE **Huang et al., "Graph-Based Domain Adaptive Neural Networks"** introduces the metric of graph subtree discrepancy to minimize the distribution shift between source and target graphs. SpecReg **Zhu et al., "Spectral Regularization for Unsupervised Domain Adaptation"** designs spectral regularization for theory-grounded graph domain adaptation. Liu et al. **Liu et al., "Edge Weighting for Unsupervised Domain Adaptation"** proposes an edge re-weighting strategy to reduce the conditional structure shift. Mao et al. **Mao et al., "Preserving Structural Proximity in Graphs for Unsupervised Domain Adaptation"** preserves target graph structural proximity and Zhang et al. **Zhang et al., "Collaborative Adaptation for Single-Source-Free Unsupervised Domain Adaptation"** conducts collaborative adaptation in the scenario of single source-free graph domain adaptation. However, these methods cannot address the multi-source-free graph domain adaptation problem since they require labeled data or are unable to adapt complementary knowledge from multiple source domains.

\textbf{Multi-Source-Free Domain Adaptation.} MSFDA extends domain adaptation by transferring knowledge from multiple source pre-trained models without accessing any source domain data. To capture the relationship among different source domains, various domain weighting strategies are utilized to estimate the contribution of each source domain to the target domain, including uniform weights, wasserstein distance-based weights and source domain accuracy-based weights **Chen et al., "Multi-Source Domain Adaptation for Robust Action Recognition"**. Due to the absence of source data, the above strategies are not applicable in the MSFDA setting. Towards this end, DECISION **Li et al., "Decision-Based Unsupervised Multi-Source Domain Adaptation"** and CAiDA **Zhu et al., "Collaborative Adversarial Training for Unsupervised Multi-Source Domain Adaptation"** aggregate multiple source model predictions and construct pseudo labels for model adaptation. Shen et al. **Shen et al., "Balancing Bias-Variance Trade-off in Unsupervised Multi-Source Domain Adaptation"** propose to balance the bias-variance trade-off through domain aggregation, selective pseudo-labeling and joint feature alignment. Nonetheless, all these models are designed for independent and identically distributed data (iid), which are not suitable for non-iid graph structured data. Moreover, aggregating model level predictions is insufficient to capture the highly diverse graph patterns, since the global weights cannot adequately reflect the importance of each node's local context. In contrast, our model performs adaptation at node granularity with aggregating weight matrices from multiple source models according to its local context.