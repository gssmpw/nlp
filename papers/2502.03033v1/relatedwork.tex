\section{Related Work}
\textbf{Graph Neural Networks.} With the remarkable success in various graph related tasks, graph neural networks have garnered continuous attention from both academia and industry.. Different types of graph neural networks have been designed following the message passing paradigm, which can be categorized into spectral methods \cite{defferrard2016convolutional,kipf2016semi,bianchi2021graph} and spatial methods \cite{velivckovic2017graph,wu2019simplifying,hamilton2017inductive}. Among them, GCN \cite{kipf2016semi} performs convolution by approximating the Chebyshev polynomial \cite{defferrard2016convolutional} using its truncated first-order graph filter. GAT \cite{velivckovic2017graph} utilizes an attention mechanism to learn different weights for dynamically aggregating node's neighborhood representations. GraphSAGE \cite{hamilton2017inductive} introduces an inductive framework that generates representations by sampling and aggregating local representations. For more details, please refer to comprehensive surveys on graph neural networks \cite{zhou2022graph,wu2020comprehensive}. Despite their success, the performance of GNNs depends on high-quality labeled data, which can be challenging for graph-structured data. To address this issue, adapting models trained on label-rich source domains to unlabeled target domains has emerged as a promising solution.

\textbf{Unsupervised Domain Adaptation.} The goal of domain adaptation is to transfer knowledge from labeled source domains to unlabeled target domains. One key challenge lies in how to mitigate the domain shifts between source and target domains \cite{liu2024rethinking,liu2024revisiting}. To reduce the distribution discrepancy, most methods focus on learning domain invariant representations, which involve either explicit or implicit constraints. For example, some works \cite{long2015learning,zellinger2017central} employ maximum mean discrepancy or central moment discrepancy to directly reduce the divergence between source and target distributions. Other studies \cite{long2018conditional,hoffman2018cycada} utilize adversarial training to make the domain discriminator unable to differentiate source and target representations. Recently, there have been endeavors dedicated to unsupervised domain adaptation for non-iid graph-structured data. Particularly, UDAGCN \cite{wu2020unsupervised} follows the adversarial training framework to learn domain invariant representations on graphs. GRADE \cite{wu2023non} introduces the metric of graph subtree discrepancy to minimize the distribution shift between source and target graphs. SpecReg \cite{you2022graph} designs spectral regularization for theory-grounded graph domain adaptation. Liu et al. \cite{liu2023structural} proposes an edge re-weighting strategy to reduce the conditional structure shift. Mao et al. \cite{mao2021source} preserves target graph structural proximity and Zhang et al. \cite{zhang2024collaborate} conducts collaborative adaptation in the scenario of single source-free graph domain adaptation. However, these methods cannot address the multi-source-free graph domain adaptation problem since they require labeled data or are unable to adapt complementary knowledge from multiple source domains.

\textbf{Multi-Source-Free Domain Adaptation.} MSFDA extends domain adaptation by transferring knowledge from multiple source pre-trained models without accessing any source domain data. To capture the relationship among different source domains, various domain weighting strategies are utilized to estimate the contribution of each source domain to the target domain, including uniform weights, wasserstein distance-based weights and source domain accuracy-based weights \cite{peng2019moment,zhao2020multi,wang2019tmda,zhu2019aligning}. Due to the absence of source data, the above strategies are not applicable in the MSFDA setting. Towards this end, DECISION \cite{ahmed2021unsupervised} and CAiDA \cite{dong2021confident} aggregate multiple source model predictions and construct pseudo labels for model adaptation. Shen et al. \cite{shen2023balancing} propose to balance the bias-variance trade-off through domain aggregation, selective pseudo-labeling and joint feature alignment. Nonetheless, all these models are designed for independent and identically distributed data (iid), which are not suitable for non-iid graph structured data. Moreover, aggregating model level predictions is insufficient to capture the highly diverse graph patterns, since the global weights cannot adequately reflect the importance of each node's local context. In contrast, our model performs adaptation at node granularity with aggregating weight matrices from multiple source models according to its local context.