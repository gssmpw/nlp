

\section{Contextual Gesture}
\begin{figure}
    % \vspace{-1em}
    \includegraphics[width=\linewidth]{figs/framework-final.pdf}
    \vspace{-1em}
    \caption{An overview of our framework. We extract keypoints and leverage chronological alignment with distillation to achieve contextualized gesture motion representation. We leverage a masking-based generator conditioned on audio to generate gesture keypoint sequences and apply image warping with refinement based on edge heatmaps for final gesture video generation. }
    \label{fig:overview}
    \vspace{-5mm}
\end{figure}

Shown in Fig.~\ref{fig:overview},  our framework targets at generating co-speech photo-realistic human videos with contextualized gesture patterns. To achieve this goal, we first learn contextual-aware gesture motion representation through knowledge distillation based on chronological gesture-speech alignment (Sec.~\ref{sec:gesture_representation}). We leverage a Masking-based Gesture Generator for gesture motion generation. (Sec.~\ref{sec:gesture_generation}) To improve the noisy hand and shoulder movement during the transfer of latent motion to pixel space, we propose a structure-aware image refinement through edge heatmaps for guidance. (Sec.~\ref{sec:image-refine}). 


\vspace{-0.2cm}
\subsection{Contextualized Gesture Representation}
\label{sec:gesture_representation}

Generating natural and expressive gestures requires capturing fine-grained contextual details that conventional approaches often overlook. Consider a speaker emphasizing the word "really" in the sentence "I really mean it" - while existing methods might generate a generic emphatic gesture, they typically fail to capture the subtle, context-specific body movements that make human gestures truly expressive. This limitation stems from relying solely on motion quantization, which often loses the nuanced relationship between speech and corresponding gestures.

To address this challenge, we propose a novel approach that integrates both audio and semantic information into the motion quantizer's codebook. This integration requires solving two fundamental problems. First, we need to understand how gestures align with speech not just at a high level, but in terms of precise temporal correspondence - when specific words or phrases trigger particular movements, and how the rhythm of speech influences gesture timing. To capture these temporal dynamics, we develop a chronological gesture-speech alignment framework using specialized contrastive learning. Second, we leverage knowledge distillation to incorporate this learned temporal alignment information into the gesture quantizer, enabling our system to generate gestures that are synchronized with speech both semantically and rhythmically.

\begin{figure*}[t!]
	\centering
         \vspace{-0.15in}
	\includegraphics[width=1.0\linewidth]{figs/pipeline-new.pdf}
	\vspace{-7mm}
	\caption{\textbf{Left}: Chronological Contrastive Learning for gesture-speech alignment. \textbf{Mid}: We distill the contextual-aware feature into latent codebook. \textbf{Right}: Gesture motion generation based on Mask and Residual Gesture Generator.}
	\label{fig:motion}
	\vspace{-5mm}
\end{figure*}

\vspace{-0.2cm}
\paragraph{Feature Representation.} We utilize 2D poses extracted from images to formulate gestures by facial and body movements. We represent a gesture motion sequence as \( G = [F; B] = [f_t; b_t]_{t=1}^{T} \), where \( T \) denotes the length of the motion, \( f \) represents the 2D facial landmarks, and \( b \) denotes the 2D body landmarks. For speech representation, we extract audio embeddings from WavLM~\cite{chen2022wavlm} and Mel spectrogram features~\cite{rabiner2010theory} and beat information using librosa~\cite{mcfee2015librosa}. For text-semantics, we extract embedding from RoBERTa~\cite{roberta}. These features are concatenated to form the speech representation. 

\vspace{-0.2cm}
\paragraph{Chronological Speech-Gesture Alignment.}
\label{sec:contrastive}
Traditional approaches to modality alignment~\cite{ao2022rhythmic, liu2022learning, Deichler_2023} rely on global representations through class tokens or max pooling, which overlook the fine-grained temporal dynamics between speech and gestures. We address this limitation by introducing chronological modality alignment.

\vspace{-0.2cm}
\paragraph{Vanilla Contrastive Alignment.} 
To align gesture motion patterns with the content of speech and beats, we first project both speech and gesture modalities into a shared embedding space to enhance the speech content awareness of gesture features. As illustrated in Fig.~\ref{fig:motion} Middle, we separately train two gesture content encoders, \(\mathcal{E}_f\) for face motion and \(\mathcal{E}_b\) for body motion, alongside two speech encoders, \(\mathcal{E}_{S_f}\) and \(\mathcal{E}_{S_b}\), to map face and body movements and speech signals into this joint embedding space. For simplicity, we represent the general gesture motion sequence as \( G \). We then apply mean pooling to aggregate content-relevant information to optimize the following loss~\cite{infonce}:
{\small
\begin{equation}
\begin{split} 
    \mathcal{L}_{\text{NCE}} &= - \frac{1}{2N} \sum_{i} \left( \log \frac{\exp{S_{ii}/\tau}}{\sum_{j} \exp{S_{ij}/\tau}} + \log \frac{\exp{S_{ii}/\tau}}{\sum_{j} \exp{S_{ji}/\tau}} \right), ~
\end{split}
\label{eq:infonce}
\end{equation}
}
where \( S \) computes the cosine similarities for all pairs in the batch, defined as \( S_{ij} = \text{cos}(z^s_i, z^g_j) \) and \(\tau\) is the temperature.

\vspace{-0.2cm}
\paragraph{Chronological Negative Examples.} While vanilla Contrastive Learning builds global semantical alignment, we further propose to address the temporal correspondence between speech and gesture. As shown in \cref{fig:motion} Left, consider a speaker saying, "After watching that video, you realize how much, more than any other president...". In this case, the gesture sequence involves "knocking at the table" when saying "more than any other," serving as a visual emphasis for "how much" to highlight the point. To encourage the model understand both semantic and rhythmic alignment between two modalities, we shuffle the words and their corresponding phonemes. By shuffling the sequence to "you realize how much, after watching that video," the semantic intention of the speech is preserved, but the rhythmic correspondence between speech and gesture is disrupted. We use Whisper-X~\cite{bain2022whisperx} to detect temporal segments in the raw sequences. We cut the audio and shuffle these segments, creating these augmented samples as additional chronological negative examples within a batch during contrastive learning.  

% In addition, we randomly mask 30\% of segments from both speech and gesture sequences within the same temporal regions during training and apply a linear classifier on the gesture embedding to predict audio beats, enhancing the temporal alignment.




% \vspace{-0.2cm}
% \paragraph{Temporal-robust Training}
% Beside negative examples for modality alignment, we randomly mask 30\% of segments from both speech and gesture sequences within the same temporal regions during training. Furthermore, we apply a linear classifier on the gesture embedding to predict speech beats, enhancing the temporal alignment.


\vspace{-0.2cm}
\paragraph{Gesture Quantization with Distillation.}
To construct context-aware motion representations, we encode alignment information into the gesture motion codebook. This allows the semantics and contextual triggers from speech to be directly fused into the motion embedding, and enables the generator to easily identify the corresponding motion representation in response to speech triggers. To achieve this goal, we leverage gesture content encoder as the teacher and distill knowledge to codebook latent representation, shown in Fig.~\ref{fig:motion} Middle. We maximize the cosine similarity over time between the RVQ quantization output and the representation from the gesture content encoder:
\begin{equation}
\mathcal{L}_{\text{distill}} = \sum_{t=1}^{T} \cos\left( p(Q_R)^t, Es(G)^t \right)
\end{equation}
% where \( p \) denotes a linear projection layer, \( Q_R \) is the final quantization output from the RVQ-VAE, \( Es(G) \) represents the output from the gesture content encoder, and \( T \) is the total time frames. We leverage motion reconstruction and commitment loss, together with distillation loss for RVQ tokenization training.
%
where \( p \) denotes a linear projection layer, \( Q_R \) is the final quantized output from the RVQ-VAE, \( Es(G) \) represents the output from the gesture content encoder, and \( T \) is the total time frames. The overall training objective is defined as:
\begin{equation}
\mathcal{L}_{\text{rvq}} = \left\| x - \hat{x} \right\|^2 + \alpha \sum_{r=1}^{R} \left\| e_r - \text{sg} \left( z_r - e_r \right) \right\|^2 + \beta \mathcal{L}_{\text{distill}}
\end{equation}
where \( \mathcal{L}_{\text{rvq}} \) combines a motion reconstruction loss, a commitment loss~\cite{oord2018neuraldiscreterepresentationlearning} for each layer of quantizer with a distillation loss, with \( \alpha \) and \( \beta \) weighting the contributions.




\vspace{-0.1cm}

\subsection{Speech-conditioned Gesture Motion Generation}
\label{sec:gesture_generation}

For Motion Generator, we adopt a similar masking-based generation procedure as in MoMask~\cite{moMask} due to its flexibility of editing and fast inference speed. We only include the generator data flows in the main paper but defer the training and inference strategy in the Appendix.


\vspace{-0.2cm}
\paragraph{Mask Gesture Generator.}
\label{subsec:maskgesture}

As shown in Fig.~\ref{fig:motion} Right, during training, we derive motion tokens by processing raw gesture sequences through both body and face tokenizers. The motion token corresponding to the source image acts as the conditioning for all subsequent frames. 
For speech control, we initialize the audio content encoder from alignment pre-training as described in Sec.~\ref{sec:contrastive}. This pre-alignment of gesture tokens with audio encoder features enhances the coherence of gesture generation. We employ cross-attention to integrate audio information and apply Adaptive Instance Normalization (AdaIN)~\cite{adain}, enabling gesture styles based on the speaker's identity.

\vspace{-0.2cm}
\paragraph{Residual Gesture Generator.}
The Residual Gesture Generator shares a similar architecture with the Masked Gesture Generator, but it includes \( R \) separate embedding layers corresponding to each RVQ residual layer. This module iteratively predicts the residual tokens from the base layers, ultimately producing the final quantized output. Please see MoMask~\cite{moMask} for further details.



\vspace{-0.2cm}
\subsection{Structure-Aware Image Generation}
\label{sec:image-refine}

Converting generated gesture motions into realistic videos presents significant challenges, particularly in handling camera movement commonly found in talkshow and speech videos. While recent 2D skeleton-based animation methods~\cite{hu2023animateanyone} offer a potential solution, our empirical analysis (detailed in the Appendix) reveals that these approaches struggle with background stability in the presence of camera motion.

To address this limitation, we draw inspiration from optical-flow based image warping methods~\cite{zhao2022thin,MRAA, FOMM}, which have shown promise in handling deformable motion. We adapt these approaches by replacing their unsupervised keypoints with our generated gesture keypoints from Sec.~\ref{sec:gesture_generation} for more precise foreground optical flow estimation. However, this estimation still presents uncertainties, resulting in blurry artifacts around hands and shoulders, particularly when speakers make rapid or large movements.

To address the uncertainties by optical-flow-based deformation during this process, we propose a Structure-Aware Generator. Auto-Link~\cite{autolink} demonstrates that the learning of keypoint connections for image reconstruction aids the model in understanding image semantics. Based on this, we leverage keypoint connections as semantic guidance for image generation.



\vspace{-0.2cm}
\paragraph{Edge Heatmaps.}
Using the gesture motion keypoints, we establish linkages between them to provide structural information. To optimize computational efficiency, we limit the number of keypoint connections to those defined by body joint relationships~\cite{wan2017deepskeletonskeletonmap3d}, rather than all potential connections in~\cite{autolink}.

For two keypoints \(\vk_i\) and \(\vk_j\) within connection groups, we create a differentiable edge map \(\mS_{ij}\), modeled as a Gaussian function extending along the line connecting the keypoints. The edge map \(\mS_{ij}\) for keypoints \((\vk_i, \vk_j)\) is defined as:
\begin{equation}
    \mS_{ij}(\vp) = \exp\left(v_{ij}(\vp)d^2_{ij}(\vp) / \sigma^2\right), 
    \label{eq:edge}
\end{equation}
where \(\sigma\) is a learnable parameter controlling the edge thickness, and \(d_{ij}(\vp)\) is the \(L_2\) distance between the pixel \(\vp\) and the edge defined by keypoints \(\vk_i\) and \(\vk_j\):
\begin{equation} 
\vd_{ij}(\vp) = \left\{ \begin{aligned} 
& \|\vp-\vk_i\|_2 & \text{ if } t \leq 0, \\ 
& \|\vp-((1-t)\vk_i + t\vk_j)\|_2 & \text{ if } 0 < t < 1, \\ 
& \|\vp-\vk_j\|_2 & \text{ if } t \geq 1, 
\end{aligned} \right.
\end{equation}
\begin{equation} 
\text{where} \quad t = \frac{(\vp-\vk_i)\cdot (\vk_j-\vk_i)}{\|\vk_i-\vk_j\|^2_2}.
\end{equation}
Here, \(t\) denotes the normalized distance between \(\vk_i\) and the projection of \(\vp\) onto the edge.

To derive the edge map \(\mS \in \mathbb{R}^{H \times W}\), we take the maximum value at each pixel across all heatmaps:
\begin{equation}
   \mS(\vp)  = \max_{ij} \mS_{ij}(\vp).
   \label{eq:max_heatmap}
\end{equation}

% \begin{figure}[t!]
% 	\centering
%          % \vspace{-0.15in}
% 	\includegraphics[width=1.0\linewidth]{figs/warping.pdf}
% 	% \vspace{-7mm}
% 	\caption{Structural-aware image warping with edge heatmaps. In addition to the optical flows and occlusions, we introduce heatmaps into the image warping procedure to provide additional spatial control over the avatar generation.}
% 	\label{fig:image-gen}
% 	\vspace{-7mm}
% \end{figure}

\vspace{-0.5cm}
\paragraph{Structural-guided Image Refinement.}  
Traditional optical-flow-based warping methods are effective for handling global deformations but often fail under large motion patterns, such as those involving hands or shoulders, resulting in significant distortions. To address this, we introduce a structure-guided refinement process that incorporates semantic guidance via structural heatmaps.

Instead of directly rendering the warped feature maps into RGB images, we first predict a low-resolution feature map of size \( 256 \times 256 \times 32 \). Multi-resolution edge heatmaps are generated and used as structural cues to refine the feature maps. After performing deformation and occlusion prediction at each scale using TPS~\cite{zhao2022thin}, the edge heatmaps are fed into the generator. Specifically, we integrate these heatmaps into the fusion block using SPADE~\cite{park2019SPADE}, enabling the precise prediction of residuals. These residuals are element-wise added to the warped feature maps, ensuring precise structural alignment.

To generate high-resolution RGB images, we employ a U-Net architecture that takes both the warped features and edge heatmaps as inputs. This design preserves fine-grained structural details while compensating for motion-induced distortions. Additional architectural details and analysis are provided in the Appendix.


% To enhance pixel-level sharpness, we propose a two-stage rendering process. Initially, we predict a low-resolution feature map of size \( 256 \times 256 \times 32 \) through the modified warping process. Rather than directly rendering the warped feature maps into RGB images, we incorporate a U-Net architecture, designed to further refine the warped features and generate the final high-resolution RGB images. Additional details and architectural analysis are provided in the Appendix.

% To improve pixel-level sharpness, directly rendering RGB images, based on the modified warping process, we first predict a low-resolution feature map of size \( 256 \times 256 \times 32 \) from the modified warping process. Instead of directly rendering the warped feature maps into RGB images, we leverage an U-Net based on StyleGAN structure to further refine the warpinging features and convert it into the final high resolution RGB images. Additional details and analysis on the model architecture are shown in the Appendix. 
% We draws inspiration from GaussianStyle~\cite{gaussianstyle}, which projects Gaussian-splatted feature maps into StyleGAN for high-quality pixel-level generation. In our method, we treat the warping and generation process in a similar way. As shown in \cref{fig:image-gen} right, we use an encoder to process the structure-aware warped features and project them into the pretrained StyleGAN for final image generation. Additional details and analysis on the projection process can be found in the Appendix.


\paragraph{Training Objective.}
We employ an adversarial loss, along with perceptual similarity loss (LPIPS) ~\cite{lpips} and pixel-level \(L1\) loss for image refinement. The reconstruction objective is defined as:  
\begin{equation}
I_{\text{rec}} = \gamma \mathcal{L}_{GAN} + \mathcal{L}_{L1} + \mathcal{L}_{\text{LPIPS}},
\end{equation}
where \(I_{gt}\) denotes the ground-truth image, and \(I_{gan}\) represents the generated image. We leverage a small weighted term of $\gamma$ to prevent GAN training collapse. 

