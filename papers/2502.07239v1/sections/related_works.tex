
\section{Related Work}
\label{sec:background}


\vspace{-0.2 cm}
\paragraph{Co-speech Gesture generation}
Most recent works on co-speech gesture generation employ skeleton- or joint-level pose representations. 
\cite{ginosar2019gestures} use an adversarial framework to predict hand and arm poses from audio, and leverage conditional generation~\cite{EverybodyDanceNow} based on pix2pixHD~\cite{wang2018pix2pixHD} for videos. Some recent works~\cite{liu2022learning, Deichler_2023, xu2023chaingenerationmultimodalgesture, ao2022rhythmic} learns the hierarchical semantics or leverage contrastive learning to obtain joint audio-gesture embedding from linguistic theory to assist the gesture pose generation. TalkShow~\cite{yi2022generating} estimates SMPL~\cite{SMPL-X:2019} poses, and models the body and hand motions for talk-shows. CaMN~\cite{liu2022beat}, EMAGE~\cite{liu2023emage} and Diffsheg~\cite{diffsheg} use large conversational and speech datasets for joint face and body modeling with diverse style control. ANGIE \cite{angie} and S2G-Diffusion~\cite{s2gdiffusion} use image-warping features based on MRAA \cite{MRAA} or TPS~\cite{zhao2022thin} to model body motion and achieve speech driven animation by learning correspondence between audio and image-warping features. However, none of these works produce structure- and speech-aware motion patterns suitable for achieving natural and realistic gesture rendering.

\vspace{-0.2cm}
\paragraph{Conditional Video Generation}
Conditional Video Generation has undergo significant progress for various modalities, like text \cite{stablevideodiffusion}, pose~\cite{dreampose,wang2023disco}, and audio \cite{ruan2022mmdiffusion}. AnimateDiff~\cite{guo2023animatediff} presents an efficient low-rank adaptation ~\cite{hu2022lora} (LoRA) to adapt image diffusion model for video motion generation. AnimateAnyone~\cite{hu2023animateanyone} construct referencenet for fine-grained control based on skeleton. Make-Your-Anchor~\cite{huang2024makeyouranchor} and Champ~\cite{zhu2024champ} improve avatar video generation through face and body based on SMPL-X conditions. EMO~\cite{tian2024emo} and EchoMimic~\cite{meng2024echomimic} leverages audio as control signal for talking head and upper body generation. However, these methods are slow in inference speed and ignore the gesture patterns or rhythmic or semantic signals from audio.

% \vspace{-0.2cm}
% \paragraph{Masked Representation Learning for Generation}
% Masked Representation Learning has been demonstrated an effective way for various modalities.~\cite{devlin2018bert, he2022masked}  MAGE~\cite{li2023mage} and Muse~\cite{chang2023muse} achieves high-quality image generation with region masking for image editing and achieve fine-grained control. Recent works~\cite{pinyoanuntapong2024mmm, wang2023t2mhifigptgeneratinghighquality, Mao_2024} bring this strategy to the motion and gesture domain and improves the motion generation speed, quality, and editing capability. Inspired by these work, we propose the masked gesture generation conditioned the audio to learn the gesture-speech correspondence. 

