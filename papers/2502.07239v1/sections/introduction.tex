\section{Introduction}
\label{sec:introduction}
  \vspace{-0.1cm}

% brief talk about why this task is important
In human communication, speech is often accompanied by gestures that enhance understanding and convey emotions~\cite{de2012interplay}. As these non-verbal cues play a vital role in effective interaction~\cite{burgoon1990nonverbal}, gesture generation plays a key component of natural human-computer interactions: equipping virtual avatars with realistic gesture capabilities become essential in creating immersive interactive experiences.

The relationships between the semantic and emotion content of speech context, the corresponding gestures, and the visual appearance of the speaker's performance are complex. As such, many recent works~\cite{yi2022generating, liu2023emage, liu2022learning, liu2022disco} address a reduced form of this problem by generating a simplified representation of the 3D motion, consisting of joints and body parts, that plausibly accompanies a given speech sample, which can then be rendered using standard rendering pipelines. Such representations capture basic motion patterns, yet they neglect the importance of the speaker's visual appearance, resulting in a lack of realism that hinders effective communication.

Other works, \eg ANGIE~\cite{angie} and S2G-diffusion~\cite{s2gdiffusion}, employ image-warping techniques, constrained by keypoints obtained from optical-flow-based deformations, for co-speech video generation.
However, such approaches encounter several critical issues.
First, these keypoints only define large-scale transformations, and thus miss subtle movements of specific body parts (\eg hands, fingers).
Second, this broad and unconstrained motions representation neglects the triggers associated with gestures, specifically the contextual information from speech. This approach fails to provide the generator with the intrinsic connections between speech context and gesture motion, making it challenging for the generator to produce motion patterns that effectively convey the speaker's metaphorical expressions or intentions.
Finally, the generated motion patterns are often unstructured and overly reactive to large motion, resulting in noisy and imprecise renderings, especially in the hands and shoulders.
Collectively, these challenges significantly limit the overall quality and realism of the generated video content.


To address these challenges, we introduce \textit{Contextual Gesture}, a framework designed to generate speech-aligned gesture motions and high-fidelity speech video outputs. Our approach begins with gesture motion representations using keypoints from human pose estimators. To uncover the intrinsic temporal connections between gestures and speech, we employ chronological contrastive learning to align these two modalities. This joint representation captures the triggers of gesture patterns influenced by speech. We incorporate speech-contextual features into the tokenization process of gesture motions through knowledge distillation, aiming to infuse the gesture representations with implicit intentions conveyed in the audio. This integration creates a clear linkage between the gestures and the corresponding speech, enabling the generation of gestures that reflect the speaker's intended meaning. 
For motion generation, we leverage a masking-based gesture generator that refines the alignment of gesture motions with the speech signal through bidirectional mask pretraining. Finally, for uplifting the latent motion generation into 2D animations, we propose a structure-aware image refinement module that generates heatmaps of edge connections from keypoints, providing image-level supervision to improve the quality of body regions with large motion.
Extensive experiments demonstrate that our method outperforms the existing approaches in both quantitative and qualitative metrics and achieves long sequence generation and editing applications.


In summary, our primary contributions are: 

\begin{enumerate}
    \vspace{-0.2cm}
    \item \textit{Contextual Gesture}, a framework that achieves joint contextualized gesture motion and video generation with various application support;
    \vspace{-0.1cm}
    \item a \textit{Context-aware gesture representation} obtained through knowledge distillation from the chronological gesture-speech aligned features from chronological contrastive learning; and
    \vspace{-0.1cm}
    \item a \textit{Structural-aware Image refinement module}, with edge heatmaps as supervision to improve video fidelity.
\end{enumerate}

\vspace{-0.3 cm}
