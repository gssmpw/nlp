\clearpage
\setcounter{page}{1}
% \maketitlesupplementary
\onecolumn
\appendix

\begin{center} 
    \centering
    \textbf{\large Contextual Gesture: Co-Speech Gesture Video Generation Through Context-Aware Gesture Representation}
\end{center}
\begin{center} 
    \centering
    \large Supplementary Material
\end{center}
 

\section{Overview}
\label{sec:Summary}
The supplementary material is organized into the following sections:
\begin{itemize}
    \item Section \ref{sec:sub_related}:  Additional Related Works
    \item Section \ref{sec:sub_dataset}:  Dataset Details and Preprocessing
    \item Section \ref{sec:sub-implement}: Additional Implementation Details
    \item Section \ref{sec:s2g-generation}: Gesture Motion Generation
    \item Section \ref{sec:s2g-alignment}: Gesture Speech Retrieval
    \item Section \ref{sec:sub-exp}: Additional Experiments
    \item Section \ref{sec:sub-time}: Time and Resource Consumption
    \item Section \ref{sec:sub_user}: User Study Details
    \item Section \ref{sec:sub-tps} TPS-based Image Warping
    \item Section \ref{sec:ethics} Ethical Ethical Considerations
    \item Section \ref{sec:limitation}: Limitations

    
\end{itemize}
For more visualization, please see the additional demo videos.


\section{Additional Related Works}
\label{sec:sub_related}
\paragraph{Masked Representation Learning for Generation}
Masked Representation Learning has been demonstated an effective representation learning for various modalities.~\cite{devlin2018bert, he2022masked} Some works explored the generation capabilities using this paradigm. MAGE~\cite{li2023mage} achieves high-quality image generation through iterative remasking. Muse~\cite{chang2023muse} extends this idea to leverage language with region masking for image editing and achieve fine-grained control. Recent Masking Models~\cite{pinyoanuntapong2024mmm, moMask, wang2023t2mhifigptgeneratinghighquality, Mao_2024} bring this strategy to the motion and gesture domain and improves the motion generation speed, quality, and editing capability. Inspired by these work, we propose the masked gesture generation conditioned the audio to learn the gesture-speech correspondence during generation.

\section{Dataset Details and Preprocessing}
\label{sec:sub_dataset}

\subsection{Preprocessing}
We found that many videos used in ANGIE~\cite{angie} and S2G-Diffusion~\cite{s2gdiffusion}, particularly for the subject \textit{Jon}, are no longer available. To address this, we replaced \textit{Jon} with \textit{Noah}. We utilized the PATS~\cite{ginosar2019gestures} metadata to download videos from YouTube and preprocess them. After filtering, we obtained 1080 videos for \textit{Oliver}, 1080 for \textit{Kubinec}, 1080 for \textit{Seth}, and 988 for \textit{Noah}. For the testing dataset, we collected 120 videos for \textit{Oliver}, 120 for \textit{Kubinec}, 120 for \textit{Seth}, and 94 for \textit{Noah}.

During the dataset preprocessing, while for image-generation we use the whole video preprocessed as above, for for the speech-gesture alignment and gesture pattern generation modules, we further preprocess the data by slicing them into smaller chunks following S2G-Diffusion~\cite{s2gdiffusion}. Specifically, based on the source training dataset, the keypoint sequences and audio sequences are clipped to 80 frames (3.2s) with stride 10 (0.4s) for training. We obtain 85971 overlapping training examples and 8867 testing examples for gesture pattern modeling.

\subsection{Feature Representation}
\paragraph{Gesture Keypoints.} 
We utilize RTMPose~\cite{rtmpose} from MMPose~\cite{mmpose2020} for whole-human-body keypoint identification. The keypoint definition is based on by 133 CoCo human pose estimation. Due to the PATS~\cite{ginosar2019gestures} only contains the upper body, we select 68 face landmarks for face motion modeling, 3 for left shoulder, 3 for right shoulder, 21 for left hand and 21 for right hand separately, which results in flattened face feature with dim of 136 and body feature with dim of 96.

\paragraph{Audio Features.} 
The audio features are pre-extracted WavLM features (dim of 1024) with additional low-level mel-spectrum and beat information with dimension of 34. We concatenate them channel-wise as the speech feature.

\subsection{Dataset License.}
The video data within PATS dataset include personal identity information, and we strictly adhere to the data usage license ``CC BY - NC - ND 4.0 International,'' which permits non-commercial use.


\section{Additional Implementation Details}
\label{sec:sub-implement}


We jointly train the framework on three speakers. The following sections provide the details for each module's training.

\paragraph{Optimizer Settings.}
All modules utilize the Adam Optimizer~\cite{kingma2014adam} during training, with a learning rate of \(1 \times 10^{-4}\), \(\beta_1 = 0.5\), and \(\beta_2 = 0.999\).

\paragraph{Speech-Gesture Alignment.}
For aligning speech with facial and bodily gestures, we implement two standard transformer blocks for encoding each modality. The latent dimension is configured to 384, accompanied by a feedforward size of 1024. We calculate the mean features for both modalities and project them using a two-layer MLP in a contrastive learning framework, with a temperature parameter set to 0.7.

\paragraph{Residual Vector Quantization (RVQ) Tokenization.}

The overall training objective for the RVQ-VAE is defined as:
\begin{equation}
\mathcal{L}_{\text{rvq}} = \mathbb{E}_{x \sim p(x)} \left[ \left\| x - \hat{x} \right\|^2 \right] + \alpha \sum_{r=1}^{R} \mathbb{E}_{z_r \sim q(z_r|x)} \left[ \left\| e_r - \text{sg} \left( z_r - e_r \right) \right\|^2 \right] + \beta \mathcal{L}_{\text{distill}}
\end{equation}
where \( \mathcal{L}_{\text{rvq}} \) combines a motion reconstruction loss, a commitment loss~\cite{oord2018neuraldiscreterepresentationlearning} for each layer of quantizer with a distillation loss, with \( \alpha \) and \( \beta \) weighting the contributions.

We employ six layers of codebooks for residual vector quantization~\cite{rvq} for both face and body modalities, each comprising 1024 codes. To address potential collapse issues during training, we implement codebook resets. The RVQ encoder and decoder are built with two layers of convolutional blocks and a latent dimension of 128. We avoid temporal down-sampling to ensure the latent features maintain the same temporal length as the original input sequences. During RVQ training, we set \(\alpha = 1\) and \(\beta = 0.5\) to balance gesture reconstruction with speech-context distillation.

\paragraph{Mask Gesture Generator.}
The generator takes sequences of discrete tokens for both face and body, derived from the RVQ codebook. This module includes two layers of audio encoders for face and body, initialized based on the Speech-Gesture Alignment. The latent dimension is again set to 384, with a feedforward dimension of 1024, and it features eight layers for both modalities. A two-layer MLP is utilized to project the latent space to the codebook dimension, and cross-entropy is employed for model training. We calculate reconstruction and acceleration loss by feeding the predicted tokens into the RVQ decoder. A reconstruction loss of 50 is maintained during training, and the mask ratio is uniformly varied between 0.5 and 1.0. For inference, a cosine schedule is adopted for decoding. 
We uniformly mask between 50\% and 100\% of the tokens during training. Following the BERT~\cite{devlin2018bert}, when a token is selected for masking, we replace it with a \texttt{[MASK]} token 80\% of the time, a random token 10\% of the time, and leave it unchanged 10\% of the time.  
The Mask Gesture Generator is trained over 250 epochs, taking approximately 1 days to complete.

\paragraph{Residual Gesture Generator.}
The Residual Gesture Generator is designed similarly to the Mask Gesture Generator but utilizes only six layers for the generator. It features four embedding and classification layers corresponding to the RVQ tokenization scheme for residual layers. During training, we randomly select a quantizer layer \( j \in [1, R] \) for learning. All tokens from the preceding layers \( t^{0:j-1} \) are embedded and summed to form the token embedding input. After generating the base layer predictions of discrete tokens from the Masked Gesture Generator, these tokens are fed into the Residual Gesture Generator. This module iteratively predicts the tokens from the base layers, ultimately producing the final quantized output. This module is trained for an additional 500 epochs, requiring about 0.5 days to finalize.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/stylegan-based.pdf}
%     \vspace{-0.5cm}
%     \caption{\small{\textbf{UNet structure based on StyleGAN} We further encode the warpped features by an encoder, which connects to the pretrained StyleGAN for final RGB rendering.}}
%     \vspace{-0.1cm}
%     \label{fig:image-gen}
% \end{figure}


\paragraph{Image Warping.}
For pixel-level motion generation, we utilize Thin Plate Splines (TPS)~\cite{zhao2022thin}. Our framework tracks 116 keypoints (68 for the face and 48 for the body). The number of TPS transformations \(K\) is set to 29, with each transformation utilizing \(N = 4\) paired keypoints. In accordance with TPS methodologies, both the dense motion network and occlusion-aware generators leverage 2D convolutions to produce \(64 \times 64\) weight maps for optical flow generation, along with four occlusion masks at various resolutions (32, 64, 128, and 256) to facilitate image frame synthesis.

\paragraph{Structure-aware Image-Refinement.} We use the UNet similar to S2G-Diffusion~\cite{s2gdiffusion} to restore missing details, further improve the hand and shoulder areas. We keep the training loss to be the same except the added conditional adversarial loss based on edge heatmap. For the network design difference, we add the multi-level edge heatmap as additional control for different resolutions (32, 64 and 128). Each correponds to a SPADE~\cite{park2019SPADE} block to inject the semantic control into the current generation.

% we leverage an U-Net based on StyleGAN structure to further refine the warpinging features and convert it into the final high resolution RGB images. We draws inspiration from GaussianStyle~\cite{gaussianstyle}, which projects Gaussian-splatted feature maps into StyleGAN for high-quality pixel-level generation. In our method, we treat the warping and generation process in a similar way. As shown in \cref{fig:image-gen} right, we use an encoder to process the structure-aware warped features and project them into the pretrained StyleGAN for final image generation.



\section{Gesture Motion Generation}
\label{sec:s2g-generation}
\vspace{-0.2cm}
\paragraph{Inference.} While existing works~\cite{liu2023emage, yi2022generating, diffsheg} leverage auto-regressive next-token prediction or diffusion-based generation process, these strategies hinder the fast synthesis for real-time applications. To resolve this problem, as in Fig.~\ref{fig:motion}, we employ an iterative mask prediction strategy to decode motion tokens during inference. Initially, all tokens are masked except for the first token from the source frame. Conditioned on the audio input, the Mask Gesture Generator predicts probabilities for the masked tokens. In the \( l \)-th iteration, the tokens with the lowest confidence are re-masked, while the remaining tokens stay unchanged for subsequent iterations. This updated sequence continues to inform predictions until the final iteration, when the base-layer tokens are fully generated. Upon completion, the Residual Gesture Generator uses the predicted base-layer tokens to progressively generate sequences for the remaining quantization layers. Finally, all tokens are transformed back into motion sequences via the RVQ-VAE decoder.

\begin{table}[t]
  \caption{Ablations of our method. We exam training \& inference strategy. Bold indicates the best performance.}
  \vspace{-0.1cm}
  \label{tab:add-ab}
  \renewcommand{\tabcolsep}{1.8pt}
  \small

    \begin{subtable}[!t]{0.5\linewidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      {\it M-Ratio} & FGD\(\downarrow\) & Div.\(\uparrow\) & PCM\(\uparrow\)\\
      \midrule
      
      Uni 0-1 & 2.13 & \textbf{14.31} & 0.56 \\
      Uni .3-1 & 1.56 & 12.44 & 0.512 \\
      \rowcolor{mygray} Uni .5-1 & \textbf{0.87} & 13.23 & 0.59 \\
      Uni .7-1  & 1.22 & 13.12 & 0.57\\
      \bottomrule
    \end{tabular}
    \caption{\small{mask-ratio during training.}}
    \label{tab:ab_mask}
  \end{subtable}
  \hspace{\fill}
  \begin{subtable}[!t]{0.5\linewidth}
    \centering
    \begin{tabular}{cccc}
      \toprule
      {\it iter.}  & FGD\(\downarrow\)  & Div.\(\uparrow\)  & PCM\(\uparrow\)\\
      \midrule
      
      \rowcolor{mygray} 5 & \textbf{0.87 }& \textbf{13.23} & 0.59 \\
      10 & 0.98 & 13.11 & 0.57\\
      15 & 1.24 & 13.04 & 0.57\\
      20 & 1.56 & \textbf{13.23} & 0.57\\
      \bottomrule
    \end{tabular}
    \caption{\small{Mask decoding steps.}}
    \label{tab:decode}
  \end{subtable}
  \vspace{-5mm}
\end{table}

\vspace{-0.2cm}
\paragraph{Training Objective.}
To train our gesture generation models, $\mathcal{L}_{mask}$, and  $\mathcal{L}_{res}$ functions for two generaors respectively by minimizing the categorical cross-entropy loss, as illustrated below:
\begin{equation}
\mathcal{L}_{mask} = \sum_{i=1}^{T} -\log p_\phi(t_i | Es(S), \mathrm{MASK}), \quad \mathcal{L}_{res} = \sum_{j=1}^{V}\sum_{i=1}^{T}-\log p_\phi(t_i^j | t_i^{1:j-1}, Es(S), j).
\end{equation}
In this formulation, $\mathcal{L}_{mask}$ predicts the masked motion tokens $t_i$ at each time step $i$ based on the input audio and the special $[\mathrm{MASK}]$ token. Conversely, $\mathcal{L}_{res}$ focuses on learning from multiple quantization layers, where $t_i^j$ represents the motion token from quantizer layer $j$ and $t_i^{1:j-1}$ includes the tokens from preceding layers. We also feed the predicted tokens into the RVQ decoder for gesture reconstructions, with velocity and acceleration losses~\cite{mdm, siyao2022bailando}.

\vspace{-0.2cm}
\paragraph{Ablation.} We evaluate the mask ratio during training and the number of inference steps during decoding. As shown in \cref{tab:decode}, our model requires only 5 inference steps, in contrast to over 50 or 100 steps in diffusion-based models. Furthermore, a uniform masking ratio between 0.5 and 1 during training yields optimal performance. 





\section{Speech-Gesture Retrieval}
\label{sec:s2g-alignment}

To validate the effectiveness of Speech-Gesture Alignment, inspired by TMR~\cite{tmr} we propose the Speech-Gesture Retrieval as the evaluation benchmark.

\noindent\textbf{Evaluation Settings.}
The retrieval performance is measures under recall at various ranks, R@1, R@2, etc.\. Recall at rank $k$ indicates the percentage of times the correct label is among the top $k$ results; therefore higher is better. We define two settings. The retrieval is based on the sliced clips, with each lasting for 4 seconds and 120 frames, in total 8176 samples.

(a) \textbf{All} test set samples for face and body motions. This set is problematic because the speech and gesture motion should not be of one-to-one mapping relationship.

(b) \textbf{Small batch} size of 32 speech-gesture pairs are randomly picked, reporting average performance.

\noindent\textbf{Evaluation Result.}
Shown in \cref{tab:s2g-alignment}, the gesture patterns and speech context do not present one-to-one mapping relationship, leading to the significantly low performance of retrieval. However, based on setting (b), within a small batch size of 32, the model achieves significantly higher performance, indicating the alignment provides the discrimination over different speech context and the motion. In addition, chronological negative examples during contrastive training enhances the robustness of retrieval.

\begin{table*}
    \centering
    \setlength{\tabcolsep}{4pt}
    \caption{\small{\textbf{Speech-to-Gesture Motion retrieval benchmark on PATS:}
    We establish two evaluation settings as described in
    Section~\ref{sec:s2g-alignment}. 
    }}
    \label{tab:s2g-alignment}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{l|ccccc|ccccc}
        \toprule
        
         \multirow{2}{*}{\textbf{Setting}}  & \multicolumn{5}{c|}{Speech-Face retrieval} & \multicolumn{5}{c}{Face-Speech retrieval} \\
         
         & \small{R@1 $\uparrow$} & \small{R@2 $\uparrow$} & \small{R@3 $\uparrow$} &  \small{R@5 $\uparrow$} & \small{R@10 $\uparrow$} & \small{R@1 $\uparrow$} & \small{R@2 $\uparrow$} & \small{R@3 $\uparrow$} & \small{R@5 $\uparrow$} & \small{R@10 $\uparrow$} \\
        \midrule
    
        (a) All  &  0.181 &  0.350 & 0.485 &  0.722 &  1.343 &  0.226  & 0.361   &0.429   &0.677   &1.207 \\
        \midrule

        (a) + chrono &  0.231  & 0.372  & 0.501  & 0.734  & 1.696 & 0.323 & 0.398  & 0.454 & 0.712 & 1.332 \\
        
        \midrule
        
        (b) Small batches &  26.230  & 45.318  & 59.330  & 77.019   &89.858 & 24.977 &44.822  &59.894 &77.775 &90.264 \\
        \midrule
        (b) + chrono & 27.437  & 47.552  & 63.193  & 74.343   & 89.996 & 26.451 & 46.432 & 61.727 & 79.779 & 91.373 \\


        \midrule

        \multirow{2}{*}{\textbf{Setting}}  & \multicolumn{5}{c|}{Speech-Body retrieval} & \multicolumn{5}{c}{Body-Speech retrieval} \\
         
         & \small{R@1 $\uparrow$} & \small{R@2 $\uparrow$} & \small{R@3 $\uparrow$} &  \small{R@5 $\uparrow$} & \small{R@10 $\uparrow$} & \small{R@1 $\uparrow$} & \small{R@2 $\uparrow$} & \small{R@3 $\uparrow$} & \small{R@5 $\uparrow$} & \small{R@10 $\uparrow$}\\
        \midrule
    
        (a) All  &  0.102 &  0.237 & 0.327 &  0.587 &  1.230 &  0.158  & 0.271   &0.406   &0.654   &1.320 \\
        \midrule

        (a) + chrono  &  0.132 &  0.257 & 0.373 &  0.603 &  1.340 &  0.178  & 0.289   &0.443   &0.671   &1.404 \\
        \midrule
    
        (b) Small batches &  25.542  & 43.660  & 57.954  & 77.471   &90.309 & 24.052 & 43.874 & 58.495 & 76.986 & 89.745 \\
        \midrule
        (b) + chrono & 28.732  & 48.569  & 59.958  &79.321 & 90.003 & 22.671 & 45.737 & 57.669 & 79.565 & 90.672 \\
                                      
    \bottomrule        
    \end{tabular}

    }
    \vspace{0.05in}
    
\end{table*}

\section{Additional Experiments}
\label{sec:sub-exp}

In the main paper, we have shown our method achieves promising joint gesture motion and video generation. To understand the disentangled gesture and video avatar generation separately, we further conduct Gesture Generation and Video Avatar Animation experiments separately to compare our method with the corresponding representative works for each domain.





\begin{table}[ht]
    \caption{Quantitative results on BEAT-X. 
    FGD (Frechet Gesture Distance) multiplied by \( 10^{-1} \), BC (Beat Constancy) multiplied by \( 10^{-1} \), Diversity, MSE (Mean Squared Error) multiplied by \( 10^{-7} \).
    The best results are in bold. }
    \label{tab:tab4}
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcccccc} 
    \toprule
    Methods & Venue & FGD $\downarrow$ & BC $\rightarrow$ & Diversity $\uparrow$ & MSE $\downarrow$\\
    \midrule
    GT &  &  & 0.703 & 11.97 & \\
    HA2G~\cite{liu2022learning} & CVPR 2022 & 12.320 & 0.677 & 8.626  & - \\
    DisCo~\cite{liu2022disco} & ACMMM 2022 & 9.417 & 0.643  & 9.912  & -  \\
    $\text{CaMN}$~\cite{liu2022beat} & ECCV 2022 & 6.644    & 0.676  & 10.86   & -  \\
    $\text{DiffSHEG}$~\cite{diffsheg} & CVPR 2024  & 7.141 & 0.743 & 8.21 & 9.571  \\
    $\text{TalkShow}$~\cite{yi2022generating} & CVPR 2023  & 6.209 & 0.695 & 13.47  & 7.791 \\
    $\text{Rhythmic Gesticulator}$~\cite{ao2022rhythmic} & SIGGRAPH 2023  & 6.453 & 0.665 & 9.132 &  \\
    $\text{EMAGE}$~\cite{liu2023emage} & CVPR 2024 & 5.512 & 0.772 & 13.06 & 7.680 \\
    \rowcolor{mygray} Ours (w/o distill) & - & 5.079 & \textbf{0.737} & 13.24 & 7.742  \\
    \rowcolor{mygray} Ours & - & \textbf{4.434} & \textbf{0.724} & \textbf{13.76} & \textbf{7.021} \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Gesture Generation}
\noindent\textbf{Experiment Settings}
We select BEAT-X~\cite{liu2023emage} as the dataset for additional gesture generation comparison. For consistency, we will exclude the image-to-animation component from our method and extend gesture representation from 2D to 3D poses. (with SMPL-X expressions for face gestures, as in the existing literature) We compare the gesture generation module of our work with representative state-of-the-art methods in co-speech gesture generation~\cite{ao2022rhythmic, yi2022generating, liu2023emage}. We further design a baseline without using contextual distillation.

\noindent\textbf{Evaluation Metrics}
We evaluate the realism of body gestures by FrÃ©chet Gesture Distance (FGD)\cite{yoon2020speech}. We include Diversity by calculating the average L1 distance across generated clips. For synchronization, we use Beat Constancy (BC)~\cite{li2021ai}.
For facial expression, we use the vertex Mean Squared Error (MSE)~\cite{xing2023codetalker} to for positional accuracy. 

\noindent\textbf{Experiment Results}
As shown in \cref{tab:tab4}, our method significantly improve the SMPL-X based co-speech gesture generation with lower FGD and higher diversity. Specifically, Our methods have present smoother gesture motion patterns compared with existing works. It demonstrates the effectiveness of contextual distillation for the motion representation learning in our framework. We defer the video comparisons in the Appendix videos for reference.

\begin{wraptable}{r}{6cm}
% \vspace{-1.6cm}
\setlength{\tabcolsep}{1mm}
{
\caption{\small{Long Sequence Generation Quality.}}
\vspace{-0.2cm}
\label{tab:long-seq}
\small{
\begin{tabular}{ccccc}
\hline
Dataset & Setting & FGD & Diversity & BAS \\
\hline
\multirow{2}{*}{PATS} & $\leq$10s & 1.303 & 13.260 & 0.996\\
 & $>$10s & 2.356 & 11.956 & 0.994  \\
\multirow{2}{*}{BEAT-X} & $\leq$10s  & 4.747 & 13.14 & 7.323 \\
 & $>$10s & 4.650  & 13.55 & 7.370\\
\hlinew{1.15pt}
\end{tabular}
}
}
% \end{center}
\vspace{-0.2cm}
\end{wraptable}

\noindent\textbf{Long Sequence generation}
To understand the capability of our framework for long sequence generation, we conduct an ablation study for both PATS and BEAT-X dataset. For BEAT-X, we cut the testing audios into segments of 256 (about 8.53 seconds) for short sequence evaluation and use raw testing audios for long sequence evaluation in \cref{tab:tab4}. Shown in \cref{tab:long-seq}, it is interesting for PATS dataset, long-sequence generation as an application in the main paper presents quality lower than normal settings. However, for BEAT-X dataset, the generation quality is not affected much. We attribute this difference caused by the dataset difference. Because PATS dataset consists training video lengths with a average of less than 10 seconds, the model presents less diverse gesture patterns. However, in BEAT-X, most of gesture video sequences are over 30 or 1 minutes, our method further benefits from this long sequence learning precess and presents higher qualities.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/image-reconsturction.pdf}
    \vspace{-0.5cm}
    \caption{\small{\textbf{Comparison of Video Avatar Animation} Though presented with worse hand structure reconstruction, we achieve better identity preserving and significantly better background motion.}}
    \vspace{-0.5cm}
    \label{fig:image-reconst}
\end{figure}


\subsection{Video Avatar Animation}
\noindent\textbf{Experiment Settings.}
We select PATS dataset as in main paper for avatar rendering comparison. We processed the videos into 512x512 for Diffusion-bassed model AnimateAnyone~\cite{hu2023animateanyone}. We extract the 2D poses by MMPOse~\cite{mmpose2020} for pose guidance for the Diffusion Model, and maintain all the training details as in AnimateAnyone for consistency.

\noindent\textbf{Experiment Results.}
We compare the gesture generation module of our work with representative AnimateAnyone~\cite{hu2023animateanyone}. As shown in \cref{fig:image-reconst}, though AnimateAnyone achieves better video generation quality for hand structure of the speaker centering in the video, it fails to maintain the speaker identity, making the avatar less similar to the source image compared with our method. In addition, due to the entanglement of camera motions and speaker gesture motions within the dataset, AnimateAnyone fails to separate two types of motions from the source training video, thus leading to significant background changes over time and dynamic inconsistency. Unlike completely relying on human skeletons as conditions in AnimateAnyone, our method benefits from Warping-based method, which has the capability of resolving the background motions in addition to the speaker motion. We defer visual comparisons in the Appendix videos.



\section{Time and Resource Consumption}
\label{sec:sub-time}

In \cref{tab:consumption}, we present a comparison of training and inference times against existing baseline methods. For audio-gesture generation, our model's training time is comparable, albeit slightly slower, than that of ANGIE~\cite{angie} and S2G-Diffusion~\cite{s2gdiffusion}, primarily due to the inclusion of additional modules. However, it is considerably faster than MM-Diffusion~\cite{ruan2022mmdiffusion}. Notably, our method excels in inference speed, outperforming all other baselines.

While the training of image-warping and image refinement requires a lot of time, our method leads to a substantial reduction in overall time and resource usage compared to MM-Diffusion and other stable-diffusion-based video generation approaches. Furthermore, the generative masking paradigm we employ significantly cuts down inference times when compared to diffusion-based models like S2G-Diffusion or the autoregressive generations in ANGIE.

We further compared image-warping based method computation requirements with Stable Diffusion-based models like AnimateAnyone~\cite{hu2023animateanyone} in \cref{tab:consumption-sd}. 


\begin{table}[!ht]
\centering

\caption{\small{\textbf{Time consumption comparison} of training (1 NVIDIA A100 GPU) and inference (1 NVIDIA GeForce RTX A6000 GPU).}}
\label{tab:consumption}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc}
\hlinew{1pt}
Name & Training & Training Breakdown & \begin{tabular}[c]{@{}c@{}} Inference\\ (video of $\sim$10 sec) \end{tabular} \\
\hline
ANGIE & $\sim$5d & Motion Repr. $\sim$3d + Quantize $\sim$0.2d + Gesture GPT $\sim$1.8d  & $\sim$30 sec\\

MM-Diffusion \ & $\sim$14d  & Generation $\sim$9d + Super-Resolution $\sim$5d& $\sim$600 sec \\

S2G-Diffusion & $\sim$5d & Motion Decouple $\sim$3d + Motion Diffusion $\sim$1.5d + Refine $\sim$0.5d & $\sim$35 sec\\
Ours & $\sim$6d & Quantize $\sim$0.2d + Mask-Gen $\sim$1.5d + Res-Gen $\sim$0.5d + Img-warp \& Refine $\sim$3.5d  & $\sim$3 sec\\

\hlinew{1pt}
\end{tabular}%
}
\end{table}




\begin{table}[!ht]
\centering

\caption{\small{\textbf{Resource consumption comparison} with Stable-Diffusion-based Image-Animation models (1 NVIDIA A100 GPU), * means our re-implementation on PATS dataset.}}
\label{tab:consumption-sd}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\hlinew{1pt}
Methods & Training$\downarrow$ & Batch Size & Resolution & Memory$\downarrow$ & Training Task & Inference$\uparrow$  \\
\hline
AnimateAnyone* & 10 days & 4 & 512 & 44 GB & Pose-2-Img & -\\
AnimateAnyone* & 5 days & 4  & 512 & 36GB & Img-2-Vid & 15s \\
\hline
Ours & 2.5 days & 64 & 256 & 64 GB & Img-Warp & $\leq$1s\\
Ours & 1 day & 64 &  256 & 48GB & Img-Refine & $\leq$ 1s\\
\hline
Ours & 3.5 days & 32 & 512 &  60GB & Img-Warp & $\leq$1s\\
Ours & 1 day & 32 & 512 & 40GB & Img-Refine & $\leq$1s\\
\hlinew{1pt}
\end{tabular}}
\end{center}
\vspace{-0.2cm}
\label{table_run_time}
\end{table}


\begin{figure}
    \vspace{-1em}
    \includegraphics[width=\linewidth]{figs/user-study.pdf}
    \vspace{-1em}
    \caption{\small{Screenshot of user study website.}}
    \vspace{-5mm}
    \label{fig:user_study}
\end{figure}



\section{User Study Details}
\label{sec:sub_user}
For user study, we recruited 20 participants with good English proficiency. To conduct the user study, we randomly select 80 videos from EchoMimicV2~\cite{meng2024echomimic}, ANGIE~\cite{angie}, S2G-Diffusion~\cite{s2gdiffusion} and ours. Each user works on 20 videos, with 4 videos from each of the aforementioned methods. The users are not informed of the source of the video for fair evaluations. A visualization of the user study is shown in ~\cref{fig:user_study}.



\section{TPS-based Image-Warping}
\label{sec:sub-tps}

In this paper, we utilize Thin Plate Splines (TPS)~\cite{zhao2022thin} to model deformations based on human poses for image-warping. Here, we provide additional details on this approach.

The TPS transformation accepts \(N\) pairs of corresponding keypoints \((p^\mathbf{D}_i, p^\mathbf{S}_i)\) for \(i=1,2,\ldots, N\) (referred to as control points) from a driving image \(\mathbf{D}\) and a source image \(\mathbf{S}\). It outputs a pixel coordinate mapping \(\mathcal{T}_{tps}\left(\cdot\right)\), which represents the backward optical flow from \(\mathbf{D}\) to \(\mathbf{S}\). This transformation is founded on the principle that 2D warping can be effectively modeled through a thin plate deformation mechanism. The TPS transformation seeks to minimize the energy associated with bending this thin plate while ensuring that the deformation aligns accurately with the control points. The mathematical formulation is as follows:
\begin{equation}
\begin{split}
\min &\iint_{\mathbb{R}^{2}} \left( \left( \frac{\partial^{2} \mathcal{T}_{tps}}{\partial x^{2}} \right)^{2} + 2 \left( \frac{\partial^{2} \mathcal{T}_{tps}}{\partial x \partial y} \right)^{2} + \left( \frac{\partial^{2} \mathcal{T}_{tps}}{\partial y^{2}} \right)^{2} \right) \, dx dy, \label{eq:tps_1} \\
&\text{s.t.} \quad \mathcal{T}_{tps}(p_{i}^{\mathbf{D}}) = p_{i}^{\mathbf{S}}, \quad i = 1,2, \ldots, N,
\end{split}
\end{equation}
where \(p_{i}^{\mathbf{D}}\) and \(p_{i}^{\mathbf{S}}\) denote the \(i^{th}\) keypoints in \(\mathbf{D}\) and \(\mathbf{S}\) respectively. As shown in \cite{zhao2022thin}, it can be demonstrated that the TPS interpolating function satisfies \cref{eq:tps_1}:
\begin{equation}
\mathcal{T}_{tps}(p)=A\left[\begin{array}{l}
p \\
1
\end{array}\right]+\sum_{i=1}^{N} w_{i} U\left(\left\|p^{\mathbf{D}}_{i}-p\right\|_{2}\right), \label{eq:tps_2}
\end{equation}
where \(p=(x, y)^{\top}\) represents the coordinates in \(\mathbf{D}\), and \(p^{\mathbf{D}}_{i}\) is the \(i^{th}\) keypoint in \(\mathbf{D}\). The function \(U(r)=r^{2} \log r^{2}\) serves as a radial basis function. Notably, \(U(r)\) is the fundamental solution to the biharmonic equation \cite{selvadurai2000biharmonic}, defined by:
\begin{equation}
\Delta^2 U=\left(\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}\right)^2 U \propto \delta_{(0,0)},
\end{equation}
where the generalized function \(\delta_{(0,0)}\) is characterized as:
\begin{align}
\delta_{(0,0)} = \begin{cases}
\infty, & \text{if } (x, y) = (0, 0) \\
0, & \text{otherwise}
\end{cases},\quad
\text{and} \iint_{\mathbb{R}^{2}} \delta_{(0,0)}(x, y) \,dx dy = 1,
\end{align}
indicating that \(\delta_{(0,0)}\) is zero everywhere except at the origin, where it integrates to one.

We denote the \(i^{th}\) keypoint in image \(\mathbf{X}\) (either \(\mathbf{D}\) or \(\mathbf{S}\)) as \(p^{\mathbf{X}}_i=(x^{\mathbf{X}}_i, y^{\mathbf{X}}_i)^{\top}\), and we define:
\begin{equation}
r_{ij} = \left\|p^{\mathbf{D}}_{i}-p^{\mathbf{D}}_{j}\right\|, \quad i,j=1,2,\ldots, N. \notag
\end{equation}
Next, we construct the following matrices:
\begin{equation}
\begin{aligned} 
K= & {\left[\begin{array}{cccc}0 & U\left(r_{12}\right) & \cdots & U\left(r_{1N}\right) \\
U\left(r_{21}\right) & 0 & \cdots & U\left(r_{2 N}\right) \\ \vdots & \vdots & \ddots & \vdots \\
U\left(r_{N 1}\right) & U\left(r_{N 2}\right) & \cdots & 0\end{array}\right] }, \quad
P =\left[\begin{array}{ccc}1 & x^{\mathbf{D}}_1 & y^{\mathbf{D}}_1 \\ 1 & x^{\mathbf{D}}_2 & y^{\mathbf{D}}_2 \\ \vdots & \vdots & \vdots \\ 1 & x^{\mathbf{D}}_N & y^{\mathbf{D}}_N\end{array}\right], \notag
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
L & =\left[\begin{array}{cc}K & P \\ P^T & 0\end{array}\right], \quad
Y = \left[\begin{array}{ccccccc} x^{\mathbf{S}}_1 & x^{\mathbf{S}}_2 & \cdots &  x^{\mathbf{S}}_N & 0 & 0 & 0 \\
y^{\mathbf{S}}_1 & y^{\mathbf{S}}_2 & \cdots &  y^{\mathbf{S}}_N & 0 & 0 & 0 \\
\end{array}\right]^{\top}. \notag
\end{aligned}
\end{equation}
We can then determine the affine parameters \(A \in \mathcal{R}^{2 \times 3}\) and the TPS weights \(w_{i} \in \mathcal{R}^{2 \times 1}\) by solving the following equation:
\begin{equation}
\begin{aligned}
\left[w_1, w_2, \cdots, w_N, A\right]^{\top} = L^{-1}Y.
\end{aligned}
\end{equation}
In \cref{eq:tps_2}, the first term \(A\left[\begin{array}{l} 
p \\ 1 \end{array}\right]\) represents an affine transformation that aligns the paired control points \((p^\mathbf{D}_i, p^\mathbf{S}_i)\) in linear space. The second term \(\sum_{i=1}^{N} w_{i} U\left(\left\|p^{\mathbf{D}}_{i}-p\right\|_{2}\right)\) accounts for nonlinear distortions that enable the thin plate to be elevated or depressed. By combining both linear and nonlinear transformations, the TPS framework facilitates precise deformations, which are essential for accurately capturing motion while preserving critical appearance details within our framework.



\section{Ethical Considerations} 
\label{sec:ethics}

While this work is centered on generating co-speech gesture videos, it also raises important ethical concerns due to its potential for photo-realistic rendering. This capability could be misused to fabricate videos of public figures making statements or attending events that never took place. Such risks are part of a broader issue within the realm of AI-generated photo-realistic humans, where phenomena like deepfakes and animated representations pose significant ethical challenges.

Although it is difficult to eliminate the potential for misuse entirely, our research offers a valuable technical analysis of gesture video synthesis. This contribution is intended to enhance understanding of the technology's capabilities and limitations, particularly concerning details such as facial nuances and temporal coherence.

In addition, we emphasize the importance of responsible use. We recommend implementing practices such as watermarking generated videos and utilizing synthetic avatar detection tools for photo-realistic images. These measures are vital in mitigating the risks associated with the misuse of this technology and ensuring ethical standards are upheld.




\section{Limitations}
\label{sec:limitation}

While our method have achieved significant improvements over existing baselines, there are still two limitations of the current work.

First, the generation quality still exhibit blurries and flickering issues. The intricate structure of hand hinders the generator in understanding the complex motions. In addition, PATS dataset is sourced from in-the-wild videos of low quality. Most frames extracted from videos demonstrate blurry hands, limiting the network learning. Thus, it is important to collect the high-quality gesture video dataset with clearer hands to further enhance the generation quality.

Second, when modeling the whole upper-body, it is hard to achieve synchronized lip movements aligned with the audio. Even though we explicit separate the face motion and body motion to deal with this problem, there is no regularization on lip movement. We would like to defer this problem to the future works that models disentangled and fine-grained motions for each face and body region.

