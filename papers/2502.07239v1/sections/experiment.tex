\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{figs/comparison-icml.pdf}
   \vspace{-0.6cm}
   \caption{\textbf{Visual comparisons.} Our method generates high-quality hand and shoulder motions, and presents metaphoric gestures when saying \textit{``90 joules,''} and \textit{``in each case.''}. Red boxes denote the blurry or unnatural gestures by other methods.}
   \label{fig:compare1}
   \vspace{-0.3cm}
\end{figure*}

\vspace{-0.2cm}
\section{Experiments}
\label{sec:exp}

\vspace{-0.2cm}
Since our work focuses on joint gesture motion and video generation, the main experiments primarily compare our approach with existing methods that also address joint generation. Comparisons for gesture motion generation and avatar video rendering are deferred to the Appendix.

\vspace{-0.2cm}
\subsection{Experimental Settings}


\textbf{Dataset and Preprocessing.}
We utilize PATS~\cite{ginosar2019gestures,ahuja2020no} for the experiments. It contains 84,000 clips from 25 speakers with a mean length of 10.7s, 251 hours in total. For a fair comparison, following the literature~\cite{angie, s2gdiffusion} and replace the missing subject, with 4 speakers are selected (\textit{Noah}, \textit{Kubinec}, \textit{Oliver}, and \textit{Seth}). All video clips are cropped with square bounding boxes, centering speaks, resized to $256\times 256$. We defer the additional details in the Appendix. 

% After filtering, we obtain around 1000 clips for each speaker, randomly divided into 90\% for training and 10\% for evaluation, 4,000 in total. 



\vspace{-0.2cm}

\subsection{Gesture Video Generation} 
\label{subsec:quantitative}

\vspace{-0.2cm}
\paragraph{Evaluation Metrics.}
For gesture motion metrics, we use \textbf{Fréchet Gesture Distance (FGD)}~\cite{yoon2020speech} to measure the distribution gap between real and generated gestures in feature space, \textbf{Diversity (Div.)}~\cite{lee2019dancing} to calculate the average feature distance between generated gestures, \textbf{Beat Alignment Score (BAS)} following \cite{li2021ai}, and \textbf{Percent of
Correct Motion parameters (PCM)}, difference of generation deviate from ground-truth following \cite{diffsheg}. We extract 2D human poses for face and body using MMPose~\cite{mmpose2020}. Note that in comparison with other models, the FGD is measued by the keypoints extracted from generated videos while in ablation studies, to prevent the effect image warping errors, FGD is measured by keypoints generated in Sec.\ref{sec:gesture_generation}.

For pixel-level video quality, we assess \textbf{Fréchet Video Distance (FVD)}~\cite{unterthiner2018towards} for the overall quality of gesture videos, \textbf{VQA$_A$} for aesthetics and \textbf{VQA$_T$} for technical quality based on Dover~\cite{wu2023dover}, pretrained on datasets with labels ranked by real users.

We further evaluate the training and inference efficiency of various methods, \textbf{Train-T} denotes the number of days for training. \textbf{Infer-T} denotes the number of seconds to produce a 10-second video.

\begin{table*}[t]
\centering
% \vspace{-0.1cm}
\caption{Quantitative results shows our method performs better in terms of gesture motions and video generation quality.}
\vspace{-0.2cm}
\label{tab:comparison}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc|ccc|cc}
\toprule
\multirow{2}{*}{Name}  & \multicolumn{4}{c|}{Gesture Motion Evaluation} & \multicolumn{3}{c|}{Video Quality Assessment} & \multicolumn{2}{c}{Speed}\\ 
& FGD $\downarrow$ & Div. $\uparrow$ & BAS $\uparrow$ & PCM $\uparrow$ $\uparrow$ & FVD $\downarrow$ & VQA$_A$ $\uparrow$ & VQA$_T$ $\uparrow$ & Train-T $\downarrow$ & Infer-T $\downarrow$\\
\midrule
Ground Truth & 0.0  & 14.01 & 1.00 & 1.00  & 0.00 & 95.69 & 5.33 & - & -\\
\midrule
MM-Diffusion & 67.56 & 4.32 & 0.65 & 0.11  & - & 77.65 & 4.14 & 14 days & 600 sec \\
ANGIE & 34.13 & 7.87 & 0.78 & 0.37  & 515.43 & 86.32 & 4.98 & \textbf{5 days} & 30 sec \\
S2G-Diffusion & 10.54  & 10.08 & 0.98 & 0.45  & 493.43 & 94.54 & 5.63 & 5 days & 35 sec\\
EchoMimicV2 & 13.65  & 9.85 & 0.98 & 0.45  & 466.84 & 95.65 & 5.98 & - & 1200 sec\\
\midrule
Ours + AnimateAnyone & \textbf{6.56} & 13.06 & 0.99 & 0.54  & 477.82 & 95.63 & 6.04 & 8.5 days & 50 sec\\
\rowcolor{mygray} Ours & 8.76 & \textbf{13.13} & \textbf{0.99} & \textbf{0.54}  & \textbf{466.43} & \textbf{96.53} & \textbf{6.12} & 6 days & \textbf{3 sec}\\
\bottomrule
\end{tabular}}
\vspace{-0.2cm}
\end{table*}

\vspace{-0.2cm}
\paragraph{Baseline Methods.}
We benchmark Contextual Gesture against several co-speech gesture video generation methods: (1) ANGIE~\cite{angie}, (2) S2G-Diffusion~\cite{s2gdiffusion}, (3) MM-Diffusion~\cite{ruan2022mmdiffusion}, and
(4) EchoMimicV2~\cite{meng2024echomimic}. The first two are conventional optical-flow-based methods. MM-Diffusion is capable of achieving joint audio-visual generation. EchoMimicV2 is the most recent diffusion based speech-avatar animation model pretrained on large scale data.








\begin{figure}[thb]
\centering
\includegraphics[width=\linewidth]{figs/mos-comparison.png}
\vspace{-0.5cm}
\caption{\textbf{User Study}. We generate 80 videos per method for evaluations of \textit{Realness}, \textit{Diversity}, and \textit{Synchronization}.}
 \vspace{-0.4cm}
\label{fig:user-study}
\end{figure}







\vspace{-0.2cm}
\paragraph{Evaluation Results.}
We present quantitative evaluations in \cref{tab:comparison}. Our approach significantly outperforms existing methods in both gesture motion and video quality metrics. We provide qualitative evaluations in \cref{fig:compare1}. MM-Diffusion is not able to handle complex motion patterns, leading to almost static results. ANGIE and S2G-Diffusion struggle with local regions, such as the hands, due to its reliance on unsupervised keypoints for global transformations, which neglects local deformations. EchoMimicV2 lacks the background motion modeling and is only capable of presenting the aligned centered avatars in the middle. In addition, it fails to achieve diversified gestures. In contrast, our method demonstrates high-quality video generation, particularly in the facial and body areas. The alignment between gesture and speech is notably enhanced through our speech-content-aware gesture latent representation. For example, when the actor says \textit{”90 joules,”} he points to the screen, and he emphasizes phrases like \textit{”so two ways”} and \textit{”in each case”} by raising his hands.




\vspace{-0.15cm}
\paragraph{User Study.}
We conducted a user study to evaluate the visual quality of our method. We sampled 80 videos from each method including EchoMimicV2, S2G-Diffusion, ANGIE and ours and invited 20 participants to conduct Mean Opinion Scores (MOS) evaluations. The rating ranges from 1 (poorest) to 5 (highest). Participants rated the videos on: (1) MOS$_1$: \textit{“How \textbf{realistic} does the video appear?”}, (2) MOS$_2$: \textit{“How \textbf{diverse} does the gesture pattern present?”}, (3) MOS$_3$: \textit{“Are speech and gesture \textbf{synchronized} in this video?”}. The videos were presented in random order to capture participants’ initial impressions. As shown in ~\cref{fig:user-study}, our method outperformed others across realness, synchronization and diversity, achieving significant performance improvement over existing methods. 
% Specifically, for synchronization, our method achieves a score of \textbf{4.14}, presenting a large improvement margin.



\begin{table}
\vspace{-0.2cm}
 \caption{\textbf{Ablation Studies} for keypoint design, gesture representation, generator architecture and image-refinement.}
 \label{tab:ab}
 \renewcommand{\tabcolsep}{2pt}
 \small
 \vspace{-0.1cm}
 \begin{subtable}[t]{0.53\linewidth}
   \centering
   \begin{tabular}{cccc}
     \toprule
     {\it Kp Repr.} & FVD\(\downarrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\)\\
     \midrule
     Unsup-kp & 387.05 & 0.05 & 27.41 \\
     2D-pose & 272.18 & 0.05 & 27.26 \\
     + flex kp & 377.14 & 0.06 & 25.36 \\
     \rowcolor{mygray} full & \textbf{225.77} & \textbf{0.04} & \textbf{27.17}\\
     \bottomrule
   \end{tabular}
   \caption{\small{Configs for keypoint design.}}
   \label{tab:ab_kp}
 \end{subtable}%
 \hspace{5mm}
 \vspace{-0.1cm}
 \begin{subtable}[t]{0.3\linewidth}
   \centering
   \begin{tabular}{ccc}
     \toprule
     {\it G-Repr.} & FGD\(\downarrow\) & PCM\(\uparrow\)\\
     \midrule
     baseline & 8.84 & 0.35 \\
     +RVQ & 3.43 & 0.37\\
     \rowcolor{mygray} + distill & 2.75 & 0.58 \\
     \rowcolor{mygray} + chrono & \textbf{0.87} & \textbf{0.59} \\
     \bottomrule
   \end{tabular}
   \caption{\small{Gesture Repr.}}
   \label{tab:ab_motion}
 \end{subtable}
 
 
 \begin{subtable}[t]{0.3\linewidth}
   \centering
   \begin{tabular}{ccc}
     \toprule
     {\it G-Gen.} & FGD\(\downarrow\) & PCM\(\uparrow\)\\
     \midrule
     w/o res & 1.62 & 0.51 \\
     concat & 3.55 & 0.51 \\
     w/o align & 1.33 & 0.53 \\
     \rowcolor{mygray} full-model & 0.87 & 0.59 \\
     \bottomrule
   \end{tabular}
   \caption{\small{Model Design.}}
   \label{tab:ab_gen}
 \end{subtable}%
 \vspace{-0.3cm}
 \hspace{10mm}
 \begin{subtable}[t]{0.6\linewidth}
   \centering
   \begin{tabular}{cccc}
     \toprule
     {\it Refine} & VQA$_A$\(\uparrow\) & VQA$_T$\(\uparrow\) & FVD\(\downarrow\)\\
     \midrule
     w/o refine & 92.15 & 5.43 & 494.35 \\
     + UNet & 93.86 & 5.51 & 478.54 \\
     + skeleton & 95.79 & 5.65 & 473.34 \\
     \rowcolor{mygray} + heatmap & \textbf{96.53} & \textbf{6.12} & \textbf{466.43} \\
     \bottomrule
   \end{tabular}
   \caption{\small{Image-refinement strategies.}}
   \label{tab:ab_refine}
 \end{subtable}
 \vspace{-0.3cm}
\end{table}


\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{figs/ablation-study-new.pdf}
   \vspace{-0.7cm}
   \caption{\textbf{Ablations.} Left: motion by unsupervised keypoints or 2d poses; 
   Middle: RVQ-based gesture representation and generation; Right: image-refinement helps hand generation.}
   \label{fig:compare2}
   \vspace{-0.3cm}
\end{figure*}

\begin{figure*}[t]
  \centering
   % \vspace{-0.1cm}
   \includegraphics[width=\linewidth]{figs/applications.pdf}
   \vspace{-0.6cm}
   \caption{\small{Our model supports multiple video gesture generation end editing applications.}}
   \label{fig:application}
   \vspace{-0.6cm}
\end{figure*}

\vspace{-0.2cm}
\subsection{Ablation Study}
\label{subsec:ablation}


We present ablation studies of keypoint design for image warping, gesture motion representation, generator architecture design, and varios comparisons of image-refinement. We defer additional experiments in the Appendix.

\noindent \textbf{Motion Keypoint Design.}  
We evaluate four settings for image-warping: (1) unsupervised keypoints for global optical-flow transformation (as in ANGIE and S2G-Diffusion), (2) 2D human poses, (3) 2D human poses augmented with flexible learnable points, and (4) full-model reconstruction with refinement. Each design is assessed using TPS~\cite{zhao2022thin} transformation, with self-reconstruction based on these keypoints for evaluation. As shown in \cref{tab:ab_kp}, learnable keypoints lead to a significant decrease in FVD, highlighting their inadequacy for motion control. The inclusion of flexible keypoints does not enhance the image-warping outcomes. Consequently, we opt to utilize 2D pose landmarks exclusively for our study.

\noindent \textbf{Motion Representation.}  
We evaluate several configurations: (1) baseline: no motion representation, relying solely on the generator to synthesize raw 2D landmarks; (2) + RVQ: utilizing Residual VQ (RVQ) to encode joint face-body keypoints; (3) + distill: learning joint embeddings for speech and gesture in both face and body motions; (4) + chrono: leverage chronological alignment for distillation. We discover RVQ significantly improve the precise pose location while distillation leads to natural movements.

\noindent \textbf{Generator Design.}  
We explore various designs for the gesture generator: (1) w/o res: no residual gesture decoder; (2) concat: instead of using cross-attention for audio control, we concatenate the audio features with gesture latent features element-wise during generation; (3) w/o align: the audio encoder is randomly initialized rather than initialized from face and body contrastive learning. Our findings indicate that the Residual Gesture Generator significantly enhances finger motion generation. The cross-attention design outperforms element-wise concatenation, while the pre-alignment of the audio encoder notably improves FGD.


\noindent \textbf{Image Refinement.}  
We examine various network designs for motion generation, specifically: (1) w/o refine: no image refinement, relying solely on image warping; (2) + UNet: employing a standard UNet; (3) + pose skeleton: integrating connected skeleton maps as in the diffusion ReferenceNet~\cite{hu2023animateanyone}; (4) + edge heatmap: substituting the previous design with our learnable edge heatmap. Our experiments reveal that the edge heatmap outperforms skeleton maps, due to the learnable thickness of connections for better semantic guidance.



\vspace{-0.2cm} 
\subsection{Application}

\paragraph{Long Sequence Generation.} As in \cref{fig:application}, to generate long sequences, we begin with the initial frame and corresponding target audio, segmented into smaller windows. After generating the first segment, the last few frames of the output serve as the new starting condition for the next segment, enabling iterative outpainting.

\vspace{-0.2cm} 
\paragraph{Video Gesture Editing.} For editing, we extract keypoints from the video, tokenize face and body movements into motion tokens, and insert mask tokens where edits are needed. By changing the speech audio or speaker identity, we can create new gesture patterns and re-render the video.

\vspace{-0.2cm} 
\paragraph{Gesture Pattern Transfer.} With different identity embeddings, we generate unique gesture patterns for the same audio input. See the demo videos in the Appendix.

\vspace{-0.2cm} 
\paragraph{Speech-Gesture Retrieval.} With chronological speech-gesture alignment, the model is capable of retrieving the best gesture motion correponding to the given speech audio in a batch of data. See additional details in the Appendix.
