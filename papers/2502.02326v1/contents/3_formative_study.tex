\section{Problem Formulation}

We illustrate our system's motivation through a scenario that highlights how data workers monitor global data states during EDA using a basic Jupyter Notebook, along with the associated challenges they encounter. 
Following this, we review related studies to assess the progress made and the remaining gaps in current tools.
From these insights, we derive design requirements to guide the development of our system.

\subsection{Motivating Scenario}
\label{sec:motivate_scenario}

The scenario demonstrates the challenges encountered while writing codes from scratch for EDA and reviewing an existing script to identify data errors and bugs.

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/motivating_scenario.png}
    \caption{Motivating scenario. The EDA process will encounter a series of challenges that require tedious efforts on programming when conducting data profiling (A), data cleaning (B), data analysis and error discovery (C), and locating error codes (D).}
    \label{fig:motivating_scenario}
\end{figure*}

Alice, a data analyst, is tasked with cleaning and analyzing a dataset on Google Play Store Apps. She begins by importing the libraries and loading the dataset. 

\textbf{Profiling and data cleaning. } 
After loading the dataset, Alice manually prints various profiling information from the data table, such as column data types and null value counts \textbf{(C1)}. 
Based on the information, she identifies several columns that require further processing. 
For example, string columns like ``Installs'' and ``Size'' should be converted to numerical formats. 
Alice writes data transformation scripts to adjust these columns.
To verify the changes, she visualizes the distribution of the transformed columns using visualization libraries, such as matploblib and seaborn, and prints the transformed tables. 
Since multiple columns require transformation, and each transformation may involve several steps, this process quickly becomes repetitive and labor-intensive, as she must repeatedly copy, edit, and run visualization scripts to monitor the results \textbf{(C1)}.
Additionally, inserting visualization scripts between transformation steps increases the overall length of the notebook, making it cumbersome to navigate through the notebook and track data changes \textbf{(C2)}. 
Alice has to scroll through the lengthy notebook to check and compare the results.


\textbf{Discover, locate and fix the error. } 
After cleaning the data, Alice begins to explore multiple data patterns by grouping data and creating visualizations. 
However, when she visualizes the mean ratings of free versus paid apps, she notices an unexpected zero value in the ``Type'' column. 
Alice wonders where this issue originated and which line of scripts may have caused it. 
She first searches and confirms there are no direct transformation scripts applied to the ``Type'' column. 
Next, she tries to scroll through the notebook to identify the intermediate data table that preceded the current one. 
However, it is difficult to find and recognize due to the lack of clear indications \textbf{(C3)}.
Moreover, as the intermediate data tables have been overwritten, she can not create visualizations on them to revisit their features on the ``Type'' column \textbf{(C4)}. 
The only visualizations available are those created earlier, which do not include the ``Type'' column. 
Therefore, Alice has to re-run each cell and manually add the visualization for the ``Type'' distribution until she identifies the code line responsible \textbf{(C5)}, which encounters multiple visualizations to insert and compare \textbf{(C2)}. 
Finally, after checking the distribution differences step by step, she discovers that the issue is caused by a transformation applied to the ``Size'' column, which mistakenly set many data rows to completely zero.
After locating the problematic code, Alice fixes it and reruns the subsequent cells to verify the correction. 
However, as the cells are re-executed, the previous visualizations are overwritten, forcing her to compare the results mentally, which makes it difficult to track the changes before and after the fix \textbf{(C4)}.

In summary, the traditional workflow to monitor the global data states during EDA encounters the following challenges:
\begin{enumerate}[label={\textbf{C\arabic*.}}]

\item \textbf{Tedious manual programming for data visualization. } Users have to manually create and update visualizations scripts, which is both time-consuming and prone to errors.

\item \textbf{Cognitive overload from comparison among cells.} To locate the special line of code that causes the important data changes, users must compare visualizations at each step of the analysis, leading to an overwhelming amount of information to remember and assess.

\item \textbf{Lack of clear indication in the relationships between data tables.} Users struggle to identify where a current data table originated or how it was transformed throughout the analysis, as these relationships are buried in the script and not visually represented.

\item \textbf{Loss of execution history and previous data states.} During EDA, previous states of data may be overwritten, and the history of re-executed cells may be obscured, making it difficult to revisit and compare data states over time.

\item \textbf{Repeatedly copying and editing of the same visualization codes.} To monitor the data states in specific columns, users repeatedly copy and paste the same visualization code, which is inefficient and redundant.

\end{enumerate}

\subsection{Progress and Gaps in Current Tools}
Several studies have attempted to enhance EDA by providing visualizations that help users better understand data states (\textbf{C1}). 
The most notable examples are LUX~\cite{lee2021lux}, Solas~\cite{epperson2022leveraging}, and AutoProfiler~\cite{autoprofiler}.
LUX and Solas recommend visualizations based on data insights and provenance, offering a quick overview of patterns and trends that align with the user's operations and specified intents. 
AutoProfiler focuses on displaying basic information about the data, such as the distribution and profiling of each column. 
We consider both data insights and profiling valuable and incorporate these factors into our chart recommendations.

On the other hand, both LUX and AutoProfiler primarily focus on local data states and lack support for understanding the global state of the data (\textbf{C2-C5}). 
LUX requires users to write specific visualization intent codes to display the current state of a data table. 
To monitor global state changes, users must repeatedly insert the codes, which only simplifies the process of writing visualization codes compared to traditional workflows. 
AutoProfiler eliminates the need for code insertion by automatically updating visualizations and profiling to reflect the most recent state. 
However, it only shows the latest state of the data and does not support revisiting or comparing previous states.
In this study, we enable users to use recommended charts as ``sight glasses'' to trace the global data flow, enabling users to track and understand how data evolves throughout the entire EDA process.

\subsection{Design Requirements}

Based on the challenges identified in the motivating scenario and the progress and gaps in current tools, we derive the following design requirements to guide our system's development:

\begin{enumerate}[label={\textbf{R\arabic*.}}]

    \item \textbf{Effective Chart Recommendation.}
    The system should recommend charts that effectively represent the key aspects of the data, considering both data insights and profiling information. These recommendations should help users quickly grasp the state of their data at any given point, reducing the need for manual chart creation.

    \item \textbf{On-Demand Tracing of Changed Related Data.} The system should allow users to trace the data flow on-demand, enabling them to focus specifically on the related intermediate data tables where changes have occurred. This targeted approach helps users concentrate on critical transformations without being overwhelmed by irrelevant information.
    
    \item \textbf{Representation of Data Relationships.}
    Given the complexity and number of intermediate data tables in a notebook, it is essential to provide visual cues that clearly indicate the flow between these data tables. 
    This flow should include information about the transformations applied, such as \code{df\_2} being generated from \code{df\_1} through filtering. These visual hints will help users quickly understand how data evolves throughout the notebook.
    
    \item \textbf{Support for Revisiting Past Intermediate Tables.} The system should provide mechanisms to revisit past intermediate tables, even if they have been overwritten or hidden due to cell re-execution. This feature is crucial for allowing users to compare historical data states with current ones, ensuring that they can accurately track changes and understand the impact of each transformation. 

    \item \textbf{Consistent Tracing of Charts.} 
    To facilitate a comprehensive understanding of how data changes over time, the system should support consistent tracing of data states. This involves tracking the changes in specific charts or visual representations as they reflect the underlying data across multiple transformations. 
\end{enumerate}
