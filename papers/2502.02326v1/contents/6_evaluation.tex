
\section{User Study}
To evaluate the effectiveness and usability of \system{}, we conducted a user study comparing our system with LUX~\cite{lee2021lux}, a state-of-the-art chart recommendation widget of Jupyter Notebooks. The study was designed to answer the following questions concerning the key features of \system{}:
\begin{itemize}
    \item Q1: Does the chart recommendation help users better understand the data states at a specific point?
    \item Q2: Does the flow view improve users' ability to understand global relationships between the tables? 
    \item Q3: Does the chart tracing across the flow assist users in tracking and locating the specific changes?
\end{itemize}
With these questions, we want to understand whether the \system{} can help analysts improve EDA efficiency and to what degree these features can help.

\subsection{Task Design}

To answer these questions, we designed EDA tasks that involve understanding local data states and tracking global data changes. 
The tasks were structured to evaluate how well LUX and \system{} assist users in these processes. 
Given the complexity and variability of EDA, which often involves iteratively programming and exploring results. It is difficult for users to program from scratch using unfamiliar datasets for EDA. 
Moreover, it is difficult to compare the outcome of different treatments when the tasks are too free-form.

Therefore, we provided participants with initial notebooks rather than asking them to write code from scratch. 
Participants were provided with two data analysis notebooks, each associated with a different dataset, both containing an intentional anomaly. 
The two notebooks are as follows: 

\begin{itemize}

    \item \textbf{N1: Google Play Store Apps.} 
    This case involved a dataset about Google Play Store apps, with an anomaly caused by a specific transformation that incorrectly set all rows to zero where the ``Size'' column equaled ``Varies with device.'' This led to an unexpected zero value in the ``Type'' column later in the analysis.

    \item \textbf{N2: COVID-19 Data.} This case focused on a dataset related to COVID-19, where several countries' recovery data was null after 2021. The anomaly was introduced through a transformation that dropped all rows with null values, resulting in missing data for certain countries post-2021.
\end{itemize}

Based on the two cases, participants were asked to complete the following tasks:

\begin{itemize}
    \item \textbf{T1: Locate the anomaly. } The task is designed to imitate the EDA scenario where the analyst writes codes to analyze the data from scratch but discovers a bug in the end. Participants were required to identify the precise line of code responsible for the anomaly. The time taken to complete this task was recorded. After finishing Task 1, participants moved on to Task 2 using the same system.
    
    \item \textbf{T2: Understand the notebook. } The task is designed to imitate the EDA scenario where the analyst wants to reuse an existing code and have to understand the data analysis process.
    Participants were asked to describe how the data evolved throughout the code execution. 
    They needed to provide specific evidence—such as tables, charts, or other printed information—to support their observations, demonstrating an understanding of the data changes beyond simply reading the code. 
\end{itemize}

To ensure the tasks are similar to the routine behavior of EDA, the participants can adjust the scripts and execute the cells by preference.
This approach allowed us to control the study's scope and focus on evaluating the systems' key features. The tasks were designed to reflect the challenges outlined in the motivating scenario (~\autoref{sec:motivate_scenario}). 
Participants completed the tasks using both \system{} and LUX, working with each system on a different script.
To ensure fair testing, the order and combination of the systems and notebooks were counterbalanced. 



\subsection{Comparison Rationale}
We chose to compare \system{} with LUX because, to our knowledge, it is the most popular chart recommendation tool for computational notebooks. 
Moreover, LUX recommends charts for user-specified cells and keeps the recommended history in the notebook, allowing users to revisit and compare data states to some extent. 
While AutoProfiler~\cite{autoprofiler} also provides functionality for checking the states of data variables, it lacks the ability to offer diverse chart recommendations and only retains the latest data states. 
Given that our research questions focus on understanding global states and tracking data evolution, AutoProfiler was not included in our comparison.

\subsection{Participants} 
We have recruited 12 data analysts through various channels like social media, blogs, and email outreach. 
These analysts come from diverse backgrounds, including sports science, computer science, and data science. The group consists of four female and eight male participants. On average, participants bring an average of 4.3 years of experience in data analysis and have been using computational notebooks for their analyses for an average of 3.6 years (ranging from 2 to 7 years). Importantly, all participants have extensive experience in creating charts using visualization libraries.

\subsection{Procedure} 
All participants began by filling out a consent form and providing demographic information, including age, education, occupation, and relevant background. 
Following this, they were introduced to the study's background and tasks (5 minutes).

\textbf{Formal study (about 40 minutes): }The formal study consisted of two sessions, each focused on one of the systems. 
Specifically, each session began with an 8-minute introduction to the system, where participants were guided through the system's features and interactions using a notebook and dataset different from those used in the tasks. 
After the introduction, participants were given 5 minutes to explore the system freely.
Participants then proceeded to complete the two tasks sequentially using the specified system and dataset. 
The first task had a maximum time limit of 8 minutes.
During the tasks, participants followed a think-aloud protocol, and all their actions, as well as the resulting notebooks, were recorded.

\textbf{Post-study questionnaires and interview (about 30 minutes): }The experiment concluded with a seven-point Likert scale questionnaire, where participants rated their experience with \system{} and LUX (\autoref{fig:question_result}). A post-study interview was conducted to explore the reasons behind their ratings. Additionally, participants were asked for feedback on the recommended charts and flow-tracing interactions. 
The entire experiment took approximately 75 minutes, and each participant was compensated with \$15.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/case_result.png}
    \caption{The recall rate and completion time of T1 using LUX and \system{}.}
    \label{fig:case_result}
\end{figure}

\subsection{Quantitative Results for T1}

As shown in~\autoref{fig:case_result}, all participants successfully located the error code line with \system{}, achieving a recall rate of 100\%. 
However, they encountered varying degrees of difficulty when using LUX to accomplish the task. 
These failures primarily stemmed from the multiple operations required to revisit the various states. 
The code that caused the errors in both cases was silent (from the code itself, it was not obvious that certain data columns had been altered). 
As a result, LUX's default recommendations often did not include these data columns, as they were not deemed relevant. 
Therefore, participants generally had to specify LUX's intent to proceed with the task manually. After specifying the intent, users had to compare the visualizations across different cells, which increased the cognitive load. Finally, rerunning the same code across multiple cells added to the time spent, leading to variability in the overall task recall rate.

For the successful cases, \system{} required less time to accomplish the task overall. Instead of repeatedly inserting and checking charts under each cell, \system{} allows users to trace the changes of a specified chart, significantly simplifying the process. However, the time to complete notebook 1 was notably longer than for notebook 2 when using \system{}. This difference is due to an additional filtering transformation after the error code execution, which caused the anomaly column to change again. As a result, participants needed to trace the data changes twice to confirm the source of the error in notebook 1.

Additionally, only two participants (P6 and P10) successfully located the error in notebook 1 using LUX. P6 identified the anomaly column after discovering the error and focused on examining the visualizations for these specific columns across different cells, occasionally using Pandas for additional support. P10, on the other hand, relied almost exclusively on Pandas to track changes.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/score_result.png}
    \caption{The Result of the Likert Scale Questions in the Post-Study Questionnaire. }
    \label{fig:question_result}
\end{figure}

\subsection{Qualitative Feedback}
\label{sec:feedback}

The result of the post-study questionnaire is shown in \autoref{fig:question_result}. We have also collected comments from participants regarding the ease of use, support for understanding local data states and global data changes, and the efficiency of tracking and locating specific changes.

\textbf{Ease of use.}
Most participants (11/12) found \system{} to be far better or better than LUX in terms of ease of use. 
While six participants noted that the learning curves for both tools were relatively low and similar—\system{} requires familiarization with its interface and interactions, while LUX involves learning several APIs—they highlighted that \system{}'s interactions were easier to remember. 
Participants described \system{} as ``intuitive and natural'' (P2, P6, P12), making it more memorable, whereas LUX often required referring back to API documentation.

Additionally, all participants mentioned that coding with LUX was more demanding than interacting with \system{}'s graphical interface. 
P3 further noted that LUX required more cognitive effort: ``I need to consider which columns to view, how to display them, and how to use the API,'' while with \system{}, ``I just click and instantly get a lot of information, then look for what’s meaningful."

\textbf{Understanding local data states. }
Most participants (9/12) found \system{} to be far better or better than LUX for understanding local data states. While both tools effectively recommended charts for operated columns and filtered by distributions, key differences highlighted \system{}'s advantages.

Participants noted that \system{} consistently surfaced relevant charts for any column, whereas LUX sometimes ``struggled to find the desired charts without precise intent'' (P1). 
LUX was more effective when the intent was clear. 
However, as P5 mentioned, ``If I already know which columns I'm interested in clearly and can write the intent, it's not much different from just writing the visualization code directly."
Additionally, P6 pointed out that LUX lacked the ability to filter columns within the results through interactions, requiring ``manual scanning of charts,'' which was seen as time-consuming. 

Several participants (P4, P11, P12) noted that \system{}'s clear visualization of the global data flow also improved their understanding of local data states, especially in notebooks where the execution order might differ from the cell order. P12 highlighted, ``The entire data flow is presented very intuitively, with clear color changes indicating modifications compared to the previous node. I don't have to rerun everything or worry about whether I'm seeing the true local state.'' With LUX, users need to verify the correct cell state manually and sometimes even rerun the notebook from the beginning.

\textbf{Understanding global data changes.}
Most participants (11/12) found \system{} far better than LUX for understanding global data changes, praising its effective data tracing features. 
The node-link graph with auto-collapsed charts was particularly appreciated for its intuitive design. 
P4 remarked, ``The different colors for nodes and links make the data flow clear,'' while P11 appreciated that ``Only the changed charts expand, so I can instantly see where the data shifts.'' 

Using LUX, users usually manually compare data states across cells. P2 and P10 found this challenging: ``I have to write code for each cell and manually compare charts, making it easy to lose track of earlier states.'' Participants also noted LUX's inconsistency across cells, with P4 mentioning that ``the charts recommended in different cells are isolated ones, and it is hard to track changes without a clear link between them.'' P5 described the process as ``disjointed,'' adding that ``LUX lacks the continuity needed to display data changes effectively.''

\textbf{Locating anomalies.}
Most participants (10/12) found \system{} far better than LUX for locating anomalies. Participants appreciated how \system{} allowed them to trace and pinpoint the source of errors really fast. P1 noted that with \system{}, they could ``easily see where the anomalies occurred and link the charts to the relevant code line,'' making it unnecessary to read every line of code closely. 

As for LUX, participants frequently mentioned the need to compare cells manually, split the cells, and even restart the notebook to track down anomalies. 
P3 mentioned, ``I had to run each cell separately and split them, which took many iterations to find the issue.'' 
P7 further described the process with LUX as a ``binary search,'' requiring extensive manual effort to narrow down the error location. This was time-consuming and inefficient, especially compared to \system{}'s streamlined tracing capabilities.