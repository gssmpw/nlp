\section{Discussion}
In this section, we summarize the feedback received in the user studies and discuss the lessons learned for better design of EDA tools.

\subsection{Tracing Backward and Forward}
Our initial use case for \system{} envisioned users executing scripts after completing a coding stage and, upon encountering anomalies, using \system{} to trace backward and identify the source of these issues. This approach aligns with the feedback discussed in \autoref{sec:feedback}, which emphasizes the system's effectiveness in pinpointing specific data changes.
An unexpected insight emerged from six participants during the user study: they expressed a desire to use \system{} not only to trace backward but also to trace forward. During EDA, they would add specific charts to the tracing list before starting transformations. As they proceeded with transformations, they could observe real-time updates, reflecting a dynamic, continuous data profiling approach. This feedback suggests that \system{} can support ongoing data profiling, aligning with concepts from previous work on continuous data profiling~\cite{autoprofiler}.


\subsection{Monitoring Global Changes to Facilitate Understanding of Local States: Where Are We?}
Another unexpected insight was that viewing global data changes within the flow helps users better understand the local data state. In a flexible and sometimes chaotic notebook environment, knowing the current state of a table within the broader global context enhances users' awareness of ``where we are'' in the data flow, making it easier to navigate and comprehend the current data context.

\subsection{Visualization as Sight Glasses of Tables.}
P2 mentioned that using charts to monitor data changes is highly intuitive, as detecting visualization changes is often easier than detecting them in raw tables or other textual data. This suggests that visualizations can accelerate the process of identifying where data changes occur. However, since visualizations provide an aggregated view of raw data and only highlight certain aspects of data patterns, they may not capture all details. To pinpoint the exact nature of data changes, users may still need to refer back to the full data tables. 
While less intuitive, data tables offer a complete and accurate representation of the data. Four participants recommended integrating a table view to complement the existing visualizations, providing a more comprehensive toolset. We plan to incorporate this feature in future updates.


\section{Limitations}
\subsection{Extending for a larger Visualization and Task Space}
As a proof-of-concept tool, we initially focused on incorporating popular chart types commonly used in data analysis. However, during the user study, participants indicated their preference for alternative visual representations.
For instance, P2 frequently employs composite charts to display multiple series on a single chart, mentioning, ``\textit{I use trend lines with scatterplots.}'' 
Composite charts are recognized for their efficiency in comparative analysis tasks~\cite{compare}.
Additionally, a participant (P11) emphasized the need to annotate heatmaps with p-values derived from statistical tests, underscoring the importance of contextual information.
Participants also expressed a desire for enhanced transformation support closely linked to visualization. For instance, P9, a machine learning researcher, stated, ``I typically work with high-dimensional data. Recommending charts alongside dimensional reduction operations could be beneficial.''
To address these valuable insights, future studies could explore expanding support for various visualization types, layouts, and analytical tasks, with a focus not limited to data facts.

\subsection{Limited Support for Interaction and Streaming Data}
In this study, we have implemented tracking the charts across different nodes. 
Extending interactions to encompass different nodes is a potential avenue for further exploration.
However, there are significant challenges associated with tracking data item changes across nodes, commonly referred to as data lineage. It's important to note that this remains an unsolved problem in the field of data science, as acknowledged in previous work~\cite{pandastutor}.
Another limitation of our tool stems from our framework's design, where charts are generated once the execution of a cell is complete.
In the context of machine learning, such as when training a model with a \code{for} loop, the execution process can be quite time-consuming. When dealing with a \code{for} loop, we specifically track the tables before and after the loop. Any tables generated within the loop are extracted, but we do not provide visualization recommendations for them.
In cases where the \code{for} loop execution is prolonged, users may have a keen interest in monitoring the changes occurring during the loop execution, akin to the functionality offered by TensorBoard. However, it's worth noting that as P9's feedback, ``\textit{the charts in TensorBoard are not satisfactory and require programming efforts}.''
Based on this feedback, the participants in our study advocate for further integration, particularly for streaming data in machine learning. This indicates a desire for improved tools that facilitate real-time monitoring and visualization of data transformations during extended machine-learning processes.

