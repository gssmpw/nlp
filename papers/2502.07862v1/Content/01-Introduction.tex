
% \mani{Use something like "quality of information" rather than fidelity. high QOI samples vs low QOI samples. }
% \mani{Some quality issues are invertible (calibrate away bias, others are not). Focus on variants of noise that can't be undone, but others can be undone}

% \mani{Structure Intro: With subheadings and bullets}
% \mani{ See if we can have example of complementary vs redundant info (move to discussion) if not able to evaluate}

% \mani{Modality A and B are strictly complementary: Give totally independent info. Should we give more resources to a modality that is more noisy}

\section{Introduction}
% Why is multimodal important
\textbf{Background:} Multimodal deep learning systems fusing sensory data from various modalities are the standard for accurate, robust sensing~\cite{chen2022multimodal, eitel2015robustness}. The robustness of multimodality arises from \emph{redundant information} captured across modalities, which mitigates the effect of sensor failure, noise corruption, and adverse environmental conditions \cite{lin2023missmodal, liu2022emotion}. Accordingly, these multimodal systems are invaluable in highly dynamic environments, where a given input modality's quality-of-information (QoI) can vary drastically across samples. QoI refers to the information content of the sensor data, where noise-corrupted modalities would be classified as low-QoI. Fluctuations in a modality's QoI can occur over long periods of time (e.g., lighting conditions over the day), or rapidly (e.g., battlefield settings or unstable sensor feeds). 


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/Intro_Figure.pdf}
        \vspace{-0.15in}
    \caption{Overview of \name. Variable depth backbones adapt to both changing compute resources and input noise characteristics}
    \vspace{-0.15in}
    \label{fig:intro_figure}
\end{figure}


\textbf{Challenges:}
Although multimodal robustness allows these deep learning systems to deal with highly variable QoI, the first challenge surrounds \emph{efficiency of such systems}. State-of-the-art multimodal networks employ \emph{static provisioning} in which inputs proceed through a fixed computational graph established by the architecture \cite{wang2024multimodal, yin2024fusion}. Consequently, each modality's data is fully processed by the network with no regard to variable input QoI, and valuable compute resources may be wasted on low-QoI modalities. In particular, systems with considerable energy or latency constraints will suffer greatly from this misallocation. \emph{We hypothesize that flexibly allocating computational resources among modalities in accordance to each modality's QoI on a per-sample basis can greatly boost model performance in compute-limited settings.} 

In addition to its inability to adapt to the varying QoI of the input modalities, static provisioning also struggles with the second challenge of \emph{dynamic compute resources}. The highly dynamic, real-world environments in which multimodal systems are particularly relevant tend to also suffer from variable computing resource availability over time. For instance, the deployment platforms can be affected by thermal throttling, energy fluctuations, or multi-tenancy. Unfortunately, statically provisioned models are unable to adjust their resource usage to meet dynamic compute resource constraints. A naive solution uses \emph{model selection} in which several models of different sizes are trained and deployed for various levels of compute resource availability. Aside from the obvious drawback of requiring an unreasonable amount of training resources, it also complicates the standard practice of initializing multimodal networks with pretrained weights prior to finetuning \cite{manzoor2023survey}. Publicly available pretrained weights only exist for a few configurations of model size, with fewer options for uncommon sensing modalities. \emph{We hypothesize that a single network, initialized with pretrained weights, which can dynamically adjust its resource usage, offers an effective solution to the challenge of fluctuating compute resources.}






% \mani{In the last sentence above it is not clear what is it that you are "shrinking". Computing requirements? What happened to allocation across modalities?}

% \mani{it might help to explicity call out the two italic sentences in the last two paragraphs above as two challenges}

% \mani{While the background and objective are clear, the research questions and associated hypotheses are not - so the text below seems more like something you stumbled on as opposed to be following a scientific process of question, associated hypothesis, and its validation or invalidation}.

% \textbf{Current Approaches:} 
% While several works minimize runtime inefficiencies through dynamic resource allocation, they fail to address the previous challenges. \emph{Early-Exit} systems solve an alternative problem of prematurely exiting on \emph{simple inputs}~\cite{meng2022adavit, teerapittayanon2016branchynet}. This paradigm is incompatible with \emph{noisy} inputs, as the entire model will execute due to low confidence. \emph{BlockDrop} \cite{wu2018blockdrop} dynamically adjusts a unimodal network according to input QoI through a controller trained with reinforcement learning, but acknowledges the challenge of convergence with such a method. \emph{LayerDrop} \cite{fan2019reducing} trains text transformers robust to various layer allocations at inference time, but addresses neither multimodality nor allocating layers depending on input QoI. \emph{There do not exist any works that jointly tackle the two previous challenges.}

\textbf{Proposed Solution:}
We propose \name, an \textbf{A}daptive \textbf{D}epth \textbf{M}ultimodal \textbf{N}etwork jointly tackling the challenges of adaptation to both dynamic compute resources and variable QoI inputs. While these challenges are agnostic to the multimodal fusion method (e.g., data-level \cite{kim2021vilt}, embedding-level \cite{jeong2024gdtm}, and late \cite{samplawski2023heteroskedastic}), we focus specifically on embedding-level fusion due to applicability and ease of implementation. Figure \ref{fig:intro_figure} provides a high-level depiction of \name. Following the standard for embedding-level fusion, each modality is processed with an independent backbone before undergoing fusion with a transformer encoder. 

First, \name addresses the challenge of \emph{dynamic compute resources} with adaptive backbones containing adjustable layer configurations. With the same set of model weights, \name activates a subset of backbone layers according to the available compute resources. We accomplish this by training large backbones initialized with pretrained weights while stochastically dropping layers in every modality's backbone through the LayerDrop technique \cite{fan2019reducing}. We extend the unimodal, text-only LayerDrop technique to not only Vision Transformers, but also multimodal networks. Such a strategy produces a novel multimodal network whose backbones are resilient to missing layers at test-time, allowing for the usage of fewer layers during resource scarcity. 

Second, given a total layer budget established by the available resources, \name addresses the challenge of \emph{dynamic QoI} by adapting the choice of selected backbone layers according to each modality's QoI. Our approach trains a multimodal controller on top of the adaptive backbones. The controller learns an optimal layer allocation among the backbones, conditioned on the relative QoI across the modalities of a given input sample. We propose a simple training procedure leveraging Gumbel-Softmax Sampling \cite{maddison2016gumbel} and the straight-through estimator \cite{bengio2013estimating} to enable end-to-end training without costly reinforcement learning. 

We benchmark \name against several baselines to demonstrate its ability to preserve accuracy while simultaneously minimizing energy and latency costs. \name is tested on both multimodal localization and multimodal action recognition tasks, reinforcing its generality and applicability. \name can match the performance of larger state-of-the-art models while reducing FLOPS by up to 75\% and latency by up to 60\%. We release our code at \url{https://anonymous.4open.science/r/ADMN-15C9/}. Our contributions are summarized as below:

\begin{itemize}[leftmargin=*,align=left]
% \begin{enumerate}

    \item We present an adaptive multimodal network where resource allocation among modality backbones is dictated by QoI characteristics and current computational resource availability at inference time for every sample.
    
    \item We adapt the LayerDrop technique from its original domain of text transformers to multimodal visual networks by introducing \emph{full-modality dropout}, and demonstrate that this technique can be generalized to diverse tasks such as localization and classification.
    
    \item We design a multimodal controller that can be trained from end-to-end with a simple method to propagate gradients to the controller.
    % instead of complex reinforcement learning.


\item Experiments on multimodal localization and human activity recognition tasks across different modalities demonstrate \name's efficiency.


% \end{enumerate}
\end{itemize}




% These systems frequently leverage neural networks to process and fuse the rich multimodal data due to their ability to approximate complex functions, employing data-level fusion~\cite{kim2021vilt}, embedding-level fusion~\cite{wu2024flexloc}, and late-fusion strategies~\cite{samplawski2023heteroskedastic}. Data-level and late-fusion strategies combine multimodal information at the input and decision layer of the networks, respectively, while embedding-level fusion occurs in the middle of model execution. Although each fusion method shines in its specific use case, embedding-level fusion is often selected as it accommodates diverse modalities while retaining cross-modal information flow~\cite{gadzicki2020fusion}. Unfortunately, embedding-level fusion suffers from two drawbacks when considering runtime efficiency - reliance on fixed, overly complex backbones, and lack of adaptability with respect to the input.
% %Data level fusion aggregates the multimodal sensor data at the beginning of the neural network. The model can make use of inter-modality information, and it is largely employed in situations of modality homogeneity with similar structure. In contrast, embedding-level fusion employs modality specific backbones to independently process sensor data, after which the extracted embeddings are fused and subject to further processing. This method allows for designing backbones specialized for processing one particular modality’s data while still retaining cross-modal reasoning. Finally, late-fusion architectures minimize cross-modal information flow, preferring to process the modalities independently and merge information at the decision level through voting or uncertainty-aware fusion.  



% \textbf{Fixed, Complex Backbones:}  First, embedding-level fusion networks independently process each modality with a deep backbone network~\cite{woo2023towards}. Pretrained off-the-shelf models are a popular choice for such backbones. When applying these pretrained backbones to a specific task in a multimodal setting, there is a concern of \emph{overparametization}. The backbone model may be overly complex for a simple downstream task, and the computation incurred by superfluous operations results in wasted energy and prolonged latency. Imagine a scenario where someone wishes to optimize the backbone size of a multimodal network in simple activity recognition. Although off-the-shelf pretrained backbones of varying sizes do exist, finding one with the optimal size can be challenging, especially for uncommon modalities. Training a specialized backbone from scratch is a solution, but is often infeasible due to compute requirements and time required. The problem is further compounded by the lack of an easy method for users to even determine the optimal backbone size, usually obtained through costly \emph{Network Architecture Search}~\cite{white2023NAS}. Aside from the concern of inefficiency, these fixed, overparameterized backbones may be incompatible with compute-limited devices or scenarios with varying available compute. \emph{Thus, it is highly desirable to establish a dynamic backbone architecture that can be initialized with pretrained weights while shrinking to meet the needs of certain tasks or constraints.}


% \textbf{Varying Quality-of-Input:} Even if the network utilizes optimally sized backbones, fluctuating quality-of-input (QoI) must be considered, where QoI refers to the degree of which an input is corrupted by noise. Realistic multimodal systems often suffer from variable QoI during inference time in which a given modality's QoI is influenced by conditions ranging from sensor failure to environmental changes. Fluctuations in a modality's QoI can occur over long periods of time (e.g., lighting conditions over the day), or rapidly (e.g., battlefield settings, unstable sensor feeds). Current multimodal systems employ static provisioning, in which multimodal samples are fully processed by each backbone with no regard to the variable QoI of each modality. In one sample, modality $m_1$ may be high QoI, but brief corruption of its sensor feed causes the next sample to be low QoI. This rigidity results in the inefficient allocation of resources towards low QoI modalities in every sample. When considering multi-modality, these resources may yield greater benefit when allocated to a different modality for that given sample. This is a particularly relevant consideration for systems that operate under energy or latency critical constraints that seek to maximize accuracy gain per unit of computation. \emph{With variable modality QoI during inference time, it is of great interest to flexibly allocate computational resources among modality backbones in accordance to each modality's QoI on a per-sample basis.} 


% \textbf{Existing Work:} While several works have explored minimizing runtime inefficiencies through dynamic allocation of resources, they do not address the previous challenges. \emph{Early-Exit} systems solve an alternative problem of prematurely exiting a backbone on \emph{simple inputs}, employing a confidence threshold that terminates execution once exceeded~\cite{meng2022adavit, teerapittayanon2016branchynet}. This paradigm is incompatible with \emph{noisy} inputs, as the model will have low confidence and thus use the entire backbone. Moreover, Early-Exit techniques calculate the confidence progressively for each layer of the backbone, which is difficult to scale to multimodal cases. \emph{LayerDrop} \cite{fan2019reducing} proposes a method to train text transformers robust to various layer allocations at inference time, but address neither the case of multimodality nor allocating layers depending on input QoI. \emph{BlockDrop} \cite{wu2018blockdrop} dynamically adjusts a unimodal network through a controller trained with reinforcement learning, but acknowledges the challenge of convergence and instability with reinforcement learning, which will be exacerbated with the large search space of multimodal networks. \emph{There do not exist any works that jointly tackle the two previous challenges}


% \textbf{Proposed Solution: } We propose \name, an \textbf{A}daptive \textbf{D}epth \textbf{M}ultimodal \textbf{N}etwork jointly tackling the two challenges. Figure \ref{fig:intro_figure} provides a high-level depiction of \name. With the same set of backbone weights, \name is capable of activating a subset of layers according to the available compute resources of a given platform, represented by the two different layer budgets. Moreover, \name adapts the choice of selected layers based on each modality's QoI on a per-sample basis. \name accomplishes this through two main components: adaptive backbones with adjustable layer configurations at inference time, and a controller that derives the optimal allocation of resources among backbones from the perceived input QoI. We devise a two-stage training process. First, we train the backbones on the target task while stochastically dropping layers in every modality's backbone. Such a strategy produces a multimodal network whose backbones are resilient to varying layer configurations. The second stage of training takes advantage of this flexibility by training a controller that learns the ideal allocation of backbone layers given input samples with varying degrees of QOI. Our proposed method is end-to-end trainable and avoids costly Reinforcement Learning techniques that suffer from convergence issues. 


% We benchmark \name against several baselines to demonstrate its ability to preserve accuracy while simultaneously minimizing energy and latency costs. \name is tested on both multimodal localization and multimodal action recognition tasks, reinforcing its generality and applicability. \name can match the performance of models while reducing FLOPS by up to 75\% and latency by up to 60\%. Our contributions are summarized as below:

% \begin{enumerate}
%     \item We adapt the LayerDrop ~\cite{fan2019reducing} technique from its original domain of text transformers to multimodal visual networks by introducing \emph{full-modality dropout}. We demonstrate that this technique can generalize to tasks spanning visual localization to classification.
%     \item We propose the first adaptive multimodal network in which allocation of resources among modality backbones is dictated at inference time depending on the noise characteristics of each input modality.
%     \item We showcase \name's utility in scenarios with variable computational resources
%     \item We design a multimodal controller can be trained in an end-to-end fashion without usage of reinforcement learning while proposing a simple method for propagating gradients over top-k sampling
%     %†\item We robustly benchmark \name across several computational budgets and distributions of noise
% \end{enumerate}



