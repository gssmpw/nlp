
%\mani{Present baselines as inspiration from prior work, give upper bound a different name with an oracle strategy}
%\mani{Given the time, show feasbility on two modalities that are very distinct (e.g., sound + IMU)}
% \begin{table*}[t!]
% \centering
% \begin{tabular}{llccccccc}
% \toprule
% \textbf{Noise Type} & \textbf{Layers} & \textbf{Upper Bound} & \textbf{Naive Alloc.} & \textbf{Image Only} & \textbf{Depth Only} & \textbf{Naive Scratch} & \textbf{Controller} \\ 
% \midrule
% \multirow{4}{*}{\textbf{Binary}}     
%     & 6   & \multirow{4}{*}{21.14 $\vert$ 0.73} & 103.95 $\vert$ 0.08 & 63.31 $\vert$ 0.29 & 64.80 $\vert$ 0.36 & 46.60 $\vert$ 0.04  & 24.90 $\vert$ 0.64\\
%     & 8   &                                 & 78.21 $\vert$ 0.31  & 56.92 $\vert$ 0.35 & 52.76 $\vert$ 0.57 & 42.61 $\vert$ 0.04  & 22.50 $\vert$ 0.70\\
%     & 12  &                                 & 27.44 $\vert$ 0.53 & 58.14 $\vert$ 0.38 & 51.49 $\vert$ 0.59 & 48.24 $\vert$ 0.04 & 21.79 $\vert$ 0.73\\
%     & 16  &                                 & 22.11 $\vert$ 0.73 & ---   & ---   & 44.54 $\vert$ 0.04  & 20.76 $\vert$ 0.74\\ 
% \midrule
% \multirow{4}{*}{\textbf{Discrete}}   
%     & 6   & \multirow{4}{*}{29.64 $\vert$ 0.44} & 112.53 $\vert$ 0.06 & 67.49 $\vert$ 0.19 & 85.00 $\vert$ 0.18 & 39.31 $\vert$ 0.04  & 51.78 $\vert$ 0.35\\
%     & 8   &                                     & 97.55  $\vert$ 0.13 & 53.17 $\vert$ 0.19 & 61.70 $\vert$ 0.31 & 36.68 $\vert$ 0.04  & 37.50 $\vert$ 0.41\\
%     & 12  &                                     & 46.92  $\vert$ 0.29 & 53.70 $\vert$ 0.23 & 58.61 $\vert$ 0.33 & 40.40 $\vert$ 0.04  & 32.45 $\vert$ 0.42\\
%     & 16  &                                     & 30.99  $\vert$ 0.43 & ---   & ---   & 37.29 $\vert$ 0.04  & 30.54 $\vert$ 0.43\\ 
% \midrule
% \multirow{4}{*}{\textbf{Continuous}} 
%     & 6   & \multirow{4}{*}{31.63 $\vert$ 0.45} & 117.25 $\vert$ 0.04 & 77.78 $\vert$ 0.08 & 83.76 $\vert$ 0.17 & 49.37 $\vert$ 0.04   & 53.77 $\vert$ 0.32 \\
%     & 8   &                                     & 105.29 $\vert$ 0.11 & 64.71 $\vert$ 0.20 & 58.00 $\vert$ 0.35 & 46.40 $\vert$ 0.04  & 42.23 $\vert$ 0.39 \\
%     & 12  &                                     & 51.69 $\vert$ 0.2 & 64.72 $\vert$ 0.18 & 55.37 $\vert$ 0.35 & 51.60 $\vert$ 0.04  & 36.62 $\vert$ 0.40 \\
%     & 16 &                                     & 33.28 $\vert$ 0.42 & ---   & ---   & 48.35 $\vert$ 0.04  & 33.82 $\vert$ 0.43\\ 
% \bottomrule
% \end{tabular}
% \caption{Localization Error (cm) of \name in comparison to each baselines}
% \label{tab:results}
% \end{table*}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.5}
\begin{table*}[t!]
\centering
\caption{\textcolor{darkgreen}{Left: GDTM Localization Error (cm) $\downarrow$}. \textcolor{darkblue}{Right: MM-Fi Classification Accuracy $\uparrow$}. Best viewed in color}
\begin{tabular}{llccccccc}
\toprule
\textbf{Noise Type} & \textbf{Layers} & \textbf{Upper Bound} & \textbf{\name} & \textbf{Naive Scratch} & \textbf{Naive Alloc.} & \textbf{Image Only} & \textbf{Depth Only} \\ 
\midrule
\multirow{4}{*}{\textbf{Binary}}     
    & 6   & \multirow{4}{*}{\textcolor{darkgreen}{21.1} $\vert$ \textcolor{darkblue}{72.5\%}} & \textcolor{darkgreen}{24.9} $\vert$ \textcolor{darkblue}{64.1\%} & \textcolor{darkgreen}{46.6} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{104.0} $\vert$ \textcolor{darkblue}{7.7\%} & \textcolor{darkgreen}{63.3} $\vert$ \textcolor{darkblue}{29.0\%} & \textcolor{darkgreen}{64.8} $\vert$ \textcolor{darkblue}{36.4\%}\\
    & 8   &                                     & \textcolor{darkgreen}{22.5} $\vert$ \textcolor{darkblue}{69.3\%} & \textcolor{darkgreen}{42.6} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{78.2} $\vert$ \textcolor{darkblue}{31.5\%}  & \textcolor{darkgreen}{56.9} $\vert$ \textcolor{darkblue}{34.9\%} & \textcolor{darkgreen}{52.8} $\vert$ \textcolor{darkblue}{57.1\%}\\
    & 12  &                                     & \textcolor{darkgreen}{21.8} $\vert$ \textcolor{darkblue}{73.1\%} & \textcolor{darkgreen}{48.2} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{27.4} $\vert$ \textcolor{darkblue}{53.1\%} & \textcolor{darkgreen}{58.1} $\vert$ \textcolor{darkblue}{37.7\%} & \textcolor{darkgreen}{51.5} $\vert$ \textcolor{darkblue}{58.6\%}\\
    & 16  &                                     & \textcolor{darkgreen}{20.8} $\vert$ \textcolor{darkblue}{73.6\%} & \textcolor{darkgreen}{44.5} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{22.1} $\vert$ \textcolor{darkblue}{72.8\%} & ---   & ---   \\ 
\midrule
\multirow{4}{*}{\textbf{Discrete}}   
    & 6   & \multirow{4}{*}{\textcolor{darkgreen}{29.6} $\vert$ \textcolor{darkblue}{44.4\%}} & \textcolor{darkgreen}{51.8} $\vert$ \textcolor{darkblue}{35.0\%} & \textcolor{darkgreen}{39.1} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{112.5} $\vert$ \textcolor{darkblue}{5.6\%} & \textcolor{darkgreen}{67.5} $\vert$ \textcolor{darkblue}{18.8\%} & \textcolor{darkgreen}{85.0} $\vert$ \textcolor{darkblue}{17.6\%}\\
    & 8   &                                     & \textcolor{darkgreen}{37.5} $\vert$ \textcolor{darkblue}{40.5\%} & \textcolor{darkgreen}{36.7} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{97.6} $\vert$ \textcolor{darkblue}{13.0\%} & \textcolor{darkgreen}{53.2} $\vert$ \textcolor{darkblue}{18.5\%} & \textcolor{darkgreen}{61.7} $\vert$ \textcolor{darkblue}{30.9\%}\\
    & 12  &                                     & \textcolor{darkgreen}{32.5} $\vert$ \textcolor{darkblue}{41.5\%} & \textcolor{darkgreen}{40.4} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{46.9} $\vert$ \textcolor{darkblue}{29.0\%} & \textcolor{darkgreen}{53.7} $\vert$ \textcolor{darkblue}{23.1\%} & \textcolor{darkgreen}{58.6} $\vert$ \textcolor{darkblue}{32.7\%}\\
    & 16  &                                     & \textcolor{darkgreen}{30.5} $\vert$ \textcolor{darkblue}{42.4\%} & \textcolor{darkgreen}{37.3} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{31.0} $\vert$ \textcolor{darkblue}{42.9\%} & ---   & ---   \\ 
\midrule
\multirow{4}{*}{\textbf{Continuous}} 
    & 6   & \multirow{4}{*}{\textcolor{darkgreen}{31.6} $\vert$ \textcolor{darkblue}{45.1\%}} & \textcolor{darkgreen}{53.8} $\vert$ \textcolor{darkblue}{31.7\%} & \textcolor{darkgreen}{49.4} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{117.3} $\vert$ \textcolor{darkblue}{4.0\%} & \textcolor{darkgreen}{77.8} $\vert$ \textcolor{darkblue}{8.3\%} & \textcolor{darkgreen}{83.8} $\vert$ \textcolor{darkblue}{17.3\%} \\
    & 8   &                                     & \textcolor{darkgreen}{42.2} $\vert$ \textcolor{darkblue}{39.0\%} & \textcolor{darkgreen}{46.4} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{105.3} $\vert$ \textcolor{darkblue}{11.1\%} & \textcolor{darkgreen}{64.7} $\vert$ \textcolor{darkblue}{19.8\%} & \textcolor{darkgreen}{58.0} $\vert$ \textcolor{darkblue}{34.9\%} \\
    & 12  &                                     & \textcolor{darkgreen}{36.6} $\vert$ \textcolor{darkblue}{40.4\%} & \textcolor{darkgreen}{51.6} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{51.7} $\vert$ \textcolor{darkblue}{20.1\%} & \textcolor{darkgreen}{64.7} $\vert$ \textcolor{darkblue}{18.2\%} & \textcolor{darkgreen}{55.4} $\vert$ \textcolor{darkblue}{35.5\%} \\
    & 16 &                                     & \textcolor{darkgreen}{33.8} $\vert$ \textcolor{darkblue}{43.1\%} & \textcolor{darkgreen}{48.4} $\vert$ \textcolor{darkblue}{3.7\%} & \textcolor{darkgreen}{33.3} $\vert$ \textcolor{darkblue}{42.0\%} & ---   & ---   \\ 
\bottomrule
\end{tabular}


\label{tab:results}
\end{table*}


\section{Evaluations}

\subsection{Experimental Setup}

\subsubsection{Datasets}
% We showcase the widespread applicability of \name through experiments on distributed, multimodal localization and multimodal human activity classification. 
The GDTM localization dataset ~\cite{jeong2024gdtm} contains multimodal data (RGB, depth, mmWave radar, multichannel audio) of a small remote-controlled car driving on an indoor track. The dataset is also \emph{distributed}, containing 3 sensor nodes each with a full set of modalities. We specifically leverage the distributed vision and depth data to localize the car. 
For human activity recognition, we use the multimodal MM-Fi \cite{yang2024mm} dataset containing 40 subjects with 27 total activities captured by RGB cameras, depth cameras, mmWave radar, and WiFi. We focus on the visual modalities (RGB, depth) for classification. 
These datasets do not naturally contain varying modality QoI, so we synthetically add Gaussian Noise to each multimodal input and clip the values between 0 and 1 to simulate different QoI. Although we focus specifically on Gaussian Noise due to its simplicity and irreversible nature, we emphasize that \name's design is generalizable and not designed specifically for Gaussian Noise. We define three categories of Gaussian Noise employed during Stage 2 Training: \emph{binary, discrete, and continuous}. 

\noindent\textbf{Binary}: Every modality has an 50\% likelihood of either suffering extreme noise corruption, or being left unmodified. This is represented by adding either $\mathcal{N}(0, 0)$ or $\mathcal{N}(0, \sigma_{i_{max}})$ Gaussian Noise to a given modality, generating various combinations of noisy and clean modalities. The real-world analog involves flickering sensor feeds alternating between clean and severely degraded transmissions. 

\noindent\textbf{Discrete}: We extend the binary case to a larger set of standard deviations encompassing noisy, yet still informative data. We add $N(0, \sigma_{i_j})$ to every modality $i$'s input data. Each modality defines a finite set of $N_i$ standard deviations $\{\sigma_{i_{1}}, \sigma_{i_{2}},\sigma_{i_{3}}...\sigma_{i_{N_i}}\}$ from which $\sigma_{i_j}$ is drawn. This setting is representative of systems deployed in environments with a finite set of conditions (e.g., indoor lighting 25\%, 50\%, 100\%), or even sensors that contain different settings such as ISO levels in cameras. 

\noindent\textbf{Continuous}: Instead of drawing $\sigma_{i_j}$ from a finite set, we instead draw $\sigma_{i_j}$ from a continuous range $[0, \sigma_{i_{max}}]$ defining the allowable range of standard deviations. The continuous case represents highly dynamic noise settings that are difficult to decompose into a finite set of scenarios. 


\subsubsection{Baselines}

We present the following baselines:

\noindent\textbf{Upper Bound:} We do not drop any layers, allocating the full 12 layers to each backbone. This represents the best performance with maximum layer budget. 

\noindent\textbf{Naive Allocation:} This baseline represents input-agnostic allocation. Given a layer budget $L$ and $M$ modalities, we naively allocate $\frac{L}{M}$ layers to each backbone following every-other allocation. 

\noindent\textbf{Image Only:} All $L$ layers are allocated to the Image modality, valid only for $L \le 12$

\noindent\textbf{Depth Only:} All $L$ layers are allocated to the Depth modality, valid only for $L \le 12$.

\noindent\textbf{Naive Scratch:} We train a network \emph{from scratch without LayerDrop} on the downstream task with each backbone containing $\frac{L}{M}$ layers. As this model is statically provisioned, we train a new ``Naive Scratch'' model for every layer budget $L$. Although impractical due to the high training cost, it nonetheless serves a useful comparison. 


\subsubsection{Metrics}
For the localization task, we use the average localization error (measured in cm) as the evaluation metric.
For the classification task, we evaluate performance using classification accuracy, a standard metric that measures the proportion of correctly predicted labels.
In addition, we calculate the FLOPs~(Floating Point Operations) of each network to assess computational efficiency, highlighting the trade-off between performance and resource consumption.



% \subsection{Main Results}

\subsection{Localization Results}

\textbf{Localization Error:} We compare \name's localization error to every baseline tested on the three noise categories in Table \ref{tab:results}, shown in \textcolor{darkgreen}{dark green}. We observe the greatest boost in performance in the Binary case, where the model achieves localization performance competitive with the Upper Bound with only 6 layers. This demonstrates that the controller has correctly allocated resources towards high QoI modalities in every sample. In comparison, the high error of the baselines reveals how input-aware layer allocation is invaluable in low-compute settings. The Discrete case, which is an extension of the Binary case with various levels of noise, also benefits from introducing \name. The 6 layer allocation incurs higher error, but the error sharply drops as we move to 8 layers. Interestingly, the Naive Scratch baseline outperforms \name in the 6 and 8 layer Discrete case, but fails to maintain its advantage at larger layer allocations. We hypothesize that the Naive Scratch models are more competitive at smaller layer budgets as proper initialization with pretrained weights has a greater impact in deeper networks. We observe similar trends in the Continuous case. 


\begin{figure}[t]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\linewidth]{Figures/GDTM_Latency.png}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\linewidth]{Figures/GDTM_Flops.png}
    \end{subfigure}
        \vspace{-0.1in}
    \caption{Latency (ms) and GFLOPs vs Layers for GDTM}
    \label{fig:localize_compute}
    \vspace{-0.15in}
\end{figure}

\textbf{Localization Compute: } Figure \ref{fig:localize_compute} shows the meaningful reductions in floating-point operations and latency when employing \name. Although the controller accounts for a significant proportion of the latency at smaller layer budgets ($\sim 20\%$), the high throughput at these latencies ($>150$ fps) surpasses most sensor sampling rates. Moreover, \name's controller utilizes a negligible amount of FLOPs. The controller consumes only 0.26 GFLOPs, which constitutes about 1\% of the model's total operations at \emph{the fewest allocation of 6 layers.} When viewing Table \ref{tab:results} in context of these metrics, we can observe the significance of \name. In the Binary case, \name localizes within 4 cm of the Upper Bound while reducing latency by $\sim$60\% and FLOPs by $\sim$75\%. With the more complex Discrete case, employing 8 layer allocation incurs 8 cm of additional error, but cuts latency and FLOPs by $\sim$50\% and $\sim$65\%, respectively. Even in the difficult Continuous case, using a 12 layer allocation reduces latency and FLOPs by 35\% and 50\%, respectively, with only 5 cm of additional error. 

\begin{figure}[t]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\linewidth]{Figures/MMFI_Latency.png}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.48\linewidth]{Figures/MMFI_Flops.png}
    \end{subfigure}
        \vspace{-0.1in}
    \caption{Latency (ms) and GFLOPs vs Layers for MM-Fi}
    \label{fig:classify_compute}
        % \vspace{-0.1in}
\end{figure}

\subsection{Classification Results}

\textbf{Classification Accuracy:} \name's classification accuracy on the MM-Fi dataset is shown in \textcolor{darkblue}{dark blue} in Table \ref{tab:results}. Similarly to its localization performance, \name exhibits significant gains in the Binary noise case over the baselines, outperforming the second best Depth-only baseline by 28\% with a 6 layer budget. We observe similar trends in the Discrete and Continuous cases. Notably, one key difference in classification is the poor performance of the Naive Scratch model. Regardless of the number of layers, training the model purely from scratch did not converge, highlighting the importance of prior weight initialization. With \name, one can initialize a full-size model with pretrained weights, enabling easier convergence, and then simply trim the model size by disabling layers. 

\textbf{Classification Compute:} We observe in Figure \ref{fig:classify_compute} that the total GFLOPs are once again dominated by the main network, with the controller accounting for only 0.2\% of the total GFLOPs at the smallest configuration of 6 layers, and 0.07\% with 16 layers. The controller constitutes a larger proportion of total latency, but the accuracy gain of QoI-aware allocation far outweighs the small latency increase.  


\subsection{Additional Modalities}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/Three_Mod_CDF.png}
        \vspace{-0.1in}
    \caption{Localization Error CDF with Three Modalities}
    \label{fig:three_mod_cdf}
        \vspace{-0.15in}
\end{figure}
To ensure that \name can generalize beyond two vision-based modalities, we incorporate mmWave radar and perform localization on GDTM with RGB, depth, and mmWave modalities. Figure \ref{fig:three_mod_cdf} shows a CDF of the error when tested on the 12-Layer Binary case, demonstrating that \name retains good performance with three modalities that contain significant heterogeneity.


\subsection{Dynamic Compute Constraints}
One drawback is that the \name controller is trained only for one particular layer budget $L$. Consequently, we must employ multiple controllers to adapt to \emph{dynamic test-time compute requirements} induced by factors such as thermal throttling. Nevertheless, the training overhead for the controllers is low (Appendix \ref{subsec:controller_overhead}), enabling a controller to be trained for every layer budget. As the main network is frozen during controller training, all the controllers are compatible with the same set of main network weights. Additionally, the controller constitutes only 2.3\% and 3.2\% of the total network parameters in the localization and classification tasks, respectively, allowing for easy storage on disk. 
As a result, \name is highly capable of tackling fluctuating compute requirements. Imagine a scenario where due to thermal throttling, 25\% of execution time lies in the 6 layer regime while the rest allows for 16 layers. Assuming that the system must function at all times, a statically provisioned model has two choices - use a six layer model at all times, or separately train and store a 6 layer model and 16 layer model. The first scenario sacrifices accuracy, while the second scenario not only requires significant investment in training time, but also must store two separate, large models in memory. 



\subsection{Ablation Study}
\label{subsec:layedrop_results}

\textbf{Efficacy of LayerDrop: } LayerDrop is integrated into two stages - initial MAE pretraining on ImageNet, and subsequent finetuning on the desired task. Figure \ref{fig:layerdrop_plot} showcases the localization error on the GDTM dataset as the number of dropped layers increases in separate unimodal depth and image networks. We observe that adding LayerDrop during finetuning has the greatest impact, but benefit is observed in both cases, allowing us to drop a meaningful amount of layers with negligible degradation. These results confirm that LayerDrop is compatible with the ViT architecture, MAE pretraining, and is also effective when finetuned on another task. We present additional results in Appendix \ref{appendix:layerdrop}. 




%\subsection{Qualitative Results}