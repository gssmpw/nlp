\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{Controller Training Overhead}
\label{subsec:controller_overhead}
We found it sufficient to train the controller for 10 and 15 epochs on the localization and classification tasks, respectively. We attribute this to the simple end-to-end training recipe in which we avoid complex reinforcement learning. On a Nvidia RTX 4090, training the localization controller took only 27 min. 


% Add the following packages to your preamble:
% \usepackage{booktabs}

\begin{table*}[h]
\centering
\begin{tabular}{lcccccccc}
\toprule
\textbf{Noise Type}       & \textbf{Seed} & \textbf{6 Layers} & \textbf{8 Layers} & \textbf{12 Layers} & \textbf{16 Layers} \\ 
\midrule
\multirow{3}{*}{\textbf{Binary}} 
                    & 100           & 25.62             & 22.40             & 21.37              & 20.87              \\
                    & 200           & 24.49             & 22.56             & 21.88              & 20.66              \\
                    & 300           & 24.60             & 22.56             & 22.10              & 20.75              \\ 
\midrule
\multirow{3}{*}{\textbf{Discrete}} 
                    & 100           & 51.96             & 36.35             & 32.24              & 29.38              \\
                    & 200           & 47.29             & 39.12             & 33.01              & 30.95              \\
                    & 300           & 56.09             & 37.02             & 32.11              & 31.27              \\ 
\midrule
\multirow{3}{*}{\textbf{Continuous}} 
                    & 100           & 54.47             & 42.92             & 36.18              & 32.55              \\
                    & 200           & 53.25             & 42.20             & 36.96              & 33.66              \\
                    & 300           & 53.59             & 41.56             & 36.73              & 35.25              \\ 
\bottomrule
\end{tabular}
\caption{\name Localization Error (cm) $\downarrow$. Shown previously in Table \ref{tab:results} with averaging}
\label{tab:admn_full_loc}
\end{table*}



% Add the following packages to your preamble:
% \usepackage{booktabs}
% Add the following packages to your preamble:
% \usepackage{booktabs}
% Add the following packages to your preamble:
% \usepackage{booktabs}

\begin{table}[h]
\centering
\begin{tabular}{lcccccc}
\toprule
\textbf{Task}       & \textbf{Seed} & \textbf{6 Layers} & \textbf{8 Layers} & \textbf{12 Layers} & \textbf{16 Layers} \\ 
\midrule
\multirow{3}{*}{\textbf{Finite}}     
                    & 100           & 73.60             & 64.32             & 56.15              & 21.54              \\
                    & 200           & 98.77             & 49.90             & 24.30              & 22.17              \\
                    & 300           & 62.36             & 63.68             & 23.23              & 21.90              \\ 
\midrule
\multirow{3}{*}{\textbf{Discrete}}   
                    & 100           & 118.14            & 90.36             & 73.87              & 32.15              \\
                    & 200           & 94.43             & 58.66             & 50.76              & 51.32              \\
                    & 300           & 103.21            & 82.20             & 51.84              & 29.81              \\ 
\midrule
\multirow{3}{*}{\textbf{Continuous}} 
                    & 100           & 87.90             & 55.42             & 75.52              & 36.51              \\
                    & 200           & 96.32             & 118.54            & 42.46              & 31.16              \\
                    & 300           & 78.65             & 86.94             & 78.57              & 34.23              \\ 
\bottomrule
\end{tabular}
\caption{Straight-Through Estimator Localization Error (cm) $\downarrow$}
\label{tab:st_full_loc}
\end{table}


\subsection{Justification of Gradient Propagation Technique in the Controller}
\label{subsec:grad_justify}
\textbf{Directly Employing the Straight-Through Estimator:}
\name utilizes the combination of standard temperature Gumbel-Softmax sampling and the straight-through estimator to propagate gradients over the discretization to the continuous logits. One natural question is whether Gumbel-Softmax Sampling is necessary, as one could theoretically apply discretization on the raw logits and propagate gradients with the straight-through estimator. In Table \ref{tab:admn_full_loc} and Table \ref{tab:st_full_loc}, we present the localization results on the GTDM dataset across different layer configurations and noise categories, with three seeds for each experiment. The results highlight that Gumbel-Softmax sampling plays an important role in model training.

This behavior can be attributed to several reasons. First, by applying the softmax function to the logits, we convert them into probability values where one logit's high probabilities come at the expense of the others. As a result, the softmax function encourages the controller to select only the $L$ best performing layers for some value of noise and minimize the probability of the remaining layers. Additionally, utilizing the Gumbel distribution also introduces \emph{stochasticity} into the sampling process. Instead of always selecting the top-L logits as the active layers, the stochasticity intuitively serves to encourage \emph{exploration} of different layer configurations. 


\textbf{Progressive Top-L Gumbel Softmax Sampling:} 
Instead of employing the straight-through estimator, one can also utilize repeated Gumbel-Softmax Sampling to emulate discrete top-L sampling. \cite{xie2019reparameterizable} proposed a method to emulate discrete top-L sampling by repeatedly applying the softmax function $L$ times while adjusting the logits each iteration. However, these methods may cause issues when applied to \name. First, methods utilizing Gumbel-Softmax Sampling to emulate discrete distributions typically have to undergo \emph{temperature annealing} \cite{maddison2016gumbel}, where the temperature is slowly decreased until the distribution is approximately categorical. Utilizing annealing can lead to a longer and more complicated training process for the controller. Additionally, the lack of explicit discretization during training may also result in a distribution shift at inference time, where the controller may learn to overrely upon partially activated layers during training. 


\subsection{Training Details: }\label{sec_appendix_training}

% Add the following packages to your preamble:
% \usepackage{booktabs}
% \usepackage{multicol}
% \usepackage{caption}

\begin{table*}[h]
\centering
\label{tab:training_configs}
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parameter}      & \textbf{Value} \\ 
\midrule
Epochs                  & 400            \\
Learning Rate           & 1E-4       \\
Scheduler               & LinearLR  \\
Optimizer               & Adam           \\
LayerDrop               & 0.2            \\
Fusion Layers           & 6              \\
Fusion Dimension        & 64            \\
Fusion Heads            & 4              \\
Modality Dropout        & 0.1            \\
Depth Noise             & [0, 3]      \\
Image Noise             & [0, 2]         \\
\bottomrule
\end{tabular}
\caption{\textbf{MM-Fi Finetuning}}
\label{tab:mmfi_details}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parameter}      & \textbf{Value} \\ 
\midrule
Epochs                  & 400            \\
Learning Rate           & 5.00E-04       \\
Optimizer               & Adam           \\
LayerDrop               & 0.2            \\
Fusion Layers           & 6              \\
Fusion Dimension        & 256            \\
Fusion Heads            & 4              \\
Modality Dropout        & 0.1            \\
Depth Noise             & [0, 0.75]      \\
Image Noise             & [0, 3]         \\
\bottomrule
\end{tabular}
\caption{\textbf{GDTM Finetuning}}
\label{tab:gdtm_details}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parameter}      & \textbf{Value} \\ 
\midrule
Epochs                  & 10/15             \\
Learning Rate           & 1.00E-03       \\
Scheduler               & LinearLR       \\
Gumbel Temperature      & 1              \\
\bottomrule
\end{tabular}
\caption{\textbf{Controller Training}}
\label{tab:controller_details}
\end{minipage}
\end{table*}



\textbf{MM-Fi Classification: } We depict the Stage 1 details for the MMFI Classification main network in Table \ref{tab:mmfi_details}. We employ the same 12 Layer ViT-Base model from pretraining. After we obtain the embeddings from the backbones, they are fused through a stack of Transformer Encoders with the details shown above. We employ a 0.2 LayerDrop rate and add Gaussian Noise with a randomly drawn standard deviation to the input for every batch. The image Gaussian Noise standard deviation is uniformly drawn from the range 0 to 2 and the depth noise is drawn from the range 0 to 3.  

\textbf{GDTM Localization: } The training details are shown in Table \ref{tab:gdtm_details}. Similarly to the MM-Fi Classification Model, we use a Transformer Encoder to perform multimodal fusion. The range of standard deviations for depth is smaller due to increased sensitivity of the depth modality in this dataset. 



\subsection{Additional LayerDrop Results}
\label{appendix:layerdrop}

% Add the following packages to your preamble:
% \usepackage{booktabs}
% Add the following packages to your preamble:
% \usepackage{booktabs}
% Add the following packages to your preamble:
% \usepackage{booktabs}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Removed Layer Indices} & \textbf{Normal Pre} & \textbf{LayerDrop Pre} & \textbf{Normal Pre} & \textbf{LayerDrop Pre} \\
                        & \textbf{+ Normal FT} & \textbf{+ Normal FT}   & \textbf{+ LayerDrop FT} & \textbf{+ LayerDrop FT} \\
\midrule
None           & 81.16\%                & 80.36\%                   & 79.91\%                   & 78.92\%                      \\
6              & 79.99\%                & 79.35\%                   & 79.61\%                   & 78.59\%                      \\
6, 8           & 77.20\%                & 76.77\%                   & 78.68\%                   & 77.72\%                      \\
4, 6, 8        & 72.97\%                & 73.94\%                   & 77.64\%                   & 76.84\%                      \\
2, 4, 6, 8     & 70.01\%                & 71.48\%                   & 76.90\%                   & 76.37\%                      \\
2, 4, 6, 8, 10 & 64.27\%                & 65.07\%                   & 75.44\%                   & 74.70\%                      \\
2, 4, 6, 7, 8, 10   & 33.91\%            & 38.76\%                   & 69.83\%                   & 69.28\%                      \\
1, 2, 4, 6, 7, 8, 10 & 13.74\%            & 23.60\%                   & 66.67\%                   & 66.94\%                      \\
\bottomrule
\end{tabular}
\caption{ImageNet-1K performance with different layer indices removed. Normal refers to a LayerDrop rate of 0, while LayerDrop refers to utilizing a LayerDrop rate of 0.2. Pre indicates the MAE pretraining stage, while FT refers to supervised finetuning on ImageNet-1K. }
\label{tab:layer_removal}
\end{table}

The ViT models are first pretrained on the ImageNet dataset with Masked Autoencoder pretraining, in which we add LayerDrop. To understand its performance on the ImageNet-1K dataset, we perform a subsequent stage of supervised learning on the ImageNet dataset to obtain the validation accuracy. Table \ref{tab:layer_removal} reveals the validation accuracy on the ImageNet-1K dataset with various dropped layers, and with LayerDrop integrated into different stages of training. When comparing the model trained without any usage of LayerDrop to the one in which LayerDrop was employed in both stages, we can observe an accuracy improvement of over 50\% when 7 layers are dropped during inference time. Curiously, given that LayerDrop is added during supervised finetuning, applying MAE pretraining with LayerDrop does not appear to be necessary in ImageNet-1K. However, the results in Figure \ref{fig:layerdrop_plot} showcase that it has an impact on downstream tasks. 

\subsection{Qualitative Results}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/Qualitative_Results_First_Part.png}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/Qualitative_Results_Second_Part.png}
    \caption{Visual Results on the GDTM Dataset highlighting the impact of noise and featuring the controller layer allocation}
    \label{fig:qualitative}
\end{figure}

In Figure \ref{fig:qualitative}, we visually showcase the noise corrupted multimodal inputs, and the corresponding layer allocation output by the \name controller with a budget of 6 layers. Given that the depth modality is naturally lower QoI, we can see that the controller gives preference towards the image modality. When Image Noise Standard Deviation is 3 and Depth Noise Standard Deviation is 0.25, the controller equally allocates layers among the two modalities. However, when the depth is entirely clean, the controller recognizes this and allocates all the layers towards the clean depth modality. These results reveal the intelligent allocation of the \name controller. For stability reasons, the first layer of the backbone is always activated. 

