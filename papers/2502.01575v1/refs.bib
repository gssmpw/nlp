
@article{prosperi_causal_2020,
	title = {Causal inference and counterfactual prediction in machine learning for actionable healthcare},
	volume = {2},
	issn = {2522-5839},
	doi = {10.1038/s42256-020-0197-y},
	language = {en},
	number = {7},
	urldate = {2022-05-27},
	journal = {Nature Machine Intelligence},
	author = {Prosperi, Mattia and Guo, Yi and Sperrin, Matt and Koopman, James S. and Min, Jae S. and He, Xing and Rich, Shannan and Wang, Mo and Buchan, Iain E. and Bian, Jiang},
	month = jul,
	year = {2020},
	pages = {369--375},

}

@misc{meir_pydts_2022,
	title = {{PyDTS}: {A} {Python} {Package} for {Discrete} {Time} {Survival} {Analysis} with {Competing} {Risks}},
	shorttitle = {{PyDTS}},
	abstract = {Time-to-event analysis (survival analysis) is used when the outcome or the response of interest is the time until a pre-speciﬁed event occurs. Time-to-event data are sometimes discrete either because time itself is discrete or due to grouping of failure times into intervals or rounding off measurements. In addition, the failure of an individual could be one of several distinct failure types; known as competing risks (events) data. This work focuses on discrete-time regression with competing events. We emphasize the main difference between the continuous and discrete settings with competing events, develop a new estimation procedure, and present PyDTS, an open source Python package which implements our estimation procedure and other tools for discrete-time-survival analysis with competing risks.},
	language = {en},
	urldate = {2022-06-03},
	publisher = {arXiv},
	author = {Meir, Tomer and Gutman, Rom and Gorfine, Malka},
	month = apr,
	year = {2022},
	note = {Number: arXiv:2204.05731
arXiv:2204.05731 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ishwaran_random_2014,
	title = {Random survival forests for competing risks},
	volume = {15},
	issn = {1468-4357, 1465-4644},
	doi = {10.1093/biostatistics/kxu010},
	abstract = {We introduce a new approach to competing risks using random forests. Our method is fully non-parametric and can be used for selecting event-specific variables and for estimating the cumulative incidence function. We show that the method is highly effective for both prediction and variable selection in high-dimensional problems and in settings such as HIV/AIDS that involve many competing risks.},
	language = {en},
	number = {4},
	urldate = {2022-07-31},
	journal = {Biostatistics},
	author = {Ishwaran, Hemant and Gerds, Thomas A. and Kogalur, Udaya B. and Moore, Richard D. and Gange, Stephen J. and Lau, Bryan M.},
	month = oct,
	year = {2014},
	pages = {757--773},
}

@article{stensrud_separable_2022,
	title = {Separable {Effects} for {Causal} {Inference} in the {Presence} of {Competing} {Events}},
	volume = {117},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2020.1765783},
	abstract = {In time-to-event settings, the presence of competing events complicates the definition of causal effects. Here we propose the new separable effects to study the causal effect of a treatment on an event of interest. The separable direct effect is the treatment effect on the event of interest not mediated by its effect on the competing event. The separable indirect effect is the treatment effect on the event of interest only through its effect on the competing event. Similar to Robins and Richardson’s extended graphical approach for mediation analysis, the separable effects can only be identified under the assumption that the treatment can be decomposed into two distinct components that exert their effects through distinct causal pathways. Unlike existing definitions of causal effects in the presence of competing events, our estimands do not require cross-world contrasts or hypothetical interventions to prevent death. As an illustration, we apply our approach to a randomized clinical trial on estrogen therapy in individuals with prostate cancer. Supplementary materials for this article are available online.},
	language = {en},
	number = {537},
	urldate = {2022-08-09},
	journal = {Journal of the American Statistical Association},
	author = {Stensrud, Mats J. and Young, Jessica G. and Didelez, Vanessa and Robins, James M. and Hernán, Miguel A.},
	month = jan,
	year = {2022},
	pages = {175--183},
}

@article{zhu_recursively_2012,
	title = {Recursively {Imputed} {Survival} {Trees}},
	volume = {107},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2011.637468},
	abstract = {We propose recursively imputed survival tree (RIST) regression for right-censored data. This new nonparametric regression procedure uses a novel recursive imputation approach combined with extremely randomized trees that allows significantly better use of censored data than previous tree based methods, yielding improved model fit and reduced prediction error. The proposed method can also be viewed as a type of Monte Carlo EM algorithm which generates extra diversity in the tree-based fitting process. Simulation studies and data analyses demonstrate the superior performance of RIST compared to previous methods.},
	language = {en},
	number = {497},
	journal = {Journal of the American Statistical Association},
	author = {Zhu, Ruoqing and Kosorok, Michael R.},
	month = mar,
	year = {2012},
	pages = {331--340},
}

@misc{xu_treatment_2022,
	title = {Treatment heterogeneity with survival outcomes},
	abstract = {Estimation of conditional average treatment effects (CATEs) plays an essential role in modern medicine by informing treatment decision-making at a patient level. Several metalearners have been proposed recently to estimate CATEs in an effective and flexible way by re-purposing predictive machine learning models for causal estimation. In this chapter, we summarize the literature on metalearners and provide concrete guidance for their application for treatment heterogeneity estimation from randomized controlled trials' data with survival outcomes. The guidance we provide is supported by a comprehensive simulation study in which we vary the complexity of the underlying baseline risk and CATE functions, the magnitude of the heterogeneity in the treatment effect, the censoring mechanism, and the balance in treatment assignment. To demonstrate the applicability of our findings, we reanalyze the data from the Systolic Blood Pressure Intervention Trial (SPRINT) and the Action to Control Cardiovascular Risk in Diabetes (ACCORD) study. While recent literature reports the existence of heterogeneous effects of intensive blood pressure treatment with multiple treatment effect modifiers, our results suggest that many of these modifiers may be spurious discoveries. This chapter is accompanied by 'survlearners', an R package that provides well-documented implementations of the CATE estimation strategies described in this work, to allow easy use of our recommendations as well as reproduction of our numerical study.},
	language = {en},
	publisher = {arXiv},
	author = {Xu, Yizhe and Ignatiadis, Nikolaos and Sverdrup, Erik and Fleming, Scott and Wager, Stefan and Shah, Nigam},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07758 [stat]},
}

@article{young_causal_2020,
	title = {A causal framework for classical statistical estimands in failure‐time settings with competing events},
	volume = {39},
	issn = {0277-6715, 1097-0258},
	doi = {10.1002/sim.8471},
	abstract = {In failure-time settings, a competing event is any event that makes it impossible for the event of interest to occur. For example, cardiovascular disease death is a competing event for prostate cancer death because an individual cannot die of prostate cancer once he has died of cardiovascular disease. Various statistical estimands have been defined as possible targets of inference in the classical competing risks literature. Many reviews have described these statistical estimands and their estimating procedures with recommendations about their use. However, this previous work has not used a formal framework for characterizing causal effects and their identifying conditions, which makes it difficult to interpret effect estimates and assess recommendations regarding analytic choices. Here we use a counterfactual framework to explicitly define each of these classical estimands. We clarify that, depending on whether competing events are defined as censoring events, contrasts of risks can define a total effect of the treatment on the event of interest, or a direct effect of the treatment on the event of interest not mediated through the competing event. In contrast, regardless of whether competing events are defined as censoring events, counterfactual hazard contrasts cannot generally be interpreted as causal effects. We illustrate how identifying assumptions for all of these counterfactual estimands can be represented in causal diagrams in which competing events are depicted as time-varying covariates. We present an application of these ideas to data from a randomized trial designed to estimate the effect of estrogen therapy on prostate cancer mortality.},
	language = {en},
	number = {8},
	urldate = {2022-08-19},
	journal = {Statistics in Medicine},
	author = {Young, Jessica G. and Stensrud, Mats J. and Tchetgen Tchetgen, Eric J. and Hernán, Miguel A.},
	month = apr,
	year = {2020},
	pages = {1199--1236},

}

@article{prentice_analysis_1978,
	title = {The {Analysis} of {Failure} {Times} in the {Presence} of {Competing} {Risks}},
	volume = {34},
	issn = {0006341X},
	doi = {10.2307/2530374},
	abstract = {Distinctproblemsin theanalysisoffailuretimeswithcompetingcausesoffailureincludethe estimationof treatmentor exposureeffiectson specificfailure types,thestudyof interrelations amongfailure types,and the estimationof failure ratesfor somecausesgiventhe removalof certainotherfailuretypes.Theusualformulationof theseproblemsis in termsof conceptuaol r latentfailuretimesfor eachfailuretype.Thisapproachis criticizedon thebasisof unwarranted assumptionsl,ackofphysicalinterpretatioanndidentifiabilitpyroblemsA. nalternativeapproach utilizingcause-specifichazardfunctionsfor observablequantities,includingtime-dependent covariates,is proposed.Cause-specifichazardfunctionsare shownto be the basic estimable quantitiesin thecompetingrisksframeworkA. method,involvingtheestimationof parameters thatrelatetime-dependenrtiskindicatorfsor somecausesto cause-specifihcazardfunctionsfor othercauses,isproposedfor thestudyof interrelationasmongfailuretypes.Furtheri,t is argued that theproblemof estimationoffailure ratesunderthe removalof certaincausesis not well poseduntila mechanismfor causeremovalis specifiedF. ollowingsucha specificationo, ne will sometimesbe in a positionto makesensibleextrapolationfsrom availabledata to situations involvingcause removal.A clinicalprogramin bone marrowtransplantatiofnor leukemia providesa settingfor discussionandillustrationof eachof theseideas.Failuredueto censoringin a survivorshisptudyleadstofurtherdiscussion.},
	language = {en},
	number = {4},
	urldate = {2022-08-20},
	journal = {Biometrics},
	author = {Prentice, R. L. and Kalbfleisch, J. D. and Peterson, A. V. and Flournoy, N. and Farewell, V. T. and Breslow, N. E.},
	month = dec,
	year = {1978},
	pages = {541},
}

@article{andersen_competing_2012,
	title = {Competing risks in epidemiology: possibilities and pitfalls},
	volume = {41},
	issn = {1464-3685, 0300-5771},
	shorttitle = {Competing risks in epidemiology},
	doi = {10.1093/ije/dyr213},
	abstract = {Background In studies of all-cause mortality, the fundamental epidemiological concepts of rate and risk are connected through a well-defined one-to-one relation. An important consequence of this relation is that regression models such as the proportional hazards model that are defined through the hazard (the rate) immediately dictate how the covariates relate to the survival function (the risk).
Methods This introductory paper reviews the concepts of rate and risk and their one-to-one relation in all-cause mortality studies and introduces the analogous concepts of rate and risk in the context of competing risks, the cause-specific hazard and the cause-specific cumulative incidence function.
Results The key feature of competing risks is that the one-to-one correspondence between cause-specific hazard and cumulative incidence, between rate and risk, is lost. This fact has two important implications. First, the na¨ıve Kaplan–Meier that takes the competing events as censored observations, is biased. Secondly, the way in which covariates are associated with the cause-specific hazards may not coincide with the way these covariates are associated with the cumulative incidence. An example with relapse and non-relapse mortality as competing risks in a stem cell transplantation study is used for illustration.
Conclusion The two implications of the loss of one-to-one correspondence between cause-specific hazard and cumulative incidence should be kept in mind when deciding on how to make inference in a competing risks situation.},
	language = {en},
	number = {3},
	urldate = {2022-08-20},
	journal = {International Journal of Epidemiology},
	author = {Andersen, Per Kragh and Geskus, Ronald B and de Witte, Theo and Putter, Hein},
	month = jun,
	year = {2012},
	pages = {861--870},
}

@article{hernan_hazards_2010,
	title = {The {Hazards} of {Hazard} {Ratios}},
	volume = {21},
	issn = {1044-3983},
	doi = {10.1097/EDE.0b013e3181c1ea43},
	language = {en},
	number = {1},
	urldate = {2022-08-20},
	journal = {Epidemiology},
	author = {Hernán, Miguel A.},
	month = jan,
	year = {2010},
	pages = {13--15},
}

@article{frangakis_principal_2002,
	title = {Principal {Stratification} in {Causal} {Inference}},
	volume = {58},
	issn = {0006341X},
	doi = {10.1111/j.0006-341X.2002.00021.x},
	abstract = {Many scientific problems require that treatment comparisons be adjusted for posttreatment variables, but the estimands underlying standard methods are not causal effects. To address this deficiency, we propose a general framework for comparing treatments adjusting for posttreatment variables that yields principal effects based on principal stratification. Principal stratification with respect to a posttreatment variable is a cross-classification of subjects defined by the joint potential values of that posttreatment variable under each of the treatments being compared. Principal effects are causal effects within a principal stratum. The key property of principal strata is that they are not affected by treatment assignment and therefore can be used just as any pretreatment covariate, such as age category. As a result, the central property of our principal effects is that they are always causal effects and do not suffer from the complications of standard posttreatment-adjusted estimands. We discuss briefly that such principal causal effects are the link between three recent applications with adjustment for posttreatment variables: (i) treatment noncompliance, (ii) missing outcomes (dropout) following treatment noncompliance, and (iii) censoring by death. We then attack the problem of surrogate or biomarker endpoints, where we show, using principal causal effects, that all current definitions of surrogacy, even when perfectly true, do not generally have the desired interpretation as causal effects of treatment on outcome. We go on to formulate estimands based on principal stratification and principal causal effects and show their superiority.},
	language = {en},
	number = {1},
	urldate = {2022-09-04},
	journal = {Biometrics},
	author = {Frangakis, Constantine E. and Rubin, Donald B.},
	month = mar,
	year = {2002},
	pages = {21--29},
}

@article{robins_new_1986,
	title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
	volume = {7},
	issn = {02700255},
	doi = {10.1016/0270-0255(86)90088-6},
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave empioyment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject’s assigned treatment protocol has been erased from the data tile. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated.},
	language = {en},
	number = {9-12},
	urldate = {2022-09-04},
	journal = {Mathematical Modelling},
	author = {Robins, James},
	year = {1986},
	pages = {1393--1512},
}

@article{geskus_data_2016,
	title = {Data {Analysis} with {Competing} {Risks} and {Intermediate} {States}},
	language = {en},
	author = {Geskus, Ronald B},
	year = {2016},
	pages = {276},
        journal={},
}

@article{nevo_causal_2021,
	title = {Causal inference for semi-competing risks data},
	issn = {1468-4357},
	doi = {10.1093/biostatistics/kxab049},
	abstract = {The causal effects of Apolipoprotein E 4 allele (APOE) on late-onset Alzheimer’s disease (AD) and death are complicated to deﬁne because AD may occur under one intervention but not under the other, and because AD occurrence may affect age of death. In this article, this dual outcome scenario is studied using the semi-competing risks framework for time-to-event data. Two event times are of interest: a nonterminal event time (age at AD diagnosis), and a terminal event time (age at death). AD diagnosis time is observed only if it precedes death, which may occur before or after AD. We propose new estimands for capturing the causal effect of APOE on AD and death. Our proposal is based on a stratiﬁcation of the population with respect to the order of the two events. We present a novel assumption utilizing the timeto-event nature of the data, which is more ﬂexible than the often-invoked monotonicity assumption. We derive results on partial identiﬁability, suggest a sensitivity analysis approach, and give conditions under which full identiﬁcation is possible. Finally, we present and implement nonparametric and semiparametric estimation methods under right-censored semi-competing risks data for studying the complex effect of APOE on AD and death.},
	language = {en},
	urldate = {2022-09-05},
	journal = {Biostatistics},
	author = {Nevo, Daniel and Gorfine, Malka},
	month = dec,
	year = {2021},
	pages = {kxab049},
}

@article{xu_bayesian_2022,
	title = {A {Bayesian} nonparametric approach for evaluating the causal effect of treatment in randomized trials with semi-competing risks},
	abstract = {We develop a Bayesian nonparametric (BNP) approach to evaluate the causal effect of treatment in a randomized trial where a nonterminal event may be censored by a terminal event, but not vice versa (i.e., semi-competing risks). Based on the idea of principal stratiﬁcation, we deﬁne a novel estimand for the causal effect of treatment on the nonterminal event. We introduce identiﬁcation assumptions, indexed by a sensitivity parameter, and show how to draw inference using our BNP approach. We conduct simulation studies and illustrate our methodology using data from a brain cancer trial. The R code implementing our model and algorithm is available for download at https://github.com/YanxunXu/BaySemiCompeting.},
	language = {en},
	author = {Xu, Yanxun and Scharfstein,, Daniel and Muller, Peter and Daniels, Michael},
	year = {2022},
	pages = {16},
        journal = {},
}

@misc{comment_survivor_2019,
	title = {Survivor average causal effects for continuous time: a principal stratification approach to causal inference with semicompeting risks},
	shorttitle = {Survivor average causal effects for continuous time},
	abstract = {In semicompeting risks problems, nonterminal time-to-event outcomes such as time to hospital readmission are subject to truncation by death. These settings are often modeled with illness-death models for the hazards of the terminal and nonterminal events, but evaluating causal treatment eﬀects with hazard models is problematic due to conditioning on survival– a post-treatment outcome– that is embedded in the deﬁnition of a hazard. Extending an existing survivor average causal eﬀect (SACE) estimand, we frame the evaluation of treatment eﬀects in the context of semicompeting risks with principal stratiﬁcation and introduce two new causal estimands: the time-varying survivor average causal eﬀect (TV-SACE) and the restricted mean survivor average causal eﬀect (RM-SACE). These principal causal eﬀects are deﬁned among units that would survive regardless of assigned treatment. We adopt a Bayesian estimation procedure that parameterizes illness-death models for both treatment arms. We outline a frailty speciﬁcation that can accommodate within-person correlation between nonterminal and terminal event times, and we discuss potential avenues for adding model ﬂexibility. The method is demonstrated in the context of hospital readmission among late-stage pancreatic cancer patients.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Comment, Leah and Mealli, Fabrizia and Haneuse, Sebastien and Zigler, Corwin},
	month = feb,
	year = {2019},
	note = {arXiv:1902.09304 [stat]},
	keywords = {Statistics - Methodology, Statistics - Applications},
}

@article{royston_restricted_2013,
	title = {Restricted mean survival time: an alternative to the hazard ratio for the design and analysis of randomized trials with a time-to-event outcome},
	volume = {13},
	issn = {1471-2288},
	shorttitle = {Restricted mean survival time},
	doi = {10.1186/1471-2288-13-152},
	abstract = {Background: Designs and analyses of clinical trials with a time-to-event outcome almost invariably rely on the hazard ratio to estimate the treatment effect and implicitly, therefore, on the proportional hazards assumption. However, the results of some recent trials indicate that there is no guarantee that the assumption will hold. Here, we describe the use of the restricted mean survival time as a possible alternative tool in the design and analysis of these trials.
Methods: The restricted mean is a measure of average survival from time 0 to a specified time point, and may be estimated as the area under the survival curve up to that point. We consider the design of such trials according to a wide range of possible survival distributions in the control and research arm(s). The distributions are conveniently defined as piecewise exponential distributions and can be specified through piecewise constant hazards and time-fixed or time-dependent hazard ratios. Such designs can embody proportional or non-proportional hazards of the treatment effect.
Results: We demonstrate the use of restricted mean survival time and a test of the difference in restricted means as an alternative measure of treatment effect. We support the approach through the results of simulation studies and in real examples from several cancer trials. We illustrate the required sample size under proportional and non-proportional hazards, also the significance level and power of the proposed test. Values are compared with those from the standard approach which utilizes the logrank test.
Conclusions: We conclude that the hazard ratio cannot be recommended as a general measure of the treatment effect in a randomized controlled trial, nor is it always appropriate when designing a trial. Restricted mean survival time may provide a practical way forward and deserves greater attention.},
	language = {en},
	number = {1},
	urldate = {2022-09-05},
	journal = {BMC Medical Research Methodology},
	author = {Royston, Patrick and Parmar, Mahesh KB},
	month = dec,
	year = {2013},
	pages = {152},
}

@article{athey_generalized_2019,
	title = {Generalized random forests},
	volume = {47},
	issn = {0090-5364},
	doi = {10.1214/18-AOS1709},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {The Annals of Statistics},
	author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
	month = apr,
	year = {2019},
}

@article{athey_estimating_2019,
	title = {Estimating {Treatment} {Effects} with {Causal} {Forests}: {An} {Application}},
	volume = {5},
	issn = {2767-3324},
	shorttitle = {Estimating {Treatment} {Effects} with {Causal} {Forests}},
	doi = {10.1353/obs.2019.0001},
	abstract = {We apply causal forests to a dataset derived from the National Study of Learning Mindsets, and discusses resulting practical and conceptual challenges. This note will appear in an upcoming issue of Observational Studies, Empirical Investigation of Methods for Heterogeneity, that compiles several analyses of the same dataset.},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {Observational Studies},
	author = {Athey, Susan and Wager, Stefan},
	year = {2019},
	pages = {37--51},
}

@article{chernozhukov_doubledebiased_2018,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	issn = {1368-4221, 1368-423X},
	doi = {10.1111/ectj.12097},
	language = {en},
	number = {1},
	urldate = {2022-09-05},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	month = feb,
	year = {2018},
	pages = {C1--C68},
}

@article{fine_proportional_1999,
	title = {A {Proportional} {Hazards} {Model} for the {Subdistribution} of a {Competing} {Risk}},
	volume = {94},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1999.10474144},
	language = {en},
	number = {446},
	journal = {Journal of the American Statistical Association},
	author = {Fine, Jason P. and Gray, Robert J.},
	month = jun,
	year = {1999},
	pages = {496--509},
}

@article{funk_doubly_2010,
	title = {Doubly {Robust} {Estimation} of {Causal} {Effects}},
	author = {Funk, Michele Jonsson and Westreich, Daniel and Wiesen, Chris and Sturmer, Til and Brookhart, M. Alan and Davidian, Marie},
	year = {2010},
	file = {kwq439.pdf:/Users/tomer/Zotero/storage/6YR2G7MG/kwq439.pdf:application/pdf},
    journal = {},
}

@article{hammer_trial_1996,
	title = {A {Trial} {Comparing} {Nucleoside} {Monotherapy} with {Combination} {Therapy} in {HIV}-{Infected} {Adults} with {CD4} {Cell} {Counts} from 200 to 500 per {Cubic} {Millimeter}},
	volume = {335},
	issn = {0028-4793, 1533-4406},
	doi = {10.1056/NEJM199610103351501},
	abstract = {Background This double-blind study evaluated treatment with either a single nucleoside or two nucleosides in adults infected with human immunodeficiency virus type 1 (HIV-1) whose CD4 cell counts were from 200 to 500 per cubic millimeter.
Methods We randomly assigned 2467 HIV-1–infected patients (43 percent without prior antiretroviral treatment) to one of four daily regimens: 600 mg of zidovudine; 600 mg of zidovudine plus 400 mg of didanosine; 600 mg of zidovudine plus 2.25 mg of zalcitabine; or 400 mg of didanosine. The primary end point was a у50 percent decline in the CD4 cell count, development of the acquired immunodeficiency syndrome (AIDS), or death.
Results Progression to the primary end point was more frequent with zidovudine alone (32 percent) than with zidovudine plus didanosine (18 percent; relative hazard ratio, 0.50; PϽ0.001), zidovudine plus zalcitabine (20 percent; relative hazard ratio, 0.54; PϽ0.001), or didanosine alone (22 percent; relative hazard ratio, 0.61; PϽ0.001). The relative hazard ratios for progression to an AIDS-defining event or death were 0.64 (P ϭ 0.005) for zidovudine plus didanosine, as compared with zidovudine alone, 0.77 (Pϭ0.085) for zidovudine plus zalcitabine, and 0.69 (Pϭ0.019) for didanosine alone. The relative hazard ratios for death were 0.55 (Pϭ0.008), 0.71 (Pϭ0.10), and 0.51 (Pϭ0.003), respectively. For zidovudine plus zalcitabine, the benefits were limited to those without previous treatment.
Conclusions Treatment with zidovudine plus didanosine, zidovudine plus zalcitabine, or didanosine alone slows the progression of HIV disease and is superior to treatment with zidovudine alone. Antiretroviral therapy can improve survival in patients with 200 to 500 CD4 cells per cubic millimeter. (N Engl J Med 1996;335:1081-90.)},
	language = {en},
	number = {15},
	urldate = {2022-09-19},
	journal = {New England Journal of Medicine},
	author = {Hammer, Scott M. and Katzenstein, David A. and Hughes, Michael D. and Gundacker, Holly and Schooley, Robert T. and Haubrich, Richard H. and Henry, W. Keith and Lederman, Michael M. and Phair, John P. and Niu, Manette and Hirsch, Martin S. and Merigan, Thomas C.},
	month = oct,
	year = {1996},
	pages = {1081--1090},
}

@article{tian_simple_2014,
	title = {A {Simple} {Method} for {Estimating} {Interactions} {Between} a {Treatment} and a {Large} {Number} of {Covariates}},
	volume = {109},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2014.951443},
	language = {en},
	number = {508},
	urldate = {2022-09-19},
	journal = {Journal of the American Statistical Association},
	author = {Tian, Lu and Alizadeh, Ash A. and Gentles, Andrew J. and Tibshirani, Robert},
	month = oct,
	year = {2014},
	pages = {1517--1532},
}

@misc{jacob_cross-fitting_2020,
	title = {Cross-{Fitting} and {Averaging} for {Machine} {Learning} {Estimation} of {Heterogeneous} {Treatment} {Effects}},
	abstract = {We investigate the ﬁnite sample performance of sample splitting, cross-ﬁtting and averaging for the estimation of the conditional average treatment eﬀect. Recently proposed methods, so-called metalearners, make use of machine learning to estimate diﬀerent nuisance functions and hence allow for fewer restrictions on the underlying structure of the data. To limit a potential overﬁtting bias that may result when using machine learning methods, cross-ﬁtting estimators have been proposed. This includes the splitting of the data in diﬀerent folds to reduce bias and averaging over folds to restore eﬃciency. To the best of our knowledge, it is not yet clear how exactly the data should be split and averaged. We employ a Monte Carlo study with diﬀerent data generation processes and consider twelve diﬀerent estimators that vary in sample-splitting, cross-ﬁtting and averaging procedures. We investigate the performance of each estimator independently on four diﬀerent meta-learners: the doubly-robust-learner, R-learner, T-learner and X-learner. We ﬁnd that the performance of all meta-learners heavily depends on the procedure of splitting and averaging. The best performance in terms of mean squared error (MSE) among the sample split estimators can be achieved when applying cross-ﬁtting plus taking the median over multiple diﬀerent sample-splitting iterations. Some meta-learners exhibit a high variance when the lasso is included in the ML methods. Excluding the lasso decreases the variance and leads to robust and at least competitive results.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Jacob, Daniel},
	month = aug,
	year = {2020},
	note = {arXiv:2007.02852 [stat]},
	keywords = {Statistics - Methodology},
}

@article{andersen_interpretability_2012,
	title = {Interpretability and importance of functionals in competing risks and multistate models: {Interpretability} and importance of functionals in competing risks and multistate models},
	volume = {31},
	issn = {02776715},
	shorttitle = {Interpretability and importance of functionals in competing risks and multistate models},
	doi = {10.1002/sim.4385},
	language = {en},
	number = {11-12},
	urldate = {2022-10-29},
	journal = {Statistics in Medicine},
	author = {Andersen, Per Kragh and Keiding, Niels},
	month = may,
	year = {2012},
	pages = {1074--1088},
}

@article{tsiatis_covariate_2008,
	title = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: {A} principled yet flexible approach},
	volume = {27},
	issn = {02776715, 10970258},
	shorttitle = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials},
	doi = {10.1002/sim.3113},
	abstract = {There is considerable debate regarding whether and how covariate adjusted analyses should be used in the comparison of treatments in randomized clinical trials. Substantial baseline covariate information is routinely collected in such trials, and one goal of adjustment is to exploit covariates associated with outcome to increase precision of estimation of the treatment effect. However, concerns are routinely raised over the potential for bias when the covariates used are selected post hoc; and the potential for adjustment based on a model of the relationship between outcome, covariates, and treatment to invite a “fishing expedition” for that leading to the most dramatic effect estimate. By appealing to the theory of semiparametrics, we are led naturally to a characterization of all treatment effect estimators and to principled, practically-feasible methods for covariate adjustment that yield the desired gains in efficiency and that allow covariate relationships to be identified and exploited while circumventing the usual concerns. The methods and strategies for their implementation in practice are presented. Simulation studies and an application to data from an HIV clinical trial demonstrate the performance of the techniques relative to existing methods.},
	language = {en},
	number = {23},
	urldate = {2022-11-09},
	journal = {Statistics in Medicine},
	author = {Tsiatis, Anastasios A. and Davidian, Marie and Zhang, Min and Lu, Xiaomin},
	month = oct,
	year = {2008},
	pages = {4658--4677},
}

@article{zhang_improving_2008,
	title = {Improving {Efficiency} of {Inferences} in {Randomized} {Clinical} {Trials} {Using} {Auxiliary} {Covariates}},
	volume = {64},
	issn = {0006-341X, 1541-0420},
	doi = {10.1111/j.1541-0420.2007.00976.x},
	abstract = {The primary goal of a randomized clinical trial is to make comparisons among two or more treatments. For example, in a two-arm trial with continuous response, the focus may be on the diﬀerence in treatment means; with more than two treatments, the comparison may be based on pairwise diﬀerences. With binary outcomes, pairwise odds ratios or log odds ratios may be used. In general, comparisons may be based on meaningful parameters in a relevant statistical model. Standard analyses for estimation and testing in this context typically are based on the data collected on response and treatment assignment only. In many trials, auxiliary baseline covariate information may also be available, and it is of interest to exploit these data to improve the eﬃciency of inferences. Taking a semiparametric theory perspective, we propose a broadly applicable approach to adjustment for auxiliary covariates to achieve more eﬃcient estimators and tests for treatment parameters in the analysis of randomized clinical trials. Simulations and applications demonstrate the performance of the methods.},
	language = {en},
	number = {3},
	urldate = {2022-11-09},
	journal = {Biometrics},
	author = {Zhang, Min and Tsiatis, Anastasios A. and Davidian, Marie},
	month = sep,
	year = {2008},
	pages = {707--715},
}

@article{lu_variable_2013,
	title = {Variable selection for optimal treatment decision},
	volume = {22},
	issn = {0962-2802, 1477-0334},
	doi = {10.1177/0962280211428383},
	abstract = {In decision-making on optimal treatment strategies, it is of great importance to identify variables that are involved in the decision rule, i.e. those interacting with the treatment. Effective variable selection helps to improve the prediction accuracy and enhance the interpretability of the decision rule. We propose a new penalized regression framework which can simultaneously estimate the optimal treatment strategy and identify important variables. The advantages of the new approach include: (i) it does not require the estimation of the baseline mean function of the response, which greatly improves the robustness of the estimator; (ii) the convenient loss-based framework makes it easier to adopt shrinkage methods for variable selection, which greatly facilitates implementation and statistical inferences for the estimator. The new procedure can be easily implemented by existing state-of-art software packages like LARS. Theoretical properties of the new estimator are studied. Its empirical performance is evaluated using simulation studies and further illustrated with an application to an AIDS clinical trial.},
	language = {en},
	number = {5},
	urldate = {2022-11-09},
	journal = {Statistical Methods in Medical Research},
	author = {Lu, Wenbin and Zhang, Hao Helen and Zeng, Donglin},
	month = oct,
	year = {2013},
	pages = {493--504},
}

@article{fan_concordance-assisted_2017,
	title = {Concordance-assisted learning for estimating optimal individualized treatment regimes},
	volume = {79},
	issn = {13697412},
	doi = {10.1111/rssb.12216},
	abstract = {In this article, we propose a new concordance-assisted learning for estimating optimal individualized treatment regimes. We first introduce a type of concordance function for prescribing treatment and propose a robust rank regression method for estimating the concordance function. We then find treatment regimes, up to a threshold, to maximize the concordance function, named prescriptive index. Finally, within the class of treatment regimes that maximize the concordance function, we find the optimal threshold to maximize the value function. We establish the convergence rate and asymptotic normality of the proposed estimator for parameters in the prescriptive index. An induced smoothing method is developed to estimate the asymptotic variance of the proposed estimator. We also establish the n1/3-consistency of the estimated optimal threshold and its limiting distribution. In addition, a doubly robust estimator of parameters in the prescriptive index is developed under a class of monotonic index models. The practical use and effectiveness of the proposed methodology are demonstrated by simulation studies and an application to an AIDS data.},
	language = {en},
	number = {5},
	urldate = {2022-11-09},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Fan, Caiyun and Lu, Wenbin and Song, Rui and Zhou, Yong},
	month = nov,
	year = {2017},
	pages = {1565--1582},
}

@article{johnson_mimic-iv_2022,
	title = {{MIMIC}-{IV} (version 2.0)},
	doi = {https://doi.org/10.13026/7vcr-e114},
	journal = {PhysioNet},
	author = {Johnson, Alistair and Bulgarelli, Lucas and Pollard, Tom and Horng, Steven and Celi, Leo Anthony and Mark, Roger},
	month = jun,
	year = {2022},
}

@article{goldberger_physiobank_2000,
	title = {{PhysioBank}, {PhysioToolkit}, and {PhysioNet}: {Components} of a {New} {Research} {Resource} for {Complex} {Physiologic} {Signals}},
	volume = {101},
	issn = {0009-7322, 1524-4539},
	shorttitle = {{PhysioBank}, {PhysioToolkit}, and {PhysioNet}},
	doi = {10.1161/01.CIR.101.23.e215},
	abstract = {Abstract
              —The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet.org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
	language = {en},
	number = {23},
	urldate = {2022-11-11},
	journal = {Circulation},
	author = {Goldberger, Ary L. and Amaral, Luis A. N. and Glass, Leon and Hausdorff, Jeffrey M. and Ivanov, Plamen Ch. and Mark, Roger G. and Mietus, Joseph E. and Moody, George B. and Peng, Chung-Kang and Stanley, H. Eugene},
	month = jun,
	year = {2000},
}

@article{johnson_mimic-iii_2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	issn = {2052-4463},
	doi = {10.1038/sdata.2016.35},
	abstract = {Abstract
            MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	language = {en},
	number = {1},
	urldate = {2022-11-11},
	journal = {Scientific Data},
	author = {Johnson, Alistair E.W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	month = dec,
	year = {2016},
	pages = {160035},
}

@article{robins_estimation_1994,
	title = {Estimation of {Regression} {Coefficients} {When} {Some} {Regressors} are not {Always} {Observed}},
	volume = {89},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1994.10476818},
	language = {en},
	number = {427},
	urldate = {2023-01-31},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
	month = sep,
	year = {1994},
	pages = {846--866},
}

@book{van_der_laan_unified_2003,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Unified {Methods} for {Censored} {Longitudinal} {Data} and {Causality}},
	isbn = {978-1-4419-3055-2 978-0-387-21700-0},
	language = {en},
	urldate = {2023-04-03},
	publisher = {Springer New York},
	author = {van der Laan, Mark J. and Robins, James M.},
	year = {2003},
	doi = {10.1007/978-0-387-21700-0},
}

@book{tsiatis_semiparametric_2006,
	address = {New York},
	series = {Springer series in statistics},
	title = {Semiparametric theory and missing data},
	isbn = {978-0-387-32448-7},
	language = {en},
	publisher = {Springer},
	author = {Tsiatis, Anastasios A.},
	year = {2006},
	keywords = {Missing observations (Statistics), Parameter estimation},
}

@article{chernozhukov_doubledebiasedneyman_2017,
	title = {Double/{Debiased}/{Neyman} {Machine} {Learning} of {Treatment} {Effects}},
	volume = {107},
	issn = {0002-8282},
	doi = {10.1257/aer.p20171038},
	abstract = {Chernozhukov et al. (2016) provide a generic double/de-biased machine learning (ML) approach for obtaining valid inferential statements about focal parameters, using Neyman-orthogonal scores and cross-fitting, in settings where nuisance parameters are estimated using ML methods. In this note, we illustrate the application of this method in the context of estimating average treatment effects and average treatment effects on the treated using observational data.},
	language = {en},
	number = {5},
	urldate = {2023-04-06},
	journal = {American Economic Review},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney},
	month = may,
	year = {2017},
	pages = {261--265},
}


@article{li_competing_2020,
	title = {A {Competing} {Risk} {Analysis} {Model} to {Determine} the {Prognostic} {Value} of {Isolated} {Tumor} {Cells} in {Axillary} {Lymph} {Nodes} for {T1N0M0} {Breast} {Cancer} {Patients} {Based} on the {Surveillance}, {Epidemiology}, and {End} {Results} {Database}},
	volume = {10},
	issn = {2234-943X},
	doi = {10.3389/fonc.2020.572316},
	abstract = {Introduction: Knowledge of the association between isolated tumor cells (ITCs) in breast cancer patients and the outcome is very limited. We aimed to determine the prognostic value of axillary lymph node ITCs for T1N0M0 female breast cancer (FBC) patients.
Methods: Data for T1N0M0 FBC patients staged ITCs negative [pN0(i−)] and positive [pN0(i+)] were extracted from the Surveillance, Epidemiology, and End Results database from 2004 to 2015. Prognostic predictors were identiﬁed by Kaplan–Meier analysis, competing risk model, and Fine–Gray multivariable regression model.
Results: A total of 94,599 subjects were included, 88,632 of whom were staged at pN0(i−) and 5,967 were pN0(i+). Patients staged pN0(i+) had worse breast cancerspeciﬁc survival (BCSS) [hazard ratio (HR): 1.298, 95\% CI = 1.069–1.576, P = 0.003] and higher breast cancer-speciﬁc death (BCSD) rate (Gray’s test, P = 0.002) than pN0(i−) group. In the Fine–Gray multivariable regression analysis, the pN0(i+) group had higher BCSD rate (HR: 1.321, 95\% CI = 1.109–1.575, P = 0.002) than pN0(i−) group. In subgroup analyses, no signiﬁcant difference in BCSD was shown between the chemotherapy and non-chemotherapy subgroup (Gray’s test, P = 0.069) or radiotherapy and non-radiotherapy subgroup (Gray’s test, P = 0.096).
Conclusion: ITC was independently related to the increase of the BCSD rate and could be identiﬁed as a reliable survival predictor for T1N0M0 FBC patients.},
	language = {en},
	urldate = {2023-04-19},
	journal = {Frontiers in Oncology},
	author = {Li, Yijun and Zhang, Huimin and Zhang, Wei and Ren, Yu and Qiao, Yan and Li, Kunlong and Chen, Heyan and Pu, Shengyu and He, Jianjun and Zhou, Can},
	month = sep,
	year = {2020},
	pages = {572316},
}

@article{yu_breast-conserving_2020,
	title = {Breast-{Conserving} {Therapy} {Versus} {Mastectomy} in {Young} {Breast} {Cancer} {Patients} {Concerning} {Molecular} {Subtypes}: {A} {SEER} {Population}-{Based} {Study}},
	volume = {27},
	issn = {1073-2748, 1073-2748},
	shorttitle = {Breast-{Conserving} {Therapy} {Versus} {Mastectomy} in {Young} {Breast} {Cancer} {Patients} {Concerning} {Molecular} {Subtypes}},
	doi = {10.1177/1073274820976667},
	abstract = {Breast-conserving therapy was once a contraindication in young breast cancer patients (aged  40 years). Emerging studies suggest that breast-conserving therapy and mastectomy could achieve similar prognosis in this population. However, the effect of molecular subtype disparity on surgical strategy in these patients remains unclear. Data from 8656 young patients (aged  40 years) diagnosed with invasive breast cancer between in 2010 and 2014 were retrospectively reviewed from the Surveillance, Epidemiology, and End Results database. The Cox proportional hazards model was used to evaluate subtype-dependent relationships between the surgical method and survival. Of the 8656 patients, 4132 (47.7\%) underwent breast-conserving therapy and 4524 (52.3\%) underwent mastectomy. The median follow-up period was 30.0 months. Patients in the breast-conserving therapy group demonstrated better overall survival and breast cancer-specific survival than those in the mastectomy group (both p {\textless} 0.05). Patients with different molecular subtypes exhibited significant differences in overall survival and breast cancer-specific survival (p {\textless} 0.001). Patients with luminal subtypes experienced better overall survival and breast cancer-specific survival than those with the triple-negative subtype. Multivariate analysis revealed that overall mortality risk of the breast-conserving therapy group was lower than that of the mastectomy group among HR(þ)HER-2(-) and HR(-)HER-2(-) patients (overall mortality risk of 36.3\% [adjusted hazard ratio ¼ 0.637 \{95\% confidence interval ¼ 0.448–0.905\}, p ¼ 0.012] and 36.0\% [adjusted hazard ratio ¼ 0.640 \{95\% confidence interval ¼ 0.455–0.901\}, p ¼ 0.010] respectively.) The breast cancerspecific mortality risk was also lower by a percentage similar to that of the overall mortality risk. In the HR(þ)HER-2(þ) group, the surgical method was an independent prognostic factor for breast cancer-specific survival (adjusted hazard ratio ¼ 0.275 [95\% confidence interval ¼ 0.089–0.849], p ¼ 0.025), while there was a trend that patients with breast-conserving therapy had better overall survival than those with mastectomy (p ¼ 0.056). In the HR(-)HER-2(þ) group, no significant difference was observed in overall survival and breast cancer-specific survival (p ¼ 0.791 and p ¼ 0.262, respectively). Breast-conserving therapy resulted in significantly better prognosis in patients with luminal and triple-negative subtypes, while no significant difference was observed in patients with the HER-2 enriched subtype. These results may be helpful in informing clinically precise decision-making for surgery in this population.},
	language = {en},
	number = {1},
	urldate = {2023-04-19},
	journal = {Cancer Control},
	author = {Yu, Ping and Tang, Hailin and Zou, Yutian and Liu, Peng and Tian, Wenwen and Zhang, Kaiming and Xie, Xiaoming and Ye, Feng},
	month = jan,
	year = {2020},
	pages = {107327482097666},
}

@incollection{scholkopf_causality_2022,
	title = {Causality for {Machine} {Learning}},
	abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artiﬁcial intelligence (AI), and for a long time had little connection to the ﬁeld of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the ﬁeld is beginning to understand them.},
	language = {en},
	urldate = {2023-04-21},
	author = {Schölkopf, Bernhard},
	month = feb,
	year = {2022},
	doi = {10.1145/3501714.3501755},
	note = {arXiv:1911.10500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, I.2, I.2, I.5, K.4, I.5, K.4},
	pages = {765--804},
}

@misc{meir_discrete-time_2023,
	title = {Discrete-time {Competing}-{Risks} {Regression} with or without {Penalization}},
	abstract = {Many studies employ the analysis of time-to-event data that incorporates competing risks and right censoring. Most methods and software packages are geared towards analyzing data that comes from a continuous failure time distribution. However, failure-time data may sometimes be discrete either because time is inherently discrete or due to imprecise measurement. This paper introduces a novel estimation procedure for discrete-time survival analysis with competing events. The proposed approach offers two key advantages over existing procedures: ﬁrst, it accelerates the estimation process; second, it allows for straightforward integration and application of widely used regularized regression and screening methods. We illustrate the beneﬁts of our proposed approach by conducting a comprehensive simulation study. Additionally, we showcase the utility of our procedure by estimating a survival model for the length of stay of patients hospitalized in the intensive care unit, considering three competing events: discharge to home, transfer to another medical facility, and in-hospital death.},
	language = {en},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Meir, Tomer and Gorfine, Malka},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01186 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
}

@article{deaton_understanding_2018,
	title = {Understanding and misunderstanding randomized controlled trials},
	volume = {210},
	issn = {02779536},
	doi = {10.1016/j.socscimed.2017.12.005},
	abstract = {Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment eﬀect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more diﬃcult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justiﬁcation is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding ‘external validity’ is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientiﬁc progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientiﬁc knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not ‘what works’, but ‘why things work’.},
	language = {en},
	urldate = {2023-04-21},
	journal = {Social Science \& Medicine},
	author = {Deaton, Angus and Cartwright, Nancy},
	month = aug,
	year = {2018},
	pages = {2--21},
}

@article{kent_personalized_2018,
	title = {Personalized evidence based medicine: predictive approaches to heterogeneous treatment effects},
	issn = {0959-8138, 1756-1833},
	shorttitle = {Personalized evidence based medicine},
	doi = {10.1136/bmj.k4245},
	abstract = {The use of evidence from clinical trials to support decisions for individual patients is a form of “reference class forecasting”: implicit predictions for an individual are made on the basis of outcomes in a reference class of “similar” patients treated with alternative therapies. Evidence based medicine has generally emphasized the broad reference class of patients qualifying for a trial. Yet patients in a trial (and in clinical practice) differ from one another in many ways that can affect the outcome of interest and the potential for benefit. The central goal of personalized medicine, in its various forms, is to narrow the reference class to yield more patient specific effect estimates to support more individualized clinical decision making. This article will review fundamental conceptual problems with the prediction of outcome risk and heterogeneity of treatment effect (HTE), as well as the limitations of conventional (one-variable-at-a-time) subgroup analysis. It will also discuss several regression based approaches to “predictive” heterogeneity of treatment effect analysis, including analyses based on “risk modeling” (such as stratifying trial populations by their risk of the primary outcome or their risk of serious treatment-related harms) and analysis based on “effect modeling” (which incorporates modifiers of relative effect). It will illustrate these approaches with clinical examples and discuss their respective strengths and vulnerabilities.},
	language = {en},
	urldate = {2023-04-21},
	journal = {BMJ},
	author = {Kent, David M and Steyerberg, Ewout and van Klaveren, David},
	month = dec,
	year = {2018},
	pages = {k4245},
}

@article{collins_new_2015,
	title = {A {New} {Initiative} on {Precision} {Medicine}},
	volume = {372},
	issn = {0028-4793, 1533-4406},
	doi = {10.1056/NEJMp1500523},
	language = {en},
	number = {9},
	urldate = {2023-04-21},
	journal = {New England Journal of Medicine},
	author = {Collins, Francis S. and Varmus, Harold},
	month = feb,
	year = {2015},
	pages = {793--795},
}

@book{hernan_causal_2010,
	address = {Boca Raton, Fla., London},
	title = {Causal inference},
	isbn = {978-1-315-37493-2},
	language = {eng},
	publisher = {CRC : Taylor \& Francis [distributor]},
	author = {Hernan, Miguel A. and Robins, James M.},
	year = {2010},
}

@book{pearl_causality_2009,
	address = {Cambridge},
	edition = {2. ed},
	title = {Causality: models, reasoning, and inference},
	isbn = {978-0-511-80316-1 978-0-521-89560-6 978-0-521-77362-1},
	shorttitle = {Causality},
	language = {eng},
	publisher = {Cambridge Univ. Press},
	author = {Pearl, Judea},
	year = {2009},
}

@article{rubin_causal_2005,
	title = {Causal {Inference} {Using} {Potential} {Outcomes}: {Design}, {Modeling}, {Decisions}},
	volume = {100},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Causal {Inference} {Using} {Potential} {Outcomes}},
	doi = {10.1198/016214504000001880},
	language = {en},
	number = {469},
	urldate = {2023-04-21},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B},
	month = mar,
	year = {2005},
	pages = {322--331},
}

@book{imbens_causal_2015,
	address = {New York, NY},
	title = {Causal inference for statistics, social, and biomedical sciences: an introduction},
	isbn = {978-1-139-02575-1},
	shorttitle = {Causal inference for statistics, social, and biomedical sciences},
	abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Imbens, Guido and Rubin, Donald B.},
	year = {2015},
	note = {OCLC: 908060101},
}

@book{kalbfleisch_statistical_2011,
	address = {Hoboken},
	edition = {2nd ed},
	title = {The {Statistical} {Analysis} of {Failure} {Time} {Data}},
	isbn = {978-1-118-03123-0},
	abstract = {Contains additional discussion and examples on left truncation as well as material on more general censoring and truncation patterns. Introduces the martingale and counting process formulation swil lbe in a new chapter. Develops multivariate failure time data in a separate chapter and extends the material on Markov and semi Markov formulations. Presents new examples and applications of data analysis},
	language = {eng},
	publisher = {John Wiley \& Sons},
	author = {Kalbfleisch, J. D. and Prentice, Ross L.},
	year = {2011},
	note = {OCLC: 770869023},
}

@article{athey_recursive_2016,
	title = {Recursive partitioning for heterogeneous causal effects},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1510489113},
	abstract = {In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without “sparsity” assumptions. We propose an “honest” approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the “ground truth” for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90\% confidence intervals, whereas coverage ranges between 74\% and 84\% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7–22\%.},
	language = {en},
	number = {27},
	urldate = {2023-05-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Athey, Susan and Imbens, Guido},
	month = jul,
	year = {2016},
	pages = {7353--7360},
}

@article{foster_subgroup_2011,
	title = {Subgroup identification from randomized clinical trial data},
	volume = {30},
	issn = {02776715},
	doi = {10.1002/sim.4322},
	abstract = {We consider the problem of identifying a subgroup of patients who may have an enhanced treatment effect in a randomized clinical trial, and it is desirable that the subgroup be defined by a limited number of covariates. For this problem, the development of a standard, pre-determined strategy may help to avoid the well-known dangers of subgroup analysis. We present a method developed to find subgroups of enhanced treatment effect. This method, referred to as “Virtual Twins”, involves predicting response probabilities for treatment and control “twins” for each subject. The difference in these probabilities is then used as the outcome in a classification or regression tree, which can potentially include any set of the covariates. We define a measure Q(Â) to be the difference between the treatment effect in estimated subgroup Â and the marginal treatment effect. We present several methods developed to obtain an estimate of Q(Â), including estimation of Q(Â) using estimated probabilities in the original data, using estimated probabilities in newly simulated data, two cross-validation-based approaches and a bootstrap-based bias corrected approach. Results of a simulation study indicate that the Virtual Twins method noticeably outperforms logistic regression with forward selection when a true subgroup of enhanced treatment effect exists. Generally, large sample sizes or strong enhanced treatment effects are needed for subgroup estimation. As an illustration, we apply the proposed methods to data from a randomized clinical trial.},
	language = {en},
	number = {24},
	urldate = {2023-05-03},
	journal = {Statistics in Medicine},
	author = {Foster, Jared C. and Taylor, Jeremy M.G. and Ruberg, Stephen J.},
	month = oct,
	year = {2011},
	pages = {2867--2880},
}

@article{hahn_bayesian_2020,
	title = {Bayesian {Regression} {Tree} {Models} for {Causal} {Inference}: {Regularization}, {Confounding}, and {Heterogeneous} {Effects} (with {Discussion})},
	volume = {15},
	issn = {1936-0975},
	shorttitle = {Bayesian {Regression} {Tree} {Models} for {Causal} {Inference}},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-15/issue-3/Bayesian-Regression-Tree-Models-for-Causal-Inference--Regularization-Confounding/10.1214/19-BA1195.full},
	doi = {10.1214/19-BA1195},
	abstract = {This paper presents a novel nonlinear regression model for estimating heterogeneous treatment eﬀects, geared speciﬁcally towards situations with small eﬀect sizes, heterogeneous eﬀects, and strong confounding by observables. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment eﬀects. First, they can yield badly biased estimates of treatment eﬀects when ﬁt to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the speciﬁcation of the response model, implicitly inducing a covariatedependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over eﬀect heterogeneity. The Bayesian causal forest model permits treatment eﬀect heterogeneity to be regularized separately from the prognostic eﬀect of control variables, making it possible to informatively “shrink to homogeneity”. While we focus on observational data, our methods are equally useful for inferring heterogeneous treatment eﬀects from randomized controlled experiments where careful regularization is somewhat less complicated but no less important. We illustrate these beneﬁts via the reanalysis of an observational study assessing the causal eﬀects of smoking on medical expenditures as well as extensive simulation studies.},
	language = {en},
	number = {3},
	urldate = {2023-05-03},
	journal = {Bayesian Analysis},
	author = {Hahn, P. Richard and Murray, Jared S. and Carvalho, Carlos M.},
	month = sep,
	year = {2020},
	file = {Hahn et al. - 2020 - Bayesian Regression Tree Models for Causal Inferen.pdf:/Users/tomer/Zotero/storage/Y27QN73X/Hahn et al. - 2020 - Bayesian Regression Tree Models for Causal Inferen.pdf:application/pdf},
}

@misc{kennedy_towards_2022,
	title = {Towards optimal doubly robust estimation of heterogeneous causal effects},
	abstract = {Heterogeneous eﬀect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment eﬀects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic error bound, which, despite its generality, yields rates faster than much of the literature. We apply the bound to derive error rates in smooth nonparametric models, and give suﬃcient conditions for oracle eﬃciency. Underlying our error bound is a general error bound for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle eﬃcient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some ﬁnite-sample properties are explored with simulations.},
	language = {en},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Kennedy, Edward H.},
	month = may,
	year = {2022},
	keywords = {Mathematics - Statistics Theory},
}

@article{kunzel_metalearners_2019,
	title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1804597116},
	abstract = {Significance
            Estimating and analyzing heterogeneous treatment effects is timely, yet challenging. We introduce a unifying framework for many conditional average treatment effect estimators, and we propose a metalearner, the X-learner, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect. We present its favorable properties, using theory and simulations. We apply it, using random forests, to two field experiments in political science, where it is shown to be easy to use and to produce results that are interpretable.
          , 
            There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
	language = {en},
	number = {10},
	urldate = {2023-05-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
	month = mar,
	year = {2019},
	pages = {4156--4165},
}

@book{breiman_classification_1984,
	address = {Abingdon},
	title = {Classification and regression trees},
	isbn = {978-1-351-46048-4},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties},
	language = {eng},
	publisher = {Routledge},
	author = {Breiman, Leo},
	year = {1984},
	note = {OCLC: 1012884854},
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2023-05-13},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	pages = {123--140},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	number = {1},
	urldate = {2023-05-13},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
	file = {Breiman - 2001 - Random Forests.pdf:/Users/tomer/Zotero/storage/JLD9XPFG/Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@article{dietterich_experimental_2000,
	title = {An {Experimental} {Comparison} of {Three} {Methods} for {Constructing} {Ensembles} of {Decision} {Trees}: {Bagging}, {Boosting}, and {Randomization}},
	abstract = {Bagging and boosting are methods that generate a diverse ensemble of classiﬁers by manipulating the training data given to a “base” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classiﬁcation noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classiﬁcation noise, bagging is much better than boosting, and sometimes better than randomization.},
	language = {en},
	author = {Dietterich, Thomas G},
	year = {2000},
	journal = {Machine Learning}
}

@article{hothorn_survival_2005,
	title = {Survival ensembles},
	volume = {7},
	issn = {1465-4644, 1468-4357},
	doi = {10.1093/biostatistics/kxj011},
	abstract = {We propose a uniﬁed and ﬂexible framework for ensemble learning in the presence of censoring. For right-censored data, we introduce a random forest algorithm and a generic gradient boosting algorithm for the construction of prognostic and diagnostic models. The methodology is utilized for predicting the survival time of patients suffering from acute myeloid leukemia based on clinical and genetic covariates. Furthermore, we compare the diagnostic capabilities of the proposed censored data random forest and boosting methods, applied to the recurrence-free survival time of node-positive breast cancer patients, with previously published ﬁndings.},
	language = {en},
	number = {3},
	urldate = {2023-05-13},
	journal = {Biostatistics},
	author = {Hothorn, T.},
	month = dec,
	year = {2005},
	pages = {355--373},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157},
	doi = {10.1214/08-AOAS169},
	language = {en},
	number = {3},
	urldate = {2023-05-13},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
}

@incollection{goos_ensemble_2000,
	address = {Berlin, Heidelberg},
	title = {Ensemble {Methods} in {Machine} {Learning}},
	volume = {1857},
	isbn = {978-3-540-67704-8 978-3-540-45014-6},
	abstract = {Ensemble methods are learning algorithms that construct a set of classi ers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classi er. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not over t rapidly.},
	language = {en},
	urldate = {2023-05-13},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dietterich, Thomas G.},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
	year = {2000},
	doi = {10.1007/3-540-45014-9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--15},
	file = {Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:/Users/tomer/Zotero/storage/UUVDVCJA/Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:application/pdf},
}

@article{schick_asymptotically_1986,
	title = {On {Asymptotically} {Efficient} {Estimation} in {Semiparametric} {Models}},
	volume = {14},
	number = {3},
	journal = {The Annals of Statistics},
	author = {Schick, Anton},
	year = {1986},
	pages = {1139--1151},
	file = {1176350055 (1).pdf:/Users/tomer/Zotero/storage/HXA5J82A/1176350055 (1).pdf:application/pdf},
}

@article{nie_inference_2011,
	title = {Inference for the {Effect} of {Treatment} on {Survival} {Probability} in {Randomized} {Trials} with {Noncompliance} and {Administrative} {Censoring}},
	volume = {67},
	issn = {0006341X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2011.01575.x},
	doi = {10.1111/j.1541-0420.2011.01575.x},
	abstract = {In many clinical studies with a survival outcome, administrative censoring occurs when follow-up ends at a prespeciﬁed date and many subjects are still alive. An additional complication in some trials is that there is noncompliance with the assigned treatment. For this setting, we study the estimation of the causal eﬀect of treatment on survival probability up to a given time point among those subjects who would comply with the assignment to both treatment and control. We ﬁrst discuss the standard instrumental variable (IV) method for survival outcomes and parametric maximum likelihood methods, and then develop an eﬃcient plug-in nonparametric empirical maximum likelihood estimation (PNEMLE) approach. The PNEMLE method does not make any assumptions on outcome distributions, and makes use of the mixture structure in the data to gain eﬃciency over the standard IV method. Theoretical results of the PNEMLE are derived and the method is illustrated by an analysis of data from a breast cancer screening trial. From our limited mortality analysis with administrative censoring times 10 years into the follow-up, we ﬁnd a signiﬁcant beneﬁt of screening is present after 4 years (at the 5\% level) and this persists at 10 years follow-up.},
	language = {en},
	number = {4},
	urldate = {2023-05-22},
	journal = {Biometrics},
	author = {Nie, Hui and Cheng, Jing and Small, Dylan S.},
	month = dec,
	year = {2011},
	pages = {1397--1405},
	file = {Nie et al. - 2011 - Inference for the Effect of Treatment on Survival .pdf:/Users/tomer/Zotero/storage/L2YQP8YJ/Nie et al. - 2011 - Inference for the Effect of Treatment on Survival .pdf:application/pdf},
}

@article{wei_monte_2023,
	title = {A {Monte} {Carlo} {Implementation} of the {EM} {Algorithm} and the {Poor} {Man}'s {Data} {Augmentation} {Algorithms}},
	language = {en},
	author = {Wei, Greg C G and Tanner, Martin A},
	year = {2023},
	file = {Wei and Tanner - 2023 - A Monte Carlo Implementation of the EM Algorithm a.pdf:/Users/tomer/Zotero/storage/9XGRZF2C/Wei and Tanner - 2023 - A Monte Carlo Implementation of the EM Algorithm a.pdf:application/pdf},
}

@article{geurts_extremely_2006,
	title = {Extremely randomized trees},
	volume = {63},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-006-6226-1},
	doi = {10.1007/s10994-006-6226-1},
	abstract = {This paper proposes a new tree-based ensemble method for supervised classiﬁcation and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem speciﬁcs by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efﬁciency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
	language = {en},
	number = {1},
	urldate = {2023-06-11},
	journal = {Machine Learning},
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	month = apr,
	year = {2006},
	pages = {3--42},
	file = {Geurts et al. - 2006 - Extremely randomized trees.pdf:/Users/tomer/Zotero/storage/7KWJ5MTL/Geurts et al. - 2006 - Extremely randomized trees.pdf:application/pdf},
}

@article{zhao_principled_2012,
	title = {Principled sure independence screening for {Cox} models with ultra-high-dimensional covariates},
	volume = {105},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X11001618},
	doi = {10.1016/j.jmva.2011.08.002},
	abstract = {It is rather challenging for current variable selectors to handle situations where the number of covariates under consideration is ultra-high. Consider a motivating clinical trial of the drug bortezomib for the treatment of multiple myeloma, where overall survival and expression levels of 44760 probesets were measured for each of 80 patients with the goal of identifying genes that predict survival after treatment. This dataset defies analysis even with regularized regression. Some remedies have been proposed for the linear model and for generalized linear models, but there are few solutions in the survival setting and, to our knowledge, no theoretical support. Furthermore, existing strategies often involve tuning parameters that are difficult to interpret. In this paper, we propose and theoretically justify a principled method for reducing dimensionality in the analysis of censored data by selecting only the important covariates. Our procedure involves a tuning parameter that has a simple interpretation as the desired false positive rate of this selection. We present simulation results and apply the proposed procedure to analyze the aforementioned myeloma study.},
	language = {en},
	number = {1},
	urldate = {2023-07-05},
	journal = {Journal of Multivariate Analysis},
	author = {Zhao, Sihai Dave and Li, Yi},
	month = feb,
	year = {2012},
	pages = {397--411},
	file = {Zhao and Li - 2012 - Principled sure independence screening for Cox mod.pdf:/Users/tomer/Zotero/storage/FWH336R3/Zhao and Li - 2012 - Principled sure independence screening for Cox mod.pdf:application/pdf},
}

@book{chan_econometrics_2022,
	address = {Cham},
	series = {Advanced {Studies} in {Theoretical} and {Applied} {Econometrics}},
	title = {Econometrics with {Machine} {Learning}},
	volume = {53},
	isbn = {978-3-031-15148-4 978-3-031-15149-1},
	url = {https://link.springer.com/10.1007/978-3-031-15149-1},
	language = {en},
	urldate = {2023-10-06},
	publisher = {Springer International Publishing},
	editor = {Chan, Felix and Mátyás, László},
	year = {2022},
	doi = {10.1007/978-3-031-15149-1},
	file = {Chan and Mátyás - 2022 - Econometrics with Machine Learning.pdf:/Users/tomer/Zotero/storage/P52T4B9Q/Chan and Mátyás - 2022 - Econometrics with Machine Learning.pdf:application/pdf},
}

@misc{wager_estimation_2017,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	url = {http://arxiv.org/abs/1510.04342},
	abstract = {Many scientiﬁc and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment eﬀect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment eﬀects that extends Breiman’s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment eﬀect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic conﬁdence intervals for the true treatment eﬀect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the ﬁrst set of results that allows any type of random forest, including classiﬁcation and regression forests, to be used for provably valid statistical inference. In experiments, we ﬁnd causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	language = {en},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Wager, Stefan and Athey, Susan},
	month = jul,
	year = {2017},
	note = {arXiv:1510.04342 [math, stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	annote = {Comment: To appear in the Journal of the American Statistical Association. Part of the results developed in this paper were made available as an earlier technical report "Asymptotic Theory for Random Forests", available at (arXiv:1405.0352)},
	file = {Wager and Athey - 2017 - Estimation and Inference of Heterogeneous Treatmen.pdf:/Users/tomer/Zotero/storage/UHIWGUFQ/Wager and Athey - 2017 - Estimation and Inference of Heterogeneous Treatmen.pdf:application/pdf},
}

@misc{nie_quasi-oracle_2020,
	title = {Quasi-{Oracle} {Estimation} of {Heterogeneous} {Treatment} {Effects}},
	url = {http://arxiv.org/abs/1712.04912},
	abstract = {We develop a general class of two-step algorithms for heterogeneous treatment eﬀect estimation in observational studies. We ﬁrst estimate marginal eﬀects and treatment propensities to form an objective function that isolates the heterogeneous treatment eﬀects, and then optimize the learned objective. This approach has several advantages over existing methods. From a practical perspective, our method is very ﬂexible and easy to use: In both steps, we can use any method of our choice, e.g., penalized regression, a deep net, or boosting; moreover, these methods can be ﬁne-tuned by cross-validating on the learned objective. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property, whereby even if our pilot estimates for marginal eﬀects and treatment propensities are not particularly accurate, we achieve the same regret bounds as an oracle who has a-priori knowledge of these nuisance components. We implement variants of our method based on both penalized regression and convolutional neural networks, and ﬁnd promising performance relative to existing baselines.},
	language = {en},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Nie, Xinkun and Wager, Stefan},
	month = aug,
	year = {2020},
	note = {arXiv:1712.04912 [econ, math, stat]},
	keywords = {Statistics - Machine Learning, Economics - Econometrics, Mathematics - Statistics Theory},
	annote = {Comment: Biometrika, forthcoming},
	file = {Nie and Wager - 2020 - Quasi-Oracle Estimation of Heterogeneous Treatment.pdf:/Users/tomer/Zotero/storage/2KALSUJ5/Nie and Wager - 2020 - Quasi-Oracle Estimation of Heterogeneous Treatment.pdf:application/pdf},
}

@book{kleinbaum_survival_2012,
	address = {New York, NY},
	series = {Statistics for {Biology} and {Health}},
	title = {Survival {Analysis}: {A} {Self}-{Learning} {Text}},
	isbn = {978-1-4419-6645-2 978-1-4419-6646-9},
	shorttitle = {Survival {Analysis}},
	url = {http://link.springer.com/10.1007/978-1-4419-6646-9},
	language = {en},
	urldate = {2023-10-06},
	publisher = {Springer New York},
	author = {Kleinbaum, David G. and Klein, Mitchel},
	year = {2012},
	doi = {10.1007/978-1-4419-6646-9},
	file = {Kleinbaum and Klein - 2012 - Survival Analysis A Self-Learning Text.pdf:/Users/tomer/Zotero/storage/RDSNGTYV/Kleinbaum and Klein - 2012 - Survival Analysis A Self-Learning Text.pdf:application/pdf},
}

@article{cui_estimating_2023,
	title = {Estimating heterogeneous treatment effects with right-censored data via causal survival forests},
	volume = {85},
	issn = {1369-7412, 1467-9868},
	doi = {10.1093/jrsssb/qkac001},
	abstract = {Forest-based methods have recently gained in popularity for non-parametric treatment effect estimation. Building on this line of work, we introduce causal survival forests, which can be used to estimate heterogeneous treatment effects in survival and observational setting where outcomes may be right-censored. Our approach relies on orthogonal estimating equations to robustly adjust for both censoring and selection effects under unconfoundedness. In our experiments, we find our approach to perform well relative to a number of baselines.},
	language = {en},
	number = {2},
	urldate = {2024-01-26},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Cui, Yifan and Kosorok, Michael R and Sverdrup, Erik and Wager, Stefan and Zhu, Ruoqing},
	month = may,
	year = {2023},
	pages = {179--211},
}

@article{rosenbaum_central_1983,
	title = {The central role of the propensity score in observational studies for causal effects},
	volume = {70},
	doi = {10.1093/biomet/70.1.41},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
	language = {en},
	number = {1},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R and Rubin, Donald B},
	year = {1983},
	pages = {41--55},
	file = {Rosenbaum and Rubin - The central role of the propensity score in observ.pdf:/Users/tomer/Zotero/storage/JXFKMLFC/Rosenbaum and Rubin - The central role of the propensity score in observ.pdf:application/pdf},
}

@article{david_cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	doi = {10.1007/978-1-4612-4380-9},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual arc available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	number = {2},
	journal = {Journal of the royal statistical society series b-methodological},
	author = {{David Cox} and Cox, David},
	month = jan,
	year = {1972},
	pages = {187--220},
}

@article{edward_l_kaplan_nonparametric_1958,
	title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
	volume = {53},
	doi = {10.1007/978-1-4612-4380-9},
	abstract = {Abstract In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: L...},
	number = {282},
	journal = {Journal of the American Statistical Association},
	author = {{Edward L. Kaplan} and Kaplan, Edward L. and {E. L. Kaplan} and {Paul Meier} and Meier, Paul},
	month = jun,
	year = {1958},
	doi = {10.1007/978-1-4612-4380-9},
	note = {MAG ID: 1979300931},
	pages = {457--481},
}

@article{john_van_ryzin_classification_1986,
	title = {Classification and {Regression} {Trees}.},
	volume = {81},
	doi = {10.2307/2288003},
	number = {393},
	journal = {Journal of the American Statistical Association},
	author = {{John Van Ryzin} and Van Ryzin, John and {Leo Breiman} and Breiman, Leo and {Jerome H. Friedman} and Friedman, Jerome H. and {Richard A. Olshen} and Olshen, Richard A. and {Charles J. Stone} and {Charles J. Stone} and {Charles J. Stone} and Stone, Charles J.},
	year = {1986},
	doi = {10.2307/2288003},
	note = {MAG ID: 3085162807},
	pages = {253},
}

@article{kravitz_evidencebased_2004,
	title = {Evidence‐{Based} {Medicine}, {Heterogeneity} of {Treatment} {Effects}, and the {Trouble} with {Averages}},
	volume = {82},
	issn = {0887-378X, 1468-0009},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0887-378X.2004.00327.x},
	doi = {10.1111/j.0887-378X.2004.00327.x},
	abstract = {Evidence‐based medicine is the application of scientific evidence to clinical practice. This article discusses the difficulties of applying global evidence (“average effects” measured as population means) to local problems (individual patients or groups who might depart from the population average). It argues that the benefit or harm of most treatments in clinical trials can be misleading and fail to reveal the potentially complex mixture of substantial benefits for some, little benefit for many, and harm for a few. Heterogeneity of treatment effects reflects patient diversity in risk of disease, responsiveness to treatment, vulnerability to adverse effects, and utility for different outcomes. Recognizing these factors, researchers can design studies that better characterize who will benefit from medical treatments, and clinicians and policymakers can make better use of the results.},
	language = {en},
	number = {4},
	urldate = {2024-02-11},
	journal = {The Milbank Quarterly},
	author = {Kravitz, Richard L. and Duan, Naihua and Braslow, Joel},
	month = dec,
	year = {2004},
	pages = {661--687},
	file = {Kravitz et al. - 2004 - Evidence‐Based Medicine, Heterogeneity of Treatmen.pdf:/Users/tomer/Zotero/storage/8K8ECYSX/Kravitz et al. - 2004 - Evidence‐Based Medicine, Heterogeneity of Treatmen.pdf:application/pdf},
}

@article{luedtke_super-learning_2016,
	title = {Super-{Learning} of an {Optimal} {Dynamic} {Treatment} {Rule}},
	volume = {12},
	issn = {2194-573X, 1557-4679},
	url = {https://www.degruyter.com/document/doi/10.1515/ijb-2015-0052/html},
	doi = {10.1515/ijb-2015-0052},
	abstract = {We consider the estimation of an optimal dynamic two time-point treatment rule defined as the rule that maximizes the mean outcome under the dynamic treatment, where the candidate rules are restricted to depend only on a user-supplied subset of the baseline and intermediate covariates. This estimation problem is addressed in a statistical model for the data distribution that is nonparametric, beyond possible knowledge about the treatment and censoring mechanisms. We propose data adaptive estimators of this optimal dynamic regime which are defined by sequential loss-based learning under both the blip function and weighted classification frameworks. Rather than a priori selecting an estimation framework and algorithm, we propose combining estimators from both frameworks using a super-learning based cross-validation selector that seeks to minimize an appropriate cross-validated risk. The resulting selector is guaranteed to asymptotically perform as well as the best convex combination of candidate algorithms in terms of loss-based dissimilarity under conditions. We offer simulation results to support our theoretical findings.},
	language = {en},
	number = {1},
	urldate = {2024-02-11},
	journal = {The International Journal of Biostatistics},
	author = {Luedtke, Alexander R. and Van Der Laan, Mark J.},
	month = may,
	year = {2016},
	pages = {305--332},
	file = {Luedtke and Van Der Laan - 2016 - Super-Learning of an Optimal Dynamic Treatment Rul.pdf:/Users/tomer/Zotero/storage/ML2HW2G3/Luedtke and Van Der Laan - 2016 - Super-Learning of an Optimal Dynamic Treatment Rul.pdf:application/pdf},
}

@misc{okasa_meta-learners_2022,
	title = {Meta-{Learners} for {Estimation} of {Causal} {Effects}: {Finite} {Sample} {Cross}-{Fit} {Performance}},
	shorttitle = {Meta-{Learners} for {Estimation} of {Causal} {Effects}},
	abstract = {Estimation of causal eﬀects using machine learning methods has become an active research ﬁeld in econometrics. In this paper, we study the ﬁnite sample performance of meta-learners for estimation of heterogeneous treatment eﬀects under the usage of sample-splitting and cross-ﬁtting to reduce the overﬁtting bias. In both synthetic and semi-synthetic simulations we ﬁnd that the performance of the meta-learners in ﬁnite samples greatly depends on the estimation procedure. The results imply that sample-splitting and cross-ﬁtting are beneﬁcial in large samples for bias reduction and eﬃciency of the meta-learners, respectively, whereas full-sample estimation is preferable in small samples. Furthermore, we derive practical recommendations for application of speciﬁc meta-learners in empirical studies depending on particular data characteristics such as treatment shares and sample size.},
	language = {en},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Okasa, Gabriel},
	month = jan,
	year = {2022},
	note = {arXiv:2201.12692 [econ, stat]},
	keywords = {Statistics - Machine Learning, Economics - Econometrics},
	annote = {Comment: 60 pages, 17 figures, 17 tables},
}

@book{rubin_multiple_1987,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Multiple {Imputation} for {Nonresponse} in {Surveys}},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	isbn = {978-0-471-08705-2 978-0-470-31669-6},
	language = {en},
	urldate = {2024-05-13},
	publisher = {Wiley},
	author = {Rubin, Donald B.},
	month = jun,
	year = {1987},
	doi = {10.1002/9780470316696},
}

@book{schafer_analysis_1997,
	address = {Boca Raton},
	edition = {1st ed},
	title = {Analysis of incomplete multivariate data},
	isbn = {978-1-4398-2186-2},
	abstract = {The last two decades have seen enormous developments in statistical methods for incomplete data. The EM algorithm and its extensions, multiple imputation, and Markov Chain Monte Carlo provide a set of flexible and reliable tools from inference in large classes of missing-data problems. Yet, in practical terms, those developments have had surprisingly little impact on the way most data analysts handle missing values on a routine basis. Analysis of Incomplete Multivariate Data helps bridge the gap between theory and practice, making these missing-data tools accessible to a broad audience. It presents a unified, Bayesian approach to the analysis of incomplete multivariate data, covering datasets in which the variables are continuous, categorical, or both. The focus is applied, where necessary, to help readers thoroughly understand the statistical properties of those methods, and the behavior of the accompanying algorithms. All techniques are illustrated with real data examples, with extended discussion and practical advice. All of the algorithms described in this book have been implemented by the author for general use in the statistical languages S and S Plus. The software is available free of charge on the Internet},
	language = {eng},
	publisher = {Chapman \& Hall/CRC},
	author = {Schafer, J. L.},
	year = {1997},
}

@article{sexton_standard_2009,
	title = {Standard errors for bagged and random forest estimators},
	volume = {53},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01679473},
	doi = {10.1016/j.csda.2008.08.007},
	abstract = {Bagging and random forests are widely used ensemble methods. Each forms an ensemble of models by randomly perturbing the fitting of a base learner. The standard errors estimation of the resultant regression function is considered. Three estimators are discussed. One, based on the jackknife, is applicable to bagged estimators and can be computed using the bagged ensemble. The two other estimators target the bootstrap standard error estimator, and require fitting multiple ensemble estimators, one for each bootstrap sample. It is shown that these bootstrap ensemble sizes can be small, which reduces the computation involved in forming the estimator. The estimators are studied using both simulated and real data.},
	language = {en},
	number = {3},
	urldate = {2024-05-13},
	journal = {Computational Statistics \& Data Analysis},
	author = {Sexton, Joseph and Laake, Petter},
	month = jan,
	year = {2009},
	pages = {801--811},
}

@article{murray_multiple_2018,
	title = {Multiple {Imputation}: {A} {Review} of {Practical} and {Theoretical} {Findings}},
	volume = {33},
	issn = {0883-4237},
	shorttitle = {Multiple {Imputation}},
	doi = {10.1214/18-STS644},
	abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in ﬂexible joint modeling and sequential regression/chained equations/fully conditional speciﬁcation approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
	language = {en},
	number = {2},
	urldate = {2024-05-13},
	journal = {Statistical Science},
	author = {Murray, Jared S.},
	month = may,
	year = {2018},
}

@article{wager_estimation_2018,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	volume = {113},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2017.1319839},
	language = {en},
	number = {523},
	urldate = {2024-07-28},
	journal = {Journal of the American Statistical Association},
	author = {Wager, Stefan and Athey, Susan},
	month = jul,
	year = {2018},
	pages = {1228--1242},
}

@article{xie_estimating_2012,
	title = {Estimating {Heterogeneous} {Treatment} {Effects} with {Observational} {Data}},
	volume = {42},
	issn = {0081-1750, 1467-9531},
	doi = {10.1177/0081175012452652},
	abstract = {Individuals differ not only in their background characteristics but also in how they respond to a particular treatment, intervention, or stimulation. In particular, treatment effects may vary systematically by the propensity for treatment. In this paper, we discuss a practical approach to studying heterogeneous treatment effects as a function of the treatment propensity, under the same assumption commonly underlying regression analysis: ignorability. We describe one parametric method and two nonparametric methods for estimating interactions between treatment and the propensity for treatment. For the first method, we begin by estimating propensity scores for the probability of treatment given a set of observed covariates for each unit and construct balanced propensity score strata; we then estimate propensity score stratum-specific average treatment effects and evaluate a trend across them. For the second method, we match control units to treated units based on the propensity score and transform the data into treatment-control comparisons at the most elementary level at which such comparisons can be constructed; we then estimate treatment effects as a function of the propensity score by fitting a nonparametric model as a smoothing device. For the third method, we first estimate nonparametric regressions of the outcome variable as a function of the propensity score separately for treated units and for control units and then take the difference between the two nonparametric regressions. We illustrate the application of these methods with an empirical example of the effects of college attendance on women’s fertility.},
	language = {en},
	number = {1},
	urldate = {2024-07-28},
	journal = {Sociological Methodology},
	author = {Xie, Yu and Brand, Jennie E. and Jann, Ben},
	month = aug,
	year = {2012},
	pages = {314--347},
}

@article{nie_quasi-oracle_2021,
	title = {Quasi-oracle estimation of heterogeneous treatment effects},
	volume = {108},
	issn = {0006-3444, 1464-3510},
	doi = {10.1093/biomet/asaa076},
	abstract = {Summary
            Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.},
	language = {en},
	number = {2},
	urldate = {2024-07-28},
	journal = {Biometrika},
	author = {Nie, X and Wager, S},
	month = may,
	year = {2021},
	pages = {299--319},
}

@article{powers_methods_2018,
	title = {Some methods for heterogeneous treatment effect estimation in high dimensions},
	volume = {37},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.7623},
	doi = {10.1002/sim.7623},
	abstract = {When devising a course of treatment for a patient, doctors often have little quantitative evidence on which to base their decisions, beyond their medical education and published clinical trials. Stanford Health Care alone has millions of electronic medical records that are only just recently being leveraged to inform better treatment recommendations. These data present a unique challenge because they are high dimensional and observational. Our goal is to make personalized treatment recommendations based on the outcomes for past patients similar to a new patient. We propose and analyze 3 methods for estimating heterogeneous treatment effects using observational data. Our methods perform well in simulations using a wide variety of treatment effect functions, and we present results of applying the 2 most promising methods to data from The SPRINT Data Analysis Challenge, from a large randomized trial of a treatment for high blood pressure.},
	language = {en},
	number = {11},
	urldate = {2024-07-28},
	journal = {Statistics in Medicine},
	author = {Powers, Scott and Qian, Junyang and Jung, Kenneth and Schuler, Alejandro and Shah, Nigam H. and Hastie, Trevor and Tibshirani, Robert},
	month = may,
	year = {2018},
	pages = {1767--1787},
	file = {Accepted Version:/Users/tomer/Zotero/storage/557QB5WE/Powers et al. - 2018 - Some methods for heterogeneous treatment effect es.pdf:application/pdf},
}

@article{leo_breiman_classification_1983,
	title = {Classification and regression trees},
	abstract = {Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index.},
	author = {{Leo Breiman} and Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
	month = jan,
	year = {1983},
	note = {MAG ID: 1594031697},
}

@article{lipkovich_overview_2023,
	title = {Overview of modern approaches for identifying and evaluating heterogeneous treatment effects from clinical data},
	volume = {20},
	issn = {1740-7745, 1740-7753},
	url = {http://journals.sagepub.com/doi/10.1177/17407745231174544},
	doi = {10.1177/17407745231174544},
	abstract = {There has been much interest in the evaluation of heterogeneous treatment effects (HTE) and multiple statistical methods have emerged under the heading of personalized/precision medicine combining ideas from hypothesis testing, causal inference, and machine learning over the past 10-15 years. We discuss new ideas and approaches for evaluating HTE in randomized clinical trials and observational studies using the features introduced earlier by Lipkovich, Dmitrienko, and D’Agostino that distinguish principled methods from simplistic approaches to data-driven subgroup identification and estimating individual treatment effects and use a case study to illustrate these approaches. We identified and provided a high-level overview of several classes of modern statistical approaches for personalized/precision medicine, elucidated the underlying principles and challenges, and compared findings for a case study across different methods. Different approaches to evaluating HTEs may produce (and actually produced) highly disparate results when applied to a specific data set. Evaluating HTE with machine learning methods presents special challenges since most of machine learning algorithms are optimized for prediction rather than for estimating causal effects. An additional challenge is in that the output of machine learning methods is typically a ‘‘black box’’ that needs to be transformed into interpretable personalized solutions in order to gain acceptance and usability.},
	language = {en},
	number = {4},
	urldate = {2024-07-28},
	journal = {Clinical Trials},
	author = {Lipkovich, Ilya and Svensson, David and Ratitch, Bohdana and Dmitrienko, Alex},
	month = aug,
	year = {2023},
	pages = {380--393},
	file = {Lipkovich et al. - 2023 - Overview of modern approaches for identifying and .pdf:/Users/tomer/Zotero/storage/2EM7SDGE/Lipkovich et al. - 2023 - Overview of modern approaches for identifying and .pdf:application/pdf},
}

@article{chipman_bart_2010,
	title = {{BART}: {Bayesian} additive regression trees},
	volume = {4},
	issn = {1932-6157},
	shorttitle = {{BART}},
	doi = {10.1214/09-AOAS285},
	language = {en},
	number = {1},
	urldate = {2024-07-28},
	journal = {The Annals of Applied Statistics},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = mar,
	year = {2010},
}

@article{hill_bayesian_2020,
	title = {Bayesian {Additive} {Regression} {Trees}: {A} {Review} and {Look} {Forward}},
	volume = {7},
	issn = {2326-8298, 2326-831X},
	shorttitle = {Bayesian {Additive} {Regression} {Trees}},
	doi = {10.1146/annurev-statistics-031219-041110},
	abstract = {Bayesian additive regression trees (BART) provides a flexible approach to fitting a variety of regression models while avoiding strong parametric assumptions. The sum-of-trees model is embedded in a Bayesian inferential framework to support uncertainty quantification and provide a principled approach to regularization through prior specification. This article presents the basic approach and discusses further development of the original algorithm that support a variety of data structures and assumptions. We describe augmentations of the prior specification to accommodate higher dimensional data and smoother functions. Recent theoretical developments provide justifications for the performance observed in simulations and other settings. Use of BART in causal inference provides an additional avenue for extensions and applications. We discuss software options as well as challenges and future directions.},
	language = {en},
	number = {1},
	urldate = {2024-07-28},
	journal = {Annual Review of Statistics and Its Application},
	author = {Hill, Jennifer and Linero, Antonio and Murray, Jared},
	month = mar,
	year = {2020},
	pages = {251--278},
}

@incollection{scholkopf_bayesian_2007,
	title = {Bayesian {Ensemble} {Learning}},
	isbn = {978-0-262-25691-9},
	url = {https://direct.mit.edu/books/book/3168/chapter/87401/Bayesian-Ensemble-Learning},
	abstract = {We develop a Bayesian “sum-of-trees” model, named BART, where each tree is constrained by a prior to be a weak learner. Fitting and inference are accomplished via an iterative backﬁtting MCMC algorithm. This model is motivated by ensemble methods in general, and boosting algorithms in particular. Like boosting, each weak learner (i.e., each weak tree) contributes a small amount to the overall model. However, our procedure is deﬁned by a statistical model: a prior and a likelihood, while boosting is deﬁned by an algorithm. This model-based approach enables a full and accurate assessment of uncertainty in model predictions, while remaining highly competitive in terms of predictive accuracy.},
	language = {en},
	urldate = {2024-07-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {The MIT Press},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
	month = sep,
	year = {2007},
	doi = {10.7551/mitpress/7503.003.0038},
	pages = {265--272},
	file = {Chipman et al. - 2007 - Bayesian Ensemble Learning.pdf:/Users/tomer/Zotero/storage/CT4CP6AH/Chipman et al. - 2007 - Bayesian Ensemble Learning.pdf:application/pdf},
}

@article{robinson_root-n-consistent_1988,
	title = {Root-{N}-{Consistent} {Semiparametric} {Regression}},
	volume = {56},
	issn = {00129682},

	doi = {10.2307/1912705},
	language = {en},
	number = {4},
	urldate = {2024-07-29},
	journal = {Econometrica},
	author = {Robinson, P. M.},
	month = jul,
	year = {1988},
	pages = {931},

}

@article{lagakos_challenge_2006,
	title = {The {Challenge} of {Subgroup} {Analyses} — {Reporting} without {Distorting}},
	volume = {354},
	issn = {0028-4793, 1533-4406},
	doi = {10.1056/NEJMp068070},
	language = {en},
	number = {16},
	urldate = {2024-07-30},
	journal = {New England Journal of Medicine},
	author = {Lagakos, Stephen W.},
	month = apr,
	year = {2006},
	pages = {1667--1669},
}


@article{shapiro_periodic_1997,
	title = {Periodic {Screening} for {Breast} {Cancer}: {The} {HIP} {Randomized} {Controlled} {Trial}},
	volume = {1997},
	issn = {1052-6773},
	doi = {10.1093/jncimono/1997.22.27},
	abstract = {This paper summarizes the findings of the first breast cancer screening trial, which was initiated in December 1963 to explore the efficacy of screening. Women aged 40-64 years were selected from enrollees in the Health Insurance Plan (HIP) of Greater New York and were randomly assigned to study and control groups. Study group women were invited for screening, an initial examination, and three annual reexaminations. Screening consisted of film mammography (cephalocaudal and lateral views of each breast) and clinical examination of breasts. Breast cancer and mortality from breast cancer were examined by treatment group (study vs. control) and by entry-age subgroup. By the end of 18 years from entry, the study group had about a 25\% lower breast cancer mortality among women aged 40-49 and 50-59 at time of entry than did the control group. However, to a large extent the difference among the 40-49-year-olds occurred in the subgroup with breast cancer diagnosed after these women had passed their 50th birthday, and the utility of screening women in their forties is questionable.},
	number = {22},
	journal = {JNCI Monographs},
	author = {Shapiro, Sam},
	month = jan,
	year = {1997},
	pages = {27--30},
}



@article{mackenzie_using_2014,
	title = {Using instrumental variables to estimate a {Cox}’s proportional hazards regression subject to additive confounding},
	volume = {14},
	issn = {1387-3741, 1572-9400},
	doi = {10.1007/s10742-014-0117-x},
	abstract = {The estimation of treatment effects is one of the primary goals of statistics in medicine. Estimation based on observational studies is subject to confounding. Statistical methods for controlling bias due to confounding include regression adjustment, propensity scores and inverse probability weighted estimators. These methods require that all confounders are recorded in the data. The method of instrumental variables (IVs) can eliminate bias in observational studies even in the absence of information on confounders. We propose a method for integrating IVs within the framework of Cox's proportional hazards model and demonstrate the conditions under which it recovers the causal effect of treatment. The methodology is based on the approximate orthogonality of an instrument with unobserved confounders among those at risk. We derive an estimator as the solution to an estimating equation that resembles the score equation of the partial likelihood in much the same way as the traditional IV estimator resembles the normal equations. To justify this IV estimator for a Cox model we perform simulations to evaluate its operating characteristics. Finally, we apply the estimator to an observational study of the effect of coronary catheterization on survival.},
	language = {en},
	number = {1-2},
	journal = {Health Services and Outcomes Research Methodology},
	author = {MacKenzie, Todd A. and Tosteson, Tor D. and Morden, Nancy E. and Stukel, Therese A. and O’Malley, A. James},
	month = jun,
	year = {2014},
	pages = {54--68},
}

@article{angrist_identification_1996,
	title = {Identification of {Causal} {Effects} {Using} {Instrumental} {Variables}},
	volume = {91},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1996.10476902},
	language = {en},
	number = {434},
	urldate = {2025-01-07},
	journal = {Journal of the American Statistical Association},
	author = {Angrist, Joshua D. and Imbens, Guido W. and Rubin, Donald B.},
	month = jun,
	year = {1996},
	pages = {444--455},
}

@article{aronow_beyond_2013,
	title = {Beyond {LATE}: {Estimation} of the {Average} {Treatment} {Effect} with an {Instrumental} {Variable}},
	volume = {21},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Beyond {LATE}},
	doi = {10.1093/pan/mpt013},
	abstract = {Political scientists frequently use instrumental variables (IV) estimation to estimate the causal effect of an endogenous treatment variable. However, when the treatment effect is heterogeneous, this estimation strategy only recovers the local average treatment effect (LATE). The LATE is an average treatment effect (ATE) for a subset of the population: units that receive treatment if and only if they are induced by an exogenous IV. However, researchers may instead be interested in the ATE for the entire population of interest. In this article, we develop a simple reweighting method for estimating the ATE, shedding light on the identification challenge posed in moving from the LATE to the ATE. We apply our method to two published experiments in political science in which we demonstrate that the LATE has the potential to substantively differ from the ATE.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Political Analysis},
	author = {Aronow, Peter M. and Carnegie, Allison},
	year = {2013},
	pages = {492--506},
}

@article{choi_estimating_2017,
	title = {Estimating the {Causal} {Effect} of {Treatment} in {Observational} {Studies} with {Survival} {Time} {End} {Points} and {Unmeasured} {Confounding}},
	volume = {66},
	issn = {0035-9254, 1467-9876},
	doi = {10.1111/rssc.12158},
	abstract = {Estimation of the effect of a treatment in the presence of unmeasured confounding is a common objective in observational studies. The Two Stage Least Squares (2SLS) Instrumental Variables (IV) procedure is frequently used but is not applicable to time-to-event data if some observations are censored. We develop a simultaneous equations model (SEM) to account for unmeasured confounding of the effect of treatment on survival time subject to censoring. The identification of the treatment effect is assisted by IVs (variables related to treatment but conditional on treatment not to the outcome) and the assumed bivariate distribution underlying the data generating process. The methodology is illustrated on data from an observational study of time to death following endovascular or open repair of ruptured abdominal aortic aneurysm. As the IV and the distributional assumptions cannot be jointly assessed from the observed data, we evaluate the sensitivity of the results to these assumptions.},
	language = {en},
	number = {1},
	urldate = {2025-01-07},
	journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	author = {Choi, Jaeun and O'Malley, A. James},
	month = jan,
	year = {2017},
	pages = {159--185},
}

@article{hernan_instruments_2006,
	title = {Instruments for {Causal} {Inference}: {An} {Epidemiologist}'s {Dream}?},
	volume = {17},
	issn = {1044-3983},
	shorttitle = {Instruments for {Causal} {Inference}},
	doi = {10.1097/01.ede.0000222409.00878.37},
	abstract = {The use of instrumental variable (IV) methods is attractive because, even in the presence of unmeasured confounding, such methods may consistently estimate the average causal effect of an exposure on an outcome. However, for this consistent estimation to be achieved, several strong conditions must hold. We review the deﬁnition of an instrumental variable, describe the conditions required to obtain consistent estimates of causal effects, and explore their implications in the context of a recent application of the instrumental variables approach. We also present (1) a description of the connection between 4 causal models— counterfactuals, causal directed acyclic graphs, nonparametric structural equation models, and linear structural equation models—that have been used to describe instrumental variables methods; (2) a uniﬁed presentation of IV methods for the average causal effect in the study population through structural mean models; and (3) a discussion and new extensions of instrumental variables methods based on assumptions of monotonicity.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Epidemiology},
	author = {Hernán, Miguel A. and Robins, James M.},
	month = jul,
	year = {2006},
	pages = {360--372},
}

@article{martinussen_instrumental_2017,
	title = {Instrumental {Variables} {Estimation} of {Exposure} {Effects} on a {Time}-to-{Event} {Endpoint} {Using} {Structural} {Cumulative} {Survival} {Models}},
	volume = {73},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0006-341X, 1541-0420},
	doi = {10.1111/biom.12699},
	abstract = {The use of instrumental variables for estimating the effect of an exposure on an outcome is popular in econometrics, and increasingly so in epidemiology. This increasing popularity may be attributed to the natural occurrence of instrumental variables in observational studies that incorporate elements of randomization, either by design or by nature (e.g., random inheritance of genes). Instrumental variables estimation of exposure effects is well established for continuous outcomes and to some extent for binary outcomes. It is, however, largely lacking for time-to-event outcomes because of complications due to censoring and survivorship bias. In this paper, we make a novel proposal under a class of structural cumulative survival models which parameterize time-varying effects of a point exposure directly on the scale of the survival function; these models are essentially equivalent with a semi-parametric variant of the instrumental variables additive hazards model. We propose a class of recursive instrumental variable estimators for these exposure effects, and derive their large sample properties along with inferential tools. We examine the performance of the proposed method in simulation studies and illustrate it in a Mendelian randomization study to evaluate the effect of diabetes on mortality using data from the Health and Retirement Study. We further use the proposed method to investigate potential benefit from breast cancer screening on subsequent breast cancer mortality based on the HIP-study.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Biometrics},
	author = {Martinussen, Torben and Vansteelandt, Stijn and Tchetgen Tchetgen, Eric J. and Zucker, David M.},
	month = dec,
	year = {2017},
	pages = {1140--1149},
}

@article{sorensen_causal_2019,
	title = {A causal proportional hazards estimator under homogeneous or heterogeneous selection in an {IV} setting},
	volume = {25},
	issn = {1380-7870, 1572-9249},
	doi = {10.1007/s10985-019-09476-y},
	abstract = {In this paper we present a framework to do estimation in a structural Cox model when there may be unobserved confounding. The model is phrased in terms of a selection bias function and a baseline model that describes how covariates affect the survival time in a scenario without exposure. In this way model congeniality is ensured. The method uses an instrumental variable. Interestingly, the formulated model turns out to have similarities to the so-called Cox–Aalen survival model for the observed data. We exploit this to enhance estimation of the unknown parameters. This also allows us to derive large sample properties of the proposed estimator.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Lifetime Data Analysis},
	author = {Sørensen, Ditte Nørbo and Martinussen, Torben and Tchetgen Tchetgen, Eric},
	month = oct,
	year = {2019},
	pages = {639--659},
}

@article{tchetgen_tchetgen_instrumental_2015,
	title = {Instrumental {Variable} {Estimation} in a {Survival} {Context}:},
	volume = {26},
	issn = {1044-3983},
	shorttitle = {Instrumental {Variable} {Estimation} in a {Survival} {Context}},
	doi = {10.1097/EDE.0000000000000262},
	abstract = {Bias due to unobserved confounding can seldom be ruled out with certainty when estimating the causal effect of a nonradomized treatment. The instrumental variable (IV) design offers, under certain assumptions, the opportunity to tame confounding bias, without directly observing all confounders. The IV approach is very well developed in the context of linear regression and also for certain generalized linear models with a non-linear link function. However, IV methods are not as well developed for regression analysis with a censored survival outcome. In this paper, we develop the instrumental variable approach for regression analysis in a survival context, primarily under an additive hazards model, for which we describe two simple methods for estimating causal effects. The first method is a straightforward two-stage regression approach analogous to twostage least squares commonly used for IV analysis in linear regression. In this approach, the fitted value from a first -stage regression of the exposure on the IV is entered in place of the exposure in the second-stage hazard model to recover a valid estimate of the treatment effect of interest. The second method is a so-called control function approach, which entails adding to the additive hazards outcome model, the residual from a first-stage regression of the exposure on the IV. Formal conditions are given justifying each strategy, and the methods are illustrated in a novel application to a Mendelian randomization study to evaluate the effect of diabetes on mortality using data from the Health and Retirement Study. We also establish that analogous strategies can also be used under a proportional hazards model specification provided the outcome is rare over the entire follow-up.},
	language = {en},
	number = {3},
	urldate = {2025-01-07},
	journal = {Epidemiology},
	author = {Tchetgen Tchetgen, Eric J. and Walter, Stefan and Vansteelandt, Stijn and Martinussen, Torben and Glymour, Maria},
	month = may,
	year = {2015},
	pages = {402--410},
}

@article{wang_bounded_2018,
	title = {Bounded, {Efficient} and {Multiply} {Robust} {Estimation} of {Average} {Treatment} {Effects} {Using} {Instrumental} {Variables}},
	volume = {80},
	issn = {1369-7412, 1467-9868},
	doi = {10.1111/rssb.12262},
	abstract = {Instrumental variables (IVs) are widely used for estimating causal effects in the presence of unmeasured confounding. Under the standard IV model, however, the average treatment effect (ATE) is only partially identifiable. To address this, we propose novel assumptions that allow for identification of the ATE. Our identification assumptions are clearly separated from model assumptions needed for estimation, so that researchers are not required to commit to a specific observed data model in establishing identification. We then construct multiple estimators that are consistent under three different observed data models, and multiply robust estimators that are consistent in the union of these observed data models. We pay special attention to the case of binary outcomes, for which we obtain bounded estimators of the ATE that are guaranteed to lie between −1 and 1. Our approaches are illustrated with simulations and a data analysis evaluating the causal effect of education on earnings.},
	language = {en},
	number = {3},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Wang, Linbo and Tchetgen Tchetgen, Eric},
	month = jun,
	year = {2018},
	pages = {531--550},
}

@article{joffe_administrative_2001,
	title = {Administrative and artificial censoring in censored regression models},
	volume = {20},
	issn = {0277-6715, 1097-0258},
	abstract = {Administrative censoring, in which potential censoring times are known even for subjects who fail, is common in clinical and epidemiologic studies. Nonetheless, most statistical methods for failure-time data do not use the information contained in these potential censoring times. Robins has proposed two approaches for using this information to estimate parameters in an accelerated failure-time model; the methods generally require the analyst to treat as censored some subjects whose failure time is observed. This paper provides a rationale for this ‘artiÿcial censoring’, discusses some of its consequences, and illustrates some of these points with data from a randomized trial of breast cancer screening. Copyright ? 2001 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {15},
	urldate = {2025-01-07},
	journal = {Statistics in Medicine},
	author = {Joffe, Marshall M.},
	month = aug,
	year = {2001},
	pages = {2287--2304},
}

@article{abadie_semiparametric_2003,
title = {Semiparametric instrumental variable estimation of treatment response models},
journal = {Journal of Econometrics},
volume = {113},
number = {2},
pages = {231-263},
year = {2003},
issn = {0304-4076},
doi = {https://doi.org/10.1016/S0304-4076(02)00201-4},
author = {Alberto Abadie},
keywords = {Treatment effects, Semiparametric estimation, Compliers, 401(k)},
abstract = {This article introduces a new class of instrumental variable (IV) estimators for linear and nonlinear treatment response models with covariates. The rationale for focusing on nonlinear models is that, if the dependent variable is binary or limited, or if the effect of the treatment varies with covariates, a nonlinear model is appropriate. In the spirit of Roehrig (Econometrica 56 (1988) 433), identification is attained nonparametrically and does not depend on the choice of the parametric specification for the response function of interest. One virtue of this approach is that it allows the researcher to construct estimators that can be interpreted as the parameters of a well-defined approximation to a treatment response function under functional form misspecification. In contrast to some usual IV models, heterogeneity of treatment effects is not restricted by the identification conditions. The ideas and estimators in this article are illustrated using IV to estimate the effects of 401(k) retirement programs on savings.}
}

@article{florens_instrumental_2012,
    author = {Florens, Jean‐Pierre and Johannes, Jan and Van Bellegem, Sébastien},
    title = {Instrumental regression in partially linear models},
    journal = {The Econometrics Journal},
    volume = {15},
    number = {2},
    pages = {304-324},
    year = {2012},
    month = {07},
    abstract = {We consider the semi‐parametric regression model Y=Xtβ+φ(Z) where β and φ(·) are unknown slope coefficient vector and function, and where the variables (X, Z) are endogenous. We propose necessary and sufficient conditions for the identification of the parameters in the presence of instrumental variables. We also focus on the estimation of β. It is known that the presence of φ may lead to a slow rate of convergence for the estimator of β. An additional complication in the fully endogenous model is that the solution of the equation necessitates the inversion of a compact operator that has to be estimated non‐parametrically. In general this inversion is not stable, thus the estimation of β is ill‐posed. In this paper, a ‐consistent estimator for β is derived in this setting under mild assumptions. One of these assumptions is given by the so‐called source condition that is explicitly interpreted in the paper. Monte Carlo simulations demonstrate the reasonable performance of the estimation procedure on finite samples.},
    issn = {1368-4221},
    doi = {10.1111/j.1368-423X.2011.00358.x},
    eprint = {https://academic.oup.com/ectj/article-pdf/15/2/304/27674395/ectj0304.pdf},
}


@article{imbens_identification_1994,
	title = {Identification and {Estimation} of {Local} {Average} {Treatment} {Effects}},
	language = {en},
        year = {1994}, 
        journal = {Econometrica},
        volume = {62},
        number = {2},
        pages = {467-475},
	author = {Imbens, Guido W and Angrist, Joshua D},
}




@article{curth_survite_2021,
author = {Curth, Alicia and Lee, Changhee and van der Schaar, Mihaela},
title = {SurvITE: learning heterogeneous treatment effects from time-to-event data},
year = {2021},
journal = {NeurIPS 21': Proceedings of the 35th International Conference on Neural Information Processing Systems},
number = {2048},
pages = {26740-26753},

}

@book{klein_survival_2006,
  title={Survival analysis: techniques for censored and truncated data},
  author={Klein, John P and Moeschberger, Melvin L},
  year={2006},
  publisher={Springer Science \& Business Media}
}


@article{angrist_lifetime_1990,
 ISSN = {00028282},
 abstract = {The randomly assigned risk of induction generated by the draft lottery is used to construct estimates of the effect of veteran status on civilian earnings. These estimates are not biased by the fact that certain types of men are more likely than others to service in the military. Social Security administrative records indicate that in the early 1980s, long after their service in Vietnam was ended, the earnings of white veterans were approximately 15 percent less than the earnings of comparable nonveterans.},
 author = {Joshua D. Angrist},
 journal = {The American Economic Review},
 number = {3},
 pages = {313--336},
 publisher = {American Economic Association},
 title = {Lifetime Earnings and the Vietnam Era Draft Lottery: Evidence from Social Security Administrative Records},
 volume = {80},
 year = {1990}
}

@article{angrist_children_1998,
 ISSN = {00028282},
 abstract = {Research on the labor-supply consequences of childbearing is complicated by the endogeneity of fertility. This study uses parental preferences for a mixed sibling-sex composition to construct instrumental variables (IV) estimates of the effect of childbearing on labor supply. IV estimates for women are significant but smaller than ordinary least-squares estimates. The IV are also smaller for more educated women and show no impact of family size on husbands' labor supply. A comparison of estimates using sibling-sex composition and twins instruments implies that the impact of a third child disappears when the child reaches age 13.},
 author = {Joshua D. Angrist and William N. Evans},
 journal = {The American Economic Review},
 number = {3},
 pages = {450--477},
 publisher = {American Economic Association},
 title = {Children and Their Parents' Labor Supply: Evidence from Exogenous Variation in Family Size},
 volume = {88},
 year = {1998}
}

@article{cornfield_smoking_1959,
    author = {Cornfield, Jerome and Haenszel, William and Hammond, E. Cuyler and Lilienfeld, Abraham M. and Shimkin, Michael B. and Wynder, Ernst L.},
    title = {Smoking and Lung Cancer: Recent Evidence and a Discussion of Some Questions},
    journal = {JNCI: Journal of the National Cancer Institute},
    volume = {22},
    number = {1},
    pages = {173-203},
    year = {1959},
    month = {01},
    abstract = {This report reviews some of the more recent epidemiologic and experimental findings on the relationship of tobacco smoking to lung cancer, and discusses some criticisms directed against the conclusion that tobacco smoking, especially cigarettes, has a causal role in the increase in bronchogenic carcinoma. The magnitude of the excess lung-cancer risk among cigarette smokers is so great that the results can not be interpreted as arising from an indirect association of cigarette smoking with some other agent or characteristic, since this hypothetical agent would have to be at least as strongly associated with lung cancer as cigarette use; no such agent has been found or suggested. The consistency of all the epidemiologic and experimental evidence also supports the conclusion of a causal relationship with cigarette smoking, while there are serious inconsistencies in reconciling the evidence with other hypotheses which have been advanced. Unquestionably there are areas where more research is necessary, and, of course, no single cause accounts for all lung cancer. The information already available, however, is sufficient for planning and activating public health measures.},
    issn = {0027-8874},
    doi = {10.1093/jnci/22.1.173},
}

@inproceedings{jaroszewicz_uplift_2014,
  title={Uplift modeling with survival data},
  author={Szymon Jaroszewicz},
  year={2014},
  booktitle={ACM SIGKDD Workshop on Health Informatics (HI-KDD) New York City},
}

@article{zhang_mining_2017,
    author = {Zhang, Weijia and Le, Thuc Duy and Liu, Lin and Zhou, Zhi-Hua and Li, Jiuyong},
    title = {Mining heterogeneous causal effects for personalized cancer treatment},
    journal = {Bioinformatics},
    volume = {33},
    number = {15},
    pages = {2372-2378},
    year = {2017},
    month = {03},
    abstract = {Cancer is not a single disease and involves different subtypes characterized by different sets of molecules. Patients with different subtypes of cancer often react heterogeneously towards the same treatment. Currently, clinical diagnoses rather than molecular profiles are used to determine the most suitable treatment. A molecular level approach will allow a more precise and informed way for making treatment decisions, leading to a better survival chance and less suffering of patients. Although many computational methods have been proposed to identify cancer subtypes at molecular level, to the best of our knowledge none of them are designed to discover subtypes with heterogeneous treatment responses.In this article we propose the Survival Causal Tree (SCT) method. SCT is designed to discover patient subgroups with heterogeneous treatment effects from censored observational data. Results on TCGA breast invasive carcinoma and glioma datasets have shown that for each subtype identified by SCT, the patients treated with radiotherapy exhibit significantly different relapse free survival pattern when compared to patients without the treatment. With the capability to identify cancer subtypes with heterogeneous treatment responses, SCT is useful in helping to choose the most suitable treatment for individual patients.Data and code are available at https://github.com/WeijiaZhang24/SurvivalCausalTree.Supplementary data are available at Bioinformatics online.},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btx174},
}

@article{tabib_nonparametric_2020,
    author = {Tabib, Sami and Larocque, Denis},
    title = {Non-parametric individual treatment effect estimation for survival data with random forests},
    journal = {Bioinformatics},
    volume = {36},
    number = {2},
    pages = {629-636},
    year = {2020},
    month = {08},
    abstract = {Personalized medicine often relies on accurate estimation of a treatment effect for specific subjects. This estimation can be based on the subject’s baseline covariates but additional complications arise for a time-to-event response subject to censoring. In this paper, the treatment effect is measured as the difference between the mean survival time of a treated subject and the mean survival time of a control subject. We propose a new random forest method for estimating the individual treatment effect with survival data. The random forest is formed by individual trees built with a splitting rule specifically designed to partition the data according to the individual treatment effect. For a new subject, the forest provides a set of similar subjects from the training dataset that can be used to compute an estimation of the individual treatment effect with any adequate method.The merits of the proposed method are investigated with a simulation study where it is compared to numerous competitors, including recent state-of-the-art methods. The results indicate that the proposed method has a very good and stable performance to estimate the individual treatment effects. Two examples of application with a colon cancer data and breast cancer data show that the proposed method can detect a treatment effect in a sub-population even when the overall effect is small or nonexistent.The authors are working on an R package implementing the proposed method and it will be available soon. In the meantime, the code can be obtained from the first author at sami.tabib@hec.ca.Supplementary data are available at Bioinformatics online.},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz602},
}



@article{hendersen_individualized_2020,
    author = {Henderson, Nicholas C and Louis, Thomas A and Rosner, Gary L and Varadhan, Ravi},
    title = {Individualized treatment effects with censored data via fully nonparametric Bayesian accelerated failure time models},
    journal = {Biostatistics},
    volume = {21},
    number = {1},
    pages = {50-68},
    year = {2020},
    month = {07},
    abstract = {Individuals often respond differently to identical treatments, and characterizing such variability in treatment response is an important aim in the practice of personalized medicine. In this article, we describe a nonparametric accelerated failure time model that can be used to analyze heterogeneous treatment effects (HTE) when patient outcomes are time-to-event. By utilizing Bayesian additive regression trees and a mean-constrained Dirichlet process mixture model, our approach offers a flexible model for the regression function while placing few restrictions on the baseline hazard. Our nonparametric method leads to natural estimates of individual treatment effect and has the flexibility to address many major goals of HTE assessment. Moreover, our method requires little user input in terms of model specification for treatment covariate interactions or for tuning parameter selection. Our procedure shows strong predictive performance while also exhibiting good frequentist properties in terms of parameter coverage and mitigation of spurious findings of HTE. We illustrate the merits of our proposed approach with a detailed analysis of two large clinical trials (N = 6769) for the prevention and treatment of congestive heart failure using an angiotensin-converting enzyme inhibitor. The analysis revealed considerable evidence for the presence of HTE in both trials as demonstrated by substantial estimated variation in treatment effect and by high proportions of patients exhibiting strong evidence of having treatment effects which differ from the overall treatment effect.},
    issn = {1465-4644},
    doi = {10.1093/biostatistics/kxy028},
}



@article{zhu_targeted_2020,
title = {Targeted estimation of heterogeneous treatment effect in observational survival analysis},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103474},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103474},
author = {Jie Zhu and Blanca Gallego},
keywords = {Survival analysis, Machine learning, Heterogeneous treatment effect, Targeted maximum likelihood estimation, Oral anticoagulants},
abstract = {The aim of clinical effectiveness research using repositories of electronic health records is to identify what health interventions ‘work best’ in real-world settings. Since there are several reasons why the net benefit of intervention may differ across patients, current comparative effectiveness literature focuses on investigating heterogeneous treatment effect and predicting whether an individual might benefit from an intervention. The majority of this literature has concentrated on the estimation of the effect of treatment on binary outcomes. However, many medical interventions are evaluated in terms of their effect on future events, which are subject to loss to follow-up. In this study, we describe a framework for the estimation of heterogeneous treatment effect in terms of differences in time-to-event (survival) probabilities. We divide the problem into three phases: (1) estimation of treatment effect conditioned on unique sets of the covariate vector; (2) identification of features important for heterogeneity using non-parametric variable importance methods; and (3) estimation of treatment effect on the reference classes defined by the previously selected features, using one-step Targeted Maximum Likelihood Estimation. We conducted a series of simulation studies and found that this method performs well when either sample size or event rate is high enough and the number of covariates contributing to the effect heterogeneity is moderate. An application of this method to a clinical case study was conducted by estimating the effect of oral anticoagulants on newly diagnosed non-valvular atrial fibrillation patients using data from the UK Clinical Practice Research Datalink.}
}


@inproceedings{chapfuwa_enabling_2021,
author = {Chapfuwa, Paidamoyo and Assaad, Serge and Zeng, Shuxi and Pencina, Michael J. and Carin, Lawrence and Henao, Ricardo},
title = {Enabling counterfactual survival analysis with balanced representations},
year = {2021},
isbn = {9781450383592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3450439.3451875},
abstract = {Balanced representation learning methods have been applied successfully to counterfactual inference from observational data. However, approaches that account for survival outcomes are relatively limited. Survival data are frequently encountered across diverse medical applications, i.e., drug development, risk profiling, and clinical trials, and such data are also relevant in fields like manufacturing (e.g., for equipment monitoring). When the outcome of interest is a time-to-event, special precautions for handling censored events need to be taken, as ignoring censored outcomes may lead to biased estimates. We propose a theoretically grounded unified framework for counterfactual inference applicable to survival outcomes. Further, we formulate a nonparametric hazard ratio metric for evaluating average and individualized treatment effects. Experimental results on real-world and semi-synthetic datasets, the latter of which we introduce, demonstrate that the proposed approach significantly outperforms competitive alternatives in both survival-outcome prediction and treatment-effect estimation.},
booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
pages = {133–145},
numpages = {13},
keywords = {time-to-event, survival analysis, representation learning, hazard ratio, counterfactual inference, causal survival analysis},
location = {Virtual Event, USA},
series = {CHIL '21}
}


@article{hu_estimating_2021,
author = {Hu, Liangyuan and Ji, Jiayi and Li, Fan},
title = {Estimating heterogeneous survival treatment effect in observational data using machine learning},
journal = {Statistics in Medicine},
volume = {40},
number = {21},
pages = {4691-4713},
keywords = {Bayesian additive regression trees, causal inference, machine learning, observational studies, survival treatment effect heterogeneity},
doi = {https://doi.org/10.1002/sim.9090},
abstract = {Methods for estimating heterogeneous treatment effect in observational data have largely focused on continuous or binary outcomes, and have been relatively less vetted with survival outcomes. Using flexible machine learning methods in the counterfactual framework is a promising approach to address challenges due to complex individual characteristics, to which treatments need to be tailored. To evaluate the operating characteristics of recent survival machine learning methods for the estimation of treatment effect heterogeneity and inform better practice, we carry out a comprehensive simulation study presenting a wide range of settings describing confounded heterogeneous survival treatment effects and varying degrees of covariate overlap. Our results suggest that the nonparametric Bayesian Additive Regression Trees within the framework of accelerated failure time model (AFT-BART-NP) consistently yields the best performance, in terms of bias, precision, and expected regret. Moreover, the credible interval estimators from AFT-BART-NP provide close to nominal frequentist coverage for the individual survival treatment effect when the covariate overlap is at least moderate. Including a nonparametrically estimated propensity score as an additional fixed covariate in the AFT-BART-NP model formulation can further improve its efficiency and frequentist coverage. Finally, we demonstrate the application of flexible causal machine learning estimators through a comprehensive case study examining the heterogeneous survival effects of two radiotherapy approaches for localized high-risk prostate cancer.},
year = {2021}
}

@article{wang_instrumental_2022,
    author = {Wang, Linbo and Tchetgen Tchetgen, Eric and Martinussen, Torben and Vansteelandt, Stijn},
    title = {Instrumental Variable Estimation of the Causal Hazard Ratio},
    journal = {Biometrics},
    volume = {79},
    number = {2},
    pages = {539-550},
    year = {2022},
    month = {11},
    abstract = {Cox's proportional hazards model is one of the most popular statistical models to evaluate associations of exposure with a censored failure time outcome. When confounding factors are not fully observed, the exposure hazard ratio estimated using a Cox model is subject to unmeasured confounding bias. To address this, we propose a novel approach for the identification and estimation of the causal hazard ratio in the presence of unmeasured confounding factors. Our approach is based on a binary instrumental variable, and an additional no-interaction assumption in a first-stage regression of the treatment on the IV and unmeasured confounders. We propose, to the best of our knowledge, the first consistent estimator of the (population) causal hazard ratio within an instrumental variable framework. A version of our estimator admits a closed-form representation. We derive the asymptotic distribution of our estimator and provide a consistent estimator for its asymptotic variance. Our approach is illustrated via simulation studies and a data application.},
    issn = {0006-341X},
    doi = {10.1111/biom.13792},
}


@InProceedings{curth_nonparametric_2021,
  title = 	 { Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms },
  author =       {Curth, Alicia and van der Schaar, Mihaela},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1810--1818},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  abstract = 	 { The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed meta-learning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes. }
}


@InProceedings{curth_insearch_2023,
  title = 	 {In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation},
  author =       {Curth, Alicia and Van Der Schaar, Mihaela},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {6623--6642},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  abstract = 	 {Personalized treatment effect estimates are often of interest in high-stakes applications – thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global ‘winner’, we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, and provide interesting insights into the relative (dis)advantages of different criteria alongside desiderata for the design of further illuminating empirical studies in this context.}
}


@article{curth_review_2024,
author = {Curth, Alicia and Peck, Richard W. and McKinney, Eoin and Weatherall, James and van der Schaar, Mihaela},
title = {Using Machine Learning to Individualize Treatment Effect Estimation: Challenges and Opportunities},
journal = {Clinical Pharmacology \& Therapeutics},
volume = {115},
number = {4},
pages = {710-719},
doi = {https://doi.org/10.1002/cpt.3159},
abstract = {The use of data from randomized clinical trials to justify treatment decisions for real-world patients is the current state of the art. It relies on the assumption that average treatment effects from the trial can be extrapolated to patients with personal and/or disease characteristics different from those treated in the trial. Yet, because of heterogeneity of treatment effects between patients and between the trial population and real-world patients, this assumption may not be correct for many patients. Using machine learning to estimate the expected conditional average treatment effect (CATE) in individual patients from observational data offers the potential for more accurate estimation of the expected treatment effects in each patient based on their observed characteristics. In this review, we discuss some of the challenges and opportunities for machine learning to estimate CATE, including ensuring identification assumptions are met, managing covariate shift, and learning without access to the true label of interest. We also discuss the potential applications as well as future work and collaborations needed to further improve identification and utilization of CATE estimates to increase patient benefit.},
year = {2024}
}




@InProceedings{shalit_estimating_2017,
  title = 	 {Estimating individual treatment effect: generalization bounds and algorithms},
  author =       {Uri Shalit and Fredrik D. Johansson and David Sontag},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3076--3085},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf},
  abstract = 	 {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a “balanced” representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.}
}

@article{kjaersgaard_instrumental_2015,
    author = {Kjaersgaard, Maiken I. S. and Parner, Erik T.},
    title = {Instrumental Variable Method for Time-to-Event Data Using a Pseudo-Observation Approach},
    journal = {Biometrics},
    volume = {72},
    number = {2},
    pages = {463-472},
    year = {2015},
    month = {11},
    abstract = {Observational studies are often in peril of unmeasured confounding. Instrumental variable analysis is a method for controlling for unmeasured confounding. As yet, theory on instrumental variable analysis of censored time-to-event data is scarce. We propose a pseudo-observation approach to instrumental variable analysis of the survival function, the restricted mean, and the cumulative incidence function in competing risks with right-censored data using generalized method of moments estimation. For the purpose of illustrating our proposed method, we study antidepressant exposure in pregnancy and risk of autism spectrum disorder in offspring, and the performance of the method is assessed through simulation studies.},
    issn = {0006-341X},
    doi = {10.1111/biom.12451},
}



@article{bo_metalearner_2024,
    author = {Na Bo and Yue Wei and Lang Zeng and Chaeryon Kang and Ying Ding},
    title = {A Meta-Learner Framework to Estimate Individualized Treatment Effects for Survival Outcomes},
    journal = {Journal of Data Science},
    volume = {22},
    number = {4},
    year = {2024},
    pages = {505--523},
    doi = {10.6339/24-JDS1119},
    issn = {1680-743X},
    publisher = {School of Statistics, Renmin University of China}
}

@article{golmakani_superlearner_2020,
title = {Super Learner for Survival Data Prediction},
title = {},
author = {Marzieh K. Golmakani and Eric C. Polley},
pages = {20190065},
volume = {16},
number = {2},
journal = {The International Journal of Biostatistics},
doi = {doi:10.1515/ijb-2019-0065},
year = {2020},
lastchecked = {2025-01-29}
}


@article{SantAnna_nonparametric_2021,
author = {Pedro H. C. Sant’Anna},
title = {Nonparametric Tests for Treatment Effect Heterogeneity With Duration Outcomes},
journal = {Journal of Business \& Economic Statistics},
volume = {39},
number = {3},
pages = {816--832},
year = {2021},
publisher = {ASA Website},
doi = {10.1080/07350015.2020.1737080},
}

@article{woodbury_bonuses_1987,
 ISSN = {00028282},
 abstract = {New claimants for Unemployment Insurance were randomly assigned to one of two experiments that were designed to speed up the return to work. In the first experiment, a 500 bonus was offered to eligible claimants who obtained employment within 11 weeks. This experiment reduced the number of weeks of insured unemployment, averaged over all assigned claimants whether or not they participated, by more than one week. In the second experiment, the 500 bonus was offered to the subsequent employer of the eligible claimant. This experiment reduced the weeks of insured unemployment for only one important group--white women--by about one week.},
 author = {Stephen A. Woodbury and Robert G. Spiegelman},
 journal = {The American Economic Review},
 number = {4},
 pages = {513--530},
 publisher = {American Economic Association},
 title = {Bonuses to Workers and Employers to Reduce Unemployment: Randomized Trials in Illinois},
 volume = {77},
 year = {1987}
}


@article{meyer_what_1996,
 ISSN = {0734306X, 15375307},
 abstract = {This article analyzes an experimental program that offered payments to unemployment insurance (UI) recipients who found a job quickly. The experiment provided exogenous differences in individual incentives which I use to test labor supply and search theories of unemployment. I examine predictions about the timing of exits from unemployment and the effect of the fixed-amount bonus on different wage level groups. I also argue that the experimental evidence does not show the desirability of a permanent program. A permanent program would sharply increase the compensation for short UI spells, likely increasing the claims rate and possibly increasing unemployment.},
 author = {Bruce D. Meyer},
 journal = {Journal of Labor Economics},
 number = {1},
 pages = {26--51},
 publisher = {[University of Chicago Press, Society of Labor Economists, NORC at the University of Chicago]},
 title = {What Have We Learned from the Illinois Reemployment Bonus Experiment?},
 volume = {14},
 year = {1996}
}


@article{bijwaard_correcting_2005,
title = {Correcting for selective compliance in a re-employment bonus experiment},
journal = {Journal of Econometrics},
volume = {125},
number = {1},
pages = {77-111},
year = {2005},
note = {Experimental and non-experimental evaluation of economic policy and models},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2004.04.004},
author = {Govert E. Bijwaard and Geert Ridder},
keywords = {Randomized experiment, Non-compliance, Treatment effect, Instrumental Variable (IV), Mixed Proportional Hazard (MPH)},
abstract = {We propose a two-stage instrumental variable estimator that is consistent if there is selective compliance in the treatment group of a randomized experiment and the outcome variable is a censored duration. The estimator assumes full compliance in the control group. We use the estimator to re-analyze data from the Illinois re-employment bonus experiment.}
}