@article{abadie_semiparametric_2003,
title = {Semiparametric instrumental variable estimation of treatment response models},
journal = {Journal of Econometrics},
volume = {113},
number = {2},
pages = {231-263},
year = {2003},
issn = {0304-4076},
doi = {https://doi.org/10.1016/S0304-4076(02)00201-4},
author = {Alberto Abadie},
keywords = {Treatment effects, Semiparametric estimation, Compliers, 401(k)},
abstract = {This article introduces a new class of instrumental variable (IV) estimators for linear and nonlinear treatment response models with covariates. The rationale for focusing on nonlinear models is that, if the dependent variable is binary or limited, or if the effect of the treatment varies with covariates, a nonlinear model is appropriate. In the spirit of Roehrig (Econometrica 56 (1988) 433), identification is attained nonparametrically and does not depend on the choice of the parametric specification for the response function of interest. One virtue of this approach is that it allows the researcher to construct estimators that can be interpreted as the parameters of a well-defined approximation to a treatment response function under functional form misspecification. In contrast to some usual IV models, heterogeneity of treatment effects is not restricted by the identification conditions. The ideas and estimators in this article are illustrated using IV to estimate the effects of 401(k) retirement programs on savings.}
}

@article{angrist_identification_1996,
	title = {Identification of {Causal} {Effects} {Using} {Instrumental} {Variables}},
	volume = {91},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1996.10476902},
	language = {en},
	number = {434},
	urldate = {2025-01-07},
	journal = {Journal of the American Statistical Association},
	author = {Angrist, Joshua D. and Imbens, Guido W. and Rubin, Donald B.},
	month = jun,
	year = {1996},
	pages = {444--455},
}

@article{athey_generalized_2019,
	title = {Generalized random forests},
	volume = {47},
	issn = {0090-5364},
	doi = {10.1214/18-AOS1709},
	language = {en},
	number = {2},
	urldate = {2022-09-05},
	journal = {The Annals of Statistics},
	author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
	month = apr,
	year = {2019},
}

@article{bo_metalearner_2024,
    author = {Na Bo and Yue Wei and Lang Zeng and Chaeryon Kang and Ying Ding},
    title = {A Meta-Learner Framework to Estimate Individualized Treatment Effects for Survival Outcomes},
    journal = {Journal of Data Science},
    volume = {22},
    number = {4},
    year = {2024},
    pages = {505--523},
    doi = {10.6339/24-JDS1119},
    issn = {1680-743X},
    publisher = {School of Statistics, Renmin University of China}
}

@inproceedings{chapfuwa_enabling_2021,
author = {Chapfuwa, Paidamoyo and Assaad, Serge and Zeng, Shuxi and Pencina, Michael J. and Carin, Lawrence and Henao, Ricardo},
title = {Enabling counterfactual survival analysis with balanced representations},
year = {2021},
isbn = {9781450383592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3450439.3451875},
abstract = {Balanced representation learning methods have been applied successfully to counterfactual inference from observational data. However, approaches that account for survival outcomes are relatively limited. Survival data are frequently encountered across diverse medical applications, i.e., drug development, risk profiling, and clinical trials, and such data are also relevant in fields like manufacturing (e.g., for equipment monitoring). When the outcome of interest is a time-to-event, special precautions for handling censored events need to be taken, as ignoring censored outcomes may lead to biased estimates. We propose a theoretically grounded unified framework for counterfactual inference applicable to survival outcomes. Further, we formulate a nonparametric hazard ratio metric for evaluating average and individualized treatment effects. Experimental results on real-world and semi-synthetic datasets, the latter of which we introduce, demonstrate that the proposed approach significantly outperforms competitive alternatives in both survival-outcome prediction and treatment-effect estimation.},
booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
pages = {133–145},
numpages = {13},
keywords = {time-to-event, survival analysis, representation learning, hazard ratio, counterfactual inference, causal survival analysis},
location = {Virtual Event, USA},
series = {CHIL '21}
}

@article{cui_estimating_2023,
	title = {Estimating heterogeneous treatment effects with right-censored data via causal survival forests},
	volume = {85},
	issn = {1369-7412, 1467-9868},
	doi = {10.1093/jrsssb/qkac001},
	abstract = {Forest-based methods have recently gained in popularity for non-parametric treatment effect estimation. Building on this line of work, we introduce causal survival forests, which can be used to estimate heterogeneous treatment effects in survival and observational setting where outcomes may be right-censored. Our approach relies on orthogonal estimating equations to robustly adjust for both censoring and selection effects under unconfoundedness. In our experiments, we find our approach to perform well relative to a number of baselines.},
	language = {en},
	number = {2},
	urldate = {2024-01-26},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Cui, Yifan and Kosorok, Michael R and Sverdrup, Erik and Wager, Stefan and Zhu, Ruoqing},
	month = may,
	year = {2023},
	pages = {179--211},
}

@article{foster_subgroup_2011,
	title = {Subgroup identification from randomized clinical trial data},
	volume = {30},
	issn = {02776715},
	doi = {10.1002/sim.4322},
	abstract = {We consider the problem of identifying a subgroup of patients who may have an enhanced treatment effect in a randomized clinical trial, and it is desirable that the subgroup be defined by a limited number of covariates. For this problem, the development of a standard, pre-determined strategy may help to avoid the well-known dangers of subgroup analysis. We present a method developed to find subgroups of enhanced treatment effect. This method, referred to as “Virtual Twins”, involves predicting response probabilities for treatment and control “twins” for each subject. The difference in these probabilities is then used as the outcome in a classification or regression tree, which can potentially include any set of the covariates. We define a measure Q(Â) to be the difference between the treatment effect in estimated subgroup Â and the marginal treatment effect. We present several methods developed to obtain an estimate of Q(Â), including estimation of Q(Â) using estimated probabilities in the original data, using estimated probabilities in newly simulated data, two cross-validation-based approaches and a bootstrap-based bias corrected approach. Results of a simulation study indicate that the Virtual Twins method noticeably outperforms logistic regression with forward selection when a true subgroup of enhanced treatment effect exists. Generally, large sample sizes or strong enhanced treatment effects are needed for subgroup estimation. As an illustration, we apply the proposed methods to data from a randomized clinical trial.},
	language = {en},
	number = {24},
	urldate = {2023-05-03},
	journal = {Statistics in Medicine},
	author = {Foster, Jared C. and Taylor, Jeremy M.G. and Ruberg, Stephen J.},
	month = oct,
	year = {2011},
	pages = {2867--2880},
}

@article{golmakani_superlearner_2020,
title = {Super Learner for Survival Data Prediction},
title = {},
author = {Marzieh K. Golmakani and Eric C. Polley},
pages = {20190065},
volume = {16},
number = {2},
journal = {The International Journal of Biostatistics},
doi = {doi:10.1515/ijb-2019-0065},
year = {2020},
lastchecked = {2025-01-29}
}

@incollection{goos_ensemble_2000,
	address = {Berlin, Heidelberg},
	title = {Ensemble {Methods} in {Machine} {Learning}},
	volume = {1857},
	isbn = {978-3-540-67704-8 978-3-540-45014-6},
	abstract = {Ensemble methods are learning algorithms that construct a set of classi ers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classi er. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not over t rapidly.},
	language = {en},
	urldate = {2023-05-13},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dietterich, Thomas G.},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
	year = {2000},
	doi = {10.1007/3-540-45014-9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--15},
	file = {Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:/Users/tomer/Zotero/storage/UUVDVCJA/Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:application/pdf},
}

@article{hendersen_individualized_2020,
    author = {Henderson, Nicholas C and Louis, Thomas A and Rosner, Gary L and Varadhan, Ravi},
    title = {Individualized treatment effects with censored data via fully nonparametric Bayesian accelerated failure time models},
    journal = {Biostatistics},
    volume = {21},
    number = {1},
    pages = {50-68},
    year = {2020},
    month = {07},
    abstract = {Individuals often respond differently to identical treatments, and characterizing such variability in treatment response is an important aim in the practice of personalized medicine. In this article, we describe a nonparametric accelerated failure time model that can be used to analyze heterogeneous treatment effects (HTE) when patient outcomes are time-to-event. By utilizing Bayesian additive regression trees and a mean-constrained Dirichlet process mixture model, our approach offers a flexible model for the regression function while placing few restrictions on the baseline hazard. Our nonparametric method leads to natural estimates of individual treatment effect and has the flexibility to address many major goals of HTE assessment. Moreover, our method requires little user input in terms of model specification for treatment covariate interactions or for tuning parameter selection. Our procedure shows strong predictive performance while also exhibiting good frequentist properties in terms of parameter coverage and mitigation of spurious findings of HTE. We illustrate the merits of our proposed approach with a detailed analysis of two large clinical trials (N = 6769) for the prevention and treatment of congestive heart failure using an angiotensin-converting enzyme inhibitor. The analysis revealed considerable evidence for the presence of HTE in both trials as demonstrated by substantial estimated variation in treatment effect and by high proportions of patients exhibiting strong evidence of having treatment effects which differ from the overall treatment effect.},
    issn = {1465-4644},
    doi = {10.1093/biostatistics/kxy028},
}

@article{hernan_instruments_2006,
	title = {Instruments for {Causal} {Inference}: {An} {Epidemiologist}'s {Dream}?},
	volume = {17},
	issn = {1044-3983},
	shorttitle = {Instruments for {Causal} {Inference}},
	doi = {10.1097/01.ede.0000222409.00878.37},
	abstract = {The use of instrumental variable (IV) methods is attractive because, even in the presence of unmeasured confounding, such methods may consistently estimate the average causal effect of an exposure on an outcome. However, for this consistent estimation to be achieved, several strong conditions must hold. We review the deﬁnition of an instrumental variable, describe the conditions required to obtain consistent estimates of causal effects, and explore their implications in the context of a recent application of the instrumental variables approach. We also present (1) a description of the connection between 4 causal models— counterfactuals, causal directed acyclic graphs, nonparametric structural equation models, and linear structural equation models—that have been used to describe instrumental variables methods; (2) a uniﬁed presentation of IV methods for the average causal effect in the study population through structural mean models; and (3) a discussion and new extensions of instrumental variables methods based on assumptions of monotonicity.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Epidemiology},
	author = {Hernán, Miguel A. and Robins, James M.},
	month = jul,
	year = {2006},
	pages = {360--372},
}

@article{kjaersgaard_instrumental_2015,
    author = {Kjaersgaard, Maiken I. S. and Parner, Erik T.},
    title = {Instrumental Variable Method for Time-to-Event Data Using a Pseudo-Observation Approach},
    journal = {Biometrics},
    volume = {72},
    number = {2},
    pages = {463-472},
    year = {2015},
    month = {11},
    abstract = {Observational studies are often in peril of unmeasured confounding. Instrumental variable analysis is a method for controlling for unmeasured confounding. As yet, theory on instrumental variable analysis of censored time-to-event data is scarce. We propose a pseudo-observation approach to instrumental variable analysis of the survival function, the restricted mean, and the cumulative incidence function in competing risks with right-censored data using generalized method of moments estimation. For the purpose of illustrating our proposed method, we study antidepressant exposure in pregnancy and risk of autism spectrum disorder in offspring, and the performance of the method is assessed through simulation studies.},
    issn = {0006-341X},
    doi = {10.1111/biom.12451},
}

@article{kunzel_metalearners_2019,
	title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1804597116},
	abstract = {Significance
            Estimating and analyzing heterogeneous treatment effects is timely, yet challenging. We introduce a unifying framework for many conditional average treatment effect estimators, and we propose a metalearner, the X-learner, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect. We present its favorable properties, using theory and simulations. We apply it, using random forests, to two field experiments in political science, where it is shown to be easy to use and to produce results that are interpretable.
          , 
            There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
	language = {en},
	number = {10},
	urldate = {2023-05-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
	month = mar,
	year = {2019},
	pages = {4156--4165},
}

@article{mackenzie_using_2014,
	title = {Using instrumental variables to estimate a {Cox}’s proportional hazards regression subject to additive confounding},
	volume = {14},
	issn = {1387-3741, 1572-9400},
	doi = {10.1007/s10742-014-0117-x},
	abstract = {The estimation of treatment effects is one of the primary goals of statistics in medicine. Estimation based on observational studies is subject to confounding. Statistical methods for controlling bias due to confounding include regression adjustment, propensity scores and inverse probability weighted estimators. These methods require that all confounders are recorded in the data. The method of instrumental variables (IVs) can eliminate bias in observational studies even in the absence of information on confounders. We propose a method for integrating IVs within the framework of Cox's proportional hazards model and demonstrate the conditions under which it recovers the causal effect of treatment. The methodology is based on the approximate orthogonality of an instrument with unobserved confounders among those at risk. We derive an estimator as the solution to an estimating equation that resembles the score equation of the partial likelihood in much the same way as the traditional IV estimator resembles the normal equations. To justify this IV estimator for a Cox model we perform simulations to evaluate its operating characteristics. Finally, we apply the estimator to an observational study of the effect of coronary catheterization on survival.},
	language = {en},
	number = {1-2},
	journal = {Health Services and Outcomes Research Methodology},
	author = {MacKenzie, Todd A. and Tosteson, Tor D. and Morden, Nancy E. and Stukel, Therese A. and O’Malley, A. James},
	month = jun,
	year = {2014},
	pages = {54--68},
}

@article{martinussen_instrumental_2017,
	title = {Instrumental {Variables} {Estimation} of {Exposure} {Effects} on a {Time}-to-{Event} {Endpoint} {Using} {Structural} {Cumulative} {Survival} {Models}},
	volume = {73},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0006-341X, 1541-0420},
	doi = {10.1111/biom.12699},
	abstract = {The use of instrumental variables for estimating the effect of an exposure on an outcome is popular in econometrics, and increasingly so in epidemiology. This increasing popularity may be attributed to the natural occurrence of instrumental variables in observational studies that incorporate elements of randomization, either by design or by nature (e.g., random inheritance of genes). Instrumental variables estimation of exposure effects is well established for continuous outcomes and to some extent for binary outcomes. It is, however, largely lacking for time-to-event outcomes because of complications due to censoring and survivorship bias. In this paper, we make a novel proposal under a class of structural cumulative survival models which parameterize time-varying effects of a point exposure directly on the scale of the survival function; these models are essentially equivalent with a semi-parametric variant of the instrumental variables additive hazards model. We propose a class of recursive instrumental variable estimators for these exposure effects, and derive their large sample properties along with inferential tools. We examine the performance of the proposed method in simulation studies and illustrate it in a Mendelian randomization study to evaluate the effect of diabetes on mortality using data from the Health and Retirement Study. We further use the proposed method to investigate potential benefit from breast cancer screening on subsequent breast cancer mortality based on the HIP-study.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Biometrics},
	author = {Martinussen, Torben and Vansteelandt, Stijn and Tchetgen Tchetgen, Eric J. and Zucker, David M.},
	month = dec,
	year = {2017},
	pages = {1140--1149},
}

@article{robins_estimation_1994,
	title = {Estimation of {Regression} {Coefficients} {When} {Some} {Regressors} are not {Always} {Observed}},
	volume = {89},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1994.10476818},
	language = {en},
	number = {427},
	urldate = {2023-01-31},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
	month = sep,
	year = {1994},
	pages = {846--866},
}

@article{sorensen_causal_2019,
	title = {A causal proportional hazards estimator under homogeneous or heterogeneous selection in an {IV} setting},
	volume = {25},
	issn = {1380-7870, 1572-9249},
	doi = {10.1007/s10985-019-09476-y},
	abstract = {In this paper we present a framework to do estimation in a structural Cox model when there may be unobserved confounding. The model is phrased in terms of a selection bias function and a baseline model that describes how covariates affect the survival time in a scenario without exposure. In this way model congeniality is ensured. The method uses an instrumental variable. Interestingly, the formulated model turns out to have similarities to the so-called Cox–Aalen survival model for the observed data. We exploit this to enhance estimation of the unknown parameters. This also allows us to derive large sample properties of the proposed estimator.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Lifetime Data Analysis},
	author = {Sørensen, Ditte Nørbo and Martinussen, Torben and Tchetgen Tchetgen, Eric},
	month = oct,
	year = {2019},
	pages = {639--659},
}

@article{tchetgen_tchetgen_instrumental_2015,
	title = {Instrumental {Variable} {Estimation} in a {Survival} {Context}:},
	volume = {26},
	issn = {1044-3983},
	shorttitle = {Instrumental {Variable} {Estimation} in a {Survival} {Context}},
	doi = {10.1097/EDE.0000000000000262},
	abstract = {Bias due to unobserved confounding can seldom be ruled out with certainty when estimating the causal effect of a nonradomized treatment. The instrumental variable (IV) design offers, under certain assumptions, the opportunity to tame confounding bias, without directly observing all confounders. The IV approach is very well developed in the context of linear regression and also for certain generalized linear models with a non-linear link function. However, IV methods are not as well developed for regression analysis with a censored survival outcome. In this paper, we develop the instrumental variable approach for regression analysis in a survival context, primarily under an additive hazards model, for which we describe two simple methods for estimating causal effects. The first method is a straightforward two-stage regression approach analogous to twostage least squares commonly used for IV analysis in linear regression. In this approach, the fitted value from a first -stage regression of the exposure on the IV is entered in place of the exposure in the second-stage hazard model to recover a valid estimate of the treatment effect of interest. The second method is a so-called control function approach, which entails adding to the additive hazards outcome model, the residual from a first-stage regression of the exposure on the IV. Formal conditions are given justifying each strategy, and the methods are illustrated in a novel application to a Mendelian randomization study to evaluate the effect of diabetes on mortality using data from the Health and Retirement Study. We also establish that analogous strategies can also be used under a proportional hazards model specification provided the outcome is rare over the entire follow-up.},
	language = {en},
	number = {3},
	urldate = {2025-01-07},
	journal = {Epidemiology},
	author = {Tchetgen Tchetgen, Eric J. and Walter, Stefan and Vansteelandt, Stijn and Martinussen, Torben and Glymour, Maria},
	month = may,
	year = {2015},
	pages = {402--410},
}

@article{zhu_recursively_2012,
	title = {Recursively {Imputed} {Survival} {Trees}},
	volume = {107},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2011.637468},
	abstract = {We propose recursively imputed survival tree (RIST) regression for right-censored data. This new nonparametric regression procedure uses a novel recursive imputation approach combined with extremely randomized trees that allows significantly better use of censored data than previous tree based methods, yielding improved model fit and reduced prediction error. The proposed method can also be viewed as a type of Monte Carlo EM algorithm which generates extra diversity in the tree-based fitting process. Simulation studies and data analyses demonstrate the superior performance of RIST compared to previous methods.},
	language = {en},
	number = {497},
	journal = {Journal of the American Statistical Association},
	author = {Zhu, Ruoqing and Kosorok, Michael R.},
	month = mar,
	year = {2012},
	pages = {331--340},
}

@article{zhu_targeted_2020,
title = {Targeted estimation of heterogeneous treatment effect in observational survival analysis},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103474},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103474},
author = {Jie Zhu and Blanca Gallego},
keywords = {Survival analysis, Machine learning, Heterogeneous treatment effect, Targeted maximum likelihood estimation, Oral anticoagulants},
abstract = {The aim of clinical effectiveness research using repositories of electronic health records is to identify what health interventions ‘work best’ in real-world settings. Since there are several reasons why the net benefit of intervention may differ across patients, current comparative effectiveness literature focuses on investigating heterogeneous treatment effect and predicting whether an individual might benefit from an intervention. The majority of this literature has concentrated on the estimation of the effect of treatment on binary outcomes. However, many medical interventions are evaluated in terms of their effect on future events, which are subject to loss to follow-up. In this study, we describe a framework for the estimation of heterogeneous treatment effect in terms of differences in time-to-event (survival) probabilities. We divide the problem into three phases: (1) estimation of treatment effect conditioned on unique sets of the covariate vector; (2) identification of features important for heterogeneity using non-parametric variable importance methods; and (3) estimation of treatment effect on the reference classes defined by the previously selected features, using one-step Targeted Maximum Likelihood Estimation. We conducted a series of simulation studies and found that this method performs well when either sample size or event rate is high enough and the number of covariates contributing to the effect heterogeneity is moderate. An application of this method to a clinical case study was conducted by estimating the effect of oral anticoagulants on newly diagnosed non-valvular atrial fibrillation patients using data from the UK Clinical Practice Research Datalink.}
}

