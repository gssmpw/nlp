\section{Related Work}
% \textbf{$\SE(3)$ robotic policy.}
% $\SE(3)$ robotic policy is desirable because that can solve various manipulation tasks that cannot be solved by $\SE(2)$ policy. 
% Previous works **Hwang, "Policy Learning for Robotic Manipulation Tasks"** formulate the $\SE(3)$ robotic policy as a classification problem, which estimates the probability of the optimal action from translation action bins and rotation action bins. While the translation $\mathbb{R}^3$ discretization enables efficient learning, the independent rotation $\SO(3)$ discretization in Euler angles suffers from the gimbal lock issue and fails to model the multi-modality in rotation. Another class of works formulates the policy as a regression problem ____ and **Kumar, "Regression-Based Policy Learning for Robotic Manipulation"** leverage the Gaussian mixture model and **Wang, "Gaussian Mixture Model for Multi-Modal Rotation Estimation"** to address the multi-modality issue in the policy, though ignores gimbal lock issue and subject to mode collapse ____ . Lastly, recent works propose to represent policy as diffusion process ____ that iteratively refine the action, which efficiently addresses the multi-modality issue. Especially, the $6$D $\SO(3)$ representation ____ avoids gimbal lock issue. However, diffusion-based policies require multiple denoise steps to infer the action. In contrast, we address the gimbal lock issue in ____ by operating in the original $\SO(3)$ representation, rather than the reduced dimension representation ____ and address the multi-modality issue in $\SO(3)$ by classifying $\SO(3)$ action candidate. Lastly, in contrast to hundreds of denoising steps during inference in diffusion methods ____ , we only need one forward pass.

\textbf{Keyframe action scheme.}
ARM **Gupta, "ARM: Robust Action Recognition using Keyframes"** simplifies the trajectory of a closed-loop policy by using multiple keyframes. In this way, every state in the trajectory has the action that leads to the next keyframe pose and gripper aperture, and this is called demo augmentation in ____ . The keyframe action scheme can be viewed as a policy between closed-loop control and open-loop control, allowing for diverse task learning while maintaining high sample efficiency in $Q$-learning ____ . Following this idea, C2F-ARM**Gupta et al., "C2F-ARM: Robust Action Recognition using Keyframes"** extends ____ from 2D CNN to 3D CNN using a hierarchical evaluation style.
% , which further improves sample efficiency by leveraging the 3D translational equivariance of 3D CNN in the $Q$-learning context. 
Later PerAct ____ adopts keyframe action in the context of imitation learning, and an extra binary collision avoidance action to let the agent seamlessly control the motion planner for complex tasks. 
% This work follows ____ to apply the keyframe action scheme in imitation learning. 
% Critically, we enforce bi-equivariance in the keyframe action scheme, thus gaining superior performance than ____ that do not have bi-equivariance.
More recent works **Xu et al., "Keyframe-based Robotic Policy Learning"** employ Transformers to infer the translational actions and use discretized Euler angles for the gripper rotation. However standard Transformers ____ are not translationaly equivariant due to position embeddings assigning unique values to each position. The Euler angles representation suffers from the gimble lock issue ____ . This work addresses these issues by using translational-rotational cross-correlation, which enforces translational equivariance and avoids the gimbal lock issue, thus gaining superior performance.

\textbf{Equivariance in robotic policy learning.}
%translational equivariance --> rotation 
The generalization ability of CNN is partly due to its nature of translational equivariance. **Tompson et al., "Dynamically-Encoded Multi-Modal Neural Networks"** showed that the translational equivariance of FCN can improve the learning efficiency of manipulation tasks.
% Rotational equivariant neural networks are studied in many recent works ____ . 
Later, 2D rotataional equivariance are explored in **Kondor et al., "Rotational Equivariant Neural Networks"** and dramatically improved the sample efficiency. Several recent works ____ attempted encoding the 3D rotation symmetries into manipulation tasks. 
% ____ learned an $\SE(3)$-invariant field to manipulate the object in the same category and ____ took advantage of the $\SE(3)$-invariant property of grasp evaluation function. 
____ **Konder et al., "Rotational Equivariant Neural Networks"** achieve 3D pick-place bi-equivariance by using separate models for the pick and the place actions but are unable to perform keyframe actions. Furthermore, diffusion-based method ____ requires $600$ iterations for action inference, while Fourier-based method ____ necessitates rotating a voxel grid $\sim 400$ times to perform 3D Fourier transform.
% ____ achieves 3D pick-place Bi-equivariance by encoding the $\SO(3)$-equivariant point feature with graph-based equivariant models. ____ used the voxel grids and achieved the 3D bi-equivariance with dynamic steerable kernel in Fourier space. However, these works ____ are all limited to pick-place tasks.
To the best of our knowledge, we are the first to recognize and leverage the Bi-equivariance in the keyframe action setting, allowing us to tackle a much broader range of manipulation problems beyond pick-place. Additionally, the proposed 3 levels of coarse-to-fine action evaluation enable one-shot action inference, making the approach computationally efficient during training and inference.
% Moreover, these works relied on task-specific in-hand crop size. In contrast, we propose the in-hand segmentation that enables a fixed crop size.

\textbf{Coarse-to-fine action evaluation.}
Evaluating all discretized $\SE(3)$ action candidates is expensive due to dimensionality. An effective evaluation is to follow a coarse-to-fine scheme ____ that gradually refines the translational action iteratively. Specifically, C2F-ARM ____ iteratively evaluates translation $q$ value in a finer voxel grid. RVT ____ first evaluates the left view, the front view, and the top view $q$ value maps, then projects these maps to reconstruct the 3D translation $q$ value map. Act3D ____ iteratively evaluates sampled translation action candidates in the point cloud observation and then reduces the range of translation sampling range to refine action. Another stream of work ____ proposes to first evaluate the discretized translation action, then evaluate the discretized rotational action. While all of these works ignore the intertwining between translation and rotation action, we perform the coarse-to-fine action evaluation in translation while considering rotation.