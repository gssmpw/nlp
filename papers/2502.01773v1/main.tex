%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{times}
% <<< from RVT

\usepackage{times}

% numbers option provides compact numerical references in the text. 
% \usepackage[numbers]{natbib}
% \usepackage{multicol}
% \usepackage[bookmarks=true]{hyperref}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
% \usepackage{color}
\usepackage{soul}
\usepackage{bbm}
% additional packages
\usepackage{booktabs}
% \usepackage[table]{xcolor}
\usepackage{vcell}
\usepackage{subcaption}
% \usepackage{subfigure}
% \usepackage[font=scriptsize,labelfont=sl,labelsep=period]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{bm}
% \usepackage{wrapfig}
\usepackage{multirow}
% \usepackage{enumitem}
% numbers option provides compact numerical references in the text.
% \usepackage[bookmarks=true]{hyperref}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{subfiles}
\usepackage{gensymb}
\newtheorem{proposition}{Proposition}

% manual commands
\newcommand{\smallsec}[1]{\noindent {\bf #1.}}
\newcommand{\method}{RVT\xspace}
\newcommand{\highlight}[1]{\textcolor{black}{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\yes}{\color{blue}{\ding{51}}}
\newcommand{\no}{\color{red}{\ding{55}}}
\newcommand{\blank}{\underline \underline \ }
\newcommand{\rpm}{\tiny \raisebox{.2ex}{$\scriptstyle\pm~$}}
\newcommand{\rpmh}{\huge \raisebox{.2ex}{$\scriptstyle\pm~$}}
\newcommand{\rpmx}{\small \raisebox{.2ex}{$\scriptstyle\pm~$}}
% \newcommand{\todo}[1]{\textcolor{red}{[{\bf todo: #1}]}}
\newcommand{\todo}[1]{\textcolor{black}{{ }}}

% Baseline Names
\newcommand{\bcz}{Image-BC\xspace}
\newcommand{\bczcnn}{Image-BC (CNN)\xspace}
\newcommand{\bczvit}{Image-BC (ViT)\xspace}
\newcommand{\unet}{C2F-ARM-BC\xspace}
\newcommand{\peract}{PerAct\xspace}
% from RVT >>>

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\SE}{\mathrm{SE}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\pick}{\mathrm{pick}}
\newcommand{\place}{\mathrm{place}}
\newcommand{\out}{\mathrm{out}}
\newcommand{\ing}{\mathrm{in}}
\newcommand{\ours}{\textsc{???}}

\DeclareMathOperator{\key}{key}
\DeclareMathOperator{\query}{query}

\newcommand{\quot}{\mathrm{quot}}
\newcommand{\irrep}{\mathrm{irrep}}
\newcommand{\Rot}{\mathrm{Rot}}
\newcommand{\regu}{\mathrm{reg}}

\newcommand{\Ind}{\mathrm{Ind}}

\newcommand{\eref}{Equation~\ref}
\newcommand{\fgref}{Figure~\ref}


\title{\LARGE \bf
% Coarse-to-Fine $\SE(3)$ Transporter
Coarse-to-Fine 3D Keyframe Transporter
}


\author{Xupeng Zhu$^{1}$, David Klee*$^{1}$, Dian Wang*$^{1}$, Boce Hu$^{1}$, Haojie Huang$^{1}$, Arsh Tangri$^{1}$, \\ Robin Walters$^{1, 2}$, Robert Platt$^{1, 2}$ \\
\small{$^{1}$Northeastern University $^{2}$Boston Dynamics AI Institute}% <-this % stops a space
\thanks{* denotes equal contribution.}% <-this % stops a space
}


\begin{document}





\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

% Symmetry is ubiquitous in robotic manipulation policy learning. Exploiting the symmetries effectively improves the policy, especially learning from limited demonstrations using imitation learning (IL). 
Recent advances in Keyframe Imitation Learning (IL) have enabled learning-based agents to solve a diverse range of manipulation tasks.  However, most approaches ignore the rich symmetries in the problem setting and, as a consequence, are sample-inefficient. This work identifies and utilizes the bi-equivariant symmetry within Keyframe IL to design a policy that generalizes to transformations of both the workspace and the objects grasped by the gripper. We make two main contributions: First, we analyze the bi-equivariance properties of the keyframe action scheme and propose a Keyframe Transporter derived from the Transporter Networks, which evaluates actions using cross-correlation between the features of the grasped object and the features of the scene. Second, we propose a computationally efficient coarse-to-fine $\SE(3)$ action evaluation scheme for reasoning the intertwined translation and rotation action. The resulting method outperforms strong Keyframe IL baselines by an average of $>10\%$ on a wide range of simulation tasks, and by an average of $55\%$ in 4 physical experiments.

% Although recent advances in Keyframe Imitation Learning (IL) allow learning-based agents to solve a diverse range of manipulation tasks, they frequently neglect the rich symmetries in the problem setting, making those methods sample-inefficient. This work identifies and utilizes the bi-equivariant symmetry within Keyframe IL, allowing the policy to adapt simultaneously to the transformation changes on both the workspace and the objects grasped by the gripper. We make two main contributions: First, we analyze the bi-equivariance in the keyframe action scheme and propose a Keyframe Transporter derived from the Transporter Networks, which evaluates actions using cross-correlation between the features of the grasped object and the features of the scene. Second, we propose a computationally efficient coarse-to-fine $\SE(3)$ action evaluation scheme for reasoning the intertwined translation and rotation action. The resulting method outperforms strong Keyframe IL baselines by an average of $>10\%$ on a wide range of simulation tasks, and by an average of $55\%$ in 4 physical experiments.

% Keyframe Imitation Learning (IL) learns an open-loop policy from closed-loop demonstrations that can solve a diverse range of manipulation tasks \cite{james2021arm, shridhar2023perceiver}. While current methods utilize the expressiveness of Transformer \cite{vaswani2017attention} and MLP \cite{popescu2009multilayer}, they frequently neglect the rich symmetries in the Keyframe IL. This work identifies and utilizes the bi-equivariant symmetry within Keyframe IL, allowing the policy to adapt simultaneously to the transformation changes on both the workspace and the objects grasped by the gripper. We make two main contributions: First, we analyze the bi-equivariance in the keyframe action scheme \cite{james2021arm} and propose a Keyframe Transporter derived from the Transporter Networks \cite{zeng2021transporter}, which evaluates actions using cross-correlation between the features of the grasped object and the features of the scene. Second, we propose a computationally efficient coarse-to-fine $\SE(3)$ action evaluation scheme for reasoning the intertwined translation and rotation action. The resulting method outperforms strong Keyframe IL baselines by an average of $>10\%$ on 18 RLBench simulation tasks \cite{james2020rlbench} and by an average of $55\%$ in 4 physical experiments.

\end{abstract}


\section{Introduction}

% Intro:
% -- Keyframe IL is hard but important problem
% -- standard approaches do not leverage geometric structure to the extent that they could; e.g. RVT and peract are both based on transformers that destroy structure in their self attention layers
% -- we propose an approach that combines the cross attention of transporter with the c2f of c2farm to better structure the model. cross attention gives us bi-equivariance; c2f gives as additional inductive bias.
% -- the result is a significant improvement in inductive bias that improves sample efficiency
% -- this combination of cross attention and c2f is novel. rvt is completely different model architecture b/c it is based on a transformer. c2f is different b/c it does not use self attention.

Imitation Learning (IL) has emerged as an important approach for manipulation tasks. 
% IL learns policies from demonstration data, mapping sensory inputs to manipulator actions. Learning policies that act in both translation and rotation actions, i.e., $\SE(3)$ gripper pose space, is particularly important and has been an area of rapid progress~\cite{gervet2023act3d,rvt,shridhar2023perceiver, james2022coarse, chi2023diffusionpolicy}. 
IL trains a neural network on human demonstrations to map sensory inputs into robot actions. Such demonstrations are usually limited, 
% Imitation Learning usually learns from limited human demonstrations, 
since collecting demonstrations by hand is expensive. To improve sample efficiency, Keyframe IL mimics a few keyframe gripper poses in the demonstration trajectory instead of mimicking the entire trajectory. Despite the improvement, Keyframe IL ignores the geometric structures in the manipulation policy.

Current research efforts ~\cite{james2022coarse, shridhar2023perceiver,rvt,gervet2023act3d} on Keyframe IL utilize the expressiveness of Transformer~\cite{vaswani2017attention} to infer translational actions and employed Multilayer Perceptrons (MLPs)~\cite{popescu2009multilayer} to evaluate discretized Euler angle for rotation actions. However, these design choices destroy the symmetries in the policy. Transformers are not translationally equivariant due to positional embeddings and fail to enforce locality due to the global attention mechanism. On the other hand, the Euler angle representation suffers from the discontinuity or the gimbal lock issue~\cite{5D_SO3}.

% Meanwhile, symmetry is inherent in keyframe action schemes. However, current works ~\cite{james2022coarse, shridhar2023perceiver,rvt,gervet2023act3d} partially utilize the symmetry in the keyframe action, thus exhibiting poor performance when learning from limited data.

% A key challenge in BC for robotic manipulation is data efficiency, i.e. the ability to learn a policy with from few demonstrations. Since demonstrations are often collected by hand, a requirement for large numbers of demonstrations is expensive and limits the flexibility of the method. Unfortunately, many current methods are not very sample efficient~\cite{padalkar2023open,bharadhwaj2023roboagent}. 

% Transporter leverages rich geometry structure in 2d pick-place, but cannot be trivially extended to 3d and non-pick-place tasks; 
% coarse-to-fine alleviates the compute burden in 3d but ignores the intertwined translation-rotation action.

%{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{

% Bi-equivariance of the robot pick-place policy has been explored in Transporter Network~\cite{zeng2021transporter} and Equivariant Transporter Network~\cite{Huang-RSS-22} wherein the desired policy generalizes to independent changes in both the pick and the place pose. However, these works use an independent pick policy and a place policy and thus are limited to pick-place tasks. Moreover, extending these $\SE(2)$ Transporter Networks to $\SE(3)$ action space is not trivial, because $\SE(3)$ action evaluation leads to significant compute increase than $\SE(2)$. For example, discretizing $\SO(2)$ action using $7.5^\circ$ bin leads to $48$ bins, but discretizing $\SO(3)$ using the similar resolution leads to $36864$ bins\cite{gorski2005healpix}, thus efficient compute is critical.

This work exploits the geometric structure of Keyframe IL to design a more sample-efficient method. We first identify a generalized form of bi-equivariant symmetry in the Keyframe IL, which extends beyond the place bi-equivariance discussed in prior works~\cite{zeng2021transporter,Huang-RSS-22,ryu2023equivariant} wherein the desired policy generalizes to independent changes in both the pick and place poses. To incorporate this property, we propose a Keyframe IL method via cross-correlation. Essentially, we derived from the place module of the Transporter Networks~\cite{zeng2021transporter, Huang-RSS-22}, where the pose actions are inferred by performing cross-correlation between a voxel representation of the scene and a dynamic kernel that represents the geometry of the grasped object. Nevertheless, extending the 2D cross-correlation in Transporter Networksto 3D suffers from curse-of-dimensionality. In 2D, the cross-correlation is performed on 3 dimensions (X, Y axes and planner rotation). In contrast, in 3D, it expands to 6 dimensions (X, Y, Z axes and roll, pitch, yaw angles), making direct computation infeasible.

%}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}

%{{{{{{{{{{{{{{{{{{{{{{{{{{{{
% The coarse-to-fine technique~\cite{Gualtieri2019hierarchical,Gualtieri2020hierarchical,james2022coarse,gervet2023act3d} effectively reduces compute in translational action evaluation by evaluating the translational actions at a coarse level of resolution, identifying areas of interest, and then zooming into the translational actions in those areas to select fine-grained actions. However, these coarse-to-fine works ignore the interaction between rotation and translation and do not reduce the compute in rotation action evaluation.

% This paper is inspired by two techniques, both of which are based on leveraging geometric structures present in the robotics domain to bias the neural network model toward good policies. 
% The first technique leverages the bi-equivariance between the pick and the place pose wherein the desired policy generalizes to independent changes in both the pick and the place pose. This is perhaps best exemplified by pick/place models like Transporter Network~\cite{zeng2021transporter} and Equivariant Transporter Net~\cite{huang2023leveraging} that explicitly encode the symmetries involved in the relative rotations of pick and place poses.
% The second technique is to leverages the locality assumption about information relevant to decision making. This is embedded in a coarse-to-fine model that evaluates $\SE(3)$ actions at a coarse level of resolution, identifies areas of interest, and then zooms into the $\SE(3)$ actions in those areas to select fine-grained actions. This approach works best in the keyframe setting~\cite{rvt} where the agent selects full 6-DOF poses toward which to move the robot hand. The model first selects an approximate region and then selects a precise action. Gualtieri et al.~\cite{Gualtieri2019hierarchical,Gualtieri2020hierarchical} demonstrated that this approach significantly improved performance in the open-loop control tasks and James et al.~\cite{james2022coarse} made a similar finding in the Reinforcement Learning (RL) setting.

% Extending these two ideas is not trivial. The Transporter Networks\cite{zeng2021transporter, huang2023leveraging, cliport} are limited to pick-place and $\SE(2)$ action space. To adopt Transporter to $\SE(3)$ keyframe setup, one needs to overcome the pick-place constraint, and efficiently compute the cross-correlation in $\SE(3)=\mathbb{R}^3\rtimes\SO(3)$. Discretizing $\SO(2)$ action using $7.5^\circ$ bin leads to $48$ bins, but discretizing $\SO(3)$ using the similar resolution leads to $36864$ bins\cite{gorski2005healpix}, thus efficient compute is critical. On the other hand, the current line of coarse-to-fine works\cite{Gualtieri2020hierarchical, james2022coarse, gervet2023act3d} in robot learning ignores the intertwining between rotation and translation, only reducing compute in the translation.

% The proposed approach unleashes the power of Transporter Networks and coarse-to-fine technique in the $\SE(3)$ keyframe action setting, where the model outputs a full 6-DoF gripper pose that describes where to move the robot hand next. We recognize that the bi-equivariance property commonly exists in the keyframe action scheme (as shown in Figure \ref{fig:bi_equ}) and propose a keyframe action via cross-correlation to embed this property. Essentially, we derived from the place module of the Transporter Networks~\cite{zeng2021transporter, Huang-RSS-22} architecture to infer the pose action by performing a cross-correlation between a voxel representation of the scene and a dynamic kernel that represents the geometry of the grasped object. Moreover, this paper extend coarse-to-fine idea \cite{Gualtieri2019hierarchical,Gualtieri2020hierarchical,james2022coarse} from $\mathbb{R}^3$ to $\SE(3)=\mathbb{R}^3\rtimes\SO(3)$ to realistically evaluate the $\SE(3)$ actions. The proposed $\SE(3)$ coarse-to-fine action inference effectively alleviates the curse-of-dimensionality of $\SE(3)$ action evaluation. Our resulting model can learn a variety of manipulation behaviors including pushing, turning, using tools, etc. Our method outperforms baselines by $>10\%$ on 18 RLBench simulation tasks and by $55\%$ on 4 real-world tasks.  

To overcome the efficiency issue in 3D cross-correlation evaluation, this paper further proposes an $\SE(3)$ coarse-to-fine (C2F) calculation, extending previous translational-only C2F methods \cite{Gualtieri2020hierarchical,james2022coarse} to translation and rotation actions simultaneously. The proposed $\SE(3)$ C2F method begins by coarsely evaluating the translation and rotation actions, identifying the best $\SE(3)$ action. Then the method refines the evaluation by zooming into the best coarse $\SE(3)$ action and evaluating its neighboring translation and rotation actions. This hierarchical method drastically improved efficiency in 3D cross-correlation.

Our resulting model can learn a wide range of manipulation behaviors, including pushing, turning, using tools, etc. with a single unified architecture. This distinguishes it from prior bi-equivariant approaches~\cite{zeng2021transporter,Huang-RSS-22,ryu2023equivariant,yenchen2022mira,ryu2023diffusionedfs,huang2024fourier}, which are limited to pick-place tasks and require separated modules for picking and placing. Moreover, in contrast to existing Keyframe IL methods\cite{shridhar2023perceiver, rvt, gervet2023act3d} that are based on Transformers and Euler angles, our method evaluates action by cross-correlation on translation and rotation, effectively embeds bi-equivariance, and mitigates the gimbal lock issue. Our method outperforms Keyframe IL baselines by $>10\%$ on 18 RLBench simulation tasks with $100$ demonstrations and achieves a $55\%$ improvement on 4 real-world tasks when trained with only $10$ demonstrations.

%}}}}}}}}}}}}}}}}}}}}}}}}}}}}

\section{Related Work}

% \textbf{$\SE(3)$ robotic policy.}
% $\SE(3)$ robotic policy is desirable because that can solve various manipulation tasks that cannot be solved by $\SE(2)$ policy. 
% Previous works \cite{james2022coarse, shridhar2023perceiver, rvt, gervet2023act3d} formulate the $\SE(3)$ robotic policy as a classification problem, which estimates the probability of the optimal action from translation action bins and rotation action bins. While the translation $\mathbb{R}^3$ discretization enables efficient learning, the independent rotation $\SO(3)$ discretization in Euler angles suffers from the gimbal lock issue and fails to model the multi-modality in rotation. Another class of works formulates the policy as a regression problem \cite{end2end_deep_visuomotor}. Following this, \cite{LSTM_GMM, zhu2023viola} leverages the Gaussian mixture model and \cite{zhao2023learning} utilizes conditional variation autoencoder to address the multi-modality issue in the policy, though ignores gimbal lock issue and subject to mode collapse \cite{florence2022implicit}. Lastly, recent works propose to represent policy as diffusion process \cite{chi2023diffusionpolicy, ryu2023equivariant, xian2023chaineddiffuser} that iteratively refine the action, which efficiently addresses the multi-modality issue. Especially, the $6$D $\SO(3)$ representation \cite{5D_SO3,chi2023diffusionpolicy} avoids gimbal lock issue. However, diffusion-based policies require multiple denoise steps to infer the action. In contrast, we address the gimbal lock issue in \cite{james2022coarse, shridhar2023perceiver, rvt, gervet2023act3d} by operating in the original $\SO(3)$ representation, rather than the reduced dimension representation \cite{5D_SO3} and address the multi-modality issue in $\SO(3)$ by classifying $\SO(3)$ action candidate. Lastly, in contrast to hundreds of denoising steps during inference in diffusion methods \cite{chi2023diffusionpolicy, ryu2023diffusionedfs, xian2023chaineddiffuser}, we only need one forward pass.

\textbf{Keyframe action scheme.}
ARM \cite{james2021arm} simplifies the trajectory of a closed-loop policy by using multiple keyframes. In this way, every state in the trajectory has the action that leads to the next keyframe pose and gripper aperture, and this is called demo augmentation in \cite{james2021arm}. The keyframe action scheme can be viewed as a policy between closed-loop control and open-loop control, allowing for diverse task learning while maintaining high sample efficiency in $Q$-learning \cite{james2021arm}. Following this idea, C2F-ARM\cite{james2022coarse} extends \cite{james2021arm} from 2D CNN to 3D CNN using a hierarchical evaluation style.
% , which further improves sample efficiency by leveraging the 3D translational equivariance of 3D CNN in the $Q$-learning context. 
Later PerAct \cite{shridhar2023perceiver} adopts keyframe action in the context of imitation learning, and an extra binary collision avoidance action to let the agent seamlessly control the motion planner for complex tasks. 
% This work follows \cite{shridhar2023perceiver} to apply the keyframe action scheme in imitation learning. 
% Critically, we enforce bi-equivariance in the keyframe action scheme, thus gaining superior performance than \cite{shridhar2023perceiver, james2022coarse, rvt, gervet2023act3d} that do not have bi-equivariance.
More recent works ~\cite{shridhar2023perceiver, rvt, gervet2023act3d} employ Transformers to infer the translational actions and use discretized Euler angles for the gripper rotation. However standard Transformers\cite{vaswani2017attention} are not translationaly equivariant due to position embeddings assigning unique values to each position. The Euler angles representation suffers from the gimble lock issue~\cite{5D_SO3}. This work addresses these issues by using translational-rotational cross-correlation, which enforces translational equivariance and avoids the gimbal lock issue, thus gaining superior performance.

\textbf{Equivariance in robotic policy learning.}
%translational equivariance --> rotation 
The generalization ability of CNN is partly due to its nature of translational equivariance. \cite{zeng2018robotic,Morrison-RSS-18} showed that the translational equivariance of FCN can improve the learning efficiency of manipulation tasks.
% Rotational equivariant neural networks are studied in many recent works \cite{cohen2016group,cohensteerable,weiler2019general,thomas2018tensor,fuchs2020se,deng2021vector,cesa2022a}. 
Later, 2D rotataional equivariance are explored in \cite{wang2021equivariant,zhu2022grasp,Huang-RSS-22,zhu2023grasp,jia2023seil,wang2022so2equivariant, wang2022onrobot, zhao2022integrating,wangequivariant} and dramatically improved the sample efficiency. Several recent works~\cite{simeonov2022neural, huang2022edge,ryu2023equivariant,huang2024fourier,ryu2023diffusionedfs,huorbitgrasp,huang2024imagination} attempted encoding the 3D rotation symmetries into manipulation tasks. 
% \cite{simeonov2022neural} learned an $\SE(3)$-invariant field to manipulate the object in the same category and \cite{huang2022edge} took advantage of the $\SE(3)$-invariant property of grasp evaluation function. 
\cite{ryu2023equivariant,ryu2023diffusionedfs,huang2024fourier} achieve 3D pick-place bi-equivariance by using separate models for the pick and the place actions but are unable to perform keyframe actions. Furthermore, diffusion-based method \cite{ryu2023diffusionedfs} requires $600$ iterations for action inference, while Fourier-based method \cite{huang2024fourier} necessitates rotating a voxel grid $\sim 400$ times to perform 3D Fourier transform.
% \cite{ryu2023equivariant,ryu2023diffusionedfs} achieves 3D pick-place Bi-equivariance by encoding the $\SO(3)$-equivariant point feature with graph-based equivariant models. \cite{huang2024fourier} used the voxel grids and achieved the 3D bi-equivariance with dynamic steerable kernel in Fourier space. However, these works \cite{zeng2021transporter, Huang-RSS-22, huang2024fourier, ryu2023equivariant, ryu2023diffusionedfs} are all limited to pick-place tasks.
To the best of our knowledge, we are the first to recognize and leverage the Bi-equivariance in the keyframe action setting, allowing us to tackle a much broader range of manipulation problems beyond pick-place. Additionally, the proposed 3 levels of coarse-to-fine action evaluation enable one-shot action inference, making the approach computationally efficient during training and inference.
% Moreover, these works relied on task-specific in-hand crop size. In contrast, we propose the in-hand segmentation that enables a fixed crop size.

\textbf{Coarse-to-fine action evaluation.}
Evaluating all discretized $\SE(3)$ action candidates is expensive due to dimensionality. An effective evaluation is to follow a coarse-to-fine scheme~\cite{Gualtieri2020hierarchical, james2022coarse, gervet2023act3d} that gradually refines the translational action iteratively. Specifically, C2F-ARM \cite{james2022coarse} iteratively evaluates translation $q$ value in a finer voxel grid. RVT \cite{rvt} first evaluates the left view, the front view, and the top view $q$ value maps, then projects these maps to reconstruct the 3D translation $q$ value map. Act3D \cite{gervet2023act3d} iteratively evaluates sampled translation action candidates in the point cloud observation and then reduces the range of translation sampling range to refine action. Another stream of work \cite{wang2021policy, wang2021equivariant, zhu2022grasp} proposes to first evaluate the discretized translation action, then evaluate the discretized rotational action. While all of these works ignore the intertwining between translation and rotation action, we perform the coarse-to-fine action evaluation in translation while considering rotation.

\section{Background}

\textbf{Equivariance.} An equivariant function possesses the property that when the input is transformed, the output transforms accordingly. For instance, consider a planner equivariant grasping function $Q$ ~\cite{zhu2022grasp}, which takes the scene $s$ as input and outputs the gripper grasping location $a$. If the scene is transformed by a planar translation and rotation $g\in \SE(2)$, the gripper pose transforms accordingly:
\begin{align}
    Q(s) = a,\quad Q(gs) = ga.
\end{align}

\textbf{Keyframe Imitation Learning.} A keyframe action policy \cite{james2021arm, james2022coarse} specifies how the robot should move by defining a sequence of desired translations and rotations, i.e., $\SE(3)$ end-effector poses. When executing a keyframe action, the robot queries a collision-free motion planner to compute a trajectory that reaches the desired pose. The keyframe policy solves a task by executing a sequence of keyframe actions.
% In this paper, we learn a policy $\pi$ that maps from the current observation $o$ onto a keyframe action. 
%Similar to ~\cite{shridhar2023perceiver,rvt,gervet2023act3d}, we formulate keyframe imitation learning as a classification task
Keyframe Imitation Learning \cite{shridhar2023perceiver,rvt,gervet2023act3d} is a classification task that learns the action-value function $Q$ over a discretized action space, aiming to maximize the value for the discretized expert keyframe action $a$, given the observation $o$. The keyframe actions are extracted from expert demonstration trajectories by identifying moments when the gripper velocity is zero or its aperture changes \cite{james2021arm}. The observation, $o = \{s, p\}$, contains a scene representation $s$ (e.g., voxel or point cloud), and proprioceptive information, $p=\{\text{T}_{ee}, s_\text{open}, t\}$, where $\text{T}_{ee}$ is the gripper pose, $s_\text{open}$ is the gripper aperture, and $t$ is the time step. The action $a = \{a_\text{T}, a_\text{open}, a_\text{collide}\}$ specifies the desired $\SE(3)$ gripper pose $a_\text{T}$ with translation and rotation, the gripper open-close action  $a_\text{open}$, and a binary flag $a_\text{collide}$ to indicate whether to ignore collisions in the motion planning. Compared to higher frequency closed-loop control policies that output arm displacements, the keyframe framework significantly reduces the time horizon over which the policy must reason and thereby simplifies the policy learning problem.

\begin{figure}
    \centering
    % \vspace{-0.5cm}
    \includegraphics[width=0.4\textwidth]{figure/transporter.png}
    \caption{The place module of Transporter Networks \cite{zeng2021transporter}, along with follow-up works \cite{Huang-RSS-22, ryu2023equivariant, huang2024fourier, ryu2023equivariant} achieves bi-equivariance in place policy (e.g., picking an ``L'' shape and placing it in an ``L''-shaped receptacle) by performing cross-correlation between the scene features $f_s$ and the in-hand features $f_{ih}$. In the computed action value map $Q_\text{place}$, the height and width represent the X and Y translations of the gripper, while the channels correspond to different gripper rotations. Therefore $Q_\text{place}$ densely evaluates each trans-rotational action.}
    \label{fig:transporter}
    \vspace{-0.2cm}
\end{figure}

\textbf{The place module of Transporter Network.} The Transporter Network~\cite{zeng2021transporter, Huang-RSS-22} includes a planar pick and a planar place module that encodes rich geometric structure. The pick module is omitted for simplicity. The place module, illustrated in Figure \ref{fig:transporter}, takes the observation $s$ and the pick location $a^*_\text{pick}$ as inputs. The place module $Q_\text{place}(s, a^*_\text{pick})$ crops the observation at the pick location as the in-hand observation: $s_{ih}=crop(s, a^*_\text{pick})$. Then both the scene observation and the in-hand observation are embedded into deep latent features, maintaining the same spatial resolution, through a $\key$ and a $\query$ Unet network $f_s = \key(s), f_{ih}^i = \query(g_i s_{ih}), i \in \{1,2,...,n\}$. $f_s$ is the scene feature containing the receptacle, and $[f_{ih}^i]$ is a stack of in-hand features corresponding to each possible rotation action $g_i = \frac{2\pi i}{c}$, produced by passing rotated versions of the in-hand object observation to the $\query$ network. The place action value $Q_\text{place}$ is the result of $2$D cross-correlation in $\SE(2)$ action space between the scene features $f_s$ and each rotated in-hand feature $f_{ih}^i$, $Q_\text{place}^i = f_s \star f_{ih}^i$. The place action $a_\text{place}\in \SE(2)$ is the argmax over the place action value, $a_\text{place} = \pi(s,s_{ih}) = \arg\max Q_\text{place}(s,s_{ih})$. As \cite{Huang-RSS-22} states, if $\key$ and $\query$ networks are equivariant, then $Q_\text{place}$ is bi-equivariant due to the bi-equivariant properties of cross-correlation. However, the original Transporter Network is incompatible with keyframe action because it uses separate, specialized networks for inferring the pick action and the place action, which are coupled in a hard-coded inference sequence.
% specialized networks for inferring the pick action (equivariant actions) and the place action (bi-equivariant actions) which are coupled in a hard-coded inference sequence.

\section{Method}

% Method:

% 3.1: Keyframe IL via cross correlation
%     -- we propose an approach based on the "place" pathway in Transporter Net
%     -- briefly outline the original 2d version of transporter modified to accommodate a gripper, as we did in equi-transporter
%     -- describe a 3d version modified to use 3d convolutions. describe how this would work with a discrete set of rotations
%     -- use equation 1 in this description. It's fine to leave the \forall g \in G in eq 1; just make it clear that this means that we are evaluating the translational convolution, not rotational.
%     -- make it clear what kind of encoders (not equivariant) we are using here.
%     -- provide a picture of transporter model that has "encoders" that can be treated as either 2d or 3d.
%     -- describe how this model encodes bi-equivariance into the model. give the equations for what bi-equivariance is and describe how the model accomplishes it.
%     -- describe how this version is intractible b/c of the large number of rotations that need to be processed.
    
% 3.2: Coarse-to-fine cross correlation model
%     -- describe how c2f can fix the problem of intractibility in principle
%     -- describe c2f in detail using equation 2.
%     -- refer to fig 2. go into detail about exactly how the c2f works in the model, i.e. that features are computed once and that multiple cross convolutions occur. Note that this is different from what was done in c2f arm or gualtieri's work where features were recomputed at each level.
%     -- in fig 2, it's a little hard to understand the orange hand orientations. you should explain that somewhere.
%     -- describe how c2f provides an additional inductive bias based on an assumption that precise position and orientation only depends on local geometry around the place location.
    
% 3.3 Implementation
%     -- similar to what you already have except to improve the notation as we discussed.


% Xupeng's plan for the method section:
% Problem statement
%     -- introduce what is Keyframe IL
%     -- states bi-equ in Keyframe IL

% 3.1: Keyframe IL via cross correlation
%     -- briefly outline the original 2d version of transporter modified to accommodate a gripper, as we did in equi-transporter
%            provide a picture of transporter model that has "encoders" that can be treated as either 2d or 3d.
%            use equation 1 in this description. It's fine to leave the \forall g \in G in eq 1; just make it clear that this means that we are evaluating the translational convolution, not rotational.
%            describe how this model encodes bi-equivariance into the model.
%     -- we propose an approach based on the "place" pathway in Transporter Net
%            b/c we can make place module compatible with keyframe action
%            b/c it encodes bi-equivariance into the model
%     -- describe a keyframe version modified to use 3d convolutions.
%            describe how this would work with a discrete set of rotations
%            make it clear what kind of encoders (not equivariant) we are using here.
%            we crop and canonicalize the in-hand observation, this enables the algorithm to switch between bi-equ and equ automatically.
%     -- describe how this version is intractible b/c of the large number of rotations that need to be processed.

% 3.2: Coarse-to-fine cross correlation model
%     -- describe how c2f can fix the problem of intractibility in principle
%     -- describe c2f in detail using equation 2.
%     -- refer to fig 2. go into detail about exactly how the c2f works in the model, i.e. that features are computed once and that multiple cross convolutions occur. Note that this is different from what was done in c2f arm or gualtieri's work where features were recomputed at each level.
%     -- in fig 2, it's a little hard to understand the orange hand orientations. you should explain that somewhere.
%     -- X(describe how c2f provides an additional inductive bias)X -> we didn't have locality bias, b/c we used Unet that has global reception field
    
% 3.3 Implementation
%     -- similar to what you already have except to improve the notation as we discussed.

\begin{figure}\centering
        \includegraphics[height=3.5cm]{figure/bi_equivairance1.png}
        % \includegraphics[height=4cm]{figure/bi_equivairance2.png}
        \caption{Bi-equivariance in keyframe policies. Second column: given a scene, the policy $\pi$ prescribes an optimal action $a$. First column: if the scene is rotated by $g_1$, the optimal action should also be rotated: $g_1a$. Third column: if the in-hand object is rotated by $g_2$, the optimal action should pre-rotate to compensate: $ag_2^{-1}$.
        % The last column: our method swings the golf club properly, while RVT~\cite{rvt} cannot distinguish the head and the grip of the golf club.
        }
        \vspace{-0.2cm}
\label{fig:bi_equ}
\end{figure}

\subsection{Bi-equivariance of keyframe policy.}

We find that the keyframe action policy exhibits bi-equivariance with respect to the pose action on both the scene and the in-hand objects (see Figure \ref{fig:bi_equ}). Consider the keyframe policy, denoted by the simplified notation $\pi(s,s_{ih})=a^*_\text{T}$ which takes as input the scene observation $s$ in the world frame and the in-hand observation $s_{ih}$ (the object held by the gripper) in the gripper frame, and outputs the keyframe pose action $a^*_\text{T}$. The first equivariance is when the scene is transformed by $g_1\in\SE(3)$, the pose action should be transformed by,
\begin{align}
    g_1 a^*_\text{T} = \pi( g_1 \cdot s, s_{ih})
    \label{equ:bi_equ1}
\end{align}
The second equivariance is when the grasped object is transformed by $g_2\in\SE(3)$, the pose action should compensate for this transformation by inversely transforming by $g_2^{-1}$,
\begin{align}
    a^*_\text{T}g_2^{-1} = \pi(s, g_2 \cdot s_{ih})
    \label{equ:bi_equ2}
\end{align}

Moreover, when there is grasped object(s), both Equations \ref{equ:bi_equ1} and \ref{equ:bi_equ2} are satisfied, defining what term as bi-equivariant actions, e.g., placing and using tools. When the action depends solely on the gripper, the bi-equivariance of the policy degenerates to equivariance due to the fixed gripper pose ($g_2$ becomes identity). We refer to this type of action as equivariant actions, e.g., grasping and pushing. This framework unifies the previously separate concepts of pick equivariance \cite{zhu2022grasp,zhu2023grasp,huang2022edge} and place bi-equivariance~\cite{Huang-RSS-22, huang2024fourier, ryu2023equivariant, ryu2023diffusionedfs} under a cohesive keyframe action scheme. 

% \textbf{Bi-equivariance of keyframe policy.} We find that the keyframe policy $\pi(o) = a^*$ exhibits bi-equivariance property (with respect to $a^*_\text{T}$), as demonstrated in Figure \ref{fig:bi_equ}. Simplified the keyframe policy $\pi(s,s_{ih})=a^*_\text{T}$ that takes as input the scene observation $s$, the in-hand observation (object) $s_{ih}$, and outputs the keyframe pose action $a^*_\text{T}$. The first equivariance is when the scene is transformed by $g_1\in\SE(3)$, the pose action should be transformed by $g_1a$ accordingly,
% \begin{align}
%     g_1 a^*_\text{T} = \pi( g_1 \cdot s, s_{ih})
%     \label{equ:bi_equ1}
% \end{align}
% The second equivariance is when the grasped object is transformed by $g_2\in\SE(3)$, the pose action should compensate for this transformation by inversely transforming by $g_2^{-1}$,
% \begin{align}
%     a^*_\text{T}g_2^{-1} = \pi(s, g_2 \cdot s_{ih})
%     \label{equ:bi_equ2}
% \end{align}
% 
% This extends the pick-place bi-equivariance~\cite{Huang-RSS-22, huang2023leveraging, huang2024fourier, ryu2023equivariant, ryu2023diffusionedfs} to keyframe action scheme. Moreover, when there is grasped object(s), Equation \ref{equ:bi_equ1}, \ref{equ:bi_equ2} hold. We term this type of action as bi-equivariant actions, e.g., placing, and using tools. When the action solely depends on the gripper, then the bi-equivariance of the policy degenerates to equivariance due to the fixed gripper pose. We term this type of action as equivariant actions, e.g., grasping and pushing.


% 3.1: Keyframe IL via cross correlation
%     -- briefly outline the original 2d version of transporter modified to accommodate a gripper, as we did in equi-transporter
%            provide a picture of transporter model that has "encoders" that can be treated as either 2d or 3d.
%            use equation 1 in this description. It's fine to leave the \forall g \in G in eq 1; just make it clear that this means that we are evaluating the translational convolution, not rotational.
%            describe how this model encodes bi-equivariance into the model.
%     -- we propose an approach based on the "place" pathway in Transporter Net
%            b/c we can make place module compatible with keyframe action
%            b/c it encodes bi-equivariance into the model
%     -- describe a keyframe version modified to use 3d convolutions.
%            describe how this would work with a discrete set of rotations
%            make it clear what kind of encoders (not equivariant) we are using here.
%            we crop and canonicalize the in-hand observation, this enables the algorithm to switch between bi-equ and equ automatically.
%     -- describe how this version is intractible b/c of the large number of rotations that need to be processed.


\begin{figure*}\centering
    % \hspace*{-1.5cm}
    % \vspace{-0.5cm}
    \includegraphics[width=0.95\textwidth]{figure/ours_method.png}
    \caption{\textbf{Coarse-to-Fine $3$D Keyframe Transporter} inferences in two steps. Left: in step 1, the in-hand features $s_{ih}$ are obtained by cropping and transforming the scene features $s$ into the gripper frame. Then the $\key$ and $\query$ U-net networks map observations $s$ and $s_{ih}$ into pyramids of latent features $f_s^l$ and $f_{ih}^l$ respectively. Middle: in step 2, the action values $Q_\text{T}^l: \hat{G}_l \rightarrow \mathrm{R}$ are computed through a coarse-to-fine cross-correlation between the latent scene features $f_s^l$ and in-hand features $f_{ih}$. At the coarse level, the evaluated actions cover a wide translational-rotational range in a coarse grid. In the end, the fine level narrows the trans-roto range but provides fine resolution for precise action evaluation.
    % At the initial level, $\hat{G}_1$ covers the entire $\SE(3)$ action space with coarse resolution. In the following levels, $\hat{G}_l$ reduces translation and rotation action range but increases resolution. 
    Lastly, gripper open-close and planner collision actions are evaluated by MLP with the features from the $key$ U-net.}
    \label{fig:c2f_bi_equ}
    \vspace{-0.2cm}
\end{figure*}

\subsection{Keyframe IL via $3$D Cross-Correlation}
\label{sec:place_only}

We propose Keyframe IL via $3$D cross-correlation, inspired by the $2$D Transporter Net~\cite{zeng2021transporter}, to capture rich geometric structures of bi-equivariance. Several modifications are made to incorporate Transporter into the keyframe action scheme, as detailed below.

One modification is the use of the place module while discarding the pick module. Keyframe actions can be viewed as a special case of placing where anything held by the gripper is considered the in-hand object being placed, and the target pose is treated as the receptacle. This framework allows the in-hand object to be the gripper itself when no object is being grasped. Consequently, actions that rely solely on the gripper (e.g., grasping, pushing), can be interpreted as placing the gripper onto the target object. We represent the in-hand object $s_{ih}$ by canonicalizing (aligning) the scene voxel map $s$ to the gripper (end-effector) frame: $s_{ih} = crop(\text{T}_\text{ee}^{-1}\cdot s)$. 
% In-hand segmentation is applied to retrieve the feature of the grasped object while removing detractors. 
If the in-hand observation consists only of the canonicalized gripper, as in the case of equivariant actions (e.g., picking), the proposed bi-equivariant module naturally simplifies to a single equivariance. Conversely, when the in-hand observation includes an object, the bi-equivariance property remains intact. This adaptability ensures compatibility with the keyframe bi-equivariance.

Another modification is the adaptation of the $2$D place module to a $3$D setting. To do so, the $\key$ and $\query$ Unets are replaced with $3$D Unet\cite{Unet, 3DUnet}, and the action value becomes the result of $3$D cross-correlation between scene features and in-hand features for each discretized pose action $a_\text{T}\in\SE(3)$.


% Despite these two modifications, extending the place module of the Transporter Network to $3$D is computationally infeasible. 
However, extending the place module of the Transporter Network to $3$D poses significant computational challenges. While $2$D cross-correlation operates across 3 dimensions (X, Y axes, and planar rotation), our method performs $3$D cross-correlation across 6 dimensions (X, Y, Z axes and roll, pitch, yaw angles). This results in an exponentially increased computational cost.

% Despite these two modifications, trivially extending the place module of the Transporter Network to $3$D is infeasible because calculating the cross-correlation in $\SE(3)$ is computationally intensive. Transporter Networks perform $2$D cross-correlation that has 3 dimensions but our method performs $3$D cross-correlation on 6 dimensions (X, Y, Z axes and roll, pitch, yaw angles).

% $2$D cross-correlation of Equation \ref{equ:place} has complexity $O(n^3m^2)$ but that of $3$D cross-correlation version is $O(n^6m^3)$, where $n$ is resolution of each dimension of action space, $m$ is resolution of the features $f, f_{ih}$. We address this issue in the next Section 

\subsection{$\SE(3)$ Coarse-to-Fine Action Evaluation}
\label{sec: cross_correlation}

% 3.2: Coarse-to-fine cross correlation model
%     -- describe how c2f can fix the problem of intractibility in principle
%     -- describe c2f in detail using equation 2.
%     -- refer to fig 2. go into detail about exactly how the c2f works in the model, i.e. that features are computed once and that multiple cross convolutions occur. X(Note that this is different from what was done in c2f arm or gualtieri's work where features were recomputed at each level.)X [Removed this statement b/c Act3D did it similarly to ours.]
%     -- in fig 2, it's a little hard to understand the orange hand orientations. you should explain that somewhere.
%     -- X(describe how c2f provides an additional inductive bias)X -> we didn't have locality bias, b/c we used Unet that has global reception field

% Even though extending Transporter Network\cite{zeng2021transporter} to $\SE(3)$ could be trivially done by implementing the $\key$ and $\query$ networks with two 3D U-net \cite{Unet, 3DUnet}, calculating the cross-correlation in Transporter Network (see Eqn.~\ref{equ:place2} in Appendix for details) in $\SE(3)$ over a voxel grid is computationally intensive.
% The complexity is $\mathrm{O}\big((mnk)^3\big)$, when $m^3$ is the size of the scene voxel map $s$, $n^3$ is the size of the in-hand voxel map $s_{ih}$, and $k$ is the number of rotations used to discretize each of row, pitch, and yaw.
We present an $\SE(3)$ coarse-to-fine cross-correlation approach, extending the translational coarse-to-fine methods \cite{james2022coarse,Gualtieri2020hierarchical,gervet2023act3d} to encompass both translation and rotation. This method drastically reduces the computational complexity while maintaining high resolution in action evaluation. Additionally, we address the gimbal lock issue in \cite{shridhar2023perceiver, rvt, gervet2023act3d} by directly rotating the in-hand features multiple times to represent rotation actions. Specifically, the method first coarsely evaluates the $\SE(3)$ pose action space to identify the best coarse action. It then refines this action by zooming into the neighborhood of the coarse action and performing a finer evaluation. By repeating this process up to $l$ levels, the final level achieves high-resolution action evaluation with a significantly reduced compute.
% reduce the complexity to $\mathrm{O}\big((mnk)^\frac{3}{l}\big)$, where $l$ is number of coarse-to-fine levels. 
% We first perform cross-correlation at the coarse discretization resolution, then focus on the neighborhood of the best action, discretize with finer resolution, and repeat. By refining multiple times, we can evaluate $\SE(3)$ actions in a very fine resolution efficiently. 
% Please note our method applied coarse-to-fine to the translation and the rotation action inference simultaneously, which differs from previous works \cite{Gualtieri2019hierarchical,james2022coarse,gervet2023act3d} that only apply to translation and disregard rotation.

Defining the lift cross-correlation between an input function $b$ and a dynamic filter $k$ under a group $G$ by,
\begin{align}
    (b \star k)[g] = \int_{x\in X}b(x)(g\cdot k)(x)dx, \forall g \in G,
\end{align}
where $X$ is the domain of $b$ and $k$, i.e., X, Y, and Z dimensions in our case. In practice, $X$ is represented as a voxel grid, and the group $G$ is approximated by a discrete group $\hat{G}$, which includes a translation grid along the XYZ axes times a rotation grid over the row, pitch, and yaw axes. The term $(g\cdot k)(x)$ translates and rotates the dynamic filter $k$ by $g$, for each $g$ in the grid $\hat{G}$. Notably, while the inputs reside in $X\in\mathbb{R}^3$ (a voxel grid), the output resides in $g\in\hat{G}\subset\SE(3)$ (a voxel grid times a rotation grid). Thus this cross-correlation lifts the input signal.

As shown on the left side of Figure \ref{fig:c2f_bi_equ}, we first use a $\key$ 3D Unet\cite{Unet, 3DUnet}, based on a convolutional neural network (CNN), to embed the scene observation $s$ into a pyramid of latent features $f_s^l$ at different voxel resolution levels $l$. Then a $\query$ 3D Unet embeds the in-hand observation $s_{ih}$ into features $f^{'l}_{ih}$ and predicts a mask $Q_\text{mask}$. The mask is then applied to the in-hand features to remove noise, resulting in the final masked features: $f^{l}_{ih} = Q_\text{mask} \cdot f^{'l}_{ih}$.
% Then at the initial resolution level $l=1$, the cross-correlations are calculated over the set of group elements $\hat{G}_1 \subset \SE(3)$, consisting of group elements $\hat{G}_1 = \mathbb{Z}^3 \times H$ where $\mathbb{Z}^3$ is a translation grid and $H$ is a rotation grid (Healpix grid~\cite{gorski2005healpix}), to a coarsely discretize the entire translation-rotation action space. 
To infer the action $a^{l*}_\text{T}$ at level $l$, the lift cross-correlation is computed over the set of group elements $\forall g \in \hat{G}_l$,
\begin{align}
    Q^l_\text{T}[g] &= (f_s^l \star_l f_{ih}^l)[g] = \sum_{x\in X}f_s^l(x)(g\cdot f_{ih}^l)(x)
\end{align}
where $Q^l_\text{T}$ represents the pose action value at level $l$, and the action is greedily selected by $a^{l*}_\text{T} = \arg\max(Q^l_\text{T})$. At the coarsest level ($l=1$), the group $\hat{G}_1$ coarsely discretizes the action space into a low-resolution voxel grid times a low-resolution rotation grid. For finer levels ($l>1$), $\hat{G}_l$ refines the neighborhood around the optimal action from the previous level $a^{l-1*}_\text{T}$, by dividing the voxel-rotation grid into multiple finer voxel-rotation grid. This process is illustrated in the middle of Figure \ref{fig:c2f_bi_equ}.
% Notice that at the initial level $\hat{G}_1$ discretizes the entire translation-rotation action space. 
% Intuitively, the first level finds the best coarse translation-rotation action, then the following level evaluates a finer neighbor of the best action. In the end, the best action in the finest translation-rotation resolution is selected. 

In practice, $Q^l_\text{T}$ is a multi-channel voxel signal, where the value at each voxel corresponds to the translational action value, and each channel represents the rotational action value. To discretize the $\SE(3)$ action space into $\hat{G}_l$, we use hierarchical voxel grids for translation and Healpix grids\cite{gorski2005healpix} for rotation. The initial voxel grid has a size of $24^3$, while the rotation grid consists of $24$ discrete rotations. At each subsequent level, a voxel-rotation grid from the previous level is divided into a finer grid of size $2^3\times8$. Using a 3-level C2F process, we evaluate $\SE(3)$ action with a final resolution equivalent to a $96^3\times36864$ grid, or $1$cm in translation and $7.5^\circ$ in rotation.

\begin{figure}
    \centering
    \vspace{-0.5cm}
    % \hspace{-1cm}
    % \begin{subfigure}[b]{0.15\textwidth}\centering\includegraphics[width=\columnwidth]{figure/in_hand_seg/stack_blocks_s.png}\caption{stack\_blocks scene $s$}\end{subfigure}
    % \begin{subfigure}[b]{0.1\textwidth}\centering\includegraphics[width=\columnwidth]{figure/in_hand_seg/stack_blocks.png}\caption{in-hand crop $s_{ih}$}\end{subfigure}
    % \begin{subfigure}[b]{0.12\textwidth}\centering\includegraphics[width=\columnwidth]{figure/in_hand_seg/stack_blocks_seg.png}\caption{in-hand segmentation $Q_\text{mask}\cdot s_{ih}$}\end{subfigure}
    
    % \hspace{-1cm}
    \begin{subfigure}[b]{0.18\textwidth}\centering\includegraphics[width=\columnwidth]{figure/in_hand_seg/place_cups_s.png}\caption{scene $s$}\end{subfigure}
    \begin{subfigure}[b]{0.14\textwidth}\centering\includegraphics[width=\columnwidth]{figure/in_hand_seg/place_cups.png}\caption{in-hand $s_{ih}$}\end{subfigure}
    \begin{subfigure}[b]{0.14\textwidth}\centering\includegraphics[width=\columnwidth]{figure/in_hand_seg/place_cups_seg.png}\caption{$Q_\text{mask}\cdot s_{ih}$}\end{subfigure}
    
    
    \caption{Visualization of learned in-hand segmentation.}
    
    \vspace{-0.2cm}
    \label{fig:in_hand_segmentation}
\end{figure}

\input{table/full_rlbench18}

\subsection{In-hand segmentation}
\label{sec:in_hand_segmentation}

Bi-equivariance assumes that the in-hand object is rigidly attached to the gripper, meaning that any gripper action will transform the in-hand object identically. However, the in-hand observation could contain distracting objects that are not grasped by the gripper, which can happen, for example, when the in-hand crop size is large, and the gripper is about to grasp or release an object.

To avoid this, we propose in-hand segmentation by adding an output channel to the $\query$ network, which predicts a mask $Q_\text{mask}$ to exclude distracting objects. The method is trained in a self-supervised manner, requiring no additional labels. We compute the ground truth in-hand segmentation mask $m$ based on the observation $(s, s_{ih}, \text{T}_\text{ee})$ at time $t$ and the observation $(s', s_{ih}', \text{T}_\text{ee}')$ at time $t+1$, as well as the gripper displacement $v = \text{T}_\text{ee}^{-1}\text{T}_\text{ee}'$,
\begin{align}
    m[x] &=
    \begin{cases}
        1 & \text{if }  x \in s_{ih} > s_{ih}' + v^{-1}(s_{ih} < s_{ih}') \\
        0 & \text{if } x \in s > s' + v(s < s') \\
        -1 & \text{elsewhere}
    \end{cases}
     \nonumber 
\end{align}
where $x$ is the $XYZ-$ coordinate of the voxel grid. We use the computed segmentation mask to train the segmentation network to predict an in-hand mask. This predicted mask is then applied to filter out the features of the distracting object by performing an element-wise dot product: $f_{ih} = Q_\text{mask} f'_{ih}$, where $f'_{ih}$ represents the embedded in-hand features from the outputs of the $\query$ Unet, see Figure.\ref{fig:in_hand_segmentation} for visualizations. This approach allows us to consistently use one large in-hand crop size across all experiments.

% The in-hand segmentation learns to mask out the features in $f'_{ih}$ that are not bi-equivariant, see Figure.\ref{fig:in_hand_segmentation} for visualizations. Notice that our method effectively segments the features that are not attached to the gripper. This improves the robustness of the bi-equivariant policy to noisy in-hand observations and allows us to use large in-hand crop sizes. In fact, we use the same in-hand crop size for all experiments.


\subsection{Bi-equivariant data augmentation}
\label{sec:data_aug}

Our method achieves discretized translational bi-equivariance through the 3D CNN backbones and approximates continuous translational and rotational bi-equivariance through bi-equivariant data augmentation. 
% The desired bi-equivariance of the method, based on Equation ~\ref{equ:bi_equ1}, \ref{equ:bi_equ2} is,
% \begin{align}
%      g_1\cdot Q_\text{T}(g_1\cdot s, g_2\cdot s_{ih})\cdot g_2^{-1} &= Q_\text{T}(s, s_{ih}), \nonumber\\
%      g_1 \in H_1,& g_2 \in H_2,
%      \label{equ:desired_bi_equ}
% \end{align}
% where the set of group elements $H_1 \subset \SE(3)$ is $[-0.15, 0.15]$m contains translations along the 3 axes and rotations of size $[-45^\circ, 45^\circ]$ along $Z-$ axis, the set of group elements $H_2 \subset \SE(3)$ has $[-0.02, 0.02]$m translations along each axis and rotations of size $[5^\circ, 5^\circ, 45^\circ]$ along the $X, Y$, or $Z$ axis. 
We augment each data point $(s, s_{ih}, a)$ in the mini-batch by applying random transformations, as described by Eqn.~\ref{equ:aug_bi_equ}, where $g_1, g_2 \in \SE(3)$ are randomly sampled,
\begin{align}
     (g_1&\cdot s, g_2\cdot s_{ih}, g_1ag_2^{-1}).
     \label{equ:aug_bi_equ}
\end{align}

\subsection{Implementation} \label{sec: implementation}
% The agent evaluates the pose action $a_\text{T}$ based on the scene voxel map $s$ and the in-hand voxel map $s_{ih}$. The in-hand voxel map $s_{ih}$ is a crop of $s$, as described in Section \ref{sec:place_only}. Besides, we include gripper pose information by adding constants to the in-hand voxel map. 
The action values $Q_\text{T}^l$ are calculated by coarse-to-fine action evaluation described in Section \ref{sec: cross_correlation}. Afterward, the agent evaluates both the gripper open action $a_\text{open}$ and the planner ignores collision action $a_\text{collide}$ using a multi-layer perceptron (MLP) based on latent features from the key network: $Q_\text{open, collide} = \text{mlp}\big(\text{maxpool}(\key(s)), \key(s)[a^*_\text{T}]\big)$, where $\text{maxpool}(\key(s))$ extracts features by maxpooling over the spatial dimension and $\key(s)[a^*_\text{T}]$ extracts features at the selected action location $a^*_\text{T}$. This MLP is similar to RVT \cite{rvt} except we do not use softmax over the feature.

The agent is trained on the expert demonstrations $\{o, a\}$ by minimizing the following loss,
\begin{align}
    \mathcal{L} = &D(Q_\text{open}, a_\text{open}) + D(Q_\text{collide}, a_\text{collide})  \nonumber\\
     & +\Sigma_{l=1}^3 D(Q_\text{T}^l, a_\text{T}^l) + \Sigma_{x}\mathbbm{1}_{m_x\ge 0}\big|\big|Q_{\text{mask}, x} - m_x\big|\big|^2_2 \nonumber
\end{align}
where $a_\text{T}^l$ is discretized expert pose action at level $l$, corresponding to the coarse-to-fine resolution $\hat{G}_l$. The indicator function $\mathbbm{1}$ equals $1$ when the mask $m_x\ge 0$, and $0$ otherwise. $Q_\text{mask}$ is the predicted in-hand segmentation mask, and $x$ refers to the position in the voxel map. We use cross-entropy loss $D$ to train action values $Q_{\{\text{T, open, collide}\}}$ and $l2$ loss $||\cdot||^2_2$ to train the in-hand segmentation mask $Q_\text{mask}$. Bi-equivariant data augmentation is applied to each sampled data point during training.
% and during evaluation the action is greedily selected: $a^* = \pi(o) = \arg\max{Q}$. For more details about training and evaluation, please see Appendix.\ref{app:data_aug}, \ref{app:train_eval}.

% We calculate the ground truth in-hand segmentation mask $m$ based on the observation $(s, s_{ih}, \text{T}_\text{ee})$ at time $t$ and the observation $(s', s_{ih}', \text{T}_\text{ee}')$ at time $t+1$ in the demonstration data, and gripper displacement $v = \text{T}_\text{ee}^{-1}\text{T}_\text{ee}'$,
% \begin{align}
%     m_x &=
%     \begin{cases}
%         1 & \text{if }  x \in s_{ih} > s_{ih}' + v^{-1}(s_{ih} < s_{ih}') \\
%         0 & \text{if } x \in s > s' + v(s < s') \\
%         -1 & \text{elsewhere}
%     \end{cases}
%      \nonumber 
% \end{align}
% where $x$ is the $XYZ-$ coordinate of the voxel grid.


\begin{figure}\centering
    % \includegraphics[width=0.48\textwidth]{figure/ours_and_rvt.png}
    
    \begin{subfigure}[b]{0.48\textwidth}\centering
    \includegraphics[width=\textwidth]{figure/simulation_tasks.png}
    \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}\centering
    \includegraphics[width=\textwidth]{figure/ours_vs_rvt_bi_equ_equ_tasks.png}
    \caption{}
    \end{subfigure}
    % \begin{subfigure}[b]{0.22\textwidth}\centering
    % \includegraphics[width=\textwidth]{figure/ours_precision_tasks.png}
    % \caption{}
    % \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}\centering
    \includegraphics[width=\textwidth]{figure/ours_vs_rvt_bi_equ_equ_tasks_detail.png}
    \caption{}
    \end{subfigure}
    % \begin{subfigure}[b]{0.22\textwidth}\centering
    % \includegraphics[width=\textwidth]{figure/ours_precision_tasks_detail.png}
    % \caption{}
    % \end{subfigure}
    \caption{(a) shows 4 out of 18 RLBench tasks~\cite{james2020rlbench}. (b) when classifying 18 tasks by the equivariance, ours has advantages on bi-equivariant and mixed equivariance tasks but underperforms RVT on equivariant tasks. (c) ``Bi-equ.'': the top 5 tasks. ``Mix Bi-equ./Equ.'': the middle 9 tasks. ``Equ.'': the button 3 tasks.
    % (a) shows 4 out of 18 RLBench tasks~\cite{james2020rlbench}. (b) when classifying 18 tasks by the equivariance, ours has advantages on bi-equivariant and mixed equivariance tasks but underperforms RVT on equivariant tasks. (c) when sorting 18 tasks based on precision, ours learn proper policies for median- and low-precision tasks but perform badly in high-precision tasks, while constantly outperforming RVT. (d) ``Bi-equ.'': the top 5 tasks. ``Mix Bi-equ./Equ.'': the middle 9 tasks. ``Equ.'': the button 3 tasks. (e) Task performance, sorted by the Success Rate of a Noise-perturbed Expert planner (NESR). ``Low-precision'': top 6 tasks. ``Median-precision'': middle 6 tasks. ``High-precision'': bottom 6 tasks.
    }
    \vspace{-0.2cm}
    \label{fig:ours_vs_rvt}
\end{figure}

\section{Experiment}
% We evaluate our approach across diverse 3D manipulation tasks in both simulated environments and real-world settings, showcasing its ability to learn a state-of-the-art policy with a small number of robot data.


\textbf{Baselines.} 
We compare our method with strong Keyframe IL baselines. Notice that we do not compare with pick-place methods\cite{zeng2021transporter, Huang-RSS-22, ryu2023equivariant, huang2024fourier} because they can not solve all 18 RLBench tasks. E.g., ``screw\_bulb" does not belong to pick-place tasks. \textbf{C2FARMBC} Coarse-to-Fine Attention-driven Robotic Manipulation Behaviour Cloning \cite{james2022coarse, shridhar2023perceiver} is an imitation learning algorithm. The method maps voxel grid input into discretized translational actions in a coarse-to-fine scheme.
% While the translational action uses direct action mapping from the voxel using Unets\cite{Unet,3DUnet}, the other actions are the outputs of an MLP based on the features from the last level of Unets. 
% The coarse-to-fine inference effectively reduced computation. 
\textbf{PerAct} Perceiver Actor \cite{shridhar2023perceiver} and \textbf{RVT} Robot View Transformer \cite{rvt} utilize Transformer backbones to map observation into the values of translational actions, though destroying translation equivariance. Unlike PerAct which uses expensive voxel input, RVT utilizes multi-view projected images. 
% \textbf{Act3D:} Actor 3D \cite{gervet2023act3d} uses a Transformer backbone to map point-cloud observation to point-cloud of continuous translational action. 
C2FARMBC employs the coarse-to-fine method but is limited to translational actions. Moreover, all of these methods represent rotation action as discretized Euler angles, which suffer from discontinuity\cite{5D_SO3}. This rotation formulation only depends on the scene observation and does not incorporate the in-hand observation.
% The translational action is refined using coarse-to-fine point cloud levels. The other actions, orientation, gripper, and collision are inferred similar to C2FARMBC. The use of point clouds provides computation advantages and translation action accuracy advantages.
% \textbf{Act3D} Actor 3D \cite{gervet2023act3d} uses transformer backbone to map point-cloud observation to point-cloud of continuous translational action. The translational action is refined using coarse-to-fine point cloud levels. The other actions, orientation, gripper, and collision are inferred similar to C2FARMBC. The use of point clouds provides computation advantages and translation action accuracy advantages.
We train all the baselines using the same parameters from the open-sourced code, except that we train on single task setup and the iteration is reduced to 15k SGD steps.

\subsection{Simulation Tasks}


\textbf{Environment:} 
% We benchmark on RLBench \cite{james2020rlbench} based on Pyrep \cite{pyrep}, Vrep \cite{vrep}, and CoppelaSim \cite{coppeliaSim}. 
We focus on the 18 tasks on the RLBench\cite{james2020rlbench}, as shown in Figure \ref{fig:ours_vs_rvt}. We generate 10 or 100 episodes of training and 100 episodes of testing demonstrations for each task. All the baselines are trained with the same training data and are tested with the same testing scenes, restored from the 100 testing demonstrations. The demonstrations include $128\times128$ RBGD camera observations from the left shoulder, right shoulder, front, and wrist camera, the proprioceptive information $p$, and the expert action $a$.

\textbf{Tasks and success metrics:} The 18 tasks are the same as \cite{shridhar2023perceiver, rvt, gervet2023act3d}, except we uses single variation. These tasks cover a wide range of manipulation policies, that includes not only pick-place (stack\_cups, stack\_blocks), but also pushing/pulling (slide\_blocks, open\_drawer), and turning (screw\_bulb, turn\_tap), etc that the pick-place methods can not solve \cite{zeng2021transporter, Huang-RSS-22, ryu2023equivariant, huang2024fourier}. The total keyframe actions in one episode for these tasks range from 2 to 14 \cite{shridhar2023perceiver}. The metric for success is binary in $\{1, 0\}$ for success or failure. The task success depends on whether the goal state is reached within 25 steps in the RLBench simulator \cite{james2020rlbench}.


\textbf{Results:} Table \ref{table:full_rlbench18} compares ours with various baselines in the 18 RLBench tasks. To make a fair comparison, we include their performance of multi-task settings as reported in the paper~\cite{shridhar2023perceiver,rvt,gervet2023act3d}. Ours outperforms all the baselines in the average success rate when trained with 10 or 100 expert demonstrations. We further analyze the performance of our method in different task groups. We first classify the 18 tasks into three categories: ``Bi-equ. tasks'': when the task mainly contains bi-equivariant actions, e.g., stack\_cups. ``Equ. tasks'': when the task only contains equivariant actions, e.g., open\_drawer. ``Mix Bi-equ./Equ. tasks'': when the task has multiple actions that contain both, e.g., put\_in\_drawer. As is shown in Figure \ref{fig:ours_vs_rvt} (b) Our method outperforms RVT in ``Bi-equ.'' and ``Mix Bi-equ./Equ.'' tasks while underperforms RVT in ``Equ.'' tasks. This indicates that our method can effectively leverage the bi-equivariant property in the task. 
% We also sort the precision requirement of each task into ``Low-'', ``Median-'', ``High-''. As is shown in Figure \ref{fig:ours_vs_rvt} (b) Our method prevails in median-low precision tasks but suffers from high precision tasks. We hypothesize this is due to the discretization error of the voxel grid. 
% For details on task type clustering standards and analysis, please see the Appendix.\ref{app:rlbench18_analysis}. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{figure/realworld_tasks.png}
    \caption{\textbf{Real world tasks.} \textbf{Left:} The first row shows a snapshot of 4 tasks, and the second row shows the distributions of the initial state. golf\_swing requires picking the club and aligning its head with the golf then pushing the ball to touch the goal. flip\_steak requires grasping and flipping the steak. slide\_plate requires picking the plate and then reorientating it to slide into the rack. insert\_toothpaste requires grasping the toothpaste and inserting it into the mug. \textbf{Right:} While our method is aware of the club head, RVT is not.}
    \vspace{-0.2cm}
    \label{fig:real_world_tasks}
\end{figure}

\input{table/real_world}


\input{table/full_ablation}

\subsection{Real-World Tasks}

In this section, we compare our method with RVT~\cite{rvt} in 4 complex real-world tasks (shown in Figure \ref{fig:real_world_tasks}). RVT is the best baseline in 18 RLBench tasks in Table \ref{table:full_rlbench18} when trained in the low data regimen (10 training demonstrations). The real-world tasks differ from simulation in 1) multimodal demonstrations \cite{LSTM_GMM}, 2) noisy observations \cite{zhou2024dynpoint}, and 3) limited demonstrations.


\textbf{Robot platform:} we set up the robot platform with a 6 DoFs UR5 manipulator, a Robotiq 85 gripper. The observation $\text{T}_\text{ee}, s_\text{open}$ comes from the manipulator and the gripper sensors, while the scene observation $s$ is reconstructed voxel grid from the front, the left, and the right RealSense D455 cameras. The pose action $a_\text{T}$ specifies the target pose for an off-the-shelf planner, i.e., MoveIt~\cite{moveit} motion planner with RRT-connect algorithm~\cite{rrtconnect}. We do not use the collision action $a_\text{collide}$. The Robot Operating System (ROS) is used for communication, and the workstation is equipped with a 12GB memory 2080Ti GPU. The demo is collected using an HTC VIVE controller \cite{shridhar2023perceiver}.


\textbf{Training and evaluation metrics:} For training, we first collect 10 demos for each task using the robot platform, then train our method and RVT with 15k SGD steps. The same hyper-parameters as simulations are used, except the size of the workspace is adjusted according to the robot platform. We do not cherry picking and test the last model checkpoint. We evaluate each baseline with 20 episodes. Each episode is initialized with randomized object orientation and location within the workspace, then the initialization is recorded by the cameras. We minimize the initial state between different baselines by restoring the scene to the recorded images. A task is considered a success when the success metric is achieved within 10 steps.

\textbf{Results:} Table~\ref{table:real-world} shows the evaluation results. We report two success rates, w/ means the overall success rate, and w/o means the success rate when removing the episodes that have planner failure. Our method significantly outperforms the baselines in all evaluation metrics and all 4 tasks. Training with as few as 10 demos, our method exhibits the ability to compensate for the changes of the in-hand object, e.g., correctly using the club head to hit the golf when the gripper could grasp the club with two orientations with $180^\circ$ angle (last column of Figure \ref{fig:real_world_tasks}). In contrast, RVT infers actions ignoring the in-hand state, e.g., occasionally hits the golf by the grip. We also find that the motion planner failure accounts for $10\%$ task failure. We believe this is orthogonal to our method and a better motion planner\cite{curobo_report23, zhang2019two} could effectively address this issue.


\subsection{Ablation Study}

In this section, we ablate each piece of the method to demonstrate its importance. We compare the performance on 9 RLBench tasks. All the baselines are trained with 100 demonstrations and tested with 100 episodes.

\footnotetext{The rotation grid and the in-hand size are reduced to Healpix1 and $16^3$ to match the computation overhead with ours that uses Healpix3 and $32^3$.\label{foot:changes}}

\textbf{Baselines:} \underline{no coarse-to-fine} ablates the coarse-to-fine action evaluation in Section \ref{sec: cross_correlation} by using only one level of cross-correlation instead of 3
% . In-hand voxel size and the rotation grid are reduced to match the GPU overhead with ours
\textsuperscript{\ref{foot:changes}}. \underline{no cross, C2F, seg} ablates the bi-equivariance of ours by removing the coarse-to-fine evaluation (Section \ref{sec: cross_correlation}), the cross-correlation (Section \ref{sec:place_only}), and the in-hand segmentation (Section \ref{sec:in_hand_segmentation}). This baseline only uses the $\key$ Unet with the same translation resolution as ours, and discretized Euler angles, which is identical to 1 level C2FARMBC. \underline{no segmentation} ablates the in-hand segmentation (Section \ref{sec:in_hand_segmentation}) by using the output of $\query$ Unet $f_{ih}'$ without the mask $Q_{mask}$. \underline{no augmentation} ablates the bi-equivariance data augmentation (Section \ref{sec:data_aug}) through training with the raw data.

\textbf{Results:} Table \ref{table:full_ablation} shows the results on the 9 tasks. When the coarse-to-fine evaluation is removed in \underline{no coarse-to-fine}, the performance is dropped by $57\%$ and the training time is tripled. This indicates that perhaps the most important piece of our method is the coarse-to-fine action inference. \underline{no cross, C2F, seg} ablates the bi-equivariance structure leading to $>40\%$ performance drop. \underline{no augmentation} shows the bi-equivariant data augmentation contributes to $6\%$ performance increment. This indicates both the bi-equivariance neural network architecture and data augmentation improve performance, while the proposed neural network architecture plays a more important role. \underline{no segmentation} shows that removing in-hand segmentation causes performance drops and a large variance, which indicates the necessity to mask out the distractors.

\section{Conclusion and Limitations}

In this paper, we propose the Coarse-to-fine 3D Keyframe Transporter that leverages the rich geometric structure in the $\SE(3)$ policy and achieves high success rates. We begin by analyzing bi-equivariance in the Keyframe IL, then introducing a $3$D cross-correlation architecture that embeds this geometric structure. Additionally, we proposed a novel coarse-to-fine evaluation to 
% decouple the large $\SE(3)$ action space thus 
significantly reduce computing. Simulation experiments show that our model outperforms multiple strong baselines on 18 RLBench tasks and the physical experiments demonstrate the method can effectively learn from a few demonstrations and generalize to random initial scenes.

One limitation of our framework is the aliasing effect \cite{cesa2022a,e2cnn} of using discretized voxel features, which impacts the stability of our dynamic filter and the performance on high-precision tasks.
This issue could be mitigated by using irreducibal representations \cite{e2cnn,cesa2022a,huang2024fourier} or using point-cloud-based features \cite{qi2017pointnet++,ryu2023diffusionedfs,gervet2023act3d}. 
Another limitation is the keyframe action does not provide fine-grained control of the trajectory.
This could be addressed by using an engineered trajectory controller \cite{curobo_report23, zhang2019two}, or by combining keyframe action with closed-loop controllers \cite{xian2023chaineddiffuser, ma2024hierarchical}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}


% \section*{ACKNOWLEDGMENT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{IEEEtran}
\bibliography{main}
% \input{main.bbl}


\end{document}
