\section{Related Work}
% \textbf{$\SE(3)$ robotic policy.}
% $\SE(3)$ robotic policy is desirable because that can solve various manipulation tasks that cannot be solved by $\SE(2)$ policy. 
% Previous works \cite{james2022coarse, shridhar2023perceiver, rvt, gervet2023act3d} formulate the $\SE(3)$ robotic policy as a classification problem, which estimates the probability of the optimal action from translation action bins and rotation action bins. While the translation $\mathbb{R}^3$ discretization enables efficient learning, the independent rotation $\SO(3)$ discretization in Euler angles suffers from the gimbal lock issue and fails to model the multi-modality in rotation. Another class of works formulates the policy as a regression problem \cite{end2end_deep_visuomotor}. Following this, \cite{LSTM_GMM, zhu2023viola} leverages the Gaussian mixture model and \cite{zhao2023learning} utilizes conditional variation autoencoder to address the multi-modality issue in the policy, though ignores gimbal lock issue and subject to mode collapse \cite{florence2022implicit}. Lastly, recent works propose to represent policy as diffusion process \cite{chi2023diffusionpolicy, ryu2023equivariant, xian2023chaineddiffuser} that iteratively refine the action, which efficiently addresses the multi-modality issue. Especially, the $6$D $\SO(3)$ representation \cite{5D_SO3,chi2023diffusionpolicy} avoids gimbal lock issue. However, diffusion-based policies require multiple denoise steps to infer the action. In contrast, we address the gimbal lock issue in \cite{james2022coarse, shridhar2023perceiver, rvt, gervet2023act3d} by operating in the original $\SO(3)$ representation, rather than the reduced dimension representation \cite{5D_SO3} and address the multi-modality issue in $\SO(3)$ by classifying $\SO(3)$ action candidate. Lastly, in contrast to hundreds of denoising steps during inference in diffusion methods \cite{chi2023diffusionpolicy, ryu2023diffusionedfs, xian2023chaineddiffuser}, we only need one forward pass.

\textbf{Keyframe action scheme.}
ARM \cite{james2021arm} simplifies the trajectory of a closed-loop policy by using multiple keyframes. In this way, every state in the trajectory has the action that leads to the next keyframe pose and gripper aperture, and this is called demo augmentation in \cite{james2021arm}. The keyframe action scheme can be viewed as a policy between closed-loop control and open-loop control, allowing for diverse task learning while maintaining high sample efficiency in $Q$-learning \cite{james2021arm}. Following this idea, C2F-ARM\cite{james2022coarse} extends \cite{james2021arm} from 2D CNN to 3D CNN using a hierarchical evaluation style.
% , which further improves sample efficiency by leveraging the 3D translational equivariance of 3D CNN in the $Q$-learning context. 
Later PerAct \cite{shridhar2023perceiver} adopts keyframe action in the context of imitation learning, and an extra binary collision avoidance action to let the agent seamlessly control the motion planner for complex tasks. 
% This work follows \cite{shridhar2023perceiver} to apply the keyframe action scheme in imitation learning. 
% Critically, we enforce bi-equivariance in the keyframe action scheme, thus gaining superior performance than \cite{shridhar2023perceiver, james2022coarse, rvt, gervet2023act3d} that do not have bi-equivariance.
More recent works ~\cite{shridhar2023perceiver, rvt, gervet2023act3d} employ Transformers to infer the translational actions and use discretized Euler angles for the gripper rotation. However standard Transformers\cite{vaswani2017attention} are not translationaly equivariant due to position embeddings assigning unique values to each position. The Euler angles representation suffers from the gimble lock issue~\cite{5D_SO3}. This work addresses these issues by using translational-rotational cross-correlation, which enforces translational equivariance and avoids the gimbal lock issue, thus gaining superior performance.

\textbf{Equivariance in robotic policy learning.}
%translational equivariance --> rotation 
The generalization ability of CNN is partly due to its nature of translational equivariance. \cite{zeng2018robotic,Morrison-RSS-18} showed that the translational equivariance of FCN can improve the learning efficiency of manipulation tasks.
% Rotational equivariant neural networks are studied in many recent works \cite{cohen2016group,cohensteerable,weiler2019general,thomas2018tensor,fuchs2020se,deng2021vector,cesa2022a}. 
Later, 2D rotataional equivariance are explored in \cite{wang2021equivariant,zhu2022grasp,Huang-RSS-22,zhu2023grasp,jia2023seil,wang2022so2equivariant, wang2022onrobot, zhao2022integrating,wangequivariant} and dramatically improved the sample efficiency. Several recent works~\cite{simeonov2022neural, huang2022edge,ryu2023equivariant,huang2024fourier,ryu2023diffusionedfs,huorbitgrasp,huang2024imagination} attempted encoding the 3D rotation symmetries into manipulation tasks. 
% \cite{simeonov2022neural} learned an $\SE(3)$-invariant field to manipulate the object in the same category and \cite{huang2022edge} took advantage of the $\SE(3)$-invariant property of grasp evaluation function. 
\cite{ryu2023equivariant,ryu2023diffusionedfs,huang2024fourier} achieve 3D pick-place bi-equivariance by using separate models for the pick and the place actions but are unable to perform keyframe actions. Furthermore, diffusion-based method \cite{ryu2023diffusionedfs} requires $600$ iterations for action inference, while Fourier-based method \cite{huang2024fourier} necessitates rotating a voxel grid $\sim 400$ times to perform 3D Fourier transform.
% \cite{ryu2023equivariant,ryu2023diffusionedfs} achieves 3D pick-place Bi-equivariance by encoding the $\SO(3)$-equivariant point feature with graph-based equivariant models. \cite{huang2024fourier} used the voxel grids and achieved the 3D bi-equivariance with dynamic steerable kernel in Fourier space. However, these works \cite{zeng2021transporter, Huang-RSS-22, huang2024fourier, ryu2023equivariant, ryu2023diffusionedfs} are all limited to pick-place tasks.
To the best of our knowledge, we are the first to recognize and leverage the Bi-equivariance in the keyframe action setting, allowing us to tackle a much broader range of manipulation problems beyond pick-place. Additionally, the proposed 3 levels of coarse-to-fine action evaluation enable one-shot action inference, making the approach computationally efficient during training and inference.
% Moreover, these works relied on task-specific in-hand crop size. In contrast, we propose the in-hand segmentation that enables a fixed crop size.

\textbf{Coarse-to-fine action evaluation.}
Evaluating all discretized $\SE(3)$ action candidates is expensive due to dimensionality. An effective evaluation is to follow a coarse-to-fine scheme~\cite{Gualtieri2020hierarchical, james2022coarse, gervet2023act3d} that gradually refines the translational action iteratively. Specifically, C2F-ARM \cite{james2022coarse} iteratively evaluates translation $q$ value in a finer voxel grid. RVT \cite{rvt} first evaluates the left view, the front view, and the top view $q$ value maps, then projects these maps to reconstruct the 3D translation $q$ value map. Act3D \cite{gervet2023act3d} iteratively evaluates sampled translation action candidates in the point cloud observation and then reduces the range of translation sampling range to refine action. Another stream of work \cite{wang2021policy, wang2021equivariant, zhu2022grasp} proposes to first evaluate the discretized translation action, then evaluate the discretized rotational action. While all of these works ignore the intertwining between translation and rotation action, we perform the coarse-to-fine action evaluation in translation while considering rotation.