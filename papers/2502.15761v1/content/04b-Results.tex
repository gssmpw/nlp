\begin{figure}[!hbp]
    \centering 
    \includegraphics[width=1.0\linewidth]{images/mqa.png}  
 \caption{Model Quality Analysis: The models were evaluated based on six different benchmarks. We chose accuracy (\textbf{$\uparrow$}) for the five benchmarks on the left and perplexity (\textbf{$\downarrow$}) for the WikiText-2 benchmark.  Each group in the bar chart represents models $m_1$ to $m_{17}$ from left to right.}
    \label{fig:Modelsqty}
\end{figure}  

%===================== ======================================
\begin{figure*}[!htbp]
    \centering 
    \vskip -0.1cm   
      % \includegraphics[width=\linewidth]{images/conssistencyNewdk.png}  
      \includegraphics[width=\linewidth]{images/CPU_Consistency_Plot.pdf}  
    \vskip -0.2478cm
    \label{fig:StabDD}
\caption{\dk{Consistency results of the four devices over time: PP (top) and TG (bottom) speeds in tokens per second across 20 sorted runs (X-axis: run number, Y-axis: speed in $t/s$).}}
    \label{fig:StabDD}
\end{figure*}
% ==================================================================
\begin{figure*}[!htbp] 
 \vskip -0.145cm 
     \centering  
     \includegraphics[width=0.807\linewidth]{images/GPU_Consistency_Plot.pdf}  
\vskip -0.245cm 
    \caption{\dk{Consistency results of the Apple Vision Pro on GPU. Left: PP results. Right: TG results. Each model was tested 20 times, and the results are plotted in sorted order.}}
    \label{fig:VPGPU}
\end{figure*}
\input{content/tables/ConsistencyTable}
\section{Performance Evaluation Results}
\label{sec:results}%----
This section presents the results of four types of experiments described in \cref{sec:exp:design}.
\subsection{Model Quality Analysis}
As shown in \cref{tab:models}, we select five different model architectures, and for each architecture, we choose various quantization settings. While lower-bit quantization reduces model size, speeds up inference, and lowers power consumption, it typically comes at the cost of reduced model performance. The results of the model quality are illustrated in \cref{fig:Modelsqty}.
\label{sec:resModelsQty}
As shown in \cref{fig:Modelsqty}, a consistent trend is observed across all benchmarks: applying lower-bit quantization settings results in reduced model performance. Models with more parameters exhibit better language understanding capabilities, leading to higher performance under the same quantization settings. Additionally, different benchmarks exhibit varying levels of sensitivity to the quantization settings. For example, Hellaswag, ARC and WikiText-2 are more sensitive compared to the other three benchmarks.

\subsection{\dk{Performance Consistency Results}}
\label{sec:resultsStability}
\dk{\Cref{fig:StabDD,fig:VPGPU} illustrate the consistency of performance across all model-device pairs, while \Cref{tab:consistency} provides the corresponding quantitative results. Across all 20 runs, the Apple Vision Pro demonstrates the most consistent results compared to other devices, with the corresponding speed also being the fastest. Notably, the GPU results for the Apple Vision Pro exhibit greater stability and smaller variance. Among the remaining devices, the Magic Leap 2 also shows a reasonable degree of stability; however, for the first three models, its variance is surprisingly high, reaching 27\%, 32\%, and 26\% for \(m_2\), \(m_3\), and \(m_3\) under the PP setup, and 20\%, 23\%, and 27\% under the TG setup, respectively. In contrast, the Meta Quest 3 and Vivo X100 Pro exhibit relatively poor performance both in terms of speed and variance. Between these two devices, the Meta Quest 3 achieves higher speeds than the Vivo X100 Pro, but there is no clear difference in variance. For some models, one device shows lower variance, while for others, the variance is higher.}\\ 
\dk{Furthermore, as shown in \Cref{tab:perfConsistencyresultsForm1} performance varies significantly for \(m_1\) over Magic Leap 2 and Meta Quest 3, but it is generally consistent across the other two devices. These findings suggest that, provided model-device pairs are carefully selected, LLM performance can be considered reliable, strengthening the case for on-device LLM use in XR environments.}
% \begin{table*}[htbp]
% \centering
% \caption{\dk{Performance consistency results for \(m_1\) across the four devices. For the results of other models and explanation of the metrics used see \Cref{tab:consistency}.}}
% \vskip -0.45cm 
% \label{tab:perfConsistencyresultsForm1}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|ccccc|ccccc}
% \hline
% \multirow{2}{*}{Metric} & \multicolumn{5}{c|}{PP Results}     & \multicolumn{5}{c}{TG Results}      \\ \cline{2-11} 
%  & ML2     & Vivoo   & Meta Q3  & VisPro   & VisPro*  & ML2     & Vivoo   & Meta Q3  & VisPro   & VisPro*  \\\hline
% \(\mu \pm \sigma\)               & 51.68 $\pm$ 4.22 & 62.68 $\pm$ 16.64 & 39.32 $\pm$ 15.66 & 292.48 $\pm$ 12.87 & 1603.59 $\pm$ 56.12 & 20.26 $\pm$ 0.05 & 22.16 $\pm$ 0.90 & 16.76 $\pm$ 3.76 & 41.94 $\pm$ 1.77 & 42.94 $\pm$ 0.80 \\ %\hline
% Range                            & [43.62, 54.59]   & [21.63, 72.04]   & [24.25, 72.59]    & [288.58, 312.17]  & [1516.96, 1696.13] & [20.17, 20.34]   & [20.54, 23.41]   & [11.61, 22.53]    & [38.90, 46.37]   & [41.76, 44.35]   \\ %\hline
% CV (\%)                          & 8.17             & 26.55            & 39.82             & 4.40              & 3.50               & 0.23             & 4.05             & 22.44             & 4.21              & 1.87              \\   
% \hline
% \end{tabular}}
% \end{table*} 
% ==============================================================================
\begin{table}[htbp]
\centering
\caption{\dk{Performance consistency results for \(m_1\) across the four devices. For the results of other models and description of metrics used, see \Cref{tab:consistency}.}}
\vskip -0.45cm 
\label{tab:perfConsistencyresultsForm1}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|ccccc}
\hline
Test& Metric & ML2     & Vivoo   & Meta Q3  & VisPro   & VisPro*  \\ \hline 
\multirow{3}{*}{PP} 
 & \(\mu \pm \sigma\) & 51.68 $\pm$ 4.22 & 62.68 $\pm$ 16.64 & 39.32 $\pm$ 15.66 & 292.48 $\pm$ 12.87 & 1603.59 $\pm$ 56.12 \\ 
 & Range  & [43.62, 54.59]   & [21.63, 72.04]   & [24.25, 72.59]    & [288.5, 312.1]  & [1516.9, 1696.1] \\ 
 & CV (\%)  & 8.17             & 26.55            & 39.82             & 4.40              & 3.50               \\   
\hline
\multirow{3}{*}{TG}% & Metric & ML2     & Vivoo   & Meta Q3  & VisPro   & VisPro*  \\ \cline{2-7}
 & \(\mu \pm \sigma\) & 20.26 $\pm$ 0.05 & 22.16 $\pm$ 0.90 & 16.76 $\pm$ 3.76 & 41.94 $\pm$ 1.77 & 42.94 $\pm$ 0.80 \\ 
 & Range  & [20.17, 20.34]   & [20.54, 23.41]   & [11.61, 22.53]    & [38.90, 46.37]   & [41.76, 44.35]   \\ 
 & CV (\%)  & 0.23             & 4.05             & 22.44             & 4.21              & 1.87              \\   
\hline
\end{tabular}}
\end{table} 

% ========================= ==================================
\begin{figure*}%[!htp]
     \centering   
     \includegraphics[width=1.0\linewidth]{images/PP_TG_Results_Main.pdf}  
\vskip -0.3645cm 
    \caption{\dk{Processing speed of the four devices in the PP test (top) and TG test (bottom) with varying string lengths: 64, 128, 256, 512, and 1024. The x-axis represents the model, while the y-axis represents the processing speed in t/s.}}
    \label{fig:resPPTG}
\end{figure*}

%-------------------------------------------------
\begin{figure}[!hbp]
     \centering   
     \includegraphics[width=1.0\linewidth]{images/PP_TG_Results_GPU.pdf}  
\vskip -0.45cm 
    \caption{\dk{Processing speed of Apple Vision Pro (GPU) in the PP test (left) and TG test (rigth). The x-axis represents the model, while the y-axis represents the processing speed in t/s.}}
    \label{fig:resPPTG-GPU}
\end{figure}
%--------------------------------------

%-------------------------------------------------
\begin{figure}[!htbp]
     \centering   
     \includegraphics[width=1.0\linewidth]{images/Errors_Count.pdf}  
\vskip -0.355cm 
 \caption{\dk{Error counts for Meta Quest 3 (left) and Apple Vision Pro (right) in PP (top) and TG (bottom). Magic Leap 2, with zero errors, and Vivo X100 Pro, with a total of four errors, one in $m_1$ (PP-64), two in $m_{10}$ (PP-64, PP-128), and one in $m_{11}$ (TG-1024) are excluded.}}
    \label{fig:resErrorsCount}
\end{figure}
%-------------------------------------------------

%-------------------------------------------------
\begin{figure*}%[!htbp]
    \centering 
   % % \vskip -0.396478cm 
   %   \includegraphics[width=0.73\linewidth]{images/BT_DK.pdf} %
   %   \includegraphics[width=0.26\linewidth]{images/BT_GPUAVP.pdf} %
    \includegraphics[width=\linewidth]{images/BT_DK5devices.pdf} %
    \vskip -0.34780cm
    \caption{\dk{Batch Test results: Batch size (X-axis) vs. processing speed (Y-axis).}}
    % \vskip -0.478cm
    \label{fig:resBT}
\end{figure*}
%-------------------------------------------------
%-------------------------------------------------
\begin{figure*}%[!htbp]
    \centering 
  %  \vskip -0.478cm
    %\includegraphics[width=1.0\linewidth]{images/threadsBB.png} %
    % \includegraphics[width=1.0\linewidth]{images/TT_DK.pdf} %
    \includegraphics[width=1.0\linewidth]{images/TT_DK5Devices.pdf} %
    \vskip -0.32478cm
\caption{\dk{Thread Test (TT) results: The X-axis represents the thread count, while the Y-axis shows the speed in tokens per second. Threads 4 and 8 deliver the fastest results. Note: Apple Vision Pro (CPU) fails for thread counts of 16 and 32.}}
    \label{fig:resThreadA}
\end{figure*}
%------------------------------------------------- 

\subsection{Results of Processing Speed and String Length}
\label{sec:resultsPPTG} 
 % \dk{\cref{tab:PPTGresults} and
 \dk{\cref{fig:resPPTG,fig:resPPTG-GPU} present the processing speed results of PP and TG with varying string lengths (prompt length and token sets). Both PP and TG were tested with values of 64, 128, 256, 512, and 1024. The processing speed varies significantly across string lengths, devices, and models. \\
 % \include{content/tables/PPTGTable}
\noindent{}\textbf{String length and PP vs TG Speed}: PP speed is consistently higher than TG speed, with a speedup of approximately two to three times across the four (CPU-based) devices, while on \gls{avp} (GPU), the PP speed is significantly higher, ranging from 10 to 19 times the TG speed. Similarly, increasing the prompt size slightly reduces the prompt processing speed in PP, whereas in TG, the speed reduction is more significant with a growing token set. This suggests that TG has a higher computational cost due to its sequential execution of tokens, whereas PP is somewhat parallelized, requiring only the loading of prompts. Though there is minor variation in the 64, 128, and 256 string lengths, the last two, 512 and 1024, are consistently the slowest. When examining the variation across string lengths, \gls{mq3} exhibits the largest variance, whereas \gls{ml2} and \gls{avp} have the smallest variance.  \\
\noindent{}\textbf{Device-Based Analysis:} If we examine the devices, the overall performance trends reveal that \gls{avp} consistently shows the highest speed in both PP and TG tests. \gls{ml2} ranks second in both PP and TG performance, while Vivo X100s Pro outperforms \gls{mq3} in TG (particularly in longer strings 512, 1024) but ranks fourth in PP. This ranking highlights the varying computational capabilities of XR devices, with AVP (both CPU and GPU) standing out as the fasted one across all conditions. \gls{ml2} shows stable performance across all variations of string lengths, with lower $CV\%$ values, indicating that it does not vary significantly with changing string lengths. In contrast, \gls{mq3} shows a higher $CV\%$, indicating greater variance with increasing string length.  \\
%-------------------------------------------------
\noindent{}\textbf{Error Counts:} \Cref{fig:resErrorsCount} shows the errors counts during the PP and TG tests. Errors mostly occurred in \gls{avp} (CPU) and \gls{mq3}, particularly at longer prompt lengths of 512 and 1024 tokens. There are a few reasons for this. First, longer strings process more slowly, leading to abrupt changes and occasional failures. Second, memory constraints or inefficiencies in sustained generation contribute to these errors. With \gls{avp}, the general user experience is also suboptimal, as the device must be actively mounted on the head, which can cause errors if not handled carefully. Overall, \gls{mq3} records the highest count of errors, with frequent retries affecting its reliability, whereas \gls{ml2} remains highly stable, leading to minimal inconsistencies (zero errors). As expected, AVP (GPU) also reported zero errors. \\
\textbf{Model-Based Analysis:} In the model-wise analysis, smaller models such as Qwen2-0.5B and Vikhr-Gemma-2B achieve the highest speeds across devices, particularly in PP. In contrast, larger models like the LLaMA-2-7B series and Mistral-7B series exhibit significantly lower processing speeds, with higher variability and instability, especially in TG. Model size also impacts processing speed, as smaller models are faster. Here, $m_1$ is the fastest (though omitted from later analysis), while $m_2$, $m_3$, and $m_5$ are the fastest among the remaining models, whereas $m_{14}$ and $m_{16}$ are the slowest.}
\subsection{Results of Parallelization with BT and TT}
\label{sec:resultsTTBT} 
\dk{This section presents the results of BT and TT, both of which are used to achieve concurrency and parallelization. Regardless of the parallelization method, the three models $m_2$, $m_3$, and $m_5$ consistently rank among the fastest across all devices, while $m_{14}$ and $m_{16}$ are the slowest.\\
\noindent\textbf{Results of BT:}  
\cref{fig:resBT} presents the results of the batch test with varying batch sizes of 128, 256, 512, and 1024. Generally, increasing the batch size leads to a decrease in performance across all four devices (except for \gls{avp} (GPU)) due to increased computational overhead. \gls{avp} (GPU), with its strong computational resources, does not show a significant performance drop with varying batch sizes, although there are some fluctuations at a batch size of 256. This indicates that GPU-based processing on \gls{avp} (GPU) is highly optimized for parallel execution and can effectively manage larger batches of input without affecting processing speed. \\
More importantly, \gls{avp} (CPU) experiences a noticeable performance drop as the batch size increases. However, the remaining devices do not follow this trend. For instance, on \gls{mq3}, batch size 128 yields the best performance, whereas for the other batch sizes, there is no significant change. Similarly, for \gls{ml2} and \gls{vivo}, variations in batch size do not appear to have a substantial impact on performance.\\
\noindent\textbf{Results of TT:}  
\cref{fig:resThreadA} presents the TT results for thread counts of 1, 2, 4, 8, 16, and 32. Across all four devices, processing speed sees the most significant increase when increasing the thread count from 1 to 4. The speed gain continues at 8 threads (or shows minor degradation), but beyond 8 threads, performance begins to decline slightly, with further degradation beyond 16 threads. Apple Vision Pro (CPU) fails at thread counts of 16 and 32, indicating its limitations in achieving this level of parallelism.\\
\gls{avp} (GPU) follows a different trend, maintaining consistently high processing speed across all thread counts. Unlike CPU-based devices, its performance remains stable even at 32 threads, demonstrating its superior ability to handle concurrent tasks. These results highlight that while the four CPU-based devices benefit from moderate threading, \gls{avp} (GPU) is significantly more efficient at scaling concurrency without experiencing notable performance degradation. Since our study primarily focuses on CPU-based implementation, we conclude that using a moderate thread count of 4, 6, or 8 yields optimal results.}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{images/memoryresults.pdf}
    \vskip -0.396478cm 
    \caption{ \dk{Memory consumption for each model-device pair. The results represent the mean values across batch sizes 128, 256, 512, and 1024. }}
    \label{fig:Memory_plot}
    \vskip -0.396478cm 
\end{figure}
\subsection{Memory Consumption Results }
\label{sec:resultsMemory}
\dk{\cref{fig:Memory_plot} presents the memory consumption results for each model-device pair. The values represent the mean memory consumption from five experiments with varying batch sizes of 128, 256, 512, and 1024. As expected, memory consumption varies across models, with most showing consistent usage across different devices.    Generally, within each model series, memory consumption increases with model size. However, minor exceptions exist, such as $m_7$ on Apple Vision Pro (GPU), which consumes more memory than other models in the Phi-3.1 series $[m_6-m_{11}]$. This size-based trend does not necessarily hold across series, even for models of similar sizes. For example, \textit{m2} (1.36 GB) consumes less memory than  $m_6$ (1.32 GB) despite having a larger size, as they belong to different series. Similarly,  $m_7$ (1.94 GB) requires more memory than \textit{$m_5$} (2.0 GB) and \textit{$m_{15}$} (2.05 GB), as all three belong to different series. Another example is  $m_{12}$ (2.63 GB) consuming more memory than $m_{16}$ (2.81 GB), despite being smaller in size.} \\
\dk{From a device-specific perspective, Apple Vision Pro demonstrates significantly lower memory consumption than the other devices, averaging $0.5782$ GB on the CPU and $0.5488$ GB on the GPU across all $17$ models. Among the remaining devices, Meta Quest 3 consumes the least memory at $2.3520$ GB, followed by Vivo X100 Pro at $2.3540$ GB, while Magic Leap 2 exhibits the highest at $2.3748$ GB. These findings suggest that Apple Vision Pro (both CPU and GPU) is approximately four times more memory-efficient than Magic Leap 2, Meta Quest 3, and Vivo X100 Pro. Its superior memory management and hardware optimizations make it the most efficient device in terms of memory consumption.}
\subsection{Battery Consumption Results}
\label{sec:resultsBattery}
\Cref{fig:BatteryTEST} presents the battery consumption results over a 10-minute experiment. \dk{We observe that for the first two series, the Qwen Series and Vikhr-Gemma Series, battery consumption remains relatively low and with lower variation across all four devices. However, for larger models in the LLaMA-2 Series and Mistral-7B Series, battery consumption increases significantly. 
\gls{avp} shows a strong correlation between model size and battery consumption, both for GPU and CPU usage. In contrast, the remaining three devices do not exhibit a significant variation with respect to model size. \gls{vivo} demonstrates the best battery life, with an average loss of only $2.5\%$ over 10 minutes. \gls{ml2} follows with a $8.5\%$ average battery loss, while \gls{mq3} records a slightly higher battery loss at $9.7\%$. \gls{avp} shows the worst in this regard, with GPU usage resulting in a $10.1\%$ battery loss and with CPU usage leading to a $12.6\%$ battery loss in 10 minuets.} 

\begin{figure}[!htbp]
    \centering 
    \includegraphics[width=1\linewidth]{images/battery.pdf}  
    \vskip -0.396478cm 
    \caption{ \dk{Battery Consumption in 10-minute experiments. }  }
    \label{fig:BatteryTEST}
\end{figure}

\subsection{Pareto Front Results}

Since we use \dk{both CPU and GPU} for inference in the Vision Pro experiments, while the other three devices use CPU only for inference, we exclude \dk{Apple Vision Pro GPU} from the Pareto calculation to ensure a fair comparison. \dk{With 4 devices and 16 models (m2-m17), we have a total of 64 device-model pairs.} Using \cref{equ:pareto} and \cref{equ:score}, we calculated the \textit{quality}, \dk{\textit{stability} and \textit{performance}}  scores for each pair, then identified the Pareto fronts and visualized them in \cref{fig:pareto}. The quality of a model can be evaluated using various benchmarks, such as accuracy and perplexity, as detailed in \cref{sec:modelsQA}. \dk{ For the performance objective, we evaluate performance using \textit{PP}, \textit{TG}, \textit{memory consumption}, and \textit{battery consumption}, with respective weights of 0.35, 0.35, 0.2, and 0.1. For the stability objective, we assess it based on the coefficient of variation (\textit{CV}) and error count of \textit{PP} and \textit{TG}, assigning 0.7 to CV and 0.3 to error count.
} 
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{images/pareto.png}
    \vskip -0.578cm 
    \caption{\dk{Pareto fronts for various device-model pairs, with different colors representing distinct device types. Performance is evaluated based on processing speed, memory consumption, and battery consumption. Stability is assessed using the \textit{CV} and \textit{error count} values of \textit{PP} and \textit{TG}. Quality is measured through evaluation results on six benchmark datasets.
    } }
    \label{fig:pareto}
\end{figure}

\dk{\Cref{fig:pareto} illustrates the Pareto front points, highlighting the optimal choices across different model sizes and devices. For example, the AVP (CPU) device achieves the highest speed among the models but exhibits relatively low stability, whereas the ML 2 device offers the greatest stability. Meanwhile, both Vivo and Meta Q3 deliver a balanced trade-off between speed and stability. At the model-pair level, $m_17$ and $m_11$ provide high quality on AVP (CPU) and Vivo, respectively. At the same time, $m_3$ is selected as a Pareto-optimal choice due to its high performance on AVP (CPU) and robust stability on Meta Q3. A similar trend is observed with $m_8$, which attains strong performance and quality on AVP (CPU) as well as commendable quality and stability on ML 2.}



