\section{Related Work}
\label{sec:related-work}

We review related work with respect to two major research directions. The first research direction is exploring the use of LLMs on XR devices, and the second research direction explores how to make LLMs directly deployable on low-resource hardware.
% \h{Research on LLMs:}\\
% \cite{Fan2024ReviewLLMs}\\
% \cite{meta2023introducingLLaMA}\\
% \cite{Brown2020NEURIPSLMareFewshotLearner}\\
% \h{On-device LLMs:}\\
% mobileLLM\\
% \h{LLM-based applications on VR/XR}\\
% \h{AI and XR:}\\
% \cite{Bozkir2024Review:LLMXR}
% \cite{HirzleXRandAIReview}\\

\subsection{LLM-powered XR Applications}
Owing to the powerful semantic understanding and extensive general knowledge of LLMs, numerous studies have explored their application in assisting various tasks within XR scenarios. 

Torre \etal~\cite{de2024llmr} introduced \emph{LLMR}, a framework that leverages LLMs for the real-time creation and modification of interactive mixed reality experiences, enabling tasks such as generating new content or editing existing works on VR/AR devices. Jia \etal~\cite{Jia2024_1} developed the \emph{VOICE} framework, which employs a two-layer agent system for conversational interaction and explanation in scientific communication, with a prototype deployable on VR devices. Kurai \etal~\cite{kurai2024magicitem} proposed \emph{MagicItem}, a tool that allows users with limited programming experience to define object behaviors within the metaverse platform. Zhang \etal~\cite{zhang2024odoragent} introduced \emph{OdorAgent}, which combines an LLM with a text-image model to automate video-odor matching. Yin \etal~\cite{yin2024text2vrscene} identified potential limitations in LLM-based automated systems and proposed the systematic framework \emph{Text2VRScene} to address them. Chen \etal~\cite{chen2024supporting} leveraged the extensive capabilities of LLMs in context perception and text prediction to enhance text entry efficiency by reducing manual keystrokes in VR scenarios. Giunchi \etal~\cite{giunchi2024dreamcodevr} developed \emph{DreamCodeVR}, a tool designed to help users, regardless of coding experience, create basic object behavior in VR environments by translating spoken language into code within an active application. Wan \etal~\cite{wan2024building} presented an LLM-based AI agent for human-agent interaction in VR, involving GPT-4 to simulate realistic NPC behavior, including context-aware responses, facial expressions, and body gestures.

As they stand, these previous contributions still require the deployed devices to connect to a cloud server for LLM inference, raising concerns about user privacy, latency, costs, and internet access requirements. In our work, we evaluate the local inference performance of various LLMs and propose a prototype capable of using local LLM inference.

\subsection{On-device LLMs}
Running LLMs on edge devices, commonly called on-device LLMs, has garnered significant research interest due to their advantages in enhancing privacy, reducing latency, and operating without the need for internet connectivity. Because of the limited memory and computing capabilities, on-device LLMs usually require resource-efficient
LLM deployment~\cite{qu2024mobile}: a trade-off between performance and model size.

Cheng \etal~\cite{cheng2023optimize} introduced the \emph{SignRound} method, which leverages signed gradient descent to optimize both rounding values and weight clipping. This approach achieves outstanding performance in 2- to 4-bit quantization while maintaining low tuning costs and eliminating additional inference overhead. Ma \etal~\cite{ma2023llm} developed the \emph{LLM-Pruner} method, which uses structural pruning to selectively remove non-essential coupled structures based on gradient information, effectively preserving the core functionality of the LLM. Their results demonstrate that the compressed models continue to perform well in tasks such as zero-shot classification and generation. Gu \etal~\cite{gu2024minillm} introduced a novel knowledge distillation method that compresses LLMs into smaller models, resulting in the student model \emph{MINILLM}. \emph{MINILLM} demonstrates superior performance compared to baseline models, producing more precise responses with reduced exposure bias, improved calibration, and enhanced long-text generation capabilities. Liu \etal~\cite{liu2024kivi} developed a tuning-free 2-bit KV cache quantization algorithm, named \emph{KIVI}, which independently quantizes the key cache per channel and the value cache per token. This approach enabled up to a 4Ã— increase in batch size, resulting in a $2.35\times$ to $3.47\times$ improvement in throughput on real LLM inference workloads. Liu \etal~\cite{liu2024mobilellm} introduced \emph{MobileLLM}, which explores the importance of model architecture for sub-billion-parameter LLMs by leveraging deep and narrow architectures, combined with embedding sharing and grouped-query attention mechanisms. 

Although these studies primarily focused on reducing model size efficiently for deployment on various devices, they did not evaluate the models' performance in real-world on-device scenarios, which is the focus of our paper.

