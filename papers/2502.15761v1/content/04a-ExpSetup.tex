\section{Experimental Setup}
\label{sec:exp:design}
\dk{This section describes the experimental setup and methodology for evaluating processing speed and error rates. The results are presented in \cref{sec:results}. Before diving into the detailed setup, we first discuss the two evaluation metrics: processing speed and error count.}\\
\noindent \textbf{Processing Speed:}
\dk{In this study, we measure the processing speed for each model-device pair \((m_i, d_j)\) by recording the time \(t_{ijk}\) taken by model \(m_i\) running on device \(d_j\) to process a string of length \(L\). The instantaneous processing speed, \(S_{inst.}\), expressed in tokens per second, is calculated as follows:}

\begin{equation}
\label{eq:instSpeed}
S_{inst.} = \frac{L}{t_{ij}}.
\end{equation}

\dk{However, instead of relying on a single run, we perform 5 runs for each model-device pair to ensure accuracy and measure stability. If the coefficient of variance (CV) across the 5 runs is less than \(40\%\), we calculate the 5-Run-Mean Speed (referred to as consistent speed, \(S_c\)) as:}

\begin{equation}
\label{eq:mean_speed}
S_c = \frac{1}{5} \sum_{k=1}^{5} S_{inst.}(k),
\end{equation}
\dk{where \( S_{\text{inst.}}(k) \) represents the instantaneous speed during the \( k^{\text{th}} \) run of each experiment, calculated via \cref{eq:instSpeed}.  This approach minimizes the impact of factors such as initialization delays, hardware fluctuations, or other environmental inconsistencies, providing a more reliable measure of processing speed.}

\noindent \textbf{Error Count:}
\dk{If the CV exceeds \(33\%\), the run is considered unstable, and the corresponding data point is excluded from further analysis. The count of such unstable runs is recorded as an error during the experiment.}

\dk{We conducted the following four types of experiments.}


\subsection{Models Quality Analysis}
\label{sec:modelsQA}
Due to the storage and computational limitations of edge devices, deploying \gls{llm} locally often requires a trade-off between model size and performance. To put our performance results in perspective, we first conducted an empirical analysis of the models we have used in this study. \dk{This section offers insights into the models evaluated in this work, presenting a comparative analysis. While this section is not directly related to device performance, we believe providing detailed information about the models is essential for understanding their capabilities and limitations.}
We select six benchmarks to evaluate each model and calculate different metrics for each run:

\begin{itemize}[noitemsep,leftmargin=8pt, topsep=3pt]
    \item Hellaswag~\cite{hellaswag}: commonsense reasoning tasks, focusing on selecting the most plausible ending to a given situation from multiple options.
    \item mmlu~\cite{hendrycks2020measuring}: evaluating a modelâ€™s knowledge and reasoning across 57 different subjects, ranging from elementary-level topics to advanced professional knowledge.
    \item ARC~\cite{clark2018thinksolvedquestionanswering}: evaluating model's ability to answer complex, grade-school science questions that require reasoning and problem-solving beyond simple fact retrieval.
    \item Truthful-qa~\cite{lin2021truthfulqa}: evaluate how accurately AI models generate truthful answers to questions, especially in cases where common misconceptions or false information could lead to incorrect responses.
    \item winogrande~\cite{ai2:winogrande}: large-scale benchmark for commonsense reasoning, specifically designed to test a model's ability to resolve ambiguous pronouns in sentences.
    \item WikiText-2~\cite{merity2016pointer}: containing over 100 million tokens, sourced from verified Wikipedia articles, designed to support research in language modeling and text generation tasks.

\end{itemize}

For the first five benchmarks, which consist of multiple-choice questions, we use accuracy as the evaluation metric. However, for WikiText-2, the task is to predict the probability of each word in a given text, reflecting the model's natural language understanding. Therefore, we use perplexity as the evaluation metric, defined as:

\begin{equation} \text{Perplexity}(P) = e^{-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i)} \end{equation}

where $P(w_i)$ represents the probability assigned by the model to the $i$-th word, and $N$ is the total number of words in the sequence.

\subsection{Evaluating Performance Consistency}
\label{sec:setStability}
\dk{Some devices may experience performance degradation over time. In these experiments, we evaluate how processing speed fluctuates across multiple runs. For each model \( m_i \) and device \( d_j \), we perform \( N \) runs and compute the mean $(\mu_{ij})$ of the speed $(S_c)$, standard deviation \( \sigma_{ij} \), and coefficient of variation ($CV_{ij}$). We also record the error count, as well as the maximum and minimum speeds observed during each experiment.}\\
\dk{In our experiments, we set \( N = 20 \), \ie each model-device pair is tested 20 times to evaluate performance variance. The mean speed across \( N \) runs is calculated as:}

\begin{equation}
\label{eq:time_efficiency}
\mu = \frac{\sum_{i=1}^{N} S_c}{N},
\end{equation}
\dk{where \( S_c \) represents the 5-Run-Mean speed measured during each run, calculated as in~\cref{eq:mean_speed}. In other words, each \( S_c \) itself is the mean of 5 runs, ensuring that the measured value is not too affected by errors or hidden factors.}


To let the devices cool down and conduct each run in comparable circumstances, we leave each device idle for two minutes between each run of the same model. We find this to be adequate, but just to be safer, between different models, we leave each device idle for ten minutes. We noted that the Meta Quest is not very prone to heating up. Heating was more noticeable on the Vivo X100s Pro, so it is always placed on a metal plate for heat dissipation. This is not representative of actual use, but this device is included in our evaluation as a baseline, not as an example of an actual XR headset. Magic Leap features a cooling fan, as does Vision Pro.\\
\dk{For each test, we conducted both PP and TG evaluations with the string length set to 64, \ie \( \text{PP} = 64 \) and \( \text{TG} = 64 \). For each model-device pair, we calculated the standard deviation, mean, and CV. The results are presented in~\cref{sec:resultsStability}.}


% \dk{The processing speed is not recorded instantaneously after a single run. Instead, we perform 5 runs, and if the coefficient of variance (CV) is less than $33\%$, we compute the mean speed for these runs as Instentenious Speed (S_i):}
% \begin{equation}
% \label{eq:speed}
% S_i= \frac{\sum_{k=1}^{5} t_{ijk}}{5}.
% \end{equation}
% \dk{However, if the CV is $33\% $ or higher, the run is considered erroneous and excluded from the analysis.}


% \dk{In our experiments, we set \( N = 20 \), \ie each model-device pair was tested 20 times to evaluate performance variance. The mean is calculated as: 

% \begin{equation}
% \label{eq:time_efficiency}
% S_{mean} = \frac{\sum_{N=1}^{20}{\frac{1}{5} \sum_{k=1}^{5} t_{ijk}}}{N}
% \end{equation}

% For each test, we conducted both PP and TG evaluations with the string length set to 64, \ie \( \text{PP} = 64 \) and \( \text{TG} = 64 \).} The results are presented in~\cref{sec:resultsStability}.  

\subsection{Processing Speed and String Length}
\label{sec:settimePPTG}
\dk{String length, comprising both \textit{Prompt Length} and \textit{Size of the Token Set}, significantly affects performance. We evaluate the performance of LLMs on XR devices with varying string lengths to assess their processing speed. For each experiment, the processing speed is calculated as in~\cref{eq:mean_speed}.}\\
\dk{We conducted experiments for five different string lengths (64, 128, 256, 512, and 1024) under the following PP and TG setups.}
\begin{itemize}[noitemsep,leftmargin=8pt,topsep=0pt]
    \item \textbf{\gls{pp}:} The PP values were set to 64, 128, 256, 512, and 1024, while TG was fixed at 0, with all other parameters set to their default values.
    \item \textbf{\gls{tg}:} The TG values were set to 64, 128, 256, 512, and 1024, while PP was fixed at 0, with all other parameters set to their default values.
\end{itemize}
The results are shown in~\cref{sec:resultsPPTG}.
\subsection{\dk{Parallelization with Thread Count and Batch Size}}
\label{sec:setTTBT}
\dk{The parameters \textit{Batch Size} and \textit{Thread Count}, which control parallelization, also significantly affect performance. We evaluate the performance of LLMs on XR devices with varying batch sizes and thread counts to assess their processing speed, calculated using~\cref{eq:mean_speed}. The two tests conducted are described below:}
\begin{itemize}[noitemsep,leftmargin=8pt,topsep=0pt]
    \item \textbf{\gls{bt}:} \dk{We set PP to 64, TG to 0, and varied the batch sizes between 128, 256, 512, and 1024, with all other parameters kept at their default values.}
    \item \textbf{\gls{tt}:} \dk{We set PP to 64 and TG to 0, while varying the thread counts between 1, 2, 4, 8, 16, and 32. All other parameters were kept at their default values.}
\end{itemize}
The results are shown in~\cref{sec:resultsTTBT}.
 
\subsection{Evaluation of Memory Consumption}
\label{sec:resultsMemory}
Memory consumption \( M_{ij} \) is measured in terms of the \gls{rss} which represents the actual physical memory usage of the relevant processes. 
For each model-device pair  ($m_i, d_j$), we measure memory usage over three runs, and report the average.
 \dk{The memory usage for PP and TG parameters was evaluated using the same approach as the performance consistency assessment. In each test, a fixed prompt was provided, and the model was instructed to generate outputs. The experiments were conducted with batch sizes of 128, 256, 512, and 1024, and their average memory consumption was recorded.}   The results are shown in~\cref{sec:resultsMemory}.
\subsection{Battery Consumption}
\label{sec:resultsBattery}  \dk{
This section presents battery consumption rates during extended \gls{pp} and \gls{tg} tests. From our initial investigation, we observed that only \gls{avp} exhibited a slight impact of model size on battery consumption, while the other three devices did not show significant variations with changing model sizes. 
Given this observation, instead of testing all 17 models, we conducted battery tests on selected models. Specifically, we chose $m_1$ as the sole model from the Qwen Series. For the remaining series, we selected the smallest and largest models: $m_2$ and $m_5$ from the Vikhr-Gemma Series, $m_6$ and $m_{11}$ from the Phi-3.1 Series, $m_{12}$ and $m_{13}$ from the LLaMA-2 Series, and $m_{14}$ and $m_{17}$ from the Mistral-7B Series. 
For each experiment, we recorded the battery level at the start and end of the experiment, allowing us to calculate battery consumption over a fixed duration of $600$ seconds (10 minutes). Each experiment was repeated three times, and we reported the mean values.  To ensure consistent conditions, we took a $600$-second break between experiments, allowing the devices to cool down. The results are presented in~\cref{sec:resultsBattery}.}
\subsection{Pareto Optimality}
Although we evaluate our five performance metrics across various devices and models, the metrics remain fragmented and cannot be directly compared with other metrics. To address this, we turn to \textit{Pareto efficiency} theory~\cite{chinchuluun2007survey}, which provides a measure of efficiency in multi-objective contexts and has been widely applied in various system design approaches~\cite{brisset2015approaches, santoro2018design}. A choice is considered Pareto optimal if no other choice exists that can improve any of its objective criteria without deteriorating at least one other criterion. In our case, $x_{1}$ is considered dominated by $x_{2}$ through objects $f$ if :

\begin{multline} \label{equ:pareto}
f_i(x_1) \leq f_i(x_2) \quad \forall i \in \{1, \dots, m\} \\
\text{and} \quad \exists j \in \{1, \dots, m\} \mid f_j(x_1) < f_j(x_2)
\end{multline}

where $i$, $j$ represent different objective indices.

A choice $x^*$ is considered Pareto optimal if no other feasible option dominates it. The set of all non-dominated designs forms the Pareto front, which represents the optimal trade-offs between all objectives.

To identify the optimal choices across devices and models, we define \dk{three objectives: \textit{quality}, \textit{performance}, and \textit{stability}. } To calculate the final score for each objective, we propose~\cref{equ:score}. For a given device-model pair and a specific objective, we first apply min-max normalization to each metric across all device-model pairs to eliminate the impact of different scales. Then, we compute a weighted sum of all metrics for that pair to obtain a single score:

\begin{equation} \label{equ:score}
    f_{o}(x) = \sum_{i=1}^{n} w_i \cdot \frac{m_i(x) - \min(m_i)}{\max(m_i) - \min(m_i)}
\end{equation}

where $o$ is the objective index, $n$ is the number of metrics used in the objective category, $w_i$ defines the weight for each metric, and $\min$ and $\max$ calculate the minimum and maximum values across the entire set of results for metric $m_i$.

In practice, some metrics, such as perplexity, are better when lower, which is the opposite of the direction in~\cref{equ:pareto}. Therefore, we take the reciprocal of each perplexity value. 
% \begin{figure}[!htb]
%     \centering
%     \includesvg[width=\columnwidth]{figures/Apple_Vision_Pro_average_Prompt_Processing_Speed.svg}
%     %\vskip -0.5cm
%     \includesvg[width=\columnwidth]{figures/Magic_Leap_average_Prompt_Processing_Speed.svg}
%     \includesvg[width=\columnwidth]{figures/Meta_Q3_average_Prompt_Processing_Speed.svg}
%     \includesvg[width=\columnwidth]{figures/Vivo_average_Prompt_Processing_Speed.svg}
%     \caption{Prompt processing speeds (in tokens/second) for different devices, averaged over five prompt sizes: 64, 128, 256, 512, 1024, for each model. Error bars indicate standard deviations. \todo{vision pro results have now been updated}}
    
%     \label{fig:pp_speed_three_devices}
% \end{figure}


% \begin{figure}[!htb]
%     \centering
%     \includesvg[width=\columnwidth]{figures/Prompt_Processing_Speed_slope_heatmap.svg}
%     \includesvg[width=\columnwidth]{figures/Token_Generation_Speed_slope_heatmap.svg}
%     \includesvg[width=\columnwidth]{figures/Batched_Token_Generation_Speed_slope_heatmap.svg}
%     \caption{ \todo{I think we should change graph type: Heatmaps do not show a big difference.. } Heatmaps showing the effects of prompt size, token set size and batch size (respectively) on prompt processing, token generation and token generation performance (respectively) for each device-model pair. Brighter colors indicate stronger effects, though absolute values remain small.}
%     \label{fig:pptgbs_heatmap}
% \end{figure}