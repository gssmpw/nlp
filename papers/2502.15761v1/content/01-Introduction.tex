%\section{Introduction}
%\IEEEPARstart{I}{n} 
%\h{Research background and Significance:}  \gls{llm} are AI models used for human language processing and have gained significant attention from researchers in human-AI interactions, conversational visualization, automated content generation, sentiment analysis, and many other interesting areas.\\
%The deployment of \glspl{llm} in extended reality (XR) environments represents a significant step toward enabling more interactive, responsive, and context-aware XR applications. However, assessing the performance of \glspl{llm} across different XR and mobile devices presents unique challenges due to the wide range of hardware configurations and processing capabilities. 
%\subsection{Background and Motivation}
\IEEEPARstart{S}{ince}  the 2022 release of OpenAI's ChatGPT interface \cite{ChatGPT-Hystor_2023}, relying on the GPT-3.5 \gls{llm}, we have witnessed the disruptive and transformative effects of \glspl{llm}.  
These models are capable of describing a wide variety of topics, respond at various levels of abstraction, and communicate effectively in multiple languages. 
They have proven capable of providing users with accurate and contextually appropriate responses. \glspl{llm} have quickly found applications in tasks such as spelling and grammar correction \cite{Eker-GrammarCorrection_2023}, generating text on specified topics \cite{Adeshola-ChatGPT-Edu_2023}, integration into automated chatbot services, and even generating source code from loosely defined software specifications \cite{Feng-Software-ChatGTP_2023}.


Research on language models, and on their multimodal variants integrating language and vision or other technologies has recently experienced rapid growth.
For instance, in computer vision, language models are combined with visual signals to achieve tasks such as verbal scene description and even open-world scenegraph generation~\cite{Koch2024}. These technologies enable detailed interpretation of everyday objects, inference of relationships among them, and estimates of physical properties like size, weight, distance, and speed.  
In user interaction and visualization research, LLMs serve as verbal interfaces to control software functionality or adjust visualization parameters~\cite{Jia2024_1,Jia2024_2}.
Through prompt engineering or fine-tuning, loosely defined text can be translated into specific commands that execute desired actions within a system, supported by language model APIs.
The capabilities of language models continue to improve significantly from one version to the next. Yet, this comes at the cost of a significant growth in size of the most advanced models. As an example, models released or operated by leading companies such as OpenAI, Meta, Microsoft, and Google now reach the scale of trillions of trainable parameters~\cite{ChatGPT-Hystor_2023}. Due to their size and complexity, the training and inference of these models are limited to dedicated data centers equipped with considerable computational and memory resources. 

Another disruptive application of \glspl{llm} involves their use in wearable technology to assist users in various environments. For instance, Meta AI is being integrated into Meta's virtual reality hardware, where images captured by the device are streamed over the network to Meta's servers for model inference, enabling image interpretation or prompt response tasks.\footnote{https://www.meta.com/blog/quest/meta-ai-on-meta-quest-3/} The overarching vision is to develop assistive hardware that is as lightweight as sunglasses but capable of aiding users in a wide range of tasks by understanding the scene they are viewing.

While such technological advances have the potential to empower users in unprecedented ways, three main drawbacks must be considered due to the heavy reliance on network connectivity and cloud-based data processing. First, users will need to maintain {\em extremely high network connectivity} to ensure that remote services can perform assistive tasks without excessive latency, which would degrade the user experience. Second, {\em data privacy} poses a significant challenge. Many environments, such as medical settings and various industries, operate under strict data privacy regulations that prohibit the transmission of sensitive information to remote servers, precluding the use of these services in such contexts. Third, the reliance on {\em subscription models} for these assistive services introduces ongoing costs for users, requiring them to periodically purchase licenses, which can accumulate into significant long-term costs.

Therefore, there is dedicated research effort focused on developing autonomous AI assistive technologies that can perform inference directly on a device. To this goal, several smaller LLMs have been released by Meta, Mistral AI, Microsoft, and Google~\cite{Liu2024}. These models are designed to potentially operate on mobile devices or extended reality hardware and are typically around billions of trainable parameters in size. Some LLMs with fewer than a billion trainable parameters have shown very good results~\cite{Liu2024}.


Local execution of \glspl{llm} on XR devices is expected to become increasingly necessary for a wide range of applications, particularly those that require real-time processing and device autonomy. However, selecting the optimal device and model for a specific application is a complex decision: device specifications and model documentation alone are not sufficient for making it, as the performance of most LLMs running on various hardware components remains largely undocumented. Furthermore, the hardware configurations of XR devices (such as CPU, memory capacity, and thermal management) and the architectures of the LLMs can significantly influence the results. Therefore, defining standardized evaluation criteria that address these complexities and ensures fairness across tests is also a research challenge. To the best of our knowledge, no comprehensive study currently provides clear guidelines for such evaluations. \\
To address these challenges, we present LoXR, a comprehensive study deploying LLMs on XR, focusing on performance evaluation across various metrics. Considering the multiple challenges and factors affecting the evaluation, we first identify key evaluation metrics, and then design an experimental setup to minimize potential bias. Specifically, we deploy 17 \glspl{llm} across four XR devices---Magic Leap 2, Meta Quest 3, Vivo X100s Pro\footnote{Vivo X100s Pro is strictly speaking not an XR device.
\dk{We include it in our comparison because smartphones are commonly used as XR devices, and this device features competitive hardware specifications in the market. }}, and Apple Vision Pro---and conduct a thorough evaluation in terms of 
\dk{performance consistency \ie stability over time,
processing speed, memory usage, and battery consumption. This results in 68 model-device pairs. A key challenge} is the effect of varying string lengths, batch sizes, thread counts, and background processes. To account for string length variation, we conducted experiments using different lengths. To mitigate hidden factors like background processes, each experiment was repeated five times, and average values were calculated. \dk{To provide task-specific analysis, experiments were conducted across all \glspl{llm} tasks: \gls{pp} for evaluating performance with varying prompt lengths, \gls{tg} for varying generated token set sizes, plus \gls{bt} and \gls{tt} for assessing parallelism and concurrency efficiency.}
We believe our experimental protocol and results will serve as a foundation for further research and development in this emerging field. 

 In summary, this paper contributes: 
\begin{itemize}[noitemsep,leftmargin=8pt, topsep=0pt]
\item An evaluation approach and an experimental setup for assessing the performance of \glspl{llm} on XR devices, that can be used as standard  guidelines for future research.
\item The deployment of 17 \glspl{llm} on four XR devices, and a comprehensive evaluation, resulting in 68 model-device pairs, across five critical metrics: consistency, processing speed, concurrency and parallelism, memory usage,   and battery consumption.
% \item The deployment of various  \glspl{llm} on XR devices, and a simple use case in the form of the \magicchat, which provides a simple interface for textual and verbal communication with \glspl{llm} on Magic Leap. 
\end{itemize}