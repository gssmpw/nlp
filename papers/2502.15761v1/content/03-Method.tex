
\section{Research Methods and Materials} 
\label{sec:method}   
The aim of this study is to deploy \glspl{llm} on XR devices and evaluate their performance in different aspects.
This section outlines our research methodology, including the key challenges, the LLM deployment approach, and the evaluation metrics and models used in this study. \\
\textbf{Problem Statement:} In this study, we deploy 17 \glspl{llm}, denoted as \( \{ m_1, m_2, \dots, m_{17} \} \) (see~\Cref{tab:models}), \dk{across four XR and mobile devices: \( d_1, d_2, d_3, \) and \( d_4 \), representing \gls{ml2},\footnote{\url{https://www.magicleap.com/magicleap-2}} \gls{mq3},\footnote{\url{https://www.meta.com/quest}} \gls{vivo},\footnote{\url{https://www.vivo.com/en}} and \gls{avp},\footnote{\url{https://www.apple.com/vision-pro}} respectively. } Our goal is to perform a comprehensive evaluation of these models on the four devices to determine how well they handle large-scale language processing tasks in resource-constrained XR environments. 

\dk{We define our evaluation using six key performance metrics: 
\( p_0, p_1, p_2, p_3, p_4, p_5 \), which represent  
\textit{Model Quality},  
\textit{Performance Consistency},  
\textit{Performance Speed} (with respect to string length and prompt length),  
\textit{Parallelism} (with respect to thread count and varying batch size),  
\textit{Memory Usage}, and  
\textit{Battery Usage}, respectively.\\
Each \gls{llm} \( m_i \in \{m_1, m_2, \dots, m_{17}\} \) is tested on each XR device \( d_j \in \{d_1, d_2, d_3, d_4\} \) for evaluation across each performance metric \( p_k \in \{p_0,p_1, p_2, p_3, p_4, p_5\}. \)
 The evaluation can be summarized as \( X_{mdk} \), as given in~\cref{eq:overall_evaluation}.
\begin{equation}
\label{eq:overall_evaluation}
X_{mdk} = \{ x_{ijk} \mid i \in \{1, \dots, 17\}, j \in \{1, \dots, 4\}, k \in \{0, \dots, 5\} \}
\end{equation}}
Here, \( x_{ijk} \) represents the performance of model \( m_i \) on device \( d_j \) for metric \( p_k \), forming the basis for cross-device and cross-model analysis. \\
\cref{fig:mainMethod}  presents an overview of our research pipeline, where each rectangular box represents a specific procedure, and the connected ovals indicate their respective outcomes. These are discussed in detail below.


\begin{figure}[tbp]%[!htbp]
    \centering 
    % \includegraphics[width=1\linewidth]{images/Mainchart.pdf}  
    % \includegraphics[width=1\linewidth]{images/LoXRMain2.pdf}   
    \includegraphics[width=1\linewidth]{images/Mainchartv5b.pdf}  
\vskip -0.245cm \caption{\dk{Overview of our research process. The rectangular boxes represent the procedures, while the ovals indicate the outcomes. We deployed 17 \glspl{llm} on four XR devices. First, we identified the key factors affecting performance. Based on these factors, we defined our evaluation criteria and conducted the experiments. Finally, we applied 3D Pareto analysis, which provided the Pareto front to highlight the best-performing model-device pairs.}}
    \label{fig:mainMethod}
\end{figure} 


\subsection{LLM Deployment on XR Devices}
We aim to deploy the \glspl{llm} locally on XR devices. For this purpose, we customize the Llama.cpp library \cite{meta2023introducingLLaMA} and build it for the four target XR devices. The resulting application is capable of loading various appropriately sized \gls{gguf} models.  Our deployment leverages the functionalities of \llama, allowing us to run basic \glspl{llm} tasks, and adjust various parameters.
For example, to evaluate different aspects in an experiment, we can control various parameters such as prompt length in \emph{Prompt Processing} (\gls{pp}) and the size of the token set in \emph{Token Generation} (\gls{tg}). To benchmark the models on various devices, the top-level scripts in Llama.cpp are compiled into binary executable files. These binaries include scripts specifically designed for testing both execution speed and model quality. \dk{ For Apple Vision Pro, we utilized Xcode\footnote{\url{https://developer.apple.com/xcode/}}, Apple's official IDE, to build and deploy testing scripts directly to the device. For the remaining three devices, the testing was performed via a shell interface opened through \gls{adb}.
}
In our current study, we selected 17 models, each running on the four chosen devices.\\
\dk{Due to compatibility constraints with Llama.cpp, we conducted testing exclusively on the CPU for Magic Leap 2, Meta Quest 3, and Vivo X100s Pro. As a smartphone, the Vivo X100s Pro theoretically supports GPU inference through frameworks like TensorFlow Lite or ONNX Runtime, but Llama.cpp does not currently offer GPU inference support for this device. Similarly, Magic Leap 2 and Meta Quest 3, being XR-specific devices, lack user-accessible GPU inference capabilities compatible with Llama.cpp. In contrast, Apple Vision Pro supports GPU inference via Metal, which we used in our experiments. These device and framework limitations necessitated restricting inference to CPUs for the first three devices.}
\subsection{Initial Investigation and Factors Identification} 
\label{sec:InitInvestigation}
 After deploying the \glspl{llm}, 
 we conducted an initial investigation through anecdotal informal tests, literature reviews, and by studying LLM documentations. The aim of this preliminary investigation was to identify the key challenges and factors affecting performance---prerequisites for a fair evaluation---and to define our evaluation approach (\cref{sec:evCriteria}).\\
We observed that XR devices experience performance degradation during prolonged tasks due to factors such as overheating, battery depletion, and background processes. Moreover, they often do not perform at a stable level. For instance, with the same model-device pair and parameters, we obtained different results across different runs of the same test. We refer to the metric used for measuring this variability as \textit{\dk{Performance Consistency}}. Another important factor is execution time: the time taken to execute a particular task. In our context, we used a relative term called \textit{Processing Speed}, measured in \textit{tokens per second}. We observed variations in processing speed depending on varying  string length, batch size, and thread count. In addition, \textit{Memory Consumption} and \textit{Battery Consumption} are also important factors to consider. Similarly, we observed different processing speeds for different tasks. For example, prompt processing and token generation exhibited varying performance, even with the same model-device pairs and parameters.\\
In addition to the parameters mentioned above, during the initial investigation, we also noticed that ensuring consistent testing conditions across devices is critical for fair comparisons but challenging to maintain.  \dk{For example, variance in a device's \textit{Performance Consistency} could create biases into the results.} LLM performance may vary due to different architectures and device-specific optimizations. Even with identical parameters, the same device and LLM can produce different results due to unknown factors. The device's environment, such as exposure to heat or obstructions to the cooling fan, can also affect performance. The concise presentation of the findings, and ensuring their reproducibility for re-evaluation and validation, is an additional challenge. Considering these challenges, we formulated a set of guidelines as our evaluation approach in~\cref{sec:evCriteria}. 
\subsection{Evaluation Approach}
\label{sec:evCriteria}
\dk{Considering the key challenges and our initial investigation (see~\Cref{sec:InitInvestigation}), we formulate a standardized evaluation approach for our assessments. First, we designed an experimental setup to minimize potential biases and ensure the validity of the results (see~\cref{sec:exp:design}).  
This setup establishes identical parameters across all devices and defines the test types to be executed.  \\
We developed several guidelines to ensure a comprehensive and standardized evaluation.  
To account for potential performance variations among \glspl{llm}, we selected 17 models from five different series (see~\Cref{tab:models}) and evaluated them across four devices. Additionally, we conducted \textit{Model Quality} analysis to provide technical insights into the selected models. These tests are device-independent, and were conducted to understand the details and properties of each model.  
Next, each model-device pair was subjected to various tests (see~\cref{sec:metricsUsed}).\\
To mitigate performance fluctuations, each experiment was repeated multiple times, and the average results were computed.  
Finally, we unified the \dk{five evaluation metrics, \ie $[P_1, P_5]$}, using \textit{Pareto Efficiency} theory to identify Pareto-optimal model-device pairs. Our evaluation framework and experimental design aim to facilitate reproducibility for other researchers. Additionally, we will provide open-source test scripts to enable replication of our study and further evaluations.}
\subsection{Models and Evaluation Metrics}
\label{sec:metricsUsed}
We use 17 LLMs \dk{from five different series} (see~\Cref{tab:models}) in our evaluations. The models were obtained from Hugging Face's model repository, a widely used platform for accessing pre-trained LLMs~\cite{huggingface2024}.
\dk{For consistency in naming conventions, we assigned unique IDs ($m_1$ to $m_{17}$) to these models.}
\begin{table}[!htb]
\caption{\dk{Models used, with their IDs, names, versions (v), quantization level (Q), layers (L), parameters (P), and sizes, sourced from Hugging Face~\cite{huggingface2024}.}}
\vskip -0.45cm 
\centering
\resizebox{\linewidth}{!}{
%\begin{tabular}{l| l l c c c c c}
\begin{tabular}{l l l c c c c c}
\hline
Series & ID & Model Name  & v & Q & L & P & Size \\
\hline
\multirow{1}{*}{Qwen} & $m_1$   & qwen2-0\_5b-instruct-fp16  & v2.0 & FP16 & 24 & 0.5B & 0.942 GB \\ 

\hline
\multirow{4}{*}{Vikhr-Gemma}
%{\rotatebox[origin=c]{45}{Vikhr-Gemma}} 
& $m_2$  & Vikhr-Gemma-2B-instruct-Q3\_K\_M & v1.3 & Q3 & 26 & 2.61B & 1.36 GB \\ 
& $m_3$ & Vikhr-Gemma-2B-instruct-Q4\_0 & v1.3 & Q4 & 24 & 2.61B & 1.51 GB \\ 
& $m_4$ & Vikhr-Gemma-2B-instruct-Q5\_0  & v1.3 & Q5 & 24 & 2.61B & 1.75 GB \\ 
& $m_5$  & Vikhr-Gemma-2B-instruct-Q6\_K  & v1.3 & Q6 & 24 & 2.61B & 2.00 GB \\ 

\hline
\multirow{6}{*}{Phi-3.1}  
& $m_6$ & Phi-3.1-mini-4k-instruct-Q2\_K  & v3.1 & Q2 & 20 & 3.82B & 1.32 GB \\ 
& $m_7$  & Phi-3.1-mini-4k-instruct-Q3\_K\_L  & v3.1 & Q3 & 20 & 3.82B & 1.94 GB \\ 
& $m_8$  & Phi-3.1-mini-4k-instruct-Q4\_K\_L &  v3.1 & Q4 & 20 & 3.82B & 2.30 GB \\ 
& $m_9$ & Phi-3.1-mini-4k-instruct-Q5\_K\_L & v3.1 & Q5 & 20 & 3.82B & 2.68 GB \\ 
& $m_{10}$ & Phi-3.1-mini-4k-instruct-Q6\_K & v3.1 & Q6 & 20 & 3.82B & 2.92 GB \\ 
& $m_{11}$  & Phi-3.1-mini-4k-instruct-Q8\_0  & v3.1 & Q8 & 20 & 3.82B & 3.78 GB \\ 

\hline
\multirow{2}{*}{LLaMA-2}  
& $m_{12}$  & llama-2-7b-chat.Q2\_K &  v2.0 & Q2 & 28 & 6.74B & 2.63 GB \\ 
& $m_{13}$  & llama-2-7b-chat.Q3\_K\_S  & v2.0 & Q3 & 28 & 6.74B & 2.75 GB \\ 

\hline
\multirow{4}{*}{Mistral-7B}  
& $m_{14}$ & Mistral-7B-Instruct-v0.3.IQ1\_M  & v0.3 & IQ1 & 28 & 7.25B & 1.64 GB \\ 
& $m_{15}$  & Mistral-7B-Instruct-v0.3.IQ2\_XS  & v0.3 & IQ2 & 28 & 7.25B & 2.05 GB \\ 
& $m_{16}$  & Mistral-7B-Instruct-v0.3.IQ3\_XS  & v0.3 & IQ3 & 28 & 7.25B & 2.81 GB \\ 
& $m_{17}$  & Mistral-7B-Instruct-v0.3.IQ4\_XS  & v0.3 & IQ4 & 28 & 7.25B & 3.64 GB \\ 

\hline
\end{tabular}
}

\label{tab:models} 
\end{table}
 

\dk{Based on the \llama documentation~\cite{meta2023introducingLLaMA} and our initial investigation, this evaluation comprises multiple tests to assess various performance parameters. For testing, we use \textit{Synthetic Prompts} automatically generated by \texttt{llama.cpp}, ensuring uniform testing conditions without reliance on an external dataset. The string length is controlled via a parameter set to 64, 128, 256, 512, and 1024 tokens.
The evaluation includes the following tests. 
\begin{itemize}
    \item \textbf{Performance Consistency:} Measures stability over time.
    \item \textbf{Processing Speed:} Analyzes performance across different string and prompt lengths, further divided into:
    \begin{itemize}
        \item \textbf{PP (Prompt Processing):} Evaluates encoding efficiency in handling input prompts of varying lengths.
        \item \textbf{TG (Token Generation):} Evaluates the device's speed in generating output tokens of varying lengths.
    \end{itemize}
    \item \textbf{Parallelization and Concurrency:} This includes:
    \begin{itemize}[noitemsep, leftmargin=8pt, topsep=0pt]  
    \item \textbf{Batch Test (BT)}: Evaluates the device's ability to process multiple input samples simultaneously by handling different batch sizes during token generation.  
    \item \textbf{Thread Test (TT)}: Measures performance scalability by varying thread counts during LLM execution.  
\end{itemize}  
\end{itemize}
} 




\dk{Here, \textbf{Batch Size} refers to the number of input samples processed in parallel. While larger batch sizes improve computational efficiency, they also demand more memory. Each of these metrics requires task-specific analysis for both models and devices. Furthermore, we conducted additional evaluations for \textit{Memory Usage} and \textit{Battery Consumption}. Finally, we performed a \textbf{Pareto Optimality} analysis to collectively examine the results across different models and devices.}  

 







