\section{Discussion}
\label{sec:discussion}
\dk{We have performed extensive testing of multiple open-source language models on four distinct devices, ranging from the two-year-old Magic Leap 2, representing the state of the art in passive see-through AR devices, to Meta Quest 3, arguably the most common VR headset worldwide, and VIVO X100s Pro as an indication of the performance of next-generation mobile computing, up to Apple Vision Pro, the highest-performance VR headset on the market. The performance in all devices shows that each of the devices currently available on the market is capable of LLM inference, achieving several tokens per second, and each of them can be used as an autonomous device for performing LLM inference for a given task. }

The deployment of \glspl{llm} was achieved through a customized implementation of the \llama~\cite{meta2023introducingLLaMA} library for each device. We selected \llama due to its C++-based architecture, which offers portability, flexibility, and seamless integration across various platforms and applications. This customization enables \llama to perform core \glspl{llm} tasks such as \gls{pp} and \gls{tg}, while providing the flexibility to adjust control parameters such as string length, batch size, and thread counts. 

Our study identifies critical performance evaluation factors and introduces a suite of testing scripts to analyze these aspects systematically. These scripts enhance the reproducibility of our findings, making them relevant for future analyses in similar contexts.  \dk{The processing speed, measured in tokens per second, was evaluated across multiple dimensions. This includes a consistency analysis to determine whether each model-device pair maintains stable performance over time or exhibits fluctuations. The impact of string length on both \gls{pp} and \gls{tg} was analyzed for each model-device combination. Additionally, the study examined the support for parallelization and concurrency control through BT and TT tests. Furthermore, memory usage and battery consumption were also assessed to provide a holistic evaluation of system performance.}


% Performance Analysis
\dk{For consistency analysis, we ran each model-device pair 20 times with conservative time intervals to observe performance variations. We reported the lowest and highest speeds, mean speed, standard deviation, and coefficient of variance (CV).
Examining~\cref{fig:VPGPU,fig:StabDD}, which present the results in ascending order, we observe that the \gls{avp} GPU exhibits the highest stability. Among CPU-based devices, the \gls{avp} CPU and \gls{ml2} demonstrate the most stable performance. The quantitative results, particularly the CV values reported in~\cref{tab:consistency,tab:perfConsistencyresultsForm1}, further reinforce this observation. While \gls{ml2} shows a similar level of stability to the \gls{avp} CPU, its first three models ($m_2$, $m_3$, and $m_4$) exhibit a CV above $20\%$ for TG and above $26\%$ for PP.
Considering the error count in~\cref{fig:resErrorsCount}, \gls{ml2} emerges as the most stable device, having recorded zero errors. In contrast, the \gls{avp} CPU encountered multiple errors. The remaining two devices, \gls{vivo} and \gls{mq3}, exhibit higher instability, with \gls{vivo} performing relatively better than \gls{mq3}. This also indicates that consistency is model-dependent, as seen with models $m_2$, $m_3$, and $m_4$, which exhibited reduced stability. }\\
%PP and TG tests 
\dk{ For $m_1$, being the smallest model, all devices performed efficiently. Among the remaining models ($m_2$ to $m_{16}$), the average inference speeds  across various devices are as follows. \gls{avp} (CPU) achieved the highest inference speed  (16.91 t/s PP, 11.04 t/s TG), followed by \gls{ml2} (12.68 t/s PP, 8.51 t/s TG), \gls{mq3} (9.52 t/s PP, 5.77 t/s TG), and \gls{vivo} (8.62 t/s PP, 6.12 t/s TG), reflecting performance variations across XR devices. These speeds meet basic conversational requirements and enable LLM inference for broader applications beyond dialogue-based tasks. The PP speed across all devices is  two to three times faster than TG speed, while for \gls{avp} GPU PP speed is 10 to 19 time faster than TG speed. This is because PP only encodes the input, whereas TG must process sequential dependencies during decoding. 
In model-based analysis, smaller models such as $m_1$, $m_2$, $m_3$, and $m_5$ are the fastest, while $m_{14}$ and $m_{16}$ were the slowest. Regarding string length, larger PP and TG inputs (512, 1024) are slower than shorter ones (64, 128, 256). Moreover, longer strings (512, 1024) frequently result in errors. Among devices, \gls{mq3} exhibited the highest number of errors, followed by the \gls{avp} CPU and then \gls{vivo}. In contrast, \gls{ml2} and the \gls{avp} GPU did not experience any significant errors or inconsistencies.\\
% Threading and Batch Processing
In batch tests (BT), performance generally decreases as batch size increases across all devices, except for the \gls{avp} GPU, which remains stable regardless of batch size. The \gls{avp} CPU, however, experiences a strong performance degradation as batch size increases, more pronounced than in other devices. For thread tests (TT), across all four devices, mid-range thread counts (4 to 8) yielded the best performance, while lower (1, 2) and higher (16, 32) thread counts resulted in lower performance. Notably, the \gls{avp} CPU fails at thread counts of 16 and 32, whereas the \gls{avp} GPU exhibits its best performance at these higher thread counts.\\
% Memory Consumption
In terms of memory consumption, significant variations were observed across different models. Generally, larger models consumed more memory. Interestingly, each of the five model series displayed distinct memory consumption trends. Some smaller models from one series consumed more memory than larger models from another series. For instance, $m_2$ (with size 1.36 GB) consumes less memory than $m_6$ (with size 1.32 GB), highlighting the model-dependent nature of memory consumption (see~\cref{fig:Memory_plot}).
}\\
 % Battery consumptions 
\dk{Battery consumption did not show significant differences across devices, although the Apple Vision Pro exhibited slightly higher battery life loss, with an average battery depletion of up to $10.1\%$ (with GPU) and $12.6\%$ (with CPU) over 10 minutes. In contrast, the Vivo X100 Pro performed the best, with only a $2.5\%$ loss, while the Magic Leap 2 demonstrated good battery efficiency with an $8.5\%$ loss. The Meta Quest 3 consumed an average of $9.7\%$ over the same duration.}
%Practical Considerations
\\
\dk{With 3D Pareto Optimality, we found \gls{avp} leading overall, \gls{ml2} demonstrating high consistency and stability, \gls{vivo} being slower but still consistent in performance, and \gls{mq3} ranking lowest across all aspects. Among the models, $m_3$, and $m_5$ performed particularly well, with smaller models generally exhibiting better results. In contrast, models from the LLaMA-2 and Mistral-7B series were the slowest.  }
Our study highlights the trade-offs between model complexity and system responsiveness. Larger models exhibit lower speed though they may offer greater accuracy and richer contextual responses. These larger models also drain batteries faster and decrease thermal stability. Conversely, lighter models enhance efficiency but may compromise response quality.  \\
At this point, we can conclude that nowadays, the generation of XR devices is capable of executing interactive conversational tasks directly on the device. This is an important step towards XR device autonomy (and for edge devices in general) and an opportunity to perform complex tasks without any network connectivity requirement. We can anticipate that the next generation of XR devices with better computational capabilities, memory capacity, and battery life, will allow for complex on-device multimodal inference tasks, aiding users in a broad spectrum of %assistive and navigational 
tasks. \\
\dk{\noindent{}\textbf{Limitations:} Apart from the \gls{avp}, our study primarily focuses on CPU-based evaluation and does not fully explore the potential of GPU- and NPU-accelerated inference for on-device LLM deployment. The limited VRAM on the three devices (\gls{ml2}, \gls{vivo}, and \gls{mq3}) restricted full model loading, which hindered GPU performance. Additionally, the specialized frameworks and configurations required for GPU and NPU optimization were beyond the scope of this work. 
Finally, since the study is primarily based on \textit{Llama.cpp} implementations, its generalizability is limited.
}\\
 \dk{\noindent{}\textbf{Future Research:} In future research, we plan to conduct an in-depth exploration of GPU and NPU-based deployment, aiming to optimize performance for lower-memory GPUs while maximizing efficiency.  Furthermore, the integration of visual language models (VLMs) capable of processing images and graphical inputs will be ideally suited for the XR domain, expanding potential applications across various fields, including education, healthcare, and robotics.
Additionally, voice- or text-based interaction with on-device \glspl{llm} can lead to several XR applications. For example,  training the voice system with a specific teacher’s voice and utilizing it as a communication medium for a teacher’s virtual avatar could create a compelling educational tool—a virtual tutor. A similar approach could also be applied to humanoid robots (or avatars in VR), enabling more natural and interactive communication.
}
\section{Conclusion}
\label{sec:conclusion}

\dk{\textit{LoXR} presents a comprehensive study on deploying LLMs on four XR devices, conducting an in-depth analysis of multiple performance factors. Given the GPU memory limitations of most XR devices, our study primarily focuses on CPU-based analysis. However, since the \gls{avp} has sufficient GPU resources, we also include GPU results for \gls{avp}, providing additional insights into the study.
 Generally, for processing speed, we can rank the four devices in the following order: \gls{avp}, \gls{ml2}, \gls{vivo}, and \gls{mq3}, although \gls{mq3} outperforms \gls{vivo} in TG.\\
\dk{In memory consumption, \gls{avp} was found the most memory-efficient. Among the remaining devices, differences were minimal, though \gls{ml2} occasionally consumed the most memory. As expected, larger models required more memory, but this trend varied across different model series. For BT tests, a lower batch size (128) yielded the best performance, while larger batch sizes reduced speed for CPU-based tests, with \gls{avp} GPU remaining largely unaffected. For TT tests, medium thread counts (4–8) were optimal across all devices. In terms of consistency, \gls{ml2} is the most stable, exhibiting the lowest variance in speed and zero errors. \gls{avp} GPU also maintains low variance. \gls{mq3} performs the worst in this aspect, while \gls{vivo} is more stable than \gls{mq3} but less consistent than the rest. } 
}\\ 
  The Pareto analysis highlighted several models on \gls{avp} due to their outstanding performance, while the Pareto fronts found on \gls{ml2} because of their consistently stable results. Additionally, a few models on \gls{vivo} and \gls{mq3} emerged on the Pareto front because of their balanced trade-offs between performance and stability. Models $m_{12}$ to $m_{17}$ were significantly slower.\\



