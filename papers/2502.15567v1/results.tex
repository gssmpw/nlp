

    This section examines the goodness of defense strategies when the defender knows that the attacker adopts a specific learning algorithm including $k$-nearest neighbors ($k$-NN) and polynomial regression. More learning scenarios are considered in the supplementary material, including kernel ridge regression, neural networks, and empirical risk minimization over general bounded function classes. In the supplement, we also explore the conditions under which defending is easier.
    Our findings help defenders assess the merits of different defense strategies and selecting the most suitable one. 
    Recall that each defense mechanism $\dm$ in comparison satisfies that $\ul_n(\dm) = \Ul_n$, except for no defense.
    With this in mind, we investigate the following defense mechanisms: 
    
    1. No Defense (``$\dno$''): the perturbation $e_i=0$ for $i=1,\ldots, n$. Note that $\dno$ has zero utility loss, differently from other defenses that incur a positive utility loss. 
    
    2. IID Noising (``$\drn$''): $e_i$'s are IID Gaussian with mean zero and variance $\sigma^2_n = \Ul_n > 0$. 
    
    3. Constant Noising (``$\dun$''): $e_i=\tau_n, i=1,\ldots, n$, where $\tau_n=\pm \sqrt{\Ul_n}$ is a constant for each $n$.

    Guided by the developed theory, we also propose novel defenses that are rate optimal under some conditions. Throughout our discussion, we focus on regression tasks unless stated otherwise. 
    % Our findings are summarized in \Autoref{tab:def}. 
    % The proofs of all theorems and propositions are included in Appendix. 

    There are two unique challenges in the theoretical development of model privacy. The first challenge pertains to non-IID observations. Classical learning theory typically assumes that observations are IID or at least independent; however, the perturbations injected into model responses can have an arbitrary dependence structure. Therefore, traditional probabilistic tools that rely on independence are not applicable. This necessitates the development of new analytical tools. 
    The second challenge involves the allowance for diminishing noise levels. Classical learning theory usually assumes a fixed noise level, but a vanishing noise is of particular interest under model privacy.
    Specifically, a vanishing noise means that the service quality remains almost unaffected for benign users, which is especially appealing to the model owner. Our findings indicate that it is possible for the model owner to defend against model stealing attacks while maintaining high service quality.

    
    % \begin{table}
    %     \centering
    %     \resizebox{\textwidth}{!}{
    %     \begin{tabular}{cccccc}
    %         \toprule
    %         \multicolumn{1}{c}{\multirow{2}{*}{Defense Mechanism}} &\multicolumn{1}{c}{\multirow{2}{*}{}}
    %         & \multicolumn{3}{c}{Attacker's Learning Algorithm} \\ 
    %         \cmidrule(l){3-6}
    %          &  & $k$-Nearest Neighbors & Polynomial Rgression & Kernel Ridge Regression & Neural Networks \\ \midrule 
    %        \multirow{2}{*}{No Defense}  & Potent (\Autoref{def:defstr}) & \xmark &\xmark&\xmark & \xmark \\ 
    %          & Rate optimal (\Autoref{def:priv}) &NA&NA&NA & NA\\ \hline
    %         \multirow{2}{*}{Constant Noising}  & Potent & \cmark &\cmark&\cmark&  \cmark\\ 
    %          & Rate optimal &\cmark&\xmark&\xmark & \xmark\\ \hline
    %          \multirow{2}{*}{Order Disguise (proposed)}  & Potent &&\cmark&&  \\ 
    %          & Rate optimal &  & \textbf{?} & & \\ \hline
    %          \multirow{2}{*}{Kernel Confusion (proposed)}  & Potent &&&\cmark&\cmark  \\ 
    %          & Rate optimal &  &  &\cmark &\cmark \\ 
    %          \bottomrule
    %     \end{tabular}}
    %     \caption{Goodness of defense mechanisms under different attack scenarios. Order Disguise denotes our proposed defense mechanism against attackers using polynomial regression, and Kernel Confusion denotes our proposed defense against attackers with kernel ridge regression or wide neural networks. NA means not applicable, and a question mark indicates that the result remains unclear.}
    %     \label{tab:def}
    % \end{table}
    

    \def\knn{\textrm{knn}}
    \subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors}
        The $k$-nearest neighbors algorithm \citep[see, e.g., ][]{gyorfi2002distribution}, denoted by $\la_k$, is a classical learning method that predicts the output at an input $x$ using the average value of the observed responses near $x$. Specifically, the observations are sorted in ascending order of $\norm{\X_i- x}_2$ as $(\X_{(1,n)}(x), \Y_{(1,n)}(x)), \ldots, (\X_{(n,n)}(x), \Y_{(n,n)}(x))$, and
        the fitted regression function is given by
        \begin{align}\label{eq:knn}
            \hat{f}_n(x) = k^{-1} \sum_{j=1}^k \Y_{(j,n)}(x),
        \end{align}
        where $k$ is a hyper-parameter decided by the attacker and may depend on $n$.
        In our analysis, the attacker's set of possible learning algorithms is $\lac_{\knn} = \{\la_k: k>0 \}$, and the defender's function belongs to a Lipschitz class $\fc_{\knn}\defeq \hld(C,1)$.
        \Autoref{thm:knn} summarizes the quantification of privacy level in terms of utility loss for each defense mechanism considered.
        
        \begin{theorem}\label{thm:knn}
           Suppose that $\lim_{n\to\infty} n^{2/d}\Ul_n =\infty $.
           Under \Autoref{asmp:same_query, asmp:bounded_input, samp:density}, the following hold:
           
                (i) For $\dno$, we have $\pl_n(\dno,\fc_{\knn}\mid\qiid, \lac_{\knn} ) \lesssim n^{-2/d}$.
                
                (ii) For $\drn$, we have $\pl_n(\drn,\fc_{\knn}\mid\qiid, \lac_{\knn}) \asymp (\Ul_n/n)^{2/(d+2)}$.
               
                (iii) For $\dun$, we have $\pl_n(\dun,\fc_{\knn}\mid\qiid, \lac_{\knn}) \asymp \Ul_n$.
      
                (iv) For any defense mechanism $\dm \in \dmc(\Ul_n)$,
                we have $\pl_n(\dm,\fc_{\knn}\mid\qiid, \lac_{\knn}) \lesssim \Ul_n$.
        \end{theorem}
        Based on \Autoref{thm:knn}, we can assess each defense's effectiveness and answer certain questions posed in \Autoref{sec:formulation}. Three key observations are summarized below, where we may omit the attack strategy and function class in the notations for ease when there is no ambiguity.
        
        1. \textbf{Defense is necessary:} Without defense, we have $\pl_n(\dno) \lesssim n^{-2/d}$, meaning that the necessary stealing sample size $n$ to build an $\epsilon$-close $\hat{f}_n$ is at most at the order of $\epsilon^{-d/2}$. In contrast, when using nature-collected data with fixed noise variance $\sigma^2$, the necessary stealing sample size is at the order of $\epsilon^{-(d+2)/2}$. 
        Thus, querying the defender significantly reduces the attacker's necessary stealing sample size compared to collecting data from nature.
        % We can also obtain this result by directly comparing the privacy level of different defenses, given that a higher privacy level implies a higher necessary stealing sample size. For example, we know defense is needed in this case since $\pl_n(\drn)/\pl_n(\dno) \to \infty$ as $n \to \infty$.
        
        2. \textbf{Potent defense mechanisms exist:} Constant Noising $\dun$ is \eco. When $\dun$ is deployed with $\Ul_n = \sigma$, the necessary stealing sample is infinity for $\epsilon < \sigma$. As a result, the attacker cannot rebuild a good model under $\dun$. 
        % $\pl_n(\dun)/\pl_n(\drn) \to \infty$. 

        3. \textbf{Rate optimal defense:} $\dun$ is rate optimal because \Autoref{thm:knn}(iv) shows that $\pl_n(\dm) \lesssim \Ul_n$ for any defense mechanism $\dm \in \dmc(\Ul_n)$, and $\dun$ achieves this rate.

        Moreover, a higher privacy level implies a higher necessary stealing sample size. Therefore, a convenient way to compare the effectiveness of different defense mechanisms is comparing their privacy levels directly. For example, we can conclude that IID Noising is better than no defense since $\pl_n(\drn)/\pl_n(\dno) \to \infty$ as $n \to \infty$. Similarly, Constant Noising is better than IID Noising because $\pl_n(\dun)/\pl_n(\drn) \to \infty$. 
 

        
        
        % \def\knn{\textrm{knn}}
    \def\preg{\textrm{poly}}
    \subsection{Polynomial Regression}\label{subsec:lr}
        This subsection studies the scenario where the attacker's learning algorithm fits a polynomial function with model selection. 
        For simplicity, we assume that $\X$ is univariate. 
        The attacker considers the following nested polynomial function class:
        \begin{align*}
            \mathcal{G} = \biggl\{f_q(\cdot;\bbeta_q): X \to \phi_q(X)^\T \bbeta_q, \phi_q(X)=(1, X^{1},  \ldots, X^{q})^\T,  \bbeta_q \in \Real^{q+1}, q=0,1,\dots, q_n \biggr\},
        \end{align*}
        where $q_n<n$ is the highest order of polynomials considered for each sample size $n$.
        To perform model selection, the attacker uses the generalized information criterion $\AIC$ \citep[see, e.g., ][]{shao1997asymptotic, ding2018model}.
        Specifically, for a given hyper-parameter $\lambda_n$ for model complexity regularization, the attacker fits the following function:
        \begin{align*}
            \hat{f}_n = \argmin_{f_q(\cdot;\bbeta_q) \in \mathcal{G}}   \biggl[ \frac{1}{n}\sum_{i=1}^n \{\Y_i-f_q(X_i;\bbeta_q)\}^2 + \frac{ \lambda_n \hat{\sigma}_{n,q}^2 q}{n} \biggr],
        \end{align*}
        where $\hat{\sigma}_{n,q}^2$ is an estimator of the variance of the random error in the regression model, assumed to be upper bounded by a constant $B>0$. The collection of possible learning algorithms is $\lac_{\preg} = \{\la_{\lambda_n}: \lambda_n \geq 2, \lambda_n / n \to 0\}$.
         We assume that the defender's model $\fd$ lies in a family $\fc_{\preg}=\{f: X \to \phi_p(X)^\T\bbeta_p, \bbeta_p \in \Real^{p+1}, \beta_{p+1} \neq 0, \norm{f}_2\leq C \}$ with a fixed order $p$ and constant $C>0$. 

        % \begin{assumption}\label{asmp:pr}
        % There exist some  positive constants $c_1,c_2,c_3,$ and $B$ such that
        %     (1) $\lim_{n\to\infty} \sigma_n^2 e^{nc} = \infty$ for any $c>0$, and there exists a $\gamma<1/(4p+2)$ such that $\P(\abs{\hat{\sigma}^2_{n,q}-\sigma^2_n} \geq \gamma \sigma^2_n) \leq e^{-nc_3}$ for all $0<q<n$. (2) $\norm{f}_2^2 \leq B $ for every $f \in \fc$. 
        %         (3) $\P(\norm{\Sigma_q\hat{\Sigma}_q^{-1}} \geq c_1+c_2\ln(1/\epsilon)) \leq \epsilon$ for all $0<q<n$ when $n$ is sufficiently large, where $\Sigma_q=\E[\phi_q(X) \{\phi_q(X)\}^T]$ is the covariance matrix and $\hat{\Sigma}_q$ is its empirical estimate.
        % \end{assumption}

        % \begin{remark}[Discussion on technical assumptions]
        %     In \Autoref{asmp:pr}, the first condition says that utility loss is not decreasing too fast, and the variance estimator is reasonably close to the truth; the second condition bounds $\fc$, the function class under consideration; and the last condition is typically met when the input $\X$ is bounded or follows a sub-Gaussian distribution. 
        % \end{remark}
        
        % \begin{theorem}[Polynomial regression]\label{thm:linear_reg}
        %     In the aforementioned scenario, the following results hold under \Autoref{asmp:pr}.
                
        %         (i) For $\dno$, we have $b_n(\dno) \lesssim e^{-nc}$ for some positive constant $c$.
                
        %         (ii) For $\drn$, we have $b_n(\drn) \asymp p\sigma_n^2/n$ under mild assumptions, as detailed in the appendix.
                
        %         (iii) For $\dun$, we have $b_n(\dun) \asymp \sigma_n^2$. 
        % \end{theorem}

 
        In this scenario, we propose a defense mechanism named Order Disguise ($\dour$). It can mislead the attacker to overfit the underlying model, significantly amplifying the privacy level compared to utility loss. $\dour$ involves constructing two perturbation directions $\e_1$ and $\e_2$ determined by the queries. While $\e_2$ traps the attacker into overfitting, $\e_1$ ensures that the fitted model performs poorly. The final perturbation $\e$ is a combination of $\e_1$ and $\e_2$, preserving the advantages of both.
        We formalize this insight in \Autoref{thm:pos} and summarize the detailed steps of the proposed defense in \Autoref{alg:poly}. 

        \begin{algorithm}[tb]
        \caption{Defense Mechanism ``Order Disguise'' ($\dour$) Against Polynomial Regression}\label{alg:poly}
        \begin{algorithmic}[1]
            \Require Defender's model $\fd$, queries $X_i, i=1, \ldots ,n$, budget of utility loss $\Ul_n$, polynomial order to mislead attacker (target order) $k \in [p, q_n)$
            \State $\Phi_k = (\phi_k(X_1), \ldots, \phi_k(X_n))^\T \in \Real^{n\times(k+1)}$, $\u=(0,\ldots,0,1)^\T \in \Real^{k+1}$
            \State $\e_1 = \Phi_k \u$, $\e_2 = \Phi_k (\Phi_k^\T \Phi_k)^{-1} \u$ 
            \State $\tilde{\e_i} = \e_i/\norm{\e_i}_2, i=1,2$
            \State $\e = \tilde{\e_1} + \sign(\e_1^\T\e_2) \tilde{\e_2}$ \Comment{$\sign(x)=1$ if $x>0$, otherwise $-1$}
            \State $\e = \sqrt{n \Ul_n} \e / \norm{\e}_2$ \Comment{Normalize to the desired utility loss level}
            \Ensure $\Y_i = \fd(X_i) + e_i, i=1, \ldots ,n$
        \end{algorithmic}
        \end{algorithm}    
        
        \begin{theorem}\label{thm:pos}
        Suppose that (i) $n \cdot \P(\abs{X} \geq [\E (X^{2q_n}) / q_n^\gamma ]^{1/(2q_n)}) \to 0$ as $n\to\infty$ for a constant $\gamma > 0$, and (ii) $\Ul_n \geq 2 B q_n \lambda_n  /n$. 
        Under \Autoref{asmp:same_query}, when the defender adopts Order Disguise ($\dour$), there exists an increasing sequence $\{k_n, n=1,2,\dots \}$ with $k_n \in [p, q_n)$ such that the attacker will fit a function $\hat{f}_n$ with the highest order as $k_n$, and $\pl_n(\dour,\fc_{\preg}\mid\qiid,\lac_{\preg}) \gtrsim k_n^\gamma \Ul_n.$ 
        As a specific case, when $X$ follows a Gaussian distribution and $q_n\gtrsim \ln n$, we can take $\gamma=2$ and $k_n=4\ln n$.
        Another example is when $X$ follows a Beta distribution $Beta(\alpha,\beta)$ with $\beta>1$ and $q_n\gtrsim n^{1/\beta}$, we can choose any $\gamma > 0$ and $k_n=n^{1/\delta}$ for any $\delta \in (1, \beta)$.  
        \end{theorem}
        % \begin{remark}[High probability]
        % \Autoref{thm:pos} imposes a mild requirement that the tail of $X$ should decay fast enough. It is not hard to verify many common distributions such as Gaussian, Exponential, and Student $t$-distributions satisfy that condition with any fixed $\gamma > 0$. 

        \Autoref{thm:pos} implies that by using the proposed Order Disguise defense mechanism, the defender can achieve a high privacy level while maintaining a small utility loss. The gain-to-loss ratio $\pl_n/\Ul_n$ can even approach infinity. 
        % The key reason for the high privacy is that $\dour$ introduces \textbf{query-specific perturbations} targeting attackers using polynomial regression. 
        In contrast, without defense, $\pl_n(\dno)$ is zero for sufficiently large $n$, as shown in \Autoref{ex:lin}. This implies that $\pl_n(\dun)\asymp \Ul_n$ under the Constant Noising defense because the attacker will learn the underlying function plus the injected constant as a whole. Also, the classical linear regression theory indicates that the privacy level under IID Noising is typically at the order of $U_n/n$. 
        While IID Noising and Constant Noising inject query-independent perturbations, Order Disguise utilizes the knowledge that the attackers use polynomial regression and carefully designs \textbf{query-specific perturbations} targeting such attackers, thus achieving a higher privacy level.
        % Therefore, with a similar analysis to the $k$-NN scenario, we know that (1) defense is necessary for the defender; (2) constant noising $\dun$ is a \eco defense but has a lower privacy level than $\dour$, hence not rate optimal.

        % \begin{remark}[Attacker that is Easier to Defend Against]
        %     \Autoref{thm:pos} together with \Autoref{thm:knn} suggest that an attacker using polynomial regression is easier to defend against than that using $k$-NN. \Autoref{thm:knn}(iv) implies that it is impossible to achieve a significantly higher privacy level than utility loss when the attacker uses $k$-NN. In contrast, with polynomial regression, the attacker can be misled to severely overfit the model, resulting in a large privacy level.
        %     We conjecture that an attacker who is more manipulable, meaning their learned model is more sensitive to the training data, is easier to defend against. We further elaborate on this idea in \Autoref{subsec:manipulate}.
        % \end{remark}
        
        \begin{remark}[Discussion on Technical Conditions]
        The first condition in \Autoref{thm:pos} ensures that there exists a direction where the overfitted model will have a large prediction error. It holds when the tail of $X$ decays sufficiently fast and $\lim_{n\to\infty} q_n \to \infty$. For example, the tail of Beta distribution $Beta(\alpha,\beta)$ is dominated by the second parameter $\beta$, and a larger $\beta$ indicates a faster speed of decay. Thus, we can guarantee the first condition when $\beta > 1$ and $q_n\gtrsim n^{1/\beta}$.
        The second condition on $\Ul_n$ ensures that the perturbation is large enough to cause the attacker to overfit, which is reasonably mild since $\lim_{n\to\infty} q_n\lambda_n/n \to 0$ is common in practice. 
        \end{remark}
        % The first condition in \Autoref{thm:pos} ensures that there exists a direction where the overfitted model will have a large prediction error, while the second condition on $\ul_n$ ensures that the perturbation is sufficiently large so that the attacker will overfit. The first condition holds when the tail of $X$ decays sufficiently fast and $\lim_{n\to\infty} q_n \to \infty$, and the second condition is mild in practice since it is common to have $\lim_{n\to\infty} q_n/n \to 0$. 
        % The second condition can be relaxed if the attacker estimates the variance with $ \hat{\sigma}^2_{n,q} =\sum_{i=1}^n \{\Y_i-f_q(X_i;\bbeta_q)\}^2/(n-q).$
        % In this case, the higher-order fitted polynomial is not penalized for its model complexity, and thus the attacker is guaranteed to overfit. 
        % Additionally, with the above variance estimator, $\AIC$ is asymptotically equivalent to the Akaike information criterion when $\lambda_n=2$ and asymptotically equivalent to the Bayesian information criterion when $\lambda_n = \ln n$~\citep{shao1997asymptotic,ding2018model}. 
        


        
         


        