\def\attstr{\textrm{AttStr}}
\def\atteff{\textrm{AttEff}}
\def\relatteff{\textrm{RelAttEff}}
\def\defstr{\textrm{DefStr}}
\def\defeff{\textrm{DefEff}}
\def\reldefeff{\textrm{RelDefEff}}
\def\utilloss{\textrm{UtilLoss}}
\def\eco{potent\xspace}


    In \Autoref{subsec:overview}, we present the background of model stealing attacks, highlighting the motivation and essential ingredients of the proposed framework. 
    In \Autoref{subsec:quantify}, we propose methods to quantify the effectiveness of attack and defense strategies with economic interpretations.
    In \Autoref{subsec:game}, we characterize the objectives and actions of both the attacker and defender, providing a foundation for theoretically analyzing the unique challenges in model privacy.


    \subsection{Background and Essential Ingredients of Model Privacy} \label{subsec:overview}
    We begin with a classical machine learning setting. Suppose a model owner has created a function $\fd$ from a large proprietary dataset to predict responses based on input $\X$. The model owner provides query-based services by responding with $\Y_i$ for any input query $\X_i$. 
    
    Meanwhile, a malicious user, or attacker, aims to build a model to either provide similar services and compete with the model owner or to discontinue paying for the service. Instead of collecting raw training data and training a model from scratch, the attacker exploits the model owner's service to significantly reduce training costs. In particular, the attacker performs a model stealing attack to reconstruct a function $\hat{f}_n$ that closely approximates $\fd$ using $n$ query-response pairs $Z_n\defeq \{(\X_i, \Y_i),i=1,\dots,n\}$. There are three essential ingredients for the attacker.
    \begin{enumerate}
        \item \textit{ Evaluation criterion} $\ell$. To evaluate the goodness of an attack, the attacker has to assess the closeness between the target function $\fd$ and the reconstructed function $\hat{f}_n$. This is measured via $\ell_A(\hat{f}_n, \fd)$, where $\ell_A$ is a loss function chosen according to the attacker's interest. For example, if the attacker wants $\hat{f}_n$ to be uniformly close to $\fd$, $\ell_A(f, g)$ could be the supremum norm $\sup_x \abs{f(x)-g(x)}$ for two functions $f$ and $g$. Alternatively, if the focus is on performance in a specific region $S$, $\ell_A(f, g)$ can be defined as $\int_{x \in S} \abs{f(x) - g(x)} dx$.
        
        \item \textit{ Attack strategy} $(\qs, \la)$. An attack strategy consists of a \textit{query strategy} $\qs$ and a \textit{learning algorithm} $\la$. The query strategy $\qs$ determines how the attacker selects $\X_i$'s. There are two types of query strategies: ``batch query'', where all $n$ queries are sent to the model owner at once, and ``sequential query'', where queries are sent one by one or in parts.
        Examples of batch query strategies include fixed designs (e.g., equally-spaced sampling over an interval) and random designs (e.g., IID sampling from a given distribution). Sequential query strategies may use previous responses to inform subsequent $\X_i$ choices. After collecting the query-response pairs $Z_n$, the attacker applies a learning algorithm $\la$ to construct $\hat{f}_n$. Common learning algorithms include linear regression, $k$-nearest neighbors, ensemble forests, and neural networks. 

        \item \textit{ Side information} $\si$.  
        Attacker's side information $\si_A$ refers to any additional information available to the attacker beyond the query-response pairs. This can include properties of the defender's model, such as knowing that $\fd$ is non-negative or is a neural network with a particular architecture, or additional data collected from other sources. Side information can enhance the attacker's learning capability.
    \end{enumerate}

    Overall, the attacker aims to choose a favorable attack strategy that allows them to accurately recover $\fd$ using a minimal number of queries.

    Given this threat, the model owner aims to prevent the attacker from easily stealing the model. As such, we name this problem as enhancing \textit{model privacy} and call the model owner the defender. There are also three key components for the defender.
    \begin{enumerate}
        \item \textit{ Evaluation criterion} $\ell$. The defender seeks to maximize the dissimilarity between $\hat{f}_n$ and $\fd$, evaluated through $\ell_D(\hat{f}_n, \fd)$. It should be noted that the defender's loss function $\ell_D$ may differ from the attacker's $\ell_A$. For example, an attacker who aims to steal a language model for a given downstream task may employ a loss function $\ell_A$ evaluated in the task-specific region $S$. Meanwhile, the defender might aim for a comprehensive defense, therefore using a different $\ell_D$ that evaluates the entire input space.

        \item \textit{ Defense mechanism} $\dm$. 
        The defender can enhance model privacy by adding perturbations to the responses, $\Y_i=\fd(\X_i)+e_i$, where $e_i$ is a perturbation determined by a defense mechanism $\dm$. Larger perturbations can increase the deviation between $\hat{f}_n$ and $\fd$. However, they also reduce the quality of service for benign users, which we refer to as \textit{utility loss}. Every defense mechanism must navigate this trade-off between privacy and utility, which is referred to as the privacy-utility trade-off.

        \item \textit{ Side information} $\si$. The defender's side information $\si_D$ refers to any knowledge about the attacker beyond the received queries. Examples include the attacker's potential query strategies and learning algorithms. This information helps the defender deploy more targeted defense mechanisms, thereby bolstering model protection.
    \end{enumerate}

    In summary, model privacy involves a competition between the attacker and defender, who engage in a non-cooperative and asymmetric interaction.


    \subsection{Goodness Quantification in Model Privacy: An Economic Perspective}\label{subsec:quantify}
        Among key ingredients identified in \Autoref{subsec:overview}, the attack strategy and defense mechanism are of most importance. This subsection provides an economic perspective on their effectiveness, offering simple and intuitive interpretations. 
        Generally speaking, for the attacker,
        % \textbf{Economic Quantification Overview:} 
        an attack strategy is considered \textit{\eco} if they can significantly reduce their cost to build a model by abusing the defender's service, compared to collecting data and training a model from scratch themselves. We illustrate this with the following example. 
        
        \begin{example}[Steal a Linear Function]\label{ex:lin}
            Suppose the defender has a linear function $\fd$ and the attacker tries to rebuild it by fitting a linear model. Without any defense mechanism, the attacker can precisely reconstruct $\fd$ with just a few query-response pairs by solving a linear equation. 
            Without querying the defender, the attacker can also collect data from nature and then build a model. The needed sample size to obtain a model $\hat{f}_n$ that is close to $\fd$, however, grows quickly as the distance between $\hat{f}_n$ and $\fd$ decreases. 
            Thus, model stealing is economically advantageous for the attacker.
        \end{example}

        We next elaborate on this idea. Let $\dnt$ denote a defense mechanism that adds perturbations such that $(\X_i,\Y_i)$'s have the same distribution as the observations collected from the nature. Let $\hat{f}_n \defeq \la(Z_n)$ denote the atacker's reconstructed function given query-response pairs $Z_n=\{(\X_i,\Y_i), i=1,\dots,n\}$ and a learning algorithm $\la$.

        % \begin{definition}[Reconstructed Function]\label{def:fhat}
        %     Given query-response pairs $Z_n=\{(\X_i,\Y_i), i=1,\dots,n\}$,  the attacker rebuilds the function $\hat{f}_n \defeq \la(Z_n)$ using a learning algorithm $\la$.
        % \end{definition}

        \begin{definition}[Necessary Stealing Sample Size] \label{def:steal_sample_size}
        The sample size needed to rebuild a function $\hat{f}_n$ that is $\epsilon$-close to $\fd$ under loss $\ell$ is defined as $n(\epsilon,\ell;\fd,\dm,\qs,\la) \defeq \inf\{n:  \E \{\ell(\hat{f}_n, \fd) \}\leq \epsilon\}$. Here, the expectation is taken over every randomness, including both the attack strategy $(\qs, \la)$ and defense mechanism $\dm$.
        \end{definition}
        
        
    \begin{definition}[Attack Strength]\label{def:atkstr}
            We define the attack strength for an attack strategy $(\qs, \la)$ with respect to the underlying function $\fd$ and defense $\dm$ as 
            \begin{align*}
            \attstr(\qs,\la \mid \fd,\dm,\ell_A) \defeq 
            \liminf_{\epsilon \to 0^+} \frac{n(\epsilon,\ell_A;\fd,\dnt,\qs,\la) \times c_{\textrm{Nature}} }{ n(\epsilon,\ell_A;\fd,\dm,\qs,\la) \times c_{\textrm{Steal}} },
            \end{align*}
            where $c_{\textrm{Nature}}, c_{\textrm{Steal}}$ are the costs of collecting a single observation from nature and the defender, respectively. An attack strategy is called \eco if $\attstr=\infty$.
    \end{definition} 

    % The interpretation of attack strength is straightforward. 
    From the attacker's perspective, attack strength is the ratio of the model reconstruction costs incurred by collecting data naturally versus by stealing from the defender.  
    It is more economical to steal the model when $\attstr > 1$, rather than collecting data independently.
    An attack with infinity strength indicates the cost of querying the defender is substantially lower compared to the cost of independent data collection. 

    In contrast to the attacker, the defender has to consider both the service quality for the benign users and the difficulty for the attacker to reconstruct $\fd$ well. 
    % the defense strength can be defined in an analogous way, measuring how many times cost the attacker has to pay by performing stealing attacks. However, preventing the attacker from rebuilding a similar model is not the only pursuit of the defender, because the defender also has to provide a reasonable service quality for general users.  
    In one extreme, suppose a defender deploys a defense of returning pure white noise. Clearly, the attacker cannot rebuild any useful model based on the defender's responses, but such a defense results in unacceptable service quality for benign users. 
    % In short, a good defense  large defense strength and limited degradation of response accuracy. 
    We thus define the utility loss of a defense $\dm$ to quantify the accuracy degradation from a benign user's perspective, and define its defense strength as follows.
    % a defense with a larger strength is considered better.
    % , which we call utility loss. 
    % A nature measure of utility loss is the average difference between perturbed response $\Y_i$ and noiseless response $\fd(\X_i)$, as defined below. 
    \begin{definition}[Utility Loss]\label{def:util}
            The utility loss of a defense mechanism $\dm$ at sample size~$n$ is defined as
            \begin{align*}
                \ul_n(\dm) \defeq \E \biggl\{ \frac{1}{n} \sum_{i=1}^n \loss_U(\fd(\X_i), \Y_i) \biggr\},
            \end{align*}
            where $\Y_i$ is determined by $\dm$ and queries, and $\loss_U$ is a loss function representing the benign users' interest. 
    \end{definition}       
    
    \begin{definition}[Defense Strength]\label{def:defstr}
        The defense strength of a defense mechanism $\dm$ with respect to $\fd$ and an attack $(\qs, \la)$ is  
        \begin{align*}
        \defstr(\dm\mid \fd,\qs,\la,\ell_D) 
                 \defeq   \liminf_{\epsilon \to 0^+} \frac{ n(\epsilon,\ell_D;\fd,\dm,\qs,\la) \times c_{\textrm{Steal}} }{n(\epsilon,\ell_D;\fd,\dnt,\qs,\la) \times c_{\textrm{Nature}} }.
        \end{align*}
        A defense mechanism $\dm$ with utility loss $\ul_n(\dm)$ is said to be \eco if $\defstr = \infty$.
    \end{definition}

    Utility loss can be understood as the average difference between the perturbed response $\Y_i$ and the noiseless response $\fd(\X_i)$. While utility loss seems to be associated with the sample size $n$, the defender can easily control it at any desired level, e.g., by scaling the magnitude of the injected perturbations. We therefore propose to only compare defense mechanisms with the same level of utility loss, and a defense with a larger strength is considered better.
    

    % Recall that a good defense mechanism $\dm$ is expected to have a large defense strength and a small utility loss. We therefore propose to only compare defense mechanisms with the same level of utility loss, and a defense with a larger strength is considered better.

    
    % It is worth mentioning that $\attstr$ and $\defstr$ are not necessary reciprocals, particularly when evaluation metrics $\ell_A$ and $\ell_D$ are distinct. 

    % Analogously, from the defender's perspective, the attacker has to spend $\defstr$ times more cost by sending queries. However, defense strength alone is inadequate for evaluating the goodness of a defense. 
    % To see it, a defense returning white noise is always \eco since it contains no useful information, but such a defense results in unacceptable service quality for benign users. Therefore, the defender aims for a defense with large defense strength and limited performance degradation, which we call utility loss. 
    % A nature measure of utility loss is the average difference between perturbed response $\Y_i$ and noiseless response $\fd(\X_i)$, as defined below. 

    
    % A good defense mechanism $\dm$ is therefore expected to have a small utility loss and a large defense strength. 
    
    
    % Lastly, we provide a concrete example of \eco attacks and defenses. 

    % \begin{example}
    %         \begin{enumerate}
    %             \item \ Linear regression with IID queries is a \eco attack strategy when there is no defense. Nevertheless, it may not be \eco if the defender adopts a defense mechanism injecting correlated noise into the response, as will be seen later in \Autoref{sec:atk}.
    %             \item \ A defense mechanism injecting a constant bias into the responses is \eco, as will be seen later in \Autoref{subsec:}.
    %         \end{enumerate}
    % \end{example}
    



    \subsection{Threat Model and Objectives of Model Privacy}\label{subsec:game}
        Building upon the essential elements and goodness quantification approach presented in previous subsections, this subsection formally states the threat model and the objectives of both the attacker and defender, completing our model privacy framework.

        \textbf{Threat Model.}
        We recall the attacker-defender interaction demonstrated in \Autoref{fig:MLaaS}. The attacker sends $n$ queries $\X_1, \ldots, \X_n$ to the defender according to a query strategy $\qs$, possibly in a sequential manner. Upon receiving the queries, the defender determines the perturbations $e_i=\dm(\X_1, \ldots, \X_i), i=1,\dots,n$ for sequential queries or $(e_1, \dots, e_n)=\dm(\X_1, \ldots, \X_n)$ for batch queries, then returns responses $\Y_i = \fd(\X_i)+e_i$ to the attacker. Here, $\dm$ is a defense mechanism, and $\fd: \mathcal{X}\to \mathcal{Y}$ is the defender's trained function that needs protection. For convenience, we assume $\mathcal{X} \in \Real^d$ and $\mathcal{Y} \in \Real$ throughout this paper, though the proposed definitions and concepts can be extended to general input and response spaces.
        Given query-response pairs $Z_n = \{(\X_i, \Y_i), i=1,\ldots,n\}$, the attacker uses a learning algorithm $\la$ to reconstruct a model $\hat{f}_n=\la(Z_n)$. Specifically, $\la$ is a measurable mapping from $Z_n$ to a measurable function of $X$. 

        \begin{remark}
        The query strategy $\qs$ is actually sample-size specific, meaning $\qs$ is a collection of finite-dimensional distributions $\{\qs_n, n=1,2,\dots\}$.  
        For batch queries, $\qs_n$ is the joint distribution of $\X_1, \dots, \X_n$. For sequential queries, $\qs_n$ can be the conditional distribution of $\X_n$ given $\X_1,\dots, \X_{n-1}$ and the responses $\Y_1,\dots,\Y_{n-1}$. 
        This creates a triangular array of random variables $\{\X_i^{(k)}, i=1,\dots, k, \ k=1,2,\dots \}$, and $\X_i$ is the abbreviation of $\X_i^{(n)}$. 
        Consequently, many quantities in model privacy can depend on $n$ and should be defined as a sequence or triangular array, such as defense mechanisms and responses. The rest of this paper mainly considers batch query strategies unless otherwise specified, thereby eliminating ambiguity related to sample size dependence.
        \end{remark}

        % \begin{table}
        %     \centering
        %     \begin{tabular}{p{0.5in}p{0.5in}p{1.1in}p{3.2in}}
        %     \toprule
        %     Type & Notion & Name & Meaning \\ 
        %     \midrule
        %      \multirow{2}{*}{Query} & $\qiid$ & IID batch query & Draws an IID sample from a given distribution \\
        %       & $\qs_{\text{ES}}$   & Equally-spaced batch query  & Draws equally-spaced points in a given area \\ \hline
        %     \multirow{3}{*}{Defense}  & $\dno$ & No defense & $e_i=0$ for $i=1,\dots,n$ \\ 
        %       &$\drn$ & IID noising & $e_i$'s are IID drawn from a distribution with zero mean \\ 
        %       &$\dun$ & Constant noising & $e_i$ is a fixed constant for $i=1,\dots,n$ \\ 
        %       &$\dnt$ & Nature noising & $e_i$'s are drawn from the natural data-generating distribution \\ 
        %         \bottomrule
        %     \end{tabular}
        %     \caption{Summary of common attack and defense strategies.}
        %     \label{tab:common_strategy}
        % \end{table}

    \textbf{Objectives.}
        Both the attacker and defender aim to make the most advantageous decisions to maximize their benefits. We denote $P_A(\qs, \la;\fd, \dm)$ as the general notion of the attacker's benefit when adopting an attack strategy $(\qs, \la)$ and facing the defender's model $\fd$ and defense mechanism $\dm$. Similarly, the defender's benefit is denoted as $P_D(\fd, \dm;\qs, \la)$. Using the cost-based goodness quantification from \Autoref{subsec:quantify} for example, the attacker's benefit can be defined as $P_A(\qs, \la;\fd, \dm) \defeq \attstr(\qs,\la\mid \fd,\dm,\ell_A)$, and the defender's benefit can take the form of $P_D(\fd, \dm;\qs, \la) \defeq \defstr(\dm \mid \fd,\qs,\la,\ell_D)\times \ind_{\ul_n(\dm) \leq U_n}$, if the defender guarantees that the utility loss level is no greater than a pre-specified constant $U_n$ for every $n$.
        
        With respect to a given $\fd$ and $\dm$, an attack strategy $(\qs^*, \la^*)$ is called the best among a collection of attacks $(\qsc, \lac)$ if
        \begin{align*}
            \qs^*, \la^* \in \argmax_{\qs \in \qsc, \la \in \lac} P_A(\qs, \la;\fd, \dm). 
            \numberthis \label{eq:best_atk}
        \end{align*}
        Analogously, with respect to an attack strategy $(\qs, \la)$, a defense mechanism $\dm^*$ is the best among a collection of defenses $\dmc$ if
        \begin{align*}
            \dm^{*} \in \argmax_{\dm \in \dmc}  P_D(\fd, \dm;\qs, \la). 
            \numberthis \label{eq:best_def}
        \end{align*}

    In summary, the threat model and objectives outlined in this subsection establish a clear framework for understanding the interaction between attackers and defenders in model privacy. By defining the key elements, such as attack strategies, defense mechanisms, and their respective benefits, we can systematically analyze and develop strategies to enhance model privacy. This framework serves as a foundation for the subsequent sections, where we delve deeper into specific strategies and their theoretical underpinnings, aiming to protect ML models from being compromised by adversarial attacks.
        



