
    Machine learning (ML) has achieved remarkable success in many applications such as content recommendation on social media platforms, medical image diagnosis, and autonomous driving. However, there is a growing concern regarding the potential safety hazards coming with ML. One particularly critical threat to ML applications is model stealing attacks~\citep{tramer2016stealing, oliynyk2023know}, where adversaries attempt to recover the learned model itself through limited query-response interactions. ML models are often highly valuable due to their expensive creation process and proprietary techniques. If a third party can reconstruct the model, it may significantly harm the model owner's interests, leading to financial losses and intellectual property breaches. Model stealing attacks target at trained models. This differs from attacks aimed at compromising datasets, which is often studied under the framework of differential privacy or its variants~\citep[see, e.g.,][]{evfimievski2003limiting, dwork2006calibrating, dong2022gaussian}.

    We exemplify a model stealing attack in \Autoref{fig:MLaaS}, which targets Machine-learning-as-a-service and an image classification task. In this scenario, the model owner trains a classification model and aims to monetize it by deploying it on a cloud server and providing paid service. Users can send queries, which in this case are images, to the server and receive model-generated labels as responses after paying for the service. However, model stealing attacks pose a serious threat to this monetization strategy, since a malicious user may easily rebuild a model with comparable accuracy to the original model based on a few query-response pairs. For instance, \citet{tramer2016stealing} reported that 100 queries are sufficient for a successful model stealing attack on a classification model trained on the handwritten digits dataset, costing less than one dollar. 
    
    Empirical studies have shown that model stealing attacks can be very effective across a wide range of applications, such as classification tasks with logistic regression, random forests, and deep neural networks~\citep{tramer2016stealing,papernot2016practical, orekondy2019knockoff}. Our experiments further demonstrate that for large language model-based classifiers, such as those based on GPT~\citep{radford2018improving}, BERT~\citep{devlin2018bert}, and XLNet~\citep{yang2019xlnet}, one could rebuild a model with similar predictive accuracy by querying only 5\% of the original model's training sample size, if there is no defense. The consequences of such attacks are significant: the attacker can stop paying for the service, potentially leading to financial losses for the model owner, or even compete with the original model owner using the stolen model. These risks highlight the urgent need for effective defense mechanisms to protect the integrity and value of ML models.

    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/FigIntro.pdf}
        \caption{An illustration of a model stealing attack in the machine-learning-as-a-service scenario. Given a trained image classification model deployed on the cloud server, a user can send queries to and receive responses from the server. A malicious user, also called an attacker, aims to reconstruct this model based on query-response pairs.}
        \label{fig:MLaaS}
    \end{figure}

    
    A large body of literature has been developed for both model stealing attacks and defenses~\citep[see, e.g.,][]{tramer2016stealing,papernot2016practical,chandrasekaran2018model,milli2019model,kesarwani2018model,juuti2019prada,lee2019defending,orekondy2019prediction,wang2020information,oliynyk2023know}. However, existing studies are often heuristic and based on practical experience, lacking a standard criterion for evaluating the effectiveness of attack and defense methods. This makes it challenging to compare different approaches. Moreover, the principles guiding which defense a service provider should adopt remain unclear. Establishing a theoretical foundation for this subject is therefore crucial, and this paper aims to address this gap.

    We present a comprehensive framework named \textit{model privacy}, serving as a foundation for analyzing model stealing attacks and defenses. Our contributions are three-fold:

    \textbf{1. Conceptual Formalization:} We establish a clear conceptual foundation for model privacy. By formalizing the actions and objectives of both attackers and defenders in query-response-based interactions, we identify fundamental factors that influence their decision-making processes. We also propose methods for quantifying the effectiveness of model stealing attacks and defenses. To our knowledge, this is the first work that formally defines model stealing attacks and defenses. 

    \textbf{2. Theoretical Analysis:} We address fundamental questions in model privacy, especially focusing on the defender's perspective. Our analysis examines the necessity of defense mechanism and identifies optimal defenses against attackers with known or unknown attack strategies. Specifically, we evaluate the privacy-utility tradeoffs of multiple defense strategies against four representative learning algorithms ($k$-nearest neighbors, polynomial regression, kernel ridge regression, and neural networks) and assess their worst-case privacy guarantees against unknown attackers. Additionally, we investigate scenarios where attacks are easier to defend against and the impact of side information on defense strategies.

    \textbf{3. Practical Implications:} Our theoretic developments provide valuable insights and guidance for designing more effective model defenses. We demonstrate that crafting attack-specific, query-dependent perturbations is critical for effective defenses. We propose new defense mechanisms with theoretical guarantees, tailored for scenarios where attackers use polynomial regression, kernel regression, and neural networks to steal models. Based on our theoretical findings, we also develop a novel defense mechanism for classification tasks. Experimental results on both simulated and real-world datasets show that our proposed defenses significantly enhance model privacy, requiring attackers to send many more queries to reconstruct a comparable model, or even making it impossible to do so.
    
    % \begin{enumerate}[label=\textbf{\arabic*.\ }]
    %     \item 
    %     \item 
    %     \item 
    % \end{enumerate}

   
    The rest of this paper is organized as follows. \Autoref{sec:motivation} formulates the model privacy framework, including threat model, goodness quantification, and objectives. It also provides insights into the proposed concepts. \Autoref{sec:formulation} proposes practical measures for evaluating the goodness of attacks and defenses, and establishes the connection with the rebuilt model's statistical risk. \Autoref{subsec:notation} introduces the setup and notations used throughout the paper. In \Autoref{sec:model_func}, we apply the model privacy framework to understand the defender's optimal decisions in various scenarios where the attacker's learning algorithm is known. \Autoref{sec:atk} focuses on defense analysis when the attacker's learning algorithm is unknown to the defender. Numerical experiments on both simulated and real-world datasets are presented in \Autoref{sec:exp} to corroborate our findings. We conclude the paper with further discussions in \Autoref{sec:con}. The supplementary document includes proofs and additional theoretical and experimental results.
    % All proofs and experimental codes are included in the supplement.

    % Furthermore, our developed model privacy framework is closely connected with various other domains. For example, model compression shares a similar goal of reconstructing the original model efficiently. Traditional active learning has a similar goal as the attacker in model privacy, both seeking to learn an accurate model with minimal training data. Understanding model privacy also enhances the safety of ML models against other model attacks, including inversion, evasion, and poisoning attacks. For a more comprehensive exploration of these connections and additional insights into related topics, including transfer learning and data privacy, we refer the readers to \Autoref{sec:dis}.