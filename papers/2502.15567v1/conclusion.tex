This paper establishes a framework called model privacy to understand and mitigate the vulnerabilities of machine learning models against model stealing attacks. 
Driven by practical needs, we propose evaluation metrics to assess an attacker's cost of stealing a model. An attack is considered potent if the attacker can reverse-engineer the defender's model at a lower cost than gathering data and training a comparable model independently. 

Within this framework, we have derived both theoretical and empirical results, yielding valuable insights into effective defense strategies. Our findings highlight three key points. 
Firstly, defense is generally necessary for protecting a model from being easily stolen. Without any defense, attackers can successfully reconstruct models across diverse architectures and attack scenarios. For instance, merely 1000 queries are sufficient to steal a large language model-based hate speech detector with 85\% accuracy, while the original model required training on over 100,000 data points (see Subsection~\ref{subsec:nlp}). 
Secondly, defenses that add IID perturbations are typically ineffective. The level of service degradation for benign users often outweighs any improvement in the model's privacy level. Thirdly, the strategic disruption of the independence in perturbations is vital for enhancing model privacy. To demonstrate this, we have introduced novel theory-inspired defenses for both regression and classification tasks that significantly impair an attacker's ability to steal models. 
% Our research also reveals how side information could affect model privacy.
In conclusion, the model privacy framework offers a unified perspective for assessing the effectiveness of various attack and defense strategies.


Looking ahead, several promising avenues exist for extending the model privacy framework. 
Firstly, a model engaging in the query-response-based interaction also risks revealing its parameters, hyper-parameters \citep{tramer2016stealing, wang2018stealing}, and even exact values of the training data \citep{fredrikson2014privacy, fredrikson2015model}. Recognizing this vulnerability, an extension of model privacy is to modify the evaluation criteria to include the protection of any model-related quantities. %such as values at specific points, derivatives, parameters, hyperparameters, architecture, and the original dataset.
Secondly, instead of stealing the defender's model $\fd$, an attacker may be interested in a more ambitious goal of stealing the function $\ft$ underlying the training data based on which $\fd$ was obtained, such as demonstrated in our hate speech detection experiment. The model privacy framework can accommodate this scenario by assessing the differences between the reconstructed function $\hat{f}_n$ and $\ft$, instead of $\fd$.
Thirdly, it is natural to integrate model privacy with game theory when the defender and attacker are in an interactive setting. In such cases, both parties must make decisions dynamically and possibly in a sequential manner, adding complexity and depth to the strategic considerations within the model privacy framework.


% However, if there is a significant divergence between $\fd$ and $\ft$, stealing from the defender may not be a wise decision for the attacker. Without side information, the attacker has no clue how good does $\fd$ approximate $\ft$. Thus, an accurate reconstruction of $\fd$ only leads to a highly biased function. Nevertheless, when there is additional information about the defender or underlying distribution, the attacker may still leverage the part of relatively useful and accurate responses from the defender to facilitate their own learning tasks. One such example is \Autoref{ex:extra_point_knn}. A defender with constant noising can be considered to have a biased function $\fd$ to $\ft$. We denote $\fd=\ft+B$, where $B$ is a constant bias term. While \Autoref{thm:knn} implies that an attacker using $k$-NN learning algorithm will reconstruct a function that contains this bias $B$, \Autoref{ex:extra_point_knn} shows that the attacker can efficiently debias the responses from the defender when additional nature-generated data is available.
