
In this section, we study the defense against an attacker whose learning algorithm is unknown to the defender. Specifically, the attacker's learning algorithm collection $\lac$ includes any measurable mappings from query-response pairs to a measurable function of $\X$. Our results address the minimum guaranteed privacy level provided by three defense mechanisms.
As a demonstrative application of the developed theory, we make the following assumption in this section, and all proofs are included in Appendix.

\begin{assumption}[H\"{o}lder function class] \label{asmp:func}
    The defender's function $\fd$ belongs to a H\"{o}lder class $\fc \defeq \hld(C,\alpha)$ with $C>0$ and $\alpha>0$.
    % , and $\norm{\fd}_{\infty} < B$ for some constant $B>0$. 
\end{assumption}


\subsection{IID Noising Defense}
    We first study the IID Noising defense $\drn$, where the defender adds IID Gaussian noise with a common variance $\sigma_n^2$
    % \todo{may extend to other distribution than Gaussian} 
    to the responses. Its privacy level against an arbitrary attacker is derived as follows.

     \begin{theorem} % [H\"{o}lder class, IID noise]
     \label{ex:holderIID}
        Under \Autoref{asmp:same_query, asmp:bounded_input, samp:density, asmp:func},
        we have
        \begin{align*}
            \pl_n(\drn,\fc\mid\qiid,\lac) \begin{cases} 
            \asymp  (\sigma_n^2/n)^{2\alpha/(2\alpha+d)}, & \sigma_n^2 \gtrsim n^{-2\alpha/d} \\ 
            \lesssim n^{-2\alpha/d} , & \sigma_n^2 \lesssim n^{-2\alpha/d}.
            \end{cases}
        \end{align*}
    \end{theorem}

    Notably, \Autoref{ex:holderIID} \textit{allows the noise variance $\sigma_n^2$ to vanish}. When the noise variance is fixed, the privacy level can be analyzed using classical minimax theory. However, the case where $\sigma^2_n \to 0$ as $n \to \infty$ has received little attention in the existing literature. Gaining an understanding of this case is crucial for model privacy, as a defense with vanishing noise can still be \eco, meaning that the attacker cannot steal a good model while the benign users' service quality is well maintained. Technically, to handle the case with vanishing noise, we need to derive the exact convergence rates of prediction error brought by randomness in queries and perturbations, respectively. 

    The privacy level derived in \Autoref{ex:holderIID} also yields the limit of the attacker's stealing error since we assumed that the attacker and the defender use the same loss, as discussed in \Autoref{sec:formulation}. For instance, under the IID Noising defense with $\sigma_n^2 \gtrsim n^{-2\alpha/d}$, an attacker uses $k$-NN can achieve the smallest stealing error at the order of $(\sigma_n^2/n)^{2\alpha/(2\alpha+d)}$. 

    % \begin{remark}[Vanishing noise]
    %     When the noise variance, $\sigma_n^2$, is fixed at $\sigma^2 > 0$, the privacy level $\pl_n$ can be analyzed using classic minimax theory. 
    %     In contrast, the case where $\sigma^2_n \to 0$ as $n \to \infty$ has received little attention in the existing literature. Gaining an understanding of this situation is crucial for model privacy, as a defense with diminishing noise can still remain \eco. As a result, the attacker cannot steal a good model while the benign users' service quality is largely preserved.
    % \end{remark}

    
   

    % \begin{remark}[General stealing limit with IID noise]
    %     We derive the following bounds on the stealing limit for a general function class $\fc$. Denote $\epsilon$-packing entropy as $S(\epsilon)$ and $\epsilon$-covering entropy as $V(\epsilon)$ \citep[see, e.g.][]{yang1999information}. 
    %     \begin{assumption}\label{asmp:atk_iid}
    %         1. $\fc$ is rich, that is, $\liminf_{\epsilon \to 0^+}S(\alpha\epsilon)/S(\epsilon) > 1$ for some $\alpha \in (0,1)$. 2. There exist positive constants $a$, $b$, and $c$ such that $S(\epsilon) \leq V(b\epsilon) \leq cS(\alpha\epsilon)$ when $\epsilon$ is sufficiently small. 
    %         3. The noise variance $\sigma_n^2 \lesssim 1$.
    %     \end{assumption}
    %         \begin{theorem}[General function class, IID noise]\label{thm:atk_iid}
    %         Let $\epsilon_n$ and $\underline{\epsilon}_n$ be such that
    %         \begin{align*}
    %             n\epsilon_n^2 = V(\epsilon_n), \quad n\underline{\epsilon}_n^2/\sigma_n^2 = V(\underline{\epsilon}_n).
    %         \end{align*}
    %         Under \Autoref{asmp:atk_iid}, we have $\underline{\epsilon}_n^2 \lesssim R_n \lesssim \epsilon_n^2$. 
    %             In particular, if $\sigma_n^2 \asymp 1$, we have $R_n \asymp \epsilon_n^2$, and the attacker can construct a learning algorithm $\la^*$ to achieve this rate.
    %         \end{theorem}
            
    %         \Autoref{thm:atk_iid} implies that $\underline{\epsilon}_n = o(\epsilon_n)$ as $\sigma_n^2 \to 0$, leading to a gap between the lower and upper bounds. 
    %         % Though we derive the exact minimax rate in \Autoref{ex:holderIID}, 
    %         It is generally unclear whether this gap can be closed, and we leave it for future work. 
    % \end{remark}
    
    

    % \begin{remark}[Query strategy matters, I]
    \Autoref{ex:holderIID} assumes that the attacker adopts an IID batch query strategy. 
    % We can prove that IID batch query is indeed optimal for the attacker. 
    The following theorem implies that in this regression task, IID batch query is the best choice for the attacker when $\sigma_n^2 \gtrsim n^{-2\alpha/d}$.
    
        \begin{theorem}[Non-IID Queries]\label{thm:IIDquery}
         Under \Autoref{asmp:same_query, asmp:bounded_input, samp:density, asmp:func}, we have $\pl_n(\drn,\fc\mid \qs,\lac) \gtrsim (\sigma_n^2/n)^{2\alpha/(2\alpha+d)}$ for every query strategy $\qs$. 
        \end{theorem}    
        % For any $p>0$, we denote the $\epsilon$-covering entropy of the function class $\mathcal{F}$ under $L_p$ loss as $V_p(\epsilon)$ \citep[see, e.g.][]{yang1999information}. 
        % \begin{assumption}\label{asmp:atk_iid}
        %     1. $\fc$ is rich, that is, $\liminf_{\epsilon \to 0^+}S(\alpha\epsilon)/S(\epsilon) > 1$ for some $\alpha \in (0,1)$. 2. There exist positive constants $a$, $b$, and $c$ such that $S(\epsilon) \leq V(b\epsilon) \leq cS(\alpha\epsilon)$ when $\epsilon$ is sufficiently small. 
        %     3. The noise variance $\sigma_n^2 \lesssim 1$.
        % \end{assumption}
        


        % Let $R_n(\qiid)$ and $R_n(\qs)$ denote the stealing limit with respect to IID batch query strategy and an arbitrary query strategy $\qs$, respectively.
        % Let $\pl_n(\drn,\fc\mid \qs,\lac)$ denote the privacy level of $\drn$ under an arbitrary query strategy $\qs$.
        % \begin{align*}
        %     R_n(\fc,\dmc,\qiid,\lac) \asymp R_n(\fc,\dmc,\qsc,\lac).
        % \end{align*}
        % Compared to \Autoref{ex:holderIID}, \Autoref{thm:IIDquery} implies that in this regression task, an attacker using IID batch query achieves the best stealing error rate when $\sigma_n^2 \gtrsim n^{-2\alpha/d}$.
        % However, the choice of query strategy has to be carefully considered for classification tasks, and IID query strategy is typically suboptimal in this case. In particular, the behavior of a classifier near the decision boundary is most critical, thus the attacker may pay more attention to those areas. \todo{Need support, using logistic example or simply cite some work?}
        
        
    % \end{remark}

    \subsection{Correlated Noising Defense}\label{subsec:corr_noise}
    Using an IID Noising defense mechanism is often insufficient to prevent model stealing attacks. To enhance protection, the defender can add noise with correlation.
    One such choice is adding stationary noise with variance $\sigma_n^2$. The correlation structure of noise is characterized by $r(\abs{i-j})\defeq\corr(e_i,e_j)$, where $e_i$ is the perturbation added to the $i$-th response. For example, $r(k)=\ind_{k=0}$ corresponds to an IID Noising defense mechanism $\drn$, $\sum_{k=0}^\infty \abs{r(k)} < \infty$ represents short-range dependent noise, and $r(k) \asymp k^{-\gamma} $ with $ 0<\gamma<1,$ is a typical example of long-range dependent noise. \Autoref{ex:hld_longrange} analyzes the privacy level of adding long-range dependent noise.

   \begin{theorem}% [H\"{o}lder class, long-range dependent noise]
    \label{ex:hld_longrange}
        Let $\dm_{\gamma}$ represent a defense mechanism that adds long-range dependent noise with $r(k)\asymp k^{-\gamma}$ for some $\gamma \in (0,1)$ and $ \ul_n(\dm_{\gamma}) = \sigma_n^2$. 
        Under \Autoref{asmp:same_query, asmp:bounded_input, samp:density, asmp:func}, we have 
        \begin{align*}
            \pl_n(\dm_{\gamma},\fc\mid \qiid,\lac)  \begin{cases} 
            \asymp (\sigma_n^2/n)^{2\alpha/(2\alpha+d)}+\sigma_n^2n^{-\gamma}, & \sigma_n^2 \gtrsim n^{-2\alpha/d} \\ 
            \lesssim n^{-2\alpha/d} , & \sigma_n^2 \lesssim n^{-2\alpha/d}.
            \end{cases}
        \end{align*}
    \end{theorem}

    \Autoref{ex:hld_longrange} demonstrates that the correlation structure of noise can be tailored to impede the attacker's ability to learn the defender's model effectively. Compared to \Autoref{ex:holderIID}, adding long-range dependent noise instead of IID noise can significantly increase privacy level. Moreover, a stronger correlation (a smaller $\gamma$) in the noise leads to a higher privacy level. 
    

    \begin{remark}[Influence of Query Strategy]
        In the context of \Autoref{ex:hld_longrange}, \citet{hall1990nonparametric, wang1996function} showed that $\pl_n(\dm_{\gamma},\fc\mid \qs,\lac) \asymp n^{-2\alpha\gamma/(2\alpha+\gamma)}$ when $\qs$ is an equally-spaced fixed design, $\sigma_n^2 \asymp 1$, and $\X$ is one-dimensional. This rate is slower compared to using an IID batch query. As a result, when the attacker uses a batch query strategy, the defender can add long-range dependent noise in the order of $X_i$'s values to hinder the attacker's ability to learn $\fd$. In contrast, if the attacker adopts a sequential query strategy that requests immediate responses, the defender can only add long-range noise in the order of queries, potentially allowing the attacker to learn $\fd$ at a faster rate of convergence. 
    \end{remark}

    % \textbf{A set of correlation structures.} Now, let us assume that the attacker only knows that $\dm$ is an element of $\dmc$, a set of defense strategies with potential correlation structures such as independent noise, or noise exhibiting short- or long-range dependence. Despite this uncertainty, we will next show that the attacker can still devise an optimal estimator that adapts to the unknown $\dm$.
    
    % \begin{theorem}[H\"{o}lder class, adaption to the defense]\label{ex:mixednoise}
    %     With the other setting the same as in \Autoref{ex:hld_longrange}, suppose the defender adopts a particular defense mechanism $\dm \in \dmc$, which includes defense mechanisms with independent, short- and long-range dependent noises. We also assume $\sigma_n^2 \asymp 1$. 
    %     % Let $R_n(\dm)$ and $R_n(\dmc)$ denote the stealing limit with respect to $\dm$ and 
    %     In this case, the attacker can construct an estimator that achieves the stealing limit as if $\dm$ is known. 
    % \end{theorem}
    % It is worth noting that the achieved convergence rate is adaptive to the unknown defense mechanism $\dm$, which might be faster than $R_n(\dmc)$. For instance, $R_n(\dm) \asymp n^{-2\alpha/(2\alpha+d)}$ for IID noise and $R_n(\dm) \asymp n^{-\min\{2\alpha/(2\alpha+d),\gamma\}}$ for long-range noise. However, for general $\dmc$, it remains unclear whether the attacker can achieve the minimax rate adaptively.
    
    % \subsection{Omnipotent Noising Defense}
    % % \textbf{Omnipotent noising.} 
    % Next, the attacker can encounter a more flexible defender, adopting defense mechanisms that we call omnipotent noising. Specifically, the defense mechanism collection $\dmc$ in this scenario admits defenses with arbitrary dependency between the perturbations $e_i$'s, as long as $n^{-1} \sum_{i=1}^n e_i^2 \leq \Ul_n$ almost surely. For technical convenience, we assume the benign user has a uniform query distribution.
    % % $\ul_n(\dm) \leq \Ul_n$ for any $\dm \in \dmc$. 
    % Then, \Autoref{ex:atk_or_hld} gives the attacker's stealing limit among all fixed designs. Here, a fixed design query strategy consists of pre-selected sets of data points $\X_1, \dots, \X_n$ for each sample size $n$. 

    % \begin{theorem}
    % % [H\"{o}lder class, omnipotent noising]
    % \label{ex:atk_or_hld}
    %     Under \Autoref{asmp:same_query, samp:density, asmp:same_loss, asmp:func, asmp:algo},
    %     suppose $\dmc=\{\dm: n^{-1}\sum_{i=1}^n e_i^2 \leq \Ul_n \text{ almost surely} \}$, and the query strategy collection $\qsc$ includes all fixed designs. 
    %     For any $\Ul_n \geq 0$, the stealing limit $R_n \asymp n^{-2\alpha/d} + \Ul_n$. Moreover, 
    %     % the equally spaced design query strategy and the interpolation learning algorithm achieve this stealing limit. In other words, 
    %     an attacker can achieve the stealing limit rate by sending equally spaced $\X_i$'s and using the $1$-nn learning algorithm.
    %     % the minimax optimal learning algorithm $\la^*$ is interpolation such that $q(\X_i) = \Y_i$, and the stealing limit $R_n \asymp n^{-2\alpha/d} + \sigma^2_n$.
    % \end{theorem}
    

    % % \begin{remark}[Break data independence]\label{rm:ind}
    %     Compared with the convergence results in \Autoref{ex:holderIID, ex:hld_longrange},
    %     % Compared with the results therein, 
    %     \Autoref{ex:atk_or_hld} has a significantly higher stealing limit rate when $\Ul_n$ is fixed or decays slowly. The stealing limit $R_n \asymp 1$ under omnipotent noising when $\Ul_n \asymp 1$, which does not vanish as the sample size goes to infinity. It suggests that the attacker cannot learn a good model when the defender is sufficiently sophisticated. The most critical contributing factor is that the defender can add perturbations with arbitrary dependency, breaking the independence assumption in classical learning theory.
    % % \end{remark}

    % \begin{remark}[Extensions]
    % \Autoref{ex:atk_or_hld} can be extended to the multivariate case, where the optimal algorithm is fitting a spline~\citep{micchelli1976optimal}. 
    % \citet{micchelli1993optimal, foucart2022optimal} studied the attacker's best attack strategy when $\mathcal{F}$ is a Hilbert space, showing that the optimal learning algorithm is solving a regularized minimization task with properly chosen penalty parameters. Nevertheless, the optimal query strategy and stealing limit $R_n$ remain unknown in those cases. \citet{micchelli1977survey} provided examples on the $\ell_p$ and $L_2[0, 1]$ spaces with $\la^*$ and $R_n$ derived explicitly. In addition, \citet{micchelli1977survey} presented more results for the case where $\loss_A(f,g) = \abs{\loss(f)-\loss(g)}$ and $\loss$ is a linear functional.
    % \end{remark}

    % \begin{remark}[Connection with optimal recovery]
    %     While optimal recovery theory~\citep{micchelli1977survey} can be used to analyze omnipotent noising with a fixed design query strategy, most studies are on a case-by-case basis. For example, \citet{micchelli1977survey} obtained the exact stealing limit for stealing a $\loss_p$-integrable function. 
    %     \citet{micchelli1993optimal, foucart2022optimal} studied the attacker's best attack strategy when $\mathcal{F}$ is a Hilbert space, showing that the optimal learning algorithm is solving a regularized minimization task with properly chosen penalty parameters. Nevertheless, the exact rate of the stealing limit and the practical learning algorithms that achieve near-optimal performance are unknown in general.
    % \end{remark}

    \def\fixq{\textrm{Fix}}
    \subsection{No Defense}
    To understand whether a defense is necessary, it is crucial to investigate the scenario where the defender does not apply any defense.
    Let $\dno$ denote the defense mechanism with no perturbation, under which $\Y_i=\fd(X_i)$. For technical convenience, we assume the benign user has a uniform query distribution. 
    Then, \Autoref{ex:atk_no} gives the privacy level of no defense against an attacker whose query strategies $\qsc_{\fixq}$ include all fixed designs. Here, a fixed design query strategy consists of pre-selected sets of data points $\X_1, \dots, \X_n$ for each sample size $n$. 
    \begin{theorem} % [H\"{o}lder class, no defense]
    \label{ex:atk_no}
       Under \Autoref{asmp:same_query, asmp:bounded_input, samp:density, asmp:func},
        suppose the attacker's query strategy collection $\qsc$ includes all fixed designs. Then, we have $\pl_n(\dno,\fc\mid \qsc_{\fixq},\lac) \asymp n^{-2\alpha/d}$. 
    \end{theorem}

    Compared to \Autoref{ex:holderIID}, where the defender adds IID noise, the privacy level is significantly smaller when there is no defense. This implies that attackers have a strong incentive to steal from a defender with no defense. 
    % However, injecting long-range dependent noise with a small correlation parameter $\gamma$ into the responses can make it worthless for attackers to steal the model, as \Autoref{ex:hld_longrange} implies a much higher privacy level than IID Noising. 

