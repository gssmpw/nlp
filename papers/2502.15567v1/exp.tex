

To demonstrate the practical application of the proposed model privacy framework, we conduct experiments on both simulated and real-world datasets to corroborate our theoretical results. Specifically, our results indicate that a specialized defense mechanism is essential to thwart model stealing attacks. In addition to the widely studied regression tasks in the earlier sections, we also conduct experiments on classification tasks, which represent another vital application domain. Notably, our theoretical framework guides the implementation of defenses that significantly bolster model privacy across both task types. The full experiments details can be found in the supplementary materials.


\def\sigmoid{\textrm{softmax}}
% \def\logit{\phi^{-1}}

    \begin{figure}[tb]
        \centering
        \includegraphics[width=0.8\linewidth]{figures/poly_merge.pdf}
        \caption{Goodness comparison of different defense mechanisms against an attacker using polynomial regression. Left: Privacy level at different sample sizes with the utility loss level $\Ul_n=0.25$.  Right: Privacy level at different utility loss levels with the sample size $n=100$. The shaded area reflects one standard error.}
        \label{fig:compare_defense}
    \end{figure}

        \begin{figure}[tb]
            \centering
                \begin{minipage}{0.48\linewidth}
                    \centering                        \includegraphics[width=\linewidth]{figures/ploy_example.pdf}
                \end{minipage}
                \hfill
                \begin{minipage}{0.48\linewidth}
                    \centering
                        \includegraphics[width=\linewidth]{figures/krr_example.pdf}
                \end{minipage}
            \caption{Examples showing the unprotected responses and perturbed responses under our proposed defense mechanism. Left: Using \Autoref{alg:poly} to defend against an attacker performing polynomial regression. Right: Using a defense proposed in the supplemtary material to defend against an attacker performing kernel ridge regression. }
            \label{fig:defense_example}
        \end{figure}    
        
\subsection{Simulated Experiments}
    
    \subsubsection{Polynomial Regression} 
    Suppose the defender owns a quadratic function $\fd(x) = (2x-1)^2$. The attacker performs polynomial regression with $q_n=n^{1/3}$ and selects the model using the Akaike information criterion (AIC) \citep{akaike1998information}. We also conduct experiments where the attacker adopts Bayesian information criterion (BIC) or Bridge Criterion (BC) \citep{ding2017bridging} as the selection criterion and found highly similar results compared to AIC. The squared error loss is used by both defender and attacker. 
    We examine five defense mechanisms: No Defense, IID Noising, Constant Noising (see \Autoref{sec:model_func}), Long-Range Correlated Noising (see \Autoref{subsec:corr_noise}), and our proposed defense Order Disguise (see Algorithm~\ref{alg:poly}).

    First, we study the privacy level of these defense methods under different sample sizes. For computational feasibility, the privacy level is evaluated on a function class containing a single function $\fd$ for all experiments in this section.
    Specifically, we vary the number of queries, $n$, with values set at $20, 50, 100, 200$, and $500$.
    For all defense mechanisms, the utility loss $\Ul_n$ is fixed at $0.25$. Equivalently, the signal-to-noise ratio, defined as $\E[\{\fd(\X)\}^2]/\E(e^2)$ with $e$ being the perturbation, is around one. 
    For each experiment, the attacker sends $n$ IID queries sampled from a Beta distribution $Beta(1,3)$ and receives responses from one of the five defense mechanisms. We then evaluate the privacy level of this defense mechanism using the test error of the rebuilt model on $1000$ IID test data drawn from $Beta(1,3)$. For every combination of sample size $n$ and defense mechanism, we run $100$ independent replicates.

    We also investigate the privacy level at different utility loss levels. Keeping all other experimental conditions constant, we conduct additional experiments with a sample size of $n=100$, varying the utility loss $\Ul_n$ from $0.01$ to $1$. 

    \textbf{Findings.}
    The results of both studies are summarized in Figure~\ref{fig:compare_defense}. 
    Clearly, the attacker can easily steal the defender's model when there is no defense, as the privacy level is zero with only 20 queries. Meanwhile, adding independent noise does not significantly improve the privacy level. The attacker can still efficiently learn a good model under IID Noising, making it not a potent defense. Although Long-Range Correlated Noising performs better than IID Noising, its privacy level remains significantly lower than that of adding perturbations with attack-specific correlation patterns, such as Constant Noising and our proposed defense Order Disguise. 
    
    Moreover, Order Disguise is the only defense mechanism that achieves a higher privacy level than utility loss, aligning with \Autoref{thm:pos}. The left panel in \Autoref{fig:defense_example} demonstrates how Order Disguise injects dependent perturbations into the responses. The shape of the perturbed responses resembles a polynomial function with a higher order than the truth, supporting our theoretical finding that Order Disguise can mislead the attacker into severe overfitting. 

    In summary, the experimental results on polynomial regression align well with our developed theory in \Autoref{subsec:lr}, leading to three critical observations: (1) Defense is necessary; (2) Defenses using independent perturbations offer limited gains in the privacy level compared to no defense. (3) Defenses using dependent perturbations tailored to the attack scenario can be the most effective.

    \subsubsection{Penalized Regression with High-Dimensional Datasets}\label{subsub:high_dim}
        Suppose the defender owns a linear function $\fd(x) = \X^\T \bbeta$, where $\bbeta \in \Real^d$ is the model parameter. The attacker performs penalized regression with built-in variable selection methods, such as LASSO~\citep{tibshirani1996regression} and Elastic Net~\citep{zou2005regularization}. We consider the high-dimensional regression case where the number of predictors $d$ is larger or comparable to the number of queries $n$. We perform seven simulation examples including both sparse and dense structure of $\bbeta$. Similar results are observed across all simulation examples, thereby we describe simulation example 1 in detail as a demonstration. Full experimental details and results of other examples are included in Appendix. 

        Following the setting of \citet{zou2005regularization, nan2014variable}, simulation example 1 generates 50 queries with $d=40$. The model parameter $\bbeta$ is chosen as $\bbeta_i= 3\times \ind_{i\leq 15}$ for $i=1,\dots, 40$. The queries $\X_i = (\X_{i,1}, \dots, \X_{i, 40}), i=1,\dots,50$ are IID generated as follows: 
        $X_{i,j} = Z_{1} + \epsilon_{i,j}, Z_{1} \sim N(0, 1), j=1,\dots,5; 
            X_{i,j} = Z_{2} + \epsilon_{i,j}, Z_{2} \sim N(0, 1), j=6,\dots,10;
            X_{i,j} = Z_{3} + \epsilon_{i,j}, Z_{3} \sim N(0, 1), j=11,\dots,15;
            X_{i,j} \sim N(0, 1), j=16,\dots, 40. $
        % \begin{align*}
        %     X_{i,j} &= Z_{1} + \epsilon_{i,j}, Z_{1} \sim N(0, 1), j=1,\dots,5, \\
        %     X_{i,j} &= Z_{2} + \epsilon_{i,j}, Z_{2} \sim N(0, 1), j=6,\dots,10, \\
        %     X_{i,j} &= Z_{3} + \epsilon_{i,j}, Z_{3} \sim N(0, 1), j=11,\dots,15, \\
        %     X_{i,j} &\sim N(0, 1), j=16,\dots, 40. \\
        % \end{align*}
        Here, $Z_{1}, Z_{2}, Z_{3}$ are IID standard Gaussian, $X_{i,j}$ are IID for $j\geq 16$, and $\epsilon_{i,j}$ are IID $N(0, 0.01)$. 
        The penalty parameters for LASSO and Elastic Net are chosen with 5-fold cross-validation. 

        Five defenses are used for comparison: No Defense, IID Noising, Constant Noising, Long-Range Correlated Noising, and our proposed method Misleading Variable Projection (abbreviated as MVP, see \Autoref{alg:lasso}). MVP is designed to mislead the attacker into fitting a model with non-significant variables. Specifically, the defender chooses a set of non-significant variables and adds perturbations to minimize the distance from $\hat{Y}$ to the space spanned by those variables.  This leads the attacker to incorrectly identify non-significant variables as significant, resulting in a mis-specified model and poor prediction performance. The privacy level is evaluated on $400$ test points, with results from 20 independent replicates presented in \Autoref{fig:compare_defense_lasso}. 

        \begin{algorithm}[tb]
        \caption{Defense Mechanism ``Misleading Variable Projection'' (MVP)}\label{alg:lasso}
        \begin{algorithmic}[1]
            \Require Defender's model $\fd$, queries $\X_i, i=1, \ldots ,n$, utility loss level $\Ul_n$, variable sampling ratio $\rho$
            \State Let $S=\{i: \bbeta_i \neq 0\}$ \Comment{The set of non-significant variables}
            \State Randomly sample and keep $\rho$ percent of $S$, and let $\abs{S}$ be its cardinality
            \State Let $\X_{i,S} \in \Real^{\abs{S}}$ be the random variable constrained on the index set $S$
            \State Let $\X_{S} = (\X_{1,S}, \dots, \X_{n,S}) \in \Real^{n\times \abs{S}}$, $Y^*=( \fd(\X_1), \dots, \fd(\X_n)) \in \Real^n$
            \State Let $\hat{Y} = \mathcal{P}_S Y^*$, where $\mathcal{P}_S = \X_{S} (\X_{S}^\T \X_{S})^{-1} \X_{S}^\T $ is the projection operator on the space spanned by $\X_{S}$
            \State Let $\u = \hat{Y} - Y^*$ and $c = \norm{\u}_2$ \Comment{Smallest distance from $Y^*$ to the space of $\X_{S}$}
            \State Let $\bv = Y^*/\norm{Y^*}_2$ \Comment{Normalize $Y^*$}
            \State $\e =  \sqrt{n} \Ul_n\u/c $ if $ \sqrt{n} \Ul_n \leq c$ else $\u + \sqrt{n\Ul_n^2 - c^2} \bv$ \Comment{Perturbation that push $\Y$ to the space of $\X_{S}$}
            \Ensure $\Y_i = \fd(\X_i) + e_i, i=1, \ldots ,n$
        \end{algorithmic}
        \end{algorithm}  
        
    \begin{figure}[tb]
        \centering
        \includegraphics[width=0.9\linewidth]{figures/lasso_sim_4_acc.pdf}
        \caption{Goodness comparison of different defense mechanisms against an attacker performing penalized regression. }
        \label{fig:compare_defense_lasso}
    \end{figure}        



        
    \textbf{Findings.}
    The results in \Autoref{fig:compare_defense_lasso} are consistent with previous experiments on the two regression tasks. Without defense, the attacker can rebuild a model with almost zero privacy level. Among all defenses, MVP provides the highest privacy level. Moreover, we empirically observe that MVP can deceive the attacker into selecting incorrect variables, as shown in \Autoref{fig:compare_defense_lasso_diff}. To evaluate variable selection accuracy, we calculate the symmetric difference between the true significant variable set $S^*\defeq \{i: \bbeta_i \neq 0\}$ and the set of variables selected by the attacker, denoted as $\hat{S}$. The symmetric difference is $\abs{S^*\backslash \hat{S}}+ \abs{\hat{S} \backslash S^*}$, where $\abs{S}$ is the cardinality of a set $S$. A smaller symmetric difference indicates a higher similarity between the two variable sets, thus a higher variable selection accuracy. From \Autoref{fig:compare_defense_lasso_diff}, we find that MVP leads to the highest symmetric difference.

    \begin{figure}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/lasso_sim_4_diff.pdf}
        \caption{Variable selection reliability of different defense mechanisms against an attacker performing penalized regression. }
        \label{fig:compare_defense_lasso_diff}
    \end{figure} 


        
% \subsection{A More Manipulable Attacker is Easier to Defend Against}
%     We support the findings in \Autoref{subsec:manipulate} by showing that an attacker with an enlarged function class is easier to defend against. In particular, with the same setup as the regression experiment in \Autoref{subsec:exp1}, but the attacker uses polynomial regression with the highest order $q_n$ takes values from $3,5,7,9$, and $11$. The privacy level significantly increases when $q_n$ increases, as presented in \Autoref{tab:lr_change_k}. 

%     \begin{table}
%             \caption{The empirical privacy level (standard error) when the attacker steals a quadratic function using polynomial regression with varied highest order $q_n$.}
%         \label{tab:lr_change_k}
%         \centering
%         \begin{tabular}{lccccc}
%         \toprule
%         $q_n$     &      3 &           5 &            7 &           9 &           11 \\ \midrule
%         Privacy level &   0.28 (0.01) &  0.3 (0.01) &  0.39 (0.03) &  0.59 (0.09) &  0.98 (0.19) \\
%         \bottomrule
%         \end{tabular}
%     \end{table}


\subsection{A real-world case study: hate speech detection}\label{subsec:nlp}
    We consider a model owner who has trained a large language model-based hate speech detection model. 
    Specifically, the defender's model $\fd$ is pretrained on the Toxic Comment Challenge Dataset~\citep{jigsaw-toxic-comment-classification-challenge}, which contains 159,566 sentences from Wikipedia's talk page edits, each labeled as either `normal' or `toxic'. For an input sentence $\X$, $\fd$ will output a predicted probability $\fd(\X) \in [0, 1]$ indicating the likelihood that the input is toxic. 
    
    An attacker, through querying the defender, aims to rebuild a detection function that performs comparably to $\fd$ at a small cost. In our experiments, the attacker's model $\hat{f}_n$ comprises a sentence BERT model~\citep{reimers2019sentence} that transforms the input sentence to an embedding vector, followed by a fully connected two-layer ReLU neural network that predicts the probability. The attacker's queries are sampled from the Toxic Comment Challenge Dataset or the Hate Speech Offensive Dataset~\citep{hateoffensive}, mimicking situations where the attacker either has knowledge of the true data distribution or can only use a surrogate dataset, respectively. The number of queries $n$ takes values of $1000, 2000$, and $5000$.


    % We adopt the same five defenses used in previous simulated experiments on classification tasks.
        Five defenses are studied in the experiments, as detailed below. It is worth noting that our proposed defense, Misleading Shift, is the only defense that adds dependent perturbations conditional on queries.

        1. No Defense, $\Y=\fd(X)$.

        2. Random Shuffle. It is a counterpart of IID Noising for classification tasks. Since the response $\Y$ is a predicted probability vector, a natural way to perturb the response is to randomly shuffle all its coordinates. In particular, let $\Y=\fd(\X)$ with probability $1-\xi$; with probability $\xi$, $\Y$ takes one of the permutation of $\fd(\X)$'s coordinates with equal probability.

        3. Deceptive Perturbation proposed by \citet{lee2019defending}. It perturbs confident responses that are close to zero or one towards the opposite side (without changing the most likely class), making them less confident.

        4. Adaptive Misinformation proposed by \citet{kariyappa2020defending}. It first trains a confounding model that has low accuracy on the original task. Then, it replaces non-confident responses that are close to $0.5$ with the confounding model's outputs.

        5.  Misleading Shift. Inspired by the developed theory, we propose a method named Misleading Shift (see \Autoref{alg:ms}). The idea is to perturb all responses towards the direction of the dominating class among the queries, thereby misleading the attacker to overfit this dominating class and enhancing model privacy. 

        
        \begin{algorithm}[tb]
        \caption{Defense Mechanism ``Misleading Shift'' for Classification Tasks}\label{alg:ms}
        \begin{algorithmic}[1]
            \Require Defender's model $\fd$, queries $\X_i, i=1, \ldots ,n$, scale parameter $\delta$, number of classes $K$
            \State Let $k \defeq \argmax_{1\leq j \leq K} \bigl\vert\{i: \fd(\X_i)_j = \max_{1\leq l\leq K} \fd(\X_i)_l \}\bigr\vert$ \Comment{\parbox[t]{.35\linewidth}{$\vert\cdot\vert$ is the cardinality of a set and $\fd(\X_i)_l$ is the $l$-th coordinate of $\fd(\X_i) \in [0,1]^{K}$}}
            \State  Let $c_k = (\ind_{k=1}, \dots, \ind_{k=K}) \in \{0,1\}^K$ \Comment{$\ind_{(\cdot)}$ is the indicator function}
            \Ensure $\Y_i = \sigmoid(\log(\fd(\X_i))+\delta c_k), i=1, \ldots ,n$
        \end{algorithmic}
        \end{algorithm}  

 
    As for the evaluation criterion, recall that the attacker aims to recover a model with high prediction performance in this real-world application. Therefore, we report the average accuracy and F1 score of the rebuilt model against the utility loss level of the defense over $10$ independent replicates. The accuracy and F1 score are obtained on a test dataset randomly sampled from the Toxic Comment Challenge Dataset. Moreover, we include two baselines for comparison: the accuracy and F1 scores of the defender's model $\fd$ (`Original Model') and a naive model that predicts zero or one with equal probability (`Naive Classifier'). The results are shown in \Autoref{fig:nlp_acc, fig:nlp_f1}.  

    \begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/nlp_tx_acc.pdf}
    \caption{Average accuracy of the attacker's rebuilt model for the hate speech detection task. }
    \label{fig:nlp_acc}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/nlp_tx_f1.pdf}
    \caption{Average F1 score of the attacker's rebuilt model for the hate speech detection task.}
    \label{fig:nlp_f1}
\end{figure}


    \textbf{Findings.} From the experiment results, it is clear that defense is necessary to prevent stealing attacks. Suppose the cost of querying one sentence from the defender is one cent, and the cost of obtaining a human-annotated sentence is also one cent. Then, the defender's training cost of $\fd$ is over \$1000. However, the attacker can steal a model achieving 90\% of $\fd$'s performance for as little as \$10 without defense. Even with a surrogate dataset, the reconstruction cost is around \$50, which is just 5\% of the defender's expense.

    Also, the results show that adding independent perturbations conditional on queries does not significantly hinder the attacker. When the defender deploys Random Shuffle or Deceptive Perturbation, the attacker can still efficiently rebuild a good model as if there were no defense. Adaptive Misinformation also shows limited effectiveness, especially when queries come from the same distribution as the training data or when the number of queries is substantial.

    In a stark contrast, our proposed defense, Misleading Shift, achieves high privacy gains across all utility loss levels. Misleading Shift adds a constant, attack-specific perturbation to all responses. This attack-specific dependent structure of perturbations exaggerates the trend of the majority class in the queries, thereby consistently misleading the attacker to overfit the majority class. This insight is corroborated by \Autoref{fig:nlp_f1}, where the rebuilt model's F1 score is close to zero under misleading shift, indicating that the model predicts most inputs as the same class. 
    
