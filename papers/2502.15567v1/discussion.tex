    In previous sections, we have established a model privacy framework for the analysis of model stealing attacks and defenses, and have illustrated its usefulness by addressing fundamental problems such as identifying optimal defense strategies in specific situations. This section aims to explore the broader implications of model privacy. 
    In \Autoref{subsec:ext}, we discuss three directions to generalize the context of model privacy.
    % Furthermore, we review the existing literature on model attacks and propose our model attack taxonomy in \Autoref{subsec:pastwork}. 
    In \Autoref{subsec:related}, we draw connections between model privacy and broader machine learning topics.
    
    \subsection{Extensions}\label{subsec:ext}
        
        \textbf{Direction I: Protecting the Intrinsic Data Distribution.} In \Autoref{subsec:overview}, we posited that the attacker seeks to steal a model $\fd$ deployed by the defender, such as a valuable empirical equation established by domain experts, or a secure face  authentication system.
        However, for tasks such as image classification and text generation, the defender's model $\fd$ is trained to learn the underlying data-generating distribution $(\X, Y) \sim \mathcal{D}$ of the training dataset. In this scenario, instead of stealing $\fd$, the attacker may be interested in stealing a function $\ft$ representing the intrinsic data distribution, such as the conditional mean $\E (Y \mid \X)$. 
        % from a dataset collected from a natural data-generating distribution $(\X, Y) \sim \mathcal{D}$
        % It is worth noting that $\fd$ may be trained from a dataset collected from a natural data-generating distribution $(\X, Y) \sim \mathcal{D}$, which aims to learn the underlying data-generating function, such as the conditional mean function $\ft \defeq \E (Y \mid \X)$. In this scenario, the attacker may instead be interested in $\ft$, the function representing the intrinsic data distribution.

        Our proposed model privacy also applies to the scenario of protecting $\ft$, with a slight modification by replacing $\fd$ with $\ft$ in essential ingredients formulated in \Autoref{sec:motivation}. For example, the necessary stealing sample size (see \Autoref{def:steal_sample_size}) is thereby defined as
        \begin{align*}
            n(\epsilon,\ell;\ft,\fd,\dm,\qs,\la) \defeq \inf\{n:  \E \{\ell(\hat{f}_n, \ft) \}\leq \epsilon\}.
        \end{align*}
        Despite the change from $\fd$ to $\ft$, we still focus on analyzing the relationship among stealing error, privacy level, and utility loss given a specific attack strategy and defense mechanism.
        
        In particular, the analysis of protecting $\ft$ is akin to protecting $\fd$ when $\fd$ approximates $\ft$ well, which is commonly seen in applications with large training datasets. Specifically, let $\tau \defeq \loss(\fd, \ft)$ denote the gap between the defender's function and function of interest. The triangular inequality gives that
        $\abs{\ell(\hat{f}_n, \fd) -  \ell(\hat{f}_n, \ft)} \leq \ell(\fd, \ft) = \tau.$
        Therefore, the estimation of $\ell(\hat{f}_n, \fd)$ is also a sensible estimation of $\ell(\hat{f}_n, \ft)$ for small $\tau$. 
        
        However, if there is a significant divergence between $\fd$ and $\ft$, stealing from the defender may not be a wise decision for the attacker. Without side information, the attacker has no clue how good does $\fd$ approximate $\ft$. Thus, an accurate reconstruction of $\fd$ only leads to a highly biased function. Nevertheless, when there is additional information about the defender or underlying distribution, the attacker may still leverage the part of relatively useful and accurate responses from the defender to facilitate their own learning tasks. One such example is \Autoref{ex:extra_point_knn}. A defender with constant noising can be considered to have a biased function $\fd$ to $\ft$. We denote $\fd=\ft+B$, where $B$ is a constant bias term. While \Autoref{thm:knn} implies that an attacker using $k$-NN learning algorithm will reconstruct a function that contains this bias $B$, 
        \Autoref{ex:extra_point_knn} shows that the attacker can efficiently debias the responses from the defender when additional nature-generated data is available.
        
        In summary, when the attacker aims at stealing $\ft$, it is challenging for the attacker to determine what parts of responses from the defender are useful, and to what extent the reconstructed function is close to $\ft$. To overcome this issue, assuming the attacker owns more knowledge on either the defender's function or $\ft$ is a promising solution.
        % In such cases, it becomes especially appealing yet complex to understand the conditions and extent to which the defender's responses will facilitate the attacker's learning process.

    % This extension significantly amplifies the scope of model privacy and enables finer-grid analysis. 
    \textbf{Direction II: Protecting Components of the Model.} 
    Besides the threat of being revealed by the attacker,
    a model engaging in the query-response-based interaction also bears the risk of revealing its parameters and hyper-parameters \citep{tramer2016stealing, wang2018stealing}, and even exact values of the training data \citep{fredrikson2014privacy, fredrikson2015model}. For example, \citet{fredrikson2015model} showed that ``an attacker can produce a recognizable image of a person, given only the person's name and the API access to a facial recognition system that returns a class confidence score.''

    Recognizing this vulnerability, we extend our model privacy framework to include the protection of any model-related quantities, such as values at specific points, derivatives, parameters, hyperparameters, architecture, and the original dataset. In this way, we significantly amplifies the scope of model privacy and enables finer-grid analysis. 
    
    % In \Autoref{subsec:overview}, we have already allowed the defender's and attacker's loss functions to evaluate partial regions instead of the full support. We propose expanding the notion of 'partial' evaluations to include various model-related quantities, such as values at specific points, derivatives, parameters, hyperparameters, architecture, and the original dataset. In this way, we significantly amplifies the scope of model privacy and enables finer-grid analysis. 

    To this end, we extend model privacy framework by allowing the defender's response (before perturbation) to take any form related to the defender's training dataset, denoted as $Z_m^*=\{(\X_i^*,Y_i),i=1,\dots,m\}$. That is, for a given space $\mathcal{Y}$ and query $\X_i$, the defender will return $\Y = f(\X_i;Z_m^*) + \epsilon$, where $\epsilon$ is the perturbation determined by a defense mechanism $\dm$, and $f(\cdot; Z_m^*)$ can be any measurable function on $\mathcal{Y}$. For example, $f(\cdot; Z_m^*) = \fd(\cdot)$ for classical model stealing attacks discussed in \Autoref{sec:motivation}; $f(\cdot; Z_m^*) = (\fd)^{(1)}(\cdot)$ when the defender returns the derivative at a specific point; $f(\cdot; Z_m^*) = \ind_{(\cdot \in Z_m^*)}$ when the defender answers whether a given query is present in the training dataset. Correspondingly, the quantity of interest will be an operator on the training data, denoted as $I(Z_m^*) \in \mathcal{Z}$ for some space $\mathcal{Z}$, and the attacker aims to reconstruct a $\hat{f}_n \in \mathcal{Z}$ as the approximation of $I(Z_m^*)$. The necessary sample size is defined as
    % \begin{align*}
         $n(\epsilon,\ell;Z_m^*,\fd,\dm,\qs,\la) \defeq \inf\{n:  \E \{\ell(\hat{f}_n, I(Z_m^*)) \}\leq \epsilon\}.$
    % \end{align*}
    % \todo{expand here}
    The analysis then follows the same flow as the usual model privacy framework. Notably, for both attacker and defender, focusing on quantities narrower than the entire model can lead to more targeted and consequently more \eco strategies.

        
  
    \textbf{Direction III: Integrating with Game Theory.}
    It is natural to integrate model privacy with game theory when the defender and attacker are in an interactive setting. 
    We illustrate the insights of game theory under a strategic game with perfect information, meaning that each player is fully aware of the other's strategy space and chooses their action simultaneously.
    Without any interaction, as analyzed in \Autoref{subsec:game}, the optimal strategies for maximizing individual interest would be:
    \begin{align*}
        \qs^{1}, \la^{1} \defeq \argmax_{\qs \in \qsc,\la \in \lac}  P_A(\qs, \la;\fd, \dm), \quad
        \dm^{1} \defeq \argmax_{\dm \in \dmc}  P_D(\fd, \dm;\qs, \la).
    \end{align*}
    Now, given those choices in mind, a rational player will adjust their strategy considering the opponent's anticipated behavior. Consequently,  the attacker will update their strategy to:
    \begin{align*}
         \qs^{2}, \la^{2} \defeq \argmax_{\qs \in \qsc,\la \in \lac} P_A(\qs, \la;\fd, \dm^{1}) ,
    \end{align*}
    while the defender will choose
    \begin{align*}
        \dm^{2} \defeq \argmax_{\dm \in \dmc} P_D(\fd, \dm;\qs^{1}, \la^{1}) ,
    \end{align*}
    and so on and so forth. This iterative process may culminate in a state of equilibrium, specifically, a Nash equilibrium, where the strategy set $\{\qs^{*}, \la^{*}, \dm^{*}\}$ satisfies:
    \begin{align*}
        \qs^{*}, \la^{*} = \argmax_{\qs \in \qsc,\la \in \lac} P_A(\qs, \la;\fd, \dm^{*})  ,
        \quad \dm^{*} = \argmax_{\dm \in \dmc} P_D(\fd, \dm;\qs^{*}, \la^{*}).
    \end{align*}

   The addition of game-theoretic interaction introduces new questions and analytical challenges. For example, when does Nash equilibrium exist? What if players engage in sequential rather than simultaneous decision-making? We leave the comprehensive examination of these game-theoretic concerns for future research endeavors.




    \subsection{Connection to Teacher-Student Learning}\label{subsec:related}
    We conclude this section by discussing the connections between the developed model privacy framework and teacher-student learning, which shares the same setup and spirit. 
    % It also worth mentioning that model privacy is also closely related to many other learning topics, including but not limited to 
    % other learning topics that share a similar setup or technical details, including but not limited to teacher-student learning and data privacy. 
    % Therefore, the development of model privacy can provide valuable insights into these related fields and vice versa. 

   % \textbf{Teacher-Student Learning.} 
   Generally speaking, teacher-student learning mimics human educational practices, where a knowledgeable teacher guides students through the learning process. In particular, the teacher will answer students' questions based on their own knowledge, aligning with the query-response pattern central to the model privacy framework. We next present two critical scenarios of teacher-student learning.
   
    The first scenario is knowledge distillation~\citep{hinton2015distilling, phuong2019towards}, which aims to extract the knowledge from a larger, more complex teacher model to a smaller student model. Originating as a strategy for model compression~\citep{hoefler2021sparsity}, knowledge distillation has garnered significant attention due to its successful applications in contexts where model sizes are constrained by computational or storage limitations. Specifically, given a large teacher model trained on a massive dataset, we strive to build a smaller student model with comparable accuracy by using the query-response pairs obtained from the teacher model. 

    The second scenario pertains to semi-supervised learning, which aims to train an accurate model using a limited set of labeled data augmented by a larger pool of unlabeled data. 
    To utilize those unlabeled data, one popular semi-supervised approach consists of the following steps: 1. Train an initial model based on labeled data solely. 2. Predict labels of unlabeled data using the current model, creating a set of pseudo-labeled data. 3. Retrain the model using both labeled and pseudo-labeled data. 4. Iterate through steps 2 and 3 until meeting a stopping criterion. Here, step 2 can be interpreted as querying the defender in model privacy, and step 3 resembles the process of model reconstruction.

    Clearly, the student and teacher correspond to the attacker and defender in model privacy. While model privacy is originally proposed to promoting defenses against an attacker so that the reconstructed model is inaccurate, it also studies the central problem in teaching-student learning: how can the attacker maximize the accuracy of their model by leveraging the defender's responses, and what this maximum accuracy is?
    Model privacy thus provides a principled way to understand the fundamental limitations of teacher-student learning, such as the efficiency of learning algorithms in terms of the minimal amount of data needed to build a model with the desired accuracy. 
    Moreover, in teacher-student learning, the student usually aims to improve model accuracy on data-generating distributions, instead of exactly recovering the teacher's model. Therefore, the quality of the teacher model is crucial for the student's training, corresponding to our discussion in \Autoref{subsec:ext}.
    

    % \textbf{Active learning.} Active learning~\citep{settles2009active, chandrasekaran2020exploring, ren2021survey} aims to train an accurate model with the minimum amount of training data.
    % This is achieved by allowing the learner to actively choose which data points to query the oracle, who in our case is the model owner, for the corresponding labels. 
    % In this way, active learning shares the same goal as that of the attacker in model privacy: optimizing the query strategy. Nevertheless, existing active learning literature focuses on scenarios where the model owner returns responses with no defense or IID noise. The introduction of defense mechanisms in model privacy suggests a novel avenue for active learning research, considering how data perturbations could influence the optimal selection of queries.

    % \textbf{Transfer learning.} Transfer learning~\citep{zhuang2020comprehensive, weiss2016survey} tries to facilitate the learning of a task by leveraging existing models or datasets from different sources but with a similar context. Take ResNet~\citep{he2016deep}, a widely used neural network in computer vision, for illustration. The idea is that ResNet can extract low- and high-level features of an image, which are often transferrable across different domains. Hence, a ResNet model pre-trained on animal images, for example, can be adapted for other image classification tasks by only retraining the output layer, while retaining the feature-extraction layers.
    % Therefore, transfer learning with a single-source model is naturally aligned with the context of model privacy. Specifically, the learner in transfer learning aims to utilize the knowledge of the model owner to improve model performance on their own tasks. 
    % In the meantime, the learner may possess side information such as partial parameters from the original model or a small amount of labeled data points for their target tasks. 
    
    
        
    % \textbf{Data privacy.}
    %     The concept of data privacy centers on safeguarding the identities and values of data points \citep{dwork2006calibrating, dwork2014algorithmic, warner1965randomized, evfimievski2003limiting, wang2021subset, ding2022interval}. 
    %     A model engaging in the query-response-based interaction, however, bears the risk of revealing the exact values of training data \citep{fredrikson2014privacy, fredrikson2015model} and conducting membership-inference \citep{shokri2017membership}. For example, \citet{fredrikson2015model} showed that ``an attacker can produce a recognizable image of a person, given only the person's name and the API access to a facial recognition system that returns a class confidence score.''
    %     % Based on model stealing attacks, 
    %     % a trained model engaging in the query-response-based interaction bears the risk of leaking information about its training data, such as  
        
    %     Recognizing this vulnerability, we extend the model privacy framework in \Autoref{subsec:ext} to include the protection of not only the model but also its underlying training data. This is achieved by incorporating a data-specific loss function that quantifies the risk of data exposure through such interactions.
      

        % TODO: reuse the below example later.
        % \subsection{Other thoughts}
        % \textbf{Query matters} For example, consider a logistic regression model with coefficient $\bm \beta$ and query variable $\bm X$ for a binary classification task. Suppose the model is well-specified and $\bm X$ is IID-generated from a distribution $p$. 
        % It is known that the maximum likelihood estimation gives a consistent estimation of the data-generating coefficients $\bm \beta^*$, with the asymptotic variance of the estimator proportional to the inverse Fisher information, denoted by $I(p)$.
        % Consider a class of distributions of $\bm X$ in the form of $\{\bm X = \tau \tilde{\bm X} \sim p_{\tau}: \tau > 0\}$, where $\tau$ is a scaling factor, and $\tilde{\bm X}$ is a fixed random vector. Assume that $\tilde{\bm X}^{\T} \bm \beta^* \leq \delta$
        % It can be verified from Expression~\ref{eq_margin1} that $I(p_{\tau})$ is decreasing in $\tau$, assuming that $\tau < 2 / \delta$. 
        % In other words, if the queries are, on average, closer to the decision boundary (namely a smaller $\tau$), the inverse Fisher information is smaller and thus the estimation accuracy is higher.
        % \begin{proposition}\label{prop_logistic}
        %     Assume that $\sup_{\tilde{\bm X}} \tilde{\bm X}^{\T} \bm \beta^* \leq \delta$ for some positive constant $\delta$. Then, $I(p_{\tau})$ is decreasing in $\tau \in (0,2 / \delta)$.
        % \end{proposition}
        % \begin{proof}[Proposition~\ref{prop_logistic}]
        %     For logistic regression models, the inverse Fisher information can be written as
        %     \begin{align*}
        %       I(p) = \E_{\bm X \sim p}\biggl(\frac{1}{(1+\exp(-{\bm X}^{\T} \bm \beta^*)) (1+\exp({\bm X}^{\T} \bm \beta^*))} \bm X \bm X^\T \biggr) .
        %     \end{align*}
        %     Thus, for $X$ in the form of $\tau \tilde{\bm X} \sim p_\tau$, we have
        %     \begin{align}
        %       I(p_\tau) = \E_{\tilde{\bm X} \sim p_1}\biggl(\frac{\tau^2}{(1+\exp(-\tau q)) (1+\exp(\tau q))} \tilde{\bm X} \tilde{\bm X}^\T \biggr) \label{eq_margin1}
        %     \end{align}
        %     where $q \de \tilde{\bm X}^{\T} \bm \beta^*$.
        %     Let $h_q$ be the function $\tau \mapsto \frac{\tau^2}{(1+\exp(-\tau q)) (1+\exp(\tau q))}$. It can be verified that
        %     \begin{align*}
        %         \nabla_{\tau} \log h_q = \frac{2}{\tau} + q\frac{1-\exp(\tau q)}{1+\exp(\tau q)} \geq \frac{2}{\tau} - q \geq \frac{2}{\tau} - \delta > 0 
        %     \end{align*}
        %     for all $\tau \in (0, 2/\delta)$.
        %     This concludes the proof.
        % \end{proof}

