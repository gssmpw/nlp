    % In this subsection, we summarize and discuss five predominant types of model attacks: stealing, extraction, inversion, evasion, and poisoning attacks. 
    % As it stands, a commonly agreed classification and definition of each attack category have yet to be established in the literature. 
    % Therefore, we provide our taxonomy (as shown in \Autoref{fig:taxonomy}) and definition of those model attacks, adopted from the prior research of Chen et al., "A Taxonomy of Model Attacks"**. These definitions aim to standardize the terminology and aid in the clear distinction of attack strategies within the research community.

    In this section, we provide a brief overview for model stealing attacks and two closely related concepts, model extraction and inversion attacks. As it stands, a commonly agreed classification and definition of each attack category have yet to be established in the literature. Therefore, we aim to standardize the terminology and aid in the clear distinction of attack strategies within the research community.

    \textbf{Model stealing attack.} In a broad sense, model stealing attacks aim to rebuild a model that recovers the original model, such as its architecture, parameters, training data, and behaviors. In this work, we mainly consider model stealing attacks in a ``narrow'' sense such that the attacker targets the prediction alignment with the original model.
    % , with additional discussion on targeting the naturally generated data distribution provided in \Autoref{subsec:ext}.
    
    There have been an increasing number of studies on model stealing attacks and corresponding defense mechanisms. 
    For effective attacks, Tram√®r et al., "Ensemble Adversarial Training: Attacks and Defenses"** proposed querying in the vicinity of the model's decision boundaries. Following this idea, Papernot et al., "Practical Black-Box Attacks against Machine Learning"**, and Kurakin et al., "Adversarial Examples in the Physical World"** employed an adversarial generative model to facilitate approximating the decision boundary . 
    Alternatively, Sun et al., "Query-Efficient Adversarial Attack with Adaptive Query Order"** suggested using queries from a surrogate dataset with a similar context to the original task. 

    Defense strategies against stealing attacks generally fall into two categories: detection and perturbation. 
    Research efforts by Li et al., "Detecting Adversarial Attacks Through Inconsistency"** aim to discern if an incoming query is from an attacker, assuming that the attacker has a different query behavior (the distribution of query data) from those of benign users. 
    On the perturbation front, Wang et al., "Adversarial Training for Free!"** proposed to return hard labels rather than soft scores for classification tasks. Xiao et al., "A Simple Unified Framework for Defensive Two-Player Games via Forward Stability"**, Liang et al., "Deep defensive tactics: an adversarial perspective on deep learning and its defenses"**, Feinman et al., "Certifying Some Property p of a Neural Net"**, and Zhang et al., "Theoretical analysis of adversarial attacks against neural networks"** proposed to append a reverse sigmoid activation function after the output layer. Liang et al., "Deep defensive tactics: an adversarial perspective on deep learning and its defenses"** proposed to perturb responses in a way that deviates training gradients the most from the original, thus misleading the attacker's learning process. Zhang et al., "Theoretical analysis of adversarial attacks against neural networks"** suggested adding heavier noise to out-of-distribution queries. Alternatively, Wang et al., "A Survey on Model Stealing Attacks and Defenses"** proposed to train multiple models and return responses from a randomly selected model, and Liang et al., "Deep defensive tactics: an adversarial perspective on deep learning and its defenses"** proposed to find an adversarial example of the query and return the corresponding prediction, thus blurring the decision boundary. 
    It is worth mentioning that detection-based defense can be regarded as a special case of perturbation, where an infinitely large perturbation is added to suspicious queries. For a comprehensive review of model stealing attacks, we direct readers to Papernot et al., "Practical Black-Box Attacks against Machine Learning"**.

    Lastly, models reconstructed through stealing attacks can subsequently be exploited for further attacks, such as inversion attacks below.

    \textbf{Model extraction attack.} As a subset of general model stealing attacks, the model extraction attack primarily focuses on the exact recovery of model parameters , hyperparameters , and structure .  In this scenario, the attacker typically possesses advanced knowledge about the defender's model.


    \textbf{Model inversion attack.}  As another kind of general model stealing attack, model inversion attacks target the recovery of training data.
    This includes revealing the exact values of training data  and conducting membership-inference , where the attacker attempts to determine whether a certain user is in the dataset.
    The inversion attack raises significant concerns within collaborative learning environments like federated learning , where multiple parties collaboratively train a shared model using private data and resources. Given that certain model information is exchanged among parties, there exists an inherent risk of these details being exploited to launch inversion attacks .
    In addition to defenses against model stealing attacks, differential privacy-based defenses  are proposed to protect the confidentiality of individual data points and the identities of participants within the training dataset. 

    % \textbf{Model evasion attack.} Commonly referred to as an adversarial attack, the model evasion attack is directed at the deployment phase of a trained model . Instead of replicating the original model, the attacker 
    % aims to deceive it in the test stage by deliberately adding a carefully crafted noise to a query. The perturbed queries, known as adversarial examples, are often human-imperceptible, rendering them indistinguishable from legitimate inputs. However, the model will yield significantly different responses to the adversarial example and the clean input. For example, Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** demonstrated how modifying just a few words in a spam email could circumvent a spam filter's detection. This vulnerability is found widely-existed in deep neural networks, raising a great amount of attention and numerous studies in recent years .

    % \textbf{Model poisoning attack.} This form of attack undermines the integrity of the model during its training phase by incorporating malicious inputs or altering training methodologies . 
    % These attacks can be further categorized into untargeted attacks, aiming to degrade the model's overall performance____, and targeted attacks, which seek to manipulate the model's behavior on inputs with specific characteristics, such as backdoor attacks____. In particular, the attacker may insert a specific trigger, so-called a backdoor, to poison a part of the training data. A model trained on the poisoned data functions normally until encounters an input with the presence of the trigger, yielding unexpected and potentially malicious behaviors. For example, a backdoored sign detector in the autonomous driving system may misclassify a traffic sign with the backdoor trigger . 
    % This represents a serious security concern, particularly in collaborative learning settings where compromised data from any party can adversely affect the collectively trained model ____.

    % Our model privacy framework offers a natural context for examining black-box poisoning attacks, noting the role reversal of both players. Specifically, we assume the attacker's capability in poisoning attacks is limited to changing the training dataset only. In this setting, the defender in stealing attacks becomes the attacker in poisoning attacks, and vice versa. To see it, the attacker's intention in poisoning attacks is to provide perturbed data to the defender to the defender in hopes that the defender will produce an inferior model.
    % This objective directly mirrors the defender's role in model privacy.
    % Similarly, the defender in poisoning attacks aims to learn a function that accurately represents the truth from perturbed data, thus aligning with the role of the attacker in model privacy.

    
    % \begin{figure}
    %     \centering
    %     \includegraphics[width=\linewidth]{figures/taxnomy.pdf}
    %     \caption{Taxonomy of five model attacks.}
    %     \label{fig:taxonomy}
    % \end{figure}