
        In this section, we demonstrate that understanding the stealing capability, characterized by the statistical risk of the rebuilt function $\E\{ \ell(\hat{f}_n, \fd)\}$, is essential for evaluating the goodness of defenses and attacks. 

        To illustrate, we consider the cost-based quantification approach introduced in \Autoref{subsec:quantify}. Recall that both defense and attack strengths are determined by the necessary stealing sample size $n(\epsilon,\ell;\fd,\dm,\qs,\la) \defeq \inf_n\{n:  \E \{\ell(\hat{f}_n, \fd) \}\leq \epsilon\}$. Therefore, they can be derived once the rebuilt function's statistical risk is understood. The following two examples further elaborate on this insight, using the squared error loss as the loss function $\ell$.

        \begin{example}[Steal in a Parametric Setting]
            Suppose the defender has a linear function $\fd$ and the attacker uses linear regression to steal it. Without any defense, the attacker can exactly recover $\fd$ with a sufficiently large number of queries, as explained in \Autoref{ex:lin}. Therefore, the necessary stealing sample size is a fixed constant for any $\epsilon>0$. In contrast, if the attacker collects data from nature containing IID noise with variance $\sigma^2>0$, the convergence rate of the linear regression estimator is $\E \norm{\hat{f}_n- \fd}_2^2 \asymp \sigma^2/n$. As a result, the necessary stealing sample size is typically at the order of $1/\epsilon$ in this scenario, making the attack strength infinity when there is no defense.
        \end{example}

        \begin{example}[Stealing in a Non-Parametric Setting]\label{ex:steal_nonpara}
            Now, suppose the attacker uses non-parametric learning algorithms, such as $k$-nearest neighbors. The convergence rate in this case is known for a variety of common function classes. Taking the Lipschitz function class on $[0,1]^d$ for example, we typically have $\E \norm{\hat{f}_n- \fd}_2^2 \asymp n^{-2/(d+2)}$ when IID noise is injected into the responses. Therefore, the necessary stealing sample size is at the order of $\epsilon^{-(d+2)/2}$.
            If the defender adds long-range dependent noise into the responses, we will prove in \Autoref{sec:atk} that $\E \norm{\hat{f}_n- \fd}_2^2 \asymp n^{-\gamma}$, where $\gamma$ represents the degree of noise correlation. Consequently, the necessary stealing sample size $n(\epsilon,\ell;\fd,\dm,\qs,\la) \asymp \epsilon^{-1/\gamma}$, implying that adding long-range dependent noise is a \eco defense when $\gamma < 2/(d+2)$. 
        \end{example}
       
        % Practitioners may consider the goodness of attack and defense with additional perspectives. Regardless, a
        As seen from the examples above, the essential issue in goodness quantification is \textit{how well an attacker can learn the underlying function with $n$ queries}, which we call the stealing capability. 
        The rebuilt model's statistical risk $\E \{ \ell(\hat{f}_n, \fd)\}$ is critical in this evaluation. 
        Based on the worst-case convergence rate over a function class $\fc$ that contains $\fd$, we define the following privacy level to capture the stealing capability under a defense $\dm$, which measures the goodness of $\dm$ against a collection of possible attack strategies. Analogously, we define the stealing error to measures the goodness of an attack $(\qs,\la)$ against a collection of possible defenses. 
        % Specifically, we consider the convergence rate of the maximum risk over a function class $\fc$ that contains $\fd$. 
        % The defender can use the following privacy level to evaluate a defense $\dm$ against a collection of possible attack strategies.
        \begin{definition}[Defender's Privacy level]\label{def:priv}
            The (worst-case) privacy level of a defense mechanism $\dm$ against a collection of query strategies $\qsc$ and learning algorithms $\lac$ at a sample size $n$ is 
             \begin{align*}
                \pl_n(\dm,\fc \mid \qsc,\lac) \defeq  \inf_{\qs \in \qsc,\la \in \lac} \sup_{\fd \in \fc} \E \{ \ell_D(\fd, \hat{f}_n) \}. 
            \end{align*}
        Here, $\hat{f}_n$ depends on $\qs$, $\la$, $\fd$, and $\dm$, since it is reconstructed from the query-response pairs.
        A defense mechanism $\dm^*$ is called rate optimal within a collection of defenses $\dmc$ if
        \begin{align*}
            \pl_n(\dm^*,\fc \mid \qsc,\lac) \asymp \sup_{\dm \in \dmc}  \pl_n(\dm,\fc \mid \qsc,\lac).
        \end{align*}
        \end{definition}

        % Similarly, facing a collection of possible defenses $\dmc$, the attacker can evaluate the goodness of their attack strategy via the worst-case statistical risk as well.

        \begin{definition}[Attacker's Stealing Error]\label{def:stealerror}
            The (worst-case) stealing error of an attack $(\qs,\la)$ with respect to a collection of possible defenses $\dmc$ at sample size $n$ is defined as 
            \begin{align*}
                r_n(\qs, \la \mid \fc,\dmc) \defeq \sup_{\fd \in \fc, \dm \in \dmc} \E \{ \ell_A(\fd, \hat{f}_n) \}.
            \end{align*}
        An attack strategy $(\qs^*, \la^*)$ is called rate optimal within $(\qsc,\lac)$ if $ r_n(\qs^*, \la^* \mid \fc,\dmc) \asymp \inf_{\qs \in \qsc, \la \in \lac}  r_n(\qs, \la \mid \fc,\dmc) $.
        \end{definition}

        Regarding interpretation, the privacy level provides a guarantee of a defense mechanism's performance against a collection of potential attack strategies. For instance, if the attacker uses $k$-nearest neighbors algorithms with $k>0$ to steal a Lipschitz function on $[0, 1]^d$, a defender injecting IID noise with a fixed noise variance achieves a privacy level at the order of $n^{-2/(d+2)}$, as proved in \Autoref{thm:knn}. Consequently, the defender can expect that the risk of the attacker's reconstructed function is at least at the order of $n^{-2/(d+2)}$, regardless the choice of $k$. Analogously, the stealing error provides a guarantee on $\hat{f}_n$'s performance from the attacker's perspective, thus quantifying the effectiveness of an attack strategy. 
        % the worst-case scenario for a defender deploying $\dun$ corresponds to an attacker using $k$-NN with $k = 1$.


        The privacy level is closely related to the stealing error when the defender and attacker have the same evaluation criteria. 
        % Specifically, given a function class $\fc$ that $\fd$ can be any function in it, the privacy level of a defense mechanism $\dm$ against $(\qsc,\lac)$ characterizes the smallest stealing error that an attacker can uniformly achieve by choosing an attack strategy in $(\qsc,\lac)$. 
         % and $\pl_n(\dm^*,\fc \mid \qsc, \lac) \lesssim  r_n(\qs^*, \la^* \mid \fc,\dmc)$
        In fact, for any given defense $\dm$ and attack $(\qs,\la)$, we have $\pl_n(\dm,\fc \mid \qs,\la) =  r_n(\qs, \la \mid \fc,\dm)$ when $\loss_A=\loss_D$, since both privacy level and stealing error aim to describe the attacker's stealing capability though from competing perspectives. 
        Those two quantities can be less aligned when $\loss_A$ and $\loss_D$ are different. For instance, if $\loss_A$ and $\loss_D$ focus on distinct regions, it is possible that the privacy level is high while the stealing error is small, with further discussion in 
        the supplementary document.
        % \Autoref{subsec:manipulate}. 
        


        \textbf{Connection to cost-based quantification.} 
        Given the same level of utility loss, a higher privacy level implies a larger necessary stealing sample size, hence a more \eco defense. Furthermore, the rate optimal defense within $\dmc$ can be roughly considered as the best defense mechanism under the cost-based quantification defined in \Autoref{eq:best_def}. Therefore, in this paper, we refer to a defense as the best if it is rate optimal. The defender's objective can be rephrased as follows:
        \begin{align*}
        \text{(Defender's Objective) } &\max_{\dm \in \dmc}  \pl_n(\dm,\fc \mid \qsc,\lac), \text{ s.t. } \ul_n(\dm) = \Ul_n. 
        \end{align*}
        Here, $\qsc$ and $\lac$ represent the potential attack strategies that the defender may encounter. For example, the defender may face an attacker using specific learning algorithms, such as polynomial regression and neural networks. In cases where the defender has no prior knowledge of the attacker's strategy, $\qsc$ and $\lac$ encompass all possible attack strategies.
        
        Similarly, a smaller stealing error $r_n(\qs,\la \mid \fc,\dmc)$ indicates to a more \eco attack, and a rate optimal attack is considered
        the best attack strategy under the cost-based quantification.
        The attacker's objectives can be rephrased as follows: 
        \begin{align*}
            \text{(Attacker's Objective) } &\min_{\qs \in \qsc, \la \in \lac}  r_n(\qs, \la \mid \fc,\dmc).
        \end{align*}
        
        In addition to finding the best strategy, both the attacker and defender strive to understand several fundamental problems in model privacy.
        Given the urgent societal concern of defending against model stealing attacks in real-world AI applications, this paper focuses primarily on the defender's perspective. We list some of the key problems below. 

        1. When is it necessary to defend a model?

        2. What is the best defense strategy?

        3. What kinds of attacks are easier to defend against?

        4. How do other key ingredients listed in \Autoref{subsec:overview} affect decision-making?
        
        % \begin{enumerate}
        %     \item \  When is it necessary to defend a model?
        %     \item \ What is the best defense strategy?
        %     \item \ What kinds of attacks are easier to defend against?
        %     \item \ How do other key ingredients listed in \Autoref{subsec:overview} affect decision-making?
        % \end{enumerate}
        % 1.  2.  3.  4.  5. How do other key ingredients listed in \Autoref{subsec:overview} affect decision-making?
        Addressing those problems will provide theory-guided solutions for developing \eco defense strategies to secure ML model services. These issues are further discussed in \Autoref{sec:model_func, sec:atk}. In particular, \Autoref{sec:model_func} discusses the defender's decision-making given a known attack strategy, while \Autoref{sec:atk} considers defending against an attacker with unknown attacks.
        
        
    \section{Setup and Notations}\label{subsec:notation}
        \textbf{Setup.} Unless specified otherwise, throughout the analysis in \Autoref{sec:model_func, sec:atk}, we assume the defender, attacker, and benign users all use the squared error loss. Specifically, $\loss_A(f,g) = \loss_D(f,g) = \norm{f-g}_2^2$ for any two functions $f$ and $g$, where the norm is with respect to query distribution $\mu$, which will be introduced soon. Additionally, for any two scalars $x$ and $y$, we have $\loss_U(x,y) =  (x-y)^2$. As explained in \Autoref{subsec:quantify}, the defenses under comparison should have the same level of utility loss. Therefore, the defense mechanism set under consideration is $\dmc(\Ul_n) = \{\dm: \ul_n(\dm) = \Ul_n\}$, where $\{\Ul_n,n=1,\dots\}$ is a sequence of non-negative utility loss levels such that $\Ul_n \lesssim 1$. In contrast to querying the defender, the attacker may obtain ``nature-collected'' data. We assume that those natural data contain IID noise with mean zero and a fixed variance $\sigma^2>0$, and its distribution follows $\mu$ as well.
        The following assumptions are frequently used throughout the analysis in \Autoref{sec:model_func, sec:atk}. 
        \begin{assumption}[Same Query Behavior]\label{asmp:same_query}
            Both benign users and the attacker will send IID batch queries sampled from an absolutely continuous distribution $\mu$ on $\Real^d$. We denote IID batch query strategy as $\qiid$. 
            % The weights of utility loss (\Autoref{def:util}) are chosen as $w_i=1/n$ for $i=1,\dots,n$.
        \end{assumption}
        
        \begin{assumption}[Bounded Input Space] \label{asmp:bounded_input}
            The input space $\mathcal{X} = [0,1]^d$.
        \end{assumption}

        \begin{assumption}[Positive Density]\label{samp:density}
            The density of $\mu$ satisfies $\mu(x) \geq c$ for all $x \in [0,1]^d$ with some constant $c>0$.
        \end{assumption}

        \newcommand{\hld}{H}
        \textbf{Notations.} For two non-negative sequences $\{a_n, n=1,2,\ldots\}$ and $\{b_n, n=1,2,\ldots\}$, $a_n \lesssim b_n$ means that $\limsup_{n\to \infty} a_n/b_n < \infty$, also denoted as $a_n = O(b_n)$; $a_n \gtrsim b_n$ means $b_n \lesssim a_n$; $a_n \asymp b_n$ means both $a_n \lesssim b_n$ and $a_n \gtrsim b_n$; and $a_n=o(b_n)$ means $\lim_{n\to \infty} a_n/b_n=0$. A dot over $\lesssim, \gtrsim, \asymp$ means up to a polynomial of $\ln(n)$. We denote the expectation, probability, covariance, correlation, and indicator function as $\E$, $\P$, $\cov$, $\corr$, and $\ind_{(\cdot)}$, respectively. For a vector $\w=(w_1, \ldots, w_d)^\T \in \Real^d$, its $\ell_p$-norm ($p \geq 0$) is 
        \begin{align*}
            \norm{\w}_0 = \sum_{i=1}^d \ind_{w_i=0}, \quad 
            \norm{\w}_p = \biggl\{\sum_{i=1}^d \abs{w_i}^p\biggr\}^{1/p} \textrm{ for } p>0.
        \end{align*}
        A function $f$ is $(C,\alpha)$-H\"{o}lder continuous if $\abs{f(\X) - f(\X')} \leq C \norm{\X - \X'}_2^\alpha$ holds for any $\X$ and $ \X'$ on $\mathcal{X}$, where $C>0$ and $\alpha>0$. The $k$-th derivative of $f$ is denoted as $f^{(k)}$.
        For $q > 0$, the $L_q(\mu)$-norm of $f$ is 
        \begin{align*}
             \norm{f}_\infty = \esssup_{\x} \abs{f(\x)}, \quad 
 \norm{f}_q = \biggl(\int |f(\x)|^q\mu(d\x)\biggr)^{1/q}\textrm{ for } q>0 .
        \end{align*}  
        H\"{o}lder class $\hld(C,\alpha)$ is a collection of $(C,\alpha)$-H\"{o}lder continuous functions. 
        Lipschitz class $\hld(C,1)$ is a special H\"{o}lder class with $\alpha=1$. 
        For ease of reference, we have summarized the key notations used throughout the paper in \Autoref{tab:notation_1}. 

        \begin{table}[tb]
            \centering
            \begin{tabular}{lp{12.5cm}}
                \toprule
                \textbf{Symbol} & \textbf{Meaning} \\
                 \midrule
                $n$, $d$ & Sample size (total number of queries sent by the attacker), dimension of input $\X$\\ \midrule
                $\fd$, $\hat{f}_n$, $\fc$ & Defender's model, attacker's reconstructed model, and defender's function class\\ \midrule
                $\dm, \dmc$ & Defense mechanism, and its possible collection \\
                \midrule
                $\la, \lac$ & Attacker's learning algorithm, and its possible collection \\
                \midrule
                $\qs, \qsc$ & Attacker's query strategy, and its possible collection \\
                \midrule
                % $\si$ & Attacker's and defender's side information \\
                % \midrule
                % $\ell_A, \ell_D$ & Attacker's and defender's loss function \\
                % \midrule
                $\X_i$, $\Y_i$, $e_i$ & A particular query, its response, and corresponding perturbation\\ \midrule
                $Z_n$ & The query-response pairs $\{(\X_i, \Y_i),i=1,\dots,n\}$\\ \midrule
                $\pl_n(\dm,\fc \mid \qsc, \lac)$ & Privacy level of a defense mechanism against attack strategies $(\qsc, \lac)$ \\ \midrule
               $r_n(\qs, \la;\mid \fc,\dmc)$ & Stealing error of an attack strategy given defense mechanisms $\dmc$ \\ \midrule
                $\ul_n(\dm)$ & Utility loss of a defense mechanism \\ \midrule
                $\Ul_n$ & A given utility loss level  \\ 
                 \bottomrule
            \end{tabular}
            \caption{A summary table of frequently used notations.}
            \label{tab:notation_1}
        \end{table}