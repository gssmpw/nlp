\section{Related Work}
\subsection{Open-Vocabulary Detection}
Open-vocabulary detection (OVD) aims to generalize beyond the limited number of base classes labeled during the training phase and detect arbitrary classes. Radford et al., "Learning Transferable Visual Models"__ leverages cross-modal contrastive learning on large-scale image-text datasets to map text and images into the same embedding space, enabling zero-shot transfer to OV detection tasks. Chen et al., "Visually Explainable Object Detection with Masked Region Attention"__ is trained through visual and linguistic knowledge distillation, refining the knowledge obtained from CLIP inference into two-stage detectors to enhance zero-shot object detection. Wang et al., "Region-CLIP: A Region-Level Visual Representation for Open-Vocabulary Object Detection"__ extend CLIP to enable it to learn region-level visual representations, enhancing CLIP's ability in open-set object detection tasks. Inspired by self-supervised learning methods, Caron et al., "Emerging Properties in Self-Supervised Vision Transformers" and Caron et al., "DINO: Self-Supervised Learning of Dense Visual Representations"__ follow the design principles of tight modality fusion based on DINO____ and large-scale grounded pre-training for zero-shot transfer. Zhang et al., "YOLO-World: A Re-parameterizable Vision-Language Path Aggregation Network for Zero-Shot Object Detection"__ proposes a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and a region-text contrastive loss to enhance the interaction between visual and linguistic information, achieving reduced computational requirements without sacrificing performance. These approaches aim to fuse multimodal information through single-stage methods with a limited number of parameters, seeking to form end-to-end vision-language understanding capabilities. However, they struggle to address the modality alignment problem between complex textual descriptions and vision, which limits their zero-shot capabilities and results in poor language generalization. These models tend to underperform on datasets they haven't been specifically fine-tuned on, especially when it comes to understanding long sentences.

\subsection{Modality Information Fusion}
Accurate Open-Vocabulary Detection relies on effective multimodal information fusion, which requires precise alignment between visual and linguistic modalities. Radford et al., "Learning Transferable Visual Models"__ matches entire images with textual descriptions, but it fails to capture the fine-grained alignment between image regions and their corresponding textual descriptions. Chen et al., "Masked Region Proposal Network for Visually Explainable Object Detection" and Liang et al., "Visual-Linguistic Pre-training for Object Detection"__ achieve alignment between potential object regions and entire textual descriptions (often simple words representing categories) by introducing RPNs or class-agnostic proposal generators. However, these methods do not fully address the understanding of complex longer sentences, which remains a challenge in achieving a more nuanced and detailed alignment of vision and language. Zhang et al., "CoOp: Cooperative Prompt Engineering for Pre-trained Vision Language Models"__ recognized that subtle changes in textual descriptions can have a significant impact on the performance of vision language pre-training models. It introduced CoOp for prompt representation learning and automated prompt engineering specifically for pre-trained vision language models. Wang et al., "DetPro: Detecting Objects with Pre-trained Vision Language Models"__ incorporates CoOp into OV object detection tasks, enabling automatic learning of prompt representations in pre-trained vision language models based on positive and negative proposals. Guo et al., "TaskCLIP: A Two-Stage Approach for Zero-Shot Object Detection with Vision-Language Alignment"__ adopts a two-stage design, combining general object detection with VLM-based object selection. It improves alignment between object images and their visual attributes by employing a transformer-based aligner to recalibrate the embeddings of both modalities. The above methods for modality information alignment involve complex training and high resource demands, hindering real-world deployment.