%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{array}
\usepackage{url}
\usepackage{color}
% Comment out this line in the camera-ready submission
\linenumbers
\urlstyle{same}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\definecolor{mypink}{RGB}{227, 119, 194}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

% \title{MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering \\ (Supplementary Material)}

\title{MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering \\ (Appendix)}


% Single author syntax
% \author{
%     Caixiong Li, Xiongwei Zhao, Xing Zhang
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
% \author{
% Caixiong Li$^{1,4*}$ \and
% Xiongwei Zhao$^2$\footnote{Equal contribution} \and
% Jinhang Zhang$^3$ \and
% Xing Zhang$^{1,4}$\footnote{Corresponding author} \\
% \affiliations
% $^1$School of Computer and Information Science, Qinghai Institute of Technology\\
% $^2$School of Information Science and Technology, Harbin Institute of Technology (Shen Zhen)\\
% $^3$Harbin Institute of Technology\\
% $^4$School of Computer Science and Technology, Qinghai University\\
% \emails
% 2023740102@qhu.edu.cn,
% xwzhao@stu.hit.edu.cn,
% zhangjinhang169@gmail.com,
% 2023740048@qhu.edu.cn
% }
% \fi

\begin{document}

\maketitle

\section{Overview}
In this appendix, more model details, main experiment results, MQADet details, and qualitative results are provided, which are organized as follows:
\begin{itemize}
    \item Sec.~\ref{model_details} provides the specific models and corresponding checkpoints used in our experiments.
    \item Sec.~\ref{llava_result} presents additional main experiment results and analysis using LLaVA-1.5 as the MLLM.
    \item Sec.~\ref{MQADet_details} details the inputs and outputs of MQADet, along with the corresponding prompts for each phase, using a concrete example.
    \item Sec.~\ref{qualitative_results} offers additional qualitative results for GPT-4o and LLaVA-1.5, where our method achieves superior performance.
\end{itemize}

\section{Model Details}\label{model_details}

The specific models and corresponding checkpoints employed in the MQADet paradigm are presented in
Table~\ref{checkpoints}. In our experiments, all the open-source models are loaded directly from HuggingFace or GitHub repository. The detailed trained weights are selected as follows:

\begin{table}[h!]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\caption{Checkpoints for the different models.}
\label{checkpoints}
\setlength{\tabcolsep}{2pt}
\centering
    \begin{tabular}{cc}
    \toprule
    \textbf{Model} &  \textbf{Checkpoints}\\  
    \midrule
    Grounding DINO & groundingdino\_swint\_ogc.pth\\
    YOLO-World & \tabincell{c}{yolo\_world\_v2\_xl\_obj365v1\_goldg\\\_cc3mlite\_pretrain.pth}\\
    OmDet-Turbo & OmDet-Turbo\_tiny\_SWIN\_T.pth\\
    LLaVA-1.5 & liuhaotian/llava-v1.5-7b\\
    \bottomrule
    \end{tabular}
\end{table}

GPT-4o is not open-sourced, consequently, the models are not identifiable. The GPT-4o API released by OpenAI is utilized to evaluate all benchmark datasets.

\section{More Main Results}\label{llava_result}

\begin{table*}[htb!]
\caption{Results comparison between MQADet and state-of-the-art dectors on RefCOCO/+/g, and Ref-L4. MLLM employs LLaVA-1.5~\protect\cite{liu2024improved}, while object detectors utilize Grounding DINO~\protect\cite{liu2025grounding}, YOLO-World~\protect\cite{cheng2024yolo}, and OmDet-Turbo~\protect\cite{zhao2024real}. The evaluation metrics are Acc@0.5, Acc@0.25, and $\Delta$. Numbers in {\color{red}red} point out the improvement gains between detectors and MQADet.}
    \label{table2}
    \setlength{\tabcolsep}{1pt}
    \centering
    \begin{tabular}{cccccccccccccccc}
    \toprule
\multirow{2}{*}{\textbf{Mehtod}}  & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{4}{c}{\textbf{RefCOCO}}  & \multicolumn{4}{c}{\textbf{RefCOCO+}} & \multicolumn{3}{c}{\textbf{RefCOCOg}}  & \multicolumn{2}{c}{\textbf{Ref-L4}}\\ 
    \cmidrule(lr){3-6}\cmidrule(lr){7-10}\cmidrule(lr){11-13}\cmidrule(lr){14-15}   
    & & train & val & testA  & testB  & train  & val & testA & testB & train  & val  & test  & val  & test\\ 
    \midrule
    \midrule
\multirow{2}{*}{G-DINO}  
    &Acc@0.25 &  48.00&	48.95&	49.83&	40.50&	48.14&	49.66&	50.58&	43.51&	42.21&	40.76&	41.96&	17.40&	17.19\\ 
    \cmidrule{2-15} 
    & Acc@0.5 &  43.14&	42.85&	45.07&	36.69&	41.77&	41.56&	43.98&	37.51&	39.43&	38.18&	39.24&	16.66&	16.34\\ 
    \midrule
\multirow{4}{*}{Ours (G-DINO)}   
    &Acc@0.25&58.75&  53.92& 56.46& 52.26 & 55.16& 53.58 & 56.12& 51.64& 67.54& 66.05& 67.40&	54.40& 45.21\\ 
    & $\Delta$ & {\color{red}+10.75} &  {\color{red}+4.97}  & {\color{red}+6.63} &  {\color{red}+11.76} &{\color{red}+7.02} &  {\color{red}+3.92} & {\color{red}+5.54} &  {\color{red}+8.13} &{\color{red}+25.33} &  {\color{red}+25.29} &{\color{red}+25.44} &  {\color{red}+37.0}&  {\color{red}+28.02}\\
    \cmidrule{2-15} 
    & Acc@0.5 & 51.79 & 46.45& 51.33& 45.19& 47.16&	44.09 &	49.13&	43.03& 61.14 & 59.51&	61.15&	49.85 & 41.13 \\
    & $\Delta$ & {\color{red}+8.65} &  {\color{red}+3.6} & {\color{red}+6.26} &  {\color{red}+8.5} &{\color{red}+5.39} &  {\color{red}+2.53} &{\color{red}+5.15} &  {\color{red}+5.52} &{\color{red}+21.71} &  {\color{red}+21.33} &{\color{red}+21.91} &  {\color{red}+33.19} &{\color{red}+24.79} \\
    \midrule
    \midrule
\multirow{2}{*}{YOLO-World} & 
    Acc@0.25& 38.79& 38.15&	42.70&	32.97&	39.24&	37.82&	38.20&	35.32&	42.44&	40.11&	43.05&	28.76&	29.94\\ 
    \cmidrule{2-15} 
    & Acc@0.5  &34.09&	32.65&	38.36&	28.47&	33.56&	31.06&	33.77&	30.65&	38.43&	36.99&	38.51&	25.25&	26.56\\ 
    \midrule
\multirow{4}{*}{Ours (YOLO-World)} & 
    Acc@0.25  & 55.66&	46.63&	45.84&	43.22&	54.12&	51.72&	50.52&	45.29&	63.01&	50.31&	64.38&	53.13&	34.77\\
    & $\Delta$ & {\color{red}+16.87} &  {\color{red}+8.48} & {\color{red}+3.14} &
    {\color{red}+10.25} & {\color{red}+14.88} &
    {\color{red}+13.9} & {\color{red}+12.32} &
    {\color{red}+9.97} & {\color{red}+20.57} &{\color{red}+10.2} & {\color{red}+21.33} &
    {\color{red}+24.37} & {\color{red}+4.83} \\
    \cmidrule{2-15} 
    & Acc@0.5 &  48.97&	39.34&	40.71&	36.74&	45.87&	42.60&	44.76&	36.07&	56.49&	44.99&	56.88&	48.14&	31.42\\ 
    & $\Delta$ & {\color{red}+14.88} &  {\color{red}+6.69} & {\color{red}+2.35} &
    {\color{red}+8.27} & {\color{red}+12.31} &
    {\color{red}+11.54} & {\color{red}+10.99} &
    {\color{red}+5.42} & {\color{red}+18.06} &
    {\color{red}+8.0} & {\color{red}+18.37} &
    {\color{red}+22.89} & {\color{red}+4.86}\\
    \midrule
    \midrule
\multirow{2}{*}{OmDet-Turbo} & 
    Acc@0.25& 49.62& 48.87&	55.44&	41.38&	48.07&	46.84&	49.03&	44.09&	46.02&	42.81&	45.01&	32.29&	32.16\\ 
    \cmidrule{2-15} 
    & Acc@0.5&  46.53&	45.43&	52.76&	37.06&	44.57&	42.96&	46.03&	37.94&	40.79&	38.27&	39.07&	28.67&	28.95\\ 
    \midrule
\multirow{4}{*}{Ours (OmDet-Turbo)}  & 
    Acc@0.25 & 59.89&	58.17&	62.83&	50.88&	56.04&	54.05&	54.20&	47.95&	63.05&	66.67&	71.88&	52.83&	48.68\\ 
    & $\Delta$ & {\color{red}+10.27} &  {\color{red}+9.3} & {\color{red}+7.39} &{\color{red}+9.5} &{\color{red}+7.97} &{\color{red}+7.21} &{\color{red}+5.17} &{\color{red}+3.86} &{\color{red}+17.03} &{\color{red}+23.86} &{\color{red}+26.87} &
    {\color{red}+20.54} &{\color{red}+16.52} \\
    \cmidrule{2-15} 
    & Acc@0.5 & 53.65&	50.78&	57.52&	43.03&	49.54&	47.81&	50.87&	41.60&	56.35&	60.12&	62.60&	47.47&	42.95\\
    & $\Delta$ & {\color{red}+7.12} &  {\color{red}+5.35} & {\color{red}+4.76} &
    {\color{red}+5.97} & {\color{red}+4.97} &
    {\color{red}+4.85} & {\color{red}+4.84} &
    {\color{red}+3.66} & {\color{red}+15.56} &
    {\color{red}+21.85} & {\color{red}+23.53} &
    {\color{red}+18.8} & {\color{red}+14.0} \\
    \bottomrule
    \end{tabular}
\end{table*}

\paragraph{Performance on LLaVA-1.5.} To further validate the OVD capabilities and transferability of MQADet, we conducted additional evaluations using LLaVA-1.5 as the MLLM. Table~\ref{table2} compares MQADet, employing LLaVA as the MLLM, with state-of-the-art detectors on the RefCOCO, RefCOCO+, RefCOCOg, and Ref-L4 datasets. MQADet achieves the highest scores for both Acc@0.25 and Acc@0.5 across four benchmarks, significantly surpassing the performances of Grounding-DINO, YOLO-World, and OmDet-Turbo. The $\Delta$ results demonstrate substantial improvements across all datasets and detectors, underscoring the paradigm's effectiveness and generalizability. Notably, regardless of whether GPT or LLaVA is adopted as the MLLMs in the TASE stage and MOOS stage, MQADetâ€™s three-stage MQA mechanism enables detectors to utilize extracted subject cues, dealing with intricate textual descriptions, and focus on a broader range of unknown categories, thereby enhancing OV detection. Additionally, neither the MLLMs (GPT-4o and LLaVA) in the TASE and MOOS stage nor the detectors in the TMOP stage require training of costly, large models. Instead, they operate as plug-and-play components, making MQADet a highly effective solution for diverse real-world detection scenarios.


\section{MQADet Details}\label{MQADet_details}

To ensure the consistency and accuracy of the experimental results, the same prompts were used for the MLLMs, with GPT-4o codenamed as \textit{gpt4o} and LLaVA as \textit{llava-v1.5-7b}, in both TASE stage and MOOS stage of MQADet. The following subsections describe in detail the specific inputs and outputs of MQADet, as well as the corresponding prompts for each phase, using a concrete example.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.45\textwidth]{pic/s1.pdf}
\caption{Specific inputs, outputs, and corresponding prompts for the TASE stage.}
\label{s1}
\end{figure}  

\subsection{TASE Stage Details}\label{Prompt_satge1}

The input to the Text-Aware Subject Extraction (TASE) stage is a textual query with complex descriptions.
\begin{itemize}
    \item \textbf{Input}: The original text input of the user query. For example, \textit{"text input": "the tall green plant in the basket is standing near the woman in black top"}
\end{itemize}

In this stage, we use prompts, presented in Figure~\ref{s1}, to guide MLLMs (GPT-4o and LLaVA-1.5) in parsing and reasoning about the target subjects.
\begin{itemize}
    \item \textbf{Output}: Reasoned target subjects. For example, \textit{"subject": "plant ."}.
\end{itemize}

% \textbf{Prompts:} 

% You are an expert in subject extraction. I will give you an input description about the object positioning, and you need to extract the subject from this description to be sure we have the right answer. For example, "the player with the bat in hand", you need to return the real target subject of this description, "player", to me. Your answer template is: \{"Subject": "Write answer here, If there are multiple objects, you can use . to divide them. For example chair . person . dog ."\}


\subsection{TMOP Stage Details}\label{Prompt_satge3}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.45\textwidth]{pic/s2.pdf}
\caption{Specific inputs, outputs, and detectors for the TMOP stage.}
\label{s2}
\end{figure}  

The outputs of the Text-Guided Multimodal Object Positioning (TMOP) stage are the total number of candidate bounding box coordinates, the corresponding number marks that OV object detectors can detect, and the marked image obtained, as shown in Figure~\ref{s2}.

\begin{itemize}
    \item \textbf{Input}: The subject prompts and the original input image are provided. For example, the subject prompts could be: \textit{"subject": "plant."}.
    \item \textbf{Output}: The candidate bounding box coordinates ([x\_min, y\_min, x\_max, y\_max]), the corresponding number of marks, and the resulting marked image. For example: 
    \begin{itemize}  
        \item \textit{"pred\_{bbox}": [[92.34, 169.19, 412.26, 348.67], [0.51, 0.25, 252.76, 383.72]]}
        \item \textit{marked image}
    \end{itemize}
\end{itemize}


% \textbf{Prompts:} 

% You are an expert in object detection. I have marked the bounding box of the candidate object and its corresponding number in the input image. I will give you a description and you need to choose the detection object that best matches the description and answer the numeric label of this detection object to be sure we have the right answer. Your answer template is: \{"Subject": "Provide your answers as an array. If there are multiple objects, use commas to separate the numeric labels of the detection objects, such as [1, 3, 7]. If no object in the input image matches the description, return: There are no objects in the image that match the description."\}

\subsection{Prompts for MOOS Stage}\label{Prompt_satge3}

We take the original text input and the marked image as MLLMs-Driven Optimal Object Selection (MOOS) stage input.
\begin{itemize}
    \item \textbf{Input}:
    \begin{itemize}  
        \item \textit{"text input": "the tall green plant in the basket is standing near the woman in black top"}
        \item \textit{marked image}
    \end{itemize}
\end{itemize}

In this stage, we use prompts, presented in Figure~\ref{s3}, to utilize MLLMs (GPT-4o and LLaVA-1.5) in aligning the intricate textual description with the optimal visual targets.
\begin{itemize}
    \item \textbf{Output}: Final reasoned targets generated by MLLMs, such as \textit{"final\_target": "[1]"}.
\end{itemize}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.45\textwidth]{pic/s3.pdf}
\caption{Specific inputs, outputs, and corresponding prompts for the MOOS stage.}
\label{s3}
\end{figure}  

\section{More Qualitative Results}\label{qualitative_results}

Our MQADet paradigm achieves superior results compared with other state-of-the-art detectors. We exhibit the visualization results on GPT-4o and LLaVA-1.5 respectively.

\paragraph{Visualization on GPT-4o.} We compare the visualization results between MQADet and three state-of-the-art OV detectors (Grounding DINO, YOLO-World, and OmDet-Turbo) on RefCOCO, RefCOCO+, RefCOCOg, and Ref-L4 four benchmarks, with GPT-4o employed as the MLLM, as shown in Figure~\ref{gpt_visual}. Specifically, the first, third, and fifth columns display the predictions of Grounding DINO, YOLO-World, and OmDet-Turbo, respectively, while the remaining columns represent the results from our paradigm. The visualizations clearly illustrate that MQADet enables the detectors to focus on more categories by identified subject cues and leverages the vision-language reasoning capabilities of MLLM to better align fine-grained visual-textual information. These results stress the robust zero-shot detection capability of MQADet across different OV detection models on all benchmark datasets.

\begin{figure*}[htb!]
\centering
\includegraphics[width=1.0\textwidth]{pic/visual1.pdf}
\caption{Qualitative comparison between MQADet and three state-of-the-art OV detectors (Grounding DINO, YOLO-World, and OmDet-Turbo) on RefCOCO/+/g, and Ref-L4 datasets, with GPT-4o employed as the MLLM. \textcolor{mypink}{Pink} words denote the subjects identified from the user query.}
\label{gpt_visual}
\end{figure*}

\paragraph{Visualization on LLaVA-1.5.} We then present the visualization results with LLaVA-1.5 employed as the MLLM on three detectors across all datasets to verify the transferability of our paradigm, as shown in Figure~\ref{llava_visual}. The visualizations demonstrate that, regardless of whether GPT or LLaVA is adopted as MLLMs, MQADet effectively bridges the logic gap between perception and reasoning, addressing the challenges of complex visual-textual misalignment and significantly improving the detection accuracy of existing detectors in open-vocabulary scenarios.


\begin{figure*}[htb!]
\centering
\includegraphics[width=1.0\textwidth]{pic/visual2.pdf}
\caption{Qualitative comparison between MQADet and three state-of-the-art OV detectors (Grounding DINO, YOLO-World, and OmDet-Turbo) on RefCOCO/+/g, and Ref-L4 datasets, with LLaVA-1.5 employed as the MLLM. \textcolor{mypink}{Pink} words denote the subjects identified from the user query.}
\label{llava_visual}
\end{figure*}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

