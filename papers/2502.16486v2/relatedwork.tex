\section{Related Work}
\subsection{Open-Vocabulary Detection}
Open-vocabulary detection (OVD) aims to generalize beyond the limited number of base classes labeled during the training phase and detect arbitrary classes. CLIP~\cite{pmlr-v139-radford21a} leverages cross-modal contrastive learning on large-scale image-text datasets to map text and images into the same embedding space, enabling zero-shot transfer to OV detection tasks. ViLD~\cite{zhong2022regionclip} is trained through visual and linguistic knowledge distillation, refining the knowledge obtained from CLIP inference into two-stage detectors to enhance zero-shot object detection. Region-CLIP~\cite{zhong2022regionclip} extend CLIP to enable it to learn region-level visual representations, enhancing CLIP's ability in open-set object detection tasks. Inspired by self-supervised learning methods, Grounding DINO~\cite{liu2025grounding} follow the design principles of tight modality fusion based on DINO~\cite{zhang2022dino} and large-scale grounded pre-training for zero-shot transfer. YOLO-World~\cite{cheng2024yolo} proposes a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and a region-text contrastive loss to enhance the interaction between visual and linguistic information, achieving reduced computational requirements without sacrificing performance. These approaches aim to fuse multimodal information through single-stage methods with a limited number of parameters, seeking to form end-to-end vision-language understanding capabilities. However, they struggle to address the modality alignment problem between complex textual descriptions and vision, which limits their zero-shot capabilities and results in poor language generalization. These models tend to underperform on datasets they haven't been specifically fine-tuned on, especially when it comes to understanding long sentences.

\subsection{Modality Information Fusion}
Accurate Open-Vocabulary Detection relies on effective multimodal information fusion, which requires precise alignment between visual and linguistic modalities. CLIP~\cite{pmlr-v139-radford21a} matches entire images with textual descriptions, but it fails to capture the fine-grained alignment between image regions and their corresponding textual descriptions. MEDet~\cite{chen2022open} and VL-PLM~\cite{zhao2022exploiting} achieve alignment between potential object regions and entire textual descriptions (often simple words representing categories) by introducing RPNs or class-agnostic proposal generators. However, these methods do not fully address the understanding of complex longer sentences, which remains a challenge in achieving a more nuanced and detailed alignment of vision and language. CoOp~\cite{zhou2022learning} recognized that subtle changes in textual descriptions can have a significant impact on the performance of vision language pre-training models. It introduced CoOp for prompt representation learning and automated prompt engineering specifically for pre-trained vision language models. DetPro~\cite{du2022learning} incorporates CoOp into OV object detection tasks, enabling automatic learning of prompt representations in pre-trained vision language models based on positive and negative proposals. TaskCLIP~\cite{chen2024taskclip} adopts a two-stage design, combining general object detection with VLM-based object selection. It improves alignment between object images and their visual attributes by employing a transformer-based aligner to recalibrate the embeddings of both modalities. The above methods for modality information alignment involve complex training and high resource demands, hindering real-world deployment.