

\section{Conclusion}

In this work, we introduced a real-time operator takeover (RTOT) paradigm to train visuomotor diffusion policies. This framework enables users to seamlessly take control of the robot from a suboptimal policy, preventing failures during execution. These takeover demonstrations are subsequently integrated into the training process, iteratively refining the policy until the desired performance is achieved.

Our experimental evaluation demonstrates the effectiveness of this approach. Despite the fact that the total time of takeover demonstrations is significantly shorter than that of the initial demonstrations, policies trained on these targeted datasets consistently outperform their counterparts. In a cyclic rice scooping task, policies enhanced with takeover demonstrations achieved superior task performance, highlighting the value of addressing failure cases during policy deployment rather than relying solely on extensive initial datasets.

Beyond showcasing the utility of the RTOT framework, we also explored the use of the Mahalanobis distance as a tool for detecting out-of-distribution (OOD) states during inference. Our analysis reveals that while the Mahalanobis distance can successfully identify undesirable states, such as those that lead to the robot becoming stuck, it is not an exact predictor of final task performance. In particular, although the Mahalanobis distance helps measure deviations from the training distribution, it does not always directly correlate with the quantity of rice scooped in our experiments. Nevertheless, these insights underline the potential of the Mahalanobis distance for enhancing policy robustness by flagging critical states during deployment.

\textbf{Contributions and Insights.}

Real-Time Operator Takeover Paradigm: We demonstrated how the RTOT framework can improve policy performance by iteratively addressing failure scenarios. The targeted nature of takeover demonstrations provides valuable training data, focusing specifically on challenging states that are often missing from initial demonstrations.

Efficiency of Takeover Demonstrations: Our results highlight the efficiency of this approach, with takeover-enhanced policies achieving better performance despite using significantly less total demonstration time. This finding underscores the importance of quality and relevance in training data over sheer quantity.

Out-of-Distribution Detection: We provided a detailed analysis of the Mahalanobis distance as an OOD measure, showing its utility in detecting undesirable states during policy execution. This metric can serve as a foundation for future automated recovery mechanisms in robot control systems.


In conclusion, the RTOT paradigm represents a significant step toward more efficient and adaptive robot learning systems. By enabling real-time human intervention and leveraging targeted demonstrations, this approach provides a practical and scalable solution to address the inherent challenges of visuomotor policy training, particularly in complex and unpredictable task environments.