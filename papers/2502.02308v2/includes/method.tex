

\begin{figure*}[ht] \centering \includegraphics[width=0.99\textwidth]{imgs/takeover_frames.png} \caption{Real-Time Operator Takeover in action: The policy $\pi_I$ controls the robot until a state is reached where the operator must take over to avoid spilling rice on the table. After the operator completes the intervention (depositing the rice in the bowl), control is seamlessly returned to $\pi_I$.} \label{fig:takeover_frames} \end{figure*}

\section{Methodology} \label{sec
}

We now describe our framework for enabling real-time operator takeover to improve visuomotor diffusion policy training.

The core concept, illustrated in Fig.\ref{fig:takeover}, involves collecting an initial set of demonstrations ($\mathcal{D}_I$) to train an initial policy ($\pi_I$). The policy is deployed in the environment while the operator remains on standby, ready to takeover if the system approaches an undesirable state. In such cases, the operator can take control in real-time by pressing a button on the VR controller. The operator intervenes to prevent the robot from spilling rice (fourth frame), guides it to successfully deposit the rice into the bowl, and then returns the control back to the policy. These takeover interventions are recorded as new demonstrations and a subsequent policy $\pi_{IT}^1$ is trained using the combined dataset of initial and takeover demonstrations. This iterative process ensures that the policy continuously improves by addressing failure cases encountered during execution, rather than relying solely on the initial demonstrations of the operator.

\subsection{Real-Time Takeover} A visuomotor diffusion policy~\cite{chi2023diffusion} is initially trained on a demonstration dataset $\mathcal{D}_I$. The demonstrations are collected by teleoperating the robot using the Quest2ROS~\cite{welle2024quest2ros} application, which relays the VR controller’s velocities to a Cartesian velocity controller on the robot. The operator controls the robot by pressing the side trigger button on the VR controller, and only actions executed while the trigger is pressed are recorded as $6$D end-effector velocities. Any pauses in operation—where the trigger is released, allowing the operator to reposition their hand or stop the robot—are omitted from the recorded data. This ensures a dataset without idle information.

Once the initial policy $\pi_I$ is trained, it is deployed on a robot by generating $6$D velocity commands. While the policy operates, a ring buffer continuously records observations. Each observation includes the end-effector camera’s RGB view and proprioceptive information such as the robot's pose. If the operator observes the policy moving toward an undesirable state (e.g., spilling rice), they press the VR controller’s trigger button to intervene. When this occurs, the takeover actions are immediately applied to the robot, overriding the policy-generated commands.

Importantly, the contents of the ring buffer (representing the sequence of observations leading up to the intervention) are also saved as part of the new takeover demonstration. This ensures that the collected data includes context from moments just before the operator's takeover, providing the policy with more nuanced information about failure recovery. While the operator continues teleoperating the robot, subsequent observations and actions are recorded until the operator deems the system to be in a desirable state again. At this point, releasing the trigger seamlessly hands control back to the policy, which resumes generating actions.

This targeted approach allows the operator to focus only on correcting problematic scenarios, avoiding redundant data collection for states already well-handled by the policy. The result is a compact and highly relevant dataset that incrementally addresses failure cases as they arise. A detailed schematic of this process is illustrated in Fig.~\ref{fig:takeover}.

\begin{figure}[t] \centering \includegraphics[width=0.99\linewidth]{imgs/takeover.png} \caption{Illustration of the Real-Time Takeover process: While the policy sends actions, observations (end-effector RGB view and robot pose) are continuously stored in a ring buffer. When the operator takes control using the VR controller, these ring buffer observations, along with subsequent data, are recorded as a new demonstration in $\mathcal{D}_T$.} \label{fig:takeover} \end{figure}

Through this iterative process, our RTOT paradigm ensures that the dataset evolves to cover a wide range of scenarios, including recovery from failure cases. This targeted, incremental improvement results in a more robust and capable policy compared to traditional imitation learning approaches.
