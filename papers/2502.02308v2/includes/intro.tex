\section{Introduction}

Imitation learning (IL) has a great potential to automate complex robotic manipulation tasks. Successful examples include many domestic tasks such as cooking shrimp and wiping wine~\cite{fu2024mobile}, serving rice and opening bottles using a bottle opener~\cite{ingelhag2024robotic}, and $6$Dof mug flipping, sauce pouring, and spreading~\cite{chi2023diffusion}.

An important ingredient of IL methods, such as Visuomotor Diffusion Policies~\cite{chi2023diffusion}, Action Chunking with Transformers~\cite{zhao2023learning}, or Visual Imitation through Nearest Neighbor~\cite{pari2021surprising}, are the expert demonstrations that serve as training data, calling for high quality. One approach to enhance the quality of demonstrations is to minimize the noise~\cite{wang2023imitation} after the data has been collected. Another common approach is to facilitate the collection of high-quality demonstrations using various teleoperation methods, such as virtual reality controllers or hand tracking~\cite{moletta2023virtual, welle2024quest2ros, iyer2024open, qin2023anyteleop}, augmented reality~\cite{van2024puppeteer, chen2024arcap}, or leader-follower puppeteering approaches~\cite{zhao2023learning, shaw2024bimanual, yang2024ace}.

\begin{figure} \centering \includegraphics[width=0.99\linewidth]{imgs/fig1.png} \caption{Real-Time Operator Takeover paradigm: after training a policy with a small number of initial demonstrations, we run the policy in the environment with the operator on standby. As soon as the policy enters an undesirable state, the operator seamlessly takes over, and only the takeover portion is recorded as new demonstrations. A new policy is trained, and the paradigm can be repeated until the desired performance is achieved.} \label{fig:fig1} \end{figure}
%\vspace{-\baselineskip}

While teleoperation systems can boost the performance of IL methods, they do not address the fundamental problem of how to cover sufficient variations in demonstrations/tasks to make the learned policy more robust. Enhancing robustness in IL is an active area of research~\cite{hansen2021generalization, zhuang2024enhancing}. Although existing approaches improve the handling of scene variations, they still struggle when encountering states that are not present in the training data. Policies often fail in such scenarios, getting stuck due to the lack of demonstrations for recovering from undesirable states.

In this work, we tackle this challenge with our Real-Time Operator Takeover (RTOT) paradigm. The core idea is that even the most skilled operator may struggle to anticipate and demonstrate recovery scenarios or failure cases the policy might encounter. Our RTOT approach addresses this limitation by first training an initial policy with a smaller set of demonstrations than typically used in the literature. This policy is then deployed in the environment while the operator remains on standby. When the operator observes the system entering or approaching an undesirable state, they seamlessly take control to teleoperate the robot back to a desirable state. This \textit{takeover} demonstrations are recorded, and a new policy is trained using both the initial and the newly recorded takeover demonstrations. As shown in Fig.~\ref{fig:fig1}, this paradigm can be repeated as many times as necessary to achieve the desired performance for any given task. Our contributions are as follows: \begin{enumerate} \item Introduction and validation of the RTOT paradigm on a real-world rice scooping task. \item Analysis of the Mahalanobis distance (a measure of distance from a point to a distribution) and its potential applications in imitation learning-based methods. \item Extensive videos showcasing the initial demonstration collection, the takeover demonstration collection, and all $50$ rice scooping experiments, available on the project website\usefootref{fn:website}. \end{enumerate}