

\section{Experimental Setup} Our primary goal is to investigate whether training visuomotor policies using real-time operator takeover leads to improved task performance.

We evaluate this through a cyclic rice scooping task, as shown in Fig.~\ref{fig:fig1}. The objective is to scoop rice from the bowl on the left (positioned in the white rectangle) and deposit it into the bowl on the right (which can be placed anywhere within its respective rectangle). To address the cyclic nature of the task, the initial demonstrations are designed to start and end at approximately the same position after successfully completing one scoop.

To begin, we collect an initial dataset $\mathcal{D}_I^{60}$ consisting of 60 expert demonstrations. These demonstrations are performed while varying the locations of the rice bowls to introduce diversity in the training data. From this comprehensive dataset, we extract two smaller subsets: the first 20 demonstrations form $\mathcal{D}_I^{20}$, and the first 40 demonstrations form $\mathcal{D}_I^{40}$. Using these datasets, we train the corresponding policies: $\pi_I^{20}$, $\pi_I^{40}$, and $\pi_I^{60}$.

Next, we deploy $\pi_I^{20}$ in the rice scooping environment as described in Section~\ref{sec}. During this deployment, we collect 20 additional takeover demonstrations, denoted as $\mathcal{D}T^{20a}$, while systematically varying the position of the rice bowl. These takeover demonstrations are then combined with the initial dataset $\mathcal{D}I^{20}$ to form an augmented dataset $\mathcal{D}{IT}^{a}$, which is used to train a new policy, $\pi{IT}^{a}$.

This new policy, $\pi_{IT}^{a}$, is subsequently deployed again in the rice scooping environment to collect an additional 20 takeover demonstrations, $\mathcal{D}T^{20b}$. The final dataset, $\mathcal{D}{IT}^{b}$, comprises the initial demonstrations and both sets of takeover demonstrations: $\mathcal{D}_{IT}^{b} = \mathcal{D}_I^{20} \cup \mathcal{D}_T^{20a} \cup \mathcal{D}_T^{20b}$. The corresponding policy trained on this dataset is the final policy evaluated in this study.

\textbf{Analysis of Dataset Lengths.}
We analyze the lengths of the different datasets used to train the policies. Fig.~\ref{fig:demo_length} shows a violin plot illustrating these differences. Notably, the initial datasets $\mathcal{D}_I^{20}$, $\mathcal{D}_I^{40}$, and $\mathcal{D}I^{60}$ have similar mean demonstration times of $13.6s$, $14.4s$, and $14.5s$, respectively, with total demonstration times of $272.0s$, $574.3s$, and $869.0s$. In contrast, the takeover dataset $\mathcal{D}{IT}^{a}$ achieves a mean demonstration time of $9.5s$, resulting in a total demonstration time of $377.8s$, which is $34\%$ shorter than $\mathcal{D}_I^{40}$ despite containing the same number of demonstrations.

This efficiency is even more pronounced in the final dataset, $\mathcal{D}_{IT}^{b}$, which has the shortest mean demonstration time of $7.76s$ and a total demonstration time of $465.6s$—a reduction of $46\%$ compared to its non-takeover counterpart, $\mathcal{D}_I^{60}$. These results demonstrate the efficiency of the takeover paradigm in generating compact and targeted training datasets without sacrificing task performance.

Additionally, the combination of initial and takeover demonstrations helps to address failure cases more effectively, since takeover demonstrations specifically focus on challenging scenarios encountered during policy deployment. This iterative process not only improves policy robustness but also reduces the overall training time required to achieve high task performance.

Videos showcasing initial demonstrations, takeover demonstrations, and task evaluations are available on the project’s website\usefootref{fn:website}.

\begin{figure}[t] \centering \includegraphics[width=0.99\linewidth]{imgs/demo_length.png} \caption{Demonstration lengths for the datasets used to train the evaluated visuomotor policies. The takeover paradigm produces significantly shorter demonstrations on average, highlighting its efficiency.} \label{fig:demo_length} \end{figure}








\section{Experimental Evaluation}

We evaluate the performance of five different policies ($\pi_I^{20}, \pi_I^{40}, \pi_{IT}^a, \pi_I^{60}, \pi_{IT}^b$) in the cyclic rice scooping task. The goal of the task is to transfer as much rice as possible from a full bowl to an empty bowl within $45$ seconds, measured in grams. For each policy, we conduct $10$ trials, each with different placement locations for the bowls. Videos of all $50$ experiments are available on the project website.

The detailed results of these experiments are shown in Fig.\ref{fig:results}, and the mean and standard deviation (std) of the scooped rice for each policy are summarized in Table\ref{tab:scoop_mean}.

\begin{figure}[t] \centering \includegraphics[width=0.99\linewidth]{imgs/rice_scooping.png} \caption{Detailed results of the cyclic rice scooping experiments. The amount of rice (in grams) is shown for each of the $10$ trials across all five evaluated policies.} \label{fig:results} \end{figure}



\begin{table}[h!]
\label{tab:scoop_mean}
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model}   & \textbf{\# Demos} & \textbf{Mean $\pm$ std [g]} \\ \hline \hline
$\pi_I^{20}$        & $20$                         & $4.0 \pm 4.0$                           \\ \hline \hline
$\pi_I^{40}$        & $40$                         & $19.8 \pm 12.9$                         \\ \hline
$\pi_{IT}^{a}$          & $40$                         & $\mathbf{35.4 \pm 11.0}$                         \\ \hline \hline
$\pi_I^{60}$         & $60$                         & $41.0 \pm 13.5$                         \\ \hline
$\pi_{IT}^{b}$          & $60$                         & $\mathbf{49.2 \pm 9.4}$                           \\ \hline
\end{tabular}
\caption{Means and Standard Deviations of the rice scooped in $45$ second over the $10$ trails.}
\end{table}

\textbf{Results and Analysis.}
The results reveal a clear trend: the performance of the baseline policies improves with the number of initial demonstrations. For example, $\pi_I^{20}$, trained on only $20$ demonstrations, achieved an average of $4.00$ grams of rice scooped across $10$ trials. Increasing the data set to $40$ demonstrations significantly increased the performance, with $\pi_I^{40}$ averaging $19.80$ grams. Using all $60$ initial demonstrations, $\pi_I^{60}$ achieved a substantial improvement, scooping an average of $41.00$ grams per trial. This highlights the importance of larger datasets to improve baseline performance in visuomotor policy training.

To examine whether real-time operator takeover paradigm enhances task performance, we compare the performance of the baseline $\pi_I^{40}$ with that of the takeover-enhanced policy $\pi_{IT}^a$. Remarkably, $\pi_{IT}^a$ achieved an average of $35.40$ grams scooped, representing a $79\%$ improvement over $\pi_I^{40}$. Similarly, the final policy, $\pi_{IT}^b$, trained on both initial and two sets of takeover demonstrations, outperformed $\pi_I^{60}$ by $20\%$, with an average of $49.20$ grams scooped per trial.

\textbf{Efficiency of Takeover Demonstrations.}
These results are particularly noteworthy given that the total demonstration time for the takeover datasets is significantly lower. For example, the total demonstration time for $\mathcal{D}_{IT}^b$ is $465.6$ seconds, which is $46\%$ shorter than the total time for $\mathcal{D}I^{60}$ ($869.0$ seconds). Despite this reduction, $\pi{IT}^b$ outperforms the baseline trained on the full dataset of initial demonstrations. This indicates that the targeted nature of takeover demonstrations, focusing on failure cases, provides more valuable information for policy training compared to redundant initial demonstrations.

\textbf{Key Insights.}
These findings highlight two important aspects. First, redundant demonstrations contribute only marginally to further performance improvement once the policy has mastered the easier portions of the task. Second, it is extremely challenging to anticipate all potential failure cases during the initial demonstration phase. The real-time operator takeover paradigm addresses these challenges by iteratively improving the policy. By taking control only when the system encounters undesirable states, the operator provides precise, context-specific demonstrations that were missing from the initial dataset.



\section{To Takeover or Not to Takeover}

In the current framework, the decision to initiate a takeover rests solely with the operator. We argue that this is a sensible approach, given that the quality of the initial demonstrations—and more broadly, the overall task performance—is highly influenced by the quality of these demonstrations~\cite{pari2021surprising}. However, having a more systematic way of assessing how close the robot is to undesirable states, or detecting when such states occur, could provide significant benefits.

Recall that we assume all expert demonstrations represent desirable states. The Denoising Diffusion Probabilistic Model (DDPM) employed here is trained to minimize the KL-divergence between the conditional distribution $p(A_t|O_t)$—where actions $A$ and observations $O$ are derived from the expert demonstrations—and the distribution of samples generated by the DDPM. Intuitively, this means the policy struggles to produce meaningful actions if the current observations (or more precisely, the embeddings of those observations) differ significantly from the data used in training. Improving the robustness of visuomotor diffusion policies against such variations in observations remains an active area of research~\cite{wang2023imitation, zhuang2024enhancing}.

This motivates the need for a metric that evaluates how far observations obtained during inference deviate from the training distribution. This metric could serve as an out-of-distribution (OOD) measure to better inform intervention decisions.

\subsection{Mahalanobis Distance as an OOD Measure}

The Mahalanobis distance provides a statistical measure of the distance between a point and a distribution, accounting for correlations between variables. It is computed as:

\begin{equation} d_M = \sqrt{(x - \mu)^\top \Sigma^{-1} (x - \mu)} \end{equation}

Here, $\mu$ is the mean vector of the distribution, $\Sigma$ is the covariance matrix, and $x$ is the queried data point.

Given the high dimensionality of the RGB images, we instead compute embeddings of the images concatenated with the robot pose. Each observation is encoded into a $1056$-dimensional vector (two $512$-dimensional embeddings for the images and two $16$-dimensional embeddings for the pose). These embeddings serve as the conditioning input for the diffusion model.

To evaluate the Mahalanobis distance, we first encode all datasets $\mathcal{D}I^{20}$, $\mathcal{D}I^{40}$, $\mathcal{D}I^{60}$, $\mathcal{D}{IT}^a$, and $\mathcal{D}{IT}^b$ using their respective trained models ($\pi_I^{20}$, $\pi_I^{40}$, $\pi_I^{60}$, $\pi{IT}^a$, and $\pi_{IT}^b$). This produces embeddings $Z_I^{20}$, $Z_I^{40}$, $Z_I^{60}$, $Z_{IT}^a$, and $Z_{IT}^b$. Similarly, all observations recorded during the experiments ($\mathcal{E}$) are encoded using these models, resulting in $H_I^{20}$, $H_I^{40}$, $H_I^{60}$, $H_{IT}^a$, and $H_{IT}^b$.

We estimate the mean ($\mu$) and covariance matrix ($\Sigma$) of each embedding set using the Minimum Covariance Determinant Estimator implemented in scikit-learn~\cite{scikit-learn}. For each encoded observation $h \in H$, the Mahalanobis distance $d_M$ is computed with Algorithm~\ref{alg:mdist}.


\begin{algorithm}
\caption{Mahalanobis Distance Calculation}
\label{alg:mdist}
\begin{algorithmic}[1]
    \State \textbf{Input:} Datasets $\mathcal{D}_I^{20}$, $\mathcal{D}_I^{40}$, $\mathcal{D}_I^{60}$, $\mathcal{D}_{IT}^a$, $\mathcal{D}_{IT}^b$, 
    \Statex \hspace{2em} trained models $\pi_{I}^{20}$, $\pi_{I}^{40}$, $\pi_{I}^{60}$, $\pi_{IT}^a$, $\pi_{IT}^b$, 
    \Statex \hspace{2em} experimental observations $\mathcal{E}$.
    \State \textbf{Output:} Mahalanobis distances $d_m$ for each observation encoding $h \in H$ with respect to each embedding set.

    \State \textbf{Step 1: Encode Dataset Embeddings} 
    \State     \[
    Z_I^{20} = \pi_{I}^{20}(\mathcal{D}_I^{20}), \quad Z_I^{40} = \pi_{I}^{40}(\mathcal{D}_I^{40}), \quad Z_I^{60} = \pi_{I}^{60}(\mathcal{D}_I^{60}), 
    \]
    \[
    Z_{IT}^a = \pi_{IT}^a(\mathcal{D}_{IT}^a), \quad Z_{IT}^b = \pi_{IT}^b(\mathcal{D}_{IT}^b)
    \]

    \State \textbf{Step 2: Encode Experimental Observations}
    \State     \[
    H_I^{20} = \pi_{I}^{20}(\mathcal{E}), \quad H_I^{40} = \pi_{I}^{40}(\mathcal{E}), \quad H_I^{60} = \pi_{I}^{60}(\mathcal{E}), 
    \]
    \[
    H_{IT}^a = \pi_{IT}^a(\mathcal{E}), \quad H_{IT}^b = \pi_{IT}^b(\mathcal{E})
    \]

    \State \textbf{Step 3: Calculate Mahalanobis Distances}
    
    \For{each  $Z \in \{Z_I^{20}, Z_I^{40}, Z_I^{60}, Z_{IT}^a, Z_{IT}^b\}$ and $H \in \{H_I^{20}, H_I^{40}, H_I^{60}, H_{IT}^a, H_{IT}^b\}$}
        \State $\mu_Z, \Sigma_Z = \texttt{MinCovDet}(Z)$
            \For{each encoding $h \in H$}
                \State
                $d_m(h, \mu_Z, \Sigma_Z) = \sqrt{(h - \mu_Z)^T \Sigma_Z^{-1} (h - \mu_Z)}$
            \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Mahalanobis Distance Analysis}

The summarized results of Algorithm~\ref{alg:mdist} are presented as violin plots in Fig.~\ref{fig:m_distance_violine}.

\begin{figure}[t] \centering \includegraphics[width=0.99\linewidth]{imgs/m_distance_violine.png} \caption{Violin plots of Mahalanobis distances for experimental observations compared to the learned distribution for each dataset.} \label{fig:m_distance_violine} \end{figure}

We observe that $\mathcal{D}_I^{20}$, the dataset with the highest average Mahalanobis distance, corresponds to the worst-performing policy, $\pi_I^{20}$. This suggests that the Mahalanobis distance provides some indication of how far an observation during rollout deviates from the training distribution. Adding more demonstrations to the training set reduces the average Mahalanobis distance of observations gathered during experiments.

However, an interesting anomaly is observed when comparing $\mathcal{D}I^{40}$ to $\mathcal{D}{IT}^a$ or $\mathcal{D}I^{60}$ to $\mathcal{D}{IT}^b$: the mean Mahalanobis distance does not directly correlate with task performance. For example, $\pi_{IT}^b$ outperforms $\pi_I^{60}$ in rice scooping but has a similar Mahalanobis distance profile.

\textbf{Case Study: Trial 3.}
To explore this further, we analyze a specific trial in Fig.~\ref{fig:mdistancet03}. For Trial number three, we compare the Mahalanobis distances of policies $\pi_I^{20}$, $\pi_{IT}^b$, and $\pi_I^{60}$. 
We observe that $\pi_I^{20}$ starts with the highest distance while $\pi_{IT}^b$ and $\pi_I^{60}$ share similar values (around $40$). The optimal task performance model, $\pi_{IT}^b$, exhibits a slight increase in distance as the trial progresses. However, none of the encountered states necessitate takeover, and the distance consistently stays below $100$ throughout the duration of the trial.

The poorly performing policy $\pi_I^{20}$ exhibits the highest Mahalanobis distance peaking at around $160$, around the $25s$ mark, when the robot's spoon moves outside the target bowl and fails to recover. The robot remains stuck in this state until the trial ends, highlighting the relationship between high Mahalanobis distance and undesirable states.
In contrast, both $\pi_I^{60}$ and $\pi_{IT}^b$ maintain relatively lower Mahalanobis distances throughout the trial, with no significant increases. While $\pi_{IT}^b$ outperforms $\pi_I^{60}$ in task performance, the Mahalanobis distance does not differentiate much between the two policies in this regard. We also observe that the sudden stops, and consequently the camera shaking, significant impact as evidenced by the last peak in $\pi_{IT}^b$ (orange) and $\pi_I^{60}$ (green) around the 45-second mark, coinciding with the abrupt halt of the robot as the experiment concluded. Additionally, the result indicates that the distance remains constant as long as there are no changes in the image caused by motion.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{imgs/mdistance_trail_3.png}
    \caption{The Mahalanobis distance of the embedded observations of trail $\#3$ of the models $\pi_I^{20}, \pi_{IT}^b,$ and $\pi_I^{60}$, with the RGB views shown for specific timesteps. }
    \label{fig:mdistancet03}
\end{figure*}

\textbf{Key Insights}
While the Mahalanobis distance may not perfectly predict task performance (e.g., rice scooped), it successfully identifies undesirable states that lead to performance degradation, such as the robot becoming stuck. This insight highlights the potential for developing automated recovery strategies based on significant increases in Mahalanobis distance. Additionally, these findings provide a deeper understanding of the role of demonstrations in improving policy robustness, underscoring the value of diverse and targeted training datasets.