\section{Background \& Related Work}
We structure the related work along the areas of visuomotor diffusion policies, imitation learning with focus on out-of-distribution detection and human-in-the loop learning systems.
\subsection{Visuomotor Diffusion Policies}

Diffusion models~\cite{ho2020denoising}, initially introduced in generative image modeling, have recently gained popularity in robotics and have been demonstrated on a variety of tasks. Compared to traditional discriminative models, diffusion models demonstrate remarkable generalization capabilities~\cite{li2023onthe}. Fundamentally, visuomotor diffusion-based policy models generate complex structured data distributions iteratively, by learning a stochastic transport map from a known prior distribution (e.g. Gaussian noise) to a desired target distribution. In robotics, the target distribution often consists of action sequences that enable a robot to accomplish specific tasks. 

A key strength of diffusion models lies in their ability to learn behaviors using a limited number of demonstrations~\cite{chi2023diffusion}, as well as requiring minimal explicit modeling of tasks or environments, making these models highly adaptable for a wide range of applications.  Recent work has leveraged diffusion models to address planning problems~\cite{kapelyukh2023dallebot, mishra2023generative, sridhar2024nomad} and perform various robotic manipulation tasks~\cite{mishra2024reorientdiff, chi2023diffusion, reuss2023goal}, often relying on images as input. Other applications include control of bi-manual mobile manipulation systems for navigating indoor environments and performing kitchen tasks~\cite{fu2024mobile}, as well as manipulating deformable objects in robot-assisted surgical scenarios~\cite{scheikl2024movement}.


\subsection{Imitation Learning and Out-of-Distribution Detection}

Imitation learning enables robots to acquire skills from expert demonstrations~\cite{pomerleau1998alvinn} by framing the problem as supervised learning from observations to actions. Recent advancements have incorporated historical context~\cite{mandlekar2021matters, shafiullah2022behaviour, brohan2023rt1, jang2022bcz}, alternative training objectives~\cite{florence2022implicit, pari2021surprising}, multi-task and few-shot learning~\cite{duan2017oneshot, james2018task, dasari2021transformers}, and language-conditioned policies for semantic understanding~\cite{shridhar2023perceiver}. These methods have improved generalization to task variations~\cite{brohan2023rt1, kim2024goal, jang2022bcz} and enabled fine-manipulation on low-cost hardware~\cite{zhao2023learning}. A key challenge in imitation learning is the compounding error~\cite{ross2010efficient}, which leads robots to difficult-to-recover, out-of-distribution (OOD) states~\cite{ross2011reduction, tu2022sample}. To mitigate this, various strategies have been explored, including on-policy expert corrections in DAgger~\cite{ross2011reduction} and its variants~\cite{kelly2019hgdagger, menda2019ensembledagger}, reward function modifications~\cite{kumar2020conservative, yu2021combo}, offline synthetic data generation~\cite{florence2020self, ke2021grasping, zhou2023nerf}, and training of recovery policies~\cite{reichlin2022manifold}. However, expert interventions can be time-consuming and impractical with current teleoperation interfaces~\cite{ke2021grasping}, motivating the proposed seamless real-time takeover paradigm.

Despite efforts in OOD state detection~\cite{yang2024generalized, sinha2022system}, diffusion-based models~\cite{chi2023diffusion} generally do not integrate these mechanisms and rather rely only on demonstrations which extensively cover the training manifold. A critical limitation remains in detecting distribution shifts during deployment while demonstrations are still being collected. To address this, we propose a human-in-the-loop framework that enables seamless intervention during OOD occurrences, facilitating targeted data collection.



\subsection{Human-in-the-Loop and Real-Time Takeover}


Human-robot interaction (HRI) has advanced considerably in recent years, fundamentally transforming how humans and robots collaborate and coexist~\cite{obaigbena2024ai}. These developments are largely driven by machine learning and the integration of multimodal data~\cite{wang2024multimodal}. In robotic manipulation, teleoperation enables natural human control of robots, leveraging human cognitive abilities and domain expertise alongside the physical capabilities of humanoid robots~\cite{darvish2023teleoperation}.

One prevalent class of methods for teleoperation employs virtual-, augmented-, and mixed-reality (VAM) frameworks, which serve as a common interface between human and robot. These frameworks have been applied to diverse areas in robotics, including motion planning and HRI~\cite{makhataeva2020augmented}, or often used to control robot manipulators via position or velocity control~\cite{barentine2021vr, xu2022shared}. Applications include object handover~\cite{ortenzi2022robot}, visualizing robot intentions during delivery tasks~\cite{chandan2021arroch}, and executing cloth manipulation tasks~\cite{moletta2023virtual}.

To the best of our knowledge, however, there is virtually no work in the realm of HRI for robotic manipulation that explicitly permits a human to seamlessly assume control to correct the robot's movement while simultaneously recording the new trajectory to make it available for training. Although some studies have employed takeover request (TOR) frameworks in autonomous driving ~\cite{lindemann2019exploring} or other have implemented action corrections to ensure successful task completion~\cite{moletta2023virtual, ma2025human}, these approaches do not incorporate the detection of out-of-distribution (OOD) states or the explicit recording of the takeover as a new demonstration. This motivates our RTOT paradigm, which we are going to explain in more details in the following section.





