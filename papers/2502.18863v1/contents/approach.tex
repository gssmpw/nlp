
\section{Our Sherlock Model}
%%公式太少

In this paper, we propose a Sherlock model to address the M-VAE task. Figure~\ref{fig:model} illustrates the framework of Sherlock, which is composed of two core components (i.e., the Global-local Spatial-enhanced MoE (GSM) module (sec~\ref{3.2}) for the global-local spatial modeling challenge and the Spatial Imbalance Regulator (SIR) (sec~\ref{3.3}) for the global and local spatial balancing challenge). Subsequently, we present our training strategies to enhance the ability of understanding spatial information (sec~\ref{3.4}).\\
\textbf{Backbone.} We choose Video-LLaVA~\cite{video-llava} and its visual encoder LanguageBind~\cite{languagebind} as the core framework. Video-LLaVA, which is optimized with a mixed dataset of images and videos, demonstrates leading performance across most image and video benchmarks. We employ Video-LLaVA as the backbone to explore the potential of Video-LLMs in extracting and localizing abnormal events.\\
\textbf{Task Formulation. }Given a video $V$ for $M$ frames, each frame is labeled with 1 or 0, where 1 and 0 represent whether this frame conveys an abnormal event. The goal of M-VAE is to interactively generate 
the quadruple ($sub$, $type$, $obj$, $sce$) for each event along with the corresponding timestamp $sta$ and $end$, where $sub$, $type$, $obj$, $sce$, $sta$ and $end$ are the subject, event type, object, scene, start time and end time of the abnormal event. As shown in Figure~\ref{fig:abstract} (a), a man steals a car at street from 23s to 25s. Therefore, the output of our M-VAE task is \{\emph{23s}, \emph{25s}, (\emph{people}, \emph{steal}, \emph{car}, \emph{street})\}.


\subsection{Global-local Spatial-enhanced MoE Module} 
\label{3.2}
As shown in Figure~\ref{fig:model}, we design a Global-local Spatial-enhanced MoE (GSM) Module for the global-local spatial modeling challenge. Inspired by Mixture-of-Experts (MoE)~\cite{nestedmoe}, we design three Local Spatial Experts (i.e., Local Action Expert, Local Object Relation Expert and Local Background Expert) and a 
Global Spatial Expert to extract spatial information, detailed as follows.

\textbf{Local Spatial Experts} contain three local spatial experts (i.e., action, object relation, and background), detailed as follows.


$\bullet$ \textbf{Local Action Expert (Action Expert, AE)}. We leverage HigherHRNet
% \footnote{\url{https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation}}
~\cite{HigherHRNet}, a well-adopted bottom-up human pose estimation network to extract local spatial action information. HigherHRNet can generate local spatial action tokens $\bm{\mathrm{T}_a}=\{\bm{t^a_1},...,\bm{t^a_i},...,\bm{t^a_m}\}$, and each token consists of 17 human joint nodes for each individual in every frame of a video sequence. 
% consisting of 17 human joint nodes for each individual in every frame of a video sequence, where
Here, $i$ denotes the $i$-th frame. Next, we apply Action Graph Attention to integrate $\bm{\mathrm{T}_a}$ with the video tokens $\bm{\mathrm{T}_v}=\{\bm{t^v_1},...,\bm{t^v_i},...,\bm{t^v_m}\}$ generated by the Video Encoder in Video-LLMs. We start by calculating the attention weights $\alpha_{kj}$ for each node $e_k$ in $\bm{t^a_i}$ relative to its neighboring node $e_j$:

\begin{equation}
    \boldsymbol{\alpha_{kj}} = \text{softmax} \left( \frac{{(\mathbf{W_a}h_k) \cdot (\mathbf{W_a}h_j)}}{\sqrt{d}} \right)
\end{equation}
where $h_k$ and $h_j$ is the features of $e_k$ and $e_j$ respectively. $\mathbf{W_{a}}$ denote the learnable weight matrix, and $d$ is the feature dimension. Then we aggregate the feature $\hat{h}_k$ of node $e_k$: $\hat{h}_k = \sum_{j \in {\mathcal{N}}(e_k)} \alpha_{kj} \cdot h_j$, where $\mathcal{N}(e_k)$ is the neighboring nodes of $e_k$. Finally the feature of $e_k$ is calculated by $h_k' = \text{ReLU}(\mathbf{W_{k}}[\hat{h}_k, h_k])$, where $\mathbf{W_{a}}$ donates the weight matrix and $[\hat{h}_k, h_k]$ is the concatenation of $\hat{h}_k$ and $h_k$.

After graph attention operation, we enhance $\bm{\mathbf{T}_a}$ using the attention mechanism with query $\bm{\mathbf{Q}_v}$, key $\boldsymbol{\mathbf{K}_a}$, and value $\bm{\mathbf{V}_a}$ calculation to obtain final action tokens: $\bm{\mathbf{T}_a'} =\operatorname{softmax}\left(\bm{\mathbf{Q}_{v}}^{\top}\cdot\bm{\mathbf{K}_{a}}\right)\cdot\bm{\mathbf{V}_{a}}$.


$\bullet$ \textbf{Local Object Relation Expert (Object Relation Expert, ORE)}. We leverage RelTR~\cite{reltr}, a well-studied one-stage object relation graph generation method to extract local spatial object relation information. RelTR can generate an object relation token $\bm{t^o_i} =\left(R_{i}, E_{i}\right)$, which represents the object relation graph of the $i$-th frame. Here, $R_i = \{\left(c_{i,1}, b_{i,1}\right),...,\left(c_{i,k}, b_{i,k} \right)\}$ is a set of $k$ detected objects, with class $c$ and corresponding bounding box $b$. The set $E_i = \{c_{i,p}, r_{i,{\left(p,q\right)}}, c_{i,q}\}$ consists of the directed edges in the graph, representing two directional edges from $c_{i,p}$ to $r_{i,{\left(p,q\right)}}$ and from $r_{i,{\left(p,q\right)}}$ to $c_{i,q}$, where $r_{i,{\left(p,q\right)}}$ denotes a relationship category. For example, an object might be represented as (\emph{man, <0.36, 0.24, 0.75, 1.62>}), and an edge as (\emph{man, near, car}). Subsequently, we apply object-aware masking with Masked Graph Transformer Networks (MaskGTN) to fully utilize object relations. We mask irrelevant object parts based on the bounding box information, and aggregate information from neighbors using a graph transformer layer (GT). 
Given an input graph of region classes and edges, MaskGTN computes updated vectors for each region and edge. Assuming we use $L$ layers of GT, with $\bm{\mathbf{H}^{(\ell)}}$ representing the features of the $\ell$-th layer, the final forward propagation is defined as follows:
\\
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\bm{\mathbf{H}^{(\ell+1)}} = \sigma\left(\sqrt{\tilde{\mathbf{D}}} \cdot \tilde{\mathbf{A}} \cdot \sqrt{\tilde{\mathbf{D}}} \cdot \boldsymbol{\mathbf{H}^{(\ell)}} \cdot \boldsymbol{\mathbf{W}^{(\ell)}}\right)
\end{equation}
where $\sigma$ is the activation function on the graph. $\tilde{\mathbf{A}}$ is the adjacency matrix of the object-relation graph, derived from $E_i$, and $\tilde{\bm{\mathbf{D}}}$ is its degree matrix, with $\tilde{\bm{\mathbf{D}}_{ii}}=\sum_{i} \tilde{\bm{\mathbf{A}}_{ij}}$. $\mathbf{W}^{\left(\ell\right)}$ is a trainable weight matrix. 


$\bullet$ \textbf{Local Background Expert (Background Expert, BE)}. We leverage SAM2~\cite{sam}, an advanced model for visual segmentation, to extract local spatial background information from videos. SAM2 can generate a background image for each frame of video. Then we leverage InternVit~\cite{internvl} to encode local spatial background information which is a large vision encoder extending the parameters of vision transformer (VIT)~\cite{vit} to 6B, formally represented as:
\begin{align}
    \label{background}
    \bm{\mathbf{T}_b} = \text{InternVit}\left(\text{SAM2}\left({v_i}\right)\right)
\end{align}
where ${v_i}$ is the $i$-th frame of video $V$. This process results in the local spatial background tokens $\bm{\mathbf{T}_b}=\{\bm{t^b_1},...,\bm{t^b_i},...,\bm{t^b_m}\}$ for the entire video sequence, with $n$ representing the total number of frames. 


\begin{table*}[]
\setlength{\abovecaptionskip}{0.5 ex}
\setlength{\belowcaptionskip}{-3 ex}
\caption{The statistics of the number of events and the duration in seconds (s) of events for each scene.}
\resizebox{\linewidth}{!}{
\label{tab:scene type}
\begin{tabular}{c|cccccccccccccc|c}
\hline
Split     & School     & Shop        & Underwater & Street      & Road        & Boat        & Wild        & Forest      & Residence   & Bank       & Commercial  & Factory    & Lawn        & Other      & Total         \\ \hline
Train     & 55 (2136s) & 107 (4130s) & 78 (3022s) & 113 (7076s) & 114 (5586s) & 115 (5203s) & 111 (4681s) & 102 (3918s) & 117 (4914s) & 89 (3380s) & 105 (5011s) & 82 (3173s) & 104 (5943s) & 56 (1497s) & 1348 (59670s) \\
Inference & 13 (534s)  & 26 (1032s)  & 19 (755s)  & 28 (1769s)  & 28 (1396s)  & 29 (1300s)  & 27 (1170s)  & 25 (979s)   & 29 (1228s)  & 22 (845s)  & 26 (1252s)  & 20 (793s)  & 26 (1485s)  & 14 (374s)  & 332 (14912s)  \\ \hline
\end{tabular}}
\end{table*}


\textbf{Global Spatial Expert} has a comprehensive understanding of the training data. Collaborate with local spatial experts to bring specialization and generalization capabilities to M-VAE tasks.

$\bullet$ \textbf{Global Spatial Expert (Global Expert, GE)}. The weight assigned to the global spatial expert complements that of the local spatial experts. Consequently, the local spatial experts acquire specialized skills for specific tasks, whereas the global spatial expert develops a comprehensive understanding of the entire training corpus. The collaboration between these two types of experts provides both specialization and generalization for our M-VAE task. In this way, we leverage LanguageBind~\cite{languagebind} in Video-LLaVA~\cite{video-llava}, which inherits the ViT-L/14 structure from CLIP and is equipped with powerful and universal visual encoding capabilities to extract global spatial information for our task. We subsequently leverage a pre-trained FFN layer by~\cite{video-llava} to align the dimension with other spatial information, formally represented as:



\begin{align}
    \label{global}
    \bm{\mathbf{T}_g} = \text{FFN}\left(\text{LanguageBind}\left({v_i}\right)\right)
\end{align}
where ${v_i}$ is the $i$-th frame of video $V$. This process yields the full set of global tokens $\bm{\mathbf{T}_g}=\{\bm{t^g_1},...,\bm{t^g_i},...,\bm{t^g_m}\}$ for the entire video sequence, with $n$ representing the total number of frames. 


After designing four experts, we ensure that the four Spatial Experts can dynamically adjust the weights of the four heterogeneous types of spatial information inspired by Mixture-of-Experts (MoE)~\cite{onellm}. As shown in Figure~\ref{fig:model}, unlike methods that embed several FFNs within LLMs, our GSM put four experts outside the LLMs to adjust weights for global and local spatial information. Based on this, we introduce a dynamic Expert Gate (EG)~\cite{router}, which controls the contribution of each expert by calculating gating weights as a soft gate. Finally, the output $\bm{\mathbf{O}}$ of GSM, based on four spatial experts and EG, is formally represented as:

\begin{align}
    \label{moe_out}
    \boldsymbol{g} &=\text{softmax}\left(\mathbf{W}_g \cdot \sum^{N}_{i=1} \left(\mathbf{S_i}\right)\right)\\
    \bm{\mathbf{O}} &= \mathrm{LayerNorm}\left(\sum^{N}_{i=1}\left(g_i\cdot \bm{\mathbf{S_i}}\right)\right)
\end{align}
where $\mathrm{LayerNorm}\left(\cdot\right)$ indicates layer normalization~\cite{layernorm}. $g_i$ (the $i$-th entry in $\boldsymbol{g}$) represents the weight of the $i$-th expert. $\mathbf{S_i}$ represents the outputs of the $i$-th Spatial expert. $N$ is the total number of spatial expert, and $\mathbf{W}_g$ being the trainable weight matrix. 





\subsection{Spatial Imbalance Regulator}
\label{3.3}
After modeling the spatial information, we design a Spatial Imbalance Regulator (SIR) including a Gated Spatial Balancing Loss (GSB) for the global-local spatial balancing challenge, detailed as follows.

\textbf{Gated Spatial Balancing (GSB) Loss.} Previous researches employ a basic Mixture of Experts (MoE)~\cite{onellm,onellm2} to model global and local spatial information. When faced with an imbalance between these two types of information, the weights assigned to experts tend to be biased toward those that appear more frequently. As shown in Figure~\ref{fig:abstract} (c), there are the most spatial elements (e.g., \emph{People}) related to local spatial action information in event quadruple. This implies that performance will deteriorate when faced with real-world data that is not processed by an action expert (e.g., \emph{object relations}). More seriously, as shown in Figure~\ref{fig:abstract} (c), global information holds significant weight in all data, which will lead to excessive training of global experts and weaken the abilities of local experts with lower weights. 
This imbalance phenomenon will greatly affect the performance of our model. Based on this, we should keep the weights of all spatial experts not too different and achieve the optimal state of relative balance where every expert is fully trained. Inspired by MoELoRA~\cite{moelora}, we propose a \textbf{G}ated \textbf{S}patial \textbf{B}alancing (GSB) Loss to balance spatial weights, as follows:


%%加上和挑战相关的
\begin{equation}
    \label{moe_loss}
    \mathcal{L}_\text{gate}  = \left(\frac{1}{N_{\text{local}}} \sum^{N_{\text{local}}}_{i=1}-\text{log}\left(g_i\right)\right)- \text{log}\left(g_{\text{global}}\right)
\end{equation}
where $N_{\text{local}}$ is the number of local expert. $g_{\text{global}}$ is the weight of global expert. The first term of Eq.(\ref{moe_loss}) is balancing between local experts, and the second term is balancing between local and global experts. The weights of four experts have already balanced when the loss is optimized to a minimum. This regulation achieves a better balance among all experts, reducing the impact of data imbalance, which effectively addresses the global-local balancing challenge. Finally, the overall loss of Sherlock can be represented as:
\begin{equation}
    \label{final_loss}
    \mathcal{L}  = \mathcal{L_{D}} + \alpha*\mathcal{L}_\text{gate}
\end{equation}
where $\alpha$ is the hyper-parameter that controls the strength of $\mathcal{L}_\text{gate}$, and $\mathcal{L_{D}}$ is the next-token prediction loss of Video-LLMs. 

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.5 ex}
\setlength{\belowcaptionskip}{-3.5 ex}
  \centering
  \includegraphics[width=\columnwidth]{image/cloud.pdf}
  \caption{The word cloud distribution of quadruple elements in the M-VAE dataset, which reveals the spatial imbalance. (e.g., The proportion of \emph{people} is the highest)}
  \Description{our method xxx}
  \label{fig:cloud}
  
\end{figure}




% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[]
\setlength{\abovecaptionskip}{0.5 ex}
\setlength{\belowcaptionskip}{-3 ex}
\label{tab:main_results}
\centering
\caption{Comparison of several Video-LLMs and Sherlock on our instruction dataset. The $\downarrow$ beside FNRs indicates the lower the metric, the better the performance. AE, ORE, BE, GE, and EG represent four Spatial Experts and Expert Gate respectively. Sub, Type, Obj, and Sce represent Subject, Event type, Object, and Scene respectively. For each task, {\color[HTML]{3166FF }Blue} and {\color[HTML]{009901} Green} donate the first and second place respectively.}
\resizebox{\linewidth}{!}{
\label{tab:main_results}
\begin{tabular}{c|cccccccccccc|cccc|cc}
\hline
                                    & \multicolumn{12}{c|}{\textbf{Event Extraction}}                                                                                                                                                                                                                                                                                                                                                                                                                                               & \multicolumn{4}{c|}{\textbf{Event Location}}                                                                                                                  & \multicolumn{2}{c}{\textbf{Anomaly Cls.}}                                     \\ \cline{2-19} 
                                    & \multicolumn{4}{c}{Single (F1)}                                                                                                                               & \multicolumn{4}{c}{Pair (F1)}                                                                                                                                 & \multicolumn{3}{c}{Quadruple}                                                                                         &                                       &                                       & \multicolumn{2}{c}{mAP@tIoU}                                                  &                                       &                                       &                                        \\  \cmidrule(r){2-5}  \cmidrule(r){6-9} \cmidrule(r){10-12} \cline{15-16}
\multirow{-3}{*}{\textbf{Models}} & Subject                               & Type                                  & Object                                & Scene                                 & Sub-Type                              & Obj-Type                              & Sub-Sce                               & Obj-Sce                               & F1                                    & T5-based                              & GPT-based                             & \multirow{-2}{*}{Average}             & 0.1                                   & 0.2                                   & 0.3                                   & \multirow{-2}{*}{Average}             & \multirow{-2}{*}{FNRs}                & \multirow{-2}{*}{F2}                  \\ \hline
Video Chat                          & 73.14                                 & 71.35                                 & 64.28                                 & 71.76                                 & 70.12                                 & 58.69                                 & 71.55                                 & 61.18                                 & 40.95                                 & 61.68                                 & 53.94                                 & 62.6                                  & 77.28                                 & {\color[HTML]{009901} \textbf{74.93}} & 66.26                                 & {\color[HTML]{009901} \textbf{72.82}} & 38.79                                 & 65.88                                 \\
Video ChatGPT                       & 61.87                                 & 59.51                                 & 54.82                                 & 46.39                                 & 54.23                                 & 49.68                                 & 43.26                                 & 41.38                                 & 39.63                                 & 57.36                                 & 50.38                                 & 49.86                                 & 74.65                                 & 70.91                                 & {\color[HTML]{009901} \textbf{67.03}} & 70.86                                 & 41.47                                 & 61.35                                 \\
Valley                              & 64.64                                 & 62.27                                 & 58.94                                 & 52.26                                 & 58.36                                 & 51.64                                 & 49.68                                 & 46.42                                 & {\color[HTML]{009901} \textbf{42.38}} & {\color[HTML]{009901} \textbf{63.34}} & 56.67                                 & 54.23                                 & 69.34                                 & 62.26                                 & 57.66                                 & 63.08                                 & 43.49                                 & 59.42                                 \\
Panda GPT                           & 73.09                                 & {\color[HTML]{009901} \textbf{75.45}} & {\color[HTML]{009901} \textbf{68.42}} & 61.93                                 & {\color[HTML]{009901} \textbf{71.96}} & {\color[HTML]{009901} \textbf{59.92}} & 59.79                                 & 59.45                                 & 41.17                                 & 54.36                                 & 48.55                                 & 60.37                                 & 76.64                                 & 62.69                                 & 57.21                                 & 65.51                                 & {\color[HTML]{009901} \textbf{35.62}} & {\color[HTML]{009901} \textbf{69.16}} \\
mPLUG-Owl                           & 52.86                                 & 37.54                                 & 40.24                                 & 37.68                                 & 31.97                                 & 28.89                                 & 33.9                                  & 27.87                                 & 22.12                                 & 30.68                                 & 32.41                                 & 34.1                                  & 61.42                                 & 53.21                                 & 46.46                                 & 53.69                                 & 56.98                                 & 51.66                                 \\
Chat-UniVi                          & 59.71                                 & 57.26                                 & 55.28                                 & 44.23                                 & 52.43                                 & 50.62                                 & 41.24                                 & 40.96                                 & 37.68                                 & 55.34                                 & 48.84                                 & 43.59                                  & 65.89                                 & 58.62                                 & 40.02                                 & 54.84                                 & 52.52                                 & 53.78                                 \\
Video-LLaVA                         & {\color[HTML]{009901} \textbf{77.85}} & 73.68                                 & 65.67                                 & {\color[HTML]{009901} \textbf{75.91}} & 69.32                                 & 59.21                                 & {\color[HTML]{009901} \textbf{73.25}} & {\color[HTML]{009901} \textbf{62.24}} & 41.32                                 & 52.94                                 & {\color[HTML]{009901} \textbf{56.74}} & {\color[HTML]{009901} \textbf{64.37}} & {\color[HTML]{009901} \textbf{78.31}} & 74.79                                 & 64.92                                 & 72.67                                 & 41.34                                 & 64.96                                 \\ \hline
\rowcolor{lightpink} Sherlock                            & {\color[HTML]{3166FF} \textbf{87.97}} & {\color[HTML]{3166FF} \textbf{82.12}} & {\color[HTML]{3166FF} \textbf{74.99}} & {\color[HTML]{3166FF} \textbf{92.15}} & {\color[HTML]{3166FF} \textbf{77.06}} & {\color[HTML]{3166FF} \textbf{66.28}} & {\color[HTML]{3166FF} \textbf{85.16}} & {\color[HTML]{3166FF} \textbf{73.17}} & {\color[HTML]{3166FF} \textbf{57.57}} & {\color[HTML]{3166FF} \textbf{75.46}} & {\color[HTML]{3166FF} \textbf{67.52}} & {\color[HTML]{3166FF} \textbf{75.22}} & {\color[HTML]{3166FF} \textbf{94.03}} & {\color[HTML]{3166FF} \textbf{82.59}} & {\color[HTML]{3166FF} \textbf{76.12}} & {\color[HTML]{3166FF} \textbf{84.24}} & {\color[HTML]{3166FF} \textbf{17.24}} & {\color[HTML]{3166FF} \textbf{83.59}} \\
\rowcolor{lightblue} w/o AE                              & 83.15                                 & 77.64                                 & 71.28                                 & 90.16                                 & 72.36                                 & 63.47                                 & 80.52                                 & 70.39                                 & 52.48                                 & 60.61                                 & 62.02                                 & 71.18                                 & 92.24                                 & 81.21                                 & 75.38                                 & 82.94                                 & 21.82                                 & 80.45                                 \\
\rowcolor{lightblue} w/o ORE                             & 83.96                                 & 78.25                                 & 72.37                                 & 90.01                                 & 74.24                                 & 64.46                                 & 81.56                                 & 70.97                                 & 54.35                                 & 72.28                                 & 65.08                                 & 72.5                                 & 91.13                                 & 82.08                                 & 74.62                                 & 82.61                                 & 22.97                                 & 78.83                                 \\
\rowcolor{lightblue} w/o BE                              & 81.16                                 & 74.65                                 & 67.88                                 & 88.07                                 & 69.29                                 & 61.12                                 & 77.64                                 & 66.64                                 & 48.63                                 & 53.04                                 & 55.94                                 & 67.71                                 & 88.62                                 & 79.09                                 & 72.24                                 & 79.98                                 & 25.36                                 & 73.51                                 \\
\rowcolor{lightblue} w/o GE                              & 79.2                                  & 74.09                                 & 66.71                                 & 84.11                                 & 70.38                                 & 60.77                                 & 75.44                                 & 66.28                                 & 46.34                                 & 63.97                                 & 57.06                                 & 66.75                                 & 86.18                                 & 78.37                                 & 69.28                                 & 77.94                                 & 28.97                                 & 71.28                                 \\
\rowcolor{lightorange} w/o EG                              & 78.83                                 & 73.96                                 & 65.02                                 & 83.15                                 & 70.15                                 & 60.26                                 & 74.15                                 & 63.37                                 & 43.64                                 & 59.14                                 & 51.82                                 & 64.86                                 & 81.31                                 & 77.68                                 & 67.88                                 & 75.62                                 & 32.58                                 & 67.07                                 \\
\rowcolor{lightorange} w/o SIR                             & 84.47                                 & 80.14                                 & 71.94                                 & 92.34                                 & 75.58                                 & 64.84                                 & 83.21                                 & 70.06                                 & 55.73                                 & 72.87                                 & 65.18                                 & 73.3                                 & 83.41                                 & 78.49                                 & 68.37                                 & 76.75                                 & 30.64                                 & 70.97                                 \\
\rowcolor{lightgreen} w/o pre-tuning                      & 78.24                                 & 74.44                                 & 64.22                                 & 82.21                                 & 68.55                                 & 57.74                                 & 72.62                                 & 62.91                                 & 42.51                                 & 57.22                                 & 50.54                                 & 63.74                                 & 79.58                                 & 75.32                                 & 65.07                                 & 73.32                                 & 34.87                                 & 66.64                                 \\ \hline
\end{tabular}}
\end{table*}





\subsection{Training Strategies for Sherlock}
\label{3.4}
In order to enhance the ability of understanding spatial information, we design a two-stage training process. Stage 1 is to enhance the ability of understanding spatial information and Stage 2 is to address the M-VAE task, detailed as follows.

\textbf{Stage 1. Pre-Tuning for spatial understanding.} 
As shown in Figure~\ref{fig:model}, we first pre-tune Video-LLaVA using four high-quality datasets. We aim for Video-LLaVA to have a good spatial understanding ability. Specifically, we selected four high-quality datasets: HumanML3D~\cite{human3d}, Ref-L4~\cite{refcoco}, RSI-CB~\cite{RSI-CB}, and COCO-Caption~\cite{coco}, as described in sec~\ref{4.1}. For each pre-tuning dataset, we enable this dataset to understand corresponding spatial information.

\textbf{Stage 2. Instruction Tuning for M-VAE task.} 
We aim to enable the model to localize abnormal events and extract quadruples through the chat paradigm. We construct an instruction tuning dataset described in sec~\ref{4.1} and instruct the pre-tuned Video-LLaVA to \emph{Extract quadruples and localize abnormal events. The quadruple includes subject, event type, object, and scene in abnormal events.} The instruction will undergo text embedding
to obtain the textual tokens $\bm{\mathbf{T}_t}$. Finally, the input of the LLM is “$\bm{\mathbf{O}}$ from Eq.(\ref{moe_out}) + $\bm{\mathbf{T}_t}$”.

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.5 ex}
\setlength{\belowcaptionskip}{-3.5ex}
  \centering
  \includegraphics[width=\columnwidth]{image/videollava2.pdf}
  \caption{Data composition for training and inference.
}
  \Description{our method xxx}
  \label{fig:videollava2}
  
\end{figure}
