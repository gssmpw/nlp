%jxma add at20240714 introduction

\section{Introduction}
Video Understanding is a foundational task in artificial intelligence, which focuses on analyzing and interpreting the content of videos to enable various applications, including video classification, activity recognition, and scene understanding~\cite{wu2023uniref++,videogrounding_dino,lin2023collaborative}. As a critical branch of video understanding, \textbf{V}ideo \textbf{A}nomaly \textbf{D}etection (VAD)~\cite{1}, which aims to automatically detect abnormal videos, has garnered significant research attention due to its wide range of applications in criminal activity detection and disaster response~\cite{2}. Prior studies on VAD mainly focus on detecting whether each video frame is abnormal or not in the video~\cite{3,4,1,2}. However, these studies overlook targeting at determining the underlying video semantic structure, i.e., “\emph{what is the abnormal type, where they have occurred, which people or things are involved}” with a given video. 
% In fields such as urban safety or industrial production lines that require high reliability and precision monitoring, using such structured processing can quickly search and screen for the required abnormal events, providing more convenient and intuitive evidence for further processing.
%introduced targeting at determin.ing the underlying video semantic structure, i.e., “ who does whaito whom, where and when and how”within a given video 

Motivated by these, this paper proposes a novel \textbf{M}ulti-scene \textbf{V}ideo \textbf{A}bnormal \textbf{E}vent \textbf{E}xtraction and \textbf{L}ocalization (M-VAE) task, aiming at
localizing abnormal events (i.e., starting and ending times of the anomaly) and extracting event quadruples (i.e. [subject of the event, event type, object of the event, scene of the event]) through a chat paradigm. Take an example of \emph{Street} scene in Figure~\ref{fig:abstract} (a), within 23s to 25s, a man bends down and pries the lock, then drives away from the street and the abnormal event quadruple is [\emph{people, steal, car, street}]. Different scene (i.e., Residence scene) is also shown in Figure~\ref{fig:abstract} (b). Within 15s to 17s, a man vandalizes a sculpture at 
one's residence and the quadruple is [\emph{people, Vandalism, Sculpture, Residence}].
This structured processing for abnormal videos can significantly improve the practicality and efficiency of video anomaly localization systems. In fields such as real-time abnormal event monitoring that require high reliability and precision monitoring, using such structured processing can quickly search and screen for the required abnormal elements, which provides more convenient and intuitive evidence for further processing. Therefore, it is worthwhile to address this new task. Nevertheless, we believe that this new task faces two key challenges.

For one thing, it is challenging to model the global-local spatial information (named global-local spatial modeling challenge). Existing video understanding models~\cite{video-llava,videochat,pandagpt} mainly focus on modeling general global information. However, local spatial information in our M-VAE task is often crucial compared to general global information, which are highly discriminative and essential for precise identification. Taking Figure~\ref{fig:abstract} (a) as an example, 
the local spatial information, such as action (bend down), object relations (<man, near, car>), and background (street), can help better identify abnormal events. However, those local spatial information (e.g., actions, object relations, backgrounds) have different heterogeneous representations (i.e., different model structures and encoders). Therefore, a single, fixed-capacity transformer-based model, often makes it difficult to capture those critical local spatial information in videos. Recently, the Mixture of Expert (MoE)~\cite{onellm,onellm2} paradigm has demonstrated scalability in multi-modal heterogeneous representation fusion tasks~\cite{onellm,onellm2,nestedmoe}. Inspired by this, a well-behaved model for our task should adopt the MoE paradigm to not only consider global spatial information but also emphasize the importance of local spatial information.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.8\columnwidth]{image/dataset.pdf}
%   \caption{The numbear of the top 20 elements in event quadruple. Different sector sizes represent different quantities. We can see that there are more elements related to local action information, and all elements are related to global spatial information, revealing necessity of our second challenge.}
%   \Description{our method xxx}
%   \label{fig:dataset}
  
% \end{figure}
For another, a straightforward approach is to employ a basic Mixture of Expert (MoE) mechanism~\cite{onellm,onellm2,nestedmoe} to treat global spatial information (i.e., general representations of videos) and local spatial information (e.g., actions) as the global expert and local experts for integrating those information. However, the data imbalance issue among local spatial information may lead to the basic MoE experts being biased towards the more frequently occurring spatial information in the dataset. The statistics in Figure~\ref{fig:abstract} (c) can illustrate this imbalance. Certain frequently appearing local information (i.e., action at 45\%), can lead to higher weight for the corresponding expert. However, in Figure~\ref{fig:abstract} (a), the object relations information, with the smallest proportion (25\%), but is the most discriminative for extracting and localizing \emph{Theft} events. More seriously, global spatial information is the most frequent and our preliminary experiments in Figure~\ref{fig:infertime} (a) reveal global expert is often more thoroughly trained and often have the highest weights. Therefore, a better-behaved MoE expert fusion mechanism should mitigate this data imbalance (named global-local spatial balancing challenge), ensuring all experts are sufficiently trained to highlight their importance.

To tackle above challenges, we propose a Global-local Spatial-sensitive LLM named Sherlock, i.e., acting like \emph{Sherlock Holmes} to track down criminal events, for M-VAE. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module to address the global-local spatial modeling challenge, which includes four spatial experts to extract spatial information and an expert gate to weigh global and local spatial information. Furthermore, this model designs a Spatial Imbalance Regulator (SIR) to address the global-local spatial balancing challenge, which includes a Gated Spatial Balancing Loss (GSB) to further balance global and local experts. Particularly, we construct a M-VAE instruction dataset to better evaluate the effectiveness of our model. Detailed experiments show Sherlock can effectively extract and localize abnormal events and surpass advanced Video-LLMs in multiple evaluation metrics. 

% Specifically, our contributions are as follows:

% \textbf{1)} We propose a new M-VAE task, which provides new ideas for the researches of video anomaly detection and promotes the new application of anomaly detection systems.

% \textbf{2)} We propose a new Sherlock  that models and balances global and local spatial information in videos to help Video-LLMs extract and localize abnormal events using MoE architecture.

% \textbf{3)} We conducted extensive experiments on our constructed M-VAE instruction dataset, and the results shows that our Sherlock  achieves better performance than current Video Language Models in M-VAE task.

