\section{Results and Discussions}



%%分三部分，抽取，定位，分类
\subsection{Experimental Results}

Table~\ref{tab:main_results} and Table!\ref{tab:plm} shows the performance comparison of different models on our M-VAE task, and we can see that: \textbf{For extraction performance}, our \textbf{Sherlock} model outperforms all baselines, with an average improvement of 10.85 ($p$-value < 0.05) over the second performance. Specifically, our \textbf{Sherlock} model surpasses the second performance by an average of 9.9 ($p$-value < 0.05), 8.59 ($p$-value < 0.05), and 9.52 ($p$-value < 0.05) in average Single, Pair, and Quadruple metrics, justifying the effectiveness of \textbf{Sherlock} on extraction task. 
\textbf{For localization performance},
our \textbf{Sherlock} model exceeds the second performance by 11.42 ($p$-value < 0.01) in average mAP@tIoU metric, justifying the effectiveness of \textbf{Sherlock} on localization task.  Furthermore, \textbf{for classification performance}, in FNRs and F2 metric, \textbf{Sherlock} surpasses the second performance in 18.38 ($p$-value < 0.01) and 14.43 ($p$-value < 0.01). This implies the importance of our global and local information and justifies the effectiveness of our \textbf{Sherlock} model on our task.




\begin{figure}[t]
\setlength{\abovecaptionskip}{1 ex}
\setlength{\belowcaptionskip}{-1 ex}
  \centering
  \includegraphics[width=\columnwidth]{image/bar6943.pdf}
  \caption{(a) is the visual comparison of our SIR and (b) is the comparison of the average inference time for a one-minute video between Sherlock and other Video-LLMs.}
  \Description{our model xxx}
  \label{fig:infertime}
  
\end{figure}

\begin{table}[]
\setlength{\belowcaptionskip}{-1 ex}
\caption{Comparison of localization and anomaly classification task with several well-performing non-LLM models.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc|ccc}
\hline
                           & \multicolumn{4}{c|}{\textbf{Anomaly Location}}                                                                                                                           & \multicolumn{2}{c}{\textbf{Anomaly Cls.}}                                   \\ \cline{2-7} 
                           &                                       & \multicolumn{2}{c}{mAP@tIoU}                                                  &                                       &                                       &                                       \\ \cline{3-4}
\multirow{-3}{*}{\textbf{Models}} & 0.1                                   & 0.2                                   & 0.3                                   & \multirow{-2}{*}{Average}             & \multirow{-2}{*}{FNRs}                & \multirow{-2}{*}{F2}                  \\ \hline
BiConvLSTM\cite{tab45}                 & 52.74                                 & 37.31                                 & 31.12                                 & 40.39                                 & 68.05                                 & 44.48                                 \\
SPIL\cite{tab44}                       & 53.28                                 & 38.89                                 & 32.91                                 & 41.69                                 & 67.84                                 & 46.87                                 \\
FlowGatedNet\cite{tab43}               & 53.64                                 & 39.64                                 & 33.18                                 & 42.15                                 & 67.24                                 & 46.55                                 \\
X3D\cite{tab42}                        & 54.52                                 & 40.05                                 & 34.96                                 & 43.17                                 & 65.08                                 & 48.65                                 \\
HSCD\cite{tab41}                      & 56.14                                 & 42.87                                 & 35.28                                 & 44.76                                 & 60.36                                 & 52.28                                 \\ \hline
\rowcolor{lightpink} \textbf{Sherlock}          & {\color[HTML]{3166FF} \textbf{94.03}} & {\color[HTML]{3166FF} \textbf{82.59}} & {\color[HTML]{3166FF} \textbf{76.12}} & {\color[HTML]{3166FF} \textbf{84.24}} & {\color[HTML]{3166FF} \textbf{17.24}} & {\color[HTML]{3166FF} \textbf{83.59}} \\ \hline
\end{tabular}}
\label{tab:plm}
\end{table}

% \textbf{3)} Influenced by the evaluation model proposed by Video-Bench~\cite{video-bench}, we used T5-based and GPT-based evaluation metrics to evaluate the extraction ability of \textbf{Sherlock} and other models. The average improvement of other models is \textbf{23.09}, while we are \textbf{17.84} ($p$-value < 0.01). This indicates that \textbf{Sherlock} can follow the instructions well and extract quadruples that meet our requirements.

\begin{figure*}[t]
\vspace{-0.3cm}
\setlength{\abovecaptionskip}{0.5 ex}
\setlength{\belowcaptionskip}{-3 ex}
  \centering
  \includegraphics[width=\textwidth]{image/case.pdf}
  \caption{Two Visualized samples to compare Sherlock with other Video-LLMs.}
  \Description{our model xxx}
  \label{fig:casestudy}
\end{figure*}

\subsection{Contributions of Each Key Component}
In order to further investigate the contributions of different modules of \textbf{Sherlock}, we conduct an ablation study on our \textbf{Sherlock} model. As shown in Table~\ref{tab:main_results}, w/o AE, w/o ORE, w/o BE, w/o GE, w/o EG, and w/o pre-tuning represent without four Spatial Experts, Expert Gate, and pre-tuning stage in sec~\ref{3.3} respectively.








\textbf{Effectiveness Study of Global and Local Spatial Expert}. From Table~\ref{tab:main_results}, we can see that: The performance of \textbf{w/o AE}, \textbf{w/o ORE}, \textbf{w/o BE} and \textbf{w/o GE} degrades in all metrics, with an average decrease of 7.54 ($p$-value < 0.01), 7.57 ($p$-value < 0.01), 4.37 ($p$-value < 0.01), and 5.68 ($p$-value < 0.01) in FNRs, F2, average map@tIoU, and average event extraction metrics. This confirms the importance of global and local information in extracting and localizing abnormal events, and \textbf{Sherlock} can better model those information well.



\textbf{Effectiveness Study of Spatial Imbalance Regulator.} From Table~\ref{tab:main_results}, we can see that: \textbf{1)} Compared with \textbf{Sherlock}, \textbf{w/o EG} shows poorer performance in all metrics, with a decrease of FNRs, F2, average map@tIoU, and average extraction performance by 15.34 ($p$-value < 0.01), 16.52 ($p$-value < 0.01), 8.62 ($p$-value < 0.05) and 10.36 ($p$-value < 0.01), respectively. This demonstrates the effectiveness of GSM in global-local spatial modeling and encourages us to consider handling heterogeneity issues between spatial information in the manner of MoE. 
\textbf{2)} From Table~\ref{tab:main_results}, we can see that compared to performance of \textbf{w/o SIR}, the performance of \textbf{w/o MG} is poorer, with FNRs, F2, average map@tIoU, and average event extraction metrics decreasing by 1.94 ($p$-value < 0.05), 3.9 ($p$-value < 0.05), 1.13 ($p$-value < 0.05) and 4.84 ($p$-value < 0.05), respectively. This further demonstrates the effectiveness of $\mathcal{L}_\text{gate}$ in global-local spatial balancing and encourages us to consider using SIR to better balance spatial information. 
\textbf{3)} In addition, we record the weights of four spatial experts after training in Figure~\ref{fig:expert} and Figure~\ref{fig:infertime} (a). We can see that the weights of all experts have been relatively balanced, and each expert has demonstrated outstanding professional abilities when facing different types of abnormal videos.




% \begin{table}[]
% \setlength{\belowcaptionskip}{-1 ex}
% \caption{Comparison of localization and anomaly classification task with several well-performing non-LLM models which is conducted on publicly available datasets.}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|cccc|ccc}
% \hline
%                            & \multicolumn{4}{c|}{\textbf{Anomaly Location}}                                                                                                                           & \multicolumn{2}{c}{\textbf{Anomaly Cls.}}                                   \\ \cline{2-7} 
%                            &                                       & \multicolumn{2}{c}{mAP@tIoU}                                                  &                                       &                                       &                                       \\ \cline{3-4}
% \multirow{-3}{*}{\textbf{Models}} & 0.1                                   & 0.2                                   & 0.3                                   & \multirow{-2}{*}{Average}             & \multirow{-2}{*}{FNRs}                & \multirow{-2}{*}{F2}                  \\ \hline
% BiConvLSTM\cite{tab45}                 & 52.74                                 & 37.31                                 & 31.12                                 & 40.39                                 & 68.05                                 & 44.48                                 \\
% SPIL\cite{tab44}                       & 53.28                                 & 38.89                                 & 32.91                                 & 41.69                                 & 67.84                                 & 46.87                                 \\
% FlowGatedNet\cite{tab43}               & 53.64                                 & 39.64                                 & 33.18                                 & 42.15                                 & 67.24                                 & 46.55                                 \\
% X3D\cite{tab42}                        & 54.52                                 & 40.05                                 & 34.96                                 & 43.17                                 & 65.08                                 & 48.65                                 \\
% HSCD\cite{tab41}                      & 56.14                                 & 42.87                                 & 35.28                                 & 44.76                                 & 60.36                                 & 52.28                                 \\ \hline
% \rowcolor{lightpink} \textbf{Sherlock}          & {\color[HTML]{3166FF} \textbf{94.03}} & {\color[HTML]{3166FF} \textbf{82.59}} & {\color[HTML]{3166FF} \textbf{76.12}} & {\color[HTML]{3166FF} \textbf{84.24}} & {\color[HTML]{3166FF} \textbf{17.24}} & {\color[HTML]{3166FF} \textbf{83.59}} \\ \hline
% \end{tabular}}
% \label{tab:plm}
% \end{table}



\textbf{Effectiveness Study of Pre-tuning}. From Table~\ref{tab:main_results}, we can see that \textbf{w/o pre-tuning}, the performance is inferior to \textbf{Sherlock}. FNRs, F2, average map@tIoU, and average event extraction metrics have decreased by 17.63 ($p$-value < 0.01), 16.95 ($p$-value < 0.01), 10.92 ($p$-value < 0.01) and 11.48 ($p$-value < 0.01), respectively. This further justifies the effectiveness of pre-tuning, as well as encourages us to use more high-quality datasets to enhance the spatial understanding ability of Video-LLMs before instruction-tuning. 





\subsection{Convergence Analysis and Practical Assessment for Sherlock}
In order to analyze the convergence of Sherlock, we record the loss of baseline Video-LLMs, Sherlock, and its variant without specific components over various training steps. The results are shown in Figure~\ref{fig:train} and we can see that: \textbf{1)} \textbf{Sherlock} demonstrates the fastest convergence compared to other Video-LLMs. At the convergence point, the loss of Sherlock is 1.05, while Video-LLaVA is 2.06. This underscores the high efficiency of Sherlock over other advanced Video-LLMs. \textbf{2)} \textbf{Sherlock} demonstrates the fastest convergence compared to its variant without specific components in Figure~\ref{fig:train}. This justifies that the spatial information along with GSM and SIR can accelerate the convergence process, which further encourages us to consider the spatial information in the M-VAE task. 

To assess practicality, we analyze the FNRs of Sherlock for each scene. As shown in Table~\ref{tab:scene}, we can observe that in every scene, Sherlock outperforms other Video-LLMs. This indicates that the possibility of misclassifying abnormal events as normal events is minimized, thereby demonstrating the importance of global and local spatial modeling of Sherlock. We also analyze the average inference time in seconds for a one-minute video. As shown in Figure~\ref{fig:infertime} (b), Sherlock does not
perform much differently from the other models in terms of inference time. This is reasonable, as some studies confirm that the MoE architecture can improve efficiency [11, 28]. This suggests that introducing more information along with a MoE module for the M-VAE task does not increase the inference time and Sherlock can maintain good inference efficiency.

% \subsection{Compared with Advanced Non-LLM Models on Public Dataset}
% In order to more comprehensively evaluate the effectiveness of Sherlock, we compare our \textbf{Sherlock} model with other advanced non-LLM models~\cite{tab41,tab42,tab43,tab44,tab45} on traditional anomaly localization and anomaly classification task based on publicly available CUVA datasets~\cite{cuva}. Specifically, we need Sherlock to determine whether each second of the video is abnormal or not without generating quadruples. As shown in Table~\ref{tab:plm}, non-LLM models not only underperform relative to other Video-LLMs presented in Table~\ref{tab:plm} but also significantly inferior to our Sherlock model. This
% further demonstrates the importance of the global and local spatial information we proposed for the M-VAE task.


\subsection{Qualitative Analysis for Sherlock}
As shown in Figure~\ref{fig:casestudy}, we visualize and compare \textbf{Sherlock} with other Video-LLMs. We randomly select two samples from our dataset and ask these models to \emph{Analyze the following video and localize the timestamp and extract the quadruple of the abnormal events}. From the figure, we can see that: \textbf{1)} Accurately localizing abnormal events and extracting correct quadruples is a huge challenge. For instance, example 2 captures a segment from 9s to 15s, where identifying the collision of the truck at road is challenging, \textbf{2)} Compared with other advanced Video-LLMs, \textbf{Sherlock} shows excellent performance in localizing abnormal events. In example 1, \textbf{Sherlock} outperforms other models in terms of accuracy. In example 2, it outperforms PandaGPT in terms of accuracy and can generate a correct quadruple. This further demonstrates the effectiveness of \textbf{Sherlock} in precisely extracting and localizing abnormal events.