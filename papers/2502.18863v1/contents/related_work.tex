\section{Related Work}

$\bullet$ \textbf{Video Anomaly Detection}. Video Understanding is a rapidly evolving research field which encompasses several tasks, including video grounding~\cite{wu2023uniref++,videogrounding_dino,lin2023collaborative}, spatial-temporal detection~\cite{GirdharCDZ19} and so on. As an important branch of video understanding, previous studies on Video Anomaly Detection (VAD) can be categorized into unsupervised, weakly-supervised, and fully-supervised categories. Unsupervised approaches focus on leveraging reconstruction techniques to identify anomalies~\cite{1,wujiandu2,wujiandu3,wujiandu4}. Weakly-supervised methods have shown promising results in identifying abnormal frames~\cite{ruojiandu2,ruojiandu3,ruojiandu4,ruojiandu5,ruojiandu6}. Fully-supervised methods are scarce due to the expensive frame-level annotations required~\cite{tab41,tab42,tab43,tab44,tab45,holmes-vad,cuva}. Different from the above studies, our Sherlock model aims to target at determining the underlying video semantic structure, providing a structured quadruple that goes beyond previous methods, facilitating the rapid detection and early warning of abnormal events in real-time.\\
$\bullet$ \textbf{Event Extraction} (EE) focuses on extracting structured information from given types of information. 
Traditional EE methods mainly extract from text documents~\cite{eetext1,eetext2,eetext3,eetext4,eetext5}.
Recently, many studies~\cite{mee1,mee2,mee3,mee4,mee5} generate similar event structures from visual image data. Different from all the above studies, we are the first to focus on extracting the abnormal event from videos and constructing a quadruple dataset, incorporating information from multiple spatial information, enriching the task of event extraction, and making it more practical for real-world applications.\\
$\bullet$ \textbf{Video-oriented Large Language Models}. The rise of ChatGPT~\cite{chatgpt} has stimulated the prosperity of Video Large Language Models which can be categorized into four major types: firstly, Video Chat~\cite{videochat} and Video LLaMA~\cite{video-llama}, which utilize BLIP-2~\cite{blip2} and Q-Former to map visual representations onto Vicuna; secondly, models like Video ChatGPT~\cite{video-chatgpt}, Otter~\cite{otter}, Valley~\cite{valley}, mPLUG-Owl~\cite{mPLUG}, and Chat-UniVi~\cite{chatunivi}, which leverage CLIP~\cite{clip} to encode visual features; thirdly, PandaGPT~\cite{pandagpt}, which adopts ImageBind~\cite{imagebind} as its core architecture for video understanding; and fourthly, VideoLLaVA~\cite{video-llava}, which aligns image and video features into a linguistic feature space using LanguageBind~\cite{languagebind}. Recently, a few studies \cite{scene1,scene2} consider incorporating spatial information in models. Besides, some studies \cite{nestedmoe,onellm,onellm2} introduce the concept of MoE into LLMs, but they only focus on efficiency, without considering the balance between different information. Different from all the above studies, we design a new Sherlock model, to address our M-VAE task, which includes a Global-local Spatial-enhanced MoE module and a Spatial
Imbalance Regulator to address the challenges of global-local modeling and balancing.


