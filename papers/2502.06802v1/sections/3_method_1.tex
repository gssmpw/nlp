\input{figures/in_game_text}
\input{tables/algorithm}

\section{Proposed Method}
In this section, we present a two-part pipeline designed to integrate content-driven insights into Roblox’s recommendation system. The pipeline comprises (1) Game Profile Generation, which involves extracting and structuring in-game text into detailed game profiles, and (2) LLM-Based Reranker, where these profiles are leveraged to re-rank existing recommendations and validate their effectiveness in enhancing relevance and personalization. Algorithm~\ref{alg:reranking} summarizes the overall process of our proposed method.


\subsection{Game Profile Generation}
The first component of our method is Game Profile Generation, which focuses on creating high-quality, structured profiles from raw in-game text. Given the diversity of games on Roblox and the inconsistency in developer-provided descriptions, we approach this by analyzing text that appears naturally within gameplay, as developers often guide players through instructions and cues during play. Formally, let $\mathcal{G}$ denote the set of games. For each game $g \in \mathcal{G}$, we extract the in-game text $T_g$. The main steps in this process are as follows:

% \subsubsection{In-Game Text Extraction}
% We start by extracting all in-game text elements that players encounter within the game environment, including instructions, background descriptions, button prompts, and other guidance. This text, denoted as $T_g$ for each game $g$, provides valuable insights into the game’s genre, objectives, themes, mechanics, and language. To effectively capture this information, we aggregate all in-game text into a single file for each game, which serves as input for the LLM. This approach, inspired by code understanding techniques, allows the model to analyze the game content holistically.

% If the concatenated in-game text $T_g$ exceeds the LLM’s maximum token limit, we employ random sampling to reduce the text length, ensuring that diverse game content is represented while fitting within the token limit. Rather than directly filtering out noisy text, we use carefully designed prompts to instruct the LLM to focus on relevant game aspects, ignoring irrelevant content. This prompt-based approach helps the LLM extract useful information about the game’s core elements, such as gameplay objectives, genre, and mechanics, without manual preprocessing. To illustrate, Figure~\ref{fig:in_game_text} shows various types of in-game text that enhance our understanding of the game, including gameplay instructions, background context, action prompts, and language cues. By consolidating this text and guiding the LLM’s focus, we generate accurate, structured game profiles that support enhanced recommendations.

\subsubsection{In-Game Text Extraction}
We extract all in-game text elements encountered by players, including instructions, background descriptions, button prompts, and other guidance, denoted as $T_g$ for each game $g$. This text provides valuable insights into the game’s genre, objectives, themes, mechanics, and language. To enable holistic analysis, we aggregate all in-game text into a single file per game, serving as input for the LLM. If $T_g$ exceeds the LLM’s token limit, we apply random sampling to reduce its length while preserving diverse content. Instead of manually filtering noisy text, we use carefully designed prompts to instruct the LLM to focus on relevant game aspects and ignore irrelevant content. This approach ensures extraction of key information, such as gameplay objectives, genre, and mechanics, without extensive preprocessing. Figure~\ref{fig:in_game_text} illustrates various types of in-game text that inform our understanding, including gameplay instructions, background context, action prompts, and language cues. This process generates structured game profiles to support enhanced recommendations

\subsubsection{Game Profile Generation via LLMs}
After preparing the in-game text file $T_g$ for each game $g$, we employ a specifically crafted prompt for the LLM to generate a structured game profile $P_g$, focusing on essential attributes while filtering out irrelevant information. The prompt directs the LLM to produce a concise summary that highlights the game’s main theme, storyline, primary objectives, and core mechanics. This summary helps the recommendation system grasp the game’s core characteristics, enabling more personalized matches with user preferences. Additionally, the LLM identifies the game genre—such as “obby” (obstacle course), “simulator,” “adventure,” or “role-playing”—which further aids in categorizing the game according to its play style. The prompt also guides the LLM to determine the target audience, considering factors such as age appropriateness (e.g., kids, teens, all ages) and gameplay appeal (e.g., casual or competitive players), ensuring that recommendations align with demographic interests.

Beyond the general overview, the prompt enables the LLM to extract key features, such as multiplayer modes, customization options, in-game purchases, and unique controls, which distinguish the game and enhance its engagement potential. It also directs the model to note any additional content, including seasonal updates, exclusive items, or special events, which contribute to the game’s dynamic appeal. If the game language is specified as “NONE,” the prompt instructs the LLM to determine the language based on the in-game text, enhancing accessibility for players by allowing language-based recommendations. Finally, the LLM assesses the game’s scale, considering aspects like game world size, level count, and gameplay duration, to provide a sense of the game’s scope and depth, which can influence player engagement. This well-structured prompt enables the LLM to output a JSON-formatted profile $P_g$ encompassing all vital attributes, essential for robust, content-driven recommendations. For the full prompt details and structure, refer to the Appendix~\ref{sec:game_profile_generation}.

\subsection{LLM-Based Reranker}
Building on the generated game profiles, the LLM-Based Reranker assesses their effectiveness by re-evaluating the initial recommendations through a content-driven, personalized process. Inspired by the Chain-of-Thought (CoT) reasoning strategy~\cite{wei2022chain}, the reranker uses carefully crafted prompts to generate a reranking strategy tailored to user preferences. By incorporating content-rich game profiles, it demonstrates how in-game text understanding enhances recommendation quality. The reranker focuses on the top 30 recommendations from the initial ranking list $\mathcal{R}_u$, which contains up to 250 games in Roblox’s "Recommended For You" sort. This focus is driven by several key factors: (1) User interaction data indicates that players predominantly engage with the top 30 games, making this subset the most relevant for optimizing user experience. (2) Improving the top 30 rankings has the highest potential impact on user satisfaction, as it directly influences the most visible and frequently accessed recommendations. (3) Reranking a smaller subset reduces computational overhead, enabling a balance between effectiveness and efficiency in the reranking process. The LLM-Based Reranker operates in three sequential steps, designed to adapt recommendations to user preferences. These include user profile generation, personalized strategy creation, and the application of this strategy to the top 30 games. Detailed prompts used for these steps are provided in the Appendix~\ref{sec:user_profile}, and Appendix~\ref{sec:game_reranking}.

\subsubsection{User Profile Generation}
The first step in the reranking process is to generate a user profile $P_u$ that encapsulates individual preferences based on the user’s recent activity. Using the generated game profiles as contextual data, we analyze the user’s play history from the past seven days, denoted as $\mathcal{H}_u$. Each game ID in this history is converted into its corresponding game profile $P_g$, producing a sequence of structured game attributes that reflect the user’s recent interactions and preferences. To ensure the validity of in-game text understanding, this process relies solely on play history and excludes any direct game IDs, which lack descriptive features. We use a carefully crafted LLM prompt to analyze this sequence of game profiles and generate a comprehensive user profile $P_u$. This prompt instructs the LLM to summarize the user’s preferences across multiple dimensions, such as game genres, themes, mechanics, and gameplay styles. For example, it considers the types of games the user played most frequently, the diversity in their preferences, and any dominant patterns in their gaming behavior.

\subsubsection{Personalized Reranking Strategy Generation}
Once the user profile $P_u$ is created, the next step is to generate a personalized reranking strategy $S_u$ tailored to the user’s preferences. This strategy is crafted by feeding the user profile into the LLM with a specially designed prompt that instructs the model to identify and prioritize attributes most relevant to the user. The prompt, inspired by the CoT reasoning approach~\cite{wei2022chain}, systematically guides the LLM to consider key aspects such as preferred game genres, gameplay mechanics, unique features, and thematic elements. The reranking strategy $S_u$ acts as a blueprint for the subsequent reranking process. For example, if a user’s profile indicates a strong preference for adventure games with exploration mechanics, $S_u$ will prioritize these attributes when re-evaluating the recommendations. The strategy explicitly avoids referencing specific game IDs, as they lack meaningful descriptive features, and instead focuses on actionable insights derived from the game profiles. 

\subsubsection{Reranking the Top 30 List}
The final step applies the personalized strategy $S_u$ to the top 30 games in the initial recommendation list, denoted as $\mathcal{R}_u^{30}$. By focusing on this concise subset, the reranker targets the most impactful portion of the ranking, where user engagement is typically highest. The LLM uses the guidelines from $S_u$ to re-evaluate and reorder these recommendations based on their alignment with the user profile $P_u$. This process leverages the structured attributes in each game profile to determine relevance and prioritization. For instance, games matching the user’s preferred genres or featuring gameplay mechanics identified in $S_u$ are ranked higher. The refined recommendation list, $\mathcal{R}'_u$, represents a personalized, content-driven ordering designed to maximize user engagement and satisfaction. By concentrating on the top 30 games, this approach balances computational efficiency with the practical needs of a user-focused recommendation system.