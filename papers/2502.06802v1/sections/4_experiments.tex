\input{tables/statistics}

\section{Experiments}

\subsection{Experimental Setup}

\subsubsection{Objective}
The primary objective of this experiment is to verify the effectiveness of the game profiles generated through in-game text understanding. To evaluate this, we designed an LLM-Based Reranker that utilizes only the generated game profiles and the user’s recent play history from the past seven days. By reranking recommendations based on these profiles, we aim to determine whether incorporating content-driven information enhances recommendation quality. Additionally, we engaged human annotators to assess the accuracy and relevance of the game profiles, verifying if they correctly represent the game content and align with gameplay experiences.

\subsection{Data Collection}
For this study, we extracted real training data from the Roblox platform, dividing it into nine sub-datasets to account for variations in user behavior and list length. We created these sub-datasets along two dimensions: the length of user play history and the length of the ranking list. Specifically, we used the (0-30, 30-70, 70-100) percentiles to segment user play history lengths, capturing different levels of user engagement and diversity in game interactions. Additionally, we limited the ranking list to (top10, top20, top30) games for reranking in certain scenarios. Overall, the dataset comprises 2,700 unique users and 18,941 unique games, providing a diverse sample for assessing the performance of the LLM-Based Reranker and game profiles. Table~\ref{tab:data_statistics} presents key statistics for each sub-dataset, including user and game counts as well as playtime details, providing an overview of the dataset’s characteristics and user engagement patterns.

\subsubsection{Environment}
All experiments were conducted using GPT-4o as our LLM model, selected for its ability to handle complex language understanding tasks and deliver accurate content-based interpretations. This model was used for both game profile generation and reranking, ensuring consistency across all steps of the recommendation pipeline.

\subsection{Baseline and Comparative Models}

\subsubsection{Baseline Model}
The baseline model used in this study is the existing Roblox ranking model. This model ranks games primarily based on ID-based features and user behavior data, without incorporating any content-derived attributes, such as game title, description, or thematic elements. As a traditional recommendation system, it relies on collaborative filtering methods, utilizing user and game IDs alongside sparse behavior data like play frequency. This model serves as a foundational comparison point, as it does not leverage in-game text understanding or content-driven game profiles.

\subsubsection{Comparative Models}
To evaluate the incremental impact of content-driven profiles and personalized reranking strategies, we include several comparative models:
\textbf{(1) Game Title-Based Reranking}: This model reranks the recommendation list solely based on the game title. By using only this minimal text feature, we can evaluate the basic effect of content-based reranking when limited to title information, giving a partial view of the game’s identity. \textbf{(2) Game Title and Description-Based Reranking}: This model expands on the previous approach by incorporating both game title and description for reranking. With additional descriptive context, this model provides a better understanding of the game content but still lacks the depth of the structured game profiles generated through in-game text understanding. \textbf{(3) LLM-Based Reranker without Personalized Strategy}: In this model, the LLM-Based Reranker uses the full, generated game profiles (including elements such as genre, mechanics, and thematic details) but does not apply a user-specific personalized strategy. Instead, it ranks games based on a generalized relevance derived from game profiles alone. This model helps us assess the quality and effectiveness of the generated game profiles without any individual user customization. \textbf{(4) LLM-Based Reranker with Personalized Strategy (Proposed Model)}: This is the full implementation of our proposed LLM-Based Reranker, which not only leverages content-rich game profiles but also incorporates a personalized reranking strategy based on recent user play history. By generating a custom reranking guideline for each user, this model aims to maximize recommendation relevance by aligning it closely with each user’s unique preferences and past interactions.

\input{tables/overall}
\subsubsection{Purpose of Comparison}
The primary purpose of this comparative analysis is to evaluate the quality and effectiveness of the generated game profiles and their impact on recommendation performance. By comparing these models, we aim to measure how well each approach reflects user interests and enhances recommendation relevance. Additionally, this analysis explores the potential for applying the LLM-Based Reranker for more personalized ranking in the future. By isolating the effects of content-driven profiles and personalized strategies, we gain insight into the value of in-game text understanding and its scalability for adaptive, user-specific recommendations on Roblox.

\subsection{Evaluation Metrics}
\subsubsection{NDCG Engagement}
To evaluate the effectiveness of the LLM-Based Reranker, we use NDCG Engagement, a variation of the traditional Normalized Discounted Cumulative Gain (NDCG) metric that measures the relevance of recommendations based on user engagement. Unlike standard NDCG, where each relevant item has a binary relevance score of 1, NDCG Engagement assigns relevance scores according to the user’s playtime within the 7 days following their initial interaction with the recommended game. This approach allows us to capture deeper insights into how well the recommendations align with user interests, as increased playtime indicates a higher level of engagement and satisfaction. The formula for NDCG Engagement is as follows:
\[
\text{NDCG}_{\text{Engagement}} = \frac{1}{Z} \sum_{i=1}^{p} \frac{2^{\text{rel}(i)} - 1}{\log_2(i + 1)},
\]
where \( p \) is the position in the ranking list, \( \text{rel}(i) \) is the relevance score based on user playtime for the game at position \( i \), \( Z \) is a normalization factor ensuring that the NDCG score is between 0 and 1, calculated based on the ideal ordering of items by relevance. For clarity, all mentions of NDCG in this paper refer to NDCG Engagement unless otherwise specified. 


\subsection{Results and Analysis}
% \input{figures/popularity_bias}
\input{figures/user_preference}


\subsubsection{Quantitative Results}
In this section, we analyze the NDCG Engagement scores for different reranking models across various percentile ranges and NDCG cutoffs (NDCG@10, NDCG@20, and NDCG@30), as shown in Table~\ref{tab:ndcg_engagement_comparison}. To ensure reliability, all experiments were run five times, and the average results are reported. The quantitative analysis highlights the performance of the Baseline model, Title-based Reranking, Title+Desc Reranking, LLM-Based Reranker without Personalized Strategy, and the Proposed LLM-Based Reranker with Personalized Strategy (proposed model).
\begin{itemize}[leftmargin=*]
    \item \textbf{Overall Improvement of the Proposed Model:} The Proposed LLM-Based Reranker with Personalized Strategy consistently demonstrates superior performance compared to the Baseline model across most percentile ranges and NDCG cutoffs. This is evident in the Improvement (\%) column, where the Proposed model achieves notable gains, especially in lower percentile ranges. The overall average improvement for the Proposed model is highest in NDCG@10 (4.90\%), followed by NDCG@20 (2.01\%), and NDCG@30 (1.51\%). These results suggest that the personalized reranking strategy is particularly effective at optimizing the top ranks, which are critical in enhancing user engagement. The substantial improvement at the higher positions (NDCG@10) emphasizes the value of personalization in capturing user interest early in the recommendation list.
    \item \textbf{Performance of Title-Based and Title+Desc Models:} Both the Title-based Reranking and Title+Desc Reranking models show a decrease in performance compared to the Baseline. While the Title+Desc model leverages both game title and description text, these features often contain noise in Roblox. Game titles and descriptions may be inconsistent or unrelated to the actual game content, as they are often created by a wide range of developers with varying levels of professionalism. Furthermore, the LLM may lack sufficient global knowledge about many Roblox games, particularly new and less popular ones, limiting its ability to interpret these text features accurately. This underlines the importance of reliable content-based features and demonstrates the limitations of using unstructured, noisy text in the absence of comprehensive domain knowledge.
    \item \textbf{LLM-Based Reranker without Personalized Strategy:} The LLM-Based Reranker without Personalized Strategy performs comparably to the Baseline model but does not outperform the Proposed model. This outcome highlights the crucial role of personalization in enhancing recommendation relevance. While the content-based reranker benefits from in-game text understanding, the absence of user-specific personalization limits its effectiveness. These findings indicate that merely incorporating content-based features is insufficient; personalized adaptation to user preferences based on in-game text understanding is essential for optimal performance.
    \item \textbf{Challenges with the 30-70 Percentile Range:} Across the 30-70 percentile range, we observe a performance decrease for most text-based models. This drop can be attributed to the diverse and less focused nature of user play histories in this range. Unlike users in the lower percentile range, who often have narrow interests, or high-engagement users, who exhibit consistent preferences, mid-engagement users tend to explore a broader variety of games without clear patterns. For example, a user might play simulation, obby, adventure, tycoon, and competition games over the past week, but their preferences may not consistently favor one genre, such as simulation, over another, like obby. Without a discernible trend in user preferences, the reranker struggles to capture specific interests accurately, especially when relying solely on in-game text features without leveraging statistical insights or behavioral patterns. This highlights the need for methods capable of resolving ambiguities in diverse play histories and creating a clearer representation of user interests for mid-engagement users.
\end{itemize}

\subsubsection{User Engagement Analysis}
Figure~\ref{fig:user_preference} illustrates the average time users spent on games at each ranking position for both the Baseline model and the LLM-based reranker. Higher values represent greater user engagement, suggesting increased interest in games at those positions. The LLM-based reranker, indicated by the dashed red line, generally places games with higher user engagement towards the top of the ranking list, particularly in the highest-ranked positions, as evidenced by the elevated average time spent. This demonstrates the LLM-based reranker’s effectiveness in surfacing games that align with user preferences, as compared to the Baseline model.

\subsubsection{Ablation Study: Impact of Different LLM Models}

To evaluate the impact of different LLM models on the performance of our proposed pipeline, we conducted an ablation study using several LLMs: Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, GPT-4o-mini, and GPT-4-turbo. We compared each model's performance against GPT-4o, which serves as the baseline, using NDCG@30 as the evaluation metric. The results reveal significant variability in the effectiveness of the LLMs. Meta-Llama-3.1-8B-Instruct was unable to follow the instructions in our prompt and, therefore, could not produce usable results. Among the remaining models, all exhibited performance degradation compared to GPT-4o. Specifically, Meta-Llama-3.1-70B-Instruct, GPT-4o-mini, and GPT-4-turbo showed relative decreases of -3.889\%, -0.832\%, and -2.269\% in NDCG@30, respectively.

\subsubsection{Case Study: Effectiveness of LLM-Based Personalized Ranking Strategy}
\input{figures/case_study_personalization}
To better understand the impact of in-game text understanding and personalized reranking, we conducted a case study on the effectiveness of the LLM-based reranker in following a personalized ranking strategy. This study demonstrates the reranker’s ability to prioritize games based on specific user preferences, highlighting its potential for delivering highly relevant recommendations. Below is an example of the ranking strategy generated by the LLM:
\begin{tcolorbox}[colback=gray!5, colframe=gray!80, title=LLM Ranking Strategy, label=strategy]
\textbf{Based on the user's preferences, the ranking logic will prioritize games as follows:}

1. \textbf{Top Priority}:
   \begin{itemize}
      \item \textcolor{red}{\textbf{Adventure}} and \textcolor{red}{\textbf{obby}} games with extensive gameplay, multiple levels, and interactive features.
      \item Games that include role-playing elements, such as character morphing and customization.
   \end{itemize}

2. \textbf{High Priority}:
   \begin{itemize}
      \item \textcolor{red}{\textbf{Adventure}} games with a focus on exploration, puzzle-solving, and dynamic activities.
      \item Games with multiplayer modes, special abilities, and rewards systems.
   \end{itemize}

3. \textbf{Medium Priority...}
\end{tcolorbox}

In this case study (see Figure~\ref{fig:case_study_ranking_list}), we demonstrate the effectiveness of our proposed LLM-based reranking model. The first row shows the original ranking list, while the second row displays the reranked results generated by our model. The LLM follows the personalized ranking strategy closely, prioritizing relevant games according to the user’s preferences as shown in Box~\ref{strategy}. For example, adventure and obby games, which were marked as high-priority, are moved to the top positions. Notably, our reranker successfully elevates highly relevant games from lower positions (such as moving a game from position 20 to position 2), showcasing its ability to refine recommendations effectively.

\subsubsection{Case Study: Enhancing Game Understanding with In-Game Text Analysis} 
\input{figures/case_study_polish_railway}

In this case study (see Figure~\ref{fig:case_study_polish_railway}), we examine a Roblox game titled Old Polish Railway Classic. Based solely on the developer-provided title and description, it’s challenging to understand the gameplay, objectives, or even the specific genre of the game. The title indicates it’s related to railways, but there’s insufficient information about how to play or what players can expect. However, by leveraging our in-game text understanding model, we generated a comprehensive game profile that accurately captures the game’s content and objectives. This profile shown in Box~\ref{gamebox} provides players with a clear description of the gameplay experience, bridging the gaps left by the developer’s brief description.

\begin{tcolorbox}[colback=gray!5, colframe=gray!80, title=Game Profile about Old Polish Railway Classic, label=gamebox]
This Roblox game appears to be a railway simulator set in Poland, featuring
various Polish cities and stations. Players can manage and operate different aspects of a
railway system, including electrifying tracks, repairing platforms and switches, and handling freight cars. The game includes significant contributions from different developers and has a detailed focus on Polish railway operations.", \textcolor{red}{"game\_genre"}: "simulator",
\textcolor{red}{"suitabl\_for"}: "teens, railway enthusiasts, simulation fans", \textcolor{red}{"features"}: "multiplayer modes, detailed railway management, Polish cities and stations, electrification of tracks, repair and maintenance tasks, various control options, in-game updates and minor changes, special abilities like toggling ABS and TCS", \textcolor{red}{"includes"}: "seasonal updates, special events, exclusive items, significant contributions from developers, thank-you notes to contributors", \textcolor{red}{"game\_language"}: "Polish", \textcolor{red}{"game\_scale"}: "The game has a large scale with multiple cities and stations, detailed management tasks, and continuous updates, indicating an extensive and immersive gameplay experience.
\end{tcolorbox}




