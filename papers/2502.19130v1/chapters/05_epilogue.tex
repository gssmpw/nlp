\section{Conclusion}

We systematically evaluated the role of consensus and voting decision protocols across three knowledge and three reasoning tasks.
Our study assessed how the number of discussion rounds and agents influences task performance.
We propose two new methods to improve answer diversity during multi-agent discussions and decisions, i.e., \acf{AAD} and \acf{CI}. 
\ac{AAD} requires each agent to contribute draft ideas at the beginning of the discussion, and \ac{CI} encourages independent reasoning steps by limiting communication between agents and only allowing them to exchange possible solutions after each turn.

Our findings show that voting performs well on reasoning tasks, outperforming consensus by up to $13.2\%$, and outperforming a single \ac{CoT} baseline by 10.4\%.
This is likely because voting-based protocols allow agents to explore multiple reasoning paths instead of a single one, as in consensus.
In comparison, consensus outperforms voting in knowledge tasks by up to $2.8\%$, because it improves fact-checking by requiring at least the agreement of the majority of agents.
Increasing the number of agents in the discussion improved task performance, while increasing the number of discussion rounds before voting decreased performance.
\ac{AAD} improved performance by up to $3.3\%$, and \ac{CI} by up to $7.4\%$ over default multi-agent debate baseline, and 6.1\% and 10.2\% over single model \ac{CoT} baseline respectively.
Our new methods enhance answer diversity and reveal a connection between answer diversity and task performance.

Future work could explore other characteristics influencing decisions, such as power relations between managers and employees. 
This could also involve examining personas within this hierarchical structure to investigate whether dominant or affectionate leaders are more effective in leading discussions \citep{AMES2009111}.


We recommend using voting in reasoning tasks and consensus in knowledge tasks, scaling up the number of agents instead of the number of rounds, and increasing the diversity of answers between agents using \ac{AAD} and \ac{CI}.

\section*{Limitations} %
Multi-agent debates are computationally expensive because they require a message from each agent in each round, quickly leading to hundreds of forward passes per model.
Because of the high computational cost and the range of decision protocols and tasks in our work, we used sampled subsets of the datasets, which can lead to some variance.
To control for that variance, we sampled with a 95\% confidence level and calculated the standard deviation of three independent runs.%
Overall, the results were markedly higher than what could be explained by the standard deviation.
More details about the dataset and other parameters can be found in \Cref{appendix:datasets}.
Despite efforts to improve answer diversity, agents often converged on similar responses, suggesting that more advanced techniques to encourage independent solutions are needed in the future.

