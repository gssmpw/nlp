\input{tables/results_70b}
\section{Additional Results}

Additional results for the first experiment to provide further information.
\subsection{Task Performance with Llama 3 70B}
\label{app:decision_70b}

Compared to the results of the Llama 3 8B model, the larger Llama 3 70B model performs much better overall, as seen in \Cref{tab:results_70b}. Most of the results are a bit better than the baseline, but the multi-agent discussions are only in a few cases able to outperform the \ac{CoT} baseline. This model does not fail to follow the prompt for the MuSR baseline and consensus-based decision protocols. Therefore, the big performance gain from the smaller model cannot be observed here. SQuAD 2.0 and StrategyQA had the largest performance gains, even outperforming the \ac{CoT} baseline, similar to the results from the smaller model. This difference in task performance can have many reasons. As \citet{li_dawn_2024} showed, smaller models are more likely to hallucinate, which reduces task performance. This can be mitigated by using multiple agents because it is less likely that two agents hallucinate the same things. Larger models tend to hallucinate less, reducing this effect for the Llama 3 70B model \citep{li_dawn_2024}. In general, Llama 3 70B has a much higher baseline for task performance, making it more difficult to improve baseline results. Many of the improvements by the Llama 3 8B model are quite small, except for the ones where the Llama 3 70B model also outperforms the \ac{CoT} baseline. This can be taken as evidence that these multi-agent discussions require specific problem structures, or else the agents are just talking about the same results for multiple rounds and agreeing with each other. If these discussions continue too long, they can drift away from the original task, which reduces task performance. This has also been observed by \citet{becker_multi-agent_2024} and an example can be seen in \Cref{example:failed_discussion}. A positive example of how discussion can help task performance can be seen in \Cref{example:good_discussion}.

\subsection{Termination percent}
\label{app:termination_percent}
The data in \Cref{tab:termination_percentages} shows the number of turns that are needed for each decision protocol to reach a final decision for the MMLU dataset. Most of the voting decision protocols are able to vote for a final answer already in the first round in which they are allowed to vote. Simple voting has the highest agreement rate, but also cumulative and ranked voting only need in a few cases another round. In contrast, the approval decision protocol only achieves this in $\sim27\%$ of the cases. About $14\%$ need another round and the rest is canceled after the fifth round. This happens because these models like to agree with each other, and therefore they tend to vote for many of the answers, which often leads to a tie. Therefore, more restrictive voting decision protocols can reach a decision more easily, as a tie is less likely. 
The consensus decision protocols require only one to two rounds to reach consensus and still achieve a higher task performance because these results are based on the MMLU dataset. The decision protocols tend to behave similarly in terms of rounds needed to create a final answer, independent of task and model.

\input{tables/termination_percent}

\section{Additional Details on Datasets}
\label{appendix:datasets}
\input{tables/datasets}
The dataset selection is very important for this work. It needs to be tested whether decision protocols perform well in multiple domains and whether some protocols perform better with specific tasks than others. Therefore, we selected datasets from different domains and divided them into two groups:

\begin{itemize}
    \item \textbf{Knowledge-based Datasets}: MMLU, MMLU-Pro, and GPQA. These still require some reasoning and domain knowledge.
    \item \textbf{Reasoning-based Datasets}: StrategyQA, MuSR, and SQuAD 2.0. These emphasize multistep reasoning and textual comprehension.
\end{itemize}

An overview of all these datasets can be found in \Cref{tab:datasets} with a description and the number of samples used for evaluation.

\subsection{Sampling Strategy}
Because multi-agent discussions are expensive, we use a small subset of each dataset that still represents the dataset effectively. This follows approaches used by \citet{yin_exchange--thought_2023, chen_reconcile_2024, becker_multi-agent_2024} and ensures a 95\% confidence level with a 5\% margin of error:

\[
n_0 = \frac{Z^2 \cdot p \cdot (1 - p)}{d^2},
\]
where \(Z = 1.96\), \(p = 0.5\), and \(d = 0.05\) \citep{thompson_sampling_2012}. For finite datasets, a finite population correction is applied:
\[
n = \frac{n_0}{1 + \frac{n_0 - 1}{N}},
\]
where \(N\) is the total number of samples in each dataset \citep{cochran_sampling_1953}. The specific sample sizes reflecting this calculation are listed in \Cref{tab:datasets}.

\subsection{Repeatability}
Each dataset was tested three times to obtain a standard deviation of the results \citep{reimers_reporting_2017, chen_reconcile_2024, becker_multi-agent_2024}, ensuring reliable and robust performance estimates across multiple evaluations.



\section{Multi-Agent Framework}
\label{sec:mallm}
For our experiments, we use the \ac{MALLM} framework\footnote{Available here: \href{https://anonymous.4open.science/r/mallm}{anonymous.4open.science/r/mallm}}.

\subsection{Architecture Overview}
\label{sec:architecture}
To better understand the different modules, we take a closer look at each component and what role it plays in creating multi-agent discussions. An overview can be found in \Cref{fig:mallm_overview} as it provides an example workflow for the framework and how a discussion is created. The discussion starts with generating personas relevant to the given task and assigning them to the participating agents. The personas are generated using the same \ac{LLM} which is later used for the agents. After that the agents start to generate solutions and improve the suggestions from the other agents. The turn order of the agents is defined by the \textit{discussion paradigm}. This also defines which answers are visible to other agents and who can talk to whom. The \textit{response generator} defines how an agent receives the other answers and also the way it responds. After a certain number of rounds or when enough agents agree, a \textit{decision protocol} is used to select the best answer either via voting or just by looking for a certain consensus threshold. If the decision protocol fails, for example, due to a tied vote, the discussion continues for another round. In the framework a parameter can be defined to terminate discussions after a certain number of rounds to make sure they do not communicate forever.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{pdf/MALLM-Overview.drawio.pdf}
    \caption{Example multi-agent discussion conducted in the \ac{MALLM} framework. It showcases the functionality of the four modules and how they work together to get an improved final solution.}
    \label{fig:mallm_overview}
\end{figure*}

\paragraph{Agent Personalities.}
The first step of the discussion is the generation of agent personas. Each of the agents participating in the discussion has a certain persona assigned to them. This can unlock more knowledge for the \ac{LLM} on a specific topic \citep{kim_persona_2024}. To get the best results, we want as diverse personas as possible while still maintaining them to be relevant to the task. The default setting for the framework is to prompt a \ac{LLM} and ask for a persona relevant to the given task \citep{wang_unleashing_2024}. After each generation it also provides the generated personas to avoid duplication. This way of generating personas provides a good starting point, but as this is built as a modular component, it can be swapped out with another function, which, for example, generates half of the agents with this method and initializes the other ones as neutral agents without a persona.
\paragraph{Response Generators.}
Another important part of multi-agent discussions is how each agent responds to the previous responses. Do we use \ac{CoT} to improve performance, or does this result in too long answers? By changing the way an agent is prompted, a lot of performance can be gained or lost. Therefore, it is key to make this as customizable as possible. The researcher has the possibility to change the default behavior (neutral answers), for example, by prompting the agent to be more critical or changing the way the discussion history is presented. The system prompt for the agent's persona can also be adjusted. \ac{MALLM} already has many different built-in response generators. The ones relevant for this work are the following.
\begin{itemize}
    \item \textbf{Free Text} is the most basic form of the agent prompt. Each agent gets a predefined number of discussion history rounds as memory. The prompt language is neutral, and the task is presented each round to mitigate the potential drift from the topic of the discussion \citep{becker_multi-agent_2024}. In addition, the agent is always asked to agree or disagree with the answer of the previous agent.
    \item \textbf{Simple} behaves very similar to the Free Text response generator, but the prompt is a bit simpler to make it easier to understand for the \ac{LLM} and reduce the context length.
    \item \textbf{Critical} forces the agent to respond very critically to the previous answer and try to find new solutions. Some studies have shown that \acp{LLM} can show some form of sycophancy, which is not helpful for a constructive discussion \citep{sharma_towards_2023}. Encouraging them to be more critical may reduce this.
    \item \textbf{Reasoning} doesn't allow the agents to communicate their final solution with the other agents. They can only share reasoning that can be used to find a final solution. In the end, each agent has to come up with its own solution without being directly influenced by other agents.
\end{itemize}
\paragraph{Discussion Paradigms.}
These paradigms define the discussion format for the entire task. They can control the order in which the agents communicate with each other, and which answers are visible only to certain agents. Currently, all the built-in discussion paradigms are static, meaning that the turn order is predefined and cannot be changed based on specific events during the discussion. However, due to the modular nature of \ac{MALLM}, a new discussion paradigm can be added, for example using an \ac{LLM} as a moderator to dynamically decide which agent should respond next. Current research by \citet{yin_exchange--thought_2023} and \citet{becker_multi-agent_2024} shows that discussion protocols have little impact on downstream task performance. \ac{MALLM} includes the following discussion paradigms, which are illustrated in \Cref{fig:discussion_paradigms}. The first four paragdims are inspired by the work of \citet{yin_exchange--thought_2023}, while the fifth was developed as part of this work.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{pdf/Memory.drawio-1.pdf}
    \caption{Illustration of Discussion Paradigms available for use in \ac{MALLM}}
    \label{fig:discussion_paradigms}
\end{figure}

\begin{itemize}
    \item \textbf{Memory} is the most basic discussion paradigm. The agents respond to the solution of the previous agents with feedback or an improved solution. All answers are visible to the other agents. 
    \item \textbf{Relay} behaves similarly to the memory paradigm. The turn order is the same, but each agent can only see the answer from the previous agent.
    \item \textbf{Report} introduces one agent as a moderator that can communicate with other agents. The other agents can only communicate with the moderator and have access to these messages only. Only the moderator can see all messages.
    \item \textbf{Debate} is similar to the report paradigm, as it also needs a moderator. Here, the other agents can communicate for a predefined number of rounds before they forward their reasoning to the moderator agent, which starts the next round of debate.
    \item \textbf{Collective Refinement} In this protocol, each agent first generates an answer independently. In each subsequent round, every agent receives the responses from all other agents at the same time. Using this shared information, each agent refines their own answer. This process continues throughout the rounds, helping agents gradually reach a shared and improved solution. There is no turn order, and all agents have the same level of knowledge in each round.
\end{itemize}

\paragraph{Decision Protocols.}
These are crucial for the framework as they decide which answer gets presented as the final answer to the problem. Multi-agent discussions produce multiple results for the same problem because each agent has its own reasoning and ideas on how to solve the problem. Therefore, some process is needed to decide which answer looks the most promising. We divide these decision protocols into three subtypes that we want to analyze. An overview of how each of these decision protocols works theoretically can be found in \Cref{fig:decision_protocols} and all prompts used for them can be found in \Cref{sec:app_prompts}.

\begin{figure*}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{pdf/MALLM-Overview.drawio.pdf}
    \caption{Example multi-agent discussion conducted in the \ac{MALLM} framework. It showcases the functionality of the four modules and how they work together to get an improved final solution.}
    \label{fig:mallm_overview}
\end{figure*}

\subparagraph{Consensus Based Decision Protocols.} 
These are the simplest kinds of decision protocols. After each answer, the next agent has to agree or disagree with the previous statement. Depending on the response generator, this happens in the same message, and the agreement is extracted with a regular expression, or this is split into multiple answers. If enough agents agree in order, there is a consensus. The final answer is extracted by instructing the last agent to solve the given task with the information available in the latest messages. The prompt used for this can be found in \Cref{sec:app_final_answer_prompt}. There are several types of consensus decision protocols available in \ac{MALLM}. \textbf{Majority consensus} requires 50\% of the agents to agree. \textbf{Supermajority consensus} requires 66\% of the agents to agree, and \textbf{unanimity consensus} requires all agents to agree.

\subparagraph{Voting Based Decision Protocols.}
For voting based decision protocols, the process differs slightly compared to consensus-based decision protocols. The agents are forced to discuss for a predefined number of turns and afterward create a final solution. In the default setting, they have to discuss for three rounds, as current research such as \citet{du_improving_2023} shows that this allows for reasonable strong improvements considering computing resources. If there happens to be a tie in the voting, the agents have to discuss it for another round, and after that, they are asked to vote again. If they do not reach a final decision before exceeding the maximum number of rounds (defined in the discussion configuration), the solution of the first agent is used. To analyze the impact of the voting procedure, different processes similar to the work of \citet{yang_llm_2024} are implemented.
\begin{itemize}
    \item \textbf{Simple Voting} Each of the agents has only one vote. They can vote for any other agent or for themselves. The agent with the most votes wins.
    \item \textbf{Ranked Voting} The agents have to rank all final answers. The best solution is chosen by adding the ranking indices for a given agent and then selecting the answer with the best cumulative rank.
    \item \textbf{Cumulative Voting} Each agent has to distribute up to 25 points to all possible answers. They can also give fewer points and freely divide the points between all agents (even themselves). The winner is selected by adding all the points for a given agent and selecting the final answer with the most points.
    \item \textbf{Approval Voting} The agent has to provide a list of solutions that it approves. After that, the approvals from all agents are counted, and the answer with the most approvals wins the vote.
\end{itemize}
\clearpage




\section{Prompts}
\label{sec:app_prompts}
\subsection{Final Answer Extraction}
\label{sec:app_final_answer_prompt}
\medskip
\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are tasked with creating a final solution based on the given input and your previous response.\\
    Task: \texttt{<task>}\\
    Input: \texttt{<input sample>}\\
    Your previous response: \texttt{<previous answer>}\\
    Extract the final solution to the task from the provided text. Remove statements of agreement, disagreement, and explanations. Do not modify the text. Do not output any text besides the solution. If there is no solution provided, just copy the previous response.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to extract the final answer of a given agent from its previous response.}
    \label{fig:extract_prompt}
\end{figure}

\subsection{Voting Prompts}
\label{app:voting_prompts}

\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are tasked with voting for the best solution from the list provided below based on the given task.\\
    Task: \texttt{<task>}\\
    Question: \texttt{<input sample>}\\
    Here are the possible solutions:\\
    Solution 1: \texttt{<agent 1 final answer>}\\
    Solution 2: \texttt{<agent 2 final answer>}\\
    Solution 3: \texttt{<agent 3 final answer>}\\
    Based on the above solutions, please provide the number of the solution you are voting for. Answer only with the number.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to get a vote from each agent for the Simple Voting decision protocol.}
    \label{fig:simple_voting_prompt}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are tasked with approving any number of solutions from the list provided below based on the given task.\\
    Task: \texttt{<task>}\\
    Question: \texttt{<input sample>}\\
    Here are the possible solutions:\\
    Solution 1: \texttt{<agent 1 final answer>}\\
    Solution 2: \texttt{<agent 2 final answer>}\\
    Solution 3: \texttt{<agent 3 final answer>}\\
    Based on the above solutions, please provide the numbers of the solutions you are approving, separated by commas. Answer only with the numbers.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to get a vote from each agent for the Approval Voting decision protocol.}
    \label{fig:approval_voting_prompt}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are tasked with distributing 10 points among the provided solutions based on the given task.\\
    Task: \texttt{<task>}\\
    Question: \texttt{<input sample>}\\
    Here are the possible solutions:\\
    Solution 1: \texttt{<agent 1 final answer>}\\
    Solution 2: \texttt{<agent 2 final answer>}\\
    Solution 3: \texttt{<agent 3 final answer>}\\
    Based on the above solutions, please distribute 10 points among the solutions. Provide your points allocation as a JSON dictionary where keys are solution numbers (as int) and values are the points. The total points should sum up to 10. Answer only with the JSON dictionary.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to get a vote from each agent for the Cumulative Voting decision protocol.}
    \label{fig:cumulative_voting_prompt}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are tasked with ranking the solutions from the most preferred to the least preferred based on the given task.\\
    Task: \texttt{<task>}\\
    Question: \texttt{<input sample>}\\
    Here are the possible solutions:\\
    Solution 1: \texttt{<agent 1 final answer>}\\
    Solution 2: \texttt{<agent 2 final answer>}\\
    Solution 3: \texttt{<agent 3 final answer>}\\
    Based on the above solutions, please provide the rankings of the solutions separated by spaces. Example: '0 2 1' if you prefer Solution 0 the most, then Solution 2, and finally Solution 1. Provide up to 5 rankings. Only answer with the rankings.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to get a vote from each agent for the Ranked Voting decision protocol.}
    \label{fig:ranked_voting_prompt}
\end{figure}







\subsection{Challenge Prompt}
\label{app:challenge_prompt}
\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are a participant in a group discussion. \\
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    The task is: \texttt{<task>}. The question is: \texttt{<question>}. \\
    This is the final answer generated by the discussion: \texttt{<final\_answer>}. \\
    Please critically evaluate this answer. If you agree with the final answer, respond with the exact word 'AGREE' to confirm. If you do not agree, respond with the exact word 'DISAGREE' to challenge the answer.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to challenge the final answer.}
    \label{fig:challenge_prompt}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{combinedprompt}
    \textbf{System Prompt:} \\
    \begingroup
    \colorbox{systemcolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    You are a participant in a group discussion. \\
    Your role: \texttt{<persona>} (\texttt{<persona description>})
    }}
    \endgroup

    \vspace{0.4em} %

    \textbf{User Prompt:} \\
    \begingroup
    \colorbox{usercolor}{\parbox{\dimexpr\linewidth-2\fboxsep\relax}{
    The task is: \texttt{<task>}. The question is: \texttt{<question>}. \\
    This is the final answer generated by the discussion: \texttt{<final\_answer>}. \\
    You dont agree with the final answer. Please provide a new answer to the question. Include the letter corresponding to your answer in the solution.
    }}
    \endgroup
\end{combinedprompt}
    \caption{Prompt used to generate a new answer in case the final answer got challenged.}
    \label{fig:challenge_new_prompt}
\end{figure}
\newpage
\onecolumn
\section{MALLM Setup}
\label{sec:mallm_setup_app}

    
\begin{configpython}[Default Parameters used for each experiment]{lst:config_default}
input_json_file_path: str = None
output_json_file_path: str = None
task_instruction_prompt: str = None
task_instruction_prompt_template: Optional[str] = None
endpoint_url: str = "https://api.openai.com/v1"
model_name: str = "gpt-3.5-turbo"
api_key: str = "-"
max_turns: int = 10
skip_decision_making: bool = False
discussion_paradigm: str = "memory"
response_generator: str = "simple"
decision_protocol: str = "hybrid_consensus"
visible_turns_in_memory: int = 2
debate_rounds: int = 2
concurrent_api_requests: int = 100
use_baseline: bool = False
use_chain_of_thought: bool = True
num_agents: int = 3
num_neutral_agents: int = 0
agent_generator: str = "expert"
agent_generators_list: list = []
trust_remote_code: bool = False
num_samples: Optional[int] = None
hf_dataset_split: Optional[str] = "test"
hf_token: Optional[str] = None
hf_dataset_version: Optional[str] = None
hf_dataset_input_column: Optional[str] = None
hf_dataset_reference_column: Optional[str] = None
hf_dataset_context_column: Optional[str] = None
use_ablation: bool = False
shuffle_input_samples: bool = False
all_agents_generate_first_draft: bool = False
all_agents_generate_draft: bool = False
policy: Optional[str] = None
voting_protocols_with_alterations: bool = False
calculate_persona_diversity: bool = False
\end{configpython}
\newpage
\section{Example Discussions}
\subsection{Successfull Voting Discussion}
All decision protocols are attached as an example to this discussion. The original discussion was created using simple voting.
\input{discussions/good_discussion}
\newpage
\subsection{Agents Tricked Discussion}
In this discussion, the agents were tricked by information provided in the context.
\input{discussions/trick_discussion}
\newpage
\subsection{Bad Voting Discussion}
In this discussion, the agents were tricked by one agent who came up with a solution not provided in the context.
\input{discussions/voting_failed_discussion}
\newpage
\subsection{Majority Consensus Discussion}
In this discussion, the agents discussed only one round, as they already had a high enough agreement score.
\input{discussions/majority_discussion}
\newpage
\section{AI Usage Card}
\input{ai-usage-card}
