\section{Methodology}

In the following, we explain the multi-agent environment, decision protocols, response generators, and datasets used in our experiments. 

\subsection{Setup}
\label{sec:setup}
We run multi-agent debates based on three key components: a discussion paradigm, a decision protocol, and an agent response generator.
Each discussion consists of three automatically generated expert personas for the task following \citet{kim_persona_2024}. 
While the number of personas can vary, \citet{yin_exchange--thought_2023} found this number to be the most efficient.
Using personas is important as it generates agents with expertise in different domains, incorporating diverse viewpoints for the final decision.
After that, the agents discuss the problem for multiple turns.
The number of turns varies from one to five, depending on the experiment setup and decision protocol.
The \textit{decision protocol} defines what criteria must be met for the discussion to end and how the final solution will be created.
Each agent can generate one answer per turn and, based on the agent's turn order, respond to the other agents as a default behavior.
If not explicitly stated, all agents exchange messages with one another.
These discussion characteristics are defined by the \textit{discussion paradigm}.
For consensus decision protocols, each agent also indicates whether they agree with the previous message.
We repeat this process each round until a certain level of agreement (defined by the decision protocol) is reached, which leads to the final solution. 
For voting decision protocols, the agents start voting after the third turn.
If they successfully agree on a solution in any turn after the third, the discussion ends.
Agents can only memorize messages from up to two turns to limit the context length provided to the agent.
We use a \textit{response generator} to define how agents respond to previous messages.
It determines how the discussion history is presented, what additional information is provided, how the persona is introduced, and in which tone they should respond (e.g., neutral or critical).
Additional details on the experimental setup can be found in \Cref{sec:mallm}.

The \acp{LLM} used for the experiments are Llama 3 8B and 70B\footnote{The two models used for this study can be found at \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{meta-llama/Meta-Llama-3-8B-Instruct} and \href{https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct}{meta-llama/Meta-Llama-3-70B-Instruct}} \citep{dubey_llama_2024}.
Within one discussion, all prompted agents use the same base model. 
The smaller model uses two NVIDIA RTX5000 with 16 GB of VRAM (for $\sim250$ hours), and the larger model uses eight NVIDIA A100 with 40GB VRAM (for $\sim40$ hours). 
A list of experiment-specific parameters is available in \Cref{sec:mallm_setup_app}.

\subsection{Agent Prompts and Decision Protocols}
\label{sec:decision_protocols_main}
\input{tables/results_8b}
Prompt design has a marked impact on multi-agent debate.
For this work, we propose three response generators.
The \textbf{Simple Response Generator} is our default in which agents are prompted to answer neutral and unbiased to previous messages. 
The \textbf{Critical Response Generator} encourages agents to critically assess prior answers and propose new solutions, countering sycophantic tendencies \cite{sharma_towards_2023}. 
The \textbf{Reasoning Response Generator} restricts agents to sharing only reasoning paths, thus avoiding bias towards agreeing with other agents' final solutions.

Decision protocols then determine the final solution of the discussion by selecting the most promising solution based on predefined mechanisms. 
We use two classes of protocols in this work. 

\textbf{Consensus decision protocols} decide on an answer by prompting the agents to converge on one shared solution. 
The solution is selected when a required level of agreement among agents is reached.
We explore three major agreement levels: \textit{majority consensus} - more than $50\%$ agreement, \textit{supermajority consensus} - more than $66\%$ agreement, and \textit{unanimity consensus} - all agents have to agree.
We extend the majority consensus of \citet{yin_exchange--thought_2023} by higher agreement levels, i.e., supermajority (66\%), unanimity (100\%).

\textbf{Voting decision protocols} allow several possible solutions to be presented in parallel during the discussion, and ultimately all agents must vote on a final solution.
If there is a tie in the voting, all agents will discuss for another round and then vote again.
Our work includes four different voting decision protocols inspired by \citet{yang_llm_2024}:
\textit{simple voting} - each agent casts one vote, and the answer with the most votes wins;
\textit{ranked voting} - each agent ranks the solutions from best to worst to find the solution that consistently has the highest rank over all individual rankings;
\textit{approval voting} - each agent casts unlimited votes, and the answer with the most votes wins; and
\textit{cumulative voting} - each agent is allowed to divide up to 25 points between all solutions. The solution with the most points wins.


\subsection{Datasets}
\label{sec:datasets_main}

We evaluate our decision protocols using six datasets: three knowledge tasks (i.e., MMLU, a broad-topic test; MMLU-Pro, a domain-specific test with challenging questions; GPQA, a specialized question set difficult for web search) and three reasoning tasks (i.e., StrategyQA, a multistep reasoning task; MuSR, a long-context murder mystery; SQuAD 2.0, a reading comprehension task with questions that may or may not have answers in the context).
Because of computational constraints, we calculate the task performance on a subset of samples with three runs and report their standard deviation.
More details about the datasets, the number of samples, and the sampling strategy can be found in \Cref{appendix:datasets}.












