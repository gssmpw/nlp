\section{Experiments}
We use a set of diverse reasoning and knowledge tasks to explore the effectiveness and limits of decision protocols in multi-agent debate.

\footnotetext{Approval Voting is left out as it consistently fails to reach a voting decision as described in \Cref{sec:experiment1}.}

\subsection{Performance of Decision Protocols}
\label{sec:experiment1}

Multi-agent debates for task solving have shown promise in recent research, but they rely on a varying choice of decision protocols hindering systematic comparison \citep{yin_exchange--thought_2023, chen_reconcile_2024}. 
This experiment systematically compares a set of seven decision protocols, changing only the decision protocol used and nothing else.
Specifically, we determine whether some protocols have advantages over others and how they behave in specific edge cases, such as for unanswerable questions.

We experiment with four voting-based (Simple, Ranked, Cumulative, Approval) and three consensus-based (Majority, Supermajority, Unanimity) methods and test them on three knowledge tasks (MMLU, MMLU-Pro, GPQA) and three reasoning tasks (SQuAD 2.0, StrategyQA, MuSR).
We use the Llama 3 8B model with and without \ac{CoT} prompting to generate baseline results for comparison with multi-agent debate.
We compare answerable and unanswerable questions of the SQuAD 2.0 dataset to inspect how decision protocols behave in edge cases.

\Cref{tab:results_8b} shows the baselines compared to multi-agent debate, with the decision protocols being the rows grouped by voting and consensus-based protocols.
The columns are the datasets grouped by knowledge and reasoning-based tasks.
Overall, multi-agent debate with consensus and voting decision protocols outperforms the \ac{CoT} baseline. 
Notably, consensus protocols perform better on knowledge-based tasks with average improvements of $2.3\%$ on MMLU, $4.9\%$ on MMLU-Pro, and $1.3\%$ on GPQA over the voting average.
Voting protocols perform better in reasoning tasks with average improvements of $13.1\%$ on SQuAD 2.0, $0.2\%$ on StrategyQA, and $26.4\%$ on MuSR over the consensus average.
The results for the Llama 3 70B model can be found in \Cref{app:decision_70b}.
On average, voting-based protocols require $3.38$ rounds and consensus-based protocols $1.42$ rounds to reach a decision, indicating that consensus is more suitable for applications that require quick decision-making as they use less test-time compute.
Interestingly, most conversations reach a decision before they are terminated after five turns.
Approval voting is the only protocol that often leads to no agreed solution ($59\%$ of cases) due to the agreeableness of the agents. 
The agents often simply vote for all answers as they do not have to decide, leading to many ties (see \Cref{app:termination_percent} for details). 
\Cref{fig:squad_divided} shows the F1-Score for SQuAD 2.0 divided into whether questions are answerable or not for each decision protocol.
Voting achieves a higher overall F1-score (56.7\%) than consensus (43.6\%), especially for answerable samples.
However, consensus is more effective for unanswerable samples, and stricter consensus methods (e.g., unanimity consensus) produce even higher scores.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pdf/hasan.pdf}
    \caption{F1{\tiny$\pm$std} of different decision protocols on SQuAD 2.0 divided into three ablation groups: (middle) samples with an answer in the context (\textbf{Sample has Answer}), (right) samples with no answer in the context (\textbf{Sample has no Answer}), and (left) the combination of both (\textbf{All Samples}). %
    Standard deviation over three runs.}
    \label{fig:squad_divided}
\end{figure}

The effectiveness of decision protocols depends on the nature of the task. 
Our results suggest that consensus excels in knowledge tasks, and voting works better for reasoning tasks.
Voting protocols might outperform consensus in reasoning tasks because they allow the exploration of multiple reasoning paths. %
One reason for the good performance of consensus in knowledge tasks is that it mitigates individual agent errors by requiring multiple agents to agree on the same statement before reaching a conclusion. 
This approach is less susceptible to manipulation or errors by any single agent. 
We found voting can be easily tricked by generating answers that seem to be correct but have some incorrect statements (see \Cref{example:trick_discussion}).
The consensus method has a distinct advantage by incorporating a safeguard of repeated checks across agents to find these small errors.
The detailed analysis of SQuAD 2.0 further supports the finding that decision protocol effectiveness is task-dependent. 


Both experiments reveal that the used decision protocol can have a marked impact on the overall performance of multi-agent debates.
With the correct choice, multi-agent debates can be made more error-resistant by using consensus decision protocols and more explorative by using voting decision protocols.



\subsection{Number of Agents and Discussion Rounds}
\label{sec:experiment-numagents}
Research by \citet{yin_exchange--thought_2023} suggests that increasing the number of rounds and agents participating in a multi-agent debate is beneficial because of test-time compute scaling.
In our experimental setup, agents communicate for three rounds before voting, following \citet{du_improving_2023}. 
As our first experiment showed, consensus reaches a decision much faster than voting.
Here, we investigate how the number of discussion rounds before voting impacts task performance, as fewer rounds with the same accuracy would improve computational efficiency.
\citet{becker_multi-agent_2024} suggests that extended discussions may lead to decreased accuracy because of agents drifting away from the original task. 
\citet{wang_rethinking_2024} shows increasing the number of agents may have positive effects. 

We conduct this experiment using the StrategyQA dataset because multi-agent debate has consistently mitigated errors of single agents using \ac{CoT} as shown in the previous experiment.
The experiment is structured in two parts.
First, we fix the number of agents to three and increase the number of discussion rounds from one to ten. 
Second, we fix the number of rounds to three and increase the number of agents from one to ten. 
Both experiments use the \textit{simple voting} decision protocol, and each condition is evaluated across three independent runs. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{pdf/numrounds-1.pdf}
  \caption{Accuracy{\tiny$\pm$std} on StrategyQA when the agents have to talk for a given number of rounds before they are allowed to vote using the simple voting decision protocol. Standard deviation over three runs.
  \vspace{-0.5cm}}
  \label{fig:num_rounds}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{pdf/numagents-1.pdf}
  \caption{Accuracy{\tiny$\pm$std} on StrategyQA with a different number of agents participating in the discussion. The final answer is created using the simple voting decision protocol. Standard deviation over three runs.
  \vspace{-0.5cm}}
  \label{fig:num_agents}
\end{figure}

\Cref{fig:num_rounds} shows accuracy relative to the number of rounds the agents have to communicate before voting starts. 
A linear regression, visualized by the dotted line, indicates that increasing discussion rounds slightly reduces accuracy. 
Still, with the increase from nine to ten rounds a rather abrupt increase in task performance can be seen.  
Separately, \Cref{fig:num_agents} shows the task performance relative to the number of agents participating in the debate. 
Here, a linear regression indicates that increasing the number of agents leads to a slight upward trend in accuracy, suggesting that larger groups improve task performance.


Having more agents generating solutions generally leads to higher accuracy.
This resembles the effect seen in self-consistency \citep{wang_self-consistency_2022}, where increasing the number of sampled answers improves task performance.
However, \Cref{fig:num_rounds} shows that increasing the number of debate rounds decreases performance, contradicting the expected benefits of self-refinement.
This aligns with recent concerns about the reliability of self-refinement, as \citet{huang_large_2024} question the findings of \citet{madaan_self-refine_2023}, suggesting that the claimed improvements may not be robust.

At the same time, we also have one case where we see an increase in task performance as the number of rounds increases. Observe the sharp upside ark in \Cref{fig:num_rounds} for turn ten.
To test whether individual rounds can recover this performance decrease, we create an ablation experiment where agents challenge the final result and propose a new solution.
This extra round allows us to verify the effects of an extended discussion and also if agents would change their final solution.
We create five different challenge scenarios and test them on two knowledge tasks (MMLU and MMLU-Pro) and two reasoning tasks (StrategyQA and SQuAD 2.0).
In the first setting, we provide agents with the final solution and ask whether they agree or want to change it. 
In the second setting, agents can access the full discussion history with prior reasoning. 
The third setting includes additional context we retrieve with RAG from Wikipedia\footnote{We use: \href{https://github.com/Multi-Agent-LLMs/context-plus}{github.com/Multi-Agent-LLMs/context-plus}}.
The fourth and fifth settings act as a baseline, introducing intentionally incorrect solutions---either irrelevant or wrong answers generated by the Qwen2 7B \citep{yang_qwen2_2024} model to test whether agents can detect incorrect solutions. 
Solutions are generated with a different model than Llama 3, to remove any bias that might occur if the same model used to detect the wrong answer also generated it \cite{wahle_are_2021}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pdf/challenge-2.pdf}
    \caption{Percentage of agents challenging the final solution with different levels of information. %
  }
    \label{fig:challenge}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pdf/challenge_improve-1.pdf}
    \caption{Percentage of answers that improve after being challenged using different levels of information. %
    \vspace{-0.3cm}}
    \label{fig:challenge_improve}
\end{figure}

We find that agents are less likely to challenge the final result when the discussion history is provided (10\% decrease), as seen in \Cref{fig:challenge}.
For the StrategyQA dataset, this change is more prominent, with a 60\% decrease in the challenge rate.
Providing additional information has no noticeable effect compared to just providing the solution.
Agents can detect irrelevant solutions ($99\%$ challenge rate) and incorrect solutions ($75\%$ challenge rate) with high accuracy.
\Cref{fig:challenge_improve} shows the percentage of challenged answers with a changed final score based on the new solution.
It can be seen that this has no positive effect, especially on the reasoning tasks (StrategyQA and SQuAD 2.0), where it leads to worse solutions in 3\% of cases.
There are no major effects on knowledge tasks (MMLU and MMLU-Pro).
Overall, the different challenge scenarios (Only Solution, Discussion History, and Additional Information) do not significantly impact the quality of the challenged answers, as they all perform very similarly.

These findings further support that more debate rounds harm task performance, suggesting the expected benefits of self-refinement may be unreliable and also differ from task to task.
Therefore, we recommend scaling the number of agents as opposed to turns to create a larger knowledge base.%



\subsection{Answer Diversity}

\label{sec:experiment2}


 
\begin{table*}[t]
\centering
\resizebox{0.65\textwidth}{!}{  
\begin{tabular}{l|lllll}
\toprule
 \textbf{Strategy}& \textbf{Baseline} & \textbf{All draft} & \textbf{Collective} & \textbf{Critical} & \textbf{Reasoning} \\ \midrule
\textbf{Answer Cosine Similarity}    & 0.888      & 0.870     & 0.845       & 0.843      & 0.916        \\ 
\textbf{Mean Accuracy}    & 58.3\%      & 62.8\%  & 65.7\%   & 59.4\% & 51.9\%               \\ 
\textbf{Delta Baseline}    & 0.0\%    & 3.3\%& 7.4\%   & 1.1\%     & -6.4\%              \\ \bottomrule
\end{tabular}}
\caption{Final answer similarity based on average cosine similarity between SBERT embeddings of final answers compared to task performance on StrategyQA dataset.
\vspace{-0.5cm}}
\label{tab:embed-strategies-transposed}
\end{table*}

In multi-agent debates, answer diversity plays an important role in improving decision-making and task performance.
A more diverse set of answers is beneficial because selecting the correct solution from multiple options is often easier than relying on a single agent's solution \citep{zheng_judging_2023}. 
This principle is also key to self-consistency approaches \citep{wang_self-consistency_2022} and explains why multiple-choice tests are often easier than open-ended ones. 
The goal of this experiment is to exploit this property and explore ways to increase answer diversity to optimize task performance.

In typical multi-agent debates, the first agent starts generating a possible solution for the given task.
The next agent can either propose a new solution or improve on the previous solution.
Throughout our experiments, we observe that agents are agreeable and often only improve the answer from the first agent without proposing an idea based on their own expertise.
We propose three methods to counteract this issue and to improve answer diversity.
First, we introduce \acf{AAD}, a method that forces each agent to generate an independent solution based on their own expertise in the first round. 
Second, we propose \acf{CI} which starts similarly to \ac{AAD} with each agent generating an independent solution, but unlike \ac{AAD} where each agent can communicate with another after the first turn, \ac{CI} prohibits communication and only shows the solutions from the previous turn to the agent in the next turn. 
The rationale behind this is that agents often immediately get biased by other proposals rather than following their own ideas which leads to less answer diversity.
\ac{CI} only works for voting decision protocols as it removes the turn order of the agents, and therefore, no iterative improvements and agreements can be made.
Third, we introduce a \textit{critical response generator} and a \textit{reasoning response generator}. 
Both test whether it is possible to improve answer diversity by prompting agents to respond more critically or by allowing them to exchange only reasoning steps and no final solutions during the discussion to avoid agreeableness towards final solutions and not reasoning ideas.
To quantify if these methods increase answer diversity, we calculate the cosine similarity between the SBERT embeddings \citep{reimers2019sentencebertsentenceembeddingsusing} of the agent's answers and correlate it with changes in task performance.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pdf/all-agents-draft.pdf}
    \caption{Comparison of agents iterating on one initial draft (baseline) vs. each of the three agents generating an initial draft separately on the StrategyQA dataset using \ac{AAD}. Standard deviation over three runs.
    \vspace{-0.5cm}}
    \label{fig:all_agents_draft}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pdf/collective-1.pdf}
    \caption{Comparison of agents discussing using the default discussion protocol compared to the \ac{CI} discussion protocol on the StrategyQA dataset. Standard deviation over three runs.
    \vspace{-0.5cm}}
    \label{fig:collective}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{pdf/response-generator-1.pdf}
    \caption{Comparison of agents using the default response generator compared to the reasoning and critical response generator on the StrategyQA dataset. Standard deviation over three runs.}
    \label{fig:response_generator}
\end{figure}
\Cref{fig:all_agents_draft} shows the accuracy of \ac{AAD} compared to the multi-agent baseline on StrategyQA for all decision protocols. 
\ac{AAD} increases the performance on average by $3.3\%$ over the baseline. 
The strongest improvements are for simple and ranked voting.
Cumulative voting, supermajority, and unanimity consensus perform a bit worse.
Approval voting and majority have no real improvement but are within the standard deviation of the baseline.

\Cref{fig:collective} shows the accuracy of \ac{CI} compared to the multi-agent baseline. 
\ac{CI} is a voting--only extension. 
On average, \ac{CI} performs 7.4\% better than the baseline.

\Cref{fig:response_generator} shows the accuracy of different response generators compared to the baseline for different decision protocols. 
The results show that the \textit{critical response generator} does not provide a reliable improvement, while the \textit{reasoning response generator} even leads to a decrease in task performance.

\Cref{tab:embed-strategies-transposed} shows the cosine similarity, mean accuracy, and delta in performance as the rows for the different improvement methods (\ac{AAD}, \ac{CI} and \textit{response generators}) as columns.
We observe a general trend that if the answer diversity increases, task performance increases, too. The \textit{reasoning response generator} decreases answer diversity, which results in a drop in task performance.


\ac{AAD} and \ac{CI} achieve higher answer diversity and, therefore, higher task performance, but the two response generators struggle to provide reliable results and may even reduce task performance. 
The critical and restrictive prompting style degrades the quality of the discussion by forcing a specific answer style that is not always beneficial.
Therefore, we recommend using our methods \ac{AAD} or \ac{CI} to limit group interactions and promote independent thinking but caution against using methods that directly change the response behavior of the agents, as this may have unwanted side effects.










