\section{Understanding AI Systems' Carbon Footprint}

\begin{figure}[t]
    \centering
\includegraphics[width=0.9\linewidth]{plots/CF.pdf}
\vspace{-1.5em}
    \caption{Trends in bit density (left) and embodied carbon footprint (right) across various DRAM memory technologies for 3 different manufacturers.}
    \label{fig:bitdensity_carbon}
    \vspace{-1em}
\end{figure}


Based on recent efforts quantifying the carbon footprint of computing systems~\cite{ACT, SCARIF, eeckhout2024focal}, the aggregate carbon emissions of AI systems is the combination of operational and embodied carbon as shown in the equation below:
{\footnotesize 
$$
    CF_{Task}  =  CF^{emb} + CF^{op} 
= (P_{host} + P_{gpu})\times t \times CI + CF^{emb}_{host} \frac{t}{LT} + CF^{emb}_{gpu} \frac{t}{LT}
 $$
}
where $P$ represents the power consumption of the host and GPU, $t$ represents the workload run-time, $LT$ represents the hardware lifetime, and $CF^{emb}$ represents the host and GPU embodied carbon.
In terms of operational emissions, we measure the power consumption of host-processing systems including CPUs, DRAM, SSD, using a combination of Intel RAPL~\cite{RAPL}; the power consumption of GPUs is empirically measured using NVML~\cite{nvml}.
The carbon intensity of the different data center power grids is based on geo-temporal variance of power grid carbon intensities~\cite{acun2023carbon, greensku, watttime,electricitymaps}.
To estimate embodied carbon, we leverage a combination of product environmental reports and life cycle analyses from industry~\cite{dellr740} and open-source tools to quantify the emissions from hardware manufacturing including the Architectural Carbon Modeling Tool (ACT), SCARIF, and GreenSKU~\cite{ACT, SCARIF, greensku}. There is a void in fine-grained modeling of different memory technology nodes for GPUs, which significantly impacts the embodied carbon footprint estimation. Furthermore, the peripheral components for cooling or power delivery are also not considered in the past models, which is essential for higher-end GPUs~\cite{ACT, SCARIF}.


In this section we describe how we extend current methods to study the embodied carbon of AI inference platforms including host systems, motherboards, and GPUs.





\subsection{Modeling Embodied Carbon of AI Systems}

The embodied carbon footprint (CF) owes to a combination of components for the host systems and GPUs. 
For host systems, we must account for the main CPU die, DRAM memory, SSD and HDD storage, printed wiring board (PWB), power delivery network (PDN), and server chassis.
Similarly, for the GPUs we must account for SoC, DRAM memory, PDN, cooling (e.g., heat sink), and PWB.
Table~\ref{tab:model} describes the salient parameters in the embodied carbon model for AI inference systems.
For many components including PDN and cooling (e.g., heat sink) given the lack of specific technology information we rely on publicly available product environmental reports from Dell~\cite{dellr740}.
Below we detail the methodology for estimating the embodied carbon of the remaining salient components. 

\subsubsection{Integrated circuits (Application Processors)} 
To compute the embodied carbon emissions of application processors (e.g., CPU die, GPU die) we rely on recently published architectural carbon modeling tools~\cite{ACT, SCARIF, imec-dtco20, eeckhout2024focal}.
For application processors we input the process technology and die area into ACT~\cite{ACT, imec-dtco20}.






\subsubsection{Memory and Storage}
Recent efforts from industry demonstrate memory and storage dominate the embodied carbon of Microsoft's data center hardware~\cite{greensku}.
To quantify the memory and storage embodied carbon, we consider both memory technologies and capacities.
For storage, we estimate SSD emissions based on the Dell R740 life cycle analysis~\cite{dellr740}, yielding an average estimate of 0.110 kg CO$_2$e per GB capacity; this is a conservative estimate compared to estimates of 0.160 kg CO$_2$e per GB from academic surveys~\cite{dirty_ssd}.
For memory, we extend existing ACT and SCARIF models by scaling wafer-scale DRAM data for different technologies from TechInsights~\cite{TechInsights,techinsights_micron_dram_2024,skhynixHynixNewsroom} by publicly available DRAM bit-densities per technology from memory vendors (e.g., SK Hynix, Micron, Samsung) as shown in Figure~\ref{fig:bitdensity_carbon}. 
The trends show newer technology nodes, given higher bit-densities, exhibit lower embodied carbon per GB capacity.
For the purposes of this work we estimate the DDR4/LPDDR5, GDDR6, and HBM2, HBM3e embodied carbon footprint as 0.29, 0.36, 0.28, 0.24 kg CO$_2$e per GB respectively.







\begin{figure}[t]
\includegraphics[width=0.9\linewidth]{plots/gpu_comparison_fixed.pdf}
\vspace{-1.5em}
    \caption{Trends in embodied carbon, power, and cloud cost for different generations of GPUs. As GPU performance increases (left to right), power consumption, cost and embodied carbon exhibit distinct trends, ACT only accounts for around 20\% in the blue SoC component ~\cite{aws-gpu,azure-gpu,lambdalabsCloudDeep}.  }
    \label{fig:cf_gpu}
    \vspace{-1em}
\end{figure}




\subsubsection{Mainboard Printed Wiring Board (PCB)}
To estimate the embodied emissions of the mainboard printed wiring board (PWB) we use the Dell R740 LCA~\cite{dellr740}. 
Given the PWB area of the Dell R740, 1925 square centimeters (176 kg CO$_2$), is significantly higher than modern server-class host systems and GPU's, we scale the PWB area for specific systems.
Thus, we estimate the embodied carbon of PWB to be:
$
    \text{Total PCB CF} = 0.048 \times Area_{PCB}
$.














\subsection{Characterizing Total Carbon}\label{sec:characterization}
Based on the carbon models and runtime energy measurement, we can study the carbon breakdown of AI inference platforms. 








\textbf{Observation 1: Along with power capacity (TDP), embodied carbon of GPUs continues to rise as systems increase in compute and memory bandwidth. }




Figure~\ref{fig:cf_gpu}  analyzes the embodied carbon estimates of various NVIDIA GPUs, breaking it down into key components such as memory, printed circuit board (PCB), power delivery network (PDN), cooling systems, and system-on-chip (SoC). The breakdown aligns with findings from the recent TPU embodied carbon modeling paper~\cite{schneider2025life}.  Figure~\ref{fig:cf_gpu} illustrates these components using stacked bars to represent embodied carbon, with the thermal design power (TDP) plotted on the right axis. We observe a clear trend: with newer GPU generations, computational performance, memory bandwidth, and energy efficiency improve. However, the embodied carbon also rises with increasing computational performance, advanced thermal solutions required, and improved memory capacity. 
For instance, compared to an NVIDIA H100, and NVIDIA L4 incurs 3$\times$ lower embodied carbon (see Section~\ref{sec:rightsize}).


\textbf{Observation 2: Host systems levy high embodied carbon costs on end-to-end AI systems, primarily due to the mainboards, memory, and storage components.}



\begin{figure}[t]
    \centering
\includegraphics[width=0.9\linewidth]{plots/cpu_breakdown.pdf}
\vspace{-1em}
    \caption{ Embodied carbon breakdown of full inference systems available in cloud offerings from Azure and LambdaLabs~\cite{azure-gpu,lambdalabsCloudDeep}, varying the number and type of GPU. Host-processing systems account for over half of the embodied carbon in AI systems, owing largely to memory, storage, and mainboard overheads.}
    \label{fig:cf_cpu_h}
    \vspace{-2em}
\end{figure}





Figure~\ref{fig:cf_cpu_h} shows the embodied carbon breakown of end-to-end inference systems including both GPUs and host systems.
The analysis highlights that the motherboard, DRAM, and storage constitute a significant portion of the embodied carbon impact.
However, for AI inference workloads, host systems are frequently underutilized compared to GPUs, leading to a disproportionate contribution of their embodied carbon footprint relative to their actual usage during operations.
This observation motivates exploring strategies such as resource sharing on the host (e.g., model multiplexing), improving utilization through CPU-based inference, extending the lifetime of host system components, and reducing the memory subsystem's footprint (Section~\ref{sec:reduce}).


\textbf{Observation 3: As low-carbon-intensity energy sources increasingly dominate hyperscalar data centers, the carbon footprint of AI infrastructure is increasingly determined from the embodied carbon of both GPUs and host systems.}


Figure~\ref{fig:cf} compares embodied and operational carbon per second across datacenters with different power sources. The host and GPU embodied carbon remain fixed, while operational carbon varies with energy source carbon intensity. In high-carbon regions (Europe, US-East), GPU operational carbon dominates. As cleaner energy is adopted, operational emissions drop, making embodied carbon more significant. GPU power accounts for most operational carbon, while the host system dominates embodied carbon. Optimizing operational carbon requires improving GPU energy efficiency, minimizing idle power consumption, dynamic workload allocation, and clean energy integration. Reducing embodied carbon involves increasing host utilization, extending hardware lifetimes, and improving resource sharing to amortize the embodied carbon cost of host components (see Section~\ref{sec:recycle}).

\textbf{Observation 4: CPU is underutilized in AI inference systems.} During auto-regressive inference, CPUs are responsible for the following operations: (1) waiting for incoming requests and processing HTTP and GRPC operations, (2) input tokenization, (3) prefix matching and request ordering, (4) allocating tensors, memory, and metadata for batches, and (5) inference post-processing including de-tokenization, speculative decoding, and beam search processing.
Profiling several models (i.e., opt125m, Metallama-3-8B, Mistral and Mixtral) and request rates (2-32 requests per second from ShareGPT~\cite{sharegpt}) on vLLM (v0.6.2) and SGLang~\cite{zheng2023efficiently}, we find average CPU utilization is around 6\% of a 16-core processor (e.g., on average only a single CPU core is used).
Newer optimization like overlapped schedulers are proposed, so CPU processing is not on the critical path of recent LLM inference schedulers~\cite{vllm2024perfupdate}. For multimodality, encoder-based models or agentic systems, CPU can have a higher utilization and thus  in itself provide an opportunity to \textit{reuse the CPU side of the host system, while \textit{recycling} or \textit{reducing} the memory subsystem (See Section~\ref{sec:reuse}).} 

\begin{figure}[t]
    \includegraphics[width=0.8\linewidth]{plots/carbon_breakdown.pdf}
    \vspace{-1.5em}
    \caption{Embodied and operational carbon breakdowns of an AI inference system comprising an NVIDIA A100 GPU. We assume a 4-year lifetime running Llama-13B. In data centers and power grids with high degrees of renewable energy (e.g., hyper-scalars), embodied carbon dominates the overall carbon footprint of AI systems.}
    \label{fig:cf}
    \vspace{-1em}
\end{figure}
