\subsubsection{\textbf{Reduce: Host System Provision}} \label{sec:reduce}
This section examines strategies to minimize host memory subsystem resource underutilization to reduce embodied carbon waste. We focus on optimizing memory and storage resources, which comprise 36\% of embodied emissions in Azure A100 offerings\footnote{Standard\_ND96asr\_A100\_v4, the number is generally higher for higher capacity instances}. These optimizations must be balanced against the benefits of CPU reuse for offline inference, as explored in Section~\ref{sec:reuse}.


\textbf{Prefix-caching-aware DRAM reduction strategy. }
DRAM supports AI inference through:
\begin{itemize}
\item Memory offloading and intermediate data storage
\item Data processing and memory allocation
\item KV Cache swap space and prefix caching
\end{itemize}
For online inference with tight SLOs, offloading is not viable for large models. The minimum DRAM capacity is determined by:
\begin{equation}
\begin{gathered}
C^{\text{DRAM}} = C^{\text{Weight (layer)}} + C^{\text{KV/activation offload for online}} + C^{\text{KV for offline}} \\
\min C^{\text{DRAM}} = M_{k v}(n) = 4 n d h_{k v} l
\end{gathered}
\end{equation}
where $d$ is model dimension, $l$ is layer count, $n$ is P90 aggregated context length with zero reuse distance, and $h_{kv}$ is KV head dimension.
To mitigate performance impact from reduced DRAM, we use profiling to understand prefix reuse distances and determine optimal CPU cache space. We route longer-context requests to DRAM-abundant servers and leverage solutions like CacheGen~\cite{CacheGen} to offload caches across multiple instances or to SSD.


\textbf{KV-offload-aware SSD reduction strategy. }
While cloud GPU offerings typically include large SSDs, this isn't fundamental for inference serving. SSDs consume approximately 2.8~W per TB idle power (>10\% of server idle power). The minimum required SSD size approximates GPU memory capacity:
\begin{equation}
\begin{gathered}
C^{\text{SSD}} = 1.2 C^{\text{GPU}} + C^{\text{Model buffer}} + C^{\text{KV Offload for offline}} \\
\min C^{\text{SSD}} = 1.2 C^{\text{GPU}}
\end{gathered}
\end{equation}
When SSD capacity is used to download and maintain models, we imagine remote storage with GPUDirect can reduce over provisioned storage.

