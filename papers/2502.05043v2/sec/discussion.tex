\section{Related Work}

\textbf{Accelerating CPU Inference:} LLM inference on CPUs, as demonstrated by frameworks like Llama.cpp and Gemma.cpp, has garnered significant attention for enabling cost-effective deployment~\cite{llama_cpp,gemma_cpp}. These systems employ optimizations such as quantization and memory-mapped weights to run models with limited resources. Recent studies also focus on improving CPU-aware inference efficiency through sparsity-aware techniques, quantization optimizations, offloading, and library enhancements (e.g., OpenBLAS~\cite{openblas}, oneDNN~\cite{onednn}, XNNPACK~\cite{xnnpack}), some of which are implemented in llama.cpp~\cite{llama_cpp}. \textit{We built our work on top of llama.cpp and come up with new solutions for codesigning resource provisioning and workload scheduling. }

\textbf{Heterogeneous LLM on Commodity Hardware:} Recently, people have gained more interest in deploying or training foundational models under decentralized or heterogeneous commodity setups~\cite{yuan2022decentralized,scao2022bloom}. To account for the heterogeneity in the interconnect bandwidth of commodity setup, Mobius~\cite{Feng2023} introduces a novel pipeline parallelism scheme enabling heterogeneous memory for large-scale model training, while bringing fewer communications than existing systems. 
\textit{While those works mostly focus on model partition and placement, we focus more on the request scheduling and resource provision. Our objective which includes carbon is also fundamentally different from performance-oriented systems. }


\textbf{LLM Serving Offloading:} Recently, memory or compute offloading has gained popularity as a research area due to the capacity limits and high costs of GPU HBM. Many works offload model weights, activations, KV cache, or computations to the CPU for \textit{offline inference} that trade latency for throughput. FlexGen~\cite{flexgen}, HeteGen~\cite{hetegen}, PowerInfer~\cite{song2023powerinfer}, and TwinPilots~\cite{twinpilots} are focused on settings with batch inference of workloads with homogeneous input / output length. 
InstInfer~\cite{instinfer} and DeepSpeed ZeRO-Inference~\cite{aminabadi2022deepspeed} further offloads to SSD, but they did not study the load balance between CPUs and GPUs.
FastDecode~\cite{fastdecode} and NEO~\cite{jiang2024neo} target online serving. NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling to balance GPU and CPU loads. Specifically for MoE models, Huang et al~\cite{huang_towards_2023} propose to swap experts to CPU memory to reduce the GPU memory consumption. PC-MoE~\cite{kong_serving_2023} proposes to offload experts under dynamic resource constraints with offline profiling and online scheduling. 
SiDA~\cite{Du2023SiDASD} dynamically loads activated experts and offloads inactivated experts according to hash tables.  
\textit{None of the proposals considers the resource provisioning aspect and carbon impact of offloading. Furthermore, naively implementing offloading without considering phases, request lengths, and capacity planning would only cause more embodied and operational carbon due to GPU overprovisioning and large PCIe-induced overhead. }


\textbf{Energy-aware Serving and Model Serving:} A plethora of work in the space of model serving has emerged, focusing on providing SLOs with scheduling, placement, batching and autoscaling solutions. For example, AlpaServe~\cite{li2023alpaserve}, Clockwork~\cite{gujarati2020serving}, Sheperd~\cite{zhang2023shepherd} and Clipper~\cite{crankshaw2017clipper} all are focusing on non-autoregreesive models. However, such general systems often overlook the distinct phases in LLM serving or autoregressive nature of LLMs. 
InFaaS~\cite{romero2021infaas} proposes a way to do model selection and autoscaling. More recently, $\mu$serve~\cite{qiu2024muserve} proposed power-aware LLM serving. 
Splitwise proposed pd disaggregation to optimize for cost, power, and throughput under SLOs ~\cite{patel2023splitwise}. These general systems are not carbon-aware, nor do they consider the role of CPU. \textit{We show in our experiments that carbon-opt is different from energy-opt, and propose co-design solutions for both resource provisioning and runtime scheduling, leaning on the insights of asymmetric carbon cost of CPU and GPUs. }
\section{Conclusion}





Our work introduces a quantitative framework that holistically optimizes AI infrastructure's carbon footprint. EcoServe demonstrates how hardware-software co-design across software provision, scheduling, and hardware provision can reduce total carbon emissions while maintaining performance. Beyond the specific optimizations presented, this work provides a foundation for carbon-aware system design that we hope will inspire future research in sustainable AI infrastructure.







