
\section{Implementation and Experimental Setup}




\textbf{Hardware Systems.}
The experimental evaluation uses a heterogeneous set of CPU and GPU hardware.
In terms of GPUs we use a combination of PCIe-based NVIDIA H100, A100, A6000, L4, and A40 GPUs, representing a diverse range of compute, memory, power, and energy-efficiency tradeoffs.
In terms of CPUs, we use a dual-socket server class Intel Sapphire Rapids CPU. The CPU frequency governor is set at performance mode with turbo boost disabled. %




\textbf{Models, Workloads and Load Generator.} 
We evaluate our system using a wide range of workloads to capture diverse scenarios. 
For models, we assess different architectures with varying sizes. For request generator, we use scaled Azure function traces (AZF) and production offline workload traces~\cite{azure_public_dataset} (Figure~\ref{fig:offline}). For workload size, we sample from public datasets listed below: 
\[\footnotesize
\begin{array}{cccc}
    \toprule
    \textbf{Model} & \textbf{TTFT} & \textbf{TPOT} & \textbf{Dataset} \\
    \midrule
    \text{Gemma-2-2B} & 0.25 \, \mathrm{s} & 0.1 \, \mathrm{s} & \text{ShareGPT~\cite{sharegpt}} \\
    \text{MetaLlama-3-8B} & 0.5 \, \mathrm{s} & 0.1 \, \mathrm{s} & \text{ShareGPT~\cite{sharegpt}} \\
    \text{Llama-13B} & 1.5 \, \mathrm{s} & 0.15 \, \mathrm{s} & \text{AFT~\cite{azure_public_dataset}} \\
    \text{Llama-70B} & 15 \, \mathrm{s} & 0.24 \, \mathrm{s} & \text{AFT~\cite{azure_public_dataset}} \\
    \text{Mixtral-8x7B} & 2.5 \, \mathrm{s} & 0.15 \, \mathrm{s} & \text{ShareGPT~\cite{sharegpt}} \\
    \text{(Online) Gemma-2-27B} & 10 \, \mathrm{s} & 0.2 \, \mathrm{s} & \text{AFT~\cite{azure_public_dataset}} \\
    \text{(Offline) Gemma-2-27B} & 24\mathrm{hr} &  \, \mathrm{N/A} & \text{LongBench~\cite{longbench} } \\
    \text{Bloom-176B} & 20 \, \mathrm{s} & 0.27 \, \mathrm{s} & \text{AFT~\cite{azure_public_dataset}} \\
    
    \bottomrule
    \end{array}
\]



\textbf{Software.} We use vLLM v0.6.2 and Llama.cpp (d5ed2b9) to deploy LLM instances across provisioned GPU and CPUs. We also use Splitwise simulator~\cite{patel2023splitwise} and integrate our carbon models to evaluate the comparison and impact of more fine-grained provision strategy (Section~\ref{sec:sensitivity}) of different scales. We implement the profiling-based performance, energy and carbon models using Python based on those frameworks under various hardware, workload traces and request generators. We implement the ILP formulation and resource allocation algorithms based on CVXpy~\cite{cvxpy}. We sample from Watttime~\cite{watttime} and GreenSKU~\cite{greensku} to get the real-time or average carbon intensity of various regions. The aggregated analysis in Figure~\ref{fig:result-summary} Left reflects  the average carbon intensity at mid level (261 gCO2/kWh). %



\section{Evaluating EcoServe}


We evaluate EcoServe across three dimensions: carbon efficiency, performance, and energy. Our evaluation focuses on the following key research questions:

\begin{itemize}[leftmargin=*]
    \item How much additional operational and embodied carbon can EcoServe save in comparison to performance, energy, and cost-optimized baselines?
    \item How does EcoServe perform to different carbon intensity and request rates?
    \item What's the overhead and scalability of the core resource allocation components in EcoServe?

    \item What are the carbon footprint implications of combining multiple categories of EcoServe's optimization strategies? What are the tradeoffs with performance? 
    \item What is the effectiveness of individual design strategy in 4Rs?
\end{itemize}
\subsection{End-to-end Evaluation}


\begin{figure*}
  \centering
    \centering
    \begin{subfigure}{0.6\textwidth}
\centering
    \includegraphics[scale=0.5,viewport=150 20 550 160]{plots/tradeoff_TTFT_TPOT_midCI.pdf}\vspace{-0.03in}
\vspace{-0.03in}
    \end{subfigure}
    \begin{subfigure}{0.38\textwidth}
      \centering
      \hspace{-0.9in} 
       \includegraphics[width=1.1\linewidth]{plots/cumulative_carbon_savings.pdf}\vspace{-0.08in} \label{fig:decompose-4R}
    \end{subfigure}
    \caption{Carbon vs. performance trade-off. Lower-right is better. (Left, Center) We compare EcoServe to performance, energy, and cost (Melange) optimized baselines for prompt latency (TTFT, center) and generation latency (TPOT, left). We show the individual improvements (i.e., reuse, reduce, rightsize, recycle) as well as the aggregated benefit.
    EcoServe achieves up to 47\% carbon reduction at comparable performance.
    (Right) Under different workload mixes, we show the cumulative benefits of successively applying EcoServe's optimizations to online and offline capacity. All experiments assume iso-throughput.}
    \vspace{-1em}
    \label{fig:result-summary}
\end{figure*}


We begin by summarizing the overall carbon and performance implications of EcoServe.
Figure~\ref{fig:result-summary} (left) demonstrates the average relative performance degradation and carbon improvements normalized to performance-optimized (\textit{perf-opt}) system configuration and resource allocation baselines. 

The baselines are configured as follows: (1)
a performance optimized (\textit{perf-opt}) configuration implements a single hardware that minimizes TTFT and TPOT, 
(2) a cost-optimized baseline (\textit{Melange}) which provisions hardware to minimize cloud cost, 
(3) an energy-optimal configuration (\textit{energy-opt}) that finds software provision (GPU resource allocation) decisions to minimize energy (with no capacity planning changes on CPUs), (4) \textit{SplitWise} -  provisioning strategy for H100 and A100, using the simulator to pick the best carbon configuration on Pareto frontier under SLO constraints.  
We drive the workload with a Poisson request generator with various arrival rates and scaled Azure Function Traces (2023) to emulate the bursty behavior of online samples (5\% - maximum system load). 





Figure~\ref{fig:result-summary} (left,middle) shows that EcoServe variants can achieve significant carbon savings compared with performance optimal configurations for both online and offline workloads. We further decompose the carbon savings from various strategies: 



\begin{itemize}[leftmargin=*]
    \item Reduce can yield around 12.4-28.6\% carbon savings. For leaner GPU offerings like T4, the savings are less than higher-end GPUs since the host is designed to scale with the GPU memory capacity.  
    \item Right sizing (RS) enables around 15.2-30.3\% better carbon, leveraging the SLO slacks for various workloads on TPOT and TTFT. The benefit is particularly significant in lower carbon intensity regions with higher load variability and higher online workload component, as EcoServe can dynamically adjust its resource allocation to match demand to different workload slices. 

    \item Optimized CPU reuse strategies can save up to 25.4\% of carbon for clusters with different percentages of offline workload. 
    \item Recycle strategy is orthogonal to all the other Rs. It can be independently deployed, yielding around 16.8\% savings compared with a homogeneous update baseline (Section~\ref{sec:recycle-eval}).
    \item When it comes to combining three strategies together, RS + Reuse are software solutions that produce more synergy, whereas Reduce and Recycle are orthogonal hardware design optimizations. On average EcoServe yields 47\% carbon savings. 
\end{itemize}
The results motivate co-designing the 4Rs for different workloads. 
\subsubsection{Performance Analysis}
\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{plots/ablation_CI.pdf}\vspace{-0.23in}
    \caption{Carbon savings and reuse configurations sampled by EcoServe under various workload lengths, SLOs and carbon intensities for a mid-sized model (Llama-70B). }
    \label{fig:ablation}
\end{figure}

From Figure~\ref{fig:result-summary}, we demonstrate that the performance of EcoServe variants is close to Perf-opt within 3 percent, except for the reuse case. The reason for a worse TPOT (around 5\%) is because of the offline workloads, where the reuse strategy prioritize the decoding of tokens with longer context length to be offloaded to CPU. The KV cache transfer This performance drop can be recovered with better hardware interconnects and other system techniques like batching transfer for smaller models to improve MBU (memory bandwidth utilization).

EcoServe (RS) provides better improvements on TPOT metric than TTFT metric in the presence of SLO slack.
This is because of the longer prompt and higher load, when we right-size the allocation periodically, the unavailability will cause the prompt scheduler to queue some requests' prompts temporarily, which results in the queuing delay. Furthermore, by design, \textit{Reuse} mostly addresses the \textit{decoding servers'} capacity saving, hence the performance on TTFT on the \textit{prompting servers} is untapped. EcoServe (Reduce) incurs a little overhead on TPOT (<5\%), but the \textit{energy-opt} allocator is worse in performance through aggressively choosing energy-opt hardware (like L4), ignoring the embodied carbon per performance. Furthermore, \textit{Splitwise} suffers in carbon and performance because of the limited hardware options, and the gap between provision and scheduling decisions (Section~\ref{sec:rightsize}). Further analysis on a specific workload is shown in Figure~\ref{fig:split-compare-2}.


\subsubsection{Breakdown Analysis.}

The benefits of carbon footprint reduction in EcoServe can be uncovered layer by layer through various strategies (Figure~\ref{fig:result-summary} Right).

For online workloads, \textit{Reduce (online resource)} allows us to save approximately 17-40\% of the carbon footprint by reducing DRAM and SSD capacity. When moving to \textit{Reuse}, online workloads seldom use CPU resources, maintaining consistent savings. The \textit{RS (online)} strategy further increases savings by employing workload-aware load balancers and resource allocators, limiting the overprovisioning of high-end hardware.


For offline workloads, savings from the \textit{Reduce} strategy are less significant. The \textit{Reduce (offline resource)} method is conservative, requiring DRAM to match the model size for offline inference on CPU. However, substantial savings come from reusing offline CPU capacity for decoding tasks. Rightsizing also yields significant benefits due to relaxed SLO and the Integer Linear Programming (ILP) formulation to optimize GPU sets through disaggregation. Longer offline workloads yield more benefits. 

For mixed workload production traces, workload (A), which has less offline demand, exhibits less benefit from aggressive \textit{Reduce (online)}; however, the overall carbon savings stay relatively similar for the two mixed workloads based on our analysis, demonstrating the generality for our approach across different types of workloads.

\subsection{Sensitivity and Scalability Study} \label{sec:sensitivity}

\subsubsection{Varying Carbon Intensity.}
To show the robustness of EcoServe's carbon savings under various geographical regions, models,, and workload patterns with different carbon intensity levels, we conducted a controlled experiment based on Splitwise code traces (for 2 minutes) on the simulation infrastructure for a medium/large model (Llama-70b, Bloom-176b) with different request arrival rates. 

We selected three power
grids: North Central Sweden (Low), California (Mid), and
Midcontinent (High). These regions represent low, medium,
and high carbon intensity levels, with carbon intensities of
17, 261, and 501 gCO2/kWh, respectively. The total carbon is shown in Figure~\ref{fig:split-compare-2}. Notably, EcoServe selects the software provision strategy that is different from SplitWise because the runtime could allocate shorter prompts to the A100. This leads to carbon savings on that trace for 26.5\% on average. 

Under a 40 H100 power-equivalent setup with
long input sequences, Splitwise achieved Pareto-optimal energy-throughput
by allocating 35 Prompt and 8 Token machines. However, its JSQ scheduling
missed workload-aware co-design opportunities. EcoServe matched throughput
with 30 H100s and 17 A100s by adaptively placing longer requests to
A100s, reducing both embodied and operational carbon by up to 38\% and 36\% for large and medium models under lower load. 

From Figure~\ref{fig:ablation}, we observe that for longer requests, lower carbon intensity level, reuse is most often used. The carbon savings for offline workload is most significant. However, when carbon intensity is higher, we save more with \textit{right-sizing} and \textit{reduce} strategy for online workloads.    





\subsubsection{Scalability of the control plane}
\begin{table}[b]\footnotesize
    \centering
    \begin{tabular}{ccccc}\hline
        cluster size & online (low) & offline (low)  & online (high) & offline (high) \\ \hline
        10  & 0.281 & 0.343 & 0.428 & 0.523 \\
        20  & 0.377 & 0.521  & 0.574 & 0.789 \\
        40  & 0.398 & 0.672  & 0.572 & 1.030\\
        80  & 0.508 & 0.883  & 0.747 & 1.272 \\
        160 & 0.585 & 0.919  & 0.901 & 1.315\\\hline
    \end{tabular}
    \caption{Overhead of control plane over different cluster sizes, workloads and system loads (Unit: second).}
    \label{tab:scalability} \hspace{-1in}
\end{table}
To show the scalability of EcoServe's control plane (ILP solver) with various cluster sizes and request arrival rates (low/high), we measured the time taken for the ILP solver and scheduler to make the right-sizing decision. 
Table~\ref{tab:scalability} shows the control plane overhead across different cluster sizes and load conditions. The overhead grows sub-linearly with cluster size, increasing by only 2.1-2.7$\times$ when scaling from 10 to 160 nodes. This favorable scaling comes from the ILP formulation's ability to exploit our carbon and latency performance model - similar workloads on similar hardwares are clustered together. Under high load, offline planning exhibits the highest overhead (1.315s at 160 nodes) due to exploring more complex hardware combinations. However, even in this worst case, the sub-2-second latency remains practical for production deployments where planning decisions typically operate on minute- or hour-level intervals. Online decisions show better scaling (0.585s at 160 nodes) by pruning out irrelevant spaces, focusing on immediate resource availability rather than long-term optimization.








    
    
    


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{plots/CI-model-load-sensitivity.pdf}\vspace{-0.2in}
    \caption{Comparison of Bloom-176B (Left) and Llama-70B (Right) with Splitwise on various carbon intensities (Low, Mid, High) and load on iso-power deployment. EcoServe consistently outperforms Splitwise, and the gap is larger under a lower request rate and higher CI. }
    \label{fig:split-compare-2}
\end{figure}



\subsection{Reuse Strategy Evaluation}
We evaluate the two design aspects of the reuse strategy: first, optimized CPU threading and tiling implementation for GEMM and long-context decoding. Second, workload- and load-aware CPU reuse strategy.
Figure ~\ref{fig:speedup-reuse} shows the speedup with various workload sizes with EcoServe's optimized \textit{reuse} strategy, compared with state-of-the-art CPU inference engine (llama.cpp) on Gemma 2/27B. The evaluation varies the core counts (56 vs 112) to show the generality of the approach. By choosing the right tiling and parallelism strategy guided by performance modeling, we improve decoding latency, which directly translates to higher throughput and carbon savings of 1.34$\times$ on average. 



\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{plots/gemma_CPU_optimization_comparison.pdf}
    \vspace{-1.5em}
    \caption{Beginning with llama.cpp as a baseline, we optimize CPU decoding. EcoServe's CPU optimization improves performance by up to 4.03$\times$ and on average by 1.34$\times$ (across batch sizes and parallelism dimensions) on an Intel SPR.}
    \label{fig:speedup-reuse}
    \vspace{-1em}
\end{figure}








\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{plots/decode_performance_energy_gemma_llama.pdf}
    \vspace{-1.8em}
    \caption{Decoding throughput (Left), operational (Middle), and embodied carbon (Right), for various models and workload sizes, comparison with a naive llama.cpp vs. optimized CPU \textit{reuse} strategy, normalized to A100 GPU performance under maximum throughput. Carbon reduction is 12, 26\% for Gemma-27B, and 40\%, 66\% for Llama-8B, respectively, under short and longer workloads. }
    \label{fig:reuse-result}
    \vspace{-1.5em}
\end{figure}





Figure~\ref{fig:reuse-result} analyzes the detailed embodied and operational carbon breakdowns compared with serving on GPUs for EcoServe's \textit{reuse}, showing the benefit of workload- and load-aware optimization. Given the CPU's lack of energy proportionality, the added operational power is relatively minor but still results in operational carbon cost with longer execution time. The free lunch from a 56-core SPR machine CPU attached to A100 allows us to achieve 0.53-2.29$\times$ the throughput of the GPU (Blue bars). Although CPU generally has worse energy efficiency, especially for larger models when both CPU and GPU are compute bound (Orange bars), EcoServe's CPU decode can achieve up to 66\% relative embodied carbon savings compared with a naive llama.cpp (CPU-Naive) strategy as 48\%. On average, EcoServe can achieve 3.51$\times$  embodied carbon savings compared with a non-optimized baseline across the above workloads. More importantly, without EcoServe's \textit{reuse} strategy (CPU-Opt), the embodied carbon for llama.cpp (CPU-naive) is worse than non-reuse (GPU) for Gemma-27B model and Llama-8B model short context cases. 
This is due to the fact that under a setting with a longer context and smaller model dimensions, the gap between CPU and GPU throughput is larger due to the batching effect, and the carbon saving is more significant in an iso-throughput setting.

 The total carbon benefit arises from the net effect of embodied and operational carbon through \textit{Reuse}. EcoServe enhances cost and embodied carbon by utilizing unused CPU cores, reducing the need for over-provisioned GPUs. Additionally, it offers advantages in long-context decoding through customized CPU kernel optimization and allows flexible carbon reduction via runtime scheduling based on workload sizes.  However, it may not work well for shorter-context larger-model decoding in terms of operational carbon. The strategy can be tailored according to the region's carbon intensity. In areas with high carbon intensity (CI), operational carbon is a primary concern, so we should minimize CPU offloading. Conversely, in regions with low CI, embodied carbon plays a larger role, allowing us to increase capacity through CPU offloading (Figure~\ref{fig:ablation). }







\begin{figure}[h]
    \centering
    \includegraphics[width=0.89\linewidth]{plots/RS_carbon_energy.pdf}
    \vspace{-1.5em}
    \caption{Relative carbon and energy improvements of rightsizing GPUs on Gemma-27B on homogeneous GPUs (i.e., H100, A100, L4), and cost-optimized provisioning (Melange). EcoServe's \textit{Right-sizing} strategy exploits LLM phases to reduce overall energy and carbon under TPOT = 100 ms (online) and TTFT = 24hr offline constraints.}
    \label{fig:result-RS}
    \vspace{-1em}
\end{figure}
\subsection{Right-sizing Strategy Evaluation}
For right-sizing, we evaluate our system on Gemma-27B with various load and workload against the following two types of baselines: 1) Melange: Cost-optimal GPU resource allocation. 2) Single hardware: pick the single hardware and its replica numbers to satisfy workload demands that disregard SLO slacks and the energy/carbon implication for hardware choices. 

Figure~\ref{fig:result-RS} shows the final outcomes for \textit{right-sizing} strategy given different workload types and loads of the system. Each request generator runs for 5 minutes length traces, and we discard the first and last 5\% of the statistics for warming up effects. The length of the workloads is randomly sampled from the open-sourced traces with both long and short prompts.  %

For the online setting (left), under a lower request rate, EcoServe consistently outperforms Melange by over 2.56$\times$  for carbon and 2.25$\times$  for energy. 
The improvement comes from separating resource allocation decisions for LLM phases; finer-grained allocation enables better utilization and energy. As an example, for request rate of 1, EcoServe will choose the most carbon-efficient hardware based on the workload size; e.g. for medium-sized workload it chooses L4 and A100 for decoding and prompting, respectively; and A100 and H100 for long prompt. 
However, Melange sticks to the best perf-per-cost hardware for both phases for each workload slice, and the quantization effect for available hardware capacity introduces more over-provisioned resources for decode phase, which negatively impacts embodied and operational carbon.

EcoServe also performs better than other single hardware choices both on carbon and energy. The closest baseline is L4 due to its higher energy and carbon efficiency. This is because of the flexibility in allocating a combination of resource based on the optimal energy or carbon profile for different workload slices under various SLO. 

Furthermore, for offline setting (offline), EcoServe consistently wins in both energy and carbon up to 1.6$\times$ with the next best baseline, as it considers both dimensions in the optimization objective.
Under the carbon intensity of US-Central / South, operational carbon is around 0.64$\times$ of the embodied, making EcoServe a desirable solution for balancing both energy and embodied carbon. 

\subsection{Recycle Strategy Evaluation} \label{sec:recycle-eval}

Figure~\ref{fig:speedup-recycle} shows absolute carbon cost (top) and relative savings (bottom) for fixed (baseline) vs. asymmetric (EcoServe) upgrades. The baseline assumes 800 kgCO$_2$e for the host, 120 kgCO$_2$e for the GPU, and 600 kgCO$_2$e in yearly operational emissions. GPU energy efficiency is estimated to double every 3.5 years on average~\cite{sun2019summarizing}. The baseline upgrades both host and GPU every 4 years, while EcoServe upgrades hosts every 9 years and GPUs every 3 years to better match efficiency gains.



The bottom plot demonstrates the relative carbon savings enabled by the asymmetric strategy. By upgrading host systems less frequently, significant embodied carbon savings are achieved due to their higher upfront embodied carbon costs. Although this can cause some negative operational carbon savings, host systems have less impact on overall operational emissions to AI inference compared to GPUs.
At the same time, more frequent GPU upgrades, which incur relatively lower embodied carbon costs, allow for earlier adoption of energy-efficient hardware, leading to operational carbon savings.
This trade-off enables EcoServe to balance embodied and operational carbon reductions.
Over the 10-year period, EcoServe asymmetric strategy is able to achieve approximately 16\% cumulative carbon saving compared to the baseline fixed strategy. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{plots/recycling_strides_9yr.pdf}
    \vspace{-1.2em}
    \caption{Trends in annual embodied and operational carbon based on EcoServe's asymmetric recycling of CPUs (9 years) and GPUs (3 years). Baseline is a fixed schedule for both CPU and GPU (4 years). }
    \label{fig:speedup-recycle}
    \vspace{-2em}
\end{figure}










