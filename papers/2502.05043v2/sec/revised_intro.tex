




The systems community has responded with numerous innovations to optimize LLM inference. Systems like vLLM~\cite{vllm2024perfupdate}, SGLang~\cite{zheng2023efficiently}, and DynamoLLM~\cite{stojkovic2024dynamollm} have significantly improved GPU utilization, throughput, and energy efficiency via sophisticated resource management. However, these approaches optimize primarily for operational metrics, overlooking a crucial aspect of AI's environmental impact: embodied carbon emissions from hardware manufacturing.

Recent sustainability reports from hyperscalers reveal that embodied carbon now accounts for over 50\% of their total emissions when using renewable energy~\cite{gupta2021chasing,microsoftSustainabilityReport,GoogleSustainabilityReport}. This presents a fundamental challenge: optimizing operational efficiency alone may not minimize total carbon impact. Our analysis reveals a critical insight: while GPUs dominate operational power consumption, CPUs and host systems contribute disproportionately to embodied carbon (Figure~\ref{fig:motivation}). Note that TDP can serve as
a proxy for operational carbon assuming a similar high utilization workload profile and same carbon intensity (assuming 80\% for both CPU and GPU).
The high embodied overheads from host systems owe to a combination of memory capacity, storage, and mother-board (e.g., PCB, peripheral interconnects). This misalignment creates an opportunity to rethink how we design AI infrastructure.


 Furthermore, AI inference workloads exhibit workload and hardware \textit{heterogeneity}—differences in model size, prompt length, decoding patterns, workload phases and SLO requirements—that create new optimization opportunities. %
 However, existing approaches fail to leverage all the heterogeneity dimensions, and co-optimize \textit{capacity planning, resource allocation, and runtime scheduling}, leading to misalignment between provisioned resources and runtime carbon reduction. For example, a strategy that only optimizes operational efficiency may increase embodied carbon by requiring frequent hardware upgrades or over-provisioning GPU capacity.

To holistically optimize embodied and operational emissions, we introduce \textbf{EcoServe}, a carbon-aware AI inference framework that co-designs \textit{capacity planning, resource allocation, and scheduling}. Unlike prior work that treats these phases independently, EcoServe’s \textit{cross-layer design} ensures that capacity planning decisions are effectively exploited at runtime. Specifically, while capacity planning determines long-term provisioning to reduce embodied carbon, runtime scheduling dynamically adapts these provisions to workload variation, ensuring optimal hardware utilization without increasing embodied or operational overhead.

At the core of EcoServe is a sustainability-driven optimization framework based on four key principles: \textbf{Reuse}, \textbf{Rightsize}, \textbf{Reduce}, and \textbf{Recycle} (4R). These principles collectively minimize carbon impact across different dimensions of system operation:
\begin{itemize}
    \item \textbf{Reuse}: Exploits underutilized host CPUs for offline inference to amortize embodied carbon across workload phases.
    \item \textbf{Rightsize}: Dynamically provisions GPUs and CPUs based on model size, execution phase, and workload demand to avoid over-provisioning.
    \item \textbf{Reduce}: Eliminates unnecessary host memory and storage resources to reduce embodied carbon overhead.
    \item \textbf{Recycle}: Extends the lifetime of host processing systems while selectively upgrading accelerators to balance embodied and operational trade-offs.
\end{itemize}

Figure~\ref{fig:motivation}(right) shows the impact of each of these design strategies on two scenarios: offline or batch processing, and online serving.
The individual strategies achieve carbon savings of 29-41\% compared to performance-optimized baselines.
While reduce and right size benefit carbon savings in online serving, offline serving benefits from a combination of reuse, reduce, and rightsize.
Altogether the different design strategies save up to 2$\times$ total carbon for both offline and online serving.
More generally, we evaluate EcoServe using both open-source datasets (ShareGPT~\cite{sharegpt}, Azure Functions~\cite{patel2023splitwise}) and production traces from a major cloud provider, as well as across heterogeneous model and hardware setups.
Our results demonstrate EcoServe's co-design solutions yield 1.4-2.2$\times$ reduction in total carbon emissions while maintaining performance targets across diverse LLM deployment scenarios. 
