\subsubsection{\textbf{Reusing CPUs for Offline Inference}}~\label{sec:reuse}
Section~\ref{sec:characterization} shows that host processing systems incur high embodied carbon overheads in AI inference infrastructure.
To maximize the utility of these resources, EcoServe opportunistically reuses host CPUs for offline inference, reducing the reliance on high-power GPUs.

\textbf{Identifying opportunities for using CPUs.} 
Due to the underutilization of host resources, we must carefully determine which operations or inference phases can be offloaded from GPUs to CPUs to maximize carbon efficiency. Note that each offload incurs additional energy costs for data movement, necessitating a selective approach to CPU reuse. %

To guide this decision, Figure~\ref{fig:roofline} presents a {roofline model analysis~\cite{williams2009roofline,yuan2024llm,nerscRooflinePerformance}} comparing the computational intensity and memory bandwidth efficiency of an Intel Sapphire Rapids 112-core CPU and an NVIDIA A100 40GB GPU.
Given the relatively lower memory bandwidth gap between the CPU and GPU, we find that {low-arithmetic-intensity operations}, such as {attention scoring and KV cache decoding}, are well-suited for CPU execution.
Additionally, for large-batch operations, GPU throughput becomes capacity-bound, making CPU reuse {a practical alternative for offline inference.
Figure~\ref{fig:roofline} highlights the maximum batch sizes feasible for GPUs (16) and CPUs (512) at a context length of 2048 in FP16.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{plots/roofline_model_b_compare_2048_star.pdf}
    \vspace{-1em}
    \caption{ Roofline models of an Intel Sapphire Rapids CPU with 112 cores compared to an NVIDIA A100. 
    We overlay different operators in the decode (solid and dash-dot line) and prompt computation phase (dashed line) for Llama-3-8B with the maximum batch size that CPU/GPU can handle at a context length of 2048.
    At large batch sizes, the GPU is capacity bound ({\textcolor{red}{\(\star\)}}) while the CPU ({\textcolor{purple}{\(\star\)}}) can efficiently process offline decoding workloads.}
    \label{fig:roofline}
    \vspace{-2em}
\end{figure}

\textbf{Optimizing CPU performance via parallelization.}
To maximize the efficiency of CPU-based inference, EcoServe optimally configures thread-level parallelism and tensor tiling to balance:
\begin{itemize}
    \item Arithmetic intensity (FLOPs/Byte),
    \item Compute vs. memory bandwidth constraints,
    \item Model size, batch size, and sequence length.
\end{itemize}
The optimal configuration depends on the CPU microarchitecture (e.g., AMX and AVX efficiency), number format (FP6, BF6, INT8, etc), and workload characteristics.
As illustrated in Figure~\ref{fig:gemm-ai}, {parallelization along the KV sequence length dimension, in addition to the batch dimension, maximizes memory bandwidth utilization across all CPU cores.} 
This is particularly beneficial for long-context workloads, where attention mechanisms contribute significantly to inference latency~\cite{flashdecoding2023,flashinfer,dao2022flashattention,dao2023flashattention,hong2023flashdecoding++}.

Following our {roofline analysis}, while prompt computation remains GPU-favorable, EcoServe implements {layered pipelining} to move KV cache transfers from GPUs to CPUs for decoding.
By optimizing these parallelism dimensions, we achieve a {3.67$\times$ speedup} compared to a baseline Llama.cpp implementation~\cite{llama_cpp}, with an average {1.4$\times$ improvement} across model sizes and sequence lengths (Figure~\ref{fig:speedup-reuse}).
As CPU inference remains a nascent field, future advancements leveraging sparsity and weight-sharing techniques are expected to further enhance CPU-based offline inference~\cite{song2023powerinfer,zhang2024h2o,Liu2023DejaVu}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.7\linewidth]{plots/AI_vs_BN_4096.pdf} 
    \vspace{-1em}
    \caption{CPU offline inference requires carefully balancing parallelism degree (PD) and arithmetic intensity of Linear operators (AI, FLOPS/Byte). The run-time scheduler co-designs tile and workload slice dimensions with architecture decisions to optimize offline performance.}
    \label{fig:gemm-ai}
    \vspace{-1.2em}
\end{figure}

\textbf{Adapting to fluctuating offline inference demand.}
Given their inherently relaxed SLOs, offline workloads offer an opportunity to leverage {CPU capacity during periods of low CPU utilization}.
EcoServe's reuse strategy is motivated by two key insights:
\begin{enumerate}
    \item {Offline inference demand is significant}: Figure~\ref{fig:offline} illustrates the ratio of online vs. offline demand in a real-world cloud deployment over a week (left) and a single day (right). On average, {offline workloads account for 21\% and 45\% of total infrastructure capacity} in Services A and B, respectively.
    \item {Offline workloads exhibit time-varying peaks}: At peak periods, offline demand reaches {27\% and 55\%}, respectively. By {scheduling offline inference to underutilized CPU nodes}, EcoServe {reduces peak GPU resource requirements}, thereby lowering total embodied carbon.
\end{enumerate}
Since offline workloads lack stringent latency constraints, EcoServe {batches large offline requests} on CPUs, thereby reducing the need for high-GPU provisioning.
For datacenters with high-carbon-intensity power grids, EcoServe can {dynamically reallocate workloads} back to GPUs when energy efficiency becomes a priority.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{plots/offline_combined_analysis.pdf}
    \vspace{-0.2in}
    \caption{Breakdown of online and offline demand for two LLM services (A and B) running in a production cloud data center over a week (left) and during a day (right). 
    Offline demand accounts for a significant portion of total serving capacity, creating opportunities for CPU reuse in offline inference.}
    \label{fig:offline}
    \vspace{-1.2em}
\end{figure}

\textbf{Load-aware reuse.}
To quantify the impact of CPU reuse, Figure~\ref{fig:offline-result} presents the required infrastructure capacity for serving Llama-3-8B inference workloads under varying demand conditions.  
We analyze two scenarios:
\begin{itemize}
    \item {Peak-aware reuse (red):} CPUs handle offline inference \textit{only} during peak demand periods.
    \item {Continuous reuse (blue):} CPUs process offline inference \textit{at all times}.
\end{itemize}
Assuming resource reallocation occurs every {4 hours, EcoServeâ€™s load-aware CPU reuse reduces offline GPU provisioning needs by {1.32$\times$ at peak demand}, contributing to substantial {embodied carbon savings}.
This estimate is {conservative}, assuming fixed batch sizes for CPU and GPU inference. By further increasing CPU batch sizes, offline capacity reductions of up to 45\% are achievable.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{plots/reuse_capacity_offline.pdf}
    \vspace{-0.2in}
    \caption{Impact of CPU reuse strategies on infrastructure capacity. Red and blue curves represent peak-only and continuous reuse, respectively. EcoServe dynamically reallocates workloads to CPUs, reducing offline GPU capacity demands by up to 1.32$\times$.}
    \label{fig:offline-result}
    \vspace{-2em}
\end{figure}
