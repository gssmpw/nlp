\section{Background}

\input{table/model.tex}

\textbf{Carbon Emissions in Computing Infrastructure}. Computing infrastructure contributes to carbon emissions in two primary ways: operational carbon and embodied carbon. Operational carbon refers to emissions from energy consumption during system operation, including direct power consumption by computing components and indirect emissions from cooling systems. 

Embodied carbon, often overlooked, encompasses emissions from manufacturing, transportation, and eventual disposal of hardware components. Prior works indicate that for typical data center hardware, embodied carbon can represent over 50\% of total lifetime emissions, with memory and storage being particularly carbon-intensive in their manufacturing process~\cite{greensku, dirty_ssd, gupta2021chasing}.


Recent work like ACT, SCARIF, FOCAL, LLMCarbon~\cite{ACT,SCARIF,faiz2023llmcarbon,eeckhout2024focal} has highlighted the importance of considering both types of emissions. However, existing modeling approaches often do not look at fine-grained GPU carbon vs Host carbon footprints. We extend these works by providing a more fine-grained carbon modeling framework, especially for GPU systems, and show how these models can be used to optimize carbon footprint of LLMs    .






\textbf{LLM Serving Systems.}
State-of-the-art LLM serving systems like vLLM, TensorRT-LLM, AlpaServe, SGLang, Sarathi-serve, DistServe, and DeepSpeed~\cite{Deepspeed,kwon2023efficient,agrawal2024taming,zhong2024distserve,patel2023splitwise,zheng2023efficiently,TensorRT,alpaserve} have made significant advances in optimizing resource utilization LLM inference.


LLM inference typically has two distinct phases and two types of service level objectives (SLO): compute-bound prompt computation phase and memory-bound decoding phase. 
\textit{Online inference} prioritizes low-latency responses for interactive tasks, while \textit{offline inference} focuses on high-throughput batch processing, usually having 24hr SLO, presenting different optimization requirements. Additionally, the large model sizes and substantial key-value (KV) cache storage make LLM workloads memory capacity-bounded, further complicating resource management.

To address these challenges, these systems leverage GPU hybrid parallelization strategies, combining data, tensor, and pipeline parallelism to optimize computation across GPUs~\cite{mei2024helix,li2022amp}. Advanced memory management techniques, such as activation checkpointing, memory offloading, and paged attention, help alleviate memory pressure and improve resource utilization. Scheduler optimizations, including dynamic batching and chunked prefill processing, and disaggregating prompting and decoding phases, ensure high throughput while meeting latency constraints, enabling these systems to efficiently handle diverse LLM inference workloads~\cite{patel2023splitwise,zhong2024distserve}.




While these systems primarily focus on performance metrics such as throughput and latency, recent works like DynamoLLM~\cite{stojkovic2024dynamollm} and Melange~\cite{griggs2024mlange} have brought attention to energy and cost efficiency of LLM inference. However, optimizing for energy consumption or cost does not  directly optimize carbon impact. Carbon optimization goes beyond energy efficiency and cost considerations, requiring a holistic understanding of factors such as the carbon intensity of the energy grid, the lifetime utilization of the host (including CPU, memory, and storage) and the accelerator system, resource provisioning decisions, and the long-term environmental impacts of the underlying infrastructure.
