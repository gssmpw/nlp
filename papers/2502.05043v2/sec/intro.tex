\section{Introduction}

The rapid growth of Large Language Models (LLMs) has led to unprecedented demands on computing infrastructure.
From 2019 to 2022, we have seen an over 100$\times$ increase in model size from GPT-2 (1.5 billion parameters) to GPT-3 (175 billion parameters) to PaLM (540 billion parameters)~\cite{chowdhery2023palm}.
The increasing compute demands have also led to dramatic increases in the power and energy demands for deploying LLM's at-scale~\cite{tschand2024mlperf, luccioni2024power}.
The Electric Power Research Institute (EPRI) projects that AI-driven data centers will account for up to 40\% of the power in local grids (i.e. Virgina, Oregon)~\cite{epri2024}.

The rising compute demand of deploying LLMs impacts not only the efficiency of the data center but also the overall environmental impact of GenerativeAI and data centers.

\begin{figure}[t]
    \hspace{-0.2in}
         \centering
 \includegraphics[width=0.9\linewidth]{plots/fig1_combined.pdf}
 \vspace{-1em}
     \caption{(Left) Breakdown of thermal design power provision (TDP) and embodied carbon between host systems (CPU) and GPU. (Right) Through the 4R strategy, EcoServe optimizes the carbon savings for various AI workloads.}
     \label{fig:motivation}
     \vspace{-1em} 
 \end{figure}

\input{sec/revised_intro}


















This paper makes the following contributions:
\begin{itemize} [leftmargin=*]
\item We introduce EcoServe, a carbon-aware framework for LLM infrastructure design that optimizes both operational and embodied emissions. Fundamentally EcoServe comprises four design strategies---\textbf{Reuse, Rightsize, Reduce, and Recycle}---motivated by insights from fine-grained embodied carbon models for AI systems and detailed profiling and analysis of inference performance and efficiency across diverse hardware and serving environments.
\item Using EcoServe we show the individual design strategies save around 29\%, 25\%, 34\%, and 41\% carbon for reuse, rightsize, reduce, and recycle respectively, on a collection of open-source LLMs on heterogeneous hardware platforms, while maintaining SLO compared to a performance-optimized designs.
\item Putting the EcoServe optimizations together using an ILP formulation to co-optimize performance, efficiency, and carbon, we demonstrate 1.4-2.2 $\times$ carbon benefits with minimal performance degradation for both online and offline workloads across a variety of LLM models and workloads.
\end{itemize}

By framing AI inference sustainability as a \textit{co-design problem} across system stacks, EcoServe represents a paradigm shift towards carbon-aware systems design. Our results show that \textit{carbon-efficient AI infrastructure need not come at the cost of performance}â€”by leveraging workload and hardware heterogeneity, sustainable AI inference can be both high-quality and performant.
