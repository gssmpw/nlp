%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-10pt}
\section{Problem Formulation and Framework}
\vspace{-3pt}
\label{sec:problem}
Consider an assistive agent asked to complete a routine, i.e., a sequence of high-level tasks $\mathcal{R} = \{\tau_1, \tau_2,...\tau_n\}$. %, which The problem statement involves creating an intelligent household assisting agent that can anticipate future tasks and perform them efficiently. By analyzing patterns in past task sequences given as context, the agent can learn to predict the most probable next tasks. Once the agent has anticipated the most probable next tasks, it can generate a plan to execute current and anticipated tasks in minimal time. 
In a household environment, each $\tau_i$ is one of a set $\mathcal{T}$ of known tasks, e.g., \textit{make the bed} or \textit{make breakfast}, which requires the robot to compute and execute a plan, i.e., a sequence of finer-granularity actions $\{a_1, \ldots, a_{m_i}\}$. For example, to \textit{water the plants}, the agent has to \textit{bring the water hose to the garden}, \textit{connect the hose to the tap}, and \textit{turn the tap on}. %To perform a task say, $\tau_i$ the list of sub-tasks are $[\tau_{i1}, \tau_{i2},...\tau_{im_{i}}]$ $\forall$ $1\leq i\leq n$. Here, $m_i$ is the total number of sub-tasks to be performed for the task $\tau_{i}$.
Since the agent can be assigned different sequences of high-level tasks, it typically considers one task (in a given sequence) at a time, computing a sequence of finer-granularity actions to complete the task at a minimum cost (or time, effort). In doing so, the agent may fail to leverage an opportunity to reduce the cost of completing a subsequent task, e.g., when the agent is fetching milk from the fridge for coffee, it can also get the ingredients for making breakfast. Our framework in Figure~\ref{fig:pipeline} leverages the generic knowledge of LLMs to anticipate one or more high-level tasks given a partial sequence and a limited number of prompts. The anticipated tasks are considered as goals in a classical planner that computes a sequence of actions to jointly achieve these goals. We describe our framework's components below.

%To perform each sub-task in list $\tau_i$, there are some \textit{state conditions} $s\in S$ that must hold {\tt true}. These state conditions can be represented as a list $[s_{ij}^{[1]}, s_{ij}^{[2]},....s_{ij}^{[k_{ij}]}]$ $\forall$ $1\leq j\leq m_{i}$. Here, $k_{ij}$ is the total number of states that must hold true to perform sub-task $\tau_{ij}$. For example, In \textit{watering the plants} task, the states that must hold true are ({\tt agent-at} \textit{garden}), ({\tt In-hand} \textit{watering-hose}), etc. These states imply that the \textit{agent} has picked up the \textit{watering hose} and has moved to the \textit{Garden}. To perform the state transition of the states $s\in S$ to {\tt true} we employ a known \textit{transition model} $F: S\times A\rightarrow S$. Here, $A$ is the action space of our environment.\\\\

% \begin{figure}[]
% \centering
% \captionsetup{font=scriptsize}
% \setlength{\belowcaptionskip}{-15pt}
% \includegraphics[height=7cm,width=0.5\textwidth]{sections/figures/Room_Map (2).png}
% \caption{A Room Map with a description of objects in a particular room and the time taken to move between rooms or locations.}
% \label{fig: room_map}
% \end{figure}

% Consider the series of activities $\mathcal{A} = \{A_i\}$ for integers $1 \leq i \leq 6$. For each activity $A_i$ there exists $n_i$ number of tasks denoted as $\tau_i = \{ \tau_{ij} \}$ for integers $1 \leq j \leq n_i$, where $n_i$ represents the total number of tasks under each activity $A_i$. 
% Each task $\tau_{ij}$ within an activity $A_i$ comprises a set of state-action transitions represented as $\psi_{ij} = \{\psi_{ij}^{[k]}\} $, for integers $1 \leq k \leq m_{ij}$, where $m_{ij}$ is the total number of state-action transitions for each task $\tau_{ij}$ within the activity $A_i$. Each element in $\psi_{ij}$ corresponds to a low-level state-action transition.
% For the sake of simplicity and without loss of generality, we will refer to the total number of tasks per activity ($n_i$) and the total number of state-action transitions per task ($m_{ij}$) as $n$ and $m$ respectively.
% Cost function \(\mathcal{C}: \psi_{ij}^{[k]} \rightarrow \mathbb{R}\) denotes the time cost of executing an action. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LLMs for task anticipation}
% LLMs like GPT-3~\cite{brown2020language} are deep network architectures trained with large volumes of text found online to process and predict text sequentially. %can be used for household task anticipation by leveraging their natural language understanding capabilities to process and generate text-based information. They process text sequentially, making them adept at capturing patterns that unfold over the course of a narrative or dialogue. 
LLMs like GPT-3~\cite{brown2020language} can be tuned to predict patterns (of tasks) in specific domains. %further enhancing their pattern recognition abilities.
In our household domain, the high-level tasks are daily living tasks, e.g., \textit{iron clothes} and \textit{vacuum the house}, that are often performed at specific times and in a specific order, e.g., \textit{wash clothes} and \textit{dry clothes} are completed before \textit{iron clothes}. Our framework uses an LLM to extract these task patterns from a small number of task routines provided as prompts. As described in Section~\ref{sec:expt}, these routines can have $\approx 20$ high-level tasks. Given a partially specified routine, the LLM can predict tasks that the agent is likely to be asked to complete next---see Figure~\ref{fig:pipeline}(a).

Our choice of using the LLMs to model and predict sequences of high-level tasks is motivated by two objectives: (i) exploiting the complementary strengths of generic LLMs and domain-specific knowledge-based planning methods; and (ii) leveraging the capabilities of an LLM with limited examples of routines of interest. As described in Section~\ref{sec:expt}, we explored the use of popular LLMs such as PaLM~\cite{chowdhery2022palm}, GPT-3.5\cite{brown2020language}, and GPT-4\cite{OpenAI2023GPT4TR}. We also explored the effect of using context-specific examples during training or execution. %However, the agent still needs to compute and execute a sequence of finer-granularity actions to accomplish each task.

%We aim to harness LLMs' experiential learning to discern household task patterns from example routines. Daily chores often follow consistent sequences and timings. By understanding these from a few examples, LLMs can predict future routines. The agent should grasp sub-task sequences and efficiently interleave tasks from varied activities.

%\vspace{-5pt}
%In our analysis, we evaluate the performance of popular LLMs, including 
%We feed the LLM contextual environment data and a few initial tasks for the day, expecting it to \textit{anticipate} the next $k$ tasks, demonstrating its pattern recognition and generalization in household settings. We also fine-tune the LLM with in-context examples. Experiment details are in Section \ref{sec:expt}.
% We provide the LLM with contextual information about potential tasks $\tau$ in the environment and example routines. A routine is defined as a sequence of tasks created by sampling any $a$ activities and interleaving their tasks.

%\vspace{-5pt}

% \begin{figure}[]
% \centering
% \captionsetup{font=scriptsize}
% \setlength{\belowcaptionskip}{-15pt}
% \includegraphics[height=7cm,width=0.5\textwidth]{sections/figures/Room_Map (2).png}
% \caption{A Room Map with a description of objects in a particular room and the time taken to move between rooms or locations.}
% \label{fig: room_map}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Action Planning with Anticipated Goals}
The tasks anticipated by the LLM are considered as goals, and our framework uses a classical planner to compute the sequence of finer-granularity actions to be executed to jointly achieve these goals. As stated earlier, the planner uses domain-specific knowledge in the form of a theory of actions; we use the STRIPS~\cite{STRIPS} subset of PDDL~\cite{pddl} enriched with types, negative preconditions, and action costs as the action language to describe this theory. We also focus on goal-based problems and discrete actions with deterministic effects; other action languages can be used to represent durative actions~\cite{gerevini:FI11} or non-determinism~\cite{mohan:JAIR19}.

% \vspace{-1em}
For any given domain, statements in PDDL describe the \texttt{domain} and the \texttt{problem} to the solved. The domain description $\mathcal{D} = \langle \mathcal{S}, \mathcal{H}\rangle$ comprises a signature $\mathcal{S}$ and a theory $\mathcal{H}$ governing the domain dynamics. The signature $\mathcal{S}$ includes a specification of \textit{types} such as \textit{location}, \textit{object}, \textit{receptacle} and \textit{agent}; \textit{constants} such as \textit{kitchen} and \textit{garden} that are specific instances of the types; and \textit{predicates} that include \textit{fluents}, \textit{statics}, and \textit{actions}. Fluents such as \textit{(agent\_at ?l - location)}, \textit{(obj\_at ?o - obj ?l - location)}, and \textit{(dropped ?o1 - obj ?r - receptacle ?l - location)} represent domain attributes whose values can change over time as a result of actions; \textit{statics} are domain attributes whose values do not change over time; and \textit{actions} such as \textit{move\_agent}, \textit{cook}, \textit{serve}, and \textit{pickup} change the value of relevant fluents. $\mathcal{D}$ also includes a specification of each action in terms of its parameters, preconditions that need to be true for the action to be executed, effects that will be true once the action is executed, and the cost of executing it. For example, Figure~\ref{fig:Action} provides the specification of the \textit{dusting} action; the preconditions are that the agent have a mop in its hand and be in a location with an object that needs to be dusted.

%Let $T^{'}$ be the list of tasks anticipated by LLM. We employ classical PDDL planners, which, given the environment's initial and current states, and the states corresponding to anticipated tasks in $T^{'}$, return a sequence of actions ${\pi^{*}}$.

% The optimal sequence of actions \(s_t^*\) is defined as the sequence \(s_t^* = s_{t1}^*, s_{t2}^*, \ldots, s_{tq}^*\) that minimizes the total cost, given by:
% \begin{equation}
% \label{eq: cost_min}
% s_t^* = \text{argmin} \left( \sum_{i=1}^{q} \mathcal{C}(\psi_{ij}^{[k]}) \right)
% \end{equation}

%Actions in $\pi^{*}$ span multiple tasks, rather than being exclusive to one. For instance, to optimize the {\tt total-cost}, an agent might \textit{pickup} both the \textit{watering-hose} and \textit{lawn-mower} in one pantry visit. These actions relate to different tasks: \textit{watering plants} and \textit{cutting grass}. The planner's goal is to minimize the overall plan duration, choosing actions that reduce task execution time. Leveraging the LLM's predictions, the planner can proactively prepare for future tasks as illustrated by Figure \ref{fig: comparison_plan}.

%Formally, the Household environment can be seen as a tuple \textit{(L, O, S, A)} where $L$ are the locations, $O$ is a finite set of Object types, $S$ corresponds to state space and $A$ is the action space of the environment. PDDL Structures the planning problem into two core components \textit{Domain} and \textit{problem}.\\\\
%\textit{Domain description} encapsulates the environment's model, specifying state variables and actions such as \textit{move}, \textit{pickup}, and \textit{putdown}. For our approach, we used the STRIPS\cite{STRIPS} subset of PDDL, enriched with types, negative preconditions, and action costs.

%In domain's {\tt predicates} section, we define binary state variables, which when grounded, occupy the state space $S$. For each state $s\in S$, the agent chooses an action $a\in A$ from the applicable actions $A(s)\subseteq A$, facilitating state transitions according to the transition model $F: S\times A\rightarrow S$. 
\begin{figure}[tbp] % Use figure* to span both columns
\captionsetup{font=scriptsize}
%\setlength{\belowcaptionskip}{-1pt}
\begin{center}
    \begin{minipage}{0.45\textwidth} % Adjust the width as needed
        \small{
        \begin{lstlisting}[language=Lisp, frame=single, breaklines=true]
(:action dusting
 :parameters(?o - obj ?l - location)
 :precondition(and(In_hand DustMop)  
                  (agent_at ?l)
                  (not(dusted ?o ?l)))
 :effect(and(dusted ?o ?l)(increase (total-cost)10))
)
        \end{lstlisting}
        }
    \end{minipage}
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{-15pt}
    \caption{Action for \textit{Dusting} the object $o$ at some location $l$.}
    \label{fig:Action}
    \vspace{0.5em}
\end{center}
\end{figure}
\begin{figure}[tb]
\centering
\captionsetup{font=scriptsize}
%\setlength{\belowcaptionskip}{-14pt}
\vspace{0.3em}
\includegraphics[width=0.48\textwidth,frame]{sections/figures/Comparison_plan_1.png}
\caption{Example plans produced with and without considering anticipated tasks.}
\label{fig:comparison_plan}
\vspace{-1em}
\end{figure}
The problem description $\mathcal{P} = \langle\mathcal{O}, \mathcal{I}, \mathcal{G} \rangle$ describes the specific scenario under consideration in terms of the set $\mathcal{O}$ of specific objects, the initial state $\mathcal{I}$ comprising ground literals of the fluents and statics, and a description $\mathcal{G}$ of the goal state in the form of relevant ground literals. The planning task is to compute a sequence of actions $\pi = (a_1, \ldots, a_K)$ that takes the system from $\mathcal{I}$ to a state where $\mathcal{G}$ is satisfied. In our case, we compute plans using the state of the art Fast Downward (FD) system in the \textit{Autotune} configuration~\cite{Helmert_2006}. This heuristic planner adapts its parameters based on instances of the domain under consideration and supports different heuristics and options. We focus on minimizing the \texttt{total cost} $C$ of the plan, i.e., if $c_k^j$ is the cost of action $a_k^j$ in plan $\pi^j$, the objective would be to compute:
\vspace{-0.7em}
\begin{align*}
    \pi^* = \arg \min_{\pi^j} C(\pi^j), \quad C(\pi^j) = \sum_{k=0}^K c_{k}^j
\end{align*}
where the cost of each aciton corresponds to the time taken by the agent to execute it. $\mathcal{P}$ also includes some helper statements that guide this search for plans. Recall that the agent will typically focus on computing a plan for one high-level task at a time, e.g., left panel of Figure~\ref{fig:comparison_plan}; when the agent tries to compute an action sequence to jointly achieve multiple high-level goals, the overall plan length and execution cost can be reduced, e.g., right panel of Figure~\ref{fig:comparison_plan}.
%can be denoted as $(O,x_0,g)$ with $O$ being a set of varied-type objects, $x_0$ the initial states and $g$ the goal states. Both $x_0$ and $g$ are defined by grounding predicates using objects, where object types must align with their predicate parameters.
%A solution to the \textit{problem} is a plan, defined as $\pi = (a_1,...a_{n-1})$, a sequence of grounded actions $a\in A$ grounded by $o\in O$ that'll solve the problem by successive application of transition model $s_{i+1} = F(s_i, a_i)$ $\forall$ $0\leq i<n$ for each $a_i\in \pi$, starting from $x_0$ such that $s_n\in g$.

%We have used the \textit{action-costs} feature of PDDL to perform optimization. For every $a\in A$ there exists a cost that represents the time taken by the agent to perform that \textit{action}. All these costs add to the {\tt total-cost} of the plan. After defining these costs, we utilize the {\tt metric} specification of PDDL to {\tt minimize} the {\tt total-cost}.

%For plan generation, we employed the SOTA, domain-independent PDDL planner autotune-1 via Fast Downward\cite{Helmert_2006}. Using heuristic search, Autotune adjusts its parameters for each planning instance, optimizing plan generation. Despite its potency, in FD, optimal planning is time-intensive. However, given an adequate time, the planner yields near-optimal plans for complex problems.

% A plan is given as a sequence of actions $\pi = [a_0, a_1,...a_{t-1}]$ where $a_i\in A$. These actions will help \textit{agent} perform all the subtasks and tasks. We propose that $\forall$ $a_i \in \pi$ where $0\leq i<t$ $\exists$ cost $C_i>0$ that adds up to the {\tt total-cost}. The {\tt total-cost} $C$ of plan $\pi$ is  given by:
% \begin{equation}
%     C[\pi] = \sum_{i=0}^{t-1}C_{i}
% \end{equation}
% The PDDL planner gives us the satisficing plan $\pi^{*}$ that minimizes the cost. 
% %\begin{center}
%     %$\pi^{*} = \arg \min_{\pi} C[\pi_{j}]$ &where $1\leq j\leq p$.
% %\end{center}
% \begin{equation}
%     \pi^{*} = \underset{{\pi}}{\arg\min} \hspace{4pt}C[\pi_{j}] \hspace{12pt} where \hspace{4pt}1\leq j\leq p.
% \end{equation}
%  Here, $\pi_j$ is the list of all the plans the planner generated before coming up with $\pi^{*}$ and $p$ is the total number of plans.
%\vspace{5pt}
% \subsection{math stuff: Math in the next section is unrelated}

% Let the set of activities be $A = [A_1, A_2,...A_n]$ where $n$ is the total number of activities. These activities include \textit{Gardening}, \textit{Cooking}, \textit{Baking} etc.\\\\
% To perform each activity there is a list of tasks that must be performed. For example, in \textit{Gardening}, the tasks to be performed are \textit{watering the plants} and \textit{cutting the grass}. To perform $A_i$ the list of tasks to be performed is $\tau_{i} = [\tau_{i1}, \tau_{i2},...\tau_{im_{i}}]$ $\forall$ $1\leq i\leq n$. Here, $m_i$ is the total number of tasks to be performed for the task $\tau_{i}$.\\\\
% To perform each task in list $\tau_i$, there are some state conditions that must hold {\tt true}. These state conditions can be represented as a list $[s_{ij}^{[1]}, s_{ij}^{[2]},....s_{ij}^{[k_{ij}]}]$ $\forall$ $1\leq j\leq m_{i}$. Here, $k_{ij}$ is the total number of states that must be true to perform task $\tau_{ij}$. For example, In \textit{watering} task of \text{gardening} the states that must hold true are ({\tt agent-at} \textit{garden}), ({\tt In-hand} \textit{watering-hose}), etc.\\\\
% $S$ represents the state space encompassing all the possible states within the environment. For any given state $s \in S$ the agent can select an action $a \in A$ from the set of applicable actions $A(s) \subseteq A$. To transition a state to {\tt true}, a known \textit{transition model} $F$ is employed, defined as $F : S\times A \rightarrow S$.\\\\
% As explained in the previous section, the plan is defined as a sequence of actions $\pi = [a_0, a_1,...a_{t-1}]$. We propose that $\forall$ $a_i$ in $0\leq i<t$ $\exists$ cost $C_i$ that adds up to the {\tt total-cost}. The {\tt total-cost} $C$ of plan $\pi$ is  given by: $$C[\pi] = \sum_{i=0}^{t-1}C_{i}$$
% The PDDL planner gives us the satisficing plan $\pi^{*}$ that minimizes the cost. 
% \begin{center}
%     $\pi^{*} = \arg \min_{\pi} C[\pi_{j}]$ where $1\leq j\leq p$.
% \end{center}
%  Here, $\pi_j$ is the list of all the plans the planner generated before coming up with $\pi^{*}$ and $p$ is the total number of plans.






%cite the planner and pddl papers and pddl book.

% Shorten the planner part; Just explain how the heuristic works and also, compare the different planners and how you used them in your research by showing the results.

% In the problem setting section, in the end, explain the integration of LLM and PDDL, explaining how you are joining all the pieces of the puzzle.

% Do the prompting thing; "Today is Wednesday, dont do the washing today".

% Change distance to time in PDDL section.



\lstset{basicstyle=\footnotesize\ttfamily}
\captionsetup{font=scriptsize}



