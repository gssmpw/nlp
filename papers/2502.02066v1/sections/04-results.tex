%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup and Results}
%%%%%%%%%%%%%%%%%%%% EXPERIMENTAL SETUP SECTION %%%%%%%%%%%%%%%%%%%%
We experimentally evaluated four hypotheses:
\vspace{-0.75em}
\begin{enumerate}
\itemsep-10pt
\item[\textbf{H1:}] LLMs are able to accurately anticipate future tasks based on a small number of prompts of task routines.
\item[\textbf{H2:}] LLMs can take into account specific contextual information for task anticipation.
%\item[\textbf{H3:}] LLMs outperform the Markov Chain sampling baseline in accurately anticipating future tasks.
\item[\textbf{H3:}] Considering anticipated tasks as joint goals reduces plan length and plan execution time compared with considering one task at a time.
% \item[H4:] The advantage of task anticipation is invariant to different LLMs. 
% \item[H5:] Classical planner based plans work better on our domain than LLM based planners.
% \item[H6:] If the anticipated task does not occur, the agent can revert back to initial state.
\item[\textbf{H4:}] Our framework allows the agent to adapt to unexpected successes and failures by interrupting plan execution and replanning if necessary.%If there is an interrupt in the execution of a plan, our framework can generate a new plan based on the current state of the environment.
\end{enumerate}
We evaluated H1 using three LLMs: PaLM \cite{chowdhery2022palm}, GPT-3.5\cite{brown2020language} and GPT-4\cite{OpenAI2023GPT4TR}. For the other hypotheses we used GPT-4 as the default LLM. For H3, we computed plans using different configurations of the Fast-Downward system~\cite{Helmert_2006}, and we evaluated H4 qualitatively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}
\label{sec:expt}
We first describe the setup process we followed to experimentally evaluate the hypotheses.

\subsubsection{Prompting LLMs and Planning}
\label{sec:llm-prompting}
% We define the following terms for the purpose of describing our experiments:
% \begin{itemize}
%     \item \textbf{High-level task sequences:} 6 high-level tasks as mentioned in \ref{sec:problem}.
%     \item \textbf{Routines:} Task sequences created by sampling any $n$ high-level task sequences and interleaving their tasks according to the task list creation method described in \ref{sec:problem}. For our experiments, we use $n=4$ unless mentioned otherwise. 
% \end{itemize}
We created a dataset $\mathcal{T}$ of high-level tasks in the household environment. These tasks belong to activities such as \textit{Cooking, Cleaning, Washing, Baking}, and \textit{Gardening}. We then generated multiple task routines $\mathcal{R}_i$, each with $\approx 20$ tasks, by sampling tasks from different activities while preserving the relative order of tasks within each activity. %We follow a pipelining approach while creating the routines, where long duration tasks are pipelined with shorter duration tasks. 
This process helped us create task routines spanning activities of daily living in a home. 
% We follow a pipelining approach while creating the routines, where long duration tasks belonging to an activity $A_i$, are pipelined with shorter duration tasks from activity other than $A_{i}$ . This methodology allows us to create task routines that span various activities, ensuring a realistic representation of household tasks. For our experiments, we use $n_a=4$ unless mentioned otherwise.\\\\
%With reference to table \ref{tabl1}, we describe the following experiments:\\
%\vspace{-0.5em}


%We introduce the set of tasks $T$ defined in Section~\ref{sec:problem} to the LLM as the potential tasks in the environment, presented in JSON format (with randomized order) to minimize hallucination. We then randomly select two routines from $R_{n_a}$ and provide them to the LLM as routines for past two days. With a notion of the routines from previous days, the LLM is asked to complete the routine for a given day. We prompt the LLM with the $1^{st}$ two tasks of the third day which we label as \textit{routine\_3\_input}. Given the input, we query the LLM to anticipate the next $k$ tasks for that day as the \textit{routine\_3\_output} ($T^{'}$). Figure \ref{fig:llm_prompts} shows an example prompt and LLM output for this experiment.\\
%\textbf{With context experiments:} We use the same prompting technique as without context experiments with added in-context examples. To fine-tune the LLM, we use a method similar to \cite{silver2023generalized}, offering two in-context examples. For this, we create 2 sets of example \textit{routine\_3\_input}s, their corresponding \textit{routine\_3\_output}s and a reasoning to understand to give the LLM an idea of the kind of output expected. 
% The essential difference between \textit{without context} and \textit{with context} experiments is that we provide in-context examples on input and expected output as shown in Figure \ref{fig:gpt_context} when performing the \textit{with context} experiments. Such in-context examples are not given to the LLM for the \textit{without context} experiments.

 %These examples serve to guide the LLM in understanding the required output format and the reasoning behind it. In contrast, the without context experiments solely rely on the LLM's ability to discern patterns and anticipate tasks from the provided routines.\\

%\vspace{-10pt}
\begin{figure}[tbp]
\captionsetup{font=scriptsize}
  \includegraphics[width=0.475\textwidth]{sections/figures/LLM_prompt_no_context.png}
\setlength{\belowcaptionskip}{-15pt}
\caption{LLM prompting example.}
\label{fig:llm_prompts}
\end{figure}

\begin{table*}[!h]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-7pt}
\begin{tabular}{| >{\centering\arraybackslash} m{1.7cm}|  >{\centering\arraybackslash} m{2.5cm}| >{\centering\arraybackslash} m{3cm}| >{\centering\arraybackslash} m{0.9cm}| >{\centering\arraybackslash} m{2.5cm}| >{\centering\arraybackslash} m{3cm}| >{\centering\arraybackslash} m{0.9cm}|}
\hline
\\[-1em]
 \multirow{2}{*}{LLMs} & \multicolumn{3}{c|}{Without context} & \multicolumn{3}{c|}{With Context} \\
\cline{2-7}
\\[-1em]
 & Miss Ratio (Miss.) $\Downarrow$ & Partial Ordering Count (POC) $\Uparrow$ & $KRCC$ $\Uparrow$ & Miss Ratio (Miss.) $\Downarrow$ & Partial Ordering Count (POC) $\Uparrow$ & $KRCC$ $\Uparrow$  \\
\hline
\\[-1em]
PaLM & 0.361 & 0.974 & 0.993 & 0.034 & 0.994 & 0.996 \\
\hline
\\[-1em]
GPT-3.5-turbo & 0.282 & 0.676 & 0.906 & 0.0698 & 0.806 & 0.976 \\
\hline
\\[-1em]
\textbf{GPT-4} & \textbf{0.037} & \textbf{0.960} & \textbf{0.995} & \textbf{0.0006} & \textbf{1.0} & \textbf{1.0} \\
\hline
\end{tabular}
\caption{Task anticipation performance of LLMs. We ran 500 experiments with $\approx$ 20 tasks per experiment. We observed a significant increase in performance after providing contextual prompts. $\Downarrow$ ($\Uparrow$) implies that lower (higher) values of the corresponding measure represents better performance. Results support \textbf{H1} and \textbf{H2}.}
\label{tabl1}
\end{table*}

We conducted experiments under two configurations. In both configurations, the dataset $\mathcal{T}$ was provided to the LLMs (in JSON format) to minimize hallucinations and to ground our LLM outputs since these outputs are mapped to goal descriptions based on closed set of PDDL statements. Tasks not in $\mathcal{T}$ will be ignored and handling such tasks is beyond the scope of this paper. In the \textbf{without context} configuration, the task routines followed during two individual days were given as input (prompt) to the LLM, which was asked to complete a partially-specified routine comprising two tasks. Figure \ref{fig:llm_prompts} shows an example prompt and LLM output at this stage. In the \textbf{with context} configuration, in addition to the two task routines (as before), we used a method similar to~\cite{silver2023generalized} to provide two contextual examples in form of the partially-specified task inputs and the corresponding expected LLM outputs. The difference between the two configurations was thus the additional contextual prompting provided in the second configuration to guide the LLM. We considered four task anticipation performance measures:
\vspace{-0.5em}
\begin{itemize}
\itemsep-10pt
    \item \textit{Miss Ratio (Miss.)}: ratio of number of tasks \textbf{not} anticipated to the length of the sampled sequence.

\item \textit{Partial Ordering Count (POC)}: measures capability to maintain the relative order of tasks in the routines.

\item \textit{Kendall rank correlation coefficient (KRCC)~\cite{Kendall}}: measures match between predicted and actual task order.
\vspace{-0.5em}
\begin{align*}
    KRCC = \frac{n_c - n_d}{\sqrt{(n_0 - n_1)(n_0 - n_2)}}
\end{align*}
where: \(n_c\) is the number of concordant pairs, \(n_d\) is the number of discordant pairs, \(n_0 = n(n-1)/2\) is the total number of pairs, \(n_1\) and \(n_2\) are the sums of ties in the first and second sequences. A pair of tasks (\(\tau_{1}, \tau_{2}\)), (\(\tau_{3}, \tau_{4}\)) is said to be concordant if \(\text{rank}(\tau_1) > \text{rank}(\tau_3)\) and \(\text{rank}(\tau_2) > \text{rank}(\tau_{4})\) or \(\text{rank}(\tau_1) < \text{rank}(\tau_3)\) and \(\text{rank}(\tau_2) < \text{rank}(\tau_{4})\). If the ranks disagree, the pair is discordant. A tie can occur when there is a repetition of tasks in the anticipated routine, but since we do not consider task repetition, $n_2$ will always be zero.
    \item \textit{Success Ratio}: Fraction of experiments (i.e., prompts) for which tasks were anticipated correctly by the LLM.
\end{itemize}
Recall that to perform any high-level task, the agent has to plan a sequence of finer-granularity actions. In our experiments, the number of actions required to accomplish any given task varied from one to 16, with the initial domain description (for planning) comprising 33 independent actions, five different rooms, 33 objects distributed over 5-10 types, and 19 receptacles. \textit{We thus set up a complex domain for experimental analysis compared with other papers that have explored task anticipation or combined LLMs with PDDL-like representations for planning}. 

    % A pair of tasks (\(\tau_{1i}, \tau_{2i}\)), (\(\tau_{1j}, \tau_{2j}\)) is said to be concordant if the ranks for both elements agree in the way they compare to each other, i.e., (\(\tau_{1i} > \tau_{1j}\) and \(\tau_{2i} > \tau_{2j}\)) or (\(\tau_{1i} < \tau_{1j}\) and \(\tau_{2i} < \tau_{2j}\)). If the ranks disagree, then the pair is discordant.

%%%%%%%%%%%%%%%%%%%%%%%% BASELINES SECTION %%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5em}
\subsubsection{Baseline}
\label{sec:baseline}
As a baseline for the task anticipation capability of the LLMs, we sampled 100 routines of tasks and created a probability transition matrix representing the likelihood of transitioning from one task to another within the dataset:
\(
P(\tau_{j}|\tau_{i}) = \frac{Count(\tau_{i}, \tau_{j})}{Count(\tau_{i})}
\)
where $P(\tau_{j}|\tau_{i})$ is the probability of transitioning from task $\tau_{i}$ to task $\tau_{j}$, $Count(\tau_{i}, \tau_{j})$ denotes the number of occurrences of the transition from task $\tau_{i}$ to task $\tau_{j}$ in the dataset, and $Count(\tau_{i})$ represents the total number of occurrences of task $\tau_{i}$ in the dataset. We then created a Markov chain of the tasks such that given an initial task, subsequent tasks are obtained by repeatedly sampling from the probability transition matrix. For the planning experiments, the baseline was planning without considering any anticipated tasks.
%%%%%%%%%%%%%%%%%%%%%%%% RESULTS SECTION %%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experimental Results}
\label{sec:expres}
Next, we describe the results of experimental evaluation.

\subsubsection{Evaluating H1 and H2}
To evaluate \textbf{H1}, we evaluated the ability of PaLM, GPT-3.5 and GPT-4 to anticipate future tasks, as stated in Section~\ref{sec:llm-prompting}. We ran 500 experiments with tasks sampled from our household dataset, with the corresponding results summarized in Table~\ref{tabl1}. Even in the absence of contextual examples, all LLMs maintained the order of tasks in a routine. However, PaLM was not able to correctly anticipate all the tasks; it missed $approx 36\%$ of the tasks. When the LLMs had access to the contextual examples, all three provided very good performance, with GPT-4 providing the correct task order $100\%$ of the time along with a very low Miss Ratio (0.06\%). Overall, the LLMs were able to anticipate tasks based on a limited number of prompts, with performance varying between the three LLMs; all three LLMs provided good task anticipation performance when they had access to the contextual examples. %the queries and provide output in a JSON format with limited hallucination. 
These results support hypotheses \textbf{H1} and \textbf{H2}.
%from learning the distribution of tasks and their relative ordering, 
%\vspace{-1em}

Next, we experimented with GPT-3.5-turbo and GPT-4 under specific conditions. Specifically, we arranged tasks from our dataset in a weekly schedule.
% ~\footnote{Supplementary material: \href{https://raraghavarora.github.io/ahsoka}{https://raraghavarora.github.io/ahsoka}}. 
After providing the LLM with the routine of tasks during each day of the week, we posed a special prompt that deviated from the expected routine. One example of such a prompt is: "Today is a Monday. I have an urgent meeting in the morning." We observed that
%An example of such a prompt is provided alongside. 
LLMs were able to respond to such prompts and respect the constraint imposed by the prompt while generating the anticipated tasks. For the specific prompt (above), the output included most of the expected tasks for Monday and two extra tasks to set up a laptop and prepare clean clothes right after breakfast. We ran 20 such experiments, and the results are summarised in Table~\ref{h2_llm}. These results further support \textbf{H2}.
% \begin{wrapfigure}{r}{0.25\textwidth} % Adjust the width as needed
%   %\vspace{-\baselineskip} % Adjust vertical spacing if necessary
%     \vspace{-10pt}
%     \hspace{-12pt}
%   \centering % Align the prompt to the right
%   \includegraphics[trim=2pt 15pt 0pt 0pt,height=38px,width=0.25\textwidth,frame]{sections/figures/prompt (1).png}
%     \vspace{-10pt}
% \end{wrapfigure}

% \begin{table}[!h]
% \centering
% \captionsetup{font=scriptsize}
% \begin{tabular}{| >{\centering\arraybackslash} m{1.2cm}|  >{\centering\arraybackslash} m{0.9cm}| >{\centering\arraybackslash} m{0.95cm}| >{\centering\arraybackslash} m{0.7cm}| >{\centering\arraybackslash} m{0.9cm}| >{\centering\arraybackslash} m{1.2cm}| }
% \hline
%  & Miss. $\Downarrow$ & POC $\Uparrow$ & \tiny{KRCC} $\Uparrow$ & Extra $\Downarrow$ & Repeat $\Downarrow$ \\
% \hline
% \\[-1em]
% PaLM & 0.034 & 0.994 & 0.996 & 0 & 0 \\
% \hline
% \\[-1em]
% GPT 3.5-turbo & 0.0698 & 0.806 & 0.976 & 0 & 0 \\
% \hline
% \\[-1em]
% \textbf{GPT-4} & \textbf{0.0006} & \textbf{1.0} & \textbf{1.0} & \textbf{0} & \textbf{0} \\
% \hline
% \\[-1em]
% Markovian & 0.5071 & 0.24 & 0.89 & 8.256 & 1.778 \\
% \hline
% \end{tabular}
% \caption{Comparison of efficiency of LLMs with the Markovian baseline for anticipating future tasks based on given initial task. All the columns show metrics in fractions of tasks averaged over 500 experiments. Results support \textbf{H3}}
% \label{hist_baseline}
% \end{table}
\begin{table}[tb]
\centering
\captionsetup{font=scriptsize}
\begin{tabular}{| c | c |}
\hline
 & Success Ratio $\Uparrow$ \\
\hline
\\[-1em]
GPT-3.5-turbo & 0.65 \\
\hline
\\[-1em]
\textbf{GPT-4} & \textbf{0.8} \\
\hline
\end{tabular}
\setlength{\belowcaptionskip}{-5pt}
\caption{Task anticipation of two LLMs with contextual information. Success ratio expressed as a fraction of tasks averaged over 20 experiments. Results support \textbf{H2}.}
\label{h2_llm}
\end{table}
\begin{table}[tb]
\centering
\captionsetup{font=scriptsize}
\begin{tabular}{| >{\centering\arraybackslash} m{1.2cm}|  >{\centering\arraybackslash} m{0.9cm}| >{\centering\arraybackslash} m{0.95cm}| >{\centering\arraybackslash} m{0.7cm}| >{\centering\arraybackslash} m{0.9cm}| >{\centering\arraybackslash} m{1.2cm}| }
\hline
 & Miss. $\Downarrow$ & POC $\Uparrow$ & \tiny{KRCC} $\Uparrow$ & Incorr. $\Downarrow$ & Repeat $\Downarrow$ \\
\hline
\\[-1em]
\textbf{GPT-4} & \textbf{0.0006} & \textbf{1.0} & \textbf{1.0} & \textbf{0} & \textbf{0} \\
\hline
\\[-1em]
Markovian & 0.413 & 0.364 & 0.908 & 6.28 & 1.49 \\
\hline
\end{tabular}
\setlength{\belowcaptionskip}{-15pt}
\caption{Task anticipation of LLMs compared with the Markovian baseline. Values of measures expressed as an average over 500 experiments. Column "Incorr." summarizes the number of anticipated tasks that were incorrect, and "Repeat" shows the average number of repetitions per experiment. Results support \textbf{H1}.}
\label{hist_baseline}
\end{table}

% LLMs contain a basic world understanding which allows them to learn from input data and change it according to the prompt. We ground the LLM to our domain by providing it with a sample space of tasks as a JSON and asking it to sample only from the space of tasks. 
\begin{figure*}[tb]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-12pt}
\setlength{\abovecaptionskip}{5pt}
\includegraphics[height=0.35\textwidth,width=0.78\textwidth,frame]{sections/figures/Interrupt_2.png}
\caption{An illustrative use case that involved an interruption during the execution of a plan computed to jointly accomplish some anticipated tasks; the agent was able to revise the anticipated tasks and replan as appropriate.}
\label{fig:interrupt}
\end{figure*}
%\subsubsection{Validating Hypothesis H3}
\vspace{-1em}
Next, we compared the task anticipation ability of LLMs with the first-order Markovian baseline described in Section~\ref{sec:baseline}. For the baseline, we generated the routine of tasks based on the transition probability function of the baseline, with the results summarized in Table~\ref{hist_baseline}. Since the baseline was based on a Markov chain, there was no way for the system to recover if it reached a faulty state; it just continued to sample tasks from the faulty state. The deviation of the baseline from the correct routine (and the LLM's output) is shown in the columns labeled \textit{Incorr.} and \textit{Repeat} in Table~\ref{hist_baseline}. Unlike this baseline, the LLMs were able to %The baseline will need a large amount of data to capture the distribution of tasks from different activities. LLMs are able to 
learn from limited prompts representing domain-specific preferences. These results further support \textbf{H1}.

%%%%%%%
\subsubsection{Evaluating hypothesis H3}
To measure the impact of considering the anticipated tasks during planning, we measured the cost of executing the computed sequence of finer-granularity actions. Specifically, we prompted the LLM with contextual examples and asked it to provide different number of anticipated tasks for a partially-specified routine. These anticipated tasks were considered as joint goals by the planning system, with the resultant plan of actions being executed by the agent. Since the cost of the actions (in the domain description) was based on the execution time, we used the total cost of any executed plan as the execution time (in seconds) of the plan. The results of these experiments are summarized in Table~\ref{table:cost}. 

In these experiments, we varied the number of anticipated tasks from zero (``Myopic"), with the agent planning to perform one task at a time, to six, i.e., with six tasks anticipated and considered jointly during planning. As the number of anticipated tasks increased, the search time limit provided to the planner was increased by units of 30 seconds. For example, when the agent jointly planned an action sequence for the current task and the next (anticipated) task, the search time limit was set to 60 seconds. In addition, we organized these experiments as paired trials with the initial conditions and the overall sequence of assigned tasks being the same in each paired trial across the different number of anticipated tasks. Each value reported in Table~\ref{table:cost} is the average of 10 repetitions of the corresponding experiment.

% \vspace{-1em}
% We ran this experiment as a function of the number of anticipated tasks, which we varied from zero (``Myopic") with the agent planning to perform one task at a time, to six, i.e., with six tasks anticipated and considered jointly during planning. As the number of anticipated tasks increased, the search time limit provided to the planner was increased by units of 30 seconds. For example, when the agent jointly planned an action sequence for the current task and the next (anticipated) task, the search time limit was set to 60 seconds.
%We organized these experiments as paired trials with the initial conditions and the assigned set of tasks being the same in each paired trial across the different number of anticipated tasks. Each value reported in Table~\ref{table:cost} summarizing some of the results of these experiments is the average of 10 repetitions of the corresponding experiment.

\begin{table}[tb]
\centering
\begin{tabular}{|>{\centering\arraybackslash} m{1.3cm}|>{\centering\arraybackslash} m{1.8cm}|>{\centering\arraybackslash} m{1.1cm}|>{\centering\arraybackslash} m{1cm}|>{\centering\arraybackslash} m{1cm}|}
\hline
 \multicolumn{2}{|c|}{Number of tasks anticipated $\rightarrow$} & \multirow{2}{*}{0(Myopic)} & \multirow{2}{*}{3 (Ours)} & \multirow{2}{*}{6 (Ours)} \\
\cline{1-2}
 Planner & Parameters & & & \\
\cline{1-5}
\\[-1em]
\multirow{2}{*}{\textbf{AT-1}} & Plan Length & 70.3 & 65.2 & 61.8 \\
\cline{2-5}
\\[-1em]
 & Exe. Time & 2051 & 1658 & 1390 \\
\cline{1-5}
\\[-1em]
\multirow{2}{*}{LAMA} & Plan Length & 65.7 & 62.5 & 61.2 \\
\cline{2-5}
\\[-1em]
 & Exe. Time & 1835 & 1613 & 1599 \\
\cline{1-5}
\\[-1em]
\multirow{2}{*}{AT-2} & Plan Length & 67.2 & 64.3 & 60.2 \\
\cline{2-5}
 & Exe. Time & 1847 & 1591 & 1377 \\
\hline
\end{tabular}
\setlength{\belowcaptionskip}{-15pt}
\caption{Planning and execution performance with three different values of the number of anticipated tasks, based on three different configurations of the Fast Downward planner; AT1, AT2, and LAMA correspond to configurations \textit{seq-sat-fd-autotune-1}, \textit{seq-sat-fd-autotune-2}, and \textit{seq-sat-lama-2011} respectively. \textit{Exe. Time} denotes the plan execution time in seconds. Results support hypothesis \textbf{H3}.}
\label{table:cost}
\end{table}
%Following this, the agent is then trained to anticipate future tasks, starting with one task ahead. For each anticipated task, the search time limit is increased by 30 seconds. For instance, when the agent plans for two tasks at once (the current task and one anticipated task), the search time limit to the planner is set to 60 seconds. This process is continued until the agent can anticipate up to six tasks ahead, where seven tasks are planned at a time with a search time limit of 210 seconds.

% \vspace{-1em}
Table~\ref{table:cost} shows the plan length and execution time as the number of anticipated tasks changes from zero to three and six, under three configuration options supported by FD: \textit{seq-sat-fd-autotune-1, seq-sat-lama-2011}, and \textit{seq-sat-fd-autotune-2}. For each configuration, as the number of anticipated tasks increased, the plan length decreased. There may be positive interaction between goals, with actions executed to achieve one goal leading to conditions that are essential preconditions for actions to be executed to achieve a subsequent goal. In such cases, planning jointly for the two goals will not make a big difference compared with pursuing one goal at a time. However, positive interaction between goals was unlikely in our experiments because tasks were anticipated by the LLM at a high level of abstraction.
% implies that one action leads to a goal state, which may be a prerequisite for actions to be executed for a future goal. In cases of positive interactions between goals, whether we complete these goals simultaneously or sequentially, the plan length remains relatively unchanged. For our work, we can consider that positive interactions between goals are less likely at the high level of abstraction at which we anticipate goals using LLMs. 
The observed reduction in plan length in Table~\ref{table:cost} thus indicates that anticipating and planning to jointly achieve multiple tasks enables the agent to complete all the tasks by executing fewer actions compared with not considering anticipated tasks. Furthermore, the plan execution time decreased with an increase in the number of anticipated tasks, indicating that the agent became more efficient when it interleaved the actions for different tasks. In these planning trials, the agent often had to use the available planning time limit to compute the plans, particularly when the task required it to sequence multiple actions and when multiple anticipated tasks had to be considered. As a result, although there was a reduction in plan length and execution time, the planning time reached a plateau and did not change much during the experiments summarized in Table~\ref{table:cost}. These results support \textbf{H3}.

%However, the planning time generally increases with the number of anticipated tasks, indicating that more computational resources are required to anticipate and plan for more tasks.
\vspace{-1em}
Figure~\ref{fig:trend} further illustrates the results of these planning experiments for the \textit{seq-sat-fd-autotune-1} configuration. Since the initial state and set of tasks varied between each set of paired trials, averaging the numbers (e.g., for execution time) across these trials may not be meaningful. In each paired trial, we thus expressed the execution time and plan length of each instance that involved one or more anticipated tasks, as a fraction of the corresponding execution time and plan length (respectively) obtained without any task anticipation (i.e., "myopic"). The average of these fractions over 10 repetitions is shown in Figure~\ref{fig:trend}. We observed a substantial ($\approx 31\%$) reduction in execution time and plan length ($\approx 12\%$) as the number of anticipated tasks increased to six. We thus conclude that anticipating future tasks and considering them during planning leads to more efficient performance; these results further support \textbf{H3}.
\begin{figure}
    \centering
    \footnotesize{
    % \begin{subfigure}{0.9\columnwidth}
        \includegraphics[height=0.3  \textwidth,width=\linewidth]{sections/figures/mean_ratios.png}
        % \caption{Trend for AT-1 while sequentially increasing the search time limit of the planner}
    % \end{subfigure}
    
    \setlength{\belowcaptionskip}{-15pt}
    \caption{Plan execution time and plan length with the number of anticipated tasks ranging from 1-6. Values expressed as ratio of the corresponding values in the myopic case (x=0). We observed a reduction of 31\% in execution cost, and 12\% in plan length.}
    \label{fig:trend}
    }
\end{figure}

\subsubsection{Evaluating hypothesis H4}
Finally, we qualitatively evaluated H4. In these experiments, the execution of a plan generated by a planner was randomly interrupted by the user. Each interrupt was accompanied by a prompt by the user, leading to a change in the agent's operation. In particular, the state of the environment before the interrupt and the prompt (used for interrupting) were sent to the LLM to generate a new routine of tasks to be accomplished by the agent. While some actions in our domain are irreversible, e.g., we cannot put the pieces of a cut fruit back together, our combination of LLMs and action planning was able to undo the effects of some actions when that is appropriate. For example, in Figure~\ref{fig:interrupt}, action execution was interrupted by the prompt ``There is an urgent meeting now. Do not prepare breakfast". In this situation, the agent was able to plan suitable actions and put the vegetables back in their original location because it was no longer necessary to cook a hot breakfast; the agent instead computed action sequences to prepare the laptop and suitable clothes for the meeting.
%for interrupt is re-sent to the LLM to generate a new sequence of tasks. While some actions in our dataset are irreversible, we demonstrate that LLMs are able to revert some tasks which are reversible, like putting vegetables back to their original position. Figure \ref{fig:interrupt} demonstrates an instance where the execution of task is interrupted with the prompt ``There is an urgent meeting now. Don't prepare the breakfast.''. LLM is able to analyze the state of the environment and the prompt, to generate action sequences which reverts the task of cooking, and prepares for the meeting by setting up the workstation and preparing fresh clothes.

% \begin{figure}
%     \centering
%     \captionsetup{font=scriptsize}
%     \includegraphics[width=\columnwidth]{sections/figures/0_3_6.png}
%     \caption{Analysis of execution time in proactive planning. The myopic planner exclusively focuses on the immediate user-provided task, whereas the orange bar illustrates the outcomes of planning for three tasks in advance, and the green bar exemplifies the cost incurred when planning for six tasks ahead through anticipation. \todo{Add error bars}}
% \end{figure}


%\begin{figure}[!htbp]
%\centering
%\captionsetup{font=scriptsize}
%\includegraphics[width=0.5\textwidth]{sections/figures/Cost_Comparison.png}
%\caption{Cost Comparison between different heuristics}
%\end{figure}

% \begin{figure}[]
% \centering
% \includegraphics[width=0.5\textwidth]{sections/figures/fig3.png}
% \caption{Heatmap for transition probability matrix of common household tasks. The colour legend on bottom left shows the high-level sequences each task belongs to.\todo{Reduce the activity list to 4 (provide complete plot of 6 activities in supplementary )}}
% \label{fig: heatmap}
% \end{figure}
% \color{black}

