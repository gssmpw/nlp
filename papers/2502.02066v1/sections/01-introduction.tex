%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%{\textcolor{orange}{
%\textbf{Scope/Research Question:}\\
%1) Are LLMs better in anticipating tasks? Fine tuning required?\\
%2) Are LLMs better for planning or integration with PDDL is a must?\\
%3) Will Anticipating a task result in a better performance(Why should we anticipate)? \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %\textbf{Para1: Start with a thought/idea about the application of anticipation in robots}\\
Consider an agent assisting humans with daily living tasks in a home. %Generally, the user provides a sequence of tasks to an agent. The agent randomly follows all the instructions provided by the user. Humans with their analytical thinking and commonsense reasoning can complete tasks efficiently by reducing time and energy on the go. Humans tend to optimize their time schedule by broadly two ways, one by multitasking, for example, a person can keep a cake for baking in the oven while simultaneously preparing the cake icing, and other by parallelization, for example, a person can pick multiple items at a time and complete the task in another room. The agent with human-like capabilities would be able to anticipate future tasks and create plans that would decrease the time spent to perform multiple tasks when in combination. (generic, need to change to specific example)\\
In the scenario in Fig.~\ref{fig:teaser}, these tasks include making the bed and cooking breakfast, with each task requiring the agent to compute and execute a sequence of finer-granularity actions, e.g., fetch the relevant ingredients to cook breakfast. Since the list of tasks can change based on resource constraints or the human's schedule, the agent is usually asked to complete one task at a time. However, the agent can be more efficient if, similar to a human, it anticipates and prepares for upcoming tasks while computing a plan of finer-granularity actions, e.g., it can fetch the ingredients for breakfast when it fetches milk to make coffee.\\
%Its current approach necessitates continuous human involvement to assign tasks. To avoid this, a close to realistic assistive agent must anticipate tasks and create efficient plans to minimize time and resource consumption. They can imitate the strategies that the humans carry out to optimize their chores through multitasking and parallelization. Multitasking, for instance, involves activities like baking a cake while preparing cake icing simultaneously. Parallelization would include picking up multiple items at once and executing tasks in separate rooms.\\
%\textbf{Para2: Write about the logical approach}\\
% \\
%\textbf{Para2:Describe shortly about other used techniques}\\
%\vspace{-0.5em}
\indent State-of-the-art methods for estimating future tasks or their costs formulate them as learning problems and use data-driven deep networks~\cite{8460924,dhakal2023anticipatory}. There is also work on using Large Language Models (LLMs) for task planning~\cite{pmlr-v205-huang23c,ding2023task,lin2023text2motion}. However, these methods predict sequences of high-level tasks or require a large labeled training dataset to compute a sequence of fine-grained actions. %\todo{Change this - they mostly work on closed-set problems}. 
They also make it difficult to leverage domain knowledge, adapt to environmental changes, or to understand the decisions made.\\
%For a common household environment, the user gives a natural language based command to the agent, which can be better taken care of by Large Language Models (LLMs). LLMs are proficient in recognizing patterns and would only require few in-context examples to provide a close to accurate result. \\
\begin{figure}[tb]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-10pt}
\includegraphics[width=0.4\textwidth]{sections/figures/Anticipation_Teaser.png}
%\includegraphics[height=180px,width=0.5\textwidth]{sections/figures/Anticipation_Teaser.png}
\caption{Anticipation example: (a) Agent individually moves the milk and then the food to the desk; (b) Agent anticipates that milk needs to be served after food, jointly moving them to eliminate an extra trip.}
\vspace{-6pt}
\label{fig:teaser}
\end{figure}
%\textbf{Para3: Describe about our method and contributions}\\
% \vspace{-0.5em}
\indent We pose high-level task anticipation and finer-granularity action execution as a combined prediction and planning problem. Our framework seeks to leverage the complementary strengths of data-driven estimation based on generic prior knowledge of household tasks, and planning based on domain-specific action theories. Specifically, our framework: %anticipate necessary tasks and aims to utilize the complementary strength of LLMs and PDDL in the following ways:\vspace{-7pt}
 %    \item We concur that LLMs are proficient in predicting sequences into the future given very few language statements that signify task specifications. We exploit the ability of LLMs to very accurately predict the next sequence of tasks over long horizon give few prompts as inputs
 % \vspace{-7pt}
 %    \item We postulate and verify a number of hypotheses, H1-H5 (see subsection \ref{sec: expt}) and showcase how Anticipatory Planning offered by AHSOKA outperforms planning in the absence of anticipation. Further we show by verifying those hypotheses that LLM adapts seamlessly to out of distribution and out of context sequences even as the framework adapts adroitly to task interruptions, something that proves very difficult for the histogram baseline sequencer.\vspace{-7pt}
 %    \item The weakness of LLM in the domain of planning is handled by Planning Domain Definition Language (PDDL)\cite{pddl}. PDDL provides a correctness guarentee to come up with a near optimal execution of a task list. PDDL domains use multiple heuristics that can be used to minimize a particular objective function(in our case cost in the form of time is considered, see \ref{eq: cost_min}).  \vspace{-7pt}
 %    %The parallelization and multitasking of tasks would affect the time to complete tasks showcases the advantage in time when compared with plans that don't take into account any sort of anticipation.
 %    \item We provide a detailed PDDL domain for household related tasks that can be used for experimentation.\vspace{-7pt}
 %    \item We tabulate comparative sequence prediction performance across popular LLMs and the histogram baseline and we ablate the planner's efficiency across varying upper bounds on planning time.\vspace{-7pt}
 %    \item Further we showcase AHSOKA's efficacy in realistic simulation environments such as Virtual Home.
% \vspace{-0.75em}
% \begin{enumerate}
% % \itemsep-8pt
% \item Leverages the generic knowledge encoded in LLMs using a small number of prompts describing potential sequences of high-level household tasks, to predict subsequent tasks given partial sequences of tasks. 
% %\vspace{-0.5em}
% \item Uses an action language to encode finer-granularity domain-specific knowledge of household tasks comprising domain and agent attributes, agent actions, axioms governing change, and heuristics to guide planning.
% %\vspace{-0.5em}
% \item Adapts a symbolic and heuristic classical planner to consider both the immediate and the anticipated tasks as goals, computing a sequence of actions to jointly minimize the cost of accomplishing these goals. 
% \vspace{-0.75em}
% \end{enumerate}
\\\indent 1. Leverages the generic knowledge encoded in LLMs using a small number of prompts describing potential sequences of high-level household tasks, in order to predict subsequent tasks given partial sequences of tasks.
\\\indent 2. Uses an action language to encode finer-granularity domain-specific knowledge of household tasks in the form of domain and agent attributes, agent actions, axioms governing change, and heuristics to guide planning.
\\\indent 3. Adapts a symbolic and heuristic classical planner to consider both the immediate and the anticipated tasks as goals, computing a sequence of actions to jointly minimize the cost of accomplishing these goals. 
\begin{figure*}[!htbp]
\centering
\captionsetup{font=scriptsize}
\setlength{\belowcaptionskip}{-10pt}
\includegraphics[height=0.4\textwidth,width=0.95\textwidth]{sections/figures/TA_Diagrams_1.png}
\caption{Our framework's pipeline: (a) user inputs prompts with sequences of household tasks to an LLM, which then predicts high-level tasks over a time horizon; (b) the sequence of tasks is mapped to a joint goal state in a finer-granularity domain description in an action language (PDDL); (c) a heuristic planner (FD) uses this description to jointly compute the sequence of actions to be executed to complete all the tasks; and (d) the plan is executed in a realistic simulation environment.\\$^*$Actions corresponding to \texttt{fold clothes} are omitted due to space restrictions.}
\label{fig:pipeline}
\end{figure*}
\\\indent We prompt existing LLMs for high-level task anticipation, use the Planning Domain Definition Language (PDDL)~\cite{pddl} as the action language, and use the Fast Downward (FD) solver~\cite{Helmert_2006} to generate fine-granularity plans for any given task. We evaluate our framework's abilities in \emph{VirtualHome}, a realistic simulation environment~\cite{puig2018virtualhome}, and in complex household scenarios involving multiple tasks, rooms, objects, and actions. We present a $31\%$ reduction in execution time and a $12\%$ reduction in plan length compared to a system that does not anticipate upcoming tasks.
%\vspace{0.4em}

%\textbf{Purpose:} Our research endeavors to offer a solution that enhances the efficiency of indoor robotic operations through anticipated planning of tasks.  \\
%\textbf{Highlighting points:}\\
%1) Our problem -$>$ Open set, Anticipatory Planning -$>$ Closed Set\\
%2) LLM bad at planning, PDDL not flexible for anticipation, combination of two would showcase better results\\
%\Madhav{This figure is not very illustrative. For example why should it complete cooking when the house is on fire, it gives such a feeling. Also it is unclear as to what is it anticipating?} \textcolor{cyan}{Changes Done} \\
%\Madhav{The teaser needs to be Impactful. It should get to the Table all the novelty in one shot. For example here you can have a regular tasklist and given a few tasks, you show it predicts the next task(s). Then you also show when inserted with an unexpected/open set task, it still manages to show the next task corresponding to the unseen task and gets back to the original task list} \textcolor{cyan}{Changes Done}


%\begin{figure*}[htbp]
%\centering
%\captionsetup{font=scriptsize}
%\setlength{\belowcaptionskip}{-10pt}
%\includesvg[width=0.95\columnwidth]{sections/figures/TA}
%\caption{AHSOKA Pipeline Diagram: (a) User provides a prompt with specific preferences to the LLM, which in turn anticipates a sequence of tasks. (b) This sequence is mapped to the goal states in the problem file. Using the domain and problem files, the planner, aided by its internal heuristic, formulates a plan. (c) Detailed low-level plan generation is showcased, with individual goal states highlighted and labeled. (d) The derived low-level plan is fed into a simulator, executing the outlined actions.}
%\label{fig:pipeline}
%\end{figure*}
%}}