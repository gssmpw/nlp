@inproceedings{kurmanji2023scrub,
    title={Towards Unbounded Machine Unlearning},
    author={Meghdad Kurmanji and Peter Triantafillou and Jamie Hayes and Eleni Triantafillou},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=OveBaTtUAT}
}

@inproceedings{fan2024salun,
    title={SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation},
    author={Chongyu Fan and Jiancheng Liu and Yihua Zhang and Eric Wong and Dennis Wei and Sijia Liu},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=gn0mIhQGNM}
}

@INPROCEEDINGS{Kong2024satml,
    author = { Kong, Zhifeng and Chaudhuri, Kamalika },
    booktitle = { 2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) },
    title = {{ Data Redaction from Conditional Generative Models }},
    year = {2024},
    volume = {},
    ISSN = {},
    pages = {569-591},
    abstract = { Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality. },
    keywords = {Filtering;Computational modeling;Machine learning;Robustness;Data models},
    doi = {10.1109/SaTML59370.2024.00035},
    url = {https://doi.ieeecomputersociety.org/10.1109/SaTML59370.2024.00035},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =apr
}

@article{moon2024feature,
    author={Moon, Saemi and Cho, Seunghyuk and Kim, Dongwoo},
    title={Feature Unlearning for Pre-trained GANs and VAEs},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={38},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/30138}, DOI={10.1609/aaai.v38i19.30138},
    abstractNote={We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST, CelebA, and FFHQ datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is more robust under the presence of malicious parties.},
    number={19},
    year={2024},
    month={Mar.},
    pages={21420-21428}
}

@article{bae2023gradientsurgeryoneshotunlearning,
      title={Gradient Surgery for One-shot Unlearning on Generative Model}, 
      author={Seohui Bae and Seoyoon Kim and Hyemin Jung and Woohyung Lim},
      year={2023},
      month={Jun},
      journal={ICML Workshop on Generative AI \& Law},
      url={https://arxiv.org/abs/2307.04550}, 
}

@misc{sun2023generativeadversarialnetworksunlearning,
      title={Generative Adversarial Networks Unlearning}, 
      author={Hui Sun and Tianqing Zhu and Wenhan Chang and Wanlei Zhou},
      year={2023},
      eprint={2308.09881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.09881}, 
}

@inproceedings{jia2023model,
    title={Model Sparsity Can Simplify Machine Unlearning},
    author={Jinghan Jia and Jiancheng Liu and Parikshit Ram and Yuguang Yao and Gaowen Liu and Yang Liu and Pranay Sharma and Sijia Liu},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=0jZH883i34}
}

@ARTICLE{chundawat2023zeroshot,
  author={Chundawat, Vikram S. and Tarun, Ayush K. and Mandal, Murari and Kankanhalli, Mohan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Zero-Shot Machine Unlearning}, 
  year={2023},
  volume={18},
  number={},
  pages={2345-2354},
  keywords={Data models;Training;Data privacy;Training data;Computational modeling;Regulation;Machine learning;Machine unlearning;machine learning security and privacy;data privacy},
  doi={10.1109/TIFS.2023.3265506}
}

@inproceedings{chen2023fast,
    title={Fast Model DeBias with Machine Unlearning},
    author={Ruizhe Chen and Jianfei Yang and Huimin Xiong and Jianhong Bai and Tianxiang Hu and Jin Hao and YANG FENG and Joey Tianyi Zhou and Jian Wu and Zuozhu Liu},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=BL9Pc7xsdX}
}

@INPROCEEDINGS{thudi2022unrolling,
    author = { Thudi, Anvith and Deza, Gabriel and Chandrasekaran, Varun and Papernot, Nicolas },
    booktitle = { 2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P) },
    title = {{ Unrolling SGD: Understanding Factors Influencing Machine Unlearning }},
    year = {2022},
    volume = {},
    ISSN = {},
    pages = {303-319},
    abstract = { Machine unlearning is the process through which a deployed machine learning model is made to forget about some of its training data points. While naively retraining the model from scratch is an option, it is almost always associated with large computational overheads for deep learning models. Thus, several approaches to approximately unlearn have been proposed along with corresponding metrics that formalize what it means for a model to forget about a data point. In this work, we first taxonomize approaches and metrics of approximate unlearning. As a result, we identify verification error, i.e., the $\ell_{2}$ difference between the weights of an approximately unlearned and a naively retrained model, as an approximate unlearning metric that should be optimized for as it subsumes a large class of other metrics. We theoretically analyze the canonical training algorithm, stochastic gradient descent (SGD), to surface the variables which are relevant to reducing the verification error of approximate unlearning for SGD. From this analysis, we first derive an easy-to-compute proxy for verification error (termed unlearning error). The analysis also informs the design of a new training objective penalty that limits the overall change in weights during SGD and as a result facilitates approximate unlearning with lower verification error. We validate our theoretical work through an empirical evaluation on learning with CIFAR-10, CIFAR-100, and IMDB sentiment analysis. },
    keywords = {Measurement;Training;Deep learning;Sentiment analysis;Computational modeling;Training data;Stochastic processes},
    doi = {10.1109/EuroSP53844.2022.00027},
    url = {https://doi.ieeecomputersociety.org/10.1109/EuroSP53844.2022.00027},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =Jun
}

@inproceedings{li2024machineunlearning,
    title={Machine Unlearning for Image-to-Image Generative Models},
    author={Guihong Li and Hsiang Hsu and Chun-Fu Chen and Radu Marculescu},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=9hjVoPWPnh}
}


@InProceedings{neel2021descent,
  title = 	 {Descent-to-Delete: Gradient-Based Methods for Machine Unlearning},
  author =       {Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {931--962},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/neel21a/neel21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/neel21a.html},
  abstract = 	 {We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion.}
}


@InProceedings{tarun2023deepunlearning,
  title = 	 {Deep Regression Unlearning},
  author =       {Tarun, Ayush Kumar and Chundawat, Vikram Singh and Mandal, Murari and Kankanhalli, Mohan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {33921--33939},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/tarun23a/tarun23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/tarun23a.html},
  abstract = 	 {With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning}
}

@inproceedings{chourasia2023forgetunlearning,
    author = {Chourasia, Rishav and Shah, Neil},
    title = {Forget unlearning: towards true data-deletion in machine learning},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Unlearning algorithms aim to remove deleted data's influence from trained models at a cost lower than full retraining. However, prior guarantees of unlearning in literature are flawed and don't protect the privacy of deleted records. We show that when people delete their data as a function of published models, records in a database become interdependent. So, even retraining a fresh model after deletion of a record doesn't ensure its privacy. Secondly, unlearning algorithms that cache partial computations to speed up the processing can leak deleted information over a series of releases, violating the privacy of deleted records in the long run. To address these, we propose a sound deletion guarantee and show that ensuring the privacy of existing records is necessary for the privacy of deleted records. Under this notion, we propose an optimal, computationally efficient, and sound machine unlearning algorithm based on noisy gradient descent.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {240},
    numpages = {46},
    location = {Honolulu, Hawaii, USA},
    series = {ICML'23}
}

@INPROCEEDINGS {chen2023boundaryunlearning,
    author = { Chen, Min and Gao, Weizhuo and Liu, Gaoyang and Peng, Kai and Wang, Chen },
    booktitle = { 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
    title = {{ Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary }},
    year = {2023},
    volume = {},
    ISSN = {},
    pages = {7766-7775},
    abstract = { The practical needs of the “right to be forgotten” and poisoned data removal call for efficient machine unlearning techniques, which enable machine learning models to unlearn, or to forget a fraction of training data and its lineage. Recent studies on machine unlearning for deep neural networks (DNNs) attempt to destroy the influence of the forgetting data by scrubbing the model parameters. However, it is prohibitively expensive due to the large dimension of the parameter space. In this paper, we refocus our attention from the parameter space to the decision space of the DNN model, and propose Boundary Unlearning, a rapid yet effective way to unlearn an entire class from a trained DNN model. The key idea is to shift the decision boundary of the original DNN model to imitate the decision behavior of the model retrained from scratch. We develop two novel boundary shift methods, namely Boundary Shrink and Boundary Expanding, both of which can rapidly achieve the utility and privacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 and Vggface2 datasets, and the results show that Boundary Unlearning can effectively forget the forgetting class on image classification and face recognition tasks, with an expected speed-up of 17x and 19x, respectively, compared with retraining from the scratch. },
    keywords = {Training;Privacy;Face recognition;Pipelines;Training data;Artificial neural networks;Machine learning},
    doi = {10.1109/CVPR52729.2023.00750},
    url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00750},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =Jun
}

@InProceedings{Golatkar2020forgettingoutside,
    author="Golatkar, Aditya
    and Achille, Alessandro
    and Soatto, Stefano",
    editor="Vedaldi, Andrea
    and Bischof, Horst
    and Brox, Thomas
    and Frahm, Jan-Michael",
    title="Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations",
    booktitle="Computer Vision -- ECCV 2020",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="383--398",
    abstract="We describe a procedure for removing dependency on a cohort of training data from a trained deep network that improves upon and generalizes previous methods to different readout functions, and can be extended to ensure forgetting in the final activations of the network. We introduce a new bound on how much information can be extracted per query about the forgotten cohort from a black-box network for which only the input-output behavior is observed. The proposed forgetting procedure has a deterministic part derived from the differential equations of a linearized version of the model, and a stochastic part that ensures information destruction by adding noise tailored to the geometry of the loss landscape. We exploit the connections between the final activations and weight dynamics of a DNN inspired by Neural Tangent Kernels to compute the information in the final activations.",
    isbn="978-3-030-58526-6"
}

@misc{sun2025forgetvectorsplayuniversal,
      title={Forget Vectors at Play: Universal Input Perturbations Driving Machine Unlearning in Image Classification}, 
      author={Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and Jiancheng Liu and Gaowen Liu and Sijia Liu and Yan Yan},
      year={2025},
      eprint={2412.16780},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.16780}, 
}

@article{kodge2024deep,
    title={Deep Unlearning: Fast and Efficient Gradient-free Class Forgetting},
    author={Sangamesh Kodge and Gobinda Saha and Kaushik Roy},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=BmI5p6wBi0},
    note={}
}

@INPROCEEDINGS {chen2023boundaryshifting,
    author = { Chen, Min and Gao, Weizhuo and Liu, Gaoyang and Peng, Kai and Wang, Chen },
    booktitle = { 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
    title = {{ Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary }},
    year = {2023},
    volume = {},
    ISSN = {},
    pages = {7766-7775},
    abstract = { The practical needs of the “right to be forgotten” and poisoned data removal call for efficient machine unlearning techniques, which enable machine learning models to unlearn, or to forget a fraction of training data and its lineage. Recent studies on machine unlearning for deep neural networks (DNNs) attempt to destroy the influence of the forgetting data by scrubbing the model parameters. However, it is prohibitively expensive due to the large dimension of the parameter space. In this paper, we refocus our attention from the parameter space to the decision space of the DNN model, and propose Boundary Unlearning, a rapid yet effective way to unlearn an entire class from a trained DNN model. The key idea is to shift the decision boundary of the original DNN model to imitate the decision behavior of the model retrained from scratch. We develop two novel boundary shift methods, namely Boundary Shrink and Boundary Expanding, both of which can rapidly achieve the utility and privacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 and Vggface2 datasets, and the results show that Boundary Unlearning can effectively forget the forgetting class on image classification and face recognition tasks, with an expected speed-up of 17x and 19x, respectively, compared with retraining from the scratch. },
    keywords = {Training;Privacy;Face recognition;Pipelines;Training data;Artificial neural networks;Machine learning},
    doi = {10.1109/CVPR52729.2023.00750},
    url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00750},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =Jun
}

@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={141--159},
  year={2021},
  organization={IEEE}
}

@article{sekhari2021remember,
  title={Remember what you want to forget: Algorithms for machine unlearning},
  author={Sekhari, Ayush and Acharya, Jayadev and Kamath, Gautam and Suresh, Ananda Theertha},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18075--18086},
  year={2021}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{fan2023salun,
  title={Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation},
  author={Fan, Chongyu and Liu, Jiancheng and Zhang, Yihua and Wong, Eric and Wei, Dennis and Liu, Sijia},
  journal={arXiv preprint arXiv:2310.12508},
  year={2023}
}

@article{warnecke2021machine,
  title={Machine unlearning of features and labels},
  author={Warnecke, Alexander and Pirch, Lukas and Wressnegger, Christian and Rieck, Konrad},
  journal={arXiv preprint arXiv:2108.11577},
  year={2021}
}

@inproceedings{golatkar2020eternal,
  title={Eternal sunshine of the spotless net: Selective forgetting in deep networks},
  author={Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9304--9312},
  year={2020}
}

@inproceedings{izzo2021approximate,
  title={Approximate data deletion from machine learning models},
  author={Izzo, Zachary and Smart, Mary Anne and Chaudhuri, Kamalika and Zou, James},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2008--2016},
  year={2021},
  organization={PMLR}
}

@inproceedings{gandikota2023erasing,
  title={Erasing concepts from diffusion models},
  author={Gandikota, Rohit and Materzynska, Joanna and Fiotto-Kaufman, Jaden and Bau, David},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2426--2436},
  year={2023}
}

@inproceedings{zhang2024forget,
  title={Forget-me-not: Learning to forget in text-to-image diffusion models},
  author={Zhang, Gong and Wang, Kai and Xu, Xingqian and Wang, Zhangyang and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1755--1764},
  year={2024}
}

@inproceedings{schramowski2023safe,
  title={Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models},
  author={Schramowski, Patrick and Brack, Manuel and Deiseroth, Bj{\"o}rn and Kersting, Kristian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22522--22531},
  year={2023}
}

@article{howard2020fastai,
  title={Fastai: a layered API for deep learning},
  author={Howard, Jeremy and Gugger, Sylvain},
  journal={Information},
  volume={11},
  number={2},
  pages={108},
  year={2020},
  publisher={MDPI}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}