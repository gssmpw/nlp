@INPROCEEDINGS{Kong2024satml,
    author = { Kong, Zhifeng and Chaudhuri, Kamalika },
    booktitle = { 2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) },
    title = {{ Data Redaction from Conditional Generative Models }},
    year = {2024},
    volume = {},
    ISSN = {},
    pages = {569-591},
    abstract = { Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality. },
    keywords = {Filtering;Computational modeling;Machine learning;Robustness;Data models},
    doi = {10.1109/SaTML59370.2024.00035},
    url = {https://doi.ieeecomputersociety.org/10.1109/SaTML59370.2024.00035},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =apr
}

@article{bae2023gradientsurgeryoneshotunlearning,
      title={Gradient Surgery for One-shot Unlearning on Generative Model}, 
      author={Seohui Bae and Seoyoon Kim and Hyemin Jung and Woohyung Lim},
      year={2023},
      month={Jun},
      journal={ICML Workshop on Generative AI \& Law},
      url={https://arxiv.org/abs/2307.04550}, 
}

@inproceedings{chen2023fast,
    title={Fast Model DeBias with Machine Unlearning},
    author={Ruizhe Chen and Jianfei Yang and Huimin Xiong and Jianhong Bai and Tianxiang Hu and Jin Hao and YANG FENG and Joey Tianyi Zhou and Jian Wu and Zuozhu Liu},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=BL9Pc7xsdX}
}

@inproceedings{chourasia2023forgetunlearning,
    author = {Chourasia, Rishav and Shah, Neil},
    title = {Forget unlearning: towards true data-deletion in machine learning},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Unlearning algorithms aim to remove deleted data's influence from trained models at a cost lower than full retraining. However, prior guarantees of unlearning in literature are flawed and don't protect the privacy of deleted records. We show that when people delete their data as a function of published models, records in a database become interdependent. So, even retraining a fresh model after deletion of a record doesn't ensure its privacy. Secondly, unlearning algorithms that cache partial computations to speed up the processing can leak deleted information over a series of releases, violating the privacy of deleted records in the long run. To address these, we propose a sound deletion guarantee and show that ensuring the privacy of existing records is necessary for the privacy of deleted records. Under this notion, we propose an optimal, computationally efficient, and sound machine unlearning algorithm based on noisy gradient descent.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {240},
    numpages = {46},
    location = {Honolulu, Hawaii, USA},
    series = {ICML'23}
}

@ARTICLE{chundawat2023zeroshot,
  author={Chundawat, Vikram S. and Tarun, Ayush K. and Mandal, Murari and Kankanhalli, Mohan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Zero-Shot Machine Unlearning}, 
  year={2023},
  volume={18},
  number={},
  pages={2345-2354},
  keywords={Data models;Training;Data privacy;Training data;Computational modeling;Regulation;Machine learning;Machine unlearning;machine learning security and privacy;data privacy},
  doi={10.1109/TIFS.2023.3265506}
}

@inproceedings{fan2024salun,
    title={SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation},
    author={Chongyu Fan and Jiancheng Liu and Yihua Zhang and Eric Wong and Dennis Wei and Sijia Liu},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=gn0mIhQGNM}
}

@inproceedings{jia2023model,
    title={Model Sparsity Can Simplify Machine Unlearning},
    author={Jinghan Jia and Jiancheng Liu and Parikshit Ram and Yuguang Yao and Gaowen Liu and Yang Liu and Pranay Sharma and Sijia Liu},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=0jZH883i34}
}

@article{kodge2024deep,
    title={Deep Unlearning: Fast and Efficient Gradient-free Class Forgetting},
    author={Sangamesh Kodge and Gobinda Saha and Kaushik Roy},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=BmI5p6wBi0},
    note={}
}

@inproceedings{kurmanji2023scrub,
    title={Towards Unbounded Machine Unlearning},
    author={Meghdad Kurmanji and Peter Triantafillou and Jamie Hayes and Eleni Triantafillou},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=OveBaTtUAT}
}

@inproceedings{li2024machineunlearning,
    title={Machine Unlearning for Image-to-Image Generative Models},
    author={Guihong Li and Hsiang Hsu and Chun-Fu Chen and Radu Marculescu},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=9hjVoPWPnh}
}

@article{moon2024feature,
    author={Moon, Saemi and Cho, Seunghyuk and Kim, Dongwoo},
    title={Feature Unlearning for Pre-trained GANs and VAEs},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={38},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/30138}, DOI={10.1609/aaai.v38i19.30138},
    abstractNote={We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST, CelebA, and FFHQ datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is more robust under the presence of malicious parties.},
    number={19},
    year={2024},
    month={Mar.},
    pages={21420-21428}
}

@InProceedings{neel2021descent,
  title = 	 {Descent-to-Delete: Gradient-Based Methods for Machine Unlearning},
  author =       {Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {931--962},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/neel21a/neel21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/neel21a.html},
  abstract = 	 {We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion.}
}

@misc{sun2023generativeadversarialnetworksunlearning,
      title={Generative Adversarial Networks Unlearning}, 
      author={Hui Sun and Tianqing Zhu and Wenhan Chang and Wanlei Zhou},
      year={2023},
      eprint={2308.09881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.09881}, 
}

@misc{sun2025forgetvectorsplayuniversal,
      title={Forget Vectors at Play: Universal Input Perturbations Driving Machine Unlearning in Image Classification}, 
      author={Changchang Sun and Ren Wang and Yihua Zhang and Jinghan Jia and Jiancheng Liu and Gaowen Liu and Sijia Liu and Yan Yan},
      year={2025},
      eprint={2412.16780},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.16780}, 
}

@InProceedings{tarun2023deepunlearning,
  title = 	 {Deep Regression Unlearning},
  author =       {Tarun, Ayush Kumar and Chundawat, Vikram Singh and Mandal, Murari and Kankanhalli, Mohan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {33921--33939},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/tarun23a/tarun23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/tarun23a.html},
  abstract = 	 {With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning}
}

@INPROCEEDINGS{thudi2022unrolling,
    author = { Thudi, Anvith and Deza, Gabriel and Chandrasekaran, Varun and Papernot, Nicolas },
    booktitle = { 2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P) },
    title = {{ Unrolling SGD: Understanding Factors Influencing Machine Unlearning }},
    year = {2022},
    volume = {},
    ISSN = {},
    pages = {303-319},
    abstract = { Machine unlearning is the process through which a deployed machine learning model is made to forget about some of its training data points. While naively retraining the model from scratch is an option, it is almost always associated with large computational overheads for deep learning models. Thus, several approaches to approximately unlearn have been proposed along with corresponding metrics that formalize what it means for a model to forget about a data point. In this work, we first taxonomize approaches and metrics of approximate unlearning. As a result, we identify verification error, i.e., the $\ell_{2}$ difference between the weights of an approximately unlearned and a naively retrained model, as an approximate unlearning metric that should be optimized for as it subsumes a large class of other metrics. We theoretically analyze the canonical training algorithm, stochastic gradient descent (SGD), to surface the variables which are relevant to reducing the verification error of approximate unlearning for SGD. From this analysis, we first derive an easy-to-compute proxy for verification error (termed unlearning error). The analysis also informs the design of a new training objective penalty that limits the overall change in weights during SGD and as a result facilitates approximate unlearning with lower verification error. We validate our theoretical work through an empirical evaluation on learning with CIFAR-10, CIFAR-100, and IMDB sentiment analysis. },
    keywords = {Measurement;Training;Deep learning;Sentiment analysis;Computational modeling;Training data;Stochastic processes},
    doi = {10.1109/EuroSP53844.2022.00027},
    url = {https://doi.ieeecomputersociety.org/10.1109/EuroSP53844.2022.00027},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month =Jun
}

