\section{Conclusion}%
\label{sec:conclusion}

This article has presented the first empirical analysis of tree-based classification model hyperparameters in terms of their susceptibility to MIA\@. It has been shown that simple human-interpretable rulesets can predict with approximately 90--95\% accuracy whether a given hyperparameter combination is one of the 20\% most vulnerable. Significantly, it has been shown that for most datasets, high target model accuracy is still obtainable with hyperparameter combinations that are not in this highest risk category.

The use of more complex rules/models would likely increase the accuracy of risk prediction, however the use of a simple Decision Tree to extract the ruleset enables easy interpretation by researchers so that they can more easily adjust their preferred hyperparameters into a safer region that maintains sufficient classification accuracy.

It is important to note that these rules cannot determine whether a hyperparameter combination can be considered safe since the absolute risk scores are highly dependant on the dataset. However, these results suggest that the relative risk of hyperparameters is consistent across most datasets and that these highest risk values can reasonably be removed from the hyperparameter tuning process and consequently before releasing a trained model into the wild without a significant loss of model performance. Some datasets/applications may still require some privacy trade-off for sufficient accuracy, but it seems reasonable that as the first stage of risk assessment it should be incumbent on the researchers wishing to release a model to first show that it is necessary to use these highest risk hyperparameters.

Once the ML model has been trained, the use of computationally cheap structural metrics as shown here can serve as sufficient, although not necessary, indicators of MIA vulnerability. Again, this approach for identifying vulnerable models cannot certify a model as safe, but potentially enables a large number of high risk models to be filtered and therefore a significant reduction in the number needing to undergo expensive MIAs such as LiRA\@.

Current work is integrating the learned rulesets and structural attacks within the open source \texttt{sacro-ml}~\cite{Smith:2024} Python package to provide a comprehensive hierarchical framework to automatically perform these risk assessments in addition to running current state-of-the-art MIAs. Future work may explore additional target models, hyperparameters, and datasets. Including, for example, the consideration of neural networks.
