\section{Background}%
\label{sec:background}

Trusted research environments (TREs)\footnote{Also known as safe havens, secure data environments, and secure research services.} offer a means for researchers to analyse confidential datasets and publish their findings. The globally recognised Five Safes approach to privacy preservation and regulatory compliance~\cite{Green:2023} requires TRE staff to independently check the disclosure risk posed by any proposed output before release is approved. This process of SDC is well understood for outputs of traditional statistical analyses. However, while the use of ML methods has rapidly expanded, a recent study of TREs by Kavianpour et~al.~\cite{Kavianpour:2022} in the UK revealed a shortfall of SDC understanding and tools to support the use of ML\@. In particular, while researchers can already use ML methods within TREs, the inability to export those trained models precludes reproducibility of results, a critical tenet of scientific practice~\cite{Begley:2015, Arnold:2019}. After Fredrikson et~al.~\cite{Fredrikson:2015} demonstrated that ML models can be vulnerable to attacks inferring aspects of training data, there has been a rapid growth in proposed attacks and counter-measures as surveyed by Hu et~al.~\cite{Hu:2022}. Regulatory authorities such as the UK Information Commissioner's Office~\cite{ico:2022} recognise this risk, but offer general advice rather than practical guidance and tools. 

A recent report by DARE UK~\cite{Dare:2022} highlighted the need to \textit{``Where possible, automate the review of outputs to support and focus the use of skilled personnel on the areas of most significant risk''}. Moreover, Jefferson et~al.~\cite{Jefferson:2022} have recently presented a detailed discussion of the implications of ML model release and proposed a set of guidelines for TREs.

\subsection{Relationship to Statistical Disclosure Control}%
\label{sec:structural}

We briefly introduce the notions of SDC and TREs, before turning our attention to the implications of this body of work for the risk assessment of ML models in practice.

\subsubsection{TREs and SDC}
For all the apparent novelty of risk assessing ML models, it is important to place these within a wider context of assessing the risk of privacy leakage from any outputs created from confidential data. From that perspective, risks such as attribute inference date back to at least the instructions for the 1850 US census:

\begin{quote}
    ``to consider the facts intrusted [sic] to them as\ldots~not to be used in any way to the gratification of curiosity, the exposure of any man's business or pursuits\ldots''~\cite{Ruggles:2023}
\end{quote}

Nowadays, national statistics institutes (and other holders of confidential information such as health authorities) routinely conduct SDC of all outputs they produce. They also offer access to researchers via TREs. TREs almost universally employ the Five Safes\footnote{Safe people, safe projects, safe settings, safe data, safe outputs.} framework to provide multiple layers of assurance to data owners, within which the final stage of checking for `safe outputs' is a vital part.

Notably, most TREs have long employed principles-based SDC~\cite{Alves:2020} with human output checkers involved to facilitate nuance and assess the `discreditability of an attack' in the interpretation of risk analysis. Similar ideas have recently been discussed for ML models~\cite{Rezaei:2023}. 

\subsubsection{Implications for ML Privacy Research}%
\label{subsec:struct_measures}

Thus for traditional forms of analyses, there is decades of well understood theory and practice surrounding the SDC of outputs such as (multi-dimensional) tables, regression models, survival statistics and a range of other outputs~\cite{Brandt:2010, Griffiths:2019} which have recently been consolidated into a coherent `statbarns' framework~\cite{Ritchie:2023}. From these we can identify three key concepts relevant to ML models:

\begin{itemize}
    \item Residual degrees of freedom risk (DoF): for regression models such as linear, logistic or probit, TREs typically require the number of records used to create the model exceeds the number of trainable parameters by at least 10.

        The equivalent of this can be directly calculated for ML models; for example, the number of values involved in specifying branches in (forests of) trees, or the number of trainable weights in neural networks.

    \item $k$-anonymity: queries should not report on groups of fewer than $k$ people. In practice, for tables this means that each cell must reach a minimum threshold count. 

        Since tree-based models (and by extension forests) create a partition of the input space in exactly the same way as tables do, this can be measured directly for a model if we have the training data and the model. For more complex cases we can derive a proxy for this for each training record: the number of other training records for which the model gives the same output (to some precision).

    \item Class disclosure risk. It should not be possible to make inference about all members of a subgroup via the publication of query results containing zeros, e.g., \textit{``in our fictitious survey no ML researchers were aware of the Secure Data Access Professionals manual~\cite{Griffiths:2019}''}\footnote{In some cases it may be argued that zeros may be structural (people born without certain organs are unlikely to develop cancers of those organs) or may be `evidential' and scientifically important: \textit{``no people with gene XYZ had disease A''}. Hence the practice of applying principles-based SDC rather than strictly applying rules.}.

        For ML models we can interpret class disclosure risks as occurring when the model's predicted probability for \textit{any} class falls below a threshold ($k$/size of training set) for \textit{any} record. In other words, a model that is completely confident in its predictions presents a class disclosure risk. While this might be seen excessively restrictive, the threshold will be small in practice, and we note that this is somewhat similar to the effects produced by the defence strategies proposed by MemGuard~\cite{memguard}. 

        We note in passing that protection against class disclosure is at best poorly catered for by DP training methods since the guarantees ($\epsilon$) multiply with the size of the subgroup, which may not be known in advance.
\end{itemize}
