\section{Introduction}%
\label{sec:introduction}

An increasing body of work has shown that machine learning (ML) models may expose confidential properties of the data on which they are trained~\cite{Hu:2022}. This has resulted in a wide range of proposed attack methods with varying assumptions that exploit the model structure and/or behaviour to infer sensitive information, e.g., to perform model extraction, attribute inference, and membership inference~\cite{Rigaki:2023, Jegorova:2023}.

One of the most fundamental attacks is to identify whether an individual record was part of the training data, known as a membership inference attack (MIA). Recognising different sources of uncertainty in the model training process, the MIA can be framed as the question: \textit{``what are the probabilities that a model $M$ comes from the population $\mathcal{M}_{\text{in}}$ (or $\mathcal{M}_{\text{out}}$) of models trained with (or without) a particular record $X$?''} For this to take a computational form requires a mapping from some set of metrics describing the structure and/or behaviour of models, and records, onto a probability distribution. Given the current state of knowledge, a key issue is how to identify one or more suitable metrics as proxies for underlying risk causes.

A common early approach uses the difference in confidence scores between predictions for samples that were in the training set and those that were not~\cite{Shokri:2017}. This approach fails to take into account that (hopefully) models will correctly identify regions where high probability records have the same label---whether they are from training set or not. The conceptual and methodological shortfalls of this approach have been well documented~\cite{Rezaie:2021}. 

More recent attacks implicitly focus on decision boundaries by considering the record-wise difficulty of making correct predictions for the original task. For example, the Likelihood Ratio Attack (LiRA) uses as proxy the prediction error (loss) for a record $X$ from the original target model $M$, comparing it to the distributions of loss values from sampled subsets from $\mathcal{M}_{\text{in}}$ and $\mathcal{M}_{\text{out}}$~\cite{Carlini:2022}. Similarly, the Quantile Regression~\cite{Bertran:2023} and Robust Membership Inference Attack (RMIA)~\cite{Zarifzadeh:2024} condition the confidence on that for other records (and models). 

A number of other risk factors/proxy measures have been established. Overfitting (quantified by generalisation error) is well-known to be a significant cause of information leakage~\cite{Yeom:2018, Yeom:2020}. Increasing model complexity therefore may pose a trade-off between accuracy and disclosure risk. For example, increasing the number of neural network parameters has been found to generally improve generalisation performance at the expense of lower privacy~\cite{Tan:2022}.

Consequently, a number of defences have been proposed~\cite{Hu:2023, Jarin:2023}. The most common of these is to incorporate information perturbation within the learning algorithm, such as adding noise through differential privacy (DP)~\cite{Dwork:2008}. However, the use of DP and knowledge distillation techniques may only mitigate some inference attacks~\cite{Liu:2022}. Moreover, it has recently been highlighted that in practice DP implementations suffer from a number of problems and standard anti-overfitting techniques can often achieve better results~\cite{Blanco-Justicia:2022}. For example, standard regularisation techniques have been shown to help mitigate the risk posed by an increasing number of model parameters~\cite{Tan:2023}.

The General Data Protection Regulation (GDPR) (and other laws, according to the data and jurisdiction) mean that before a ML model trained on confidential data can be released into the public domain, its vulnerability to attack should be assessed. However, some of the currently most effective attacks require considerable computational effort. For example, running a LiRA attack typically involves sampling (and hence training) 50--100 shadow models from $\mathcal{M}_{\text{out}}$. Therefore, from the perspective of people training models, and those tasked with that risk assessment, there is significant value in being able to rapidly identify vulnerable models. 

In this paper we investigate two approaches to this, which also cast light on the closely related issue of what are the underlying causes of model vulnerability, and what can be measured as proxies for those. First we explore the relationship between model hyperparameters and risk. That is, we use an empirical approach to learn simple human-interpretable rules that identify whether a model is \textit{relatively} high risk based on the hyperparameters independent of the training dataset and before any model fitting. These rules may provide additional insight into the causes of disclosure risk and enable researchers to avoid training unnecessarily high risk models, e.g., such combinations could be removed from a typical hyperparameter search. Furthermore, they may provide additional information to support making an assessment of whether a model should be publicly released.

However, since the risk of information disclosure depends on characteristics of both model and dataset, ascertaining the \textit{absolute} risk posed by a model must be performed after training. Therefore in the second strand of work we leverage decades of statistical disclosure control (SDC) practice for risk-assessing tables and regression models to design a number of computationally cheap structural measures based on the trained model and how it partitions the training data.

The contributions of this paper are:

\begin{enumerate}
    \item{In terms of \textit{ante-hoc} analysis of hyperparameter choices, we show that:}
        \begin{enumerate}
            \item The rank order of risk for different hyperparameter combinations is preserved across different datasets, i.e., is dataset independent.
            \item Risk is not correlated with target accuracy. 
        \end{enumerate}
        Therefore we can identify a region of unnecessary risk---which can be characterised by simple human-readable models, and used to help researchers avoid creating risky models in advance.
    \item In terms of \textit{post-hoc} analysis of trained model structure and how it partitions the decision space we show that:
        \begin{enumerate}
            \item We can design useful cheap-to-compute structural metrics based on properties of target models and their behaviour with training sets. 
            \item Empirical results suggest a combination of these structural metrics constitutes a sufficient but not necessary indicator of whether a target model is vulnerable to MIA attacks. In other words, a classifier using these structural metrics to predict MIA output has high precision but lower recall---although the latter improves if we take into account the statistical significance of MIA observations.
        \end{enumerate}
        This suggests that, like the conditioned loss metric at the heart of LiRA and other state-of-the-art attacks, they are proxies for some as yet unquantified latent property of models. 
    \item We make clear the fundamental difference between being able to rule out release of a model (as being disclosive) and being able to say that it is safe.
\end{enumerate}

These contributions allow us to avoid computationally costly MIA attacks and provide actionable insights into how to reduce disclosure risk.
