\section{Methodology}%
\label{sec:methodology}

\subsection{Attacks and Performance Metrics}

LiRA~\cite{Carlini:2022} is used here to perform membership inference and assess the disclosure risk presented by a range of fitted tree-based classification models. LiRA is a fine-grained black-box evaluation metric-based MIA~\cite{Niu:2023} and (especially when using sufficient shadow models) is representative of the current state-of-the-art. Moreover, it places an emphasis on an attacker scenario that corresponds to TRE concerns: \textit{``If a membership inference attack can reliably violate the privacy of even just a few users in a sensitive dataset, it has succeeded''}.

Building on this line of thinking, we note that attackers will almost always have access to some records they know are non-members of the training set---either because they have artificially created them, or perhaps because they may be their own records. We posit that armed with that information, and aware (from the literature) that in most cases the MIA cannot make a confident prediction, attackers will prefer a model that: (i) does not make high confidence member predictions for records they know are non-members; (ii) is correct when it predicts non-member with high confidence. 

From these premises we derive a frequency difference metric $\text{FDIF}_z$, calculated as follows:
    \begin{enumerate}
        \item For each training record we note: the probability (softmax value) of the MIA's prediction for the class member; and the true (binary) value.
        \item This list is sorted by decreasing confidence of the member prediction.
        \item The difference between the proportion of true positives in the extreme subsets; that is, the top and bottom $z \times N$ records respectively is calculated. The more positive this $\text{FDIF}_z$ value, the better the MIA is at discriminating when it is confident. Here we report scores for $z \in \{0.02,0.01,0.001\}$.
        \item Finally we calculate $P(\text{FDIF}_z)$: the probability of observing this (or greater) value as an effect of stochastic sampling if the MIA was making purely random guesses at the extrema.
    \end{enumerate}

The attack uses 100 shadow models to estimate the loss distributions. For each target model hyperparameter combination, 10 random training and test data splits are performed and a LiRA attack executed. Since the ML privacy community has yet to reach a consensus on a single disclosure risk metric, here a range of metrics are computed. The results are therefore averaged over the 10 runs to produce a set of risk metrics for a given target model hyperparameter combination on a dataset.

In addition to several MIA success metrics, several target model metrics are also examined for correlations. These are described in Table~\ref{table:metrics}.

\input{table-metrics.tex}

\subsection{Statistical Significance of Attack Vulnerabilities}%
\label{sec:tpr_ssd}

\subsubsection{Preliminaries}

Assume a researcher has access to a dataset $\mathcal{D}$ of size $N=|\mathcal{D}|$ which they split into disjoint training and test sets $\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{test}}$ of sizes $N_{\text{train}}$ and $N_{\text{test}}$ respectively. They use these to train and test a \textit{target} model $M$ for a classification problem with $L$ classes, so $M: Domain(\mathcal{D}) \rightarrow [0,1]^L$. For simplicity we assume the outputs of $M$ have been normalised (e.g., via a softmax function) so that they represent the predicted class probabilities for each record. 

MIAs may be created using synthetic datasets which approximate the underlying distribution from which $\mathcal{D}$ is drawn. However, as reported by Carlini et~al.~\cite{Carlini:2022} and Shokri et~al.~\cite{Shokri:2017}, and especially Ye et~al.~\cite{Ye:2022}, the success of those attacks increases as sources of uncertainty in that approximation are reduced or removed.

Thus, in recognition of the pace at which new MIAs are being invented, in our worst-case scenario, we remove those uncertainties to estimate an upper bound on the risk. Hence we explicitly use the sets of outputs (either directly as probabilities or converted to losses) as the basis for conducting different MIAs.

\begin{itemize}
    \item Note that without loss of generality we assume that an attack creates an MIA regression model $A: M(\mathcal{D}) \rightarrow \mathcal{R}$. Given a  target model output $x' = M(x), x \in \mathcal{D}$, we condition the outputs so that $A(x')$: represents the estimated probability that $x \in \mathcal{D}_{\text{train}}$.

\item These $N$ membership estimates can then be sorted into decreasing order and used to produce a ROC curve, which via interpolation can be probed to produce estimates of the true positive rate (TPR) for different thresholds of false positive rate (FPR).

    \item A specific MIA classifier is created by applying a threshold to the MIA regression model's predictions, above which the label `\textit{member}' is output. We note that the likely accuracy of different MIA classifiers will vary since the difficulty of inferring membership will vary between records, as reported elsewhere~\cite{Carlini:2022, Zarifzadeh:2024}. Therefore the estimation procedure described below will need to be repeated for different thresholds.
\end{itemize}

\subsubsection{From True Positive Rates to Attack Vulnerability}

Following Carlini et~al.~\cite{Carlini:2022} it has become standard practice to compare different MIAs based on values for the TPR at different low FPR regimes. We will denote these TPR@$z$, where typically the FPR $z \in \{0, 0.001, 0.01\}$.

However, from the perspective of an SDC output checker tasked with making a decision on whether to approve the release of the trained model $M$, what would be more helpful is to quantify the likelihood of observing different numerical MIA classifier results (TPR@$z$) by chance.
We assess this via a null hypothesis:

\begin{itemize}
    \item $H_0(z)$: The classifier produced by choosing a threshold $t_z$ such that $\text{FPR}=z$ is making uniform random selections (without replacement) from $\mathcal{D}$ when constructing $\mathcal{X}$ the set of records it predicts to be members: $\mathcal{X}\subset \mathcal{D}\mid \forall  x \in \mathcal{X}, A(x)>t_z$.
\end{itemize} 

To assess the likelihood of this null hypothesis $H_0(z)$ we proceed by first deriving an expression for the number of records $n=|\mathcal{X}|$ the MIA classifier will predict as `members' based on the threshold $t_z$ and on the number of true positives it predicts, TPR@$z$.

Let $\text{TP}_z, \text{FP}_z$ be the number of true/false positive records in $\mathcal{X}$, and assume that when the (floating point) true/false positive rates are interpolated from empirical data we take the floor of those values to map onto specific records included in $\mathcal{X}$. By definition:

\begin{eqnarray}
    n =&\text{TP}_z + \text{FP}_z \nonumber\\
    = & \lfloor(\text{TPR@}z \cdot N_{\text{train}}) + \lfloor (z \cdot N_{\text{test}}) \label{eqn:line-1}
\end{eqnarray}

We then apply a one-tail test for $H_0(z)$, assessing the over-representation of true positives in $\mathcal{X}$, using a hypergeometric distribution to calculate the probability of observing $\text{TP}_z$ or more true positives when using a process that makes selections without replacement from $\mathcal{D}$ and stops as soon as $\text{FP}_z$ incorrect `member' predictions have been made. The null hypothesis $H_0(z)$ is rejected at confidence level $\alpha$ if $\alpha > P(\text{TP}_z^+ | H_0, z)$, where

\begin{eqnarray}
    P(\text{TP}_z^+ | H_0, z) &=& 1 - \sum_{i=0}^{i<\text{TP}_z} P(i|\mathcal{D},n) \\
    &=& 1 - \sum_{i=0}^{i<\text{TP}_z} \frac{ C(N_{\text{train}},i)\; C(N_{\text{test}},\text{FP}_z)}{C(N,(i+\text{FP}_z))} \label{step3} \\
    &=& 1 - \sum_{i=0}^{i<\text{TP}_z} \frac{\frac{N_{\text{train}}!}{i!(N_{\text{train}}-i)!}\frac{N_{\text{test}}!}{\text{FP}_z! (N_{\text{test}}-\text{FP}_z)!}}{\frac{N!}{(i+\text{FP}_z)! (N - i - \text{FP}_z)!}} 
\end{eqnarray}

\noindent where Equation~\ref{step3} recognises that it is $\text{FP}_z$ that is fixed rather than $n$ when applying the hypergeometric distribution, and $C(a,b)= \frac{a!}{b!(a-b)!}$ is the number of ways of selecting $b$ items from a total of $a$ without replacement. 

Thus after some rearrangement we assert that with $(1-\alpha) \times 100$\% confidence, a model $M$ is LiRA vulnerable at a FPR of $z$ if:

\begin{equation}
    1 - \alpha < \frac{N_{\text{train}}!N_{\text{test}}!}{N!\text{FP}_z!(N_{\text{test}}-\text{FP}_z)!}\sum_{i=0}^{i<\text{TP}_z} \frac{(i+\text{FP}_z)! (N-i-\text{FP}_z)!}{i!(N_{\text{train}}-i)}\label{vulnerableprob}
\end{equation}\label{eq:vulerability}

\subsection{Target Models}

The target models assessed here are Random Forest and Decision Tree classifiers from the widely used open source Python \texttt{scikit-learn}~\cite{Pedregosa:2011} package, and the \texttt{XGBoost} classifier~\cite{Chen:2016}. The hyperparameter space is sampled with a range of values for the most commonly used hyperparameters; all others are set to their implementation defaults.

The sets of values used to define all the possible combinations of hyperparameter settings for the Decision Tree classifier, Random forest classifier and XGBoost are given in Table~\ref{table:parameters_combined}.

\input{table-params_combined.tex}
 
Tree-based models have been shown to provide state-of-the-art performance for tabular data classification tasks~\cite{Schwartz-Ziv:2022, Chen:2016} and therefore a selection of tabular datasets has been chosen here to provide a range of different landscape complexities for target model fitting. Binary classification problems are used here to aid comparison; increasing class number is known to increase the vulnerability to MIA~\cite{Shokri:2017}. Each dataset is split into three partitions, stratified by target label. The first subset is used for target model fitting, and the second is used for assessing target model accuracy. The LiRA attack then uses a combination of both the known training set and the third (unseen) subset to evaluate the MIA success.

\subsection{Datasets}

The mimic2-iaccd dataset was created for the purpose of investigating the effectiveness of indwelling arterial catheters in hemodynamically stable patients with respiratory failure for mortality outcomes. The dataset is derived from MIMIC-II~\cite{Raffa:2016}, the publicly accessible critical care database. It contains summary clinical data and outcomes for 1776 patients. The following repetitive and uninformative features were removed: `service unit', `day icu intime', `hosp exp flg', `icu exp flg', `day 28 flg' and `sepsis flg'. The target label selected was `censor flg'.

The in-hospital-mortality dataset was created to characterise the predictors of in-hospital mortality for intensive care unit admitted patients. Data were extracted from the MIMIC-III~\cite{Zhou:2021} database from 1177 heart failure patients. Features `ID' and `group' were removed and the target label selected was `outcome'. 
 
The Indian liver dataset~\cite{Ramana:2022}, contains 416 liver patient records and 167 non liver patient records. The dataset was collected from north east of Andhra Pradesh, India. This dataset contains 441 male patient records and 142 female patient records. The feature `gender' was transformed to numerical values and the target label selected was ‘Selector’, which identifies whether patients have liver disease or not. 
 
The synth-ae dataset~\cite{SyntheticData} was created by a Bayesian statistical model of de-identified patient information collected in the English National Health Service from 2014 to 2018. The following features were removed: `AE Arrivea Date', `AE Arrive HourOfDay', `Admission Method', `ICD10 Chapter Code', `Treatment Function Code', `Length Of Stay Days', `ProvID'. Any patients with missing data were also removed. Non-numerical features were transformed to numerical. Only the first 5000 rows were kept to generate the models. The target label selected was `Admitted flag'.

The sick and mammography datasets are open datasets available from OpenML\footnote{\url{https://openml.org}} with ID 41946 and 310, respectively.

\subsection{Experiments with Risk Correlation}

It is here hypothesised that while the absolute risk cannot be predicted for a given hyperparameter combination because it depends on characteristics of the dataset, the relative risk may generalise across datasets.

Since many different risk metrics have been proposed, it is first necessary to (i) select a single representative measure to analyse the relative risk; and (ii) establish that this metric is likely to generalise across different datasets. Therefore, for each target model algorithm, pair-wise rank correlation statistics are computed to compare different MIA and target model metrics across a range of hyperparameter combinations for each dataset. Based on these results a single representative measure is selected. Then, to test whether the disclosure risk of hyperparameter combinations are likely to generalise across datasets using this metric, the results for each dataset are sorted by order of hyperparameter combination and Kendall rank correlation $\tau$ coefficients are computed to perform pair-wise comparisons.

The hypothesis is then tested by assessing the performance of models trained to predict whether a given hyperparameter combination is relatively high or low risk as a binary classification task. That is, for each target model type, a Decision Tree classifier is used to learn a simple human-interpretable ruleset that predicts whether a hyperparameter combination is relatively high or low risk. Risk is here defined as high if the previously chosen metric is in the top 20\% of all hyperparameter combinations. While this is a somewhat arbitrary threshold, the aim is to prove that a simple set of rules can be identified that predict whether a hyperparameter combination poses unnecessarily high risk.

Each of these trees is thus fitted using the hyperparameter values and corresponding (discretised) MIA scores of a single dataset (mimic2-iaccd) and the extracted rules are then evaluated on the remaining datasets to test generalisation performance. In order to extract rules that are easy to interpret, the trees are restricted to a maximum depth of 5; using a simple grid search for the other hyperparameters. The best performance was found with a minimum number of samples per split of 2, a minimum number of samples per leaf of 2, the entropy criterion, and a balanced class weight.

\subsection{Experiments with Structural Metrics}%
\label{sec:method_structural}

For each of the combinations of algorithm-dataset-hyperparameter we analysed the model produced and its predictions for the training set, and used these to capture the structural metrics described in Section~\ref{subsec:struct_measures}. 

Note that different to the \textit{ante-hoc} `unnecessary risk' metric explored above, this is a \textit{post-hoc} analysis, since the minima (e.g., samples per leaf) and maxima (e.g., depth of a tree) defined via an algorithm's hyperparameters will be lower/upper bounds that may not be reached if the data does not merit it.

We then join these with the various metrics captured from the LiRA attacks and use this combined data to examine two complementary hypotheses:

\begin{itemize}
    \item Hypothesis 1: Structural risks are \textbf{necessary} conditions for model vulnerability. In other words, if a given structural metric is 0 then there will be no MIA risk. In the following analysis we insist on 100\% support for hypothesis 1 because this is a statement about \textbf{safety}.

    \item Hypothesis 2: The presence of structural risks are \textbf{sufficient} conditions for model vulnerability. In other words, if (the product of a combination of) structural metric(s) is 1 there will be non-zero LiRA risk. We insist on high, but not 100\% support for hypothesis 2 because this is a statement about \textbf{risk}.
\end{itemize}
