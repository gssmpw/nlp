\section{Results}%
\label{sec:results}

\subsection{Results for Risk Correlation}%

\subsubsection{Risk Correlation Across Metrics}

Table~\ref{table:kendall_mimic} shows the Kendall $\tau$ rank correlation statistics for each metric with pair-wise comparisons for the XGBoost classifier averaged over all datasets. Strong correlations between all of the MIA metrics can be seen. Target model generalisation error also positively correlates with risk, however target model test accuracy does not, indicating that it is strongly dataset dependant. These correlations are also seen on the other datasets with both Random Forest and Decision Tree classifiers (not shown). Since these metrics are highly correlated, the MIA AUC is used for simplicity in the remainder of the analysis. Regardless of the chosen metric, the following results should hold.

\input{table-kendall_mimic.tex}

\subsubsection{Risk Correlation Across Datasets}

Table~\ref{table:kendall_combined} shows the correlation of risk across different datasets. That is, the Kendall $\tau$ rank correlation statistics for dataset pair-wise comparisons of the MIA AUC for each target model hyperparameter combination. As can be seen, there is a strong correlation of risk with respect to hyperparameter combination across the different datasets for each of the target models. For Decision Tree models the mean is 0.64 (min=0.45, max=0.81); for Random Forest models the mean is 0.72 (min=0.51, max=0.88); and for XGBoost models the mean is 0.67 (min=0.48, max=0.84). This suggests that it may be possible to train a classification model that identifies the relative risk of a hyperparameter combination for most datasets.

\input{table-kendall_combined.tex}

\subsubsection{Risk Prediction Across Datasets}

Table~\ref{table:accuracy_combined} shows the precision, recall, and accuracy of the simple rulesets at predicting whether a given target model hyperparameter combination is in the top 20\% highest MIA AUC (high risk) or in the bottom 80\% (low risk). Note that the risk prediction rules for each target model were derived by training a Decision Tree classifier on the results from the mimic2-iaccd dataset then distilling the tree into a set of rules. The mean accuracy on the unseen datasets is 91.4\% (min=89.1\%, max=93.9\%) for Decision Tree target models; 95.2\% (min=92.9\%, max=96.2\%) for Random Forest; and 94.1\% (min=90.8\%, max=97.2\%) for XGBoost target models, showing that the very simple ruleset is able to accurately differentiate the hyperparameter combinations with the highest relative risk on each dataset.

\input{table-accuracy_combined.tex}

Scatter plots showing the relationship between target model accuracy (test set AUC) and privacy risk (MIA AUC) for different hyperparameter combinations of the Decision Tree classifier can be seen in Figure~\ref{fig:scatter_dt}. Similar plots are shown for the Random Forest classifier target model in Figure~\ref{fig:scatter_rf} and for the XGBoost classifier target model in Figure~\ref{fig:scatter_xgboost}. The high accuracy of the risk rulesets is illustrated by colouring the points red where the rules predict a hyperparameter combination to have an MIA AUC in the top 20\% highest risk. Overall, it can clearly be seen that removing the high risk combinations would eliminate a large portion of significantly vulnerable models.

Interestingly, the plots show that removing the hyperparameter combinations predicted as high risk results in little to no loss of maximum achievable target AUC\@ for most datasets. In the few cases where there is a hyperparameter combination classified as high risk that results in a higher target AUC, the difference is small, yet the reduction in MIA AUC is much larger. For example, the Decision Tree target model hyperparameter combinations with the highest accuracy are categorised as low risk for every dataset except the hospital dataset. Moreover, on the hospital dataset, the loss of accuracy (AUC) that would be experienced by removing the high risk combinations is only 0.0011. As another example, removing the high risk hyperparameters for Random Forest target models on the sick dataset, would result in a reduction in the maximum achievable accuracy (AUC) of only 0.005, but a reduction in MIA AUC of 0.13. 

\input{figure-scatter_dt.tex}
\input{figure-scatter_rf.tex}
\input{figure-scatter_xg.tex}

Appendix~\ref{sec:rules} shows the extracted and compressed rules for identifying high risk hyperparameter combinations in tree-based models. As expected, model complexity is a significant risk factor with larger values for \texttt{max\_depth} and \texttt{n\_estimators} being key features along with smaller values of \texttt{min\_samples\_leaf} and \texttt{min\_child\_weight}. A deeper tree with fewer samples per node allows the model to learn more intricate relationships in the data, potentially overfitting. Similarly, a larger number of trees in an ensemble can lead to overfitting if not carefully managed.

The greediness of the search algorithm also significantly contributes to risk with \texttt{splitter} and \texttt{bootstrap} controlling how the trees are built. The \texttt{splitter} parameter determines the strategy used to choose the best split point for each node in a tree and a greedy approach evaluates all possible splits and chooses the one that maximises the chosen criterion. This can lead to trees that are highly sensitive to the specific training data and may not generalise well. Similarly, \texttt{bootstrap} controls whether the individual trees are trained on random subsets of the training data, which can introduce diversity into the ensemble, reducing overfitting. If bootstrapping is used in a way that reduces diversity, the trees may become too similar and prone to the same overfitting tendencies, again increasing risk.

A combination of higher potential model complexity and greedy search algorithms are therefore much more likely to lead to models that are highly susceptible to overfitting and thus pose a higher disclosure risk in practice.

\subsection{Results for Structural Risks}%
\label{sec:structural-results}

To examine support for the two different hypotheses stated in Section~\ref{sec:method_structural} we aggregated the results from LiRA and structural attacks on all (73610) combinations of dataset-algorithm-hyperparameters.

Figure~\ref{fig:violin-dataset-metric} shows the histograms of the observed values of different MIA metrics (columns) for different datasets (rows) separated into models with combined structural risk 0 (low risk; green bars) and 1 (high risk; red bars). 

\input{figure-violin.tex}

As can be seen:
\begin{itemize}
    \item The risk metrics have different distributions; higher where there is an identified combined structural risk (red).
    \item Except for the TPR@0.001 metric and the two biggest datasets, there are always cases with no combined structural risk but non-zero MIA risk (green bars).
    \item In contrast, only in a tiny number of cases is there evidence of models having 0 value for MIA risk metrics but 1 for the combined structural risk metric (red bars).
\end{itemize}

Hypothesis 1 proposed that when there was no structural risk, the LiRA attack risk metrics would also be near zero. The second observation above is illustrated in Figure~\ref{fig:heatmaps} (left). This shows conclusive evidence that this hypothesis can be rejected. In other words, \textbf{structural risk is not necessary} for vulnerability to membership inference.

Hypothesis 2 took the alternative position that the presence of structural risk was predictive of a model being vulnerable to a LiRA attack. As Figure~\ref{fig:heatmaps} (right) shows there are only a handful of such cases that do not also display MIA vulnerability. 

\input{figure-heatmaps.tex}

While these counterexamples cannot be ignored, from the perspective of an analyst making pragmatic decisions about whether a trained model poses a privacy risk, there is a clear message. The combined structural measure presents \textbf{sufficient but not necessary} evidence of MIA vulnerability, without the need to run computationally expensive attacks.

\subsubsection{Analysis of Where LiRA Attack Metrics are Statistically Significant}

Section~\ref{sec:tpr_ssd} describes a procedure for calculating whether the TPR at a given FPR is in fact statistically significant. For the FDIF metric we also have the equivalent calculation of probability that the result is observed by chance. Figure~\ref{fig:ecdf_metrics_ssd} shows the results (95\% confidence level) of using this analysis to mask the results (i.e., metric values are set to 0 if the null hypothesis cannot be rejected) and then plotting the cumulative distribution of observed metric values. From these plots we can summarise that:

\input{figure-ecdf.tex}

\begin{itemize}
    \item In the overwhelming majority of cases, the numbers of true positive cases identified by LiRA for a fixed (low) proportion of false positives are not high enough to reject the null hypothesis that the attack is just making random guesses. Of the 73600 different algorithm-dataset-hyperparameter combinations tested, only 451 were statistically significant for TPR@0.01 and 97 for TPR@0.001. \\
In Figure~\ref{fig:ecdf_metrics_ssd} this is evidenced in the two left hand plots. The distributions of observed $\text{TPR@}z$ metrics for raw data (ignoring statistical significance) show a clear difference between whether structural risk is present (plain red line) or not (plain green). However that difference disappears for the significant results (lines with markers).
    \item By way of contrast, the FDIF metrics are almost always significant, i.e., the pairs of plain and cross-marked lines are close. The main difference is for $\text{FDIF}_{0.001}$ when only the top and bottom 0.1\% of records (ranked by attack confidence) are considered. Even in this case the difference between the extrema is significant in more than 55\% of cases where the structural metric is 0 and 90\% of cases where the structural risk metric is 1.

    \item For all cases, the cumulative frequency of cases where the structural metric is 1 (red line) grows more slowly than when it is 0. In other words, LiRA risk metrics are higher when the structural risk is present.
\end{itemize}
