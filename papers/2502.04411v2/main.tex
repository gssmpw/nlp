
%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% if you use cleveref..
% \usepackage[capitalize,noabbrev]{cleveref}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

 
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{graphicx}

\usepackage{subfigure}
\usepackage{subcaption}

% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}
\usepackage{import}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{color, colortbl}
\definecolor{greyC}{RGB}{180,180,180}
\definecolor{greyL}{RGB}{235,235,235}
\usepackage{footmisc}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage[noend]{algpseudocode}
\usepackage{bm}
\usepackage[flushleft]{threeparttable}
\usepackage[nospace]{cite}
\usepackage[]{hyperref} 
\usepackage{makecell}

\newcommand{\zhenheng}[2]{\textcolor{blue}{#1}} % stands for new content added by zhenheng
\newcommand{\xxxx}[2]{\textcolor{red}{#1}} % stands for new content added by xxxx
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother


% \bibliographystyle{abbrv,unsrt}
% \bibliographystyle{abbrv}
% \bibliographystyle{unsrt}
% \usepackage[sorting=none,firstinits=true]{biblatex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{enumitem}


% \usepackage{algcompatible}
\usepackage{hyperref}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{wrapfig}


% separate toc for maintext and appendix
\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{condition}[theorem]{Condition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]



\newcommand{\unishrinkFigure}{\vspace{-0.0cm}}
\newcommand{\unishrinkBlock}{\vspace{-0.0cm}}
\newcommand{\unishrinkSection}{\vspace{-1.0cm}}


% \makeatletter
% \newcommand{\rmnum}[1]{\romannumeral #1}
% \newcommand{\Rmnum}[1]{\expandafter@slowromancap\romannumeral #1@}
% \makeatother

% \uppercase\expandafter{\romannumeral20}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\input{macros}
\input{std_macros}
\input{math_commands.tex}


\definecolor{mygray}{gray}{.92}
\newcommand{\gray}{\cellcolor{mygray}}
\definecolor{baselinecolor}{rgb}{1, 1, 1}
\newcommand{\baseline}{\cellcolor{baselinecolor}}
\definecolor{ourmethodcolor}{rgb}{0.94, 0.97, 1}
\newcommand{\ours}{\cellcolor{ourmethodcolor}}
\newcommand{\revise}[1]{\textcolor{blue}{#1}}
\newcommand{\needrevise}[1]{\textcolor{red}{#1}}
% \newcommand{\noisy}[1]{\textcolor{gray}{#1}}
% \newcommand{\noisy}[1]{\textcolor[rgb]{0.67, 0.29, 0.27}{#1}}
\newcommand{\noisy}[1]{\textcolor[rgb]{0.70, 0.29, 0.27}{#1}}
\newcommand{\up}[1]{\textcolor[rgb]{0.8, 0, 0}{#1}}
\newcommand{\down}[1]{\textcolor[rgb]{0, 0.6, 0}{#1}}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing}

\begin{document}

\twocolumn[
\icmltitle{Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts \\ and Uncertainty Based Routing}
% \icmltitle{Mediator: Memory-efficient Layer-wise LLM Merging with Sparsified Uncertainty Based Routing}

% \icmltitle{Mediator: Memory-efficient Layer-wise LLM Merging with Sparsified Task-level Uncertainty Based Routing}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kunfeng Lai}{equal,hkustgz}
\icmlauthor{Zhenheng Tang}{equal,hkust}
\icmlauthor{Xinglin Pan}{hkustgz}
\icmlauthor{Peijie Dong}{hkustgz}
\icmlauthor{Xiang Liu}{hkustgz}
\icmlauthor{Haolan Chen}{tecent}
\icmlauthor{Li Shen}{sysu}
%\icmlauthor{}{sch}
\icmlauthor{Bo Li}{hkust}
\icmlauthor{Xiaowen Chu}{hkustgz,hkust}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{hkustgz}{The Hong Kong University of Science and Technology (Guangzhou)}
\icmlaffiliation{hkust}{The Hong Kong University of Science and Technology}
\icmlaffiliation{tecent}{Platform and Content Group, Tencent}
\icmlaffiliation{sysu}{Sun Yat-sen University}
\icmlcorrespondingauthor{Xiaowen Chu}{xwchu@hkust-gz.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% With the development of large language models (LLMs) and there are rapidly increasing multiple downstream-based finetuned models, how to effectively merge these models to improve the performance of the merged model is a critical issue. Some works propose to average model weights to merge models, but this kind of method neglects the parameter conflicts between the finetuned models, thus leading to the performance degradation of the merged model. While some works propose to directly avoid the parameter conflicts by saving the finetuned models and conduct model selection during the inference stage, this kind of method requires too much storage costs. Considering that the LLMs consumes too much storage resources, this kind of method further amplifies the storage costs and makes it impractical in the real-world deployment. In this work, we find that different model layers have different levels of parameter conflicts. Based on this observation, we propose adaptive layer-wise model averaging and expert routing to merge the models. We select the layers with less parameter conflicts to conduct model averaging. While for large parameter conflicts, we propose to use expert routing to merge the models. To further reduce the storage costs, inspired by the sparsity of the task arithmetics, we propose to decouple multiple finetuned experts into a dense expert and multiple sparse experts. During inference, according to the test sample, we select the sparse expert together with the dense expert to complete inference. The proposed method jointly consider the common and unique knowledge of the finetuned models, thus improving the overall performance of the merged model. And the storage costs are significantly reduced by the layer-wise and sparse expert routing.


\begin{abstract}
Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. 
In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we  average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts.
To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. 
We conduct extensive experiments on both LLaMA  and Qwen  with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.



% Model Merging is porposed to aggregate different Large Language Models (LLMs) finetuned on different datasets into a stronger unified one. Existing works overlook parameter conflicts between fine-tuned models, leading to performance degradation, or maintain individual models and perform selection during inference impose excessive storage and compute costs. 
% In this work, we observe that different model layers exhibit varying levels of parameter conflicts. Building on this insight, we propose to average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts.
% To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. During inference, we exploit Bayesian inference to dynamically select the appropriate sparse experts alongside the dense expert based on the input data. Our proposed method integrates both common and unique knowledge from fine-tuned models, thereby improving the overall performance of the merged model while significantly reducing storage costs through layer-wise and sparse expert routing.
\end{abstract}


\section{Introduction}
\label{sec:Introduction}
Finetuning Large Language Models (LLMs) enables them to adapt to downstream applications including sentiment analysis~\citep{sun2023sentiment}, text summarization~\citep{fang2024multillmtextsummarization}, mathematical reasoning~\citep{ruis2024proceduralknowledgepretrainingdrives}, code writing~\citep{jiang2024surveylargelanguagemodels},  roleplay chatting \citep{chen2025rolplay} so on.  Open-source platforms such as Huggingface~\citep{wolf2019huggingface} and torchvision~\citep{marcel2010torchvision} facilitate access to a diverse array of highly trained expert models with varying capabilities. Considering the computational resources are scarce and implementing green computing~\citep{samsi2023wordswattsbenchmarkingenergy,you2022zeusunderstandingoptimizinggpu,stojkovic2024greenerllmsbringingenergyefficiency,bai2024beyond}, the community is increasingly interested in how to merge these models to create a superior LLM that retains the strengths of finetuned ones without retraining~\citep{yang2024model,lu2024twin,Du:2024aa,Yadav:2023aa}. 


% The machine learning community has witnessed an exponential increase in pre-trained and fine-tuned model checkpoints in recent years.

% making the retraining these models on all datasets impractical~\citep{bai2024beyond}. Merging fine-tuned models presents a viable strategy for enhancing performance while conserving resources~\citep{yang2024model,lu2024twin,Du:2024aa,Yadav:2023aa}.

% Given the proliferation of multiple finetuned models, the community is increasingly interested in how to merge these models to create a superior LLM that retains the strengths of each finetuned variant without the need for retraining. 




\begin{figure}[th]
    \setlength{\abovedisplayskip}{-2pt}
    \subfigbottomskip=-1pt
    \subfigcapskip=1pt
    \setlength{\abovecaptionskip}{-2pt}
    \centering
    \includegraphics[width=0.8\linewidth]{motivation.pdf}
    \caption{Knowledge conflict across finetuned LLMs and math and code dataset. Deeper color means larger parameter conflicts. And it is difficult for the linear averaged model to achieve low loss of both tasks.}
    \label{fig:knowledge-conflict}
\vspace{-0.1cm}
\end{figure}
\vspace{-0.1cm}


% Current research on model merging primarily investigates integrating multiple finetuned models to harness both shared and unique knowledge, thereby generating a unified model that can perform well across various downstream tasks~\citep{yang2024model,jhunjhunwala2023towards,qu2022generalized,FisherMerging_NeurIPS2022,utans1996weight}. 

One predominant merging strategy is model averaging~\citep{yang2024model,FisherMerging_NeurIPS2022,thennal2024fisher,DARE_Arxiv2023}, which computes weighted averages of parameters to synthesize collective knowledge~\citep{FisherMerging_NeurIPS2022,Yadav:2023aa}. However, model averaging faces challenges from parameter conflicts arising from diverse finetuning tasks, leading to performance degradation as shown in Figure~\ref{fig:knowledge-conflict}. Another direction is model routing~\citep{lu2024twin,muqeeth2024soft,Yang:2023aa,Du:2024aa,Lu:2024aa,He:2024aa,Wei:2024aa,Chen:2024aa}, which aggregates models and performs model selection during inference. This method avoids parameter conflicts but incurs significant computing and storage (system) costs due to maintaining all finetuned models. This motivates us to rethink the following questions:

\emph{How to better merge common and unique knowledge from various finetuned models while simultaneously avoiding parameter conflicts and minimizing system costs?}

% We employ cosine and Euclidean distance metrics to analyze the parameter relationships between different models.

To answer this question, we firstly quantify the conflicts between finetuned LLMs. We employ sign consistency between different task arithmetics that are difference between the finetuned LLM and the original LLM to measure the conflicts. We find that the front and last layers tend to exhibit the highest levels of conflict, suggesting that these layers are particularly sensitive to averaging. In contrast, the central layers demonstrate comparatively lower levels of conflict, indicating that they retain more common knowledge.

% This observation aligns with findings from the LISA optimizer, which suggests that parameters in the central layers undergo less optimization, potentially leading to a more generalized knowledge representation. By recognizing these differences in parameter conflict across layers, we can develop more effective merging strategies that leverage the strengths of each layer while mitigating the adverse effects of conflicting parameters. This layered approach not only preserves unique task knowledge but also enhances the overall performance of the merged model, all while addressing the critical issue of storage efficiency. **[Introduction]**

Then, we introduce Mediator as an adaptive model merging framework to enhance LLM merging with little storage and computation costs. Inspired by the varying degrees of layer-wise parameter conflicts, we propose adaptive merging that averages layers with lower conflict levels, thereby capturing the common knowledge~\citep{Yadav:2023aa,he2024localize} shared among LLMs while minimizing conflicts~\citep{Yadav:2023aa}. Concurrently, layers with significant conflicts are regarded as experts to be routed during inference, preserving unique task-specific knowledge without dilution~\citep{Yadav:2023aa,he2024localize}. 

% This dual methodology enhances the merged model's overall performance and retains the distinct capabilities of each finetuned model, while also reducing storage costs by eliminating the need to maintain multiple separate models.

% The experts in the merged models refer to the layers of different finetuned models. 


While direct compression of finetuned LLMs results in significant information loss~\citep{dong2024pruner,sun2024a}, we leverage both 
layer-wise model merging and the high sparsity of task 
arithmetics~\citep{Yadav:2023aa} to decompose models into base and task-specific components~\citep{TaskArithmetic_ICLR2023,he2024localize,Yang:2023aa,Tang:2024aa}. 
By integrating these two techniques, our approach reduces storage from 50\% to 7\% with minimal accuracy loss while preserving layer-specific knowledge.


Observing that LLMs are finetuned on the complete sentences of their downstream tasks instead of the splited sub-sequences, to better preserve task-specific knowledge and improve overall model performance, we propose task-level expert routing instead of token-level routing~\citep{Lepikhin:2020aa,Sukhbaatar:2024aa,Zhou:2022aa,Jiang:2024aa}. With these designs, our merged LLM achieves high efficiency with minimal performance degradation (0.06\% $\sim$ 0.3\%). Our evaluations show that we can effectively run a model comparable to a 7B × 4 LLM ensemble on a single RTX 4090 GPU, making high-performance LLM more accessible in resource-constrained environments(Appendix~\ref{appx:system-performance-consumer}).

% This approach not only enhances model effectiveness but also reduces storage and computing costs. 


% Our design choice is motivated by the observation that during finetuning phases, experts are trained on complete sentences of their downstream tasks and thus cannot access token-level sequence distributions from other tasks, making token-level routing unnecessary.

% Through these two techniques, our unified model achieves high efficiency with minimal performance 
% degradation (0.06\% ~ 0.3\%). Our evaluations show that we can effectively run a model comparable 
% to a 7B × 4 MoE architecture on a single NVIDIA RTX 4090 GPU (24GB VRAM), making high-performance 
% language models more accessible in resource-constrained environments.

Considering the out-of-distribution (OOD) samples, we select and merge appropriate experts based on the task uncertainty of the input data. Thus, the unified model can select appropriate experts based on the characteristics of the input data. For example, the input may incorporate both codes and mathematical reasoning, then the unified model can select the experts that are trained on both codes and mathematical reasoning. 

We finetune pretrained LLMs with Chain-of-thoughts~\citep{CoT} enhanced datasets, showing that the model merge of Mediator can successfully preserve the reasoning ability~\citep{zelikman2022star,kojima2022large,guo2025deepseek}. As far as we know, we are the first to conduct cutting-edge LLM merging based on the finetuning with CoT enhanced downstream tasks. Our main contributions can be summarized as follows:

\begin{itemize}[leftmargin=*]
\item \noindent We investigate and demonstrate that different layers of fine-tuned models exhibit varying levels of parameter conflicts (Section~\ref{sec:Understanding-Conflict}). Then we propose Mediator, an adaptive layer-wise model merging approach to average layers with minimal conflicts and use task-level expert routing for layers with significant conflicts (Section~\ref{sec:AdaptiveMerging}).

\item \noindent We propose a method in Mediator to decouple fine-tuned experts into one dense expert and several sparse experts (Section~\ref{sec:expert-decomposition}), achieving high compression ratio while maintaining accuracy. Our approach enables dynamic expert selection based on task uncertainty (Section~\ref{sec:expert-routing}), effectively handling OOD data.


\item \noindent We conduct experiments based on the modern LLMs including LLaMA and Qwen with CoT enhanced finetuning and the real-world cutting-edge LLM evaluation tasks. Results show that our method achieves significant performance improvements and less system cost compared to existing methods. (Section~\ref{sec:Experiments}).
\end{itemize}


\vspace{-0.1cm}
\section{Preliminary and Related Works}
\vspace{-0.1cm}
\label{sec:Preliminary}
% In this section, we will introduce the basic concepts of language modeling~\citep{} and LLM finetuning~\citep{}, as well as the typical model merging methods including model averaging~\citep{} and model routing~\citep{}.

% \subsection{Language Modeling}\label{sec:LanguageModel}
\subsection{Language Modeling and LLM Finetuning}\label{sec:LanguageModel}

\textbf{Task Data Distribution.}
Given a set of different downstream tasks $\Tcal $, based on the sampling task $\tau \in \Tcal$, the pretraining document (data sample) is a sequence $x_{1:T}$ of tokens with the maximum length $T$ generated from a distribution $p_{\tau} = p(x_{1:T} \vert \tau)  = p(\obs_1, \dots, \obs_T \vert \tau)$~\citep{xie2022an,wies2023learnability,hahn2023theory,li2024language}. And we define the pretraining data is sampled from $p(x|\Tcal^\star) = \int_{\tau^\star \in \Tcal^\star} p(\obs_1, \dots, \obs_T \vert \tau)p(\tau^\star)d \tau^\star$. Each token $\obs$ is sampled from a vocabulary $\obsset$. And both ($\Tcal$ and $\Tcal^\star$ belong to a large task family $\Omega$, i.e. $\Tcal,\Tcal^\star \subset \Omega$.

% and  $\Tcal^\star \subset \Omega$.

% \subset \Omega

% Thus, the complete data distribution is:
% \begin{align}
%     p(\obs_1, \dots, \obs_T) = \int_{\tau \in \Tcal}  p_{\tau}(\obs_1, \dots, \obs_T \vert \tau) p(\tau) d\tau.
% \end{align}

\textbf{Language Modeling.}
Current LLMs~\citep{gpt3_2020,touvron2023llama2,xie2022an} usually utilize the next word prediction as the language modelling, which predicts the next token $x_t$ given the previous tokens $x_{1:t-1}$ for all $t=1,\dots, T$. Formally, a LLM parameterized by $\theta$ is a distribution $f_{\theta}(x_t \vert x_{1:t-1})$. And it is pretrained on a huge corpus sampled from the pretraining distribution $p(x|\Tcal^\star)$~\citep{xie2022an}. 


% \subsection{LLM Finetuning}\label{sec:LLM-Finetuning}
% \textbf{Downstream Tasks and Backbone LLM.} Current model merging works~\citep{yang2024model,Tang:2023aa} mainly consider finetuning a pretrained LLM on different downstream tasks $\tau \in \Tcal$. Then, this backbone LLM is finetuned on the task-specific data distribution $p_{\tau}$ for each task $\tau \in \Tcal$.

% we assume there is a backbone LLM shared with the same architecture and the same initial pretraied parameters that will be finetuned, which is a common practice in the model merging~\citep{} called \textit{homogenous model merging}. Then, this backbone LLM is finetuned on the task-specific data distribution $p_{\tau}$ for each task $\tau \in \Tcal$.


\textbf{Finetuning LLM.} Normally, for each downstream task $\tau \in \Tcal$, finetuning LLM is to minimize the cross-entropy loss function as below:
\begin{align}
    \begin{small}
    L_\text{CE}(\theta, \tau)= -\sum_{t=1}^T \mathbb{E} [p_{\tau}(x_t|x_{1:t-1})\cdot\log f_{\theta}(x_t|x_{1:t-1})]. \notag
    \end{small}
\end{align}
After finetuning, the model parameters $\theta$ are updated to $\theta_{\tau}$. 

% Note that the finetuned model parameters $\theta_{\tau}$ are \textit{task-specific}.

% \begin{definition}[Task-specific Finetuning]
%     The finetuned model parameters $\theta_{\tau}$ are task-specific after finetuning on task $\tau$, when the parameter $\theta_{\tau}$ satisfies:
% \begin{align}
%     & L_\text{CE}(\theta_{\tau}, \tau) < L_\text{CE}(\theta, \tau), \label{eq:task-specific-with-original} \\
%     \text{and} \ \ & L_\text{CE}(\theta_{\tau}, \tau) < L_\text{CE}(\theta_{\hat{\tau}}, \tau), \label{eq:task-specific-with-other}
% \end{align}
% for any two different tasks $\tau, \hat{\tau} \in \Tcal$.
% \end{definition}
% \begin{remark}
%     Equation~\ref{eq:task-specific-with-original} shows that the finetuning process helps to reduce the loss of the task $\tau$ for the finetuned model. Equation~\ref{eq:task-specific-with-other} shows that the finetuning on another task $\hat{\tau}$ cannot achieve a lower loss of task $\tau$ than the finetuned model on task $\tau$. These two conditions are widely observed in existing literature including model merging and multi-task learning~\citep{Du:2024aa,yang2024model}. We will show later that such a task-specific finetuning is a normal phenomenon in experiment section~\ref{sec:Understanding-Conflict}.
% \end{remark}








\vspace{-0.1cm}
\subsection{Model Merging}\label{sec:Model Merging}
% Building upon the finetuned LLMs, we formally define model merging as the process of integrating multiple fine-tuned large language models (LLMs) into a single unified model $f_{\phi}$ parameterized by $\phi$. 

% Formally, let $\{\theta_1, \theta_2, \dots, \theta_{n_{\tau}}\}$ represent the parameter sets of $n_{\tau}$ fine-tuned models, each optimized for distinct downstream tasks. Model merging aims to derive a unified parameter set $\phi$ that encapsulates the knowledge from all individual models:
% \begin{align}
%     \phi = \mathcal{M}(\theta_1, \theta_2, \dots, \theta_{n_{\tau}}),
% \end{align}
% where $\mathcal{M}$ denotes the merging function that combines the parameters in a manner that preserves task-specific competencies while enhancing overall performance. The goal of model merging is to find the optimal $\phi$ that achieves the low loss on all tasks $\Tcal$ as following

Given finetuned task-specific LLMs $\{\theta_1, \theta_2, \dots, \theta_{n_{\tau}}\}$ finetuned on task set $\Tcal$, where $n_{\tau}=|\Tcal|$, model merge aims to find a unified model parameterized by $\phi$ that can achieve the low loss on all tasks $\Tcal$ as following

% \begin{align}\label{eq:model-merging-goal}
%     L_\text{CE}(\phi, \Tcal)  \leq \sum_{\tau \in \Tcal} L_\text{CE}(\theta_{\tau}, \tau)  < L_\text{CE}(\theta, \Tcal).
% \end{align}
\begin{align}\label{eq:model-merging-goal}
    \min_{\phi}  L_\text{CE}(\phi, \Tcal) = \frac{1}{n_{\tau}} \sum_{\tau \in \Tcal} L_\text{CE}(\phi, \tau).
\end{align}
Different from training $\phi$ that can be optimized towards any direction, model merging aims to exploit combining $\{\theta, \theta_1, \theta_2, \dots, \theta_{n_{\tau}}\}$ to obtain the $\phi$. The current model merging methods include following two categories.

% in which we write $L_\text{CE}(\cdot, \Tcal) = \frac{1}{n_{\tau}} \sum_{\tau \in \Tcal} L_\text{CE}(\cdot, \tau)$ for simplicity. 


% The second inequality is easily satisfied by the finetuning process (refer to Equation~\ref{eq:task-specific-with-original}). However, the first inequality is challenging to satisfy, as different tasks $\tau \in \Tcal$ has different data distribution, which leads to different finetuned model parameters $\theta_{\tau}$ that have higher loss on other tasks $\Tcal \setminus \tau$. The simple averaging cannot guarantee their performance (refer to Equation~\ref{eq:task-specific-with-other}). In current literature and our experimental study, there is $\sum_{\tau \in \Tcal} L_\text{CE}(\theta_{\tau}, \tau) < L_\text{CE}(\phi, \Tcal)  < L_\text{CE}(\theta, \Tcal)$. 


% There are two common methodologies for model merging as follows.
% Averaging is a straightforward approach where the parameters of the fine-tuned models are averaged to form the merged model. Mathematically, this can be expressed as $\phi = \frac{1}{n_{\tau}} \sum_{\tau \in \Tcal} w_\tau \theta_{\tau}, in which $w_\tau$ is the averaging weight and $\sum_{\tau \in \Tcal} w_\tau = 1$.
% \begin{align}
%     \phi = \frac{1}{n_{\tau}} \sum_{\tau \in \Tcal} w_\tau \theta_{\tau},
% \end{align}

% Model averaging is a straightforward approach where the parameters of the fine-tuned models are averaged to form the merged model. Mathematically, this can be expressed as:
% \begin{align}
%     \phi = \frac{1}{n_{\tau}} \sum_{\tau \in \Tcal} \theta_{\tau}.
% \end{align}
% This method leverages the collective knowledge of all models, aiming to create a balanced representation that performs reasonably well across all tasks. However, model averaging often suffers from the performance gap between the merged model and individual fine-tuned models due to knowledge conflicts, where specific parameters optimized for particular tasks may interfere with each other when averaged. Advanced model averaging methods~\citep{} seek for some better parameter importance metrics as the averageing weights to avoid influcing the sensitive parameters~\citep{}, but the performance gap still exists.



\textbf{Model Averaging.} Averaging parameters to fuse the knowledge from different finetuned models is straightforward. Mathematically, averaged model is $\phi = \sum_{\tau \in \Tcal} w_\tau \theta_{\tau}$, in which $w_\tau$ is the averaging weight and $\sum_{\tau \in \Tcal} w_\tau = 1$. Considering different model parameters have different importance on downstream tasks~\citep{kirkpatrick2017overcoming,sun2024a,dong2024pruner}, assigning larger weights to more important parameters. Current methods usually utilize Taylor expansion~\citep{lee2018snip,FisherMerging_NeurIPS2022} to measure the importance of the parameters. However, the knowledge conflicts still exist for parameters that have high importance simultaneously.


% \textbf{Model routing.} The routing refers to aggregate models and introduces gating layers to select the most relevant models for each task like an Mixture of Experts (MoE) framework~\citep{}. Some layers are averaged and shared among all tasks, while some layers are task-specific and only used for the corresponding task. This approach typically employs a selection mechanism to activate the most relevant model based on the input task $\tau$:
% \begin{align}
%     \phi = \mathcal{S}(\tau; \theta_1, \theta_2, \dots, \theta_{n_{\tau}}).
% \end{align}
% By selectively utilizing specific models for corresponding tasks, this method preserves the unique strengths of each fine-tuned model. The extreme model routing is to select a complete finetuned model for each task, like the model routing~\citep{}. However, the routing introduces significant storage overhead, as all individual models must be maintained, and may result in the loss of shared knowledge that could enhance cross-task performance. Besides, current routing methods~\citep{} fail to understand which parameters capture the common knowledge and which parameters capture the task-specific knowledge~\citep{}.

\textbf{Model Routing.} To completely avoid the conflicts, another way is to collect models together and select the most relevant models for each task. This approach typically employs a selection mechanism to activate the most relevant model based on the input task $\tau$~\citep{Yang:2023aa} or sequence $x_{1:t}$ at $t$-th token like the Mixture of Experts (MoE)~\citep{tang2024moemerging}. Current methods propose different routing and re-training mechanisms to improve the performance~\citep{He:2024aa, Wei:2024aa,sukhbaatar2024branchtrainmix}. However, these methods fail to consider merging the parameters to find the common knowledge that can be shared across different tasks, and cause large memory and computational costs. Due to the limited space, we left detailed discussions about related works in Appendix~\ref{appx:more-related-works}).


\textbf{Out-of-distribution Data.} In real-world deployment, the test data $x$ may come from other distributions instead of the $p(x|\tau)_{\tau \in \Tcal}$. To this end, we need to consider how to handle OOD data $x$ within merging LLMs. In this work, we mainly consider two OOD cases and tackle them in Section~\ref{sec:Method}.


% do not consider the adaptivity between different layers.


% The routing refers to aggregate models and introduces gating layers to select the most relevant models for each task like an Mixture of Experts (MoE) framework~\citep{}. Some layers are averaged and shared among all tasks, while some layers are task-specific and only used for the corresponding task. This approach typically employs a selection mechanism to activate the most relevant model based on the input task $\tau$:
% \begin{align}
%     \phi = \mathcal{S}(\tau; \theta_1, \theta_2, \dots, \theta_{n_{\tau}}).
% \end{align}
% By selectively utilizing specific models for corresponding tasks, this method preserves the unique strengths of each fine-tuned model. The extreme model routing is to select a complete finetuned model for each task, like the model routing~\citep{}. However, the routing introduces significant storage overhead, as all individual models must be maintained, and may result in the loss of shared knowledge that could enhance cross-task performance. Besides, current routing methods~\citep{} fail to understand which parameters capture the common knowledge and which parameters capture the task-specific knowledge~\citep{}.


\vspace{-0.1cm}
\section{Understanding Conflict between LLMs}\label{sec:Understanding-Conflict}
\vspace{-0.1cm}

\begin{table}[ht]
    \centering
    \setlength{\abovedisplayskip}{-2pt}
    \subfigbottomskip=-1pt
    \subfigcapskip=1pt
    \setlength{\abovecaptionskip}{-2pt}
    \caption{Accuracy of finetuning Llama 3.2 3B.}
    \resizebox{0.9\columnwidth}{!}{
    \begin{tabular}{c|cccc}
    \toprule
    Model & GSM8K Math  & TriviaQA & H.Eval Code & All tasks \\
    \midrule
    $\theta$ (Pretrained) & 27.52 & 57.71 & 22.56 & 35.93 \\
    $\theta_1$ (Math SFT) & \textbf{46.47} & 54.59 & 25.00 & 42.02 \\
    $\theta_2$ (QA SFT) & 32.75 & \textbf{61.45} & 28.05 & 40.75 \\
    $\theta_3$  (Coding SFT) & 33.13 & 57.71 & \textbf{40.85} & 43.90 \\
    $\phi_{\text{AVG}}$ & 42.61 & 60.99 & 31.30 & 44.97 \\
    $\phi_{\text{SEL}}$ & \textbf{46.47} & \textbf{61.45} & \textbf{40.85} & \textbf{49.59} \\
    \bottomrule
\end{tabular}}
\label{tab:preliminary-experiments}
\end{table}
\vspace{-0.1cm}

\textbf{Preliminary Experiments.} We finetune Llama-3.2-3B on three datasets and evaluation with according tasks (details in Section~\ref{sec:Experiments}). Table~\ref{tab:preliminary-experiments} shows the performance of the merged model and the individual finetuned models. We use the $P(\theta, \tau)$ to represent the performance of the $\theta$ on $\tau$.

% which usually is negatively related to the $L_\text{CE}(\theta, \tau)$.,........

\textbf{Comparing Performance on All Tasks.} We write $P_{\text{ORI}} = P(\theta, \Tcal)$ as the performance of the original model $\theta$ on all tasks $\Tcal$, the $P_{\text{AVG}} = P(\phi_{\text{AVG}}, \Tcal)$ as the performance of the averaged model $\phi_{\text{AVG}}$ on all tasks $\Tcal$. We write the $P_{\text{SEL}} = \frac{1}{n_{\tau}}  \sum_{\tau \in \Tcal} P(\phi_{\text{SEL}}, \tau)$ as the performance of the selection based model $\phi_{\text{SEL}}$ on all tasks $\Tcal$. The results show that the
\begin{align}\label{eq:SEL-AVG-ORI}
    P_{\text{ORI}} < P_{\text{AVG}} < P_{\text{SEL}},
\end{align}
which means that finetuning and averaging can successfully increase the model performance. However, simply averaged model cannot recover the task performance of the corresponding finetuned models.


% However, due to the parameter conflicts between different finetuned LLMs, simply averaging may lead to the performance degradation on some tasks compared to the individual finetuned models.


\textbf{Comparing Performance on Individual Models.} Table~\ref{tab:preliminary-experiments} shows that while the model $\theta_{\tau}$ has the best performance on its according task $\tau$, its performance on other tasks $\Tcal \setminus \tau$ is lower than $\phi_{\text{AVG}}$. This indicates that the averaged model can still benefit from merging knowledge from different finetuned models. Thus, a better merging strategy should be able to average the parameters that have less parameter conflict to find the common knowledge that can be shared across different tasks and avoid the parameter conflict that degrades the performance of the finetuned model.




% This phenomenon indicates that the averaged model can be benefited from merging knowledge from different finetuned models. This motivates us to average the parameters that have less parameter conflict to find the common knowledge that can be shared across different tasks and avoid the parameter conflict that destroys the performance of the finetuned model.

% \textbf{Comparing Performance on Individual Tasks.} We write $P_{\text{IDV}}(\tau) = P(\theta_{\tau}, \Tcal)$ as the performance of the finetuned model $\theta_{\tau}$ on all tasks. Table~\ref{tab:preliminary-experiments} shows that while the model $\theta_{\tau}$ has the best performance on the task $\tau$, its performance on other tasks $\Tcal \setminus \tau$ is lower than the averaged model $\phi_{\text{AVG}}$. Thus, the overall performance $\frac{1}{n_{\tau}} \sum_{\tau \in \Tcal}  P(\theta_{\tau}, \Tcal)$ is lower than $P_{\text{AVG}}$. This phenomenon indicates that the merged model can be benefited from merging knowledge from different finetuned models. This motivates us to average the parameters that have less parameter conflict to find the common knowledge that can be shared across different tasks and avoid the parameter conflict that destroys the performance of the finetuned model.


% \begin{align}\label{eq:preliminary-results}
%     \sum_{\tau \in \Tcal} P(\theta_{\tau}, \tau) < \sum_{\tau \in \Tcal} P(\theta_{\tau}, \Tcal) < P(\phi, \Tcal)  < \sum_{\tau \in \Tcal} P(\theta_{\tau}, \tau) < \frac{1}{n_{\tau}} \sum_{\tau \in \Tcal} P(\theta_{\tau}, \Tcal)  < P(\theta, \Tcal).
% \end{align}

% Revisiting the model merging goal in Equation~\ref{eq:model-merging-goal}, we can see the challenge is the potential parameter conflict among different tasks (Equation~\ref{eq:task-specific-with-other}) caused by different task data distribution $p_{\tau}$.  To this end, we propose to firstly measure and understand the degree of parameter conflicts.



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/cot_conflict_ratio.pdf}
    \vspace{-0.1cm}
    \caption{Parameter conflict distribution across different layers of finetuned models (Qwen 2.5 7B).}
\label{fig:parameter-conflict}
\vspace{-0.1cm}
\end{figure}
\vspace{-0.1cm}


\begin{definition}[Task Arithmetic]\label{def:task-arithmetic}
    A task arithmetic on task $\tau$ is the parameter difference between the finetuned LLM $\theta_{\tau}$ and the pre-trained LLM $\theta$, i.e., $\Delta_{\tau} = \theta_{\tau} - \theta$. 
\end{definition}

% \begin{remark}
% Task arithmetics are updates on the finetuned LLM $\theta_{\tau}$ based on task $\tau$. Given a pretrained LLM $\theta$, one can recover the finetuned LLM $\theta_\tau = \theta + \Delta_\tau$.
% \end{remark}


\begin{figure*}[!th]
    \centering
    \includegraphics[width=0.8\linewidth]{framework.pdf}
    \vspace{-0.1cm}
    \caption{The framework of Mediator.}
\vspace{-0.1cm}
\label{fig:Mediator}
\end{figure*}


\textbf{Denoising Parameters.}
Due to the stochastic optimization process in finetuning, some elements in $\Delta_\tau$ are noisy and do not influence the performance~\citep{Yadav:2023aa,he2024localize}. Thus, before measuring the parameter conflicts~\citep{Yadav:2023aa,he2024localize}, we firstly denoise the parameters by removing the elements in $\Delta_\tau$ that have have small magnitude. To this end, we model the update directions of different elements as the Gaussian distribution $\Ncal_{\text{UPD}}(\mu_{\text{UPD}}, \sigma_{\text{UPD}}^2)$, where $\mu_{\text{UPD}}$ is the mean of the update direction and $\sigma_{\text{UPD}}^2$ is the variance. Then, we denoise the parameters by removing the elements within range ($\mu_{\text{UPD}} - \sigma_{\text{UPD}}$, $\mu_{\text{UPD}} + \sigma_{\text{UPD}}$) and obtain the new parameter arithmetic $ \hat{\theta}_{\tau} = \theta + \hat{\Delta}_{\tau}$ (details in Appendix~\ref{appx:measuring-parameter-conflicts}).


% Based on the estimated $\mu_{\text{UPD}}$ and $\sigma_{\text{UPD}}$, we can regard the elements with magnitude less than $\mu_{\text{UPD}} - 3\sigma_{\text{UPD}}$ as the elements that do not influence the performance on task $\tau$. Thus, we can denoise the parameters by removing the elements with magnitude less than $\mu_{\text{UPD}} - 3\sigma_{\text{UPD}}$ and obtain the new parameter arithmetic $ \hat{\theta}_{\tau} = \theta + \hat{\Delta}_{\tau}$. 

% Then, the parameter conflict can be measured as $d_{\cos}(\hat{\theta}_{\tau}, \hat{\theta}_{\hat{\tau}})$ and $d_{\text{Euc}}(\hat{\theta}_{\tau}, \hat{\theta}_{\hat{\tau}})$.




\textbf{Measuring Parameter Conflict.} Following~\citep{Yadav:2023aa}, we measure the parameter conflict between different finetuned models using a sign-based approach. For layer $l \in \Lcal$, given parameters $w_i^l \in \theta_{\tau_i}^l$ and $w_j^l \in \theta_{\tau_j}^l$ from two different models, we consider them conflicting if they have opposite signs, i.e., $sgn(w_i^l w_j^l) = -1$. 
We define the conflict ratio of layer $l$ as the proportion of conflicting parameters in that layer:
% \begin{align}
%     d_l = (\frac{\sum_{i,j} \mathbb{I}(sgn(w_i^l w_j^l) = -1)}{|\theta^l|}),
% \end{align}
\begin{align}
    d_l = (\sum_{i,j} \mathbb{I}(sgn(w_i^l w_j^l) = -1))/|\theta^l|,
\end{align}
where $|\theta^l|$ is the total number of parameters in layer $l$, and $\mathbb{I}(\cdot)$ is the indicator function. 


Figure~\ref{fig:parameter-conflict} shows that the parameter conflict is higher in the front and last layers, and lower in the central layers. This phenomenon indicates that central layers share more common knowledge, while the front and last layers capture more task-specific knowledge. 




% \begin{align}
%     & d_{\cos}(\theta_{\tau}^l, \theta_{\hat{\tau}}^l) = \frac{\theta_{\tau}^l \cdot \theta_{\hat{\tau}}^l}{\|\theta_{\tau}^l\| \|\theta_{\hat{\tau}}^l\|}, \\
%     & d_{\text{Euc}}(\theta_{\tau}^l, \theta_{\hat{\tau}}^l) = \|\theta_{\tau}^l - \theta_{\hat{\tau}}^l\|.
% \end{align}


% We left the . 



\vspace{-0.1cm}
\section{The Design of Mediator}\label{sec:Method}
% In this section, we will introduce the design of Mediator. 
The high-level idea of Mediator is to hybridly combine averaging and routing to preserve the downstream knowledge and avoid the parameter conflict (Sectino~\ref{sec:AdaptiveMerging}) according to parameter conflict distribution across different layers. We decompose the downstream models as the base model and experts of task arithmetics, which brings into the opportunity to sparsify the task arithmetics to prune out the noisy parameters to reduce the memory costs (Section~\ref{sec:expert-decomposition}). Lastly, considering the OOD inputs, we propose a Bayesian expert routing (Section~\ref{sec:expert-routing}) to better combine knowledge from different experts. 

% The average operation $\Mcal$ can be any averaging operation, such as the unified average, or using the importance metrics as weights like first-order Taylor expansion~\citep{wanda,SNIP}, or averaging based on the denoise weights like TIES~\citep{Yadav:2023aa}. We left the details of different averaging operations in Appendix~\ref{appx:averaging-operations}.





\subsection{Adaptive Layer-wise Model Averaging and Routing}\label{sec:AdaptiveMerging}
% Inspired by the empirical observation in Figure~\ref{fig:parameter-conflict}, Mediator leverages the parameter conflict distribution across different layers to adaptively merge the finetuned models. As shown in Figure~\ref{fig:Mediator}, 

Inspired by the empirical observation in Figure~\ref{fig:parameter-conflict}, Mediator averages layers with less parameter conflicts, and route layers with more parameter conflicts. As shown in Figure~\ref{fig:Mediator}, Mediator calculates the conflicts $d_l$ across different layers. Then, Mediator models the layer-wise conflicts as a Gaussian distribution $d_l \sim \Ncal(\mu, \sigma)$.

Then, for each layer index $l$, Mediator average layer parameters if the conflict $d_l$ is less than the $\mu + \sigma$, otherwise routing this layer. We denote the averaged layer parameters as $\phi_{\text{AVG}}^l$ and the routing layer parameters as $\phi_{\text{UP}}^l$. Algorithm~\ref{algo:adaptive-merging} shows this detailed process. The average operation $\Mcal$ can be any averaging operation, such as the unified average, importance based~\citep{FisherMerging_NeurIPS2022}, or subspace based~\citep{Yadav:2023aa}. In our experiments, we mainly use the denoised parameters $\hat{\theta}_{\tau}$ defined in Section~\ref{sec:Understanding-Conflict} to conduct averaging like TIES~\citep{Yadav:2023aa}  (details of averaging operations in Appendix~\ref{appx:averaging-operations}). Note that all attention layers are averaged, because they are found to save non-specific domain knowledge~\citep{Sukhbaatar:2024aa}.


\begin{algorithm}[htp]
	\caption{Adaptive Merging and Routing with Sparsified Expert Decomposition in Mediator}
	\label{algo:adaptive-merging}
	\textbf{Input: } Different finetuned models $\theta_1, \theta_2, \dots, \theta_{n_{\tau}}$.\\
    \textbf{Output:} The merged layers $\Phi$.\\
    \begin{algorithmic}[1]
        \STATE Calculate the conflict distribution $\left\{d_l\right\}_{l=1, \cdots, |\Lcal|}$;
        \STATE Estimate $\mu, \sigma$ based on $\left\{d_l\right\}_{l=1, \cdots, |\Lcal|}$;
        \FOR{layer $ l=1, \cdots, |\Lcal|$}
            \IF{$d_l < \mu + \sigma$}
                % \STATE Average the layer parameters $\phi_{\text{AVG}}^l = \Mcal(\theta_1^l, \theta_2^l, \dots, \theta_{n_{\tau}}^l)$;
                \STATE $\phi_{\text{AVG}}^l = \Mcal(\theta_1^l, \theta_2^l, \dots, \theta_{n_{\tau}}^l)$; 
                \ELSE
                \STATE $\bar{\theta}^{l} = 1/n_{\tau} \sum_{\tau\in\Tcal} \theta_{\tau}^l $;
                \STATE $\Delta_{\tau}^l = \theta_{\tau}^l - \bar{\theta}^{l}$, $\Delta_{\star}^l = \theta^l - \bar{\theta}^{l}$;
                \STATE $\hat{\Delta}_{\tau}^l = \text{Denoise}(\Delta_{\tau}^l)$;
                \STATE $\phi_{\text{UP}}^l = \left\{\bar{\theta}^{l}, \hat{\Delta}_{1}^l,\hat{\Delta}_{2}^l,\dots,\hat{\Delta}_{n_{\tau}}^l,\hat{\Delta}_{\star}^l  \right\}$;
                \ENDIF
            \STATE Insert $\phi_{\text{AVG}}^l$ or $\phi_{\text{UP}}^l$ into $\Phi_{\text{AVG}}$ and $\Phi_{\text{UP}}$;
        \ENDFOR
        \STATE \textbf{Return} $\Phi = \{\Phi_{\text{AVG}}, \Phi_{\text{UP}}\}$.
\end{algorithmic}
\vspace{0.1cm}
\end{algorithm}





\subsection{Expert Decomposition}\label{sec:expert-decomposition}
% \begin{definition}[Task-specific Expert]
%     A task-specific expert $\theta_{\tau}^l$ is a finetuned LLM layer with indexed by $l$ that is trained on a specific task $\tau$.
% \end{definition}

The routing layer occupies $n_{\tau} \times M_l$ memory, where $n_{\tau}=|\Tcal|$ and $M_l$ is the memory of each layer in original model. Large $n_{\tau}$ significanly increases the memory cost of routing layers, thus leading to weak scalability. Thus, we consider compressing the routing layers to reduce the memory cost.

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.40\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/combined_qwen4b_distribution_comparison.pdf}
%         \caption{first}
%         \label{fig:sub1}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.40\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/combined_qwen7b_distribution_comparison.pdf}
%         \caption{second}
%         \label{fig:sub2}
%     \end{subfigure}
%     \caption{1 and 2}
%     \label{fig:big}
% \end{figure}

\begin{figure}[htb!]
    % \setlength{\abovedisplayskip}{-2pt}
    % \setlength{\abovecaptionskip}{-2pt}
    \subfigbottomskip=-1pt
    \subfigcapskip=1pt
  \centering
     \subfigure[Qwen 1.5 4B]{\includegraphics[width=0.45\linewidth]{figures/combined_qwen4b_distribution_comparison.pdf}}
     \subfigure[Qwen 2.5 7B]{\includegraphics[width=0.45\linewidth]{figures/combined_qwen7b_distribution_comparison.pdf}}
     \caption{Comparing magnitudes of task arithmetic and pretrained model parameters.}
    \label{fig:ta_distribution}
    \vspace{-0.3cm}
\end{figure}



% Algorithm~\ref{algo:adaptive-merging} 

However, previous LLM pruning or quantization~\citep{dong2024pruner,sun2024a} on $\left\{\theta^{l}\right\}$ cannot achieve high compression ratio on $\theta_\tau$. Different from directly compressing the finetuned model, we find that the task arithmetic $\Delta_{\tau}$ shows significantly higher sparsity due to its noisy updates~\citep{Yadav:2023aa}. As shown in Figure~\ref{fig:ta_distribution}, the empirical magnitudes of the task arithmetic are significantly smaller than the model parameters. This indicates that the $\Delta_{\tau}$ can be sparsified with a higher degree to reduce the memory cost. 


% like gradient compression~\citep{tang2020survey,stich2018sparsified}.


% However, directly compressing the finetuned model $\theta_{\tau}$ might lose too much information of the original base model like the model compression if the compression ratio is high~\citep{dong2024pruner,sun2024a}. Different from directly compressing the finetuned model, we find that the task arithmetic $\Delta_{\tau}$ shows significantly higher sparsity due to its noisy updates~\citep{Yadav:2023aa}. This indicates that the $\Delta_{\tau}$ can be sparsified with a higher degree to reduce the memory cost like gradient compression~\citep{tang2020survey,stich2018sparsified}.

To this end, we can decompose a finetuned LLM $\theta_{\tau}$ into its base model $\theta$ and the task-specific expert $\Delta_{\tau}$ (task arithmetic in Definition~\ref{def:task-arithmetic}). Each $\Delta_{\tau}$ will be sparsified as $\hat{\Delta}_{\tau}$ by removing the elements that can be seen as noise (Section~\ref{sec:Understanding-Conflict}). Thus, the memory cost is reduced from $n_{\tau} \times M_l$ to $n_{\tau} \times M_l \times c$, where $c$ is the compression ratio. When inferencing, we can use the $\theta + \hat{\Delta}_{\tau}$ to approximately recover the finetuned model $\theta_{\tau}$.



\textbf{Out-of-distribution Data \uppercase\expandafter{\romannumeral1}} (OOD to $\Tcal$ but in-distribution to pretraining data $p(x|\Tcal^\star)$): The finetuned model $\theta_\tau$ may not be able for processing some test-time questions $x^{\tau^{\star}} \sim p(x|\Tcal^\star)$ that sampled from other distributions instead of $p(x|\tau) $ for any $\tau \in \Tcal$. Considering the the modern LLM $\theta$ is pretrained on the huge corpus~\citep{gpt3_2020}, the $\theta_\tau$ might keep the original knowledge in  $x^{\tau^{\star}}$. However, the finetuning may cause forgetting problem~\citep{zhu2024model}. Thus, to preserve pretraining knowledge, we make a calibration on task arithmetic as $\Delta_{\tau} = \theta_{\tau} - \bar{\theta}$ and saves the pretraining arithmetic as $\Delta_{\star} = \theta - \bar{\theta}$, where $\bar{\theta} = 1/n_{\tau} \sum_{\tau \in \Tcal} \theta_{\tau}$.

% \Tcal^\star

% \sim p(x|\tau^{\text{OOD,1}})

% \Rmnum{1}, \rmnum{2}


\subsection{Expert Routing}\label{sec:expert-routing}
% \textbf{Expert Router.} Equation~\ref{eq:SEL-AVG-ORI} shows that the averaged model $\phi_{\text{SEL}}$ has the best performance than $\phi_{\text{AVG}}$, implying that selecting the task-specific experts for its trained tasks can result in better performance. Thus, the intuitive idea is to choose the task-specific experts for each data sample, i.e. for task $\tau$ choosing the experts $\left\{\theta_{\tau}^l\right\}_{l \in \Lcal}$ which include all task-specific knowledge.

In the token-level expert routing like pretrained MoE~\citep{Jiang:2024aa,Sukhbaatar:2024aa}, there are routers for all layers trained to select the task-specific experts for each token. The token-level routing implies that the router is designed for selecting experts at different token index. In other words, for a same sequence $x_{1:T}$ sampled from $p(x_{1:T} | \tau)$, the token-level router may select different experts for different tokens $t \in \left\{1, \cdots, T\right\}$. 




% experts of it are trained on different subsequences, 






\begin{algorithm}[htp]
\caption{Uncertainty based Expert Routing in Mediator}
\label{algo:routing}
\textbf{Input: } The question prompt $x_{1:t}$, router $\pi_{\kappa}$, $\Phi_{\text{UP}}$, $k$, $\beta$.\\
\textbf{Output:} Reconstructed routing layers $\Phi_{\text{UP},x}$. \\
\begin{algorithmic}[1]
% \STATE \textit{\# Train and fuse}
\STATE $ \Tcal_k^x = \text{Indexes of TopK}(\left\{\pi_{\kappa}(\tau|x) \right\},k)$;
\STATE Calculate and cache $\left\{ h(\tau|x)  \right \} $ (Equation~\ref{eq:temperature-scale});
\FOR{$\phi_{\text{UP}}^l \in \Phi_{\text{UP}}$}
    \STATE $ \phi_{\text{UP},x}^l = \bar{\theta}^{l} + \sum_{\tau\in\Tcal_k^x} h(\tau|x) \hat{\Delta}_{n_{\tau}}^l$;
    \STATE Insert $\phi_{\text{UP},x}^l$ into $\Phi_{\text{UP},x}$;
\ENDFOR
\STATE \textbf{Return} $\Phi_{\text{UP},x}$.
% \STATE $\phi_{\text{UP}}^l = \left\{\bar{\theta}^{l}, \hat{\Delta}_{1}^l,\hat{\Delta}_{2}^l,\dots,\hat{\Delta}_{n_{\tau}}^l,\hat{\Delta}_{\text{OOD}}^l  \right\}$;
% \ENDIF
% \STATE Insert $\phi_{\text{AVG}}^l$ or $\phi_{\text{UP}}^l$ into $\Phi_{\text{AVG}}$ and $\Phi_{\text{UP}}$;
\end{algorithmic}
\vspace{0.1cm}
\end{algorithm}


\textbf{Task-level Routing.} However, different from pretrained MoE where one sequence $x_{1:T}$ might be splited into different subsequences and feed-forwarded to different experts, the finetuned LLM $\theta_{\tau}$ is trained on the complete sequence $x_{1:T}\sim p_{\tau}$. Therefore, a subsequence $x_{1:t} \subset x_{1:T}$ might be OOD to the another model $\theta_{\hat{\tau}}$. Therefore, the most suitable experts for each data sample might be the experts that are trained on the task $\tau$, i.e. $\left\{\theta_{\tau}^l\right\}_{l \in \Lcal}$ for each subsequence $\left\{ x_{1:t} \right\}_{t=1, \cdots, T}$ in $x_{1:T}\sim p_{\tau}$. 

% However, as illustrated in Section~\ref{sec:Understanding-Conflict}, the finetuned LLM $\theta_{\tau}$ is trained on the full sequence $x_{1:T}\sim p_{\tau}$. 



We also provide a theoretical understanding to understand the differences between task-level and token-level routing following the Bayesian inference interpretation for the in-context learning~\citep{xie2022an} in Appendix~\ref{appx:theoretical-analysis}).
% \begin{theorem}\label{thm:task-level-router}
%     TODO.....
%     The most suitable experts for each data sample might be the experts that are trained on the task $\tau$, i.e. $\left\{\theta_{\tau}^l\right\}_{l \in \Lcal}$ for each subsequence $\left\{ x_{1:t} \right\}_{t=1, \cdots, T}$ in $x_{1:T}\sim p_{\tau}$.
% \end{theorem}
% \begin{remark}
%     The Theorem~\ref{thm:task-level-router} shows that the task-specific experts are the most suitable experts for each data sample $x_{1:T}\sim p_{\tau}$. 
% \end{remark}
Moreover, we also empirically show in Section~\ref{sec:Experiments} that the trained token-level routers~\citep{Sukhbaatar:2024aa} also frequently select the task-specific experts for each data sample. 

% This indicates that the task-level router is suitable and enough for selecting experts, which helps to save memory and computation costs of routing, which is empirically validated in Section~\ref{sec:Experiments}.

\textbf{Out-of-distribution Data \uppercase\expandafter{\romannumeral2}} (OOD but close to $\Tcal$ and $\Omega$): Considering that $\tau$ is continuous~\citep{xie2022an}, while the test data $x^{\text{OOD}} \sim p_{\hat{\tau}}(x|\hat{\tau})$ ($\hat{\tau} \notin \Tcal, \Tcal^\star$) has different distribution from $p_{\tau}$ for any $\tau \in \Tcal, \Tcal^\star$, the $\hat{\tau}$ might be close to or a linear combination of multiple $\tau_1, \tau_2, \dots, \tau_{n_\tau} \in \Tcal$ and $\tau \in \Tcal^\star$. Any finetuned model $\theta_\tau$ may not be able to process some test-time questions $x^{\text{OOD}}$. A better way to conduct inference on $x^{\text{OOD}}$ is to combine multiple experts $\left\{\Delta_{\tau} \right\}_{\tau \in \Tcal}$ and $\Delta_\star$ together. Thus, we propose the Algorithm~\ref{algo:routing} and the following design.


% Table~\ref{tab:preliminary-experiments} and Equation~\ref{eq:SEL-AVG-ORI} show that selecting the task-specific experts for each input according to its trained tasks can result in the best performance, i.e. selecting experts $\Delta_{\tau}$ for input $x_{1:t}\sim p_{\tau}$. However, the test data in real-world deployment might have distribution shifts or be OOD to the finetuned tasks $\Tcal$. 

\textbf{Uncertainty-based Expert Selection.} We propose the uncertainty-based expert selection to combine the experts from different tasks into a new expert $\bar{\Delta}$ for the test data $x$. Thus, we need to estimate the posterior distribution $p(\bar{\Delta} | x)$ for the test data $x$. We introduce a small classifier $\kappa$ use cross-entropy loss to learn the likelihood $\pi_{\kappa} (\tau| x )$ for the training dataset including all data pairs $\left\{(x, \tau)| x \sim p_{\tau}\right\}_{\tau \in \Tcal\cup \left\{\tau^\star\right\} }$ (details of constructing this dataset and learning $\pi_\kappa (\tau|x)$ is in Appendix~\ref{appx:expert-routing}). For the training data $x\sim p_{\tau}$, we have known that its best expert is $\Delta_{\tau}$, thus having $\bar{\Delta} = \Delta_{\tau}$. For a test data $x$, we introduce the temperature scaling to soft the logits $h_\kappa$ to obtain the prediction uncertainty $\pi_{\kappa} (\tau| x )$, like OOD detection~\citep{Guo2017OnOCO} and model distillation~\citep{Hinton2015DistillingDTK} as follows: 
\begin{align}
% \bar{\Delta} = \sum_{\tau \in \Tcal}\Delta_{\tau} h_\tau, \ 
h(\tau|x) = \frac{e^{\pi_\kappa(\tau| x)/\beta}}{\sum_{\tau \in \Tcal} e^{\pi_\kappa(\tau| x)/\beta}}.
\label{eq:temperature-scale}
\end{align}

Then, the final obtained expert is $\bar{\theta}^{l} + \sum_{\tau\in\Tcal_k^x} h(\tau|x) \hat{\Delta}_{n_{\tau}}^l$ as shown in Algorithm~\ref{algo:routing}. Then the whole feed-forward process is as same as the original model. We further provide system optimization in the real-world serving as follows.





% The final merged expert is $\theta + \bar{\Delta}$.



% we consider to exploit the outputed logits $h_\kappa$ from the classifier $\kappa$ to estimate the prediction uncertainty of the test data $x$. 


% Then, we can use the Bayesian inference to estimate the posterior distribution $p(\bar{\Delta} | x)$ as follows:
% \begin{align}
%     p(\bar{\Delta} | x) \propto \sum_{\tau \in \Tcal} p(\bar{\Delta} | y, x) p(y | x)
% \end{align}



% To this end, we propose to leverage the Bayesian inference to combine the experts from different tasks into a new expert $\bar{\Delta}$ for the test data $x$. Thus, we need to estimate the posterior distribution $p(\bar{\Delta} | x)$ for the test data $x$. We label the task $\tau$ as the $y_\tau$ and introduce a label variable with $p(\bar{\Delta} = \Delta_{\tau} | y = y_\tau) = 1$ and $p(\bar{\Delta} \neq \Delta_{\tau} | y = y_\tau) = 0$. For the training data $x\sim p_{\tau}$, we have known that its best expert is $\Delta_{\tau}$. Thus, we introduce a small classifier $\kappa$ use cross-entropy loss to learn the likelihood $\pi_{\kappa} (y| x )$ for the training dataset including all data pairs $\left\{(x, y_\tau)| x \sim p_{\tau}\right\}_{\tau \in \Tcal}$.

% Then, we can use the Bayesian inference to estimate the posterior distribution $p(\bar{\Delta} | x)$ as follows:
% \begin{align}
%     p(\bar{\Delta} | x) \propto \sum_{\tau \in \Tcal} p(\bar{\Delta} | y, x) p(y | x)
% \end{align}





\subsection{System-level Optimization}\label{sec:SystemOptimization}

We compare the inference times of Mediator with those of existing LLMs. The incremental time overhead primarily arises from expert routing and the loading of expert parameters. Fortunately, by using task-level routing, we only run a small classifier \( \kappa \) and load the expert parameters once per sample. To accelerate inference, we explore two methods for optimizing the loading of expert parameters. For sparse expert parameters, we store them in the CPU and prefetch the parameters of the upcoming layer while simultaneously performing computations in the current layer. This approach masks the long loading times associated with expert uploads. Additionally, we accelerate the integration of sparse experts (i.e., Algorithm~\ref{algo:routing}) into the dense backbone by using CUDA's atomicAdd for parallel processing. For non-sparse expert parameters, we store them on disk and utilize ZipNN~\citep{hershcovitch2024zipnn} to accelerate the loading of parameters from disk to CPU. Our results show that the optimized inference time of Mediator remains between 0.2s and 0.4s longer than that of existing LLMs per sample (details in Appendix~\ref{appx:system-optimization}).


\vspace{-0.1cm}
\section{Experiments}\label{sec:Experiments}
% \subsection{Experimental Setup}\label{sec:ExperimentalSetup}

\textbf{Models and Evaluation Tasks.} We conduct comprehensive experiments on cutting-edge LLMs including Qwen-1.5-4B, Qwen-2.5-7B~\citep{yang2024qwen2}, LLaMA-3.1-3B, and LLaMA-3.2-8B~\citep{dubey2024llama}. We select different evaluation tasks to effectively demonstrate model capability in resolving parameter conflicts during model merging, including GSM8K of mathematical question-answering~\citep{cobbe2021gsm8k}, TriviaQA~\citep{joshi2017triviaqa} of a large-scale Wikipedia-based question answering dataset, HumanEval~\citep{chen2021humaneval} of Python programming tasks, WinoGrande~\citep{sakaguchi2019winogrande} of logical reasoning, MMLU~\citep{hendrycks2021mmlu} of vertical domain knowledge (as OOD to the finetuned models). 




% We adopt the ms-swift~\citep{zhao2024swiftascalablelightweightinfrastructure} framework to finetune the given pretrained LLM. 
% We provide the details of how we construct the finetuning datasets in Section~\ref{appx:finetuning-data-generation} and details of hyperparameters in Section~\ref{appx:detailed-hyper-parameters}. 

\textbf{Finetuning Settings.} The finetuning datasets are constructed by augmenting some publicly  datasets (task related but without overlap) with GPT-4o~\citep{gilardi2023chatgpt} and Chain-of-Thoughts~\citep{CoT}. For each finetuning process, we use at least 180K training samples to ensure sufficient performance improvement on the corresponding task, which helps validate the effectiveness of our experiments (Details of constructing finetuning datasets in Appendix~\ref{appx:finetuning-data-generation} and hyperparameters in Appendix~\ref{appx:detailed-hyper-parameters}). \textit{To the best of our knowledge, this is the first LLM merging study with CoT enhanced finetuning and evaluated with generative tasks.} 


\begin{table}[htbp]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Comparing performance of different model merging methods on Llama 3.1 3B.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccccc}
\toprule
Alg./Tasks & GSM. & TrA. & Wino. & H.Eval & MMLU & AVG. \\
\midrule
Base & 27.52 & 57.71 & \textbf{69.69} & 22.56 & \textbf{54.08} & 46.31 \\
Math & \textbf{46.47} & 54.59 & 69.06 & 25.00 & 52.73 & 49.57 \\
QA & 32.75 & \textbf{61.45} & \textbf{69.69} & 28.05 & 54.17 & 49.22 \\
Code & 33.13 & 57.71 & 68.59 & \textbf{40.85} & 53.09 & \textbf{50.67} \\
All data & 44.12 & 47.74 & 69.21 & 34.76 & 53.75 & 49.92 \\
\midrule
TIES & 42.61 & 60.99 & 71.11 & 31.30 & 54.32 & 51.27 \\
PCB & 46.02 & 60.39 & 71.27 & 29.88 & 54.21 & 52.35 \\    
Twin & 39.04 & 52.45 & 69.27 & 29.94 & 53.91 & 48.11 \\
BTX & 45.19 & \textbf{62.05} & 71.87 & 28.05 & 54.44 & 52.33 \\
\textbf{Mediator}  & \textbf{46.47} & 61.02 & \textbf{72.03} & \textbf{40.42} & \textbf{54.91} & \textbf{54.97} \\
\bottomrule
\end{tabular}
}
\label{tab:model_results_llama3b}
\end{table}
\vspace{-0.1cm}



\begin{table}[h]
\caption{Performance of different models and algorithms on Llama-3.2 8B.}
\vspace{-0.1in}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccccc}
\toprule
    Alg./Tasks & GSM. & TrA. & Wino. & H.Eval & MMLU & AVG. \\
    \midrule
    Base & 56.33 & 72.39 & 73.64 & 27.44 & \textbf{67.99} & 59.56 \\
    Math & \textbf{77.18} & 73.99 & 74.98 & 20.12 & 62.10 & 61.67 \\
    QA & 69.60 & \textbf{74.14} & \textbf{75.45} & 31.71 & 62.21 & 62.43 \\
    Code & 61.41 & 73.94 & 74.59 & \textbf{62.80} & 62.73 & \textbf{67.09} \\
    All data & 70.89 & 69.77 & 75.06 & 48.17 & 62.94 & 65.37 \\
    \midrule
    TIES & 76.04 & 76.78 & 74.19 & 53.05 & 62.36 & 68.48 \\
    PCB & 76.04 & 76.89 & 74.35 & 53.66 & 62.42 & 68.67 \\
    Twin & 76.80 & 72.71 & 74.49 & 59.14 & 64.43 & 69.51 \\
    BTX & 76.72 & 73.99 & 75.22 & 60.98 & 65.68 & 70.52 \\
    \textbf{Mediator} & \textbf{76.95} & \textbf{76.70} & \textbf{75.69} & \textbf{62.80} & \textbf{67.87} & \textbf{71.80} \\
\bottomrule
\end{tabular}
}
\label{tab:perf_llama_8b}
\end{table}
\vspace{-0.1cm}


\textbf{Baselines.} 
We compare pretrained, finetuned models, and the state-of-the-art static and dynamic merging methods with Mediator. The \textit{static merging methods} include TIES~\citep{TiesMerging_NeurIPS2023} and PCB-merging~\citep{Du:2024aa} achieve the best performance in weighted average method and do not require calibration data, and also partly consider OOD evaluation tasks. The dynamic merging methods include BTX~\citep{sukhbaatar2024branchtrainmix} with token-level routing and the twin-merge~\citep{Lu:2024aa} with task-level routing and SVD decomposition (Details of hyperparameters and optimization of these baselines in Appendix~\ref{appx:detailed-hyper-parameters}).





\vspace{-0.1cm}
\subsection{Main Results}\label{sec:main-results}
% We conduct comprehensive experiments comparing different model sizes of Qwen (4B, 7B) and Llama (3B, 8B)
%  across multiple dimensions. 
% Our evaluation spans across GSM8K (mathematical reasoning), HumanEval (programming ability), TriviaQA (question answering), 
% WinoGrande (logical reasoning), and MMLU (comprehensive knowledge). 
% From the analysis in the Appendix, we find that there is some overlap between different task datasets (e.g., MMLU also contains math problems). 
\textbf{Fine-grained Comparison on All Tasks.} Table~\ref{tab:model_results_llama3b} and ~\ref{tab:perf_llama_8b} show the fine-grained performance on each tasks and their overall averaged one of different methods and algorithms. In most of time, the finetuned LLM can achieve the best performance across all single and merged models on its specialized domain, like Math finetuned models on GSM8K and Code finetuned models on HumanEval. While the merged LLMs can generally outperform single models on the averaged performance, their specialized domain performance is weaker. However, Mediator can catch up the domain performance of the specialized models, and almost always outperform other merged models. Also, the overall performance on all tasks of Mediator is consistently better than other methods. Expert routing methods includes BTX and Mediator are likely to further improve performance. This aligns with findings from TIES~\citep{TiesMerging_NeurIPS2023} and Twin-merging~\citep{lu2024twin}. As model scale increases, the improvement margins of all merging algorithms decrease, which may be attributed to the enhanced comprehensive capabilities of individual finetuned models (or experts). We provide more fine-grained results about QWEN-1.5 and 2.5 in Table~\ref{tab:model_results_qwen4b} and ~\ref{tab:main-results-Qwen-7B} in Appendix~\ref{appx:more-experiment-results}.



\begin{table}[h]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Overall Performance on all tasks of all methods.} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccccccc}
\toprule
 Model/Algo. &  Qwen-4B & Llama-3B   &  Qwen-7B &  Llama-8B  \\
\midrule
 base model       &  48.87  &  46.31  &   67.63    &  59.56 \\
 all data sft     &  49.04  &  49.92   &  64.59   &  65.37 \\
 TIES             &  49.50  &  51.27   &  68.62  &  68.48    \\
 Twin           &   48.80    &   48.11    &    68.91       &  69.51 \\
 PCB             &  49.93  &  52.35   &  65.86    &  68.67 \\
 BTX             &   49.94  &  52.33    &  69.61   &  70.52\\
 \textbf{Mediator}             &  \textbf{51.40} ($\uparrow$2.9\%) &  \textbf{54.97}($\uparrow$5.0\%)  &  \textbf{71.00} ($\uparrow$2.0\%)  &  \textbf{71.80} ($\uparrow$1.8\%)\\
\bottomrule
\end{tabular}}
\label{tab:major-results}
\end{table}
\vspace{-0.1cm}
% with the most significant gain of 5.04\% observed in Llama 3B. 

% While Twin merge shows enhanced performance when scaling from 3B-4B to 7B-8B models due to reduced losses in LoRA (and SVD) matrix decomposition at larger model scales. 
% since we use unified exploration parameters to explore PCB's merging parameters.

\textbf{Overall Comparison.} As shown in Table~\ref{tab:major-results}, the advantages of PCB over TIES become less pronounced at larger model scales, and even shows performance degradation on Qwen-7B, which demonstrates PCB's instability. Dynamic routing approaches include BTX and Mediator show stable performance improvements. Our method demonstrates consistent improvements across different models.


% \vspace{-0.1cm}

% \midrule
%  Improvement &  $\uparrow$2.92\% &  $\uparrow$5.04\%   &  $\uparrow$2.00\%  &  $\uparrow$1.82\% \\


% In detail, we observe two interesting findings from the results of Llama 3B and Llama 8B: 
% 1) Mediator achieves the highest scores in most subtasks (except for TriviaQA and Winogrande, where it still performs very close to the best), 
% demonstrating the stability of the Mediator algorithm in performance improvement. 
% 2) PCB merging, BTX and Mediator can all potentially exceed the scores of task-specific finetuned models. 


% \rowcolor{greyL} \multicolumn{7}{c}{Single Model} \\

\textbf{Post-Training Time After Merging.} As many model merging methods like Twin, PCB and BTX require post-training, it is critical to compare the extra training time. Table~\ref{tab:system-performance-post-training-time} shows the post-training time of different methods. PCB merging require weight exploration thus leads to higher time. The BTX with token-level routing needs to completely train the layer-wise routers for each token, thus, the post-training time of them is significantly high. In contrast, for task-level routing approaches like Twin-merging and Mediator, taking the lowest time.

% When the the number of finetuned models increases, the post-training time of token-level 
% routing methods will be significantly expensive and infeasible. For PCB merging, we limit the model merging weight exploration to around 100 iterations, 
% resulting in exploration times between 3-6 hours. In contrast, for task-level routing approaches like Twin-merging and Mediator, we employ the same router training strategy, which takes only 1-2 hours.

\begin{table}[ht]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Post Training Time (Hours).} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
Model/Algo & LlaMA-3B & LlaMA-8B & Qwen-4B & Qwen-7B\\
\midrule
Twin       & 1.33     & 1.87    & 1.60    & 1.80    \\
PCB        & 3.42     & 5.75    & 5.73    & 5.80    \\
BTX        & 7.55     & 12.52   & 8.83    & 12.18   \\
Mediator   & 1.35     & 2.03    & 1.57    & 1.78    \\
% Twin & 1h20min & 1h52min & 1h36min & 1h48m\\
% PCB & 3h25m  & 5h45m & 5h44m & 5h48m \\
% BTX & 7h33m  & 12h31m  & 8h50m &  12h11m \\
%  Mediator  & 1h21m &  2h2m & 1h34m & 1h47m \\
\bottomrule
\end{tabular}}
\label{tab:system-performance-post-training-time}
\end{table}
\vspace{-0.1cm}




% measureaverage of 400 samples
\textbf{Inference Time.} Table~\ref{tab:system-performance-inference-time} shows the inference time of different methods. As the token-level routing methods need to load and compute the layer-wise routers for each token, the inference time of them is significantly higher (more than 2x) than our method. We have detailed our inference acceleration process in Section~\ref{sec:SystemOptimization} and Appendix~\ref{appx:system-optimization}.
\begin{table}[h]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
    \caption{Inference time per sample (seconds).} 
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cccc}
        \toprule
         Model/Algo &  LlaMA-3B &   LlaMA-8B &  Qwen-4B &  Qwen-7B\\
                    &  32 layers & 32 layers & 40 layers & 28 layers \\
        \midrule
        Base model   & 1.452 & 3.600 & 3.112 & 3.057 \\
        Twin & 1.725 & 4.151 & 3.792 & 3.648 \\
        BTX  & 3.237 & 8.68 & 7.082 & 7.153 \\
        Mediator  & 1.609 &  4.053 & 3.674 & 3.489 \\
        \bottomrule
\end{tabular}
}\label{tab:system-performance-inference-time}
\end{table}
\vspace{-0.1cm}




\textbf{Memory Cost.} Table~\ref{tab:system-performance-memory-costs} shows the memory costs of different methods. Our method significantly reduces the memory costs compared to saving all finetuned models because there is only one router for all experts, and the sparsified experts saving, and the layer-wise merging strategy (Details of formally comparing memory costs of merging methods in Appendix~\ref{appx:more-related-works}).

\begin{table}[ht]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Comparing memory costs.} 
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc}
    \toprule
    Model/Algo & LLaMA-3B & LLaMA-8B & Qwen-4B & Qwen-7B\\
    \midrule
    Base model   & 9G & 33G & 11G & 31G \\
    Twin-merging & 10G & 35G & 13G & 32G \\
    BTX (MOE) & 37G & 80G & 40G & 78G \\
    Mediator  & 10G &  35G & 13G & 33G \\
    \bottomrule
\end{tabular}}
\label{tab:system-performance-memory-costs}
\end{table}
\vspace{-0.1cm}


\begin{table}[h]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Performance of scaling up finetuned models.} 
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{l|llllllllll}
\toprule
Alg./Tasks & GSM. & TrA. & Wino. & H.Eval & MMLU & I.Eval & C.Eco & C.Med & C.law & AVG.\\
\midrule
Base & 47.16 & 44.54 & 56.75 & 41.46 & 54.45 & 30.70 & 49.09 & 55.10 & 41.67 & 46.77 \\
Math & \textbf{50.95} & 46.95 & 54.62 & 26.83 & 53.54 & 34.05 & 45.45 & 55.10 & 45.83 & 45.92 \\
QA & 45.56 & \textbf{48.02} & \textbf{57.93} & 39.02 & 52.32 & 31.65 & 43.64 & 59.18 & 45.83 & 47.02 \\
code & 43.29 & 46.39 & 54.14 & \textbf{43.29} & 54.82 & 31.65 & 43.64 & 59.18 & 45.83 & 46.91 \\
Instruct. & 47.54 & 40.96 & 55.09 & 37.80 & \textbf{54.88} & \textbf{38.37} & 52.73 & 59.18 & 50.00 & 48.51 \\
Economy & 45.56 & 46.24 & 57.93 & 28.86 & 54.21 & 32.13 & \textbf{56.36} & 55.10 & 45.83 & 46.78 \\
medicine & 39.12 & 44.50 & 56.67 & 1.83 & 54.63 & 28.30 & 50.09 & \textbf{61.22} & 41.66 & 42.00 \\
Law & 40.64 & 46.64 & 56.59 & 0.61 & 54.61 & 27.94 & 41.82 & 57.14 & \textbf{58.33} & \textbf{47.15} \\
All Data  & 43.75 & 46.25 & 56.43 & 40.85 & 54.60 & 35.37 & 49.09 & 55.10 & 41.67 & 46.90 \\
\midrule
TIES & 47.38 & 47.19 & 55.80 & 36.59 & 55.38 & 34.17 & 40.00 & 57.14 & 50.00 & 47.07 \\
PCB & 47.38 & 47.19 & 55.80 & 36.59 & 55.45 & 34.29 & 41.82 & 57.14 & 50.80 & 47.38 \\
Twin & 47.91 & 44.78 & 57.54 & 40.85 & 53.01 & 37.53 & 53.32 & 59.56 & 50.00 & 49.38 \\
BTX & 48.44 & 46.94 & 57.85 & 42.68 & 54.93 & 36.93 & 54.40 & 60.36 & 58.33 & 51.18 \\
\textbf{Mediator} & \textbf{50.64} & \textbf{48.04} & \textbf{57.93} & \textbf{44.51} & \textbf{55.12} & \textbf{38.50} & \textbf{56.01} & \textbf{61.17} & \textbf{58.33} & \textbf{52.25} \\
\bottomrule
\end{tabular}
}
\label{tab:scaling-up-finetuned-models}
\vspace{-0.1cm}
\end{table}



\subsection{Ablation Studies}\label{sec:AblationStudies}
\textbf{Scalability of Finetuned Models.} 
To verify the scalability of Mediator, we finetune another 4 LLMs according to the following 4 extra evaluation tasks including: (1) \textit{Instruction Following} with IFEval.~\citep{zhou2023ifeval} which assess models ability to accurately interpret and execute natural language instructions; (2-4) \textit{Medicine, College Economics and Law} from CEval.~\citep{huang2023ceval} which assess knowledge and capabilities across various academic and professional domains. We utilize four accordingly domain datasets for finetuning including Magpie \citep{xu2024magpie}, IndustryInstruction \citep{IndustryInstruction_Finance-Economics}, DISC-Med \citep{bao2023discmedllm}, DISC-Law \citep{yue2023disclawllm} without overlap with IFEval and CEval (Details in Appendix~\ref{appx:finetuning-data-generation}). 

Table~\ref{tab:scaling-up-finetuned-models} demonstrate several key findings: 
1) Static merging methods like PCB and TIES show diminished performance improvements after task expansion, performing even worse than instruction-following finetuned models in overall scores. The similar performance between PCB merging and TIES aligns with findings from the TIES paper, which noted degraded model capabilities when merging more than three tasks; 2) Dynamic merging approaches like BTX, Twin and Mediator maintain relatively stable performance after task expansion; 3) Mediator consistently outperforms BTX by a margin of 2.09\% and achieves the best scores across all individual tasks, showing its good scalability.





\begin{table}[h]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Performance under Different Temperature $\beta$.}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|ccccccc}
\toprule
Temperature & 0.1 & 0.5 & 1.0 & 1.25 & 1.50 & 1.75 & 2.0 \\
\midrule
Mediator & 50.31 & 50.35 & 50.64 &  51.02 & 51.40 & 50.84 & 50.92 \\
\bottomrule
\end{tabular}
}
\label{tab:temperature}
\end{table}
\vspace{-0.1cm}


\begin{table}[htp]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Model Performance w/o layer-wise merging.}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{l|ccc}
    \toprule
    Model scale & with averaging & w/o averaging & perf. gap \\
    \midrule
    Qwen-1.5 & 51.40 & 51.43 & -0.06\% \\
    Qwen-2.5 & 71.00 & 71.29 & -0.27\% \\
    \bottomrule
\end{tabular}
}
\label{tab:layer-wise-compression}
\end{table}
\vspace{-0.1cm}

\begin{table}[htp]
\centering
\setlength{\abovedisplayskip}{-2pt}
\subfigbottomskip=-1pt
\subfigcapskip=1pt
\setlength{\abovecaptionskip}{-2pt}
\caption{Performance with Different Compression Ratio.}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|cccccc}
    \toprule
    Compression Ratio & 10\% & 12\% & 14\% & 16\% & 18\% &  20\%  \\
    \midrule
    Mediator & 96.6\% & 97.9\% & 100\% & 97.2\% & 97.2\% & 96.6\%  \\
    \bottomrule
\end{tabular}
}
\label{tab:intra-layer-compression-ratio}
\end{table}
% \vspace{-0.1cm}




\textbf{Different Temperatures.} 
In the routing process, the hyperparameter temperature $\beta$ is a key factor. Table~\ref{tab:temperature} shows performance change of Mediator with different temperatures. Results show that only around 2.2\% score variation, the $\beta=1.5$ achieves the highest performance, $\beta=0.1$ almost equals to the Top-1 routing, results in the lowest performance.

% We find that temperature leads to a 2.2\% score variation in model merging. 
% 1) When temperature = 0.1, Mediator approaches Top-1 routing, resulting in a score decrease since achieving 100\% accurate task-level routing is challenging. 
% 2) We observe that the overall score peaks when temperature equals 1.5, where we can effectively merge the Top-2 models.

% \textbf{Compression Ratio.} 
% The compression algorithm in Mediator consists of two key components:
% 1) 
% We examine the conflict ratio of each layer by analyzing the sign conflicts between parameters. 
% We define the compression ratio $c_{route}$ as 
% the proportion of layers that require routing in the final model. 
% minimal performance impact with 4 experts, and only ~0.27\% accuracy loss with 8 experts. Given the significant compression ratios achieved 
% (3.5x for 4 experts, 7x for 8 experts), this minor performance trade-off is acceptable.

\textbf{Layer-wise Merging.} Layers with low conflicts are averaged thus reducing $n_\tau \times$ memory occupation. Table~\ref{tab:layer-wise-compression} averaging or not on Qwen-1.5 with 4 and 8 experts show almost no performance loss of the parameter averaging. Given the significant compression ratios achieved (3.5x for 4 experts, 7x for 8 experts), this minor performance trade-off is acceptable.

% \textbf{Intra-layer Compression.} For each sparsified expert, Within each layer, we perform further noise reduction and compression based on parameter magnitudes,
% building upon task arithmetic. Our experiments on Qwen-1.5-4B with 4 experts show that around 78\% of parameters have non-zero values after task arithmetic. 
% The average ratio of parameters retained across all layers compared to their original complete parameters is denoted as $c_{avg}$. 
% As shown in Table~\ref{tab:intra-layer-compression-ratio}, the model achieves optimal performance when $c_{avg} = 14\%$. This indicates that 
% parameters with smaller magnitudes from task arithmetic are likely noise, which aligns with experiments from \citep{TiesMerging_NeurIPS2023}.
% Intra-layer 

\textbf{Compression Ratios of Experts.} For each sparsified expert, within each routing layer, we compare different compression ratios in Table~\ref{tab:intra-layer-compression-ratio}.
Results show that the optimal performance is obtained when 14\% parameters are left. This indicates that parameters with smaller magnitudes from task arithmetic are likely noise, which aligns with experiments from \citep{TiesMerging_NeurIPS2023}.


% Overall, for merging 4 models, we can compress the model to 27\% of its original size with negligible accuracy loss. 
% For merging 8 models, we can achieve a compression ratio of approximately 13.6\%.
 
 
% All experiments were conducted on an A800 GPU with 80GB VRAM, Intel Xeon 6348 CPU, and 100GB RAM. To ensure fair comparison, we standardized the evaluation settings across all methods: setting temperature to 0, 
% maximum generated tokens to 128, and evaluating on 100 randomly sampled examples each from GSM8K, TriviaQA, HumanEval, and MMLU benchmarks, reporting average performance.



% To ensure our test dataset is not out-of-distribution (OOD), 
% we sampled 200 examples each from the training sets of math, coding, and QA tasks for evaluation. 
% Since MMLU belongs to the ``others'' category without a training set, we randomly selected 200 examples from its test set. 

\textbf{Comparison between Task-level routing and Token-level routing.}
Fig.~\ref{fig:train_token_heat_map} in Appendix demonstrates the expert selection probabilities of the BTX model across different tasks (MMLU, math, coding, and QA). The results indicate that 1) bottom and upper layers show obvious task preference; 2) middle layers suggesting some shared logical processing across tasks in these layers, which explains why averaging middle layers that have less conflicts in Mediator results in small performance loss.

% 3) For MMLU data, which is OOD, we observe varying expert selections across different layers, with higher layers tending to route to QA experts, likely because MMLU consists largely of knowledge-based question-answering across various domains.


\section{Limitations}\label{sec:Limitations }
\textbf{Parameter Conflict Theory.} Our empirical analysis in Section~\ref{sec:Understanding-Conflict} provides initial insights, but a theoretical foundation for parameter conflicts remains unexplored.

\textbf{Memory Efficiency.} Despite reducing memory use versus storing all finetuned models, Mediator’s storage demands remain high.

\textbf{Scaling Challenges.} While scaling to 8 models is feasible, expanding to hundreds or thousands (e.g., for personalization~\citep{chan2024scaling}) poses deployment challenges.

\textbf{Loading Time.} Loading experts per sample remains a system bottleneck. Although optimized (Section~\ref{sec:SystemOptimization}), faster loading requires further research.

\section{Conclusion}\label{sec:Conclusion }
We propose Mediator, a framework for merging LLMs that addresses parameter conflicts through adaptive layer-wise strategies, which average low-conflict layers and routing high-conflict ones via task-specific experts. This preserves individual model strengths while integrating shared knowledge, improving performance and reducing system costs.

By decomposing experts into a dense core and sparse components, Mediator minimizes storage without sacrificing efficacy. Dynamic expert selection via task uncertainty enhances adaptability across diverse inputs. Experiments on LLaMA and Qwen demonstrate significant performance gains over existing methods, with CoT enhanced datasets further enhancing reasoning capabilities.

Mediator advances efficient LLM merging methods, balancing resource constraints with practical versatility. Future work should explore theoretical foundations of parameter conflicts, large-scale deployment optimizations, and faster expert loading mechanisms.

\section*{Impact Statements}
\textbf{Societal Impacts.} Our approach demonstrates significant effectiveness by enabling the deployment of merging 7B x 4 LLMs with only 24GB VRAM. Compared to ensemble learning with these models, our method not only maintains better accuracy but also requires significantly less computational resources and demonstrates superior performance. This breakthrough in resource efficiency makes advanced language models more accessible and cost-effective.

\textbf{Ethical Concerns.} We declare no conflicts of interest that could inappropriately influence our work. All experiments were conducted using publicly available resources. Our study does not involve human subjects, data collection from individuals, or experiments on protected groups. The models and basic datasets used in this work are publicly available and widely used in the research community. We have made efforts to ensure our experimental design and reporting of results are fair, unbiased, and do not misrepresent the capabilities or limitations of the methods presented.

\textbf{Reproducibility.} For openness of LLM research, we declare our code and the CoT enhanced crafted finetuning datsets will be made available to ensure reproducibility. We will provide detailed documents of code implemnetation. And we have provided the details of all hyper-parameters of implementing Mediator and optimizing baselines.


\textbf{Potential Applications.} The technology may have significant potential across specialized vertical domains. Considering that many vertical domains,  personalized LLM agents~\citep{li2024personalllmagentsinsights}, LLM applications like roleplay chatting~\citep{chan2024scaling,} and professional domain-specific writing~\citep{gómezrodríguez2023confederacymodelscomprehensiveevaluation}, an LLM service provider may need to simultaneously deploy different finetuned LLMs. Our technology enables efficient and effective serving multiple popular LLM applications, and merging knowledge from different LLMs together. 

% while maintaining high performance standards through reduced computational requirements.


% \section{Limitations}\label{sec:Limitations}
% \textbf{Batch Inference.} Currently, Mediator can only perform single-task inference. Since it needs to dynamically determine the fusion weights of each expert through the router for each task, and generate models and perform inference in real-time, batch inference is not possible.

% \textbf{Understanding the Parameter Conflict.} We have provided some empirical analysis on the parameter conflict in Section~\ref{sec:Understanding-Conflict}. However, the parameter conflict is a complex phenomenon, and we still lack a theoretical understanding of it. Future works can further explore the parameter conflict and its relationship with the model performance.

% \textbf{Memory Efficiency.} While our method has significantly reduced the memory footprint compared to saving all finetuned models, it still requires a large amount of memory to store the finetuned models.

% \textbf{Scaling up More Finetuned Models.} While we show that our method can scale up to 8 finetuned models, in the practical deployment, there are still some challenges to scale up to hundreds or thousands and even more of finetuned models like the personalization agents~\citep{chan2024scaling}.


% \textbf{Optimizing Loading Time.} To serve different data samples in practical deployment, the system needs to load different finetuned experts. As analyzed in Section~\ref{sec:SystemOptimization} and Appendix~\ref{appx:system-optimization}, this process will be a main bottleneck for the system performance. While we have done some optimizations, the loading time is still a problem. Future works can further optimize the loading time.



% \section{Conclusion}\label{sec:Conclusion}




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{appendix.tex}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
