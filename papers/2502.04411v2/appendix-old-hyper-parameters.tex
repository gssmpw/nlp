
% \begin{itemize}
%     \item Learning rate: 1.2e-5 with cosine decay schedule
%     \item Batch size: 1 sequences per batch for 7B and 8B models, 2 sequences per batch for 3B and 4B models
%     \item Maximum sequence length: 4096 tokens for math and QA tasks, 7000 tokens for coding tasks.
%     \item Training epochs: 2 epochs
%     \item Optimizer: AdamW with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1e-8$
%     \item Warmup steps: 0.05 of total steps
% \end{itemize}

% \subsection{Hyperparameters for PCB-merging}

% For PCB-merging, we use the following hyperparameters:

% \begin{itemize}
%     \item Weight clipping ratio: 0.1 (clips weights with magnitude in the bottom 10\% to zero) as it is recommended by the original paper for LLM generalization tasks~\citep{Du:2024aa}.
%     \item Model merging exploration parameters:
%     \begin{itemize}
%         \item Number of random exploration steps: 200
%         \item initial weights for random exploration: (0.4, 0.4, 0.4, 0.4) for 3B, 4B, 7B, 8B models for 4 experts and $(0.2) \times 8$ for all models with 8 experts.
%         \item Validation batch size: 8 samples per task
%         \item Early stopping patience: 10 steps without improvement
%     \end{itemize}
% \end{itemize}

% The weight clipping ratio and exploration parameters are kept consistent across all model sizes and tasks to ensure fair comparison. 
% The validation batch size is reduced to 4 for 7B and 8B models due to memory constraints.

% \subsection{Hyperparameters for PCB-merging Optimization}

% To improve the overall performance of PCB-merging, we implemented several optimizations based on the original PCB-merging framework:

% \begin{itemize}
%     \item Layer-wise model merging: Instead of merging entire models at once, we perform merging layer by layer to:
%     \begin{itemize}
%         \item Reduce memory overhead during merging
%         \item Enable parallel processing of different layers
%         \item Allow for layer-specific merging weights
%     \end{itemize}
    
%     \item Overlapped model I/O: We implemented asynchronous model loading and saving to:
%     \begin{itemize}
%         \item Overlap I/O operations with computation
%         \item Reduce total merging time by up to 40\%
%         \item Enable streaming of large models
%     \end{itemize}
% \end{itemize}

% These optimizations significantly improved both the efficiency and effectiveness of PCB-merging, making it more practical for large-scale experiments. 
% The layer-wise approach in particular reduced peak memory usage by approximately 60\% while maintaining or improving final model performance.

% \subsection{Hyperparameters for the LoRA for Twin-merging finetuning}

% For Twin-merging, we use LoRA finetuning instead of SVD to achieve higher precision. For each task, we train for 2 epochs with the following hyperparameters:

% \begin{itemize}
%     \item Training epochs: 2
%     \item Batch size: 16 (reduced to 8 for 7B and 8B models)
%     \item Learning rate: 1.5e-4 with cosine decay schedule
%     \item Optimizer: AdamW

% \begin{itemize}
%     \item LoRA rank: 32
%     \item LoRA alpha: 32
%     \item LoRA dropout: 0.1
%     \item Target modules: query and value matrices in attention layers
% \end{itemize}

% \subsection{Hyperparameters for Task-Level Router}

% For fair comparison, we adopt the same task-level router architecture as Mediator across all experiments:

% \begin{itemize}
%     \item Router backbone: First 9 layers from the pretrained LLM
%     \item Additional layers: 2 feed-forward network (FFN) layers on top
%     \item Input: Task embedding and model state
% \end{itemize}

% This router configuration was chosen to match Mediator's architecture and ensure consistent model capacity across different methods. 
% The combination of pretrained LLM layers and additional FFN layers provides sufficient modeling power while maintaining reasonable computational overhead. 
% The router's architecture remains fixed across all model sizes and tasks to enable fair comparisons.

% \subsection{Hyperparameters for Branch-train Mix (BTX)}
% For BTX router training, we combine the original open-source BTX project (https://github.com/Leeroo-AI/mergoo) with our implementation in 
% ms-swift (https://github.com/modelscope/ms-swift). The specific hyperparameters are:

% \begin{itemize}
%     \item Training epochs: 2
%     \item Batch size: 2 (reduced to 1 for 7B and 8B models)
%     \item Learning rate: 1.5e-6 with linear decay schedule
%     \item Optimizer: AdamW with weight decay 0.001
    
%     \item Router architecture:
%     \begin{itemize}
%         \item Input dimension: 2 layers of FFN
%         \item Hidden dimension: 256
%         \item Output dimension: Number of experts
%         \item Dropout: 0.1
%     \end{itemize}
%     \item Warmup steps: 0.05 of total steps
%     \item Evaluation frequency: Every 1000 steps
% \end{itemize}

% The router is trained on a balanced dataset sampled from all tasks to ensure equal representation. We use early stopping with patience of 2 epochs based on validation accuracy.

% \subsection{Hyperparameters for Mediator}
% For Mediator training, we use the same single-task finetuned experts as described in Appendix~\ref{appx:single-task-finetuning}. 
% The task-level router consists of the first 9 layers from the pretrained LLM (with gradient stopped) and 2 additional FFN layers. 
% During router training, we sample 2000 examples from each task domain (mathematics, coding, question answering, law, economics, instruction following, and medicine). 
% The specific hyperparameters for router training are:

% \begin{itemize}
%     \item Training epochs: 2
%     \item Batch size: 256 (reduced to 128 for 7B and 8B models)
%     \item Learning rate: 3e-4 with cosine decay schedule
%     \item Optimizer: AdamW
%     \item Warmup ratio: 10\% of total steps
    
%     \item Router architecture:
%     \begin{itemize}
%         \item Frozen backbone: First 9 layers from pretrained LLM
%         \item Trainable layers: 2 FFN layers
%         \item Hidden dimension: 1280
%         \item Output dimension: Number of experts
%         \item Dropout: 0.05
%     \end{itemize}
% \end{itemize}












\paragraph{GSM8K.}~\citep{cobbe2021gsm8k} A dataset of 8,500 high-quality elementary school math word problems (about 7,500 training, about 1,000 test) designed to evaluate mathematical reasoning 
capabilities. The problems feature diverse language styles and formats while avoiding templated designs. They use basic arithmetic operations with natural language solutions. 
The dataset aims to improve LLM performance on multi-step math reasoning tasks and address issues with error-prone calculations.

\paragraph{TriviaQA.}~\citep{joshi2017triviaqa} A large-scale dataset for retrieval-based question answering and reading comprehension. 
It contains complex questions requiring cross-sentence inference, with significant syntactic and lexical variations between questions and answer sentences. 
The dataset provides challenging evaluation scenarios that better approximate human-like question answering.

\paragraph{HumanEval.}~\citep{chen2021humaneval} Developed by OpenAI, this benchmark consists of human-written programming tasks where models must complete missing Python code snippets based on provided inputs. 
The problems simulate real-world programming challenges requiring context understanding, reasoning, and multi-step operations across varying difficulty levels and abstraction layers. 
It serves as a standard for evaluating NLP models' code comprehension and generation capabilities.

\paragraph{WinoGrande.}~\citep{sakaguchi2019winogrande} A large-scale commonsense reasoning dataset of approximately 2800 questions developed by University of Washington researchers. 
Questions are presented as fill-in-the-blank tasks with two options and correct answers, with dataset bias reduced through the AfLite algorithm. 
The benchmark evaluates models' commonsense reasoning abilities in understanding and generating relevant text.

\paragraph{MMLU.}~\citep{hendrycks2021mmlu} The Massive Multitask Language Understanding benchmark, developed by Stanford researchers, covers 57 subjects ranging from basic mathematics to US history, 
computer science, law, and ethics. Using multiple-choice questions of varying difficulty levels, it comprehensively tests models' knowledge and problem-solving capabilities 
across diverse disciplines.

\paragraph{IFEval.}~\citep{zhou2023ifeval}  A comprehensive benchmark dataset designed to evaluate instruction-following capabilities of language models. It contains carefully curated 
instruction-response pairs across diverse task categories including text generation, analysis, and reasoning. The dataset aims to assess models' ability to accurately 
interpret and execute natural language instructions while maintaining coherence and relevance in responses. The evaluation spans multiple dimensions including instruction comprehension, 
output quality, and adherence to specified constraints.

\paragraph{CEval.}~\citep{huang2023ceval} A comprehensive Chinese evaluation suite designed to assess language models' knowledge and capabilities across various academic and professional domains. 
It consists of multiple-choice questions drawn from professional qualification exams and academic tests in China. For our evaluation, we specifically focus on three key subjects:
(1) Medicine - testing clinical knowledge, diagnosis, and treatment principles from medical licensing exams;
(2) College Economics - evaluating understanding of micro/macroeconomics concepts, market principles, and economic theories;
(3) Law - assessing comprehension of Chinese legal principles, regulations, and judicial procedures.
These subjects were chosen to evaluate models' domain-specific expertise in technically demanding professional fields.