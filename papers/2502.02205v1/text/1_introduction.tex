
The control of physical systems described by partial differential equations (PDEs) is crucial in various scientific and engineering domains, including fluid dynamics \citep{hinze2001second}, controlled nuclear fusion \citep{schuster2006role}, and mathematical finance \citep{soner2004stochastic}. While traditional control algorithms have been extensively studied over the years \citep{1580152, protas2008adjoint}, advancements in neural networks have led to the development of numerous deep learning-based approaches \citep{farahmand2017deep, holl2020learning, hwang2022solving}. Among these, diffusion models have gained increasing attention to PDE control due to their ability to model high-dimensional and nonlinear data \citep{vahdat2022lion, li2024synthetic}, and have demonstrated impressive performance in various control tasks \citep{chi2023diffusion, wei2024generative, hu2024wavelet}.

However, existing deep learning-based methods for PDE control generally overlook \textit{safety} -- meaning ensuring control sequences satisfy predefined constraints, mitigating risks, and preventing hazards \citep{dawson2022safe, liu2023datasets}. In real-world applications, controlling PDE systems particularly requires addressing safety concerns \citep{735940, argomedo2013lyapunov}. For instance, in fluid dynamics, minor control errors can cause turbulence or structural damage, while in controlled nuclear fusion, violating safety constraints may lead to catastrophic failures. This lack of safety enforcement remains a major bottleneck in applying machine learning to scientific and engineering problems, posing a critical challenge for deployment in high-stakes applications.

Despite its importance, safe PDE control remains challenging. First, ensuring safety requires preventing methods without formal guarantees from interacting with the environment, restricting the approach to an offline setting with pre-collected data. However, such data is often suboptimal and includes unsafe samples, creating a significant gap between the observed distribution and the near-optimal, safe distribution \citep{xu2022constraints, liu2023datasets}. 
Second, the method must resolve the inherent conflict between optimizing control performance and satisfying safety constraints (\emph{e.g.}, aggressive control policies that reduce error may violate safety thresholds) \citep{liu2023datasets, zheng2024safe}.

To address these challenges, we propose \emph{Safe Diffusion Models for PDE Control} (\proj), a method that adapts diffusion models to varying safety constraints by quantifying their uncertainty. 
In offline settings, training data are often sub-optimal and unsafe, which causes the distribution learned by diffusion models to deviate significantly from the desired optimal and safe distribution. Inspired by conformal prediction \citep{vovk2005algorithmic, Tibshirani2019ConformalPU}, we propose quantifying diffusion models' safety uncertainty under distribution shifts and incorporating it through both post-training and inference phases.
This uncertainty quantification, termed as \textit{uncertainty quantile}, allows us to compute a \emph{conformal interval}, within which the safety score of the diffusion model interacting with the real environment is expected to lie. By leveraging this uncertainty quantification for predicted safety, diffusion models can be optimized with a clear safety optimization objective, to satisfy various safety constraints by constraining their conformal intervals within safe boundaries. 
Specifically, we incorporate uncertainty quantification into both the post-training and inference-time fine-tuning processes. Technically, we perform \textit{post-training with reweighted loss}, where the weighting accounts for the uncertainty quantile of safety instead of the original predicted safety score. This refinement improves the model's control performance while enhancing its adherence to safety constraints, leading to the generation of more optimal and safer control sequences. To further ensure the safety of PDE control, we introduce \textit{inference-time fine-tuning}, where the diffusion model is iteratively adjusted with guidance and fine-tuning based on specific control targets and uncertainty quantile to improve the safety of the final outputs within few iterations.

Our main contributions are as follows:
\textbf{(1)} We introduce safety constraints into deep learning-based control of PDE systems and propose our method \proj.
\textbf{(2)} To address significant distribution shifts, we quantify the safety uncertainty of diffusion models and design a conformal adaptation process, which incorporates post-training with a reweighted loss and inference-time fine-tuning to promote the output distribution becoming more optimal and safer. 
\textbf{(3)} We design safe control tasks in the contexts of the 1D Burgers' equation, 2D incompressible fluid, and the controlled nuclear fusion scenario, and evaluate various methods. Extensive experiments on these datasets demonstrate that \proj is the only method to successfully meet all safety constraints while achieving superior control objectives.









