\begin{algorithm}[ht]
    \small
    \caption{Algorithm of \proj}
    \label{alg_total}
    \begin{algorithmic}[1]
    \STATE \textbf{Require} Training set $D_{\textrm{train}}$, Calibration set $D_{\textrm{cal}}$, control targets\
    \STATE $\theta$ $\gets$ Pre-train with $\mathcal{L}_\textrm{diffusion}$ on $D_{\textrm{train}}$ \\
    \STATE $\theta$ $\gets$ Post-train with $\mathcal{L}_\textrm{post-train}$ on $D_{\textrm{train}}$ and $D_{\text{cal}}$ (Sec \ref{sec:posttrain}, Alg \ref{algpost}) \\
    \STATE $\theta, \w$ $\gets$ Fine-tune during guided inference with $\G$ and $\mathcal{L}_\textrm{fine-tune}$ on control targets and $D_{\textrm{cal}}$ (Sec \ref{sec:finetune}, Alg \ref{alg})
\hspace{0.8cm} \\
    \STATE \textbf{return} model parameters $\theta$ and control sequences $\w$
    \end{algorithmic}
\end{algorithm}

In this section, we introduce our proposed method \proj, with its overall framework outlined in Figure \ref{fig:overview} and Algorithm \ref{alg_total}. Firstly, in Section \ref{sec:conformal}, we propose the uncertainty quantile, which is the quantification of uncertainties. As shown in the middle of the figure, it is embedded throughout the algorithm, originating from the concern that the model's predictive uncertainty may cause a gap between the actual and predicted safety scores, potentially leading to unsafe events not anticipated by the model.

Secondly, in Section \ref{sec:posttrain} and \ref{sec:finetune}, we introduce the post-training and inference-time fine-tuning based on the uncertainty quantile, respectively. In the post-training phase following pre-training, we employ a reweighted loss function to guide the model's output distribution to favor regions with more optimal objectives and greater safety in general, after which the post-trained model can subsequently be used for specific control tasks. As for inference-time fine-tuning, we aim to make task-specific adjustments based on the post-trained model, enabling better control performance and safety guarantee for the specific control tasks with few iterations.

\subsection{Uncertainty Quantification of Diffusion Models}
\label{sec:conformal}

In offline safe control problems \cite{xu2022constraints, gu2022review}, the gap between pre-collected data and the target distribution exacerbates models' prediction errors, which can be critical in ensuring safety. To address this issue, we employ the conformal prediction technique to obtain uncertainty quantification for predicted safety, without requiring additional assumptions about the model and the data distribution.

The intuition behind the original conformal prediction \cite{vovk2005algorithmic} is to use the trained model on the calibration set to obtain a score set. The score set is then used to endow the model's prediction with a \emph{conformal (confidence) interval}. Under the assumption of exchangeability\footnote{Exchangeability means that exchanging examples in the calibration set and in inference does not alter their joint distribution.}, the actual prediction during inference will lie within the conformal interval with a guaranteed probability. The details of conformal prediction are provided in Appendix \ref{app:cp}. 

However, there is typically a distribution shift between the calibration set and the sequences generated by the diffusion model for control tasks during inference. Therefore, we account for this distribution shift, thereby introducing the \textit{shifted score set}. Based on it, we get the uncertainty quantification metric named \textit{uncertainty quantile} and finally the \textit{conformal interval}. The detailed description of this process is as follows.


\textbf{Calibration set and pre-training.} To achieve the goal mentioned above, we first set aside a portion of the original training data as the \textit{calibration set} \( D_{\text{cal}} \), which will be used later to estimate the model's prediction errors. The remaining data, which will be applied to operations that can alter the model’s outcomes, is referred to as the training data \( D_{\text{train}} \). After pre-training with \( D_{\text{train}} \) as described in Eq. \ref{eq:training_obj}, we get the diffusion model $p_\theta$ which models the joint distribution of $[\u,\w]$. 

\textbf{Shifted score set under distribution shift.} \textit{Score set}, obtained on the calibration set, is a set of model prediction uncertainties regarding the safety score $\score$, which is defined as 
\begin{equation}
    \S\coloneqq\{|\score(\pu(\w_i)) - \score(\u_i)|: (\u_i, \w_i)\in D_{\text{cal}}\}\cup\{\infty\}.
    \label{eq:scoreset}
\end{equation}
Here $\u_i$ and $\w_i$ are the system state trajectory and control signal sequence of the $i^\text{th}$ example, $\pu(\w)$ is the system state trajectory conditioned on control $\w_i$ as predicted by the model, where $\theta$ is the model parameter.

Furthermore, taking into account the distribution shift between the calibration set $D_\text{cal}$ and data generated based on control targets, we apply weighting to the score set $\S$ to get the \textit{shifted score set} as
\begin{equation}
\scalebox{1}{$
    \Tilde{\S}\coloneqq\{\omega_{\textrm{norm}}(\u_i,\w_i)\Delta\score_i: \Delta\score_i\in \S\}.
    \label{eq:weightedscore}
$}
\end{equation}
Intuitively, this weighting is because each sample in the calibration set has a different probability of appearing in the final data distribution generated by the model. According to conformal prediction under covariate shift \cite{Tibshirani2019ConformalPU}, here the calculation of the weights is as follows:
\begin{equation}\omega(\u_i,\w_i)\coloneqq\frac{\Tilde{p}(\u_i,\w_i)}{p(\u_i,\w_i)}
\end{equation}
where $p(\u, \w)$ is the probability density function of the calibration set and training set, and $\Tilde{p}(\u, \w)$ is the probability density function of the model-generated data during inference. $\omega_\textrm{norm}$ is the normalization of $\omega$ defined as
\begin{equation}
    \omega_\textrm{norm}(\u_i,\w_i)=\frac{\omega(\u_i,\w_i)}{\sum_{(\u_i, \w_i)\in D_{\text{cal}}}{\omega(\u_j,\w_j)}}.
\end{equation}
The specific calculation of $\omega_\textrm{norm}$ regarding \proj will be detailed in Section \ref{sec:posttrain}.

\textbf{Uncertainty quantile.} Based on the shifted score set $\Tilde{\S}$ (Eq. \ref{eq:weightedscore}), the \textit{uncertainty quantile} is $\text{Quantile}((1-\alpha)(1+\frac{1}{|D_\text{cal}|});\Tilde{\S})$\footnote{It means obtaining the element with $(1-\alpha)(1+\frac{1}{|D_\text{cal}|})$'th quantile from the shifted score set $\Tilde{\S}$.}, where $1-\alpha$ is the coverage probability and $|D_\text{cal}|$ is the cardinality of $D_\text{cal}$. The quantile represents the threshold below which a specific proportion of $\Tilde{\S}$ falls. The adjustment term $1+1/|D_\text{cal}|$ accounts for the finite size of the calibration set, ensuring valid coverage even with limited data. Hereafter, it is abbreviated as $Q(1-\alpha;\Tilde{\S})$.

We can then obtain the \textit{conformal interval} as:
\begin{align}
\notag
\textrm{CI}_\theta(1-\alpha,D_\textrm{cal})\coloneqq[\score(\pu(\w))-Q(1-\alpha;\Tilde{\S}), \\ 
\score(\pu(\w))+Q(1-\alpha;\Tilde{\S})]. 
\end{align}
With at least $1-\alpha$ probability, the true $\score$ is covered by this interval, which can be demonstrated through the following theorem.
\begin{theorem}
     Assume that samples in the calibration set $D_\textrm{cal}\sim p$ are independent, and the test set $(\u,\w)\sim \Tilde{p}$ is also independent with the calibration set. Assume $p$ is absolutely continuous with respect to $\Tilde{p}$, then
     \begin{align}
         \mathbb{P}(\score(\u)\in\textrm{CI}_\theta(1-\alpha,D_\textrm{cal})) \geq 1-\alpha.
     \end{align}
     (See Appendix \ref{app:theory} for proof.)
\end{theorem}

\subsection{Post-training with Reweighted Loss}
\label{sec:posttrain}

The aim of post-training is to transform the model’s distribution into $p^*(\u,\w)\propto p(\u,\w) \cdot e^{-\mathcal{W}(\u,\w)}$, where $\W(\u, \w)$ contains both the control objective $\J$ and the safety constraints involving the model's uncertainty. Specifically, it is formulated as:
\begin{equation}
\scalebox{0.97}{$ \mathcal{W}(\u,\w) = \max[\score(\u)+Q(1-\alpha;\Tilde{\S})-\score_0, 0] + \gamma\J(\u,\w), $}
    \label{eq:w}
\end{equation}
where $\gamma$ is the weight of $\J$.

So we propose the reweighted post-training loss as 
\begin{align}
&\mathcal{L_\textrm{post-train}}\coloneqq\mathbb{E}_{\t\sim U(1,\T),(\u,\w)\sim p(\u,\w),\bepsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})} \notag \\ 
&[e^{-\mathcal{W}(\u,\w)}\|\mathbf{\bepsilon} - \bepsilon_\theta(\sqrt{\bar{\alpha}_\t}[\u,\w] + \sqrt{1-\bar{\alpha}_\t} \bepsilon,\t)\|_2^2],
\label{eq:posttrain_obj}
\end{align}
which steers the generated data distribution towards high $e^{-\mathcal{W}}$ regions. Compared to the pre-training loss in Eq. \ref{eq:training_obj}, the samples are reweighted, assigning higher weights to those with lower $\mathcal{W}$. This loss function enables the model to adjust the generated data distribution to align with our target distribution, as described in the following theorem:
\begin{theorem}
    Trained with $\mathcal{L_\textrm{post-train}}$ in Eq. \ref{eq:posttrain_obj}, the diffusion model's distribution is $p^*(\u,\w)\propto p(\u,\w) e^{-\mathcal{W}(\u,\w)}.$\\
    (See Appendix \ref{app:theory} for proof.)
\end{theorem}

Specifically, in the calculation of the uncertainty quantile $Q$, $\omega(\u_i,\w_i)=Cp_\theta(\u_i,\w_i)e^{-\mathcal{W}(\u_i,\w_i)}/p(\u_i,\w_i)$. In practical implementation, we let 
\begin{align}
    \omega(\u_i,\w_i)=Ce^{-\mathcal{W}(\u_i,\w_i)},
\end{align}
and the constant $C$ will be eliminated during normalization. Through the following theorem, we demonstrate that when the KL divergence between $p$ and $p_\theta$ is small, this approximation does not affect the results of the uncertainty quantile.
\begin{theorem}
    If $D_{KL}(p||p_\theta)\leq\epsilon$, then with $1-C\sqrt{\epsilon}$ probability, $\textrm{Quantile}(1-\alpha;\sum_{i=1}^N w_{\textrm{\textrm{norm}},i}\delta_{v_i})=\textrm{Quantile}(1-\alpha;\sum_{i=1}^N \tilde{w}_{\textrm{norm},i}\delta_{v_i})$, where $w_{\textrm{norm},i}$ and $\tilde{w}_{\textrm{norm},i}$ are the normalized $w_i$ and $\Tilde{w}_i$, $w_i=p_\theta(v_i) e^{-\mathcal{G}(v_i)}/p(v_i)$, and $\tilde{w}_i=e^{-\mathcal{G}(v_i)}$.\\
    (See Appendix \ref{app:theory} for proof.)
\end{theorem}

The algorithm of post-training is provided in Algorithm \ref{algpost} in Appendix \ref{app:algpost}.

\subsection{Inference-time Fine-tuning}
\label{sec:finetune}

During inference, knowing the specific control tasks, we guide the model to further optimize the generation of control sequences tailored to these tasks. We cyclically use the guidance to generate samples and then fine-tune the model parameters with these samples, thereby iteratively improving the model's safety and performance. In this process, similar to post-training, we incorporate the uncertainty quantile $Q$ into both the guidance and fine-tuning loss to prevent violations of safety constraints caused by the model’s uncertainty.

\textbf{Guidance $\G$.} Guidance is the first approach we adopt to steer the model’s output toward satisfying both the control objectives and safety constraints. It plays a role during the sampling process of the diffusion model. The specific denoising step of implementing guidance follows Eq. \ref{eq:1ddpm_inference}, and the form of guidance still follows $\mathcal{W}$ in Eq. \ref{eq:w}, \emph{i.e.} 
\begin{align}
    \G(\u,\w) = \W(\u,\w),
    \label{eq:guidance}
\end{align}
incorporating the uncertainty quantile $Q$ to account for safety risks arising from uncertainty.

\textbf{Fine-tuning.} The second approach for adjusting the output data distribution of the model is fine-tuning, which achieves the adjustment by optimizing the model parameters $\theta$. Specifically, retaining the computation graph for all denoising steps with respect to $\theta$ would result in an unmanageable memory overhead. Therefore, when we need to keep the computation graph during denoising for gradient calculation, we only retain the computation graph of the final denoising step. To optimize the safety score and  objective simultaneously, we form the fine-tune loss as:
\begin{equation}
\begin{aligned}
    &\mathcal{L}_{\text{fine-tune}} = \sum_{(\u_\theta,\w_\theta)\in D_{\text{sampled}}}\W(\u_\theta,\w_\theta),
    \label{eq:finetune}
\end{aligned}
\end{equation}
where $\u_\theta$ and $\w_\theta$ are from $D_\text{sampled}$ sampled according to the guidance described above.

The entire algorithm of inference-time finetuning is provided in Algorithm \ref{alg} in Appendix \ref{app:alg}.

